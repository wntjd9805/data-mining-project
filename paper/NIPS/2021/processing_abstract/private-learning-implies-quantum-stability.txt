Learning an unknown n-qubit quantum state ρ is a fundamental challenge in quan-tum computing. Information-theoretically, it is known that tomography requires exponential in n many copies of ρ to estimate its entries. Motivated by learning the-ory, Aaronson et al. introduced many (weaker) learning models: the PAC model of learning states (Proc. of Royal Society A’07), shadow tomography (STOC’18) for learning “shadows" of a state, a model that also requires learners to be differentially private (STOC’19) and the online model of learning states (NeurIPS’18). In these models it was shown that ρ can be learned “approximately" using linear in n many copies of ρ. But is there any relationship between these models? In this paper we prove a sequence of (information-theoretic) implications from differentially-privatePAC learning to online learning and then to quantum stability. Our main result generalizes the recent work of Bun, Livni and Moran (Journal of the ACM’21) who showed that ﬁnite Littlestone dimension (of Boolean-valued concept classes) implies PAC learnability in the (approximate) differentially private (DP) setting.We ﬁrst extend their work to the real-valued setting, and further extend to the setting of learning quantum states. Key to our results is our generic quantum online learner, Robust Standard Optimal Algorithm (RSOA), which is robust to adversarial imprecision. We then show information-theoretic equivalences betweenDP learning quantum states in the PAC model, learnability of quantum states in the one-way communication model, online learning of quantum states, quantum stability, various combinatorial parameters and give further applications to gentle shadow tomography and noisy quantum state learning. 