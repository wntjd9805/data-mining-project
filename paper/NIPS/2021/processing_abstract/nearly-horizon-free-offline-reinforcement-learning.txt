We revisit ofﬂine reinforcement learning on episodic time-homogeneous MarkovDecision Processes (MDP). For tabular MDP with S states and A actions, or linearMDP with anchor points and feature dimension d, given the collected K episodes data with minimum visiting probability of (anchor) state-action pairs dm, we obtain nearly horizon H-free sample complexity bounds for ofﬂine reinforcement learning when the total reward is upper bounded by 1. Speciﬁcally:• For ofﬂine policy evaluation, we obtain an ˜O error bound for the plug-in estimator, which matches the lower bound up to logarithmic factors and does not have additional dependency on poly (H, S, A, d) in higher-order term. 1Kdm⇣q⌘ 1Kdm+ min(S,d)Kdm• For ofﬂine policy optimization, we obtain an ˜O sub-optimality gap for the empirical optimal policy, which approaches the lower bound up to logarithmic factors and a high-order term, improving upon the best known result by [1] that has additional poly (H, S, d) factors in the main term.To the best of our knowledge, these are the ﬁrst set of nearly horizon-free bounds for episodic time-homogeneous ofﬂine tabular MDP and linear MDP with anchor points. Central to our analysis is a simple yet effective recursion based method to bound a “total variance” term in the ofﬂine scenarios, which could be of individual interest.⇣q⌘ 