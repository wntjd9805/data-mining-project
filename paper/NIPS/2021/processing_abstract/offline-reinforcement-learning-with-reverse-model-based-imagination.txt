In ofﬂine reinforcement learning (ofﬂine RL), one of the main challenges is to deal with the distributional shift between the learning policy and the given dataset. To address this problem, recent ofﬂine RL methods attempt to introduce conservatism bias to encourage learning in high-conﬁdence areas. Model-free approaches directly encode such bias into policy or value function learning using conservative regular-izations or special network structures, but their constrained policy search limits the generalization beyond the ofﬂine dataset. Model-based approaches learn forward dynamics models with conservatism quantiﬁcations and then generate imaginary trajectories to extend the ofﬂine datasets. However, due to limited samples in of-ﬂine datasets, conservatism quantiﬁcations often suffer from overgeneralization in out-of-support regions. The unreliable conservative measures will mislead forward model-based imaginations to undesired areas, leading to overaggressive behav-iors. To encourage more conservatism, we propose a novel model-based ofﬂineRL framework, called Reverse Ofﬂine Model-based Imagination (ROMI). We learn a reverse dynamics model in conjunction with a novel reverse policy, which can generate rollouts leading to the target goal states within the ofﬂine dataset.These reverse imaginations provide informed data augmentation for model-free policy learning and enable conservative generalization beyond the ofﬂine dataset.ROMI can effectively combine with off-the-shelf model-free algorithms to enable model-based generalization with proper conservatism. Empirical results show that our method can generate more conservative behaviors and achieve state-of-the-art performance on ofﬂine RL benchmark tasks. 