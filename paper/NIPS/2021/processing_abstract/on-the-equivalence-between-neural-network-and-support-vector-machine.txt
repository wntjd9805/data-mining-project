Recent research shows that the dynamics of an inﬁnitely wide neural network (NN) trained by gradient descent can be characterized by Neural Tangent Kernel (NTK)[27]. Under the squared loss, the inﬁnite-width NN trained by gradient descent with an inﬁnitely small learning rate is equivalent to kernel regression with NTK[4]. However, the equivalence is only known for ridge regression currently [6], while the equivalence between NN and other kernel machines (KMs), e.g. support vector machine (SVM), remains unknown. Therefore, in this work, we propose to establish the equivalence between NN and SVM, and speciﬁcally, the inﬁnitely wide NN trained by soft margin loss and the standard soft margin SVM with NTK trained by subgradient descent. Our main theoretical results include establishing the equivalence between NN and a broad family of `2 regularized KMs with ﬁnite-width bounds, which cannot be handled by prior work, and showing that everyﬁnite-width NN trained by such regularized loss functions is approximately a KM.Furthermore, we demonstrate our theory can enable three practical applications, including (i) non-vacuous generalization bound of NN via the correspondingKM; (ii) nontrivial robustness certiﬁcate for the inﬁnite-width NN (while existing robustness veriﬁcation methods would provide vacuous bounds); (iii) intrinsically more robust inﬁnite-width NNs than those from previous kernel regression. 