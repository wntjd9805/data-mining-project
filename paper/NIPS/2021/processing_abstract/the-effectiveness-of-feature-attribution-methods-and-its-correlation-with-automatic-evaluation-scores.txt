Explaining the decisions of an Artiﬁcial Intelligence (AI) model is increasingly critical in many real-world, high-stake applications. Hundreds of papers have either proposed new feature attribution methods, discussed or harnessed these tools in their work. However, despite humans being the target end-users, most attribution methods were only evaluated on proxy automatic-evaluation metrics[60, 78, 80]. In this paper, we conduct the ﬁrst user study to measure attribution map effectiveness in assisting humans in ImageNet classiﬁcation and StanfordDogs ﬁne-grained classiﬁcation, and when an image is natural or adversarial (i.e. contains adversarial perturbations). Overall, feature attribution is surprisingly not more effective than showing humans nearest training-set examples. On a harder task of ﬁne-grained dog categorization, presenting attribution maps to humans does not help, but instead hurts the performance of human-AI teams compared toAI alone. Importantly, we found automatic attribution-map evaluation measures to correlate poorly with the actual human-AI team performance. Our ﬁndings encourage the community to rigorously test their methods on the downstream human-in-the-loop applications and to rethink the existing evaluation metrics. 