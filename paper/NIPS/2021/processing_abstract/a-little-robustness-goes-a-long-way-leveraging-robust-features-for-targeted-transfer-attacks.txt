Adversarial examples for neural network image classiﬁers are known to be trans-ferable: examples optimized to be misclassiﬁed by a source classiﬁer are often misclassiﬁed as well by classiﬁers with different architectures. However, targeted adversarial examples—optimized to be classiﬁed as a chosen target class—tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classiﬁer. Here, we show that train-ing the source classiﬁer to be “slightly robust”—that is, robust to small-magnitude adversarial examples—substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as dif-ferent as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called “robust” classiﬁers. 