Most existing temporal action localization (TAL) methods rely on a transfer learn-ing pipeline, ﬁrst optimizing a video encoder on a large action classiﬁcation dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task dis-crepancy problem for the video encoder – trained for action classiﬁcation, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-ﬁdelity (LoFi) video encoder optimization method. Instead of always using the full training conﬁgurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes oper-able under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to ﬂow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can signiﬁcantly enhance the perfor-mance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical ﬂow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-ﬁ/loﬁ_action_localization . 