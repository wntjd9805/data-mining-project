Even when unable to run experiments, practitioners can evaluate prospective poli-cies, using previously logged data. However, while the bandits literature has adopted a diverse set of objectives, most research on off-policy evaluation to date focuses on the expected reward. In this paper, we introduce Lipschitz risk func-tionals, a broad class of objectives that subsumes conditional value-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT risks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework that ﬁrst estimates a tar-get policy’s CDF and then generates plugin estimates for any collection of Lipschitz risks, providing ﬁnite sample guarantees that hold simultaneously over the entire class. We instantiate OPRA with both importance sampling and doubly robust esti-mators. Our primary theoretical contributions are (i) the ﬁrst uniform concentration inequalities for both CDF estimators in contextual bandits and (ii) error bounds on our Lipschitz risk estimates, which all converge at a rate of O(1/ n).√ 