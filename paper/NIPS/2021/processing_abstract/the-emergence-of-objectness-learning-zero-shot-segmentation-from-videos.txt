Humans can easily segment moving objects without knowing what they are. That objectness could emerge from continuous visual observations motivates us to model segmentation and movement concurrently from unlabeled videos. Our premise is that a video contains different views of the same scene related by moving components, and the right region segmentation and region ﬂow allow view synthesis which can be checked on the data itself without any external supervision.Our model ﬁrst deconstructs video frames in two separate pathways: an appearance pathway that outputs feature-based region segmentation for a single image, and a motion pathway that outputs motion features for a pair of images. It then binds them in a conjoint region ﬂow feature representation and predicts segment ﬂow that provides a gross characterization of moving regions for the entire scene. By training the model to minimize view synthesis errors based on segment ﬂow, our appearance and motion pathways learn region segmentation and ﬂow estimation automatically without building them up from low-level edges or optical ﬂow respectively.Our model demonstrates the surprising emergence of objectness in the appearance pathway, surpassing prior works on 1) zero-shot object segmentation from a single image, 2) moving object segmentation from a video with unsupervised test-time adaptation, and 3) semantic image segmentation with supervised ﬁne-tuning. Our work is the ﬁrst truly end-to-end learned zero-shot object segmentation model from unlabeled videos. It not only develops generic objectness for segmentation and tracking, but also outperforms image-based contrastive representation learning without augmentation engineering. 