Transformers have recently gained increasing attention in computer vision. How-ever, existing studies mostly use Transformers for feature representation learning, e.g. for image classiﬁcation and dense predictions, and the generalizability ofTransformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of im-ages. We ﬁnd that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatena-tion in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simpliﬁed decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Addition-ally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simpliﬁed decoder is computationally more efﬁcient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identiﬁcation, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv. 