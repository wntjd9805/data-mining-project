Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision trans-formers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Speciﬁcally, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a ﬁne-grained representation in discriminative regions while keeping high efﬁciency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computa-tional complexity by 40%-60% while maintaining comparable performance on image classiﬁcation. Extensive experiments on object detection and segmenta-tion further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack. 