Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a ﬁxed number of patches, each of which is treated as a token. Generally, representing an image with more to-kens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the to-ken number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of “easy” images which can be accurately predicted with a mere number of 4x4 tokens, while only a small frac-tion of “hard” ones need a ﬁner representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically conﬁgure a proper num-ber of tokens for each input image. This is achieved by cascading multipleTransformers with increasing numbers of tokens, which are sequentially acti-vated in an adaptive fashion at test time, i.e., the inference is terminated once a sufﬁciently conﬁdent prediction is produced. We further design efﬁcient fea-ture reuse and relationship reuse mechanisms across different components of theDynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method signiﬁcantly outperforms the competitive baselines in terms of both theoretical computational efﬁciency and practical inference speed. Code and pre-trained mod-els (based on PyTorch and MindSpore) are available at https://github.com/ blackfeather-wang/Dynamic-Vision-Transformer and https://github. com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore. 