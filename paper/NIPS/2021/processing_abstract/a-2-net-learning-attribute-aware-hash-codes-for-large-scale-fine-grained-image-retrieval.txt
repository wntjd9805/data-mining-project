Our work focuses on tackling large-scale ﬁne-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the ﬁne-grained details in the query. It is desirable to alleviate the challenges of both ﬁne-grained nature of small inter-class variations with large intra-class variations and explosive growth of ﬁne-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A2-NET) for generating attribute-aware hash codes to not only make the retrieval process efﬁcient, but also establish explicit correspondences between hash codes and visual attributes. Speciﬁcally, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsu-pervisedly distill high-level attribute-speciﬁc vectors from the appearance-speciﬁc visual representations without attribute annotations. A2-NET is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their rep-resentation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on ﬁve benchmark ﬁne-grained datasets show our superiority over competing methods.More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of ﬁne-grained objects. 