Modelers use automatic differentiation (AD) of computation graphs to imple-ment complex deep learning models without deÔ¨Åning gradient computations.Stochastic AD extends AD to stochastic computation graphs with sampling steps, which arise when modelers handle the intractable expectations common in re-inforcement learning and variational inference. However, current methods for stochastic AD are limited: They are either only applicable to continuous ran-dom variables and differentiable functions, or can only use simple but high vari-ance score-function estimators. To overcome these limitations, we introduceStorchastic, a new framework for AD of stochastic computation graphs. Stor-chastic allows the modeler to choose from a wide variety of gradient estimation methods at each sampling step, to optimally reduce the variance of the gradi-ent estimates. Furthermore, Storchastic is provably unbiased for estimation of any-order gradients, and generalizes variance reduction techniques to any-order derivative estimates. Finally, we implement Storchastic as a PyTorch library at github.com/HEmile/storchastic. 