Domain adaptation (DA) attempts to transfer the knowledge from a labeled source domain to an unlabeled target domain that follows different distribution from the source. To achieve this, DA methods include a source classiﬁcation objective LS to extract the source knowledge and a domain alignment objective LD to diminish the domain shift, ensuring knowledge transfer. Typically, former DA methods adopt some weight hyper-parameters to linearly combine the training objectives to form an overall objective L. However, the gradient directions of these objectives may conﬂict with each other due to domain shift. Under such circumstances, the linear optimization scheme might decrease the overall objective value at the expense of damaging one of the training objectives, leading to restricted solutions. In this paper, we rethink the optimization scheme for DA from a gradient-based perspec-tive. We propose a Pareto Domain Adaptation (ParetoDA) approach to control the overall optimization direction, aiming to cooperatively optimize all training objectives. Speciﬁcally, to reach a desirable solution on the target domain, we de-sign a surrogate loss mimicking target classiﬁcation. To improve target-prediction accuracy to support the mimicking, we propose a target-prediction reﬁning mecha-nism which exploits domain labels via Bayes’ theorem. On the other hand, since prior knowledge of weighting schemes for objectives is often unavailable to guide optimization to approach the optimal solution on the target domain, we propose a dynamic preference mechanism to dynamically guide our cooperative optimization by the gradient of the surrogate loss on a held-out unlabeled target dataset. Our theoretical analyses show that the held-out data can guide but will not be over-ﬁtted by the optimization. Extensive experiments on image classiﬁcation and semantic segmentation benchmarks demonstrate the effectiveness of ParetoDA. Our code is available at https://github.com/BIT-DA/ParetoDA. 