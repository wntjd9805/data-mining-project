We propose a framework which makes it feasible to directly train deep neural networks with respect to popular families of task-speciﬁc non-decomposable per-formance measures such as AUC, multi-class AUC, F -measure and others. A feature of the optimization model that emerges from these tasks is that it involves solving a Linear Programs (LP) during training where representations learned by upstream layers characterize the constraints or the feasible set. The constraint ma-trix is not only large but the constraints are also modiﬁed at each iteration. We show how adopting a set of ingenious ideas proposed by Mangasarian for 1-normSVMs – which advocates for solving LPs with a generalized Newton method – provides a simple and effective solution that can be run on the GPU. In particu-lar, this strategy needs little unrolling, which makes it more efﬁcient during the backward pass. Further, even when the constraint matrix is too large to ﬁt on theGPU memory (say large minibatch settings), we show that running the Newton method in a lower dimensional space yields accurate gradients for training, by utilizing a statistical concept called sufﬁcient dimension reduction. While a num-ber of specialized algorithms have been proposed for the models that we describe here, our module turns out to be applicable without any speciﬁc adjustments or relaxations. We describe each use case, study its properties and demonstrate the efﬁcacy of the approach over alternatives which use surrogate lower bounds and often, specialized optimization schemes. Frequently, we achieve superior compu-tational behavior and performance improvements on common datasets used in the literature. 