In this paper, we apply the self-attention from the state-of-the-art Transformer inAttention Is All You Need [88] for the ﬁrst time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efﬁcacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the ﬁrst time that the softmax normalization in the scaled dot-product attention is sufﬁcient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projec-tion is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnor-malized data. Finally, we present three operator learning experiments, including the viscid Burgers’ equation, an interface Darcy ﬂow, and an inverse interface coefﬁcient identiﬁcation problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows signiﬁcant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts. 