The quadratic computational and memory complexities of the Transformer’s at-tention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear uniﬁed nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a ﬁxed length as input and an additional corresponding output, which allowsLuna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efﬁciency of Luna compared to a variety of strong baseline methods including the full-rank attention and other efﬁcient sparse and dense attention methods. The implementation of our model is available at https://github.com/XuezheMax/fairseq-apollo. 