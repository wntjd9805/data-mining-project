Deep learning systems frequently fail at out-of-context (OOC) prediction, the prob-lem of making reliable predictions on uncommon or unusual inputs or subgroups of the training distribution. To this end, a number of benchmarks for measuringOOC performance have been recently introduced. In this work, we introduce a framework unifying the literature on OOC performance measurement, and demon-strate how rich auxiliary information can be leveraged to identify candidate sets of OOC examples in existing datasets. We present NOOCH: a suite of naturally-occurring “challenge sets”, and show how varying notions of context can be used to probe speciﬁc OOC failure modes. Experimentally, we explore the tradeoffs between various learning approaches on these challenge sets and demonstrate how the choices made in designing OOC benchmarks can yield varying conclusions. 