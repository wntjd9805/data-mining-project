Since its introduction a decade ago, relative entropy policy search (REPS) has demonstrated successful policy learning on a number of simulated and real-world robotic domains, not to mention providing algorithmic components used by many recently proposed reinforcement learning (RL) algorithms. While REPS is well-known in the community, there exist no guarantees on its performance when using stochastic, gradient-based solvers. In this paper we aim to ﬁll this gap by providing guarantees and convergence rates for the sub-optimality of a policy learned usingﬁrst-order optimization methods applied to the REPS objective. We ﬁrst consider the setting in which we are given access to exact gradients and demonstrate how near-optimality of the objective translates to near-optimality of the policy. We then consider the setting of stochastic gradients and introduce a technique that uses generative access to the underlying Markov decision process to compute parameter updates that maintain favorable convergence to the optimal regularized policy.