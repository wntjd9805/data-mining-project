The mean ﬁeld theory of multilayer neural networks centers around a particu-lar inﬁnite-width scaling, in which the learning dynamics is shown to be closely tracked by the mean ﬁeld limit. A random ﬂuctuation around this inﬁnite-width limit is expected from a large-width expansion to the next order. This ﬂuctuation has been studied only in the case of shallow networks, where previous works em-ploy heavily technical notions or additional formulation ideas amenable only to that case. Treatment of the multilayer case has been missing, with the chief difﬁ-culty in ﬁnding a formulation that must capture the stochastic dependency across not only time but also depth.In this work, we initiate the study of the ﬂuctuation in the case of multilayer net-works, at any network depth. Leveraging on the neuronal embedding framework recently introduced by Nguyen and Pham [17], we systematically derive a system of dynamical equations, called the second-order mean ﬁeld limit, that captures the limiting ﬂuctuation distribution. We demonstrate through the framework the com-plex interaction among neurons in this second-order mean ﬁeld limit, the stochas-ticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting ﬂuctuation. A limit theorem is proven to relate quantitatively this limit to the ﬂuctuation realized by large-width networks.We apply the result to show a stability property of gradient descent mean ﬁeld training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with “minimal ﬂuctuation” (in fact, vanishing ﬂuctua-tion) in the learned output function, even after the network has been initialized at or has converged (sufﬁciently fast) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayer networks with a loss func-tion that is not necessarily convex in a more general setting. 