Graph Contrastive Learning (GCL) has emerged to learn generalizable representa-tions from contrastive views. However, it is still in its infancy with two concerns: 1) changing the graph structure through data augmentation to generate contrastive views may mislead the message passing scheme, as such graph changing action de-prives the intrinsic graph structural information, especially the directional structure in directed graphs; 2) since GCL usually uses predeﬁned contrastive views with hand-picking parameters, it does not take full advantage of the contrastive informa-tion provided by data augmentation, resulting in incomplete structure information for models learning. In this paper, we design a directed graph data augmentation method called Laplacian perturbation and theoretically analyze how it provides contrastive information without changing the directed graph structure. Moreover, we present a directed graph contrastive learning framework, which dynamically learns from all possible contrastive views generated by Laplacian perturbation.Then we train it using multi-task curriculum learning to progressively learn from multiple easy-to-difﬁcult contrastive views. We empirically show that our model can retain more structural features of directed graphs than other GCL models because of its ability to provide complete contrastive information. Experiments on various benchmarks reveal our dominance over the state-of-the-art approaches. 