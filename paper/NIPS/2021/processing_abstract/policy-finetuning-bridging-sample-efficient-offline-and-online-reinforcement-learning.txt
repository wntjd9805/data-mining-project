Recent theoretical work studies sample-efﬁcient reinforcement learning (RL) ex-tensively in two settings: learning interactively in the environment (online RL), or learning from an ofﬂine dataset (ofﬂine RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy ﬁnetuning, that is, online RL where the learner has additional access to a “reference policy” µ close to the optimal policy π(cid:63) in a certain sense. We con-sider the policy ﬁnetuning problem in episodic Markov Decision Processes (MDPs) with S states, A actions, and horizon length H. We ﬁrst design a sharp ofﬂine reduction algorithm—which simply executes µ and runs ofﬂine policy optimization on the collected dataset—that ﬁnds an ε near-optimal policy within (cid:101)O(H 3SC (cid:63)/ε2) episodes, where C (cid:63) is the single-policy concentrability coefﬁcient between µ andπ(cid:63). This ofﬂine result is the ﬁrst that matches the sample complexity lower bound in this setting, and resolves a recent open question in ofﬂine RL. We then establish an Ω(H 3S min{C (cid:63), A}/ε2) sample complexity lower bound for any policy ﬁne-tuning algorithm, including those that can adaptively explore the environment. This implies that—perhaps surprisingly—the optimal policy ﬁnetuning algorithm is either ofﬂine reduction or a purely online RL algorithm that does not use µ. Finally, we design a new hybrid ofﬂine/online algorithm for policy ﬁnetuning that achieves better sample complexity than both vanilla ofﬂine reduction and purely online RL algorithms, in a relaxed setting where µ only satisﬁes concentrability partially up to a certain time step. Overall, our results offer a quantitative understanding on the beneﬁt of a good reference policy, and make a step towards bridging ofﬂine and online RL. 