Numerous recent works show that overparameterization implicitly reduces variance for min-norm interpolators and max-margin classiﬁers. These ﬁndings suggest that ridge regularization has vanishing beneﬁts in high dimensions. We challenge this narrative by showing that, even in the absence of noise, avoiding interpolation through ridge regularization can signiﬁcantly improve generalization. We prove this phenomenon for the robust risk of both linear regression and classiﬁcation, and hence provide the ﬁrst theoretical result on robust overﬁtting. 