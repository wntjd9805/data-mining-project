Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than ConvolutionsNeural Networks (CNNs). Nonetheless, surprisingly, we ﬁnd these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks.In this paper, we aim to provide the ﬁrst fair & in-depth comparisons betweenTransformers and CNNs, focusing on robustness evaluations.With our uniﬁed training setup, we ﬁrst challenge the previous belief thatTransformers outshine CNNs when measuring adversarial robustness. More surprisingly, we ﬁnd CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers’ training recipes.While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enablingTransformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely beneﬁted by the Transformer’s self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs. 