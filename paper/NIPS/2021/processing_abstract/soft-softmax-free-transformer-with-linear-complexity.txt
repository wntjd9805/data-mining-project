Vision transformers (ViTs) have pushed the state-of-the-art for various visual recog-nition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural LanguageProcessing. However, an in-depth analysis in this work shows that they are either theoretically ﬂawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Speciﬁcally, conventional self-attention is computed by normaliz-ing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the ﬁrst time, a softmax-free transformer or SOFT is proposed. To remove soft-max in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention ma-trix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that ourSOFT signiﬁcantly improves the computational efﬁciency of existing ViT variants.Crucially, with a linear complexity, much longer token sequences are permitted inSOFT, resulting in superior trade-off between accuracy and complexity. 