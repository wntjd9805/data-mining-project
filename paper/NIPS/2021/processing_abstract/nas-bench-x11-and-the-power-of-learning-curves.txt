While early research in neural architecture search (NAS) required extreme compu-tational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-ﬁdelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate bench-marks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the ﬁnal validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-ﬁdelity algorithms, showing that it leads to improvements over popular single-ﬁdelity algo-rithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11. 