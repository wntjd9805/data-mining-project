In computer vision and natural language processing, innovations in model architec-ture that increase model capacity have reliably translated into gains in performance.In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algo-rithms often use small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overﬁtting; however, this hypothesis is untested.In this paper we investigate how RL agents are affected by exchanging the smallMLPs with larger modern networks with skip connections and normalization, fo-cusing speciﬁcally on actor-critic algorithms. We empirically verify that naïvely adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that instability from taking gradients through the critic is the culprit. We demonstrate that spectral normalization (SN) can mitigate this issue and enable stable training with large modern architectures. After smoothing with SN, larger models yield signiﬁcant performance improvements — suggesting that more “easy” gains may be had by focusing on model architectures in addition to algorithmic innovations. 