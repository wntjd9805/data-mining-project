Recent works have suggested that ﬁnite Bayesian neural networks may sometimes outperform their inﬁnite cousins because ﬁnite networks can ﬂexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of ﬁnite networks differ from the ﬁxed representations of inﬁnite networks remains incomplete. Perturbative ﬁnite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading ﬁnite-width corrections to the average feature kernels for any Bayesian network with linear readout and Gaussian likelihood have a largely universal form. We illustrate this explicitly for three tractable network architectures: deep linear fully-connected and convolutional networks, and networks with a single nonlinear hidden layer.Our results begin to elucidate how task-relevant learning signals shape the hidden layer representations of wide Bayesian neural networks. 