Learning a sequence of tasks without access to i.i.d. observations is a widely studied form of continual learning (CL) that remains challenging. In principle, Bayesian learning directly applies to this setting, since recursive and one-off Bayesian up-dates yield the same result. In practice, however, recursive updating often leads to poor trade-off solutions across tasks because approximate inference is nec-essary for most models of interest. Here, we describe an alternative Bayesian approach where task-conditioned parameter distributions are continually inferred from data. We offer a practical deep learning implementation of our framework based on probabilistic task-conditioned hypernetworks, an approach we term poste-rior meta-replay. Experiments on standard benchmarks show that our probabilistic hypernetworks compress sequences of posterior parameter distributions with virtu-ally no forgetting. We obtain considerable performance gains compared to existingBayesian CL methods, and identify task inference as our major limiting factor.This limitation has several causes that are independent of the considered sequential setting, opening up new avenues for progress in CL. 