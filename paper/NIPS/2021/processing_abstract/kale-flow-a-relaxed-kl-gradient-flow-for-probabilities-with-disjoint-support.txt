We study the gradient ﬂow for a relaxed approximation to the Kullback-Leibler (KL) divergence between a moving source and a ﬁxed target distribution. This approximation, termed the KALE (KL Approximate Lower bound Estimator), solves a regularized version of the Fenchel dual problem deﬁning the KL over a restricted class of functions. When using a Reproducing Kernel Hilbert Space (RKHS) to deﬁne the function class, we show that the KALE continuously inter-polates between the KL and the Maximum Mean Discrepancy (MMD). Like theMMD and other Integral Probability Metrics, the KALE remains well-deﬁned for mutually singular distributions. Nonetheless, the KALE inherits from the limitingKL a greater sensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the KALE gradient ﬂow particularly well suited when the target distribution is supported on a low-dimensional manifold.Under an assumption of sufﬁcient smoothness of the trajectories, we show the global convergence of the KALE ﬂow. We propose a particle implementation of the ﬂow given initial samples from the source and the target distribution, which we use to empirically conﬁrm the KALE’s properties. 