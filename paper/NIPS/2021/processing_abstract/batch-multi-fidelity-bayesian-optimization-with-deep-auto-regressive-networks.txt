Bayesian optimization (BO) is a powerful approach for optimizing black-box, expensive-to-evaluate functions. To enable a ﬂexible trade-off between the cost and accuracy, many applications allow the function to be evaluated at differentﬁdelities. In order to reduce the optimization cost while maximizing the beneﬁt-cost ratio, in this paper we propose Batch Multi-ﬁdelity Bayesian Optimization with Deep Auto-Regressive Networks (BMBO-DARN). We use a set of Bayesian neural networks to construct a fully auto-regressive model, which is expressive enough to capture strong yet complex relationships across all the ﬁdelities, so as to improve the surrogate learning and optimization performance. Furthermore, to enhance the quality and diversity of queries, we develop a simple yet efﬁcient batch querying method, without any combinatorial search over the ﬁdelities. We propose a batch acquisition function based on Max-value Entropy Search (MES) principle, which penalizes highly correlated queries and encourages diversity. We use posterior samples and moment matching to fulﬁll efﬁcient computation of the acquisition function, and conduct alternating optimization over every ﬁdelity-input pair, which guarantees an improvement at each step. We demonstrate the advantage of our approach on four real-world hyperparameter optimization applications. 