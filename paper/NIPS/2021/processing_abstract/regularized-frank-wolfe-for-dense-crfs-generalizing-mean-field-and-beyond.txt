We introduce regularized Frank-Wolfe, a general and effective algorithm for in-ference and learning of dense conditional random ﬁelds (CRFs). The algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which are equivalent to minimizing a regularized energy function. Our proposed method is a generalization of existing algorithms such as mean ﬁeld or concave-convex procedure. This perspective not only offers a uniﬁed analysis of these algorithms, but also allows an easy way of exploring different variants that potentially yield better performance. We illustrate this in our empirical results on standard semantic segmentation datasets, where several instantiations of our regularized Frank-Wolfe outperform mean ﬁeld infer-ence, both as a standalone component and as an end-to-end trainable layer in a neural network. We also show that dense CRFs, coupled with our new algorithms, produce signiﬁcant improvements over strong CNN baselines. 