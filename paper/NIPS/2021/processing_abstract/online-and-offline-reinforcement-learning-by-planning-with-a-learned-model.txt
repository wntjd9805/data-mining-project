Learning efﬁciently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the ofﬂine case when learning from a ﬁxed dataset. However, to date no single uniﬁed algorithm has demonstrated state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efﬁcient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of ofﬂine Reinforcement Learning (ofﬂine RL). Combining Reanalyse with theMuZero algorithm, we introduce MuZero Unplugged, a single uniﬁed algorithm for any data budget, including ofﬂine RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or ofﬂine RL settings.MuZero Unplugged sets new state-of-the-art results in the RL Unplugged ofﬂineRL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting. 