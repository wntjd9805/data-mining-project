Consider the sequential optimization of a continuous, possibly non-convex, and expensive to evaluate objective function f . The problem can be cast as a GaussianProcess (GP) bandit where f lives in a reproducing kernel Hilbert space (RKHS).The state of the art analysis of several learning algorithms shows a signiﬁcant gap between the lower and upper bounds on the simple regret performance. When N is the number of exploration trials and γN is the maximal information gain, we prove an ˜O((cid:112)γN /N ) bound on the simple regret performance of a pure exploration algorithm that is signiﬁcantly tighter than the existing bounds. We show that this bound is order optimal up to logarithmic factors for the cases where a lower bound on regret is known. To establish these results, we prove novel and sharp conﬁdence intervals for GP models applicable to RKHS elements which may be of broader interest. 