In this paper, we provide theoretical results of estimation bounds and excess risk upper bounds for support vector machine (SVM) with sparse multi-kernel representation. These convergence rates for multi-kernel SVM are established by analyzing a Lasso-type regularized learning scheme within composite multi-kernel spaces. It is shown that the oracle rates of convergence of classiﬁers depend on the complexity of multi-kernels, the sparsity, a Bernstein condition and the sample size, which signiﬁcantly improve on previous results even for the additive or linear cases. In summary, this paper not only provides uniﬁed theoretical results for multi-kernel SVMs, but also enriches the literature on high-dimensional nonparametric classiﬁcation. 