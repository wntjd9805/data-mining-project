We consider the problem of ofﬂine reinforcement learning (RL) — a well-motivated setting of RL that aims at policy optimization using only historical data. Despite its wide applicability, theoretical understandings of ofﬂine RL, such as its optimal sample complexity, remain largely open even in basic settings such as tabularMarkov Decision Processes (MDPs). In this paper, we propose Off-Policy DoubleVariance Reduction (OPDVR), a new variance reduction based algorithm for ofﬂineRL. Our main result shows that OPDVR provably identiﬁes an (cid:15)-optimal policy with (cid:101)O(H 2/dm(cid:15)2) episodes of ofﬂine data in the ﬁnite-horizon stationary transition setting, where H is the horizon length and dm is the minimal marginal state-action distribution induced by the behavior policy. This improves over the best known upper bound by a factor of H. Moreover, we establish an information-theoretic lower bound of Ω(H 2/dm(cid:15)2) which certiﬁes that OPDVR is optimal up to logarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal sample complexity under alternative settings such as the ﬁnite-horizon MDPs with non-stationary transitions and the inﬁnite horizon MDPs with discounted rewards. 