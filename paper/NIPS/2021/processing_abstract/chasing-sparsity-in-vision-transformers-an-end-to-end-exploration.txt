Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacriﬁcing the achievable accuracy. We carry out the ﬁrst-of-its-kind comprehensive exploration, on taking a uniﬁed approach of integrating sparsity inViTs “from end to end”. Speciﬁcally, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a ﬁxed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the ﬁnal output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads insideViTs. We further co-explore data and architecture sparsity for additional efﬁciency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain signiﬁcantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we ﬁnd that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing “free lunch”. For example, our sparsiﬁed DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32%FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE. 