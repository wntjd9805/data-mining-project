Sequence-to-Sequence (Seq2Seq) neural text generation models, especially the pre-trained ones (e.g., BART and T5), have exhibited compelling performance on various natural language generation tasks. However, the black-box nature of these models limits their application in tasks where speciﬁc rules (e.g., control-lable constraints, prior knowledge) need to be executed. Previous works either design speciﬁc model structures (e.g., Copy Mechanism corresponding to the rule“the generated output should include certain words in the source input”) or imple-ment specialized inference algorithms (e.g., Constrained Beam Search) to execute particular rules through the text generation. These methods require the careful design case-by-case and are difﬁcult to support multiple rules concurrently. In this paper, we propose a novel module named Neural Rule-Execution TrackingMachine (NRETM) that can be equipped into various transformer-based generators to leverage multiple rules simultaneously to guide the neural generation model for superior generation performance in an uniﬁed and scalable way. Extensive experiments on several benchmarks verify the effectiveness of our proposed model in both controllable and general text generation tasks. 