In reinforcement learning, a promising direction to avoid online trial-and-error costs is learning from an ofﬂine dataset. Current ofﬂine reinforcement learning methods commonly learn in the policy space constrained to in-support regions by the ofﬂine dataset, in order to ensure the robustness of the outcome policies.Such constraints, however, also limit the potential of the outcome policies. In this paper, to release the potential of ofﬂine policy learning, we investigate the decision-making problems in out-of-support regions directly and propose ofﬂineModel-based Adaptable Policy LEarning (MAPLE). By this approach, instead of learning in in-support regions, we learn an adaptable policy that can adapt its behavior in out-of-support regions when deployed. We conduct experiments onMuJoCo controlling tasks with ofﬂine datasets. The results show that the proposed method can make robust decisions in out-of-support regions and achieve better performance than SOTA algorithms. 