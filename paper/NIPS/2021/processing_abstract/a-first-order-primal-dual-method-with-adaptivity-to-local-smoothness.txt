(cid:105) −Ax, y (cid:104)We consider the problem of ﬁnding a saddle point for the convex-concave objective g∗(y), where f is a convex function with locally minx maxy f (x) +Lipschitz gradient and g is convex and possibly non-smooth. We propose an adaptive version of the Condat-V˜u algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity and the norm of recently computed gradients through a simple rule involving (k−1) ergodic convergence rate. of f . Under standard assumptions, we prove anFurthermore, when f is also locally strongly convex and A has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm.OA (cid:107) (cid:107) 