We investigate a low-rank model of quadratic classiﬁcation inspired by previous work on factorization machines, polynomial networks, and capsule-based architec-tures for visual object recognition. The model is parameterized by a pair of afﬁne transformations, and it classiﬁes examples by comparing the magnitudes of vectors that these transformations produce. The model is also over-parameterized in the sense that different pairs of afﬁne transformations can describe classiﬁers with the same decision boundary and conﬁdence scores. We show that such pairs arise from discrete and continuous symmetries of the model’s parameter space: in particular, the latter deﬁne symmetry groups of rotations and Lorentz transformations, and we use these group structures to devise appropriately invariant procedures for model alignment and averaging. We also leverage the form of the model’s decision boundary to derive simple margin-based updates for online learning. Here we explore a strategy of passive-aggressive learning: for each example, we compute the minimum change in parameters that is required to predict its correct label with high conﬁdence. We derive these updates by solving a quadratically constrained quadratic program (QCQP); interestingly, this QCQP is nonconvex but tractable, and it can be solved efﬁciently by elementary methods. We highlight the concep-tual and practical contributions of this approach. Conceptually, we show that it extends the paradigm of passive-aggressive learning to a larger family of nonlinear models for classiﬁcation. Practically, we show that these models perform well on large-scale problems in online learning. 