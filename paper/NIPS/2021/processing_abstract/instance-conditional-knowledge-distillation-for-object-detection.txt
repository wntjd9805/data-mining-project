Knowledge distillation has shown great success in classiﬁcation, however, it is still challenging for detection. In a typical image for detection, representations from different locations may have different contributions to detection targets, making the distillation hard to balance. In this paper, we propose a conditional distillation framework to distill the desired knowledge, namely knowledge that is beneﬁcial in terms of both classiﬁcation and localization for every instance. The framework introduces a learnable conditional decoding module, which retrieves information given each target instance as query. Speciﬁcally, we encode the condition informa-tion as query and use the teacher’s representations as key. The attention between query and key is used to measure the contribution of different features, guided by a localization-recognition-sensitive auxiliary task. Extensive experiments demon-strate the efﬁcacy of our method: we observe impressive improvements under various settings. Notably, we boost RetinaNet with ResNet-50 backbone from 37.4 to 40.7 mAP (+3.3) under 1× schedule, that even surpasses the teacher (40.4 mAP) with ResNet-101 backbone under 3× schedule. Code has been released on https://github.com/megvii-research/ICD. 