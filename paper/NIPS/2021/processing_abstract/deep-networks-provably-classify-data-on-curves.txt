Data with low-dimensional nonlinear structure are ubiquitous in engineering and scientiﬁc problems. We study a model problem with such structure—a binary classiﬁcation task that uses a deep fully-connected neural network to classify data drawn from two disjoint smooth curves on the unit sphere. Aside from mild regularity conditions, we place no restrictions on the conﬁguration of the curves.We prove that when (i) the network depth is large relative to certain geometric properties that set the difﬁculty of the problem and (ii) the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To our knowledge, this is the ﬁrst generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. Our analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime, where the network depth plays the role of a ﬁtting resource in solving the classiﬁcation problem. In particular, via ﬁne-grained control of the decay properties of the NTK, we demonstrate that when the network is sufﬁciently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization. 