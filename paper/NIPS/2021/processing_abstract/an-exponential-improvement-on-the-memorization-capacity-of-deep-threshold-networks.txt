(e1/δ2It is well known that modern deep neural networks are powerful enough to mem-orize datasets even when the labels have been randomized. Recently, Vershynin (2020) settled a long standing question by Baum (1988), proving that deep thresh-old networks can memorize n points in d dimensions using+√n) neurons and (d + √n) + n) weights, where δ is the minimum distance between the points. In this work, we improve the dependence on δ from exponential to almost ( d linear, proving thatδ + n) weights are sufﬁcient. Our construction uses Gaussian random weights only in the ﬁrst layer, while all the subsequent layers use binary or integer weights. We also prove new lower bounds by connecting memorization in neural networks to the purely geometric problem of separating n points on a sphere using hyperplanes. ( 1δ + √n) neurons and (e1/δ2OOOO (cid:101) (cid:101) (cid:101) (cid:101) 