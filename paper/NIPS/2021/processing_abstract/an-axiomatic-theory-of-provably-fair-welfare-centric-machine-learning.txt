We address an inherent difﬁculty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justiﬁed alternative setting, and study-ing the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objec-tives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimiza-tion tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We deﬁne a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justiﬁcation via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply deﬁning utility as negative loss and maximizing welfare. Building upon these concepts, we deﬁne fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justiﬁed) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on ﬁrm theoretical ground, as it yields statistical — and in some cases computational — efﬁciency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees. 