Existing bias mitigation methods for DNN models primarily work on learning debi-ased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classiﬁcation head, even with biased representations as in-puts? To this end, we propose a new mitigation technique, namely, RepresentationNeutralization for Fairness (RNF) that achieves fairness by debiasing only the task-speciﬁc classiﬁcation head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classiﬁcation head of the DNN model. The key idea of RNF is to discourage the classiﬁcation head from capturing undesirable correlation between fairness sensitive information in encoder representations with speciﬁc class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-ampliﬁed model to generate proxy an-notations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination ofDNN models with minimal degradation in task-speciﬁc performance. 