Mutual information (MI) maximization provides an appealing formalism for learn-ing representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant in-formation, while retaining the information necessary for control. Much prior work on these methods has addressed the practical difﬁculties of estimating MI from samples of high-dimensional observations, while comparatively less is understood about which MI objectives yield representations that are sufﬁcient for RL from a theoretical perspective. In this paper, we formalize the sufﬁciency of a state representation for learning and representing the optimal policy, and study several popular MI based objectives through this lens. Surprisingly, we ﬁnd that two of these objectives can yield insufﬁcient representations given mild and common as-sumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations. 