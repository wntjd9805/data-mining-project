In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of M possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We ﬁrst show that a general instance of LMDPs requires at least Ω((SA)M ) episodes to even approximate the optimal policy. Then, we consider suﬃcient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With suﬃcient separation, we provide an eﬃcient algorithm with local guarantee, i.e., providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical suﬃciency assumptions common in the PredictiveState Representation (PSR) literature (e.g., [6]) and a reachability assumption, we show that the need for initialization can be removed. 