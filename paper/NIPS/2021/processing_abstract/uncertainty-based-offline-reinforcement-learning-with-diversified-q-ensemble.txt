Ofﬂine reinforcement learning (ofﬂine RL), which aims to ﬁnd an optimal policy from a previously collected static dataset, bears algorithmic difﬁculties due to function approximation errors from out-of-distribution (OOD) data points. To this end, ofﬂine RL algorithms adopt either a constraint or a penalty term that explicitly guides the policy to stay close to the given dataset. However, prior methods typically require accurate estimation of the behavior policy or sampling from OOD data points, which themselves can be a non-trivial problem. Moreover, these methods under-utilize the generalization ability of deep neural networks and often fall into suboptimal solutions too close to the given dataset. In this work, we propose an uncertainty-based ofﬂine RL method that takes into account the conﬁdence of the Q-value prediction and does not require any estimation or sampling of the data distribution. We show that the clipped Q-learning, a technique widely used in online RL, can be leveraged to successfully penalize OOD data points with high prediction uncertainties. Surprisingly, we ﬁnd that it is possible to substantially outperform existing ofﬂine RL methods on various tasks by simply increasing the number of Q-networks along with the clipped Q-learning. Based on this observation, we propose an ensemble-diversiﬁed actor-critic algorithm that reduces the number of required ensemble networks down to a tenth compared to the naive ensemble while achieving state-of-the-art performance on most of theD4RL benchmarks considered. 