Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make signiﬁcant headway on these is-sues by projecting generated and real samples into a ﬁxed, pretrained feature space.Motivated by the ﬁnding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves im-age quality, sample efﬁciency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet In-ception Distance (FID) on twenty-two benchmark datasets. Importantly, ProjectedGANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.Figure 1: Convergence with Projected GANs. Evolution of samples for a ﬁxed latent code during training on the AFHQ-Dog dataset [5]. We ﬁnd that discriminating features in the projected feature space speeds up convergence and yields lower FIDs. This ﬁnding is consistent across many datasets. 