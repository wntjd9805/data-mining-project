We give relative error coresets for training linear classiﬁers with a broad class of loss functions, including the logistic loss and hinge loss. Our construction achieves (1 ± (cid:15)) relative error with ˜O(d · µy(X)2/(cid:15)2) points, where µy(X) is a natural complexity measure of the data matrix X ∈ Rn×d and label vector y ∈ {−1, 1}n, introduced in [MSSW18]. Our result is based on subsampling data points with probabilities proportional to their (cid:96)1 Lewis weights. It signiﬁcantly improves on existing theoretical bounds and performs well in practice, outperforming uniform subsampling along with other importance sampling methods. Our sampling dis-tribution does not depend on the labels, so can be used for active learning. It also does not depend on the speciﬁc loss function, so a single coreset can be used in multiple training scenarios. 