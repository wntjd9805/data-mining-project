We study discrete-time mirror descent applied to the unregularized empirical risk in matrix sensing. In both the general case of rectangular matrices and the particular case of positive semideﬁnite matrices, a simple potential-based analysis in terms of the Bregman divergence allows us to establish convergence of mirror descent—with different choices of the mirror maps—to a matrix that, among all global minimizers of the empirical risk, minimizes a quantity explicitly related to the nuclear norm, theFrobenius norm, and the von Neumann entropy. In both cases, this characterization implies that mirror descent, a ﬁrst-order algorithm minimizing the unregularized empirical risk, recovers low-rank matrices under the same set of assumptions that are sufﬁcient to guarantee recovery for nuclear-norm minimization. When the sensing matrices are symmetric and commute, we show that gradient descent with full-rank factorized parametrization is a ﬁrst-order approximation to mirror descent, in which case we obtain an explicit characterization of the implicit bias of gradientﬂow as a by-product. 