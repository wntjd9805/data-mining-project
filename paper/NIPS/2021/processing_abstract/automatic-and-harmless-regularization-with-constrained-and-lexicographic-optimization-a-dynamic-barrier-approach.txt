Many machine learning tasks have to make a trade-off between two loss functions, typically the main data-ﬁtness loss and an auxiliary loss. The most widely used approach is to optimize the linear combination of the objectives, which, however, requires manual tuning of the combination coefﬁcient and is theoretically unsuit-able for non-convex functions. In this work, we consider constrained optimization as a more principled approach for trading off two losses, with a special emphasis on lexicographic (lexico) optimization, a degenerated limit of constrained opti-mization which optimizes a secondary loss inside the optimal set of the main loss.We propose a dynamic barrier gradient descent algorithm which provides a uni-ﬁed solution of both constrained and lexicographic optimization. We establish the convergence of the method for general non-convex functions. Through a number of experiments on real-world deep learning tasks, we show that 1) lexico optimiza-tion provides a tuning-free approach to incorporating side loss functions without hurting the main objective, and 2) constrained and lexico optimization combined provide an automatic approach to proﬁling Pareto sets, especially in non-convex problems on which linear combination methods fail. 