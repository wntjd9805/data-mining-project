Empowered by neural networks, deep reinforcement learning (DRL) achieves tremendous empirical success. However, DRL requires a large dataset by in-teracting with the environment, which is unrealistic in critical scenarios such as autonomous driving and personalized medicine. In this paper, we study how to incorporate the dataset collected in the ofﬂine setting to improve the sample ef-ﬁciency in the online setting. To incorporate the observational data, we face two challenges. (a) The behavior policy that generates the observational data may de-pend on unobserved random variables (confounders), which affect the received rewards and transition dynamics. (b) Exploration in the online setting requires quantifying the uncertainty given both the observational and interventional data.To tackle such challenges, we propose the deconfounded optimistic value itera-tion (DOVI) algorithm, which incorporates the confounded observational data in a provably efﬁcient manner. DOVI explicitly adjusts for the confounding bias in the observational data, where the confounders are partially observed or unob-served. In both cases, such adjustments allow us to construct the bonus based on a notion of information gain, which takes into account the amount of information acquired from the ofﬂine setting. In particular, we prove that the regret of DOVI is smaller than the optimal regret achievable in the pure online setting when the confounded observational data are informative upon the adjustments. 