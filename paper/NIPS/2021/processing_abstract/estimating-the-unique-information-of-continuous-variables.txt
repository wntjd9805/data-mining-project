The integration and transfer of information from multiple sources to multiple tar-gets is a core motive of neural systems. The emerging ﬁeld of partial information decomposition (PID) provides a novel information-theoretic lens into these mecha-nisms by identifying synergistic, redundant, and unique contributions to the mutual information between one and several variables. While many works have studied aspects of PID for Gaussian and discrete distributions, the case of general contin-uous distributions is still uncharted territory. In this work we present a method for estimating the unique information in continuous distributions, for the case of one versus two variables. Our method solves the associated optimization problem over the space of distributions with ﬁxed bivariate marginals by combining copula decompositions and techniques developed to optimize variational autoencoders.We obtain excellent agreement with known analytic results for Gaussians, and illus-trate the power of our new approach in several brain-inspired neural models. Our method is capable of recovering the effective connectivity of a chaotic network of rate neurons, and uncovers a complex trade-off between redundancy, synergy and unique information in recurrent networks trained to solve a generalized XOR task. 