As large-scale language model pretraining pushes the state-of-the-art in text gen-eration, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via ﬁne-tuning remains the popu-lar approach, it incurs a signiﬁcant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MUCOCO—a ﬂexible and modular algorithm for controllable inference from pretrained models. We formu-late the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe signiﬁcant improvements over baselines.1 