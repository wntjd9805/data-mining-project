Convolutional neural networks perform a local and translationally-invariant treat-ment of the data: quantifying which of these two aspects is central to their success remains a challenge. We study this problem within a teacher-student framework for kernel regression, using ‘convolutional’ kernels inspired by the neural tangent kernel of simple convolutional architectures of given ﬁlter size. Using heuristic methods from physics, we ﬁnd in the ridgeless case that locality is key in determin-ing the learning curve exponent β (that relates the test error (cid:15)t ∼ P −β to the size of the training set P ), whereas translational invariance is not. In particular, if theﬁlter size of the teacher t is smaller than that of the student s, β is a function of s only and does not depend on the input dimension. We conﬁrm our predictions onβ empirically. We conclude by proving, under a natural universality assumption, that performing kernel regression with a ridge that decreases with the size of the training set leads to similar learning curve exponents to those we obtain in the ridgeless case. 