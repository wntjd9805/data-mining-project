When machine learning systems meet real world applications, accuracy is only one of several requirements. In this paper, we assay a complementary perspective originating from the increasing availability of pre-trained and regularly improv-ing state-of-the-art models. While new improved models develop at a fast pace, downstream tasks vary more slowly or stay constant. Assume that we have a large unlabelled data set for which we want to maintain accurate predictions. Whenever a new and presumably better ML models becomes available, we encounter two prob-lems: (i) given a limited budget, which data points should be re-evaluated using the new model?; and (ii) if the new predictions differ from the current ones, should we update? Problem (i) is about compute cost, which matters for very large data sets and models. Problem (ii) is about maintaining consistency of the predictions, which can be highly relevant for downstream applications; our demand is to avoid negativeﬂips, i.e., changing correct to incorrect predictions. In this paper, we formalize the Prediction Update Problem and present an efﬁcient probabilistic approach as answer to the above questions. In extensive experiments on standard classiﬁcation benchmark data sets, we show that our method outperforms alternative strategies along key metrics for backward-compatible prediction updates. 