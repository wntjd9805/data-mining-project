A Bayesian treatment can mitigate overconﬁdence in ReLU nets around the training data. But far away from them, ReLU Bayesian neural networks (BNNs) can still underestimate uncertainty and thus be asymptotically overconﬁdent. This issue arises since the output variance of a BNN with ﬁnitely many features is quadratic in the distance from the data region. Meanwhile, Bayesian linear models withReLU features converge, in the inﬁnite-width limit, to a particular Gaussian process (GP) with a variance that grows cubically so that no asymptotic overconﬁdence can occur. While this may seem of mostly theoretical interest, in this work, we show that it can be used in practice to the beneﬁt of BNNs. We extend ﬁniteReLU BNNs with inﬁnite ReLU features via the GP and show that the resulting model is asymptotically maximally uncertain far away from the data while theBNNs’ predictive power is unaffected near the data. Although the resulting model approximates a full GP posterior, thanks to its structure, it can be applied post-hoc to any pre-trained ReLU BNN at a low cost. 