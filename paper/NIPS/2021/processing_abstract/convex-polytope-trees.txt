A decision tree is commonly restricted to use a single hyperplane to split the covariate space at each of its internal nodes. It often requires a large number of nodes to achieve high accuracy. In this paper, we propose convex polytope trees (CPT) to expand the family of decision trees by an interpretable generalization of their decision boundary. The splitting function at each node of CPT is based on the logical disjunction of a community of differently weighted probabilistic linear decision-makers, which also geometrically corresponds to a convex polytope in the covariate space. We use a nonparametric Bayesian prior at each node to infer the community’s size, encouraging simpler decision boundaries by shrinking the number of polytope facets. We develop a greedy method to efﬁciently constructCPT and scalable end-to-end training algorithms for the tree parameters when the tree structure is given. We empirically demonstrate the efﬁciency of CPT over existing state-of-the-art decision trees in several real-world classiﬁcation and regression tasks from diverse domains. 