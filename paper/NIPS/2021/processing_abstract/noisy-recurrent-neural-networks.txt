We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Speciﬁcally, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We ﬁnd that, under reasonable assumptions, this implicit regularization promotes ﬂatter minima; it biases towards models with more stable dynamics; and, in classiﬁcation tasks, it favors models with larger classiﬁcation margin. Sufﬁcient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations. 