In practice, imitation learning is preferred over pure reinforcement learning when-ever it is possible to design a teaching agent to provide expert supervision. However, we show that when the teaching agent makes decisions with access to privileged information that is unavailable to the student, this information is marginalized dur-ing imitation learning, resulting in an “imitation gap” and, potentially, poor results.Prior work bridges this gap via a progression from imitation learning to reinforce-ment learning. While often successful, gradual progression fails for tasks that require frequent switches between exploration and memorization. To better address these tasks and alleviate the imitation gap we propose ‘Adaptive Insubordination’ (ADVISOR). ADVISOR dynamically weights imitation and reward-based rein-forcement learning losses during training, enabling on-the-ﬂy switching between imitation and exploration. On a suite of challenging tasks set within gridworlds, multi-agent particle environments, and high-ﬁdelity 3D simulators, we show that on-the-ﬂy switching with ADVISOR outperforms pure imitation, pure reinforce-ment learning, as well as their sequential and parallel combinations. 