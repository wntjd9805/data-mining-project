√We develop algorithms for private stochastic convex optimization that adapt to the hardness of the speciﬁc function we wish to optimize. While previous work pro-vide worst-case bounds for arbitrary convex functions, it is often the case that the function at hand belongs to a smaller class that enjoys faster rates. Con-cretely, we show that for functions exhibiting κ-growth around the optimum, i.e., f (x) ≥ f (x(cid:63)) + λκ−1(cid:107)x − x(cid:63)(cid:107)κ 2 for κ > 1, our algorithms improve upon theκ−1 . Crucially, they achieve standard these rates without knowledge of the growth constant κ of the function. Our al-gorithms build upon the inverse sensitivity mechanism, which adapts to instance difﬁculty [2], and recent localization techniques in private optimization [25]. We complement our algorithms with matching lower bounds for these function classes and demonstrate that our adaptive algorithm is simultaneously (minimax) optimal over all κ ≥ 1 + c whenever c = Θ(1). d/nε privacy rate to the faster ( d/nε)√κ 