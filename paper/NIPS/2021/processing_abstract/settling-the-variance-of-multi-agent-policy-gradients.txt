Policy gradient (PG) methods are popular reinforcement learning (RL) methods where a baseline is often applied to reduce the variance of gradient estimates. In multi-agent RL (MARL), although the PG theorem can be naturally extended, the effectiveness of multi-agent PG (MAPG) methods degrades as the variance of gra-dient estimates increases rapidly with the number of agents. In this paper , we offer a rigorous analysis of MAPG methods by, ﬁrstly, quantifying the contributions of the number of agents and agents’ explorations to the variance of MAPG estimators.Based on this analysis, we derive the optimal baseline (OB) that achieves the mini-mal variance. In comparison to the OB, we measure the excess variance of existingMARL algorithms such as vanilla MAPG and COMA. Considering using deep neu-ral networks, we also propose a surrogate version of OB, which can be seamlessly plugged into any existing PG methods in MARL. On benchmarks of Multi-AgentMuJoCo and StarCraft challenges, our OB technique effectively stabilises training and improves the performance of multi-agent PPO and COMA algorithms by a signiﬁcant margin. Code is released at https://github.com/morning9393/Optimal-Baseline-for-Multi-agent-Policy-Gradients. 