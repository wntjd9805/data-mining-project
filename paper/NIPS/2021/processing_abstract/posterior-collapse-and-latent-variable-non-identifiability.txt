Variational autoencoders model high-dimensional data by positing low-dimensional latent variables that are mapped through a ﬂexible distribution parametrized by a neural network. Unfortunately, variational autoencoders often suffer from posterior collapse: the posterior of the latent variables is equal to its prior, rendering the variational autoencoder useless as a means to produce meaningful representations.Existing approaches to posterior collapse often attribute it to the use of neural networks or optimization issues due to variational approximation. In this paper, we consider posterior collapse as a problem of latent variable non-identiﬁability.We prove that the posterior collapses if and only if the latent variables are non-identiﬁable in the generative model. This fact implies that posterior collapse is not a phenomenon speciﬁc to the use of ﬂexible distributions or approximate inference. Rather, it can occur in classical probabilistic models even with exact inference, which we also demonstrate. Based on these results, we propose a class of latent-identiﬁable variational autoencoders, deep generative models which enforce identiﬁability without sacriﬁcing ﬂexibility. This model class resolves the problem of latent variable non-identiﬁability by leveraging bijective Brenier maps and parameterizing them with input convex neural networks, without special variational inference objectives or optimization tricks. Across synthetic and real datasets, latent-identiﬁable variational autoencoders outperform existing methods in mitigating posterior collapse and providing meaningful representations of the data. 