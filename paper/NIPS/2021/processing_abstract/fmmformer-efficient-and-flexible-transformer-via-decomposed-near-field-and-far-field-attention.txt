We propose FMMformers, a class of efﬁcient and ﬂexible transformers inspired by the celebrated fast multipole method (FMM) for accelerating interacting parti-cle simulation. FMM decomposes particle-particle interaction into near-ﬁeld and far-ﬁeld components and then performs direct and coarse-grained computation, respectively. Similarly, FMMformers decompose the attention into near-ﬁeld and far-ﬁeld attention, modeling the near-ﬁeld attention by a banded matrix and the far-ﬁeld attention by a low-rank matrix. Computing the attention matrix for FMM-formers requires linear complexity in computational time and memory footprint with respect to the sequence length. In contrast, standard transformers suffer from quadratic complexity. We analyze and validate the advantage of FMMformers over the standard transformer on the Long Range Arena and language modeling benchmarks. FMMformers can even outperform the standard transformer in terms of accuracy by a signiﬁcant margin. For instance, FMMformers achieve an average classiﬁcation accuracy of 60.74% over the ﬁve Long Range Arena tasks, which is signiﬁcantly better than the standard transformer’s average accuracy of 58.70%. 