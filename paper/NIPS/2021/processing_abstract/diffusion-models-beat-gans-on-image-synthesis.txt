We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by ﬁnding a better architecture through a series of ablations. For condi-tional image synthesis, we further improve sample quality with classiﬁer guidance: a simple, compute-efﬁcient method for trading off diversity for ﬁdelity using gradi-ents from a classiﬁer. We achieve an FID of 2.97 on ImageNet 128 128, 4.59 onImageNet 256 512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we ﬁnd that classiﬁer guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 512. 256 256 and 3.85 on ImageNet 512 256, and 7.72 on ImageNet 512⇥⇥⇥⇥⇥ 