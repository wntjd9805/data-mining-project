Adversarial examples are often cited by neuroscientists and machine learning re-searchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired compo-nents to visual neural networks as a way to improve their adversarial robustness.One surprisingly effective component for reducing adversarial vulnerability is re-sponse stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations inﬂuence the internal representations of standard, ad-versarially trained, and biologically-inspired stochastic networks. We ﬁnd distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity medi-ate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneﬁcial to machine and biological computation.1 