We introduce ViSER, a method for recovering articulated 3D shapes and dense 3D trajectories from monocular videos. Previous work on high-quality reconstruction of dynamic 3D shapes typically relies on multiple synchronized cameras, strong category-speciﬁc priors, or 2D keypoint supervision. We show that none of these are required if one can reliably estimate long-range correspondences in a video, making use of only 2D object masks and two-frame optical ﬂow as inputs. ViSER infers correspondences by matching 2D pixels to a canonical, deformable 3D mesh via video-speciﬁc surface embeddings that capture the view-independent appearance features of each surface point. These embeddings behave as a continuous set of keypoint descriptors deﬁned over the mesh surface, which can be used to establish dense long-range correspondences across pixels. The surface embeddings are implemented as coordinate-based MLPs that are ﬁt to each video via self-supervised losses. Experimental results show that ViSER compares favorably against prior work on challenging videos of humans with loose clothing and unusual poses as well as animal videos from DAVIS and YTVOS. 