Quantization is a popular technique that transforms the parameter representation of a neural network from ﬂoating-point numbers into lower-precision ones (e.g., 8-bit integers). It reduces the memory footprint and the computational cost at inference, facilitating the deployment of resource-hungry models. However, the parameter perturbations caused by this transformation result in behavioral disparities between the model before and after quantization. For example, a quantized model can mis-classify some test-time samples that are otherwise classiﬁed correctly. It is not known whether such differences lead to a new security vulnerability. We hypothe-size that an adversary may control this disparity to introduce speciﬁc behaviors that activate upon quantization. To study this hypothesis, we weaponize quantization-aware training and propose a new training framework to implement adversarial quantization outcomes. Following this framework, we present three attacks we carry out with quantization: (i) an indiscriminate attack for signiﬁcant accuracy loss; (ii) a targeted attack against speciﬁc samples; and (iii) a backdoor attack for controlling the model with an input trigger. We further show that a single compro-mised model defeats multiple quantization schemes, including robust quantization techniques. Moreover, in a federated learning scenario, we demonstrate that a set of malicious participants who conspire can inject our quantization-activated backdoor.Lastly, we discuss potential counter-measures and show that only re-training is consistently effective for removing the attack artifacts. Our code is available at https://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation. 