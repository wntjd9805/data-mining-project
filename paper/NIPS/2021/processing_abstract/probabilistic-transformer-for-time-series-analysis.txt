Generative modeling of multivariate time series has remained challenging partly due to the complex, non-deterministic dynamics across long-distance time steps.In this paper, we propose deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. In contrast to previously proposedSSMs, our approaches use attention mechanism to model non-Markovian dynam-ics in the latent space and avoid recurrent neural networks entirely. We also extend our models to include several layers of stochastic variables organized in a hierar-chy for further expressiveness. Compared to transformer models, ours are proba-bilistic, non-autoregressive, and capable of generating diverse long-term forecasts with accounted uncertainty. Extensive experiments show that our models consis-tently outperform competitive baselines on various tasks and datasets, including time series forecasting and human motion prediction. 