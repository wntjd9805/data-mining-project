Many reinforcement learning (RL) environments consist of independent entities that interact sparsely. In such environments, RL agents have only limited inﬂuence over other entities in any particular situation. Our idea in this work is that learning can be efﬁciently guided by knowing when and what the agent can inﬂuence with its actions. To achieve this, we introduce a measure of situation-dependent causal inﬂuence based on conditional mutual information and show that it can reliably detect states of inﬂuence. We then propose several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning. All modiﬁed algorithms show strong increases in data efﬁciency on robotic manipulation tasks. 