While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that TransformerAttention can be closely related under certain data conditions to Kanerva’s SparseDistributed Memory (SDM), a biologically plausible associative memory model.We conﬁrm that these conditions are satisﬁed in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.