Generative adversarial networks (GANs) typically require ample data for train-ing in order to synthesize high-ﬁdelity images. Recent studies have shown that training GANs with limited data remains formidable due to discriminator overﬁt-ting, the underlying cause that impedes the generator’s convergence. This paper introduces a novel strategy called Adaptive Pseudo Augmentation (APA) to en-courage healthy competition between the generator and the discriminator. As an alternative method to existing approaches that rely on standard data augmentations or model regularization, APA alleviates overﬁtting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminator adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime. We provide a theoretical analysis to examine the convergence and rationality of our new training strategy. APA is simple and effective. It can be added seamlessly to powerful contemporary GANs, such as StyleGAN2, with negligible computational cost.Code: https://github.com/EndlessSora/DeceiveD. 