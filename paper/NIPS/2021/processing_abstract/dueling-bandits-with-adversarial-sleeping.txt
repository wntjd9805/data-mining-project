We introduce the problem of sleeping dueling bandits with stochastic preferences and adversarial availabilities (DB-SPAA). In almost all dueling bandit applica-tions, the decision space often changes over time; eg, retail store management, online shopping, restaurant recommendation, search engine optimization, etc. Sur-prisingly, this ‘sleeping aspect’ of dueling bandits has never been studied in the literature. Like dueling bandits, the goal is to compete with the best arm by sequen-tially querying the preference feedback of item pairs. The non-triviality however results due to the non-stationary item spaces that allow any arbitrary subsets items to go unavailable every round. The goal is to ﬁnd an optimal ‘no-regret’ policy that can identify the best available item at each round, as opposed to the standard ‘ﬁxed best-arm regret objective’ of dueling bandits. We ﬁrst derive an instance-speciﬁc lower bound for DB-SPAA Ω((cid:80)K−1 (cid:80)K log T∆(i,j) ), where K is the number of i=1 items and ∆(i, j) is the gap between items i and j. This indicates that the sleeping problem with preference feedback is inherently more difﬁcult than that for classical multi-armed bandits (MAB). We then propose two algorithms, with near optimal regret guarantees. Our results are corroborated empirically. j=i+1 