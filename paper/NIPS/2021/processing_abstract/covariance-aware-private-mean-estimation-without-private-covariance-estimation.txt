We present two sample-efﬁcient differentially private mean estimators for d-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given n (cid:38) d/α2 samples from such a distribution with mean µ and covariance Σ, our estimators output ˜µ such that (cid:107)˜µ − µ(cid:107)Σ ≤ α, where (cid:107) · (cid:107)Σ is the Mahalanobis distance. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require Ω(d3/2) samples.Each of our estimators is based on a simple, general approach to designing differ-entially private mechanisms, but with novel technical steps to make the estimator private and sample-efﬁcient. Our ﬁrst estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance, without releasing the covariance itself. Its sample complexity guarantees hold more generally for subgaussian dis-tributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy. 