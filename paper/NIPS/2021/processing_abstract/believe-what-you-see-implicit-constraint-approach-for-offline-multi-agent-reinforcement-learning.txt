Learning from datasets without interaction with environments (Ofﬂine Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios. However, compared with the single-agent counterpart, ofﬂine multi-agent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current ofﬂine RL algorithms are ineffective in multi-agent systems due to the accumulated extrapola-tion error. In this paper, we propose a novel ofﬂine RL algorithm, named ImplicitConstraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation.Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results demonstrate that the extrapo-lation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent ofﬂine tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ. 