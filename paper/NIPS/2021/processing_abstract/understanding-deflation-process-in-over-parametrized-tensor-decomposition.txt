In this paper we study the training dynamics for gradient ﬂow on over-parametrized tensor decomposition problems. Empirically, such training process often ﬁrst ﬁts larger components and then discovers smaller components, which is similar to a tensor deﬂation process that is commonly used in tensor decomposition algorithms.We prove that for orthogonally decomposable tensor, a slightly modiﬁed version of gradient ﬂow would follow a tensor deﬂation process and recover all the tensor components. Our proof suggests that for orthogonal tensors, gradient ﬂow dynamics works similarly as greedy low-rank learning in the matrix setting, which is a ﬁrst step towards understanding the implicit regularization effect of over-parametrized models for low-rank tensors. 