Reinforcement learning has achieved great success in many applications. However, sample efﬁciency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been signiﬁcant progress in sample efﬁcient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efﬁcient model-based visual RL algorithm built on MuZero, which we name EfﬁcientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the ﬁrst time an algorithm achieves super-human performance on Atari games with such little data. EfﬁcientZero’s performance is also close to DQN’s performance at 200 million frames while we consume 500 times less data. EfﬁcientZero’s low sample complexity and high performance can bring RL closer to real-world applicability.We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.Figure 1: Our proposed method EfﬁcientZero is 170% and 180% better than the previous SoTA performance in mean and median human normalized score and is the ﬁrst to outperform the average human performance on the Atari 100k benchmark. The high sample efﬁciency and performance ofEfﬁcientZero can bring RL closer to the real-world applications. 