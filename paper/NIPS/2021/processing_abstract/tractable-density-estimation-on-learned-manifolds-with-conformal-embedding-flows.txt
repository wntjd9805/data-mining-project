Normalizing ﬂows are generative models that provide tractable density estimation via an invertible transformation from a simple base distribution to a complex target distribution. However, this technique cannot directly model data supported on an unknown low-dimensional manifold, a common occurrence in real-world domains such as image data. Recent attempts to remedy this limitation have introduced geometric complications that defeat a central beneﬁt of normalizing ﬂows: exact density estimation. We recover this beneﬁt with Conformal Embedding Flows, a framework for designing ﬂows that learn manifolds with tractable densities. We argue that composing a standard ﬂow with a trainable conformal embedding is the most natural way to model manifold-supported data. To this end, we present a series of conformal building blocks and apply them in experiments with synthetic and real-world data to demonstrate that ﬂows can model manifold-supported distributions without sacriﬁcing tractable likelihoods. 