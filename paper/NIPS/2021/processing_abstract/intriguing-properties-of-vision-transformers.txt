Vision transformers (ViT) have demonstrated impressive performance across nu-merous machine vision tasks. These models are based on multi-head self-attention mechanisms that can ﬂexibly attend to a sequence of image patches to encode con-textual cues. An important question is how such ﬂexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b) The robustness towards occlusions is not due to texture bias, instead we show that ViTs are signiﬁcantly less biased towards local textures, compared toCNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classiﬁcation datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to ﬂexible and dynamic receptive ﬁelds possible via self-attention mechanisms. Code: https://git.io/Js15X. 