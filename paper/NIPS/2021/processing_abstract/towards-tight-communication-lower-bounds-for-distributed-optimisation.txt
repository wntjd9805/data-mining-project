PWe consider a standard distributed optimisation setting where N machines, each holding a d-dimensional function fi, aim to jointly minimise the sum of theN i=1 fi(x). This problem arises naturally in large-scale distributed functions optimisation, where a standard solution is to apply variants of (stochastic) gra-dient descent. We focus on the communication complexity of this problem: our main result provides the ﬁrst fully unconditional bounds on total number of bits which need to be sent and received by the N machines to solve this problem under point-to-point communication, within a given error-tolerance. Speciﬁcally, we show that ⌦(N d log d/N ") total bits need to be communicated between theN i=1 fi(x). The machines to ﬁnd an additive ✏-approximation to the minimum of result holds for both deterministic and randomised algorithms, and, importantly, requires no assumptions on the algorithm structure. The lower bound is tight under certain restrictions on parameter values, and is matched within constant factors for quadratic objectives by a new variant of quantised gradient descent, which we describe and analyse. Our results bring over tools from communication complexity to distributed optimisation, which has potential for further applications.P 