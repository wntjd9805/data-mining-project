Conditional Generative Adversarial Networks (cGANs) are implicit generative models which allow to sample from class-conditional distributions. Existing cGANs are based on a wide range of different discriminator designs and training objectives. One popular design in earlier works is to include a classiﬁer during training with the assumption that good classiﬁers can help eliminate samples gen-erated with wrong classes. Nevertheless, including classiﬁers in cGANs often comes with a side effect of only generating easy-to-classify samples. Recently, some representative cGANs avoid the shortcoming and reach state-of-the-art per-formance without having classiﬁers. Somehow it remains unanswered whether the classiﬁers can be resurrected to design better cGANs. In this work, we demonstrate that classiﬁers can be properly leveraged to improve cGANs. We start by using the decomposition of the joint probability distribution to connect the goals of cGANs and classiﬁcation as a uniﬁed framework. The framework, along with a classic energy model to parameterize distributions, justiﬁes the use of classiﬁers for cGANs in a principled manner. It explains several popular cGAN variants, such as ACGAN, ProjGAN, and ContraGAN, as special cases with different levels of approximations, which provides a uniﬁed view and brings new insights to under-standing cGANs. Experimental results demonstrate that the design inspired by the proposed framework outperforms state-of-the-art cGANs on multiple benchmark datasets, especially on the most challenging ImageNet. The code is available at https://github.com/sian-chen/PyTorch-ECGAN. 