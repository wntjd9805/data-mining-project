In this paper we consider multi-objective reinforcement learning where the objec-tives are balanced using preferences. In practice, the preferences are often given in an adversarial manner, e.g., customers can be picky in many applications. We formalize this problem as an episodic learning problem on a Markov decision pro-cess, where transitions are unknown and a reward function is the inner product of a preference vector with pre-speciﬁed multi-objective reward functions. We consider two settings. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. We provide a model-based algorithm that achieves a nearly minimax optimal regret bound (cid:101)O(cid:0)(cid:112)min{d, S} · H 2SAK(cid:1), where d is the number of objectives, S is the number of states, A is the number of actions, H is the length of the horizon, and K is the number of episodes. Furthermore, we consider preference-free exploration, i.e., the agent ﬁrst interacts with the environment without specifying any preference and then is able to accommodate arbitrary preference vector up to (cid:15) error. Our proposed algorithm is provably efﬁcient with a nearly optimal trajectory complexity (cid:101)O(cid:0)min{d, S} · H 3SA/(cid:15)2(cid:1). This result partly resolves an open problem raised byJin et al. [2020]. 