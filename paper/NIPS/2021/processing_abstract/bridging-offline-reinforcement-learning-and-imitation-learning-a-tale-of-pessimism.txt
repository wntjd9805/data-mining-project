Ofﬂine (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a ﬁxed dataset without active data collection. Based on the composition of the ofﬂine dataset, two main methods are used: imitation learning which is suitable for expert datasets, and vanilla ofﬂine RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown. To bridge this gap, we present a new ofﬂine RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla ofﬂine RL. The new framework is centered around a weak version of the concentrability coefﬁcient that measures the deviation of the behavior policy from the expert policy alone. Under this new framework, we ask: can one develop an algorithm that achieves a minimax optimal rate adaptive to unknown data composition? To address this question, we consider a lower conﬁdence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in ofﬂineRL. We study ﬁnite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in both contextual bandits and RL, LCB achieves a faster rate of 1/N for nearly-expert datasets compared to the usual rate of 1/N in ofﬂine RL, where N is the batch dataset sample size. In contextual bandits, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to ofﬂine RL. We further show that LCB is almost adaptively optimal in tabular MDPs.√ 