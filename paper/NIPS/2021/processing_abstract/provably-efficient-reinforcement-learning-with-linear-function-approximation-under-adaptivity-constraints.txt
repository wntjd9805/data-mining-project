We study reinforcement learning (RL) with linear function approximation under the adaptivity constraint. We consider two popular limited adaptivity models: the batch learning model and the rare policy switch model, and propose two efﬁcient online RL algorithms for episodic linear Markov decision processes, where the transition probability and the reward function can be represented as a linear function of some known feature mapping. In speciﬁc, for the batch learning model, our d3H 3T + dHT /B) regret, proposed LSVI-UCB-Batch algorithm achieves an (cid:101)O( where d is the dimension of the feature mapping, H is the episode length, T is the number of interactions and B is the number of batches. Our result suggests that it sufﬁces to use only (cid:112)T /dH batches to obtain (cid:101)O( d3H 3T ) regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an (cid:101)O((cid:112)d3H 3T [1 + T /(dH)]dH/B) regret, which implies that dH log T policy d3H 3T ) regret. Our algorithms achieve the switches sufﬁce to obtain the (cid:101)O( same regret as the LSVI-UCB algorithm (Jin et al., 2020), yet with a substantially smaller amount of adaptivity. We also establish a lower bound for the batch learning model, which suggests that the dependency on B in our regret bound is tight.√√√ 