Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high ﬁdelity posterior approximations while maintaining computational efﬁciency and scalability. We tackle this challenge by introducing a novel varia-tional structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inﬂexibility of the factorized structure in Dropout posterior and then propose an improved method called VariationalStructured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complex-ity, and consequently induces statistical dependencies in the approximate posterior.Theoretically, VSD successfully addresses the pathologies of previous VariationalDropout methods and thus offers a standard Bayesian justiﬁcation. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection. 