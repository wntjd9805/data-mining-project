The growing literature on “benign overﬁtting” in overparameterized models has been mostly restricted to regression or binary classiﬁcation settings; however, most success stories of modern machine learning have been recorded in multiclass set-tings. Motivated by this discrepancy, we study benign overﬁtting in multiclass linear classiﬁcation. Speciﬁcally, we consider the following popular training algo-rithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classiﬁer. Our ﬁrst key ﬁnding is that under a simple sufﬁcient condition, all three algorithms lead to classiﬁers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. Second, we derive novel error bounds on the accuracy of the MNI classiﬁer, thereby showing that all three training algo-rithms lead to benign overﬁtting under sufﬁcient overparameterization. Ultimately, our analysis shows that good generalization is possible for SVM solutions beyond the realm in which typical margin-based bounds apply. 