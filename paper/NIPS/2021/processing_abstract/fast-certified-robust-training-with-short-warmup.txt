Recently, bound propagation based certiﬁed robust training methods have been proposed for training neural networks with certiﬁable robustness guarantees. De-spite that state-of-the-art (SOTA) methods including interval bound propagation (IBP) and CROWN-IBP have per-batch training complexity similar to standard neural network training, they usually use a long warmup schedule with hundreds or thousands epochs to reach SOTA performance and are thus still costly. In this paper, we identify two important issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improveIBP training. These two issues make certiﬁed training difﬁcult and unstable, and thereby long warmup schedules were needed in prior works. To mitigate these issues and conduct faster certiﬁed training with shorter warmup, we propose three improvements based on IBP training: 1) We derive a new weight initialization method for IBP training; 2) We propose to fully add Batch Normalization (BN) to each layer in the model, since we ﬁnd BN can reduce the imbalance in ReLU activation states; 3) We also design regularization to explicitly tighten certiﬁed bounds and balance ReLU activation states during wamrup. We are able to obtain 65.03% veriﬁed error on CIFAR-10 (✏ = 8 255 ) and 82.36% veriﬁed error on Tiny-ImageNet (✏ = 1 255 ) using very short training schedules (160 and 80 total epochs, respectively), outperforming literature SOTA trained with hundreds or thousands epochs under the same network architecture. The code is available at https://github.com/shizhouxing/Fast-Certified-Robust-Training. 