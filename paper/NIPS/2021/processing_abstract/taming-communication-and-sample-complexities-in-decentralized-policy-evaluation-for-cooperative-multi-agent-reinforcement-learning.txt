Cooperative multi-agent reinforcement learning (MARL) has received increasing attention in recent years and has found many scientiﬁc and engineering applica-tions. However, a key challenge arising from many cooperative MARL algorithm designs (e.g., the actor-critic framework) is the policy evaluation problem, which can only be conducted in a decentralized fashion. In this paper, we focus on decentralized MARL policy evaluation with nonlinear function approximation, which is often seen in deep MARL. We ﬁrst show that the empirical decentral-ized MARL policy evaluation problem can be reformulated as a decentralized nonconvex-strongly-concave minimax saddle point problem. We then develop a decentralized gradient-based descent ascent algorithm called GT-GDA that enjoys a convergence rate of O(1/T ). To further reduce the sample complexity, we pro-pose two decentralized stochastic optimization algorithms called GT-SRVR andGT-SRVRI, which enhance GT-GDA by variance reduction techniques. We show that all algorithms all enjoy an O(1/T ) convergence rate to a stationary point of the reformulated minimax problem. Moreover, the fast convergence rates of GT-SRVR and GT-SRVRI imply O((cid:15)−2) communication complexity and O(m n(cid:15)−2) sam-ple complexity, where m is the number of agents and n is the length of trajectories.To our knowledge, this paper is the ﬁrst work that achieves O((cid:15)−2) in both sample and communication complexities in decentralized policy evaluation for cooperativeMARL. Our extensive experiments also corroborate the theoretical results of our proposed decentralized policy evaluation algorithms.√ 