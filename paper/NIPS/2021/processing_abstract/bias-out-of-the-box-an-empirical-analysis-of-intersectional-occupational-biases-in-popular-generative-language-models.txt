The capabilities of natural language models trained on large-scale data have in-creased immensely over the past few years. Open source libraries such as Hugging-Face have made these models easily available and accessible. While prior research has identiﬁed biases in large language models, this paper considers biases contained in the most popular versions of these models when applied ‘out-of-the-box’ for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Speciﬁcally, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model onHuggingFace, with over half a million downloads per month. We assess biases re-lated to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political afﬁliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and ﬁnd: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by ﬁtting 262 logistic models; (iii) For most occupations, GPT-2 reﬂects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reﬂect or correct for existing inequalities. 