Ofﬂine reinforcement learning (RL) deﬁnes the task of learning from a ﬁxed batch of data. Due to errors in value estimation from out-of-distribution actions, most ofﬂine RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modiﬁcations to make an RL algorithm work ofﬂine comes at the cost of additional complexity. Ofﬂine RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We ﬁnd that we can match the performance of state-of-the-art ofﬂine RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overheads of previous methods. 