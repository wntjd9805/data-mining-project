Double Q-learning (Hasselt, 2010) has gained signiﬁcant success in practice due to its effectiveness in overcoming the overestimation issue of Q-learning. However, the theoretical understanding of double Q-learning is rather limited. The only existing ﬁnite-time analysis was recently established in (Xiong et al., 2020), where the polynomial learning rate adopted in the analysis typically yields a slower con-vergence rate. This paper tackles the more challenging case of a constant learning rate, and develops new analytical tools that improve the existing convergence rate by orders of magnitude. Speciﬁcally, we show that synchronous double Q-learning attains an (cid:15)-accurate global optimum with a time complexity of ˜Ω, and (cid:16) the asynchronous algorithm achieves a time complexity of ˜Ω, whereD is the cardinality of the state-action space, γ is the discount factor, and L is a parameter related to the sampling strategy for asynchronous double Q-learning.These results improve the existing convergence rate by the order of magnitude in terms of its dependence on all major parameters ((cid:15), 1 − γ, D, L). This paper presents a substantial step toward the full understanding of the fast convergence of double-Q learning. (1−γ)7(cid:15)2 (cid:17)L (1−γ)7(cid:15)2 (cid:16) ln D (cid:17) 