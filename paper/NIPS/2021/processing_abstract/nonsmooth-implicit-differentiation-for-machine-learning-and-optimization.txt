In view of training increasingly complex learning architectures, we establish a non-smooth implicit function theorem with an operational calculus. Our result applies to most practical problems (i.e., deﬁnable problems) provided that a nonsmooth form of the classical invertibility condition is fulﬁlled. This approach allows for formal subdifferentiation: for instance, replacing derivatives by Clarke Jacobians in the usual differentiation formulas is fully justiﬁed for a wide class of nons-mooth problems. Moreover this calculus is entirely compatible with algorithmic differentiation (e.g., backpropagation). We provide several applications such as training deep equilibrium networks, training neural nets with conic optimization layers, or hyperparameter-tuning for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we present numerical experiments showcasing the extremely pathological gradient dynamics one can encounter when applying implicit algorithmic differentiation without any hypothesis. 