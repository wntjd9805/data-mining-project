We study the ofﬂine reinforcement learning (ofﬂine RL) problem, where the goal is to learn a reward-maximizing policy in an unknown Markov Decision Process (MDP) using the data coming from a policy µ. In particular, we consider the sample complexity problems of ofﬂine RL for ﬁnite-horizon MDPs. Prior works study this problem based on different data-coverage assumptions, and their learning guarantees are expressed by the covering coefﬁcients which lack the explicitIn this work, we analyze the Adaptive characterization of system quantities.Pessimistic Value Iteration (APVI) algorithm and derive the suboptimality upper bound that nearly matches (cid:115)OH (cid:88) (cid:88) h=1 sh,ah dπ(cid:63) h (sh, ah) (V (cid:63)VarPsh,ah dµ h(sh, ah) h+1 + rh) (cid:114) 1 n . (1) h(sh, ah) > 0 if dπ(cid:63)In complementary, we also prove a per-instance information-theoretical lower bound under the weak assumption that dµ h (sh, ah) > 0. Differ-ent from the previous minimax lower bounds, the per-instance lower bound (via local minimaxity) is a much stronger criterion as it applies to individual instances separately. Here π(cid:63) is a optimal policy, µ is the behavior policy and dµ h is the marginal state-action probability. We call (1) the intrinsic ofﬂine reinforcement learning bound since it directly implies all the existing optimal results: minimax rate under uniform data-coverage assumption, horizon-free setting, single policy concentrability, and the tight problem-dependent results. Later, we extend the result to the assumption-free regime (where we make no assumption on µ) and obtain the assumption-free intrinsic bound. Due to its generic form, we believe the intrinsic bound could help illuminate what makes a speciﬁc problem hard and reveal the fundamental challenges in ofﬂine RL. 