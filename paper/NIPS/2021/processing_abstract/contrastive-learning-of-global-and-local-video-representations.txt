Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning represen-tations speciﬁc to downstream scenarios, i.e., global representations suitable for tasks such as classiﬁcation or local representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classiﬁcation) and the tasks that require local ﬁne-grained spatio-temporal information (e.g., localization).We achieve this by optimizing two contrastive objectives that together encour-age our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, signiﬁcantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classiﬁcation, lip reading, deepfake detection, event and sound localization.1 