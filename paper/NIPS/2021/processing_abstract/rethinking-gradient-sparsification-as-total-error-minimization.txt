Gradient compression is a widely-established remedy to tackle the communication bottleneck in distributed training of large deep neural networks (DNNs). Under the error-feedback framework, Top-k sparsiﬁcation, sometimes with k as little as 0.1% of the gradient size, enables training to the same model quality as the uncompressed case for a similar iteration count. From the optimization perspective, we ﬁnd that Top-k is the communication-optimal sparsiﬁer given a per-iteration k element budget. We argue that to further the beneﬁts of gradient sparsiﬁcation, especially for DNNs, a different perspective is necessary — one that moves from per-iteration optimality to consider optimality for the entire training.We identify that the total error — the sum of the compression errors for all it-erations — encapsulates sparsiﬁcation throughout training. Then, we propose a communication complexity model that minimizes the total error under a commu-nication budget for the entire training. We ﬁnd that the hard-threshold sparsiﬁer, a variant of the Top-k sparsiﬁer with k determined by a constant hard-threshold, is the optimal sparsiﬁer for this model. Motivated by this, we provide convex and non-convex convergence analyses for the hard-threshold sparsiﬁer with error-feedback. We show that hard-threshold has the same asymptotic convergence and linear speedup property as SGD in both the case, and unlike with Top-k sparsiﬁer, has no impact due to data-heterogeneity. Our diverse experiments on various DNNs and a logistic regression model demonstrate that the hard-threshold sparsiﬁer is more communication-efﬁcient than Top-k. Code is available at https://github.com/sands-lab/rethinking-sparsification. 