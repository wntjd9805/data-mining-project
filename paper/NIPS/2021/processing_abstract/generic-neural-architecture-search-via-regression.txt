Most existing neural architecture search (NAS) algorithms are dedicated to and evaluated by the downstream tasks, e.g., image classiﬁcation in computer vision.However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. In this paper, we attempt to answer two fundamental questions related to NAS. (1) Is it necessary to use the performance of speciﬁc downstream tasks to evaluate and search for good neural architectures? (2) Can we perform NAS effectively and efﬁciently while being agnostic to the downstream tasks? To answer these questions, we propose a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-speciﬁc labels but instead adopts regression on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufﬁcient usage of training samples. Extensive experiments across 13 CNN search spaces and one NLP space demonstrate the remarkable efﬁciency of GenNAS using regression, in terms of both evaluating the neural architectures (quantiﬁed by the ranking correlation Spearman’s ρ be-tween the approximated performances and the downstream task performances) and the convergence speed for training (within a few seconds). For example, onNAS-Bench-101, GenNAS achieves 0.85 ρ while the existing efﬁcient methods only achieve 0.38. We then propose an automatic task search to optimize the combination of synthetic signals using limited downstream-task-speciﬁc labels, further improving the performance of GenNAS. We also thoroughly evaluate Gen-NAS’s generality and end-to-end NAS performance on all search spaces, which outperforms almost all existing works with signiﬁcant speedup. For example, onNASBench-201, GenNAS can ﬁnd near-optimal architectures within 0.3 GPU hour.Our code has been made available at: https://github.com/leeyeehoo/GenNAS 