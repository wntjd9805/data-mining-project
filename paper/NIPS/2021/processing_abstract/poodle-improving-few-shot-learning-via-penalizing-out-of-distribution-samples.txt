In this work, we propose to leverage out-of-distribution samples, i.e., unla-beled samples coming from outside target classes, for improving few-shot learn-ing. Speciﬁcally, we exploit the easily available out-of-distribution samples (e.g., from base classes) to drive the classiﬁer to avoid irrelevant features by maximizing the distance from prototypes to out-of-distribution samples while minimizing that to in-distribution samples (i.e., support, query data). Our ap-proach is simple to implement, agnostic to feature extractors, lightweight with-out any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures. Our code is available at https://github.com/VinAIResearch/poodle. 