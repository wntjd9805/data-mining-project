Complex activities often involve multiple humans utilizing different objects to com-plete actions (e.g., in healthcare settings, physicians, nurses, and patients interact with each other and various medical devices). Recognizing activities poses a chal-lenge that requires a detailed understanding of actors’ roles, objects’ affordances, and their associated relationships. Furthermore, these purposeful activities com-prise multiple achievable steps, including sub-activities and atomic actions, which jointly deﬁne a hierarchy of action parts. This paper introduces Activity Parsing as the overarching task of temporal segmentation and classiﬁcation of activities, sub-activities, atomic actions, along with an instance-level understanding of actors, objects, and their relationships in videos. Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a new representation of spatial-temporal graphs containing hyperedges (i.e., edges with higher-order relationships). In addition, we introduce Multi-Object Multi-Actor (MOMA), the ﬁrst benchmark and dataset dedicated to activity parsing. Lastly, to parse a video, we propose the HyperGraphActivity Parsing (HGAP) network, which outperforms several baselines, including those based on regular graphs and raw video data.∗These authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).