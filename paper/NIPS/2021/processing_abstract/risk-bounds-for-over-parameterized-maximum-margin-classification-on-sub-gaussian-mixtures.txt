Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can ﬁt the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this “benign overﬁtting” phenomenon of the maximum margin classiﬁer for linear classiﬁcation problems. Speciﬁcally, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classiﬁer in the over-parameterized setting. Our results precisely characterize the condition under which benign overﬁtting can occur in linear classiﬁcation problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression. 