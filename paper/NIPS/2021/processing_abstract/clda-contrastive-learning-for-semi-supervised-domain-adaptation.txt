Unsupervised Domain Adaptation (UDA) aims to align the labeled source distri-bution with the unlabeled target distribution to obtain domain invariant predictive models. However, the application of well-known UDA approaches does not gener-alize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where few labeled samples from the target domain are available. This paper proposes a simpleContrastive Learning framework for semi-supervised Domain Adaptation (CLDA) that attempts to bridge the intra-domain gap between the labeled and unlabeled target distributions and the inter-domain gap between source and unlabeled target distribution in SSDA. We suggest employing class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment between the original(input image) and strongly augmented unlabeled target images to mini-mize the intra-domain discrepancy. We have empirically shown that both of these modules complement each other to achieve superior performance. Experiments on three well-known domain adaptation benchmark datasets, namely DomainNet,Ofﬁce-Home, and Ofﬁce31, demonstrate the effectiveness of our approach. CLDA achieves state-of-the-art results on all the above datasets. 