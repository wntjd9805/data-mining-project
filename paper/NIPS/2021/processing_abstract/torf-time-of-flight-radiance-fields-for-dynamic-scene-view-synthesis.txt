Neural networks can represent and accurately reconstruct radiance ﬁelds for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-ﬂight (ToF) camera, and introduce a neural repre-sentation based on an image formation model for continuous-wave ToF cameras.Instead of working with processed depth maps, we model the raw ToF sensor mea-surements to improve reconstruction quality and avoid issues with low reﬂectance regions, multi-path interference, and a sensor’s limited unambiguous depth range.We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the beneﬁts and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.