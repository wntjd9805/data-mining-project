Most real world applications require dealing with stochasticity like sensor noise or predictive uncertainty, where formal speciﬁcations of desired behavior are inherently probabilistic. Despite the promise of formal veriﬁcation in ensuring the reliability of neural networks, progress in the direction of probabilistic speciﬁcations has been limited. In this direction, we ﬁrst introduce a general formulation of probabilistic speciﬁcations for neural networks, which captures both probabilistic networks (e.g., Bayesian neural networks, MC-Dropout networks) and uncertain inputs (distributions over inputs arising from sensor noise or other perturbations). We then propose a general technique to verify such speciﬁcations by generalizing the notion of Lagrangian duality, replacing standard Lagrangian multipliers with "functional multipliers" that can be arbitrary functions of the activations at a given layer. We show that an optimal choice of functional multipliers leads to exact veriﬁcation (i.e., sound and complete veriﬁcation), and for speciﬁc forms of multipliers, we develop tractable practical veriﬁcation algorithms.We empirically validate our algorithms by applying them to Bayesian NeuralNetworks (BNNs) and MC Dropout Networks, and certifying properties such as adversarial robustness and robust detection of out-of-distribution (OOD) data. On these tasks we are able to provide signiﬁcantly stronger guarantees when compared to prior work – for instance, for a VGG-64 MC-Dropout CNN trained on CIFAR-10 in a veriﬁcation-agnostic manner, we improve the certiﬁed AUC (a veriﬁed lower bound on the true AUC) for robust OOD detection (on CIFAR-100) from 0% → 29%. Similarly, for a BNN trained on MNIST, we improve on the (cid:96)∞ robust accuracy from 60.2% → 74.6%. Further, on a novel speciﬁcation – distributionally robust OOD detection – we improve on the certiﬁed AUC from 5% → 23%. 