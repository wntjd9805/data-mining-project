This paper presents new variance-aware conﬁdence sets for linear bandits and linear mixture Markov Decision Processes (MDPs). With the new conﬁdence sets, we obtain the follow regret bounds: bř• For linear bandits, we obtain an rOppolypdq kq data-dependent regret bound, where d is the feature dimension, K is the number of rounds, and σ2 k is the unknown variance of the reward at the k-th round. This is theﬁrst regret bound that only scales with the variance and the dimension but no explicit polynomial dependency on K. When variances are small, this bound can be signiﬁcantly smaller than the rΘ worst-case regret bound.K k“1 σ2 1 `K? d˘`• For linear mixture MDPs, we obtain an rOppolypd, log HqKq regret bound, where d is the number of base models, K is the number of episodes, andH is the planning horizon. This is the ﬁrst regret bound that only scales logarithmically with H in the reinforcement learning with linear function approximation setting, thus exponentially improving existing results, and resolving an open problem in [Zhou et al., 2020a].?We develop three technical ideas that may be of independent interest: 1) applica-tions of the peeling technique to both the input norm and the variance magnitude, 2) a recursion-based estimator for the variance, and 3) a new convex potential lemma that generalizes the seminal elliptical potential lemma. 