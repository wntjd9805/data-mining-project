Episodic training is a core ingredient of few-shot learning to train models on tasks with limited labelled data. Despite its success, episodic training remains largely understudied, prompting us to ask the question: what is the best way to sample episodes? In this paper, we ﬁrst propose a method to approximate episode sampling distributions based on their difﬁculty. Building on this method, we perform an extensive analysis and ﬁnd that sampling uniformly over episode difﬁculty out-performs other sampling schemes, including curriculum and easy-/hard-mining.As the proposed sampling method is algorithm agnostic, we can leverage these insights to improve few-shot learning accuracies across many episodic training algorithms. We demonstrate the efﬁcacy of our method across popular few-shot learning datasets, algorithms, network architectures, and protocols. 