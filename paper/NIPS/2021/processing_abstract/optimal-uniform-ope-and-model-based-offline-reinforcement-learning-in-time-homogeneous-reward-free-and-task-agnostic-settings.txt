This work studies the statistical limits of uniform convergence for ofﬂine policy evaluation (OPE) problems with model-based methods (for episodic MDP) and provides a uniﬁed framework towards optimal learning for several well-motivated ofﬂine tasks. Uniform OPE supΠ |Qπ − ˆQπ| < (cid:15) is a stronger measure than the point-wise OPE and ensures ofﬂine learning when Π contains all policies (the global class). In this paper, we establish an Ω(H 2S/dm(cid:15)2) lower bound (over model-based family) for the global uniform OPE and our main result establishes an upper bound of ˜O(H 2/dm(cid:15)2) for the local uniform convergence that applies to all near-empirically optimal policies for the MDPs with stationary transition. Here dm is the minimal marginal state-action probability. Critically, the highlight in achieving the optimal rate ˜O(H 2/dm(cid:15)2) is our design of singleton absorbing MDP, which is a new sharp analysis tool that works with the model-based approach. We generalize such a model-based framework to the new settings: ofﬂine task-agnostic and the ofﬂine reward-free with optimal complexity ˜O(H 2 log(K)/dm(cid:15)2) (K is the number of tasks) and ˜O(H 2S/dm(cid:15)2) respectively. These results provide a uniﬁed solution for simultaneously solving different ofﬂine RL problems. 