It is generally recognized that ﬁnite learning rate (LR), in contrast to inﬁnitesimalLR, is important for good generalization in real-life deep nets. Most attempted explanations propose approximating ﬁnite-LR SGD with Itˆo Stochastic DifferentialEquations (SDEs), but formal justiﬁcation for this approximation (e.g., (Li et al., 2019a)) only applies to SGD with tiny LR. Experimental veriﬁcation of the approx-imation appears computationally infeasible. The current paper clariﬁes the picture with the following contributions: (a) An efﬁcient simulation algorithm SVAG that provably converges to the conventionally used Itˆo SDE approximation. (b) A theo-retically motivated testable necessary condition for the SDE approximation and its most famous implication, the linear scaling rule (Goyal et al., 2017), to hold. (c)Experiments using this simulation to demonstrate that the previously proposed SDE approximation can meaningfully capture the training and generalization properties of common deep nets. 