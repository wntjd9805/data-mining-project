Generative ﬂows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of ﬂows and diffusion for categorical data such as language or image segmentation: ArgmaxFlows and Multinomial Diffusion. Argmax Flows are deﬁned by a composition of a continuous distribution (such as a normalizing ﬂow), and an argmax function.To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood. 