In this paper, we present token labeling—a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classiﬁcation loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to com-pute the training loss in a dense manner. Speciﬁcally, token labeling reformulates the image classiﬁcation problem into multiple token-level recognition problems and assigns each patch token with an individual location-speciﬁc supervision gen-erated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy onImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous mod-els (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pretrained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and model are publicly available at https://github.com/zihangJiang/TokenLabeling. 