Evaluating adversarial robustness amounts to ﬁnding the minimum perturbation needed to have an input sample misclassiﬁed. The inherent complexity of the underlying optimization requires current gradient-based attacks to be carefully tuned, initialized, and possibly executed for many computationally-demanding iterations, even if specialized to a given perturbation model. In this work, we overcome these limitations by proposing a fast minimum-norm (FMN) attack that works with different (cid:96)p-norm perturbation models (p = 0, 1, 2,), is ro-bust to hyperparameter choices, does not require adversarial starting points, and converges within few lightweight steps. It works by iteratively ﬁnding the sam-ple misclassiﬁed with maximum conﬁdence within an (cid:96)p-norm constraint of size (cid:15), while adapting (cid:15) to minimize the distance of the current sample to the decision boundary. Extensive experiments show that FMN signiﬁcantly out-performs existing (cid:96)0, (cid:96)1, and (cid:96)∞-norm attacks in terms of perturbation size, convergence speed and computation time, while reporting comparable perfor-mances with state-of-the-art (cid:96)2-norm attacks. Our open-source code is available at: https://github.com/pralab/Fast-Minimum-Norm-FMN-Attack.∞ 