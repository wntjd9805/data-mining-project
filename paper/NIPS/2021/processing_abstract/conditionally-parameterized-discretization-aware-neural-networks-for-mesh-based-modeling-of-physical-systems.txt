Simulations of complex physical systems are typically realized by discretizing partial differential equations (PDEs) on unstructured meshes. While neural net-works have recently been explored for the surrogate and reduced order modeling of PDE solutions, they often ignore interactions or hierarchical relations between input features, and process them as concatenated mixtures. We generalize the idea of conditional parameterization – using trainable functions of input param-eters to generate the weights of a neural network, and extend them in a ﬂexible way to encode critical information. Inspired by discretized numerical methods, choices of the parameters include physical quantities and mesh topology features.The functional relation between the modeled features and the parameters is built into the network architecture. The method is implemented on different networks and applied to frontier scientiﬁc machine learning tasks including the discovery of unmodeled physics, super-resolution of coarse ﬁelds, and the simulation of unsteady ﬂows with chemical reactions. The results show that the conditionally-parameterized networks provide superior performance compared to their tradi-tional counterparts. The CP-GNet - an architecture that can be trained on very few data snapshots - is proposed as the ﬁrst deep learning model capable of standalone prediction of reacting ﬂows on irregular meshes. 