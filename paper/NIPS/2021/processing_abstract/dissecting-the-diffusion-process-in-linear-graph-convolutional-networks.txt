Graph Convolutional Networks (GCNs) have attracted more and more attentions in recent years. A typical GCN layer consists of a linear feature propagation step and a nonlinear transformation step. Recent works show that a linear GCN can achieve comparable performance to the original non-linear GCN while being much more computationally efﬁcient. In this paper, we dissect the feature prop-agation steps of linear GCNs from a perspective of continuous graph diffusion, and analyze why linear GCNs fail to beneﬁt from more propagation steps. Fol-lowing that, we propose Decoupled Graph Convolution (DGC) that decouples the terminal time and the feature propagation steps, making it more ﬂexible and capa-ble of exploiting a very large number of feature propagation steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a large margin and makes them competitive with many modern variants of non-linear GCNs. Code is available at https://github.com/yifeiwang77/DGC. 