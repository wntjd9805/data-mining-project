There have been many recent advances in representation learning; however, unsu-pervised representation learning can still struggle with model identiﬁcation issues related to rotations of the latent space. Variational Auto-Encoders (VAEs) and their extensions such as β-VAEs have been shown to improve local alignment of latent variables with PCA directions, which can help to improve model dis-entanglement under some conditions. Borrowing inspiration from IndependentComponent Analysis (ICA) and sparse coding, we propose applying an L1 loss to the VAE’s generative Jacobian during training to encourage local latent variable alignment with independent factors of variation in images of multiple objects or images with multiple parts. We demonstrate our results on a variety of datasets, giving qualitative and quantitative results using information theoretic and modular-ity measures that show our added L1 cost encourages local axis alignment of the latent representation with individual factors of variation. 