Contrastive self-supervised learning (CSL) has attracted increasing attention for model pre-training via unlabeled data. The resulted CSL models provide instance-discriminative visual features that are uniformly scattered in the feature space.During deployment, the common practice is to directly ﬁne-tune CSL models with cross-entropy, which however may not be the best strategy in practice. Al-though cross-entropy tends to separate inter-class features, the resulting models still have limited capability for reducing intra-class feature scattering that exists inCSL models. In this paper, we investigate whether applying contrastive learning to ﬁne-tuning would bring further beneﬁts, and analytically ﬁnd that optimiz-ing the contrastive loss beneﬁts both discriminative representation learning and model optimization during ﬁne-tuning. Inspired by these ﬁndings, we proposeContrast-regularized tuning (Core-tuning), a new approach for ﬁne-tuning CSL models. Instead of simply adding the contrastive loss to the objective of ﬁne-tuning,Core-tuning further applies a novel hard pair mining strategy for more effective contrastive ﬁne-tuning, as well as smoothing the decision boundary to better ex-ploit the learned discriminative feature space. Extensive experiments on image classiﬁcation and semantic segmentation verify the effectiveness of Core-tuning. 