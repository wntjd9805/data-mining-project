Graph neural networks (GNNs) are widely used for modelling graph-structured data in numerous applications. However, with their inherently ﬁnite aggregation layers, existing GNN models may not be able to effectively capture long-range dependencies in the underlying graphs. Motivated by this limitation, we propose a GNN model with inﬁnite depth, which we call Efﬁcient Inﬁnite-Depth GraphNeural Networks (EIGNN), to efﬁciently capture very long-range dependencies.We theoretically derive a closed-form solution of EIGNN which makes training an inﬁnite-depth GNN model tractable. We then further show that we can achieve more efﬁcient computation for training EIGNN by using eigendecomposition.The empirical results of comprehensive experiments on synthetic and real-world datasets show that EIGNN has a better ability to capture long-range dependen-cies than recent baselines, and consistently achieves state-of-the-art performance.Furthermore, we show that our model is also more robust against both noise and adversarial perturbations on node features. 