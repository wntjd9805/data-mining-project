KL-regularized reinforcement learning from expert demonstrations has proved successful in improving the sample efﬁciency of deep reinforcement learning al-gorithms, allowing them to be applied to challenging physical real-world tasks.However, we show that KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations can suffer from patholog-ical training dynamics that can lead to slow, unstable, and suboptimal online learning. We show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efﬁciency and online policy performance. Finally, we show that the pathology can be remedied by non-parametric behavioral reference policies and that this allows KL-regularized reinforcement learning to signiﬁcantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks. 