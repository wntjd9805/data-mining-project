In the paper, we propose a class of efﬁcient mirror descent ascent methods to solve the nonsmooth nonconvex-strongly-concave minimax problems by using dynamic mirror functions, and introduce a convergence analysis framework to conduct rigorous theoretical analysis for our mirror descent ascent methods. For our stochastic algorithms, we ﬁrst prove that the mini-batch stochastic mirror descent ascent (SMDA) method obtains a gradient complexity of O(κ3(cid:15)−4) forﬁnding an (cid:15)-stationary point, where κ denotes the condition number. Further, we propose an accelerated stochastic mirror descent ascent (VR-SMDA) method based on the variance reduced technique. We prove that our VR-SMDA method achieves a lower gradient complexity of O(κ3(cid:15)−3). For our deterministic algorithm, we prove that our deterministic mirror descent ascent (MDA) achieves a lower gradientκ(cid:15)−2) under mild conditions, which matches the best known complexity of O( complexity in solving smooth nonconvex-strongly-concave minimax optimization.We conduct the experiments on fair classiﬁer and robust neural network training tasks to demonstrate the efﬁciency of our new algorithms.√ 