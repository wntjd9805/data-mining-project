Generative adversarial networks (GANs) have recently found applications in im-age editing. However, most GAN-based image editing methods often require large-scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high-quality, high-precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car.EditGAN builds on a GAN framework that jointly models images and their seman-tic segmentations [1, 2], requiring only a handful of labeled examples – making it a scalable tool for editing. Speciﬁcally, we embed an image into the GAN’s latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modiﬁes the image. To amortize op-timization, we ﬁnd “editing vectors” in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN’s training data. We demon-strate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks. Project page: https://nv-tlabs.github.io/editGAN. 