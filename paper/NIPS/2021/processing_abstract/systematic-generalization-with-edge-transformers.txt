Recent research suggests that systematic generalization in natural language un-derstanding remains a challenge for state-of-the-art neural models such as Trans-formers and Graph Neural Networks. To tackle this challenge, we propose EdgeTransformer, a new model that combines inspiration from Transformers and rule-based symbolic AI. The ﬁrst key idea in Edge Transformers is to associate vector states with every edge, that is, with every pair of input nodes—as opposed to just every node, as it is done in the Transformer model. The second major innovation is a triangular attention mechanism that updates edge representations in a way that is inspired by uniﬁcation from logic programming. We evaluate Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing1. In all three settings, the Edge Transformer outperformsRelation-aware, Universal and classical Transformer baselines. 