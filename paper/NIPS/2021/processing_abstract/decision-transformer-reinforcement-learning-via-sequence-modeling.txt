We introduce a framework that abstracts Reinforcement Learning (RL) as a se-quence modeling problem. This allows us to draw upon the simplicity and scalabil-ity of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Un-like prior approaches to RL that ﬁt value functions or compute policy gradients,Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can gen-erate future actions that achieve the desired return. Despite its simplicity, DecisionTransformer matches or exceeds the performance of state-of-the-art model-free ofﬂine RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.Figure 1: Decision Transformer architecture1. States, actions, and returns are fed into modality-speciﬁc linear embeddings and a positional episodic timestep encoding is added. Tokens are fed into a GPT architecture which predicts actions autoregressively using a causal self-attention mask. 1Our code is available at: https://sites.google.com/berkeley.edu/decision-transformer 35th Conference on Neural Information Processing Systems (NeurIPS 2021).Figure 2: Illustrative example of ﬁnding shortest path for a ﬁxed graph (left) posed as reinforcement learning. Training dataset consists of random walk trajectories and their per-node returns-to-go (middle). Conditioned on a starting state and generating largest possible return at each node, DecisionTransformer sequences optimal paths. 