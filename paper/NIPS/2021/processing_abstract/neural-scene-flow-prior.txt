Before the deep learning revolution, many perception algorithms were based on runtime optimization in conjunction with a strong prior/regularization penalty. A prime example of this in computer vision is optical and scene ﬂow. Supervised learning has largely displaced the need for explicit regularization. Instead, they rely on large amounts of labeled data to capture prior statistics, which are not always readily available for many problems. Although optimization is employed to learn the neural network, the weights of this network are frozen at runtime. As a result, these learning solutions are domain-speciﬁc and do not generalize well to other statistically different scenarios. This paper revisits the scene ﬂow problem that relies predominantly on runtime optimization and strong regularization. A central innovation here is the inclusion of a neural scene ﬂow prior, which uses the architecture of neural networks as a new type of implicit regularizer.Unlike learning-based scene ﬂow methods, optimization occurs at runtime, and our approach needs no ofﬂine datasets—making it ideal for deployment in new environments such as autonomous driving. We show that an architecture based exclusively on multilayer perceptrons (MLPs) can be used as a scene ﬂow prior.Our method attains competitive—if not better—results on scene ﬂow benchmarks.Also, our neural prior’s implicit and continuous scene ﬂow representation allows us to estimate dense long-term correspondences across a sequence of point clouds.The dense motion information is represented by scene ﬂow ﬁelds where points can be propagated through time by integrating motion vectors. We demonstrate such a capability by accumulating a sequence of lidar point clouds. 