This paper introduces the concept of offline reinforcement learning (offline RL) which aims to learn a reward-maximizing policy in an unknown Markov Decision Process (MDP) using static data generated from a behavior policy. Offline RL is applicable in situations where online exploration is challenging but historical data is abundant, such as in the fields of medicine and autonomous driving. The paper explores two threads of theoretical investigations in offline RL: offline policy evaluation (OPE) and offline learning. The authors propose a new perspective called uniform OPE, which unifies OPE and offline learning tasks. The paper focuses on the statistical limit and connection to optimal offline learning in the context of finite horizon stationary MDPs and model-based approaches. The results show that the global uniform OPE has a lower bound, while the local uniform OPE achieves a minimax rate, demonstrating optimal offline learning. The model-based approach is also highlighted for its adaptability to challenging tasks in offline RL.