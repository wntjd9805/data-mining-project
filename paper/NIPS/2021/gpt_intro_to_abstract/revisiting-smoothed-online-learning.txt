This paper introduces the concept of smoothed online learning (SOL), where an online learner incurs a switching cost for changing predictions between rounds. SOL is relevant in real-world applications where changing actions incurs additional costs. The paper focuses on two performance metrics for SOL: competitive ratio and dynamic regret with switching cost. The competitive ratio measures the worst-case ratio of the total cost incurred by the learner compared to the optimal cost, while dynamic regret with switching cost measures the difference between the learner's total cost and that of an arbitrary comparator sequence. The paper explores different algorithms, including the greedy algorithm and online balanced descent (OBD), and proposes a modified version of the Ader algorithm called Smoothed Ader (SAder) to minimize dynamic regret with switching cost in online convex optimization. The proposed algorithms achieve optimal regret bounds and adapt to changes in comparators.