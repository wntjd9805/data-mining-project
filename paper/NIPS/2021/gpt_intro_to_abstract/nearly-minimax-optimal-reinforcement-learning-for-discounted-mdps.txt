Reinforcement learning (RL) aims to design algorithms that learn optimal policies through interactions with an unknown dynamic environment. Markov decision processes (MDPs) are commonly used in RL due to their ability to describe state transitions over time. Discounted MDPs, a type of MDP, are often used to describe sequential tasks without interruption or restart. Several algorithms have been proposed to solve discounted MDPs with generative models. However, it remains unknown whether online RL algorithms without generative models can achieve optimality. This paper introduces a practical model-based algorithm, UCBVI-γ, for discounted MDPs. The algorithm leverages a refined Bernstein-type bonus and the law of total variance to provide tighter upper confidence bounds. The paper presents upper and lower bounds for the regret of UCBVI-γ, showing that it is nearly optimal up to logarithmic factors. The algorithm's regret is compared to previous online algorithms for learning discounted MDPs.