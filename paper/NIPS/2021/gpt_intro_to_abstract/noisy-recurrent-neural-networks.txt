Recent research has explored the idea of viewing recurrent neural networks (RNNs) as discretizations of ordinary differential equations (ODEs) driven by input data. This approach, known as the "formulate in continuous time, and then discretize" approach, has led to improved reliability and robustness to data perturbations. In this paper, we extend this concept to include noise in the form of stochastic differential equations (SDEs), resulting in Noisy RNNs (NRNNs). We demonstrate that NRNNs can serve as a stochastic learning strategy, with implicit regularization leading to wider minima and improved generalization and robustness. Additionally, we show the relationship between NRNNs and stochastic generalizations of neural ODEs, providing a framework for more reliable and robust RNN classifiers. We provide experimental results on benchmark datasets to validate our approach.