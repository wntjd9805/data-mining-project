The success of deep neural networks can be attributed to the intermediate features or representations learned by them. However, there is still limited understanding of how to characterize these representations and why representation learning occurs. This paper explores the idea of model stitching as a tool to study neural network representations, comparing it to other similarity metrics and providing evidence for the intuition that different models can have similar internal representations. The results confirm the intuitions that different models can learn similar representations and that better models with more data, larger width, or more training epochs can improve the performance of other models. This quantitative evidence provides insight into the behavior of neural representations that was not achievable with prior methods.