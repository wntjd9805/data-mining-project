Transformer-based pretrained language models have achieved impressive results in natural language processing tasks but require a large number of labeled examples for fine-tuning. This paper addresses the problem of transferring knowledge from multiple source tasks to improve a pretrained language model for few-shot text classification problems. The authors propose the use of gradients as features to represent tasks and modulate a base learner for rapid generalization. They utilize a pre-trained BERT model with adapter modules, which proves to be more efficient and effective compared to traditional fine-tuning and state-of-the-art meta-learning approaches. Experimental results demonstrate the superiority of their approach on a collection of diverse few-shot text classification tasks.