The cost of training language models, specifically Transformer-based language models, has become increasingly expensive. Transformers, despite their high training cost, have shown state-of-the-art performance on many Natural Language Processing tasks. One of the main computational bottlenecks in Transformers is the self-attention mechanism, which has high time and space complexity. This limits the ability of Transformers to process long sequences and large batch sizes with limited resources. In this paper, we address the challenges of improving computational efficiency and training stability in Transformers. We propose a new attention model called Kernelized Attention, which replaces the softmax structure with Gaussian kernels. We also introduce Skyformer, a method that accelerates kernelized attention by adapting the Nystr√∂m method. We provide theoretical analysis and experimental results showing that Skyformer achieves better accuracy with reduced computational costs compared to other methods. Overall, our contributions include revisiting the connection between self-attention and kernel methods, introducing kernelized attention to stabilize Transformers, and proposing Skyformer for accelerated kernelized attention.