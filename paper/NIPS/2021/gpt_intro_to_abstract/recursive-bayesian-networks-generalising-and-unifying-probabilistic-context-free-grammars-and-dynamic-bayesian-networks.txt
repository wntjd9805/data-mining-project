Long-term dependencies with a nested hierarchical structure pose a significant challenge in modeling sequential data across various domains. Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are two widely used models for sequential data, each with its own strengths. PCFGs excel at modeling hierarchical dependencies in symbolic data but struggle with parsing methods due to their discrete nature, while DBNs offer a fixed set of random variables for each time step but cannot represent nested hierarchical structures. In this paper, we propose Recursive Bayesian Networks (RBNs) as a novel class of probabilistic models that combine the strengths of PCFGs and DBNs, allowing for nested hierarchical dependencies alongside discrete or continuous random variables. This paper presents the theoretical framework of RBNs, generalizes inside and outside probabilities, derives analytic approximations for Gaussian RBNs, and provides evaluation results on synthetic data and hierarchical music analysis.