In this paper, we address the challenge of long-term forecasting in time series analysis. Current deep forecasting models, especially those based on Transformers, have shown promising results. However, traditional Transformers are computationally inefficient for long-term forecasting due to the quadratic complexity of the sequence length. To overcome this limitation, we propose an original architecture called Autoformer, which incorporates a decomposition strategy to extract predictable components from the time series. In addition, we introduce an Auto-Correlation mechanism that takes advantage of series periodicity to enhance the information aggregation process. Our approach achieves state-of-the-art accuracy on six benchmark datasets across various real-world applications.