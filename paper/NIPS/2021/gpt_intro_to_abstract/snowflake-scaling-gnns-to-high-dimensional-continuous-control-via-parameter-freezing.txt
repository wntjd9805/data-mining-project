Graph neural networks (GNNs) have emerged as powerful models for complex domains, such as drug discovery, fraud detection, computer vision, and particle physics. In the field of reinforcement learning, GNNs have shown promising results on locomotion control tasks with small state and action spaces, outperforming traditional multilayer perceptron (MLP) models. However, GNNs have struggled to scale effectively on higher-dimensional tasks, resulting in a trade-off between training task performance (MLPs) and transfer performance (GNNs). This paper investigates the factors contributing to poor GNN scaling and proposes a method called SNOWFLAKE, which freezes certain parameters in the GNN architecture to improve training performance in high-dimensional environments. Experimental results demonstrate that SNOWFLAKE significantly improves asymptotic performance, sample complexity, and zero-shot transfer compared to regular GNNs and MLPs on high-dimensional tasks.