This paper revisits the problem of supervised classification using quadratic features of the data and explores the influence of properties of the data distribution on generalization error. By leveraging results from matrix concentration, the authors propose an improved bound that uses more refined properties of the data distribution, such as the second moment matrix. The paper highlights the importance of intrinsic dimension in real-world data distributions and its impact on the sample complexity of classifiers, particularly neural networks. The authors also discuss the relevance of quadratic classifiers in understanding the role of intrinsic dimension in neural network learning. The paper presents theoretical and practical developments of nuclear-norm regularization for quadratic classification, including new bounds on Rademacher complexity and computable generalization error bounds. Experimental results on synthetic data demonstrate the influence of input distribution isotropy on the generalization properties of quadratic classifiers.