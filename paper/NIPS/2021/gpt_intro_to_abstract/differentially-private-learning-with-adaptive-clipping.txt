Deep learning has become widely used in various applications, leveraging large amounts of data for training. However, research has shown that trained models can extract sensitive information about individual training examples, raising privacy concerns. Differential privacy (DP) is a commonly used technique for quantifying and bounding data privacy leakage during learning tasks. In this paper, we focus on user-level DP, where neighboring datasets differ by the addition or removal of all data from a single user. We employ the Federated Averaging algorithm to achieve user-level DP by distributing training data on user devices and aggregating locally computed updates. Clipping the L2 norm of the model update and adding Gaussian noise ensures DP for the update, and composition techniques extend the privacy guarantee to the final model. However, setting an appropriate clipping threshold is crucial for maintaining both utility and privacy. The clipping bias-variance trade-off has been observed empirically and analyzed theoretically. Additionally, the norms of updates can vary during the training process, making it difficult to select a fixed clipping norm or a parameterized clipping norm schedule. Hyperparameters play a significant role in DP techniques, and tuning them can be computationally expensive and inefficient. This introduces additional costs for both computation and privacy. Therefore, finding efficient ways to set hyperparameters while maintaining privacy guarantees is essential for real-world systems.