The introduction of this computer science paper highlights the importance of estimating the geospatial location of images for various applications like robot navigation, 3D reconstruction, and autonomous driving. Cross-view geo-localization, which matches ground images with aerial/satellite images, is a promising solution to this problem. The use of convolutional neural networks (CNNs) with various techniques like NetVlad layers, capsule networks, and attention mechanisms has been explored, but their performance is hindered by the locality assumption. This paper proposes the use of Transformer, which excels in global contextual reasoning and enables flexible position-dependent representations. Additionally, a novel self-cross attention mechanism is introduced to enhance information flow across Transformer blocks. The proposed model, called L2LTR, achieves state-of-the-art performance in cross-view matching tasks, demonstrating its superiority in learning visually discriminative and position-aware representations.