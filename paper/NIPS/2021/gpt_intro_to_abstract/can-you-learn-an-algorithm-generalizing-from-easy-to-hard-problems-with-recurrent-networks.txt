This paper explores the ability of recurrent neural networks to extrapolate their knowledge and solve harder problems by increasing their iteration budget. Results show that recurrent networks can generalize to more challenging problems without the need for additional training or parameters. This ability is unique to recurrent networks and cannot be achieved by standard feed-forward networks. The study focuses on three reasoning problems - computing prefix sums, solving mazes, and playing chess - and demonstrates that increasing the iteration budget improves the performance of recurrent models. The paper aims to create recurrent architectures that encode scalable problem-solving methods rather than memorizing input-output mappings.