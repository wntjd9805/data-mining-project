Self-supervised learning has become an attractive approach for learning effective deep representations in computer vision, natural language processing, and machine learning tasks. Previous methods in this area have focused on generating multiple views and assuming one view to be predictive of another. However, we identify a problem with this popular augmentation pipelineâ€”excessive distortions applied to the original image can produce views that deviate significantly in semantics. We argue that these deviated views behave as out of distribution (OOD) samples, leading to a failure of the model to generalize in certain parameter regions. We empirically demonstrate the detrimental impact of OOD noise on downstream task performance and propose a new model, UOTA, that effectively suppresses the influence of such noise without introducing extra computational complexity. The UOTA model achieves improved performance over a range of self-supervised learning approaches by automatically balancing the bias-variance trade-off in the mean squared error of the deep estimator. We emphasize the significance of OOD samples and the bias-variance trade-off in self-supervised learning and present the first formal analysis of this issue.