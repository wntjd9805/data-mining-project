This paper investigates the theoretical properties of Policy Gradient (PG) methods for Reinforcement Learning (RL). The PG method parameterizes the policy function and conducts gradient ascent search to improve the policy. The main problem considered in this paper is the policy optimization for a general utility function. The paper aims to close the gap in sample complexity between PG methods and value-based methods. Additionally, the paper proposes a gradient truncation mechanism to address the issue of distribution shift in variance-reduced PG methods. The paper also explores policy optimization for nonlinear utility functions and develops the TSIVR-PG algorithm to solve the problem. The technical contributions include avoiding the estimation of Jacobian matrix and analyzing the convergence of SARAH/Spider methods.