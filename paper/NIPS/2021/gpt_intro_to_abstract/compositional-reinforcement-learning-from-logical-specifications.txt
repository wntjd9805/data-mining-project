Reinforcement learning (RL) is a promising approach for learning control policies in continuous control tasks, but specifying the goal can be challenging. Existing approaches require the user to manually compose rewards for subtasks, leading to poor reward functions. Recent work has introduced high-level languages for specifying RL tasks, but they often rely on model-free RL and struggle with long-horizon tasks. In this paper, we propose DIRL, a compositional RL algorithm that leverages the structure of the specification to decompose the problem into high-level planning and low-level control. We convert the specification into an abstract graph and use a forward graph search algorithm to compute a sequence of subtasks. Our algorithm outperforms state-of-the-art deep RL algorithms for learning policies from specifications, demonstrating its effectiveness in complex continuous control tasks.