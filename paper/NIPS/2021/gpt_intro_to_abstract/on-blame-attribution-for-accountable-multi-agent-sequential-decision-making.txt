With the increasing use of artificial intelligence (AI) in everyday life, accountability has become a central concern in AI research. While previous work has focused on explainability in AI systems, this paper addresses the other critical aspect of accountability - blame attribution. Specifically, the authors study the task of allocating blame to agents in multi-agent decision making, where blame represents the degree of an agent's contributions to the inefficiency of the system. Drawing on concepts from cooperative game theory, the authors develop and analyze blame attribution methods that satisfy desirable properties, such as performance monotonicity and Blackstone consistency. They also explore the impact of uncertainty on these methods through simulation-based experiments.