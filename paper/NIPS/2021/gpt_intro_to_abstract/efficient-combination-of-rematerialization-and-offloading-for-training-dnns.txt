In this paper, we address the issue of increasing model size in machine learning, particularly in the field of Natural Language Processing (NLP), and the resulting memory challenges on GPU architectures. We focus on reducing memory usage through techniques such as rematerialization and ofﬂoading. Rematerialization involves deleting unnecessary activations from memory during the forward phase and recomputing them during the backward phase, while ofﬂoading aims to minimize memory requirements by trading transfers between the CPU and GPU for computation. We present a solution to combine rematerialization and ofﬂoading optimally, which has not been explored in previous literature. The related work, model and notations, algorithmic solution, and experimental results are discussed in subsequent sections.