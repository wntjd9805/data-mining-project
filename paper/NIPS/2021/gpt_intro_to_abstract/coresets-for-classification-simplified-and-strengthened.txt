Coresets are widely used in scalable machine learning to select a smaller subset of data points that can yield near-optimal results compared to the full dataset. In this paper, we focus on coresets for linear classification, where the goal is to compute the minimizer of the classiﬁcation loss function. We introduce the concept of relative error coresets and propose a method that uses Lewis weights to construct such coresets for various hinge-like loss functions. Our main result shows that sampling ˜O points according to Lewis weights and reweighting appropriately can achieve a relative error guarantee for logistic loss, hinge loss, and other similar loss functions. We compare our method with existing approaches and demonstrate its superior performance through experimental evaluation.