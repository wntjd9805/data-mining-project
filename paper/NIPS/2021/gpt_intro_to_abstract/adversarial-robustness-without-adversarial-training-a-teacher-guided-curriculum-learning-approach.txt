This paper addresses the issue of adversarial robustness in deep neural network (DNN) models used for vision problems. Goodfellow et al. identified the problem of DNN models predicting different outcomes for similar images. Many techniques for fooling or defending against adversarial attacks have been proposed, but they often come with additional costs or rely on iterative optimization. In this work, the authors propose a non-iterative robust training method that achieves adversarial robustness by restricting perturbations to object pixels. Their method outperforms other techniques in terms of both natural and adversarial accuracies and requires significantly less training time compared to adversarial training techniques. Experimental results on various datasets support the effectiveness of their approach.