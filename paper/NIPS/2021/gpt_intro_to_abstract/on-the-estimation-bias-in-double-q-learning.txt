Value-based reinforcement learning with neural networks has shown great promise in solving complex decision-making problems across various real-world applications. However, one issue that has been observed is the overestimation bias in value prediction, particularly in Q-learning algorithms. This bias can propagate through greedy action selections and lead to suboptimal decision-making. One method to address this issue is double Q-learning, which uses a second value function to reduce the risk of overestimation. In this paper, we present a different perspective on how one-step errors accumulate in stationary solutions and propose a novel value estimator called doubly bounded estimator. We demonstrate the effectiveness of our approach through extensive evaluations on Atari benchmark tasks, showing significant improvements in sample efficiency and convergence performance compared to baseline algorithms.