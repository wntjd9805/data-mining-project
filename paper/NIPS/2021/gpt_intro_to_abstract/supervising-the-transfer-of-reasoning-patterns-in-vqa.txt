This paper introduces the problem of Visual Question Answering (VQA), where a model is tasked with answering questions based on images. The current state-of-the-art models in VQA rely on dataset biases and shortcuts, leading to a lack of generalization. The authors propose a new method for transferring reasoning patterns from models trained on perfect visual input to models trained on noisy visual representations. They use program prediction as an additional auxiliary loss to supervise the sequence of reasoning operations, which helps maintain the learned function's objective during knowledge transfer. The paper provides a theoretical analysis and experimental results demonstrating the effectiveness of their proposed approach.