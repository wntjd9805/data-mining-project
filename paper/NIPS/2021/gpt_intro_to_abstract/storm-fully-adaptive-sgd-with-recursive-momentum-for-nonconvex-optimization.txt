Non-convex models have become increasingly important in machine learning and data science over the past decade. Stochastic gradient descent (SGD) is the main method for training these models, but its performance is highly influenced by the learning rate, which requires careful tuning. Adaptive approaches like AdaGrad and Adam, as well as non-adaptive heuristics, are popular but still require hyper-parameter tuning. Momentum-based methods have proven to be crucial, but the setting of momentum and its interplay with the learning rate have not been extensively studied. In this work, we explore momentum-based adaptive methods for stochastic non-convex optimization problems and aim to find an approximate stationary point. We introduce STORM+, a parameter-free method that achieves optimal convergence rates and adapts to the variance of gradients. We also propose a novel way to set the learning rate by introducing an adaptive interplay between learning rate and momentum parameters.