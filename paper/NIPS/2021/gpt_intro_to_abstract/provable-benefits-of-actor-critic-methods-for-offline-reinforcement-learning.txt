The problem of offline or batch policy learning in reinforcement learning (RL) poses unique challenges due to incomplete information about the Markov decision process (MDP) encoded in the available dataset. In this paper, we address the problem of policy learning using linear function approximation in the offline setting and propose an actor-critic algorithm, called PACLE, that incorporates pessimism to optimize a lower bound on the value of the optimal policy. We show that the PACLE algorithm enjoys online learning-style guarantees and provide tight confidence intervals and estimation error bounds. Theoretical results, including an upper bound and a minimax lower bound, are presented along with a detailed discussion of their consequences.