This paper explores the idea of treating reinforcement learning as a sequence generation problem and investigates whether contemporary sequence modeling techniques can be used as a viable reinforcement learning algorithm. The authors propose a Trajectory Transformer model that utilizes a Transformer architecture to model the distribution of state-action-reward trajectories. Instead of traditional trajectory optimizers, they use beam search as a planning algorithm. This unified approach handles various aspects of reinforcement learning, such as actor-critic algorithms and model-based learning, within a single sequence model framework. The authors evaluate their model in the offline regime using prior interaction data and show that the Trajectory Transformer outperforms conventional dynamics models in long-horizon prediction. When decoded using a modified beam search procedure, the model achieves competitive results on offline RL benchmarks and demonstrates effectiveness in imitation learning, goal-reaching, and planning for sparse-reward tasks. The findings suggest that the algorithms and architectures used in unsupervised learning can bring similar benefits to reinforcement learning.