In this paper, the authors discuss the limitations of existing efficient Transformers in vision tasks and propose a novel softmax-free Transformer called SOFT. They introduce a Gaussian kernel-based self-attention mechanism without the need for softmax normalization, leading to linear complexity in both computation and memory usage. They also develop a low-rank matrix decomposition algorithm for approximation with guaranteed robustness. Extensive experiments demonstrate that the SOFT models outperform state-of-the-art CNNs and ViT variants in visual recognition tasks, achieving a better accuracy/complexity trade-off.