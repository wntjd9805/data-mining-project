This paper addresses the concern of assessing confidence in deep neural networks (DNN) predictions for critical safety systems by quantifying their generalization capability. Existing methods based on VC-dimension and norm-based bounds do not consider test samples, making them ineffective for out-of-distribution (OOD) detection. The paper introduces the pNML learner, which assigns probabilities to potential outcomes by adding the test sample to the training set and finding the ERM solution. The pNML regret, associated with the generalization error, is derived analytically for a single layer NN and is shown to be low for inputs in certain subspaces or far from the decision boundary. The pNML regret is adapted to pretrained DNNs using the softmax function without requiring additional data. Evaluation on various benchmarks demonstrates superior performance in OOD detection.