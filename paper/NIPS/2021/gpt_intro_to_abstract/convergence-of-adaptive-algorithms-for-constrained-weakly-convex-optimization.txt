Adaptive first-order methods such as AdaGrad, RMSprop, and Adam have been widely used in neural network training. However, recent work has identified technical issues with Adam and RMSprop and proposed a new variant called AMSGrad. While the convergence of these methods has been studied for convex optimization problems, their behavior for constrained and/or nonsmooth stochastic nonconvex optimization remains open. In this paper, we establish the convergence of AMSGrad for solving a specific optimization problem with weak convexity and nonconvex domains. We show that AMSGrad achieves a logarithmic convergence rate and can be used with a mini-batch size of 1 and constant moment parameters. Our results are compared with state-of-the-art methods, and we also present a numerical experiment demonstrating the robustness of AMSGrad compared to SGD and SGD with momentum.