Reinforcement learning (RL) is a popular framework in which an agent interacts with an unknown environment, receives feedback, and optimizes its performance accordingly. Learning from real-world samples can be slow, costly, or dangerous, while learning from simulations can be fast, cheap, and safe. However, the reality-gap can limit the fidelity of simulations, leading to the question of whether we can bridge the gap and train an agent that performs well in both environments. In this paper, we propose a method to simultaneously learn from real and simulated environments by controlling the rate at which we collect and use samples from each. We analyze the convergence of our approach and provide theoretical analysis of the properties of the replay buffer. Additionally, we demonstrate our findings in a sim-to-real simulation.