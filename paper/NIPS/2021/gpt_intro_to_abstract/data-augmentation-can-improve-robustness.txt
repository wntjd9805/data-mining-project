Despite the success of neural networks, they are not inherently robust, as adversarial perturbations can cause incorrect predictions with high confidence. Various defenses have been proposed, but many are easily broken by different adversaries. This paper explores the combination of model weight averaging with data augmentation techniques to improve robustness. The authors demonstrate that data augmentation techniques such as Cutout, CutMix, and MixUp can improve robustness when combined with model weight averaging. They achieve new state-of-the-art robust accuracies and conduct experiments to show generalizability and explore the trade-off between robust overfitting and underfitting. They also provide empirical evidence of how weight averaging exploits data augmentation by ensembling model snapshots.