Attentional mechanisms are widely used in deep architectures and have greatly improved the performance of various tasks involving data with temporal and spatial orders. Unlike recurrent or convolutional architectures, attentional mechanisms are order invariant, allowing models to access information at any position in a sequence or space. However, capturing positional information is crucial for Transformer-alike models. In this paper, we focus on designing a position encoding method for multi-dimensional spatial positions, such as pixel positions in images or object bounding boxes. Existing methods fall short in capturing desired positional similarity and suffer from scalability issues. To address these challenges, we propose a novel positional encoding method that maps multi-dimensional positions into a vector space using Fourier features and an MLP. Our method is learnable, parameter-efficient, and capable of handling test samples with arbitrary length. We demonstrate the effectiveness of our approach in tasks such as image generation, object detection, image classification, and natural language generation in graphical user interfaces, outperforming existing methods in terms of accuracy and learning speed.