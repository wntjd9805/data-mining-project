Simultaneous multimodal sensations are important for human perceptual learning, but designing a unified model for modality fusion is challenging in artificial learning systems. The differences in input representations between audio and vision pose additional challenges. Most existing multimodal fusion approaches involve ad-hoc schemes like "late-fusion" which integrate separate audio and visual networks via their output representations. In this paper, we propose a new transformer-based model called Multimodal Bottleneck Transformer (MBT) for audiovisual fusion in video. MBT restricts the flow of cross-modal information between latent units through fusion bottlenecks, which allows the model to collect and condense relevant inputs from each modality. This approach avoids the computational cost of full pairwise attention, resulting in improved performance with less computation. We demonstrate the effectiveness of MBT on image and spectrogram patches and achieve state-of-the-art results on various audio-visual benchmarks.