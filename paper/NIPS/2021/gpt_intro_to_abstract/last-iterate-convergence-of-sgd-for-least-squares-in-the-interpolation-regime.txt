Stochastic gradient descent (SGD) is a widely used algorithm in large-scale statistics and optimization. Its versatility and adaptability make it a popular choice for supervised machine learning problems. However, the recent success of deep neural networks has brought new challenges to SGD. In this paper, we examine how SGD can benefit from a noiseless model, where the model perfectly fits the data and generalizes well. We explore the convergence properties of SGD in this setting and provide insights into the behavior of the final iterate of the algorithm. Our contributions include showing the convergence rate of constant step-size SGD and deriving bounds for the non-parametric least-squares problem. We also present an explicit recursion on the eigenspaces of the covariance matrix, which is of interest for future analysis. Our work provides a direct Lyapunov technique to handle the fluctuations induced by the stochasticity of the gradients without the need for variance reduction methods.