Graph neural networks (GNNs) have gained significant attention due to their effectiveness in real-world tasks and their connections to spectral graph theory. However, training GNNs on large-scale graphs can be resource-intensive, and transferring learned information across different graphs and domains remains a challenge. In this paper, we propose a framework for transfer learning of GNNs, focusing on direct transfer between source and target graphs. We introduce a novel view of graphs as samples from joint distributions of ego-graph structures and node features, and design a transferable GNN model based on this view. We analyze the transferability of GNNs by evaluating the differences in node features and graph structures between the source and target graphs. Our theoretical conclusions are validated through synthetic experiments and real-world evaluations, demonstrating the effectiveness of our proposed framework in improving GNN transfer learning performance.