We consider solving finite-sum optimization problems using stochastic gradient descent (SGD), which is commonly used in machine learning for large-scale problems. While the classical approach is to sample indices uniformly at random, in practice, without-replacement sampling is more common. Analyzing the performance of without-replacement schemes has been challenging due to the statistical dependencies created between iterations. In this paper, we rigorously study the benefits of without-replacement sampling and find that it does not significantly improve over with-replacement sampling unless the number of epochs exceeds the problem's condition number. We provide lower and upper bounds for both single shuffling and random reshuffling schemes, showing the limitations of without-replacement sampling. Our findings are supported by experiments on quadratic functions.