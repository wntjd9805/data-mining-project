Kernel machines generally work well with low-dimensional and small to medium-scaled data, but the choice of kernel function is often limited to standard options. Recent developments in kernel learning aim to enhance the expressiveness of kernels but still face limitations. This paper proposes a generic framework called Kernel Functional Optimization (KFO) to address these shortcomings. KFO provides a flexible form of kernel learning with computational efficiency and allows for the use of Bayesian optimization to find the best kernel. Hyperkernels are incorporated into the Bayesian framework, and methods are provided to ensure positive definiteness of the kernels. The proposed method is evaluated on synthetic and real-world datasets, demonstrating superior performance compared to state-of-the-art baselines. The contributions of this paper include the introduction of a novel approach for finding the best non-parametric kernel using hyperkernels and Bayesian functional optimization, methods for ensuring positive definiteness, convergence guarantees, and empirical results proving usefulness.