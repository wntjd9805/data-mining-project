Image representations that generalize well are crucial for various computer vision tasks, including image retrieval, face identification, and image classification. However, achieving effective generalization to out-of-distribution (OOD) data is more challenging than in-distribution. Current Deep Metric Learning (DML) benchmarks use fixed data splits, which limits the evaluation of generalization and fails to capture distribution shifts. In this paper, we propose a new benchmark called ooDML to evaluate OOD generalization in changing zero-shot learning settings. We construct training and testing data splits of increasing difficulty and assess the performance of current DML approaches. Our experiments demonstrate that the standard evaluation splits are often similar to i.i.d. settings, while our ooDML benchmark provides a more comprehensive perspective on OOD generalization. Additionally, we conduct a large-scale study of representative DML methods on ooDML, considering different regularizations and observing how conceptual differences impact performance as the distribution shift becomes harder. Finally, we explore few-shot DML as a means to achieve systematic and consistent OOD generalization. Overall, our contributions include proposing the ooDML benchmark, analyzing the current DML method landscape, and examining few-shot DML for improved OOD generalization.