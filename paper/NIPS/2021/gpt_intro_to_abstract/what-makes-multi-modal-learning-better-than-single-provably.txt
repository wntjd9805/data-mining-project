In this paper, we investigate the performance of multi-modal learning compared to uni-modal learning in the context of deep multi-modal research. While deep multi-modal learning has shown promising results in practice, the theoretical understanding of its performance is limited. Existing works often make strict assumptions on probability distributions across different modalities that may not hold in real-life applications, and they do not consider the generalization performance of multi-modal learning. Therefore, we address the fundamental question of whether multi-modal learning can provably outperform uni-modal learning. We analyze the performance of multi-modal learning from two perspectives: under what conditions multi-modal learning performs better than uni-modal learning, and what factors contribute to the performance gains. We propose a theoretical framework based on multi-modal fusion approaches and provide the first theoretical analysis of the relationship between the population risk and the distance between the learned latent representation and the true representation. We focus on the scenario where there are M and N modalities, with N being a subset of M, and demonstrate that the latent representation learned from M modalities is closer to the true representation compared to the representation learned from N modalities. We conduct experiments to validate our theoretical observations and derive practical implications for modality selection in multi-modal learning.