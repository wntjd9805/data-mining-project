Modern machine learning encoder-decoder architectures have achieved impressive performance on sequence-to-sequence tasks such as machine translation, language modeling, and speech-to-text, largely due to the use of attention mechanisms. However, despite their success, there is still limited understanding of how these architectures solve tasks using attention. In this paper, we analyze three different encoder-decoder architectures and propose a method for decomposing hidden state dynamics to isolate input and temporal behaviors. Through our analysis, we shed light on the mechanisms underlying attention formation and explore how they vary across tasks and architectures. Specifically, we investigate the role of temporal and input components in determining the attention matrix, showing that the input behavior plays a larger role and that architectures with attention and/or recurrence exhibit different dynamics. Additionally, we study synthetic settings to gain insights into the mechanisms of common sequence-to-sequence structures. Our contributions include a decomposition approach that aids in understanding network behavior and insights into the dynamics of attention mechanisms in various architectures and tasks.