Machine learning models are increasingly being used for consequential decision-making tasks, such as loan approval and medical diagnosis. To provide transparency and recourse for individuals affected by these decisions, various methods have been developed to explain the decisions made by these models. One commonly used method is counterfactual explanations, which aim to identify the minimal changes an individual can make to receive a positive outcome. However, there is limited research on the limitations of counterfactual explanations, particularly in relation to the stability and reliability of the results, their susceptibility to manipulation, and their trustworthiness for fairness assessments. In this paper, we propose a formal framework to address these limitations, specifically focusing on counterfactual explanations that rely on hill-climbing. We demonstrate the sensitivity of these explanations to small input changes and their vulnerability to manipulation. We also introduce a novel training objective for adversarial models that seemingly provide fair recourse but can be easily manipulated by adding perturbations. We evaluate our framework on various datasets and counterfactual explanation methods, highlighting the vulnerability of these explanations and proposing techniques to enhance their robustness.