This paper investigates the characteristics and performance of visual transformers (ViT) compared to convolutional neural networks (CNNs) for robustness and generalization in safety-critical applications. The authors analyze three transformer families (ViT, DeiT, and T2T) across fifteen vision datasets, focusing on the fundamental differences between convolution and self-attention. The results show that ViTs exhibit strong robustness against occlusions, distributional shifts, adversarial perturbations, and patch permutations. Additionally, off-the-shelf ViT features demonstrate exceptional generalization capabilities. The paper also introduces novel design choices, including an architectural modification to encode shape information and an ensemble-based feature transfer approach. Overall, the findings highlight the potential of ViTs for addressing challenges in safety-critical applications and achieving state-of-the-art performance.