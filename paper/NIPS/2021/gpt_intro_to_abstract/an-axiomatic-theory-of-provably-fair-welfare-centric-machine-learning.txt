Contemporary machine learning systems exhibit differential accuracy and bias across gender, race, and other protected groups. This exacerbates existing inequality, leading to disproportionate false-arrest rates in policing and health disparities in medical settings. To address these issues, welfare-centric machine learning methods have been proposed to optimize fairness and accuracy. However, traditional welfare metrics rely on positive utility values, which are not natural for many machine learning tasks that minimize negatively connotated loss values. In this paper, we introduce the concept of malfare, which measures societal harm, and cast fair machine learning as a malfare minimization problem. We develop a generic notion of fair machine learning called fair-PAC (FPAC) learning, which guarantees epsilon-optimal malfare with high probability. We demonstrate that many standard PAC-learners can be converted to FPAC learners, and that malfare-minimization naturally generalizes risk-minimization to produce fairness-sensitive machine learning objectives. Additionally, we explore the statistical and computational aspects of FPAC learning, showing that it is equivalent to PAC learning for certain loss functions, and that it can be achieved in polynomial time under convex optimization assumptions.