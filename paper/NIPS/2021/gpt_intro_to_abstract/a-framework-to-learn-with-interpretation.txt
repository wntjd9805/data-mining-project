Interpretability in machine learning systems has become a highly researched topic due to their widespread adoption in various domains, including law, healthcare, and defense. In this paper, we focus on the concept of interpretability and distinguish it from explainability, defining interpretability as the ability to provide human-understandable insights into the decision-making process. We explore two main approaches to interpreting complex models, namely post-hoc approaches and "by design" methods. Post-hoc approaches analyze pre-trained systems locally, while "interpretable by design" methods integrate interpretability into the learning process itself. However, both approaches have their advantages and drawbacks. To address this challenge, we propose a new task called Supervised Learning with Interpretation (SLI), which involves jointly learning a predictive model and an interpreter model to achieve both interpretability and prediction accuracy. We introduce FLINT (Framework to Learn With INTerpretation) as a solution for SLI when interpreting deep neural network classifiers. FLINT incorporates high-level attribute functions that leverage the outputs of hidden layers of the neural network, enabling both local and global interpretations. We also present a criterion for promoting conciseness and diversity in the attribute functions and conduct experiments on image classification datasets to demonstrate the effectiveness of FLINT compared to state-of-the-art approaches. Finally, we specialize FLINT for post-hoc interpretability and provide additional results in the supplementary material.