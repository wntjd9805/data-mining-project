Classification tasks rely on labeled data, but obtaining annotations can be expensive and time-consuming, particularly in domains like medicine. Active Learning (AL) aims to accelerate the learning process by selectively choosing data to be annotated. However, a comprehensive theory of AL is lacking, and considerations such as the cost of training deep neural networks and expert capabilities need to be addressed. This paper proposes a novel acquisition function for AL that focuses on the uncertainty component and utilizes proper scoring rules instead of classification errors. The approach is evaluated in the context of text classification using pretrained language models and neural network ensembles. Additionally, the paper introduces a method to combine ensembling and validation sets, as well a technique to achieve diversity in batch comparisons. Experimental results demonstrate the superiority of the proposed AL model over existing techniques.