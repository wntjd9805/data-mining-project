Reinforcement learning (RL) has numerous applications in various fields, making the development of trustworthy RL systems crucial. While adversarial attacks on supervised learning models have been extensively studied, there is still limited understanding of adversarial attacks on RL models. This paper introduces a suite of novel attacks called action poisoning attacks, where an attacker manipulates the agent's actions. The paper investigates action poisoning attacks in both white-box and black-box settings and proposes attack strategies that can force RL agents to choose actions according to the attacker's target policy. The proposed black-box attack scheme, LCB-H, is the first to provably work against RL agents. The study also explores the impact of the LCB-H attack on the UCB-H algorithm and highlights the importance of understanding the risks of adversarial attacks for the safe application of RL models.