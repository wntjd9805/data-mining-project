Uncertainty quantification (UQ) in machine learning plays a crucial role in quantifying the confidence of predictions, which has implications in various applications. Different methods exist for representing predictive uncertainty, such as parametric distribution estimates, density function estimates, and quantile estimates. Quantiles, in particular, offer advantages in modeling complex distributions, interpretability, construction of prediction intervals, and efficient sampling. This paper explores the limitations of the commonly used pinball loss in quantile regression and proposes alternative methods to address these limitations. The proposed methods include model agnostic quantile regression, explicit balancing of calibration and sharpness, centered intervals, and encouraging individual calibration. The paper presents experimental results on benchmark datasets and a real-world uncertainty estimation task in nuclear fusion.