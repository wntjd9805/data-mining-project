This paper introduces a new framework for estimating the generalization capability of trained neural networks. The framework evaluates the accuracy of a network on perturbed training samples and constructs perturbation response (PR) curves. From these curves, two new measures called Gi-score and Pal-score are derived to compare a network's PR curve to that of an idealized network unaffected by perturbation. The proposed framework outperforms the state-of-the-art approach in predicting generalization on various tasks. Furthermore, the framework can also predict a network's ability to be invariant to specific perturbations.