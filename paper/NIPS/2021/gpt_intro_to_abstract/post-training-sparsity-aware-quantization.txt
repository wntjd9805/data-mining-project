Deep neural networks (DNNs) are widely used in various applications, but they require significant computational resources. Quantization is a common method to improve DNN execution performance, but reducing the number of bits often leads to decreased model accuracy. This paper presents sparsity-aware quantization (SPARQ), a method that leverages activation sparsity to minimize quantization noise. SPARQ uses both granularities of entire integer 8-bit values and INT8 representation zero-value bits for quantization. The paper proposes different configurations of SPARQ and evaluates its performance on image classification models, comparing it with previous post-training quantization methods. The results show that SPARQ achieves a good trade-off between model accuracy and hardware overhead, making it a practical choice for implementation on systolic arrays, Tensor Cores, and NVIDIA Sparse Tensor Cores.