Enabling free-viewpoint video of a human in motion, based on a sparse set of views, is a challenging task with numerous applications in fields such as immersive visualization, virtual clothing, fitness, and AR/VR. Previous research has primarily focused on static scenes and rigid objects, while novel-view synthesis for dynamic scenes, especially those involving human motion, remains complex. This study proposes a unified model that combines robust reconstruction and photo-realistic rendering to support reliable viewpoint generalization. To handle the complexity of dynamic scenes, the proposed model incorporates additional domain knowledge in the form of a human body model. By leveraging this model, the system can generate novel views, poses, and shapes, even beyond the ones observed during training, facilitating free-viewpoint rendering of human subjects. The framework ensures both high-quality image rendering and plausible temporal reconstructions for the visualization of human performance capture.