This paper focuses on advancing the analytical understanding of the Hessian map of a neural network. The authors aim to analyze the structural properties of the Hessian induced by the model architecture, particularly its range and rank deficiency. By understanding the range of the Hessian map, insights can be gained into how gradients change between iterations. The paper introduces an exact formula and tight upper bound on the rank of the Hessian, which reveal a significant redundancy in the parameterization of neural networks. This provides a precise measure of the complexity of neural networks and sheds light on the degree of overparameterization. The contributions of the paper include characterizing the structure of the Hessian range, proving tight bounds and formulas for the Hessian rank, demonstrating the validity of these bounds throughout training, establishing degeneracy of the Hessian at the minimum, and investigating the effects of architectural components on rank. The analysis also reveals interesting properties of the Hessian spectrum and additional redundancies due to repeated eigenvalue plateaus.