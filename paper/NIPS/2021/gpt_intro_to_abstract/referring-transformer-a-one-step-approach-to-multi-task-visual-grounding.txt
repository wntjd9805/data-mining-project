This paper introduces a one-stage transformer-based architecture for multi-modal grounding tasks, specifically referring expression comprehension (REC) and segmentation (RES). Unlike existing two-stage approaches, this architecture simultaneously performs REC and RES without the need for a proposal mechanism. It leverages a transformer encoder to encode image and lingual context, and a contextualized transformer decoder to directly decode contextualized phrase queries into bounding boxes and segmentation masks. The model outperforms state-of-the-art methods on both REC and RES tasks and benefits from pre-training on an external dataset. The paper also demonstrates the effectiveness of multi-task learning and provides detailed ablations to validate the design.