We consider the problem of supervised learning where we want to find a prediction function θ mapping an input point x to a label y and is learned from a set of observations. The goal is to find θ that minimizes the expected risk L and derive probabilistic upper bounds on the excess risk. This paper focuses on the upper bounds for a specific estimator θ and introduces the concept of source and capacity conditions for achieving faster rates of convergence. The paper also discusses the limitations of Tikhonov regularization and proposes the iterated Tikhonov regularization scheme as an alternative that addresses these limitations. The main result of the paper is a probabilistic upper bound on the excess risk that is optimal under certain assumptions on the learning task.