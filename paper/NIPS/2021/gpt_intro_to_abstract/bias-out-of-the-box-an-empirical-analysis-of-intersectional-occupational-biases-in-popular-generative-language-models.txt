This paper examines the potential biases in popular open-source text generation models, focusing on the small GPT-2 model. The increased use of these models in various applications highlights the need to understand biases towards protected classes. The paper discusses the importance of specifying biases and considers both representational and allocational harms. The authors generate sentence completions using GPT-2 and compare the model's associations with real-world occupation data. The contributions of this paper include a data collection protocol for studying intersectional biases and an analysis of GPT-2's biases for various demographics.