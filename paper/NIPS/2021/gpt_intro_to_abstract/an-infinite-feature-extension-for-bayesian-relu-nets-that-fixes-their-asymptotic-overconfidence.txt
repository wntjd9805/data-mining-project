Approximate Bayesian methods have been used to address the overconfidence issue of neural networks (NNs) by converting them into Bayesian neural networks (BNNs). Previous studies have shown that BNNs exhibit lower confidence than standard NNs when scaling input to infinity. However, BNNs can still be asymptotically overconfident due to the loose uncertainty bound. In this paper, we propose a method to add back the missing uncertainty in finite ReLU BNNs by extending the cubic spline kernel and using it to model residuals of BNNs. This extension fixes the asymptotic overconfidence issue without affecting the predictive mean of BNNs. The theoretical analysis shows that our method models the uncertainty that ReLU BNNs lack and ensures that the output variance asymptotically grows cubically. Empirical results confirm the effectiveness of our approach.