Video captioning is a challenging task in computer vision and natural language processing, as it requires understanding visual content and generating coherent sentences. Existing methods primarily use recurrent neural networks (RNNs) or Transformers to generate sentences in a left-to-right order. This paper proposes a graph structured model for video captioning that explicitly models the hierarchical structure in the sentence and captures long-range dependencies between words. The model constructs a multi-modal dependency tree, integrating visual and textual information, and utilizes a tree-structured reinforcement learning algorithm with a node-level reward. Experimental results on various datasets demonstrate the effectiveness of the proposed method in generating fluent and relevant sentences, particularly for long and complex descriptions. The contributions of this paper include the multi-modal dependency tree construction method and the novel tree-structured reinforcement learning algorithm.