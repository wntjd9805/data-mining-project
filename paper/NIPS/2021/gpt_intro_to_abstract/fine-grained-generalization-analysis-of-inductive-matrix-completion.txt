Matrix completion (MC) is a machine learning problem that aims to recover missing entries in a partially observed matrix. It is widely used in various domains such as recommender systems and social network analysis. The SoftImpute algorithm is one of the popular methods for MC, which encourages low-rank solutions. However, in many applications, additional information is available alongside the incomplete matrix. Inductive matrix completion (IMC) utilizes this side information to improve the completion task. This paper focuses on providing a better theoretical understanding of IMC in the approximate recovery case. The authors present distribution-free bounds that decrease with the number of samples, which outperform existing state-of-the-art results. They also introduce an adjusted trace norm regularization technique for IMC and demonstrate its superiority in experiments. The paper concludes with a review of related work and presents experimental results.