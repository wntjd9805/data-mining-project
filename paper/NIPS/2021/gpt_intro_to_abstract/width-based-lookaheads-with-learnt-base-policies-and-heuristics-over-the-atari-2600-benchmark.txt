The Atari-2600 games have been widely used as benchmark environments for evaluating autonomous agents. In this paper, we explore two main approaches: reinforcement learning (RL) and planning methods, applied to the Atari-2600 games. While RL methods have shown success in surpassing human performance, they require long training times. In contrast, planning agents do not require training and can make real-time decisions. Width-based planning agents, which prioritize novel states, have demonstrated strong performance. However, previous methods rely on extensive feature engineering or internal state information. We propose new width-based planning and learning methods that leverage a systematic learning schedule and analyze the characteristics of Atari-2600 games. Our contributions include an analysis of previous width-based planning methods, new approaches for playing the games, a methodical learning schedule, and insights into the influence of game characteristics on planning performance.