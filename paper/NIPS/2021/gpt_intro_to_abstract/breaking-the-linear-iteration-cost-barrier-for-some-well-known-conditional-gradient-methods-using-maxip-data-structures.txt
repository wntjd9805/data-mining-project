Conditional gradient methods (CGM) have been widely used in machine learning, but the computational overhead of these methods can be a practical concern. This paper focuses on reducing the running time of the optimization process by analyzing the two components of total running time: the number of iterations and the cost spent per iteration. The authors propose the use of data-structures to reduce iteration cost, specifically through the application of approximate maximum inner product search problem (MaxIP) data-structures. They highlight the success of locality sensitive hashing (LSH) in achieving efficient cost reduction and discuss its use in gradient sampling, neural network training, and iterative machine teaching. Despite the practical success of these cost-efficient algorithms, the theoretical analysis of combining approximate MaxIP data-structures with CGM is not well-understood. To address this gap, the paper presents a theoretical formulation for combining approximate MaxIP and CGM with guarantees of sublinear time cost per iteration. The authors also propose efficient transformations to formulate the direction search in the Frank-Wolfe algorithm as a projected approximate MaxIP problem. They provide theoretical results demonstrating that the sublinear Frank-Wolfe algorithm retains the same asymptotic convergence as CGMs while achieving provable sublinear time cost per iteration. The trade-offs between savings in iteration cost and the increase in the number of iterations to accelerate total running time are also analyzed. The contributions of the paper include the development of theoretical CGM formulations for provable sublinear time cost per iteration, extending these results to other algorithms, and proposing solutions to the challenges of LSH type approximate MaxIP. The paper is organized into sections that cover related works, the proposed algorithm and convergence analysis, proof sketches, societal impact, and the conclusion.