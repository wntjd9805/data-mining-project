The ability to reason about component objects and their relations in a scene is crucial for various robotics and AI tasks. Although there has been extensive research on inferring underlying objects in a scene, understanding component relations remains challenging. Existing multi-modal language and vision models have achieved success in encoding object properties but struggle with encoding object relations. To address this issue, we propose a factorization approach that utilizes separate models to encode individual relations and then combines them to represent a relational scene description. We show that our approach can generate and edit images with multiple composed relations, infer underlying relational scene descriptions, and generalize to previously unseen relation descriptions.