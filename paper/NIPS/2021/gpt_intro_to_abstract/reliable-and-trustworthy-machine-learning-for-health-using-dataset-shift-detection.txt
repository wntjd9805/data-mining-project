Advances in artificial intelligence and machine learning have significantly improved the accuracy and accessibility of medical diagnostic and screening tools. However, there are concerns about the performance of these AI systems in real-world settings, particularly when faced with unseen data. This problem is especially critical in medical applications, where mistakes can have significant repercussions. In this paper, we propose the use of out-of-distribution detection to improve the performance and user-perceived trustworthiness of health-related models. We benchmark our approach using publicly available deep learning models for skin lesion classification, Parkinson's disease severity assessment, and lung sound classification. Our results show that out-of-distribution detection can effectively exclude data that lies outside the training dataset's distribution with high accuracy. Furthermore, we demonstrate that presenting users with the certainty metric improves their trust in the models and their willingness to make medical decisions based on them. Our contributions include identifying the limitations of current health deep learning models, evaluating the utility of out-of-distribution detection for medical screening and diagnosis, and assessing the impact of dataset shift information on user trust.