Despite the success of deep neural networks in various domains, miscalibrated predictions remain a challenge. Over- and under-confidence contribute to miscalibration, which is empirically significant in deep neural networks. Calibration error (CE) measures a model's miscalibration by quantifying the divergence between its predicted probability of correctness and its empirical probability of correctness. Low CE models are crucial in domains that rely on well-modeled uncertainty, such as autonomous vehicle navigation and medical diagnostics. Additionally, calibration is beneficial for improving model fairness and detecting out-of-distribution data. However, common measures of CE based on discrete binning are not trainable using gradient-based methods, limiting the direct optimization of calibration. In this paper, we introduce new objectives based on differentiable binning schemes that can efficiently and directly optimize for calibration. We propose soft (overlapping, continuous) bins to estimate CE, enabling its use as a secondary loss for training and post-hoc recalibration. We compare our soft calibration objectives with existing methods and demonstrate their superior performance on in-distribution test sets and under distribution shift. Furthermore, we improve upon temperature scaling by directly optimizing the temperature parameter for soft calibration error, resulting in better performance across various datasets and calibration error measures. Overall, our work highlights the advantages of objectives that better incentivize calibration, leading to improved uncertainty estimates in and out-of-distribution scenarios.