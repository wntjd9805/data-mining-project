The introduction of this computer science paper discusses the significant progress made by deep neural networks and the interest in deploying efficient versions of these models. It introduces the concept of neural network pruning as a compression method to remove connections while maintaining accuracy. The paper highlights the limitations of existing pruning methods and proposes a new approach called Alternating Compressed/DeCompressed (AC/DC) training. AC/DC is a hybrid method that co-trains sparse and dense models, resulting in highly accurate sparse models and providing computational savings. The paper presents convergence guarantees for AC/DC and demonstrates its superior performance compared to previous sparse training methods, as well as its flexibility in structure and real-world speedups. Furthermore, AC/DC allows for accurate dense/sparse co-training, providing insights into the training dynamics and the accuracy differences between sparse and dense models.