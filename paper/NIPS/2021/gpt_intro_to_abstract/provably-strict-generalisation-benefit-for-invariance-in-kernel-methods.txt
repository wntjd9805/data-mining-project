This paper introduces the concept of incorporating invariance in models and explores its benefits in the field of computer science, specifically in kernel ridge regression. While previous works provided only worst-case guarantees on the performance of invariant algorithms, this paper aims to provide a rigorous theoretical justification for the use of invariance. The authors present a precise characterisation of the generalisation benefit of invariance in kernel ridge regression through the use of feature averaging. The main results show a provably strict generalisation benefit for invariant, feature-averaged models. The paper also discusses the structure of reproducing kernel Hilbert spaces in relation to invariant functions and the use of orbit-averaging for transforming models to be invariant. The findings have implications for analyzing invariance in other kernel algorithms. The paper concludes by presenting assumptions, technical conditions, and an outline of the research methodology.