The paper introduces a new Vision-and-Language Pre-training (VLP) framework called ALign BEfore Fuse (ALBEF) to address limitations in existing VLP methods. ALBEF uses a detector-free image encoder and text encoder to independently encode images and texts, and then employs a multimodal encoder to fuse the features through cross-modal attention. The framework introduces an intermediate image-text contrastive (ITC) loss that aligns the image and text features, improves the understanding of semantics, and learns a common low-dimensional space for image-text matching. To handle noisy supervision, the paper proposes a method called Momentum Distillation (MoD) that allows the model to leverage a larger uncurated web dataset. The theoretical justifications for ALBEF are provided from the perspective of mutual information maximization. Experimental results demonstrate the effectiveness of ALBEF in various Vision-and-Language tasks, outperforming existing methods in terms of performance and inference speed.