Distributionally robust optimization (DRO) has gained attention in the field of machine learning due to its ability to handle various types of data challenges such as noise, adversarial data, and imbalanced classification data. This paper introduces a DRO formulation that tackles these challenges by minimizing a loss function over a set of observed data and a divergence measure between the data and uniform probabilities. The paper also highlights the limitations of existing stochastic primal-dual methods for solving DRO problems with non-convex loss functions, particularly in terms of memory cost, sampling, and constraints on data. To address these issues, the paper proposes an efficient online algorithm that focuses on a family of DRO problems with a specific divergence function. The proposed algorithm is compared with existing methods and is shown to have practical advantages for large-scale KL-regularized DRO problems, making it suitable for deep learning applications with irregular data. The paper also presents important theoretical contributions in stochastic non-convex optimization and provides empirical evidence of the effectiveness of the proposed algorithm for deep learning on imbalanced data.