The paper introduces the High-Resolution Transformer (HRFormer), which addresses the limitations of the Vision Transformer (ViT) for dense prediction tasks such as pose estimation and semantic segmentation. The ViT lacks fine-grained spatial details and the capability to handle multi-scale variation. HRFormer adopts a multi-resolution parallel design and incorporates convolution in the stem and first stage. It maintains a high-resolution stream using parallel medium- and low-resolution streams. The HRFormer block employs local-window self-attention and a depth-wise convolution to enhance information exchange and expand the receptive field. Experimental results show that HRFormer achieves competitive performance on various benchmarks, outperforming existing models in terms of accuracy, parameter efficiency, and computational complexity.