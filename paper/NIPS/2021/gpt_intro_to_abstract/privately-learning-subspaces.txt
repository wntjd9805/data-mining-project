Differentially private algorithms often suffer from poor performance in high-dimensional settings, where their error or sample complexity grows polynomially with the dimension. However, when the data has an underlying low-dimensional structure, there is a possibility of overcoming this curse of dimensionality. This paper focuses on the problem of learning low-dimensional structure from data subject to differential privacy. The authors consider the cases of exact and approximate learning of subspaces and propose algorithms that can identify or approximate the underlying subspace. The algorithms are shown to be differentially private and the sample complexity does not depend on the ambient dimension, but rather on the true dimension of the subspace. The results are optimal and robust to input point corruption.