This paper introduces a transformation-equivariant self-interpretable model called SITE for classification tasks. The model generates data-dependent prototypes for each class and formulates predictions as the inner product between prototypes and extracted features. The interpretations are directly involved in the prediction process, making them faithful to the final results. Transformation and reconstruction regularization are introduced to the prototypes to ensure meaningful and understandable interpretations while also enforcing transformation equivariance. SITE provides understandable and faithful interpretations without requiring additional domain knowledge and maintains high expressive power in prediction. The paper also proposes a new quantitative metric, the self-consistency score, to measure the robustness of interpretations to geometric transformations. Experimental results demonstrate the effectiveness of SITE in terms of interpretability and accuracy compared to black-box models.