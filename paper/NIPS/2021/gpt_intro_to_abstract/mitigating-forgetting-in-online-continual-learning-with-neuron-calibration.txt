This paper addresses the challenge of catastrophic forgetting in artificial learning systems when switching to new tasks. The research on continual learning has emerged to define learning protocols for sequential arrival of training data from different tasks. This paper focuses on online continual learning, where the data for each task appears as a stream of samples. To mitigate catastrophic forgetting, the paper explores the replay-based approach, which uses a limited access to past task data for rehearsal. However, replay-based approaches often lead to a data-imbalance issue known as the stability-plasticity dilemma. This paper proposes a novel approach called neuron calibration, which adjusts the transformation functions in layers of deep neural networks to achieve a better balance between stability and plasticity. The proposed approach is task-agnostic and achieves better performance compared to other continual learning algorithms.