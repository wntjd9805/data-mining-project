Training machine learning models in a centralized fashion is becoming more difficult and less accessible for users as datasets and models grow larger. Decentralized and distributed approaches, such as federated learning (FL), have gained attention as a replacement. However, FL faces challenges such as communication bottlenecks, data heterogeneity, system heterogeneity, and privacy concerns. In this paper, we propose two new algorithms, FedDR and asyncFedDR, that combine randomized block-coordinate strategy, nonconvex Douglas-Rachford splitting, and asynchronous implementation to address these challenges. Our algorithms achieve the best known communication complexity and allow for partial user participation, making them more practical and applicable in FL. Additionally, asyncFedDR addresses the challenge of heterogeneous computing power and communication capability.