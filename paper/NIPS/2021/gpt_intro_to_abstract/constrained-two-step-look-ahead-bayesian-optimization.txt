In this paper, we address the problem of constrained optimization with a continuous black-box function and continuous black-box constraints. We focus on derivative-free, time-consuming-to-evaluate, and noise-free functions and constraints. This problem arises in various applications, including tuning hyperparameters of machine learning models subject to runtime or fairness constraints and policy optimization in reinforcement learning with safety constraints. Existing methods for constrained Bayesian optimization (CBO) are myopic, meaning they only consider immediate improvements in solution quality and ignore future improvements enabled by a function evaluation. We argue that non-myopic methods are valuable in constrained settings, as they can efficiently learn the boundary between feasible and infeasible regions, which is crucial for finding global optima under constraints. We propose a novel non-myopic computationally efficient method for batch CBO that outperforms myopic methods and requires less computation. Our approach optimizes stochastic acquisition functions using the likelihood ratio method, overcoming the discontinuities in the CBO surface. This work builds upon the unconstrained two-step optimal method and introduces a new likelihood-ratio-based approach for gradient estimation.