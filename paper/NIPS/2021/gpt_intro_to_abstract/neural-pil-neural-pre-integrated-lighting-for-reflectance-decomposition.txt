Inverse rendering is a task that involves decomposing a scene into its physical properties, such as geometry and materials. This paper aims to recover the 3D shape and spatially-varying bidirectional reflectance distribution function (SVBRDF) of an object under different lighting conditions. The approach uses coordinate-based scene representation networks and differentiable rendering techniques to learn the decomposition of shape, illumination, and SVBRDF. The authors propose a novel pre-integrated lighting (PIL) network to replace the expensive illumination integration step in rendering with a learned network. Additionally, a smooth manifold auto-encoder (SMAE) is presented to learn low-dimensional representations of light and BRDFs. The networks are trained on a dataset of high-quality environment maps and materials, and empirical analysis demonstrates the accuracy and effectiveness of the proposed approach in estimating shape and material properties.