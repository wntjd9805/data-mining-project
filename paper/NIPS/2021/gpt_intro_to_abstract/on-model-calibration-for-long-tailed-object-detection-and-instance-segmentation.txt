Object detection and instance segmentation are essential tasks in computer vision, but there is a significant performance drop when detecting rare objects due to insufficient training samples. Most existing works address this problem in the model training phase, but in this paper, we propose a post-processing calibration technique that adjusts a classifier's confidence scores among classes to improve object detection performance. We apply this technique to a pre-trained object detector and demonstrate its effectiveness in detecting both rare and common objects. Our approach, called Normalized Calibration (NORCAL), is model-agnostic and can consistently improve baseline models and models dedicated to the long-tailed distribution. NORCAL improves both standard average precision and category-independent APFixed metric, indicating that it enhances proposal ranking within each class without sacrificing frequent class predictions. Our analysis shows that NORCAL improves both precision and recall for each class, making it suitable for various evaluation metrics. Overall, NORCAL is a simple component that can enhance the performance of object detectors during inference.