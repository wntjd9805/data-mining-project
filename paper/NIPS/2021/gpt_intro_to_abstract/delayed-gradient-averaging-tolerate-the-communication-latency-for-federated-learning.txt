Federated Learning has gained attention for its ability to enable multi-clients distributed training without compromising user privacy. However, the networking conditions in federated scenarios pose unique challenges, such as limited bandwidth and high latency. While several techniques have been proposed to address bandwidth constraints, the issue of network latency has been largely overlooked. In this paper, we propose a novel method called Delayed Gradient Aggregation (DGA) to address the latency bottleneck in federated learning. By delaying gradient averaging to future iterations, DGA allows for parallel communication and computation, thus improving scalability even under extreme latency. We demonstrate the effectiveness of DGA through extensive experiments and show that it achieves comparable convergence to traditional methods while significantly improving speed, even under high latency conditions. Overall, our algorithm provides a scalable solution for federated training in high-latency environments.