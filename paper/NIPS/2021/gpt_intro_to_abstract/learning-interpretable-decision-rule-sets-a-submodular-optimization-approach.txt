Interpretability is an increasingly important aspect when deploying machine learning models in high-stake decision-making situations. Black box models, such as random forests and deep neural networks, often achieve high prediction accuracy but lack transparency in how they make predictions. Additionally, machine learning models are vulnerable to spurious correlations, making them less robust to distribution shifts and adversarial attacks. Black box models are difficult to audit and diagnose, further exacerbating these issues. In this paper, the authors focus on the development of inherently interpretable models, specifically decision rule sets for binary classification. Rule sets are more suited for tabular data with mixed-type features and complex high-order feature interactions, and are easier to interpret and learn from compared to other rule-based models like decision trees and decision lists. The learning of rule sets involves two major challenges: subset selection for constructing rules and selecting a subset of rules to form a rule set. Early algorithms utilized greedy sequential covering strategies or two-stage association classification techniques, neither of which had a global objective optimizing interpretability. This paper introduces a submodular optimization perspective on rule set learning, combining accuracy and interpretability. The authors propose an on-the-fly approach that solves optimization subproblems to approximate the global maximum, and develop an algorithm based on iterative local combinatorial search for efficiency. The contributions of this paper are summarized as: formulating interpretable rule set learning as a regularized submodular maximization problem, discovering a submodular decomposition for rule construction, and providing comprehensive experimental evaluation demonstrating competitive performance in both prediction and interpretability.