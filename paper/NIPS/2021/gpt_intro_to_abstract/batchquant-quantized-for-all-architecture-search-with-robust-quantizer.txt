Deploying deep learning models on resource-constrained edge devices requires careful model optimization, including pruning and quantization. Existing works have shown the effectiveness of model optimization techniques but also the increased human labor required. To address this issue, automated methods like neural architecture search (NAS) and automated quantization policy search have emerged. This paper focuses on finding the best combination of architecture and mixed-precision quantization policy. However, combining these two search spaces is challenging, and previous methods have used proxies to estimate performance. To overcome the limitations of previous approaches, this paper proposes BatchQuant (BQ), a formulation for stable mixed-precision supernet training, and leverages the NSGA-II algorithm for producing a Pareto set of quantized architectures. The contributions of this paper include training one-shot weight-sharing supernets without retraining, proposing the BatchQuant formulation, and discovering quantized subnets at state-of-the-art efficiency.