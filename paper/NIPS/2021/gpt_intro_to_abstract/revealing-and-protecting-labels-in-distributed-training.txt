This paper introduces a method called Revealing Labels from Gradients (RLG) that can reveal the entire set of labels used in computing a weight update in deep learning models. The method is applicable to a wide variety of architectures and can be applied to parameter updates generated from mini-batches or multiple steps. The paper also discusses the effects of gradient quantization and sparsification techniques in preventing RLG. Additionally, the paper demonstrates that the knowledge of training labels can be used to fully reconstruct sequences in sequence models such as automatic speech recognition, while previous methods have limited success. Experimental results on image classification and automatic speech recognition tasks are presented. The paper also provides a link to the code implementation.