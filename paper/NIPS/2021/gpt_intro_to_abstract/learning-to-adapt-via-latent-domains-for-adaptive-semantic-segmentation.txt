Semantic segmentation is a widely-used task in computer vision that assigns semantic labels to individual pixels in images. Deep neural networks trained on labeled data have shown significant progress in image semantic segmentation, but acquiring such data can be expensive. To address this issue, synthetic images with ground truth labels can be generated, but models trained solely on synthetic data tend to perform poorly on real data. Domain adaptation methods, specifically adversarial learning, have been used to align representations of different domains and improve performance. In this paper, we propose a meta-learning framework for single-target and multi-target domain adaptation in semantic segmentation. We generate a latent domain through style transfer and construct domain pairs for meta-learning, which transfers meta-knowledge of adaptation across different domains. Our approach achieves state-of-the-art performance on benchmark datasets.