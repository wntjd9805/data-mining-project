When training a classifier, it is typically assumed that the training and test samples come from the same underlying distribution. However, in adversarial machine learning, this assumption is intentionally violated by perturbing the samples using the classifier itself to increase the error rate. As a result, the classifier performs poorly on adversarially generated input distributions. Previous methods have been proposed to defend neural networks against adversarial attacks, but it has been shown that these networks remain vulnerable to larger perturbations or other forms of attacks. In this paper, we propose a new approach to adversarial robustness by considering natural and adversarial images as distinct input domains and leveraging domain adaptation techniques. We introduce the classification-based Hâˆ†H-divergence to measure the distance between these domains and formulate a bound on the adversarial classification error. We then propose an algorithm that trains a classifier, domain discriminator, and feature extractor to minimize losses and learn features that are both predictive and insensitive to adversarial attacks. Our empirical results demonstrate that our method outperforms previous defense methods in terms of robustness against various types and strengths of attacks.