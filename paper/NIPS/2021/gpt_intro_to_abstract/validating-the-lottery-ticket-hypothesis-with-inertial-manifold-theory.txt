Deep learning techniques have achieved remarkable performance in various applications by utilizing large model size and over-parameterization. However, this comes at the cost of high training cost, slow inference speed, and large memory consumption. Neural network pruning has emerged as a potential solution to reduce these requirements while maintaining accuracy. However, most existing pruning methods still require training a dense network, resulting in non-trivial training costs. Additionally, these methods are based on ad-hoc heuristic pruning strategies, lacking theoretical guarantees and efficient procedures. The Lottery Ticket Hypothesis (LTH) has shown promising results in finding sparse subnetworks within dense networks, but lacks efficient algorithms and theoretical evidence. This paper introduces a novel approach that verifies the existence of winning lottery tickets within neural networks and provides an efficient algorithm for their discovery. By leveraging dynamical systems theory and inertial manifold theory, the authors demonstrate the theoretical soundness and efficiency of their method compared to existing techniques. Evaluation on real datasets confirms the superior performance of their approach.