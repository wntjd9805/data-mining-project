In this paper, the authors address the problem of finding good explanations using test-time ablation metrics in natural language processing tasks. They argue that standard feature importance (FI) explanations are heavily influenced by the out-of-distribution nature of counterfactual model inputs, resulting in socially misaligned explanations. To resolve this issue, they propose a training algorithm that exposes models to counterfactual inputs during training. Additionally, the authors compare different Replace functions, which play a crucial role in evaluating explanations, and recommend the use of certain functions over others. They also introduce several novel search-based methods for identifying FI explanations, demonstrating their effectiveness through experiments with Transformer models and multiple text classification datasets.