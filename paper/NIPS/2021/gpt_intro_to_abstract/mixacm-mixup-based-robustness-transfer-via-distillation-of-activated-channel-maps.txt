Deep learning models have shown impressive performance on various challenging tasks, but they are susceptible to small changes in the input space, known as adversarial examples. Many defense mechanisms have been proposed to train models to be robust against these perturbations, with adversarial training being the most effective. However, adversarial training is more challenging and computationally expensive compared to normal training, and it degrades the performance of models on natural examples. In this paper, we propose a new approach for robustness transfer by distilling intermediate features of a robust teacher model. Our method does not require extra gradient computation and works well with smaller models and fewer data samples. We conduct extensive experiments to demonstrate the effectiveness of our method on various datasets and learning settings, showing that it can transfer robustness without adversarial examples and improve clean accuracy significantly.