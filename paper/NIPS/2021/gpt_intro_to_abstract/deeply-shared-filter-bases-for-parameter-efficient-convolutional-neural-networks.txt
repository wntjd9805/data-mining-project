Modern convolutional neural networks (CNNs) often consist of large identical convolution blocks that primarily refine features rather than learning new ones. Recursive sharing of weights has been explored as an efficient approach to parameter reduction in CNNs. However, naive sharing of parameters across multiple convolution layers can lead to problems such as vanishing and exploding gradients, limiting the overall representation power. In this work, we propose a method to separate convolution layers into shareable and non-shareable parts, enabling effective training while avoiding gradient problems. We introduce a factorized convolution operator that splits the operator into a filter basis and coefficients, allowing for shared filters within vector subspaces and diverse features through non-shared coefficients. To address the issue of performance degradation from recursive parameter sharing, we propose orthogonality regularization to control the vanishing/exploding gradients problem by enforcing the orthonormality of shared filter bases during training. Theoretical and empirical results demonstrate the efficacy of our approach in improving gradient flow and reducing parameter redundancy. Our method achieves significant parameter savings while consistently outperforming over-parameterized networks in image classification and object detection tasks. Even in compact models like MobileNetV2, our approach achieves additional parameter savings and superior performance, showcasing the effectiveness of recursive convolution blocks and orthogonality regularization in learning better feature representations.