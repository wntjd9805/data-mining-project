This paper introduces a method for directly mapping the low-dimensional space of language representations using "representation embeddings" inspired by previous work. The authors demonstrate the existence of low-dimensional structure within the space of language representations and explore the relationships between different language models. They also show that this low-dimensional structure is reflected in brain responses predicted by the representation embeddings, suggesting their potential for predicting brain mapping. The paper makes contributions in understanding language processing hierarchies and predicting which representations map well to different areas in the brain.