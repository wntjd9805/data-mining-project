This paper introduces BlendGAN, a framework for arbitrary stylized face generation. BlendGAN utilizes a flexible blending strategy and a generic artistic dataset to fit arbitrary styles without relying on style-consistent training images for each style. The framework analyzes a stylized face image as composed of two latent parts: a face code controlling face attributes, and a style code controlling artistic appearance. A self-supervised style encoder is trained to extract the style representation from artistic face images, and a weighted blending module is proposed to blend the face and style latent codes. By controlling the indicator in the blending module, the stylization effect can be controlled. Additionally, a large-scale dataset of high-quality artistic face images, Artstation-Artistic-face-HQ (AAHQ), is presented. Experimental results demonstrate that BlendGAN generates stylized face images with higher visual quality and style diversity for both latent-guided and reference-guided synthesis.