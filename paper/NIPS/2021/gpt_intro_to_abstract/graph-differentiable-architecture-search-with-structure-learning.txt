In real-world applications, graph neural networks (GNNs) have been widely used to learn and extract knowledge from relational graph-structured data. However, designing optimal GNN architectures for different tasks is time-consuming and labor-intensive. To address this issue, graph architecture search has been proposed to automatically design optimal GNN architectures. Despite the success of existing works, there are still fundamental questions that remain unanswered, such as how graph architecture search selects desired architectures and how optimal these architectures are. In this paper, we focus on the gradient-based architecture search approach and analyze its behavior using the DARTS method. We discover that DARTS on GNN is able to evaluate the usefulness of information in node features and graph structure and select appropriate operations based on this evaluation. We also find that the performance of gradient-based graph architecture search can be degraded by noises in the graphs. To overcome this problem, we propose the Graph differentiable Architecture Search model with Structure Optimization (GASSO), which allows for differentiable search of the architecture and adaptive adjustment of the graph structure through joint optimization. Our experimental results on benchmark datasets demonstrate that GASSO outperforms state-of-the-art methods and exhibits better denoising ability. Overall, this paper contributes by providing theoretical insights into how graph neural architecture search selects desired architectures, conducting measurement studies to evaluate the performance of gradient-based graph architecture search, and proposing the GASSO model that achieves superior performance in architecture search and graph structure adjustment.