The deep learning community heavily relies on transfer learning, especially in the field of computer vision. Pretraining convolutional networks on large image collections is the standard approach for various applications. Similarly, natural language processing systems utilize language models pretrained on vast unlabeled corpora. However, training these models is time-consuming and resource-intensive. Model hubs have attempted to address this issue by providing pretrained model checkpoints, but they are limited in terms of datasets and tasks. An alternative approach is collaborative training, where independent parties combine their computational resources. However, fully utilizing such collaborations is challenging due to device heterogeneity and network limitations. In this study, we propose Distributed Deep Learning in Open Collaborations (DeDLOC), a system that adapts to available hardware to maximize training throughput. We analyze the challenges, formulate a novel distributed training algorithm, and validate its effectiveness with unsupervised pretraining experiments. We also demonstrate competitive results achieved through collaborative training with actual volunteers.