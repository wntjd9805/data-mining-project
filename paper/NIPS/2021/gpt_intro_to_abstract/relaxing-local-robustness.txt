The discovery of adversarial examples in deep learning has raised security concerns, prompting researchers to develop methods for ensuring robustness against perturbations. Current approaches focus on point-wise local robustness and use verified-robust accuracy (VRA) as a metric. However, in certain contexts, VRA may not be the most desirable objective. This paper introduces relaxed top-k (RTK) robustness as an alternative, which allows for considering the correct label among the model's top-k predictions. The authors also introduce affinity robustness to address directed adversarial attacks. They demonstrate how neural networks can be modified to incorporate certifiable RTK and affinity robustness and validate their effectiveness in reducing rejection rates and improving certified accuracy. Additionally, they show that these relaxed robustness variants can lead to interesting class hierarchies in a network's prediction space.