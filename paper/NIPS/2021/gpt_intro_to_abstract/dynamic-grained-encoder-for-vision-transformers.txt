This paper addresses the high computational cost of vision transformers, which are gaining attention as an alternative to CNNs for vision tasks. The authors propose a Dynamic Grained Encoder (DGE) that reduces spatial redundancy in image features through a data-dependent routing process. They demonstrate the effectiveness of DGE through experiments on three vision transformers, showing reduced computational complexity without sacrificing performance on image classification, and improved accuracy on the ImageNet validation set. The method also shows robustness and generalization in object detection and segmentation tasks. Overall, this dynamic network mechanism offers a promising solution to reducing computational costs in vision transformers.