This paper introduces COCO-LM, a new self-supervised learning approach for pretraining language models. COCO-LM improves upon the ELECTRA strategy by introducing two new pretraining tasks - corrective language modeling (CLM) and sequence contrastive learning (SCL). CLM detects and corrects tokens in corrupted sequences, combining replaced token detection and language modeling, while SCL aligns text sequences and enforces uniformity in the representation space. Experimental results on benchmark datasets show that COCO-LM outperforms state-of-the-art pretraining approaches in effectiveness and efficiency. The paper also provides insights into the advantages of CLM and SCL in learning token representations and ensuring alignment in the representation space for better generalization.