In recent years, deep learning in computer vision tasks has heavily relied on convolutional neural networks (CNNs) due to their spatial equivariance encoded by convolutional layers, which facilitate general-purpose visual representations and strong performance. However, a recent breakthrough in the form of Vision Transformers (ViTs) has shown that these models can achieve equal or even superior performance in image classification tasks by using self-attention instead of convolution. This raises questions about how ViTs solve these image-based tasks, whether they learn the same inductive biases as CNNs, and the role of scale in learning these representations. In this paper, we investigate these questions, comparing the internal representation structure of ViTs and CNNs, analyzing the utilization of local/global spatial information, assessing the impact of large-scale pre-training data, examining the effects of skip connections, exploring the preservation of input spatial information, and studying the effects of dataset scale on transfer learning. Our findings provide insights into the differences between ViTs and CNNs and their implications for classification and transfer learning.