Deep reinforcement learning has achieved significant empirical successes in both continuous and discrete problem settings, with on-policy and off-policy algorithms. Interpreting RL as probabilistic inference in the continuous control domain has led to algorithms with strong empirical performance. In this paper, the authors derive the Maximum a posteriori Policy Optimisation (MPO), Advantage Weighted Regression (AWR), and Soft Actor Critic (SAC) algorithms from a single objective function, clarifying their algorithmic connections. They then conduct experiments to identify the sources of performance gaps, revealing implementation techniques and code details that contribute to these gaps. Their findings show which design choices are highly co-adapted and specific to certain algorithms, while also identifying transferable choices that significantly benefit multiple algorithms. The authors propose that decoupling algorithmic innovations from implementation or code details can lead to better understanding and development of reinforcement learning algorithms.