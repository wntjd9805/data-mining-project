State-of-the-art pretrained language models (PLMs) in natural language processing (NLP) have achieved success on various benchmarks, but their large parameter sizes pose storage and deployment challenges. Additionally, fine-tuning these models on low-resource datasets can lead to instability and poor performance. Inspired by John von Neumann's quote, this paper aims to explore the minimum number of additional parameters needed to achieve state-of-the-art performance on NLP tasks after training a PLM. The paper introduces COMPACTER, a method that combines ideas from adapters, low-rank methods, and hypercomplex multiplication layers to fine-tune PLMs with a favorable trade-off between trainable parameters, task performance, and memory footprint. COMPACTER outperforms other parameter-efficient fine-tuning methods on standard benchmarks like GLUE and SuperGLUE, as well as in low-resource settings.