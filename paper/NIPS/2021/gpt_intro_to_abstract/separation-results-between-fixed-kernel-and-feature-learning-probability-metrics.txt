The field of generative modeling, which aims to generate artificial samples of a target distribution based on true samples, is divided into explicit and implicit generative models. This paper focuses on the advantages of using "adaptive" or "feature-learning" function classes as discriminators in both types of models. Previous works have shown that these function classes yield better generative performance compared to "lazy" or "kernel" function classes. The paper provides theoretical results that highlight the benefits of feature-learning over kernel discriminators. The experimental evidence supports the idea that generative models with kernel discriminators perform poorly due to weaker metrics that fail to distinguish between different distributions, particularly in high dimensions.