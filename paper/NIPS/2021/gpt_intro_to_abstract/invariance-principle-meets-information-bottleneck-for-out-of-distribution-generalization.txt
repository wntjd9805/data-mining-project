Recent research has shown that deep learning models are susceptible to exploiting shortcuts or spurious features, leading to a failure to generalize out-of-distribution. This issue becomes particularly concerning when it occurs in real-life applications, such as COVID-19 detection. In response, invariant risk minimization and other related works have been proposed to address these generalization failures. However, there are situations where these approaches fail when invariant features capture all the label information in the input. This paper revisits the fundamental assumptions in linear regression tasks and demonstrates that OOD generalization is significantly harder for linear classification tasks. The authors propose an approach that combines the information bottleneck principle with the invariance principle to address these challenges and show its effectiveness in both linear unit tests and real datasets.