The central question in reinforcement learning (RL) is how to learn quickly in a new environment. Meta-RL addresses this issue by training an agent on a large set of training environments. This paper focuses on the offline (batch) approach to meta-RL (OMRL), which is particularly useful in domains with expensive data collection. The authors propose a method called Bayesian Offline Reinforcement Learning (BOReL) that combines off-policy Q-learning with the VariBAD algorithm. They also explore the problem of MDP ambiguity, where the agent struggles to identify the correct environment in different parts of the state space. The authors provide formalizations and data collection strategies to mitigate this issue. Experimental results demonstrate that BOReL achieves better exploration than existing meta-RL methods and improves the sample efficiency of conventional VariBAD in the online setting. This study contributes to the understanding of exploration in the offline setting and extends the VariBAD algorithm to off-policy RL.