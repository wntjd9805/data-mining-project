We study continuous action zero-sum games in which the objective is to minimize a function over the action space X while the opponent seeks to maximize the same function over the action space Y. We focus on nonconvex-PŁ and nonconvex-SC zero-sum games, which have various applications in fields such as fair classification, distributionally robust optimization, and adversarial training. Existing work in this area has primarily focused on convergence rates to approximate stationary points without considering game-theoretic equilibrium concepts. In this paper, we aim to bridge this gap by examining the global convergence of gradient-based learning algorithms in nonconvex-PŁ/SC zero-sum games to game-theoretically meaningful equilibria. We specifically analyze the τ-GDA algorithm, which involves different learning rates for the maximizing and minimizing players. Our results show that τ-GDA has global convergence guarantees to strict local minmax equilibria in nonconvex-PŁ/SC zero-sum games, and we provide convergence rates for both deterministic and stochastic problems. The practical motivation for studying these games arises from their applications in machine learning problems, such as fair classification, adversarial robustness, and distributionally robust optimization. We provide examples that demonstrate how these problems can be transformed into unconstrained nonconvex-PŁ/SC zero-sum games.