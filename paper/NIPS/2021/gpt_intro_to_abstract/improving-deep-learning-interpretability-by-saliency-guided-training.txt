Deep Neural Networks (DNNs) have been widely used in various tasks, but interpreting their complex networks is still challenging. Reliable explanations are crucial for critical domains like medicine, neuroscience, finance, and autonomous driving. In this paper, we propose a new training procedure called saliency guided training that improves the interpretability of DNNs. This training procedure produces sparse, meaningful, and less noisy gradients without sacrificing model performance. We demonstrate the effectiveness of saliency guided training in image classification, sentiment analysis, and multivariate time series classification tasks across different neural architectures, showing reduction in saliency noise, sparser saliency maps, and improved comprehensiveness of explanations. We also observe increased precision and recall of saliency maps and reduction in the vanishing saliency issue of RNNs. Our results show significant improvements in the explanations produced by various gradient-based saliency methods after training.