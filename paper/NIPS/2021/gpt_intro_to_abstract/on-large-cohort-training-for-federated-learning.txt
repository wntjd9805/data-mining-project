Federated learning (FL) is a method that enables learning a model from multiple clients without directly sharing training data. In this paper, the focus is on cross-device FL, where the goal is to learn across a large population of edge devices. One characteristic of cross-device FL is partial client participation, where the server only communicates with a subset of clients at a time. Increasing the cohort size, i.e., the number of clients involved in each communication round, is believed to improve convergence rates and enhance privacy. However, this paper explores the impact of cohort size in realistic cross-device settings and finds that increasing the cohort size may not necessarily lead to significant convergence improvements in practice. Large-cohort training can also introduce optimization and generalization issues. The authors provide empirical analysis, identify challenges unique to federated learning, and propose partial solutions to address these challenges. The paper concludes by discussing limitations, open problems, and the practical implications of the findings.