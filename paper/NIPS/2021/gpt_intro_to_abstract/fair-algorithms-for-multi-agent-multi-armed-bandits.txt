The classical multi-armed bandit problem involves a principal pulling arms to generate stochastic rewards, but without prior knowledge of the arms' quality. A learning algorithm is used to determine the best arm over time, and its performance is measured in terms of cumulative regret. This paper extends the problem to a multi-agent variant, where the rewards affect multiple agents. The goal is to make fair collective decisions by maximizing the Nash social welfare, which maximizes the product of expected utilities for all agents. The paper proposes multi-agent variants of three classic algorithms and derives regret bounds that consider the Nash social welfare objective. These bounds require addressing challenges such as optimizing a complicated function and dealing with an infinite space of distributions. The contributions of the paper are both conceptual, highlighting a multi-agent viewpoint, and technical, providing regret bounds for the Nash social welfare objective.