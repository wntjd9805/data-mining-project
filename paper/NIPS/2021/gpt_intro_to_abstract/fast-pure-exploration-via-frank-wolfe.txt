This paper investigates a learning problem in the context of pure exploration tasks in stochastic bandits. The goal is to answer a given question about the reward distributions of different arms using as few arm pulls as possible. The paper proposes a generic learning strategy called Frank-Wolfe-based Sampling (FWS) that iteratively approaches the optimal allocation of arm pulls. The expected sample complexity of FWS is derived and shown to match the lower bound as the certainty level approaches zero. The performance of FWS is illustrated in various pure exploration problems, showing its effectiveness compared to existing algorithms. The use of the Frank-Wolfe algorithm in this context is explored and its convergence is confirmed. The analysis of FWS overcomes several obstacles related to the objective function and the evolving estimate of the expected rewards.