The goal of recovering the underlying 3D structure from 2D observations has been a longstanding challenge in computer vision. While neural volumetric representations have emerged as a popular approach, they are less robust and unsuitable for modeling 3D objects from sparse views in real-world settings. In this paper, we propose Neural Reflectance Surfaces (NeRS), a surface-based neural representation that captures real-world 3D structure using a 2D manifold embedded in 3D space. NeRS combines a neural displacement field for shape with neural texture fields to capture appearance, allowing it to learn efficiently and robustly from a sparse set of images. We evaluate NeRS on a dataset of real-world objects, demonstrating its superior performance compared to neural volumetric and mesh-based approaches. We also introduce a new evaluation protocol for in-the-wild novel view synthesis. Our work highlights the advantages of neural surface representations and provides a foundation for future research in this area.