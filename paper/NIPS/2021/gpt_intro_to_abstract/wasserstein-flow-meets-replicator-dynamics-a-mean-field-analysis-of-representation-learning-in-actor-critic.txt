In reinforcement learning, the goal is for an agent to learn the optimal policy that maximizes the expected total reward. Policy-based RL algorithms achieve this by optimizing the expected total reward as a function of the policy, which involves policy evaluation and policy improvement. Actor-critic methods, which consist of an actor (policy) and a critic (value function), have been successful in RL applications with large state spaces. However, the theoretical understanding of the benefits of using expressive function approximators like neural networks is limited. In this work, we aim to understand the role of representation learning in neural actor-critic algorithms. We examine convergence and optimality and study the evolution of the feature representation associated with the neural network. We focus on a specific version of actor-critic with temporal-difference learning for policy evaluation and proximal policy optimization for policy improvement. Using two-timescale updates, we analyze the continuous-time limiting regime with infinite network width to reveal the connection between actor updates and replicator dynamics and critic updates and semigradient flow in the Wasserstein space. We show that neural actor-critic achieves global optimality at a sublinear rate and induces a data-dependent feature representation. Our analysis relies on infinite-dimensional variational inequalities, mean-field perspective on neural networks, and two-timescale stochastic approximation. This is the first paper to provide convergence and optimality guarantees for neural actor-critic.