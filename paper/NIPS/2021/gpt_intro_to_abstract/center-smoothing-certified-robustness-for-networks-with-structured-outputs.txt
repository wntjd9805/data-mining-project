The study of adversarial robustness in machine learning has become increasingly important as deep neural networks have been shown to be vulnerable to attacks. These attacks can significantly degrade the performance of models and have been demonstrated in various domains, including image classification, automatic speech recognition, image segmentation, facial recognition, and more. This widespread presence of attacks poses a threat to critical systems where safety and reliability are crucial. Previous adversarial defenses have primarily focused on classification tasks, but certifiable defenses that guarantee robustness regardless of attack strategies have also been explored. This paper introduces a technique called center smoothing, which aims to achieve provable robustness for functions with structured outputs, such as images and text. The method utilizes a randomized smoothing approach and minimal assumptions on the underlying distance metric. The paper presents the procedure and demonstrates its effectiveness in generating certifiable outputs while supporting various output metrics.