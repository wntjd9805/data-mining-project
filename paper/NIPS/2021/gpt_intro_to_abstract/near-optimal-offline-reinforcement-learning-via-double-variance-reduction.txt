Offline reinforcement learning (RL) is a technique that aims to learn optimal policies using a static dataset collected by a behavior policy. It is applicable in scenarios where online interaction with the environment is infeasible or undesirable. However, there is still a lack of precise theoretical understanding of offline RL. Previous work has focused on online RL or the generative model setting, but not on offline RL. Additionally, the sample complexity of offline RL in tabular Markov Decision Processes (MDPs) remains unsettled. Some existing work has provided sample complexity bounds for evaluating fixed policies but not for policy optimization. In this paper, we propose a new algorithm called OPDVR for offline RL and establish its sample complexity bounds in various settings. Our algorithm achieves improved sample complexity in the finite-horizon non-stationary setting and optimal sample complexity in the stationary and infinite-horizon settings. We also present a sharp analysis of offline RL with stationary transitions and introduce a doubling technique to overcome initialization dependence defects in previous variance reduction algorithms.