This paper investigates the mechanisms behind hierarchical learning in neural networks. While empirical evidence suggests that neural networks can learn hierarchically by combining simpler concepts in intermediate and deeper layers, the theoretical understanding of this process is still lacking. The paper aims to identify naturally structured and interpretable classes of hierarchical functions and demonstrate how regular neural networks can learn them. The key objectives include capturing naturally occurring data, interpreting the inner workings of neural networks, and ensuring regularity of the network architecture. The paper introduces a new structurally-defined class of hierarchical functions and provides guarantees for learning by regular neural networks. The main results include shedding light on the learning mechanism of staircase functions and proving that regular networks can efficiently learn them. Theorems and experimental results are presented to support the findings, and the potential extension of the techniques to other function spaces is discussed.