Recent successes in reinforcement learning (RL) have been achieved in domains such as game playing and robotics control. However, RL algorithms still suffer from sample inefficiency, requiring a large number of training samples to reach human-level performance. This inefficiency may be due to the algorithms' limited ability to leverage prior knowledge. In contrast, humans excel at transferring skills to new scenarios. Meta-reinforcement learning (meta-RL) aims to address this issue by formalizing the learning and transfer of prior knowledge in RL. In this paper, we propose a unified framework for estimating higher-order derivatives of value functions in meta-RL. We show that our framework unifies and extends several existing methods in the field and provides insights into the trade-off between bias and variance in the estimates. Additionally, we demonstrate the practical implementation of our framework using auto-differentiation libraries. Experimental results validate the effectiveness of our approach.