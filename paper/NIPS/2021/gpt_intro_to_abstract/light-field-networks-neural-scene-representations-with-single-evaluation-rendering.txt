This paper addresses the problem of inferring a representation of a scene's 3D shape and appearance given 2D image observations. The authors propose a novel neural scene representation called Light Field Networks (LFNs), which encode a scene by directly mapping an oriented camera ray to the observed radiance. LFNs eliminate the need for querying opacity and RGB at 3D locations, significantly speeding up rendering compared to volumetric methods. The authors demonstrate that LFNs can encode information about scene geometry and introduce the use of Pl√ºcker coordinates to parameterize 360-degree light fields. By embedding LFNs in a meta-learning framework, they show reconstruction and novel view synthesis of simple scenes from sparse 2D image supervision. The authors also demonstrate that LFNs can encode both appearance and geometry by extracting sparse depth maps from their derivatives. Overall, the proposed LFNs offer real-time rendering, reduced memory utilization, and show promising results in scene reconstruction and novel view synthesis.