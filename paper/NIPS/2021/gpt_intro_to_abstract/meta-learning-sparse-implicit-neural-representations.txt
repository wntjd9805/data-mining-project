This paper introduces the concept of implicit neural representations (INRs) in the field of computer science. INRs are a new paradigm for signal representation that use neural networks with continuous activation functions to approximate the mapping between coordinates and signal values. The continuity of INRs offers practical benefits such as superresolution and inpainting tasks, without the need for discrete representations. However, scaling INRs to handle a large set of signals poses challenges in terms of memory and computation requirements. Existing approaches address this problem through shared network structures, meta-learning, or weight quantization, but each approach has its limitations. To overcome these limitations, the paper proposes a new framework based on neural network pruning. The goal is to train a set of sparse INRs that efficiently represent diverse signals. The paper formulates the problem and introduces a pruning algorithm called Meta-SparseINR, specifically designed for INRs. Experimental results demonstrate the effectiveness of Meta-SparseINR in fitting seen and unseen signals with improved performance compared to baseline methods.