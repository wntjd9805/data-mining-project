Human pose recovery is an active area of research in computer vision, with applications in augmented reality, virtual shopping, and human-robot interaction. Recent advancements in human pose recovery have been attributed to the availability of 3D pose supervision from large-scale datasets. However, models trained on synthetic and in-studio datasets often struggle with poor performance on diverse datasets. To address this challenge, some approaches use weaker forms of supervision, such as 2D pose keypoints, but these methods have limitations in terms of scalability and generalizability. In this paper, we propose a self-adaptive monocular 3D human pose recovery framework that does not rely on strong ground-truth or auxiliary target supervision. Our framework leverages foreground silhouettes, which are robust to domain shifts, and extracts a skeleton-like topology to provide stronger supervision for articulation-centric tasks. We demonstrate the effectiveness of our approach on various challenging domains and achieve state-of-the-art performance.