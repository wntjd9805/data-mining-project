Federated Learning (FL) is a popular approach for machine learning in massively distributed datasets, where multiple worker nodes collaborate to learn a joint model using only local data. In FL, the data is collected or off-loaded to worker nodes, which work together with a server node to learn a centralized model. This paper introduces the Stochastic Two-Sided Momentum (STEM) algorithm, which utilizes momentum-assisted stochastic gradient directions for both worker node and server node updates. The algorithm achieves an optimal trade-off between minibatch sizes and the number of local updates, resulting in the optimal sample and communication complexities for FL. The paper also explores a special case of STEM that reveals insights into the classical FedAvg algorithm. These findings provide practical guidelines for designing effective FL algorithms.