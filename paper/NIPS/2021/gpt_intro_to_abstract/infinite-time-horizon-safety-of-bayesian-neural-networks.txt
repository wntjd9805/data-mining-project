Deep neural networks (DNNs) have achieved success in various domains, leading to interest in applying them in safety-critical applications such as autonomous vehicles and healthcare systems. However, certifying the safety of DNNs in these domains remains a challenge. While there is a significant body of literature on the safety verification of DNNs, the formal safety verification of Bayesian neural networks (BNNs) has received less attention. This paper addresses this gap by studying the safety verification problem for BNN policies in safety-critical systems over an infinite time horizon. The authors propose a method for computing safe weight sets for BNNs, ensuring safety for every system execution. They also introduce a novel algorithm for learning a safe positive invariant, which is used to verify the safety of the weight set. The methodology is evaluated on various benchmark applications.