Stochastic convex optimization (SCO) is a widely studied problem in machine learning and statistics. While the problem has been well-understood in the context of observed datasets, it has become evident that additional considerations such as privacy preservation and adaptivity are necessary to address modern learning problems. Previous works have focused on worst-case scenarios and minimax optimality, but they fail to reflect achievable performance. In this paper, we address the problem of adaptivity in SCO under privacy constraints. We introduce the concept of growth classes and develop private adaptive algorithms that adapt to the actual growth of the objective function. We provide optimality guarantees for our algorithms and contribute new results on optimal algorithms for SCO under pure differential privacy constraints. Additionally, our algorithms provide high-probability loss bounds, which are superior to existing results that only provide bounds on expected loss. Lastly, we provide information-theoretic lower bounds and capture the optimal dependence on problem parameters.