This paper introduces CLIP-It, a language-guided multimodal summarization model for videos. The model takes a video and a natural language text, either a video description or a user-defined query, as inputs and generates a summary video based on the text. Unlike existing methods, CLIP-It combines both generic and query-focused video summarization tasks, allowing for open-ended natural language queries for customization. The model utilizes a Transformer with positional encoding to attend to all frames simultaneously while preserving their order. Experimental results demonstrate the effectiveness of CLIP-It, achieving performance improvements on standard benchmarks and state-of-the-art results on both generic and query-focused datasets.