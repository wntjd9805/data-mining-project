In this paper, we address the problem of improving the sample efficiency of reinforcement learning agents in complex environments by pre-training low-level skills. We focus on the control of (simulated) robots, where there is a natural hierarchical decomposition of tasks. Prior work in hierarchical reinforcement learning (HRL) has relied on the assumption that low-level skills should control the center of mass of the robot, which is suitable for navigation tasks but may not be applicable to other problem classes. We propose a new approach that acquires a hierarchy of skills of increasing complexity, which can be composed with a high-level policy. These skills are pre-trained in an environment without rewards and are used for exploration in downstream tasks. Our experiments on sparse-reward tasks with simulated bipedal robots demonstrate the effectiveness of our approach in selecting different skills for each task, as well as the superiority of our hierarchical skill framework over existing approaches. We also introduce a benchmark suite of tasks for evaluating motor skills and HRL methods. Overall, our work contributes a novel unsupervised pre-training approach, a three-level hierarchical policy for skill selection, a benchmark suite of tasks, and a thorough experimental analysis of the implications of prior knowledge in skills.