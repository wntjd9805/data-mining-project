This paper introduces a novel spatio-temporal learning framework called the shifted chunk Transformer, which leverages the recent advancements of Transformer models in NLP. The framework extracts fine-grained intra-frame features with low computational complexity by dividing each frame into image chunks and using locality-sensitive hashing. Additionally, a shifted self-attention module is designed to consider object motion effects, and a pure-Transformer-based clip encoder models inter-frame relationships. The proposed approach outperforms previous state-of-the-art methods on various action recognition benchmarks.