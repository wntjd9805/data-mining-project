Representing clothed humans as neural implicit functions is a growing area of research in computer vision. Previous works have focused on reconstructing clothed humans from different types of data, such as monocular images or RGBD videos, but they do not capture pose-dependent cloth deformation. Traditional parametric models can represent pose-dependent soft tissue deformations, but they cannot be easily extended to model clothed humans. In this paper, we propose a meta-learning approach called MetaAvatar, which incorporates priors of dynamic neural Signed Distance Fields (SDFs) of clothed humans. This enables fast fine-tuning for generating new avatars given only a few depth images. We use a hypernetwork to predict the parameters of the neural implicit function, allowing us to capture high-frequency details of diverse cloth types. Our approach yields controllable neural SDFs with dynamic surfaces in minutes, making it suitable for building personalized human avatars from commodity RGBD sensors.