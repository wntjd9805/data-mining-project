Stochastic optimization (SO) is a widely studied problem in machine learning, statistics, and operations research. In particular, differentially private stochastic optimization (DP-SO) has received significant attention in the past decade. However, there are still significant challenges in this field, including the lack of linear-time algorithms for nonsmooth DP-SO and the limited understanding of DP-SO in non-convex cases. In this paper, we aim to address these challenges and propose faster and more accurate methods for DP-SO. Our contributions include nearly linear-time algorithms for convex generalized linear losses in the ℓ2-setting and dimension-independent excess risk bounds in the ℓ1-setting.