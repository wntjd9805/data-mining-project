Generative models have made significant progress in generating realistic natural language, synthetic images, and human speech/music. However, there is still room for improvement, particularly in terms of diversity and scalability. Current state-of-the-art models, such as GANs, have limitations in capturing diversity and are challenging to train. Likelihood-based models offer more diversity but lack visual fidelity and sampling speed. Diffusion models, a class of likelihood-based models, have shown promising results but still lag behind GANs on difficult generation datasets. In this paper, we propose improvements to diffusion models by enhancing the model architecture and developing a strategy to balance diversity and fidelity. We present experimental results that demonstrate the effectiveness of our approach, achieving state-of-the-art performance on unconditional and conditional image synthesis tasks.