Partially observable Markov decision processes (POMDPs) provide a general framework for describing decision problems where the underlying dynamics are Markovian, but the observations provide only partial information about the states. However, the computational and statistical complexity of POMDPs poses challenges for finding optimal policies. In this paper, we focus on a specific type of POMDP called a latent Markov decision process (LMDP), where the hidden variables have slow dynamics or remain static in each episode. We explore reinforcement learning for LMDPs and address the lack of understanding and challenges in this area. Our work considers episodic LMDPs with a finite time-horizon, where a static latent variable selects one of multiple MDPs at the beginning of each episode without being revealed to the agent. This paper introduces the formal description of LMDPs and presents insights and solutions for tackling these problems.