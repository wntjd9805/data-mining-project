Reinforcement learning (RL) aims to find an optimal control policy that maximizes expected cumulative reward in an unknown environment. While standard RL algorithms focus on maximizing a single objective, many real-world applications require the learned policy to also satisfy safety constraints. This paper explores the Constrained Markov Decision Process (CMDP) formalism for modeling safety criteria and proposes two algorithms, OptPess-LP and OptPess-PrimalDual, that guarantee zero or bounded safety constraint violation while achieving an O(K) regret with respect to the performance objective. The algorithms differ based on whether a strictly safe policy is known a priori or not, and their exploration strategies are designed to balance optimism and pessimism to ensure safety. The proposed algorithms outperform existing methods by greatly reducing constraint violation, which is crucial for safety-critical applications.