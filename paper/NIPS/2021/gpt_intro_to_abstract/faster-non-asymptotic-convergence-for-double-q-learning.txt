Double Q-learning is a model-free reinforcement learning algorithm that is widely used for learning an optimal policy. It improves upon vanilla Q-learning by using two Q-estimators to estimate the Q-function value and update the Q-function, reducing the overestimation of the Q-function. Double Q-learning has shown promising results in both finite state-action space settings and infinite settings, and its convergence properties have been explored theoretically. However, previous studies have focused on polynomial learning rates, and it remains unclear if a constant learning rate can significantly improve the convergence rate. In this paper, we provide affirmative answers to this question by establishing sharper finite-time bounds for double Q-learning with constant learning rates, improving upon existing bounds. Our results encourage the use of constant learning rates in practice for better convergence performance.