This paper addresses the problem of disentangling high-dimensional data, such as images, by identifying the underlying factors of variation. The authors propose a novel method called ZeroDIM, which can disentangle a subset of partially labeled attributes and separate them from a complementary set of residual attributes that are never explicitly specified. They demonstrate that current semi-supervised methods and disentanglement methods that assume full supervision struggle in this disentanglement setting. To overcome this, they leverage CLIP, a language-image embedding model, to obtain partial labels without manual annotation. The authors validate their method on synthetic data and real-world image manipulation tasks, achieving state-of-the-art results. Overall, the contributions of this work include the introduction of a novel disentanglement method, the replacement of manual human annotation with partial labels from CLIP, and the demonstration of superior performance on both synthetic and real-world scenarios.