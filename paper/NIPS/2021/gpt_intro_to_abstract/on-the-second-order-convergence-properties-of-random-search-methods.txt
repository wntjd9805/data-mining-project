We consider the problem of solving a non-convex optimization problem where the derivatives of the objective function are not directly accessible or can only be approximated at a high computational cost. This problem setting has gained attention in machine learning applications such as adversarial attacks, reinforcement learning, meta-learning, online learning, and conditional optimization. We focus on a class of derivative-free methods known as random direct-search methods, which evaluate the objective function over multiple directions to ensure descent using a small step size. Direct-search algorithms have been popular due to their good performance, global convergence guarantees, and straightforward implementation. In machine learning, non-convex objective functions introduce additional challenges, such as the presence of saddle points and suboptimal local minima. Instead of aiming for a global minimizer, we often seek a second-order stationary point (SOSP) where the gradient vanishes and the Hessian is positive definite. Many machine learning problems have numerous saddle points that yield suboptimal solutions and are difficult to escape from. Previous analyses of random direct search have focused mainly on convergence to first-order stationary points, and computing or storing a full Hessian is expensive in high dimensions. In this paper, we explore the complexity of finding second-order stationary points with random search methods and propose a variant called RSPI that overcomes the exponential complexity scaling of the vanilla random search approach. Our empirical results demonstrate the superiority of RSPI in terms of algorithm iterations and wall-clock time compared to standard random-search methods.