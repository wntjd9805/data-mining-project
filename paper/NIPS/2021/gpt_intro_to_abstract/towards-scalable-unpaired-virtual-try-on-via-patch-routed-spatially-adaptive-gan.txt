Image-based virtual try-on is an emerging application in computer vision that aims to computationally transfer a garment onto a person in an image, revolutionizing the shopping experience and reducing product returns. However, existing methods heavily rely on paired training data and are unable to exchange garments directly between two person images, limiting their scalability and applicability. In this paper, we propose PASTA-GAN, a novel solution for unpaired virtual try-on that can accurately synthesize garment shape and style. Our approach incorporates a patch-routed disentanglement module to separate the spatial information of garments, enabling the synthesis network to generate accurate style regardless of spatial variations. We also introduce a spatially-adaptive residual module to mitigate feature misalignment. Our experiments demonstrate that PASTA-GAN outperforms previous approaches in both unpaired and paired try-on scenarios.