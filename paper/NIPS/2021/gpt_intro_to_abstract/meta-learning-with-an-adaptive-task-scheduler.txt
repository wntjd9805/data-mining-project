Meta-learning has gained popularity in recent years as a method to efficiently learn new tasks by training a meta-model from a set of historical tasks. However, the assumption that all tasks are equally important and should be sampled uniformly is often not valid. Some tasks may be noisy or there may be an uneven distribution of tasks across clusters. To address this, a task scheduler is introduced to determine which tasks should be used for meta-training. This paper proposes an Adaptive Task Scheduler (ATS) that uses a neural scheduler to predict the probability of each training task being sampled. The neural scheduler considers the loss of the meta-model with respect to a task and the similarity between gradients of the meta-model, improving the generalization ability of the meta-model. Experimental results demonstrate the superiority of the proposed scheduler over existing methods in various settings.