Neural network pruning is a technique that can reduce the computational cost of model training and inference, as well as potentially mitigate overfitting. The Lottery Ticket Hypothesis (LTH) suggests that a randomly initialized dense neural network always contains a winning ticket, a sub-network that can achieve the same testing accuracy as the original network in less training time. However, the theoretical justification for winning tickets remains limited. This paper presents a systematic analysis of learning pruned neural networks in the oracle-learner setup, providing theoretical support for the improved generalization of winning tickets. The contributions include proposing an accelerated gradient descent algorithm for learning pruned models, analyzing the sample complexity of pruned networks, characterizing the optimization landscape and improved generalization of winning tickets. Notations and definitions are also provided for clarity.