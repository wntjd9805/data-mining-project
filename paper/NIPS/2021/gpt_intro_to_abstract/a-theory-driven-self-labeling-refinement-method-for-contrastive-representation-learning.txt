Self-supervised learning (SSL) is a popular approach in computer science that enables feature learning without manual annotations. One leading technique in SSL is contrastive learning, which trains a network to bring representations of different image crops from the same instance closer together, while pushing representations of different instances further apart. However, the use of one-hot labels in contrastive learning is problematic as it does not accurately reflect the semantic similarity between samples. In this paper, we propose a self-labeling refinement method called SANE, which iteratively generates more accurate and informative labels for contrastive learning. Our method consists of two modules: Self-Labeling Refinery (SLR) and Momentum Mixup (MM). SLR estimates semantic similarity between a query and its keys (positive and negatives) and combines this information with the vanilla one-hot labels to generate improved labels. MM further reduces label noise and increases augmentation diversity. We provide theoretical analysis and experimental results to demonstrate the effectiveness of our approach.