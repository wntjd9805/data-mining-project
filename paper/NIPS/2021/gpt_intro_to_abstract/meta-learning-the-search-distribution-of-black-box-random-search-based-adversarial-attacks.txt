Neural networks have proven to be vulnerable to small imperceptible perturbations in the input known as adversarial examples. This issue raises concerns for the real-world implementation of deep learning in safety-critical domains like autonomous driving. Adversarial examples not only have practical implications but also provide insights into the nature of artificial neural networks and their differences from biological counterparts. Evaluating adversarial robustness is a challenging task that often involves approximate methods called adversarial attacks. These attacks, categorized as white-box and black-box attacks, aim to find adversarial examples using different levels of model access. While white-box attacks assume full access to the model, they can be hindered by gradient obfuscation. On the other hand, black-box attacks provide an additional perspective on model robustness with limited model information. Randomized search schemes have shown promise in black-box attacks but require manual design. This paper proposes a method that utilizes gradient-based meta-learning to optimize controllers for schedule and proposal distribution in random search-based attacks. The proposed Meta Square Attack (MSA) reduces the need for manual design and performs well in black-box settings with different query regimes. The results demonstrate improvements in robust accuracy compared to existing approaches for various threat models and datasets.