In this paper, we investigate a class of constrained optimization problems that involve minimizing a convex function subject to linear constraints. Specifically, we focus on problems where the objective function satisfies a higher-order smoothness-like condition called M-quasi-self-concordance. This condition is met by several prominent problems in machine learning and numerical methods such as logistic regression and softmax regression. Previous work has introduced a technique known as width reduction, which improves the iteration complexity of these problems. However, existing approaches are problem-specific and rely on specialized methods and analyses. In this paper, we present a unified approach to achieving fast convergence rates using width reduction, which extends beyond previously studied problems to encompass a wider range of quasi-self-concordant losses. Our method offers improved convergence rates without the need for explicit acceleration schemes, complementing recent work based on a ball-oracle approach. We also highlight the potential connection between width reduction and Monteiro-Svaiter acceleration techniques. Additionally, we introduce our main results and applications, including a width-reduced method that provides an approximate solution for quasi-self-concordant functions. We prove the efficiency of our method for problems such as softmax regression and `p regression, and we also extend our results to cover general-self-concordant functions. These findings contribute to advancing the field of constrained optimization, particularly in solving logistic regression problems.