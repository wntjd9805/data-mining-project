This paper introduces a data-driven framing of control in reinforcement learning (RL) settings. Instead of defining tasks in terms of manually designed reward functions, the authors propose a method called example-based control, where tasks are specified through a collection of example success states. The algorithm learns to predict future success directly from transitions and success examples, without the need for a separate reward function. The paper presents algorithmic, theoretical, and empirical benefits of this approach, demonstrating its superiority over state-of-the-art imitation learning methods and reward function learning methods. The method is shown to successfully solve complex tasks and generalize to new environments with varying shapes and goal locations.