Machine learning and statistical pattern recognition often involve estimating unknown data distributions based on independent and identically distributed (i.i.d.) samples. One important consideration in this process is the bias-variance trade-off, which refers to the tension between the amount of prior information about the data distribution and the number of samples needed for a high-confidence solution. Neural networks provide a way to control this trade-off by adjusting the depth and size of the network. Increasing these parameters makes the class of functions represented by the network more expressive, reducing bias but increasing variance or complexity. Universal approximation theorems demonstrate that even a single hidden layer can reduce bias to any desired level by increasing network size. However, recent work suggests that increasing depth can also provide significant reductions in size. This paper aims to better understand the function classes represented by different network architectures, which could have implications for the mathematical foundations, algorithms, and statistical learning of neural networks. The authors propose a conjecture that additional layers strictly increase the set of representable functions and present partial progress towards proving it. They also explore the relationship between ReLU networks and p-term max functions and provide upper bounds on the size of networks necessary for expressing continuous piecewise linear functions.