This paper focuses on the setting of learning with expert advice, where a learner selects a probability distribution over experts, observes their losses, and tries to minimize regret. The classical "worst-case" online learning paradigm assumes adversarial losses, but this paper explores easier notions of data and performance measures. Specifically, the paper considers relaxing the performance measure to quantile regret and regret within the semi-adversarial paradigm. The authors introduce a follow-the-regularized-leader (FTRL) algorithm with new root-logarithmic regularizers that achieve minimax optimal performance for both quantile and semi-adversarial regret. They provide guarantees for quantile regret and improve the dependence on the number of experts in the regret bound for the semi-adversarial paradigm. The paper presents a novel local-norm analysis of FTRL and demonstrates that the choice of regularizer leads to trade-offs in the regret bound.