Deep reinforcement learning (DRL) has achieved significant success in various scenarios due to advancements in neural networks. However, learning an expressive function approximator in DRL requires collecting a large dataset, which is often challenging in real-world scenarios where trial and error is unsafe or unethical. To overcome this barrier, this paper examines the incorporation of observational data, which is abundantly available but possibly confounded, to improve sample efficiency in RL. The paper proposes a class of confounded Markov decision processes (MDPs) and presents two algorithms, DOVI and DOVI+, that correct for confounding bias in the observational data. These algorithms enable the estimation of causal effects and the construction of exploration bonuses. The paper provides theoretical guarantees on the regret of DOVI and DOVI+, showing that they achieve near-optimal performance in the online setting.