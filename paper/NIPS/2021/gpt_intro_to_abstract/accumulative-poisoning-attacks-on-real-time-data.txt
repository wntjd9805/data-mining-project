This paper addresses the issue of poisoning attacks in machine learning services that collect training data from the outside world. Previous studies have focused on poisoning offline datasets, but recent work explores the poisoning of real-time data streaming, where adversaries can interact with the training process. To mitigate the threat of poisoning attacks, various defense mechanisms have been proposed, but they may not be effective in real-time settings. The paper demonstrates through simulation experiments that a single update step in the training process can significantly degrade the model's accuracy. The authors introduce an accumulative poisoning attack strategy that bypasses defense mechanisms and highlights the need for more robust defenses in real-time data streaming scenarios. Experimental results on MNIST and CIFAR-10 datasets support the effectiveness of the accumulative poisoning attacks, which emphasize the importance of preserving the integrity of shared online or federated models.