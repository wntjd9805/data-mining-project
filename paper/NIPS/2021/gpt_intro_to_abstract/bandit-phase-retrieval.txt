In this computer science paper, the authors study the low-rank bandit problem known as phase retrieval and focus on its unique information structure and its impact on algorithm design choices. They find that standard approaches to solve this problem do not achieve optimal regret, and instead propose an explore-then-commit algorithm with adaptive exploration that learns to gain information at a faster rate. The paper provides upper bounds on the regret and simple regret, matching existing lower bounds up to logarithmic factors. Additionally, the authors show that existing upper bounds for information-directed sampling may be loose, suggesting that conjectured lower bounds for low-rank bandits may not be true.