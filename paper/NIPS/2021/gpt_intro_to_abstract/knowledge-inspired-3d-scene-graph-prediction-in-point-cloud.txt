Scene graph parsing from 3D data is a valuable tool for scene understanding, VR/AR scene interactions, and robot navigation. However, 3D data is often incomplete and presents visual challenges, making object and relationship recognition difficult. In this paper, we propose a novel 3D scene graph prediction method that incorporates prior knowledge in the form of class-dependent prototypical memories. We re-formulate the problem into two sub-tasks: knowledge learning and knowledge-inspired scene graph prediction. We use a graph auto-encoder, called meta-embedding, to learn class prototypes that encode structural knowledge without visual confusion. Our scene graph prediction model combines geometric features with the corresponding meta-embedding to predict relation triplets. Experimental results demonstrate the effectiveness of our approach, achieving state-of-the-art performance on the 3D semantic scene graph dataset.