This paper introduces Anderson acceleration, a method to accelerate reinforcement learning algorithms by leveraging historical data. The paper establishes the connection between Anderson acceleration and quasi-Newton methods in policy iteration and provides improved convergence guarantees under certain assumptions. The paper also proposes a Stable Anderson Acceleration (Stable AA) method, which incorporates stable regularization and the MellowMax operator to enhance stability and convergence. Experimental results on various Atari games demonstrate the effectiveness of the Stable AA method in terms of faster convergence and better performance compared to existing Anderson acceleration baselines. The paper provides a unified analytic framework that sheds light on Anderson acceleration in reinforcement learning algorithms, focusing on acceleration, convergence, and stabilization.