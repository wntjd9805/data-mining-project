Deep learning has revolutionized feature representations in computer science. Self-Supervised Learning (SSL) is a popular approach that first learns a generic feature representation through unsupervised pretext tasks and then fine-tunes it for specific applications. However, most SSL works focus on improving stage-2 performance without fully understanding the learned feature representation. In this paper, we aim to formally understand what constitutes a good feature representation by examining the gap between SSL and Supervised Learning (SL), and when SSL can surpass SL. We propose the use of Higgins' definition of a disentangled representation to define what is considered "good". We introduce a new algorithm called Iterative Partition-based Invariant Risk Minimization (IP-IRM) that guarantees the learning of disentangled representations in an SSL fashion. We provide theoretical justifications and experimental results to support our approach.