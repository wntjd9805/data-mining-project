During development, deep learning models are typically trained and validated on data from the same distribution. However, in real-world scenarios, there are often changes in sensor quality, weather conditions, and image acquisition and processing, leading to a shift in the distribution of the input data known as covariate shift. This shift can decrease the performance and accuracy of the model, but it has been observed that the model's confidence remains high even when accuracy is degraded. Model calibration aims to align confidence with empirical accuracy and provides important uncertainty information for decision-making in applications such as self-driving and active learning. Existing calibration methods have been studied for in-distribution data but perform poorly under distribution shift. This paper proposes a study of sensitivity from a geometric perspective and explores the reasons behind the insensitivity of a model's confidence to distribution shift. It also introduces a decomposition of the model's norm and angular similarity into instance-dependent and instance-independent components, and proposes a training and inference scheme to encourage the norm to reflect distribution changes. The proposed method achieves state-of-the-art results in calibration metrics while being one of the simplest calibration methods to implement.