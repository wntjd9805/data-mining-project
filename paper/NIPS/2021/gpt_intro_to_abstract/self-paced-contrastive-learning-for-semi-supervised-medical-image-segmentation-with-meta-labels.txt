This paper discusses the importance of pre-training neural networks in deep learning and the impact it has on performance. It highlights the debate surrounding pre-training and the findings that show it can hinder performance in certain scenarios. The paper then explores the concept of contrastive learning and its potential benefits for pre-training models. It also addresses the limitations of current semi-supervised learning approaches in the field of medical imaging and proposes a novel self-paced contrastive learning method that takes into account the noisiness of weak labels from meta data. The paper presents the contributions of this work, including the introduction of a self-paced strategy for contrastive learning, the demonstration of the usefulness of contrastive loss on meta-data, and the improvement achieved by combining multiple meta-labels in the learning framework. The proposed approach is empirically validated on five medical imaging datasets and is shown to outperform existing methods in terms of performance and the use of labeled data.