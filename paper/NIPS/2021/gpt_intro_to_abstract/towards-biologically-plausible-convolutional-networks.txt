Convolutional networks are widely used in various domains, including visual, speech recognition, text classification, and time series classification. They have been inspired by the simple and complex cells in the visual cortex and have been successful in reproducing activity in the visual system. However, as a model of the visual system, convolutional networks are problematic because they share weights, which is not biologically plausible. Locally connected networks have been proposed as an alternative, but they perform worse than convolutional networks on image classification tasks. In this paper, two mechanisms are considered to bridge the gap between biologically plausible locally connected networks and convolutional networks. The first is extensive data augmentation, which requires more training data but still fails to close the performance gap. The second mechanism is dynamic weight sharing, which implements a sleep-like phase and lateral connections to facilitate weight sharing. This approach performs almost as well as convolutional networks and achieves better fit to the ventral stream data. The study suggests that convolutional networks may be biologically plausible with the addition of lateral connectivity and Hebbian learning. Convolutional networks remain a good model organism for neuroscience as they consume less memory and have faster processing speed than locally connected networks.