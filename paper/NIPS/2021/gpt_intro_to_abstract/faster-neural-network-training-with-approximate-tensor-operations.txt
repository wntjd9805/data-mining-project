Approximation techniques for accelerating deep neural network (DNN) training have gained attention in recent years. This paper proposes a novel approach to accelerate DNN training by approximating tensor operations through sampling. The authors compare different algorithms for approximating matrix multiplication and find column-row sampling (CRS) to be the most suitable for training. They also discuss the similarities and differences between sampling-based approximations and Dropout regularization. The authors aim to answer two main questions: can neural networks be trained using approximate tensor operations, and what are the relations between using exact or approximate operations during training? They analyze the effects of approximations on linear regression and show that sampling-based approximations in the backward pass provide convergence guarantees. They also propose a new TopK-CRS algorithm for matrix product approximation and extend the approach to convolutions in CNNs. The techniques are implemented in PyTorch and evaluated on various DNN architectures, demonstrating significant reductions in computation and training time without sacrificing model accuracy. The authors also develop a sampling technique for distributed training, leading to speedup in multi-node training. The contributions of this work include deriving convergence guarantees, developing novel sampling algorithms, extending algorithms to convolutions, and demonstrating improvements in computation, communication, and training time on popular neural network architectures.