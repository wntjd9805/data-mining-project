Large-scale Transformer networks have become very successful in various natural language tasks. However, the increasing size of these models poses challenges in deployment and training costs. To address this issue, compression techniques such as sparsity have been proposed. This paper introduces NxMTransformer, a method to induce NxM semi-structured sparsity in large-scale Transformer networks using the ADMM technique. Experimental results show that NxMTransformer outperforms state-of-the-art techniques in terms of accuracy for natural language processing tasks. The paper also analyzes the impact of ADMM on fine-tuning accuracy and demonstrates that NxMTransformer is complementary to other model compression techniques. The proposed method allows for compatibility with existing hardware and can be easily converted to the deployment format.