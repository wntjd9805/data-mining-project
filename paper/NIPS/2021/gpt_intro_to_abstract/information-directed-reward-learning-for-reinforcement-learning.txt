This paper introduces Information Directed Reward Learning (IDRL), a general active reward learning approach for learning a model of the reward function from expensive feedback. Unlike prior methods that aim to uniformly reduce the model's error, IDRL focuses on finding a good policy. It can use arbitrary Bayesian reward models and different types of queries, making it more versatile. The authors provide an exact and efficient implementation of IDRL using Gaussian process reward models and various types of queries, as well as an approximation using a deep neural network reward model and a policy gradient algorithm. Extensive evaluations in simulated environments demonstrate the superiority of IDRL over previous methods.