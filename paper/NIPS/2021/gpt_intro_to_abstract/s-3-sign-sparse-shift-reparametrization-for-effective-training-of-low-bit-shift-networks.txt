Deep neural networks (DNNs) have shown great success in various tasks, but their training and inference are resource-intensive due to their over-parametrization and costly multiplication operations. To make DNNs feasible in resource-constrained scenarios, researchers have proposed low-bit neural networks with binary or ternary weights that replace multiplication with cheaper operations like sign flip or bit-shift. In this paper, we focus on low-bit shift neural networks that replace multiplication with bit-shift operations. However, these networks suffer from accuracy degradation and weight initialization sensitivity. To address these issues, we analyze the training diagram of low-bit shift networks and identify the design flaw of the weight quantizer as the cause. We propose a new training strategy called S3 re-parameterization, which regularizes the sparsity parameter to optimize the (cid:96)0 norm of the discrete weights. Our approach surpasses previous methods and achieves state-of-the-art performance with only 3-bit weight representation on the ImageNet dataset. We also introduce weight dynamics indices and show that our method better aligns with full-precision weights during training. Our research opens up possibilities for similar techniques to be used in training low-bit networks.