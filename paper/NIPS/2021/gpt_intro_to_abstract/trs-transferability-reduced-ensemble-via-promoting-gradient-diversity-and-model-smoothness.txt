Machine learning systems, particularly those based on deep neural networks (DNNs), have become prevalent in various applications. However, recent studies have demonstrated the susceptibility of DNNs to adversarial examples, which manipulate the models by introducing small perturbations to the original data. Various attack strategies have been proposed to generate adversarial examples, both in digital and physical environments. Surprisingly, it has been observed that adversarial examples created for one model can effectively attack another model, even without access to its internal workings, posing a significant threat to DNNs. Thus, it is crucial to understand the theoretical underpinnings of this adversarial transferability and leverage this knowledge to develop robust ensemble models. In this paper, we address these issues by investigating the conditions that influence adversarial transferability theoretically and proposing a Transferability Reduced Smooth (TRS) ensemble approach to reduce transferability and enhance model robustness. We conduct comprehensive experiments to evaluate TRS against strong white-box and black-box attacks, demonstrating its superiority in terms of robustness and transferability reduction compared to existing state-of-the-art ensemble methods. Our work contributes to both theoretical insights into adversarial transferability and practical techniques for developing robust machine learning ensembles.