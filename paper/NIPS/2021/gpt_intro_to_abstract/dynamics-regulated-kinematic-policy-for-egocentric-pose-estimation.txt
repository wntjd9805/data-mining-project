This paper addresses the challenging task of inferring a wearer's global 3D full-body pose and interaction with objects in the scene using video captured by a single head-mounted wearable camera. The ability to accurately estimate human motion and human-object interaction from a front-facing camera is crucial for applications such as virtual and augmented reality, sports analysis, and wearable medical monitoring. Existing approaches to egocentric pose estimation fall into two paradigms: kinematics-based and dynamics-based. While kinematics-based approaches can achieve accurate pose estimates, they often violate physical constraints. On the other hand, dynamics-based approaches output physically-plausible human motion but are difficult to learn and generalize. To address these limitations, the authors propose a hybrid approach that combines the strengths of both paradigms. They train a dynamics-based humanoid controller using a large human motion database and then use an object-aware kinematic policy to specify target poses for the controller to mimic. The two aspects are synchronized, allowing for intelligent motion planning and adaptation based on the simulation state. The proposed model is optimized through supervised and reinforcement learning, resulting in improved robustness to real-world scenarios. Experimental results demonstrate the effectiveness of the model in estimating 3D poses and human-object interactions.