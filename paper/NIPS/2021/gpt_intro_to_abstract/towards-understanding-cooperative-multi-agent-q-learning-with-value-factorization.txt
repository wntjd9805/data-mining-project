Cooperative multi-agent reinforcement learning (MARL) has shown promise in addressing coordination problems in various applications, including robotics, autonomous cars, and sensor networks. However, MARL faces challenges such as multi-agent credit assignment, non-stationarity, and scalability. Recent advancements in deep learning techniques have led to progress in cooperative MARL, particularly in value-based methods. This paper introduces a theoretical framework, Factorized Multi-Agent Fitted Q-Iteration (FMA-FQI), to analyze cooperative MARL with value factorization. The framework investigates two popular value factorization methods, linear factorization and Individual-Global-Max (IGM) factorization. The paper provides insights into the algorithmic properties of these approaches and addresses fundamental questions regarding their effectiveness, convergence, and performance. The contributions of this paper include the formalization of FMA-FQI as a theoretical framework, the analysis of linear value factorization, the study of IGM value factorization, and empirical evaluations of deep MARL approaches based on the theoretical implications.