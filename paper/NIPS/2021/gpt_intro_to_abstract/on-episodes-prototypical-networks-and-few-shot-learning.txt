Few-shot learning (FSL) is a popular problem in machine learning, involving classifying examples from previously unseen classes with limited training data. FSL is important for various applications and is often used as a way to validate meta-learning algorithms. However, recent studies have shown that simple baseline methods can outperform established FSL meta-learning methods, casting doubt on the importance of episodes in FSL. In this paper, we investigate the practical usefulness of episodic learning in metric-based nonparametric classifiers, specifically Matching and Prototypical Networks. We find that episodic learning is detrimental to performance, analogous to randomly discarding examples from a batch, and introduces unnecessary hyperparameters. Without episodic learning, these methods achieve competitive accuracy on FSL benchmarks.