Deep Neural Networks optimized by Gradient Descent (GD) methods have shown impressive versatility and performance across various domains. Despite their success, the ability of these networks to improve with increasing model size in overparameterized regimes is counterintuitive and surprising. Contrary to the classical view of overfitting, recent research has proposed the Double Descent (DD) phenomenon as an explanation, indicating better generalization as model size grows beyond the interpolation threshold. In this paper, we explore the DD phenomenon in the context of least squares problems and provide theoretical insights and empirical evidence to understand its underlying causes. Our findings reveal the impact of optimization and the relationship between the peaking behavior of the risk and optimization error. By examining the behavior of simple neural networks, we further validate the implications of our results.