This paper introduces a deep hashing model called OrthoHash that aims to address the challenges of binary code representation in large-scale image retrieval systems. The model focuses on maximizing the cosine similarity between continuous codes and their corresponding binary orthogonal target to achieve both discriminability and quantization error minimization. By unifying these objectives into a single loss function, OrthoHash eliminates the need for weight tuning and simplifies the optimization process. Furthermore, the model leverages margin, label smoothing, and batch normalization techniques to improve intra-class variance, handle multi-label classification, and enforce code balancing. Experimental results demonstrate the effectiveness of OrthoHash on various retrieval tasks, surpassing the state-of-the-art performance on instance-level retrieval.