Reinforcement learning (RL) has achieved remarkable success in learning good decision sequences in stochastic environments. While previous successes have mostly been in simulated environments, RL is increasingly being used to address real-world problems. This paper focuses on the difference between RL in simulated versus real-world environments, particularly in terms of state observations. In simulated environments, the agent can fully observe the state without any additional costs, whereas real-world environments often involve noisy and/or costly state observations. This paper introduces a special class of partially observable Markov decision processes (POMDPs) called Action-Contingent Noiselessly Observable MDPs (ACNO-MDPs) that capture real-world scenarios where state information is complete and noiseless when observed, but may be missing if the agent chooses not to observe. Two specific application domains, healthcare and user-adaptive experiences, are discussed to illustrate the relevance of ACNO-MDPs. The paper also presents algorithms for solving tabular and continuous ACNO-MDPs, providing theoretical guarantees and experimental results to support their effectiveness.