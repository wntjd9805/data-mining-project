Privacy-preserving machine learning has gained significant attention as individuals' data can be used to train statistical models that may inadvertently reveal sensitive information. Differential privacy has emerged as an important tool for quantifying the trade-off between privacy and accuracy in data analysis and learning. Most existing work on differentially private machine learning focuses on the statistical learning setting, where labeled data is assumed to be drawn independently and identically distributed (i.i.d.) from an unknown population distribution. However, this assumption is unlikely to hold in practice as data distributions may change over time or be influenced by the algorithm's previous predictions. Therefore, this paper explores the setting of online learning, where the data sequence can be arbitrary, and considers a notion of privacy that can adapt to the algorithm's predictions. The paper specifically focuses on differentially private online classification and presents results on mistake bounds and regret for classes of different sizes. The results demonstrate that finiteness of the Littlestone dimension is necessary and sufficient for online learnability with differential privacy. This finding strengthens the connection between privately learnable hypothesis classes in the offline and online settings.