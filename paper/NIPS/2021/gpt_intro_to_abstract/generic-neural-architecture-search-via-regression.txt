Existing neural architecture search (NAS) approaches mainly focus on finding high-performing architectures for specific downstream tasks. However, these approaches either search directly on the target task or use a proxy dataset and transfer the architecture to the target task. In this paper, we propose GenNAS, a Generic Neural Architecture Search method, that addresses the limitations of existing NAS approaches. GenNAS adopts a regression-based proxy task using synthetic signals for network training and evaluation, which allows for efficient evaluation of neural architectures without requiring specific knowledge of downstream tasks. We also introduce an automated proxy task search to further enhance the performance of GenNAS. Experimental results demonstrate that GenNAS outperforms existing NAS approaches in terms of ranking correlation and achieves state-of-the-art performance for end-to-end NAS with significantly faster convergence, making it highly efficient and easily applicable to various datasets and tasks.