Deep learning has achieved impressive results in various domains, but the presence of adversarial examples reveals the lack of robustness in deep neural networks. Adversarial attacks can also affect deep reinforcement learning agents, posing a risk in safety critical applications. While heuristic defenses have been proposed, they often fail against stronger attacks. Robustness verification algorithms have shown promise in improving robustness, but most existing approaches focus on classification tasks and not deep RL agents. In this paper, we present RADIAL-RL, a novel framework for training robust deep RL agents using adversarial loss functions based on robustness verification bounds. We demonstrate the effectiveness of RADIAL-RL on popular RL algorithms, achieving superior performance, computational efficiency, and resistance to stronger adversarial perturbations compared to existing works. Additionally, we propose an evaluation method, Greedy Worst-Case Reward (GWC), for assessing RL agent performance under the strongest adversaries.