Cooperative Multi-Agent Reinforcement Learning (CoMARL) has shown promise in solving real-world tasks such as traffic light control, sensor network management, and autonomous vehicle coordination. However, the non-stationary nature of these tasks poses challenges in designing efficient learning methods. Existing methods based on Centralized Training Decentralized Execution (CTDE) have been proposed, but there is a significant performance discrepancy between policy-based and value-based approaches. This discrepancy can be attributed to the inadequate utilization of the centralized training procedure. To address this, we propose the Coordinated Proximal Policy Optimization (CoPPO) algorithm, an extension of PPO that directly coordinates the agents' policies by dynamically adapting the step sizes during policy updates. Through empirical studies, we demonstrate the efficacy and interpretability of CoPPO in a penalty game and the StarCraft II micromanagement benchmark, highlighting its superior performance against strong baselines.