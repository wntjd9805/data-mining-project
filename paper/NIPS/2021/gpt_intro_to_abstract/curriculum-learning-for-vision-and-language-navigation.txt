The Vision-and-Language (VLN) navigation task aims to build smart robots that can perceive the environment, understand human language instructions, and unify multi-modal information to take actions. While many state-of-the-art methods have been proposed, few consider how much the agent learns from the dataset. In this paper, we focus on the education of VLN agents without changing the model structure or modifying the data. We observe that the distribution of sample difficulty within the dataset is often neglected, leading to poor performance on "easy" tasks. Inspired by curriculum learning, we propose a curriculum-based training paradigm for VLN agents and demonstrate its effectiveness in improving both navigation performance and training efficiency. Our contributions include incorporating human prior knowledge into the training process, designing a VLN curriculum, and validating the benefits of curriculum learning.