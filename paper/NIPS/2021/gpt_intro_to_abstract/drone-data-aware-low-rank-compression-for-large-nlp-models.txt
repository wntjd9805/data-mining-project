This paper explores a simpler method to accelerate the inference time of large-scale Natural Language Processing (NLP) models like BERT. Instead of complex architecture redesign, the authors investigate whether low-rank matrix approximation, a classical model compression approach, can be applied to Transformers. Despite the initial skepticism that low-rank compression would not work for BERT, the authors propose a novel low-rank approximation algorithm that leverages the data distribution in NLP applications. They mathematically formulate the generalized low-rank approximation problem and provide a closed-form solution for the optimal rank-k decomposition of the weight matrices. Experimental results show that the proposed approach, named DRONE (data-aware low-rank compression), outperforms SVD and can significantly accelerate the BERT model without sacrificing too much test performance. Furthermore, DRONE can be applied to distilled BERT models to further improve the compression rate.