Progress in scaling up generative language models has led to impressive results on various tasks, but the social impact and cultural context of these models are now receiving increased attention. This paper addresses the risks and potential harms of language models, focusing on undesirable behavior in specific social contexts and the lack of a universal standard for offensive or harmful content. The authors propose a process called PALMS (Process for Adapting Language Models to Society) that adjusts the behavior of a pretrained language model to align with predefined norms and values. They demonstrate that it is possible to modify a language model's behavior with a limited number of samples, resulting in values-targeted models that outperform baseline models on toxicity scoring, human evaluations, and co-occurrence evaluations. PALMS is an iterative process that allows for the addition of new training examples based on performance.