Reinforcement learning has emerged as a powerful technique for tackling complex planning and control problems. However, solving long-horizon problems remains a challenge due to the exponential explosion of possible strategies. One promising solution is to leverage programs to guide agent behavior, consisting of a domain-specific language (DSL), task-specific programs, and low-level neural policies. However, this approach requires significant programming overhead and is particularly complicated for partially observed environments. To address these challenges, we propose model predictive program synthesis (MPPS), which automatically synthesizes guiding programs for program-guided reinforcement learning. MPPS utilizes a conditional generative model of the environment and a high-level goal specification to generate robust programs that handle uncertainty. By shifting the burden of reasoning about new tasks from the user to the agent, MPPS reduces programming overhead and encourages exploration. We validate our approach in 2D Minecraft-inspired and box-world environments, demonstrating its superior performance compared to non-program-guided approaches and its potential for transfer learning to continuous environments.