This paper introduces a novel approach called continuous augmented positional embedding (CAPE) to improve the absolute sinusoidal positional encodings in the Transformer architecture. Instead of discrete positions, continuous positions are utilized to better align with the continuous nature of image, sound, or video data. Additionally, a specific augmentation technique is employed during training to preserve some information about relative token positions. The authors empirically evaluate CAPE on various application domains and demonstrate its superior performance compared to other positional embeddings in terms of generalization. The paper also presents the application of CAPE in a single vision Transformer (UniViT) that outperforms single-resolution baselines and showcases its adaptability to images of any size. Furthermore, a CAPE-based adaptive training scheme for automatic speech recognition (ASR) is proposed, eliminating the need for padding.