Deep neural networks are being increasingly used in high-stakes applications, such as healthcare, but the safe deployment of these models requires not only accuracy but also robustness to distribution shift. Detecting out-of-distribution (OOD) inputs is crucial for model safety, and various approaches have been developed for OOD detection. However, the difficulty of this task depends on the semantic closeness of the outliers to the inlier classes. While there has been impressive progress in detecting far-OOD inputs, the state-of-the-art for near-OOD detection is much lower. In this paper, we explore the use of large-scale pre-trained transformers, such as Vision Transformers (ViT) and Contrastive Language-Image Pre-training (CLIP), for near-OOD detection. We show that these models significantly outperform previous approaches, improving AUROC scores on benchmark tasks. Additionally, we introduce the concept of outlier exposure with known outliers and demonstrate its effectiveness in improving OOD detection. Overall, our contributions include significant improvements in near-OOD detection using pre-trained transformers and the exploration of outlier exposure methods for enhanced OOD detection.