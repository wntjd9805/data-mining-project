This paper addresses the need for human understandable explanations of decisions made by convolutional neural networks (CNNs) in image classification. It argues that the current approach of using a single saliency map to visualize important regions in an image may not be sufficient for users to construct a comprehensive mental model of the classification decision. To address this, the paper introduces Structured Attention Graphs (SAGs), which provide a visualization that decomposes saliency maps into sub-regions and highlights common and distinct structures across maps. The paper presents a systematic evaluation of high-conÔ¨Ådence local attention maps and demonstrates the effectiveness of SAGs through a user study.