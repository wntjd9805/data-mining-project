Estimating Kullbackâ€“Leibler (KL) divergence from data samples is crucial in various machine learning problems. However, existing techniques for KL divergence estimation, such as those based on variational techniques or neural networks, often suffer from issues such as unreliability and instability. In this paper, we aim to understand the underlying problem in KL estimation using discriminator networks and propose a novel approach to address these challenges. Our method involves constructing a discriminator function using deep networks that lies in a smooth function space, specifically the Reproducing Kernel Hilbert Space (RKHS). By controlling the complexity of the RKHS space, we can achieve more reliable and stable KL divergence estimates. We provide theoretical guarantees for our proposed estimator and demonstrate its effectiveness through empirical experiments. Our method shows promising results in terms of improving KL divergence estimation and reducing variance, making it competitive with state-of-the-art approaches in mutual information estimation and Variational Bayesian applications.