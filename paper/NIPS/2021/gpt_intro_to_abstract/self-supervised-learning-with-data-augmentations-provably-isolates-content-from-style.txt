Learning good representations from unlabeled data is important for more capable and data-efficient learning systems. Self-supervised learning (SSL) has become the dominant approach for unsupervised representation learning. SSL leverages the known structure of the data to extract a supervisory signal from unlabelled observations, allowing for the application of supervised learning techniques. While SSL has been successful in natural language processing and speech recognition, it faces challenges in continuous or high-dimensional domains like vision. To address this, recent advances in representation learning have utilized data augmentation to generate similar views. However, the theoretical understanding of the success of data augmentation techniques is still lacking. In this paper, we aim to better understand the empirical success of SSL with data augmentation by studying the identifiability of the representation through a latent variable model.