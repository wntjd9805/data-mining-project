Adversarial attacks on deep neural networks (DNNs) have gained significant attention due to their ability to manipulate prediction outcomes by adding imperceptible perturbations to natural examples. Adversarial robustness, the ability of a trained model to maintain its predictive power under such attacks, is crucial for safety-critical applications. In this paper, we propose using the Information Bottleneck (IB) to enhance adversarial robustness. We show that using IB as a learning objective leads to improved robustness in DNNs. However, calculating mutual information is computationally expensive, so we explore the use of the Hilbert-Schmidt independence criterion (HSIC) as a tractable substitute. We present HSIC-Bottleneck-as-Regularizer (HBaR), which incorporates HSIC as a regularizer in addition to commonly used losses for DNNs. Our experimental results demonstrate that HBaR significantly outperforms previous methods and enhances the adversarial robustness even without adversarial training. We provide theoretical justifications and empirical evidence to support our claims. The rest of the paper is organized as follows: we discuss related work, the standard setting of adversarial robustness and HSIC, provide a theoretical justification for HBaR, present our experiments, and conclude.