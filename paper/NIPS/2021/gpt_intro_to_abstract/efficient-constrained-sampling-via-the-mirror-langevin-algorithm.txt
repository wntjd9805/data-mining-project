In this paper, we address the problem of constrained sampling by proposing a discretization of the mirror-Langevin diffusion, called the mirror-Langevin algorithm (MLA). We demonstrate that our algorithm has a vanishing bias as the step size tends to zero, unlike previous approaches. By analyzing our algorithm, we establish a stronger connection between sampling and optimization, without relying on technical assumptions. Moreover, our analysis combines ideas from optimization and the calculus of optimal transport. We provide convergence guarantees for different classes of potentials and show the practical applicability of MLA in Bayesian logistic regression. Our work contributes to the growing trend of applying optimization techniques to sampling problems.