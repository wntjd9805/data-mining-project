Real-world problems in areas such as economic design and policy making often involve multi-agent games with two levels of thinking. This paper focuses on the concept of Stackelberg equilibrium, which is a more appropriate solution for these games compared to symmetric solution concepts like Nash equilibrium. The authors study the learning of Stackelberg equilibria in general-sum games, particularly in situations where the rewards can only be learned from noisy samples. Existing theoretical studies provide guarantees for finding Stackelberg equilibria but do not cover the bandit feedback setting, where the game can only be learned from random samples. The authors propose algorithms for learning approximate Stackelberg equilibrium in bandit games, bandit-RL games, and linear bandit games. They provide theoretical upper and lower bounds for sample complexity in each case. Overall, this work addresses the gap in understanding the learning of Stackelberg equilibria in general-sum games and provides algorithms with sample complexity guarantees for various game scenarios.