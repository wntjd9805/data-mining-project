Recurrent neural networks (RNN) have shown great success in modeling sequential data such as natural language processing and speech recognition. These networks can be interpreted as ordinary differential equations (ODE), leading to the development of continuous-depth models for handling irregularly-sampled time-series data. On the other hand, kernel methods for deep learning offer insights into the functions learned by the networks. By leveraging the neural ODE paradigm for RNN, we demonstrate that RNN can be viewed as linear predictors over a specific space associated with the signature of the input sequence. This signature transform, along with an RKHS structure, allows us to frame classical recurrent architectures as a kernel method. This approach enables us to provide generalization bounds and stability guarantees for RNN. Experimental results illustrate the theory.