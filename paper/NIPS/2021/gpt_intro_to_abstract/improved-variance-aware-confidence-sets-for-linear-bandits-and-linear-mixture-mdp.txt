This paper presents research on sequential decision-making problems in computer science, specifically in the areas of bandits and reinforcement learning. The authors explore the use of linear function approximation for generalization in cases where the state-action space is large. They investigate the minimax-optimal regret bound for linear bandits and propose algorithms that adapt to the variance magnitude to achieve smaller regret bounds. Additionally, the paper addresses the exploitation of variance information in reinforcement learning and extends the study to RL with linear function approximation in the linear mixture Markov Decision Process setting. The authors introduce new, variance-aware conÔ¨Ådence sets for linear bandits and linear mixture MDP, and provide details on their techniques, including elimination with peeling and recursion-based variance estimation. The results demonstrate improved regret bounds that depend on the variance and feature dimension, while also highlighting the potential for simultaneously overcoming challenges in RL related to large state-action spaces and long planning horizons.