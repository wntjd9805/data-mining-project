This paper addresses the problem of stochastic optimization in the context of non-smooth non-convex functions commonly found in machine learning applications. The authors focus on the stochastic model-based method (SMOD) and investigate its performance compared to modern stochastic gradient descent (SGD). They also explore the potential benefits of minibatching and the applicability of momentum techniques in the SMOD algorithm family. The contributions of the paper include extending SMOD to the minibatch setting, developing new extrapolated model-based methods with momentum, and providing new convergence results for convex optimization. The analysis and results offer insights into improving the practical efficiency of SMOD and provide complexity guarantees for various algorithms in different optimization settings.