Deep learning models have achieved impressive performance on tasks within the same distribution they were trained on. However, this performance often does not stem from human-like reasoning but rather from relying on spurious correlations. Invariant Risk Minimization (IRM) is a recent method that aims to learn causal features that are invariant across different distributions, leading to better out-of-distribution generalization. While previous studies have mainly focused on synthetic settings and simpler models, this paper investigates IRM in natural settings with complex models, using natural language inference (NLI) as a test case. The experiments show that IRM outperforms empirical risk minimization (ERM) on out-of-distribution test sets in both synthetic and natural bias scenarios, but in more naturalistic settings, IRM is not able to completely discard the bias while ERM does not rely solely on the bias. The paper also highlights three criteria that govern the success of IRM: bias prevalence, bias strength, and training data size. Overall, these findings emphasize the need for a more naturalistic approach to understanding the settings in which IRM can perform well.