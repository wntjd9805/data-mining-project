Federated learning (FL) is a popular distributed learning framework that aims to protect users' data privacy by aggregating model updates instead of sharing user data with a central server. However, recent research has demonstrated the potential for gradient inversion attacks, where an attacker can recover private user data by observing the gradients. This poses a significant threat to the FL community as it undermines the main premise of privacy protection. Moreover, studies suggest that such attacks can be enhanced when additional side-information, such as knowledge of the data distribution or batch norm statistics, is available to the attacker. In this paper, we investigate how to maximize the utilization of prior information in gradient inversion attacks. We propose an efficient algorithm called GIAS that leverages a generative model as a prior and achieves improved data recovery. Additionally, we address the scenario where the user data distribution is unknown and present a meta-learning framework called GIML that learns a generative model solely from gradients. Our experimental results demonstrate the effectiveness of GIAS and GIML in various FL scenarios, highlighting the significant privacy leakage and the need for more robust defense mechanisms in FL.