Transformers have had significant success in natural language processing tasks, leading to breakthroughs in language understanding. Recent research has explored the application of transformers in vision tasks, challenging the traditional bias of convolutional neural networks. However, this transformational approach introduces an aliasing issue due to the subsampling operations performed on image patches. Increasing the sampling rate or integrating anti-aliasing filters into the attending process can mitigate this problem. This paper proposes the Aliasing Reduction Module (ARM), which consists of an anti-aliasing filter and an external modulation module. Extensive investigations are conducted to explore filtering choices and module placement, leading to improvements in feature localization and prediction accuracy. The contributions of this research include the exploration of anti-aliasing techniques for vision transformers, the development of ARM, and the observation of improved generalization and data efficiency in state-of-the-art vision transformers.