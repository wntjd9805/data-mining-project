Video recognition is a crucial task in computer vision with various applications such as intelligent surveillance and multimedia retrieval. Deep learning, particularly convolutional neural networks (CNNs), has significantly improved the performance of video recognition. However, most existing architectures rely on global average pooling (GAP) to generate video representations, which overlooks rich statistical information in spatio-temporal features and fails to capture complex dynamics. This paper introduces a model-agnostic approach called Temporal-attentive Covariance Pooling (TCP) to address these limitations. TCP integrates a temporal attention module, temporal covariance pooling, and fast matrix power normalization to capture complex temporal dynamics and exploit the geometry of covariance spaces. Experimental results on multiple video benchmarks demonstrate the effectiveness of TCPNet in improving recognition performance over state-of-the-art methods.