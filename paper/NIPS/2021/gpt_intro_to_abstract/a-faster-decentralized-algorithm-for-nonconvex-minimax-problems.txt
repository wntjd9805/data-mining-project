This paper introduces the concept of minimax optimization and its applications in machine learning tasks. It discusses the existing algorithms for solving minimax optimization problems and their limitations. The paper then focuses on the decentralized training paradigm and the need for decentralized minimax optimization algorithms. The authors propose a new accelerated decentralized algorithm called DM-HSGD to solve decentralized nonconvex-strongly-concave minimax optimization problems. The algorithm is designed to work with non-identical distributed data and provides theoretical guarantees. The convergence and complexity analysis of the algorithm are also presented. Experimental results are discussed to validate the effectiveness of the proposed algorithm. The paper concludes by summarizing the contributions and providing an outline of the rest of the paper.