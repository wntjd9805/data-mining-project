Improving sample efficiency is an important step towards effective real-world Reinforcement Learning (RL). One approach to achieving this is through offline RL, where a new policy is learned from data collected by a behavior policy without interacting with the environment. However, current dominant paradigms in deep offline RL literature heavily rely on actor-critic style algorithms that use off-policy evaluation to learn the critic. In this paper, we challenge this paradigm by proposing a simple baseline algorithm that performs only one step of policy improvement using the behavior Q function. Surprisingly, we find that this one-step algorithm outperforms more complicated iterative algorithms on a wide range of benchmark tasks. We further investigate the reasons for the success of the one-step algorithm and identify issues related to off-policy evaluation in the iterative algorithms. We demonstrate that avoiding off-policy evaluation can help mitigate these issues. Additionally, we provide guidance on when iterative algorithms may be more appropriate based on the dataset size and behavior policy coverage. Our contributions include demonstrating the superior performance of the one-step baseline, analyzing failure modes of off-policy evaluation, and identifying the scenarios where one-step algorithms are likely to outperform iterative approaches.