One major mystery in deep learning is why deep neural networks generalize despite overparameterization. Recent works have studied the implicit bias of gradient descent (GD) and its connection to margin maximization. GD has been shown to converge to the max-margin solution for linear logistic regression on linearly separable data, even without explicit regularization. This paper extends the theoretical understanding of GD to SGD, other gradient-based methods, and different loss functions. The authors specifically focus on the implicit bias of GD on two-layer neural networks with Leaky ReLU activation and logistic loss. They analyze the convergence behavior of GD and show that it converges to a global-max-margin linear classifier, under the assumption of symmetric linearly separable data. Additionally, the authors provide insights on the simplicity bias of GD, indicating that it learns linear functions in the early phase of training and increases in complexity over time. However, they also demonstrate that global-max-margin classifiers can be fragile, as GD can converge to a suboptimal linear classifier with suboptimal margin by adding a few extra data points.