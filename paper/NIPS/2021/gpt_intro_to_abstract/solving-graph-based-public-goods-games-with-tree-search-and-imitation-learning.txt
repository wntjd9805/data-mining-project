This paper introduces a method for finding desirable equilibria in graph-based best-shot public goods games. These games involve individuals making investment decisions in a good that benefits either themselves or their neighbors. The authors propose an approach that exploits the correspondence between equilibria and Maximal Independent Sets (MIS) in graphs. They define a Markov Decision Process (MDP) that incrementally grows an MIS and use Monte Carlo Tree Search (MCTS) to find equilibrium configurations. Additionally, they propose a Graph Imitation Learning method that uses demonstrations of the MCTS policy to learn a policy parametrized by a graph neural network. The results show that their method outperforms existing approaches, especially in cases where costs for acquiring the public good differ among players. The learned policy achieves close performance to the search method but is significantly faster to evaluate. The proposed method has applications beyond public goods games in graph-based decision-making problems.