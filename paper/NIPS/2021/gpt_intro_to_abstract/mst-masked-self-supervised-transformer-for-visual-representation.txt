This paper introduces a new masked self-supervised transformer approach called MST for computer vision tasks. The approach utilizes the self-attention map to guide the masking of local patches, improving the understanding of local context semantics without damaging the crucial structure. Additionally, a global image decoder is used to recover the spatial information of the image, enhancing the model's ability for downstream dense prediction tasks. The effectiveness and transferability of the proposed method are validated through extensive experiments, outperforming previous state-of-the-art methods on ImageNet, MS COCO, and Cityscapes datasets.