Reinforcement learning (RL) is a framework for solving sequential decision-making problems, but many RL algorithms lack safety considerations, making them unreliable in real-world applications. Transfer learning incorporates prior knowledge or skills to improve RL algorithms, but using the expected return as a measure of optimality may lead to excessive risk-taking. Therefore, risk-awareness is crucial in designing practical RL systems. This paper introduces a risk-aware successor feature framework (RaSF) that aims to maximize the entropic utility of return in Markov Decision Processes (MDPs). RaSF enables task generalization and allows for different levels of risk aversion. Empirical evaluations demonstrate that RaSF achieves a better trade-off between return and risk and avoids catastrophic outcomes, while providing excellent generalization on novel tasks.