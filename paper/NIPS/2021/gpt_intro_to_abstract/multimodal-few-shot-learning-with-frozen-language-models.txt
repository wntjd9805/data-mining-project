Auto-regressive transformers have demonstrated impressive abilities in natural language processing, including few-shot learning, rapid adaptation to new tasks and styles, and retrieval of relevant knowledge. However, these models are limited to text-based modalities and lack understanding of visual information. In this paper, we propose Frozen, a method for providing a pre-trained language model with access to visual information without altering its weights. Frozen consists of a neural network that encodes images into the word embedding space of the language model to generate captions. We train the image encoder by back-propagating gradients through the frozen language model. Frozen exhibits zero-shot performance on multimodal tasks and improves with a small number of in-context examples. It enables open-ended and unconstrained interpretation of images while retaining the capabilities of large language models. Our contributions include the presentation of Frozen as a modular and scalable approach, the demonstration of transferability of task adaptation and knowledge transfer to the multimodal setting, and quantification of these capabilities on various benchmarks.