Contextual bandit is a popular approach in sequential decision tasks, such as news article recommendation systems, where the learner must make decisions to maximize cumulative rewards without knowing the compensation mechanisms. Estimation or learning in bandit problems is important, as reward information is only observed for the chosen arm, leading to missing data. This paper proposes a novel algorithm, Doubly Robust Thompson Sampling (DRTS), that applies the Doubly Robust (DR) method to handle missing data in the bandit setting. DRTS utilizes contextual information for all arms, not just the chosen arm, and achieves an improved regret bound compared to existing algorithms. The paper also presents novel estimation error bounds, concentration inequalities, and eliminates forced sampling phases.