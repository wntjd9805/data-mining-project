In this paper, we explore the effectiveness of knowledge distillation in deep neural networks. While previous work has suggested that distilling knowledge from a larger teacher network to a smaller student network can improve generalization, we find that this is not always the case. We distinguish between fidelity, the ability of the student to match the teacher's predictions, and generalization, the student's performance on unseen data. We show that achieving high fidelity in knowledge distillation can be challenging, and propose two hypotheses to explain this: an identifiability problem and an optimization problem. Our conclusions highlight the limitations and benefits of knowledge distillation.