This paper challenges the dominant paradigm of parametric modeling in deep learning and introduces Non-Parametric Transformers (NPTs) as a new approach. NPTs allow models to use the entire dataset as input and explicitly learn interactions between datapoints. By leveraging both parametric and non-parametric predictive mechanisms, NPTs can learn to reason about general relationships between inputs. The paper presents the use of multi-head self-attention to model relationships and proposes a training objective for NPTs. The experimental results demonstrate that NPTs can capture the causal mechanism generating the data while still utilizing the power of parametric deep learning.