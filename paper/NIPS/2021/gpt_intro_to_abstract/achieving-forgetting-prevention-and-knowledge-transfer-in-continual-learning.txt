This paper explores the problem of continual learning (CL) in natural language processing (NLP) tasks, specifically focusing on preventing catastrophic forgetting (CF) and facilitating knowledge transfer across tasks. The authors propose a novel neural architecture called CTR (Capsules and Transfer Routing) that addresses the limitations of fine-tuning pre-trained language models like BERT for CL. CTR utilizes CL-plugin modules inserted into BERT to achieve CF prevention and knowledge transfer, leveraging capsules to represent each task and a transfer routing algorithm for improved accuracy. Experimental results demonstrate the effectiveness of CTR compared to existing methods.