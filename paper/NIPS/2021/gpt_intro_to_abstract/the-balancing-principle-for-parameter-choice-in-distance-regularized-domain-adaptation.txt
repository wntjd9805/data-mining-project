This paper introduces the problem of unsupervised domain adaptation in machine learning, where the goal is to improve the performance of an algorithm on a target domain using knowledge from a related source domain. The focus is on feature representation learning approaches, which map the data into a new feature space where the source and target data representations appear similar. A key challenge in these approaches is choosing the parameter that penalizes the distance between the representations of the two domains. Existing methods for parameter choice are either heuristic or limited by assumptions, and can fail if the density ratio between the domains is unbounded. In this work, the authors propose a principled method called the Balancing Principle for Domain Adaptation (BPDA) for choosing the parameter. The BPDA minimizes a target error bound that quantifies the distance between the domains and different learning errors. The authors demonstrate the effectiveness of the BPDA compared to state-of-the-art methods on various domain adaptation methods and datasets.