The paper introduces a novel approach for Bayesian optimization (BO) of objective functions defined by a series of time-consuming-to-evaluate functions arranged in a directed acyclic network. The approach leverages the structure of the function network to improve optimization efficiency by modeling individual nodes using distinct Gaussian processes (GPs) and recursively incorporating observations into a non-Gaussian posterior on the overall output. The points to evaluate are chosen using the expected improvement computed with respect to this posterior. The method outperforms standard BO by leveraging internal node information and can reduce the effective dimensionality of the input space. The paper demonstrates the asymptotic consistency of the method and its ability to discover the global optimum without densely measuring the feasible domain. Numerical experiments show significant acceleration in optimization and outperformance of competing methods.