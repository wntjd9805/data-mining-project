Deep neural networks (DNNs) have achieved significant success in various applications, but they are vulnerable to adversarial examples created by adding imperceptible perturbations to clean images. There are two main categories of attacks: iterative algorithms, which are computationally expensive, and generative methods, which produce perturbations faster by training a deep network. In this paper, we focus on the transferability of perturbations obtained with generative methods and investigate their ability to generalize across different target architectures, data, and tasks. Our approach leverages the similarities in mid-level features extracted by DNNs with different architectures, data, or tasks, and trains a perturbation generator by maximizing the distance between normal and perturbed features. Our experiments demonstrate higher success rates compared to existing universal and transferable attacks.