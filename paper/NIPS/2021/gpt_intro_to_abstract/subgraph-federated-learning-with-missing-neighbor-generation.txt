Graph mining leverages links among connected nodes in graphs to conduct inference. Graph neural networks (GNNs) have gained attention for their impressive performance and generalizability in graph mining tasks. However, training a well-performed GNN model requires data that not only suffices but also follows a similar distribution as general queries, which is often limited and biased in reality. Federated learning (FL) has been successful in training machine learning models with distributed data without sharing actual data, enhancing performance and generalizability. In this paper, we propose FedSage, a framework for subgraph federated learning using GraphSage, and address the challenges of learning from multiple local subgraphs and dealing with missing links across subgraphs. We also present FedSage+, an extension of FedSage that generates missing neighbors to improve performance. Experimental results on real-world datasets demonstrate the superiority of our models compared to locally trained classifiers, and further analysis confirms the convergence and generalization ability of our frameworks.