Data distribution in the real world can differ greatly from the training dataset, leading to performance drops on test data for deployed deep learning models. Test data annotation can help mitigate this, but it can be costly. In this paper, we propose a framework for accuracy estimation and error detection that utilizes self-training on ensembles. We provide provable guarantees for the framework, showing that it accurately estimates accuracy and identifies mis-classified points. Our experiments show that the method achieves state-of-the-art results on various datasets. This framework can be instantiated using different ensemble methods, without assuming anything about the test distribution.