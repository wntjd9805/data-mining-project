Recent advances in offline reinforcement learning (RL) have enabled the training of policies for real-world scenarios using previously collected data. However, when dealing with multi-task problems, where multiple tasks need to be solved using available data, there is a challenge of effectively utilizing the heterogeneous dataset. This paper aims to understand the impact of data sharing on RL performance in the offline setting and proposes a reliable method for selectively sharing data across tasks. The authors hypothesize that data sharing can be detrimental due to the distribution shift between the policy represented in the data and the policy being learned. To address this, they propose the conservative data sharing (CDS) algorithm, which reduces distributional shift by selectively sharing data based on a learned conservative estimate of Q-values. The effectiveness of CDS is evaluated through extensive empirical analysis on various multi-task RL benchmarks, showcasing its superior performance compared to other methods.