Representation learning is a crucial task in machine learning, especially for discrete real-world data such as natural language sentences and graphs. While standard embedding techniques like Word2Vec and GloVe work well for Euclidean spaces, they may not be optimal for hierarchical or graph-like data. Hyperbolic space, with its constant negative curvature, has shown promising results for embedding hierarchical structures. However, the use of ordinary floating-point numbers in hyperbolic space can lead to significant numerical errors. Previous approaches, such as scaling factors and high precision floating-point numbers, have their limitations. In this paper, we propose a solution that represents hyperbolic space using multiple-component floating-point numbers (MCF), offering both accuracy guarantees and GPU acceleration. We provide algorithms for computing with MCF in hyperbolic space and demonstrate the effectiveness of our approach in embedding tasks.