Recent advancements in deep learning have led to increasingly complex models with a large number of parameters and data requirements, resulting in high training costs. To address this issue, large-scale machine learning techniques that parallelize model training across multiple computing resources have been employed. However, the overhead of model synchronization can hinder the training process. One commonly used strategy to reduce synchronization frequency is large-batch training, where the batch size is increased. While large batch sizes can accelerate training speed, they may also degrade model generalization performance. Existing works have proposed learning rate scheduling methods to mitigate this performance loss, but manual tuning is time-consuming and limits widespread adoption. To overcome these challenges, we propose SimiGrad, a fine-grained batch size adaptation approach that optimizes batch size at the iteration level using a novel gradient similarity measurement. We integrate SimiGrad into mainstream machine learning frameworks and demonstrate through extensive experiments that it outperforms state-of-the-art methods in terms of both average batch size and model generalization performance. Specifically, we achieve a record-breaking batch size of 77k in BERT-Large pretraining with improved performance compared to existing methods. Our main contributions include the development of an accurate gradient similarity measurement, a fully automated adaptive batching algorithm, and the integration of SimiGrad into mainstream frameworks, which is available as an open-source plugin tool.