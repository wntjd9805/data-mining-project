This paper explores the theoretical understanding of the relationship between pretrained language models and downstream natural language processing tasks. The authors propose a generative model framework to connect the pretraining and downstream settings, and analyze the effectiveness of head tuning and prompt tuning as adaptation strategies. The theoretical analysis is conducted on two data-generating distributions, a Hidden Markov Model (HMM) and an HMM variant with additional memory variables. The authors prove that under certain conditions, linear classification heads and attention-based classification heads can recover the downstream labels from pretrained model outputs. Empirical evaluations using synthetic data from HMMs support the theoretical findings, showing the improved performance of prompt tuning when non-degeneracy conditions are relaxed, and the enhanced performance of head tuning with memory-augmented HMMs.