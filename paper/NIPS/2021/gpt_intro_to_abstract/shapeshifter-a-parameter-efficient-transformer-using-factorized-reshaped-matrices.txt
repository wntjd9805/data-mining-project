This paper introduces a new approach to reducing the number of parameters in natural language models. The authors propose a matrix decomposition technique that reshapes and reorders matrix dimensions prior to the decomposition, resulting in a compact model with smaller matrices. Unlike traditional low-rank factorization, this method allows for increased expressiveness and can reduce the size of a simple encoder-decoder Transformer to as little as 4 million parameters. The technique can be applied to any matrix in the model without modifying the architecture.