Imitation learning (IL) has been shown to be successful in complex tasks with sparse rewards and high-dimensional observations. However, IL often requires privileged information that is unavailable to the student during inference. This paper introduces a solution called Adaptive Insubordination (ADVISOR), which bridges the imitation gap by adaptively weighting imitation and reinforcement learning losses. ADVISOR is evaluated on thirteen tasks, demonstrating its performance, robustness, and sample efficiency across various learning methods and environments.