Machine learning (ML) models are increasingly being used in sensitive applications, such as loan approval and employment screening. However, these models can encode discriminatory behavior and negatively impact protected groups. To address this, researchers have introduced various fairness measures, including minimax fairness. This paper focuses on achieving minimax fairness in the data-collection stage of the ML pipeline. The goal is to construct a training set with appropriate proportions of elements from different protected groups to ensure a classifier trained on this dataset achieves minimax fairness. The paper proposes an adaptive sampling scheme based on optimism to construct the training set and provides theoretical guarantees and empirical results to demonstrate the effectiveness of the approach.