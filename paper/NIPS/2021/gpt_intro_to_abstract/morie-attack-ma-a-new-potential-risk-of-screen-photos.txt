Deep Neural Networks (DNNs) have shown remarkable capabilities in various vision tasks, but they are also vulnerable to security risks. Adversarial attacks, in which small perturbations to input images can fool DNN models, have been extensively studied. In this paper, we focus on a non-typical distortion called moiré pattern, which occurs when the frequency of details in an image exceeds the sensor's resolution. We investigate whether moiré pattern can be a potential physical-world attack on DNNs and propose a Moiré Attack (MA) that mimics the shooting process of camera sensors. Our experiments demonstrate the effectiveness and controllability of MA, as well as its high attack success rate and robustness against image transformations.