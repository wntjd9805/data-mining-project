This paper introduces a framework called latent set prediction (LSP) for deep set prediction tasks in computer science. Set prediction is the task of predicting multiple elements in a set without a specific ordering. Traditional deep learning models are not well-suited for set prediction tasks due to their reliance on specific ordering and loss functions. The LSP framework utilizes a Transformer model coupled with a permutation-invariant loss function, known as Permutation Invariant Training (PIT), to enable more flexible set prediction. The paper also addresses the challenge of applying set prediction to sequence domains that require teacher forcing, proposing a solution using a latent space and a Euclidean distance metric. The contributions of this work include a framework that eliminates the need for hand-crafted distance metrics, efficient prediction for sequence predictions with teacher forcing, and a convergence proof for set prediction under the LSP framework.