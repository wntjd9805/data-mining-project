Recently, deep learning-based text-to-speech (TTS) models have gained popularity. Some of these models generate mel-spectrograms autoregressively but suffer from slow inference speed and robustness issues. Others use a non-autoregressive architecture to generate mel-spectrograms in parallel, resulting in faster inference and improved robustness. In this paper, we propose PortaSpeech, a portable and high-quality generative TTS model that addresses the goals of fast inference, lightweight deployment, high-quality synthesis, expressiveness, and diversity. We achieve these goals by adopting a VAE with an enhanced prior and a flow-based post-net architecture, using a lightweight VAE to capture prosody while minimizing model size, and introducing a linguistic encoder with mixture alignment to enhance prosody modeling. Experimental results on the LJSpeech dataset demonstrate that PortaSpeech outperforms other state-of-the-art TTS models in terms of voice quality and prosody while achieving significant model size and memory reduction. Our contributions include the analysis of VAE and normalizing flow characteristics in TTS, the introduction of mixture alignment in the linguistic encoder, and the development of a lightweight and high-quality TTS model.