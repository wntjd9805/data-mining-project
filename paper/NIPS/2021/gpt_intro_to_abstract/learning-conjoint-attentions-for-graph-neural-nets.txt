In this paper, we introduce Conjoint Attentions (CAs), a class of generic graph attention mechanisms, for Graph Neural Networks (GNNs). Unlike existing graph attentions that solely rely on node features, CAs incorporate heterogeneous factors, including internal and external factors, to compute attention coefficients. We analyze the expressive power and discriminant capacity of CA layers and demonstrate the potential of Graph conjoint attention networks (CATs) for various learning tasks. Through extensive benchmarking and comparison studies, we validate the effectiveness of the proposed attention mechanisms.