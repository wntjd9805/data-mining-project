The introduction of the paper discusses the transformer architecture and its successful application in natural language processing tasks. It also highlights the potential of vision transformer models in capturing long-term dependencies and learning diverse interactions between spatial locations. However, the computational complexity of self-attention and pure MLP models poses challenges for vision tasks. To address this, the paper introduces a new architecture called Global Filter Network (GFNet) that leverages the frequency domain to model interactions among spatial locations. The proposed architecture replaces the self-attention sub-layer with a series of operations, including discrete Fourier transform, element-wise multiplication, and inverse Fourier transform. This approach offers log-linear complexity and compatibility with larger feature maps and hierarchical architectures. Experimental results demonstrate the effectiveness of GFNet in outperforming existing vision transformer and MLP models in terms of efficiency, generalization ability, and robustness.