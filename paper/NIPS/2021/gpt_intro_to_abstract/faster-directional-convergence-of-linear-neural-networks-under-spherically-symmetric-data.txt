This paper focuses on analyzing the convergence behavior of gradient flow for deep linear networks with binary cross-entropy loss. Previous works have mainly analyzed linear networks with quadratic loss or exponential-type loss. The authors aim to understand the entire training dynamics and provide a global analysis of gradient flow for deep linear nets with logit loss. They prove the global convergence of gradient flow under certain assumptions and establish explicit convergence rates. Additionally, they investigate the weight norm change pattern during training and validate their findings through numerical experiments.