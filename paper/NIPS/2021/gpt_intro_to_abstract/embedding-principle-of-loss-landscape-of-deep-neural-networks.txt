Understanding the loss landscape of deep neural networks (DNNs) is crucial for developing a theory of deep learning. However, quantifying the exact structure of the loss landscape is challenging due to its complexity, high dimensionality, and dependence on various factors. In this paper, we propose a general embedding operation that allows us to map network parameters from narrower to wider DNNs. Using this embedding principle, we demonstrate that the loss landscape of any network contains all critical points of narrower networks. We show that this embedding principle is independent of the training data and choice of loss function, and it is intrinsic to the layer-wise architecture of DNNs. Furthermore, we provide evidence that the training of wider DNNs can experience critical points from narrower DNNs, suggesting a potential mechanism for the good generalization performance of heavily overparameterized DNNs. Our findings shed light on the underlying properties of the loss landscape and contribute to a better understanding of deep learning theory.