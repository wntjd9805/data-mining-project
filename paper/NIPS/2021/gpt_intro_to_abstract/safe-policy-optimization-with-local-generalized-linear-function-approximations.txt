Applying reinforcement learning (RL) to applications with unknown safety constraints is challenging, especially in the exploration phase where conventional algorithms cannot guarantee safety. Safe exploration methods utilizing Gaussian processes (GP) have been studied, but they often make assumptions that do not hold in many environments, such as Lipschitz continuity and regularity. Additionally, previous GP-based safe exploration suffers from inconsistency between theoretical guarantees and computational cost. In this paper, we propose a feature-based safe exploration approach that takes into account the ability of robots to obtain feature vectors for inferring safety. This formulation is more realistic for real-world robots and allows us to handle safety that changes steeply. Our goal is to develop a theoretically guaranteed and empirically efficient safe RL algorithm that can be applied to real, large-scale problems.