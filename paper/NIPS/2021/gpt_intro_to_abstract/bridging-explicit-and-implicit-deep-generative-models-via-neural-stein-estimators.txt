In this paper, we propose a joint learning framework that combines explicit and implicit generative models in order to leverage the strengths of both. Explicit models define a density function but suffer from computational intractability, while implicit models can generate samples directly but struggle with instability and mode collapse. Our framework utilizes an explicit model to estimate the unnormalized density and an implicit generator model to minimize the statistical distance between true and generated samples. We introduce a Stein discrepancy to encourage consensus between the two models. Our contributions include theoretical proofs of mutual regularization, stabilization of training dynamics, and improved performance in detecting modes and generating high-quality samples. Experimental results support our findings.