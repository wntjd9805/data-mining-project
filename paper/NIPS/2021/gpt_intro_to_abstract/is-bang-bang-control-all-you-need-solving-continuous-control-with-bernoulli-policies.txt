Real-world robotics tasks often require control policies over continuous action spaces. These policies are typically represented as continuous probability distributions, such as Gaussians, to allow for more refined decision-making. However, practitioners have observed that extremal switching, or bang-bang control, can naturally emerge even under Gaussian policies. While recent work has focused on preventing or improving bang-bang control, the extent and reasons for its emergence in reinforcement learning (RL) are still largely unknown. In this paper, we provide theoretical intuition for bang-bang behavior by drawing connections to minimum-time problems. We also perform a series of experiments to optimize controllers using various RL methods and compare the original algorithms with modified versions that utilize bang-bang controllers. Our theoretical justifications and empirical results confirm the emergence of bang-bang policies in standard continuous control benchmarks, and demonstrate the competitive performance of explicitly enforced bang-bang policies. Furthermore, we explore the impact of action costs and penalties on both performance and exploration. Additionally, we propose a discrete action space version of Maximum A Posteriori Policy Optimization (MPO) to avoid gradient estimators and provide evidence for emergent bang-bang behavior. Overall, this work contributes by showcasing the competitive performance of bang-bang control, establishing theoretical connections to optimal control, and discussing the trade-offs associated with action penalties and exploration in RL.