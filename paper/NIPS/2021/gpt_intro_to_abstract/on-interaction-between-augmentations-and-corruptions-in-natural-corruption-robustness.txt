Distribution shift, particularly in image corruptions, poses a challenge for robust machine learning models in computer vision. Traditional models trained on clean images lack performance when faced with corruptions in test data. Data augmentation techniques have shown promise in improving corruption robustness by expanding the training set with low-level transformations. However, a key limitation is that these approaches assume unknown test corruptions, as known corruptions can be easily applied as augmentations. This paper aims to explore the relationship between data augmentation and test-time corruptions, examining the impact of perceptual similarity on corruption error. The authors introduce an empirical measure called Minimal Sample Distance (MSD), quantify the distance between augmentation schemes and corruption transformations, and demonstrate that augmentation-corruption perceptual similarity strongly predicts corruption error. Experimental results on CIFAR/ImageNet-C datasets reveal that common data augmentation methods generalize poorly beyond these benchmarks, emphasizing the need for caution when introducing additional augmentation for fair robustness evaluations. The study concludes that while generalization among perceptually similar transforms is possible, significant variations in generalization capability exist among different augmentation schemes. The implications of these findings provide insights for researchers to assess the corruption error of augmentation schemes and their potential for generalization to dissimilar corruptions.