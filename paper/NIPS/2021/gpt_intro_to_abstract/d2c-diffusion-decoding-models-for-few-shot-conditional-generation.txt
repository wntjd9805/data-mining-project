Generative models trained on large amounts of unlabeled data have achieved great success in various domains. However, downstream applications often require conditioning signals, which can be costly to acquire. This paper explores the use of contrastive self-supervised learning methods to improve few-shot conditional generation. The proposed Diffusion-Decoding models with Contrastive representations (D2C) is a VAE that uses contrastive self-supervised learning to obtain a latent space with transferrability and few-shot capabilities. D2C learns a diffusion model over the latent representations and combines a discriminative model and generative diffusion model for conditioning signal. Experimental results show that D2C outperforms state-of-the-art VAEs and diffusion models in unconditional and conditional generation tasks. D2C is also significantly faster and preferred by human evaluations for image manipulation tasks.