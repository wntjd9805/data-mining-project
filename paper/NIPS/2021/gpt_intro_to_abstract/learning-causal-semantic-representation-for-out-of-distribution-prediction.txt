Deep learning has revolutionized artificial intelligence, but its success relies heavily on the assumption that the training and test data follow the same distribution. However, in real-world scenarios, the test data often differ from the training data. This discrepancy poses challenges for machine learning models, causing errors and hindering their applicability in risk-sensitive situations. To address this issue, this paper proposes a Causal Semantic Generative model (CSG) that leverages causal relations to separate semantic and variation factors and develops out-of-distribution (OOD) prediction methods. The paper focuses on single-domain training scenarios, including OOD generalization and domain adaptation. The proposed methods are based on the causal invariance principle, which ensures transferability and reliability across domains. The paper presents novel reformulations of the objective function and provides theoretical guarantees for identifying the latent cause of prediction and achieving accurate predictions. The contributions of this work include theoretical guarantees for identifying the latent cause, effective methods for OOD prediction, and improved performance in image classification tasks.