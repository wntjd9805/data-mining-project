Learning from high-dimensional data is a challenging task in computer science, often requiring strong assumptions. One example of this is learning Lipschitz functions, which typically necessitates an exponential number of samples due to the curse of dimensionality. Many high-dimensional machine learning problems involve structured data, such as images or graphs, and exhibit invariance to certain transformations of the input data, like permutations or rotations. This paper investigates the concept of geometric stability, which refers to the smoothness of a target function under these transformations. The authors quantify the benefits of geometric stability in terms of sample complexity using target functions defined on the sphere in d dimensions. They examine the smoothing operator that replaces the prediction with an average over transformations, and explore the approximation rates and generalization properties of invariant kernel methods. The main contributions include comparing the spectral properties of usual and invariant kernels, analyzing the improvement factor in sample complexity with varying sample size and the structure of the transformation subset, and extending the analysis to geometrically stable functions. The proofs in this paper utilize comparisons between the dimensions of invariant and non-invariant spherical harmonics and demonstrate the decay of their ratio as the degree tends to infinity.