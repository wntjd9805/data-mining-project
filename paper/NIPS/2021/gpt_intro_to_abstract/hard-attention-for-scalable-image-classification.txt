In image classification, deep neural networks (DNNs) are typically optimized for a specific input resolution, which presents challenges when dealing with images of higher resolution. The increase in computational and memory requirements is a significant obstacle in scaling model architectures. To address this issue, we propose Traversal Network (TNet), a multi-scale hard-attention architecture that selectively visits informative image regions to leverage high-resolution information while avoiding the quadratic increase in complexity. TNet offers a trade-off between accuracy and complexity by adjusting the number of attended image regions, and its hard-attention mechanism provides interpretability without additional computational cost. Experimental results on ImageNet and fMoW datasets demonstrate the efficacy of TNet in achieving higher accuracy with faster processing compared to baselines operating on the same resolution.