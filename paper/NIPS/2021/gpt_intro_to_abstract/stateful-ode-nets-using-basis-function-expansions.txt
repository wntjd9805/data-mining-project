In recent years, the interpretation of neural networks as discretizations of differential equations has revealed a connection between deep learning and dynamical systems. One type of network known as ordinary differential equation networks (ODE-Nets) has shown promise in modeling time series, smooth density estimation, and preserving the topology of input spaces. However, ODE-Nets often struggle with predictive accuracy for tasks like image classification due to their shallow parameterizations and lack of support for layers with internal state. Additionally, traditional techniques for regularization and compression in neural networks do not readily apply to ODE-Nets. To address these limitations, this paper proposes a novel stateful ODE-Net model that combines numerical analysis and basis function transformations to improve predictive accuracy and compressibility. The paper introduces a stateful ODE-Block and stateful normalization layers, and demonstrates the effectiveness of these innovations through experiments on image classification tasks. The paper also presents a methodology for compressing ODE-Nets without retraining or revisiting data, leveraging parameter interpolation and projection techniques. Furthermore, the paper explores the advantages of higher-order integrators in compression, highlighting their implicit regularization capabilities.