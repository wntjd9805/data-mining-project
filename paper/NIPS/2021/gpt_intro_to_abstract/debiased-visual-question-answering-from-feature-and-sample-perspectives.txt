Vision-and-language tasks, such as visual question answering (VQA), require models to perform visual-textual reasoning to understand the physical world. However, many existing VQA models rely on superficial correlations and biases between questions and answers, rather than reasoning based on the image. This leads to poor generalization ability. In this paper, we propose a method called D-VQA to overcome biases in both language and vision modalities. Our method includes feature-based bias detection modules to filter and remove negative biases, and sample-based negative examples to improve the sensitivity of the model. Experimental results on VQA datasets demonstrate the effectiveness of our approach in alleviating biases and improving VQA performance.