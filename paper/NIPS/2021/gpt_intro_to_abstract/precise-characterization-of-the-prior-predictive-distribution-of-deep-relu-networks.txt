Standard neural networks initialized with Gaussian weights tend to Gaussian processes in the infinite width limit, known as neural network Gaussian process (NNGP). However, little is known about the finite width case. This paper aims to better understand the implications of using Gaussian priors with the compositional structure of the network architecture and study the prior predictive distribution. The properties of the prior predictive distribution are challenging to study due to the compositional structure of neural networks. The authors leverage the Mellin transform and Meijer-G function to gain theoretical insights into Bayesian neural network (BNN) priors. The contributions include characterizing the prior predictive density of finite-width ReLU networks, analyzing the moments and heavy-tailedness of the distribution, connecting the results to the infinite width setting, and introducing generalized He priors. The paper is organized into sections that introduce the relevant notation, prior works, mathematical tools, and then present the main derivations and their consequences.