Machine learning algorithms need to not only perform well on training data, but also generalize well to unseen data. However, deep learning models have been shown to be vulnerable to adversarial input, where slight alterations to an image can cause the model to predict the wrong label. Previous research has focused on designing adversarial attacks, detecting attacked samples, and modifying training processes, but the generalization performance of these methods is still poor. This paper extends algorithmic stability analysis to adversarial training and proposes the injection of noise into the training process as a solution. The effectiveness of this method is explored in various tasks and models, demonstrating improvement in generalization error.