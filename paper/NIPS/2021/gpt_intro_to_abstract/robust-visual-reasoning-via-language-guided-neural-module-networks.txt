This paper introduces a novel language-guided adaptive convolution layer for neural module networks (NMN) in tasks such as visual question answering and visual referring expression recognition. The proposed layer enables the modules to adaptively select informative visiolinguistic relationships and attend to relevant objects of interest from visual and textual inputs. Experiments on various benchmarks demonstrate the superiority of the approach, with significant improvements in accuracy compared to previous works. Additionally, a new benchmark is introduced to evaluate the model's generalization capabilities. The key contributions of this work include the proposed layer, state-of-the-art results, and robustness in handling adversarial perturbations and novel compositions of unseen concepts.