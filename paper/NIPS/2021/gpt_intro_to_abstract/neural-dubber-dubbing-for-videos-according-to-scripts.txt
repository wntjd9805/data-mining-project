This paper introduces the concept of automatic video dubbing (AVD), which involves synthesizing human speech that is synchronized with a given video according to the corresponding text. The authors propose a model called Neural Dubber that generates high-quality and lip-synced speech by utilizing a text-video aligner and an image-based speaker embedding module. The model is evaluated on single-speaker and multi-speaker datasets, demonstrating its ability to produce speech that is temporally synchronized with video and has a reasonable timbre. The results show that Neural Dubber performs on par with state-of-the-art text-to-speech models and outperforms FastSpeech 2 in terms of audio quality.