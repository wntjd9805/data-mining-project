This paper introduces a novel class of attacks on machine learning models that target the data batching procedure used during training. These attacks, named BRRR attacks, exploit Batch Reordering, Reshuffling, and Replacing to significantly affect the integrity and availability of the models. The attacks do not require underlying model access or knowledge of the dataset, instead focusing on disrupting the stochasticity of gradient descent. The paper also presents theoretical analysis, evaluation on computer vision and language benchmarks, and demonstrates the ability to poison models and introduce backdoors.