Reinforcement learning (RL) has achieved significant success in various domains, but training RL agents to achieve human-level or superhuman performance often requires a large number of samples. The sample efficiency of RL algorithms has been a central focus of research, and two main settings, online RL and offline RL, have been studied. While there are similarities in learning goals, the algorithms and theories in these two settings appear rather different and disconnected. This paper aims to bridge the gap between online and offline RL by proposing a new setting called policy finetuning. In this setting, the learner has access to an initial reference policy and is asked to learn a near-optimal policy through interactive exploration. The paper provides theoretical analysis and algorithms for policy finetuning in finite-horizon Markov Decision Processes, offering insights into sample complexity and the performance of different algorithm designs.