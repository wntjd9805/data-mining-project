Data compression plays a crucial role in reducing communication volume and overhead in distributed learning. Previous research has explored techniques such as quantization and error compensation to achieve efficient compression without significantly degrading convergence speed. However, applying error compensation to variance-reduced algorithms has proven suboptimal, leading to slower convergence speeds. In this paper, we propose ErrorCompensatedX, a general method for error compensation that outperforms previous approaches by fully compensating all error history. We also provide a theoretical analysis framework that decomposes the convergence rate into two terms, allowing for easy evaluation and optimization of compressed algorithms. This paper presents the first general result for error compensation algorithms in the context of distributed learning.