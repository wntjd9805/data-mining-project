This paper investigates the use of randomly initialized gradient descent for solving the asymmetric low-rank matrix factorization problem. The authors aim to provide a thorough analysis of the convergence behavior of gradient descent for this problem, with a focus on the gap between practical performance and theoretical guarantees. The paper presents new techniques to overcome technical challenges and establishes the first polynomial convergence rate for randomly initialized gradient descent. The main result is a theorem that proves the convergence of gradient descent with a certain learning rate and initialization. The authors also provide a corollary for gradient flow in the limit of small learning rates. This analysis sheds light on the behavior of gradient descent and its potential applications in other non-convex problems.