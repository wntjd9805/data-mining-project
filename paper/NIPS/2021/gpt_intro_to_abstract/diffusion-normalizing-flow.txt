Generative models are machine learning models used to estimate data distributions and generate new samples. There are two types of generative models: flow-based models, where the latent and data spaces are connected through an invertible map, and diffusion models, where noise is gradually added and removed from the data. In this paper, we propose a new generative modeling algorithm called Diffusion Normalizing Flow (DiffFlow) that combines elements of both flow-based and diffusion models. DiffFlow extends the normalizing flow method by making the sampling trajectories stochastic and trainable, resulting in improved performance in terms of sampling quality and likelihood. We also develop a stochastic adjoint algorithm to efficiently train the DiffFlow model and apply it to various generative modeling tasks with synthetic and real datasets, demonstrating its effectiveness compared to other methods.