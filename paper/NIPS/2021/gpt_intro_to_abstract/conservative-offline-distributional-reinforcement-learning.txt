In this paper, we introduce a novel approach called Conservative Offline Distributional Actor-Critic (CODAC) to address the limitations of existing offline reinforcement learning (RL) algorithms. Most existing offline RL algorithms focus on maximizing the expected cumulative reward, neglecting the quantification of risk and ensuring safe behavior. To overcome this challenge, we adapt distributional RL to the offline setting and propose CODAC, which uses a penalty to ensure that the quantiles of the learned return distribution lower bound those of the true return distribution. We provide theoretical guarantees and demonstrate the effectiveness of CODAC through experiments on risk-sensitive robot navigation tasks and the D4RL Mujoco suite. Our results show that CODAC outperforms baselines and achieves state-of-the-art results in both risk-sensitive and risk-neutral RL settings. Additionally, CODAC is able to compute quantile lower-bounds and gap-expanded quantiles even in high-dimensional continuous-control problems.