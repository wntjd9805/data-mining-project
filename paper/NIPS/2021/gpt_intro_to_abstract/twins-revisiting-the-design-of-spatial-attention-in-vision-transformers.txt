Recent research has focused on Vision Transformers as an alternative to convolutional neural networks (CNNs) for visual perception tasks. Vision Transformers offer greater flexibility in modeling long-range dependencies, can process multi-modal input data, and are expected to replace CNNs in next-generation visual perception systems. However, Transformers have a heavy computational complexity due to the spatial self-attention operation, which grows quadratically with the input image size. To address this issue, localized self-attention methods have been proposed but they lack connections between different windows, resulting in a limited receptive field. This paper revisits the design of spatial attention in vision transformers and proposes two architectures: Twins-PCPVT, which utilizes a globally sub-sampled attention, and Twins-SVT, which uses a spatially separable self-attention mechanism. Both architectures are efficient, easily implemented, and perform favorably against state-of-the-art vision transformers in various visual tasks.