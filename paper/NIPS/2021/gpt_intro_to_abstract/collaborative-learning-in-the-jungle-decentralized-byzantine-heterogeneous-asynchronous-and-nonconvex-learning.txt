Collaborative machine learning has become crucial in distributed systems due to the distributed nature of data, cost of data transfers, privacy concerns, and the need for effective communication between machines. However, in practical distributed settings, various challenges such as hardware failures, software bugs, communication issues, data corruption, and malicious attacks need to be addressed. In this paper, we focus on collaborative learning in a fully decentralized, Byzantine, heterogeneous, and asynchronous environment with non-convex loss functions. We formulate the problem and propose an equivalence between collaborative learning and the abstract problem of averaging agreement. We present optimal algorithms for averaging agreement and demonstrate their applicability in different settings. Furthermore, we implement and evaluate our algorithms in a distributed environment, showcasing the efficiency and effectiveness of our approach.