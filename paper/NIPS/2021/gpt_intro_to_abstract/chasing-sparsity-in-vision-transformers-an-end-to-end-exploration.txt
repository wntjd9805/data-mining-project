Recent advancements in deep learning have led to the development of large deep networks with billions of parameters. However, these models require substantial computational resources, memory usage, and training time. In this paper, we focus on slimming down the inference and training of vision transformers, which have demonstrated impressive performance in computer vision tasks. We explore the integration of sparsity in vision transformers, aiming to find sparse subnetworks that maintain the accuracy while reducing the model size and training memory overhead. We propose three approaches: Sparse Vision Transformer Exploration (SViTE), Structured Sparse Vision Transformer Exploration (S2ViTE), and Sparse Vision Transformer Co-Exploration (SViTE+). We conduct extensive experiments on ImageNet, demonstrating significant computation savings and improved accuracy with the use of sparsity.