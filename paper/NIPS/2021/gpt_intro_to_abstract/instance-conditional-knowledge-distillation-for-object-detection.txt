Deep learning has seen significant advancements in recent years, particularly in the field of Deep Neural Networks (DNNs). However, the computational and memory requirements of advanced DNNs hinder their deployment in resource-limited devices. To overcome this issue, techniques such as network pruning, quantization, mobile architecture design, and knowledge distillation (KD) have been proposed. KD, in particular, has gained popularity as it allows for the enhancement of a smaller target network without introducing additional burden or modifications during inference. While KD has shown promise in classification tasks, its effectiveness in object detection is limited. This limitation arises due to the complex nature of detection tasks, where the localization of objects is challenging, and multiple objects can be present in a single image. This paper presents a novel framework called Instance-Conditional Knowledge Distillation (ICD), which explicitly addresses the challenge of knowledge distillation in object detection. The ICD framework leverages a decoding network and conditional modeling to locate and transfer instance-specific knowledge. Experimental results on benchmark datasets demonstrate significant improvements in detection performance compared to existing methods, even surpassing the performance of larger backbone models.