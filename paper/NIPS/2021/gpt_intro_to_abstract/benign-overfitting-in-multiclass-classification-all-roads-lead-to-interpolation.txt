This paper examines the optimization and generalization properties of high-dimensional linear multiclass classification in the context of Gaussian covariates and Gaussian mixture models. The authors establish a condition under which the multiclass support vector machine (SVM) solution is equivalent to the One-vs-All SVM classifier, and this equivalence holds for both cross-entropy loss and squared loss. They also provide bounds on the error of the minimum-norm interpolating classifier and analyze conditions for benign overfitting. These findings contribute to a better understanding of loss function equivalence and generalization in multiclass classification, which is a significant advancement in this area of research.