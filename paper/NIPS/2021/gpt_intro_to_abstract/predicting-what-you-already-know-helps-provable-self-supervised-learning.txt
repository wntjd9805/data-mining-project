Self-supervised learning has become a popular approach in machine learning, as it eliminates the need for extensive data collection and annotations. However, the underlying principles of self-supervised learning are not well understood. In this paper, we investigate the conceptual connection between pretext and downstream tasks in self-supervised learning and propose a method that guarantees both expressivity and sample efficiency. We focus on a reconstruction-based self-supervised learning method and demonstrate its effectiveness under the assumption of approximate conditional independence. Our results show that this approach can learn useful representations with much fewer labeled samples compared to traditional methods. We provide theoretical analysis and experimental evidence to support our findings.