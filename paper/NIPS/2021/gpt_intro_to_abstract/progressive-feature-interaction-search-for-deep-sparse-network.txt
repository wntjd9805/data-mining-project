Deep sparse networks (DSN) are specialized deep networks designed to handle learning from sparse and categorical features, which are common in many industrial applications such as click-through rate prediction, advertisement recommendation, and fraud detection. The key to the success of DSN lies in the design of complex feature-interaction layers. However, current approaches suffer from high search costs and limited prediction performance. To address these challenges, this paper proposes a PROFIT (PROgressive Feature InTeraction Search) method that utilizes neural architecture search (NAS) to design lightweight and effective models. The proposed method addresses the efÔ¨Åciency-accuracy dilemma by designing a distilled search space and developing a progressive search algorithm that prioritizes lower-order cross-features. Experimental results on real-world benchmark datasets demonstrate the high accuracy, low computation cost, and small model size of the searched models.