The introduction of this computer science paper discusses the recent work by Gulrajani & Lopez-Paz on the DomainBed benchmark, which challenges conventional wisdom in domain generalization. The authors argue that understanding out-of-distribution generalization remains poorly understood and propose that it depends on the classifier's in-distribution generalization and the discrepancy between training and testing data distributions. They mention the theory of Ben-David et al. on unsupervised domain adaptation and propose a methodology to investigate whether models trained using empirical risk minimization exhibit good out-of-distribution generalization. The authors reveal that the theory has limited predictive power and explore other measures that can predict out-of-distribution performance. The paper's contributions include a large-scale empirical study on deep neural networks, an evaluation of candidate measures for explaining domain generalization, and the finding that certain measures outperform theoretical quantities.