The active learning paradigm focuses on a scenario where labeled data is expensive to acquire, but unlabeled samples are abundant. In this context, an agent sequentially fits its parameters using available labeled data and selects a batch of unlabeled samples to be labeled and incorporated into its training set. Active learning is particularly important for deep neural networks, which require large amounts of labeled data. However, existing active learning methods often do not work well with neural networks and do not scale to different settings. This paper introduces a probabilistic perspective of neural active learning and proposes a practical algorithm called BAIT that performs well in deep classification problems and can be extended to regression settings. BAIT also works effectively with both neural and convex models. The paper provides theoretical insights, empirical evidence, and computational tractability for the proposed algorithm.