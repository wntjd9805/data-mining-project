This paper introduces the concept of fairwashing attacks, which involve manipulating post-hoc explanation techniques to mislead users' trust in machine learning models. The authors empirically demonstrate that fairwashing attacks are likely to be highly manipulable. They evaluate the generalization and transferability of fairwashing attacks, showing that manipulated explanations can generalize to other groups and transfer to different models. The authors also argue that detecting fairwashing attacks based on fidelity alone is not a viable solution and propose the use of the Fairness In The Rashomon Set (FaiRS) framework as a more effective metric. This work raises concerns about the transparency and trustworthiness of machine learning models and calls for further research in developing robust and detectable explanation techniques.