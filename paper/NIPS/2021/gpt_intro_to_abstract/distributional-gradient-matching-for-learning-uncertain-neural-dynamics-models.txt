In this paper, we address the problem of uncertainty quantification in nonlinear dynamical systems. We propose a novel approach called Distributional Gradient Matching (DGM) that avoids costly and non-robust numerical integration. Instead, we jointly train a probabilistic smoother and a neural dynamics model to capture epistemic uncertainty. Our approach focuses on the posterior/predictive state distribution, providing a computationally efficient and statistically accurate mechanism for prediction. We experimentally demonstrate the effectiveness of DGM on learning challenging, chaotic dynamical systems and generalizing to new unseen initial conditions.