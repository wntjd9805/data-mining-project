Deep neural networks have made significant progress in achieving high predictive accuracy for supervised learning tasks. However, they still struggle to provide meaningful confidence values and estimates of predictive uncertainty. This is particularly important in real-world scenarios where data distributions can differ from the training data. Bayesian deep learning, which combines deep learning with Bayesian probability theory, aims to address this challenge by learning a distribution over model parameters or sampling an ensemble of likely models. In this paper, we focus on Markov Chain Monte Carlo (MCMC) methods for inference in Bayesian neural networks (BNNs). Despite recent advances in approximate inference procedures for BNNs, there are still unresolved questions regarding the "cold posterior effect" (CPE), which refers to the improved predictive performance of BNNs when the Bayes posterior is artificially sharpened. We contribute novel evidence that sheds light on existing explanations for the CPE and highlight the roles of dataset curation, data augmentation, and the prior. Our results show that the CPE can arise independently from these factors, suggesting that a simple fix for cold posteriors is unlikely.