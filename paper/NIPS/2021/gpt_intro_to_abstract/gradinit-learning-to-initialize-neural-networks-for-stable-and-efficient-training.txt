The initialization of network parameters is crucial in the training stability and performance of deep neural networks. Although clever initialization rules have been designed, models with many layers or multiple branches may still suffer from instability. This paper proposes a simple method, GradInit, for learning the initialization of any network architecture. Instead of deriving closed-form expressions, GradInit rescales each random weight tensor directly by a learned scalar coefficient. Empirical results show that GradInit accelerates convergence, reduces gradient variance, and improves training. It is architecture-agnostic and works with both Adam and SGD optimizers. Experimental results in both vision and language domains demonstrate the effectiveness of GradInit in various architectures and tasks. GradInit also provides insights into potential causes of instability and allows for improved initialization rule design.