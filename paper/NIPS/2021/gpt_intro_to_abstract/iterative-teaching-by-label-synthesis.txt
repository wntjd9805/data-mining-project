Machine teaching, a field of study in computer science, focuses on constructing a minimal dataset for a learner to effectively learn a target concept. This concept has various applications, such as crowd sourcing and model robustness. Machine teaching can be done in batch or sequential fashion. While prior work has mostly focused on batch machine teaching, which involves providing a batch set of training samples to the learner in one shot, sequential machine teaching bridges the gap between machine teaching and practical learning algorithms by considering iterative learners, like neural networks. One challenge in sequential teaching is providing teaching examples to the iterative learner effectively and efficiently, especially in the pool-based teaching scenario. Consequently, the selection of teaching examples becomes a computationally complex task. In this paper, we propose a teaching framework called Label Synthesis Teaching (LAST) that addresses the problem of sample selection by randomly sampling teaching examples from the pool and synthesizing their labels. We compare LAST with iterative machine teaching (IMT) and argue that LAST avoids the complexity of selecting teaching samples in a large pool by restricting teaching to the label space. We draw inspiration from knowledge distillation, label smoothing, and privileged information to justify our approach and its potential benefits. We then discuss LAST under the omniscient scenario, where the teacher has complete knowledge about the learner, and touch upon the black-box teaching scenario, where the teacher has limited knowledge about the learner. Lastly, we highlight how LAST provides a unified view of soft label methods and connects iterative machine teaching to classic learning algorithms.