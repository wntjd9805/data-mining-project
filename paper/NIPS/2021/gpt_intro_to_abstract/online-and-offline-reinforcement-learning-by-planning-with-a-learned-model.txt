Offline reinforcement learning is a promising approach for learning valuable policies from real-world datasets in various domains such as robotics, healthcare, and education. While online reinforcement learning algorithms have shown impressive results, applying them to offline data presents challenges due to off-policy issues. Additionally, model-based reinforcement learning has focused on efficiently learning from limited data, making it suitable for offline RL. However, there has been a lack of a unified algorithm that excels in both online and offline settings. In this paper, we introduce the Reanalyse algorithm, which offers a simple yet effective technique for policy and value improvement in any data budget, including fully offline scenarios. We explore its potential uses and demonstrate its effectiveness in data-efficient learning and offline RL, leading to the development of MuZero Unplugged. Experimental results on Atari and the RL Unplugged benchmark for Atari and DM Control validate the algorithm's effectiveness in both online and offline cases.