Video object segmentation (VOS) is the process of identifying and segmenting target instances in a video sequence. This paper focuses on the semi-supervised setting where the initial segmentation is given and the algorithm must infer the segmentation for the remaining frames. The goal is to improve upon current methods by proposing a minimalistic form of matching networks called Space-Time Correspondence Network (STCN). Unlike existing methods that build a memory bank and affinity for every object in the video, STCN builds a single affinity matrix using only RGB relations, making it more efficient and robust. The paper also explores the construction of affinities and suggests using the negative squared Euclidean distance as a similarity measure for better memory coverage. The proposed STCN outperforms existing methods in terms of simplicity, efficiency, and effectiveness, achieving state-of-the-art performance at over 20 frames per second.