Large volumes of data collected at edge devices, such as smartphones, are valuable resources for training high-performance models. Federated learning is a promising approach for large-scale learning that addresses privacy concerns by training a global model using data from multiple devices. However, current federated learning systems face challenges with slow devices called stragglers and various forms of adversarial attacks. Asynchronous federated learning schemes have been effective in handling stragglers, but they are not well-suited for integrating with aggregation methods to combat adversaries. Different types of adversarial attacks, such as model poisoning and data poisoning, degrade the performance of federated learning systems. Existing methods for handling adversarial attacks are substantially degraded when the portion of adversaries is large. This paper proposes Sageﬂow, a robust federated learning strategy that simultaneously handles both stragglers and adversaries. The approach involves grouping models based on staleness and performing periodic global aggregations. Intra-group defense strategies, including entropy-based filtering and loss-weighted averaging, are utilized to counter adversarial attacks. Experimental results show that Sageﬂow outperforms existing defense methods with only a small portion of public data.