The rapid advances in deep learning have led to the emergence of federated learning, where the training of deep networks is decentralized and pushed to edge clients. However, a challenge in federated learning is training with non-IID data, which results in unstable convergence and suboptimal model performance. In this paper, we explore solutions to address this challenge, including client drift mitigation, aggregation schemes, data sharing, and personalized federated learning. We identify the classifier as a key factor in performance degradation and propose a novel approach called Classifier Calibration with Virtual Representations (CCVR) to rectify the decision boundaries of the deep network after federated training. Our experimental results show that CCVR achieves significant accuracy improvements over existing federated learning algorithms, setting a new state-of-the-art.