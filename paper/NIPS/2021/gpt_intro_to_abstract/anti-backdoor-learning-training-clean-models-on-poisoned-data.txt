This paper introduces the concept of backdoor attacks in machine learning models, where backdoor triggers are implanted during training to create a strong but irrelevant correlation between a trigger pattern and a target class. The paper explores existing defense methods against backdoor attacks, categorizing them into detection and erasing methods. However, it highlights the challenge of training a clean model on poisoned data and proposes a novel Anti-Backdoor Learning (ABL) scheme that addresses this issue. The ABL method identifies two weaknesses of backdoor attacks - faster learning on backdoored data and target-class dependency - and uses a gradient ascent based anti-backdoor mechanism to unlearn the backdoor correlation. The paper concludes by demonstrating the effectiveness of ABL in robustly training clean models on poisoned data, making it a valuable addition to existing defense methods.