Variational autoencoders (VAEs) are powerful generative models used in various applications such as image generation, music synthesis, and speech generation. However, VAE priors often fail to match the aggregate posterior, resulting in regions in the latent space that are not decoded to data-like samples. To address this issue, this paper proposes an energy-based model (EBM) prior that reweights the base prior to bring it closer to the aggregate posterior. Unlike existing methods, this approach eliminates the need for computationally expensive MCMC sampling and instead uses noise contrastive estimation (NCE) to train the EBM prior. The paper introduces a new EBM prior called noise contrastive prior (NCP) and demonstrates its effectiveness in improving the generative quality of VAEs. Additionally, the paper discusses the training of hierarchical NCPs in models with multiple latent variable groups and shows scalability in this setting.