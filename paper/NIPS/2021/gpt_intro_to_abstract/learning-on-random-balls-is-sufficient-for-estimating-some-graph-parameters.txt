This paper explores the use of graph learning methods, specifically Graph Neural Networks (GNN), in various domains such as computer vision, complex network analysis, molecule modeling, and physics simulations. The authors discuss the representational limits of GNNs and their potential for approximating all functions on graphs. They also address the scalability issue of handling large graphs and propose a random ball sampling GNN (RBS-GNN) model. The authors introduce the concept of the Benjamini-Schramm topology in the space of all graphs and prove the estimability of functions and their continuity in this topology. They provide learning-theoretic results, demonstrating that the functions representable by RBS-GNNs are generalizable and identifying size-generalizable functions. Unlike previous studies, the authors do not assume anything about the graph class but focus on the continuity of the graph functions. Their results are model-agnostic and provide a systematic view of general graph parameters learning.