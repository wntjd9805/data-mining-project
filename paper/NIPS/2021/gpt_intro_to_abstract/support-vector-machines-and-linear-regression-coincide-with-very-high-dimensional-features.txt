The support vector machine (SVM) and ordinary least squares (OLS) are well-established methods for fitting linear models in classification and regression tasks. This paper examines the scenario where these models yield the same hypothesis for high-dimensional data. The phenomenon of support vector proliferation (SVP), where every training example becomes a support vector, is studied. The connection between SVP, linear classification, and linear regression is explored, highlighting the breakdown of classical analyses in high-dimensional settings. The paper poses several questions regarding the generality, occurrence threshold, and generalization of SVP, and provides theoretical and empirical results to address these questions. The findings contribute to understanding the conditions under which SVM and OLS coincide.