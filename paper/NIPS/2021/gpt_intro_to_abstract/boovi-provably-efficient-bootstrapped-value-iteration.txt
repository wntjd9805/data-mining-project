Reinforcement learning (RL) with function approximation has shown success in various applications, but efficient exploration of complex state spaces remains challenging. Existing RL algorithms lack temporally-extended exploration, limiting their performance on difficult tasks. Two approaches to efficient exploration in RL are optimism in the face of uncertainty and posterior sampling. Optimism-based approaches use upper confidence bounds (UCBs) to direct exploration, but constructing UCBs for general function approximators is unclear. Posterior-based approaches use randomized value functions and achieve near-optimal performance, but require modifications for linear models. In this paper, we propose BooVI, a variant of bootstrapped LSVI that combines the advantages of both approaches. BooVI uses posterior sampling to construct an optimistic version of the value functions, and demonstrates practical and theoretical advantages over existing algorithms.