We consider the problem of transporting probability mass from a source distribution P to a target distribution Q using a Wasserstein gradient ï¬‚ow in probability space. Previous approaches, such as the Unadjusted Langevin Algorithm (ULA) and Stein Variational Gradient Descent (SVGD), leverage the analytic expression of the density of the target distribution. However, these approaches are limited to settings where the distributions share the same support and the density of the target distribution is well-defined and available. In this paper, we propose a new approach called the KL Approximate Lower bound Estimator (KALE), which is a relaxed approximation of the KL divergence. The KALE is well-defined for any source and target, regardless of their relative absolute continuity. We construct the Wasserstein gradient flow of the KALE and show its global convergence under certain regularity conditions. We also introduce the KALE particle descent algorithm and present empirical results demonstrating its effectiveness compared to the Maximum Mean Discrepancy (MMD) flow.