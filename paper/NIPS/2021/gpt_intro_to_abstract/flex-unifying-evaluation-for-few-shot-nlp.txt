Few-shot learning is a critical challenge in developing efficient and robust natural language processing (NLP) techniques. However, there is a lack of consensus on the best techniques and whether they outperform simple baselines. Existing evaluation suites in the few-shot NLP research community are disjointed, lack realistic testing setups, and do not ensure accurate and precise evaluation estimates. This paper introduces FLEX, a high-quality benchmark for few-shot NLP evaluation that addresses these issues. FLEX consists of episodes with variable shot numbers and class imbalance, includes zero-shot episodes and textual labels, and balances statistical accuracy and computational requirements. Additionally, the paper introduces UniFew, a prompt-based model for few-shot learning in NLP that eliminates the need for complex prompt engineering. The release of FLEX includes a toolkit for benchmark creation and few-shot NLP model development.