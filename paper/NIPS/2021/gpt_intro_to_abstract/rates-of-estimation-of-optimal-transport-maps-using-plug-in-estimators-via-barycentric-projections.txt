The problem of finding a "nice" map T0(·) such that T0(X) ∼ ν, where X ∼ µ and Y ∼ ν, has numerous applications in machine learning. This paper focuses on the estimation of the optimal transport (OT) map using the standard squared Euclidean cost function. The main challenge in constructing estimators for T0 is that the explicit forms of the measures µ, ν are unknown, and only random samples are available. The main goal of this paper is to study the rates of convergence of general plug-in estimators for T0, and to estimate the squared Wasserstein distance between µ and ν. The paper shows that kernel smoothing techniques can be used to obtain plug-in estimators that mitigate the curse of dimensionality.