This paper introduces the field of Reinforcement Learning (RL) and Stochastic Approximation (SA) theory in the context of Multi-Agent Reinforcement Learning (MARL). The goal of MARL is for multiple agents to cooperate and find action policies that maximize collective rewards obtained over time. While MARL has shown empirical success in various domains, theoretical analyses are still limited. The paper aims to address this gap by analyzing a family of Distributed Stochastic Approximation (DSA) algorithms, which are viewed as special cases of MARL methods. The update rule for the DSA scheme is described, involving a weighted average of estimates obtained through gossip and a refinement based on local computations. The framework allows for the driving function to depend on all estimates, not just the agent's own, and computations run synchronously on a common clock.