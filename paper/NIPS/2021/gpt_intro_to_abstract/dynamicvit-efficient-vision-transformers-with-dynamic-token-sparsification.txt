The introduction of this computer science paper discusses the progress in computer vision achieved through the use of CNN-type architectures and the recent trend of replacing CNN with transformers for various vision tasks. The paper proposes a method to accelerate transformer-like models by introducing sparsity through token pruning, taking advantage of the self-attention module's ability to handle variable-length token sequences. The authors introduce a lightweight prediction module called DynamicViT, which dynamically determines and prunes uninformative tokens in a hierarchical manner. The module is optimized jointly with the vision transformer backbone through two specialized strategies. Experimental results on ImageNet demonstrate the trade-off between speed and accuracy, with significant reductions in computational overhead and improved throughput. The paper concludes by highlighting the potential of this method to open new avenues for accelerating transformer-like models.