Relative entropy policy search (REPS) is a widely used algorithm for learning agent policies in reinforcement learning (RL) scenarios. It has shown success in various challenging tasks, both in simulation and real-world robot applications. The mathematical foundations and algorithmic components of REPS have also influenced the development of subsequent algorithms with their own practical accomplishments. The REPS algorithm is based on convex duality and uses a Kullback Leibler (KL)-regularized version of the max-return objective to optimize policies. However, there are two main practical difficulties with this approach: the optimization of the dual objective and the use of gradient-based solvers in stochastic settings. This paper addresses these challenges by providing guarantees on the near-optimality of derived policies and proposing a stochastic gradient descent procedure that utilizes a plug-in estimator for REPS gradients. These solutions ensure both practical performance and strong theoretical guarantees for REPS.