Deep neural networks (DNNs) have made significant advances in recent times and have been deployed in many real-world applications. However, DNNs often suffer from biases and show discrimination towards certain demographics, leading to adverse effects in high-stake applications like criminal justice and loan approvals. Existing debiasing methods focus on learning debiased representations at the encoder level or incorporating explanation during model training. These methods aim to remove bias from deep representations, but they require additional meta-data and annotations, which are expensive or unavailable in most real-world applications. In this paper, we propose the Representation Neutralization for Fairness (RNF) framework, which mitigates discrimination in DNN models by debiasing only the task-specific classification head. Our framework does not require access to instance-level sensitive attribute annotations and achieves significant reduction of discrimination while maintaining task performance. Experimental results on benchmark datasets demonstrate the effectiveness of our approach and its complementarity to existing debiasing methods.