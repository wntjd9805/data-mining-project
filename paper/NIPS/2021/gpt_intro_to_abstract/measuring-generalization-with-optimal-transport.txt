This paper addresses the gap between theory and practice in deep learning by proposing margin bounds based on optimal transport. The authors show that the expected optimal transport cost of matching two independent random subsets of the training distribution can serve as an alternative to Rademacher complexity and capture the structural properties of the data distribution. These k-variance normalized margin bounds are able to predict generalization error effectively on the PGDL challenge data. The authors also provide a theoretical analysis highlighting the importance of feature distributions in generalization.