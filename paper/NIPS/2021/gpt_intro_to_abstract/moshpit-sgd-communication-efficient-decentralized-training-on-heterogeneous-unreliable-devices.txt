Recent advancements in deep learning have been driven by the scaling of model and dataset sizes. This trend has led to significant improvements in various domains, such as computer vision and natural language processing. However, training these large models can be time-consuming, often requiring distributed training on multiple machines. The dominant approach to distributed deep learning is data-parallel training, where each worker processes a fraction of the training batch and exchanges gradients with peers. While communication-efficient protocols, like all-reduce, have been developed to address the network overload issue, they require specialized infrastructure with high-bandwidth networking. This paper proposes a decentralized matchmaking algorithm called Moshpit All-Reduce, which allows participants to dynamically organize themselves into groups. This approach enables the use of communication-efficient protocols while operating in unreliable hardware and network conditions. The authors present convergence rates and equivalence to centralized SGD for their Moshpit SGD algorithm and demonstrate its efficiency in realistic conditions through experiments on popular models.