Comparing data distributions is a common task in various fields, including statistical analysis and distribution testing. Optimal transport theory offers a useful framework for measuring the distance between distributions. The Wasserstein Distance, which is defined as the cost of an optimal transport plan, takes into account the underlying geometry of the data space, providing a meaningful correspondence between points in different distributions. However, computing optimal transport plans exactly is computationally challenging. In this paper, we propose a regularized form of the optimal transport problem that can be solved by studying a joint density with specific marginals. We introduce a direct sampling strategy using score-based generative models, which eliminates the need for a barycentric projection and improves the quality of samples. Our contributions include adapting pretrained score-based generative models for high-dimensional regularized optimal transport, addressing the conditional sampling problem, proving convergence and optimization error bounds for neural network parameters, and demonstrating the effectiveness of our method on synthetic and image data.