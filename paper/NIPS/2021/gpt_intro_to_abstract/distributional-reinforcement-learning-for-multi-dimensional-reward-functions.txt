In this paper, we address the problem of capturing correlated randomness in source-specific returns in value-based reinforcement learning. We propose a distributional RL algorithm called MD3QN that models the joint return distribution from multiple sources of reward. We establish convergence results for the joint distributional Bellman operator and propose an empirical algorithm that minimizes the Maximum Mean Discrepancy (MMD) loss over the joint return distribution and its Bellman target. Experimental results demonstrate that our method outperforms previous algorithms and accurately models the joint return distribution from all sources of rewards.