Scaling up deep learning models to handle extremely large datasets has been pivotal in the field of computer science. However, the high memory usage and computational cost associated with these models have posed challenges in terms of real-time deployment and resource-limited devices. To address these issues, researchers have explored the quantization of large models, which involves replacing high-precision parameters with lower-precision ones. While quantization can reduce the carbon footprint of training and inference, it can also lead to increased bias. Various methods have been proposed to achieve quantized neural networks, such as explicit or implicit regularization techniques during gradient training. Additionally, the structure of quantization can enhance the speed of inference. Impressive results have been achieved with quantized networks on tasks like object detection and natural language processing. However, a thorough understanding of the inner workings of quantization methods, such as BinaryConnect (BC), is lacking. This paper aims to improve our understanding of BC by connecting it with well-established theory and algorithms. The authors present a principled theory for constructing proximal quantizers, unify existing efforts, and propose a new quantizer design. They also demonstrate the relationship between BC and the dual averaging algorithm, and propose ProxConnect (PC) as a generalization of BC with improved convergence properties. Experimental results on CIFAR-10 and ImageNet datasets demonstrate the superiority of PC over BC and ProxQuant.