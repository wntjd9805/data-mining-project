In recent years, Transformer models have gained popularity in natural language processing (NLP) and there has been a growing interest in adapting them to computer vision (CV) tasks. This paper introduces the concept of focal attention, a new attention mechanism that captures both short- and long-range interactions in Transformer layers for high-resolution input images. By focusing on local regions for fine-grained attention and using coarse-grained attention globally, the proposed focal attention mechanism effectively models visual dependencies while reducing computational cost. A series of Focal Transformers are developed and evaluated on image classification, object detection, and semantic segmentation tasks, consistently outperforming state-of-the-art Vision Transformers. These results demonstrate the efficacy of focal attention in modeling global interactions in Vision Transformers.