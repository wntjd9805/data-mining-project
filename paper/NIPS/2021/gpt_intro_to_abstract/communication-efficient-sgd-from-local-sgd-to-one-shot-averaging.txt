This paper introduces the concept of Stochastic Gradient Descent (SGD), a commonly used algorithm for minimizing convex functions. The authors discuss the challenges of parallelizing SGD due to communication bottlenecks and propose a method called "synchronized parallel SGD" that relies on frequent gradient sharing among workers. However, they acknowledge that this method is not ideal for large-scale optimization applications. To address this issue, they explore alternative methods such as mini-batch parallel SGD and local SGD, which reduce the number of communication rounds and synchronization delays. The authors aim to answer two key questions: how many communication rounds are needed for local SGD to achieve similar convergence rates as synchronized parallel SGD, and can One-Shot Averaging (OSA) achieve linear speed-up in a more general scenario? They provide theoretical analyses and simulations to support their findings. Overall, the paper highlights the importance of communication strategies for efficient optimization algorithms in the field of computer science.