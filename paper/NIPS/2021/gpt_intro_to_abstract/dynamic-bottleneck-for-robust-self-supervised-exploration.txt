The tradeoff between exploration and exploitation is a major challenge in reinforcement learning (RL). Enhancing exploration efficiency while maintaining exploitation is a common approach, but it becomes difficult when extrinsic rewards are sparse or unavailable. In this paper, we propose a Dynamic Bottleneck (DB) model for self-supervised exploration, which generates a dynamics-relevant representation of the current state-action pair. We use the Information-Bottleneck principle to maximize the mutual information between the representation and the next state, while minimizing the mutual information between the state-action pair and the representation to discard dynamics-irrelevant information. We further construct a DB-bonus for exploration, which measures the novelty of state-action pairs. Experimental results on Atari games with dynamics-irrelevant noise show that our self-supervised exploration with DB-bonus is robust and outperforms state-of-the-art methods.