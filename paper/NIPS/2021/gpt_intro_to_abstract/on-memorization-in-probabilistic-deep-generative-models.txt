In recent years, there have been significant advancements in generative modeling using deep learning techniques such as variational autoencoders, generative adversarial networks, normalizing flows, and diffusion networks. These models aim to learn the data distribution of a given dataset for various applications, including generating realistic synthetic data and detecting anomalies. Evaluating generative models involves assessing the extent to which training observations are memorized by the learning algorithm, and this has implications for data privacy, model stability, and generalization performance. Current techniques for assessing memorization, such as comparing generated samples to their nearest neighbors in the training set, have limitations and can be easily fooled. In this paper, we propose a new measure of memorization for generative models and introduce a practical estimator for this score. We conduct experiments using variational autoencoders and demonstrate that highly memorized observations may not necessarily be outliers and can occur early in the training process. We also discuss approaches to limit memorization in practice.