Deep neural networks are widely used in critical applications such as medical diagnosis and autonomous driving. However, these applications often involve covariate shift, where the target data distribution differs from the distribution used for training. Accurately representing uncertainty is crucial for robustness to these shifts. Bayesian methods, known for their ability to represent model uncertainty, are commonly benchmarked on out-of-distribution generalization tasks. However, recent research has shown poor out-of-distribution generalization performance of Bayesian neural networks (BNNs) with high-fidelity inference through Hamiltonian Monte Carlo (HMC). In this paper, we aim to understand, demonstrate, and remedy this concerning behavior. We show that BNNs perform poorly for different types of covariate shift and propose a novel prior to improve generalization under covariate shift. Additionally, we propose another prior to address the effect of non-zero mean corruptions.