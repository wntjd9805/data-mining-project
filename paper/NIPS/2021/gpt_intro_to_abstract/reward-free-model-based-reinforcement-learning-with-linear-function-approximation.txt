In reinforcement learning, the design of reward functions to encourage desired agent behavior can be time-consuming and inefficient. To address this issue, a new paradigm called Reward-Free Exploration (RFE) has been proposed, which explores the environment without using any reward function. This paper focuses on understanding the statistical efficiency of reward-free RL with linear function approximation. Two reward-free model-based RL algorithms are proposed for linear mixture/kernel MDPs, and their sample complexity is analyzed. Additionally, a lower bound is established for any reward-free algorithm. The contributions of the paper include the introduction of a new exploration-driven reward function and two algorithm proposals with improved sample complexities.