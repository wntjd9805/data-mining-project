This paper explores the concept of value equivalence (VE) in reinforcement learning (RL) agents and proposes a new formulation called proper value equivalence (PVE) that focuses on value functions. The authors demonstrate that PVE reduces the space of functions that need to be considered and does not collapse to a singleton in the limit, allowing for multiple beneficial models. They also show that minimizing the loss of the MuZero algorithm can be understood as minimizing a PVE error, and propose a modification to MuZero that results in improved performance in the Atari Learning Environment.