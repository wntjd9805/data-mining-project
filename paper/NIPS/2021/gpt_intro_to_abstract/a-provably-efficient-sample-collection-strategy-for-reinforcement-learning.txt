Online reinforcement learning (RL) presents the challenge of balancing exploration and exploitation to optimize behavior. Planning a policy for information gathering in a specific region of the Markov decision process (MDP) is difficult without precise knowledge of the environment dynamics. This paper introduces a decoupled approach consisting of an "objective-specific" algorithm that prescribes sample requirements and an "objective-agnostic" algorithm that efficiently collects these samples. The proposed algorithm, GOSPRL (Goal-based Optimistic Sampling Procedure for RL), is flexible and achieves sample complexity guarantees comparable to or better than existing algorithms in various exploration problems. Numerical simulations support the theoretical findings and demonstrate the superiority of combining GOSPRL with problem-specific algorithms.