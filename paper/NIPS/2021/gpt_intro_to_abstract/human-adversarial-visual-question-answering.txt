Visual question answering (VQA) is an important evaluation task in vision and language research, offering a mechanism to understand machine comprehension of images through natural language queries. While the performance of VQA models has plateaued on popular datasets, raising questions about the extent to which the problem has been solved, dynamic data collection emerges as an intriguing method to investigate further. This paper introduces Adversarial VQA (AdVQA), a large evaluation dataset that challenges existing VQA models and highlights their performance shortcomings. Through extensive analysis and comparison with existing datasets, the authors hope to drive further progress in the field and emphasize the need for continued improvement.