Learning deep generative models over discrete probability spaces has been highly successful in various domains, including computer vision, natural language processing, and robotics. These models typically utilize the softmax function to obtain probability distributions from neural networks. However, achieving sparsity in high-dimensional dense distributions is crucial for many downstream tasks. Previous approaches, such as sparsemax, have been proposed to address this issue but often result in collapsing the multimodality of distributions. In this paper, we present a novel sparse normalization function called evidential softmax (ev-softmax) that maintains multimodality while producing sparse probability distributions. We demonstrate the effectiveness of ev-softmax in various tasks and show that it outperforms existing normalization functions in terms of distributional accuracy. Additionally, we provide a full-support continuous approximation of ev-softmax that is compatible with common probabilistic loss functions, such as negative log-likelihood and KL divergence. Our proposed approach can directly optimize discrete generative models for objective functions with probabilistic interpretations, distinguishing it from post-hoc sparsiÔ¨Åcation methods.