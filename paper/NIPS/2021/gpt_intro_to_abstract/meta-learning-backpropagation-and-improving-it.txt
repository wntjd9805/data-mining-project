Meta learning is a promising approach that involves learning the learning algorithm (LA) itself, reducing the need for human designers to manually craft useful learning algorithms. While recent advancements in meta learning have focused on generalization from training tasks to similar test tasks or environments, there is still a reliance on human-designed and unmodifiable inner-loop components such as backpropagation. In this paper, we propose the Variable Shared Meta Learning (VSML) principle, which introduces a novel way of using sparsity and weight-sharing in neural networks (NNs) for meta learning. By replacing the weights of a neural network with tiny LSTMs, we create a system where many RNNs pass messages to each other, enabling meta optimization of backprop-like algorithms. Our VSML RNN shows strong generalization capabilities, learning online learning algorithms from scratch that outperform gradient descent and generalize to datasets outside of the meta training distribution.