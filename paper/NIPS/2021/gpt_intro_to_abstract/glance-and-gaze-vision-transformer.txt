Convolutional Neural Networks (CNNs) have been widely successful in computer vision tasks, but they lack the ability to globally represent long-range dependencies. Recently, Transformers have emerged as a strong alternative, naturally learning global features. However, Transformers require more memory and computation costs than CNNs, especially for natural images. To address this, various strategies have been proposed, but they have limitations. In this paper, we propose the Glance-and-Gaze Transformer (GG-Transformer) which combines the long-range dependency modeling ability of Transformers with the locality of convolutions. The GG-Transformer consists of parallel branches that perform adaptive-dilated self-attention and depth-wise convolutions. We evaluate the GG-Transformer on multiple vision tasks and benchmarks, demonstrating its efficiency and superior performance compared to previous state-of-the-art Transformers.