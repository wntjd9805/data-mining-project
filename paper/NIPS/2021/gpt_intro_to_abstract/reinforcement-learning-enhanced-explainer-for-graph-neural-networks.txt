Graph Neural Networks (GNNs) have achieved impressive results in various machine learning and reasoning tasks on graph data. However, one major drawback of GNNs is the lack of interpretability in their predictions. To address this issue, several GNN explainers have been proposed, but they have limitations in terms of connectivity and generality. In this paper, we propose RG-Explainer, a framework that utilizes reinforcement learning to explain GNNs' predictions. Our framework consists of three components - starting point selection, iterative graph generation, and stopping criteria learning - to generate an explanatory graph that interprets the predicted label of a given node or graph instance. We also incorporate constraints to ensure the compactness and meaningfulness of the explanatory graph. Our approach exhibits better generalization ability and outperforms state-of-the-art GNN explainers in terms of performance and interpretability, as demonstrated through extensive experiments on synthetic and real-world datasets.