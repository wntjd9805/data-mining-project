Graph-structured data are prevalent in various real-world applications, leading to a growing interest in graph representation learning. Graph neural networks (GNNs) are widely used in predicting node, edge, and graph tasks, but they struggle to capture long-range dependencies. Existing approaches either suffer from oversmoothing or require excessive computational cost. In this paper, we propose an Efficient Infinite-Depth Graph Neural Network (EIGNN) model that effectively captures very long-range dependencies in graphs. We derive a closed-form solution for EIGNN without the need for iterative solvers and improve its efficiency using eigendecomposition. Experimental results demonstrate that EIGNN outperforms other baseline GNN models in capturing long-range dependencies and is more robust against noise and adversarial perturbations. The paper provides an overview of GNNs, implicit models, and the oversmoothing problem, introduces the EIGNN model, discusses training and computation efficiency, and presents empirical comparisons with other GNN methods.