Reinforcement learning (RL) has shown great potential in enabling artificial agents to solve complex tasks in uncertain environments. Recent advancements in RL with deep neural networks have led to significant progress in autonomous decision making. While there has been extensive research on extracting high-quality features in RL, the incorporation of effective architectures for temporal features has been overlooked. In this paper, we propose a new architecture called Flow of Latents for Reinforcement Learning (Flare) that improves the utilization of temporal features in deep RL from pixels. Through experiments, we demonstrate that Flare achieves optimal performance in state-based RL without explicit access to state velocity, outperforms existing model-free methods on challenging pixel-based continuous control tasks, and exceeds the performance of baseline algorithms on several Atari games.