Modern Reinforcement Learning (RL) faces challenges when dealing with practical problems that have a large number of states. In order to overcome this, function approximation using deep neural networks has been applied, leading to successful RL applications in various domains. However, RL with function approximation raises theoretical challenges such as generalization, limited expressiveness, and exploration. Existing theoretical results rely on strong structural assumptions, which are rarely applicable in real-world scenarios. This paper aims to identify weak structural assumptions that enable sample-efficient RL and proposes a new complexity measure called Bellman Eluder (BE) dimension. The authors introduce the GOLF algorithm, which learns near-optimal policies for RL problems of low BE dimension in a polynomial number of samples. The OLIVE algorithm is also reanalyzed, showing its capability to learn low BE dimension problems with slightly weaker assumptions but with higher sample complexity compared to GOLF.