In statistical learning theory, the objective is to find the best function in a class of closed, convex sets that maps an input to a target, based on a joint distribution of the input and target variables. Risk minimization is a popular approach for this task, but it requires knowledge of the distribution, which is often unknown in practice. Instead, empirical risk minimization (ERM) is used, where observations from the distribution are used to approximate the true risk minimizer. However, most ERM analyses assume independently and identically distributed observations, which limits their applicability. In this paper, we analyze ERM with convex loss functions in the presence of heavy-tailed and non-iid data, using small-ball techniques and concentration inequalities to handle the interaction between the noise and inputs. Our work fills a gap in theoretical guarantees for learning scenarios with heavy-tailed and dependent data.