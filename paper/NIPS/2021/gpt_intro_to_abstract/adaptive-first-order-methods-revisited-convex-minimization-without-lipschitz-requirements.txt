First-order methods are widely used in learning theory and continuous optimization due to their applicability and flexibility. The analysis of these methods typically focuses on two regularity conditions: Lipschitz continuity of the objective function and Lipschitz continuity of its gradient. Different convergence rates have been observed depending on whether the optimizer has access to perfect gradients or stochastic gradients. To bridge this gap, adaptive methods have been developed to interpolate between these regimes. Two state-of-the-art methods, ACCELEGRAD and UNIXGRAD, achieve a convergence rate that is the "best of all worlds" in both smooth and non-smooth problems. However, these methods do not account for problems that do not adhere to the Lipschitz regularity requirements. In these cases, a breakthrough was made by considering a "Lipschitz-like" gradient continuity condition and a "relative continuity" condition using Bregman divergence. Non-adaptive mirror descent methods have been successful in achieving optimal convergence rates in these extended problem classes. In this paper, we propose an adaptive mirror descent method called ADAMIR that achieves order-optimal rates in the non-Lipschitz framework. ADAMIR has desirable properties such as achieving optimal convergence rates in relatively continuous and relatively smooth problems, as well as an average convergence rate with stochastic gradients. The method is applicable to both constrained and unconstrained problems without requiring prior knowledge of the problem's regularity class or oracle model. The method relies on an adaptive step-size policy based on "Bregman residuals" to handle the exploding variation of non-Lipschitz functions near the boundary of the problem's domain.