With the increasing use of Deep Neural Networks (DNNs) in critical systems, such as autonomous vehicles and industrial robots, there is a need to ensure the reliability of these algorithms, particularly for non-specialists who may treat them as black boxes. This paper proposes a simple method, called DOCTOR, that can detect whether a classifier's prediction is likely to be correct or not. The method is applicable in both black box and partially black box scenarios and outperforms existing methods in detecting misclassification errors. The experimental results demonstrate the effectiveness of DOCTOR in different datasets and architectures.