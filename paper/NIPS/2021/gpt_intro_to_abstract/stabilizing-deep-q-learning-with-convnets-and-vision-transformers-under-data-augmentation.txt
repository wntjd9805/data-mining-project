Reinforcement Learning (RL) from visual observations has achieved significant success in various applications, but generalizing learned skills to novel environments remains challenging. Current methods often overfit to the training environment, especially for high-dimensional observation spaces like images. Data augmentation has shown promise in increasing the variability in training data and improving generalization. However, indiscriminate application of data augmentation can lead to high-variance Q-targets and over-regularization. In this paper, we propose SVEA, a framework for stabilizing Q-value estimation under data augmentation in off-policy RL. Our method addresses these problems by applying augmentation only in Q-value estimation of the current state, optimizing Q-value estimation jointly over augmented and unaugmented observations, and optimizing the actor strictly on unaugmented data. We empirically evaluate our method on various tasks and demonstrate improved Q-value estimation and generalization compared to previous state-of-the-art methods, with lower computational cost. Additionally, we show that our method scales to RL with Vision Transformers.