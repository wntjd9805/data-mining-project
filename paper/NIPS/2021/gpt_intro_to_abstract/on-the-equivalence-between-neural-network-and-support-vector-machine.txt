Recent research has focused on deep learning theory from the perspective of infinite-width neural networks (NN) and the Neural Tangent Kernel (NTK). While existing theories have explored the connection between NN and kernel regression, little has been done to connect NN with Support Vector Machines (SVM). In this paper, we establish the equivalence between NN and SVM for the first time and show that this equivalence extends to a family of L2 regularized kernel models (KMs). By establishing this equivalence, we shed light on the understanding of NN, especially in terms of training, generalization, and robustness for classification problems. We derive the dynamics of SVM and NN under the soft margin loss and show that they are exactly the same in the infinite-width limit, leading to the same linear convergence rate. Furthermore, we generalize our theory to include various loss functions with L2 regularization and demonstrate the practical benefits of our approach, including the computation of non-vacuous generalization bounds and improved robustness.