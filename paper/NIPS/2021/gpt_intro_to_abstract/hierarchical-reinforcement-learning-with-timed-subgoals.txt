Hierarchical reinforcement learning (HRL) has emerged as a powerful approach for tackling complex, long-horizon tasks. By breaking down the problem into subtasks and using a high-level policy to select subgoals for a low-level policy to achieve, HRL has demonstrated unprecedented efficiency in learning demanding tasks. However, a major challenge in HRL is the non-stationarity of the effective environment caused by the changing behavior of the lower level during training. This paper proposes a solution to this problem by introducing timed subgoals, which allow the higher level to control the transition times of the hierarchy. This approach eliminates the non-stationarity and enables stable concurrent learning, even in dynamic environments. The proposed algorithm, Hierarchical reinforcement learning with Timed Subgoals (HiTS), is evaluated on benchmark tasks and novel challenges, achieving stable solutions and outperforming existing methods. The use of timed subgoals, combined with hindsight action relabeling, is theoretically shown to remove the non-stationarity of the data generation process.