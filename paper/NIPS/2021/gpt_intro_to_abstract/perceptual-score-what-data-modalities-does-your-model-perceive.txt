Machine learning advancements in the past decade have facilitated significant improvements in tasks such as visual question answering and reasoning. These improvements are primarily attributed to the availability of large datasets, advancements in computational performance, and a better understanding of encoding inductive biases into deep neural networks. However, it is crucial to address concerns regarding the potential exploitation of dataset biases by deep-net architectures. To assess the reliance of classifiers on different data modalities, we introduce the concept of perceptual score, which measures the degree to which a model depends on a specific modality. By utilizing the perceptual score, we discover a consistent trend in multi-modal models for visual question answering, where recent state-of-the-art models rely less on visual data and instead infer answers from textual cues. This trend raises concerns about the reasoning abilities of these models. Additionally, we analyze model biases by decomposing the perceptual score into data subset contributions. We propose the perceptual score as a metric for evaluating the perceptiveness of multi-modal models and encourage further research in this area. Our experiments encompass multiple datasets and models, revealing the tendency of multi-modal models to ignore certain modalities and take shortcuts. Furthermore, we identify sources of bias in popular multi-modal datasets such as VQA-CP and SocialIQ, such as sentiment bias and shifts in training priors.