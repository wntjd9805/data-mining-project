Convolutional neural networks (CNNs) have been successful in computer vision tasks due to their ability to extract high-level features from images. However, as CNNs have become deeper, the challenge of efficiently passing information through layers has emerged. Residual connections and skip connections have been used to address this challenge, allowing layers to have direct access to previous ones. In this paper, we introduce the concept of layer aggregation, exemplified by DenseNet, to study network designs on feature reuse. However, densely-connected stages in DenseNet lead to a quadratic increase in network parameters, limiting the storage of new information. To address this, we propose a lightweight recurrent layer aggregation (RLA) module with fewer parameters. Our RLA module replaces dense connectivity with recurrent connections, achieving a parameter count independent of network depth. It can be easily added to existing CNNs for improved feature extraction. We provide a schematic diagram in Figure 1 and demonstrate the compatibility of RLA with various CNN architectures. We perform extensive experiments and show that RLA consistently improves model performance on CIFAR, ImageNet, and MS COCO datasets, surpassing state-of-the-art networks. Our contributions include defining layer aggregation, proposing the RLA module, investigating its designs, and demonstrating its effectiveness across different tasks and architectures.