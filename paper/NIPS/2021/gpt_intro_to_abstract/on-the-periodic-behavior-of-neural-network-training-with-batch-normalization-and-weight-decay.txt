Normalization approaches, such as batch or layer normalization, are essential for training deep neural networks. However, the impact of normalization on the training process is not fully understood. In this paper, we investigate the surprising periodic behavior that occurs when training a neural network with a combination of batch normalization (BN) and weight decay regularization (WD). We examine the dynamics of training with BN and WD and analyze the interplay between the "centripetal force" of WD and the "centrifugal force" of the loss gradients. Previous works have contradictory views on the dynamics of the norm of scale-invariant weights in this training setup. Our work aims to resolve this contradiction by studying the equilibrium and instability presumptions. We discover that the training process exhibits a consistent periodic behavior, with regular oscillations between instabilities and new periods of training. We provide theoretical analysis, empirical evidence, and practical scenarios to support our findings.