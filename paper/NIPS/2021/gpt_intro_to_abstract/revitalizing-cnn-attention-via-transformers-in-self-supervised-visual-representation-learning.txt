In this paper, we introduce a CNN Attention REvitalization (CARE) framework that incorporates transformer guidance to make CNN encoders more attentive. Our framework consists of two streams, the C-stream and the T-stream, where the T-stream utilizes transformers to improve feature attention in the CNN encoders. We perform self-supervised learning in both streams simultaneously, with the T-stream output supervising the C-stream. The experimental results on various recognition tasks demonstrate that the CARE framework significantly improves the performance of CNN encoder backbones, surpassing state-of-the-art approaches.