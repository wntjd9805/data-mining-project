AI systems that continuously learn from new data while maintaining knowledge of previous concepts face the challenge of catastrophic forgetting, where new updates override previously acquired knowledge. To address this problem, this paper proposes a reinforced memory management (RMM) approach for class-incremental learning (CIL) in image classification. RMM dynamically allocates memory between old and new classes, considering the recognition difficulty of each old class before discarding any data. The proposed method leverages reinforcement learning and utilizes a hierarchical policy function to maximize cumulative evaluation accuracy. The paper presents extensive experiments on three benchmarks, showing the superiority of RMM over existing methods. The technical contributions include the RMM algorithm, a pseudo task generation strategy, and comprehensive evaluation and interpretation of RMM using top-performing models as baselines.