Neural networks, particularly Transformer networks, have become the default choice for many language applications due to their effectiveness. However, these models are large and resource-intensive, making them challenging to train and deploy. To address this issue, researchers are exploring methods to reduce model sizes while maintaining performance. One promising approach is to simulate particle interactions over time, treating each layer of the Transformer as a step in the simulation. However, the current approach does not consider the temporal nature of the network's evolution, leading to inefficiencies. In this paper, we propose a new method called TransEvolve that uses time-evolution functionals to approximate the underlying dynamics of the system. Experimental results demonstrate that TransEvolve outperforms the Transformer model with fewer trainable parameters and faster training speed on various tasks.