Traditionally, reinforcement learning (RL) focuses on online learning where the RL agent interacts with its environment. In offline RL, the agent instead learns from a fixed dataset, eliminating the need for expensive or challenging data collection. However, offline RL algorithms tend to underperform due to "extrapolation error," which affects policy evaluation and improvement. This paper explores a minimalist approach to offline RL, proposing a single adjustment to the TD3 algorithm that significantly improves performance. The proposed algorithm, TD3+BC, adds a behavior cloning term to regularize the policy, requiring only minimal changes to the code. The algorithm is evaluated on the D4RL benchmark and demonstrates favorable results compared to other offline RL algorithms, while being easier to implement and reducing computation costs. These findings suggest that simpler approaches to offline RL have been underexplored in favor of more complex algorithms.