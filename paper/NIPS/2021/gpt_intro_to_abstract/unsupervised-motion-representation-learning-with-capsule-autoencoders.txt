Real-world movements convey significant information beyond their physicality, such as communication and identity. Understanding and analyzing these movements are crucial for artificial intelligence agents to effectively interact with the dynamic environment. However, learning motion pattern representations poses challenges, including high-dimensional input data, modeling long-term dependencies, intra-class variation, and insufficient data annotation. In this paper, we propose the Motion Capsule Autoencoder (MCAE), an unsupervised capsule framework inspired by viewpoint-invariant capsule-based image representations. MCAE leverages snippet-segment hierarchy to learn transformation-invariant motion representations for keypoints. We introduce Trajectory20, a synthetic dataset with diverse motion patterns and controllable intra-class variations. Extensive experiments on Trajectory20 and real-world skeleton human action datasets showcase the effectiveness of MCAE. Ablation studies are also conducted to evaluate the impact of regularizers and key hyperparameters.