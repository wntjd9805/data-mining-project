This paper introduces the concept of nonsmooth implicit differentiation in the context of differentiable programming. The authors address the gap between the use of algorithmic differentiation tools and the lack of theoretical explanation for their success in the nonsmooth world. They propose a versatile theory of nonsmooth differentiation and showcase its impact on the training of neural networks and hyperparameter optimization. The paper also presents a mathematical model for propagating derivatives using conservative calculus, which allows for a rigorous study of training methods in Deep Learning. The authors establish a nonsmooth conservative implicit function theorem and provide convergence guarantees for mini-batched stochastic algorithms. Various examples are presented to illustrate the concepts and limitations of the proposed approach.