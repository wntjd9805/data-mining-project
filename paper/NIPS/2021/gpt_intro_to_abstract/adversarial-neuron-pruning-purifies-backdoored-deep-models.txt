Deep neural networks (DNNs) have achieved significant success in various tasks but rely on large amounts of computation and data. This has led to the outsourcing of training in "Machine Learning as a Service" platforms, creating potential risks of training-time attacks such as backdoor attacks. Backdoor attacks stealthily establish a relationship between a trigger pattern and a target label within DNNs, posing a threat to real-world applications. While previous methods have focused on defense during training, this paper addresses the scenario of repairing models after training without knowledge of the trigger pattern. The paper introduces a novel model repairing method called Adversarial Neuron Pruning (ANP), which prunes the most sensitive neurons in DNNs to remove the injected backdoor. Experimental results demonstrate that ANP provides state-of-the-art defense against various backdoor attacks, even with a small amount of clean data.