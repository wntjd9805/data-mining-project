Federated learning (FL) is a distributed learning approach that allows edge devices to train a shared model without sharing their local training data. However, recent studies have shown that edge devices can easily conduct model poisoning attacks by manipulating the local training process. These attacks can either be untargeted, aiming to increase the error rate of the global model, or targeted, aiming to generate specific misclassifications. Existing defenses focus on preventing model pollution during aggregation but fail against strong attacks. In this paper, we propose a client-based defense called FL-WBC that mitigates model poisoning attacks. We introduce a quantitative estimator called Attack Effect on Parameter (AEP) to assess the impact of attacks on the global model. Our defense effectively mitigates attacks in minimal communication rounds without significant accuracy drops. We also evaluate our defense against state-of-the-art attacks and demonstrate its effectiveness in enhancing the robustness of FL.