Recent advancements in deep neural networks have led to the widespread use of Q-learning for solving reinforcement learning problems in various applications. However, it is well-known that Q-learning suffers from overestimation bias, which hinders its learning efficiency. The ensemble method has emerged as a potential solution by using multiple Q-function approximators to improve action value estimation. However, the optimal ensemble size is still an open question. This paper proposes an adaptive ensemble size adaptation method, AdaEQ, to minimize estimation bias during the learning process. AdaEQ utilizes the approximation error as feedback to dynamically adjust the ensemble size, aiming to drive the bias close to zero. Experimental results on the MuJoCo benchmark demonstrate that AdaEQ outperforms state-of-the-art ensemble methods by achieving higher average returns while keeping the estimation bias minimal.