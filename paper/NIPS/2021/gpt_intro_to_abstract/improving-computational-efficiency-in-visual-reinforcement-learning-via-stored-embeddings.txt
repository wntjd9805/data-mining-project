This paper introduces SEER, a technique for reducing computational overhead and memory requirements in deep reinforcement learning (RL) algorithms. The technique involves freezing the lower layers of CNN encoders early in training, allowing for improved compute-efficiency and memory-efficiency. The authors also leverage the memory-efficiency of SEER to increase replay capacity, resulting in improved sample-efficiency in constrained-memory settings. Experimental results demonstrate that SEER maintains performance while significantly reducing computation, making it compatible with existing RL algorithms. Overall, this paper presents SEER as a promising approach for achieving compute- and memory-efficient deep RL.