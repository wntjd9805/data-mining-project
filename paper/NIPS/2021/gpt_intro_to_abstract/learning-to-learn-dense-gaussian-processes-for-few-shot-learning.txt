Meta learning, also known as learning to learn, has emerged as a powerful approach for acquiring shared knowledge from related tasks in order to quickly solve novel tasks. This has led to advancements in few-shot learning, where models are able to make reliable predictions on new tasks with only a small amount of labeled data. In this paper, we focus on the use of Gaussian processes with deep kernels as a learning model for few-shot learning. We propose a method that utilizes a dense set of inducing variables to capture shared knowledge from experienced tasks and improve the learning of new tasks. These inducing variables are adaptively updated for each task, allowing the model to better fit any novel task at test time. Our experiments demonstrate that this approach achieves substantial performance improvements over existing methods and delivers state-of-the-art results on common few-shot learning benchmarks.