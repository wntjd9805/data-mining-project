This paper introduces a class of deep neural networks (DNNs) called efficiently axiomatically attributable DNNs, which can compute closed-form axiomatic feature attributions that satisfy proposed axioms. By removing the bias term of each layer, these networks can be instantiated from a wide range of regular DNNs, resulting in significant improvements in efficiency. The paper also establishes a theoretical connection between Gradient and Integrated Gradients for nonnegatively homogeneous DNNs. Experimental results show that efficiently axiomatically attributable DNNs achieve accurate attributions at a fraction of the computational cost and outperform state-of-the-art attribution methods for training regular networks with an attribution prior.