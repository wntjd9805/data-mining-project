Providing auxiliary tasks to Deep Reinforcement Learning (Deep RL) agents has become a common approach for enhancing the learning of state representations in main tasks. However, existing auxiliary tasks require careful design and fixed semantics. In this paper, we propose a different approach where random action-conditional prediction tasks are generated using general value functions (GVFs), allowing for a diverse set of predictions based on random features of observations and actions. Surprisingly, our empirical findings demonstrate that learning random GVFs can yield competitive control performance compared to hand-crafted auxiliary tasks. We validate our approach using Atari games and DeepMind Lab tasks, comparing it against baseline auxiliary tasks such as multi-horizon value prediction, pixel control, and CURL. Furthermore, we show the benefits of exploiting the richness of GVFs, including their temporal depth and action-conditionality. We also provide evidence that the learned random GVF representations are useful for the main task, outperforming end-to-end-trained actor-critic baselines in stop-gradient experiments where representation learning is solely influenced by random-GVF auxiliary tasks without using rewards from RL.