Advanced deep generative models have made significant advancements in natural image synthesis, but they have limitations in synthesizing 3D-consistent views and representing explicit 3D object shapes. To overcome these limitations, researchers have proposed new generative models that represent 3D scenes as neural radiance fields. These models allow for explicit control of viewpoint and preserve 3D consistency during image synthesis. Additionally, they have shown the potential for unsupervised learning of 3D shapes from 2D images. This paper introduces ShadeGAN, a shading-guided generative model that addresses the shape-color ambiguity in previous 3D-aware image synthesis methods. ShadeGAN learns more accurate 3D shapes by explicitly modeling shading and satisfying a multi-lighting constraint. It also incorporates an efficient rendering technique using surface tracking, reducing training and inference time without sacrificing image quality. Experimental results demonstrate the effectiveness of ShadeGAN in synthesizing photorealistic images and improving downstream tasks like 3D shape reconstruction and image relighting.