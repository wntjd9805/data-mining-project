Optimizing cost functions of discrete variables with respect to parameter distributions is a common challenge in machine learning. The use of Monte Carlo sampling to approximate these expectations has been effective but comes with high variance in the estimates. This paper explores different strategies for stochastic gradient estimation with discrete random variables, including the reparameterization trick and the REINFORCE estimator. The authors introduce the Augment-REINFORCE-Merge (ARM) estimator for binary variables and Augment-REINFORCE-Swap (ARS) and Augment-REINFORCE-Swap-Merge (ARSM) estimators for categorical variables, which show promise but underperform compared to a baseline approach. The paper presents a novel derivation of the DisARM/U2G estimator using importance sampling and evaluates its performance against other estimators, showing improved results without increased computation.