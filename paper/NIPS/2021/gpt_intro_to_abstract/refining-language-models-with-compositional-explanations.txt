Recent advancements in neural language models have led to impressive results in natural language processing tasks. However, when these models are applied to different domains, they may suffer from poor performance and unintended biases due to spurious feature patterns learned from upstream datasets. In this paper, we propose a framework called REMOTE that addresses these limitations by soliciting complex and compositional explanations from human annotators and refining the model using broadened coverage during regularization. We extract executable logic rules from human-provided explanations and update the model weights accordingly. Our framework is the first to gather feature-level supervision from complex human explanations and quantifies and regularizes feature interactions. Experimental results demonstrate the effectiveness of our approach in hate speech classification and sentiment analysis tasks, showcasing notable performance improvements and reduction of unintended biases.