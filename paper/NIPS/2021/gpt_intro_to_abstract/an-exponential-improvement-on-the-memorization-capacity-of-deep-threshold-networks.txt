The current paradigm in training neural networks involves training them until they can fit the training dataset perfectly, even to the extent of interpolating randomized labels. This phenomenon of memorization has been of interest for decades, and previous studies have explored the minimum number of neurons needed for memorization. While the question remains open for threshold networks, recent work has proved that (âˆšn) neurons are sufficient for ReLU activated networks. In this paper, we investigate the memorization power of threshold networks and present a new result that shows the exponential improvement in the dependence on distance. We also compare our findings with existing work and provide upper and lower bounds for the number of neurons needed to memorize a dataset. The rest of the paper is structured to provide related works, formal definitions, main results, construction details, and a conclusion.