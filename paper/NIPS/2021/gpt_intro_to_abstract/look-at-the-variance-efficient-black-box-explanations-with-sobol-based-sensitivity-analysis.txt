Deep neural networks are widely used in various domains with significant implications, but their lack of interpretability and explainability poses challenges. While current explainability methods have improved trust and identified biases, they rely on access to internal states and are limited in black-box scenarios. This paper proposes a general method that leverages variance-based sensitivity analysis and Sobol indices to explain predictions from black-box models. The method incorporates perturbation masks and supports standard and real-valued intensity perturbations. It efficiently calculates Sobol indices using Quasi-Monte Carlo sequences and yields high-quality explanations with fewer forward passes. Experimental results demonstrate the effectiveness of the proposed method for image and text classification tasks.