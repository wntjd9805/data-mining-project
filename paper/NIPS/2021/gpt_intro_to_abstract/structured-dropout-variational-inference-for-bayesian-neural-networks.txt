Bayesian Neural Networks (BNNs) offer a probabilistic interpretation for deep learning models by inferring a posterior distribution instead of point estimates, improving model generalization and uncertainty quantification. However, computing the posterior of non-linear BNNs is infeasible, leading to the development of approximate inference methods. This paper proposes a novel structured variational inference framework called Variational Structured Dropout (VSD), which combines the flexibility of Bayesian inference and the inductive bias of Dropout regularization. VSD achieves computational efficiency, proper prior and approximate posterior, expressive approximation, and flexible hierarchical modeling, making it a promising approach for BNNs. Extensive experiments validate the effectiveness of VSD compared to popular variational inference methods.