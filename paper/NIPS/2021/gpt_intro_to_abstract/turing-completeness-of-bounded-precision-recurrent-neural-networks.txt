Symbolic and sub-symbolic processing are two methods of representing and processing information. Combining these capabilities can be achieved by running algorithms on a neural substrate, such as a recurrent neural network (RNN) that can simulate a Universal Turing Machine (UTM). Previous works have shown the possibility of simulating a UTM using an RNN with unbounded-precision neurons. This paper presents an alternative simulation using bounded-precision neurons and introduces the concept of growing memory modules, inspired by biological memory systems. The proposed bounded-precision RNN with growing memory modules can simulate any Turing Machine, providing a practical and biologically inspired approach to combining symbolic and sub-symbolic capabilities. The paper also discusses the relationship between the number of neurons and precision in simulating Turing Machines.