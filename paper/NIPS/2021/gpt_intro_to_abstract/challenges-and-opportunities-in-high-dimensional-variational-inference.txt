In this paper, we address the challenges and complexities of black-box variational inference (BBVI) methods in Bayesian posterior approximation. We highlight the non-trivial interplay between the approximating family, divergence measure, gradient estimators, and stochastic optimizer, particularly in high-dimensional posteriors. While previous research has focused on improving predictive accuracy, accurately summarizing the posterior itself relies on the careful selection of these method components. We argue that the commonly used low-dimensional illustrations do not always translate to higher-dimensional settings and propose a comprehensive framework for evaluating the reliability of BBVI. By examining the pre-asymptotic behavior of the density ratio between the target and approximate distributions, we show that heavy-tailed density ratios exhibit large bias and high variance, impairing the performance of unbiased estimators. We demonstrate the diminishing benefits of heavy-tailed approximations and divergences favoring mass-covering as the dimensionality of the target distribution increases. To validate our framework, we conduct an extensive empirical study using both simulated and real datasets, considering various target distributions and approximating families. Based on our findings, we provide justified recommendations for design choices in different scenarios, including low- to moderate-dimensional and high-dimensional posteriors.