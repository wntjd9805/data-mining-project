The paper addresses the problem of privately learning mixtures of Gaussian distributions under differential privacy constraints. The authors introduce a notion of locally small cover, which allows for approximate differentially private algorithms to learn classes of distributions. They present sample complexity upper bounds for learning mixtures of unbounded d-dimensional axis-aligned Gaussians. The main technique used is a reduction from learning mixtures to list-decodable learning of distributions, where the authors utilize stability-based histograms to achieve approximate differential privacy. The paper also poses open problems related to the sample complexity of learning mixtures of univariate and high-dimensional Gaussians under differential privacy.