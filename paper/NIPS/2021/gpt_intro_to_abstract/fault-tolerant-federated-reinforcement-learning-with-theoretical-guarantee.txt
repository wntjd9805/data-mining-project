Reinforcement learning (RL) has been widely applied to various real-world decision-making problems, but its poor sample efficiency hinders its success in practical applications. Federated Reinforcement Learning (FRL) has emerged as a solution to this challenge by allowing multiple RL agents to collectively build a better decision-making policy without sharing their raw trajectories. However, existing FRL frameworks lack theoretical convergence guarantees and are vulnerable to random failures or adversarial attacks. In this paper, we propose FedPG-BR, the first FRL framework that is theoretically principled and resilient to faults. We provide a theoretical analysis of FedPG-BR's convergence and demonstrate its effectiveness on various RL benchmark tasks.