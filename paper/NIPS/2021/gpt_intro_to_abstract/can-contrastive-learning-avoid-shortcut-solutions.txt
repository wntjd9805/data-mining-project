Representations trained with contrastive learning have shown success in various vision tasks, but there is a concern that models may rely on shortcut solutions that limit generalization. This paper investigates the relationship between contrastive instance discrimination and feature learning, demonstrating that optimizing the InfoNCE loss alone does not prevent shortcut solutions. However, the authors propose a technique called implicit feature modification that encourages the use of multiple input features during instance discrimination. This method reduces feature suppression without trade-offs and improves generalization on downstream tasks. The paper provides analysis on feature suppression, explores the impact of adjusting instance discrimination difficulty, and introduces the implicit feature modification technique as a solution.