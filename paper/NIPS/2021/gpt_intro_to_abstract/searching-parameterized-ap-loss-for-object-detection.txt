Deep neural networks have achieved significant success in object detection, with loss functions playing a crucial role in training these networks. The evaluation of object detection methods typically utilizes the Average Precision (AP) metric, which measures both localization and classification performance. However, existing object detectors employ separate losses for localization and classification tasks, leading to a misalignment between network training and evaluation that can degrade performance. To address this issue, previous approaches have explored hand-crafted losses or manual gradient estimation for the non-differentiable AP metric. However, these methods have limitations in terms of performance. In this paper, we propose the Parameterized AP Loss, which approximates the non-differentiable portions of the AP metric using a family of continuous parameterized functions. We employ a reinforcement-learning-based search process to find the optimal loss function parameters that maximize the AP score on the evaluation set. Experimental results demonstrate that our Parameterized AP Loss consistently outperforms existing delicately designed losses in various object detectors. Our contributions include a unified formula that captures both localization and classification tasks, an automatic search for optimal parameters, and improved performance compared to existing losses.