Models of the environment play a crucial role in reinforcement learning (RL) agents by enabling them to make predictions and enhance action selection, policy learning, and value function learning. In this paper, we explore the idea of augmenting model-learning by promoting joint self-consistency between a learned model and value function. Traditional methods treat the model as a fixed estimate of the environment, while our approach updates both the model and value to be mutually consistent. This fosters flexible information flow between the reward function, transition model, and approximate value and serves as a valuable regularizer. We investigate self-consistency in both tabular and deep RL settings and propose a variant based on a semi-gradient temporal difference objective that accelerates value learning and policy optimization. Through evaluations in Atari, Sokoban, and Go environments, we demonstrate that self-consistency improves sample efficiency. Furthermore, we conduct experiments to gain insights into how the self-consistency update aids learning.