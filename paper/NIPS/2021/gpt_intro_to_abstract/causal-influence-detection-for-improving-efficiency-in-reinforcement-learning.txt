Reinforcement learning (RL) has shown promise in developing versatile artificial agents that can adapt to real-world complexities. However, RL faces challenges in tasks like object manipulation due to the inefficiency of training, which requires extensive interaction. In this paper, we address the issue of sample efficiency in RL by examining the situation-dependent nature of control from a causal inference perspective. We introduce a measure that captures the causal influence of actions on the environment and propose integrating it into RL algorithms for both exploration and learning. Our approach enables agents to prioritize actions with higher predicted causal impact and improve data efficiency. Empirical evaluations in robotic manipulation environments demonstrate the effectiveness of our approach.