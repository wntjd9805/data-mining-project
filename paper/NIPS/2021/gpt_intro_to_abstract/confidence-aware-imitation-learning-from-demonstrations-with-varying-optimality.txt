In this paper, we explore an imitation learning setting where a well-performing policy is learned from a mixture of demonstrations with varying optimality. Unlike standard imitation learning, where the demonstrations are optimal, this setting allows for a larger and more diverse source of data. However, the use of suboptimal demonstrations presents challenges in selecting useful demonstrations and filtering out negative effects. To address these challenges, we propose using a measure of confidence to indicate the likelihood that a demonstration is optimal. By reweighting demonstrations with a confidence score, we can learn from suboptimal demonstrations while avoiding the negative effects of malicious ones. We introduce a new algorithm, called Confidence-Aware Imitation Learning (CAIL), which jointly learns a well-performing policy and the confidence for each demonstration. Our algorithm utilizes a bi-level optimization framework and demonstrates convergence to optimal confidence assignments. Experimental results show that the learned confidence accurately characterizes the optimality of demonstrations and that the learned policy outperforms other imitation learning approaches.