Neural networks optimization by gradient descent poses a challenge in terms of quantitative theoretical description, as current understanding is limited to qualitative explanations. This paper focuses on the theoretical analysis of gradient descent in large neural networks and large datasets, using tools from various mathematical fields such as partial differential equations, kernel methods, and spin glass theory. The authors investigate the explicit leading terms in the long-term evolution of the loss under gradient descent and propose power law formulas for loss evolution. They argue that the exponents in these formulas are determined by the input dimension and the smoothness of the activation and target functions. The paper presents experimental results and discusses different scenarios, including approximation by shallow ReLU networks, different activation functions, and deep networks.