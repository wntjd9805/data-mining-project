This paper addresses the problem of nonconvex, gradient-free optimization in situations where it is not possible to exhaustively explore the entire domain. The focus is on understanding how Gaussian Process (GP) optimization performs with a limited number of function evaluations relative to the domain size. The paper introduces two optimization algorithms, similar to expected improvement (EI) and upper confidence bound (UCB) methods, and derives problem-independent regret bounds for both discrete and continuous domains. The results show that the proposed bounds are non-vacuous even with limited exploration and scale better with the size of the function domain compared to previous work. The goal is to gain a better understanding of the performance of GP optimization in this setting rather than proposing new algorithms.