Deep learning (DL) methods have reached a plateau in their ability to model all requirements in new applications, hindered by traditional objective functions. This limitation slows down the democratization of AI, particularly in domains such as healthcare where data is often highly imbalanced. In these cases, accuracy is an inappropriate metric, and area under the curve (AUC), including AUROC and AUPRC, is more suitable. While previous research has focused on maximizing AUROC, optimizing AUPRC has not been thoroughly explored. This paper aims to address this gap by proposing stochastic optimization algorithms for DL that maximize AUPRC. The challenges lie in the complexity of AUPRC computation and its non-convex nature. The proposed method uses a surrogate loss to cast the objective as a sum of non-convex compositional functions. By leveraging recent advances in stochastic compositional optimization, both adaptive (Adam-style) and non-adaptive (SGD-style) algorithms are developed, with provable convergence under mild conditions. Empirical studies demonstrate the superior performance of the proposed method in terms of AUPRC, particularly in highly imbalanced datasets.