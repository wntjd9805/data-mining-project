The Gaussian mechanism is widely used in differentially private learning algorithms, but it poses challenges due to its continuous nature and limitations in distributed learning settings. To address these issues, the binomial and discrete Gaussian mechanisms were introduced, but they also have drawbacks. In this paper, we propose the multi-dimensional Skellam mechanism, which adds noise based on the difference of two independent Poisson random variables. The Skellam noise is discrete, can be easily sampled, and is closed under summation. We analyze the privacy guarantees and performance of the Skellam mechanism, demonstrating that it achieves comparable results to the Gaussian mechanism in a federated learning setting while preserving privacy and communication budgets. Our method is implemented in TensorFlow Privacy and TensorFlow Federated.