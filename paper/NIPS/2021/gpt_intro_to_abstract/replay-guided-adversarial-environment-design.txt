Deep reinforcement learning (RL) approaches have demonstrated success in challenging domains, but they often fail to transfer to slightly different environments. To address this, agents can benefit from automatically adapting the distribution over environment variations during training. Two recent methods, PAIRED and PLR, provide agent-adaptive mechanisms for selecting levels on which to train RL agents. PAIRED employs an adversarial approach to environment design, while PLR selectively replays existing levels to maximize learning potential. In this paper, we argue that PLR is an effective form of Unsupervised Environment Design (UED) and propose a new algorithm called Dual Curriculum Design (DCD) that encompasses PAIRED and PLR. We provide the first theoretical characterization of PLR and introduce a variant called PLR⊥, which achieves provable robustness. We also present REPAIRED, a method that combines PLR⊥ and PAIRED. Experimental results demonstrate the effectiveness of our methods in improving generalization in maze and car racing domains.