In this paper, we address the problem of training direct Average Precision (AP) with stochastic gradient-based optimization in computer vision tasks. The AP loss is non-differentiable, which poses a challenge for optimization. We propose a new surrogate loss that provides an upper bound for AP and ensures robust training. Additionally, we introduce a training objective that improves the non-decomposability of AP. We validate our method through experimental evaluation and show that it outperforms state-of-the-art methods in image retrieval tasks. Our approach does not require memory or computation overhead and remains competitive even with small batches.