This paper discusses the importance of training deep neural networks using stochastic gradient descent (SGD) with a finite learning rate (LR) for optimal performance. It examines the implicit bias of SGD towards good generalization and explores the use of stochastic differential equations (SDEs) to model the evolution of network parameters. The paper introduces a new numerical method, Stochastic Variance Amplified Gradient (SVAG), to test the closeness between SGD and its corresponding SDE. The paper also provides empirical testing and theoretical insights into the failure of the linear scaling rule (LSR) at large LR/batch sizes. Overall, the paper aims to shed light on the SDE approximation and its implications for understanding the behavior of SGD in deep learning.