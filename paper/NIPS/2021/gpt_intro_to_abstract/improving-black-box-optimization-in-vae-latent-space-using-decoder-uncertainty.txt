We address the problem of optimizing an expensive black-box objective function in a high-dimensional discrete space, such as finding new molecules for drug design or generating computer programs. Variational autoencoders (VAEs) have been successful in modeling various discrete data modalities by learning a lower-dimensional continuous representation in latent space. However, previous methods have struggled with exploring areas in latent space without available training data, resulting in invalid or low-quality decoded objects. In this paper, we propose a method that leverages the uncertainty of the decoder network to guide the optimization process in latent space, improving the trade-off between black-box objective values and validity of generated objects. We introduce an algorithm to quantify uncertainty in high-dimensional discrete data and demonstrate how it can be incorporated into various optimization frameworks. We validate our approach in digit generation, arithmetic expressions approximation, and molecule generation tasks using different decoder architectures.