The problem of domain adaptation (DA) arises when the training and test data distributions are different, making standard supervised learning assumptions invalid. This paper focuses on unsupervised DA (UDA), where no labeled information from the target domain is available. Previous works have proposed algorithms for UDA based on minimizing the error on the source domain and a divergence measure between the marginal feature distributions of the source and target domains. However, recent evidence suggests that these methods can fail in certain scenarios. To further understand the limitations of UDA, this paper proposes a lower bound on the target domain error, providing a necessary condition for successful learning. The authors also explore different data distributions where UDA can succeed, fail, or have equal likelihood of success and failure. Additionally, the paper introduces novel data poisoning attacks to evaluate the vulnerability of UDA methods to adversarial settings. The results show that UDA methods can dramatically fail in the presence of poisoned data, highlighting the need for future methods to be evaluated in adversarial settings.