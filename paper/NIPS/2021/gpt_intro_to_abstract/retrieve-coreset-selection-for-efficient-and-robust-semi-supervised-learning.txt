Deep learning algorithms have achieved remarkable success in various tasks, but their performance heavily relies on the availability of labeled data. This paper addresses the challenges of costly labeled data creation and the limitations of current state-of-the-art semi-supervised learning (SSL) algorithms. The authors propose RETRIEVE, a coreset selection framework that aims to train SSL models more efficiently and robustly. RETRIEVE selects an unlabeled data coreset that minimizes the model loss on the labeled dataset, leading to faster convergence and improved performance. The authors demonstrate the effectiveness of RETRIEVE in traditional SSL and robust SSL scenarios, achieving significant speedups without compromising accuracy compared to existing methods. Implementation techniques, empirical results, and comparisons with baselines are presented to illustrate the advantages of RETRIEVE.