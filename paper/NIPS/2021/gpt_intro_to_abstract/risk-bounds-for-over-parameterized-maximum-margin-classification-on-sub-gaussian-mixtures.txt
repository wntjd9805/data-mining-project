In modern machine learning, complex models like deep neural networks are popular for fitting noisy training data while achieving small test errors. Overfitting is not unique to deep learning and has been studied in other models like kernel methods and linear models. Recent works have explored how over-parameterization can lead to small population risk, particularly in linear regression and ridge regression. However, there are still limitations in understanding overfitting in linear classification, especially in the impact of the data covariance matrix on the risk. In this paper, we investigate benign overfitting in a general sub-Gaussian mixture model with both isotropic and anisotropic settings. We prove a risk bound for the maximum margin classifier and show its equivalence to the minimum norm interpolator. Our results provide insights into the influence of the eigenvalues of the covariance matrix on the classifier and offer a tighter and more general risk bound than existing results. The contributions of our paper include establishing a tight population risk bound, analyzing lower bounds achieved by the classifier, and introducing intermediate results of independent interest.