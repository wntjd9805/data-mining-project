Privacy concerns in statistics and machine learning have become increasingly prevalent, as many estimators and trained models expose sensitive information about the data set. These privacy violations have been demonstrated through various attacks, such as reconstruction attacks, membership-inference attacks, and unwanted memorization of training data. To overcome these concerns and enable analysis of sensitive data, it is crucial to develop statistical estimators and machine learning algorithms that not only make accurate inferences about the population but also protect the privacy of individual contributors. This work focuses on studying statistical estimators that satisfy a condition known as differential privacy, which ensures that no attacker can learn significantly more about an individual than they would have without their contribution to the data set. Differential privacy is widely applicable in statistics and machine learning tasks, with its adoption by major companies such as Google, Apple, Facebook, LinkedIn, and the U.S. Census Bureau.