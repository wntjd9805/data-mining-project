Bayesian Inference often requires approximations to compute or sample from the posterior density. Variational Inference is a popular category of approximation methods that use a simpler variational family to approximate the posterior density. However, the exclusive Kullback-Leibler divergence, commonly used to measure dissimilarity, has limitations. As a result, the α-divergence and Rényi’s α-divergence have gained attention as alternative measures. This paper introduces the (α, Γ)-descent algorithm, which approximates the posterior density using a mixture model and optimizes the mixture weights through α-divergence minimization. The algorithm expands the parametric variational family, capturing complex and multimodal posterior densities. The authors analyze the Power Descent algorithm and prove its convergence when α < 1. They also explore extensions to the case α = 1 and investigate the connections between Power Descent and Entropic Mirror Descent. Numerical experiments are conducted to compare the behavior of Power Descent and the newly introduced Rényi Descent algorithm.