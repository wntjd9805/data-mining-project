Computational level theories of behavior aim to understand why a system behaves the way it does and what its computational goals are. These goals are often based on the reward hypothesis, which assumes that goals can be defined as the maximization of expected value. In order to understand human sensorimotor behavior, it is important to quantify its goals and purposes in terms of costs and benefits. Stochastic optimal control provides a framework for formulating behavioral goals and solving optimization problems. However, including sensory feedback in stochastic optimal control poses a more complex problem that can be formulated as a partially observable Markov decision process. This paper introduces a probabilistic formulation of inverse optimal feedback control under signal-dependent noise to recover the cost function underlying an agent's behavior from observed data. The formulation allows for the recovery of the agent's belief and the experimenter's uncertainty about the inferred belief.