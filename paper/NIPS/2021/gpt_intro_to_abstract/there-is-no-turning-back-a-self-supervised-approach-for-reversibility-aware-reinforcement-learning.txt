This paper addresses the problem of estimating the reversibility of actions in the Reinforcement Learning (RL) context. The authors argue that considering irreversibility can lead to safer decision-making and more efficient exploration in environments with intrinsic risk factors. They propose a method of learning the temporal order of events directly from the agents' experience and classify transitions with high confidence as irreversible. The contributions of this work include formalizing the link between reversibility and precedence estimation, proposing a practical algorithm for learning temporal order, and introducing exploration and control strategies that incorporate reversibility. The effectiveness of these strategies is demonstrated in tasks such as the Sokoban puzzle game.