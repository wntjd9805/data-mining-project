Federated learning enables distributed clients to solve a learning objective on sensitive data without sharing it with a coordinating server. However, fully global federated training may not always be ideal due to heterogeneity in clients' data distributions and privacy constraints. To address this, partially local federated learning is explored, where models are partitioned into global and local parameters. This approach improves robustness to client data heterogeneity, communication cost, and enables training on sensitive user-specific parameters. In this work, a model-agnostic framework called FEDRECON is proposed, which combines federated training of global parameters with reconstruction of local parameters. The proposed method shows improved performance in collaborative filtering and next word prediction applications, enables fast adaptation to clients' personal data, and matches the performance of other federated personalization techniques with less communication. The successful deployment of this approach at scale for collaborative filtering in a real-world mobile keyboard application is also described.