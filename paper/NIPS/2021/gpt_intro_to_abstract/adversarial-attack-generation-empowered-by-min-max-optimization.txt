Training machine learning models to be robust against adversaries is a challenging problem, particularly for deep neural networks. Adversarial training (AT), a form of min-max training, has been successful in achieving state-of-the-art defense performance against input perturbations. However, little research has explored the potential of different min-max formulations and optimization techniques in the context of adversarial attack generation. This paper introduces a novel min-max framework that demonstrates substantial performance gains and the ability to interpret risks, addressing various adversarial attack tasks. The proposed framework, called alternating one-step projected gradient descent-ascent (APGDA), shows improved convergence rates and outperforms conventional methods on CIFAR-10. Additionally, the framework allows for weighted assessment of importance in mixed learning tasks and can be adapted for defense purposes.