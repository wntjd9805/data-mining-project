Convolution is widely used in neural networks for visual recognition but its stationarity limits its expressivity. Dynamic convolution and self-attention have emerged as alternatives, but they still have limitations in learning relational patterns in videos. In this paper, we propose a relational dynamic feature transform called relational self-attention (RSA) that leverages rich structures of spatio-temporal relations in videos. By combining relational components with dynamic kernels and context features, RSA captures visual appearance and spatio-temporal motion dynamics effectively. We provide a unified analysis of recent dynamic feature transforms and demonstrate the superior performance of RSA compared to convolution and self-attention counterparts on various video understanding tasks.