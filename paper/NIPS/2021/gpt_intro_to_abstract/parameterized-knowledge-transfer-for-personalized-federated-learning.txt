Federated Learning (FL) is a collaborative training paradigm that allows multiple clients to train a shared machine learning model without sharing their private data. However, a challenge in FL is the statistic heterogeneity in users' local datasets, which leads to poor generalization of the trained global model. To address this issue, personalized federated learning (pFL) has been proposed, which employs personalized models for each client. Existing pFL methods require clients to have identical model structures and sizes, limiting their applicability in practical scenarios where clients may have unique models. In this paper, we propose a novel training framework called KT-pFL that accommodates heterogeneous model structures and achieves personalized knowledge transfer in each FL training round. We introduce a knowledge coefficient matrix to facilitate collaboration between clients with similar data distributions. Experimental results demonstrate that KT-pFL improves training efficiency and reduces communication overhead compared to traditional pFL methods.