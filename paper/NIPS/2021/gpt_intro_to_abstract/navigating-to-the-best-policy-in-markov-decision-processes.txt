The paper explores the problem of best policy identification (BPI) for infinite-horizon discounted Markov Decision Processes (MDPs). The authors focus on the online setting where observations are obtained through sequential actions and transitions in the MDP. They propose the MDP-NaS algorithm, which provides instance-dependent bounds on the sample complexity in the asymptotic regime. The algorithm incorporates a new sampling rule that balances exploration and exploitation, and the paper introduces a stopping rule based on the Generalized Likelihood Ratio test. The authors show that this stopping rule outperforms existing methods and improves the sample complexity bound.