Deep neural networks (DNNs) have become the go-to tool for various applications in computer vision and natural language processing, but their performance comes at a cost of extensive infrastructure requirements. The compression of DNNs has thus become a crucial area of research, with techniques such as quantization, knowledge distillation, and pruning being employed. Pruning, in particular, is widely studied and popular for improving DNN resource efficiency. While unstructured pruning achieves high compression ratios, it fails to reduce computational footprint in modern hardware. In contrast, structured pruning methods, such as block or filter pruning, are more hardware-friendly but often sacrifice accuracy for high compression ratios. This paper aims to address three remaining questions in the field: how to rank different sparsity masks, whether fine-grained sparsity masks can accelerate training, and if it is possible to change the type of sparsity mask structure without re-training.