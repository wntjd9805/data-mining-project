Online linear optimization is a challenging problem in which we repeatedly play against an opponent who selects cost vectors based on our previous actions. The goal is to minimize regret, which measures the difference between our total cost and the cost of the optimal action in hindsight. In this paper, we explore algorithms for online linear optimization and focus on the case where the cost vectors are drawn independently from a fixed distribution. We introduce the concept of Follow-the-Leader (FTL) algorithm, which achieves a pseudo-regret bound against easy opponents. However, FTL does not guarantee good performance against adversarial opponents, and we aim to design a universal algorithm that performs well in both scenarios. We present the Online Lazy Gradient Descent algorithm, which achieves O(N) regret against any opponent and O(log N) regret against independent and identically distributed (i.i.d) opponents. Our main contributions include formalizing assumptions about the domain and cost vectors, proving the regret bounds, and providing theoretical analysis using differential geometry.