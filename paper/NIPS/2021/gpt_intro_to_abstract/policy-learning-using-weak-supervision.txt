Recent breakthroughs in policy learning have opened up possibilities for applying reinforcement learning and behavioral cloning in practical fields such as robotics and self-driving. However, most existing approaches rely on high-quality supervision signals, such as reward or expert demonstrations, which are often difficult or costly to obtain. The outputs of reward functions are also subject to various sources of randomness, including biases and noise from sensors and inconsistencies in human feedback. Learning from weak supervision signals, such as noisy rewards or low-quality demonstrations, remains a significant challenge in policy learning. While some works have explored these issues separately, a unified solution for robust policy learning in imperfect situations is lacking. In this paper, we propose a meta-framework called weakly supervised policy learning and introduce a principled solution called PeerPL. Inspired by the idea of peer loss, PeerPL treats weak supervision as noisy information and evaluates the learning agent's policy based on correlated agreement with this imperfect supervision. Our approach punishes over-agreement to prevent overfitting and does not require prior knowledge of the corruption level in supervision signals. The contributions of this paper include the formulation of weakly supervised policy learning problems, the introduction of PeerPL as a method for policy evaluation, theoretical guarantees of optimal policy recovery, and strong evidence of improved performance compared to state-of-the-art solutions. Code for PeerPL is available online.