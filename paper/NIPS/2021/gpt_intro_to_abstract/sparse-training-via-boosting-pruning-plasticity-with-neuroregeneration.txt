Neural network pruning is a commonly used technique to reduce the size and computational costs of neural network architectures. Post-training and before-training pruning have gained popularity, but they come with high training costs. In this paper, we focus on during-training pruning and investigate its potential for improving performance. We introduce the concept of pruning plasticity and propose a method called GraNet that incorporates zero-cost neuroregeneration to enhance pruning plasticity. Our experiments demonstrate that GraNet achieves state-of-the-art performance in both dense-to-sparse and sparse-to-sparse training scenarios, surpassing existing methods. Additionally, we find that the subnetworks learned by GraNet are more accurate than those learned by other gradual pruning methods.