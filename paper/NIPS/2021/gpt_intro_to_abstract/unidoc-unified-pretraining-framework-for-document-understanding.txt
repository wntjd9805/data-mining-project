The field of document intelligence involves extracting and understanding information from complex documents that contain various elements and layouts. Traditional pretraining methods for document understanding, inspired by the success of BERT, have limitations in capturing the hierarchical structure, multimodal content, and spatial layout of documents. In this paper, we propose a unified pretraining framework called UDoc that integrates visual and textual information using a transformer architecture. Our model addresses the limitations of current pretraining setups by incorporating image information, encoding sentences hierarchically, and combining convolution and self-attention mechanisms. We introduce novel pretraining tasks, such as Masked Sentence Modeling and Visual Contrastive Learning, to effectively capture contextual information and cross-modal correlations. Through extensive experiments and analyses, we demonstrate the effectiveness of UDoc for various downstream tasks in document understanding.