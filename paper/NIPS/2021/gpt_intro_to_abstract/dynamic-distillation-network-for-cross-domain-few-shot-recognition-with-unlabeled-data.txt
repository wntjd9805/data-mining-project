This paper introduces a method for few-shot learning across extreme domain differences. The method utilizes dynamic distillation and combines labeled source data and unlabeled target data to learn a better representation for few-shot evaluation on the target domain. The proposed method outperforms the current state of the art in the BSCD-FSL benchmark, achieving higher classification accuracies for both 1-shot and 5-shot scenarios. Additionally, the method also shows superior performance for in-domain few-shot classification on miniImageNet and tieredImageNet datasets.