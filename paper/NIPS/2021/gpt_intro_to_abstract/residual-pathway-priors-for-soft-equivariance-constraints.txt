The use of deep learning in various domains has necessitated the search for appropriate inductive biases to match models to structured data. These biases, such as recurrence, local connectivity, equivariance, or differential equations, improve generalization by reducing the hypotheses explored. However, hard coding these restrictions may not accurately reflect the complexities of the real world. Symmetries, in particular, can be delicate and easily disrupted by small perturbations. To address this, the authors propose Residual Pathway Priors (RPPs), a method for converting hard architectural constraints into soft priors. RPPs have a prior bias towards equivariant solutions but are not limited to equivariance, allowing for the handling of problems with approximate symmetries. The authors demonstrate the effectiveness of RPPs in various domains and show improved performance in model-free reinforcement learning tasks. An implementation of RPPs in PyTorch is also provided.