The rise of deep learning has spurred research on the theoretical aspects of deep neural networks. One key question is whether it is possible to design efficient learning algorithms for neural networks that are both time-efficient and sample-efficient. In this paper, we focus on the problem of learning a depth-2 feedforward neural network with ReLU activations, and aim to develop polynomial-time algorithms for this task. We address the challenges introduced by the presence of bias terms and propose techniques based on tensor decomposition to recover the network parameters. We also discuss the limitations of our approach and present robust guarantees for tensor decompositions. Overall, our work contributes to the understanding of learning algorithms for neural networks with bias terms, paving the way for efficient solutions in practical settings.