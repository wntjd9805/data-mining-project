Deep neural networks have achieved impressive results in various supervised learning applications, but annotating large-scale datasets for training these models can be laborious and costly. To address this issue, domain adaptation methods have been developed to transfer knowledge from a well-studied source domain to an unfamiliar target domain. Traditional domain adaptation requires a related source domain that shares the same label space as the target domain, but in real-life scenarios, the target label space may only be a subset of the source label space. This mismatch presents a challenge in aligning the two domains, as aligning all source domains with the small target domain could result in negative transfer. To alleviate negative transfer, partial domain adaptation algorithms have been proposed to reduce the influence of irrelevant categories. However, these methods discard all irrelevant classes, potentially losing valuable information. In this paper, we propose an implicit semantic response alignment module for existing partial domain adaptation models, which extracts implicit semantics from all categories, including the shared and unshared ones, to reduce distribution discrepancy on the semantic level. Our method exploits the relationships among different categories by extracting implicit semantics from visual features and uses an attention-based weighting schema to align the source and target data distribution based on the implicit semantic topics shared between the two domains. We demonstrate the effectiveness of our method on various benchmarks and provide detailed explorations of our proposed method.