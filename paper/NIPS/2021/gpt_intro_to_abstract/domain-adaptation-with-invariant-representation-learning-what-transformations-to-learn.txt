Unsupervised domain adaptation (UDA) is a common setting in supervised learning where labeled training and unlabeled test data come from different distributions. The main challenge in domain adaptation is to use the labeled source domain observations to learn a predictor that performs well in the target domain. This paper focuses on the problem of conditional shift, where both the conditional and marginal distributions change across domains. The paper proposes the use of two separate encoding functions to infer a latent representation that is meaningful for prediction in the target domain. The paper also introduces an efficient implementation of these encoding functions and a principled approach to ensure the useful information in the latent representation. Empirical evaluation is provided to demonstrate the effectiveness of the proposed method.