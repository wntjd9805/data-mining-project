This paper addresses the limitations of current machine learning methods by proposing a novel framework called Abductive Learning (ABL) that integrates machine learning models with first-order logical reasoning models. By allowing full-featured logical reasoning, ABL can consult symbolic background knowledge bases and reduce the need for large amounts of labeled data. To improve the quality of abduced labels, the paper introduces a similarity-based consistency measure that takes into account the similarities and dissimilarities between samples. The proposed ABductive Learning with Similarity (ABLSim) approach is evaluated on four neuro-symbolic tasks and is shown to generate higher quality labels and accelerate model training compared to other methods. ABLSim also demonstrates comparable results when facing the challenge of abduction with a reduced knowledge base.