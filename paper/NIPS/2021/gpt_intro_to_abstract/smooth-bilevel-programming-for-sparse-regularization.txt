Regularized empirical risk minimization is a widely used technique in supervised learning, particularly for linear models. In this paper, we focus on sparsity enforcing penalties, such as the Lasso, group-Lasso, and trace norm regularizers. These regularizers can be expressed as an infimum of quadratic functions, particularly in the case of block-separable functionals. However, solving sparsity regularized problems can be challenging due to the non-smoothness of the regularizer. To address this, we propose a re-parameterization combined with a bi-level programming approach to transform the problem into a smooth program that can be efficiently solved using optimization techniques like quasi-Newton. We study the theoretical and algorithmic implications of this approach and compare it with proximal gradient methods. Our contributions include a new versatile algorithm for sparse regularization, theoretical analysis of the method, and a comprehensive numerical study that demonstrates its effectiveness compared to other popular algorithms.