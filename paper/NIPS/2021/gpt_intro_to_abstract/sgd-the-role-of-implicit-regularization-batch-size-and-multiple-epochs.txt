In this paper, we address the problem of stochastic optimization and its application to convex and non-convex learning problems. We explore the effectiveness of algorithms such as Stochastic Gradient Descent (SGD), Gradient Descent (GD), and Regularized Empirical Risk Minimization (RERM) in solving the optimization problem. We also investigate the concept of implicit regularization in SGD and its relation to RERM. Furthermore, we study the performance of SGD compared to GD and RERM in terms of sample complexity and the number of iterations. We demonstrate scenarios where SGD outperforms GD and RERM, and provide insights into the behavior of multi-epoch SGD. Lastly, we discuss the implications of our findings in the context of deep learning and propose further research directions.