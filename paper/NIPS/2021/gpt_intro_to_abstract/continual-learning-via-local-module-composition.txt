The goal of continual learning (CL) is to learn efficiently from a non-stationary stream of tasks without forgetting previous tasks. Parameter sharing and modular learning are two approaches that aim to balance knowledge retention and expansion. However, existing modular methods face challenges in adding new modules and retrieving task-specific structural knowledge. In this paper, we propose a local module composer (LMC) approach that equips each module with a local structural component to self-determine its relevance for a given input. We demonstrate the performance and versatility of LMC in various experiments and highlight its advantages, including not requiring task IDs at test time, balancing parameter sharing, instantiating a sub-linear number of modules, enabling out-of-distribution (OOD) generalization, and allowing the combination of independently trained models without fine-tuning.