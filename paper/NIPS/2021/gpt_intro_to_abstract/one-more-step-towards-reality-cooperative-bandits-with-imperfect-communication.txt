The cooperative multi-armed bandit problem involves a group of agents collectively solving a multi-armed bandit while communicating with one another. In real-world environments, communication networks are rarely static and messages can be lost or corrupted. This paper explores the multi-agent bandit problem under real-world communication scenarios, including time-varying communication networks, delays in message delivery, and corrupted messages. The authors propose robust communication learning algorithms for each scenario and analyze their regret rates. Experimental results show improvements in the case of perfect communication and lower bounds on group regret for algorithms based on message-passing.