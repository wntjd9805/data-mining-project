In this paper, we introduce MERLOT, a model that learns commonsense representations of multimodal events by self-supervised pretraining over unlabelled YouTube videos. We train MERLOT to match individual video frames with contextualized representations of the associated transcripts and contextualize those representations over time. Our model achieves state-of-the-art performance on various video tasks and demonstrates a strong understanding of everyday events and situations. We also show that MERLOT outperforms baselines on tasks requiring long-term temporal coherence and that pretraining on diverse videos improves downstream performance. Overall, our contributions include MERLOT, the YT-Temporal-180M corpus, and experiments demonstrating MERLOT's strong performance across tasks and modalities. Code, data, and models are available for public research use.