Machine learning algorithms are widely used and performance guarantees are desirable for their deployment. Traditional performance guarantees often take the form of generalization bounds, which bound the expected risk of algorithms. However, interpreting these bounds can be ambiguous, and it is not always the case that more data will decrease the expected risk. Non-monotonic behavior of risk curves, such as peaking and dipping, has been observed in previous works. This paper presents an algorithm that is both consistent and risk-monotonic, answering questions raised in previous research. The algorithm is a "wrapper" that takes any base learning algorithm and produces a risk-monotonic version with similar excess risk rates. The results hold under a relaxed assumption on the loss process and are derived using concentration inequalities based on the FREEGRAD algorithm. This approach allows for the design of new concentration inequalities and enables fast excess risk rates under the Bernstein condition. Overall, this work contributes to the understanding of generalization and provides insights into achieving risk monotonicity in machine learning algorithms.