Recent studies have shown that larger Transformer models tend to perform better in terms of data and model size. However, these larger models are slower to train and evaluate and require complex engineering to facilitate communication between workers. To address these challenges, researchers have studied Mixtures-of-Experts (MoE) models, which route computation through a subset of the model's weights. In this paper, we propose a simple and efficient routing strategy based on hashing input tokens for Transformers used in Natural Language Processing (NLP). We compare our approach to other routing strategies, such as Switch Transformers and BASE Layers, and demonstrate its effectiveness on multiple datasets. Our approach requires no extra parameters or changes to the objective function or assignment algorithm, making it robust, fast, and easy to implement. We provide detailed analysis to explain why our method works and recommend it for training very large models with limited compute budget where parameter choices are restricted.