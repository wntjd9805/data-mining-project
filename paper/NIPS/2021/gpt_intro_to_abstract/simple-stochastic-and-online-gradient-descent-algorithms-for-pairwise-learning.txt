Pairwise learning tasks are fundamental in many important learning tasks, such as AUC maximization, metric learning, and minimum error entropy principle. In this paper, we focus on the problem of designing efficient online gradient descent algorithms for pairwise learning. We propose simple SGD and OGD algorithms that pair the current instance with the previous instance, utilizing a First-In-First-Out (FIFO) buffering strategy. We establish stability results for these algorithms and derive optimal excess generalization bounds in both convex and nonconvex scenarios. Additionally, we develop a localization version of our algorithms under (ε, δ)-differential privacy constraints and derive optimal utility bounds. Experimental results validate the effectiveness of our proposed algorithms.