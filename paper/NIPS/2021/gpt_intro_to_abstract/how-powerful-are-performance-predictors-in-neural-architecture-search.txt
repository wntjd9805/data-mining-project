Neural architecture search (NAS) is a popular area of machine learning that aims to automate the process of developing neural architectures for a given dataset. Various NAS techniques have been proposed, but there is a lack of understanding of how these methods compare to each other. In this study, we conduct a large-scale investigation of 31 predictors across four search spaces and four datasets. We compare model-based methods, learning curve extrapolation methods, zero-cost methods, and weight sharing methods, evaluating their performance under different initialization time and query time budgets. We find that predictors from different families can be combined to achieve higher performance. Our findings provide recommendations for the best predictors to use in NAS under different runtime constraints. We also release a library of performance predictors and ensure reproducibility of our results.