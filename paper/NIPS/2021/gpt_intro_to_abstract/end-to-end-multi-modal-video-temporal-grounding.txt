This paper introduces a multi-modal framework for text-guided video temporal grounding, which involves localizing the starting and ending time of a video segment corresponding to a text query. The proposed framework utilizes RGB images, optical flow, and depth maps as visual cues to improve the recognition of objects, actions, and scenes in videos with complex backgrounds. The authors present a fusion scheme with co-attentional transformers to dynamically combine features from different modalities and an intra-modal module for self-supervised contrastive learning to enhance feature representations. Extensive experiments demonstrate the effectiveness of the framework for video temporal grounding, showing superior performance compared to existing methods. The contributions of the work include the multi-modal framework, the dynamic fusion mechanism, and the application of self-supervised contrastive learning.