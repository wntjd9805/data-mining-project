Variational auto-encoders (VAEs) have been widely used in various domains of unsupervised learning, such as density estimation, image generation, text generation, music generation, and recommendation systems. VAEs extend deterministic auto-encoders to probabilistic generative modeling by parameterizing an approximate posterior distribution over latent variables. However, the encoder of a fitted VAE tends to map an image and its semantics-preserving transformation to different parts of the latent space, leading to inconsistent representations. In this paper, we propose a method called consistency-regularized VAE (CR-VAE) to enforce consistency in VAEs. We maximize the likelihood of images while minimizing the Kullback-Leibler divergence between the approximate posterior distributions when conditioning on the image and its transformation. Experimental results show that CR-VAEs yield better representations and improve generalization performance compared to their base VAEs.