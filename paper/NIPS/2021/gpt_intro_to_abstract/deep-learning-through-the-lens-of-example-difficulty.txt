This paper addresses the limitations of existing notions of example difficulty in understanding deep learning models. The authors propose a new measure of example difficulty called prediction depth, which takes into account the processing of data within a converged model. They also introduce three types of difficulty based on high-level concepts related to example difficulty. The paper presents empirical findings that support the effectiveness of prediction depth in capturing example difficulty and its relationship with the consistency and accuracy of predictions. Additionally, the authors show how prediction depth can unify various phenomena in deep learning. By conducting experiments on different architectures and datasets, they demonstrate the robustness of their results.