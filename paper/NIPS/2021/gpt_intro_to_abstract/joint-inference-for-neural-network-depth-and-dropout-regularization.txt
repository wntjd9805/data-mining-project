Both dropout regularization and network depth play crucial roles in the success of deep neural networks (DNNs). Dropout regularization sparsiÔ¨Åes a neural network's structure by randomly deleting neurons and their connections during training to prevent overfitting. However, deep models tend to be overly confident with their predictions without uncertainty calibration. Probabilistic inference approaches for network structure selection propose to reduce training cost and build smaller models, but they cannot scale the network structure to accommodate incrementally available data. In this paper, we propose a Bayesian model selection framework that jointly infers network depth and performs dropout regularization. We model the network depth as a stochastic process using the beta process, allowing it to go to infinity theoretically. The framework provides well-calibrated uncertainty estimates for predictions and achieves superior performance compared to state-of-the-art methods.