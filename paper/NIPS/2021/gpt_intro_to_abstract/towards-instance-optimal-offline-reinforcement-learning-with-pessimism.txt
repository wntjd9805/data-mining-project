In offline reinforcement learning, the goal is to learn a reward-maximizing policy in an unknown environment using historical data. Unlike online RL, offline RL is preferred when online interactions are expensive or unethical. Existing literature on offline RL often relies on data-coverage assumptions to provide performance bounds, but the empirical performance of offline RL algorithms suggests that these bounds may not capture the true difficulty of different decision processes and behavior policies. In this work, we aim to derive provably efficient bounds that are adaptive to individual instances and require minimal assumptions. We specifically analyze the adaptive pessimistic value iteration (APVI) algorithm and provide a near-optimal adaptive bound under weak assumptions. We also introduce an intrinsic offline learning bound that characterizes the system structures of specific problems and holds even for instances that do not satisfy standard data-coverage assumptions. Additionally, we present a modified AVPI and obtain an adaptive bound that characterizes the suboptimality gap independent of the behavior policy. These results contribute to the understanding and improvement of offline RL.