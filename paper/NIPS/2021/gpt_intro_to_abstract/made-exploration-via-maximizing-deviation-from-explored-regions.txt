Online reinforcement learning (RL) is a valuable tool for agents to learn tasks in the absence of expert demonstrations. Balancing exploration and exploitation is crucial for RL agents to learn a satisfactory policy. In environments with sparse rewards, intrinsic motivation is often used to facilitate exploration by adding an intrinsic reward to the extrinsic reward. Provable exploration methods construct upper confidence bound (UCB) bonuses based on value estimate uncertainty. However, computing UCB bonuses in non-linear function approximation and tabular settings can be challenging. Various approaches have been proposed to design intrinsic rewards, but they suffer from detachment, derailment, local optima, and catastrophic forgetting issues. In this paper, we propose a new algorithm for exploration by maximizing deviation from explored regions, resulting in improved performance. We present the algorithm and show that it can be easily added to any RL algorithm. Our empirical studies demonstrate the effectiveness of our approach and its ability to improve the optimization rate in policy gradient methods. Additionally, our experiments on MiniGrid and DeepMind Control Suite show that our algorithm outperforms existing baselines in terms of sample efficiency and returns.