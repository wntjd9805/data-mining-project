Recently, reinforcement learning (RL) has achieved success in various domains. However, applying RL to real-world scenarios is challenging due to the expensive or risky nature of interacting with the real world. To address this, ofﬂine RL, which learns from a fixed dataset without environment interaction, is a viable option. However, ofﬂine RL faces the distribution shift issue, leading to extrapolation error. Most off-policy RL algorithms fail in ofﬂine tasks due to over-generalization. While modern ofﬂine methods have achieved success in single-agent tasks, many real-world scenarios involve multi-agent systems with larger action spaces. Current ofﬂine algorithms are unsuccessful in multi-agent tasks despite adopting value-decomposition structures. In this paper, we propose Implicit Constraint Q-learning (ICQ), an algorithm that effectively alleviates extrapolation error in multi-agent RL. We evaluate ICQ on challenging multi-agent ofﬂine tasks based on StarCraft II and demonstrate its generality in various task scenarios.