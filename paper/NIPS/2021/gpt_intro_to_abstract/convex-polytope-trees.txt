Decision trees are popular models in domains such as medicine and criminal justice due to their interpretability and resistance to irrelevant predictor variables. However, their accuracy is often limited, and current approaches to improve performance, such as using ensemble methods, harm interpretability. Previous work has introduced oblique tree models to address this trade-off, but their large size remains a challenge. In this paper, we propose convex polytope decision trees (CPT) as a generalization of oblique trees, extending the decision boundaries to more flexible geometric shapes. We use the gamma process to adaptively infer the number of polytope facets and regularize the model's capacity. Our CPT approach offers higher accuracy while remaining interpretable, and we present two differentiable methods for training CPT models. Experimental results show that CPT outperforms state-of-the-art decision tree algorithms in terms of accuracy and model size. Our contributions include the proposal of an interpretable generalization of oblique decision trees, regularization techniques for CPT, scalable training methods, and experimental evaluation demonstrating improved performance.