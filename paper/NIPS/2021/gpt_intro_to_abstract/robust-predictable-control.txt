This paper introduces the concept of compressed policies in reinforcement learning (RL) by addressing the problem of agents using too many bits of information from their environment. It argues that agents that rely on fewer bits of information are more robust, can handle high-dimensional sensory inputs, and learn representations that are more broadly applicable. The paper proposes an RL algorithm called robust predictable control (RPC) that combines ideas from information bottlenecks, model-based RL, and bits-back coding. Experimental results show that RPC achieves higher returns, produces more robust policies, and exhibits good generalization and composability for hierarchical RL.