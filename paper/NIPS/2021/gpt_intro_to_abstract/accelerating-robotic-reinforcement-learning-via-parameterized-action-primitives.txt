Meaningful exploration in robotic reinforcement learning systems remains a challenge, as random motions often fail to achieve desired tasks. Previous approaches have relied on engineered rewards or training in simulation to overcome this exploration problem. Another difficulty in RL for robotics is the need to learn both task objectives and robot control simultaneously. In this paper, we propose a new approach called Robot Action Primitives for RL (RAPS) that uses parameterized primitives to redefine the policy-robot interface. These primitives, such as lift and push, are easy to design and can be reused across tasks. We conduct a thorough empirical evaluation comparing RAPS to other action parameterization methods and prior approaches that use expert data. Additionally, we investigate the generality of RAPS across different RL algorithms, its ability to perform complex manipulation tasks in sequence, and its impact on exploration. Results show that RAPS outperforms existing methods in various manipulation environments.