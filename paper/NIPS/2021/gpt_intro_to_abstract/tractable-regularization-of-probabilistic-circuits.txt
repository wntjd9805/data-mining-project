Probabilistic Circuits (PCs) are widely used in Tractable Probabilistic Models (TPMs) due to their ability to encompass various TPM circuit representations. PCs combine probabilistic graphical models (PGMs) and neural networks (NNs) and allow for efficient computation of probabilistic inference queries. However, advancements in PCs have led to increased overfitting. Existing regularization techniques for PGMs and NNs are not suitable for PCs, leading to unwanted bias and ineffective results. In this paper, we propose two novel regularization techniques tailored specifically for PCs: data softening and entropy regularization. These methods improve generalization performance and achieve state-of-the-art results on discrete density estimation benchmarks. The proposed approaches leverage the structural properties of PCs and provide tractable solutions for regularization objectives.