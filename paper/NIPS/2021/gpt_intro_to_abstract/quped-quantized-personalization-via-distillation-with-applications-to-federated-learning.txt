Federated Learning (FL) is a learning procedure that aims to train machine learning models using data from multiple edge devices without collecting the clients' data. This approach has been recognized to have limitations in settings where data is distributed heterogeneously, leading to the need for personalized learning. Personalized FL involves clients maintaining personalized models locally and utilizing other clients' data via a global model. However, the resource diversity among clients, which is often overlooked in personalized FL literature, may require clients to learn personalized models with different precision and dimension. In this paper, we propose a model compression framework for personalized FL that addresses both data and resource heterogeneity. This framework allows collaboration among clients with different resource requirements and enables learning personalized quantized models (PQMs). We conduct experiments on image and text classification and show that our approach outperforms existing methods in terms of performance and efficiency. Our work is distinct from distributed/federated learning approaches that focus on compressing models/gradients for communication efficiency, as our main objective is personalized quantization for inference.