Human pose estimation systems have gained significant attention in computer science due to their numerous applications. However, existing systems often suffer from poor cross-dataset generalization, as they rely on datasets captured in constrained settings or with limited size and diversity. In this paper, we propose a self-supervised domain adaptation framework for 3D human pose estimation that avoids dataset bias by not relying on paired supervision or auxiliary cues. Our approach involves cross-modal alignment between unpaired samples from different datasets, using novel relation distillation techniques. We introduce inter-entity relations that capture both lower and higher order non-local interactions in the pose and motion spaces, and develop frozen neural networks to compute relational energies. We demonstrate the effectiveness of our approach through extensive experiments and achieve state-of-the-art performance on popular benchmarks.