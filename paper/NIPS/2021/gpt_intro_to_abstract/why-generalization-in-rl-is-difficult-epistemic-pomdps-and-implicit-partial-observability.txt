Generalization is a key challenge in machine learning, but the problem of generalizing to new test-time contexts has received less attention in reinforcement learning (RL) research. Previous works have shown that RL policies struggle to generalize well to new situations, even within the same distribution as the training contexts. While empirical risk minimization approaches have been successful in supervised learning, we demonstrate that they can be sub-optimal for generalization in RL. We present an anecdotal example of a robotic zookeeper training on a set of zoos, where a learned classification strategy is optimal in training environments but fails to generalize effectively at test-time. We argue that the agent's epistemic uncertainty about the environment renders it implicitly partially observed, leading to the need for more intricate policies for optimal generalization. We formalize this observation as an epistemic partially observed Markov decision process (POMDP) and propose LEEP, an algorithm that maximizes return to address the generalization problem in RL. Our work highlights the importance of considering epistemic uncertainty and designing policies that can gracefully handle failure to generalize. Empirical results on ProcGen benchmark tasks demonstrate the effectiveness of LEEP in improving test-time performance compared to standard RL methods.