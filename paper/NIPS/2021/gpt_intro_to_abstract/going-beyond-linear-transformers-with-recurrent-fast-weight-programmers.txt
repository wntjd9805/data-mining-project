The Transformer has gained popularity in processing sequential data, with success in neural machine translation and other natural language processing tasks. However, the computational complexity of Transformers increases quadratically with sequence length, making them infeasible for dealing with very long or infinite sequences. Recent work has proposed linearized Transformer variants with linear time and space complexities, but their performance does not fully match that of regular Transformers. In this paper, we explore the connection between linear Transformers and Fast Weight Programmers (FWPs) and propose a variant called Recurrent FWPs (RFWPs) that incorporate recurrent connections. Our experiments show that RFWPs are competitive with regular Transformers in language modeling and perform well on synthetic algorithmic tasks and reinforcement learning problems. This work contributes to the study of powerful FWPs for sequence processing and addresses limitations of existing auto-regressive Transformer models by incorporating recurrence.