Reinforcement learning (RL) involves an agent solving a decision-making problem in an unknown dynamic environment to maximize long-term rewards. While exploration is a key challenge in RL, previous studies have focused mainly on fully observable environments. Limited research has been done on exploration in partially observable environments, particularly in the presence of a small observation space. This paper introduces the concept of reward-mixing Markov decision processes (RM-MDPs) and aims to provide the first polynomial-time algorithm for learning near-optimal policies in RM-MDPs with two reward models. The proposed algorithm addresses challenges related to identifiability and leverages the idea of uncertainty in higher-order moments. Technical contributions include the design of an efficient exploration algorithm and new tools to analyze errors in estimated higher-order correlations.