Despite the common assumption in stochastic multi-armed bandit (MAB) models that playing an action does not alter the environment, researchers have recently focused on settings with temporal dependencies between the player's actions and the reward distributions. This paper introduces the problem of Matroid Blocking Semi-Bandits (MBB), which combines combinatorial constraints and time dynamics, making it a more challenging setting. The goal is to maximize the player's expected cumulative reward over a fixed time horizon. The paper shows that the MBB problem is NP-hard and proposes a (1 1/e)-approximation algorithm that leverages the diminishing returns property hidden in the matroid structure. Furthermore, the paper connects the MBB problem to the Recurrent Submodular Welfare (RSW) problem and provides an efficient (1 1/e)-approximation algorithm for RSW, which implies the same approximation guarantee for the full-information case of MBB. The paper also discusses implications for the bandit setting where reward distributions are initially unknown, aiming to provide an upper bound on the regret.