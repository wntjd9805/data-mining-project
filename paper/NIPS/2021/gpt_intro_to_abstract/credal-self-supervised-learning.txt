Recent progress in machine learning, particularly in deep learning, has been driven by the availability of large amounts of labeled data. However, labeling data for supervised learning can be expensive and time-consuming. As a result, many datasets only have a small portion of labeled instances, while the majority are unlabeled. Semi-supervised learning (SSL) methods aim to leverage both labeled and unlabeled data concurrently. One popular SSL technique is self-training, which generates pseudo-labels for unlabeled instances based on the learner's current hypothesis. However, using pseudo-labels can introduce bias into the learning process. In this paper, we propose a novel approach to pseudo-labeling using credal sets, allowing the learner to represent uncertainty and lack of knowledge in a more flexible and faithful manner. We evaluate the effectiveness of this approach compared to conventional probabilistic pseudo-labeling methods, demonstrating improved generalization performance and better model calibration while reducing training time.