Deep learning has achieved significant success in recent years, leading to a focus on improving the technical tools that support its progress. One important tool is the estimation of the local geometry of the loss function, often done through approximations of the second-order information. Directly using such information is not feasible due to its computational requirements. Therefore, efficient numerical approximations, such as the empirical Fisher approximation, have been developed. This paper introduces two efficient algorithms for computing the inverse of the empirical Fisher approximation, allowing for efficient estimation of pruning statistics and optimization in deep neural networks. The algorithms provide exact matrix-free computations with linear computational and storage costs and match or improve state-of-the-art results in neural network pruning and optimization. The algorithms are implemented as the M-FAC library, which incorporates additional optimizations such as GPU acceleration. Experimental results show that the algorithms achieve significant improvements in pruning accuracy and compete with state-of-the-art optimizers in terms of validation accuracy.