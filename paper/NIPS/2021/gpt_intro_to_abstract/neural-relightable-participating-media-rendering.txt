This paper addresses the problem of inferring the bounding geometry and scattering properties of participating media objects from observed images. Traditional methods have relied on structured lighting patterns or discrete representations, but these require prior knowledge of the objects' geometry. The paper proposes a novel neural representation for learning relightable participating media, using a set of posed images with varying lighting conditions. The method involves disentangling the physical properties of the media, such as volume density and scattering albedo, and simulating global illumination through an embedded ray marching process. The proposed approach achieves better visual quality and numerical performance compared to state-of-the-art methods, while allowing for relighting, scene editing, and insertion into virtual environments.