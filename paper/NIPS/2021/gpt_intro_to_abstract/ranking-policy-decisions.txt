Reinforcement learning is a powerful method for training policies in complex environments, but existing policies often prioritize performance over simplicity. This paper proposes a method for ranking policy decisions based on their importance relative to a goal, with the aim of highlighting the most important parts of a policy. The ranking is computed using spectrum-based fault localization (SBFL) techniques borrowed from the software testing domain. The paper evaluates the ranking method by creating new, simpler policies without retraining and shows that these pruned policies maintain high performance. The method is applicable to black-box RL policies and does not require assumptions about the policy's training or representation. The code for reproducing the experiments is available on GitHub.