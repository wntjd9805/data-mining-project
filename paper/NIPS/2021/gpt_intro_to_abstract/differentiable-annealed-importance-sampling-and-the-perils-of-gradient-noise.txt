Marginal likelihood (ML) is an essential measurement in Bayesian learning for evaluating how well a model describes a dataset. Computing ML is often computationally challenging, especially when dealing with high-dimensional model parameters or latent variables. Annealed importance sampling (AIS) is a popular algorithm for approximating ML, but its non-differentiable Metropolis-Hastings (MH) correction limits its optimization and analysis. In this paper, we propose differentiable AIS (DAIS) by combining AIS with Hamiltonian Monte Carlo (HMC) and removing the MH step. This allows for gradient-based optimization and mini-batch computation. We analyze the convergence of DAIS in the setting of Bayesian linear regression and explore a stochastic variant using gradients from a subset of the dataset. Our simulations confirm the effectiveness of DAIS and demonstrate its application to variational autoencoders (VAEs) and generative model evaluation.