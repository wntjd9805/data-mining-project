This paper introduces a scene- and object-aware transformer (SOAT) model for vision-and-language navigation (VLN). The VLN task requires an agent to follow a path through an environment based on natural language navigation instructions. One of the challenges in VLN is learning the grounding between referring expressions in the instructions and the corresponding visual regions, due to limited visual diversity in training datasets. To address this, the authors propose an approach that uses both scene-level and object-level features, encoding the inductive bias that the world is composed of objects and scenes. They demonstrate that their approach outperforms a strong baseline approach on the Room-to-Room (R2R) and Room-Across-Room (RxR) datasets, particularly on instructions with multiple object references.