In this paper, we address the problem of minimizing a µ-strongly convex function using a stochastic gradient estimator. We investigate whether it is possible to transform the estimator into a nearly unbiased estimator of the minimizer. We propose an optimum estimator, called ˆx?, that has low bias and variance and achieves the same bias bound as T iterations of SGD. We demonstrate the applicability of our estimator in various stochastic optimization methods and show its potential for improving complexity bounds in different scenarios. We also discuss potential applications in non-smooth differentially private stochastic convex optimization.