Feature attribution is a key approach for explaining the predictions of black-box neural networks. However, existing methods show dissimilar results, making it difficult to evaluate which method correctly explains the prediction. In this paper, we propose InputIBA, a novel method that measures the predictive information of input features by searching for a bottleneck that only passes input features corresponding to predictive deep features. Our methodology provides fine-grained attributions without assuming any architecture-specific restrictions. We comprehensively evaluate InputIBA against other methods from different schools of thought and show its effectiveness in visual and natural language processing tasks.