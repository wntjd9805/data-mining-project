This paper explores the use of standard pruning algorithms to investigate the generalization properties of sparse neural ODEs and continuous normalizing flows. The authors investigate how the inner dynamics and modeling performance of a continuous flow are affected by methodically pruning its neural network architecture. They find that pruning can improve generalization in neural ODEs and uncover the underlying reasons for this improvement by conducting a Hessian-based empirical investigation. The results show that pruning reduces the value of the Hessian's eigenvalues, flattening the loss surface and leading to better generalization. Additionally, pruning helps avoid mode-collapse in generative modeling tasks and enables the discovery of minimal and efficient neural ODE representations.