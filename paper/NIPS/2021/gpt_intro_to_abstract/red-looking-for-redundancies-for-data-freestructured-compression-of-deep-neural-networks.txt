Modern Deep Neural Networks (DNNs) have achieved remarkable performance in various machine learning tasks, such as object classification, detection, and segmentation. However, these DNNs often require high computational runtime, which limits their deployment on edge devices. To address this issue, several approaches for DNN compression have been proposed, including architecture compression through pruning and tensor decomposition. While unstructured compression methods can remove more weights, they result in sparse weight matrices that require specialized hardware or libraries. Additionally, there is a trade-off between data-driven and data-free compression methods, with data-free methods being more convenient for privacy concerns but generally underperforming compared to data-driven methods. In this paper, we introduce RED, a novel data-free structured compression framework that leverages adaptive scalar hashing of weight distributions to introduce redundancies in DNNs at the neuron and tensor levels. Our experimental results demonstrate that RED outperforms state-of-the-art data-free methods and is competitive with existing data-driven approaches.