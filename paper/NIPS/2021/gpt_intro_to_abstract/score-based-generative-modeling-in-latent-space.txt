This paper introduces the Latent Score-based Generative Model (LSGM), a new approach for learning score-based generative models (SGMs) in latent space using a variational autoencoder (VAE) framework. The LSGM leverages the VAE to map data to a latent space and applies the score-based generative model there. The SGM is trained to model the distribution over the embeddings of the data set, and novel data synthesis is achieved by generating embeddings from a simple base distribution and iteratively denoising them. The LSGM offers advantages such as improved synthesis speed, increased expressivity, and the ability to tailor encoders and decoders for non-continuous data. The paper also presents technical contributions, including a new denoising score matching objective and techniques for variance reduction in training deep LSGMs. Experimental results demonstrate state-of-the-art performance in terms of fidelity and likelihood on various datasets.