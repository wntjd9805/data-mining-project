Research in multi-agent reinforcement learning (MARL) has recently gained significant attention, particularly in cooperative settings involving human-AI coordination. However, most work in this area has focused on self-play settings, where agents are trained to work well together but fail to generalize to independently trained AI agents or humans. To address this limitation, the zero-shot coordination setting was introduced, aiming to find training strategies that enable independently trained agents to coordinate at test time. In this paper, we propose a method called synchronous-k-level reasoning with a best response (SyKLRBR), which combines the cognitive-hierarchies (CH) framework with K-level reasoning (KLR) to scale KLR to large partially observable coordination problems. We demonstrate the effectiveness of our method by achieving high scores in cross-play evaluations and improving ad-hoc teamplay performance, surpassing recent algorithms that rely on additional information beyond game-provided observations. Our results highlight the importance of synchronous training and the choice of graph structure in adapting cognitive hierarchy ideas to deep MARL settings, and we hope that our findings will inspire further exploration of game theory literature for solving high-dimensional problems in the future.