Complex reinforcement learning (RL) problems require the agent to infer a useful representation of the world state from observations. This paper introduces the concept of using a bisimulation metric (BSM) to construct a representation space that groups behaviorally similar states together, effectively ignoring task-irrelevant distractors. The Deep Bisimulation for Control (DBC) algorithm is discussed as a method for mapping observations into a learned space that follows a policy-dependent BSM, but it is shown to have issues of robustness. To address these issues, the authors propose stabilizing the state representation space through a norm constraint and altering the encoder training method. The contributions of this work include generalizing theoretical value function approximation bounds, analyzing the BSM loss formulation to identify potential embedding pathologies, introducing constraint methods for better embedding quality, and devising training modifications based on intrinsic motivation and inverse dynamics regularization. Experimental results demonstrate the improved robustness of the proposed method to both sparsity and distractors in the environment.