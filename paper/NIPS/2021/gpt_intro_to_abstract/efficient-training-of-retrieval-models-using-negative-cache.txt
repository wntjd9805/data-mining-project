Learning to represent objects as embeddings is crucial in large-scale information retrieval tasks, such as recommendation systems, vision, and natural language processing. Training two separate neural networks, known as dual encoders, is a popular approach for this task. However, sampling good negative pairs from a large pool of documents is challenging and computationally expensive. Different strategies have been proposed to address this issue, such as in-batch negatives and using a retrieval index. In this work, we propose a method that utilizes a large negative cache and Gumbel-Max sampling to efficiently sample negatives. We analyze the convergence of our algorithm and develop a streaming version to scale to large datasets. Experimental results on MS MARCO and TREC 2019 passage retrieval tasks demonstrate the effectiveness and efficiency of our approach.