Interpretability methods, particularly feature attribution, are widely used in computer science to provide instance-specific explanations of model predictions. These methods rank or score input coordinates based on their importance in model prediction. Input gradient attribution, a fundamental technique in feature attribution, ranks input coordinates based on the magnitude of input gradients. However, popular attribution methods that utilize input gradients fail simple sanity checks and may produce unreliable results. In this paper, we introduce an evaluation framework called DiffROAR to analyze whether input gradient attributions satisfy a necessary assumption for accurately reflecting model behavior. Our experiments on real-world datasets show that input gradients of standard models often violate this assumption, while adversarially trained models consistently satisfy it. We further validate our findings using a semi-real dataset called BlockMNIST, where we can identify feature leakage as a potential cause for the violation of the assumption by input gradients. We also provide theoretical analysis and discuss related work. The code and datasets are publicly available.