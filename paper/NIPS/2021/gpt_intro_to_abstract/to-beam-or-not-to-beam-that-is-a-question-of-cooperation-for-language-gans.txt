The paper introduces the concept of Natural Language Generation (NLG) and highlights the limitations of current state-of-the-art models in terms of generating reliable and fluent sequences. The authors identify the limitations of Teacher Forcing and Exposure Bias in NLG models and propose the use of a sequence level objective to overcome these limitations. They suggest using a learned discriminator as an alternative to NLG metrics for evaluating the quality of generated text. The paper presents two concurrent approaches, Generative Adversarial Networks (GANs) and cooperative decoding, for improving the generation of human-like text. The authors then propose SelfGAN, a framework that combines cooperative decoding with GANs in a self-training process. They also introduce a new decoding algorithm called Coop-MCTS based on Monte Carlo Tree Search to address the left-to-right curse in existing cooperative decoding algorithms. The paper concludes by showcasing the benefits of SelfGAN and Coop-MCTS in improving the quality of generated text in Summarization and Question Generation tasks.