Adversarial attacks pose a significant threat to the deployment of deep learning systems, prompting the need for understanding the underlying vulnerabilities of neural networks. This paper investigates the susceptibility of Convolutional Neural Networks (CNNs) to adversarial attacks and identifies the invariance to circular shifts in CNNs as a potential cause for their lack of robustness. The study analyzes a simple example of a binary classification problem and demonstrates that shift-invariant CNNs exhibit lower adversarial robustness compared to fully connected (FC) networks. Theoretical analysis reveals that shift invariance impacts the margin of linear classifiers, further supporting the explanation of the CNNs' vulnerability. Additionally, the paper explores the effects of shift invariance on the robustness of FC and CNN networks on various datasets, including MNIST, Fashion-MNIST, SVHN, CIFAR-10, and ImageNet. Experimental results confirm that reducing the degree of shift invariance improves adversarial robustness. Overall, this research enhances the understanding of the relationship between shift invariance and adversarial robustness in neural networks and provides insights into potential approaches for improving robustness. Code for the study can be found at https://github.com/SongweiGe/shift-invariance-adv-robustness.