Reinforcement learning (RL) has been widely successful in various real-world applications using deep neural networks. However, the theoretical understanding of RL with neural network function approximation in large state spaces is still incomplete. Existing work mainly focuses on tabular or linear function approximations, but the highly non-linear nature of neural networks poses challenges. In this paper, we aim to address the question of what structural properties allow for sample-efficient algorithms in RL with neural function approximations. We study two-layer neural networks and structured polynomials under different RL settings and propose sample-efficient algorithms with provable near-optimal policy learning. Our techniques are based on neural network recovery and algebraic geometry.