In recent years, there has been a growing emphasis on evaluating the robustness of machine learning classifiers by considering additional uncertainty or bounds in addition to their performance on a test set. Most research in this area has focused on two specific settings: worst-case robustness and robustness to random perturbations. However, these two notions of robustness have typically been treated as separate concepts. This paper proposes a unified framework that generalizes and combines these two notions, advocating for a more fine-grained spectrum of robustness definitions. The authors argue that robustness to random perturbations and worst-case robustness can be interpreted as different q norms of the loss function evaluated over the perturbation distribution. The paper introduces a simple approach to evaluating these robustness norms using path sampling and Markov chain Monte Carlo methods. The proposed approach is demonstrated to show a trade-off between different levels of robustness and outperforms naive estimation techniques. The potential for training more robust classifiers using these estimators is also explored. The code for reproducing the experiments is provided in the given GitHub repository.