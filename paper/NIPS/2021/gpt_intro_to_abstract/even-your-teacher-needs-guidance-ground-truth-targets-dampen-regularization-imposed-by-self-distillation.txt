Knowledge distillation, a technique introduced by Hinton et al. (2015), allows for the transfer of knowledge from a teacher neural network to a student network. This paper focuses on self-distillation, a special case where the teacher and student networks have the same architecture, and aims to improve predictive performance rather than compressing the model. The self-distillation procedure involves using the outputs from a trained model as new targets for retraining the same model, resulting in improved generalization and predictive performance. The paper also discusses the regularization of over-parameterized neural networks and its connection to kernel ridge regression through the Neural Tangent Kernel (NTK). The theoretical analysis shows that the solution at each distillation step can be calculated based on the initial fit, and self-distillation amplifies regularization while ensuring non-zero solutions. The optimal distillation weight can be efficiently estimated for neural networks. Experimental results and code for reproducibility are provided.