Multi-view multi-person 3D pose estimation is a critical task that has applications in various real-world scenarios. Previous literature has primarily focused on reconstruction-based and volumetric approaches, which rely on intermediate tasks and estimate 3D poses for each person separately, resulting in inefficiencies and increased computation cost. In this paper, we propose a simplified and efficient pipeline by directly regressing 3D poses from multi-view images without relying on intermediate tasks. We address the challenges of skeleton joints detection and association for multiple persons by developing a novel Multi-view Pose transformer (MvP) model. The MvP model uses learnable positional embeddings and a specific attention mechanism to fuse multi-view information and globally reason over joint predictions. We introduce a hierarchical query embedding scheme that captures person-joint relations and enables better localization of 3D joints. Additionally, we propose a geometrically-guided projective attention mechanism and a RayConv operation to effectively fuse multi-view information and improve the accuracy of 3D pose estimation. Experimental results on benchmark datasets demonstrate the effectiveness of our MvP model in achieving state-of-the-art results with improved speed. Our contributions include a simple and direct regression approach for multi-view multi-person 3D pose estimation, a tailored hierarchical joint query embedding scheme, and a novel projective attention module along with a RayConv operation for effective fusion of multi-view information.