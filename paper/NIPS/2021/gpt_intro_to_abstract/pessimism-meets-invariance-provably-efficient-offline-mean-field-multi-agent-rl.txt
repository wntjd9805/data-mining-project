Significant progress has been made in multi-agent reinforcement learning (MARL) for various sequential decision making problems, such as social welfare optimization, fleet control of autonomous vehicles, and multiplayer online battle arena games. However, the computational cost of MARL increases exponentially as the number of agents grows. One solution is the mean-field regime, where a large number of homogeneous agents are considered interchangeable. This allows the interaction among agents to be captured by mean-field quantities, reducing the computational burden. While most existing results focus on online MARL, we propose the first pessimistic algorithm, named SAFARI (peSsimistic meAn-Field vAlue iteRatIon), for mean-field MARL in the offline setting. SAFARI incorporates a mean-embedding approach for value function approximation and integrates an uncertainty quantifier to avoid being misled by irrelevant trajectories in the experience data. We establish a data-dependent upper bound on the suboptimality of SAFARI and demonstrate its effectiveness through numerical experiments. This paper also provides an extension of the algorithm to the online setting.