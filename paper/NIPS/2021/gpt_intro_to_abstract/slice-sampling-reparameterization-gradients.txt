Probabilistic modeling is a powerful approach for understanding complex real-world processes, but it often presents computational challenges. In many modern probabilistic models, inference is formulated as optimization of a probabilistic objective. Variational inference is a notable example, where a simpler distribution is optimized to approximate a posterior distribution. Monte Carlo methods have allowed for the extension of variational inference to non-conjugate Bayesian models and neural networks. These probabilistic objectives also arise in various machine learning and computational science applications. However, computing these objectives is often challenging, leading to the need for gradient estimation via Monte Carlo sampling. Two popular classes of Monte Carlo gradient estimators are the score function estimator and the reparameterization gradient estimator. Reparameterization gradients have lower variance compared to score function gradients when the loss function is sufficiently smooth. However, they are primarily limited to distributions with tractable normalizing constants. This limitation hinders their applicability to complex models such as energy-based models and non-conjugate Bayesian models. Generalizing reparameterization with Markov Chain Monte Carlo (MCMC) to unnormalized distributions is an important direction for developing effective gradient estimators. While some recent work has focused on reparameterization gradients for unnormalized distributions, challenges remain in extending these methods to general unnormalized distributions. One promising alternative to current MCMC methods for reparameterized gradients is slice sampling, an auxiliary-variable MCMC method that can be applied to unnormalized probability distributions without an accept/reject step. In this paper, we develop reparameterization gradients for samples generated from slice sampling, making them applicable to distributions known only up to a normalizing constant. We demonstrate the effectiveness of slice sampling reparameterization gradients through simulations and explore their application to deep generative modeling, approximate inference, and Bayesian sensitivity analysis.