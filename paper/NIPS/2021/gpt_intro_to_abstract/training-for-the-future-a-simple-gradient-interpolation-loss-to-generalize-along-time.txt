Many organizations follow a machine learning pipeline where models are trained periodically on collected data and then deployed to serve requests in the future. The motivation behind periodic retraining is the understanding that data distribution is not constant, requiring a revision of the decision boundary. In this paper, we explore whether we can further improve training by considering the deployment period when training the model. We formulate the problem as prediction tasks over an evolving joint distribution, conditioned on time. We propose a training algorithm that leverages temporal information and a novel time-dependent activation function to extrapolate well to the future. Our method is empirically evaluated on real-world datasets and compared against existing methods and baselines, demonstrating strong performance.