Modern systems neuroscientists have access to advanced neural interfaces that provide a qualitatively different picture of brain activity than was achievable in the past. However, these interfaces face a trade-off between the number of neurons that can be accessed and the number that can be simultaneously monitored. This trade-off exists in both multi-photon calcium imaging and electrophysiological interfaces. In this paper, we propose selective backpropagation through time (SBTT), a method to train deep generative models of latent dynamics from data where the set of observed variables varies. We explore the application of SBTT to state space modeling of neural population activity and demonstrate its effectiveness in achieving more efficient and higher-fidelity inference of latent dynamics.