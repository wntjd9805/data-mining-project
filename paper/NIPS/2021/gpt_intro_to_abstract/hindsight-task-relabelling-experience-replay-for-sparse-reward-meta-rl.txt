In this paper, we introduce a new algorithm called Hindsight Task Relabeling (HTR) that applies the concept of hindsight relabeling from conventional reinforcement learning (RL) to meta-reinforcement learning (meta-RL). We address the challenge of learning with sparse rewards by relabeling data collected on true training tasks as pseudo-expert data for hindsight tasks. This allows us to bootstrap meta-training with the reward signal needed to train the agent. We demonstrate the effectiveness of HTR by achieving state-of-the-art performance on challenging sparse reward environments that previously required shaped reward functions to solve. Our approach not only learns adaptation strategies with sparse reward, but also achieves performance comparable to existing approaches using shaped rewards.