Generative modeling involves inverting an inference process, either deterministically or stochastically. Variational autoencoders provide flexibility in choosing generative and inference models. Diffusion-based models freeze the inference path, using a fixed discrete-time Markov chain for inference and a separate Markov chain for generation. Song et al. connect diffusion-based models with score matching by using stochastic differential equations (SDEs). They propose a plug-in reverse SDE generative model, which is obtained by learning the score function of the inference process. However, little is known about the relationship between score matching loss and the plug-in reverse SDE. In this paper, we introduce a variational framework that connects score matching with maximum likelihood. We derive a functional evidence lower bound and show that matching the score maximizes a lower bound on the log marginal density of the plug-in reverse SDE. Our results extend to a family of equivalent SDEs, including an equivalent ordinary differential equation as a limiting case.