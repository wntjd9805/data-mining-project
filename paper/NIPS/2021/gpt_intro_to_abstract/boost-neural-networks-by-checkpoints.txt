DNN ensembles have shown improved performance compared to individual networks, but their training is computationally expensive. To address this issue, a technique called Checkpoint Ensemble (CPE) has been introduced, which utilizes checkpoints from one training process instead of training additional networks. However, CPE often produces similar checkpoints, reducing the desired diversity in the ensemble. In this paper, we propose a novel approach called CBNN (Checkpoint-Boosted Neural Network) that boosts the diversity of checkpoints by adaptively reweighting training samples. We analyze the advantages of CBNN, including its ability to reduce exponential loss for faster convergence and its effectiveness in handling imbalanced datasets. The adaptability of CBNN eliminates the need for manual weight assignments in imbalanced learning methods.