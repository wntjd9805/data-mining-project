Modern machine learning models rely on complex architectures and large amounts of training data, resulting in over-parameterization. These factors require efficient optimization methods for training. Nonconvex optimization problems arise due to sophisticated model architectures, and distributed computing is necessary for handling large training data sizes. The large number of parameters poses challenges for communication among machines, leading to the need for compression techniques. This paper focuses on solving the nonconvex distributed optimization problem using communication-efficient first-order methods with biased compression operators. The goal is to provide clean convergence analysis without unrealistic assumptions. The remainder of the paper discusses key concepts, presents theoretical results, and provides experimental findings.