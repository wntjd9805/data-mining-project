In this paper, we explore the concept of game-theoretic learning and the ability of players to adapt and achieve a Nash equilibrium through empirical observations. We discuss the challenges posed by the impossibility result of Hart and Mas-Colell, which shows that uncoupled dynamics cannot converge to Nash equilibrium in all games. To address these challenges, we focus on relaxing the feedback requirements of learning processes and understanding the stability of equilibria under popular learning algorithms. We introduce the concept of "follow the generalized leader" (FTGL), a flexible algorithmic framework that encompasses various action choice mechanisms and feedback models. We provide quantitative insights into the convergence speed of FTGL methods by introducing a rate function that captures the sensitivity of the induced choice map. We show that the convergence speed depends on the choice of regularizer and learning rate, rather than the type of feedback employed. These findings highlight the fundamental differences between regret minimization and convergence to Nash equilibrium.