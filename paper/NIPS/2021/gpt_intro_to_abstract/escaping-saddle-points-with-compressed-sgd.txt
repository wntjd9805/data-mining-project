Stochastic Gradient Descent (SGD) and its variants are widely used in machine learning, especially in distributed implementations on clusters with a central server and multiple workers. In this work, we focus on the compressed SGD approach, where a compressed version of the gradient is communicated instead of the full gradient. While previous research has shown that compressed SGD can converge to a first-order stationary point (FOSP), which may be a local minimum, saddle point, or local maximum, our goal is to demonstrate that it can also converge to an approximate second-order stationary point (SOSP). We present the first result showing the convergence of compressed methods to an Îµ-SOSP.