Deep Neural Networks (DNNs) have proven to be vulnerable to adversarial and random perturbations, raising concerns about their safety in critical fields such as defense and self-driving vehicles. This paper introduces the concept of a posteriori certification, which aims to verify the correct behavior of a trained network in a local neighborhood of inputs. The certification mechanism should be sound, certifying the network only when the expected property holds, and complete, certifying the network whenever the property holds. The paper also discusses the assessment of corruption robustness, considering both worst-case analysis and random perturbations. The challenge lies in efficiently estimating the probability of property violation, and this paper proposes a scalable and efficient procedure with theoretical guarantees on soundness and completeness.