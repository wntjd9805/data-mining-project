Deep generative models are valuable for unsupervised representation learning by incorporating priors that reflect the underlying domain's structure. However, training models with structured priors presents challenges beyond the standard strategy of maximizing a reparameterized evidence lower bound. Recent advancements in reparameterized variational inference, such as wake-sleep methods, annealing, Sequential Monte Carlo, Gibbs sampling, and MCMC updates, have been proposed to address these challenges. However, these methods are either model-specific or not easily compatible with other techniques. In this paper, we introduce nested variational inference (NVI), which combines nested importance sampling with variational inference. NVI optimizes divergence at each level of nesting and incorporates importance resampling to enhance sampling quality without sacrificing differentiability. This approach achieves lower variance weights and maintains high sample diversity compared to existing methods.