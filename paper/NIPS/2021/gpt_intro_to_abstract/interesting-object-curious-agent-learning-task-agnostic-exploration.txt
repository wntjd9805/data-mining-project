Exploration is a crucial problem in developing intelligent agents that can behave like humans. Reinforcement learning typically studies exploration in two settings: task-driven exploration, where the agent aims to maximize long-term rewards, and task-agnostic exploration, where the agent explores in the absence of external rewards. However, the task-agnostic setup is unrealistic as it assumes isolated environments and tabula-rasa agents. In this paper, we propose a paradigm shift towards multi-environment transfer-exploration, where agents learn exploration policies from prior knowledge and experiences. We introduce Change-Based Exploration Transfer (C-BET), which combines agent-centric and environment-centric exploration, allowing agents to seek out unseen areas and interesting components of the environment. Our experiments demonstrate the effectiveness of C-BET in multi-environment setups and its competitiveness with prior methods in unseen environments. We advocate for research to focus on learning from multiple environments and transferring experiences in exploration.