In recent years, numerical methods inspired by optimal transport have become popular in solving machine learning problems. One such method is the Wasserstein distance, which defines metrics between probability measures. However, the computational cost of Wasserstein distances in large-scale settings is often prohibitive. To address this issue, the Sliced-Wasserstein distance (SW) has been proposed as a computationally cheaper alternative. SW compares probability measures by computing the Wasserstein distance between their univariate projections. While SW has been successfully applied in various tasks and offers theoretical properties, its accuracy depends on the number of projections and the variance of the one-dimensional Wasserstein distances. In this paper, we propose a novel technique that leverages concentration results on random projections to approximate SW without relying on a finite set of random projections. Our method eliminates the need for tuning hyperparameters and can significantly reduce computational time. We provide theoretical guarantees on the induced error and validate our approach with experiments on synthetic data. We also demonstrate the advantages of our method in a generative modeling problem in machine learning.