Supervised learning often requires collecting labeled data, which can be expensive. To reduce costs, previous works have explored methods such as collecting labels from non-experts or online queries, but these approaches introduce noise into the labels. Various techniques have been proposed to learn from noisy labels, including modeling label noise, designing robust losses, and selecting trustworthy samples. However, the effectiveness of these methods is limited when dealing with realistic label noise. In this paper, we investigate the impact of a network's architecture on its robustness to noisy labels, particularly in relation to its alignment with the target and noise functions. We measure a network's robustness by analyzing the predictive power of its learned representations and find that networks with better alignment to the target function are more robust. We provide theoretical support for our findings and validate them through empirical experiments on synthetic graph algorithmic tasks and image classification datasets using different architectures. Additionally, we demonstrate that our findings improve upon state-of-the-art noisy label training methods.