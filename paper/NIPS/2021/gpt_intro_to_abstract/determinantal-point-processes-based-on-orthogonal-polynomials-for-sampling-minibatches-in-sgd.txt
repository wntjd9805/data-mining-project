The paper introduces the problem of minimizing an empirical loss with a penalty term and explores the use of stochastic gradient descent (SGD) for efficient computation. It highlights the importance of reducing the variance of gradient estimators in SGD and discusses the potential of determinantal point processes (DPPs) for achieving this. The authors propose a combination of continuous DPPs based on orthogonal polynomials and kernel density estimators to obtain gradient estimators with low variance. The paper presents theoretical analysis, demonstrating that the proposed approach yields variance reduction. The use of orthogonal polynomials and DPPs provides novel insights into discrete subsampling and offers a tailored approach for data distribution. The notations and assumptions used throughout the paper are also specified.