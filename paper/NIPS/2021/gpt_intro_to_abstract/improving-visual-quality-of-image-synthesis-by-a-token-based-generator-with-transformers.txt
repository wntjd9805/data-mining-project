Unconditional image synthesis has made significant progress with the style-based generator architecture, which uses an intermediate latent space to control the style of generated images. However, the style control via adaptive instance normalization (AdaIN) operation can lead to content-independent style and entangled representations. In this paper, we propose a token-based generator, TokenGAN, that views image synthesis as a visual token generation problem. By using content tokens and style tokens, we achieve content-aware and fine-grained style control. We employ a visual Transformer network to model the relations between the tokens and demonstrate that the token-based generator produces high-fidelity images without convolutions. Experimental results show the effectiveness of the proposed approach in image synthesis tasks.