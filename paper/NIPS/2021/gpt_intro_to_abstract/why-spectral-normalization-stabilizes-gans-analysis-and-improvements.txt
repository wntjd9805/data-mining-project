Generative adversarial networks (GANs) are widely recognized for their ability to generate high-resolution, photorealistic images. The objective of GANs is to produce random samples from a target data distribution using a generator and discriminator function trained through adversarial training. However, the training process of GANs can be unstable, leading to failure. Spectral normalization (SN) has been proposed as a successful technique to improve stability in GAN training, but the reasons for its effectiveness remain unclear. This paper aims to analyze the failure modes of GAN training and understand why SN can mitigate exploding and vanishing gradients. The authors present theoretical proofs and empirical evidence that SN controls gradients and introduce Bidirectional Scaled Spectral Normalization (BSSN) as an improved version of SN. BSSN combines insights from Xavier and Kaiming initialization techniques to achieve better sample quality and training stability. The goal of this work is not to propose the best normalization or regularization technique, but rather to gain insights that can inform more effective alternatives.