Federated learning (FL) has gained attention due to its ability to learn from fragmented data while preserving privacy. However, concerns have been raised regarding the potential disparity induced by FL systems in local clients. This paper addresses the problem of algorithmic fairness and performance consistency in FL. Existing research on algorithmic fairness has mostly focused on individual learning scenarios, and there is a need for studying the impact of FL on model fairness. The authors propose a new federated learning framework, FCFL, that achieves a fair and consistent model for all local clients by optimizing the model to achieve Pareto optimality. Theoretical analysis and experiments demonstrate the effectiveness of FCFL in achieving fairness guarantees in each client.