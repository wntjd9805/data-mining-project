The Lottery Ticket Hypothesis (LTH) proposes the existence of sparse subnetworks within over-parameterized neural networks that can achieve similar or better performance compared to the original dense networks. However, current methods for finding these winning tickets, such as Iterative Magnitude-based Pruning (IMP), are expensive and unstable. In this paper, we explore the transferability of winning tickets across different network architectures, focusing on architectures within the same design family but of different depths. We introduce the Elastic Lottery Ticket Hypothesis (E-LTH) and propose strategies to stretch or squeeze winning tickets for deeper or shallower networks. Our experiments on CIFAR-10 and ImageNet show promising results, suggesting that winning tickets can be transferred between architectures from the same design family. We also provide explanations for the preliminary success and discuss the potential extension of our findings to more general notions.