This paper focuses on the analysis of language models in the context of machine teaching, exploring their ability to learn from minimal teaching sets for few-shot inference. The acquired priors of language models are compared with humans and program induction systems, with a particular emphasis on patterns preferred by a simplicity prior. It is found that language models such as GPT-3 exhibit similar or better performance than humans, and this performance is negatively correlated with the complexity of the concept. Additionally, language models and humans differ in their capability for producing explanations, with human explanations correlating with concept complexity. The paper emphasizes the importance of understanding language models and their relation to other systems to ensure their safe and effective use, although it also acknowledges the potential for malicious manipulation of prompts. The remaining sections of the paper delve into related work, experimental design, results, and implications of the findings.