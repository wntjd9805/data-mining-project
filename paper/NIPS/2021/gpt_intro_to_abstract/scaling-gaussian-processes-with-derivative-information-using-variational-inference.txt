Gaussian processes (GPs) are commonly used in probabilistic machine learning for regression tasks that require quantifying uncertainty. However, when derivative information is available, such as in Bayesian optimization or certain regression settings, GP inference becomes computationally expensive. In this paper, we propose a new method that scales GPs with derivative information using stochastic variational approximations. By decomposing the expected log likelihood term, we can use stochastic gradient descent with minibatches containing label and derivative information. We also introduce inducing directional derivatives to sparsify the derivative information, resulting in faster training time. We demonstrate the effectiveness of our approach on various synthetic functions and real-world tasks, showcasing improved performance even when no derivative information is available.