Labeled datasets often contain noisy labels, necessitating the development of noise-robust learning algorithms. This paper proposes two new noise-robust loss functions based on two key observations. The first observation is that provably-robust loss functions can underfit the training data. The second observation is that standard networks show low consistency around noisy data points. The paper introduces the use of Jensen-Shannon divergence (JS) as a loss function, which interpolates between noise-robust mean absolute error (MAE) and cross entropy (CE) for improved convergence. Additionally, a generalized version of Jensen-Shannon divergence (GJS) is used to encourage consistency in predictions on perturbed inputs. The paper presents empirical evidence and demonstrates state-of-the-art results on various datasets, noise types, and rates.