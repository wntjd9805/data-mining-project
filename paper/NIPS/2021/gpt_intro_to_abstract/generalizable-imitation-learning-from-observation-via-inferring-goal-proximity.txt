Humans have the ability to learn tasks from demonstrations and apply their learned behaviors to new situations. This is achieved by extracting the underlying structure of the task instead of memorizing specific actions. One important element of task structure is task progress, which measures how much of the task has been completed. In this paper, we propose a novel imitation learning method that utilizes task progress to improve generalization to unseen states and goals. Previous approaches to learning from demonstration suffer from accumulated errors and drift away from demonstrated states. Adversarial imitation learning methods address this issue but often overfit to expert demonstrations. To overcome these limitations, we propose an imitation learning from observation method that learns a task progress estimator and uses it as a dense reward for training a policy. We show through extensive experiments that our method achieves better generalization and performance compared to state-of-the-art techniques.