With the increasing size of deep learning models and their use in low-resource environments, network compression techniques are becoming more important. Network pruning, which involves reducing the size of neural networks by removing unnecessary connections, is a commonly used method. However, there is still a lack of understanding regarding why and when neural networks can be effectively compressed. This paper aims to develop a theoretical framework to address this gap in knowledge and explore the relationship between network compressibility and generalization. The authors base their theoretical results on recent findings related to the stochastic gradient descent algorithm used in training deep neural networks. They show that under specific conditions, fully connected neural networks can be compressed without significant loss in performance. The results also demonstrate that compressibility is linked to improved generalization. The authors conduct experiments to validate their theoretical findings.