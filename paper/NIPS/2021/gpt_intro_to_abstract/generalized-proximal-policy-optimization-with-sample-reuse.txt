In recent years, model-free deep reinforcement learning has been successful in solving complex simulated control tasks, but its real-world adoption remains limited due to the need for stable, reliable performance and efficient use of limited data. Existing model-free algorithms often sacrifice one goal for the other. On-policy methods like Proximal Policy Optimization (PPO) offer stability but require significant data collection, while off-policy algorithms store samples in a replay buffer for efficiency but suffer from distribution shifts. To address these challenges, we propose Generalized Proximal Policy Optimization with Sample Reuse (GePPO), which combines the advantages of on-policy and off-policy approaches. By extending policy improvement guarantees to the off-policy setting, developing connections between PPO's clipping mechanism and our policy improvement lower bound, and introducing an adaptive learning rate method, our algorithm achieves a balance between stability and sample efficiency. We provide theoretical evidence and experimental results on high-dimensional continuous control tasks to demonstrate the effectiveness of GePPO.