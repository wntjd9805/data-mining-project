Decentralized optimization has become a popular approach for scaling distributed machine learning models. This approach eliminates the need for a central coordinator in distributed training, allowing for high scalability. Current research has focused on reducing synchronization overheads for decentralized training through non-blocking communication, local steps in between communication rounds, and quantization of communication. However, these methods often rely on synchronous communication and are difficult to generalize. In this paper, we propose SwarmSGD, a variant of stochastic gradient descent (SGD) that supports all three communication reduction approaches in an asynchronous gossip model. We prove the convergence of SwarmSGD and demonstrate its effectiveness through experiments on image classification and machine translation tasks. Our results show that SwarmSGD achieves faster convergence rates compared to previous methods while reducing synchronization costs.