This paper addresses the problem of uncertainty estimation in regression problems, which is crucial for deploying machine learning in risk-sensitive domains. The common approach for uncertainty estimation is to learn a quantile function or prediction interval, but it has been observed that quantile regression often exhibits under-coverage bias. Although alternative approaches have been proposed, a fundamental understanding of this bias is still lacking. This paper presents a theoretical study on the coverage of quantile regression in a new regime where the number of samples is proportional to the dimension, showing that quantile regression exhibits inherent under-coverage bias. The paper also disentangles the sources of this bias and performs experiments to validate the findings. The technical analysis builds on recent understandings of empirical risk minimization problems in the high-dimensional proportional limit.