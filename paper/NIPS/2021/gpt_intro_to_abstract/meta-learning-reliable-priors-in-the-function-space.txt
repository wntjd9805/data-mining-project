Learning new concepts and skills efficiently is a fundamental aspect of human intelligence, but machine learning algorithms often lack this adaptive capability. Meta-learning has emerged as a promising approach to enable systems to learn efficiently by leveraging experience from previous related tasks. In this paper, we focus on meta-learning probabilistic prior beliefs to improve prediction accuracy with limited training data and enhance uncertainty estimates. Uncertainty estimates are crucial for sequential decision-making tasks such as Bayesian optimization and reinforcement learning. However, existing methods for meta-learning often suffer from overconfidence in these uncertainty estimates, leading to suboptimal performance in exploration tasks. We propose a novel approach to directly regularize meta-learned priors in the function space, utilizing a stochastic process hyper-prior and a KL-divergence approximation. Our functional meta-learning algorithm, F-PACOH, integrates seamlessly into sequential decision algorithms, and experimental results demonstrate its superiority in transfer learning and lifelong learning scenarios for Bayesian optimization. F-PACOH consistently provides well-calibrated uncertainty estimates, paving the way for potential applications in the optimization and calibration of complex systems under changing conditions.