This paper addresses the challenge of spatially localizing narrated interactions in videos without strong supervision. The authors propose a recognition system that learns to match entire sentences to regions containing multiple objects and actions. They leverage self-supervision from a large corpus of narrated videos and introduce an evaluation dataset with bounding box annotations for interactions described by natural language sentences. Their approach involves contrasting aggregated representations from the video and narration modalities using attention models and optimizing a loss over sentence-level representations. The authors demonstrate the effectiveness of their approach in localizing interactions and objects in videos, as well as grounding phrases in images. Ultimately, their work has important implications for advancing machine perception and vision-and-language research.