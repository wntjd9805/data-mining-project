Designing efficient reinforcement learning (RL) algorithms for environments with large state and action spaces is an important task in the RL community. Function approximation, which uses pre-defined functions to approximate the value function or transition dynamics, has been extensively studied. While previous works have focused on RL with linear function approximation and provided regret and PAC guarantees, these measures do not guarantee convergence to the optimal policy. To address this limitation, a new performance measure called uniform-PAC has been proposed, which guarantees convergence to the optimal policy with high probability. However, existing uniform-PAC algorithms are not suitable for large state and action spaces. In this paper, we propose new uniform-PAC RL algorithms with linear function approximation for contextual linear bandits and episodic linear MDPs, achieving comparable sample complexity to state-of-the-art algorithms. Our contributions include the development of a novel algorithm called UPAC-OFUL for contextual linear bandits, and a FLUTE algorithm for episodic linear MDPs. These algorithms achieve uniform-PAC guarantees and improve upon existing approaches.