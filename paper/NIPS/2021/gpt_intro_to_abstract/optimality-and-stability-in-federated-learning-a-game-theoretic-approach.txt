Recent advances in machine learning have enabled the application of powerful prediction algorithms to various domains. However, in real-world scenarios, data is often spread across multiple locations and cannot be centralized for training. Federated learning emerges as a solution, allowing individual agents to learn local model parameters using their own data and then transmitting these parameters to a coordinating entity to form a global model. While federated learning has been widely studied for stability, there has been little analysis of optimality in this context. This paper presents a framework for evaluating optimality and stability in federated learning and proposes an efficient algorithm for calculating an optimal federating arrangement. Additionally, it proves the first-ever constant bound on the Price of Anarchy in this game, demonstrating that the worst stable arrangement is no more than nine times the cost of the best arrangement. Theoretical in nature, this work offers insights into the trade-off between stability and optimality in federated learning.