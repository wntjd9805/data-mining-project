Neural implicit functions have gained popularity as a representation paradigm for modeling complex signals. Unlike discrete representations, neural implicit functions can capture high-resolution details in various domains. However, implementing these functions with common neural structures becomes challenging when dealing with signals with high frequencies. Recent studies have shown that mapping input coordinates or intermediate features to positional encodings can enhance the performance of deep implicit networks. However, there are still concerns regarding the manual tuning of frequency scales, the adaptability to different inputs, and the global selection of frequencies. In this paper, we propose Spatially-Adaptive Progressive Encoding (SAPE) as a solution to these challenges. SAPE gradually uncovers signal components with increasing frequencies and allows the progression of encoding frequencies to vary among local spatial portions. Our approach enables MLP networks to fit a range of fine details without the need for parameter tuning or specific preprocessing. SAPE is encoding-agnostic and can be applied to various coordinate-based neural implicit functions, improving their performance in different domains.