Recently, the concept of over-parametrization has gained attention in the optimization of neural networks. The Neural Tangent Kernel (NTK) theory has shown that zero training loss can be achieved when the network is sufficiently over-parametrized. However, the NTK dynamics, known as lazy training, where neurons do not move much, can be unnatural in certain settings and lead to poorer generalization performance. This paper explores different regimes of over-parametrization and analyzes dynamics beyond lazy training. The paper focuses on the problem of tensor decomposition and shows that gradient flow on over-parametrized tensor decomposition follows a tensor deflation process, fitting any orthogonal tensor to desired accuracy. The goal of the paper is to contribute to the understanding of the implicit bias of over-parametrized gradient descent for low-rank tensor problems.