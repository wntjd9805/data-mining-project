Deep learning models have achieved significant success in various tasks, but their high computational costs and long processing times hinder their efficiency. This is especially critical in latency-sensitive applications, such as deploying reinforcement learning systems in real-time or controlling autonomous cars. Current approaches to reduce processing complexity involve model compression or response approximation. However, these methods still require samples to pass through the entire model. Inspired by the idea of early exit methods, we propose a new family of zero waste models called Zero Time Waste (ZTW) that intelligently aggregates information from previous internal classifiers (ICs). Our approach includes cascade connections and ensembling to reuse information and improve predictions. We evaluate ZTW on various classification benchmarks and show its effectiveness in saving computation without sacrificing accuracy. Additionally, we introduce the metric of Hindsight Improvability to measure how efficiently the model reuses past information. Overall, our work contributes to improving the efficiency of neural network models and demonstrates the applicability of early exit methods in reinforcement learning scenarios.