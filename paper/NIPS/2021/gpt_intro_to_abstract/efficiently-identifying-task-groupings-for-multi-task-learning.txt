Many challenges in applied machine learning require a single model to perform well on multiple tasks while adhering to strict inference-time constraints. Examples include autonomous vehicles, robotic arms, and online movie recommendation systems. Multi-task learning has the potential to improve modeling performance by introducing an inductive bias and focusing attention on relevant features. However, it can also lead to degraded performance if tasks compete for model capacity or cannot build a shared representation. The problem of deciding which tasks should be trained together is an understudied and complex issue. While recent work has developed new optimization schemes, the task grouping problem remains challenging and often relies on human experts. In this paper, we propose an efficient framework to select task groupings without sacrificing performance. We measure inter-task affinity by training all tasks together and quantifying the effect of one task's gradient update on another task's loss. Our approach is applicable to any paradigm with shared parameters and outperforms other task grouping methods in the convex setting under mild conditions. Empirically, our approach decreases runtime by more than an order of magnitude while achieving competitive performance on challenging multi-task image benchmarks.