The stochastic multi-armed bandit (MAB) problem is a widely studied online learning framework with various applications. In this problem, a learning agent interacts with an environment in a T-round game, where the objective is to maximize expected rewards or minimize cumulative expected regret. The agent faces the exploration-exploitation dilemma in each round, and algorithms such as the upper confidence bound (UCB) and Thompson sampling (TS) have been popularly used. However, these algorithms are primarily designed for single-arm selection and may not adequately model real-world applications where the agent's action involves a combination of arms. This motivates the study of the combinatorial MAB (CMAB) problem, where the agent selects a combination of base arms as an action in each round. TS-type algorithms have garnered interest in CMAB problems, but their performance with approximation oracles, commonly used for NP-hard combinatorial optimization problems, remains an open question. In this paper, we formulate CMAB problems with the greedy oracle, a common oracle for offline problems, and analyze the theoretical performance of the TS algorithm with the greedy oracle for a specific problem instance of probabilistic maximum coverage (PMC). Our results illustrate the difficulty of achieving good theoretical performance with the TS algorithm in CMAB problems and provide a near-matching problem-dependent regret upper bound. These results contribute to a better understanding of TS with approximation oracles in CMAB problems.