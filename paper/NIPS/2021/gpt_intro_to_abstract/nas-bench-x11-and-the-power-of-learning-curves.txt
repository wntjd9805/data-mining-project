In recent years, algorithms for neural architecture search (NAS) have been widely used to find architectures that achieve top performance on various datasets. However, concerns have been raised regarding the lack of reproducibility and fair comparisons within NAS research. To address this, tabular benchmarks such as NAS-Bench-101 and NAS-Bench-201 were created, allowing researchers to simulate NAS experiments and reach statistically significant conclusions at a low computational cost. To further extend the benefits of these benchmarks, surrogate benchmarks like NAS-Bench-301 were proposed, which model larger and more realistic NAS search spaces. Despite the advantages of multi-fidelity techniques in hyperparameter optimization, they have been under-utilized in the NAS community. In this paper, we address this gap by introducing surrogate benchmarks NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, which provide full learning curve information for all architectures. We also present a framework for converting single-fidelity NAS algorithms into multi-fidelity algorithms using learning curve extrapolation, and demonstrate improvements on popular NAS algorithms. Overall, our work bridges the gap between different areas of AutoML and facilitates the development of effective multi-fidelity techniques in the future.