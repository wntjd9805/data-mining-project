Deep learning methods have achieved significant success in various fields, but they often suffer from catastrophic forgetting when confronted with non-i.i.d. data samples or incremental learning scenarios. Continual learning aims to address this issue by enabling a learning agent to sequentially acquire and retain knowledge without interference or forgetting, similar to how animals learn. This paper integrates the hypothesis of generative concept learning and the Bayesian approach to address the continual learning problem. The authors propose a generative classifier called PGLR, which improves upon existing discriminative approaches by mitigating catastrophic forgetting. Meta-learning is utilized to derive proper inductive biases and enhance the learning process. The proposed method achieves state-of-the-art accuracy compared to existing approaches and can be implemented in a fully incremental setting, making it suitable for scenarios with limited data or computational resources.