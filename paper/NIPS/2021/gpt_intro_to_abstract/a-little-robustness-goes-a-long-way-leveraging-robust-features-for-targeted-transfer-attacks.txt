Neural-network image classiÔ¨Åers are vulnerable to adversarial examples, which are imperceptibly perturbed images that cause misclassifications. Previous research has focused on generating successful perturbations, but this paper explores the use of "slightly-robust" convolutional neural networks (CNNs) as the source network for constructing targeted adversarial examples. The study shows that adversarial examples constructed with respect to slightly-robust CNNs transfer successfully to different CNN architectures and even transformer architectures. This finding suggests that slightly-robust networks rely on features that overlap with every non-robust network. Additionally, the paper examines the role of adversarial loss functions and demonstrates the increase in transferability as the robustness of the source network increases. The main contributions include demonstrating increased transferability of adversarial examples generated with slightly-robust CNNs, exploring the role of adversarial loss functions, and explaining the superior transferability of targeted adversarial examples in slightly-robust networks.