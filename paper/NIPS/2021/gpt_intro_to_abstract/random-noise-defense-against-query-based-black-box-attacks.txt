Deep neural networks (DNNs) have been successfully applied in safety-critical tasks, but they are vulnerable to adversarial examples. In this paper, we focus on defending against query-based black-box attacks where the attacker has no knowledge of the model. We propose a lightweight defense strategy called Random Noise Defense (RND) that adds random noise to each query, making it difficult for the attacker to find successful attacks. We provide a theoretical analysis of RND's defense performance and demonstrate its effectiveness against both standard and adaptive attacks. Additionally, we propose combining RND with a lightweight Gaussian augmentation Fine-tuning (RND-GF) for a better trade-off between defense and clean accuracy. Our experiments on CIFAR-10 and ImageNet confirm the theoretical analysis and show the effectiveness of RND-GF in improving robust accuracy. The main contributions of this work are the theoretical and empirical analysis of RND, the analysis of RND's performance against adaptive attacks, the proposal of RND-GF as a stronger defense strategy, and the extensive experiments that verify the effectiveness of our defense methods against state-of-the-art attacks.