Bandit problems are a class of online decision-making problems where an agent aims to maximize a scalar reward by interacting with the environment. Linear bandits, which use function approximation for the reward function, have been extensively studied but have limited representation power. In this paper, we investigate non-linear bandit problems and design stochastic zeroth-order gradient-like algorithms to attain minimax regret for structured polynomials. Our algorithms achieve optimal dimension dependency and can be extended to reinforcement learning in the generative setting. We also prove that solving polynomial equations achieves regret equal to the intrinsic algebraic dimension of the underlying polynomial class. However, we show that UCB algorithms have a sub-optimal sample complexity, indicating the need for improved algorithms in the noiseless setting.