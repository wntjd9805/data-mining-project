Normalization techniques are important for training deep neural networks (DNNs), with BatchNorm being the most popular method. BatchNorm has several advantageous properties that stabilize DNN training, but it also has limitations in certain scenarios. This paper explores alternative normalization layers to address these shortcomings and replace BatchNorm. The authors evaluate different normalization techniques, network architectures, batch sizes, and learning rates on CIFAR-100. The results show that each normalization method has its own success and failure modes. The paper aims to generalize the properties that explain these modes and determine when alternative techniques can replace BatchNorm. The contributions of the paper include stable forward propagation, informative forward propagation, and stable backward propagation analysis for different normalization techniques.