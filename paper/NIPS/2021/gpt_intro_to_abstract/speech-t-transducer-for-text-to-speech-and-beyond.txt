In this paper, we introduce a Transformer-based Transducer model called SpeechTransducer (Speech-T) for text-to-speech (TTS) synthesis. We address the challenges of applying Transducer to TTS, such as the differences in speech generation compared to text token generation in automatic speech recognition (ASR). We propose a lazy forward algorithm and a diagonal constraint to improve alignment learning between text and speech. Additionally, we extend Speech-T to support both TTS and ASR simultaneously in a single model. Experimental results demonstrate that Speech-T achieves competitive voice quality in TTS, supports streaming TTS, and provides the benefits of joint modeling TTS and ASR. This work contributes to leveraging the advantages of Transducer for TTS and demonstrates the feasibility of using a single model for both TTS and ASR tasks.