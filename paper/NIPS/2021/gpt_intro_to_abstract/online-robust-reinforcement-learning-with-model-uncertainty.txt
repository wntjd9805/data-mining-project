This paper addresses the problem of robust Markov decision process (MDP) and reinforcement learning (RL) in scenarios where the environment on which a learned policy will be deployed is different from the one used to generate the policy. Previous studies have focused on model-based approaches but these require a known model of the uncertainty set and large memory storage. In this paper, a model-free approach is proposed that learns a robust policy using a single sample trajectory from a misspecified MDP. The authors develop a robust Q-learning algorithm for the tabular case and extend it to cases with function approximation. They also introduce a novel extension of the gradient TD method to robust RL. Numerical experiments demonstrate the robustness and improved performance of the proposed algorithms.