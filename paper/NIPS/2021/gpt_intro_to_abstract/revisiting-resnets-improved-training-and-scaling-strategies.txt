The performance of vision models is influenced by various factors such as architecture, training methods, and scaling strategies. While new architectures often receive attention, the impact of changes in training methodology and hyper-parameters is less publicized. This paper focuses on studying the effect of training methods and scaling strategies on the widely used ResNet architecture. The authors survey modern training techniques and regularization methods, discovering the benefits of reducing weight decay values in conjunction with other regularization techniques. They also propose new scaling strategies and introduce ResNet-RS models that outperform EfÔ¨ÅcientNets on the speed-accuracy Pareto curve. Through empirical experiments, the paper demonstrates the efficacy of the improved training and scaling strategies and their applicability to various downstream tasks. The findings emphasize the importance of considering each factor individually to understand the performance of different architectures. Overall, the contributions of the paper include empirical studies on regularization techniques and scaling, the introduction of ResNet-RS models, and the comparison of supervised representations with self-supervised representations on computer vision tasks.