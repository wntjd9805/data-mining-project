This paper introduces TransGAN, a generative adversarial network (GAN) that is completely free of convolutions and instead uses pure transformer-based architectures. The paper addresses the instability and mode collapse issues that are commonly encountered in GAN training by developing a unique training recipe tailored to TransGAN. Additionally, the paper presents a novel architecture design for TransGAN, including a memory-friendly generator, a multi-scale discriminator, and a grid self-attention mechanism. Experimental results demonstrate that TransGAN achieves competitive performance compared to state-of-the-art GANs, both in terms of image quality and scalability to higher resolutions.