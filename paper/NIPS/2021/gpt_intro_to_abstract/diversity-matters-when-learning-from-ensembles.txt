Deep Ensemble (DE) is a popular method for ensembling deep neural networks, delivering high prediction accuracy and uncertainty estimation. While several studies have examined the effectiveness of DE, more sophisticated approximate Bayesian inference algorithms have struggled to match its performance. However, DE incurs significant overhead in terms of inference time and memory usage. To address this problem, Knowledge Distillation (KD), a method for transferring knowledge from a large teacher network to a smaller student network, has been proposed. Previous approaches have directly distilled the ensembled outputs of DE teachers to the student network, but the empirical performance of these distilled networks has been inferior to DE teachers. This paper introduces a method that aims to amplify the diversity of student networks learning from DE teachers. By using a perturbed training set, the authors demonstrate that their method improves the diversity of predictions and leads to better performance, particularly in terms of uncertainty estimation. Experimental results on standard image classification benchmarks validate the effectiveness of the proposed approach.