Free-viewpoint video of human performers has a wide range of applications, but existing systems require expensive equipment and struggle with sparse observations. This paper introduces a scalable solution that uses sparse camera views to represent and render arbitrary human performances. The proposed method combines temporal and multi-view transformers to aggregate appearance information from sparse observations across time, resulting in improved synthesis results for unseen motions and characters. The effectiveness of the approach is demonstrated on two datasets, outperforming recent methods and even person-specific methods for novel poses. The contributions of this paper include a new feed-forward method for synthesizing novel-view videos, a generalizable neural radiance representation, and a combination of temporal and multi-view transformers for robust aggregation of information.