Recent advancements in deep learning have provided significant breakthroughs in image, video, and audio processing. However, deep learning still has limitations such as low interpretability, vulnerability to adversarial attacks, and difficulty in solving problems with logical constraints. To overcome these limitations, researchers have proposed the integration of neural networks with logical reasoning, known as neurosymbolic artificial intelligence systems. This paper focuses on improving the SATNet architecture, a differentiable MAXSAT solver that combines logical reasoning and visual understanding. However, it has been observed that SATNet training relies on label leakage, which limits its ability to solve composite visual reasoning problems. This issue, known as the Symbol Grounding Problem, is considered a fundamental prerequisite for practical logical reasoning in artificial intelligence. In this work, the authors propose a self-supervised pre-training method that overcomes the Symbol Grounding Problem and achieves significant improvements in solving ungrounded Visual Sudoku problems. The contributions of this paper include a self-supervised clustering and distillation process, a Symbol Grounding Loss for training logical constraint layers, and the introduction of a Proofreader that improves the performance of SATNet on Visual Sudoku.