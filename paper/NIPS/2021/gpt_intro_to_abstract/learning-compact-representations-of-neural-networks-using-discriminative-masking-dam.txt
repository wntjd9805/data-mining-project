A central goal in deep learning is to learn compact representations of features in neural networks that can be useful in various machine learning tasks. This includes unsupervised representation learning and network pruning in supervised learning. While previous techniques have focused on continuous approximations of sparsity penalties, we are interested in structured network pruning, specifically at the neuron level. Current state-of-the-art methods suffer from training instabilities and the need for additional fine-tuning. In this paper, we propose a new single-stage structured pruning method called DiscriminAtive Masking (DAM) that learns compact representations without requiring fine-tuning. DAM utilizes a monotonically increasing gate function to gradually prune neurons while selectively refining others during training. Our approach achieves state-of-the-art performance in various applications and network architectures, provides a differentiable approximation for enforcing sparsity, and opens up new avenues for research in learning compact representations.