Policy optimization methods are widely used in reinforcement learning and have shown empirical success in domains such as computer games and robotics. However, these methods often rely on unrealistic assumptions to ensure global exploration, making them theoretically less appealing compared to other methods. Recent works have introduced exploration bonuses to policy optimization to address this issue and have provided favorable guarantees without additional exploratory assumptions. Despite these advances, limitations such as worse regret rates and requiring full-information feedback still exist. In this work, we propose a new type of exploration bonuses called dilated bonuses that improve exploration compared to existing methods. We apply this idea to advance policy optimization for learning episodic MDPs with adversarial losses and bandit feedback. Our results demonstrate improved regret rates, computational efficiency, and the critical role of dilated bonuses in achieving sublinear regret.