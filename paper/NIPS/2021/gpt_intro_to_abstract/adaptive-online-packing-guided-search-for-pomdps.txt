POMDPs, or Partially Observable Markov Decision Processes, extend MDPs by considering state uncertainty. In POMDPs, the agent does not have direct access to the actual state but receives observations as clues. To handle this uncertainty, maintaining a belief, a distribution on the state space, is a popular approach. However, exact belief updating requires extensive computations, making it impractical for large state spaces. An alternative method is to approximate the belief using weighted particles and update it with particle filtering. Solving POMDPs optimally or near-optimally is proven to be computationally complex, making online planning algorithms crucial. These algorithms focus on the current belief and take advantage of offline solutions and heuristics to expedite the search process. Previous approaches using simple belief approximation methods suffer from high variance and limited sample sizes. To address these limitations, we propose an online planning algorithm, named AdaOPS (Adaptive Online Packing-guided Search), which incorporates adaptive particle filtering and belief packing. Adaptive particle filtering resamples and adjusts the sample size based on belief dispersion, enabling accurate approximation with a small number of particles. Belief packing merges similar beliefs to balance estimation bias and variance. By converging to the (cid:15)-optimal policy, our algorithm demonstrates improved performance compared to existing methods.