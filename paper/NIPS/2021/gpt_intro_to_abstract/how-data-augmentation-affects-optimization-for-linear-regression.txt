Data augmentation is a popular technique used in training models using gradient-based optimization. However, the choice of augmentation hyperparameters can greatly affect the selected minimizer of the training loss and the resulting model quality. Existing methods for selecting and scheduling augmentations often lack theoretical grounding and require extensive computational resources. This paper aims to bridge the gap between empirical practices and theoretical results by studying the effect of jointly scheduling learning rate and data augmentation schemes. The authors focus on linear regression with mean squared error loss and analyze the convergence and resulting optimum of the optimization trajectory. They provide sufficient conditions for convergence and characterize the augmentation-dependent optimum. The results are then applied to gradient descent with additive input noise and random projections, showing the importance of joint scheduling for convergence to the minimum norm optimizer. Overall, this work contributes to understanding the impact of augmentation and provides insights into optimizing the learning process.