Object detection is a critical task with numerous real-world applications, but achieving a balance between speed and accuracy remains a challenge. To address this, researchers have focused on developing compact deep networks through methods like pruning, quantization, and knowledge distillation. While previous knowledge distillation techniques have primarily focused on image classification, this paper introduces a knowledge distillation approach for object detection. The authors propose a classifier-to-detector knowledge distillation method to improve the performance of a student detector using a classification teacher. They also develop distillation strategies to enhance both the classification accuracy and localization ability of the student. Experimental results on the COCO2017 benchmark demonstrate the effectiveness of their approach, outperforming existing distillation techniques.