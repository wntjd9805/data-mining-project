Reinforcement learning (RL) using value function approximation through deep neural networks has shown significant success in narrow problem domains. However, when RL systems are placed in larger systems, challenges arise due to positive bias in value computation and the lack of honest assessments. This paper explores the tension between correcting the overestimation and leveraging it for exploration. The hypothesis is that the degree of estimation bias and the efficacy of an optimistic strategy depend on the environment, optimization stage, and learner context. The authors propose a framework called Tactical Optimism and Pessimism (TOP) that actively moves along the optimism/pessimism spectrum by measuring aleatoric and epistemic uncertainty. TOP achieves state-of-the-art results in continuous control problems by dynamically balancing optimism and pessimism through a multi-armed bandit approach.