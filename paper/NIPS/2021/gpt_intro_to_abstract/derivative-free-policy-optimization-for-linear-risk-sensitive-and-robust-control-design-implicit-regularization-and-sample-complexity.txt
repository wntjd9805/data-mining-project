Recent progress in reinforcement learning (RL) has focused on policy optimization (PO) methods for continuous control tasks. Most existing research has investigated the theoretical properties of model-free PG methods for simple baselines, leaving the theory of PO methods for risk-sensitive/robust control largely unexplored. In this paper, we study the sample complexity of model-free PG methods on two important baseline problems in risk-sensitive/robust control: the linear exponential quadratic Gaussian (LEQG) and linear quadratic (LQ) disturbance attenuation problems. We develop a unified PO perspective for both problems, taking into account their challenging optimization landscapes and non-coercive objective functions. Our work provides the first sample complexity results for model-free PG methods for linear control problems with risk-sensitivity/robustness concerns and addresses the convergence of policy-based methods for competitive multi-agent RL. Additionally, our results analyze minimax optimization and provide sample complexity analysis for PG methods solving the LQR problem with system noises and a potentially indefinite state-weighting matrix.