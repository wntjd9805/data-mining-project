Deep neural networks (DNNs) are widely used in various applications but their vulnerability to adversarial examples has raised concerns, particularly in safety-critical domains like autonomous driving. While methods such as adversarial training improve the empirical robustness of DNNs, they lack provable robustness guarantees. To address this, recent works focus on certifiable robustness evaluation using verifiers. In this paper, we identify two key issues in certifiable training: suboptimal weight initialization and imbalanced ReLU activation states. We propose improvements including a new weight initialization method, the use of batch normalization, and the addition of regularizers to stabilize bounds and balance activation states. Our experiments demonstrate that these improvements lead to more efficient training of certifiably robust models that outperform previous state-of-the-art methods.