Deep neural networks have achieved promising performance on various tasks, but their applicability in real-world scenarios is limited by the assumption that training and testing samples come from the same data distribution. To address this limitation, domain generalization (DG) techniques have been proposed to train models that can generalize to unseen target domains. Existing DG approaches focus on deriving domain-invariant features or using meta-learning techniques to simulate domain shifts during training. However, these methods do not guarantee the generalizability of learned representations to describe unseen domain data. To overcome this limitation, some works have leveraged data generation techniques to diversify the source distributions and improve model generalization. In this paper, we propose an Adversarial Teacher-Student Representation Learning framework for domain generalized visual classification. Our approach combines the concepts of multi-view learning and domain generalized representation learning through an adversarial learning scheme. We propose a co-training scheme that utilizes original images as inputs to the teacher network and takes stylized augmentations as input to the student network. The teacher network learns from the student network's distilled knowledge obtained from augmented novel-domain data, enabling it to be more generalizable to data with out-of-source distributions. Additionally, we introduce an adversarial novel domain augmentation stage that aims to produce unseen yet plausible domain data by maximizing the discrepancy between augmented and existing domains while preserving semantic information. Experimental results on benchmark datasets demonstrate the effectiveness of our method in domain generalization.