Reinforcement Learning (RL) has achieved remarkable successes in various domains, such as playing video games, surpassing champions in Go, and defeating professional players in StarCraft. However, most efficient RL algorithms are limited to small state spaces, while RL applications often involve large state spaces with rich observations. To address this challenge, existing work in RL theory relies on structural assumptions, such as low-rank transitions. In this paper, we initiate the study of agnostic RL, which relaxes the assumption of realizability and focuses on learning to perform well regardless of how closely the policy class represents the Bellman-optimal policy. We propose a sample-efficient algorithm for agnostic RL and provide insights into the learnability of RL with structural assumptions. Our contributions include new upper and lower bounds on sample complexity, as well as an exploration-based algorithm that finds a sub-optimal policy. We also discuss the potential for future research in agnostic RL.