This paper introduces Luna, a linear unified nested attention mechanism, as an approach to improve the time and memory efficiency of Transformers in handling long input sequences. Luna utilizes two nested attention functions to approximate regular softmax attention, packing the input sequence into a fixed-length sequence and then unpacking it. Compared to Linformer, Luna can handle variable-length sequences and autoregressive attention. Experimental results demonstrate that Luna achieves competitive or better performance while offering significant gains in speed and memory efficiency. In addition, Luna performs well with small projection lengths, further enhancing its efficiency.