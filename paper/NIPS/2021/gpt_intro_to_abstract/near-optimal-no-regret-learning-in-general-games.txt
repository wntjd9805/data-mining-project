Online learning has a long history connected to the development of game theory, convex optimization, and machine learning. Fictitious play was proposed as a method to solve two-player zero-sum games, showing convergence to Nash equilibria, but with slow convergence in matrix games and non-convergence in non-zero-sum games. However, the use of specialized no-regret learning procedures, such as Nesterov's excessive gap technique, can improve convergence rates. This paper presents a simple algorithm called Optimistic Hedge that guarantees O(m log^4 T) regret for each player in general-sum multi-player games, achieving polylogarithmic regret and approximation to coarse correlated equilibria. The results strengthen the plausibility of the common assumption made in the literature that agents will choose to use no-regret algorithms.