Discovering the statistical and causal dependencies of variables in a data-generating system is a key scientific pursuit. Bayesian networks (BNs) and structural equation models are commonly used, but the task of learning the structure of a BN from observations is computationally challenging. Most existing methods provide a single plausible graph, while Bayesian structure learning aims to infer a full posterior distribution over BNs to quantify uncertainty. However, working with a posterior over BNs is challenging due to the discrete nature of current approaches. In this paper, we propose DiBS, a novel framework for Bayesian structure learning that operates in the continuous space of a latent probabilistic graph representation. Our approach allows for inference of both the conditional distribution parameters and the graph structure, making it applicable to more flexible BN models. Experimental results show that DiBS outperforms existing methods and enables precise causal graph inference and better predictions under interventions.