This paper introduces a problem of risk-averse decision making in model-based Bayesian reinforcement learning (RL). The uncertainty in RL arises from both imperfect knowledge of the underlying model (epistemic uncertainty) and the inherent stochasticity of the system (aleatoric uncertainty). The authors propose addressing this problem by optimizing the conditional value at risk (CVaR) of the total return in a Bayes-Adaptive MDP (BAMDP) with an augmented state space. They formulate the CVaR optimization as a stochastic game against an adversarial environment and present an algorithm based on Monte Carlo tree search (MCTS) and Bayesian optimization. The experimental results demonstrate the effectiveness of the proposed algorithm in mitigating both epistemic and aleatoric uncertainty. This work is the first to simultaneously address both sources of uncertainty and to present an MCTS algorithm for CVaR optimization in sequential decision making problems.