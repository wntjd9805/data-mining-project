This paper introduces online reinforcement learning algorithms with linear function approximation under adaptivity constraints. The authors consider time-inhomogeneous episodic linear Markov decision processes (MDPs) where the transition probability and reward function are unknown. Two scenarios of limited adaptivity are studied: the batch learning model and the rare policy switch model. For each model, the authors propose a variant of the LSVI-UCB algorithm and provide regret bounds. The algorithms achieve the same regret as LSVI-UCB with significantly fewer batches/policy switches, enabling parallel learning and improving the scalability of RL algorithms with linear function approximation. The paper concludes with a discussion of related works, theoretical results, a numerical experiment, and future directions.