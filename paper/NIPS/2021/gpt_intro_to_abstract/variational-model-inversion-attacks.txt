Recent advances in deep neural networks have led to their widespread use in various applications, such as facial recognition, personalized medicine, and intelligent assistants. However, these powerful networks are trained on large datasets that may contain sensitive information. To protect individuals' privacy, publishers only release the trained models and not the original dataset. Unfortunately, these published models can still leak information about the training set, which leads to privacy attacks. This paper focuses on a specific privacy attack called the model inversion (MI) attack, where an attacker tries to recover the original training set by only having access to the trained classifier. The paper frames MI attacks as a variational inference problem and proposes a practical objective based on statistical divergence. Empirical results show that the proposed method improves attack accuracy and diversity compared to existing methods.