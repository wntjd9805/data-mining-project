Meta-learning has become a crucial tool for adapting prior knowledge to new tasks under data and computational constraints. A meta-learner leverages related source tasks to uncover inductive biases and reduce the complexity of learning new tasks. Representation learning, such as learning a feature extractor, is a common approach in meta-learning. However, fine-tuning the entire network has been shown to provide substantial performance gains compared to just learning the final linear layer. This paper aims to analyze the sample complexity of fine-tuning in a more realistic setting where tasks only approximately share the same representation. The authors propose a theoretical framework and demonstrate that fine-tuning with an initial representation can quickly adapt to new tasks with fewer samples compared to methods that use "frozen representation" objectives. The paper also provides an in-depth analysis of the linear representation setting and extends the analysis to general function classes.