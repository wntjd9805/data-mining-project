Recurrent neural networks (RNNs) have shown success in learning complex patterns for sequential input data. However, training RNNs faces the challenge of vanishing or exploding gradients, which negatively impact training stability. In this paper, we explore different optimization methods to address this issue, including penalty methods and bilevel optimization. We propose a new family of RNNs called SBO-RNN that leverages stochastic bilevel optimization to optimize both the model parameters and the hidden state vectors. We demonstrate through experiments that our approach achieves superior performance with improved training stability, fewer parameters, and faster convergence.