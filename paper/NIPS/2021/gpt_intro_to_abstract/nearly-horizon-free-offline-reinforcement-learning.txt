Reinforcement learning (RL) has proven successful in various applications, such as game-playing, robotics, and algorithm design. However, in real-world applications like education, health, conversational AI, and recommendation systems, direct interactions with the environment may be expensive or impossible. This led to the emergence of offline reinforcement learning (RL), which focuses on evaluating and improving policies based on past experiences without real-time interactions. This paper aims to overcome the "curse of horizon" in offline policy evaluation (OPE) by introducing marginalized importance sampling (MIS) based estimators to adjust the distribution mismatch between the target policy and the empirical data. They also provide nearly horizon-free bounds for OPE and offline policy optimization (OPO) in time-homogeneous tabular MDP and linear MDP with anchor points, using a novel recursion-based method to bound the "total variance" term. The paper concludes by discussing the implications and generalization of the results.