Policy Space Response Oracles (PSRO) is a reinforcement learning (RL) method that has shown impressive results in finding approximate Nash equilibria (NE) in large two-player zero-sum games. PSRO has the advantage of being compatible with continuous actions, unlike other deep RL methods such as self-play. However, PSRO may require exponentially growing iterations in the worst case. To address this limitation, we propose a new algorithm called Extensive-Form Double Oracle (XDO), specifically designed for extensive-form games. XDO guarantees convergence to an approximate NE in a number of iterations linear in the number of infostates, outperforming PSRO. Additionally, we introduce a neural version of XDO called Neural XDO (NXDO) that can be used in games with large action spaces and generalizes over infostates through neural network strategies. NXDO achieves better performance than PSRO and NFSP on modified Leduc poker and sequential continuous-action games, making it the first method capable of finding approximate NE in high-dimensional continuous-action sequential games.