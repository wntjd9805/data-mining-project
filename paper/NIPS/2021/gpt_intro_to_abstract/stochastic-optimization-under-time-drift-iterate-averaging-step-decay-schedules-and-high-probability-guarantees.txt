Stochastic optimization is a fundamental component of machine learning, but current theories and algorithms assume a fixed data distribution throughout the learning process. However, in many real-world scenarios, the data distribution changes over time due to various factors. This paper focuses on two examples: stochastic tracking, where the goal is to track a moving target driven by an unknown stochastic process, and concept drift in online learning, where the true hypothesis changes over time. The paper presents finite-time efficiency estimates for the tracking error of the proximal stochastic gradient method under time drift, considering the interplay between the learning rate, noise variance, and strength of the time drift. The results demonstrate the effectiveness of a geometrically decaying step size schedule, challenging the conventional use of constant step sizes under time drift.