Machine learning for source code is an important area of research in computer science, with the goal of building models that can understand the semantic meaning of programs. Previous approaches have used sequential models from natural language processing, but they struggle to capture the structural complexity of source code. Some works have leveraged the Abstract Syntax Tree (AST) of code to model its structure, while others have used structured graph neural networks. However, these approaches have limitations such as the need for synchronous message passing and the inability to leverage long-range interactions. In this paper, we propose integrating path encoding into the Transformer model to capture the structural information of source code. We introduce two types of paths, relative paths representing the relationships between code tokens and absolute paths representing the behavior of individual tokens. We demonstrate the effectiveness of our approach in code summarization tasks across different programming languages, outperforming existing baselines. Our contributions include the development of TPTRANS, a novel code representation model, and an empirical study on the relationship between path encoding methods.