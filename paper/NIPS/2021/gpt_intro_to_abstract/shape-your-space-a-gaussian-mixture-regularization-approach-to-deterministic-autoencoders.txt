Variational autoencoders (VAEs) are a popular generative learning framework widely used in various applications. The VAE framework combines autoencoders with variational inference, where the encoder maps input data to a lower-dimensional latent space and the decoder maps the latent space back to the original input space. The VAE training objective poses challenges and the simplistic prior assumption often leads to a trade-off between reconstructed sample quality and prior regularization. Recent work has shown that choosing more flexible priors improves generative performance. Different regularization techniques, such as the use of the Wasserstein distance, have been proposed to shape the latent space during training. However, closed-form solutions for these metrics are limited, requiring numerical approximations. This paper proposes a deterministic training scheme for autoencoders that overcomes the need for post-hoc density estimation and is applicable to expressive priors. The proposed method derives a deterministic regularization loss from the non-parametric Kolmogorov-Smirnov test, enabling efficient shaping of the latent space during training. The paper evaluates the proposed approach in terms of sampling quality and expressiveness, comparing it to other VAE variants and investigating its capability to model discrete and complex structured inputs.