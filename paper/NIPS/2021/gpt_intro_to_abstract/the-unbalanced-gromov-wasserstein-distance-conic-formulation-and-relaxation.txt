Comparing data distributions on different metric spaces is a fundamental problem in machine learning. This problem arises in various applications such as surfaces, graph matching, regression in quantum chemistry, and natural language processing. To formalize and analyze these problems, data is modeled as metric measure spaces (mm-spaces). In this paper, we propose a class of distances between unbalanced mm-spaces and provide an efficient numerical scheme to approximate the associated divergences. Specifically, we introduce Csiszár divergences, which perform a pointwise comparison, and the Gromov-Wasserstein distance, which generalizes optimal transport to isometries. We discuss the applications and computational challenges associated with these distances. By relaxing the constraint of µ being a probability distribution, our proposed distances are more suitable for practical machine learning applications.