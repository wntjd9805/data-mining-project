Self-supervised learning (SSL) is a promising approach for learning useful representations without labeled data. Contrastive SSL methods have achieved impressive results in learning image representations. Similar approaches have been adapted to graph neural networks, but they often require complex designs and architectures. In this paper, we propose Canonical Correlation Analysis inspired Self-Supervised Learning on Graphs (CCA-SSG), a simple and effective approach that eliminates the need for negative samples and intricate components. Our objective aims to maximize correlation between augmented views of the same input while decorrelating different dimensions of a single view's representation. We show that our model is an instantiation of the Information Bottleneck Principle and demonstrate its competitive performance on node classification benchmarks. Our approach is agnostic to data format and can potentially be applied to other scenarios beyond graph-structured data.