Reinforcement learning (RL) coupled with deep neural networks has become a powerful solution for learning complex decision-making policies. However, many RL methods require large amounts of data, which is not feasible in various real-world settings. This has led to a growing interest in "offline RL," where existing datasets are used to learn decision-making policies. In this paper, we focus on a scenario of severe data scarcity, where multiple agents with unique state transition dynamics are observed through a single historical trajectory. We propose a framework called PerSim, which leverages the limited offline data to learn personalized policies for each agent. We address challenges such as data scarcity, heterogeneity, and sub-optimal policies, and provide methodological, theoretical, algorithmic, and experimental contributions. Our experimental results demonstrate the effectiveness of PerSim in producing accurate personalized simulators and improving reward performance compared to state-of-the-art methods.