Language learning can benefit from grounded visual cues, but current large-scale language models lack sufficient multi-modal grounding information. Previous work has explored methods for grounding language to visual information, but image-text datasets are smaller and less diverse than text-only corpora. To address these limitations, this paper proposes a Video-and-Language Knowledge Distillation method (VIDLANKD) that transfers knowledge from a teacher model to a student model by minimizing the distance between contextualized text representations. The teacher model is pretrained on a multi-modal dataset using video and language encoders, and different knowledge distillation objectives are used to avoid approximation errors. Experimental results demonstrate that language models trained with VIDLANKD outperform baseline models and models trained with vokenization on various natural language understanding benchmarks. The proposed method also successfully learns linguistic world knowledge and physical/temporal commonsense abilities from video data. Overall, this paper contributes a novel cross-modal knowledge distillation method, the use of rich video-text data, empirical improvements on language understanding benchmarks, and analysis on learned knowledge from videos.