Deep neural networks (DNNs) have revolutionized various fields, but their theoretical properties are still not well understood. This lack of understanding has led to a focus on explaining how and why neural networks learn. Generalization error, which measures the performance difference between training and test data, has been studied to gain insights into network learning. However, generalization in neural networks contradicts classical statistical learning theory. Recent analyses have shifted focus to the dynamics of deep neural networks, connecting generalization error to the fractal dimension of the optimization trajectories. However, previous studies have limitations and are not applicable in everyday training. In this paper, we address these limitations by leveraging topological data analysis (TDA) and develop a topological intrinsic dimension (ID) estimator. We then use this estimator to compute and visualize generalization properties in deep learning. By incorporating differentiable TDA tools, we can regularize training towards solutions with better generalization, even without access to the test dataset. Our experiments demonstrate that our ID estimator correlates highly with generalization error and that our topological regularizer improves test accuracy and lowers generalization error. Our contributions include making a novel connection between statistical learning theory and TDA, providing a generic computational framework for the generalization error, leveraging persistent homology for training regularization, and providing extensive experiments to validate our framework. We believe that our work opens new theoretical and computational directions in deep learning and release our source code for further development.