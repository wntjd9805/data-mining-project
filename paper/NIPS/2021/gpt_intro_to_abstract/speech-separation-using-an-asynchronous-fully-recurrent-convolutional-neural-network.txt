Speech separation is a crucial step in speech recognition in noisy environments, aiming to extract individual speeches from a mixture of speeches. Recent advancements in speech separation methods at the waveform level have shifted away from traditional time-frequency domain methods and introduced multi-scale fusion (MSF) methods to achieve impressive results. In this paper, we explore the potential for even better MSF methods by drawing inspiration from the observations of mammalian sensory systems, which utilize MSF in their processing. We propose a fully recurrent convolutional neural network (FRCNN) model with asynchronous updating, termed A-FRCNN, for speech separation. We compare the performance of A-FRCNN against the synchronous FRCNN (S-FRCNN) and demonstrate the merits of the A-FRCNN in comprehensive MSF.