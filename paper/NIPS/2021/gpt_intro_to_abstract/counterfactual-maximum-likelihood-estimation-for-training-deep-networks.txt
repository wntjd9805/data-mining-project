Deep neural networks have achieved great success in various tasks, but they often learn spurious correlations instead of the true underlying causal mechanisms. This can lead to poor generalization and reliance on irrelevant features. In this paper, we propose a Counterfactual Maximum Likelihood Estimation (CMLE) framework to mitigate the impact of spurious correlations. CMLE operates on an interventional distribution, which involves intervening on the label variable to remove observable confounders. We focus on predicting the outcome of an action given an input, and we experiment with relationship prediction and conditional generation tasks. We derive two upper bounds of the expected interventional negative log-likelihood using only observational data, and we show that our framework improves performance on challenging datasets and reduces spurious correlations without sacrificing accuracy.