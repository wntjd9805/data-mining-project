Quantization is a popular method for compressing neural networks, but it typically requires access to the original training data for fine-tuning. In cases where the original data is inaccessible for reasons like privacy or security, data-free quantization becomes necessary. Generative methods have shown promise in generating synthetic samples that resemble the original dataset, but there is still a gap between data-free quantized models and those fine-tuned with original data. We propose Qimera, a method that uses superposed latent embeddings to create boundary supporting samples, addressing the missing piece in current data-free compression techniques. Our experiments demonstrate that Qimera achieves state-of-the-art performance in data-free quantization, surpassing existing methods.