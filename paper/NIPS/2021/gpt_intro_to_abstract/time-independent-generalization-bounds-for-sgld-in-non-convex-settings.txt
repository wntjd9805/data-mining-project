Investigating the generalization error of learning algorithms is a crucial problem in machine learning, with recent interest in studying the generalization capabilities of stochastic gradient descent (SGD) and its variants. One such variant, stochastic gradient Langevin dynamics (SGLD), has been shown to work well in various settings. However, there has been limited exploration of SGLD's learning capabilities, particularly in non-convex settings. This paper aims to fill that gap by obtaining expected generalization error bounds for SGLD under dissipativity and smoothness assumptions. The analysis includes both discrete-time and continuous-time algorithms, yielding time-independent bounds that decay to zero as the sample size increases. The proof technique relies on uniform stability and the contraction property of Langevin diffusions in Wasserstein distance. The results diverge from traditional stability-based bounds, as some of the obtained bounds diverge as the learning rate approaches zero. Overall, the paper contributes to understanding the generalization capabilities of SGLD in non-convex learning problems.