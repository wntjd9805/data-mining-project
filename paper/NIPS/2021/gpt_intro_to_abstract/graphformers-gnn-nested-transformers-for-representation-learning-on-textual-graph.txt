In this paper, we propose "GNN-nested Transformers" (GraphFormers) as a model architecture for combining graph neural networks (GNNs) and language models. We highlight the fusion of GNNs and language models through an iteratively workflow, where the text encoding and graph aggregation are performed together. This allows for the exchange of information between linked nodes, enhancing the representation quality. Compared to the existing cascaded Transformers-GNN models, GraphFormers achieve more efficient utilization of cross-node information on the graph without significantly increasing running costs. We further improve GraphFormers' representation quality and practicability by implementing progressive training and introducing unidirectional graph attention to reduce unnecessary computations. Experimental evaluations on three million-scale textual graph datasets demonstrate that GraphFormers outperform existing baselines in terms of link prediction accuracy while maintaining comparable efficiency.