The empirical success of Deep Learning has prompted the need for theoretical foundations explaining the efficiency of learning from high-dimensional data with neural networks. However, there are still large gaps between positive and negative results for learning, including for simple neural network architectures. This paper focuses on establishing negative results for improper learning in the distribution-specific setting, where the learner can exploit known input distribution peculiarities. Previous work has shown the hardness of learning two-hidden-layer neural networks, but open questions remain regarding the simpler case of learning one hidden-layer neural network over a standard Gaussian input. The paper investigates the difficulty of learning cosine neurons in the presence of noise and presents a proof that this learning task is hard for any polynomial-time algorithm based on a cryptographic assumption. The paper also explores the broader context of inferring hidden structures in noisy high-dimensional data and provides an upper bound for the computational threshold using the Lenstra-Lenstra-Lov√°sz (LLL) lattice basis reduction algorithm.