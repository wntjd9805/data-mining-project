In recent years, deep learning methods have achieved significant success in various domains, including natural language processing, computer vision, and graph analysis. Inspired by these advancements, researchers in the field of chemistry have also started exploring the application of deep learning techniques for molecule-based tasks such as retrosynthesis and drug discovery. Molecules can be represented as graphs, where atoms are nodes and chemical bonds are edges, to preserve their internal structural information. Graph Neural Networks (GNNs) and their variants have been used for molecular property prediction with promising results. However, GNNs typically require large amounts of labeled data for training, which poses a challenge as labeled molecules are scarce due to the constraints of wet-lab experiments and quantum chemistry calculations. This paper addresses these challenges by proposing a self-supervised pre-training approach for GNNs, leveraging the success of similar approaches in natural language processing and computer vision. The authors argue that existing self-supervised learning tasks for GNNs do not effectively exploit the rich semantic information from graph motifs, which are significant subgraph patterns indicative of the characteristics of the whole graph. To address this, the paper introduces Motif-based Graph Self-Supervised Learning (MGSSL) and Multi-level self-supervised pre-training. MGSSL includes a novel motif generation task using the BRICS algorithm to derive semantically meaningful motifs for molecular graphs. A motif-based generative pre-training framework is designed to generate molecular graphs motif-by-motif, considering the topology and attribute predictions at each step. Multi-level self-supervised pre-training is introduced to adaptively adjust the weights of different self-supervised learning tasks based on the Frank-Wolfe algorithm. The proposed methods are evaluated on a benchmark dataset and outperform state-of-the-art baselines, demonstrating their effectiveness. The implementation code is publicly available.