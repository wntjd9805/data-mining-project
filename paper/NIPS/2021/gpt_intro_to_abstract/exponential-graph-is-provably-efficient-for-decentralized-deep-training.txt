Efficient distributed training methods are essential for large-scale deep learning tasks. Parallel stochastic gradient descent (SGD) is commonly used, but it incurs high bandwidth cost or latency due to global coordination across all nodes. Decentralized SGD based on partial averaging offers a promising alternative, with lower communication overhead per iteration. However, decentralized SGD requires more iterations to achieve the same convergence as parallel SGD, resulting in slower convergence. The choice of network topology plays a crucial role in balancing communication and iteration complexity. This work focuses on exponential graphs, specifically static and one-peer exponential graphs, and analyzes their connectivity and averaging effectiveness. The results show that one-peer exponential graphs have the same convergence rate as static exponential graphs while offering significantly lower communication and transient iteration complexity. Extensive experiments validate the theoretical findings.