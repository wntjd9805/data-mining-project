Learning from a continuous stream of data, known as continual learning (CL) or lifelong learning, has gained significant attention. In this paper, we focus on online task-free CL, which lacks task identifiers and boundaries, simulating real-world data streams. Memory-based methods have been used for task-free continual learning, where a small number of training examples are stored and replayed. However, using stored examples in their original form may lead to diminishing utility over time. To address this, we propose a novel memory-based CL framework called Gradient based Memory EDiting (GMED), which directly edits the stored examples to enhance their effectiveness in overcoming catastrophic forgetting. GMED prioritizes the editing of "interfering" examples and enforces the proximity of edited examples to the original samples. The effectiveness of GMED is demonstrated through extensive experiments on benchmark datasets, showcasing consistent improvements and establishing a new state-of-the-art performance. Our contributions include the introduction of GMED and a comprehensive evaluation of its performance across various datasets and parameter setups.