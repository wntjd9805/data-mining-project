Reinforcement learning (RL) is a promising approach for teaching autonomous agents complex behaviors, but its adoption in robotics is hindered by challenges such as the assumption of a specific initial state distribution. Current RL algorithms rely on human supervision or engineered reset mechanisms, which are time-consuming and limit the autonomy of the learning process. To address these challenges, this paper presents Value-accelerated Persistent Reinforcement Learning (VaPRL), a goal-conditioned RL method that creates an adaptive curriculum of starting states for the agent. VaPRL reduces the reliance on external reset mechanisms and achieves up to a 30% improvement in sample efficiency compared to state-of-the-art methods. Experimental results demonstrate its effectiveness in various robotic control tasks.