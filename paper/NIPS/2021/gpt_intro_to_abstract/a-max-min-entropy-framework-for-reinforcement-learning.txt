The introduction of this computer science paper discusses the use of the maximum entropy framework in reinforcement learning (RL) domains. Maximum entropy RL aims to maximize both the return and the entropy of policy distribution by encouraging the policy to choose multiple actions probabilistically. This approach has shown improvements in exploration, robustness, and final performance in various control tasks. However, the iterative implementation of the maximum entropy strategy may hinder exploration in certain scenarios. To address this limitation, the paper proposes a max-min entropy framework for RL, which aims to learn policies that reach states with low entropy while maximizing the entropy of these states. The proposed algorithm, implemented using an iterative actor-critic approach, demonstrates significant enhancements in exploration capability and performance compared to existing RL algorithms.