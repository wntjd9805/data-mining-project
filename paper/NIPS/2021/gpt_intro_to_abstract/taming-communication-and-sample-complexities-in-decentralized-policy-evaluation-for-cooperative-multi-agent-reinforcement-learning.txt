This paper explores the challenges of developing efficient policy evaluation (PE) algorithms for multi-agent reinforcement learning (MARL). It focuses on the cooperative MARL paradigm, where agents work together to find an optimal global policy. The paper proposes a decentralized PE approach using nonlinear function approximations and introduces the GT-GDA algorithm for solving the decentralized non-convex-strongly-concave minimax problem. Additionally, two variance-reduced algorithms are developed to further reduce sample complexity. Theoretical analysis and numerical results are provided to validate the proposed algorithms. The paper addresses the importance and challenges of efficient PE algorithms in MARL and presents new contributions in this area.