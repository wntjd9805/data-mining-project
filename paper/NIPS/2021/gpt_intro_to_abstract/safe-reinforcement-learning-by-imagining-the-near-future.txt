Reinforcement learning (RL) is widely used for sequential decision-making tasks, but in domains with potential safety risks, ensuring a safe policy is crucial. This paper introduces a model-based approach to safe exploration in RL, focusing on estimating system dynamics and using the model for planning. By anticipating safety violations through forward prediction, the authors propose a reward function that guarantees safety and present a model-based algorithm called Safe Model-Based Policy Optimization (SMBPO). Experimental results demonstrate that SMBPO effectively reduces safety violations in various continuous control tasks compared to model-free safe RL algorithms. Code for the algorithm is available at the provided GitHub repository.