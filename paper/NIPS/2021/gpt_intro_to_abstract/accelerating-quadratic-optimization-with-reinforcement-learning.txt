Efficiently solving quadratic programs (QPs) is crucial in various fields such as finance, robotic control, and operations research. While existing methods have limitations in terms of scalability and convergence rate, the Alternating Direction Method of Multipliers (ADMM) has proven to be an efficient first-order optimization algorithm. However, it requires tuning of hyperparameters, particularly the step size parameter œÅ, which affects convergence. In this paper, we propose RLQP, an accelerated QP solver based on ADMM that utilizes reinforcement learning (RL) to adapt the internal parameters of the algorithm and minimize solve times. We provide two RL formulations, scalar and vector, and our experimental results demonstrate that RLQP outperforms the state-of-the-art solver OSQP by up to 3x in reducing convergence times and generalizes well across different problem classes.