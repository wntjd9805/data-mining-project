Understanding the performance of neural networks is a major challenge in the machine learning community. While progress has been made in analyzing neural networks from a theoretical perspective, many questions remain unanswered. One such question is why the currently used training algorithms converge to solutions that generalize well, even without explicit regularization. The concept of implicit bias has emerged as a way to understand this phenomenon, suggesting that the optimization procedure implicitly selects a particular solution. Previous studies have focused on gradient descent algorithms, but there is a lack of research on stochastic gradient descent, which is widely used in practice. In this paper, we investigate the implicit bias of stochastic gradient descent and compare it with gradient descent. We focus on simplified neural networks, specifically diagonal linear networks, and analyze stochastic gradient flow as a continuous model for stochastic gradient descent. Our main result shows that stochastic gradient flow converges to a zero-loss solution with better generalization properties compared to gradient flow. We also provide experimental validation and further analysis of our model.