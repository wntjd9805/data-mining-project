Deep neural networks (DNNs) have demonstrated impressive performance in various tasks, but their lack of interpretability hinders the analysis of knowledge representations. Existing research on explainable AI has primarily focused on explaining the knowledge encoded within DNNs and evaluating the representation power of these networks. This paper aims to analyze the quality of knowledge representations in DNNs for 3D point cloud processing by designing novel metrics to assess regional sensitivities, spatial smoothness, and representation complexity. The proposed metrics provide new insights into the vulnerabilities and strengths of DNNs for 3D point clouds, revealing limitations in rotation robustness, extraction of rotation-robust features, and modeling of local 3D structures. Additionally, the spatial smoothness and representation complexity metrics shed light on the effects of adversarial training on DNNs. Overall, this work contributes to a deeper understanding of knowledge representations in DNNs for 3D point cloud processing.