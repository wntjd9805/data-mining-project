Multi-modal audio-visual learning has become an important area in machine learning and computer vision. Previous methods in this field have utilized pre-trained backbones to extract features from visual and auditory modalities, but there is a lack of attention to audio-visual representation learning and the integration of pose as a modality. In this paper, we propose a human-centric audio-visual representation learning architecture inspired by ViLBERT that aims to improve audio-visual sound source separation. Our model takes video, audio, and pose keypoints as inputs and co-attends among these modalities to generate enriched representations. We demonstrate that our model produces highly contextualized representations that improve sound source separation and show competitive performance on other audio-visual tasks. Our approach does not rely on global frame-wise features or external proposal mechanisms, and we introduce spectrogram mask prediction as a pre-training task to enhance task-specific feature learning.