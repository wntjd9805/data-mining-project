In high-stakes applications such as autonomous vehicles and medical diagnosis, well-founded decisions made by machine learning systems are crucial. However, these systems can exhibit unintended behavior when confronted with novel situations due to pathologies in models and their training datasets. This paper focuses on the concept of overinterpretation, where a classifier finds strong class-evidence in regions of an image that lack semantically salient features. Overinterpretation can be harder to diagnose and can result from true statistical signals in the underlying dataset distribution. The paper analyzes popular convolutional neural network architectures on benchmark datasets to characterize overinterpretation. The study introduces the concept of Sufficient Input Subsets (SIS) to identify which features are used by a model for its decision-making process. The paper demonstrates that classifiers trained on CIFAR-10 and ImageNet datasets can rely on SIS subsets that contain few pixels and lack understandable semantic content. The benchmarks contain statistical shortcuts that classifiers exploit instead of learning complex semantic relationships. The study shows that overinterpretation can harm classifier performance. Mitigation techniques such as model ensembling and input dropout are explored, but better training data is emphasized as a solution. The paper introduces the Batched Gradient SIS masking algorithm and provides a pipeline for detecting and evaluating overinterpretation. Strategies for mitigating overinterpretation are identified. The paper concludes that overinterpretation is a common failure mode of ML models and emphasizes the need for carefully curated training data to eliminate overinterpretation artifacts.