This paper introduces a novel method, called constrained backpropagation (CBP), which incorporates given constraints into the backpropagation algorithm used in deep learning. The method utilizes a Lagrangian function as the objective function, allowing for any set of constraints on the weights. The authors also propose the pseudo-Lagrange multiplier method (pseudo-LMM) to handle constraint functions that are nondifferentiable at the optimal point. They analyze the kinetics of pseudo-LMM and introduce optimal constraint functions with gradually vanishing unconstrained-weight windows. The performance of CBP is evaluated on various deep neural networks pre-trained using backpropagation with full-precision weights, and the results show improved classification accuracy compared to previous state-of-the-art approaches.