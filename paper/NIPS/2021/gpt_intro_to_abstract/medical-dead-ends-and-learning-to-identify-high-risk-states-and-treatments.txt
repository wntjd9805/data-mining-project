Off-policy Reinforcement Learning (RL) is crucial in safety-critical domains like robotics, education, and healthcare, where data collection is expensive or carries risks. However, the performance of off-policy RL algorithms degrades significantly in fully offline settings without additional interactions with the environment, especially when data is limited. In this paper, we present a novel RL-based method called Dead-end Discovery (DeD) that aims to identify treatments to avoid rather than selecting optimal treatments. DeD uses retrospective analysis of observed outcomes to constrain the policy's scope and avoid future dead-ends or regions with inevitable negative outcomes. We validate DeD in a toy domain and evaluate its performance using real health records of septic patients in an ICU setting. DeD successfully identifies dead-ends and shows that a significant portion of treatments administered to terminally ill patients reduce their chances of survival. We propose the use of DeD in other data-constrained sequential decision-making problems in safety-critical applications of RL.