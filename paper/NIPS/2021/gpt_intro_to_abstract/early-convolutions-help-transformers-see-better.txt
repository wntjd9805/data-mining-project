Vision transformer (ViT) models have gained attention as an alternative to convolutional neural networks (CNNs) in computer vision tasks. While ViTs offer the potential to improve performance, researchers have encountered challenges in optimizing ViT models. ViTs are sensitive to the choice of optimizer, learning hyperparameters, training schedule length, and network depth. This paper hypothesizes that the issues primarily stem from the early visual processing performed by ViT, specifically the use of large-kernel, large-stride convolutions. To validate this hypothesis, the paper replaces the patchify stem in ViT with a standard convolutional stem and observes improved convergence, the ability to use different optimizers without impacting accuracy, increased stability to learning rate and weight decay, and reduced ImageNet top-1 error. The results show that incorporating convolutional inductive bias into ViTs can be beneficial and that ViTs equipped with a convolutional stem outperform state-of-the-art CNNs. The paper recommends using a lightweight convolutional stem for ViT models as a more robust and high-performing architectural choice.