The popular bounded difference inequality is a fundamental tool in algorithm analysis, bounding the deviation probability of a function from its mean. However, this inequality cannot be applied when the conditional ranges are infinite, limiting its usefulness in certain situations. To address this limitation, we extend the Hoeffding and Bernstein-type inequalities from sums to general functions using the entropy method. Our contributions include concentration inequalities for sub-Gaussian and sub-exponential distributions. We demonstrate the applicability of these results in learning theory, vector valued concentration, and the method of Rademacher complexities. Our bounds also allow for the extension of Kontorovich's inequality to the sub-exponential case, with implications for algorithmic stability.