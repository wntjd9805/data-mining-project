This paper addresses the problem of learning finite-horizon Markov Decision Processes (MDPs) with unknown transition. The goal is to minimize regret, which is the difference between the learner's total loss and that of the optimal stationary policy. Previous studies have achieved logarithmic regret in the stochastic setting and square root of T regret in the adversarial setting. However, achieving both in a single algorithm with unknown transition estimation is challenging. In this work, the authors propose an algorithm that achieves polylogarithmic regret and simultaneously ensures worst-case robustness under unknown transition. They introduce three new techniques, including a loss-shifting trick and a new framework for dealing with unknown transition. The analysis shows that the transition estimation error is only polylogarithmic in T, leading to the desired best-of-both-worlds guarantee.