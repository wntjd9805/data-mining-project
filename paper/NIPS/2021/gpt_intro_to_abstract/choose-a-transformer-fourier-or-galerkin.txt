This paper introduces a new approach to operator learning for partial differential equations (PDEs) using attention mechanisms without softmax normalization. The authors propose two interpretations of these attention operators and prove that their linear variant has comparable approximation capacity to a Petrov-Galerkin projection. They combine these operators with the Fourier Neural Operator (FNO) model to improve its evaluation accuracy in PDE solution operator learning benchmark problems. Experimental results on three benchmark problems demonstrate the computational and memory efficiency, as well as the accuracy, of the proposed approach compared to conventional softmax normalization. The authors provide open-source software to reproduce their results.