This paper explores the use of algorithms for reinforcement learning (RL) to solve real-world problems by examining the expressivity of reward as a signal. The authors investigate the concept of a task and how it is encoded in rewards, considering three different types of tasks in the context of finite Markov Decision Processes (MDPs). They then analyze the expressivity of reward, focusing on Markov reward functions. The study reveals that there are tasks that cannot be expressed by a Markov reward function, but the authors also propose polynomial-time algorithms to determine when such a reward function exists and construct it if possible. The paper concludes with empirical experiments and highlights the implications of these findings for understanding the nature of reward maximization.