In this paper, we explore the concept of sparsity in machine learning models and its benefits in terms of memory savings, computational speed, interpretability, and generalizability. We examine the traditional approach of solving a regularized empirical risk minimization problem to achieve sparsity, where a sparsity-inducing regularization penalty is applied. However, popular methods for sparsifying deep neural networks, such as variational dropout, do not fit the traditional regularization penalty form. We aim to understand the types of solutions encouraged by these methods and compare them based on their regularization penalties. By analyzing adaptive dropout methods, we establish a connection between these methods and regularized empirical risk minimization, demonstrating that they can be characterized by effective regularization penalties. We provide experimental evidence supporting the effectiveness of these methods in inducing sparsity and validate our findings by applying variational dropout to deep network sparsification. Our work offers a general framework for obtaining the effective regularization penalty for adaptive dropout algorithms and provides guidance for choosing sparsifying approaches in practice. Additionally, it establishes a baseline for comparing new adaptive dropout methods.