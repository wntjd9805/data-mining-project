Video prediction is a challenging task that involves predicting future events based on limited knowledge. This task has various applications in fields such as robotic control, video interpolation, autonomous driving, and motion planning. Deep learning methods, specifically recurrent neural networks (RNNs) with LSTM and GRU units, have been widely used for video prediction due to their ability to model sequential data. However, the temporal receptive field of these models is still limited, making it difficult to capture long-term motion information. Some approaches have used 3D convolutional layers to broaden the temporal receptive field, but they are computationally expensive. Other approaches have focused on improving the model's expressivity through structure-oriented or loss-oriented methods. Despite these efforts, the temporal receptive field remains a limiting factor in accurately predicting videos with complex scenarios. To address this issue, we propose the Motion-Aware Unit (MAU), which efficiently broadens the temporal receptive field by incorporating attention and fusion modules. Experimental results demonstrate that our proposed MAU outperforms existing methods in video prediction and early action recognition tasks.