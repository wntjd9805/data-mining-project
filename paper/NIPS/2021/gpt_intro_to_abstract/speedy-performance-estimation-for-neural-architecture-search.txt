Reliably estimating the generalization performance of neural architectures is critical to the success of Neural Architecture Search (NAS), but it often requires large computational resources. This paper introduces a simple model-free method called Training Speed Estimation (TSE) to efficiently estimate the relative performance ranking of different architectures. TSE measures the training speed by summing the training losses during training and has been shown to outperform existing approaches in predicting performance ranking. The method is applicable to a variety of search spaces and datasets and can improve the efficiency of NAS algorithms.