This paper introduces the concept of multimodal meta-learning, which involves classification tasks from multiple input and label domains. It discusses Multimodal Model-Agnostic Meta-Learning (MMAML) as a framework for handling multimodal task distributions and explores the knowledge transfer between different modes. The paper proposes a new quantification method inspired by transference in multi-task learning to understand knowledge transfer at a micro-level. It also presents a new multimodal meta-learner and discusses its interpretation of the modulation mechanism. The paper's main contributions include a method to quantify knowledge transfer and a multimodal meta-learner that outperforms existing methods.