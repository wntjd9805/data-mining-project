This paper addresses the need for explainability methods in the deployment of deep black-box models into critical decision-making. Specifically, the focus is on counterfactual explanations that synthesize interpretable changes to an image to support user-specified hypotheses. Existing approaches rely on pre-trained generative models, which limits their utility in scenarios where access to training data or generative models is restricted. In this paper, the authors propose a deep model inversion approach called DISC that generates counterfactual explanations by exploring the vicinity of a given query image. DISC utilizes stronger image priors, a novel manifold consistency objective, and a progressive optimization strategy to introduce meaningful changes to the query image. Empirical studies demonstrate the effectiveness of DISC in producing visually meaningful explanations and robust counterfactuals. The contributions of this work include a general framework for on-the-fly counterfactual generation, novel objectives for consistency to the data manifold, a progressive optimization strategy, a classifier discrepancy metric, and empirical studies with natural and medical image classifiers.