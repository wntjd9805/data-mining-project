Boosting is a widely studied and applied concept in machine learning, particularly in binary supervised learning. However, many important problems involve classification into multiple target classes, such as image object recognition or language modeling. Therefore, there is a need to generalize boosting to the multiclass setting. This paper explores different approaches to multiclass boosting, including the extension of weak learnability assumptions. The authors introduce a weak learning assumption based on agnostic PAC learning, where the weak learner aims to minimize classification loss within a base class. The goal of the boosting algorithm is to learn target concepts outside the base class by aggregating the weak hypotheses. The paper examines the resources required for multiclass boosting, including the number of examples, oracle calls, and accuracy of the weak learner. They propose a variant of AdaBoost that can learn any task satisfying the multiclass weak learning assumption. However, the sample complexity of the weak learner is exponentially worse than the booster's, highlighting the need to improve the interaction between the two. The paper also presents an impossibility result, showing that the discrepancy between the sample complexities of the booster and weak learner is inevitable and requires a trade-off between the number of oracle calls and the weak learner's accuracy. This research provides insights into the complexity of multiclass boosting and the relationship between the weak learner and boosting algorithm.