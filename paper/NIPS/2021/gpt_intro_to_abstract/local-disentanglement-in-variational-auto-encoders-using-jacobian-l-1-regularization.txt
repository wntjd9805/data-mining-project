Unsupervised representation learning is a challenging task in computer science, where the goal is to organize and discover patterns in image data without any additional information about how the images were generated. This paper introduces a new approach called the Jacobian L1 Regularized Variational Auto-Encoder (JL1-VAE) that addresses the issue of model identification in representation learning algorithms. By adding an L1 cost to the generation function's Jacobian matrix, the JL1-VAE encourages a preferred orientation of the latent space, leading to more useful representations of image data. The paper presents qualitative and quantitative results demonstrating the effectiveness of the JL1-VAE on various datasets.