Today, the collection of valuable data at distributed edge nodes has become increasingly common, with mobile phones, wearable client devices, and smart vehicles/drones being key examples. However, there are privacy concerns surrounding the direct uploading of local data to a central server for model training. To address this issue, federated learning (FL) has emerged as a solution wherein server uploading of local data is not necessary. FL involves a large group of distributed clients collaborating to train a single global model without sharing their data. In FL, the objective is to minimize the averaged version of the local losses computed at each node using local data, rather than minimizing the loss function on a centralized dataset. The learning process in FL involves iterative aggregation of local model updates.