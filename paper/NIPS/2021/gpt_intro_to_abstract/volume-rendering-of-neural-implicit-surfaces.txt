Volume rendering techniques have been used to render volume density in radiance fields by applying the volume rendering integral. Recent advancements in representing density and radiance fields as neural networks have shown promising results in predicting novel views using sparse input images. However, while this neural volume rendering approach leads to good generalization of novel viewing directions, it often produces noisy and low-fidelity geometry approximations. In this paper, we propose VolSDF, a novel model for the density in neural volume rendering. Our approach represents density as a function of the signed distance to the scene's surface, which provides several benefits including better geometry approximation and a more accurate rendering. We demonstrate the effectiveness of VolSDF in reconstructing surfaces and compare it to existing methods such as NeRF and IDR, showing superior performance and disentanglement results. Our work combines the advantages of volume rendering and neural implicit surfaces, avoiding the drawbacks of both approaches.