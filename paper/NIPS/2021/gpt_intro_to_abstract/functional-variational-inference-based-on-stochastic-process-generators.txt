Bayesian neural networks (BNNs) have been studied for decades, but still suffer from shortcomings such as multiple posterior modes and difficulty in Bayesian inference in weight space. To address these issues, there has been a recent interest in applying Bayesian non-parametrics to neural nets, treating BNNs as probability measures over functions. However, existing functional BNNs (f-BNNs) and variational implicit processes (VIPs) have their own limitations. In this paper, we propose a new solution involving variational inference in function space using stochastic processes. We introduce a new objective function and a new class of flexible variational distributions called stochastic process generators (SPGs). We compare our method against existing weight-space and function-space inference methods and demonstrate its superiority.