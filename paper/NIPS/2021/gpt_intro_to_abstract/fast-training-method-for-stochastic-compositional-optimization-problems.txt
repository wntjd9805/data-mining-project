The paper introduces the problem of stochastic compositional optimization in machine learning and highlights its importance in various applications. It discusses the challenges posed by the compositional structure of the problem and the limitations of existing methods in achieving efficient training. The paper proposes two novel decentralized training methods for the stochastic compositional optimization problem and provides theoretical analysis to establish their convergence rate and sample complexity. The effectiveness of the proposed methods is confirmed through empirical evaluation on a model-agnostic meta-learning task.