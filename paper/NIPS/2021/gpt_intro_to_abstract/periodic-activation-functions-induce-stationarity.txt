Deep feedforward neural networks are widely used in various fields, such as artificial intelligence, machine learning, medicine, and data analysis. However, concerns have arisen regarding the robustness, fairness, and interpretability of these models. This has led to the emergence of Bayesian deep learning, which aims to incorporate prior knowledge into models and perform probabilistic inference. In this paper, we focus on stationary models, which maintain consistency and induce higher uncertainty for out-of-distribution samples. We explore the use of periodic activation functions to establish a correspondence between network weights and the covariance function of the Gaussian process, and show that placing a Student-t prior on the weights corresponds to a Mat√©rn covariance prior. Experimental results demonstrate that periodic activation functions provide comparable performance, avoid overconfident predictions, and enable robust out-of-domain detection.