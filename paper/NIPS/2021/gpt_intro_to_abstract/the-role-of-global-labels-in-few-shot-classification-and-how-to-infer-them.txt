This paper focuses on the problem of few-shot learning (FSL) in meta-learning, where new tasks need to be learned quickly with limited training data. Meta-learning methods that leverage past experiences have been proposed to improve model generalization in FSL. However, it is unclear if these methods share common lessons or if there are alternative approaches. This paper addresses this question by investigating the impact of data augmentation, network architecture, and feature pre-training. The authors also propose a novel framework called Meta Label Learning (MeLa) that infers latent global labels and leverages pre-training to improve meta-learners' generalization performance. Experimental results demonstrate the effectiveness of MeLa on benchmark datasets.