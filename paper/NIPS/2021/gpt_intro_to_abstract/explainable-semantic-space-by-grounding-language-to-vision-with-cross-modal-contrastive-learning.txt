This paper explores the concept of visually grounded language models, which combine language and vision models to match images and texts. The authors analyze the semantic space obtained with this model and find that semantic embeddings are organized and clustered by visual attributes. They also highlight the usefulness of this model for compositional language understanding and cross-modal image search. The paper expects that this visually grounded language model will contribute to understanding the computational basis of grounded cognition. The introduction provides background on the importance of grounding language to knowledge about the world and contrasts this with existing natural language processing models that rely solely on textual data.