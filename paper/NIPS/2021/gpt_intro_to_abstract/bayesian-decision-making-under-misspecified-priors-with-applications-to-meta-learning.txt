Bayesian decision-making algorithms are widely used due to their empirical performance and flexibility in incorporating prior knowledge. However, in practical applications, the chosen prior is an approximation of the true environment, leading to the question of how sensitive these algorithms are to prior misspecification. While existing work focuses on longer-horizon problems, we explore the sensitivity bounds in shorter-horizon scenarios, specifically in the Bayesian bandit setting. We establish distribution-independent bounds on the sensitivity of Bayesian algorithms to prior misspecification, considering the performance of Thompson sampling and related algorithms. Additionally, we apply our results to Bayesian meta-learning and extend the findings to contextual bandits and Bayesian POMDPs. Our experimental results demonstrate the benefits of meta-learning and using advanced algorithms in structured environments.