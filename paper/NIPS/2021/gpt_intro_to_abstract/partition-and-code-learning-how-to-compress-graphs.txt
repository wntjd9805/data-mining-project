Lossless data compression, a fundamental problem in computer science, is governed by the entropy of the underlying data distribution. The relationship between compressibility and machine learning has been established, indicating that better modeling of data distribution leads to more efficient compression. While ordered data such as text, images, and video can be effectively compressed, graph-structured data present challenges due to graph isomorphism, the absence of an inherent ordering, and the need to account for the description length of the learned model. In this paper, we introduce the Partition and Code (PnC) framework for graph compression, which addresses these challenges. PnC learns to decompose graphs into subgraphs and codes recurring subgraphs with a learned dictionary, resulting in significant improvements in compression efficiency compared to existing methods. Theoretical results demonstrate the benefits of PnC, while practical algorithms based on low-parameter probability estimation, dictionary selection, and partitioning using Graph Neural Networks are presented. Experimental evaluations on real-world graph distributions validate the effectiveness of PnC in achieving compression gains.