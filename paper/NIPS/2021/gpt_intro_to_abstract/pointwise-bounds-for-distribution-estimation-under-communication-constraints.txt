Learning a distribution from its samples in distributed settings poses challenges due to communication constraints and the increasing amount of data generated by devices. Existing communication-efficient distribution learning schemes achieve worst-case optimal results, but their estimation error scales linearly with the alphabet size of the distribution. This paper proposes a local minimax complexity approach to capture the hardness of estimating a specific distribution and introduces a two-round interactive scheme that achieves optimal estimation performance. Experimental results show that the proposed scheme outperforms existing schemes, particularly in cases with skewed distributions. Lower bounds are also derived to capture the local complexity of high-dimensional estimation under information constraints.