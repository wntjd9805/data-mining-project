In this paper, we investigate the advantages of topographic organization in the brain and explore how we can apply this knowledge to develop better inductive biases for deep neural network architectures. We introduce the concept of redundancy reduction and Independent Component Analysis (ICA) as ways to transform input data into a neural code with maximally independent activations. However, linear transformations such as ICA cannot eliminate higher-order dependencies. To address this issue, we propose using a topographic organization of feature activations to explicitly model these remaining dependencies. We also discuss the idea of equivariance, which ensures that symmetry transformations are preserved in the deeper layers of a neural network. By combining topographic organization and equivariance, we aim to encourage approximate equivariance through induced topology in feature space. Additionally, we introduce the principle of temporal coherence, which emphasizes slow and smooth feature transformations over time. We propose a Topographic Variational Autoencoder that leverages these concepts to learn approximately equivariant capsules from sequence data in an unsupervised manner.