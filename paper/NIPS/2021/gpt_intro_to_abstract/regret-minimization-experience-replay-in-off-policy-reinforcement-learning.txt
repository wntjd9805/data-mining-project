This paper introduces a new approach to prioritized experience replay (PER) in off-policy reinforcement learning. Previous PER strategies, which prioritize samples based on TD error or corrective feedback, often have an objective mismatch with the overall goal of minimizing policy regret. To address this issue, the authors formulate an optimization problem that directly minimizes policy regret and develop an optimal prioritization strategy. They propose two estimation methods to assess on-policiness and Q value accuracy, and introduce algorithms ReMERN and ReMERT. Experimental results demonstrate the superior performance of ReMERN and ReMERT compared to standard off-policy RL methods.