This paper addresses the challenge of comprehensively understanding all kinds of video events by jointly comprehending audio-visual events. The authors propose a strategy that leverages audio and visual data across different videos to explore shared information of each category and exploit the dependency between event categories. They develop an audio-visual event co-occurrence module that jointly considers the relationship of categories in audio, visual, and audio-visual modalities. The proposed method is evaluated on the LLP dataset and demonstrates favorable performance compared to state-of-the-art methods. The main contributions of this work are leveraging cross-video data, developing an audio-visual event co-occurrence module, and achieving superior performance in various settings.