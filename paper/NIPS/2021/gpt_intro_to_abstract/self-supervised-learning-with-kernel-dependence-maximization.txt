Learning general-purpose visual representations without human supervision is a long-standing goal in machine learning. The use of unsupervised learning methods, such as likelihood-based generative models or self-supervised models, has been explored for this purpose. However, designing good pretext tasks for self-supervised learning is challenging and lacks theoretical guidance. In this paper, we propose a new self-supervised loss called SSL-HSIC, which is based on the statistical dependence between transformed images and their identities. We present a unified view of contrastive learning, showing that popular methods like InfoNCE can be seen as approximations of SSL-HSIC. We also demonstrate the advantages of SSL-HSIC, including direct estimation from mini-batches and penalization of trivial solutions. Our method achieves high accuracy on various image understanding tasks, surpassing other state-of-the-art methods.