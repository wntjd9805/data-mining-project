Reconstructing dynamic scenes with articulated objects from a sequence of monocular frames remains a challenging task in computer vision. While significant progress has been made in reconstructing rigid scenes, the reconstruction of deforming mesh for articulated objects is still elusive. This paper proposes ViSER, a method that aims to estimate the deforming mesh of articulated objects using a segmented monocular video without relying on mesh templates or category-specific priors. ViSER utilizes video-specific embeddings to establish long-range pixel correspondences and reconstructs articulated 3D shapes through the use of self-supervised losses. The proposed method achieves state-of-the-art results in reconstructing articulated 3D shapes and trajectories, making it applicable to diverse videos, including those involving humans with challenging clothing and poses. Additionally, ViSER recovers meaningful part segmentation and blend skinning weights without requiring manual effort from 3D artists.