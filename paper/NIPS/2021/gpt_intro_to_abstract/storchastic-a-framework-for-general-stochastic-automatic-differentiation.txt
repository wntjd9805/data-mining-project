In this paper, we introduce Storchastic, a new framework for general stochastic automatic differentiation (AD). Storchastic aims to support deep learning modelers by allowing them to focus on defining stochastic deep learning models without having to worry about complex gradient estimation implementations. We extend the existing DiCE framework to incorporate various gradient estimation techniques and provide a formalization and proof that performing n-th order differentiation on the Storchastic surrogate loss gives unbiased estimates of the n-th order derivative of the stochastic computation graph. Additionally, we introduce a technique for extending variance reduction using control variates to any-order derivative estimation. We implement Storchastic as an open source library for PyTorch and demonstrate its effectiveness through experiments and comparisons with existing stochastic AD frameworks.