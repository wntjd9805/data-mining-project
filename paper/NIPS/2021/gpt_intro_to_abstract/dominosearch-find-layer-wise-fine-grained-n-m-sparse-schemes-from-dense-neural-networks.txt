Modern deep neural networks (DNNs) have achieved remarkable success in computer vision and language processing tasks, but they come with limitations in terms of memory footprint, computation, energy, and storage. To address these challenges, various model compression techniques have been proposed, including neural network pruning. However, traditional pruning methods struggle to balance regular sparse structure and high compression ratio. In this paper, we introduce a novel framework called DominoSearch that leverages layer-wise fine-grained N:M sparsity to achieve higher accuracy compared to uniform counterparts. We propose an iterative algorithm that finds sparse schemes based on pre-trained dense weights using a magnitude-based criterion and a weight penalty. Our framework demonstrates state-of-the-art accuracy-tradeoff results on the ImageNet and CIFAR100 datasets, outperforming both unstructured and structured sparsity approaches.