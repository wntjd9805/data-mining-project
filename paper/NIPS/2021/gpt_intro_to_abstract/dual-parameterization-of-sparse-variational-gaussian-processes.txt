Gaussian processes (GPs, [31]) have become widely used in probabilistic machine learning, but their application is challenging due to poor scaling in the number of data points and difficult approximate inference in non-conjugate models. Variational inference has emerged as a solution, with sparse variational GP methods tackling these challenges by reducing computation to O(nm2). The standard mean-covariance parameterization used in these methods requires O(m2) memory. This paper introduces a dual parameterization for SVGP that improves learning and inference speed, achieving a tighter lower bound on the marginal likelihood, and matching the memory complexity of other SVGP methods. The formulation avoids sluggish automatic differentiation for computing natural gradients and revives a forgotten parameterization from the early 2000s.