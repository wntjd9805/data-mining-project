Federated learning (FL) is a large-scale learning framework where training data is distributed among numerous clients, such as mobile phones or network sensors. This allows for data privacy and security as the client data is not transmitted over the network. FL can be categorized into two settings: cross-device and cross-silo. In the cross-device setting, there may be an extremely large number of clients, such as all active Android phones, and communication occurs over a highly unreliable network. This presents unique challenges not present in the cross-silo setting, where there is a smaller number of reliable clients. The de facto standard algorithm for the cross-device setting is FEDAVG, but it suffers from issues such as client drift and slower convergence. To address these deficiencies, we propose a new framework called MIME that mitigates client drift and can adapt centralized optimization algorithms to the federated setting. Our framework shows promising results in reducing client drift and achieving faster convergence compared to existing methods in empirical validation.