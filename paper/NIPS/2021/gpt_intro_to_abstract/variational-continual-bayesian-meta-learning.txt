Meta-learning aims to enhance model generalization and data efficiency by transferring knowledge among low-resource tasks. However, conventional meta-learning assumes a stationary task distribution, which is often unrealistic in an online setting. This paper introduces a Variational Continual Bayesian Meta-Learning (VC-BML) algorithm that addresses the issues of negative knowledge transfer and catastrophic forgetting in streaming low-resource tasks. VC-BML uses a mixture of dynamically updated distributions for meta-parameters and employs structured variational inference to approximate posterior distributions. Experimental results demonstrate that VC-BML outperforms state-of-the-art baselines in handling non-stationary task distributions.