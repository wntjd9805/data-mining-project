Visual Transformers (VTs) have emerged as a promising alternative to Convolutional Neural Networks (CNNs) in computer vision. They have been applied to various tasks, including image classification, object detection, segmentation, tracking, image generation, and 3D data processing. Inspired by the Transformer model in natural language processing, VTs offer a unified information-processing paradigm for visual and textual domains. However, VTs lack the typical CNN inductive biases, which require a large amount of training data. To address this issue, a second generation of VTs has been proposed, combining convolutional layers with attention layers to provide a local inductive bias. In this paper, we compare different second-generation VTs using medium-small datasets and show that their classification accuracy varies significantly. We also compare VTs with ResNets and demonstrate that VTs can achieve comparable accuracy with smaller datasets. Furthermore, we propose an auxiliary self-supervised pretext task and corresponding loss function for regularization in small training sets or few epochs regimes. This task focuses on learning spatial relations between output token embeddings and has consistently improved the accuracy of VTs in various training scenarios. Overall, our contributions include empirical comparisons of VTs, the introduction of a relative localization auxiliary task for VT training regularization, and the demonstration of its benefits in training speed and generalization.