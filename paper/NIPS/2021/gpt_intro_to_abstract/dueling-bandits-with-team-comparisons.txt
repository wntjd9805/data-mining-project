In this paper, we address the limitations of the multi-arm bandit (MAB) model in the context of decision making under uncertainty. While the MAB model is effective in incorporating the exploration-exploitation tradeoff, it assumes that real-valued rewards are always available for decision-making. However, in many applications, such as recommendation systems or online games, it is more natural to compare two actions and observe which one is better. To address this, we introduce the dueling bandit model, where the learner selects pairs of actions and observes the binary "winner" of the duel between them. We focus specifically on the case where the learner needs to select two disjoint teams for a duel, which arises in sports or online games. Our goal is to minimize the number of duels required to identify a Condorcet winning team, while considering the limitations of deducing pairwise relations and the exponential number of teams. We present algorithms that overcome these challenges and provide sample complexity upper bounds.