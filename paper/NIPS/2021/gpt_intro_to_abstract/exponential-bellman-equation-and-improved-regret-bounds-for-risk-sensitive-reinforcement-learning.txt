Risk-sensitive reinforcement learning (RL) is crucial for practical and high-stake applications, such as self-driving and robotic surgery. This paper focuses on the entropic risk measure of cumulative rewards and explores the online setting of risk-sensitive RL under K-episode MDPs with a horizon length of H. The authors propose two model-free algorithms, RSVI and RSQ, that achieve a regret upper bound of e|β|H^2 * e|β|H^-1 * |β|H * poly(H) * K, without assuming knowledge of the transition distribution or access to a simulator. However, there exists a wide gap between the upper and lower bounds, with the upper bound having an additional factor of e|β|H^2 that dominates the lower bound exponentially. To address this, the authors refine the algorithmic design and analysis, introducing a transformation called the exponential Bellman equation and a novel exploration mechanism called doubly decaying bonus. The proposed algorithms achieve nearly optimal regret bounds, improving upon existing ones by an exponential factor in terms of the horizon length and risk sensitivity.