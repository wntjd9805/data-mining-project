The problem of training deep neural networks on large annotated datasets is a computationally demanding task. Traditional iterative optimization algorithms are used to find optimal parameters for a given neural network, but obtaining these parameters remains a bottleneck in large-scale machine learning pipelines. In this paper, we propose a new parameter prediction task where a hypernetwork leverages past experience gained from optimizing other networks. We introduce a framework that replaces iterative optimization with a single forward pass of the hypernetwork, allowing for more efficient training of neural networks. We design a diverse space of neural architectures and improve on the Graph HyperNetworks method to achieve high accuracy in predicting parameters for unseen architectures. Our approach shows good out-of-distribution generalization and can predict parameters for large and deep architectures in a fraction of a second without any gradient updates. We introduce a standardized benchmark and demonstrate the usefulness of our model for initializing neural networks. Overall, our framework and results provide a more efficient paradigm for training networks.