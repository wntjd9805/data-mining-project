In this paper, we address the limitations of using loss re-weighting in over-parameterized models, particularly with diagonal class weights. We propose new logit-adjusted losses for cost-sensitive learning that are calibrated and demonstrate their effectiveness on benchmark image classification tasks. Our main contributions include highlighting the pitfalls of loss re-weighting in over-parameterized settings, proposing new losses for cost-sensitive learning, demonstrating significant gains over loss re-weighting in training ResNet models, and comparing our approach to post-hoc correction strategies.