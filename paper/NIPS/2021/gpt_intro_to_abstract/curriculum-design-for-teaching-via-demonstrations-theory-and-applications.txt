Imitation learning is a popular paradigm for acquiring new skills by imitating a teacher's behavior, particularly in settings where explicit definition of desired behavior is challenging. Behavioral Cloning (BC) and Inverse Reinforcement Learning (IRL) are two common approaches to imitation learning. While previous work has focused on designing efficient learning algorithms from the learner's perspective, there is limited research on reducing the number of demonstrations required from the teacher's perspective. In this paper, we address this issue by proposing a Teaching via Demonstrations (TvD) framework, where a helpful teacher designs a personalized curriculum to accelerate the learner's convergence. We introduce an interactive curriculum algorithm that is agnostic to the learner's dynamics, making it applicable to a wide range of learner models. Our approach is inspired by previous work on curriculum design for supervised learning and reinforcement learning algorithms. We define difficulty scores for demonstrations based on the teacher's optimal policy and the learner's current policy, and study their impact on the learning progress for popular imitation learners. We present experimental results that validate our curriculum strategy in synthetic environments and demonstrate its acceleration of the learning process. We compare our algorithm to existing approaches on curriculum design for imitation learning and highlight its advantages in terms of interaction, knowledge of learner dynamics, convergence guarantees, and applicability to learner-centric settings.