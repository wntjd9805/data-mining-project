This paper introduces the Memory-Economic Sparse Training (MEST) framework for accurate and fast execution on edge devices. The authors propose enhancements to explore various sparse topologies and apply a soft memory bound to relax memory footprint during sparse training. They compare the accuracy of MEST against state-of-the-art sparse training algorithms and investigate the impact of sparsity schemes on accuracy, memory footprint, and training speed. They also explore data efficiency in sparse training and propose a two-phase approach for removing less informative examples without affecting accuracy. Experimental results demonstrate that MEST outperforms existing methods in terms of accuracy, training acceleration rate, and memory footprint reduction.