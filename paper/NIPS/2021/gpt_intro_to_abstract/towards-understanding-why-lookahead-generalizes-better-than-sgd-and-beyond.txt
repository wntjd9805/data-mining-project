This paper introduces the concept of the lookahead algorithm as a solution for optimizing deep neural networks. The algorithm utilizes both "fast weights" and "slow weights" to update network parameters and has been shown to improve test performance compared to standard optimizers. However, the theoretical reasons for this improvement and the benefits of using a stagewise optimization strategy with lookahead are still unclear. In this work, the authors provide a theoretical perspective on the test performance improvement of lookahead and demonstrate the advantages of the stagewise optimization strategy. They also propose a new stagewise locally-regularized lookahead algorithm that exhibits faster convergence speed, smaller generalization error, and better test performance than conventional lookahead. The findings of this study contribute to a better understanding of the optimization and generalization properties of lookahead in deep neural network training.