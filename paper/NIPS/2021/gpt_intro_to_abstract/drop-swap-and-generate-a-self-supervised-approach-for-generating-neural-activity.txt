Understanding how different populations of neurons work together to encode sensory inputs and movements is a fundamental goal of neuroscience. Unsupervised learning approaches have been developed to build representations of neural population activity without specific labels or decoding objectives, providing insights into neural responses, individual differences, and remapping of neural responses. However, the challenge of learning representations that can distinguish different sources of variability remains. In this paper, we introduce a novel unsupervised approach called Swap-VAE inspired by computer vision methods for decomposing images. We aim to disentangle the neural representation of movement by identifying the content (target location) and style (movement dynamics). We employ a self-supervised approach using transformations of the input data to train the representation to be invariant and maintain temporal consistency. Additionally, we use a generative model to reconstruct the original inputs and disentangle latent factors. We demonstrate the effectiveness of our approach on synthetic data and non-human primate reaching datasets and propose measures to quantify the disentanglement of behavior. Our contributions include the proposed Swap-VAE method for learning meaningful representations and generating realistic neural activities, a novel latent space augmentation called BlockSwap, and metrics to evaluate disentanglement in behavior representations.