Contextual multi-armed bandits are a fundamental problem in online learning, where a learner interacts with an adversary in a repeated game. The learner's goal is to select actions based on observed contexts to minimize cumulative loss. The regret, which measures the difference between the learner's cumulative loss and the smallest loss of an optimal policy, is typically used as a performance measure. While optimal regret bounds can be achieved in full-information games, they are impossible to achieve in multi-armed bandits. Contextual bandits, however, are an intermediate setting between the two and it is unknown if model selection is possible. In this paper, we address this question by providing upper and lower bounds for model selection in contextual bandits with finite-sized policy classes. We also present an impossibility result for adapting to the number of switch points under adaptive adversaries and resolve an open problem on second-order bounds for full-information games.