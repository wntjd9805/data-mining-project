The challenge of balancing exploitation and exploration is crucial in reinforcement learning problems. In the contextual bandit setting, Thompson sampling has been widely used and proven to be effective. However, adapting Thompson sampling to the more complex Markov decision process (MDP) setting has been limited to small-scale problems. Model-free posterior sampling algorithms have been proposed as an alternative but lack theoretical performance guarantees. In this paper, we present a posterior sampling algorithm that works with a Q-function class and provides worst-case regret guarantees under general structural assumptions. Our algorithm achieves near-optimal regret bounds, matching those of OFU-based algorithms, and improves upon the best known regret bounds for posterior sampling approaches.