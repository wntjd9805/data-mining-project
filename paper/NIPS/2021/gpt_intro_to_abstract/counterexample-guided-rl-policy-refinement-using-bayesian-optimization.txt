Classical Reinforcement Learning (RL) algorithms have shown impressive results in games such as GO, leading to their application in safety-critical domains like autonomous driving. However, these policies are typically trained in simulation environments and may perform poorly in the real world due to domain uncertainty and the potential for unintended or harmful behavior. Safe RL aims to address these issues by learning policies that maximize return while respecting safety specifications. Existing approaches in safe RL can be categorized into three schools of thought, but establishing safety invariants for Neural Network controllers is challenging. In this work, we propose a counterexample guided policy refinement method to verify and correct pre-optimized policies, targeting failure traces caused by rare failure trajectories or environmental uncertainties. We adapt counterexample guided refinement techniques from formal methods to the context of RL and demonstrate the effectiveness of our approach on continuous space and action environments using Proximal Policy Optimization. The paper is organized into sections that cover related work, the methodology for uncovering failure trajectories, policy refinement, case studies, and concluding remarks.