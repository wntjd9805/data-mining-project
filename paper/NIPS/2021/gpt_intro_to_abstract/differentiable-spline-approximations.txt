Differentiable programming has revolutionized algorithm design by utilizing gradient-based optimization to optimize algorithm parameters, enabling end-to-end trainable systems like deep neural networks to leverage data structure and achieve improved performance. This approach has been widely adopted in various fields, including scientific computing, image processing, physics engines, computational simulations, and graphics. In many applications, incorporating structural constraints into machine learning models can be advantageous, particularly in areas such as physics simulations, graphics rendering, and network engineering. However, there are challenges in implementing differentiable programming due to the need for simple gradient calculations across all layers. For example, in computer graphics applications involving spline functions, which approximate continuous shapes or surfaces, there is often a lack of simple gradient calculations due to discontinuous or discrete co-domains. To address this, continuous approximations are typically used, but this can lead to suboptimal results. In this paper, we present a principled approach that allows differentiable programming for spline functions without resorting to continuous relaxation. We propose using spline function approximations as "layers" in differentiable programs and develop efficient algorithms for forward and backward passes in spline approximation problems. Our approach exploits the locality property of splines, resulting in a block-structured Jacobian matrix representation that facilitates efficient backpropagation. We demonstrate the effectiveness of our approach in three applications: image segmentation, 3D point cloud reconstruction, and finite element analysis for solving partial differential equations.