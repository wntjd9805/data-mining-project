Novel-view synthesis (NVS) aims to render photorealistic images of a scene from different viewpoints, relying on both geometry and appearance information. However, NVS often requires a diverse set of images and struggles with visual artifacts. Depth sensors in mobile devices offer additional visual information that many NVS techniques do not utilize. In this paper, we propose TöRF1, an implicit neural representation for scene appearance that leverages color and time-of-flight (ToF) images. By optimizing a continuous neural radiance field, TöRF reduces the number of required images for static NVS and improves the tractability of monocular dynamic NVS. Our work includes a physically-based neural volume rendering model, an optimization method for dynamic scenes, and evaluations that demonstrate superior view synthesis compared to existing techniques.