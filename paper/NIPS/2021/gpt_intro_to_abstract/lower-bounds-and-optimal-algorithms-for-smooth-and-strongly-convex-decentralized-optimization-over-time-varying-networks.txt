In this work, we address the decentralized optimization problem where each function is stored on a compute node and the nodes are connected through a communication network. We consider smooth and strongly convex functions and aim to minimize the objective function over a distributed network. We highlight the applicability of this problem in various domains such as sensor networks, network resource allocation, cooperative control, distributed spectrum sensing, power system control, and supervised machine learning. We also discuss the potential use of decentralized optimization in new-generation federated learning systems. Additionally, we focus on the scenario where the communication network is time-varying, which is prevalent in many practical applications. We present lower bounds on the communication and computation complexities for solving the decentralized optimization problem over time-varying networks and propose optimal algorithms that match these lower bounds. Furthermore, we provide experimental results demonstrating the effectiveness and competitiveness of our methods compared to existing baseline methods.