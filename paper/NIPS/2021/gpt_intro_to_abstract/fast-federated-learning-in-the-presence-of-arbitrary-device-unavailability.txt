Federated learning is a machine learning setting where a central server coordinates with multiple devices to collectively train a shared model. This approach offers benefits such as preserving data privacy and utilizing idle computing resources. However, the variations among devices pose challenges. This paper aims to investigate the impact of device variations on federated learning from an optimization perspective. Specifically, it addresses variations in data distribution, local tasks, computing and communication speeds, and availability patterns. The paper proposes an algorithm, called Memory-augmented Impatient Federated Averaging (MIFA), that accommodates device unavailability and achieves optimal convergence rates. Experimental results on real-world datasets validate the effectiveness of the proposed approach.