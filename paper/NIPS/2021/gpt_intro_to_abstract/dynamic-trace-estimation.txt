Implicit or "matrix-free" trace estimation is a widely used computational technique in linear algebra and has become increasingly important in machine learning and data science. The goal of this technique is to compute an approximation to the trace of a given matrix A, where the diagonal entries of A cannot be accessed explicitly. This problem arises in various applications, such as loss functions involving neural networks, graph adjacency matrices, log-determinant approximation, matrix norm and spectral sum estimation, eigenvalue counting, spectral density estimation, deep learning, and neural network weight quantization. The existing method for trace estimation, Hutchinson's estimator, has limitations in terms of accuracy and computational efficiency. In this paper, we propose an algorithm called DeltaShift that improves on Hutchinson's estimator for dynamic trace estimation, where the matrix A undergoes changes over time. We provide theoretical analysis and show that DeltaShift achieves better computational efficiency with fewer matrix-vector multiplications compared to Hutchinson's estimator. Additionally, we present a conditional lower bound and an improvement to DeltaShift under more stringent assumptions.