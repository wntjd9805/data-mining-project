This work focuses on the problem of unconstrained bilevel optimization, where the optimization of one problem affects the objective function of another. Specifically, the paper considers problems of the form minx E[f(x, y*(x); ξ)], subject to y*(x) = arg miny E[g(x, y; ς)], where f and g represent stochastic samples of the upper and lower level objectives, respectively. The paper introduces the SUSTAIN algorithm, which uses momentum-assisted stochastic gradient estimators to improve the quality of gradient estimation and achieves optimal complexity bounds matching those of stochastic gradient algorithms for single-level problems. The algorithm does not require explicit Hessian inversion and has favorable dependence on problem dimension.