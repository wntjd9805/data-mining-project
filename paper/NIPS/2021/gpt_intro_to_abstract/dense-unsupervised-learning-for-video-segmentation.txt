Unsupervised learning of visual representations has made significant advancements, surpassing supervised feature learning methods, but the majority of these approaches require substantial computational resources and do not effectively handle video data. This paper introduces a novel framework for video object segmentation (VOS) that learns task-relevant invariances in a computationally efficient manner. The proposed approach leverages feature equivariance to similarity transformations and utilizes a self-training objective to generate pseudo labels for anchor assignment. The framework achieves state-of-the-art video segmentation accuracy with improved computational and data efficiency. The paper presents an overview of the approach, the methodology used, and the achieved results.