Abstract
Wasserstein gradient ﬂows provide a powerful means of understanding and solving many diffusion equations. Speciﬁcally, Fokker-Planck equations, which model the diffusion of probability measures, can be understood as gradient descent over entropy functionals in Wasserstein space. This equivalence, introduced by Jordan,
Kinderlehrer and Otto, inspired the so-called JKO scheme to approximate these diffusion processes via an implicit discretization of the gradient ﬂow in Wasserstein space. Solving the optimization problem associated to each JKO step, however, presents serious computational challenges. We introduce a scalable method to approximate Wasserstein gradient ﬂows, targeted to machine learning applications.
Our approach relies on input-convex neural networks (ICNNs) to discretize the
JKO steps, which can be optimized by stochastic gradient descent. Unlike previous work, our method does not require domain discretization or particle simulation.
As a result, we can sample from the measure at each time step of the diffusion and compute its probability density. We demonstrate our algorithm’s performance by computing diffusions following the Fokker-Planck equation and apply it to unnormalized density sampling as well as nonlinear ﬁltering. 1

Introduction
Stochastic differential equations (SDEs) are used to model the evolution of random diffusion processes across time, with applications in physics [63], ﬁnance [22, 52], and population dynamics [35]. In machine learning, diffusion processes also arise in applications ﬁltering [34, 21] and unnormalized posterior sampling via a discretization of the Langevin diffusion [70].
The time-evolving probability density ρt of these diffusion processes is governed by the Fokker-Planck equation. Jordan, Kinderlehrer, and Otto [32] showed that the Fokker-Planck equation is
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
equivalent to following the gradient ﬂow of an entropy functional in Wasserstein space, i.e., the space of probability measures with ﬁnite second order moment endowed with the Wasserstein distance.
This inspired a simple minimization scheme called JKO scheme, which consists an implicit Euler discretization of the Wasserstein gradient ﬂow. However, each step of the JKO scheme is costly as it requires solving a minimization problem involving the Wasserstein distance.
One way to compute the diffusion is to use a ﬁxed discretization of the domain and apply standard numerical integration methods [18, 49, 15, 17, 40] to get ρt. For example, [50] proposes a method to approximate the diffusion based on JKO stepping and entropy-regularized optimal transport. However, these methods are limited to small dimensions since the discretization of space grows exponentially.
An alternative to domain discretization is particle simulation. It involves drawing random samples (particles) from the initial distribution and simulating their evolution via standard methods such as
Euler-Maruyama scheme [36, 9.2]. After convergence, the particles are approximately distributed according to the stationary distribution, but no density estimate is readily available. (cid:77)
Another way to avoid discretization is to parameterize the density of ρt. Most methods approximate only the ﬁrst and second moments ρt, e.g., via Gaussian approximation. Kalman ﬁltering approaches can then compute the dynamics [34, 39, 33, 61]. More advanced Gaussian mixture approximations
[65, 1] or more general parametric families have also been studied [64, 69]. In [48], variational methods are used to minimize the divergence between the predictive and the true density.
Recently, [24] introduced a parametric method to compute JKO steps via entropy-regularized optimal transport. The authors regularize the Wasserstein distance in the JKO step to ensure strict convexity and solve the unconstrained dual problem via stochastic program on a ﬁnite linear subset of basis functions. The method yields unnormalized probability density without direct sample access.
Recent works propose scalable continuous optimal transport solvers, parametrizing the solutions by reproducing kernels [10], fully-connected neural networks [62], or Input Convex Neural Networks (ICNNs) [37, 44, 38]. In particular, ICNNs gained attention for Wasserstein-2 transport since their gradients ∇ψθ : RD → RD can represent OT maps for the quadratic cost. These continuous solvers scale better to high dimension without discretizing the input measures, but they are too computationally expensive to be applied directly to JKO steps.
Contributions. We propose a scalable parametric method to approximate Wasserstein gradient
ﬂows via JKO stepping using input-convex neural networks (ICNNs) [6]. Speciﬁcally, we leverage
Brenier’s theorem to bypass the costly computation of the Wasserstein distance, and parametrize the optimal transport map as the gradient of an ICNN. Given sample access to the initial measure ρ0, we use stochastic gradient descent (SGD) to sequentially learn time-discretized JKO dynamics of ρt.
The trained model can sample from a continuous approximation of ρt and compute its density dρt dx (x).
We compute gradient ﬂows for the Fokker-Planck free energy functional FFP given by (5), but our method generalizes to other cases. We demonstrate performance by computing diffusion following the Fokker-Planck equation and applying it to unnormalized density sampling as well as nonlinear
ﬁltering.
Notation. P2(RD) denotes the set of Borel probability measures on RD with ﬁnite second moment.
P2,ac(RD) denotes its subset of probability measures absolutely continuous with respect to Lebesgue measure. For ρ ∈ P2,ac(RD), we denote by dρ dx (x) its density with respect to the Lebesgue measure.
Π(µ, ν) denotes the set of probability measures on RD × RD with marginals µ and ν. For measurable
T : RD → RD, we denote by T (cid:93) the associated push-forward operator between measures. 2