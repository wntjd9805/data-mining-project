Abstract
Many instances of algorithmic bias are caused by subpopulation shifts. For example,
ML models often perform worse on demographic groups that are underrepresented in the training data. In this paper, we study whether enforcing algorithmic fairness during training improves the performance of the trained model in the target domain.
On one hand, we conceive scenarios in which enforcing fairness does not improve performance in the target domain. In fact, it may even harm performance. On the other hand, we derive necessary and sufﬁcient conditions under which enforcing algorithmic fairness leads to the Bayes model in the target domain. We also illustrate the practical implications of our theoretical results in simulations and on real data. 1

Introduction
There are many instances of distribution shifts causing performance disparities in machine learning (ML) models. For example, Buolamwini and Gebru [7] report commercial gender classiﬁcation models are more likely to misclassify dark-skinned people (than light-skinned people). This is (in part) due to the abundance of light-skinned examples in training data. Similarly, pedestrian detection models sometimes have trouble recognizing dark-skinned pedestrians [22]. Another prominent example is the poor performance of image processing models on images from developing countries due to the scarcity of images from such countries in publically available image datasets [20].
Unfortunately, many algorithmic fairness practices were not developed with distribution shifts in mind. For example, the common algorithmic fairness practice of enforcing performance parity on certain demographic groups [1, 11] implicitly assumes performance parity on training data generalize to the target domain, but distribution shifts between the training data and target domain renders this assumption invalid.
In this paper, we consider subpopulation shifts as a source of algorithmic biases and study whether the common algorithmic fairness practice of enforcing performance parity on certain demographic groups mitigate the resulting (algorithmic) biases in the target domain. Such algorithmic fairness practices are common enough that there are methods [1, 2] and software (e.g. TensorFlow Constrained
Optimization [10]) devoted to operationalizing them. There are other sources of algorithmic biases
*Equal Contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(e.g. posterior drift [17]), but we focus on algorithmic biases caused by subpopulation shifts in this paper. Our main contributions are: 1. We propose risk proﬁles as a way of summarizing the performance of ML models on subpopula-tions. As we shall see, this summary is particularly suitable for studying the performance of risk minimization methods. 2. We show that enforcing performance parity during training may not mitigate performance dispari-ties in the target domain. In fact, it may even harm overall performance. 3. We decompose subpopulation shifts into two parts, a recoverable part orthogonal to the fair con-straint and a non-recoverable part, and derive necessary and sufﬁcient conditions on subpopulation shifts under which enforcing performance parity improves performance in the target domain (see
Section 4.4).
One of the main takeaways of our study is a purely statistical way of evaluating the notion of algorithmic fairness for subpopulation shift: an effective algorithmic fairness practice should improve overall model performance in the target domain. Our theoretical results characterize when this occurs for risk-based notions of algorithmic fairness. 2 Problem setup
A be the set of
We consider a standard classiﬁcation setup. Let
Y be the set of possible values of the sensitive attribute. In this setup, training possible labels, and and test examples are tuples of the form (X, A, Y )
. If the ML task is predicting whether a borrower will default on a loan, then each training/test example corresponds to a loan. The features in X may include the borrower’s credit history, income level, and outstanding debts; the 0, 1 label Y encodes whether the borrower defaulted on the loan; the sensitive attribute may be
} the borrower’s gender or race.
∈ X × A × Y
Rd be the feature space,
X ⊂
∈ { (cid:101)
X × A × Y
. We consider
P be probability distributions on
Let P ∗ and
P as the distribution of the training data and P ∗ as the distribution of data in a hypothetical target domain. For example, P ∗ may
P is a biased sample in which certain demographic be the distribution of data in the real world, and (cid:101) groups are underrepresented. The difference P ∗
P is the distribution shift. In practice, distribution
− shifts often arise due to sampling biases during the (training) data collection process, so we call (cid:101)
P ∗ and
P as the bias (in the training data).
Henceforth E∗ (resp. ˜E) will denote expectation under P ∗ (resp. ˜P ). The set of all hypotheses under
R+ denotes the loss function under consideration. consideration is denoted by
H
In Section 3 and 4 we assume the set of sensitive attribute A is discrete. The case with continuous A is relegated to the supplementary document (Appendix 2).
P unbiased and biased respectively and refer to P ∗
Y × Y (cid:55)→ and (cid:96) :
− (cid:101) (cid:101) (cid:101) 3 Beneﬁts and drawbacks of enforcing risk parity
To keep things simple, we start by considering the effects of enforcing risk parity (RP). This notion is closely related to the notion of demographic parity (DP). Recall DP requires the output of the
ML model h(X) to be independent of the sensitive attribute A: h(X)
A. RP imposes a similar condition on the risk of the ML model.
Deﬁnition 3.1 (risk parity). A model h satisﬁes risk parity with respect to the distribution P on
⊥ if
X × Y × A
EP (cid:96)(h(X), Y )
A = a
= EP (cid:96)(h(X), Y )
|
A = a(cid:48) for all a, a(cid:48)
| and all h
.
∈ H
∈ A (cid:2) (cid:3)
RP is widely used in practice to measure algorithmic bias in ML models. For example, the US
National Institute of Standards and Technology (NIST) tested facial recognition systems and found that the systems misidentify blacks at rates 5 to 10 times higher than whites [21]. By comparing the error rates of the system on blacks and whites, NIST is implicitly adopting RP as its deﬁnition of algorithmic fairness. (cid:2) (cid:3)
It is not hard to see that RP is equivalent to linear constraints on the risk proﬁle (R ˜P (h)) of an ML model with respect to
P : (cid:101)
RP (h) (cid:44)
EP (cid:96)(h(X), Y ) (cid:8) (cid:2) 2
A = a a∈A .
| (cid:3)(cid:9) (3.1)
F is a set of risk
Figure 1: Fair risk minimization problem when there are two groups. Recall proﬁles and is the set of risk proﬁles that satisfy risk parity. The horizontal and vertical coordinates of the risk proﬁles represent the risk of the model on the majority and minority subpopulations.
R and the fair risk
In the left plot, we see the empirical risk minimization (ERM) optimal point minimization (FRM) optimal point
RF . In the center plot, we see that FRM can both improve and harm performance in the target domain (as long as the assumptions of 3.2 are satisﬁed). The green dotted line separates the P ∗
A’s that lead to worse and improved performance in the target domain: if P ∗
A falls below the green line, then FRM harms performance in the target domain. In the right plots, we reproduce this effect in a simulation. As the fraction of samples from the minority group decreases in the target domain, there is a point beyond which enforcing fairness harms accuracy (in the target domain). We refer to Appendix C for the simulation details.
R (cid:101) (cid:101) (cid:44) as the set of all risk proﬁles with respect to the training distribution
For notational simplicity deﬁne
˜P , i.e.
. The risk proﬁle of a model summarizes its performance on
|
R subpopulations. In terms of risk proﬁles, RP with respect to distribution P requires RP (h) = c1
R. This is a linear constraint. The set of all risk proﬁles that satisfy the RP for some constant c constraint with respect to the training distribution ˜P constitutes the following subspace:
R
∈ H}
R(h)
{
∈ h
RP
F
RP( ˜P ) (cid:44)
≡ F
R ˜P (h)
{
∈
R|A|
R ˜P (h) = c1, 1
R|A|, c
R
.
}
∈
∈
Therefore, we enforce RP by solving (the empirical version of) a constraint risk minimization problem (cid:12) (cid:12)
E (3.2) (cid:96)(h(X), Y ) minh∈H subject to R ˜P (h) minR∈R subject to R ˜P (h)
[0, 1]|A| is the marginal distribution of A and consequently
PA, R ˜P (h) (cid:105) (cid:104)
RP(cid:41)
=
RP (cid:41) ≡ (cid:40)
∈ F
PA, R
∈ F (cid:40) (cid:101) (cid:101) (cid:2) (cid:3)
,
E (cid:105) (cid:104) (cid:2) (cid:101) (cid:101) (cid:101)
∈
·(cid:105)
PA
, (cid:104)·
R
= (cid:101)
F = R (cid:101)P ( 0, 1
}
{ hF and its corresponding risk proﬁle as (cid:96)(h(X), Y )
. where is euclidean inner product on R|A|. We deﬁne the minimizer
Note that, in (3.2), the inner product (cid:3) hF ). See Figure 1 for a graphical of (3.2) as depiction of (3.2) when there are two groups (
). Here we see the main beneﬁt of
A summarizing model performance with risk proﬁles: risk minimization problems are equivalent to linear optimization problems in terms of risk proﬁles (objective and constrain functions are linear in terms of risk proﬁle). This allows us to simplify our study of the effects of enforcing risk parity by reasoning about the risk proﬁles of the resulting models. Hereafter, we refer to this approach as fair risk minimization (FRM). It is not new; similar constrained optimization problems have appeared in the algorithmic fairness literature (e.g. see [1, 12, 10]). Our goal is evaluating whether this approach mitigates algorithmic biases and improves model performance in the target domain. There are efﬁcient algorithms for solving (4.3). One popular algorithm is a reductions approach by Agarwal et al. [1], which solves a sequence of weighted classiﬁcation problems with appropriately chosen weights to satisfy the desired algorithmic fairness constraints. This algorithm outputs randomized classiﬁers, which justiﬁes the subsequent convexity assumption on the set of risk proﬁles. (cid:101) (cid:101)
In order to relate model performance in the training and target domains, some restrictions on the distribution shift/bias is necessary, as it is impossible to transfer performance parity during training to the target domain if they are highly disparate. At a high-level, we assume the distribution shift is a subpopulation shift [16]. Formally, we assume that the risk proﬁles of the models with respect to P ∗ and the proﬁles with respect to
P are identical:
E∗ (cid:96)(h(X), Y )
A = a (cid:101)
|
E
= (cid:96)(h(X), Y )
A = a for all a
, h
∈ A
.
∈ H
| (3.3) (cid:2) (cid:3) (cid:2) (cid:101) 3 (cid:3)
∈ H i.e. R ˜P (h) = RP ∗ (h) for all h
. We note that this assumption is (slightly) less restrictive than the usual subpopulation shift assumption because it only requires the expected value of the loss (instead of the full conditional distribution of (X, Y ) given A to be identical in the training and target domains.
Furthermore, this assumption is implicit in enforcing risk-based notions of algorithmic fairness. If the risk proﬁles are not identical in the training and target domains, then enforcing risk-based notions of algorithmic fairness during training is pointless because performance parity during training may not generalize to the target domain. We are now ready to state our characterization of the beneﬁts and drawbacks of enforcing RP. To keep things simple, we assume there are only two groups: a majority group and a minority group. As we shall see, depending on the marginal distribution of the subpopulations in the target domain P ∗
A, enforcing RP can harm or improve overall performance in the target domain.
Theorem 3.2. Without loss of generality, let ﬁrst entry of majority group in the training data i.e. ˜P (A = 1). Assume
PA be the fraction of samples from the (cid:101)
R2;
R ⊆
P and P ∗ are identical; is the risk proﬁle of the risk minimizer; (cid:101) is the risk proﬁle of the fair risk minimizer. (cid:105)
R0) = 1. there are only two groups and the set of risk proﬁles 2. subpopulation shift: the risk proﬁles with respect to
R (cid:44) arg minR∈R(cid:104) 3. (
R1, 4. ((
RF )0) =
RF )1, ( (cid:101) (cid:101) (cid:101)
Then we have: (cid:101) (cid:101)
PA, R
RF (cid:44) arg minR∈R∩FRP (cid:104) (cid:101) (cid:101)
P ∗
A,
PA, R
≤ (cid:104)
RF
R (cid:105) (cid:105) (cid:104)
P ∗
A,
P ∗
A, (cid:105) (cid:40)
≥ (cid:104)
RF (cid:101) (cid:105) (cid:101) if P ∗(A = 1) otherwise . (cid:101)R0−( (cid:101)RF )0 (cid:101)R0− (cid:101)R1
≥
Therefore, enforcing RP harms overall performance in the target domain in the ﬁrst case, while improves in the second. (cid:101) (cid:101)
In hindsight, this result is intuitive. If P ∗
PA (e.g. the minority group is underrepresented in the training data but not by much), then enforcing RP may actually harm overall performance in the target domain. This is mainly due to the trade-off between accuracy and fairness in the IID setting (no distribution shift). If there is little difference between the training and target domains, then we expect the trade-off between accuracy and fairness to manifest (albeit to a lesser degree than in IID settings).
A is close to (cid:101) 4 Beneﬁts and drawbacks of enforcing conditional risk parity 4.1 Risk-based notions of algorithmic fairness
∈ H
In this section, we consider more general risk-based notions of algorithmic fairness, namely Condi-tional Risk Parity (CRP) which is deﬁned as follows:
Deﬁnition 4.1 (Conditional Risk Parity). a model h is said to satisfy CRP if:
EP (cid:96)(h(X), Y )
A = a, V = v
|
= EP (cid:96)(h(X), Y )
|
A = a(cid:48), V = v (4.1) for all a, a(cid:48)
, v (cid:2)
∈ A
∈ V
, where V is known as the discriminative attribute [18]. (cid:2) (cid:3) (cid:3)
To keep things simple, we assume V is ﬁnite-valued, but it is possible to generalize our results to risk-based notions of algorithmic fairness with more general V ’s (see Appendix B). We also point out that this deﬁnition of CRP does not cover calibrations where one conditions on the model outcome ˆY .
It is not hard to see that risk parity is a special case of (4.1) in which V is a trivial random variable.
Another prominent instance is when V = Y , i.e. the risk proﬁle satisﬁes:
EP (cid:96)(h(X), Y )
|
A = a, Y = y
= EP (cid:96)(h(X), Y )
A = a(cid:48), Y = y
|
, y (cid:2)
∈ A for all a, a(cid:48)
. Deﬁnition 4.1 is motivated by the notion of equalized odds (EO) [14] in classiﬁcation. Recall EO requires the output of the ML model h(X) to be independent of the sensitive attribute A conditioned on the label: h(X)
Y . CRP imposes a similar condition on the risk of the ML model; i.e. the risk of the ML model must be independent of the sensitive attribute conditioned on the discriminative attribute (with label as a special case). Therefore CRP can
∈ Y
⊥
A (cid:3) (cid:2) (cid:3)
| 4
be viewed as a general version of EO, where we relax the conditional independence of h to equality of conditional means. CRP is also closely related to error rate balance [9] and overall accuracy equality [5] in classiﬁcation.
Like RP, (4.1) is equivalent to linear constraints on the risk proﬁles of ML models. Here (with a slight abuse of notation) we deﬁne the risk proﬁle of a classiﬁer h under distribution P as:
RP (h) (cid:44)
EP (cid:96)(h(X), Y )
A = a, V = v
| a∈A,v∈V (4.2)
Compared to (3.1), (4.2) offers a more detailed summary of the performance of ML models on subpopulations that not only share a common value of the sensitive attribute A, but also a common value of the discriminative attribute V . The general fairness constraint (4.1) on the training distribution
˜P is equivalent to R ˜P (h)
CRP, where (cid:3)(cid:9) (cid:8) (cid:2)
CRP (cid:44)
F
∈ F
R ˜P (h)
∈
{
R|A|×|Y|
CRP is a linear subspace deﬁned as:
F
R|Y|, h
R ˜P (h) = 1u(cid:62), 1
R|A|, u
∈
∈
|
.
∈ H}
In this section, we study a version of (3.2) with this general notions of algorithmic fairness:
E (cid:96)(h(X), Y ) minh∈H subject to R ˜P (h) minR∈R (cid:104) subject to R
∈ F
∈ F (cid:101)
[0, 1]|A|×|V| is the marginal distribution of (A, V ), i.e.
PA,V , R
PA,V , R
CRP(cid:41) (cid:3) (cid:105)
CRP(cid:41) (cid:40) (cid:40)
= (cid:101) (cid:2)
∈
PA,V where
. hF ).
As before we deﬁne the minimizer of (4.3) as (cid:3)
We note that (4.2) has the same beneﬁt as the deﬁnition in (3.1): the fair risk minimization problem in (4.3) is equivalent to a linear optimization problem in terms of the risk problems. This considerably (cid:101) simpliﬁes our study of the efﬁcacy of enforcing risk-based notions of algorithmic fairness.
= hF and its corresponding risk proﬁle as (cid:96)(h(X), Y )
F = R (cid:101)P ( (cid:2)
R (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:105) (cid:104)
E
, (4.3) 4.2 Subpopulation shift in the training data
|
| (cid:2) (cid:2) a
∀
=
H
, h
E∗
∈ V
, v (cid:101)
∈ A
∈ H (cid:3)
∈ H (cid:96)(h(X), Y ) (cid:96)(h(X), Y )
A = a, V = v
A = a, V = v
P are identical:
Similar to equation (3.3), we assume that the risk proﬁles with respect to P ∗ and
E
. (4.4) (cid:3) i.e. R ˜P (h) = RP ∗ (h) for all h
. This deﬁnition of subpopulation shift (equation (4.4)) is (cid:101) borrowed from the domain adaptation literature (see [16, 19]). The difference in our deﬁnition is that we require equality of the expectations of the loss functions, whereas these works assume the distributions to be equal for the sub-populations. Note that under subpopulation shift R ˜P are RP (cid:63) are equal over
. In the remaining part of Section 4 we shall drop the probability in subscript and denote them as R. We note the crucial role of the discriminative attributes in (4.4): the risk proﬁles are only required to be identical on subpopulations that share a value of the discriminative attribute. A good choice of discriminative attributes keeps the training data informative by ensuring the risk proﬁles are identical on the training data and at test time. Here are two examples of good discriminative attributes.
Example 4.2 (Under-representation bias). In binary classiﬁcation, training data may suffer from under-representation bias. This kind of bias arises when positive examples from disadvantaged groups are under-represented in the training data. Here is an example of a data generating process that suffers from label bias: (i) sample training examples (Xi, Yi, Ai) from P ∗, (ii) discard training examples from the disadvantaged group (Ai = 0) with positive label (Yi = 1) with probability β.
This leads to
P ∗(X, Y, A)
∝
P (X, Y, A) (1
Because there are fewer positive examples from the disadvantaged group in the training data (com-pared to test data), this kind of bias causes the ML model to predict mostly negative outcomes for the disadvantaged group. In practice, this kind of bias may creep into the training data more subtly. For example, if human judgements is a crucial part of the data generating process, then implicit biases may lead to over-representation of negative examples from disadvantaged groups in the training data
[24].
).
A = 0, Y = 1
β)1
}
{ (1
−
− (cid:101)
·
For training data with underrepresentation bias, a good choice of discriminative attribute is the label. This is because the training data is a ﬁltered version of the data at test time, and the ﬁltering process only depends on the label (and sensitive attribute). Thus the class conditionals at test time are preserved in the training data; i.e.
PX|a,y = P ∗
X|a,y for all a
, y
.
∈ A
∈ Y (cid:101) 5
Figure 2: Example in which enforcing algorithmic fairness harms performance in the target domain despite the Bayes decision rule in the target domain satisfying the algorithmic fairness constraint. 4.3 Fair risk minimization may not improve overall performance
We start by showing that fair risk minimization may not improve overall performance. Without other stipulations, this is implied by a result similar to Theorem 3.2 for more general risk-based notions of algorithmic fairness. Perhaps more surprising, is fair risk minimization may not improve overall performance even if the Bayes decision rule in the target domain is algorithmically fair:
CRP. (cid:105) ⊆ F
A,V , R arg minR∈R(cid:104)
P ∗
Figure 2 shows such a problem instance. The triangle is the set of risk proﬁles, and the dotted bottom of the triangle intersects the fair constraint (i.e. the risk proﬁles on the dotted line are algorithmically
P is chosen so that the risk proﬁle of (unconstrained) risk minimizer fair). The training objective on biased training data
R is the vertex on the top and the risk proﬁle of fair risk minimizer (also on
RF is the vertex on left. The test objective points to the right, so points close biased training data) to the right of the triangle have the best overall performance in the target domain. We see that
R is closer to the right of the triangle than
, i.e. (cid:105) ≤ (cid:104) (cid:101)
RF . This counterexample is not it has better performance in the target domain in comparison to surprising: the assumption that R∗ is fair is a constraint on P ∗,
, and
; it imposes no constraints
F
R
P ∗, on
RF
R (cid:101) (cid:105) ≤ (cid:104)
P adversarially, it is possible to have
RF , which immediately implies
P . By picking
P ∗, (cid:104)
P ∗,
P ∗,
RF
. (cid:105)
R (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:101) (cid:105) (cid:104) 4.4 When does fair risk minimization improve overall performance (cid:101) (cid:101) (cid:101) (cid:101)
The main result in this section provides necessary and sufﬁcient conditions for recovering the unbiased
Bayes’ classiﬁer with (4.3). As the unbiased Bayes’ classiﬁer is the (unconstrained) optimal classiﬁer in the target domain, enforcing a risk-based notion of al-gorithmic fairness will improve overall performance in the target domain if it recovers the unbiased Bayes’ classiﬁer.
At ﬁrst blush, it is tempting to think that because the unbiased Bayes classiﬁer satisﬁes CRP, then enforc-ing this constraint always increases accuracy, this is not the case as described in the previous paragraph.
Our next theorem characterizes the precise condition under which it is possible to improve accuracy on the target domain by enforcing fairness constraint:
Theorem 4.3. Under the assumptions 1. The risk set 2. The risk proﬁles with respect to is convex.
R
P and P ∗ are identical. 3. The unconstrained risk minimizer on un-i.e. (cid:101) biased data is algorithmically
P ∗, R arg minR∈R(cid:104) (cid:105) ⊆ F fair;
CRP. the fair risk minimization (4.3) obtains h that R(h) = R∗ if and only if
∈ H such
P
−
Figure 3: Total recovery from training bias by enforcing risk parity. In this example, the
P ∗ is always orthogonal to training bias the risk parity constraint (blue line) because
P and P ∗ are probability distributions. When the training bias does not affect the risk pro-ﬁles, enforcing risk parity allows us to totally (cid:101) overcome the training bias. Unfortunately, to show an example in which the risk decom-poses into recoverable and non-recoverable parts, we need (at least) two more dimensions. (cid:101)
ΠF (P ∗
A,V −
PA,V )
−
P ∗
A,V ∈ N
R(R∗) +
⊥
CRP,
F (4.5) (cid:101) 6
A,V (resp. where P ∗ risk proﬁle with respect to P ∗, the fair hyperplane. (cid:101)
N
PA,V ) is the marginal of P ∗ (resp.
R(R∗) is the normal cone of
P ) with respect to (A, V ), R∗ is the optimal at R∗ and ΠF is the projection on
R (cid:101)
R
This assumption that is convex is innocuous because it is possible to convexify the risk set by considering randomized decision rules. To evaluate a randomized decision rule H, we sample a decision rule h from H and evaluate h. It is not hard to see that the risk proﬁles of randomized decision rules are convex combinations of the risk proﬁles of (non-randomized) decision rules, so including randomized decision rules convexiﬁes the risk set. The third assumption is necessary for recovery of the unbiased Bayes classiﬁer. If the unbiased Bayes classiﬁer is not algorithmically fair, then there is no hope for (4.3) to recover it as there will always be a non-negligible bias term.
This assumption is also implicit in large swaths of the algorithmic fairness literature. For example,
Buolamwini and Gebru [7] and Yang et al. [23] suggest collecting representative training data to improve the accuracy of computer vision systems on individuals from underrepresented demographic groups. This suggestion implicitly assumes the Bayes classiﬁer on representative training data is algorithmically fair.
N
Theorem 4.3 characterizes the biases in the training data from which can be completely removed by enforcing appropriate algorithmic fairness constraints. The key insight from this theorem is a decomposition of the training bias into two parts: a part orthogonal to the fair constraint and
R(R∗). Enforcing an appropriate risk-based notion of algorithmic fairness the remaining part in overcomes the ﬁrst part of the bias. This occurs regardless of the magnitude of this part of the bias (see Corollary 4.4), which is also evident from our computational results. The second part of the bias
R(R∗), (the part in then the unconstrained risk minimizer on training data remains R∗. The magnitude of the bias in this set cannot be too large, and enforcing algorithmic fairness constraints does not help overcome this part of the bias. Although we stated our main result only for ﬁnite-valued discriminative attributes for simplicity of exposition, please see Appendix 2 for a more general version of Theorem 4.3 that applies to more general (including continuous-valued) discriminative attributes.
Corollary 4.4. A sufﬁcient condition for (4.5) is
R(R∗)) represents the “natural” robustness of R∗ to changes in P ∗: if
P is in
N
N (cid:101)
P ∗
PA,V
⊥
CRP.
−
A,V ∈ F
Corollary 4.4 allows large differences between the differences are conﬁned to in explore the implications of Corollary 4.4 for risk parity and CRP.
PA,V and its unbiased counterpart P ∗
A,V , as long as
⊥. Intuitively, (4.3) enables practitioners to recover from large biases
⊥. We
⊥ because the algorithmic fairness constraint “soaks up” any component of
PA,V in
F
F
F (cid:101) (cid:101)
⊥
Risk Parity: For RP, V is trivial random variable, hence
RP means that it has mean
= 1. Hence, the Bayes’ classiﬁer can be recovered 0. This is true for any under any perturbation. More speciﬁcally, recall the example of women historically underrepresented in STEM ﬁelds mentioned in the Introduction. Such training data is biased in its gender representation which differs at test time where women are better represented. Classiﬁers trained on biased data with the risk Parity fairness constraint will generalize better at test time.
PA, 1 (cid:105) (cid:104)
A, 1 (cid:105)
A ∈ F
P ∗ (cid:104)
PA as
P ∗
PA
−
= (cid:101) (cid:101) (cid:101) (cid:101) (cid:101)
−
−
P ∗
P ∗
PA,Y
A,Y .
A,Y ∈ F
⊥
Conditional risk parity: In this case V = Y and the condition
CRP implies that the sum of each column of
A,Y must be 0. Hence, to recover the Bayes classiﬁer under
PA,Y (cid:101) equalized odds fairness constraints, we are allowed to perturb P ∗
A,Y in such a way, that they have the same column sums: i.e. for any label, we are allowed to perturb the distribution of protected attributes for that label, but we have to keep the marginal distribution of the label to be same for both
PA,Y and P ∗
In practice, it is unlikely that the training bias is exactly orthogonal to the fair constraint, which (cid:101) happens only if the second part of the bias (i.e. the part in the normal cone at R∗) is small enough.
Theorem 4.3 provides a general characterization along with a precise notion of this “small enough” condition.
Remark 4.5. Theorem 4.3 can further be generalized for any discrete/continuous A and V (as deﬁned in (4.1), the proof for the continuous case can be found in the supplementary document).
Thus, our theory applies to many fairness constraints which fall under the setup in Equation (4.1), where V can be any discriminative attribute. However, our conditions do not cover calibration where one conditions on the model outcome ˆY . 7
4.5