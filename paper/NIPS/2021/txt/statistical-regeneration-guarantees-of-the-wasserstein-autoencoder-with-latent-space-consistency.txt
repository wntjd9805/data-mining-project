Abstract
The introduction of Variational Autoencoders (VAE) has been marked as a break-through in the history of representation learning models. Besides having several accolades of its own, VAE has successfully ﬂagged off a series of inventions in the form of its immediate successors. Wasserstein Autoencoder (WAE), being an heir to that realm carries with it all of the goodness and heightened generative promises, matching even the generative adversarial networks (GANs). Needless to say, recent years have witnessed a remarkable resurgence in statistical analyses of the GANs.
Similar examinations for Autoencoders, however, despite their diverse applicability and notable empirical performance, remain largely absent. To close this gap, in this paper, we investigate the statistical properties of WAE. Firstly, we provide statistical guarantees that WAE achieves the target distribution in the latent space, utilizing the Vapnik–Chervonenkis (VC) theory. The main result, consequently ensures the regeneration of the input distribution, harnessing the potential offered by Optimal Transport of measures under the Wasserstein metric. This study, in turn, hints at the class of distributions WAE can reconstruct after suffering a compression in the form of a latent law. 1

Introduction
Unsupervised data generation has been a time-hallowed problem in machine learning theory. Though not its strongest suit, Variational Autoencoders (VAE) [1] have been implemented for generative purposes to a great extent. Once emerged as a crucial unsupervised method for representation learning, its poor performances in this particular area somewhat depreciate its appeal. On the contrary, generative adversarial networks (GANs) [2] have proven to be superior when it comes to the quality of the generated image samples. The theoretical elegance of the adversarial training method can be duly credited for this achievement. In an attempt to adopt this merit, such ‘adversarial loss’ functions were incorporated into VAE objectives [3, 4]. The Wasserstein metric, also being used in GANs [5] previously, was an inevitable choice, since it brings along the beckoning of Transportation Theory.
Using the same to measure the reconstruction loss in a VAE, Wasserstein Autoencoder (WAE) [6] was conceived. By approaching generative modeling from an Optimal Transport (OT) point of view, it not only achieved remarkable reconstruction standards but also strengthened the theoretical backbone.
An appropriate WAE framework consists of two components, namely an encoder (E) and a decoder (D), which are represented by neural networks in practice. Samples from an input distribution (PX ) are fed into the encoder, which aims at resulting in a target law (PZ) in the latent space (Z). The decoder deals with the reconstruction of the original distribution from this ‘encoded’ signal. Mathematically, the objective of WAEs is to minimize L = R(D, E) + Ω(E) over the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
set of probabilistic encoders E. Here, R() ensures the reconstruction of samples put in whereas,
Ω() ensures the attainment of the latent distribution. Needless to say, R() is represented by the
Wasserstein discrepancy.
As the name suggests, our primary interest lies in investigating whether a suitably characterized WAE can learn a distribution, which represents its latent behaviour, without losing the memory of the initial signal it was fed. When it comes to the question of memorizing the input distribution in the form of reconstruction, theoretical guarantees in the context of GANs, have been provided rigorously. In the absence of a ﬂexible discriminator, WAEs need more care and closer observation. Moreover, the concurrent task of sculpting a desired latent distribution adds nuances to its mechanism. Most of the existing literature on deep generative models, under adversarial loss, misses out on this very point.
Against such a backdrop, this ﬁrst of its kind study substantiates the simultaneous performances of
WAE, from a theoretical viewpoint.
Our analysis starts with a special family of WAEs, namely f -Wasserstein Autoencoders (f -WAE)
[7], which arises due to the speciﬁc choice of Ω() as f -divergences (1). We follow a non-parametric approach in deﬁning both the networks involved and the distributions under consideration. The transformations induced by the encoder and the decoder are conceived as measurable functions between spaces. The nature of an ‘ideal’ encoder in our language, is a network that preserves information. Whereas, the decoder is characterized by a map that propagates the information retained, precisely. Our statistical assumptions on the same reﬂect these very ideas. As a result, the two concurrent objectives of f -WAE boil down to non-parametric estimation of laws, under vastly different circumstances.
Our major ﬁndings can be summarized in the following way,
• Given a non-negative margin of error, f -WAEs can learn latent space distributions with a convergence rate of O(n− 1 2 ), under fairly gentle assumptions on the input and target law (1). The proof involves a realistic characterization of the transformation, induced by the encoder network.
• Under a geometry preserving representation of the decoder map, Hölder densities can be successfully reconstructed by f -WAEs up to a constant margin (2). Our work also produces deterministic concentration bounds on the regeneration error and hence, the rate of convergence of the empirical laws.
• A particular instance of our analysis goes on to show that the input distribution converges strongly to its latent counterpart under encoding, in the form of empirical measures. Also, f -WAEs can exactly reproduce samples from smooth densities in the sense of weak conver-gence (4).
• Inferences, identical to that in case of f -WAE, can be drawn for the original WAE [6], under additional assumptions on the decoder (2). 2