Abstract
Learning in a multi-target environment without prior knowledge about the targets requires a large amount of samples and makes generalization difficult. To solve this problem, it is important to be able to discriminate targets through semantic understanding. In this paper, we propose goal-aware cross-entropy (GACE) loss, that can be utilized in a self-supervised way using auto-labeled goal states alongside reinforcement learning. Based on the loss, we then devise goal-discriminative attention networks (GDAN) which utilize the goal-relevant information to focus on the given instruction. We evaluate the proposed methods on visual navigation and robot arm manipulation tasks with multi-target environments and show that GDAN outperforms the state-of-the-art methods in terms of task success ratio, sample efficiency, and generalization. Additionally, qualitative analyses demonstrate that our proposed method can help the agent become aware of and focus on the given instruction clearly, promoting goal-directed behavior. 1

Introduction
Reinforcement learning (RL) has been expanding to various fields including robotics, to solve increasingly complex problems. For instance, RL has been gradually mastering skills such as robot arm/hand manipulation on an object [2, 48, 36, 22] and navigation to a target destination [18, 39].
However, to benefit humans like the R2-D2 robot in the Star Wars, RL must extend to realistic settings that require interaction with multiple objects or destinations, which is still challenging for RL.
For this reason, it is important for multi-target tasks to be considered. We use the term multi-target tasks to refer to tasks that require the agent to interact with variable goals. In a multi-target task, targets are possible goal candidates, which may be objects or key entities that play a decisive role in determining the success or failure of the task execution. The goals may be selected among the targets by the current multi-target task, specified with a cue or an instruction such as “Bring me a {spoon, cup or specific object}” or “Go to the {kitchen, livingroom or specific destination}”. The states in which the agent reaches the goal are called goal states.
Reinforcement learning allows learning multi-target or instruction-based tasks in an end-to-end manner. Latest reinforcement learning studies on these tasks mainly focus on prior knowledge about targets [27, 30, 8]. Other studies focus on learning representation on the environment [46] or about the targets only implicitly [44]. These methods lead to insufficient understanding of the goal, and sample-inefficiency and generalization problems arise.
For this matter, we propose a Goal-Aware Cross-Entropy (GACE) loss and Goal-Discriminative
Attention Networks (GDAN)2 for multi-target tasks in reinforcement learning. These methods, unlike
∗Corresponding authors. †AI Institute, Seoul National University 2Code available at https://github.com/kibeomKim/GACE-GDAN 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Architecture from Goal-Aware Cross-Entropy and GDAN (b) Attention in ActorCritic
Figure 1: Overview of the proposed architecture. (a) A goal-aware visual representation learning method by LGACE from goal-aware cross-entropy loss. The loss updates the feature extractor and the goal-discriminator. (b) In GDAN, the ActorCritic utilizes the information from the goal-discriminator as the query for goal-directed actions. See the details in Section 3.3 and 3.4. previous studies without prior knowledge, allow semantic understanding of goals including their appearances and other characteristics. The agent automatically labels and collects goal states data through trial-and-error in a self-supervised manner. Based on these self-collected data, we use GACE loss as an auxiliary loss to train a goal-discriminator that learns goal representations. Lastly, the
GDAN extracts the information from the goal-discriminator as a goal-relevant query, with which an attention is performed to infer goal-directed actions.
We additionally propose visual navigation and robot arm manipulation tasks as benchmarks for multi-target task experiments. These tasks involve targets which are the multiple types of objects randomly placed within the environments. In the benchmarks, we make these tasks visually complex using randomized background textures, allowing us to evaluate generalization.
In summary, the contributions of this paper are as follows:
• We propose a Goal-Aware Cross-Entropy loss for learning auto-labeled goal states in a self-supervised manner, solving instruction-based multi-target tasks in reinforcement learning.
• We additionally propose Goal-Discriminative Attention Networks that use the goal-relevant query from the goal-discriminator to focus exclusively on important goal-related information.
• Our method achieves significantly better performances in terms of the success ratio, sample-efficiency, and generalization on visual navigation and robot arm manipulation multi-target tasks. In particular, compared with the baseline methods, our method excels in Sample
Efficiency Improvement metric by 17 times in visual navigation task and by 4.8 times in manipulation task.
• We present two instruction-based benchmarks for goal-based multi-target environments, to interact with multiple targets for realistic settings. These benchmarks are made publicly available, along with the implementation of our proposed method. 2