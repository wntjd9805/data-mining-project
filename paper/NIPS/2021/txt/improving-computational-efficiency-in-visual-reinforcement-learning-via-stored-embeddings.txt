Abstract
Recent advances in off-policy deep reinforcement learning (RL) have led to im-pressive success in complex tasks from visual observations. Experience replay improves sample-efﬁciency by reusing experiences from the past, and convolutional neural networks (CNNs) process high-dimensional inputs effectively. However, such techniques demand high memory and computational bandwidth. In this paper, we present Stored Embeddings for Efﬁcient Reinforcement Learning (SEER), a simple modiﬁcation of existing off-policy RL methods, to address these computa-tional and memory requirements. To reduce the computational overhead of gradient updates in CNNs, we freeze the lower layers of CNN encoders early in training due to early convergence of their parameters. Additionally, we reduce memory requirements by storing the low-dimensional latent vectors for experience replay instead of high-dimensional images, enabling an adaptive increase in the replay buffer capacity, a useful technique in constrained-memory settings. In our experi-ments, we show that SEER does not degrade the performance of RL agents while signiﬁcantly saving computation and memory across a diverse set of DeepMind
Control environments and Atari games. 1

Introduction
Success stories of deep reinforcement learning (RL) from high dimensional inputs such as pixels or large spatial layouts include achieving superhuman performance on Atari games [30, 37, 1], grandmaster level in Starcraft II [50] and grasping a diverse set of objects with impressive success rates and generalization with robots in the real world [21]. Modern off-policy RL algorithms [30, 15, 11, 12, 39, 22, 24] have improved the sample-efﬁciency of agents that process high-dimensional pixel inputs with convolutional neural networks (CNNs; LeCun et al. 25) using past experiential data that is typically stored as raw observations in a replay buffer [28]. However, these methods demand high memory and computational bandwidth, which makes deep RL inaccessible in several scenarios, such as learning with much lighter on-device computation (e.g. mobile phones or other light-weight edge devices).
For compute- and memory-efﬁcient deep learning, several strategies, such as network pruning [13, 8], quantization [13, 17] and freezing [53, 36] have been proposed in supervised learning and unsupervised learning for various purposes (see Section 2 for more details). In computer vision,
Raghu et al. [36] and Brock et al. [5] showed that the computational cost of updating CNNs can be reduced by freezing lower layers earlier in training, and Han et al. [13] introduced a deep compression, which reduces the memory requirement of neural networks by producing a sparse network. In natural language processing, several approaches [46, 42] have studied improving the computational efﬁciency of Transformers [49]. In deep RL, however, developing compute- and memory-efﬁcient techniques has received relatively little attention despite their serious impact on the practicality of RL algorithms.
In this paper, we propose Stored Embeddings for Efﬁcient Reinforcement Learning (SEER), a simple technique to reduce computational overhead and memory requirements that is compatible with various 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
off-policy RL algorithms [10, 15, 39]. Our main idea is to freeze the lower layers of CNN encoders of RL agents early in training, which enables two key capabilities: (a) compute-efﬁciency: reducing the computational overhead of gradient updates in CNNs; (b) memory-efﬁciency: saving memory by storing the low-dimensional latent vectors to experience replay instead of high-dimensional images.
Additionally, we leverage the memory-efﬁciency of SEER to adaptively increase replay capacity, resulting in improved sample-efﬁciency of off-policy RL algorithms in constrained-memory settings.
SEER achieves these improvements without sacriﬁcing performance due to early convergence of
CNN encoders.
The main contributions of this paper are as follows:
• We present SEER, a compute- and memory-efﬁcient technique that can be used in conjunction with most modern off-policy RL algorithms [10, 15].
• We show that SEER signiﬁcantly reduces computation while matching the original performance of existing RL algorithms on both continuous control tasks from DeepMind Control Suite [45] and discrete control tasks from Atari games [2].
• We show that SEER improves the sample-efﬁciency of RL agents in constrained-memory settings by enabling an increased replay buffer capacity. 2