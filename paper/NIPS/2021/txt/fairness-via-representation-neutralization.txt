Abstract
Existing bias mitigation methods for DNN models primarily work on learning debi-ased encoders. This process not only requires a lot of instance-level annotations for sensitive attributes, it also does not guarantee that all fairness sensitive information has been removed from the encoder. To address these limitations, we explore the following research question: Can we reduce the discrimination of DNN models by only debiasing the classiﬁcation head, even with biased representations as in-puts? To this end, we propose a new mitigation technique, namely, Representation
Neutralization for Fairness (RNF) that achieves fairness by debiasing only the task-speciﬁc classiﬁcation head of DNN models. To this end, we leverage samples with the same ground-truth label but different sensitive attributes, and use their neutralized representations to train the classiﬁcation head of the DNN model. The key idea of RNF is to discourage the classiﬁcation head from capturing undesirable correlation between fairness sensitive information in encoder representations with speciﬁc class labels. To address low-resource settings with no access to sensitive attribute annotations, we leverage a bias-ampliﬁed model to generate proxy an-notations for sensitive attributes. Experimental results over several benchmark datasets demonstrate our RNF framework to effectively reduce discrimination of
DNN models with minimal degradation in task-speciﬁc performance. 1

Introduction
Deep neural networks (DNNs) have made signiﬁcant advances in recent times [1, 2, 3], and have been deployed in many real-world applications. However, DNNs often suffer from biases and show discrimination towards certain demographics, especially in high-stake applications, such as criminal justice, employment, loan approval, credit scoring, etc [4, 5, 6]. For example, COMPAS, an algorithmic recidivism predictor, is likely to associate African-American offenders with higher risk scores compared to Caucasians while having a similar proﬁle [7]. This brings signiﬁcant harm to both society and individuals, thus leading to recent focus on mitigation techniques to alleviate the adverse effects of DNN biases.
Existing debiasing methods usually work on learning debiased representations at the encoder-level. One representative family of methods perform mitigation by explicitly learning debiased representations, either through adversarial learning [8, 9, 10] or invariant risk minimization [11, 12].
Another family of methods [13, 14, 15] implicitly learn debiased representations by incorporating explanation during model training to suppress it from paying high attention to biased features in the original input. Essentially, the above methods aim to remove the bias from deep representations.
Learning debiased representations is a technically challenging problem. Firstly, it is hard to remove all fairness sensitive information in the encoder. The suppression of fairness sensitive information
∗ Part of the work was done while the ﬁrst author was an intern at Microsoft Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
might also remove useful information that is task relevant. Secondly, most existing debiasing methods assume access to additional meta-data such as fairness sensitive attributes and a lot of annotations corresponding to the protected groups to guide the learning of debiased representations. However, such resources are expensive to obtain, if not unavailable, for most real world applications.
To address these limitations, we explore the following research question: Can we reduce the discrim-ination of DNN models by only debiasing the task-speciﬁc classiﬁcation head, even with a biased representation encoder? Our work is motivated by the empirical observation that standard training can result in the classiﬁcation head capturing undesirable correlation between fairness sensitive information and speciﬁc class labels. Some recent works [16, 17, 18] have explored such spurious or shortcut learning behavior of DNNs in various applications. To this end, we propose the RNF (Representation Neutralization for Fairness) framework for mitigation, motivated by the Mixup work [19, 20]. We ﬁrst train a biased teacher network via standard cross entropy loss. In the second stage, we freeze the representation encoder of the biased teacher, and only update the classiﬁcation head via representation neutralization. This discourages the model from associating biased features with speciﬁc class labels, and enforces the model to focus more on task relevant information. To address low-resource settings, our RNF framework does not require any access to the protected attributes during training. To this end, we train a bias-ampliﬁed model using generalized cross entropy loss that is used to generate proxy annotations for sensitive attributes. Experimental results over several benchmark tabular and image datasets demonstrate our RNF framework to signiﬁcantly reduce discrimination of DNN models with minimal degradation of the task performance. The major contributions of our work can be summarized as follows:
• We analyze bias propagation from the encoder representations to the ﬁnal task-speciﬁc layer demonstrating that DNN models heavily rely on undesirable correlations for prediction.
• We introduce RNF, a bias mitigation framework for DNN models via representation neutralization.
Our RNF framework achieves mitigation without any access to instance-level sensitive attribute annotations, and instead relies on self-generated proxy annotations.
• Experimental results on several benchmark datasets demonstrate the effectiveness of our RNF framework via debiasing only the classiﬁcation head while using biased representations as input.
Additionally, we show RNF to be complementary to existing methods that learn debiased encoders and can be further improved within our framework. 2