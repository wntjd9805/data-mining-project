Abstract
Recent research in deep learning has investigated two very different forms of
“implicitness”: implicit representations model high-frequency data such as images or 3D shapes directly via a low-dimensional neural network (often using e.g., sinusoidal bases or nonlinearities); implicit layers, in contrast, refer to techniques where the forward pass of a network is computed via non-linear dynamical systems, such as ﬁxed-point or differential equation solutions, with the backward pass computed via the implicit function theorem. In this work, we demonstrate that these two seemingly orthogonal concepts are remarkably well-suited for each other. In particular, we show that by exploiting ﬁxed-point implicit layer to model implicit representations, we can substantially improve upon the performance of the conventional explicit-layer-based approach. Additionally, as implicit representation networks are typically trained in large-batch settings, we propose to leverage the property of implicit layers to amortize the cost of ﬁxed-point forward/backward passes over training steps – thereby addressing one of the primary challenges with implicit layers (that many iterations are required for the black-box ﬁxed-point solvers). We empirically evaluated our method on learning multiple implicit representations for images, audios, videos, and 3D models, showing that our (Implicit)2 approach substantially improve upon existing models while being both faster to train and much more memory efﬁcient. 1

Introduction
→ C
, where
The concept of “implicitness” has recently been applied to various contexts of machine learning research. One particular thread is implicit representations [19, 22, 28, 27], which aims to learn a continuous representation of high-frequency, often discretely-measured signals such as large images.
Rd, an implicit representation of it is a
Formally, given a spatial-temporal coordinate input x function Φ : Rd is the space of desired quantity (e.g. color, volume density, distance, etc.) and Φ is usually parameterized with a neural network. Such implicitness has multiple beneﬁts over the conventional discrete (e.g., grid-based) representations; e.g., as Φ is deﬁned on a continuous domain, an implicitly represented input consumes much less storage than the original input; as another example, Φ is differentiable, hence allowing for the computation of higher-order derivatives.
However, the training of these Φ networks themselves are usually quite memory-consuming, since we usually deal with very high-resolution images and videos, and training is typically done in full-batch mode (e.g., when training on a batch size of 512 512, a simple 4-layer MLP with 1024 hidden units already requires > 16 GB memory just to store the intermediate activations).
×
∈
C
A recent (orthogonal) usage of implicitness in deep learning comes from the notion of implicit layers/models, where the word is used to characterize model architectures (rather than input repre-sentations). The nomenclature comes from the concept of implicit vs. explicit functions: instead of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
representing a model as an explicit stacking of layers, an implicit model solves a non-linear dynamical system [10] (e.g., ODEs [7, 9] or ﬁxed-points [1, 32]). Importantly, these implicit models decouple the forward and backward passes and analytically differentiate via the implicit function theorem [14].
This enables these models to use constant training memory, irrespective of the forward pass trajectory.
Moreover, recent works on deep equilibrium models [1] have been shown to be able to achieve results on par with the state-of-the-art explicit models (e.g., Transformers [31]) on very large-scale vision or
NLP tasks [1, 2]. However, despite training memory efﬁciency, these implicit layers are also slower to compute (e.g., by 2-3
), since the solvers are usually iterative in nature and ultimately require
× many function evaluations.
Despite the success of these “implicit” methods in their respective areas, the two concepts so far have rarely intersected: existing models used for implicit representations are neither implicit in nature (since they are still stacked multi-layer explicit operators trained end-to-end in the feedforward manner), nor do they exploit any properties of the implicit functions during training (e.g., implicit differentiation). In this paper, we argue that implicit representations and implicit layers are remarkably well-suited to each other, and their use together can compensate for their aforementioned drawbacks.
Speciﬁcally, we propose to combine the best of both worlds by replacing the explicit models used in conventional implicit representation learning with implicit layers, whose output is deﬁned by a
ﬁxed-point condition, thus dubbed (Implicit)2 networks. Formally, let x be the input and F be an (often shallow) layer, the (Implicit)2 approach learns the following implicit representation Φ:
Φ(x) = W z(cid:63) + b, where z(cid:63) = F (z(cid:63); x)
Importantly, we show that these two “implicitness” complement each other well, especially in two important aspects. First, the unique large-batch training scheme of implicit representations as well as the forward/backward decoupling properties of implicit models permit us to amortize the cost of the iterative solver that would otherwise make implicit models slow to train. We show that by ﬁxed-point reuse (in the forward pass) and Jacobian approximation (in the backward pass), training an implicit model on implicit representation tasks requires almost no computation overhead. Second, implicit models allow us to train these high-frequency input tasks with a much lower memory consumption while not losing speed or performance, thus signiﬁcantly lowering the hardware requirement and computation budget of training implicit representations. We evaluate (Implicit)2 on multiple implicit representations for images, videos, and audios, showing substantial improvement upon existing competitive explicit-model methods like SIREN [27] or MFN [11] using equivalent-sized networks. (1) 2