Abstract
The ﬁrst order derivative of a data density can be estimated efﬁciently by denoising score matching, and has become an important component in many applications, such as image generation and audio synthesis. Higher order derivatives provide additional local information about the data distribution and enable new applications.
Although they can be estimated via automatic differentiation of a learned density model, this can amplify estimation errors and is expensive in high dimensional settings. To overcome these limitations, we propose a method to directly estimate high order derivatives (scores) of a data density from samples. We ﬁrst show that denoising score matching can be interpreted as a particular case of Tweedie’s formula. By leveraging Tweedie’s formula on higher order moments, we generalize denoising score matching to estimate higher order derivatives. We demonstrate empirically that models trained with the proposed method can approximate second order derivatives more efﬁciently and accurately than via automatic differentiation.
We show that our models can be used to quantify uncertainty in denoising and to improve the mixing speed of Langevin dynamics via Ozaki discretization for sampling synthetic data and natural images. 1

Introduction
The ﬁrst order derivative of the log data density function, also known as score, has found many applications including image generation [23, 24, 6], image denoising [20, 19] and audio synthesis [9].
Denoising score matching (DSM) [29] provides an efﬁcient way to estimate the score of the data density from samples and has been widely used for training score-based generative models [23, 24] and denoising [20, 19]. High order derivatives of the data density, which we refer to as high order scores, provide a more accurate local approximation of the data density (e.g., its curvature) and enable new applications. For instance, high order scores can improve the mixing speed for certain sampling methods [2, 18, 12], similar to how high order derivatives accelerate gradient descent in optimization [11]. In denoising problems, given a noisy datapoint, high order scores can be used to compute high order moments of the underlying noise-free datapoint, thus providing a way to quantify the uncertainty in denoising.
Existing methods for score estimation [8, 29, 25, 32], such as denoising score matching [29], focus on estimating the ﬁrst order score (i.e., the Jacobian of the log density). In principle, high order scores can be estimated from a learned ﬁrst order score model (or even a density model) via automatic differentiation. However, this approach is computationally expensive for high dimensional data and score models parameterized by deep neural networks. For example, given a D dimensional 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
n p
-th order score value from an existing n-th order score model by distribution, computing the q automatic differentiation is on the order of D times more expensive than evaluating the latter [25].
Moreover, computing higher-order scores by automatic differentiation might suffer from large estimation error, since a small training loss for the ﬁrst order score does not always lead to a small estimation error for high order scores.
` 1
To overcome these limitations, we propose a new approach which directly models and estimates high order scores of a data density from samples. We draw inspiration from Tweedie’s formula [4, 16], which connects the score function to a denoising problem, and show that denoising score matching (DSM) with Gaussian noise perturbation can be derived from Tweedie’s formula with the knowledge of least squares regression. We then provide a generalized version of Tweedie’s formula which allows us to further extend denoising score matching to estimate high order scores. In addition, we provide variance reduction techniques to improve the optimization of these newly introduced high order score estimation objectives. With our approach, we can directly parameterize high order scores and learn them efﬁciently, sidestepping expensive automatic differentiation.
While our theory and estimation method is applicable to scores of any order, we focus on the second order score (i.e., the Hessian of the log density) for empirical evaluation. Our experiments show that models learned with the proposed objective can approximate second order scores more accurately than applying automatic differentiation to lower order score models. Our approach is also more computationally efﬁcient for high dimensional data, achieving up to 500 speedups for second order score estimation on MNIST. In denoising problems, there could be multiple clean datapoints consistent with a noisy observation, and it is often desirable to measure the uncertainty of denoising results. As second order scores are closely related to the covaraince matrix of the noise-free data conditioned on the noisy observation, we show that our estimated second order scores can provide extra insights into the solution of denoising problems by capturing and quantifying the uncertainty of denoising. We further show that our model can be used to improve the mixing speed of Langevin dynamics for sampling synthetic data and natural images. Our empirical results on second order scores, a special case of the general approach, demonstrate the potential and applications of our method for estimating high order scores.
ˆ 2