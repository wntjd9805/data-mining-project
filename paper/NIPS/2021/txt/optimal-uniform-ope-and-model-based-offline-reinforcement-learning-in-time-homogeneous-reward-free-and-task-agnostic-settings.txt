Abstract
This work studies the statistical limits of uniform convergence for ofﬂine policy evaluation (OPE) problems with model-based methods (for episodic MDP) and provides a uniﬁed framework towards optimal learning for several well-motivated ofﬂine tasks. Uniform OPE supΠ |Qπ − ˆQπ| < (cid:15) is a stronger measure than the point-wise OPE and ensures ofﬂine learning when Π contains all policies (the global class). In this paper, we establish an Ω(H 2S/dm(cid:15)2) lower bound (over model-based family) for the global uniform OPE and our main result establishes an upper bound of ˜O(H 2/dm(cid:15)2) for the local uniform convergence that applies to all near-empirically optimal policies for the MDPs with stationary transition. Here dm is the minimal marginal state-action probability. Critically, the highlight in achieving the optimal rate ˜O(H 2/dm(cid:15)2) is our design of singleton absorbing MDP, which is a new sharp analysis tool that works with the model-based approach. We generalize such a model-based framework to the new settings: ofﬂine task-agnostic and the ofﬂine reward-free with optimal complexity ˜O(H 2 log(K)/dm(cid:15)2) (K is the number of tasks) and ˜O(H 2S/dm(cid:15)2) respectively. These results provide a uniﬁed solution for simultaneously solving different ofﬂine RL problems. 1

Introduction
Ofﬂine reinforcement learning (ofﬂine RL) targets at learning a reward-maximizing policy in an unknown Markov Decision Process (MDP) using a static data generated by running a behavior policy
[Lange et al., 2012, Levine et al., 2020]. This framework is widely applicable in applications where online exploration is demanding but historical data are plentiful. Examples include medicine [Liu et al., 2017] (safety concerns limit the applicability of unproven treatments but electronic records are abundant) and autonomous driving [Codevilla et al., 2018] (building infrastructure for testing new policy is expensive while collecting data from current setting is almost free).
Parallel to its practical signiﬁcance, recently there is a surge of theoretical investigations towards ofﬂine RL via two threads: ofﬂine policy evaluation (OPE), where the goal is to estimate the value of a target (ﬁxed) policy V π [Jiang and Li, 2016, Liu et al., 2018, Kallus and Uehara, 2020, 2019, Uehara and Jiang, 2019, Nachum et al., 2019, Xie et al., 2019, Yin and Wang, 2020, Duan et al., 2020, Wang et al., 2021, Zhang et al., 2021a] and ofﬂine (policy) learning which intends to output a near-optimal policy [Chen and Jiang, 2019, Le et al., 2019, Xie and Jiang, 2021, 2020, Liu et al., 2020b, Hao et al., 2020, Zanette, 2021, Jin et al., 2020c, Hu et al., 2021, Yin et al., 2021b, Rashidinejad et al., 2021].
Yin et al. [2021a] initiates the studies for ofﬂine RL from the new perspective of uniform convergence in OPE (uniform OPE for short) which uniﬁes OPE and ofﬂine learning tasks. Generally speaking, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
given a policy class Π and ofﬂine data with n episodes, uniform OPE seeks to coming up with OPE estimators (cid:98)V π 1 ||∞ < (cid:15). The task is to achieve this with the optimal episode complexity: the “minimal” number of episodes n needed as a function of (cid:15), failure probability
δ, the parameters of the MDP as well as the behavior policy µ in the minimax sense. 1 satisfy supπ∈Π || (cid:98)Qπ 1 and (cid:98)Qπ 1 − Qπ
To further motivate the readers why uniform OPE should be considered, we state its relation to ofﬂine learning. Indeed, uniform OPE to RL is analogous of uniform convergence of empirical risk in statistical learning [Vapnik, 2013]. In supervised learning, it has been proven that almost all learnable problems are learned by an (asymptotic) empirical risk minimizer (ERM) [Shalev-Shwartz et al., 2010]. In ofﬂine RL, the natural counterpart is the empirical optimal policy (cid:98)π(cid:63) := argmaxπ (cid:98)V π 1 and with uniform OPE it further ensures (cid:98)π(cid:63) is a near-optimal policy for the ofﬂine learning via: 0 ≤ Qπ(cid:63) 1 − Q(cid:98)π(cid:63) 1 = Qπ(cid:63) 1 − (cid:98)Qπ(cid:63) 1 + (cid:98)Qπ(cid:63) 1 − (cid:98)Q(cid:98)π(cid:63) 1 + (cid:98)Q(cid:98)π(cid:63) 1 − Q(cid:98)π(cid:63) 1 ≤ 2 sup
π
|Qπ 1 − (cid:98)Qπ 1 |. (1)
On the policy evaluation side, there is often a need to evaluate the performance of a data-dependent policy. Uniform OPE sufﬁces for this purpose since it will allow us to evaluate policies selected by safe-policy improvements, proximal policy optimization, UCB-style exploration-bonus as well as any heuristic exploration criteria (please refer to Yin et al. [2021a] and the references therein for further discussions). In this paper, we study the uniform OPE problem under the ﬁnite horizon stationary
MDPs and focus on the model-based approaches. Speciﬁcally, we consider two representative class: global policy class Πg (contains all (deterministic) policies) and local policy class Πl (contains policies near the empirical optimal one, see Section 2.1). We ask the following question:
What is the statistical limit for uniform OPE and what is its connection to optimal ofﬂine learning?
We answer the ﬁrst part by showing the global uniform OPE requires a lower bound of
Ω(H 2S/dm(cid:15)2)1 for the family of model-based approach and the local uniform OPE can achieve
˜O(H 2/dm(cid:15)2) minimax rate by the model-based plug-in estimator and this implies optimal ofﬂine learning. Importantly, the procedure of the model-based approach via learning (cid:98)π(cid:63) through planning over the empirical MDP has a wider range of use in ofﬂine RL as it naturally adapts to the challenging tasks like ofﬂine task-agnostic learning and ofﬂine reward-free learning. See Section 1.2. 1.1