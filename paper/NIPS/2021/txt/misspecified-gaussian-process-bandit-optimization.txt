Abstract
We consider the problem of optimizing a black-box function based on noisy bandit feedback. Kernelized bandit algorithms have shown strong empirical and theoretical performance for this problem. They heavily rely on the assumption that the model is well-speciﬁed, however, and can fail without it. Instead, we introduce a misspeciﬁed kernelized bandit setting where the unknown function can be ✏–uniformly approximated by a function with a bounded norm in some Repro-ducing Kernel Hilbert Space (RKHS). We design efﬁcient and practical algorithms whose performance degrades minimally in the presence of model misspeciﬁcation.
Speciﬁcally, we present two algorithms based on Gaussian process (GP) methods: an optimistic EC-GP-UCB algorithm that requires knowing the misspeciﬁcation error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspeciﬁcation. We provide upper bounds on their cumulative regret in terms of ✏, the time horizon, and the underlying kernel, and we show that our algorithm achieves optimal dependence on ✏ with no prior knowledge of misspeciﬁcation. In addition, in a stochastic contextual setting, we show that EC-GP-UCB can be effectively combined with the regret bound balancing strategy and attain similar regret bounds despite not knowing ✏. 1

Introduction
Bandit optimization has been successfully used in a great number of machine learning and real-world applications, e.g., in mobile health [42], environmental monitoring [40], economics [27], hyperparameter tuning [26], to name a few. To scale to large or continuous domains, modern bandit approaches try to model and exploit the problem structure that is often manifested as correlations in rewards of "similar" actions. Hence, the key idea of kernelized bandits is to consider only smooth reward functions of a low norm belonging to a chosen Reproducing Kernel Hilbert Space (RKHS) of functions. This permits the application of ﬂexible nonparametric Gaussian process (GP) models and Bayesian optimization methods via a well-studied link between RKHS functions and GPs (see, e.g., [18] for a concise review).
A vast majority of previous works on nonparametric kernelized bandits have focused on designing algorithms and theoretical bounds on the standard notions of regret (see, e.g., [40, 9, 35]). However, they solely focus on the realizable (i.e., well-speciﬁed) case in which one assumes perfect knowledge of the true function class. For example, the analysis of the prominent GP-UCB [40] algorithm assumes the model to be well-speciﬁed and ignores potential misspeciﬁcation issues. As the realizability assumption may be too restrictive in real applications, we focus on the case where it may only hold approximately. In practice, model misspeciﬁcations can arise due to various reasons, such as incorrect choice of kernel, consideration of an overly smooth function class, hyperparameter estimation errors, etc. Hence, an open question is to characterize the impact of model misspeciﬁcation in the kernelized setting, and to design robust algorithms whose performance degrades optimally with the increasing level of misspeciﬁcation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we study the GP bandit problem with model misspeciﬁcation in which the true unknown function might be ✏-far (as measured in the max norm) from a member of the learner’s assumed hypothesis class. We propose a novel GP bandit algorithm and regret bounds that depend on the misspeciﬁcation error, time horizon, and underlying kernel. Speciﬁcally, we present an algorithm that is based on the classical uncertainty sampling approach that is frequently used in
Bayesian optimization and experimental design. Importantly, our main presented algorithm assumes no knowledge of the misspeciﬁcation error ✏ and achieves standard regret rates in the realizable case.