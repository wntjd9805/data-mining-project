Abstract
Machine learning models trained on uncurated datasets can often end up adversely affecting inputs belonging to underrepresented groups. To address this issue, we consider the problem of adaptively constructing training sets which allow us to learn classiﬁers that are fair in a minimax sense. We ﬁrst propose an adaptive sampling algorithm based on the principle of optimism, and derive theoretical bounds on its performance. We also propose heuristic extensions of this algorithm suitable for application to large scale, practical problems. Next, by deriving algorithm independent lower-bounds for a speciﬁc class of problems, we show that the performance achieved by our adaptive scheme cannot be improved in general. We then validate the beneﬁts of adaptively constructing training sets via experiments on synthetic tasks with logistic regression classiﬁers, as well as on several real-world tasks using convolutional neural networks (CNNs). 1

Introduction
Machine learning (ML) models are increasingly being applied for automating the decision-making process in several sensitive applications, such as loan approval and employment screening. However, recent work has demonstrated that discriminatory behaviour might get encoded in the model at various stages of the ML pipeline, such as data collection, labelling, feature selection, and training, and as a result, adversely impact members of some protected groups in rather subtle ways (Barocas and Selbst, 2016). This is why ML researchers have started to introduce a large number of fairness measures to include the notion of fairness in the design of their algorithms. Some of the important measures of fairness include demographic parity (Zemel et al., 2013), equal odds and opportunity (Hardt et al., 2016; Woodworth et al., 2017), individual fairness (Dwork et al., 2012), and minimax fairness (Feld-man et al., 2015). The minimax fairness is particularly important in scenarios in which it is necessary to be as close as possible to equality without introducing unnecessary harm (Ustun et al., 2019).
These scenarios are common in areas such as healthcare and predicting domestic violence. A measure that has been explored to achieve this goal is predictive risk disparity (Feldman et al., 2015; Chen et al., 2018; Ustun et al., 2019). Instead of using the common approach of putting constraints on the norm of discrimination gaps, Martinez et al. (2020) has recently introduced the notion of minimax
Pareto fairness. These are minimax classiﬁers that are on the Pareto frontier of prediction risk, i.e., no decrease in the predictive risk of one group is possible without increasing the risk of another one.
In this paper, we are primarily interested in the notion of minimax fairness in terms of the predictive risk. However, instead of studying the training phase of the ML pipeline, our focus is on the data-collection stage, motivated by Jo and Gebru (2019) and Holstein et al. (2018). In particular, we study the following question: given a ﬁnite sampling budget, how should a learner construct a training set consisting of elements from different protected groups in appropriate proportions to ensure that a classiﬁer trained on this dataset achieves minimax fairness?
Our work is motivated by the following scenario: suppose we have to learn a ML model for performing a task (e.g., loan approval) for inputs belonging to different groups based on protected attributes, such
∗Electrical and Computer Engineering Department at UCSD ({shshekha,grﬁelds,tjavidi}@ucsd.edu)
†Google Research (ghavamza@google.com) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
as race or gender. Depending upon the distribution of input-label pairs from different groups, the given task may be statistically harder for some groups. Our goal is to ensure that the eventual ML model has optimal predictive accuracy for the worst-off group. We show that, under certain technical conditions, this results in a model with comparable predictive accuracy over all groups. One approach for this would be to train separate classiﬁers for each group. However, this is often not possible as the group-membership information may not be available at the deployment time, or it may be forbidden by law to explicitly use the protected characteristics as an input in the prediction process (Lipton et al., 2017, § 1). To ensure having a higher proportion of samples from the harder groups without knowing the identity of the hard or easy groups apriori, we consider this problem in an active setting, where a learner has to incrementally construct a training set by drawing samples one at a time (or a batch at a time) from different groups. Towards this goal, we propose and analyze an adaptive sampling scheme based on the principle of optimism, used in bandits literature (e.g., Auer et al. 2002), that detects the harder groups and populates the training set with more samples from them in an adaptive manner. We also wish to note that bias in ML has multi-faceted origins and that our work here addresses dataset construction and cannot account for bias introduced by model selection, the underlying data distribution, or other sources as discussed in Hooker (2021), Suresh and Guttag (2019). We also endeavor to ensure minimax fairness, but in some contexts another notion of fairness, such as those mentioned above, may be more appropriate or equitable. In general, application of our algorithm is not a guarantee that the resulting model is wholly without bias.
Our main contributions are: 1) We ﬁrst propose an optimistic adaptive sampling strategy, Aopt, for training set construction in Section 3. This strategy is suited to smaller problems and admits theoretical guarantees. We then introduce a heuristic variant of Aopt in Section 4 that is more suitable to practical problems involving CNNs. 2) We obtain upper bounds on the convergence rate of Aopt, and show its minimax near-optimality by constructing a matching lower bound in Section 3.1. 3)
Finally, we demonstrate the beneﬁts of our algorithm with empirical results on several synthetic and real-world datasets in Section 5.