Abstract
A critical aspect of human visual perception is the ability to parse visual scenes into individual objects and further into object parts, forming part-whole hierar-chies. Such composite structures could induce a rich set of semantic concepts and relations, thus playing an important role in the interpretation and organiza-tion of visual signals as well as for the generalization of visual perception and reasoning. However, existing visual reasoning benchmarks mostly focus on ob-jects rather than parts. Visual reasoning based on the full part-whole hierarchy is much more challenging than object-centric reasoning due to ﬁner-grained concepts, richer geometry relations, and more complex physics. Therefore, to better serve for part-based conceptual, relational and physical reasoning, we introduce a new large-scale diagnostic visual reasoning dataset named PTR. PTR contains around 70k RGBD synthetic images with ground truth object and part level annotations regarding semantic instance segmentation, color attributes, spatial and geometric relationships, and certain physical properties such as stability. These images are paired with 700k machine-generated questions covering various types of reasoning types, making them a good testbed for visual reasoning models. We examine several state-of-the-art visual reasoning models on this dataset and observe that they still make many surprising mistakes in situations where humans can easily infer the correct answer. We believe this dataset will open up new opportunities for part-based reasoning. PTR dataset and baseline models are publicly available 2. 1

Introduction
A long-standing challenge in the ﬁeld of artiﬁcial intelligence is to enable machines to reason and answer questions about visual scenes. Several datasets [64, 16, 29, 49, 57] have been proposed to tackle this challenge. They mostly focus on object-level features without emphasizing much on detailed part-level understanding. However, there is strong psychological evidence that human beings parse visual scenes into part-whole hierarchies (e.g., from a scene to the objects, from an object to its parts), which are currently missing from existing datasets.
Inspired by Aristotle’s quote “the whole is greater than the sum of its parts”, there has been a long history of research in psychology concerning how parts and wholes are related, beginning with the
Gestalt psychologists [47]. There have also been recent efforts on how to represent such part-whole hierarchies using neural networks [19]. Introducing part-whole hierarchies into visual reasoning
∗The work was done while Yining Hong was a research intern at MIT-IBM Watson AI Lab. 2Project page: http://ptr.csail.mit.edu/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
Figure 1: Exemplar scene graphs, questions, and programs of our Part Reasoning (PTR) dataset. PTR contains ﬁve question types: concept, relation, analogy, arithmetic and physics. Top left shows scenes from our dataset, paired with hierarchical scene graphs in the middle. Top right presents questions, answers and hints to the answers. A functional program of a question is shown in the bottom. brings two unique challenges. The ﬁrst is how to discriminate objects using part-level attributes.
Unlike previous works which refer to objects based on holistic attributes such as object category, the speciﬁc ontology nature of objects and their parts requires a detailed study. The second is how to leverage part-level attributes to interconnect objects. Objects of various categories are interrelated via some common parts (e.g., leg, central support, pedestal). The reasoning performed on one category should thus be easily generalized to reasoning on unseen categories with shared parts. Human beings are endowed with such modular but interconnected perceptual systems. They can effortlessly discriminate between the two tables in Figure 1 [I] based on the differences in the tops and drawers, as well as observe a connection between the bed and chair due to the similarity of legs. It remains to be explored whether machines have the same hierarchical perceptual capabilities.
Based on the highly composite part-whole hierarchies, visual reasoning tasks can be richer, more complex, as well as more challenging. First, the introduction of parts enriches the diversity of both visual perception and question understanding. Apart from basic object-level properties such as color and category, the visual scenes and natural language questions extend upon the properties of object parts, the composition of which makes the image-question pairs more distinctive. Second, the relations between parts go beyond simple spatial relations. Parts can often be approximated as oriented geometric primitives (e.g., line, plane), and there naturally exist abundant geometric relations between these primitives such as “perpendicular" and “parallel" (e.g., Q2 in Figure 1[II]). Analogical reasoning can also be established given such relationships. Third, reasoning on parts enables the understanding of implicit properties of objects, such as physics. For example, the arrangements of the parts and their geometric relations affect the stability of the objects (e.g., Q3, Q4 in Figure 1[I]).
In this paper, we present a large-scale ParT Reasoning dataset, or PTR for short, a benchmark for part-based conceptual, relational and physical reasoning. It includes ∼ 70k scenes paired with 700k questions containing part-whole structures. We include over 10k objects from the PartNet [45] dataset across ﬁve object categories (chair, table, bed, refrigerator, cart) with rich geometric and structural variations to construct our scenes. Our dataset consists of ﬁve types of questions: concept, geometry, analogy, arithmetic, and physics. We provide scene graph annotations including the ground-truth locations, segmentations, and properties for all objects and parts, as well as all the object-object 2
Dataset
VQA [3]
VCR [61]
GQA [28]
CLEVR [29]
RAVEN [62]
PTR (ours) 3D Objects (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51)
Parts Relation Diagnostic Geometry Analogy (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51)
Physics (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51)
Table 1: Comparison between PTR and other visual reasoning benchmarks. and part-part relationships for model diagnostic. We also provide functional programs paired with questions.
We analyze a suite of state-of-the-art visual reasoning models on the PTR dataset and ﬁnd that they all struggle with it, especially in relational, analogical, and physical reasoning. One oracle neural symbolic model performs better than other models but highly relies on additional supervision such as object and part masks and visual attributes from simulations. Also, the performances of all the models are inferior to human performance by a large margin, suggesting that there’s still a long way to go to equip machines with human-like hierarchical perceptual and reasoning abilities. 2