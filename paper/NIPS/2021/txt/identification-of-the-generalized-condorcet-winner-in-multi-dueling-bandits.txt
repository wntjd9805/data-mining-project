Abstract
The reliable identiﬁcation of the “best” arm while keeping the sample complexity as low as possible is a common task in the ﬁeld of multi-armed bandits. In the multi-dueling variant of multi-armed bandits, where feedback is provided in the form of a winning arm among a set of k chosen ones, a reasonable notion of best arm is the generalized Condorcet winner (GCW). The latter is an arm that has the greatest probability of being the winner in each subset containing it. In this paper, we derive lower bounds on the sample complexity for the task of identifying the GCW under various assumptions. As a by-product, our lower bound results provide new insights for the special case of dueling bandits (k = 2). We propose the Dvoretzky–Kiefer–Wolfowitz tournament (DKWT) algorithm, which we prove to be nearly optimal. In a numerical study, we show that DKWT empirically outperforms current state-of-the-art algorithms, even in the special case of dueling bandits or under a Plackett-Luce assumption on the feedback mechanism. 1

Introduction
The standard multi-armed bandit (MAB) problem describes a sequential decision scenario, in which one of ﬁnitely many choice alternatives must be selected in each time step, resulting in the observation of a numerical reward of stochastic nature. One important and extensively studied variant of the
MAB setting is the dueling bandits problem, where a duel consisting of two arms is chosen in each time step and one of the duelling arms is observed as the winner [4]. Recently, the multi-dueling bandits setting has been introduced [7, 40, 31] as a generalization with multiple practically relevant applications, such as algorithm conﬁguration [13] or online retrieval evaluation [36]. Instead of pairs of arms, in this generalization a set consisting of k ≥ 2 arms can be chosen in each time step. These arms compete against each other and determine a single winner, which is observed as feedback by the learner. The outcomes of the (multi-)duels in the (multi-)dueling bandit scenario are typically assumed to be of time-stationary stochastic nature in the sense that whenever arms a1, . . . , ak compete against each other, then ai wins with some underlying (unknown) ground-truth probability P(ai|{a1, . . . , ak}).
One often targeted learning task in the context of multi-armed bandits and its variants is the problem of identifying the best among all arms. While for standard MABs, the canonical deﬁnition of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
“best arm” is the arm with highest expected reward, the picture is less clear for its variants. In the realm of dueling bandits, any arm that is likely to win (i.e., with probability > 1/2) in each duel against another arm is called the Condorcet winner (CW). This notion dates back to the 18th century
[8] and also appears in the social choice literature [16, 17], where the data is typically assumed to be available in the form of a list containing total rankings over all alternatives from different voters. In practice, the Condorcet winner does not necessarily exist due to the presence of preferential cycles in the probabilistic model in the sense that ai is likely to win against aj, aj against ak, and ak against ai. For the theoretical analysis of the best-arm-identiﬁcation problem, this issue is overcome in the literature either by the consideration of alternative optimality concepts such as Borda winner or
Copeland winner, which are guaranteed to exist, or by simply assuming the existence of the CW.
In this paper, we focus on ﬁnding a generalized variant of the CW in the multi-dueling bandits setting under the assumption that it exists. There have been several suggestions for generalizations of the
CW in social choice. For example, a weighted variant is introduced in [30], where the weights control the relevance given to the ranking positions of the alternatives, while in [25] the notion of a k-winner is deﬁned as an alternative that (in some appropriate sense) outperforms all other arms among any k alternatives. In contrast to our work, these papers focus on ofﬂine learning tasks and suppose full rankings over all alternatives to be given. In this paper, we adapt the notion of generalized Condorcet winner (GCW) as in [1], i.e., a GCW is an arm ai that outperforms each arm aj in every query set S containing both ai and aj, in the sense that P(ai|S) ≥ P(aj|S).
Regarding the dueling bandits setting as the multi-dueling setting where the allowed multi-duels S are exactly those with |S| = 2, the GCW is indeed a generalization of the Condorcet winner. We analyze the sample complexity of (probabilistic) algorithms that are able to identify the GCW with high probability under the assumption of mere existence as well as more restrictive assumptions.
We provide upper and lower bounds for this task, which depend on the desired conﬁdence, the total number m of alternatives, the size k of allowed query sets as well as the underlying unknown preference probabilities P(ai|S).
We start in Section 2 with a brief literature overview on the multi-dueling bandits scenario. Section 3 introduces the basic formalism and a precise deﬁnition of the considered GCW identiﬁcation problem.
It also gives a rough, simpliﬁed overview of the sample complexity bounds obtained in this paper. In
Section 4, we discuss the special case m = k, in which the GCW identiﬁcation problem essentially boils down to the task of ﬁnding the mode of a categorical distribution. We provide solutions to this problem and prove their sample complexity to be optimal up to logarithmic factors in the worst-case sense. Section 5 focuses on lower bounds for the general case m ≥ k, and in Section 6, we discuss several upper bounds. In Section 7, we empirically compare the algorithms discussed before, prior to concluding in Section 8. For the sake of convenience, detailed proofs of all theoretical results presented in the paper are deferred to the supplemental material. 2