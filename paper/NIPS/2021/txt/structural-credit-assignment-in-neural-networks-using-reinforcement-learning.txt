Abstract
Structural credit assignment in neural networks is a long-standing problem, with a variety of alternatives to backpropagation proposed to allow for local training of nodes. One of the early strategies was to treat each node as an agent and use a reinforcement learning method called REINFORCE to update each node locally with only a global reward signal. In this work, we revisit this approach and investigate if we can leverage other reinforcement learning approaches to improve learning. We ﬁrst formalize training a neural network as a ﬁnite-horizon reinforcement learning problem and discuss how this facilitates using ideas from reinforcement learning like off-policy learning. We show that the standard on-policy
REINFORCE approach, even with a variety of variance reduction approaches, learns suboptimal solutions. We introduce an off-policy approach, to facilitate reasoning about the greedy action for other agents and help overcome stochasticity in other agents. We conclude by showing that these networks of agents can be more robust to correlated samples when learning online. 1

Introduction
Training neural networks involves structural credit assignment: attributing credit (or blame) to nodes in the network for correct (or incorrect) predictions. The output from a node early in the network impacts all the outputs downstream and ﬁnally the prediction outputted at the end of the network.
Our goal is to adjust the weights that produced the output for this node, so that the prediction would have been more accurate. The most widely used solution for the structural credit assignment problem is backpropagation [44], namely gradient descent on the loss for the outputs.
Moving beyond backprop provides more ﬂexibility in training neural networks. Backprop requires differentiability of activations and losses for the network, as well as synchronicity for computing the gradient and updating the weights. To update a node internal to the network, a full feedforward and backward pass needs to be computed, with global gradient information sweeping backwards from the output. Ideally, for online agents operating real-time, with computational constraints, we would have nodes that update each step, locally and asynchronously.
To make progress towards this lofty goal, we revisit an old idea: treating each node as an agent. Work in reinforcement learning (RL), including ideas like eligibility traces, were in fact inspired by Klopf
[23] and the hedonistic neuron. It is not surprising that the idea of using an RL agent for each node is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
found in early work, including the original REINFORCE algorithm [61], which is a policy gradient approach using sampled returns. Most work treating each node as an agent uses the REINFORCE update, often with baselines for variance reduction, including work on learning with spiking neurons
[14] and CoAgent Networks (CoANs) [56; 55; 24]. The work in CoANs 1) nicely formalizes the idea of a collection of agents—each agent corresponding to a node or subset of nodes—cooperating to maximize return and 2) provides a general theorem on the validity of using the REINFORCE update.
For this reason, we adopt their terminology and use CoANs to refer to networks composed of agents.
More recently, other algorithmic ideas from reinforcement learning, beyond REINFORCE, have begun to affect training of (stochastic) neural networks. The ideas of critics and baselines, which reduce the variance of policy gradient updates, have been well-developed for stochastic computation graphs [60]. This work provides a uniﬁcation of gradient derivations, but as yet not an investigation into practical algorithms for structural credit assignment in neural networks. Other work on learning under stochastic neurons has typically used REINFORCE as a basic method, and explored other heuristics to improve learning, such as straight-through estimators [8], rather than improved RL approaches. Other work on credit assignment is loosely inspired by the idea of bootstrapping in RL, including synthetic gradients [20; 26] and ﬁxed-point propagation [36].
Overall, however, the broader space of RL algorithms has not been leveraged to learn CoANs.
One reason for this omission could be that the structural credit assignment problem within the neural network has not been clearly deﬁned as an RL problem; rather, it was simply intuitive to use
REINFORCE approaches for each node. Even the theory from the original CoANs work focused on the return in the environment—since CoANs were used to solve a reinforcement learning problem— and did not explicitly formalize the structural credit assignment problem within the network. Another reason could be that many straightforward ideas are not effective, as we show in this work.
To facilitate the use of RL algorithms, we ﬁrst formalize the structural credit assignment problem as a
ﬁnite horizon RL problem. We show local policy gradient updates provide an unbiased estimate of the joint gradient for structural credit assignment, ensuring REINFORCE is a sound approach. We then discuss key ideas from RL—namely exploration and off-policy learning—that can be leveraged to improve learning in CoANs. We show that REINFORCE can train multi-layered networks, but faces issues with suboptimality due to coagents learning under nondeterminism of fellow coagents. We provide an in-depth study highlighting this problem and measuring the entropy of different parts of the network. This in-depth study motivates the difﬁculties in using the common on-policy approaches, and we discuss and show how off-policy learning is a more promising direction. Finally, we discuss the advantages of CoANs when moving away from the standard iid learning setting, showing it can perform better than backprop on a continual learning problem with a highly correlated dataset. 1.1 Other