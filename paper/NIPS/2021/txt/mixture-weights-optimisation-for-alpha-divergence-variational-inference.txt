Abstract
This paper focuses on α-divergence minimisation methods for Variational Inference.
We consider the case where the posterior density is approximated by a mixture model and we investigate algorithms optimising the mixture weights of this mixture model by α-divergence minimisation, without any information on the underlying distribution of its mixture components parameters. The Power Descent, defined for all α ̸= 1, is one such algorithm and we establish in our work the full proof of its convergence towards the optimal mixture weights when α < 1. Since the
α-divergence recovers the widely-used exclusive Kullback-Leibler when α → 1, we then extend the Power Descent to the case α = 1 and show that we obtain an Entropic Mirror Descent. This leads us to investigate the link between Power
Descent and Entropic Mirror Descent: first-order approximations allow us to introduce the Rényi Descent, a novel algorithm for which we prove an O(1/N ) convergence rate. Lastly, we compare numerically the behavior of the unbiased
Power Descent and of the biased Rényi Descent and we discuss the potential advantages of one algorithm over the other. 1

Introduction
Bayesian Inference involves being able to compute or sample from the posterior density. For many useful models, the posterior density can only be evaluated up to a normalisation constant and we must resort to approximation methods.
One major category of approximation methods is Variational Inference, a wide class of optimisation methods which introduce a simpler variational family Q and use it to approximate the posterior density (see for example Variational Bayes [1, 2] and Stochastic Variational Inference [3]). The crux of these methods consists in finding the best approximation of the posterior density among the family
Q in the sense of a certain measure of dissimilarity, most typically the exclusive Kullback-Leibler divergence.
However, the exclusive Kullback-Leibler divergence is known to have some undesirable properties, e.g. posterior variance underestimation, difficulty to capture multimodal posterior densities [4, 5, 6].
As a consequence, the α-divergence [7, 8] and Rényi’s α-divergence [9, 10] have gained a lot of attention recently and are presented as more general alternatives that permit to bypass the issues of the exclusive Kullback-Leibler when α < 1 [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22].
Noticeably, [19] introduced the (α, Γ)-descent, a general family of gradient-based algorithms that approximate the posterior density by a mixture model and that are able to optimise the mixture weights of this mixture model by α-divergence minimisation. The benefit of these types of algorithms is that (i) they expand the traditional parametric variational family to better capture complex (e.g.
∗Corresponding author: kamelia.daudel@stats.ox.ac.uk 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
multimodal) posterior densities (ii) they allow, in an Sequential Monte Carlo fashion [23], to select the mixture components according to their overall importance in the set of components parameters, without needing to know the distribution of the mixture components parameters. The (α, Γ)-descent step is then paired up with an Exploration step that acts on the mixture components parameters, so that a complete algorithm is obtained by alternating between these two steps [19].
The key algorithm of [19] is the Power Descent, which sets Γ(v) = [(α − 1)v + 1]η/(1−α) with α ̸= 1 and η > 0 in the (α, Γ)-descent. Indeed, when α < 1 and as the dimension increases, numerical experiments in [19] show that the Power Descent outperforms the Entropic Mirror Descent (a classical algorithm from the optimisation litterature corresponding to Γ(v) = e−ηv with η > 0).
Nonetheless, the global convergence of the Power Descent algorithm when α < 1, as stated in [19,
Theorem 4], assumes the existence of the limit and does not provide conditions that satisfy this assumption. Furthermore, even though the convergence towards the global optimum is derived, there is no convergence rate available for the Power Descent when α < 1. After recalling the basics of the
Power Descent algorithm in Section 2, we make the following contributions in this paper:
• In Section 3, we prove the full convergence of the Power Descent algorithm towards the optimal mixture weights when α < 1 (Theorem 2).
• Since the α-divergence becomes the traditional exclusive Kullback-Leibler when α → 1, we next investigate the extension of the Power Descent to the case α = 1 in Proposition 1 from Section 4 and we obtain that the Power Descent recovers an Entropic Mirror Descent performing exclusive
Kullback-Leibler minimisation.
• We further study the connections between Power Descent and Entropic Mirror Descent by con-sidering first-order approximations, in the hope of finding an algorithm close to the Power Descent and for which we can prove a convergence rate when α < 1. As a result, we are able to go beyond the (α, Γ)-descent framework from [19] by introducing an algorithm closely-related to the Power
Descent, that is proved to converge at an O(1/N ) rate when α < 1 as a consequence of Theorem 3 from Section 4. We call this algorithm the Rényi Descent due to the link we establish between the
Rényi Descent and Entropic Mirror Descent steps applied to the Variational Rényi bound [12].
• Finally, we run some numerical experiments in Section 5 to compare the behavior of the Power
Descent and of the Rényi Descent altogether, before discussing the potential benefits of one approach over the other. 2