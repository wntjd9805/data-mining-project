Abstract
Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task conﬁgurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspeciﬁed environment are automatically adapted during training to the agent’s capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized
Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both
PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments conﬁrm that our new method, PLR⊥, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR⊥ improves the performance of PAIRED, from which it inherited its theoretical framework. 1

Introduction
While deep reinforcement learning (RL) approaches have led to many successful applications in challenging domains like Atari [21], Go [35], Chess [36], Dota [4], and StarCraft [40] in recent years, deep RL agents still prove to be brittle, often failing to transfer to environments only slightly different from those encountered during training [44, 9]. To ensure learning of robust and well-generalizing policies, agents must train on sufﬁciently diverse and informative variations of environments (e.g. see
Section 3.1 of [8]). However, it is not always feasible to specify an appropriate training distribution or a generator thereof. Agents may therefore beneﬁt greatly from methods that automatically adapt the distribution over environment variations throughout training [10, 17]. Throughout this paper we will call a particular environment instance or conﬁguration (e.g. an arrangement of blocks, race tracks, or generally any of the environment’s constituent entities) a level.
Two recent works [10, 17] have sought to empirically demonstrate this need for a more targeted agent-adaptive mechanism for selecting levels on which to train RL agents, so to ensure efﬁcient learning and generalization to unseen levels—as well as to provide methods implementing such mechanisms.
The ﬁrst method, Protagonist Antagonist Induced Regret Environment Design (PAIRED) [10],
∗Equal contribution. Correspondence to msj@fb.com and michael_dennis@berkeley.edu . 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) DR (b) PAIRED (c) REPAIRED (d) PLR⊥ (e) Human
Figure 1: Randomly drawn samples of CarRacing tracks produced by different methods. (a) Domain
Randomization (DR) produces tracks of average complexity, with few sharp turns. (b) PAIRED often overexploits the difference in the students, leading to simple tracks that incidentally favor the antagonist. (c) REPAIRED mitigates this degeneracy, recovering track complexity. (d) PLR⊥ selects the most challenging randomly generated tracks, resulting in tracks that more closely resemble human-designed tracks, such as (e) the Nürburgring Grand Prix. introduces a self-supervised RL paradigm called Unsupervised Environment Design (UED). Here, an environment generator (a teacher) is co-evolved with a student policy that trains on levels actively proposed by the teacher, leading to a form of adaptive curriculum learning. The aim of this coevolution is for the teacher to gradually learn to generate environments that exemplify properties of those that might be encountered at deployment time, and for the student to simultaneously learn a good policy that enables zero-shot transfer to such environments. PAIRED’s speciﬁc adversarial approach to environment design ensures a useful robustness characterization of the ﬁnal student policy in the form of a minimax regret guarantee [31]—assuming that its underlying teacher-student multi-agent system arrives at a Nash equilibrium [NE, 24]. In contrast, the second method, Prioritized Level Replay (PLR) [17], embodies an alternative form of dynamic curriculum learning that does not assume control of level generation, but instead, the ability to selectively replay existing levels. PLR tracks levels previously proposed by a black-box environment generator, and for each, estimates the agent’s learning potential in that level, in terms of how useful it would be to gather new experience from that level again in the future. The PLR algorithm exploits these scores to adapt a schedule for revisiting or replaying levels to maximize learning potential. PLR has been shown to produce scalable and robust results, improving both sample complexity of agent training and the generalization of the learned policy in diverse environments. However, unlike PAIRED, PLR is motivated with heuristic arguments and lacks a useful theoretical characterization of its learning behavior.
In this paper, we argue that PLR is, in and of itself, an effective form of UED: Through curating even randomly generated levels, PLR can generate novel and complex levels for learning robust policies.
This insight leads to a natural class of UED methods which we call Dual Curriculum Design (DCD).
In DCD, a student policy is challenged by a team of two co-evolving teachers. One teacher actively generates new, challenging levels, while the other passively curates existing levels for replaying, by prioritizing those estimated to be most suitably challenging for the student. We show that PAIRED and PLR are distinct members of the DCD class of algorithms and prove in Section 3 that all DCD algorithms enjoy similar minimax regret guarantees to that of PAIRED.
We make use of this result to provide the ﬁrst theoretical characterization of PLR, which immediately suggests a simple yet highly counterintuitive adjustment to PLR: By only training on trajectories in replay levels, PLR becomes provably robust at NE. We call this resulting variant PLR⊥ (Section 4).
From this perspective, PLR effectively performs level design in a diametrically opposite manner to PAIRED—through prioritized selection rather than active generation. A second corollary to the provable robustness of DCD algorithms shows that PLR⊥ can be extended to make use of the
PAIRED teacher as a level generator while preserving the robustness guarantee of PAIRED, resulting in a method we call Replay-Enhanced PAIRED (REPAIRED) (Section 5). We hypothesize that in this arrangement, PLR⊥ plays a complementary role to PAIRED in robustifying student policies.
Our experiments in Section 6 investigate the learning dynamics of PLR⊥, REPAIRED, and their replay-free counterparts on a challenging maze domain and a novel continuous control UED setting based on the popular CarRacing environment [5]. In both of these highly distinct settings, our methods provide signiﬁcant improvements over PLR and PAIRED, producing agents that can perform out-of-distribution (OOD) generalization to a variety of human designed mazes and Formula 1 tracks.
In summary, we present the following contributions: (i) We establish a common framework, Dual
Curriculum Design, that encompasses PLR and PAIRED. This allows us to develop new theory, which provides the ﬁrst robustness guarantees for PLR at NE as well as for REPAIRED, which 2
augments PAIRED with a PLR-based replay mechanism. (ii) Crucially, our theory suggests a highly counterintuitive improvement to PLR: the convergence to NE should be assisted by training on less data when using PLR—namely by only taking gradient updates from data that originates from the
PLR buffer, using the samples from the environment distribution only for computing the prioritization of levels in the buffer. (iii) Our experiments in a maze domain and a novel car racing domain show that our methods signiﬁcantly outperform their replay-free counterparts in zero-shot generalization.
We open source our methods at https://github.com/facebookresearch/dcd. 2