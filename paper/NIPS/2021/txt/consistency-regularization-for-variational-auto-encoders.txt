Abstract
Variational auto-encoders (vaes) are a powerful approach to unsupervised learning.
They enable scalable approximate posterior inference in latent-variable models using variational inference (vi). A vae posits a variational family parameterized by a deep neural network—called an encoder—that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a vae has the undesirable property that it maps a given observation and a semantics-preserving transformation of it to diﬀerent latent representations.
This “inconsistency" of the encoder lowers the quality of the learned representations, especially for downstream tasks, and also negatively aﬀects generalization. In this paper, we propose a regularization method to enforce consistency in vaes. The idea is to minimize the Kullback-Leibler (kl) divergence between the variational distri-bution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation.
This regularization is applicable to any vae. In our experiments we apply it to four diﬀerent vae variants on several benchmark datasets and found it always improves the quality of the learned representations but also leads to better generalization.
In particular, when applied to the nouveau variational auto-encoder (nvae), our regularization method yields state-of-the-art performance on mnist, cifar-10, and celeba. We also applied our method to 3D data and found it learns representations of superior quality as measured by accuracy on a downstream classiﬁcation task.
Finally, we show our method can even outperform the triplet loss, an advanced and popular contrastive learning-based method for representation learning. 1 1

Introduction
Variational auto-encoders (vaes) have signiﬁcantly impacted research on unsupervised learning. They have been used in several areas, including density estimation (Kingma & Welling, 2013; Rezende et al., 2014), image generation (Gregor et al., 2015), text generation (Bowman et al., 2015; Fang et al., 2019), music generation (Roberts et al., 2018), topic modeling (Miao et al., 2016; Dieng et al., 2019), and recommendation systems (Liang et al., 2018). Vaes have also been used for diﬀerent representation learning problems such as semi-supervised learning (Kingma et al., 2014), anomaly detection (An & Cho, 2015; Zimmerer et al., 2018), language modeling Bowman et al. (2015), active learning (Sinha et al., 2019), continual learning (Achille et al., 2018), and motion prediction of agents (Walker et al., 2016). This widespread application of vae representations makes it critical that we focus on improving them. vaes extend deterministic auto-encoders to probabilistic generative modeling. The encoder of a vae parameterizes an approximate posterior distribution over latent variables of a generative model. The encoder is shared between all observations, which amortizes the cost of posterior inference. Once
ﬁtted, the encoder of a vae can be used to obtain low-dimensional representations of data, (e.g. for 1Code for this work can be found at https://github.com/sinhasam/CRVAE 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b) (c)
Figure 1: Illustration of the inconsistency problem in vaes and how cr-vaes address this problem.
The red dots correspond to the representations of few images from mnist. The blue dots correspond to the representations of the transformed images. The transformations used here are rotations, translations, and scaling; they are semantics-preserving. The arrows connect the representations of any two pairs of an image and its transformation. The shorter the arrow, the better. (a): The vae maps the two sets of images to diﬀerent areas in the latent space. (b): Even when trained with the original dataset augmented with the transformed images, the vae still maps the two sets of images to diﬀerent parts in the latent space. (c): The cr-vae maps an image and its transformation to nearby areas in the latent space. downstream tasks.) The quality of these representations is therefore very important to a successful application of vaes.
Researchers have looked at ways to improve the quality of the latent representations of vaes, often tackling the so-called latent variable collapse problem—in which the approximate posterior distribu-tion induced by the encoder collapses to the prior over the latent variables (Bowman et al., 2015; Kim et al., 2018; Dieng et al., 2018; He et al., 2019; Fu et al., 2019).
In this paper, we focus on a diﬀerent problem pertaining to the latent representations of vaes for image data. Indeed, the encoder of a ﬁtted vae tends to map an image and a semantics-preserving transformation of that image to diﬀerent parts in the latent space. This “inconsistency" of the encoder aﬀects the quality of the learned representations and generalization. We propose a method to enforce consistency in vaes. The idea is simple and consists in maximizing the likelihood of the images while minimizing the Kullback-Leibler (kl) divergence between the approximate posterior distribution induced by the encoder when conditioning on the image, on one hand, and its transformation, on the other hand. This regularization technique can be applied to any vae variant to improve the quality of the learned representations and boost generalization performance. We call a vae with this form of regularization, a consistency-regularized variational auto-encoder (cr-vae).
Figure 1 illustrates the inconsistency problem of vaes and how cr-vaes address this problem on mnist. The red dots are representations of a few images and the blue dots are the representations of their transformations. We applied semantics-preserving transformations: rotation, translation, and scaling. The vae maps each image and its transformation to diﬀerent parts in the latent space as evidenced by the long arrows connecting each pair (a). Even when we include the transformed images to the data and ﬁt the vae the inconsistency problem still occurs (b). The cr-vae does not suﬀer from the inconsistency problem; it maps each image and its transformation to nearby areas in the latent space, as evidenced by the short arrows connecting each pair (c).
In our experiments (see Section 4), we apply the proposed technique to four vae variants, the original vae (Kingma & Welling, 2013), the importance-weighted auto-encoder (iwae) (Burda et al., 2015), the β-vae (Higgins et al., 2017), and the nouveau variational auto-encoder (nvae) (Vahdat
& Kautz, 2020). We found, on four diﬀerent benchmark datasets, that cr-vaes always yield better representations and generalize better than their base vaes. In particular, consistency-regularized nouveau variational auto-encoders (cr-nvaes) yield state-of-the-art performance on mnist and cifar-10. We also applied cr-vaes to 3D data where these conclusions still hold. 2 Method
We consider a latent-variable model pθ(x, z) = pθ(x|z) · p(z), where x denotes an observation and z is its associated latent variable. The marginal p(z) is a prior over the latent variable and pθ(x|z) 2
is an exponential family distribution whose natural parameter is a function of z parameterized by θ, e.g. through a neural network. Our goal is to learn the parameters θ and a posterior distribution over the latent variables. The approach of vaes is to maximize the evidence lower bound (elbo), a lower bound on the log marginal likelihood of the data,
Lvae = elbo = Eqφ(z|x) (cid:20) log (cid:18) pθ(x, z) qφ(z|x) (cid:19)(cid:21) (1) where qφ(z|x) is an approximate posterior distribution over the latent variables. The idea of a vae is to let the parameters of the distribution qφ(z|x) be given by the output of a neural network, with parameters φ, that takes x as input. The parameters θ and φ are then jointly optimized by maximizing a Monte Carlo approximation of the elbo using the reparameterization trick (Kingma & Welling, 2013).
Consider a semantics-preserving transformation t(˜x|x) of data x (e.g. rotation or translation for images.) A good representation learning algorithm should provide similar latent representations for x and ˜x. This is not the case for the vae that maximizes Equation 1 and its variants. Once ﬁt to data, the encoder of a vae is unable to yield similar latent representations for a data x and its tranformation
˜x (see Figure 1). This is because there is nothing in Equation 1 that forces this desideratum.
We now propose a regularization method that ensures consistency of the encoder of a vae. We call a vae with such a regularization a cr-vae. The regularization proposed is applicable to many variants of the vae such as the iwae (Burda et al., 2015), the β-vae (Higgins et al., 2017), and the nvae (Vahdat & Kautz, 2020). In what follows, we use the standard vae, the one that maximizes
Equation 1, as the base vae to regularize to illustrate the method.
Consider an image x. Denote by t(˜x|x) the random process by which we generate ˜x, a semantics-preserving transformation of x. We draw ˜x from t(˜x|x) as follows:
˜x ∼ t(˜x|x) ⇐⇒ (cid:15) ∼ p((cid:15)) and ˜x = g(x, (cid:15)). (2)
Here g(x, (cid:15)) is a semantics-preserving transformation of the image x, e.g. translation with random length (cid:15) drawn from p((cid:15)) = U[−δ, δ] for some threshold δ. A cr-vae then maximizes
Lcr-vae(x) = Lvae(x) + Et(˜x|x) [Lvae(˜x)] − λ · R(x, φ) where the regularization term R(x, φ) is
R(x, φ) = Et(˜x|x) [kl (qφ(z|˜x)||qφ(z|x))] . (3) (4)
Maximizing the objective in Equation 3 maximizes the likelihood of the data and their augmentations while enforcing consistency through R(x, φ). Minimizing R(x, φ), which only aﬀects the encoder (with parameters φ), forces each observation and the corresponding augmentations to lie close to each other in the latent space. The hyperparameter λ ≥ 0 controls the strength of this constraint.
The objective in Equation 3 is intractable but we can easily approximate it using Monte Carlo with the reparameterization trick. In particular, we approximate the regularization term with one sample from t(˜x|x) and make the dependence to this sample explicit using the notation R(x, ˜x, φ).
Algorithm 1 illustrates this in greater detail. Although we show the application of consistency regularization using the vae that maximizes the elbo, Lvae(·) in Equation 3 can be replaced with any vae objective. 3