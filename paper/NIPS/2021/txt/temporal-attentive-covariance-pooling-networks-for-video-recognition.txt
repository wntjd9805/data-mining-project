Abstract
For video recognition task, a global representation summarizing the whole contents of the video snippets plays an important role for the final performance. However, existing video architectures usually generate it by using a simple, global average pooling (GAP) method, which has limited ability to capture complex dynamics of videos. For image recognition task, there exist evidences showing that covari-ance pooling has stronger representation ability than GAP. Unfortunately, such plain covariance pooling used in image recognition is an orderless representative, which cannot model spatio-temporal structure inherent in videos. Therefore, this paper proposes a Temporal-attentive Covariance Pooling (TCP), inserted at the end of deep architectures, to produce powerful video representations. Specifi-cally, our TCP first develops a temporal attention module to adaptively calibrate spatio-temporal features for the succeeding covariance pooling, approximatively producing attentive covariance representations. Then, a temporal covariance pool-ing performs temporal pooling of the attentive covariance representations to char-acterize both intra-frame correlations and inter-frame cross-correlations of the calibrated features. As such, the proposed TCP can capture complex temporal dynamics. Finally, a fast matrix power normalization is introduced to exploit geometry of covariance representations. Note that our TCP is model-agnostic and can be flexibly integrated into any video architectures, resulting in TCPNet for effective video recognition. The extensive experiments on six benchmarks (e.g.,
Kinetics, Something-Something V1 and Charades) using various video architec-tures show our TCPNet is clearly superior to its counterparts, while having strong generalization ability. The source code is publicly available. 1

Introduction
Video recognition aims to automatically analyze the contents of videos (e.g., events and actions), and has a wide range of applications, including intelligent surveillance, multimedia retrieval and recommendation. The great progress of deep learning, especially convolutional neural networks (CNNs) [48, 52, 22, 61], remarkably improves performance of video recognition. Since videos in the wild involve complex dynamics caused by appearance changes and variation of visual tempo, powerful video representations potentially provide high performance of video recognition. However, most of the existing video recognition architectures based on 3D CNNs [53, 43, 2, 11, 54, 10] or 2D
CNNs [35, 27, 33, 39] usually generate the final video representations by leveraging a global average pooling (GAP). Such GAP simply computes the first-order statistics of convolution features in a temporal orderless manner, which discards richer statistical information inherent in spatio-temporal features and has limited ability to capture complex dynamics of videos.
âˆ—Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of our TCPNet for video recognition. Its core is a Temporal-attentive Covariance
Pooling (TCP) integrated at the end of deep recognition architectures to produce powerful video rep-resentations. Specifically, our TCP consists of a temporal attention module for adaptively calibrating spatio-temporal features, a temporal covariance pooling to characterize intra-frame correlations and inter-frame cross-correlations of the calibrated features in a temporal manner, and a fast matrix power normalization for use of geometry of covariances. Refer to Section 3.2 for details.
There are a few researches studying global high-order pooling for improving performance of video recognition [45, 29, 7]. Sanin et al. [45] propose spatio-temporal covariance descriptors as video representations, where covariance descriptors are built with hand-crafted features of candidate regions and a subset of the optimal descriptors are learned by LogitBoost classifiers. However, such a method is designed based on the idea of the classical shallow architectures, without making the most of the merits of deep learning paradigm. In [29], two tensor-based feature representations (namely SCK and DCK) are introduced for action recognition, where multiple subsequences and
CNN classifier scores are used to compute tensor representations while a support vector machine is trained for classification (see Figures 2&4 and the third paragraph of Section 5.2). Under end-to-end deep architectures, Diba et al. [7] propose a temporal linear encoding (TLE) method for video classification. Specifically, TLE adopts a bilinear pooling [37] or compact bilinear pooling [15] to aggregate the features output by 2D and 3D CNNs as the entire video representations. TLE shows better performance than the original fully-connected (FC) layer, but it fails to fully exploit temporal information and geometry of covariance representations, limiting effectiveness of covariance pooling for video recognition. Recently, there also exist evidences showing that global covariance (second-order) pooling can significantly improve performance of deep CNNs in various image-related tasks, such as fine-grained visual recognition [37, 15], large-scale image classification [31, 30, 60] and visual question answering [13, 68]. However, these methods aim to generate image representations, without considering important temporal information inherent in videos. Meanwhile, naive usage of image-related covariance pooling (called plain covariance pooling in this work) technique for video-related tasks cannot make full use of the advantage of covariance pooling [63].
As discussed previously, though global covariance pooling shows the potential to improve recognition performance, aforementioned works suffer from the following drawbacks: (1) Shallow approach [45] fails to make the most of the merits of deep learning paradigm; (2) Plain covariance pooling meth-ods [37, 15, 31, 30, 60] as well as TLE [7] cannot adequately model complex temporal dynamics, and in the meanwhile TLE considers no geometry of covariances. To overcome these issues, this paper proposes a Temporal-attentive Covariance Pooling (TCP), which can be integrated into existing deep architectures to produce powerful video representations. Different from the plain covariance pooling methods which perform orderless aggregation along temporal dimension, our TCP presents a temporal pooling of attentive covariance representations to capture complex temporal dynamics. Specifically, as illustrated in Figure 1, our TCP consists of a temporal attention module, a temporal covariance pooling, and a fast matrix power normalization of covariances. The temporal attention module is developed to adaptively calibrate spatio-temporal features for the subsequent covariance pooling.
The covariance pooling of the calibrated features can be approximatively regarded as producing attentive covariance representations, having the ability to handle complex dynamics. Then, temporal pooling of the attentive covariance representations (i.e., temporal covariance pooling) characterizes both intra-frame correlations and inter-frame cross-correlations of the calibrated features, which can fully harness the complex temporal information. The fast matrix power normalization [30] is finally 2
introduced to exploit geometry of covariance spaces. Note that our TCP is model-agnostic and can be flexibly integrated into existing 2D or 3D CNNs in a plug-and-play manner, resulting in an effective video recognition architecture (namely TCPNet).
The overview of our TCPNet is shown in Figure 1. To verify its effectiveness, extensive experiments are conducted on six video benchmarks (i.e., Mini-Kinetics-200 [66], Kinetics-400 [2], Something-Something V1 [19], Charades [46], UCF101 [49] and HMDB51 [23]) using various deep architectures (e.g., TSN [59], X3D [10] and TEA [33]). The contributions of our work are summarized as follows. (1) We propose an effective TCPNet for video recognition, where a model-agnostic Temporal-attentive
Covariance Pooling (TCP) integrated at the end of existing deep architectures can produce powerful video representations. (2) To our best knowledge, our TCP makes the first attempt to characterize both intra-frame and inter-frame correlations of the calibrated spatio-temporal features in a temporal manner, which approximatively performs temporal pooling of the attentive covariance representations.
The proposed TCP provides a compelling alternative to GAP for improving recognition performance of deep video architectures. (3) We perform a comprehensive study of global covariance pooling for deep video recognition architectures, and conduct extensive experiments on six challenging video benchmarks. The results show our TCPNet has strong generalization ability while performing favorably against state-of-the-arts. 2