Abstract
Extracting the interaction rules of biological agents from movement sequences pose challenges in various domains. Granger causality is a practical framework for analyzing the interactions from observed time-series data; however, this frame-work ignores the structures and assumptions of the generative process in animal behaviors, which may lead to interpretational problems and sometimes erroneous assessments of causality. In this paper, we propose a new framework for learning
Granger causality from multi-animal trajectories via augmented theory-based be-havioral models with interpretable data-driven models. We adopt an approach for augmenting incomplete multi-agent behavioral models described by time-varying dynamical systems with neural networks. For efﬁcient and interpretable learning, our model leverages theory-based architectures separating navigation and motion processes, and the theory-guided regularization for reliable behavioral modeling.
This can provide interpretable signs of Granger-causal effects over time, i.e., when speciﬁc others cause the approach or separation. In experiments using synthetic datasets, our method achieved better performance than various baselines. We then analyzed multi-animal datasets of mice, ﬂies, birds, and bats, which veriﬁed our method and obtained novel biological insights. 1

Introduction
Extracting the interaction rules of real-world agents from data is a fundamental problem in a variety of scientiﬁc and engineering ﬁelds. For example, animals, vehicles, and pedestrians observe other’s states and execute their actions in complex situations. Discovering the directed interaction rules of such agents from observed data will contribute to the understanding of the principles of biological
∗fujii@i.nagoya-u.ac.jp 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
agents’ behaviors. Among methods analyzing directed interactions within multivariate time series,
Granger causality (GC) [23] is a practical framework for exploratory analysis [49] in various ﬁelds, such as neuroscience [65] and economics [3] (see Section 2). Recent methodological developments have focused on inferring GC under nonlinear dynamics (e.g., [73, 31, 81, 55, 43]).
However, the structure of the generative process in biological multi-agent trajectories, which include navigational and motion processes [54] regarded as time-varying dynamical systems (see Section 3.1), is not fully utilized in existing base models of GC including vector autoregressive [27] and recent neural models [73, 31, 81]. Ignoring the structures of such processes in animal behaviors will lead to interpretational problems and sometimes erroneous assessments of causality. That is, incorporating the structures into the base model for inferring GC, e.g., augmenting (inherently) incomplete behavioral models with interpretable data-driven models (see Section 3.2), can solve these problems. Furthermore, since data-driven models sometimes detect false causality that is counterintuitive to the user of the analysis, e.g., introducing architectures and regularization to utilize scientiﬁc knowledge (see Sections 3.2 and 4.2) will be effective for a reliable base model of a GC method.
In this paper, we propose a framework for learning GC from biological multi-agent trajectories via augmented behavioral models (ABM) using interpretable data-driven neural models. We adopt an approach for augmenting incomplete multi-agent behavioral models described by time-varying dynamical systems with neural networks (see Section 3.2). The ABM leverages theory-based archi-tectures separating navigation and motion processes based on a well-known conceptual behavioral model [54], and the theory-guided regularization (see Section 4.2) for interpretable and reliable behavioral modeling. This framework can provide interpretable signs of Granger-causal effects over time, e.g., when speciﬁc others cause the approach or separation.
The main contributions of this paper are as follows. (1) We propose a framework for learning
Granger causality via ABM, which can extract interaction rules from real-world multi-agent and multi-dimensional trajectory data. (2) Methodologically, we realized the theory-guided regularization for reliable biological behavioral modeling for the ﬁrst time. The theory-guided regularization can leverage scientiﬁc knowledge such that “when this situation occurs, it would be like this” (i.e., domain experts often know an input and output pair of the prediction model). Existing methods in Granger causality did not consider the utilization of such knowledge. (3) Biologically, our methodological contributions lies in the reformulation of a well-known conceptual behavioral model [54], which did not have a numerically computable form, such that we can compute and quantitatively evaluate it. (4)
In the experiments, our method achieved better performance than various baselines using synthetic datasets, and obtained new biological insights and veriﬁed our method using multiple datasets of mice, birds, bats, and ﬂies. In the remainder of this paper, we describe the background of GC in
Section 2. Next, we formulate our ABM in Section 3, and the learning and inference methods in
Section 4. 2 Granger Causality
GC [23] is one of the most popular and practical approaches to infer directed causal relations from observational multivariate time series data. Although the classical GC is deﬁned by linear models, here we introduce a more recent deﬁnition of [73] for non-linear GC. Consider p stationary time-series x = {x1, ...xp} across timesteps t = {1, ..., T } and a non-linear autoregressive function gj, such that (1) t−1, xj
≤t) + εj t ,
≤t = (..., xj xj t+1 = gj(x1
≤t, ..., xp where xj t ) denotes the present and past of series j and εj t represents independent noise. We then consider that variable xi does not Granger-cause variable xj, denoted as xi (cid:57) xj, if and only if gj(·) is constant in xi
≤t. Granger causal relations are equivalent to causal relations in the underlying directed acyclic graph if all relevant variables are observed and no instantaneous (i.e., connections between two variables at the same timestep) connections exist [59]. Many methods for Granger causal discovery, including vector autoregressive [27] and recent deep learning-based approaches [73, 31, 81], can be encapsulated by the following framework. First, we deﬁne a function fθ (e.g., an multilayer perceptrons (MLP) in [73], a linear model in [27]), which learns to predict the next time-step of the test sequence x. Then, we ﬁt fθ to x by minimizing some loss (e.g., mean squared error) L: θ(cid:63) = argminθ L(x, fθ). Finally, we apply some ﬁxed function h (e.g., thresholding) (e.g., [45]) to the learned parameters to produce a Granger causal graph estimate for x: ˆGx = h(θ(cid:63)). 2
Furthermore, we need to differentiate between positive and negative Granger-causal effects (e.g, approaching and separating). Based on the deﬁnition of [45], we deﬁne the effect sign as follows: if gj(·) is increasing in all xi
≤t, then we say that variable xi has a positive effect on xj , if gj(·) is decreasing in xi
≤t, then xi has a negative effect on xj. Note that xi can contribute both positively and negatively to the future of xj at different delays.
Overall, the causality measures, however elaborate in construction, are simply statistics estimated from a model [71]. If the model inadequately represents the system properties of interest, subsequent analyses based on the model will fail to address the question of interest. The inability of the model to represent key features of interest can cause interpretational problems and sometimes erroneous assessments of causality. Therefore, in our case, incorporating the structures of the generative process for animal behaviors (i.e., Eq.(2)) in a numerically computable form will be required. We thus propose the ABM based on a well-known conceptual model [54] in biological sciences in the next section. 3 Augmented behavioral model
Our motivation for developing interpretable behavior models is to obtain new insights from the results of Granger causality. In this section, we ﬁrstly formulate a well-known conceptual behavioral model [54] so that it can be computable. Second, we propose (multi-animal) ABMs with theory-based architectures based on scientiﬁc knowledge. Further, we discuss the relation to the existing explainable neural models [2]. The diagram of our method is described in Appendix C. 3.1 Formulation of a conceptual behavioral model
In movement ecology, which is a branch of biology concerning the spatial and temporal patterns of behaviors of organisms, a coherent framework [54] has been conceptualized to explore the causes, mechanisms, and patterns of movement. For example, two alternative structural representations
[54] were proposed to model a new position pt+1 from its current location pt (for details, see
Appendix A): the motion-driven case pt+1 = fU (fM (Ω, fN (Φ, rt, wt, pt), rt, wt, pt)) + εt, and the navigation-driven case pt+1 = fU (fN (Φ, fM (Ω, rt, wt, pt), rt, wt, pt)) + εt, where wt is the internal state, Ω is the motion capacity, Φ is the navigation capacity, and rt is the environmental factors (these are conceptual parameters). fM , fN , and fU are conceptual functions to represent actions of the motion (or planning), navigation, and movement progression processes, respectively.
For efﬁcient learning of the weights in the model (i.e., coefﬁcient of Granger causality) in this paper, we consider a simple case with homogeneous navigation and motion capacities, and internal states.
Moreover, to make the contribution of fM , fN , and fU interpretable after training from the data for extracting unknown interaction rules (and for obtaining scientiﬁc new insights), one of the simpliﬁed processes for agent i is represented by t+1 = f i xi
U (f i
N (ri t, xi t), f i
M (ri t, xi t), ri t, xi t) + εi t, (2) where xi ∈ Rd includes location pi and velocity for the agent i. We here consider ri ∈ R(p−1)dr including p − 1 other agents’ dr-dimensional information. This formulation does not assume either motion-driven or navigation-driven case. Such behaviors have been conventionally modeled by mathematical equations such as force- and rule-based models (e.g., reviewed by [77, 47]). Recently, these models have become more sophisticated by incorporating the models into hand-crafted functions representing anticipation (e.g., [29, 51]) and navigation (e.g., [8, 76]).
However, these conventional and recent models are sometimes too simplistic and customized for the speciﬁc animals, respectively; thus it is sometimes difﬁcult to deﬁne the dynamics of general biological multi-agent systems (i.e., multiple species of animals). Therefore, methods for learning parameters and interaction rules of behavioral models are needed. There have been some researches to estimate speciﬁc parameters (and their distributions) of the interpretable behavior models (e.g.,
[84, 85, 15]), and others to model the parameters and rules in purely data-driven manners (i.e., sometimes uninterpretable) only for accurate prediction (e.g., [16, 28]). In the proposed framework, we consider ﬂexible data-driven interpretable models to focus on inferring GC for exploratory analysis from the observed data without speciﬁc knowledge of the species and obtaining additional data.
Recently, some attempts have been made to explore ﬂexible and interpretable models bridging theory-based and data-driven approaches. For example, a paradigm called theory-guided data science has 3
been proposed [30], which leverages the wealth of scientiﬁc knowledge for improving the effective-ness of data-driven models in enabling scientiﬁc discovery. For example, scientiﬁc knowledge can be used as architectures or regularization terms in learning algorithms in physical and biological sciences (e.g., [61, 21]). In biological multi-agent systems, an approach extract interpretable dynamical information based on physics-based knowledge [18] from multi-agent interaction data, and another approach made a particular module such as observation (e.g., [19]) interpretable in mostly black-box neural models. However, these data-driven models did not sufﬁciently utilize the above scientiﬁc knowledge of multi-animal interactions. In the next subsection, to make the model (e.g., of GC)
ﬂexible and interpretable, we propose an ABM with theory-based architectures. 3.2 Augmented behavioral model with theory-based architectures
In this subsection, we propose a ABM using interpretable neural models with theory-based archi-tectures for learning GC from multi-animal trajectories. In general, it is scientiﬁcally beneﬁcial if a model mimics the data-generating process well, e.g., because existing scientiﬁc insights can be leveraged or revalidated. In our case of GC, additionally, it is expected to eliminate obvious erroneous causality by utilizing existing knowledge, we thus propose a theory-based ABM for learning GC.
Generally, scientiﬁc knowledge can be used to inﬂuence the architecture of data-driven scientiﬁc models. Most design considerations are mainly motivated to simplify the learning procedure, mini-mize the training loss, and ensure robust generalization performance [30]. In some cases, domain knowledge can be used designing neural models by decomposing the overall problem into modular sub-problems. For example, in our problem, to describe the overall process of multi-animal behaviors, modular neural models can be learned for different sub-processes, such as the navigation, planning, and movement processes (f i
U , respectively) described in Section 3.1. This will help in using the power of learning frameworks while following a high-level organization in the architecture that is motivated by domain knowledge [30]. Speciﬁcally, to accurately model the relationships between agents (ﬁnally interpreted as causal relationships) with limited information in usual GC settings, we explicitly formulate the f i
M , and f i
N , f i
U and estimate f i
M from data.
M , and f i
N and f i
N , f i
In summary, our base ABM can be expressed as xi t =
K (cid:88) (cid:16) k=1
F i,t,k
N (hi t−k) (cid:12) F i,t,k
M (hi t−k) (cid:17) t−k + εi hi t, (3) t−k ∈ Rd and all others’ state ri t−k ∈ Rdh is a vector concatenating the self state xi where hi t−k ∈
R(p−1)dr , and (cid:12) denotes a element-wise multiplication. K is the order of the autoregressive model.
F i,t,k
N , F i,t,k
M : Rdh → Rd×dh are matrix-valued functions that represent navigation and motion functions, which are implemented by MLPs. For brevity, we omit the intercept term here and in the following equations. The value of the element of F i,k
N is [−1, 1] is like a switching function value, i.e., a positive or negative sign to represent the approach and separation from others. The value of the element of F i,k
M is a positive value or zero, which changes continuously and represents coefﬁcients of time-varying dynamics. Relationships between agents x1, ..., xp and their variability throughout (cid:17) time can be examined by inspecting coefﬁcient matrices Ψi t−k)
.
N (hi
F i,t,k
N (hi
We separate Ψi t−k) for two reasons: interpretability and efﬁcient use of scientiﬁc knowledge. The interpretability of two coefﬁcients F i,k
N and F i,k
M contributes to the understanding of navigation and motion planning processes of animals (i.e., signs and amplitudes in the GC effects), respectively. The efﬁcient use of scientiﬁc knowledge in the learning of a model enables us to incorporate the knowledge into the model. The effectiveness was shown in the ablation studies in the experiments. Speciﬁc forms of Eq. (3) are described in Appendices E.2 and G.2. The formulation of the model via linear combinations of the interpretable feature hi t−k for an explainable neural model is related to the self-explanatory neural network (SENN) [2]. t−k) and F i,t,k t−k) (cid:12) F i,t,k into F i,t,k
M (hi
M (hi
θt,k
θt,k
= (cid:16) 3.3 Relation to self-explanatory neural network
SENN [2] was introduced as a class of intrinsically interpretable models motivated by explicitness, faithfulness, and stability properties. A SENN with a link function g(·) and interpretable basis concepts h(x) : Rp → Ru follows the form f (x) = g(θ(x)1h(x)1, ..., θ(x)uh(x)u), (4) 4
where x ∈ Rp are predictors; and θ(·) is a neural network with u outputs (here, we consider the simple case of d = 1 and dr = 1). We refer to θ(x) as generalized coefﬁcients for data point x and use them to explain contributions of individual basis concepts to predictions. In the case of g(·) being sum and concepts being raw inputs, Eq. (4) simpliﬁes to f (x) = (cid:80)p i=1 θ(x)ixi. In this paper, we regard the movement function f i
M as θ for the
U , f i following interpretable modeling of f i
M . Appendix B presents additional properties
SENNs need to satisfy and the learning algorithm, as deﬁned by [2]. Note that our model does not always satisfy the requirements of SENN [2, 45] due to the modeling of time-varying dynamics (see
Appendix B). SENN was ﬁrst applied to GC [45] via generalized vector autoregression model (GVAR): xt = (cid:80)K k=1 Ψθk (xt−k)xt−k + εt, where Ψθk : Rp → Rp×p is a neural network parameterized by
θk. Ψθk (xt−k) is a matrix whose components correspond to the generalized coefﬁcients for lag k at timestep t. The component (i, j) of Ψθk (xt−k) corresponds to the inﬂuence of xj t−k on xi t.
However, the SENN model did not use scientiﬁc knowledge of multi-element interactions and may cause interpretational problems and sometimes erroneous assessments of causality.
U as g(·) and the function of f i
N , and f i
N and f i 4 Learning with theory-guided regularization and inference
Here, we describe the learning method of the ABM including theory-guided regularization. We ﬁrst overview the learning method and deﬁne the objective function. We then explain the theory-guided regularization for incorporating scientiﬁc knowledge into the learning of the model. Finally, we describe the inference of GC by our method. Again, the overview of our method is described in
Appendix C. 4.1 Overview
To mitigate the inference in multivariate time series, Eq. (3) for each agent is summarized as the following expression: xt =
K (cid:88) (cid:104)(cid:16) k=1
N (h1
F 1,t,k t−k) (cid:12) F 1,t,k
M (h1 t−k) (cid:17) h1 t−k, . . . , (cid:16)
N (hp
F p,t,k t−k) (cid:12) F p,t,k
M (hp t−k) (cid:17) (cid:105) hp t−k
+ εt, (5) where xt and εt concatenate the original variables for all p agents (various F s and P sis are learned parameters). We train our model by minimizing the following penalized loss function with the mini-batch gradient descent
T (cid:88) t=K+1 (cid:16)
Lpred( ˆxt, xt) + λLsparsity(Ψt) + γLT G(Ψt, ΨT G t (cid:17)
)
+
T −1 (cid:88) t=K+1
βLsmooth(Ψt+1, Ψt), (6) t
θt,1
θt,K
θt,K
, . . . , Ψ1
, . . . , Ψp
], . . . , [Ψp
] in a row; ΨT G where {xt}T t=1 is a single observed time series of length T with d-dimensions and p-agents; ˆxt is the one-step forecast for the t-th time point based on Eq. (5); Ψt ∈ Rpd×Kdh is deﬁned as a concatenated matrix of [Ψ1 is a coefﬁcient determined by
θt,1 the following theory-guided regularization; and λ, β, γ ≥ 0 are regularization parameters. The loss function in Eq. (6) consists of four terms: (i) the mean squared error (MSE) prediction loss, (ii) a sparsity-inducing penalty term, (iii) theory-guided regularization, and (iv) the smoothing penalty term. The sparsity-inducing term Lsparsity is an appropriate penalty on the norm of Ψt. Among possible various regularization terms, in our implementation, we employ the elastic-net-style penalty term [86, 56] Lsparsity(Ψt) = 1
, with α = 0.5, based on [45].
Note that other penalties can be also easily adapted to our model. The smoothing penalty term, given by Lsmooth(Ψt+1, Ψt) =
F , is the average norm of the difference between generalized coefﬁcient matrices for two consecutive time points. This penalty term encourages smoothness in the evolution of coefﬁcients with respect to time [45]. To avoid overﬁtting and model selection problems, we eliminate unused factors based on prior knowledge (for details, see
Appendices E.2 and G.2).
α (cid:107)Ψt(cid:107)1 + (1 − α) (cid:107)Ψt(cid:107)2
T −K−1 (cid:107)Ψt+1 − Ψt(cid:107)2
T −K (cid:17) (cid:16)
F 1 4.2 Theory-guided regularization
The third term in Eq. (6) is the theory-guided regularization for reliable Granger causal discovery by leveraging regularization with scientiﬁc knowledge. Here we utilize theory-based and data-driven prediction results and impose penalties in the appropriate situations as described below. Again, let ˆxt be the prediction from the data.
In addition to the data, we prepare some input-output 5
pairs ( ˜xt−k≤t, ˜xt) based on scientiﬁc knowledge. We call them pairs of theory-guided feature and prediction, respectively. In this case, we assume that the theory-guided cause or weight of the ABM
ΨT G is uniquely determined. When the difference between ˆxt and ˜xt is below a certain threshold, t we assume that the cause (weight) of ˆxt is equivalent to the cause of ˜xt.
In animal behaviors, the theory-guided prediction utilizes the intuitive prior knowledge such that the agents go straight from the current state if there is no interaction. In this case, ˜xt includes the same velocity as the previous step and the corresponding positions after going straight. The penalty is t ∈ Rpd×K(p−1)dr is the expressed as LT G(Ψt, ΨT G weight matrix regarding others’ information (i.e., eliminating the information of the agents themselves from Ψt) and σ is a parameter regarding the threshold. Note that here the matrix Ψ corresponding t(cid:107)2
F = (cid:107)Ψ(cid:48) (cid:107)2 is a zero matrix representing no interaction with others (i.e., (cid:107)Ψ(cid:48) to ΨT G t
T −K exp((cid:107)xt − ˜xt(cid:107)2 (cid:48)T G t (cid:48)T G t
F , where Ψ(cid:48) 2/σ)(cid:107)Ψ(cid:48)
) = 1 t − Ψ t(cid:107)2
F ). t
Next, we can consider the general cases. All possible combinations of the pairs are denoted as the direct product H0 := L × M × · · · × M = {(l, m1, . . . , mp) | l ∈ L ∧ m1 ∈
M ∧ · · · ∧ mp ∈ M }, where L = {1, . . . , p} and M = {−1, 0, 1} if we consider the sign of Granger causal effects (otherwise, M = {0, 1}). However, if we consider the pairs ( ˜xt−k≤t, ˜xt) uniquely determined, it will be a considerably fewer number of combina-tions by avoiding underdetermined problems. We denote the set of the uniquely-determined combinations as H1 ⊂ H0. We can then impose penalties on the weights: LT G(Ψt, ΨT G
) = (cid:16) exp((cid:107)xt − ˜xt(cid:107)2 (cid:80) l,m1,...,mp∈H1 1
F /σ)(cid:107)Ψ(cid:48)
∈
|H1|(T −K)
Rpd×K(p−1)dr is the weight matrix regarding others’ information in Ψt. In animal behaviors, due to unknown terms, such as inertia and other biological factors, the theory-guided prediction utilizes the only intuitive prior knowledge such that the agents go straight from the current state if there are no interactions (i.e., |H1| = 1).
, where Ψ t − Ψ (cid:48)T G t,l,m1,...,mp (cid:48)T G t,l,m1,...,mp (cid:107)2
F (cid:17) t 4.3
Inference of Granger causality
Once Ψt is trained, we quantify strengths of Granger-causal relationships between variables by aggregating matrices Ψt across all K, d, dr, t into summary statistics. Although most neural GC methods [73, 55, 31, 81, 45] did not provide an obvious way for handling multi-dimensional time series (i.e., d > 1), our main problems include two- or three-dimensional positional and velocity data for each animal. Therefore, we compute the norm with respect to spatial dimensions d, dr, and the sign of the GC separately. That is, we aggregate the obtained generalized coefﬁcients into matrix
S ∈ Rp×p as follows:
Si,j = signmax
K+1≤t≤T



 signmax 1≤k≤K
 median q=1,...,dr u=1,...,d (Ψi,j)




 (cid:18) max
K+1≤t≤T max 1≤k≤K (cid:16) (cid:107) (Ψi,j)t,k (cid:107)F (cid:17)(cid:19)
, (7) where Ψi,j ∈ R(T −K)×K×d×dr is computed by reshaping and concatenating Ψt over K + 1 ≤ t ≤
T . (cid:107) (Ψi,j)t,k (cid:107)F is the Frobenius norm of the matrix (Ψi,j)t,k ∈ Rd×dr for each t, k. The signmax is an original function to output the sign of the larger value of the absolute value of the maximum and minimum values (e.g., signmax({1, 2, −3}) = −1). If we do not consider the sign of Granger causal effects, we ignore the coefﬁcient of the signed function. If we investigate the GC effects over time, we eliminate max function among t. Note that we only consider off-diagonal elements of adjacency matrices and ignore self-causal relationships. Intuitively, Si,j are statistics that quantify the strength of the Granger-causal effect of xi on xj using magnitudes of generalized coefﬁcients. We expect
Si,j to be close to 0 for non-causal relationships and Si,j (cid:29) 0 if xi → xj. Note that in practice Si,j is not binary-valued, as opposed to the ground truth, which we want to infer, because the outputs of
Ψi,j are not shrunk to exact zeros. Therefore, we need a procedure deciding for which variable pairs
Si,j are signiﬁcantly different from 0. To infer a binary matrix of GC relationships, we use a heuristic threshold. For the detail, see Appendix D.1. 5