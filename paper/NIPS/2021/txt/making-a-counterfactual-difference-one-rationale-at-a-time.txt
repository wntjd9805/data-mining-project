Abstract
Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classiﬁer with the goal of maximizing the mutual information (MMI) between the "selected" text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors.
In this work, we investigate whether counterfactual data augmentation (CDA), without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effec-tiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema [19] on two multi-aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest. 1

Introduction
Research in neural model interpretability has been cast as important and received signiﬁcant recent attention [21]. Within the ﬁeld of natural language processing (NLP), rationales have been a popular method for providing interpretability in the form of extracted subsets of text [10]. Rationale models typically consist of two cooperating modules where one module, the "rationale selector", selects the rationale from a source document, and the other module, the "classiﬁer", acts on only the selected rationale without seeing the rest of the document. There is interpretability through sparsity and exclusivity.
A common approach for training these rationale models is based on the maximum mutual information criteria (MMI) [7]. With the MMI criteria, rationale selectors seek the subset of text that carries the most information about the target label. Often, sparsity and coherency constraints are used to keep the rationales interpretable. Within many datasets, however, spurious patterns and co-varying aspects can cause the rationale selector to pick up on patterns that do not capture a desired relationship between input text and target labels. As a result, the rationalized model can have undesirable behaviour like predicting a hotel is very clean because it is in a convenient location. Nonsensical rationales or 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
explanations might decrease trust in the model, and in some cases, suggest the model might generalize poorly [26].
In this work, we propose a general counterfactual data augmentation (CDA) [22] approach to aid rationale models trained with MMI. We show that theoretically our CDA approach can effectively improve the performance of rationale selectors by lowering the mutual information between spurious signals and aspects of interest. Empirically, we show that models trained on our CDA datasets learn higher quality rationales than those trained on the original dataset when both use the same MMI criteria. More importantly, the most signiﬁcant advantage of our CDA approach is that it does not require human intervention. We use rationales from an initial, noisy model and replace them with new text that changes the target label using a generative neural model. In this way, our CDA approach is completely hands off and does not need input from human experts or crowd workers.
We ﬁrst show that in the extremely ideal scenario where the initial rationale selector is perfect, our
CDA approach can eliminate the mutual information between spurious signals and the target label.
Next, we show in the common, realistic scenario where the rationale selector is noisy and imperfect, our CDA approach can still yield gains. Finally, the effectiveness of our CDA approach is compared against several baselines including an improved MMI-based rationale schema [19] on two common multi-aspect review datasets, TripAdvisor [29] and RateBeer [24]. Multi-aspect datasets are our main focus as they are guaranteed to contain spurious patterns and co-varying aspects; we primarily used MMI-based baselines because the fundamental goal our CDA approach is to reduce the mutual information between spurious signals and aspects of interest. 2 Counterfactual Data Augmentation and Multi-Aspect Datasets 2.1 Deﬁnitions and Notations
We use upper-case letters to denote random variables, X and Y , and lower-case letters to denote samples from these variables, x and y. I(X, Y ) marks the mutual information between X and Y .
Mutual information is deﬁned as the reduction in uncertainty of a random variable due to knowledge in another random variable, I(X, Y ) = H(Y ) − H(Y |X), where H(X) and H(Y |X) are the
Shannon entropy and conditional entropy respectively [8]. 2.2 Problem Formulation
This work follows the same rationale concept introduced in [19]. Speciﬁcally, one neural module extracts text from a document and another neural module classiﬁes the extracted text. Later it has been shown that the rationalization criteria aims "to maximize the mutual information between selected features and the response variable" [7] deﬁned as: max
G
I(XM ; Y ) subject to M ∼ G(X) (1) where M denotes a binary rationale mask over the input produced by a rationale selector G. That is, the goal of the rationale selector is to select the subset of features in X that are most informative of the label Y under some constraints deﬁned by the selector G. In this work, our selector constraints affect the size and coherency of the rationales.
Here we consider a multi-aspect dataset, D with features X and labels Y: X is a set of features, or a sequence of words in NLP, and Y is a vector, possibly one dimensional, of numerical scores. In a multi-aspect dataset, a single document can discuss multiple attributes of a single object. For example, a single beverage review might discuss its appearance, taste, and smell. We assume some subset of the features, X1, belong to the target aspect label, Y1, while other features, X2, are spurious or non-causal. These features could belong to other aspects or be artifacts of the dataset. In the following, for simplicity, we will use <X, Y1> or <X1, X2, Y1> interchangeably based on the context.
Our goal is to estimate or model the score for aspect Y1 by using the function f given only X1, as follows: Y1 = f (X1). There is one such model corresponding to each < X1, Y1 > pair. In this work, we use multi-aspect datasets, and model each aspect individually. This simulates the more common case where a dataset has a single output label of interest and all signals in the dataset that do not pertain to that label are considered spurious or belonging to other, not-estimated aspects. 2
When predicting Y1, our ideal model would focus on X1 and ignore all other features, X2. Speciﬁcally, for a given sample <x, y>, our selector would select a subset of x such that x1 = xM . A rationale selector might fail to extract x1 effectively for two primary reasons: ﬁrst, Chen et al. [7] showed it is intractable to ﬁnd a solution to Eqn 1 and thus they derive a variational approximation with Monte
Carlo based gradient estimates. Second, datasets can contain artifacts such that spurious patterns might contain signiﬁcant mutual information with the target label. Along the same lines but even more concerning, other aspects in a dataset might be highly correlated with the target label. For example, beers that smell good usually taste good as well.
Since the goal of the selector is to select features that maximize I(XM , Y1), we can assist the selector in ﬁnding X1 by lowering the mutual information between the other spurious features and the desired label, I(X2, Y1). Drawing on ideas from [22], we do this through a general counterfactual data augmentation (CDA) scheme where, in the counterfactual dataset, superscript c, we ﬂip the label of the document from Y1 to Y c 1 and replace the text selected by the rationale selector, X1, with an inference, X c 1 and X2 described as: 1, generated by a class conditioned masked language model (MLM) using Y c 1 ← 1 − Y1; X c
Y c 1 ← arg max p(X1|1 − Y1, X2) (2)
X1
In our generated counterfactual dataset < X c 1 >, X c 1, X2, Y c 1 is the newly generated counterfactual,
X2 is the original spurious feature set, and we assign Y c = 1 − Y1. We will show, in the augmented dataset which is the concatenation of datasets < X1, X2, Y1 > and < X c 1, X2, 1 − Y1 >, we are lowering the mutual information I(X2, Y1). We will use superscript a for the augmented dataset: Da
= <X a 1 ,X a 2 , Y a 1 >. 2.3 Lowering I(X2, Y1), an idyllic case
Take a dataset of beer reviews where each document contains a description of the taste and smell of a beer as well as a numerical score for only the smell aspect. The task is to estimate the smell score while using the smell text as the rationale. Figure 1 demon-strates our process. An example of a concise document in this dataset might be This beer smells great. It tastes terriﬁc.
In this document x1 is the phrase smells great and x2 is tastes terriﬁc. Both have positive sen-timent but our only label for this docu-ment is y1 = 1 for the smell sentiment.
Our counterfactual document could be
This beer smells awful. It tastes ter-riﬁc and our label becomes y1 = 0. We can see that in the augmented dataset the phrase tastes terriﬁc maps to both positive and negative labels and therefore p(Y1|X2) = p(Y1). Finally, I(X2, Y1) is 0 and I(X1, Y1) is unchanged.
With perfect knowledge of the ground truth rationales and the process X c 1 ← p(X1|1 − Y1, X2), we can craft counterfactual documents and therefore a counterfactual dataset that perfectly eliminates
I(X2, Y1) while preserving I(X1, Y1). However, the challenge is that ground truth rationales are not provided in the training data. Therefore, it is important for us to show that even when rationales selected by the initial rationale selector are noisy and imperfect, we can still lower I(X2, Y1) in the augmented dataset and beneﬁt subsequent models trained with MMI.
Figure 1: Toy example to demonstrate our approach. In the augmented dataset Da, the mutual information, I, between the smell score and the smell text is preserved while the mutual information between the smell score and the taste text is eliminated. 2.4 Dealing with a Noisy Initial Selector 2 , Y a
Here we are working on the augmented dataset: Da = <X a
I(X a improving a poor rationale selector, so here we track what happens to both I(X a 2 , Y a when the rationale selector is not perfect. Our new goal is to reduce I(X a 2 , Y a>. To completely eliminate 1 ), our CDA approach requires a perfect rationale selector. We were originally motivated by 1 ) and I(X a 1 , Y a 1 ) 1 ) more than we reduce 1 , X a 2 , Y a 3
1 , Y a
I(X a on our algorithm’s beneﬁts under some assumptions. 1 ). We analyze the procedure in the worst case scenario in order to determine a lower bound
In an extremely erroneous case, we say that for a given
<x1, x2, y1>, our initial rationale selector mistakenly selects x2 when aiming for x1. When creating the corresponding counterfactual document, we still have yc 1 = 1 − y1, but we modify the document according to xc 2 ← arg maxX2 p(x2|1 − y1, x1) instead of the process deﬁned by Eqn 2. Going back to our concise example, this would be the counterfactual document This beer tastes good. It also smells bad. In this extremely erroneous case, we have decreased I(X1, Y1) while I(X2, Y1) re-mains unchanged. Thus, we deﬁne the worst case scenario as reducing I(X1, Y1) at some error rate α and keeping
I(X2, Y1) constant at the same rate.
If we say that this error happens to all samples with a rate
α, we can analyze conditions that must be present in the original dataset so that our CDA approach is beneﬁcial.
Let’s ﬁrst deﬁne ∆I a as the change in mutual information from the original to the augmented dataset.
∆I a
Xi,Yj
= I(Xi, Yj) − I(X a i , Y a j ) (3)
Figure 2: CDA beneﬁts (y-axis) as func-tion of error rate α. Each line represents p(Y1|X2) = p(Y1|X1) ± c.
In order for our CDA approach to be beneﬁcial, we need to decrease I(X2, Y1) more than I(X1, Y1).
That is:
∆I a
X2,Y1
− ∆I a
X1,Y1
> 0 (4)
For samples in the original dataset where the initial selector results in an error, X1 would map to 1 − Y1 for the corresponding counterfactuals. Furthermore, the augmented dataset would map X1 to both Y1 and 1 − Y1 for these original erroneous samples and corresponding counterfactuals, and thus, X1 is no longer informative of Y1 on such samples. That is, in the augmented dataset and with proportion α, p(Y a 1 ). For the remaining 1 − α portion of non-error samples,
CDA can successfully capture p(Y1|X1) in the original dataset and p(Y c 1) in the counterfactual dataset. To quantify the conditional distributions of the augmented dataset, we make two assumptions:
ﬁrst, we assume that we can successfully generate the counterfactuals, and thus, we can assume p(Y c 1) = p(Y1|X1); second, we assume that the erroneous samples happen randomly across the original dataset, and the marginal distributions of the original and augmented datasets are the same.
Based on these two assumptions, we have: 1 ) = p(Y a 1 |X a 1 |X c 1 |X c
For X2, the success and failure cases are reversed. p(Y a 1 |X a 1 ) = αp(Y1) + (1 − α)p(Y1|X1) p(Y a 1 |X a 2 ) = (1 − α)p(Y1) + αp(Y1|X2)
We can now expand Eqn 4 using the deﬁnition I(X, Y ) = H(Y ) − H(Y |X). 1 |X a 1 )
Using the deﬁnition, H(Y |X) = −E log p(Y |X), we can expand this further to 2 ) + H(Y1|X1) − H(Y a 0 < −H(Y1|X2) + H(Y a 1 |X a 0 < −E log p(Y1|X1) + E log p(Y a 1 |X a 1 ) + E log p(Y1|X2) − E log p(Y a 1 |X a 2 ) (5) (6) (7) (8)
Eqn 8 describes the conditions that must be met in our original dataset in order to yield gains from our CDA procedure for some error rate α. It is impossible to calculate this relation directly because it will require exact knowledge of our ground-truth rationales. In order to shed some light on this,
Figure 2 shows the efﬁcacy of the CDA approach when we approximate X1 and X2 with binary variables, p(Y1|X1) = 3 2 . When X1 and X2 are equally informative of Y1, the beneﬁts of CDA decrease linearly with the error rate, and intuitively, our error rate must be less than 50% to see gains. When X2 is more informative than X1, we have a higher error budget to see any beneﬁt, and when X1 is more informative than X2, our error budget is smaller. According to this analysis, we have a higher error budget when spurious signals offer the same or more information about the target label. When the spurious signals carry much less information, the initial selector must have a low error rate in order to beneﬁt from the CDA approach. 4 , p(X1) = p(X2) = p(Y1) = 1 4
With the datasets used in this work, we can gain insight by approx-imating X1 and X2 with guessable bigrams. Here is another concise example: X1 is a binary variable that indicates the occurrence of the phrase no lacing and X2 is a binary variable for the phrase light bod-ied. For a beer’s appearance aspect, no lacing is a strong indicator for a negative score, and for its palate aspect, light bodied is a strong negative indicator. As beer’s appearance is highly correlated with its palatability, both phrases indicate a low appearance score. Taking
Y1 as the appearance score, X1 as the occurrence of no lacing, and light bodied as X2, we can numerically use Eqn 8 to examine how
CDA helps us for varying error rates in Figure 3. CDA is beneﬁcial whenever the curve is above zero. In the fully correlated dataset,
"Correlated", described in section 4.1, the information carried by light bodied is closer to that of no lacing than it is in the decorre-lated dataset. As a consequence, in the correlated dataset, we have a higher error budget and more opportunities for our CDA.
Figure 3: CDA beneﬁts when approximating X1 and X2 as the occurrence of bi-grams 3 Architecture and Implementation 3.1 Rationale Framework
The original rationale framework, as viewed in this work, was introduced by [19]. It was not our goal to change the core rationalization algorithm, so we re-implemented the algorithm and updated some details. At a high level, our rationale framework is the same in that we use one network to select the rationale and another network to classify the text. The rationale framework can be visualized by the blue portion of Figure 4.
The original implementation [19] used RNNs for both networks, REINFORCE [30] for dealing with the discontinuity introduced by the binary rationale mask, and variable percentage rationales. In this work, we use transformers for both networks because of their effectiveness over RNNs in NLP [28], the simpler straight-through method [3] [5] instead of REINFORCE, and we use ﬁxed percentage rationales because it eliminates sparsity related hyperparameters. Our ﬁxed percentage rationales differ from Chen et al. [7] in that we use the top-K tokens during training and inference whereas
Chen et al. [7] use an iterative re-sampling approach with Gumbel-softmax reparameterization [15].
The classiﬁer is trained only to make quality predictions against the labels while using the rationale.
This is the cross-entropy between the labels and the classiﬁer’s prediction, Ly. We follow [19] by using a coherency regularizer for the rationale selector: Lr = λr 1...T |mt − mt−1| where T is
T the total number of tokens in a document, m is the rationale mask, and λr is the hyper-parameter used to tune coherency. This encourages the rationales to be contiguous. For the datasets evaluated in this work, coherency is a useful inductive bias. The loss for the rationale selector Ls is the coherency regularizer and the cross-entropy between the labels and the classiﬁer’s prediction. This is
Ls = Lr + Ly. (cid:80) 3.2 Counterfactual Predictor
The CDA process described earlier, Eqn 2, requires us to generate new documents with the text and label ﬂipped for just one aspect. That is, we are sampling a new doc-ument from p(X1|X2, 1 − Y1). The key challenge is that we are not provided with ground truth rationales or coun-terfactuals with which to learn how this data generation process works. This can be viewed under the lens of unsu-pervised style transfer for which there is signiﬁcant prior work in the NLP domain [20] [23]. Our method for gener-ating counterfactual documents leverages many ideas from these works, and our main contribution here is connecting these ideas to the rationale framework. We incorporate the rationale framework in the counterfactual generation process because our goal is to lower the mutual
Figure 4: CF Predictor training ﬂow. 5
information between the spurious signals and the target label. An off-the-shelf style transfer method might focus on signals other than that selected by our initial rationale selector.
Figure 4 shows the CF Predictor’s training ﬂow. At its core, the CF Predictor leverages class-dependent Masked-language models (MLM) [9]. We replace the original document’s rationale with an inference from a MLM and leave the rest of the document unchanged. The MLMs are trained to produce documents with the desired class through reinforcement learning (RL) [12], and they are trained to produce realistic documents through adversarial learning [11]. We use straight-through [3] to propagate gradients through token selection during training. For a dataset with binary labels, we train two separate MLMs: one for generating class-0 documents and another for generating class-1 documents [31]. The loss for the counterfactual predictor, LCF P is deﬁned as:
LCF P = λRLLRL − λALA (9) where LRL is the classiﬁer loss after passing the counterfactual through the rationale selector and the classiﬁer. It is the cross-entropy between the desired label, all ones or zeros, and the predicted label.
LA is the loss component from adversarial training. Following [11] and [32], our discriminator, D, seeks to distinguish generated documents from the originals, so the discriminator’s loss, LD, is 1
LA
λA where LA is the cross-entropy between real-fake labels and the prediction given by the discriminator.
λA and λRL balance the adversarial and classiﬁcation losses. We include 1 when training the
λA discriminator to hamstring it relative to the predictor. We found that this generally helped us avoid mode collapse commonly seen when adversarially training generative models. For LA, we mask the contribution of original documents without the desired label. When training the class-1 counterfactual predictor, we show the discriminator real documents X with Y1 = 1 and counterfactual documents,
X c, where the original documents’ labels were Y1 = 0.
During training, the counterfactual is produced in one step. All words not included in the selected rationale from the original document, XM , are kept in the counterfactual document. The kept tokens are X\XM . All words in the selected rationale are replaced by the CF Predictor using one prediction from the MLMs in a greedy fashion. After training, when producing the counterfactual dataset, the counterfactual documents still keep all non-rationale tokens. We now replace the rationale tokens from left to right using the output of the CF Predictor MLMs. A counterfactual token at position t is decoded according to the following process. xc 1,t = arg max x1,t p(x1,t|x2, 1 − y1, xc 1,0...t) (10)
We found this to be a good trade-off between greedy decoding and a more expensive beam search.
Greedy decoding could generate frequent, repeated tokens while beam search could be an unnecessary expense for generating documents that reﬂect the target distribution, but do not necessarily need to pass the bar of human readers.
Figure 5 illustrates an original document and its counterfactual generated by our CF predictor. Notice here the initial selector was successful in identifying the smell aspect. The inclusion of the original and its counterfactual document in the augmented dataset successfully decreases the mutual information between the non-smell text and the smell label. 4 Experiments 4.1 Datasets
We conduct experiments using datasets from two sources. This ﬁrst source contains reviews compiled by Wang et al. [29] from TripAdvisor.com. We use the training, dev, and test sets curated by Bao et al.
[2] and used for rationalization by Chang et al. [5]. The label is binary, and we focus on the location aspect. There are 198 test samples with human-annotated rationales.
The second source consists of reviews collected by McAuley et al. [24] from RateBeer. Each review is a paragraph of text with ﬁve numerical scores in the range of 1 to 5 for the appearance, smell, palate, taste, and overall aspects of the beer. We focus on the appearance, smell, and palate aspects.
We created training and dev datasets for each aspect that follow the source distribution. Following
[5], we binarize this data so that all reviews with a score ≥ 3 are class 1 and all reviews with a score
≤ 2 are class 0. We then balance the dataset between classes. Additionally, we use procedurally decorrelated datasets created by Lei et al. [19], and binarize these datasets as well. We now have two 6
Label
Document negative positive presentation : 12 oz clear bottle , label is metallic with an emblem of a buck . inkjetted on the neck i ’m not sure if this is out of date , or was produced then . appearance : pours clear , somewhere between an amber and a brown color . average head , but it fades pretty quickly to leave just a hint of lace . smell : a bit of grain , fairly mild . this is sort of a generic beer aroma , without much to pick out speciﬁcally . taste : a little bit of sweetness and malt this tastes a bit like a light marzen . overall impression : it is n’t bad it is sort of a generic beer with a bit more ﬂavor than your average macro sort of like a yuengling . nothing to write home about
, but it ’s drinkable enough . presentation : 12 oz clear bottle , label is metallic with an emblem of a buck . inkjetted on the neck i ’m not sure if this is out of date , or was produced then . appearance : pours clear , somewhere between an amber and a brown color . average head , but it fades pretty quickly to leave just a hint of lace . smell : caramel malt and toffee dominate the nose with a bit of raisin and generic beer aroma , without much to pick out speciﬁcally . taste : a little bit of sweetness and malt this tastes a bit like a light marzen . overall impression : it is n’t bad it is sort of a generic beer with a bit more ﬂavor than your average macro sort of like a yuengling . nothing to write home about , but it ’s drinkable enough .
Figure 5: A subset of the augmented dataset for beer smell. The rationale is bold in the original document (top). The replaced words are bold in the counterfactual document (bottom). x1 and y1 have changed from original to counterfactual. Ground truth rationales and counterfactuals are not provided in any training data. datasets for each RateBeer aspect: a "correlated" set and a "decorrelated" set. Appendix Section A.2 shows correlation matrices before and after decorrelating the data as well as additional dataset details.
In the correlated datasets, the correlations and presumably the mutual information between aspects are much higher than in the decorrelated datasets. This makes the rationalization task more difﬁcult.
Additionally, McAuley et al. [24] provides about 1,000 holdout samples where the aspect speciﬁc text is annotated by human experts. 4.2 Baselines
Three MMI-based baselines are used. First, "MMI" is the original Rationalization scheme [19] that has been updated and re-implemented to use the most recent NLP models as described in section 3.1.
This model is trained on the original, unaugmented dataset.
Second, Factual Data Augmentation (FDA) is used to verify that our gains are not only due to data augmentation but due to the CDA procedure. FDA augments the original dataset with new samples generated by the CF predictor models, but instead of ﬂipping the label and passing it to the 1 − y1 component, we pass the sample to the component of the CF predictor with the same label as the original document during inference. The new samples are produced using the following process: yc 1 ← y1; xc 1 ← arg max x1 p(X1|y1, x2) (11)
The third baseline, simple substitution using antonyms (ANT), does not use neural models to augment counterfactual data. The counterfactual is generated by replacing words in the rationale with antonyms from WordNet [25] [4]. Note that we only accept antonyms that are in the vocabulary used by the models and antonyms that have the same part of speech as labeled by NLTK [4]. 4.3 Experiment Settings and Assumptions
We train the rationale selector and the classiﬁer together, early stop based on the selector cost, freeze the selector, and ﬁnally ﬁne-tune the classiﬁer on the original dataset. For all of the data sets and models, we use the dev set for early stopping (more details in Appendix Section A.3). Our MLM transformers [28] were pretrained on unlabeled data from the TripAdvisor and RateBeer datasets separately. For the TripAdvisor dataset, we pretrain on all data that does not appear in the location aspect’s train, dev, or annotated dataset from [5]. For the RateBeer datasets, we pretrain on all data that does not appear in any train, dev, or annotated dataset from [19] and the correlated datasets. We used a masking rate of 10% and masked tokens were treated as described in [9]. For the rationale models, the transformers are initialized from models pre-trained with random masking. We found 7
Hotel - Location we only stayed one night
. the location was great and the premises was beautiful . the room was a typical hotel room but was decent , clean and spacious enough for our needs . we were given a ﬁrst
ﬂoor room next to the pool . it was relatively quiet . the staff were informative and friendly . the daily parking was a total rip - off at $ 20/day but i would think that the rates are similar anywhere else you stay on the island .
Correlated - Smell pours
, with caramel and brown sugar sweetness arising to greet good nose sweetness with a dash of hopped bitterness well . well carbonated , this is a sturdy , very drinkable beer that has joined me for much leisure time . i wo n’t apologize for that ; nor will you after a couple brown ales .
. the taste blends the
, it was a good enough stay . all in all
ﬁngers brown body thick froth dark two tan it . on of a
Figure 6: Examples from the annotated sets. Hotel-Location (top) and Correlated-Smell (bottom).
Human annotations are underlined. CDA rationales are in blue. MMI rationales are in red. Overlaps between CDA and MMI are in magenta. that contiguous masking was better for pre-training the CF predictor. This contiguous masking is similar to that from Joshi et al. [16], but masked tokens are treated the same as in Devlin et al.
[9] and there is only one contiguous span. The rationale selector and classiﬁer are initialized from the random-masking transformers. For the rationale selectors, following [5], we set the rationale percentage to 10% for all datasets. The CF predictor components and the GAN discriminator are initialized from the contiguous-masking transformers. These models all have a vocabulary with 215 tokens, 8 layers, 8 attention heads, and a hidden dimension of 256. Appendix Section A.4 shows our server conﬁgurations and more details on our experiment setup.
Models are selected and reported based on the best performance on the dev set across a grid search.
All methods are evaluated using the same grid search when training the rationale model. The model is selected before ﬁne-tuning the classiﬁer with a frozen rationale selector. The initial selector used to train the counterfactual predictors was the selected MMI model with a ﬁne-tuned classiﬁer. The parameters and checkpoints for the CF Predictor models are tuned and chosen to maximize the accuracy of the training documents’ predicted label as compared to the target label (measured by the original rationale model) and to maximize the entropy in the inserted counterfactual tokens. The CF
Predictor model is chosen from a grid search, using only the training dataset, across λA and λRL.
For the TripAdvisor dataset, we repeat the experiment three times: training an initial rationale selector, training a counterfactual predictor, generating a counterfactual dataset, and training a new rationale model. Additionally, we train two additional rationale models with different random seeds and the selected hyperparameters. Consequently, there are nine rationale models for each of the four methods, and we report the mean and standard deviation across those nine models. In the RateBeer datasets, we repeated the same experiments two times and then trained two additional models with different random seeds and the selected hyperparameters for each rationale model. As a result, we have a total of six runs for each method for the RateBeer datasets. All models are in Tensorﬂow [1]. Our software is publicly released 1. 4.4 Results
We evaluate the rationale models by the precision (Rat. Prec.) as compared to human annotations.
This token-level metric is taken as the mean across samples in the annotated set. We also report the accuracy of the classiﬁer on the development set (Dev. Acc.).
As shown in Tables 1, 2, and 3, our CDA approach outperforms all baselines on 6 7 experiments. As expected, CDA’s performance over other methods is most pronounced on the correlated RateBeer datasets compared to on the decorrelated ones. Only on the decorrelated appearance dataset, CDA performs second to MMI. This may be because on the decorrelated appearance dataset the spurious aspects have very low mutual information with the target label, appearance, and therefore the error margin for the initial selector is very tight and CDA is expected to yield low gains as discussed in
Section 2.4. 1github.com/mlplyler/CFs_for_Rationales 8
The baseline augmentation schemes (FDA, ANT) produced mixed results. The FDA scheme should not have changed the mutual information between the spurious aspects and the target label. Whenever it performed worse than MMI alone, it most likely introduced noise into the dataset. While the ANT scheme might be effective in lowering the mutual information between the spurious aspects and the target label, the generated counterfactual does not likely model p(x1|1 − y1, x2), so we expect it to also reduce I(X1, Y1).
Table 1: TripAdvisor - Location
Empirically, we have demonstrated that models trained on the CDA augmented data tend to outperform models trained on the original datasets. This lends credence to the idea that our scheme is indeed lowering the mutual information between the spurious aspects and the target la-bel. The dataset and method pairs with very high variance are dragged down by degenerated runs where the rationale selector has little or no skill. These runs were seen when varying the random seed with the selected hyperparame-ters. Removing the degenerate runs can bring the mean rationale precision of the high variance experiments closer to CDA, but we believe these degenerate models seen primarily in the baselines align well with our theory that CDA allows the rationale selector to more easily identify X1.
MMI 78.16 ± 5.83
FDA 80.61 ± 4.38
ANT 69.79 ± 2.90
CDA 78.11 ± 6.77 26.14 ± 13.25 31.36 ± 10.38 12.25 ± 3.64 39.76 ± 10.48
Dev. Acc.
Rat. Prec.
Case Study: Figure 6 presents a case study comparing the rationales selected by MMI and our proposed CDA approach in the TripAdvisor and Correlated Beer datasets respectively. As shown in Figure 6, our CDA models can select the text that aligns better with human annotations and they successfully avoid selecting sentiment-carrying text that is not relevant to the aspect of interest.
Table 2: Correlated RateBeer Results
Appearance
Smell
Palate
Dev. Acc.
Rat. Prec.
Dev. Acc.
Rat. Prec.
Dev. Acc.
Rat. Prec.
MMI 76.37 ± 3.05
FDA 75.73 ± 3.64
ANT 62.09 ± 8.76
CDA 73.30 ± 2.93 56.28 ± 17.22 41.02 ± 5.09 43.25 ± 16.51 67.79 ± 10.63 79.49 ± 4.53 76.43 ± 5.08 57.70 ± 9.87 81.82 ± 0.69 45.39 ± 16.45 41.67 ± 23.73 11.72 ± 7.16 61.84 ± 6.76 82.67 ± 0.99 77.60 ± 6.06 61.93 ± 7.59 80.65 ± 0.64 34.98 ± 8.79 32.35 ± 14.41 8.62 ± 12.31 41.24 ± 1.72
Table 3: Decorrelated RateBeer Results
Appearance
Smell
Palate
Dev. Acc.
Rat. Prec.
Dev. Acc.
Rat. Prec.
Dev. Acc.
Rat. Prec.
MMI 81.33 ± 0.93
FDA 80.67 ± 1.32 71.41 ± 9.97
ANT
CDA 80.24 ± 1.07 86.92 ± 7.35 81.97 ± 6.00 67.67 ± 27.68 82.82 ± 8.60 77.99 ± 5.46 79.66 ± 1.23 58.81 ± 6.31 79.18 ± 1.43 79.71 ± 10.56 83.62 ± 3.52 9.32 ± 10.76 86.66 ± 2.92 68.64 ± 10.09 77.26 ± 1.90 57.75 ± 7.98 76.81 ± 0.85 46.39 ± 29.21 66.03 ± 3.76 10.83 ± 11.15 67.79 ± 3.20 5