Abstract
Hyperbolic space has become a popular choice of manifold for representation learn-ing of various datatypes from tree-like structures and text to graphs. Building on the success of deep learning with prototypes in Euclidean and hyperspherical spaces, a few recent works have proposed hyperbolic prototypes for classiﬁcation. Such approaches enable effective learning in low-dimensional output spaces and can exploit hierarchical relations amongst classes, but require privileged information about class labels to position the hyperbolic prototypes. In this work, we propose
Hyperbolic Busemann Learning. The main idea behind our approach is to position prototypes on the ideal boundary of the Poincaré ball, which does not require prior label knowledge. To be able to compute proximities to ideal prototypes, we introduce the penalised Busemann loss. We provide theory supporting the use of ideal prototypes and the proposed loss by proving its equivalence to logistic regression in the one-dimensional case. Empirically, we show that our approach provides a natural interpretation of classiﬁcation conﬁdence, while outperforming recent hyperspherical and hyperbolic prototype approaches. 1

Introduction
Classiﬁcation by prototypes has a long tradition in machine learning. Foundational solutions such as the Nearest Mean Classiﬁer [46] represent classes as the mean prototypes in a ﬁxed feature space.
Following approaches that learning a metric space for class prototypes [28], deep learning with prototypes as points in network output spaces has gained traction [14, 17, 26, 32, 36, 41, 49, 51]. In line with foundational approaches, the prototypes are positioned by computing the mean over all training examples for each class. Deep learning by mean prototypes has shown to be effective for tasks such as few-shot learning [14, 32, 36, 41] and zero-shot recognition [41, 49, 51].
To avoid the need to re-learn prototypes or to enable the use of prior label knowledge, several recent works have proposed deep networks with prototypes in non-Euclidean output spaces. Hyperspherical prototype approaches initialize prototypes on the sphere or its higher-dimensional generalization, based on pair-wise angular separation [29, 39] or with the help of word embeddings [4, 43] using a cosine similarity loss between example outputs and class prototypes. Hyperspherical prototypes alleviate the need to re-position prototypes continuously and allow to optionally include prior semantic knowledge about the classes. Recently, a few works have extended prototype-based learning to the hyperbolic domain, where prototypes are obtained by embedding label hierarchies [23, 24]. While hyperbolic prototype approaches provide more hierarchically coherent classiﬁcation results, prior hierarchical knowledge is required to embed prototypes. This paper strives to combine the best of both non-Euclidean worlds, namely efﬁcient low-dimensional embeddings from hyperbolic prototypes and the knowledge-free positioning from hyperspherical prototypes.
We make three contributions in this work. First, we introduce a hyperbolic prototype network with class prototypes given as points on the ideal boundary of the Poincaré ball model of hyperbolic geometry. Second, we propose the penalized Busemann loss, which enables us to compute proximities between example outputs in hyperbolic space and prototypes at the ideal boundary, an impossible task for existing distance metrics, which put the ideal boundary at inﬁnite distance from all other 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
Figure 1: Visualization of the output space for Hyperbolic Busemann Learning in two dimen-sions on CIFAR-10. The penalized Busemann loss optimizes images to be close to their corresponding ideal prototypes while forcing all examples towards the origin to avoid over-conﬁdence, e.g., for the deer, top plane, and ship examples. Ambiguous cases, such as the horse and bottom plane examples are projected closer to the origin, providing a natural interpretation of classiﬁcation conﬁdence. points in hyperbolic space. Third, we provide a theoretical link between our hyperbolic prototype approach and logistic regression. We show that our choices for output manifold, for prototypes on the ideal boundary, and for the proposed loss provide a direct generalization of the logistic regression model. Experiments on three datasets show that our approach outperforms both hyperspherical and hyperbolic prototype approaches. Moreover, our hyperbolic output space provides a natural interpretation of closeness to the ideal boundary as classiﬁcation conﬁdence, see Figure 1. The code is available at https://github.com/MinaGhadimiAtigh/Hyperbolic-Busemann-Learning. 2