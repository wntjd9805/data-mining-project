Abstract
We study the approximation power of Graph Neural Networks (GNNs) on latent position random graphs. In the large graph limit, GNNs are known to converge to certain “continuous” models known as c-GNNs, which directly enables a study of their approximation power on random graph models. In the absence of input node features however, just as GNNs are limited by the Weisfeiler-Lehman isomorphism test, c-GNNs will be severely limited on simple random graph models. For instance, they will fail to distinguish the communities of a well-separated Stochastic Block
Model (SBM) with constant degree function. Thus, we consider recently proposed architectures that augment GNNs with unique node identiﬁers, referred to as
Structural GNNs here (SGNNs). We study the convergence of SGNNs to their continuous counterpart (c-SGNNs) in the large random graph limit, under new conditions on the node identiﬁers. We then show that c-SGNNs are strictly more powerful than c-GNNs in the continuous limit, and prove their universality on several random graph models of interest, including most SBMs and a large class of random geometric graphs. Our results cover both permutation-invariant and permutation-equivariant architectures. 1

Introduction
Graph Neural Networks (GNNs) are deep architectures deﬁned over graph data that have garnered a lot of attention in recent years. They represent the state-of-the-art in many graph Machine Learning (graph ML) problems, and have been successfully applied to e.g. node clustering [7], semi-supervised learning [24], quantum chemistry [17], and so on. See [5, 44, 18, 21] for reviews.
As the universality of Multi-Layers Perceptrons (MLP) is one of the foundational theorems in deep learning – that is, any continuous function can be approximated arbitrarily well by an MLP – in the last few years the approximation power of GNNs has been a topic of great interest. In the absence of special node features, i.e. when one has only access to the graph structure, the crux of the problem has been proven to be the capacity of GNNs to solve the graph isomorphism problem, that is, deciding when two graphs are permutations of each other or not (a difﬁcult problem for which no polynomial algorithm is known [4]). Indeed, this property is directly linked to the approximation power of GNNs [8, 3]. In this light, the landmark paper [45] proves that classical GNNs are at best as powerful as the famous Weisfeiler-Lehman (WL) test [43] for graph isomorphism. Since then, several works [45, 31, 8] have derived new architectures, for instance involving high-order tensors [31], with improved discriminative power equivalent to “higher-order” variants of the WL test. In another line of works, several recent papers have advocated the use of unique node identiﬁers [28, 27], with 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
strategies to preserve the permutation-equivariance/invariance of GNNs coined Structural Message
Passing (SMP) in [41] (see Sec. 2.3). We call these models Structural GNN (SGNN) here. SGNNs have been proved to be strictly more powerful than the WL test in [41], and even universal on graphs with bounded degrees, however for powerful layers that cannot be implemented in practice.
When the size of the graphs grows, the notion of graph isomorphism becomes somewhat moot: large graphs might share properties (number of well-connected communities, and so on) but are never isomorphic to each other. GNNs have nevertheless proven successful in identifying their large-scale structures, e.g. for node clustering [7]. Several papers have therefore used tools from random graph theory and graphons to study the behavior of GNNs in the large-graph limit. In [22, 38], GNNs are shown to converge to limiting “continuous” architectures (coined c-GNNs in [22]). A few works have studied the discriminative power of c-GNNs on graphons [30], however, analogously to how the WL test will fail on regular graphs, c-GNNs are severely limited on graph models with almost-constant degree function (Fig. 1), and the question is still largely open.
Contribution and outline.
In this paper, we study the convergence of SGNNs on large ran-dom graphs towards “c-SGNNs”, and analyze the approximation power of c-GNNs and c-SGNNs. After some preliminary results in
Sec. 2, we study the convergence of SGNNs in Sec 3.
In Sec. 4 and 5, we show that c-SGNNs are strictly more powerful than c-GNNs in both permutation-invariant and equiv-ariant case. We then prove the universality of c-SGNNs on several popular models of ran-dom graphs, including Stochastic Block Models (SBMs) and radial kernels. For instance, we show in Sec. 5.3 that c-SGNNs can perfectly identify the communities of most SBMs, in-cluding some for which any c-GNN provably fails (Fig. 1). The code for the numerical illus-trations is available at https://github.com/ nkeriven/random-graph-gnn.
Figure 1: Illustration of Prop. 7 (Sec. 5.3). On an
SBM with constant degree function, a GNN (top) might overﬁt the training set, but converges to a constant c-GNN. On the contrary, there exists a c-SGNN (bottom) that perfectly separates the communities. Details can be found in App. E.