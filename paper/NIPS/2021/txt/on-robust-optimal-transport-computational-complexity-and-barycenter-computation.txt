Abstract
We consider robust variants of the standard optimal transport, named robust optimal transport, where marginal constraints are relaxed via Kullback-Leibler divergence.
We show that Sinkhorn-based algorithms can approximate the optimal cost of robust optimal transport in (cid:101)O( n2
ε ) time, in which n is the number of supports of the probability distributions and ε is the desired error. Furthermore, we investigate a ﬁxed-support robust barycenter problem between m discrete probability distribu-tions with at most n number of supports and develop an approximating algorithm based on iterative Bregman projections (IBP). For the speciﬁc case m = 2, we show that this algorithm can approximate the optimal barycenter value in (cid:101)O( mn2
ε ) time, thus being better than the previous complexity (cid:101)O( mn2
ε2 ) of the IBP algorithm for approximating the Wasserstein barycenter. 1

Introduction
The recent advance in computation with optimal transport (OT) problem [12, 3, 13, 7, 22, 26, 20] has led to a surge of interest in using that tool in various domains of machine learning and statistics. The range of its applications is broad, including deep generative models [4, 16, 36], scalable Bayes [33, 34], mixture and hierarchical models [24], and other applications [32, 29, 10, 17, 37, 35, 8].
The goal of optimal transport is to ﬁnd a minimal cost of moving masses between (supports of) probability distributions. It is known that the estimation of transport cost is not robust when there are outliers. To deal with this issue, [38] proposed a trimmed version of optimal transport. In particular, they search for truncated probability distributions such that the transport cost between them is minimized. However, their trimmed optimal transport is non-trivial to compute, which hinders its usage in practical applications. Another line of works proposed using unbalanced optimal transport (UOT) to solve the sensitivity of optimal transport to outliers [5, 31]. More speciﬁcally, their idea is to assign as small as possible masses to outliers by relaxing the marginal constraints of OT through a penalty function such as the Kullback-Leibler (KL) divergence. This direction of robust optimal transport has been shown to have good performance in generative models and domain adaptation [5]. Although this approach achieved considerable success, the full picture of its computational complexity has remained missing. (cid:63) Khang Le and Huy Nguyen contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our Contribution: In the paper, we provide a comprehensive study of the computational complexity of robust optimal transport and its corresponding barycenter problem when the probability distribu-tions are discrete and have at most n components. Our contribution is twofold and can be summarized as follows: (1) On robust optimal transport, we consider two versions corresponding to two ways of relaxing marginal constraints in the standard optimal transport problem via the KL divergence. We show that two scaling algorithms computing these robust formulations have the complexities (cid:101)O(n2/ε), where ε denotes the desired error for the computed cost. These complexities are lower than the complexity of the Sinkhorn algorithm for solving the optimal transport problem, which is (cid:101)O(n2/ε2) [13], and match the complexity of the Sinkhorn algorithm that solves the UOT problem [27]. Furthermore, we show how the above complexity can be improved by utilizing the low-rank approximation method to speed up the matrix-vector computations in the loop similar to
[2], and obtain the improved computing time of (cid:101)O(nr2 + nr
ε ), where r is the approximated rank. (2) On robust barycenter problem, where the goal is to determine a probability measure that minimizes its robust optimal cost to a given set of m ≥ 2 probability measures, we propose
ROBUSTIBP algorithm for solving the robust barycenter problem, which is inspired by the iterative Bregman projection (IBP) algorithm for solving the traditional barycenter problem [6].
We show that when m = 2, the complexity of ROBUSTIBP algorithm is at the order of (cid:101)O(mn2/ε), better than that of the IBP algorithm for solving the traditional barycenter problem [19], which is (cid:101)O(mn2/ε2). To the best of our knowledge, the ROBUSTIBP is also the ﬁrst practical algorithm obtaining the near-optimal complexity (cid:101)O(mn2/ε) for solving the barycenter problem even under only the setting m = 2.
Organization: The paper is organized as follows. In Section 2, we provide the background on the optimal transport problem and some of its variants that have robust effects. In Section 3, we discuss in-depth the variant where only one marginal constraint is relaxed, study the computational complexity of a Sinkhorn-based algorithm that solves it, and then brieﬂy introduce the fully-relaxed formulation. We also establish the complexities of these algorithms after applying Nyström method.
Subsequently, we present our study of the robust barycenter problem in Section 4. In Section 5, we carry out empirical studies to illustrate the theories before concluding with a few discussions in
Section 6. The proofs of our theoretical results are in the supplementary material.
Notation: We let [n] stand for the set {1, 2, . . . , n} while Rn
+ indicates the set of all vectors with non-negative entries. For a vector x ∈ Rn and p ∈ [1, ∞), we denote (cid:107)x(cid:107)p as its (cid:96)p-norm and diag(x) as the diagonal matrix with x on the diagonal. The natural logarithm of a vector a = (a1, ..., an) ∈ Rn
+ is denoted by log a = (log a1, ..., log an), 1n stands for a vector of length n that all of its entries equal to 1, and ∂xf refers to the partial differentiation of function f with respect to x. For any given space X ⊂ Rd, we denote by P(X ) the space of all probability measures on X . Given an integer n > 0 and a real number ε > 0, the notation a = O (b(n, ε)) means that a ≤ C · b(n, ε) where C is independent of n and ε. Meanwhile, the notation a = (cid:101)O(b(n, ε)) indicates the previous inequality may depend on a logarithmic function of n and ε. For any two probability measures x = (x1, . . . , xn) and y = (y1, . . . , yn) with the same supports, the generalized Kullback-Leibler (cid:2)xi log (cid:0) xi (cid:3). Finally, the entropy of a matrix divergence is deﬁned as KL(x(cid:107)y) = (cid:80)n yi
X is given by H(X) = (cid:80)n (cid:1) − xi + yi i,j=1 −Xij(log Xij − 1). i=1 2