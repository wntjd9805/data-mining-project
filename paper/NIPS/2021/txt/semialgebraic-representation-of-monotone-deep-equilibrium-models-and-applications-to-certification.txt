Abstract
Deep equilibrium models are based on implicitly deﬁned functional relations and have shown competitive performance compared with the traditional deep networks.
Monotone operator equilibrium networks (monDEQ) retain interesting performance with additional theoretical guaranties. Existing certiﬁcation tools for classical deep networks cannot directly be applied to monDEQs for which much fewer tools exist.
We introduce a semialgebraic representation for ReLU based monDEQs which allows to approximate the corresponding input output relation by semideﬁnite programming (SDP). We present several applications to network certiﬁcation and obtain SDP models for the following problems : robustness certiﬁcation, Lipschitz constant estimation, ellipsoidal uncertainty propagation. We use these models to certify robustness of monDEQs w.r.t. a general Lq norm. Experimental results show that the proposed models outperform existing approaches for monDEQ certiﬁcation.
Furthermore, our investigations suggest that monDEQs are much more robust to
L2 perturbations than L∞ perturbations. 1

Introduction
With the increasing success of Deep Neural Networks (DNN) (e.g. computer vision, natural language processing), one witnesses a signiﬁcant increase in size and complexity (topology and activation functions). This generates difﬁculties for theoretical analysis and a posteriori performance evaluation.
This is problematic for applications where robustness issues are crucial, for example inverse problems (IP) in scientiﬁc computing. Indeed such IPs are notoriously ill-posed and as stressed in the March 2021 issue of SIAM News [1], “Yet DL has an Achilles’ heel. Current implementations can be highly unstable, meaning that a certain small perturbation to the input of a trained neural network can cause substantial change in its output. This phenomenon is both a nuisance and a major concern for the safety and robustness of DL-based systems in critical applications—like healthcare—where reliable computations are essential". Indeed, the Instability Theorem [1] predicts unavoidable lower bound on Lipschitz contants, which may explain the lack of stability of some DNNs, over-performing on training sets. This underlines the need to evaluate precisely a posteriori critical indicators, such as
Lipschitz constants of DNNs. However, obtaining an accurate upper bounds on the Lipschitz constant of a DNN is a hard problem, it reduces to prove a global inequality “Ψ(x) ≥ 0 for all x in a domain”, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
i.e., to provide a proof of positivity for a function Ψ, which has no simple explicit expression. Even for modest size DNNs this task is practically challenging, previous successful attempts [8, 28] were restricted in practice to no more than two hidden layers with less than a hundred nodes. More broadly existing attempts to DNN certiﬁcation rely either on zonotope calculus, linear programming (LP) or hierarchies of SDP based on positivity certiﬁcates from algebraic geometry [34], which may suffer from the curse of dimensionality.
Recently, implicit deep learning [12] arises as a generalization of the recursive rules of traditional feedforward neural networks. Speciﬁcally, Deep Equilibrium Models (DEQ) [2] have emerged as a potential alternative to classical DNNs. With their much simpler layer structure, they provide competitive results on machine learning benchmarks [2, 3]. Training DEQs involves solving ﬁx-point equations, which requires further conditions to make the iteration converge. Fortunately, Monotone operator equilibrium network (monDEQ) introduced in [45] satisﬁes such conditions. Moreover, the authors in [33] provide explicit bounds on global Lipschitz constant of monDEQs (w.r.t. the
L2-norm) which can be used for robustness certiﬁcation.
From a certiﬁcation point of view, DEQs have the deﬁnite advantage of having only one implicit layer compared to DNNs and therefore is potentially more amenable to sophisticated techniques (e.g. algebraic certiﬁcates of positivity) which rapidly face their limit even for classical DNNs with modest size (but long depth). Therefore monDEQs constitute a class of DNNs for which robustness certiﬁcation, uncertainty propagation or Lipschicity could potentially be investigated in a more satisfactory way than classical networks. Contrary to DNNs, for which a variety of tools have been developped, certiﬁcation of DEQ modeld is relatively open, the only available tool is the Lipschitz bound in [33].
Contribution
We present three general semialgebraic models of ReLU monDEQ for certiﬁcation (p ∈ Z+ ∪{+∞}):
• Robustness Model for network Lp robustness certiﬁcation.
• Lipschitz Model for network Lipschitz constant estimation with respect to any Lp norm.
• Ellipsoid Model for ellipsoidal outer-approximation of the image by the network of a polyhedra or an ellipsoid.
All these models can be used for robustness certiﬁcation, a common task which we consider experi-mentally. And the main originality of this work is to successfully apply the proposed techniques to monDEQ networks (especially for the Ellipsoid model which is a novel form of reachability analysis), which was not proposed before. Both Lipschitz and Ellipsoid models can in addition be used for further a posteriori analyses. Interestingly, all three models are given by solutions of semideﬁnite programs (SDP), obtained by Shor relaxation of a common semialgebraic representation of ReLU monDEQs. Our models are all evaluated to simple ReLU monDEQs on MNIST dataset similar as
[45, 33] on the task of robustness certiﬁcation. We demonstrate that all three models ourperform the approach of [33] and the Robustness Model being the most efﬁcient. Our experiments also suggest that DEQs are much less robust to L∞ perturbations than L2 perturbations, in contrast with classical
DNNs [36] which have been shown to be robust on MNIST dataset to the level we are considering (ε = 0.1).