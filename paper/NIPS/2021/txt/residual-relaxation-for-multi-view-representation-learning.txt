Abstract
Multi-view methods learn representations by aligning multiple views of the same image and their performance largely depends on the choice of data augmentation.
In this paper, we notice that some other useful augmentations, such as image ro-tation, are harmful for multi-view methods because they cause a semantic shift that is too large to be aligned well. This observation motivates us to relax the ex-act alignment objective to better cultivate stronger augmentations. Taking image rotation as a case study, we develop a generic approach, Pretext-aware Resid-ual Relaxation (Prelax), that relaxes the exact alignment by allowing an adaptive residual vector between different views and encoding the semantic shift through pretext-aware learning. Extensive experiments on different backbones show that our method can not only improve multi-view methods with existing augmenta-tions, but also beneﬁt from stronger image augmentations like rotation. 1

Introduction
Without access to labels, self-supervised learning relies on surrogate objectives to extract meaningful representations from unlabeled data, and the chosen surrogate objectives largely determine the qual-ity and property of the learned representations [24, 19]. Recently, multi-view methods have become a dominant approach for self-supervised representation learning that achieves impressive down-stream performance, and many modern variants have been proposed [22, 14, 1, 23, 2, 12, 3, 4, 11, 5].
Nevertheless, most multi-view methods can be abstracted and summarized as the following pipeline: for each input x, we apply several (typically two) random augmentations to it, and learn to align these different “views” (x1, x2, . . . ) of x by minimizing their distance in the representation space.
In multi-view methods, the pretext, i.e., image augmentation, has a large effect on the ﬁnal perfor-mance. Typical choices include image re-scaling, cropping, color jitters, etc [2]. However, we ﬁnd that some augmentations, for example, image rotation, is seldom utilized in state-of-the-art multi-view methods. Among these augmentations, Figure 1a shows that rotation causes severe accuracy drop in a standard supervised model. Actually, image rotation is a stronger augmentation that largely affects the image semantics, and as a result, enforcing exact alignment of two different rotation an-gles could degrade the representation ability in existing multi-view methods. Nevertheless, it does not mean that strong augmentations cannot provide useful semantics for representation learning.
In fact, rotation is known as an effective signal for predictive learning [10, 30, 21]. Differently, predictive methods learn representations by predicting the pretext (e.g., rotation angle) from the cor-∗Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Comparison of augmentations. (b) A toy example of residual relaxation.
Figure 1: Left: the effect of different augmentations of CIFAR-10 test images with a supervised model (trained without using any data augmentation, more details in Appendix A). Right: an illus-tration of the exact alignment objective of multi-view methods (z(cid:48) z) and the relaxed residual
→← alignment of our Prelax (z(cid:48) z). As the rotation largely modiﬁes the image semantics, our
Prelax adopts a rotation-aware residual vector r to bridge the representation of two different views.
→←
− r responding view. In this way, the model is encouraged to encode pretext-aware image semantics, which also yields good representations.
To summarize, strong augmentations like rotation carry meaningful semantics, while being harmful for existing multi-view methods due to large semantic shift. To address this dilemma, in this paper, we propose a generic approach that generalizes multi-view methods to cultivating stronger augmen-tations. Drawing inspirations from the soft-margin SVM, we propose residual alignment, which relaxes the exact alignment in multi-view methods by incorporating a residual vector between two views. Besides, we develop a predictive loss for the residual to ensure that it encodes the seman-tic shift between views (e.g., image rotation). We name this technique as Pretext-aware REsidual
ReLAXation (Prelax), and an illustration is shown in Figure 1b. Prelax serves as a generalized multi-view method that is adaptive to large semantic shift and combines image semantics extracted from both pretext-invariant and pretext-aware methods. We summarize our contributions as follows:
• We propose a generic technique, Pretext-aware Residual Relaxation (Prelax), that general-izes multi-view representation learning to beneﬁt from stronger image augmentations.
• Prelax not only extracts pretext-invariant features as in multi-view methods, but also en-codes pretext-aware features into the pretext-aware residuals. Thus, it can serve as a uniﬁed approach to bridge the two existing methodologies for representation learning.
• Experiments show that Prelax can bring signiﬁcant improvement over both multi-view and predictive methods on a wide range of benchmark datasets. 2