Abstract
We develop scalable methods for producing conformal Bayesian predictive in-tervals with ﬁnite sample calibration guarantees. Bayesian posterior predictive distributions, p(y | x), characterize subjective beliefs on outcomes of interest, y, conditional on predictors, x. Bayesian prediction is well-calibrated when the model is true, but the predictive intervals may exhibit poor empirical coverage when the model is misspeciﬁed, under the so called M-open perspective. In contrast, conformal inference provides ﬁnite sample frequentist guarantees on predictive conﬁdence intervals without the requirement of model ﬁdelity. Using ‘add-one-in’ importance sampling, we show that conformal Bayesian predictive intervals are efﬁciently obtained from re-weighted posterior samples of model parameters. Our approach contrasts with existing conformal methods that require expensive reﬁtting of models or data-splitting to achieve computational efﬁciency. We demonstrate the utility on a range of examples including extensions to partially exchangeable settings such as hierarchical models. 1

Introduction
We consider Bayesian prediction using training data Z1:n = {Yi, Xi}i=1:n for an outcome of interest
Yi and covariates Xi ∈ Rd. Given a model likelihood fθ(y | x) and prior on parameters, π(θ) for
θ ∈ Rp, the posterior predictive distribution for the response at a new Xn+1 = xn+1 takes on the form (cid:90) p(y | xn+1, Z1:n) = fθ(y | xn+1) π(θ | Z1:n) dθ , (1) where π(θ | Z1:n) is the Bayesian posterior. Asymptotically exact samples from the posterior can be obtained through Markov chain Monte Carlo (MCMC) and the above density can be computed through Monte Carlo (MC), or by direct sampling from an approximate model. Given a Bayesian predictive distribution, one can then construct the highest density 100 × (1 − α)% posterior predictive credible intervals, which are the shortest intervals to contain (1 − α) of the predictive probability.
Alternatively, the central 100 × (1 − α)% credible interval can be computed using the α/2 and 1 − α/2 quantiles. Posterior predictive distributions condition on the observed Z1:n and represent subjective and coherent beliefs. However, it is well known that model misspeciﬁcation can lead
Bayesian intervals to be poorly calibrated in the frequentist sense (Dawid, 1982; Fraser et al., 2011), that is the long run proportion of the observed data lying in the (1 − α) Bayes predictive interval is not necessarily equal to (1 − α). This has consequences for the robustness of such approaches and trust in using Bayesian models to aid decisions.
Alternatively, one can seek intervals around a point prediction from the model, (cid:98)y = (cid:98)µ(x), that have the correct frequentist coverage of (1 − α) . This is precisely what is offered by the conformal prediction framework of Vovk et al. (2005), which allows the construction of prediction bands with
ﬁnite sample validity without assumptions on the generative model beyond exchangeability of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
data. Formally, for Zi = {Yi, Xi}i=1:n, Zi allows us to construct a conﬁdence set Cα(Xn+1) from Z1:n and Xn+1 such that iid∼ P and miscoverage level α, conformal inference
P(Yn+1 ∈ Cα(Xn+1)) ≥ 1 − α. (2)
Note that P is over Z1:n+1, that is we also treat Xn+1 as random; see Lei and Wasserman (2014) for a discussion on the difﬁculties of attaining coverage conditional on Xn+1. In this paper we develop computationally efﬁcient conformal inference methods for Bayesian models including extensions to hierarchical settings. A general theme of our work is that, somewhat counter-intuitively, Bayesian models are well suited for the conformal method.
Conformal inference for calibrating Bayesian models was previously suggested in Melluish et al. (2001), Vovk et al. (2005), Wasserman (2011) and Burnaev and Vovk (2014), where it is referred to as “de-Bayesing”, “frequentizing” and “conformalizing”, but only in the context of conjugate models.
Here, we present a scalable MC method for conformal Bayes, implementing full conformal Bayesian prediction using an ‘add-one-in’ importance sampling algorithm. The automated method can construct conformal predictive intervals from any Bayesian model given only samples of model parameter values from the posterior θ ∼ π(θ | Z1:n), up to MC error. Such samples are readily available in most
Bayesian analyses from probabilistic programming languages such as Stan (Carpenter et al., 2017) and PyMC3 (Salvatier et al., 2016). We also extend conformal inference to partially exchangeable settings which utilize the important class of Bayesian hierarchical models, and note the connection to Mondrian conformal prediction (Vovk et al., 2005, Chapter 4.5). Previously, the extension of conformal prediction to random effects was introduced in Dunn et al. (2020) in a non-Bayesian setting, with a focus on prediction in new groups, as well as within-group predictions without covariates. We will see that the Bayesian hierarchical model allows for a natural sharing of information between groups for within-group predictions with covariates. We discuss the motivation behind using the
Bayesian posterior predictive density as the conformity measure for both the Bayesian and the frequentist, and demonstrate the beneﬁts in a number of examples. 1.1