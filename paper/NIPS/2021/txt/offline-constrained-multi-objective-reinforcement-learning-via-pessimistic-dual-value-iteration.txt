Abstract
In constrained multi-objective RL, the goal is to learn a policy that achieves the best performance speciﬁed by a multi-objective preference function under a constraint.
We focus on the ofﬂine setting where the RL agent aims to learn the optimal policy from a given dataset. This scenario is common in real-world applications where interactions with the environment are expensive and the constraint violation is dangerous. For such a setting, we transform the original constrained problem into a primal-dual formulation, which is solved via dual gradient ascent. Moreover, we propose to combine such an approach with pessimism to overcome the uncertainty in ofﬂine data, which leads to our Pessimistic Dual Iteration (PEDI). We establish upper bounds on both the suboptimality and constraint violation for the policy learned by PEDI based on an arbitrary dataset, which proves that PEDI is provably sample efﬁcient. We also specialize PEDI to the setting with linear function approximation. To the best of our knowledge, we propose the ﬁrst provably efﬁcient constrained multi-objective RL algorithm with ofﬂine data without any assumption on the coverage of the dataset. 1

Introduction
There has been increased interest in multi-objective RL in recent years. Compared with traditional single-objective RL, the goal of the multi-objective one depends on a preference function, which takes the multiple objectives as input and outputs a scalar. The multi-objective optimization problems are usually constrained, as otherwise, it may cause danger or malfunction in applications. For example, consider a home automation system that helps humans monitor and control home attributes which can be regarded as multi-objectives. Users at different times may value different aspects of its services.
Some may think highly of lighting at night, while others may concern the climate. We can formulate the users’ preference as a preference function on the multiple objectives. Therefore, the system is dealing with a multi-objective optimization problem. However, the system cannot optimize this problem without any constraints, which might go against human’s will, such as generating extreme climate inside a house or performing unacceptably energy-consuming operations.
We formulate the constrained multi-objective Markov decision process (CMOMDP), which is similar to the constrained Markov decision process (CMDP) (Altman, 1999). The difference is that the objectives are multiple and constraints can be nonlinear. We aim to minimize the value of a preference function, which takes multiple objectives as input and outputs a scalar. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Most existing RL methods assume full accessibility of the environment for the agent, which tends to be impractical as the exploration could be expensive (Gottesman et al., 2019) and dangerous (Shalev-Shwartz et al., 2016). Hence, we consider the ofﬂine case where the agent has only a historical dataset collected a priori and has no further interactions with the environment. This setting is common and embodied in various scenarios such as healthcare (Chakraborty and Murphy, 2014) and auto-driving (Sun et al., 2020). However, ofﬂine RL is less understood theoretically (Levine et al., 2020) than online RL, and how to maximally exploit the dataset remains unclear.
In this paper, we study the ofﬂine CMOMDP. The challenges are threefold: (i) Different from CMDPs, the nonlinearity of the preference function and constraints of
CMOMDPs makes the analysis challenging. Moreover, constraints can be neither convex nor concave in the policy, which means the optimization problems are usually non-convex. It brings great difﬁculty to designing a provably efﬁcient algorithm. (ii) Since further interaction with the environment is prohibited, the dataset’s quality is not guaran-teed. The data collected by the experimenter may not sufﬁciently cover the trajectories induced by the optimal policy. Therefore, the information of the optimal policy could be limited, which probably makes the suboptimality and constraint violation arbitrarily large. (iii) As the CMOMDP can be considered a generalization of CMDP (see the reduction in Appendix
C), any difﬁculties emerging in the CMDP will arise here too. For example, due to constraints, the Bellman optimality equation may not hold anymore. Therefore, most existing ofﬂine RL algorithms based on dynamic programming are inapplicable.
Challenge (i) is inherent in CMOMDPs, and certain requirements on the preference function and constraints are inevitable. Hence, We suppose they satisfy some conditions. For instance, a geometric analog of Slater’s condition, which is usually assumed in CMDPs, is imposed. To tackle challenge (ii), existing works have put a great effort. One possible principle is pessimism, which applies penalties to ensure pessimistic estimation. This method is successful in ordinary RL, and we extend it to CMOMDPs. We also note that we only impose a minimal assumption on the ofﬂine dataset’s compliance in our analysis. For challenge (iii), we apply the convex conjugate and Fenchel’s duality to transform the problem into a primal-dual formulation.
In summary, our work answers the following question:
Is it possible to develop a provably efﬁcient ofﬂine algorithm for constrained multi-objective reinforcement learning with minimal assumptions on the dataset?
We propose the Pessimistic Dual Iteration (PEDI) algorithm. Theoretical contributions are as follows: (i) By transforming the original constrained optimization problem of CMOMDPs into a primal-dual formulation via convex conjugate and duality, we develop the algorithm for general CMOMDPs with ofﬂine dataset, which iterates in a dual gradient ascent manner. Then, we instantiate the algorithm for linear kernel CMOMDPs, which is a large class of CMOMDPs that includes the tabular case. (ii) We show in Appendix F the signiﬁcance of pessimism in ofﬂine CMOMDPs. To summarize, we decompose the discrepancy between a value function and the optimal one into spurious correlation, intrinsic uncertainty, and optimization error, among which the spurious correlation is the most difﬁcult to control. However, by maintaining pessimistic estimates of the value functions, PEDI eliminates the spurious correlation. (iii) We establish theoretical guarantees for PEDI. Speciﬁcally, we demonstrate that two metrics, the suboptimality and the constraint violation, can be bounded from above by the optimiza-K) and the intrinsic uncertainty, where K is the number of iterations. tion error of O(1/
When specialized to linear kernel CMOMDPs, the upper bound of suboptimality matches the information-theoretic lower bound up to the optimization error and multiplicative factors of constants related to the CMOMDP, which suggests the near-optimality of PEDI.
√ (iv) We show that the error of PEDI is data-dependent, i.e., it depends on how well the dataset covers the trajectories induced by the optimal policy. When the trajectories are assumed further to be sufﬁciently covered, the suboptimality and constraint violation are (cid:101)O(1/
N ) where N is the number of trajectories in the dataset.
√ 2
To the best of our knowledge, we are the ﬁrst to propose a provably efﬁcient ofﬂine algorithm that considers the constrained multi-objective RL without any assumptions on the coverage of the dataset.
As a by-product, our method can be viewed as a highly generalized one as it can easily reduce to certain simpler cases such as CMDPs, which is discussed in Appendix C. 1.1