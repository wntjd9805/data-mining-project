Abstract
A current remarkable improvement of unsupervised visual representation learning is based on heavy networks with large-batch training. While recent methods have greatly reduced the gap between supervised and unsupervised performance of deep models such as ResNet-50, this development has been relatively limited for small models. In this work, we propose a novel unsupervised learning framework for small networks that combines deep self-supervised representation learning and knowledge distillation within one-phase training. In particular, a teacher model is trained to produce consistent cluster assignments between different views of the same image. Simultaneously, a student model is encouraged to mimic the prediction of on-the-ﬂy self-supervised teacher. For effective knowledge transfer, we adopt the idea of domain classiﬁer so that student training is guided by discriminative features invariant to the representational space shift between teacher and student.
We also introduce a network driven multi-view generation paradigm to capture rich feature information contained in the network itself. Extensive experiments show that our student models surpass state-of-the-art ofﬂine distilled networks even from stronger self-supervised teachers as well as top-performing self-supervised models.
Notably, our ResNet-18, trained with ResNet-50 teacher, achieves 68.3% ImageNet
Top-1 accuracy on frozen feature linear evaluation, which is only 1.5% below the supervised baseline. 1

Introduction
Recently, there has been a growing attention in unsupervised and self-supervised learning where the goal is to effectively learn useful features from a large amount of unlabeled data. Current self-supervised visual representation learning methods appear to approach and possibly even outperform the fully-supervised counterpart [8, 25, 29]. All top-performing self-supervised learning (SSL) algorithms for visual representation involve training deep models on powerful computers. In particular, their smallest architecture is ResNet-50 [26], and the networks are trained with large batches (e.g., 4096 images) on multiple specialized hardware devices such as 128 TPU cores [6, 8, 25]. Yet, this heavy implementation is not a viable option in a resource-limited environment, and there is evidence that the SSL methods even do not work well on light models (Figure 1 and [22]). Also, we need strong small networks that can operate on a system on a chip (SoC) for real-world applications.
Existing methods on SSL have demonstrated that deeper models learn general visual representation more effectively with unlabeled data [8, 40]. Moreover, it has been empirically shown that predictive performance of bigger networks is better transferred into smaller ones [9]. Inspired by these, we are 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Method
Supervised
SimCLR [8]
MoCo-v2 [11]
BYOL [25]
BYOL-GA [25]
SwAV [6]
OSS (ours)
Pretrained Epochs 100 200 400 800 69.8 47.1 48.6 44.2 54.2 57.7 60.0 49.9 49.9 47.5 56.9 61.2 64.1 51.8 51.9 46.8 61.4 63.7 65.8 53.4 56.1 47.1 61.9 64.9 68.3
Figure 1: ImageNet Top-1 accuracy of linear probe on ResNet-18 representations obtained from various self-supervised methods with 256-batch training. BYOL-GA denotes the implementation of BYOL with gradient accumulation (to 4096 batches). Green dotted line indicates supervised
ResNet-18. Our method (red star) surpasses state-of-the-art self-supervised methods. interested in pretraining a light-weight model by distilling representational knowledge from a deep self-supervised network instead of directly performing self-supervised training on a small one.
Most of previous distillation methods in unsupervised representation learning literature are ofﬂine in the sense that they leverage an already trained self-supervised teacher to transfer feature information to students [22, 31, 40, 57]. Often, their sequential training pipelines require extra pre/post-processing steps in order to boost performance. Unlike these approaches, we propose a novel unsupervised representation learning framework for small networks that combines deep self-supervised training and knowledge distillation [30] within one-phase training. To be speciﬁc, our teacher learns clusters and representation, and at the same time the student is trained to align its prediction to the clustering of On-the-ﬂy Self-Supervised teacher. From this, we refer to our method as OSS. The main advantage over ofﬂine distillation is that our teacher better distills proper signals into the student at each training stage by solving the same task online. In distillation process, it is difﬁcult for a low-capacity student to perfectly mimic the large teacher’s behavior [36]. To deal with this, we incorporate the idea of domain classiﬁer [23]. That is, we add a feature classiﬁer trained not to be able to distinguish between teacher and student embeddings, and this leads to the emergence of representational-space-invariant features in the course of joint training.
There is evidence that increasing the number of different views improves the quality of resulting features in SSL [6, 8, 55]. Existing approaches to multi-view generation rely on random image transformation techniques. Different from these methods, we introduce a new network driven paradigm in order to utilize rich feature information contained in the network itself. In particular, we apply random dropout and empirically show that this apparently improves label efﬁciency (Section 5).
Our method is conceptually simple but surprisingly works well with typical 256-batch training on a single 8-GPU machine. Extensive experiments show that our student networks outperform state-of-the-art self-supervised models (Figure. 1) as well as students from top-performing distillation approaches even with stronger self-supervised teachers (Tables 1 and 5) on linear evaluation. We also evaluate learned representations on series of vision tasks with multiple network architectures and various benchmark datasets and demonstrate signiﬁcant performance gain (Tables 2, 3 and 4 and
Figure 3). Overall, we make the following contributions.
• We propose a novel unsupervised visual representation learning framework for small net-works that simultaneously conducts self-supervised teacher training and knowledge distilla-tion.
• We introduce a network driven view generation paradigm and an adversarial distillation scheme to better capture representational information from networks. This is the ﬁrst work to show their effectiveness in the context of unsupervised visual representation learning.
• The proposed method signiﬁcantly improves unsupervised pretraining performance of small models, and its effectiveness is shown under multiple settings of network architectures, datasets and vision tasks. 2
Figure 2: Overview of proposed architecture. OSS framework takes as input two randomly aug-mented views x1 and x2, and they are processed by teacher’s and student’s backbones fθ and gη.
The processed outputs are then projected to lower dimensional embeddings z and w via teacher’s and student’s projection heads hT and hS. We add dropout to each linear layer in both projection heads (hdrop’s) for generating additional views of embeddings ˜z and ˜w. We note that h and hdrop share weights. Network training is guided by cluster prediction mechanism, and the target cluster assignments are obtained by online clustering using the mapping of teacher’s feature embeddings z to prototype vectors C. To distill useful discriminative feature information invariant to the representa-tional space shift between teacher and student, we use a feature classiﬁer connected to the last layer of each projection head with a gradient reversal layer (GR). 2