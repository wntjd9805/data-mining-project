Abstract
Meta-reinforcement learning (meta-RL) has proven to be a successful framework for leveraging experience from prior tasks to rapidly learn new related tasks, how-ever, current meta-RL approaches struggle to learn in sparse reward environments.
Although existing meta-RL algorithms can learn strategies for adapting to new sparse reward tasks, the actual adaptation strategies are learned using hand-shaped reward functions, or require simple environments where random exploration is sufﬁcient to encounter sparse reward. In this paper, we present a formulation of hindsight relabeling for meta-RL, which relabels experience during meta-training to enable learning to learn entirely using sparse reward. We demonstrate the ef-fectiveness of our approach on a suite of challenging sparse reward goal-reaching environments that previously required dense reward during meta-training to solve.
Our approach solves these environments using the true sparse reward function, with performance comparable to training with a proxy dense reward function. 1

Introduction
Reinforcement learning (RL) has seen tremendous success applied to challenging games (Mnih et al., 2015; Silver et al., 2017) and robotic control (Lillicrap et al., 2015; Levine et al., 2016), driven by advances in compute and the use of deep neural networks as powerful function approximators in
RL algorithms. However, agents trained using deep RL often struggle to meaningfully utilize past experience to learn new tasks, even if the new tasks differ only slightly from tasks seen during training (Zhang et al., 2018; Packer et al., 2018; Cobbe et al., 2019). In contrast, humans are adept at utilizing prior experience to rapidly acquire new skills and adapt to unseen environments.
Meta-reinforcement learning (meta-RL) aims to address this limitation by extending the RL frame-work to explicitly consider structured distributions of tasks (Schmidhuber, 1987; Bengio et al., 1990;
Thrun & Pratt, 1998). Whereas conventional RL is concerned with learning a single task, meta-RL is concerned with learning to learn, that is, learning how to quickly learn a new task by leveraging prior experience on related tasks. Meta-RL methods generally utilize a limited amount of experience in a new environment to estimate a latent task embedding which conditions the policy, or to compute a policy gradient which is used to directly update the parameters of the policy.
A major challenge in both RL and meta-RL is learning with sparse rewards. When rewards are sparse or delayed, the adaptation stage in meta-RL becomes extremely difﬁcult: inferring the task at hand requires receiving reward signal from the environment, which in the sparse reward setting only happens after successfully completing the task. Due to this inherent incompatibility between meta-RL and sparse rewards, existing meta-RL algorithms that consider the sparse reward setting either only work in simple environments that do not require temporally-extended exploration strategies for adaptation (Duan et al., 2016; Stadie et al., 2018), or train exclusively using dense reward functions (Gupta et al., 2018b; Rakelly et al., 2019), which are designed to encourage an agent to learn adaptation strategies that can be directly applied to the original sparse reward setting. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Hindsight relabeling in goal-conditioned RL (b) Hindsight Task Relabeling (HTR) in meta-RL
Figure 1: In goal-conditioned RL (a), an agent must navigate to a provided goal location g (ﬁlled circle, revealed to the agent). An unsuccessful attempt for goal g provides no sparse reward signal, but can be relabelled as a successful attempt for goal g(cid:48), creating sparse reward that can be used to train the agent. In meta-RL (b), the task T (i.e., goal, hollow circle) is never revealed to the agent, and instead must be inferred using experience on prior tasks and limited experience (τ1:t−1) on the new task. In (b), there is no shared optimal task T (cid:48) to relabel all attempts with. HTR relabels each attempt τ under its own hindsight task T (cid:48), and modiﬁes the underlying meta-RL training loop to learn adaptation strategies on the relabelled tasks. Note that we include multiple trajectories τ in (b) vs a single trajectory in (a) to highlight the adaptation stage in meta-RL, which does not exist in goal-conditioned RL and requires signiﬁcantly different sampling and relabeling procedures.
In this paper, we show that the concept of hindsight relabeling (Andrychowicz et al., 2017) from conventional RL can be applied in meta-RL to enable learning to learn in the sparse reward setting.
Our key insight is that data collected on the true training tasks can be relabelled as pseudo-expert data for easier hindsight tasks, bootstrapping meta-training with the reward signal needed to train the agent. We introduce a new algorithm for off-policy meta-RL, which we call Hindsight Task
Relabeling (HTR), and demonstrate its effectiveness by achieving state-of-the-art performance on a collection of challenging sparse reward environments that previously required shaped reward to solve.
Not only is our approach able to learn adaptation strategies only using sparse reward, but it also learns strategies with comparable performance to existing approaches that use shaped reward functions. 2