Abstract
We can compress a rectiﬁer network while exactly preserving its underlying func-tionality with respect to a given input domain if some of its neurons are stable.
However, current approaches to determine the stability of neurons with Rectiﬁed
Linear Unit (ReLU) activations require solving or ﬁnding a good approximation to multiple discrete optimization problems. In this work, we introduce an algorithm based on solving a single optimization problem to identify all stable neurons. Our approach is on median 183 times faster than the state-of-art method on CIFAR-10, which allows us to explore exact compression on deeper (5 × 100) and wider (2 × 800) networks within minutes. For classiﬁers trained under an amount of (cid:96)1 regularization that does not worsen accuracy, we can remove up to 56% of the connections on CIFAR-10 dataset. The code is available at the following link, https://github.com/yuxwind/ExactCompression. 1

Introduction
For the past decade, the computing requirements associated with state-of-art machine learning models have grown faster than typical hardware improvements [5]. Although those requirements are often associated with training neural networks, they also translate into larger models, which are challenging to deploy in modest computational environments, such as in mobile devices.
Meanwhile, we have learned that the expressiveness of the models associated with neural networks— when measured in terms of their number of linear regions —grows polynomially on the number of neurons and occasionally exponentially on the network depth [69, 65, 73, 81, 36, 37]. Hence, we may wonder if the pressing need for larger models could not be countered by such gains in model complexity. Namely, if we could not represent the same model using a smaller neural network. More speciﬁcally, we consider the following deﬁnition of equivalence [49, 79]:
Deﬁnition 1. Two neural networks N1 and N2 with associated functions f1 : Rn0 → Rm and f2 : Rn0 → Rm are local equivalent with respect to a domain D ⊆ Rn0 if f1(x) = f2(x) ∀x ∈ D.
There is an extensive literature on methods for compressing neural networks [18, 11], which is aimed at obtaining smaller networks that are nearly as good as the original ones. These methods generally produce networks that are not equivalent, and thus require retraining the neural network for better accuracy. They may also lead to models in which the relative accuracy for some classes is more affected than that of other classes [43]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Compressing a neural network while preserving its associated function is a relatively less explored topic, which has been commonly referred to as lossless compression [79, 83]. However, that term has also been used for the more general case in which the overall accuracy of the compressed network is no worse than that of the original network regardless of equivalence [95]. Hence, we regard exact compression as a more appropriate term when equivalence is preserved.
Exact compression has distinct beneﬁts and challenges. On the one hand, there is no need for retraining and no risk of disproportionately affecting some classes more than others. On the other hand, optimization problems that are formulated for exact compression need to account for any valid input as opposed to relying on a sample of inputs. In this paper, we focus on how to scale such an approach to a point in which exact compression starts to become practical for certain applications.
In particular, we introduce and evaluate a faster algorithm for exact compression based on identifying all neurons with Rectiﬁed Linear Unit (ReLU) activation that have linear behavior, which are denoted as stable. In other words, those are the neurons for which the mapping of inputs to outputs is always characterized by a linear function, which is either the constant value 0 or the preactivation output. We can remove or merge such neurons—and even entire layers in some cases—while obtaining a smaller but equivalent neural network. Our main contributions are the following: (i) We propose the algorithm ISA (Identifying Stable Activations), which is based on solving a single Mixed-Integer Linear Programming (MILP) formulation to verify the stability of all neurons of a feedforward neural network with ReLU activations. ISA is faster than solving
MILP formulations for every neuron—either optimally [90] or approximately [79]. Compared to [79], the median improvement is of 83 times on MNIST dataset (183 times on CIFAR-10 dataset and 137 times on CIFAR-100 dataset) —and in fact greater in larger networks. (ii) We reduce the runtime with a GPU-based preprocessing step that identiﬁes neurons that are not stable with respect to the training set. The median improvement for that part alone is of 3.2 times on MNIST dataset. (iii) We outline and prove the correctness of a new compression algorithm, LEO++ (Lossless Expres-siveness Optimization, as in [79]), which leverages (i) to perform all compressions once per layer instead of once per stable neuron [79]. (iv) We leverage the scalability of our approach to investigate exact compressibility on classiﬁers that are deeper (5 × 100) and wider (2 × 800) than previously studied in [79] (2 × 100). We show that approximately 20% of the neurons and 40% of the connections can be removed from
MNIST classiﬁers trained with an amount of (cid:96)1 regularization that does not worsen accuracy. 2