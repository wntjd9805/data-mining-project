Abstract
Transformer has been widely used for self-supervised pre-training in Natural
Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised
Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Speciﬁcally, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4% and its comparable variant DINO by 1.0%. For dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object detection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch pre-training. 1

Introduction
As Yann LeCun said, “if intelligence is a cake, the bulk of the cake is unsupervised learning”. This sentence reﬂects that Un-/Self-supervised Learning played a central role in the resurgence of deep learning. Common approaches focus on designing different pretext tasks [10, 29, 14, 3, 4, 6, 5, 13, 1, 36] and aim to learn useful representations of the input data without relying on human annotations. It then uses those representations in downstream tasks, such as image classiﬁcation, objection detection, and semantic segmentation.
∗Work done as an intern at SenseTime Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In computer vision, previous methods focus on designing different pretext tasks. One of the most promising directions among them is contrastive learning/instance discrimination [17, 23], which regards each instance in the training dataset as a single category. Based on instance discrimination
[14, 4, 6, 5, 13, 1], some methods show the effectiveness in the image classiﬁcation task. They successfully bridge the performance gap between self-supervised and full-supervised methods.
However, almost all of self-supervised learning methods, which formulate the learning as image-level prediction using global features, are suboptimal in the pixel-level predictions [14, 1, 13], such as object detection and semantic segmentation. Also, InfoMin [35] ﬁnds that high-level features do not truly matter in transferring to dense prediction tasks. Here, current self-supervised learning may overﬁt to image classiﬁcation while not being well tamed for downstream tasks requiring dense prediction.
Meanwhile, large-scale pre-trained models have become the prevailing formula for a wide variety of Natural Language Processing (NLP) tasks due to its impressive empirical performance. These models typically abstract semantic information from massive unlabeled corpora in a self-supervised manner. The Masked Language Modeling (MLM) [10] has been widely utilized as the objective for pre-training language models. In the MLM setup, a certain percentage of tokens within the input sentence are randomly masked, and the objective is to predict the original information of the masked tokens based only on its context. In NLP tasks, we found that the different mask strategies used in the MLM framework had a great impact on the performance of the model. However, in the ﬁeld of vision, images have higher-dimensional, noisy, and redundant format compared to text. The main information of input images is randomly distributed in tokens. If tokens are randomly masked, it will lead to poor performance. Some of previous methods use random tokens, such as iGPT [3] and ViT
[11]. iGPT trains self-supervised Transformers using an amount of 6801M parameters and achieves 72.0% Top-1 accuracy on ImageNet by masking and reconstructing pixels, while ViT trains ViT-B model on the JFT-300M dataset, and the result is signiﬁcantly lower than the supervised model.
The random MLM is prone to mask the tokens of crucial region for images, resulting in misunder-standing, and is not suitable for directly applying to self-supervised vision Transformers. In order to avoid masking the tokens of crucial region, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of patches without damaging the crucial structure for self-supervised learning. Notably, the strategy would not increase the training time. Also, predicting original tokens alone may cause the model to over-emphasize local region, and therefore suppress the ability to recognize objects. Hence, in this paper, we present a novel Masked
Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. In addition, a global image decoder is further exploited to recover the spatial information of the image and is thus more friendly to the downstream dense prediction tasks.
We validate our method on multiple visual tasks. In particular, on the ImageNet linear evaluation protocol, we reach 76.9% top-1 accuracy with DeiT-S and achieve the state-of-the-art performance.
Overall, we make the following contributions:
• We propose a new masked self-supervised transformer approach called MST. It makes full use of self-attention map to guide the masking of local patches, thus enhancing the understanding of local context semantics in pre-training without damaging the crucial structure.
• Our method can effectively recover the spatial information of the image by a global image decoder, which is vital for the downstream dense prediction task and greatly improves the versatility and scalability of the pre-training model.
• Extensive experiments demonstrate the effectiveness and transfer ability of our method. Speciﬁ-cally, the results on ImageNet [9], MS COCO [18] and Cityscapes [8] show that our method outperforms previous state-of-the-art methods. 2