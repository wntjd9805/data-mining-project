Abstract
We consider running multiple instances of multi-armed bandit (MAB) problems in parallel. A main motivation for this study are online recommendation systems, in which each of N users is associated with a MAB problem and the goal is to exploit users’ similarity in order to learn users’ preferences to K items more efﬁciently. We consider the adversarial MAB setting, whereby an adversary is free to choose which user and which loss to present to the learner during the learning process. Users are in a social network and the learner is aided by a-priori knowledge of the strengths of the social links between all pairs of users. It is assumed that if the social link between two users is strong then they tend to share the same action. The regret is measured relative to an arbitrary function which maps users to actions. The smooth-ness of the function is captured by a resistance-based dispersion measure Ψ. We present two learning algorithms, GABA-I and GABA-II which exploit the network structure to bias towards functions of low Ψ values. We show that GABA-I has an expected regret bound of O((cid:112)ln(N K/Ψ)ΨKT ) and per-trial time complexity of O(K ln(N )), whilst GABA-II has a weaker O((cid:112)ln(N/Ψ) ln(N K/Ψ)ΨKT ) regret, but a better O(ln(K) ln(N )) per-trial time complexity. We highlight im-provements of both algorithms over running independent standard MABs across users. 1

Introduction
During the last decade multi-armed bandits (MAB) have received a great deal of attention in machine learning and related ﬁelds, due to their wide practical and theoretical importance. The central problem is to design a decision strategy whereby a learner explores sequentially the environment in order to
ﬁnd the best item (arm) within a prescribed set. At each step in the exploration the learner chooses an arm, after which feedback (typically a loss or reward corresponding to the selected arm) is observed from the environment. Then the next decision is made by the learner based on past interactions, and the process repeats. The goal is to design efﬁcient exploration strategies which incur a small cumulative loss in comparison to the cumulative loss that would have been obtained by always
* Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
selecting the best arm in hindsight. Applications of MAB are numerous, including recommender systems [1], clinical trials [2], and adaptive routing [3], among others.
In this paper we study the problem in which the learner is facing several MAB problems that are related according to a prescribed interaction graph. A main motivation behind this problem are online recommendation systems, whereby each of several users is associated with a MAB problem (task), where the arms correspond to a ﬁnite set of products, and the graph represents a social network among users. The goal is to exploit users’ similarity in order to improve the efﬁciency of learning users’ preferences via online exploration of products. In the standard full information setting, a lot of work has been done showing that techniques from multitask learning are effective in reducing the amount of data needed to learn each of the individual tasks, both in the statistical and adversarial settings, see
[4, 5, 6, 7, 8, 9, 10, 11] and references therein. Graphs have been used to model task relationships, with different tasks’ parameters encouraged to be close according to the graph topology. In contrast, multitask learning in the bandit setting is much less explored.
The algorithms that we present exploit the network homophily principle [12] which formulates that users that are connected in the network have similar preferences, that is, they tend to share preferred recommendations. We will show that our algorithms exploit graph structure and enjoy potentially much smaller regret bounds than the cumulative regret of standard MAB run independently on each user. Since the original graph may be dense, we exploit a randomized sparsiﬁcation technique to develop fast prediction algorithms. Our approach builds upon previous work on online learning over graphs [13, 14] to generate a perfect full oriented binary tree, whose leaves are in one-to-one correspondence with the nodes of the original graph. This construction approximately preserves the relevant graph properties in expectation, and provides the starting point for designing our efﬁcient algorithms. A further ingredient in our algorithm is provided by the method of specialists [15, 16].
Our learning strategies combine the above ingredients to devise efﬁcient online algorithms under partial feedback.
Contributions. We introduce two Gang of Adversarial BAndit algorithms, GABA-I and GABA-II that learn jointly MAB models for N users over K possible actions. Both algorithms are designed to exploit network structure while being (extremely) computationally efﬁcient. We de-rive expected (over the algorithms’ randomizations) regret bounds. The bounds scale with the dispersion measure Ψ ∈ [1, N ] of the best actions over the graph. For GABA-I the bound1 is of order of O((cid:112)ln(N K/Ψ)ΨKT ), where T is the number of trials, and has a per-trial time complexity of O(K ln(N )). On the other hand GABA-II has a weaker expected regret bound of O((cid:112)ln(N/Ψ) ln(N K/Ψ)ΨKT ) but is faster, having a per-trial time complexity of
O(ln(K) ln(N )). Thus the GABA-I algorithm improves on algorithms that treat each user in-ln N ) and in the worst dependently, as in the best case the regret improves from O( case the regret degrades by at most a constant factor. GABA-II has slightly weaker regret bounds; however, it is more computationally efﬁcient.
N ) to O(
√
√
Outline of Main Results. The social network graph G is determined by a set of undirected links between users {ωu,v}N u<v where ωu,v ∈ [0, ∞) indicates the magnitude of the link between user u and v. For all t ∈ [T ] we have a user ut ∈ [N ] and a loss vector (cid:96)t ∈ [0, 1]K which are selected by
Nature before learning begins and are unknown to Learner; i.e., Nature is a deterministic oblivious adversary (see e.g., [17, Section 5.1]). Learning then proceeds in trials t = 1, 2, . . . , T . On trial t: 1. Nature reveals user ut ∈ [N ] to Learner, 2. Learner selects action at ∈ [K], 3. Nature reveals loss (cid:96)t,at ∈ [0, 1] to Learner.
Before reﬂecting on the N -user case we review the well-known results for the single user (N = 1).
The seminal EXP3 algorithm [18] obtains the following (uniform) regret bound2 , (cid:104) (cid:88)
E (cid:105) (cid:96)t,at t∈[T ]
− min a∈[K] (cid:88) t∈[T ] (cid:96)t,a ∈ O (cid:16)(cid:112)K ln(K)T (cid:17)
, (1) 1The bounds of GABA-I and GABA-II however depend on oracular knowledge of optimal tuning parameters.
We discuss this as well as a means of lessening this dependency following Corollary 5. 2An algorithm was given in [19] that removed the ln K term from the regret. 2
where the expectation is with respect to the internal randomization of the EXP3 algorithm. In the
N -user setting, by running a copy of EXP3 independently for each user, we may obtain a uniform regret bound of (see e.g., [20]) (cid:105) (cid:17) (cid:96)t,y(ut) ∈ O (cid:16)(cid:112)K ln(K)N T
, (cid:104) (cid:88)
E t∈[T ] (cid:96)t,at
− min y:[N ]→[K] (cid:88) t∈[T ] (2)
√ i.e., for each user u the best action is y(u) and we now pay an additional constant factor of
N in our regret. In this work we exploit the social network structure to prove a non-uniform regret bound for the GABA-I algorithm (see Corollary 4) of
R(y) := E (cid:104) (cid:88) (cid:105)
− (cid:96)t,at (cid:88) t∈[T ] t∈[T ] (cid:32)(cid:115) (cid:96)t,y(ut) ∈ O
K ln (cid:19) (cid:18) KN
Ψ(y) (cid:33)
Ψ(y)T
, (3) (cid:113) ln( eN for any mapping of users to actions y : [N ] → [K]. The non-uniform regret now depends on
Ψ(y) ∈ [1, N ] (see (5)) which measures dispersion of users’ ‘best’ actions across the network.
Thus, by taking network structure into account, we may upper bound the scaling in the regret with respect to the number of users by O(
Ψ(y) )Ψ(y)). When the best action across the network is nearly uniform then the dispersion Ψ(y) ∈ O(1), in contrast when the dispersion is maximal then Ψ(y) = N thus in the best case the regret with respect to the number of users improves from
O( ln N ) and in the worst case the regret only increases by a constant factor. The
ﬁrst algorithm GABA-I obtains the regret (3) while requiring O(K ln N ) time to predict and update.
The second algorithm GABA-II’s regret (see Corollary 5) is larger by a O((cid:112)ln N/Ψ(y)) factor but now prediction is at an even faster O(ln(K) ln(N )) time per trial, that is, prediction time improves exponentially with respect to the cardinality of the action set [K]. Thus both algorithms support very large user networks and the second algorithm allows efﬁcient prediction with very large action sets.
N ) to O(
√
√