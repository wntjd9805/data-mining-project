Abstract
Natural language instructions for visual navigation often use scene descriptions (e.g., ‘bedroom’) and object references (e.g., ‘green chairs’) to provide a bread-crumb trail to a goal location. This work presents a transformer-based vision-and-language navigation (VLN) agent that uses two different visual encoders – a scene classiﬁcation network and an object detector – which produce features that match these two distinct types of visual cues. In our method, scene features contribute high-level contextual information that supports object-level processing. With this design, our model is able to use vision-and-language pretraining (i.e., learning the alignment between images and text from large-scale web data) to substantially improve performance on the Room-to-Room (R2R) [1] and Room-Across-Room (RxR) [2] benchmarks. Speciﬁcally, our approach leads to improvements of 1.8% absolute in SPL on R2R and 3.7% absolute in SR on RxR. Our analysis reveals even larger gains for navigation instructions that contain six or more object references, which further suggests that our approach is better able to use object features and align them to references in the instructions. 1

Introduction
The vision-and-language navigation (VLN) task [1] requires an agent to follow a path through an environment that is speciﬁed by natural language navigation instructions. A central component of this task is associating (or grounding) the instruction to visual landmarks in the environment. Figure 1 provides an illustrative example from the Room-to-Room (R2R) dataset [1]: ‘Exit the bedroom and turn left. Continue down the hall and into the room straight ahead and stop before the desk with two green chairs.’ The visual landmarks in this instruction include scene descriptions (e.g., ‘bedroom’ and ‘hall’) and speciﬁc object references (e.g., ‘desk’ and ‘two green chairs’). To be successful, a
VLN agent should be able to (a) recognize and (b) ground both types of visual cues.
Learning the appropriate grounding between referring expressions in an instruction and the corre-sponding visual regions is difﬁcult in VLN due to the limited visual diversity seen in training. For example, the Room-to-Room (R2R) [1] and Room-Across-Room (RxR) [2] datasets only use 61 unique training environments, so models simply cannot learn about the long-tail of visual cues that appear in new testing (or validation) scenes. To address this challenge, recent work has shown the promise of transferring visual grounding with multimodal representation models that are pretrained on a large amount of image-text web data before ﬁnetuning on the embodied VLN task [3, 4]. In this work, we build on this general approach.
For visual recognition, most VLN methods [1, 4–14] ﬁrst encode observations with a convolutional network that was trained to solve an image-level classiﬁcation task – either using ImageNet [15] or the Places [16] scene recognition dataset. While ImageNet features may identify objects mentioned in the instructions and Places features might match the scene descriptions, neither solution was
˚Correspondence to abhinavmoudgil95@gmail.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustrative VLN Instruction. Visual landmarks in the instruction include scene descriptions (e.g., ‘bedroom’ and ‘hall’) and object references (e.g., ‘desk’ and ‘green chairs’). Unlike most previous methods, our model uses both scene (
) to match the visual references in the instructions. We employ a novel attention mechanism and view aggregation strategy to select the most relevant features for action prediction.
) and object features ( explicitly trained to recognize both types of visual cues, which is a limitation that we address in this paper. Furthermore, pretrained multimodal representation models have typically been trained with features from object detection networks rather than the image-level representations commonly used in VLN. Despite this, prior work leveraging these models [4] has continued to use the standard set of image-level features – leading to a signiﬁcant domain shift between pretraining and VLN ﬁnetuning.
In this work, we address these issues 1) by using multiple visual encoders to explicitly encode the inductive bias that the world is composed of objects and scenes and 2) by using object-level features from a detection network that match the features used to pretrained multimodal representation models.
The starting point for our work is a VLNœBERT [4] agent that processes scene-level features from a
Places [16] CNN with a transformer-based [17] multimodal representation model that is modiﬁed with a recurrence mechanism designed for the VLN task. Our work extends this approach by including object features as an additional input to the multimodal processing. However, we ﬁnd that simply adding object features to VLNœBERT does not improve (but rather slightly reduces) performance.
Thus, we propose architectural changes that allow the model to take better advantage of these two distinct types of visual information. Speciﬁcally, we change the attention pattern within the the transformer to effectively freeze the scene representations and focus the processing on the object-level inputs. The result is a new VLN agent that produces contextualized object representations by using scene features as high-level contextual cues.
We experiment with our proposed approach on the Room-to-Room (R2R) [1] and Room-Across-Room (RxR) [2] datasets. Empirically, we ﬁnd that our model substantially improves VLN performance over our VLNœBERT baseline on R2R and outperforms state-of-the-art methods on English language instructions in RxR. Speciﬁcally, our proposed approach improves success weighted by path length (SPL) on the unseen validation split in R2R by 1.8 absolute percentage points. On RxR – a more challenging dataset due to indirect paths and greater variations in path length – we see even larger improvements. Success rate (SR) improves by 3.7 absolute percentage points, alongside a gain of 2.4 absolute percentage points on the normalized dynamic time warping (NDTW) metric. Through ablation experiments we ﬁnd that (consistent with the observations in [3]) vision-and-language pretraining is vital to our approach, which suggests that strong visual grounding is key for using object-level features in VLN. Additionally, on RxR instructions that include six or more object references (i.e., object-heavy instructions), our method has even larger improvements over VLNœBERT of 7.9 absolute percentage points in SR.
To summarize, we make the following contributions:
• We propose a scene- and object-aware transformer (SOAT) model for vision-and-language navigation that uses both scene-level and object-level features – explicitly encoding the inductive bias that the world is composed of objects and scenes. Our model uses a novel at-tention masking technique and view aggregation strategy, which both improve performance. 2
• We demonstrate that our approach outperforms a strong baseline approach by 1.8 absolute percentage points in SPL on R2R and by 3.7 absolute percentage points in SR on RxR.
• We show that our method has signiﬁcantly stronger performance on instructions that mention six or more objects (7.9 absolute percentage points of improvement in SR), which further suggests that our model is better able to recognize and ground object references. 2