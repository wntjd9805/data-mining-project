Abstract
We propose a novel end-to-end solution for video instance segmentation (VIS) based on transformers. Recently, the per-clip pipeline shows superior performance over per-frame methods leveraging richer information from multiple frames. How-ever, previous per-clip models require heavy computation and memory usage to achieve frame-to-frame communications, limiting practicality. In this work, we pro-pose Inter-frame Communication Transformers (IFC), which signiﬁcantly reduces the overhead for information-passing between frames by efﬁciently encoding the context within the input clip. Speciﬁcally, we propose to utilize concise memory tokens as a means of conveying information as well as summarizing each frame scene. The features of each frame are enriched and correlated with other frames through exchange of information between the precisely encoded memory tokens.
We validate our method on the latest benchmark sets and achieved state-of-the-art performance (AP 42.6 on YouTube-VIS 2019 val set using the ofﬂine inference) while having a considerably fast runtime (89.4 FPS). Our method can also be applied to near-online inference for processing a video in real-time with only a small delay. The code is available at https://github.com/sukjunhwang/IFC. 1

Introduction
With the growing interest toward the video domain in computer vision, the task of video instance segmentation (VIS) is emerging [1]. Most of the current approaches [1, 2, 3, 4] extend image instance segmentation models [5, 6, 7, 8] and take frame-wise inputs. These per-frame methods extend the concept of temporal tracking by matching frame-wise predictions of high similarities. The models can be easily customized to real-world applications as they run in an online [9] fashion, but they show limitations in dealing with occlusions and motion blur that are common in videos.
On the contrary, per-clip models are designed to overcome such challenges by incorporating multiple frames while sacriﬁcing the efﬁciency. Previous per-clip approaches [10, 11, 12] aggregate informa-tion within a clip to generate instance-speciﬁc features. As the features are generated per instance, the number of instances in addition to the number of frames has a signiﬁcant impact on the overall computation. Recently proposed VisTR [11] adapted DETR [13] to the VIS task and reduced the inference time by inserting the entire video, not a clip, to its ofﬂine end-to-end network. However, its full self-attention transformers [14] over the space-time inputs involve explosive computations and memories. In this work, we raise the following question: can a per-clip method be efﬁcient while attaining great accuracy?
To achieve our goal, we introduce Inter-frame Communication Transformers (IFC) to greatly reduce the computations of the full space-time transformers. Similar to recent works [15, 16, 17] that alleviate the explosive computational growth inherent in attention-based models [14, 18], IFC takes a decomposition strategy utilizing two transformers. The ﬁrst transformer (Encode-Receive, E) encodes 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
each frame independently. To exchange the information between frames, the second transformer (Gather-Communicate, G) executes attention between a small number of memory tokens that hold concise information of the clip. The memory tokens are utilized to store the overall context of the clip, for example “a hand over a lizard” in Fig. 1. The concise information assists detecting the lizard that is largely occluded by the hand in the ﬁrst frame, without employing an expensive pixel-level attention over space and time. The memory tokens are only in charge of the communications between frames, and the features of each frame are enriched and correlated through the memory tokens.
We further reduce overheads while taking advantage of per-clip pipelines by concisely representing each instance with a unique convolutional weight [7]. Despite the changes of appearances at different frames, the instances of the same identity share commonalities because the frames originated from the same source video. Therefore, we can effectively capture instance-speciﬁc characteristics in a clip with dynamically generated convolutional weights. In companion with the segmentation, we track instances by uniformly applying the weights to all frames in a clip. Moreover, all executions of our spatial decoder are instance-agnostic except for the ﬁnal layer which applies instance-speciﬁc weights.
Accordingly, our model is highly efﬁcient and also suitable for scenes with numerous instances.
In addition to the efﬁcient modeling, we provide optimizations and an instance tracking algorithm that are designed to be VIS-centric. By the deﬁnition of APVIS, the VIS task [1] aims to maximize the objective similarity: space-time mask IoU. Inspired by previous works [13, 19, 20], our model is optimized to maximize the similarity between bipartitely matched pairs of ground truth masks and predicted masks. Furthermore, we again adopt the similarity maximization for tracking instances of same identities, which effectively links predicted space-time masks using bipartite matching. As both of our training and inference algorithms are fundamentally designed to address the key challenge of
VIS task, our method attains an outstanding accuracy.
From these improvements, IFC sets the new state-of-the-art: 42.6% AP and more surprisingly, in 89.4 fps. Furthermore, our model also shows great speed-accuracy balance under near-online settings, which leads to a huge practicality. We believe that our model can be a powerful baseline for video instance segmentation approaches that follow the per-clip execution. 2