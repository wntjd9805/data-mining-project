Abstract
Policy gradient (PG) gives rise to a rich class of reinforcement learning (RL) methods. Recently, there has been an emerging trend to accelerate the existing PG methods such as REINFORCE by the variance reduction techniques. However, all existing variance-reduced PG methods heavily rely on an uncheckable importance weight assumption made for every single iteration of the algorithms.
In this paper, a simple gradient truncation mechanism is proposed to address this issue.
Moreover, we design a Truncated Stochastic Incremental Variance-Reduced Policy
Gradient (TSIVR-PG) method, which is able to maximize not only a cumulative sum of rewards but also a general utility function over a policy’s long-term visiting distribution. We show an ˜O((cid:15)−3) sample complexity for TSIVR-PG to ﬁnd an (cid:15)-stationary policy. By assuming the overparameterization of policy and exploiting the hidden convexity of the problem, we further show that TSIVR-PG converges to global (cid:15)-optimal policy with ˜O((cid:15)−2) samples. 1

Introduction
In this paper, we investigate the theoretical properties of Policy Gradient (PG) methods for Rein-forcement Learning (RL) [43]. In view of RL as a policy optimization problem, the PG method 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
parameterizes the policy function and conduct gradient ascent search to improve the policy. In this paper, we consider the soft-max policy parameterization
πθ(a|s) = exp{ψ(s, a; θ)} a(cid:48) exp{ψ(s, a(cid:48); θ)} (cid:80) (1) where (s, a) is a state-action pair and ψ is some smooth function. Potentially, one can set the function
ψ to be some deep neural network with weights θ and input (s, a). The main problem considered in this paper is the policy optimization for a general utility function: max
θ
R(πθ) := F (λπθ ), (2) where F is a general smooth function, and λπθ denotes the unnormalized state-action occupancy measure (also referred to as the visitation measure). For any policy π and initial state distribution ξ,
λπ(s, a) :=
+∞ (cid:88) t=0
γt · P (cid:16) st = s, at = a (cid:12) (cid:12) π, s0 ∼ ξ (cid:17)
, (3) where γ stands for the discount factor and P denotes the probability of a certain event. When F is linear, the problem reduces to the standard policy optimization problem where the objective is to maximize a cumulative sum of rewards. When F is nonlinear, problem (2) goes beyond standard
Markov decision problems: examples include the max-entropy exploration [16], risk-sensitive RL
[50], certain set constrained RL [27], and so on.
In the standard cumulative-return case (i.e., F is linear), numerous works have studied PG methods in various scenarios, see e.g. [47, 5, 58, 22, 21, 37, 38, 23]. When directly optimizing over the policy space without any parameterization, the policy mirror descent (PMD) method [23] achieves an ˜O((cid:15)−2) sample complexity to ﬁnd an O((cid:15))-optimal solution. However, for the more practical parameterized policy optimization, to the authors’ best knowledge, the most recent variant of PG methods, using the SARAH/Spider stochastic variance reduction technique [13, 29], ﬁnd a local (cid:15)-stationary policy using O((cid:15)−3) samples [48, 33]. This poses a contrast with the known ˜O((cid:15)−2) sample complexity results that can be achieved by various value-based methods [4, 42, 41] and are provably matching information-theoretic lower bounds [12, 3, 4]. In this paper, we attempt to close this gap and prove an ˜O((cid:15)−2) sample complexity bound for a PG method. Most importantly, when it comes to PG estimation, the application of the variance reduction technique typically relies on certain off-policy PG estimator, resulting in the difﬁculty of distribution shift. We notice that none of the existing variance-reduced PG methods attempt to address this challenge. Instead, they directly make an uncheckable assumption that the variance of the importance weight is bounded for every policy pair encountered in running the algorithm, see e.g. [31, 48, 49, 33]. In this paper, we propose a simple gradient truncation mechanism to ﬁx this issue.
Next, let us go beyond cumulative return and consider policy optimization for a general utility where
F may be nonlinear. However, much less is known in this setting. The nonlinearity of F invalidates the concept of Q-function and value function, leading to the failure of policy gradient theorem [44].
To overcome such difﬁculty, [51] showed that the policy gradient for the general utilities is the solution to a min-max problem. However, estimating a single PG is highly nontrivial in this case. It is still unclear how to make PG methods to use samples in a most efﬁcient way.
In this paper, we aim to investigate the convergence and sample efﬁciency of the PG method, using episodic sampling, for both linear F (i.e., cumulative rewards) and nonlinear F (i.e., general utility).
Observe that problem (2) is an instance of the Stochastic Composite Optimization (SCO) problem
[45, 46]: minx f (Eν[gν(x)]), which involves an inner expectation that corresponds to the occupancy measure λπ. Motivated by this view point, we attempt to develop stochastic policy gradient method with provable ﬁnite-sample efﬁciency bounds.
Main results. Our main results are summarized below.
• We propose the TSIVR-PG algorithm to solve problem (2) via episodic sampling. It provides a conceptually simple stochastic gradient approach for solving general utility RL.
• We provide a gradient truncation mechanism to address the distribution shift difﬁculty in variance-reduced PG methods. Such difﬁculty has never been addressed in previous works. 2
• We show that TSIVR-PG ﬁnds an (cid:15)-stationary policy using ˜O((cid:15)−3) samples if F and ψ are general smooth functions. When F is concave and ψ satisﬁes certain overparameterization condition, we show that TSIVR-PG obtains a gloal (cid:15)-optimal policy using ˜O((cid:15)−2) samples.
Technical contribution. Our analysis technique is also of independent interest in the relating areas.
• For stochastic composite optimization (SCO), most existing algorithms require estimating the Jacobian matrix of the inner mapping, which corresponds to ∇θλπθ in our setting. This is in practice prohibitive if the Jacobian matrix has high dimensions, which is exactly the case in our problem. Unlike SCO algorithms such as [24, 54, 53, etc.], our analysis enables us to avoid the Jacobian matrix estimation.
• For the stochastic variance-reduced gradient methods, our analysis implies a convergence of SARAH/Spider methods to global optimality and a new O((cid:15)−2) sample complexity for nonconvex problems with “hidden convexity” structure, which has not been studied in the optimization community yet. 2