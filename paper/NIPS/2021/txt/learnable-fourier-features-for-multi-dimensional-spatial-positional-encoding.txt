Abstract
Attentional mechanisms are order-invariant. Positional encoding is a crucial com-ponent to allow attention-based deep model architectures such as Transformer to address sequences or images where the position of information matters. In this paper, we propose a novel positional encoding method based on learnable Fourier features. Instead of hard-coding each position as a token or a vector, we represent each position, which can be multi-dimensional, as a trainable encoding based on learnable Fourier feature mapping, modulated with a multi-layer perceptron.
The representation is particularly advantageous for a spatial multi-dimensional position, e.g., pixel positions on an image, where L2 distances or more complex positional relationships need to be captured. Our experiments based on several public benchmark tasks show that our learnable Fourier feature representation for multi-dimensional positional encoding outperforms existing methods by both improving the accuracy and allowing faster convergence. 1

Introduction
Attentional mechanisms are a central component in many deep architectures [1, 25], which allow a model to selectively focus on speciﬁc information in the context. Transformer [38] and its many variants, such as [29, 38, 16, 3], which are solely based on attentional mechanisms, have advanced the state of the art on many tasks that involve data with inherent temporal and spatial orders, e.g., machine translation [38], image generation [16], and object detection [3].
In contrast to recurrent [14, 34, 27] or convolutional architectures [18], which automatically capture the ordinal information as computation progresses based on sequential or spatial dependencies, attentional mechanisms are order invariant. It allows a model to directly access information at an arbitrary position in a sequence or space. The lack of ordinal information in the model is not an issue when attentional mechanisms are combined with a recurrent or convolutional architecture [1, 25].
However, it is crucial for Transformer-alike models where the entire model is built based on attentional mechanisms.
To capture positional information in the data, e.g., the token position in a sentence or the pixel coordinates in an image, positional encoding has been introduced [10, 38], where a position in a
∗Currently at Apple. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
one or two-dimensional space is mapped to a vector space by either learning or heuristics-based approaches. The representation of an input, by combining both its positional encoding and content representation, e.g., word embeddings, then participates in downstream computation for attentional mechanisms. The original Transformer model uses a ﬁxed sinusoidal encoding with predeﬁned wavelengths [38]. However, the predeﬁned features lack ﬂexibility and may not capture important position information in a task-dependent manner. To encode positions in a more ﬂexible and data-driven way, position embedding approaches (e.g., one used in BERT [8]) introduce trainable embedding vectors for each (absolute or relative) position. Unfortunately, this data-driven approach comes at the cost of introducing a large amount of extra learnable parameters proportional to sequence lengths times the hidden dimension size. Moreover, it is non-trivial to apply position embedding to problems with variable sequence lengths.
In this paper, we consider the problem of designing a position encoding for multi-dimensional spatial positions, such as pixel positions in an image or object bounding boxes in a spatial structure such as
UIs. Existing methods typically use sinusoidal position encoding with hand-crafted frequencies or learned embedding to encode each dimension independently and then combine the resulting vector representations via concatenation, e.g., [29, 3, 9]. Unfortunately, these approaches, by concatenating the representation of each dimension, are not effective to capture desired positional similarity on an image, such as L2 distance or more complex positional relationships. While embedding-based approaches have the potential to learn complex positional relationships, since the number of unique positions grows exponentially to the input dimension, the approach incurs large overhead in 2D and could be infeasible scaling to a higher dimensional space. In addition, special treatments are needed to adjust the learned position embedding when the test image sizes differ from training, such as bicubic interpolation used in DeiT [37] or Vision Transformer [9]. To avoid these special adjustments, it is an important for positional encoding to handle unseen positions.
The main contributions of our work are as follows. We design a novel positional encoding method that learns a function to map multi-dimensional positions into a vector space. The function extracts position information based on a set of Fourier features and passing them to an MLP. The encoding function is learnable and is initialized in such a way that the inner products of our positional encodings approximate Euclidean distances. The inductive bias can be desirable in a 2D or higher-dimensional space and by learning from the data, the representation can be adapted to a speciﬁc problem. Since our method learns an encoding function instead of embedding vectors for each position, it is naturally inductive and can handle test samples with arbitrary length. Our method is parameter-efﬁcient, in the sense that the number of parameters do not grow with sequence length. To allow complex positional relationships, our representation is also composable by encoding each subset of dimensions, in a multi-dimensional space, using a shared learnable Fourier features. We evaluate our method on a number of tasks where Transformer-based models have been used for problems with multi-dimensional positions, including image generation [16], object detection [3] and image classiﬁcation [9], which all involve 2D positions (vertical and horizontal) in images. We also evaluate our method on natural language generation in graphical user interfaces, which involve modeling a sparse spatial structure of UI objects on the screen, where each object is characterized by 4-coordinate values (top, left, bottom, and right) [20]. These experiments show that our positional encoding method consistently outperforms existing methods by both improving accuracy and accelerating learning. 2