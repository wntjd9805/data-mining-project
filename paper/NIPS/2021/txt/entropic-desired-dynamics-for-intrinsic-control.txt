Abstract
An agent might be said, informally, to have mastery of its environment when it has maximised the effective number of states it can reliably reach. In practice, this often means maximizing the number of latent codes that can be discriminated from future states under some short time horizon (e.g. [15]). By situating these latent codes in a globally consistent coordinate system, we show that agents can reliably reach more states in the long term while still optimizing a local objective. A simple instantiation of this idea, Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT), assumes ﬁxed additive latent dynamics, which results in tractable learning and an interpretable latent space. Compared to prior methods, EDDICT’s globally consistent codes allow it to be far more exploratory, as demonstrated by improved state coverage and increased unsupervised performance on hard exploration games such as Montezuma’s Revenge. 1

Introduction
Endowing reinforcement learning agents with the ability to learn effectively from unsupervised interaction with the environment, i.e. without access to an extrinsic reward signal, has the potential to make reinforcement learning practical in settings where the tasks the agent will face are initially unknown or where task feedback is expensive. The natural question is: what should the agent learn in the absence of extrinsic rewards? One appealing guiding principle is maximizing the number of states the agent can reach and to which it can reliably return.
Intrinsic control methods have shown promise in this direction. By maximizing the mutual information between a latent code z and future states reached by a policy conditioned on this code, intrinsic control methods learn to map latent codes to behaviors from which the code can be inferred. One major limitation of such approaches is that the latent codes z are usually sampled from a ﬁxed prior distribution p(z). Using a ﬁxed prior means that such approaches are unable to learn codes that correspond to states that cannot be reached in the time horizon T , since any code can be sampled in any state. Simply increasing the time horizon T does not solve the problem since it leads to a sparser learning signal. Learning a state-dependent prior has proven to be difﬁcult and has been shown to lead to fewer learned codes/goal states [15]. This inability to learn how to reach distant states limits the usefulness of such intrinsic control approaches.
We propose to sidestep this limitation by replacing the ﬁxed code distribution p(z) with a ﬁxed dynamics model over codes p(zt|zt−1). Our algorithm, Entropic Desired Dynamics for Intrinsic
ConTrol (EDDICT), learns to map sequences of latent codes sampled from this dynamics model to behaviors for which the state transition dynamics in the environment match the latent code dynamics.
EDDICT learns to map each zt to a state that is reachable from the state corresponding to zt−1,
∗Correspondence to stevenhansen@deepmind.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
z0
τ0 z1
τ1
τ0 z1
τ1 z0
τ0 z1
τ1 z0
τ0 z1
τ1 (a) (b) (c) (d)
Figure 1: Graphical models for various priors and posteriors of interest. Circles denote random variables which are observed (shaded) or latent (white), with diamonds denoting deterministic quantities. (a) Prior over a particular trajectory consisting of two sub-trajectories {τ0, τ1}, and auxiliary variables {z0, z1}. (b) Posterior inference with independent codes, as in prior work. (c)
Naive posterior inference for the sub-trajectory {z1, τ1}, conditioned on the past. (d) Posterior inference with hindsight. Despite z0 being observed, we infer z1 based on the most likely code z0 to have generated τ0, using the variational reverse predictor (dashed line). allowing it to reach states much farther than the time horizon T using sequences of codes z. We show that even highly constrained latent dynamics (i.e. additive noise) are sufﬁcient to both interpret latent codes in terms of their corresponding locations in state space, and encourage exploratory behavior to a far greater extent when compared to prior methods. 2 Notation
Our environment is a special case of a Markov Decision Process (MDP) without rewards or terminal signals: M : (S, A, P, P0). S is the state space, A is the action space, P (st+1 | st, at) the conditional distribution representing the state transition dynamics when taking action at ∈ A from state st ∈ S and P0(s) the initial state distribution. For simplicity, we present our method in the episodic setting with episodes of length T = M K, but relax this assumption in practice. Agents interact with the environment according to a policy πθ(a | s) with parameters θ, yielding trajectories
τ = [s0, a0, s1, a1, · · · sT ], and distributed as pπθ (τ ) = P0(s0) (cid:81)T −1
It will be useful for us to segment a given trajectory τ into sub-trajectories of length K, with
τ = [s0, τ0, τ1, · · · ], and τi = [aiK, siK+1, aiK+1, · · · a(i+1)K−1, s(i+1)K]. Note that τi is deﬁned to include s(i+1)K, but not the state siK from which aiK was sampled. With a slight abuse of notation and denoting τ−1 := s0, we rewrite pπθ (τ ) = P0(s0) (cid:81)M −1 i=0 pπ(τi | τi−1), with M the number of sub-trajectories per episode and pπθ (τi | τi−1) = (cid:81)(i+1)K−1
Hierarchical agents sample a high-level goal or latent variable z ∼ p(z) every K steps, and interact with the environment via a parametric conditional low-level policy πθ(a | s, z), which can be thought of as a ﬁxed duration option [42]. Composing p(z), πθ(a | s, z) and transition dynamics yields an augmented trajectory Λ = [s0, z0, τ0, z1, τ1, · · · ] 3, whose distribution decomposes as pπθ (Λ) =
P0(s0) (cid:81)M i=0 p(zi)pπθ (τi | τi−1, zi), with pπθ (τi | τi−1, zi) deﬁned analogously to pπθ (τi | τi−1) with conditional policy πθ(at | st, zi). t=0 P (st+1 | st, at)πθ(at | st).
P (st+1 | st, at)πθ(at | st). 2 t=iK
To simplify exposition, we index sequences at the timescale of sub-trajectories using index i, e.g.
[zi, τi, zi+1, τi+1, · · · ] and reserve index t for indexing sequences at the granular timescale of actions, e.g. [st, at, st+1, at+1, · · · ]. Concretely, indexing by i should be interpreted as iK, as in si := siK.
For a general sequence x = [x0, x1, · · · ], we deﬁne x<t := [x0, · · · , xt−1] and extend this notation to augmented trajectories as follows: Λ<i := [s0, z0, τ0, · · · , zi−1, τi−1]. 3 Method
We would like to learn goal directed agents, which are capable of reaching any state s ∈ S, given a goal or state embedding z ∈ Z. Extending earlier work on empowerment [40, 34], Variational 2The dependence on τi−1 is thus due to siK ∈ τi−1. 3We avoid introducing new notation for the corresponding semi-MDP, as our present notation allows us to reason about sub-trajectories, for both standard policies π(a | s) and conditional policies π(a | s, z). 2
Intrinsic Control (VIC) [19] and related methods (e.g. [15]) propose to achieve this by learning a conditional policy π(a | s, z) which maximizes I(z; τ ), the mutual information between the latent code z and (possibly a subset of) the resulting trajectory obtained by following π. Unfortunately, this objective can be difﬁcult to train in practice as we scale both the number of options and the horizon over which the code is executed [1, 15]. Our method addresses both of these issues in a principled manner by introducing temporal dependencies between a sequence of latent codes, evolving under simple linear dynamics, which decomposes the full objective into a sum of local mutual information lower-bounds, without loss of coherence of the global code.
Entropic Desired Dynamics for Instrinsic Control (EDDICT) can be understood from the perspective of divergence minimization [20, 13, 27]. Concretely, we can deﬁne a prior policy µ which induces a distribution pµ(Λ) over the space of augmented trajectories. We then learn a (posterior) policy π by minimizing the KL-divergence between pπθ (Λ) and pµ(Λ). 3.1 VIC as Divergence Minimization
Given a prior policy µ, we construct a prior distribution over an augmented space (τ, z), with auxiliary variables [3] z ∈ Z, as pµ(z, τ ) = pµ(τ )qw(z | τ ). The conditional qw(z | τ ) is a learnt predictor, with parameters w ∈ Ω, which aims to predict z from the underlying trajectory. (cid:104) p(z)pπθ (τ | z)(cid:107)pµ(z, τ )
We can show that an entropy regularized version of VIC is obtained by maximizing Oent-VIC(θ, w) =
−KL the parameters of πθ and qw, with p(z) a ﬁxed or learnt distribution over options. Intuitively, we seek a code conditioned policy which generates trajectories having high probability under our trajectory prior, and from which z can be inferred in hindsight.
After some algebra, this simpliﬁes to: wrt. (cid:105)
Oent-VIC(θ, w) = Ez∼p(z)
τ ∼pπθ (τ |z) (cid:104) log qw(z | τ ) − log p(z)
−
T −1 (cid:88) log (cid:124) (cid:123)(cid:122) a(cid:13) Iq(z;τ ) (cid:125) t=0 (cid:124) (cid:105)
πθ(at | st, z)
µ(at | st) (cid:123)(cid:122) b(cid:13) regularizer (cid:125)
In the above, Iq(z; τ ) refers to the variational lower-bound [2, 34] to the mutual information I(z; τ ) =
E[log pπθ (z | τ ) − log p(z)] , using reverse predictor q trained to approximate the true posterior distribution pπθ (z | τ ) .
In expectation, the regularization terms correspond to a sum of KL-divergences between our conditional policy and the prior over actions.4 The original objective OVIC w := qw(z | s0, sT ), is obtained by dropping this regularizer and choosing the reverse predictor qVIC which predicts z from the ﬁrst and last states of the trajectory.
Since we focus on discrete action spaces, we set µ to a uniform distribution over actions, causing b(cid:13) to revert to standard entropy rewards [48]. In practice, we optimize the above objective using a value-based reinforcement learning algorithm and (cid:15)-greedy policies (in lieu of a Boltzmann policy), and thus omit these terms. Note that the auxiliary variable perspective of VIC can also be found in
Hausman et al. [23]. 3.2
Incorporating Temporal Dynamics
Instead of sampling a single goal to be reached within the duration of the episode, it may be preferable to sample a sequence of codes either as relative (or local) goals, parameterized relative to the agent’s current position, or as way points, a sequence of global goal coordinates which the agent should visit in sequence.
Relative vs Global Codes. Local goals can be implemented for VIC by resampling a latent code every K steps and maximizing a sum of local objectives of the type Iq(zi; si+1 | si), with option zi initiated from state si. We describe these codes as having local semantics, as an option zi should only be inferable in the context of the relationship between its initiation state si and ﬁnal state si+1. In essence, each zi represents a local displacement which the low-level policy should execute. In contrast, the strategy of sampling way points in some global frame of reference would require maximizing 4The regularization terms emerge from the fact that the transition dynamics are shared by both pµ(τ ) and pπ(τ | z), and thus cancel out in the computation of the KL-divergence. 3
Iq(zi; si+1). Unfortunately, this would seem to require learning a state-dependent high-level policy which gives higher probability to goals zi which are reachable (in K steps) from si.
Ours is a hybrid of these two approaches: by specifying goals relative to previously sampled codes, in the form of a Markov chain with simple linear dynamics, EDDICT can recover codes with global semantics while avoiding the need to explicitly train a high-level policy.
EDDICT Prior. As in Section 3.1, we specify a joint distribution over the set of sub-trajectories
{τi} and auxiliary variables {zi}, i∈[0,M −1]. Recall that τ−1 := s0. Our prior for an augmented trajectory Λ is given by: pµ(Λ | s0) =
M −1 (cid:89) i=0 pµ(τi | τi−1)qw(zi | τi), again with µ a uniform distribution over actions. As we shall see, making the a priori assertion that zi is conditionally independent of τi−1 given τi will ensure that our objective breaks down as a sum of local objectives, amenable to greedy optimization. This prior is illustrated in Fig. 1a. Using the
:= qw(zi | si+1), which predicts zi from si+1 alone (the last state of τi) will reverse predictor qEDDICT then induce codes with global goal semantics. w
EDDICT Posterior. We structure our posterior around goal-conditioned policies πθ(a | s, z), but modiﬁed to account for the temporal structure of our prior. We incorporate temporal dependencies between the latent codes in the form of a Markov chain p(zi | zi−1) with initial distribution p(z0).
Deﬁning p(z0 | z−1) := p(z0), we write: pπθ (Λ | s0) =
M −1 (cid:89) i=0 p(zi | zi−1)pπθ (τi | τi−1, zi)
We now expand the negative KL-divergence corresponding to this choice of prior, posterior and reverse predictor:
−KL [pπθ (Λ | s0)(cid:107)pµ(Λ | s0)] = Epπθ (Λ|s0) (cid:110) M −1 (cid:88) i=0 log (zi | si+1) (cid:20) qEDDICT w p(zi | zi−1) pµ(τi | τi−1) pπθ (τi | τi−1, zi) (cid:21)(cid:41)
The objective is then obtained by dropping action entropy terms.
O(θ, w) =
M −1 (cid:88) i=0
Epπθ (Λ<i) Epπθ (zi,τi|Λ<i)
 (cid:122)
 log qEDDICT w

 (cid:123)
 (zi | si+1) − log p(zi | zi−1)
 c(cid:13) (cid:125)(cid:124) (1) (cid:124) (cid:123)(cid:122)
O(i)(θ,w;zi−1,τi−1) (cid:125)
The objective thus breaks down as a sum of M terms, deﬁned5 as O(i)(θ, w) =
E[O(i)(θ, w; zi−1, τi−1)]. It is worth pointing out that in expectation, c(cid:13) constitutes a valid lower-bound to I(zi; si+1 | zi−1) despite the reverse predictor not conditioning on zi−1. 3.3 EDDICT Objective
We obtain EDDICT by incorporating (i) greedy optimization, (ii) hindsight correction, and (iii) linear dynamics into the objective of Equation 1.
Greedy Optimization. Deﬁne the effective entropy as the difference in log-probabilities given by the reverse predictor and the high-level policy over options (cf. c(cid:13)). As written, the objective aims to maximize the long term sum of effective entropies: concretely, each code zi should seek to be entropic and discernible from si+1 but also lead to states from which future options are themselves 5Note that we use O(i) to refer to the i-th term of Eq. 1, which is a function of a particular value of zi−1 and
τi−1. O(i) is reserved for the expected value of O(i) under pπθ (Λ<i). 4
discernible. The variance of any return estimator will thus increase with the number of option periods.
To avoid this issue, EDDICT optimizes Eq. 1 in a greedy-manner as:
OGREEDY(θ, w) =
M −1 (cid:88) i=0 (cid:104)
Epπ(Λ<i)
O(i)(θ, w; zi−1, τi−1) (cid:105)
, (2) where we have omitted the policy parameters from the sampling distribution pπ(Λ<i), which is thus considered ﬁxed with respect to the optimization process. Concretely, this can be implemented by treating each option period as a pseudo-episode, i.e. using discount factors which are zero on option boundaries as shown in Algorithm 1.
Hindsight Correction. Unfortunately, the above objective is rather brittle as the distribution over zi is conditioned solely on zi−1, and ignores the underlying state in which the code is sampled. We can improve on this open-loop formulation by reasoning in hindsight. From Eq. 2, O(i) is computed in expectation under pπ(Λ<i) which includes the joint pπ(zi−1, τi−1 | Λ<i−1). We rewrite this joint as pπ(τi−1 | Λ<i−1)pπ(zi−1 | τi−1) ≈ pπ(τi−1 | Λ<i−1)qEDDICT (zi−1 | si), since qw is a variational approximation to the true posterior by construction. Incorporating this approximation to Eq. 2 yields the ﬁnal objective: w
OEDDICT(θ, w) =
M −1 (cid:88) i=0
Epπ(Λ<i)EqEDDICT w (zi−1|si) (cid:104)
O(i)(θ, w; zi−1, τi−1) (cid:105)
. (3)
Concretely, when sampling zi ∼ p(zi | zi−1), we thus condition on the code most likely to have yielded state si, under the reverse predictor. Importantly, this objective induces a cross-entropy term between the target distribution qEDDICT (zi | si+1). This ensures that predictions made from si+1 are consistent with those from si, under our latent state dynamics. (zi−1 | si)p(zi | zi−1) and qEDDICT w w
Linear Dynamics The ﬁnal piece of the puzzle concerns the choice of code distribution. We cannot employ the VIC strategy of a ﬁxed entropic distribution, since our codes form a Markov chain. We would further like to avoid the full HRL problem, which would require us to have a parameterized high-level policy over options. Choosing an AR(1) process as the conditional code distribution satisﬁes both of these requirements and we thus set p(zi | zi−1) = zi−1 + ∆i, with ∆i sampled from either an isotropic Gaussian or a uniform distribution on the disc. Another useful property of the
AR(1) process is that it ensures that the marginal code entropy increases monotonically with each option period (more states visited) while the conditional entropy remains constant (same number of states reachable from any given state), as shown in Fig. 4b. Finally, hard coding the dynamics to be linear, versus learning a parametric policy over codes, naturally imposes an interpretable Euclidian topology in code space, as shown in Fig. 4a. 3.4 Algorithm
We now provide a more mechanistic view of EDDICT. Algorithm 1 presents an online version of the algorithm, with details of the distributed setup used in our experiments presented below.
We optimize our objective using a distributed deep reinforcement learning system [14], based on
Peng’s Q(λ) [37] and (cid:15)-greedy policies. The system consists of a centralized learner, a replay buffer [32], and a set of distributed workers each interfacing with their own copy of the environment.
Given the latest parameter values and current state of the environment si (local to each worker), actors sample zi and generate sub-trajectory τi by executing π(a | s, zi) for K steps in the environment.
The resulting (si, ∆i, τi) is then fed back to the replay buffer, from which the learner consumes data to perform off-policy updates. Storing the initiation state si and offset ∆i, allows the learner to recompute the code zi as required using the most up-to-date version of the reverse predictor. Intrinsic rewards derived from the reverse predictor are similarly computed on the learner.
In practice, the learner maximizes OEDDICT by summing two losses. The ﬁrst implements policy iteration by minimizing the mean-squared error between a target return, computed by Peng’s Q(λ) under a target network [32], and the current Q-value estimates. Our greedy optimization procedure yields a single non-zero reward, log qw(zi
| si+1), which is received upon option termination.
The second loss corresponds to the cross-entropy loss of the reverse predictor found in Eq. 3.
With qw(z | s) := N (fw(s), 1) for some parametric function fw, this amounts to minimizing 5
Algorithm 1: EDDICT
Input
: Environment dynamics P , initial state s0, policy πθ, code predictor qw(z | s) := N (fw(s), 1), option period K, discount γ, code dimension d.
τ ← [s0], i ← 0 repeat
∆z ∼ U (Dd) zi ← fw(siK ) + ∆z for t ← iK : (i + 1)K − 1 do
// e.g. uniform over a disc, isotropic normal at ∼ π(a|st, zi; θ) st+1 ∼ P (st+1|st, at)
// Compute intrinsic rewards. Note: entropy of code distribution is constant under linear dynamics.
// (optional) add entropy rewards. rt+1 ← log qw(zi | st+1) if t=(i+1)K−1 else 0
γt+1 ← 0 if t=(i+1)K−1 else γ st+1 ← [zi, st+1]
Append at, st+1, rt+1, γt+1 to τ .
// parametric or epsilon-greedy
// augment state with code
Update θ with any reinforcement learning algorithm on the sub-trajectory τ .
// Minimize cross-entropy loss from Eq. 3, for linear dynamics and Gaussian reverse predictor.
Update w by gradient descent on (cid:107)∆i − (fw(s(i+1)K) − fw(siK))(cid:107)2 2
τ ← [s(i+1)K], i ← i + 1 (cid:107)∆i − (fw(si+1) − fw(si))(cid:107)2 2. This loss is extremely intuitive: we train the reverse predictor such that the inferred latent state from si, matches the inferred state from si+1 under our latent dynamics.
As in [41], we found that an uninformative prior performed best in practice (despite our choice of isotropic Gaussian for the predictor), and thus sample ∆i from a uniform distribution on the disc 6.
Concretely, we parameterize the action-value function Qθ(s, a, z) as an MLP operating on state embeddings, derived from a ResNet [24], and linear action and code embeddings. In our experiments, the reverse predictor qw operates on the same state embeddings as the Q-function, with gradients from both objectives being backpropagated into the ResNet. Complete details of the architecture can be found in the Appendix. 4