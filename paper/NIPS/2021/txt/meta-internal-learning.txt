Abstract
Internal learning for single-image generation is a framework where a generator is trained to produce novel images based on a single image. Since these models are trained on a single image, they are limited in their scale and application. To overcome these issues, we propose a meta-learning approach that enables training over a collection of images, in order to model the internal statistics of the sample image more effectively. In the presented meta-learning approach, a single-image
GAN model is generated given an input image, via a convolutional feedforward hy-pernetwork f . This network is trained over a dataset of images, allowing for feature sharing among different models and for interpolation in the space of generative mod-els. The generated single-image model contains a hierarchy of multiple generators and discriminators. Therefore, the meta-learner needs to be trained in an adversar-ial manner, which requires careful design choices that we justify by a theoretical analysis. Our results show that the models obtained are as suitable as single-image
GANs for many common image applications, and signiﬁcantly reduce training time per image, without loss in performance, and introduce novel capabilities, such as interpolation and feedforward modeling of novel images. Our code is available at: https://github.com/RaphaelBensTAU/MetaInternalLearning. 1

Introduction
In the ﬁeld of internal learning, one wishes to learn the internal statistics of a signal in order to perform various downstream tasks. In this work, we focus on Single image GANs [31, 32, 13, 8], which present extremely impressive results in modeling the distribution of images that are similar to the input image, and in applying this distribution to a variety of applications. However, given that there is no shortage of unlabeled images, one may ask whether a better approach would be to model multiple images, and only then condition the model on a single input image. Doing so, one could (i) beneﬁt from knowledge and feature sharing between the different images, (ii) better deﬁne the boundaries between the distribution obtained from the input image and those of other images, (iii) possibly avoid the costly training phase for a novel image, and instead employ feedforward inference, and (iv) mix different single-image models to create novel types of images.
∗Corresponding author - bensadoun@mail.tau.ac.il 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
From the algorithmic standpoint, this multi-image capability can be attempted using various forms of conditioning. For example, one can add a one-hot vector as an input, or, more generally, a vector signature, and train multiple images using the same single image method. One can also add a complete layer of a conditioning signal to the RGB input. Alternatively, one can employ StyleGAN-like conditioning and modify the normalization of the layers [16]. More generally, observing that this scenario is a meta-learning problem, one can employ methods such as MAML [5] for learning a central network and its variants per image. After performing many such attempts over a long period of time, we were unable to achieve a desirable level of performance with any of these methods.
Instead, we advocate for a meta-learning solution that is based on hypernetworks [9]. Hypernetworks consist of two main components: a primary network g that performs the actual computation, and the hypernetwork f that is used for conditioning. The parameters (weights) of g are not learned conventionally. Instead, they are given as the output of f for the conditioned input signal. Following a single-image GAN setting with a hierarchical structure, we have two hypernetworks fg and fd, which produce the weights of the multiple generators and discriminators for input image I dynamically.
Our method allows for training on multiple images at once, obtaining similar results for various applications to those previously demonstrated for single-image training. It also allows us to interpolate between single-image GANs derived from pairs (or more) of images. Finally, we are able to ﬁt a new unseen image in a fraction of the time required for training a new single-image GAN, i.e.our method enables inference generation for a novel image.
As far as we can ascertain, ours is the ﬁrst method to perform adversarial training using hypernetworks.
We provide a theoretical analysis of the proper way to perform this. This analysis demonstrates both the sufﬁciency of our algorithm for minimizing the objective function and the necessity of various components in our method. 2