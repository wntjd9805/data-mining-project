Abstract
Vision transformers (ViTs) have pushed the state-of-the-art for various visual recog-nition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language
Processing. However, an in-depth analysis in this work shows that they are either theoretically ﬂawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Speciﬁcally, conventional self-attention is computed by normaliz-ing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the ﬁrst time, a softmax-free transformer or SOFT is proposed. To remove soft-max in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention ma-trix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our
SOFT signiﬁcantly improves the computational efﬁciency of existing ViT variants.
Crucially, with a linear complexity, much longer token sequences are permitted in
SOFT, resulting in superior trade-off between accuracy and complexity. 1

Introduction
Recently the step change brought by Transformers [33] in natural language processing (NLP) [10, 4] seems to have arrived in vision [11, 41, 47, 46]. Indeed, with less inductive bias in its architecture design than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants have shown to be able to outperform CNNs on various vision tasks [8, 15]. However, there is a bottleneck in any Transformer based model, namely its quadratic complexity in both computation and memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens (e.g., words or image patches) as input, the self-attention module iteratively learns the feature representations by relating one token to all other tokens. This results in a quadratic complexity O(n2) with the token sequence length n in both computation (time) and memory (space) since an n × n sized attention matrix needs to be computed and saved during inference. This problem is particularly acute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP even with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from modeling images at high spatial resolutions, which are often crucial for visual recognition tasks.
∗Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan
University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b)
Figure 1: Top1-Accuracy on ImageNet [9] validation set with respect to parameters and the memory usage corresponding to the token sequence length in practice compared to other methods. (a) Com-parison with CNN models: RegNet [26], ResNet [13] and Transformer models: PVT [35], DeiT [31],
ViT [11], T2T-ViT [41], Twins-SVT [6] and SAN10 [45]; (b) Comparison with Transformer [33],
Linformer [34], Nyströformer [39] and Performer [5]. The memory usage is measured with a batch size of 1 on a 16GB Tesla V100.
A natural solution is to reduce the complexity of self-attention computation via approximation.
Indeed, there have been a number of attempts in NLP [34, 5, 18, 39]. For example, [34] takes a naive approach by shortening the length of Key and Value via learnable projections. Such a coarse approximation would inevitably cause performance degradation. In contrast, [5, 17] both leverage the kernel mechanism to approximate softmax normalization to linearize the computation in self-attention.
[18] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [39] uses Nyström matrix decomposition to reconstruct the full attention matrix with polynomial iteration for approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization is simply duplicated across the matrix decomposition process, which is theoretically unsound. We empirically found that none of these methods are effective when applied to vision (see Sec. 4.2).
In this work, we identify that the limitations of existing efﬁcient Transformers are caused by the use of softmax self-attention, and for the ﬁrst time propose a softmax-free Transformer. More speciﬁcally, in all existing Transformers (with or without linearization), a softmax normalization is needed on top of scaled dot-product between token feature vectors [33]. Keeping this softmax operation challenges any subsequent linearization efforts. To overcome this obstacle, we introduce a novel softmax-free self-attention mechanism, named as SOFT, with linear complexity O(n) in both space and time.
Speciﬁcally, SOFT uses Gaussian kernel to deﬁne the similarity (self-attention) function without the need for subsequent softmax normalization. With this softmax-free attention matrix, we further introduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of the approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably computing the Moore-Penrose inverse of the matrix.
We make the following contributions. (I) We introduce a novel softmax-free Transformer with linear space and time complexity. (II) Our attention matrix approximation is achieved through a novel matrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual recognition tasks, we design a family of generic backbone architectures with varying capacities using
SOFT as the core self-attention component. Extensive experiments show that with a linear complexity (Figure 1b), our SOFT models can take in as input much longer image token sequences. As a result, with the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on
ImageNet [9] classiﬁcation in the accuracy/complexity trade-off (Figure 1a). 2