Abstract
Bayesian model averaging, obtained as the expectation of a likelihood function by a posterior distribution, has been widely used for prediction, evaluation of uncertainty, and model selection. Various approaches have been developed to efﬁciently capture the information in the posterior distribution; one such approach is the optimization of a set of models simultaneously with interaction to ensure the diversity of the individual models in the same way as ensemble learning. A representative approach is particle variational inference (PVI), which uses an ensemble of models as an empirical approximation for the posterior distribution. PVI iteratively updates each model with a repulsion force to ensure the diversity of the optimized models.
However, despite its promising performance, a theoretical understanding of this repulsion and its association with the generalization ability remains unclear. In this paper, we tackle this problem in light of PAC-Bayesian analysis. First, we provide a new second-order Jensen inequality, which has the repulsion term based on the loss function. Thanks to the repulsion term, it is tighter than the standard Jensen inequality. Then, we derive a novel generalization error bound and show that it can be reduced by enhancing the diversity of models. Finally, we derive a new
PVI that optimizes the generalization error bound directly. Numerical experiments demonstrate that the performance of the proposed PVI compares favorably with existing methods in the experiment. 1

Introduction
Bayesian model averaging (BMA) has been widely employed for prediction, evaluation of uncertainty, and model selection in Bayesian inference. BMA is obtained as the expectation of a likelihood function by a posterior distribution and thus it contains information of each model drawn from the posterior distribution [21]. Since estimating the posterior distribution is computationally difﬁcult in practice, various approximations have been developed to efﬁciently capture the diversity in the posterior distribution [21, 1, 2].
One of these recently proposed approaches involves optimizing a set of models simultaneously with interaction to ensure the diversity of the individual models, similar to ensemble learning. One notable example is particle variational inference (PVI) [17, 28], which uses an ensemble as an empirical approximation for the posterior distribution. Such PVI methods have been widely employed in variational inference owing to their high computational efﬁciency and ﬂexibility. They iteratively update the individual models and the update equations contain the gradient of the likelihood function and the repulsion force that disperses the individual models. Thanks to this repulsion term, the obtained ensemble can appropriately approximate the posterior distribution. When only one model is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
used in PVI, the update equation is equivalent to that of the maximum a posteriori (MAP) estimation.
Other methods have been developed apart from PVI, especially for latent variable models, which have introduced regularization to the MAP objective function to enforce the diversity in the ensemble.
A notable example of such methods is the determinantal point process (DPP) [29].
Despite successful performances of these methods in practice [22, 18, 17, 28, 5, 27], a theoretical understanding of the repulsion forces remains unclear. Some previous studies considered PVI as a gradient ﬂow in Wasserstein space with an inﬁnite ensemble size [16, 12] and derived the convergence theory. However, an inﬁnite ensemble size is not a practical assumption and no research has been conducted to analyze the repulsion force related to the generalization.
BMA can be regarded as a special type of ensemble learning [26], and recent work has analyzed the diversity of models in ensemble learning in light of the PAC-Bayesian theory [19]. They reported that the generalization error is reduced by increasing the variance of the predictive distribution. However, the existing posterior approximation methods, such as PVI and DPPs, enhance the diversity with the repulsion of the parameters or models rather than the repulsion of the predictive distribution. We also found that the analysis in a previous work [19] cannot be directly extended to the repulsion of parameters or loss functions (see Appendix F). In addition, when the variance of the predictive distribution is included in the objective function in the variational inference, the obtained model shows large epistemic uncertainty, which hampers the ﬁtting of each model to the data (see Section 5).
Based on these ﬁndings, this study aims to develop a theory that explains the repulsion forces in PVI and DPPs and elucidates the association of the repulsion forces with the generalization error. To address this, we derive the novel second-order Jensen inequality and connect it to the PAC-Bayesian generalization error analysis. Our second-order Jensen inequality includes the information of the variance of loss functions. Thanks to the variance term, our bound is tighter than the standard Jensen inequality. Then, we derive a generalization error bound that includes the repulsion term, which means that enhancing the diversity is necessary to reduce the generalization error. We also show that PVI and DPPs can be derived from our second-order Jensen inequality, and indicate that these methods work well from the perspective of the generalization error. However, since these existing methods do not minimize the generalization error upper bound, there is still room for improvement.
In this paper, we propose a new PVI that directly minimize the generalization error upper bound and empirically demonstrate its effectiveness.
Our contributions are summarized as follows: 1. We derive a novel second-order Jensen inequality that inherently includes the variance of loss functions. Thanks to this variance term, our second-order Jensen inequality is tighter than the standard Jensen inequality. We then show that enhancing the diversity is important for reducing the generalization error bound in light of PAC-Bayesian analysis. 2. From our second-order Jensen inequality, we derive the existing PVI and DPPs. We demonstrate that these methods work well even at a ﬁnite ensemble size, since their objective functions includes valid diversity enhancing terms to reduce the generalization error. 3. We propose a new PVI that minimizes the generalization error bound directly. We nu-merically demonstrate that the performance of our PVI compares favorably with existing methods. 2