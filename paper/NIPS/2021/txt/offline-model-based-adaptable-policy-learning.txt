Abstract
In reinforcement learning, a promising direction to avoid online trial-and-error costs is learning from an ofﬂine dataset. Current ofﬂine reinforcement learning methods commonly learn in the policy space constrained to in-support regions by the ofﬂine dataset, in order to ensure the robustness of the outcome policies.
Such constraints, however, also limit the potential of the outcome policies. In this paper, to release the potential of ofﬂine policy learning, we investigate the decision-making problems in out-of-support regions directly and propose ofﬂine
Model-based Adaptable Policy LEarning (MAPLE). By this approach, instead of learning in in-support regions, we learn an adaptable policy that can adapt its behavior in out-of-support regions when deployed. We conduct experiments on
MuJoCo controlling tasks with ofﬂine datasets. The results show that the proposed method can make robust decisions in out-of-support regions and achieve better performance than SOTA algorithms. 1

Introduction
Recent studies have shown that reinforcement learning (RL) is a promising approach for real-world applications, e.g., sequential recommendation systems [1, 2, 3, 4] and robotic locomotion skill learning [5, 6]. However, the trial-and-error of RL in the real world [7] obstructs further applications in cost-sensitive scenarios [8].
Ofﬂine (batch) RL learns a policy within a static dataset collected by a behavior policy without additional interactions with the environment [8, 9, 10, 11]. Since it avoids costly trial-and-error in real-world environments, ofﬂine RL is a promising way to handle the challenge in cost-sensitive applications. A signiﬁcant challenge of ofﬂine RL is in answering counterfactual queries, which asks about how the performance (e.g., Q value) would have been if the agent were to execute an unseen action sequence, then learning to make optimal decisions based on the performance [8].
Fujimoto et al. [10] have shown that the distributional shift of states and actions, which comes from the discrepancy between evaluated policies and behavior policies, often leads to large extrapolation error in value function estimation. In traditional model-free algorithms, the extrapolation error in value function estimation hurts the generalization performance of the learned policies. Since the additional samples, which can correct value estimation errors, are unavailable in the ofﬂine setting, the performance of learned policies based on value function is unstable [10].
On the other hand, model-based RL techniques, which learn dynamics models from collected datasets and learn the value function and policies based on the dynamics models, do not need to estimate
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the value functions rely on the collected datasets. However, similar challenges occur in dynamics model approximation. The dynamics model might overﬁt the limited dataset and suffer extrapolation errors in regions that behavior policies have not visited, which causes instability of the learned policy when deployment [12]. Here we call it out-of-support regions. Moreover, in model inference, the compounding error, that is, the accumulated prediction errors between simulation trajectories and reality, would be large even if the one-step prediction error is small [13, 14]. Recent studies in ofﬂine model-based RL [12, 15] have made signiﬁcant progress in MuJoCo tasks [16]. These methods constrain policy sampling in dynamics models for robust policy learning. By using large penalty [12] or trajectory truncation [15, 12] in the regions with large prediction uncertainty (uncertainty is a designed metric to evaluate the conﬁdence of prediction correctness) or compounding error, policy exploration is constrained in the regions of dynamics models where the predictions are corrected with high conﬁdence, so as to avoid exploiting regions with risks of large extrapolation error. However, the constraints on dynamics models lead to a conservative policy learning process, which limits the potential of leveraging dynamics models: The visits on states and actions in out-of-support regions are more likely to be inhibited by the constraints, making the learned policy restrict the agent to be in similar regions as the behavior policy.
From the perspectives of counterfactual queries, we consider that model-based RL is promising to handle ofﬂine RL — ideal reconstructed dynamics models can simulate the transition dataset without the distributional-shift problem given any policy, and the value function can be estimated via the “simulated” transition dataset directly. The bottleneck of ofﬂine model-based RL comes from the policy learning in the approximated dynamics model with extrapolation error. In this paper, instead of learning by tightly constraining policy exploration in in-support regions, we investigate decision-making in out-of-support regions directly. Finally, we propose a new ofﬂine policy learning framework, ofﬂine Model-based Adaptable Policy LEarning (MAPLE), to address the aforementioned issues. Ideally, MAPLE tries to model all possible transition dynamics in the out-of-support regions.
Then an Adaptable policy is learned to be aware of each case to adapt its behavior to reach optimal performance. In the practical version of MAPLE, we use an ensemble technique to construct ensemble dynamics models. To be aware of each case of the transition dynamics and learn an adaptable policy, we use a meta-learning technique that introduces an extra environment-context extractor structure to represent dynamics patterns, and the policy adjusts itself according to the environment contexts.
We conduct experiments on the MuJoCo tasks. The results show that the sampling regions for robust ofﬂine policy learning can be extended via constructing transition patterns in out-of-support regions to cover the real case. The output adaptable policy yields better performance than SOTA algorithms when deployed. MAPLE gives a new direction to handle the ofﬂine policy learning problem in the dynamics models: Besides constraining on sampling and training dynamics models with better generalization, we can also model out-of-distribution regions by constructing all possible transition patterns. 2