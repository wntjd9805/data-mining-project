Abstract
Deep networks have shown remarkable results in the task of object detection.
However, their performance suffers critical drops when they are subsequently trained on novel classes without any sample from the base classes originally used to train the model. This phenomenon is known as catastrophic forgetting.
Recently, several incremental learning methods are proposed to mitigate catas-trophic forgetting for object detection. Despite the effectiveness, these methods require co-occurrence of the unlabeled base classes in the training data of the novel classes. This requirement is impractical in many real-world settings since the base classes do not necessarily co-occur with the novel classes. In view of this limitation, we consider a more practical setting of complete absence of co-occurrence of the base and novel classes for the object detection task. We propose the use of unlabeled in-the-wild data to bridge the non co-occurrence caused by the missing base classes during the training of additional novel classes. To this end, we introduce a blind sampling strategy based on the responses of the base-class model and pre-trained novel-class model to select a smaller relevant dataset from the large in-the-wild dataset for incremental learning. We then design a dual-teacher distillation framework to transfer the knowledge distilled from the base- and novel-class teacher models to the student model using the sampled in-the-wild data. Experimental results on the PASCAL VOC and MS COCO datasets show that our proposed method significantly outperforms other state-of-the-art class-incremental object detection methods when there is no co-occurrence be-tween the base and novel classes during training. Our source code is available at https://github.com/dongnana777/Bridging-Non-Co-occurrence. 1

Introduction
Deep learning have shown remarkable performance in a wide variety of tasks, and even surpass human experts in numerous tasks. However, humans are still better than machines in continually acquiring, fine-tuning and transferring knowledge throughout their lifetime. This is because deep networks suffer from catastrophic forgetting [18, 21], i.e. a phenomenon that causes a deep network to forget previously acquired knowledge on the old base classes when trained on new novel classes. As a result, this causes the performance of the deep networks to drop significantly on the base classes. In the task of image-based object detection, object detectors are trained on large-scale datasets and then deployed on real-world applications [9, 11, 8, 24, 5, 15, 23, 17, 16, 1, 2, 39, 33, 34, 38, 36, 37, 35, 32].
As these applications are often carried out in dynamic environments where novel object classes are continually presented, the ability for the deep networks to learn novel object classes without forgetting the base object classes becomes an imperative requirement. A naive approach to achieve this endeavor
∗Work fully done while first author is a visiting PhD student at the National University of Singapore. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Co-occurrence statistics of the base classes in the novel datasets from PASCAL VOC under various splits of “base + novel" classes. n g e tti n s 19+1 15+5 10+10 c t s b j e o f o r e u m b base classes novel classes base classes novel classes base classes novel classes o r e a 2
-4
-74
-c l e y b i c 14
-24
-666
-d b i r 0
-8
-158
-a t b o 0
-6
-188
-b o ttl e 108
-260
-890
-s b u 0
-2
-256
-r c a 8
-140
-1398
-a t c 28
-112
-180
-a i r h c 440
-1094
-2492
-c o w 0
-4
-246
-b l e g t a d i n i n 38
-164
--620 g o d 28
-156
--1076 e o h r s 0
-20
--812 e b i k r m o t o 0
-8
--780 n o p r s e 344
-1594
--10894 p p l a d o tt e 116
--1250
-1250 n t s p e h e 0
--706
-706 a f o s 148
--850
-850 t r a i n 0
--656
-656 t v
-734
-734
-734 is to retain all training data for the base classes and train the deep network concurrently with the base and novel training data from scratch. However, pragmatic issues such as privacy can limit the accessibility to the base class dataset previously used to train the base model.
Recently, several incremental learning methods [40, 27, 4, 10, 20, 31] are proposed to overcome catastrophic forgetting in the object detection task. Despite the impressive performance, these methods rely on the co-occurrence of unlabeled base classes in the training data of the novel classes.
Due to the fact that the base and novel class datasets are obtained from the same dataset to simulate incremental learning, the base classes inevitably co-occur in the background of the novel class training data. Table 1 shows the co-occurrence statistics of the base classes in the novel datasets from
PASCAL VOC under several commonly used experiment settings in the existing works. For example, class 1-19 are chosen as the base classes and class 20 (“tv") as the novel class. We can see from the table that the “areo", “bicycle", “bottle", etc. , objects in the base classes co-occur in the sample images of the novel “tv" object class. Despite the absence of ground truth labels of the base classes on the novel training data, knowledge of base classes can be replayed and distilled into the novel model. However, this reliance on the co-occurrence of unlabeled base classes in the training data of novel classes severely limits the practicality of most incremental learning approaches. This is due to the inherent non co-occurrence of the base and novel classes in most real-world data.
In this paper, we propose the use of the abundance in-the-wild data to bridge the non co-occurrence of base classes in the training data of the novel classes. To this end, we first introduce a blind sampling strategy to select relevant data from the in-the-wild data that contains large amounts of irrelevant images with neither the base nor novel class information. Specifically, images with high probability response from the given base model and the novel model trained with the novel class training data are selected in the blind sampling step. We further design a dual-teacher distillation framework where the images selected from the blind sampling strategy are used to distill knowledge from the base and novel teacher models to the student model. Particularly, our dual-teacher distillation framework consists of: 1) A class remodeling step that remodels the irrelevant classes as background class in the base and novel model, respectively. This ensures the class probabilities of the disjoint set of classes in the base and novel models can be compared appropriately to the student model. 2) Image-level distillation with region of interest (RoI) masks from the pseudo ground truth of the bounding boxes obtained in the blind sampling step. These RoI masks are used to mask out the irrelevant regions of the feature maps in the distillation loss. 3) Instance-level distillation with the response heatmaps of the object detectors. This response heatmaps is essential in transferring both positive (high response regions) and negative (low response regions) information from the base and novel teacher models to the student model.
Our main contributions are as follows: 1. We tackle a more challenging and realistic scenario of incremental learning for object detection, where there is no co-occurrence of the base classes in the training data of the novel classes. This contrasts with previous approaches whose success depend largely on such co-occurrences. 2. We propose a blind sampling strategy to effectively select useful data from large amounts of unlabeled in-the-wild data. We also design a dual-teacher distillation framework which utilizes the sampled data to distill knowledge from the base and novel teacher models to the student model. 3. Extensive experiments conducted on two standard object detection datasets (i.e. MS COCO and
PASCAL VOC) demonstrate the significant performance improvement of our approach over existing state-of-the-art. 2
2