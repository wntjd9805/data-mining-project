Abstract
We propose a fully differentiable architecture for simultaneous semantic and in-stance segmentation (a.k.a. panoptic segmentation) consisting of a convolutional neural network and an asymmetric multiway cut problem solver. The latter solves a combinatorial optimization problem that elegantly incorporates semantic and boundary predictions to produce a panoptic labeling. Our formulation allows to directly maximize a smooth surrogate of the panoptic quality metric by backprop-agating the gradient through the optimization problem. Experimental evaluation shows improvement by backpropagating through the optimization problem w.r.t. comparable approaches on Cityscapes and COCO datasets. Overall, our approach of combinatorial optimization for panoptic segmentation (COPS) shows the utility of using optimization in tandem with deep learning in a challenging large scale real-world problem and showcases beneﬁts and insights into training such an architecture. 1

Introduction
Panoptic segmentation is the task of simultaneously segmenting different semantic classes and instances of the same class [37]. Panoptic segmentation is challenging since neural networks (NN) may produce conﬂicting predictions (i.e. boundaries separating instances that are not closed contours, instance voting schemes with multiple maxima per instance etc.). Therefore most approaches combine
NNs with a post-processing step to compute a ﬁnal panoptic segmentation that resolves the conﬂicting evidence produced by NNs. In general, joint training of NNs with post-processing algorithms is an active research area. In our work we propose a fully differentiable approach for panoptic segmentation, our post-processing being a combinatorial optimization problem.
In this work we pursue the bottom-up approach building segmentations directly from pixels and combine CNNs with the asymmetric multiway cut problem (AMWC) [42]. The latter is an elegant combinatorial optimization problem that combines semantic and afﬁnity predictions and directly produces a panoptic labeling. We train CNN and AMWC jointly so that the supervisory signal for training the CNN is inﬂuenced by the computations of the combinatorial optimization stage. The loss we propose to use for this training differs from common lower-level CNN losses and is a smooth surrogate closely corresponding to the ﬁnal panoptic quality metric [37]. We show in this work how our conceptual contributions, i.e. using AMWC as a differentiable module and training on surrogate panoptic quality loss can be made to work together and yield performance improvements.
The general idea of combining optimization and neural networks and train them jointly has recently enjoyed resurgent interest. The fundamental problem for the speciﬁc task of combinatorial optimiza-tion is that the output of combinatorial problem is 0–1 valued, hence the loss landscape becomes piecewise constant and simply differentiating through a solver is not possible anymore. Several methods have been proposed to address this problem [10, 24, 26, 31, 56, 64]. To our knowledge our work is the ﬁrst to utilize the perturbation techniques [24, 64] on a large-scale setting with scalable 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
but suboptimal heuristic solvers. We give evidence that training works in this setting and gives perfor-mance beneﬁts. To this end we propose a robust extension of the backpropagation technique [64] that gives better empirical convergence.
Our architecture is inspired by [14, 16] and consists of a ResNet-50 backbone, a semantic segmen-tation branch for computing class costs and an afﬁnity branch for boundary predictions. Semantic and afﬁnity costs are taken as input by the AMWC solver that returns a panoptic labeling. We ﬁrst pre-train semantic and afﬁnity branches with simple cross-entropy losses obtaining a strong baseline that achieves a performance similar or better than other bottom-up approaches [16, 27, 69]. We
ﬁnetune subsequently with the AMWC solver and the panoptic surrogate loss via our new robust backpropagation approach and show further performance improvements.
Current state-of-the-art approaches use very large networks (e.g. Max-DeepLab [65] uses transformers containing more parameters than a ResNet-101). This might lead to the impression that advances in panoptic segmentation require deeper and more sophisticated architecture. We show that our simpler model can be signiﬁcantly improved by a fully differentiable approach and argue that simpler models have not yet reached their full potential. Also, our simpler architecture allows for a more controlled setting and makes it easier to identify crucial components and measure to which extent performance improvements can be achieved.
Contributions
Optimization for segmentation: We propose AMWC [42] as an expressive and tractable combi-natorial optimization formulation to be used in an fully differentiable architecture for panoptic segmentation. We also propose a scalable heuristic for its solution.
Panoptic loss surrogate: We propose a surrogate loss function that approximates the panoptic loss metric and can be used in our training setup.
Backpropagation: We give an extension of the perturbation technique [64] for backpropagating gradients through combinatorial solvers, improving training with suboptimal heuristic solvers.
Experimental validation: We conduct experiments on Cityscapes [19] and COCO [47] and show the beneﬁts of fully differentiable training against comparable approaches.
Our code is available at https://github.com/aabbas90/COPS. 2