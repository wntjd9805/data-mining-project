Abstract
Traditional methods for kernel selection rely on parametric kernel functions or a combination thereof and although the kernel hyperparameters are tuned, these methods often provide sub-optimal results due to the limitations induced by the parametric forms. In this paper, we propose a novel formulation for kernel selection using efﬁcient Bayesian optimisation to ﬁnd the best ﬁtting non-parametric kernel.
The kernel is expressed using a linear combination of functions sampled from a prior Gaussian Process (GP) deﬁned by a hyperkernel. We also provide a mechanism to ensure the positive deﬁniteness of the Gram matrix constructed using the resultant kernels. Our experimental results on GP regression and Support Vector
Machine (SVM) classiﬁcation tasks involving both synthetic functions and several real-world datasets show the superiority of our approach over the state-of-the-art. 1

Introduction
Kernel machines (Hofmann et al., 2008) generally work well with low-dimensional and small to medium-scaled data. In most kernel machines, the kernel function is chosen from the standard bag of popular kernels (Genton, 2001, Stein, 2015) such as Squared Exponential kernel (SE), Matérn kernel and Periodic kernel, or a weighted combination thereof (Aiolli and Donini, 2015, Gönen and
Alpaydın, 2011, Rakotomamonjy et al., 2007). Recent developments (Jang et al., 2017, Wilson and
Adams, 2013) in kernel learning parameterise the kernel function to boost the expressiveness of the kernel. However, the expressiveness of such kernels remains limited by the chosen parametric form and thus they often fall short in providing the best kernel function for complex data distributions.
There have been some early attempts to design an optimal non-parametric kernel to remove the limitations associated with the parametric forms. Ong et al. (2003, 2005) proposed a hyperkernel framework by deﬁning a Reproducing Kernel Hilbert Space (RKHS) on the space of kernels i.e., a kernel on kernels to support kernel learning. They formulate a semideﬁnite programming (Vanden-berghe and Boyd, 1996) based optimisation problem using the representer theorem (Steinwart and
Christmann, 2008, Vapnik, 1999) to ﬁnd the best kernel. However, their method suffers from two key limitations: (i) their way of enforcing the positive deﬁniteness property produces a restrictive search space, resulting in a sub-optimal solution, and (ii) the computational complexity of their method scales with the dataset size, making it infeasible for larger datasets. Benton et al. (2019) proposed
Functional Kernel Learning (FKL), which extends the function space view of the Gaussian Process (GP) for kernel learning. FKL uses a transformed GP over a spectral density to deﬁne a distribution over kernels. However, the formulation of kernel functionals using the spectral densities induces strong assumptions on the properties such as periodicity, stationarity, etc. and thus are not generally applicable. Malkomes et al. (2016) proposed an automated kernel selection (BOMS) using Bayesian optimisation. The kernel space in BOMS is deﬁned by the base kernels and the associated grammar to combine them. Although the search space is constructed by summing or multiplying the base kernels, the resultant kernel space is restricted in the compositional space of parametric forms. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we propose a generic framework called Kernel Functional Optimisation (KFO) to address the aforesaid shortcomings. First, it provides a ﬂexible form of kernel learning whose computational complexity is decoupled from dataset size. Next, it allows us to use a computationally efﬁcient Bayesian optimisation method to ﬁnd the best kernel. We incorporate hyperkernels into our Bayesian framework that allows us to search for the optimal kernel in a Hilbert space of kernels spanned by the hyperkernel (Ong et al., 2005). We draw kernel functionals from a (hyper) GP distribution ﬁtted using a hyperkernel. As the kernel drawn from the hyper-GP may be indeﬁnite, we provide ways to ensure positive deﬁniteness by transforming indeﬁnite, or Kre˘ın (Oglic and Gärtner, 2019, Ong et al., 2004) kernel space into a positive deﬁnite kernel space. The optimisation of kernel functionals necessitates solving larger covariance matrices and thus adds to the computational burden of the overall process. To speed up the computations, we perform a low-rank decomposition of the covariance matrix. Further, we provide a theoretical analysis of our method showing that it converges efﬁciently as in its cumulative regret grows only sub-linearly and eventually vanishes.
We evaluate the performance of our method on both synthetic and real-world datasets using SVM classiﬁcation (Diehl and Cauwenberghs, 2003, Scholkopf and Smola, 2001, Burges, 1998) and
GP regression tasks. Comparison of predictive performance against the state-of-the-art baselines demonstrates the superiority of our method. Further, we compare with the state-of-the-art performance reported in the latest survey paper on classiﬁer comparison (Zhang et al., 2017) and ﬁnd that our method provides the best performance on most of the datasets. Our main contributions in this paper are as follows: (i) we propose a novel approach for ﬁnding the best non-parametric kernel using hyperkernels and Bayesian functional optimisation (Section 3), (ii) we provide methods to ensure positive deﬁniteness of the kernels optimised (Section 3), (iii) we derive the convergence guarantees to demonstrate that the regret grows sub-linearly for our proposed method (Section 4), (iv) we provide empirical results on both synthetic and real-world datasets to prove the usefulness (Section 5). 2