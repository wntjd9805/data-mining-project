Abstract
Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efﬁciency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Lin-ear State-Space Layer (LSSL) maps a sequence u (cid:55)→ y by simply simulating a linear continuous-time state-space representation ˙x = Ax + Bu, y = Cx + Du.
Theoretically, we show that LSSL models are closely related to the three aforemen-tioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share fea-tures of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices A that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classiﬁcation, real-world healthcare regression tasks, and speech. On a difﬁcult speech classiﬁcation task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences. 1

Introduction
A longstanding challenge in machine learning is efﬁciently modeling sequential data longer than a few thousand time steps. The usual paradigms for designing sequence models involve recurrence (e.g. RNNs), convolutions (e.g. CNNs), or differential equations (e.g. NDEs), which each come with tradeoffs. For example, RNNs are a natural stateful model for sequential data that require only constant computation/storage per time step, but are slow to train and suffer from optimization difﬁculties (e.g., the "vanishing gradient problem" [39]), which empirically limits their ability to handle long sequences. CNNs encode local context and enjoy fast, parallelizable training, but are not sequential, resulting in more expensive inference and an inherent limitation on the context length.
NDEs are a principled mathematical model that can theoretically address continuous-time problems and long-term dependencies [37], but are very inefﬁcient.
Ideally, a model family would combine the strengths of these paradigms, providing properties like parallelizable training (convolutional), stateful inference (recurrence) and time-scale adaptation 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (Three views of the LSSL) A Linear State Space Layer layer is a map ut ∈ R → yt ∈ R, where each feature ut (cid:55)→ yt is deﬁned by discretizing a state-space model A, B, C, D with a parameter ∆t.
The underlying state space model deﬁnes a discrete recurrence through combining the state matrix A and timescale ∆t into a transition matrix A. (Left) As an implicit continuous model, irregularly-spaced data can be handled by discretizing the same matrix A using a different timescale ∆t. (Center) As a recurrent model, inference can be performed efﬁciently by computing the layer timewise (i.e., one vertical slice at a time (ut, xt, yt), (ut+1, xt+1, yt+1), . . .), by unrolling the linear recurrence. (Right) As a convolutional model, training can be performed efﬁciently by computing the layer depthwise in parallel (i.e., one horizontal slice at a time (ut)t∈[L], (yt)t∈[L], . . .), by convolving with a particular ﬁlter. (differential equations), while handling very long sequences in a computationally efﬁcient way.
Several recent works have turned to this question. These include the CKConv, which models a continuous convolution kernel [44]; several ODE-inspired RNNs, such as the UnICORNN [47]; the
LMU, which speeds up a speciﬁc linear recurrence using convolutions [12, 58]; and HiPPO [24], a generalization of the LMU that introduces a theoretical framework for continuous-time memorization.
However, these model families come at the price of reduced expressivity: intuitively, a family that is both convolutional and recurrent should be more restrictive than either.
Our ﬁrst goal is to construct an expressive model family that combines all 3 paradigms while preserving their strengths. The Linear State-Space Layer (LSSL) is a simple sequence model that maps a 1-dimensional function or sequence u(t) (cid:55)→ y(t) through an implicit state x(t) by simulating a linear continuous-time state-space representation in discrete-time
˙x(t) = Ax(t) + Bu(t) y(t) = Cx(t) + Du(t), (1) (2) where A controls the evolution of the system and B, C, D are projection parameters. The LSSL can be viewed as an instantiation of each family, inheriting their strengths (Fig. 1):
• LSSLs are recurrent. If a discrete step-size ∆t is speciﬁed, the LSSL can be discretized into a linear recurrence using standard techniques, and simulated during inference as a stateful recurrent model with constant memory and computation per time step.
• LSSLs are convolutional. The linear time-invariant systems deﬁned by (1)+(2) are known to be explicitly representable as a continuous convolution. Moreover, the discrete-time version can be parallelized during training using convolutions [12, 44].
• LSSLs are continuous-time. The LSSL itself is a differential equation. As such, it can perform unique applications of continuous-time models, such as simulating continuous processes, handling missing data [45], and adapting to different timescales.
Surprisingly, we show that LSSLs do not sacriﬁce expressivity, and in fact generalize convolutions and RNNs. First, classical results from control theory imply that all 1-D convolutional kernels can be approximated by an LSSL [59]. Additionally, we provide two results relating RNNs and ODEs 2
that may be of broader interest, e.g. showing that some RNN architectural heuristics (such as gating mechanisms) are related to the step-size ∆t and can actually be derived from ODE approximations.
As corollaries of these results, we show that popular RNN methods are special cases of LSSLs.
The generality of LSSLs does come with tradeoffs. In particular, we describe and address two challenges that naive LSSL instantiations face when handling long sequences: (i) they inherit the limitations of both RNNs and CNNs at remembering long dependencies, and (ii) choosing the state matrix A and timescale ∆t appropriately are critical to their performance, yet learning them is computationally infeasible. We simultaneously address these challenges by specializing LSSLs using a carefully chosen class of structured matrices A, such that (i) these matrices generalize prior work on continuous-time memory [24] and mathematically capture long dependencies with respect to a learnable family of measures, and (ii) with new algorithms, LSSLs with these matrices A can be theoretically sped up under certain computation models, even while learning the measure A and timescale ∆t.
We empirically validate that LSSLs are widely effective on benchmark datasets and very long time series from healthcare sensor data, images, and speech.
• On benchmark datasets, LSSLs obtain SoTA over recent RNN, CNN, and NDE-based methods across sequential image classiﬁcation tasks (e.g., by over 10% accuracy on sequential CIFAR) and healthcare regression tasks with length-4000 time series (by up to 80% reduction in RMSE).
• To showcase the potential of LSSLs to unlock applications with extremely long sequences, we introduce a new sequential CelebA classiﬁcation task with length-38000 sequences. A small LSSL comes within 2.16 accuracy points of a specialized ResNet-18 vision architecture that has 10x more parameters and is trained directly on images.
• Finally, we test LSSLs on a difﬁcult dataset of high-resolution speech clips, where usual speech pipelines pre-process the signals to reduce the length by 100x. When training on the raw length-16000 signals, the LSSL not only (i) outperforms previous methods by over 20 accuracy points in 1/5 the training time, but (ii) outperforms all baselines that use the pre-processed length-160 sequences, overcoming the limitations of hand-crafted feature engineering.
Summary of Contributions
• We introduce Linear State-Space Layers (LSSLs), a simple sequence-to-sequence transformation that shares the modeling advantages of recurrent, convolutional, and continuous-time methods.
Conversely, we show that RNNs and CNNs can be seen as special cases of LSSLs (Section 3).
• We prove that a structured subclass of LSSLs can learn representations that solve continuous-time memorization, allowing it to adapt its measure and timescale (Section 4.1). We also provide new algorithms for these LSSLs, showing that they can be sped up computationally under an arithmetic complexity model Section 4.2.
• Empirically, we show that LSSLs stacked into a deep neural network are widely effective on time series data, even (or especially) on extremely long sequences (Section 5). 2 Technical