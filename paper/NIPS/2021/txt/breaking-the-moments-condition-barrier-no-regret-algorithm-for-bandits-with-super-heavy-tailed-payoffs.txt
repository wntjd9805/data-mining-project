Abstract
Despite a large amount of effort in dealing with heavy-tailed error in machine learning, little is known when moments of the error can become non-existential: the random noise η satisﬁes Pr[|η| > |y|] ≤ 1/|y|α for some α > 0. We make the
ﬁrst attempt to actively handle such super heavy-tailed noise in bandit learning problems: We propose a novel robust statistical estimator, mean of medians, which estimates a random variable by computing the empirical mean of a sequence of empirical medians. We then present a generic reductionist algorithmic framework for solving bandit learning problems (including multi-armed and linear bandit problem): the mean of medians estimator can be applied to nearly any bandit learning algorithm as a black-box ﬁltering for its reward signals and obtain similar regret bound as if the reward is sub-Gaussian. We show that the regret bound is near-optimal even with very heavy-tailed noise. We also empirically demonstrate the effectiveness of the proposed algorithm, which further corroborates our theoretical results. 1

Introduction
Multi-armed bandit (MAB) problems have been introduced by Robbins (Robbins, 1952), and have since become a standard model for modeling sequential decision-making problems. In an MAB instance, there are ﬁnite number of arms, pulling each of which an agent receives a random reward (payoff) with unknown distribution. An agent then aims to maximize the received rewards by pulling the arms strategically for a number of times. The MAB problems frequently arise in practice: e.g., clinical trials and online advertising. When the number of arms becomes inﬁnite in practice, the stochastic linear bandit (Abe and Long, 1999; Auer, 2002; Dani et al., 2008) generalizes the classical
MAB by assuming the underlying reward distribution possesses a linear structure. Linear bandits achieve tremendous success such as online advertisement and recommendation systems (Li et al., 2010; Chu et al., 2011; Li et al., 2016).
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Due to the online learning nature of a bandit problem, we measure the performance of an agent via regret, which measures the differences of the rewards collected from the best arm to those collected from the agent. When the reward distribution is benign, e.g., with sub-Gaussian tails†, there are a number of efﬁcient algorithms (Bubeck and Cesa-Bianchi, 2012; Lattimore and Szepesvári, 2020),
AT )‡, where A is the number of arms and which obtain a worst-case regret bound of the form (cid:101)O(
T is the total number of arm pulls. Any algorithm with regret bound sublinear in T is effectively learning as its average regret tends to 0 when T → ∞. In the linear setting, a number of results (Dani et al., 2008; Abbasi-Yadkori et al., 2011) achieve (cid:101)O(poly(d)
T ) regret bounds, eliminating the dependence on A. Here d is the ambient dimension of the problem. A related performance measure is about the number of pulls to identify the best arm (Soare et al., 2014; Tao et al., 2018; Jedra and
Proutiere, 2020). We note that usually a regret minimization algorithm can be converted to identify the best arm with high probability.
√
√
Nevertheless, in many practical scenarios, we encounter non-sub-Gaussian noises in the observed payoffs, e.g., the price ﬂuctuations in ﬁnancial markets (Cont and Bouchaud, 2000; Rachev, 2003;
Hull, 2012), and/or ﬂuctuations of neural oscillations (Roberts et al., 2015). In such scenarios, the previously mentioned algorithms may fail. To tackle this problem, Bubeck et al. (2013) makes the
ﬁrst attempt to study the stochastic MAB problem with heavy-tailed noise. Speciﬁcally, for the rewards with a ﬁnite second order moment, by utilizing more robust statistical estimators, Bubeck et al. (2013) achieves regret bound of the same order as in the bounded/sub-Gaussian loss setting.
Then, Medina and Yang (2016); Shao et al. (2018); Xue et al. (2020) study the heavy-tailed linear bandits. They consider a general characterization of heavy-tailed payoffs in bandits, where the reward takes the form r = µ + η, where µ is an unknown but ﬁxed number and η is a random noise, whose distribution has a ﬁnite moment of order 1 + (cid:15). Here (cid:15) ∈ (0, 1]. For this setting, they establish a sublinear regret bound (cid:101)O(T 1+(cid:15) ). Unfortunately, when (cid:15) = 0, these regret bounds will be linear in
T , failing to learn in such situations. To further account for many such real-world scenarios, where the payoff noise has super-heavy tails (e.g., only (1 + (cid:15))-th moment for (cid:15) ∈ (0, 1) exists, or Cauchy distribution whose mean does not exist), new algorithms need to be developed: 1
Can we design an efﬁcient algorithm that provably learns for bandits with super heavy-tailed payoffs?
In this paper, we give the afﬁrmative answer to this question. Without loss of generality, we consider the linear setting, which includes MAB as a special case. In this setting, each arm is viewed as a vector in Rd. The random reward of the arm x is speciﬁed as θ(cid:62)x+η, where θ ∈ Rd is an unknown but ﬁxed vector and η is a super heavy-tailed symmetric random noise such that Pr(|η| > y) ≤ 1/yα for any y > 0 and some α > 0. One of the key challenges in this setting is that the mean of y may not exist.
The previous robust mean estimators (Bubeck et al., 2013), such as truncated empirical mean and median of means (Bubeck et al., 2013; Medina and Yang, 2016; Shao et al., 2018; Xue et al., 2020) which require the estimation of the mean, cannot effectively handle this super heavy-tailed noise. On the other hand, since the mean does not exist, we are also required to measure the performance of the agent with high-probability pseudo-regret (deﬁned in Section 2.2). To tackle these challenges, we propose a novel estimator: mean of medians. We then present a generic algorithmic framework to apply it in any existing bandit algorithm. Below, we summarize our contributions:
• We propose a novel robust statistical estimator: mean of medians. Speciﬁcally, we simply split (cid:101)n samples into k blocks and takes the mean of the median in each block. Theoretically, we can prove that, by utilizing (cid:101)n(α) samples, the super heavy-tailed noise is reduced to the bounded noise with high probability. Here (cid:101)n(α) is a constant which depends on α.
• For the super heavy-tailed linear bandits, we propose a new algorithmic framework. In detail, by simply combing the above mean of medians estimator and an arbitrary provably efﬁcient bandit algorithm, we obtain a new algorithm that can be proved efﬁcient for regret minimization problems and best arm identiﬁcation problems. Our obtained sample bounds and regret bounds can be nearly optimal.
†For any ζ > 0, a random variable X is said to be ζ-sub-Gaussian if it holds that E[et(X−E[X])] ≤ eζ2t2/2 for any t > 0.
‡ (cid:101)O(·) ignores logarithm factors. 2
• We instantiate our framework with Student’s t-noises and compare with previous methods.
Our experiments demonstrate our method can signiﬁcantly outperform existing algorithms in these environments, and is strictly consistent with our theoretical guarantees. 1.1