Abstract
Prior AI successes in complex games have largely focused on settings with at most hundreds of actions at each decision point. In contrast, Diplomacy is a game with more than 1020 possible actions per turn. Previous attempts to address games with large branching factors, such as Diplomacy, StarCraft, and Dota, used human data to bootstrap the policy or used handcrafted reward shaping. In this paper, we describe an algorithm for action exploration and equilibrium approximation in games with combinatorial action spaces. This algorithm simultaneously per-forms value iteration while learning a policy proposal network. A double oracle step is used to explore additional actions to add to the policy proposals. At each state, the target state value and policy for the model training are computed via an equilibrium search procedure. Using this algorithm, we train an agent, DORA, completely from scratch for a popular two-player variant of Diplomacy and show that it achieves superhuman performance. Additionally, we extend our methods to full-scale no-press Diplomacy and for the ﬁrst time train an agent from scratch with no human data. We present evidence that this agent plays a strategy that is incompatible with human-data bootstrapped agents. This presents the ﬁrst strong evidence of multiple equilibria in Diplomacy and suggests that self play alone may be insufﬁcient for achieving superhuman performance in Diplomacy. 1

Introduction
Classic multi-agent AI research domains such as chess, Go, and poker feature action spaces with at most thousands of actions per state, and are therefore possible to explore ex-haustively a few steps deep. In contrast, modern multi-agent
AI benchmarks such as StarCraft, Dota, and Diplomacy fea-ture combinatorial action spaces that are incredibly large. In
StarCraft and Diplomacy, this challenge has so far been ad-dressed by bootstrapping from human data [32, 24, 3, 11]. In
Dota, this challenge was addressed through careful expert-designed reward shaping [4].
In contrast, in this paper we describe an algorithm that trains agents through self play without any human data and can accommodate the large action space of Diplomacy, a long-standing AI benchmark game in which the number of legal actions for a player on a turn is often more than 1020 [24].
Figure 1: France (blue) would be in a strong position were it not for ex-actly two actions that Austria (red) can play to dislodge the unit in Tyr.
The probability of sampling one of them randomly is roughly 10−6.
We propose a form of deep RL (reinforcement learning) where at each visited state the agent approximates a Nash equilibrium for the stage game with 1-step lookahead search, plays the equilibrium policy, and uses the equilibrium value as the training target. Since the full ac-tion space is too large to consider, this equilibrium is computed over a smaller set of candidate 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
actions. These actions are sampled from an action proposal network that is trained in unison with the value model. We show that our deep Nash value iteration algorithm, when initialized with a human-bootstrapped model, defeats multiple prior human-derived agents in no-press Diplomacy by a wide margin.
Next, we investigate training an agent completely from scratch with no human data in Diplomacy. In order to improve action exploration when training from scratch, the proposal actions are augmented with exploratory actions chosen through a double-oracle-like process, and if these discovered actions end up as part of the equilibrium then they will be learned by the proposal network. Using this technique, which we call DORA (Double Oracle Reinforcement learning for Action exploration), we develop a superhuman agent for a popular two-player variant of Diplomacy. Our agent is trained purely through self-play with no human data and no reward shaping.
We also train a DORA agent for the 7-player game of no-press Diplomacy, the ﬁrst no-press Diplo-macy agent to be trained entirely from scratch with no human data. When 6 DORA agents play with 1 human-data-based agent, all human-data-based agents perform extremely poorly. However, when one DORA agent plays with 6 human-data-based agents, the bot underperforms. These results suggest that self-play in 7-player Diplomacy may converge to equilibria that are incompatible with typical human play. This is in contrast to previous large-scale multi-agent benchmarks like mul-tiplayer poker [9] and validates Diplomacy as a valuable benchmark for learning policies that are compatible with human behavior. 2