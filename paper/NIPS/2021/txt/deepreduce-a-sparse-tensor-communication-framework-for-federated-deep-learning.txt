Abstract
Sparse tensors appear frequently in federated deep learning, either as a direct artifact of the deep neural network’s gradients, or, as a result of an explicit sparsiﬁ-cation process. Existing communication primitives are agnostic to the challenges of deep learning; consequently, they impose unnecessary communication overhead.
This paper introduces DeepReduce, a versatile framework for the compressed communication of sparse tensors, tailored to federated deep learning. DeepReduce decomposes sparse tensors into two sets, values and indices, and allows both inde-pendent and combined compression of these sets. We support a variety of standard compressors, such as Deﬂate for values, and Run-Length Encoding for indices.
We also propose two novel compression schemes that achieve superior results: curve-ﬁtting based for values, and bloom-ﬁlter based for indices. DeepReduce is orthogonal to existing gradient sparsiﬁers and can be applied in conjunction with them, transparently to the end-user, to signiﬁcantly lower the communication overhead. As a proof of concept, we implement our approach on TensorFlow and PyTorch. Our experiments with real models demonstrate that DeepReduce transmits 320% less data than existing sparsiﬁers, without affecting accuracy.
Code is available at https://github.com/hangxu0304/DeepReduce. 1

Introduction
In federated learning [43, 47, 55, 72], the training is typically performed by a large number of resource-constrained client devices (e.g., smartphones), operating on their private data and computing a local model. Periodically, a subset of the devices is polled by a central server that retrieves their gradients, updates the global model and broadcasts it back to the clients. The most constrained resource in client devices is the network, either because the practically sustained bandwidth between remote clients and the server is low (typically, in the order of 25-50Mbps), or, the ﬁnancial cost (e.g., data plans for 4G/5G mobile connections) is high. On the other hand, deep neural network model sizes have been steadily increasing at a much faster rate than the available bandwidth. Consequently, in federated learning, it is imperative to reduce the communicated data volume.
One key observation that can help with reducing this volume is that the data exchanged during the Deep Neural Network (DNN) training often correspond to sparse tensors, i.e., tensors with many zero-value elements. Sparse tensors may be: (i) direct artifacts of the training process; for instance, the gradients of the NCF [35] and DeepLight [18] models consist of roughly 40% and 99% zero elements, respectively; or (ii) explicitly generated by sparsiﬁcation [6, 51, 69, 71, 76, 81], 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a)
Index
Value 1 7 0 0.0 4.0 4.6 0.0 5.2 0.0 0.0 5.8 6 2 3 4 5 (b) (1, 4.0)  (2, 4.6)  (4, 5.2)  (7, 5.8) (c) 0 i=0   i=1         i=2                i=3 1 1 0 0 1 0 1 (key,value)
V(i) = a * i + b, a=0.6  b=4.0
Size in bits : 8 * f32=256 4*(i32 + f32)=256 2*f32 + 8*b1=72
Figure 1: (a) Dense tensor format needs 256 bits; (b) sparse method sends parameters, (a, b) as well as an 8-bit string, i.e., 72 bits in total. key, value h i form also needs 256 bits; (c) our a commonly used lossy (e.g., Top-r or Random-r) compression approach that selects only a few elements (with r typically < 1%).
Figure 1.a depicts an example tensor containing 8 real values, 4 of which are zero; its dense represen-tation would require 256 bits. Typically, sparse tensors are represented as a set of pairs h (see Figure 1.b), where key is the index; notice, however, that the representation also requires 256 bits, negating the beneﬁt of sparsity. We show an example of our improved approach in Figure 1.c. We consider the indices as an ordered list represented by a Boolean array of 8 bits, such that the ith bit is ‘1’ if and only if the corresponding gradient element is non-zero. Moreover, we ﬁt a function, V (i) = a i + b to the gradient values, with parameters, a = 0.6 and b = 4.0. By transmitting only the bit string and parameters (a, b), we can reconstruct the original tensor while requiring only 72 bits. key, value i key, value i h
·
The above example demonstrates a signiﬁcant margin for additional compression for sparse tensors.
Recent works (e.g., SKCompress [40]) take advantage of these opportunities, but rely on a tightly coupled index and value compression algorithm that beneﬁts only some scenarios (see Section 6).
In practice, there exist complex trade-offs among data volume, model accuracy, and computational overhead, in conjunction with system aspects, such as the network bandwidth and the communication library (e.g., NCCL [2], or Gloo [29]). Given that no single solution ﬁts all scenarios, practitioners need the ﬂexibility to adjust how sparse tensors are compressed and transmitted for each particular
DNN model and system conﬁguration.
In this paper, we propose DeepReduce, a framework for transmitting sparse tensors via a wide-area network, tailored for large-scale federated DNN training. Our contributions include: (i) The DeepReduce framework, described in Section 3, that decomposes the sparse tensor into two sets, indices and values, and allows for independent and combined compression. By decoupling indices from values, our framework enables synergistic combination of a variety of compressors in a way that beneﬁts each particular DNN and training setup. DeepReduce resides between the machine learning framework (e.g., Tensorﬂow, PyTorch) and the communication library. It exposes an easy-to-use API that encapsulates a variety of existing methods, such as Run Length [83] and Huffman encoding [38] for index compression; as well as Deﬂate [20] and QSGD [7] for value compression.
DeepReduce also provides an index reordering abstraction, which is useful for combining value with index compressors. (ii) Two novel compressors for sparse tensors: a Bloom-ﬁlter based index compressor (Section 4) sparse representation; that reduces the size of keys by 50%, compared to the traditional and a curve-ﬁtting based value compressor (Section 5) that reduces the large values array to a small set of parameters. Both of our compressors do not affect the quality of the trained model. key, value h i (iii) An evaluation of DeepReduce on a variety of DNN models and applications (Section 6). We demonstrate the practical applicability of our framework by realistic federated learning deployments on geographically remote, large-scale cloud infrastructures, and show that DeepReduce compresses already-sparse data by up to 320%, without affecting the training quality. 2