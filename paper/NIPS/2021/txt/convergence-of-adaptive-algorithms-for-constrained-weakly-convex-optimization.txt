Abstract
We analyze the adaptive ﬁrst order algorithm AMSGrad, for solving a constrained stochastic optimization problem with a weakly convex objective. We prove the
˜O(t−1/2) rate of convergence for the squared norm of the gradient of Moreau envelope, which is the standard stationarity measure for this class of problems.
It matches the known rates that adaptive algorithms enjoy for the speciﬁc case of unconstrained smooth nonconvex stochastic optimization. Our analysis works with mini-batch size of 1, constant ﬁrst and second order moment parameters, and possibly unbounded optimization domains. Finally, we illustrate the applications and extensions of our results to speciﬁc problems and algorithms. 1

Introduction
Adaptive ﬁrst order methods have become a mainstay of neural network training in recent years.
Most of these methods build on the AdaGrad framework [14], which is a modiﬁcation of online gradient descent by incorporating the sum of the squared gradients in the step size rule. Based on the practical shortcomings of AdaGrad for training neural networks, RMSprop [32] and Adam [21] proposed to use exponential moving averages for gradients and squared gradients (also known as moment estimations) with parameters β1 and β2, respectively. These methods have seen a huge practical success.
The recent work [28] identiﬁed a technical issue that affects Adam and RMSprop and proposed a new
Adam-variant called AMSGrad that does not suffer from the same problem. Theoretical properties of AMSGrad, AdaGrad and their variants for nonconvex optimization problems are studied in a number of recent papers [3, 4, 11, 24, 34, 37]. These works focus on unconstrained smooth stochastic optimization, where the standard analysis framework of the stochastic gradient descent (SGD) [18] can be used. Convergence of adaptive methods for the more general setting of constrained and/or nonsmooth stochastic nonconvex optimization has remained open, while these settings have broad practical applications [7, 13, 20, 25, 27, 33].
In this work, we take a step towards this direction and establish the convergence of AMSGrad for solving the problem
{f (x) = Eξ∼P [f (x; ξ)]} , min x∈X (1) where f : Rd → R is ρ-weakly convex, X ⊆ Rd is closed convex, and ξ is a r.v. following a ﬁxed unknown distribution P. This template captures the setting of previous analyses when f is L-smooth, as this implies L-weak convexity, and X = Rd. However, there exist many applications when
X (cid:54)= Rd [20, 25, 27, 33] or when f is not L-smooth [7, Section 2.1],[13, 15].
Constrained stochastic minimization with nonconvexity presents challenges not met in the convex setting [5, 19]. In particular, until the recent work [7], even for SGD, increasing mini-batch sizes were required for convergence in constrained nonconvex optimization. To study the behavior of AMSGrad for solving (1), we build on the analysis framework of [7]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[4]
[5]
[7]
[26]
This work f
L-smooth
L-smooth
ρ-weak. cvx.
ρ-weak. cvx.
ρ-weak. cvx.
Constraints
× (cid:88) (cid:88) (cid:88) (cid:88)
β1 const. 0 0 const. const. mini-batch size Diagonal Adaptive 1
√ t
∼ 1 1 1 (cid:88) (cid:88)
×
× (cid:88) (cid:88) (cid:88)
×
× (cid:88)
Table 1: Comparison with adaptive methods for smooth nonconvex optimization and SGD-based methods for weakly convex optimization. Column “diagonal” refers to coordinatewise step sizes and “adaptive” refers to step sizes depending on observed gradients á la AdaGrad.
Contributions. We show that AMSGrad achieves O(log(T )/ for solving (1). Key speciﬁcations for this result are the following:
√
T ) rate for near-stationarity, see (5),
• We can use a mini-batch size of 1.
• We can use constant moment parameters β1, β2 which are used in practice [1, 4, 21, 28].
• We do not assume boundedness of the domain X .
Next, we particularize our results for constrained optimization with L-smooth objectives and for a variant of RMSprop. We also extend our analysis for the scalar version of AdaGrad with ﬁrst order moment estimation. For easy reference, we compare our results with state-of-the-art in Table 1.
Finally, in a numerical experiment for robust phase retrieval, we observe that AMSGrad is more robust to variation of initial step sizes, compared to SGD and SGD with momentum.
Algorithm 1 AMSGrad [28]
Input: x1 ∈ X , αt = α√ t
β1 < 1, β2 < 1, m0 = v0 = 0, ˆv0 = δ1, 1 ≥ δ > 0. for t = 1, 2 . . . T do
, for t ≥ 1, α > 0,
Sample ξt ∼ P iid and set gt such that Eξt[gt] ∈ ∂f (xt) (see Assumption 1) mt = β1mt−1 + (1 − β1)gt vt = β2vt−1 + (1 − β2)g2 t
ˆvt = max(ˆvt−1, vt) xt+1 = P ˆv1/2
X (xt − αtˆv−1/2 t mt) t end for
Output: xt∗ , where t∗ is selected uniformly at random from {1, . . . , T }. 1.1 Examples of weakly convex problems
The class of problems we consider in this paper include constrained problems with L-smooth objectives which are, for example, studied in [5] in the context of adversarial attacks and adaptive methods. Other important examples with weak convexity are composite objectives h(c(x)), where h is a convex Lipschitz continuous function and c is a smooth map with Lipschitz continuous Jacobian.
Concrete examples of weakly convex problems are listed in [7, Section 2.1], which include robust phase retrieval, sparse dictionary learning, Conditional Value-at-Risk, and many others. 1.2