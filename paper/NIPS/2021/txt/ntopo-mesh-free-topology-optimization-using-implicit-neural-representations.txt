Abstract
Recent advances in implicit neural representations show great promise when it comes to generating numerical solutions to partial differential equations. Compared to conventional alternatives, such representations employ parameterized neural networks to deﬁne, in a mesh-free manner, signals that are highly-detailed, contin-uous, and fully differentiable. In this work, we present a novel machine learning approach for topology optimization—an important class of inverse problems with high-dimensional parameter spaces and highly nonlinear objective landscapes. To effectively leverage neural representations in the context of mesh-free topology optimization, we use multilayer perceptrons to parameterize both density and dis-placement ﬁelds. Our experiments indicate that our method is highly competitive for minimizing structural compliance objectives, and it enables self-supervised learning of continuous solution spaces for topology optimization problems. 1

Introduction
Deep neural networks are starting to show their potential for solving partial differential equations (PDEs) in a variety of problem domains, including turbulent ﬂow, heat transfer, elastodynamics, and many more [1, 2, 3, 4, 5]. Thanks to their smooth and analytically-differentiable nature, implicit neural representations with periodic activation functions are emerging as a particularly attractive and powerful option in this context [4]. In this work, we explore the potential of implicit neural representations for structural topology optimization—a challenging inverse elasticity problem with widespread application in many ﬁelds of engineering [6].
Topology optimization (TO) methods seek to ﬁnd designs for physical structures that are as stiff as possible (i.e. least compliant) with respect to known boundary conditions and loading forces while adhering to a given material budget. While TO with mesh-based ﬁnite element analysis is a well-studied problem [7], we argue that mesh-free methods provide unique opportunities for machine learning. We propose the ﬁrst self-supervised, fully mesh-free method based on implicit neural representations for topology optimization. The core of our approach is formed by two neural networks: a displacement network representing force-equilibrium conﬁgurations that solve the forward problem, and a density network that learns optimal material distributions in the domain of interests. To leverage the power of these representations, we cast TO as a stochastic optimization problem using Monte Carlo sampling. Compared to conventional mesh-based TO, this setting 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
introduces new challenges that we must address. To account for the nonlinear nature of implicit neural representations, we introduce a convex density-space objective that guides the neural network towards desirable solutions. We furthermore introduce several concepts from FEM-based topology optimization methods into our learning-based Monte Carlo setting to stabilize the training process and to avoid poor local minima.
We evaluate our method on a set of standard TO problems in two and three dimensions. Our results indicate that neural topology optimization with implicit representations is able to match the performance of state-of-the-art mesh-based solvers. To further explore the potential advantages of this approach over conventional methods, we show how our formulation enables self-supervised learning of continuous solution spaces for this challenging class of problems.
Figure 1: Neural topology optimization pipeline. We compute optimal material distributions by alternately training two neural networks: the displacement network Φu and the density network Φρ, mapping spatial coordinates ω to equilibrium displacements u and optimal densities ρ, respectively.
In each iteration, we ﬁrst update Φu by minimizing the total potential energy of the system. We then perform sensitivity analysis to compute density-space gradients which, after applying our sensitivity
ﬁltering, give rise to target density ﬁelds ˆρ. Finally, we update Φρ by minimizing the convex objective
Ltopo based on mean squared error between current and target densities. 2