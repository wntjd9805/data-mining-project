Abstract
We study Online Lazy Gradient Descent for optimisation on a strongly convex
N ) regret against adversarial domain. The algorithm is known to achieve O( opponents; here we show it is universal in the sense that it also achieves O(log N ) expected regret against i.i.d opponents. This improves upon the more complex meta-algorithm of Huang et al [20] that only gets O(
N log N ) and O(log N ) bounds.
In addition we show that, unlike for the simplex, order bounds for pseudo-regret and expected regret are equivalent for strongly convex domains.
√
√ 1

Introduction i=1 ai · (xi − x∗) small for x∗ ∈ argmin{(cid:80)N
Online linear optimisation is a repeated game with one opponent. On turn n we know the cost vectors a1, a2, . . . , an−1 ∈ Rd and select an action xn from the domain X ⊂ Rd. The opponent observes xn and chooses the next cost vector an ∈ Rd and we pay cost an · xn. Our goal is to select actions to make the regret (cid:80)N
If the cost vectors are selected by an adversarial opponent then a1, a2, . . . , an−1 give no information about an and selecting a good action seems like an impossible task. Despite this, there are algorithms that get O(
N ) per turn which vanishes as N → ∞. The central algorithms in the ﬁeld are Hedge [18, 34]; Prod [9]; and (sub)Gradient Descent, which has Greedy [41] and Lazy [34] variants. All three algorithms get
O(
N ) regret. The ﬁrst two are specialised to X the simplex. The latter works for any compact convex domain. The bulk of online optimisation literature is reﬁnements of the central algorithms.
N ) regret against such opponents. The loss in performance against x∗ is O(1/ i=1 ai · x : x ∈ X}.
√
√
√
In many cases however the cost vectors are non adversarial. For example they might be determined by
ﬂuctuations in trafﬁc, the weather or the stock market over a short period of time. The standard model of easy opponents is cost vectors drawn independently from some ﬁxed distribution. In the easy setting the history a1, . . . , an provides information about the next cost vector. Indeed (cid:101)an+1 = a1+...+an is an increasingly reliable sequence of estimates for a = E[an+1] and it seems reasonable to play xn+1 to minimise (cid:101)an+1 · x. This is called Follow-the-Leader (FTL). For example if X is the simplex
FTL gives pseudo-regret E [ (cid:80)N i=1 a·(xi −y∗)] ≤ O(1) for y∗ ∈ argmin{a·x : x ∈ X}. Note this is weaker than bounding the expected regret E(cid:2) (cid:80)N i=1ai · (xi − x∗)(cid:3). On the other hand FTL gives no guarantee against adversarial opponents, and we might not know in advance whether the opponent is i.i.d or adversarial. Hence we would like an algorithm that is universal in that it always gets O(
N ) regret but specialises to a much stronger bound against i.i.d opponents.
√ n
Recently Huang et al [20] solved this problem for smooth, strongly convex domains. For these domains they showed FTL gives O(log N ) regret if (cid:107) (cid:80)n i=1 ai(cid:107) grows linearly.1 The result was generalised to nonsmooth domains by [22, 26]. Huang et al then combine FTL with the (A,B)-Prod
N log N ) regret against adversarial opponents. In this work meta-algorithm of [31] to ensure O(
√ 1 Without smoothness they show the growth condition occurs with high probability against i.i.d opponents. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Algorithm 1: Online Lazy Gradient Descent
Data: Cost vectors a1, a2, . . . ∈ Rd. Parameter η > 0. Domain X ⊂ Rd. 1 for n = 0, 1, . . . do xn+1 = ΠX
Receive an+1 and pay cost an+1 · xn+1
− η√ n i=1 ai (cid:80)n (cid:17) (cid:16) 2 3 we improve and extend their results. In particular we show the meta-algorithm is unnecessary as the much simpler Online Lazy Gradient Descent already achieves O(
N ) and O(log N ) bounds.
√
Terminology and Notation
Throughout (cid:107) · (cid:107) is the Euclidean norm. Given a vector subspace V ⊂ Rd and linear operator
A : V → V write the operator norm as (cid:107)A(cid:107) = max{(cid:107)Av(cid:107) : v ∈ V, (cid:107)v(cid:107) = 1}. It is known (see (cid:112)|λj| for λj the eigenvalues of AT A. Moreover if
[10] Theorems 4.1 and 4.2) that (cid:107)A(cid:107) = maxj
A is symmetric we have (cid:107)A(cid:107) = maxj |µj| for µj the eigenvalues of A. The set Z ⊂ Rd is called
λ-strongly convex to mean for each x, y ∈ Z and α ∈ [0, 1] the ball B(αx + (1 − α)y, r) is contained 2 α(1 − α)(cid:107)x − y(cid:107)2. The pseudo-regret P[RN ] and expected regret E[RN ] are deﬁned: in Z for r = λ (cid:34) N (cid:88)
P[RN ] = E i=1 a·xi − min x∈X (cid:35) a·x
N (cid:88) i=1
E[RN ] = E ai ·xi − min x∈X (cid:34) N (cid:88) i=1 (cid:35)
N (cid:88) ai ·x i=1 (1)
The Jensen Inequality implies P[RN ] ≤ E[RN ]. We write ΠX for the Euclidean projection onto the domain X. The Online Lazy Subgradient algorithm is given as Algorithm 1. Note this is different from the so-called Greedy Subgradient algorithm with starting point x1 = 0 and recursive update xn+1 = ΠX (xn −ηan/ n). Section 5 Figure 2(b) suggests Greedy Subgradient is not universal.
√
Summary of Contributions
Throughout we make the following assumptions on the domain and cost vectors. Both assumptions are nontrivial. For example Assumption 1 holds on the Euclidean and (cid:96)p balls for p ∈ (1, 2] but not for the simplex or polytopes. Assumption 2 on the cost vectors is a typical model of an easy opponent and fails if the opponent is intelligent and adapts to our past moves.
Assumption 1. The domain X ⊂ Rd is compact, m-strongly convex and contains the origin. Write
D = max{(cid:107)x(cid:107) : x, y ∈ X}. The boundary M = ∂X is a (d−1)-dimensional C 2 manifold. Namely each z ∈M has a neighborhood U in Rd and C 2 function F : U → R with nonzero gradient such that
M∩U = {x ∈ U : F (x) = 0}. Such a function is called a coordinate patch at z.
Henceforth write N (x) for the outwards unit normal at x ∈ M. To represent N : M → Rd locally we can choose a coordinate patch F : U → R at x∗ and write ∇N = ∇F (cid:107)∇F (cid:107) . Since F is C 2 the matrix
∇N of partial derivatives exists.
Assumption 2. The cost vectors a1, a2, . . . are i.i.d with expectation E[an] = a. For all n we have (cid:107)an(cid:107) ≤ L and (cid:107)an − a(cid:107) ≤ R and E(cid:107)an − a(cid:107) ≤ δ. Let x∗ ∈ argmin{a · x : x ∈ X} be the expected minimiser. The domain X being strongly convex implies x∗ is unique.
Our main result is Theorem 3 which says Online Lazy Gradient Descent achieves O(cid:0) L2 m(cid:107)a(cid:107) log N (cid:1) expected-regret under Assumptions 1 and 2. This bound is essentially tight by [20] Theorem 9.
The improved behaviour against i.i.d opponents is remarkable since the algorithm was designed with the adversarial case in mind, and predates the recent interest in universal algorithms. Our second contribution is Theorem 2 which states the gap between expected regret and pseudo-regret is (cid:1). Theorem 2 is of independent interest since it fails if the domain is the simplex.
O(cid:0) L2 m(cid:107)a(cid:107)
From a technical standpoint, our Gradient Descent analysis goes beyond that of [20, 22, 26] for FTL.
Their central idea is that the normal vector to M at xn ∈ argmin{(a1 + . . .+an−1) · x : x ∈ X} points along An−1 = − a1+...+an−1
. Using (cid:107)An −An−1(cid:107) = O(1/n) and how strong convexity says the unit normal changes quickly as we vary the basepoint, they prove (cid:107)Θn −Θn+1(cid:107) = O(1/n) for the normal vectors Θn, Θn+1 at xn, xn+1. Summing the resulting series gives a logarithmic regret n 2
√ n). and the resulting series give only an O( bound. Unfortunately this method is insufﬁcient to analyse Gradient Descent instead of FTL as it
N ) bound. Thus we only gives (cid:107)Θn −Θn+1(cid:107) = O(1/ follow a new approach where we prove a high probability bound (cid:107)N (xn)− N (x∗)(cid:107) = O(1/ n) for the unit normals. For comparison existing works do not mention the expected minimiser x∗ at all.
Then we use strong convexity to see the boundary surface locally looks like a quadratic with axis pointing along −a. Hence (cid:107)xn −x∗(cid:107) = O(1/ n) and only O(1/n) of the displacement is in the a-direction; and the larger orthogonal component does not contribute to pseudo-regret. Summing the series gives an O(log N ) bound for pseudo-regret. To convert this into a bound for expected regret we use our Theorem 2 which says the gap between expected regret and pseudo-regret is ﬁnite.
√
√
√
Our differential geometry methods are more nuanced than existing works. Existing works consider only the magnitude (cid:107)xn − xn+1(cid:107) but we consider both the magnitude and direction of xn − x∗. The relevant object is ∇N (z)v in Lemmas 1-5 and Proposition 1 which captures the rate of change of the unit normal as we perturb the basepoint in the v-direction. 2