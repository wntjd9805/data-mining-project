Abstract
Decentralized optimization is emerging as a viable alternative for scalable dis-tributed machine learning, but also introduces new challenges in terms of synchro-nization costs. To this end, several communication-reduction techniques, such as non-blocking communication, quantization, and local steps, have been explored in the decentralized setting. Due to the complexity of analyzing optimization in such a relaxed setting, this line of work often assumes global communication rounds, which require additional synchronization. In this paper, we consider decentralized optimization in the simpler, but harder to analyze, asynchronous gossip model, in which communication occurs in discrete, randomly chosen pairings among nodes.
Perhaps surprisingly, we show that a variant of SGD called SwarmSGD still con-verges in this setting, even if non-blocking communication, quantization, and local steps are all applied in conjunction, and even if the node data distributions and un-derlying graph topology are both heterogenous. Our analysis is based on a new connection with multi-dimensional load-balancing processes. We implement this algorithm and deploy it in a super-computing environment, showing that it can outperform previous decentralized methods in terms of end-to-end training time, and that it can even rival carefully-tuned large-batch SGD for certain tasks. 1

Introduction
Decentralized optimization has recently emerged as a promising approach for scaling the distributed training of machine learning models, in particular via stochastic gradient descent (SGD) [Lian et al., 2017, Tang et al., 2018, Koloskova et al., 2019a]. Its key advantage is that it removes the need for a central coordinator node in distributed training, and therefore it can allow for high scaling.
The general decentralized optimization setting is the following: we are given n nodes, each with a subset of data from some distribution, which can communicate over some underlying graph topol-ogy. In each global round, each node samples some local data, performs a local gradient step, and it is paired with a neighbor, which may be chosen randomly. The nodes exchange model infor-mation pairwise, and then update their models, often via direct model averaging. Variants of this setting have been analyzed since pioneering work by Tsitsiklis [1984], for various estimation and optimization algorithms [Xiao and Boyd, 2004, Nedic and Ozdaglar, 2009, Johansson et al., 2009,
Shamir and Srebro, 2014] and have seen renewed interest given its applicability to training deep neural networks (DNNs) at scale, e.g. [Lian et al., 2017, 2018, Assran et al., 2018].
Recently, there has been signiﬁcant focus on reducing the synchronization overheads for decentral-ized training, usually employing three approaches: 1) implementing faster non-blocking communi-cation between communication partners at a round [Lian et al., 2018, Assran et al., 2018], which may cause them to see stale versions of their models, 2) allowing nodes to take local steps in between 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
their communication rounds [Wang and Joshi, 2018, Koloskova et al., 2020], and 3) applying quan-tization to the communication [Lu and De Sa, 2020, Tang et al., 2018, Koloskova et al., 2019a,b].
The above impressive line of work contributes a rich set of algorithmic and analytic ideas; however, one common limitation is that the algorithms are usually set in the synchronous gossip model, which requires all nodes to perform their communication in lock-step rounds, and share a common notion of time, thus reducing their practicality. To mitigate this fact, some references, e.g. [Lian et al., 2018, Assran et al., 2018, Lu and De Sa, 2020] partially relax this requirement, although they do so at the cost of additional assumptions, or reduced guarantees, as we discuss in related work. Another relative limitation is that the analyses are usually customized to the bespoke communication-reduced methods being applied, and therefore are hard to generalize to other methods.
Our Contribution. In this paper, we consider decentralized SGD-based optimization in the simpler, but harder to analyze, asynchronous gossip model [Xiao and Boyd, 2004], in which communication occurs in discrete, randomly chosen pairings among nodes, and does not require a common notion of time. We prove that a new variant of SGD we call SwarmSGD converges in this setting, even though it supports all three communication-reduction approaches mentioned above in conjunction.
Our analysis generalizes to heterogeneous data distributions and communication topologies.
At a high level, SwarmSGD works as follows. Each node i maintains a local model estimate Xi based on which gradients are generated, and a shared buffer where quantized models are stored for communication with other nodes. In each step, node i ﬁrst computes a sequence of H local gradient steps, which it does not yet apply. Next, the node chooses communication partner j, uniformly at random among its neighbors. Then, node i reads from its own communication buffer and from the communication buffer of j, obtaining quantized models Qi and Qj. A subtlety here is that Qi is not necessarily the quantized version of the model Xi, since other nodes can write concurrently to i’s buffer. The node i then averages Qi with Qj, and updates the neighbor’s remote buffer to the quantized average. Finally, it applies its local gradient steps to the resulting average, adopts this as its next model Xi, and a writes quantized version of it in its own shared buffer. This procedure can be implemented in a deadlock-free, non-blocking manner, by using either shared-memory or the remote direct-memory access (RDMA) calls supported by MPI [Woodall et al., 2006]. Importantly, the communication partner j does not need to block its computation during communication, and may be contacted by more than one interaction partner during a single local step, although we do assume that individual reads and writes are performed atomically.
A key component of this procedure is the quantization scheme: directly using an unbiased quantizer, e.g. [Alistarh et al., 2017] would destroy convergence guarantees, as the quantization error would be proportional to the model norm, which may not be bounded. Instead, we use a customized variant of the quantization scheme of Davies et al. [2021], whose error depends on the distance between the point being quantized (the model), and an arbitrary reference point, provided as a parameter. We prove that each node can reliably use its own model as a reference point to quantize and de-quantize messages placed in its buffer by other nodes. In turn, this requires care in the analysis.
Speciﬁcally, the key observation behind our analysis is exactly in showing that the nodes’ local mod-els stay well-enough concentrated around their mean throughout optimization to allow for correct decoding of quantized models, which in turn implies joint convergence by the nodes towards a point of vanishing gradient. This concentration follows via a non-trivial super-martingale argument. If nodes take a constant number of local SGD steps between communication steps, then SwarmSGD has Θ( n) speedup to convergence for non2-convex objectives. This matches results from previous work which considered decentralized dynamics but with global synchronization [Lian et al., 2017].
Experimental Validation. We apply SwarmSGD to train deep neural networks on image classiﬁ-cation and machine translation (NMT) tasks, deployed on the Piz Daint supercomputer [Piz, 2019].
Experiments conﬁrm the intuition that the average synchronization cost of SwarmSGD per itera-tion is low: it stays at less than 10% of the batch computation time, and remains constant as we increase the number of nodes. For example, using SwarmSGD, we are able to train a Transformer-XL [Vaswani et al., 2017] model on WMT17 (En-Ge) 1.5× faster than a highly-optimized large-batch SGD baseline, and to slightly higher accuracy, without additional hyper-parameter tuning. At the same time, due to the reduced communication frequency, Swarm also improves upon the speed of the previous practical decentralized methods, e.g. [Lian et al., 2017, 2018, Assran et al., 2018].
Importantly, we also note that, in less overparametrized settings such as training residual CNNs [He et al., 2016] on ImageNet [Russakovsky et al., 2015], nodes do need to perform more iterations over the dataset relative to the baseline in order to recover full accuracy. This is predicted by the analy-sis, and conﬁrms similar ﬁndings in previous work [Assran et al., 2018]. Overall, our method does
√ 2
√
T n) convergence rates, under analytical assumptions. appear well-suited to training large modern models at node counts where global synchronization among all nodes is prohibitively expensive.