Abstract
Deep neural networks have achieved human-level capabilities in various learning tasks. However, they generally lose performance in more realistic scenarios like learning in a continual manner. In contrast, humans can incorporate their prior knowledge to learn new concepts efﬁciently without forgetting older ones. In this work, we leverage meta-learning to encourage the model to learn how to learn continually. Inspired by human concept learning, we develop a generative classiﬁer that efﬁciently uses data-driven experience to learn new concepts even from few samples while being immune to forgetting. Along with cognitive and theoretical insights, extensive experiments on standard benchmarks demonstrate the effectiveness of the proposed method. The ability to remember all previous concepts, with negligible computational and structural overheads, suggests that generative models provide a natural way for alleviating catastrophic forgetting, which is a major drawback of discriminative models. The code is publicly available at https://github.com/aminbana/GeMCL. 1

Introduction
Deep learning methods have gained great success in a wide variety of tasks in various ﬁelds [26, 28].
However, in many real-world scenarios, where the data samples are neither i.i.d. nor available all at once, most of them lose performance on the older data points; a phenomenon known as catastrophic forgetting [13]. Continual learning, also called lifelong learning, aims to tackle this problem [48].
Animals as intelligent learning agents can easily learn a sequence of different tasks with minimum interference and forgetting [55, 12]. When facing new problems, the brain does not rewire all of the wirings [37, 5] but utilizes lifelong accumulated inductive biases to efﬁciently learn new concepts
[9, 4]. In the ﬁeld of cognitive and neuroscience, there exist two special cases of concept learning named prototypical and exemplar-based learning [44, 2, 59, 7]. Bayesian inference is another aspect of learning in the brain which aims to consider uncertainty [24, 31, 16, 4, 23]. In this paper, we integrate the hypothesis of generative concept learning and the Bayesian approach by considering a distribution over each concept. Figure 1 compares different characteristics and expressive power of the mentioned concept learning approaches. Inspired by the biological mechanism of learning, we suggest that the generative approach in category learning is an important principle for continual learning problems. Although the Bayesian approach requires proper priors about each concept,
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A simple schematic of exemplar-based learning (left), prototypical-based learning (center), and the proposed Bayesian approach (right). Exemplar-based learning suffers from high memory consumption and also the risk of overﬁtting, like 1-NN classiﬁers. Prototypical-based learning is memory efﬁcient but also prone to underﬁtting. Our probabilistic approach enables using complex enough distributions over each category to capture class variations. meta-learning can be used to provide data-driven prior knowledge or statistics from the previously encountered concepts similar to the biological intelligent systems [4].
Meta-learning is an endeavor to gather experience from solving related problems in order to solve new unseen ones more efﬁciently; which have been used to improve the learning process in different areas [19]. In this work, we employ meta-learning to derive proper inductive biases as well as a powerful feature extractor for a continual learning problem from a collection of similar continual learning problems. By taking advantage of these inductive biases, we formulate the category learning problem from a probabilistic generative approach. Our main contributions are as follows:
• We take motivation from cognitive and neuroscience about meta-continual learning in intelligent animals and especially make use of bio-inspired concept learning to eradicate catastrophic forgetting.
• We investigate the problem of discriminative approaches in continual learning and propose generative classiﬁers as a general way of mitigating catastrophic forgetting.
• Encouraged by generative classiﬁers, we propose a new discriminative classiﬁer called
PGLR, to reduce the inherent catastrophic forgetting problem of discriminative approaches.
• Existing prototypical learning is improved by adding extra statistics through the Bayesian approach.
• Different components of the proposed method can be implemented in a fully incremental setting which is useful in the shortage of data or computational resources.
• We achieve state-of-the-art accuracy with a notable gap compared to the existing methods. 2