Abstract
In this paper, we study gradient methods for training deep linear neural networks with binary cross-entropy loss. In particular, we show global directional conver-gence guarantees from a polynomial rate to a linear rate for (deep) linear networks with spherically symmetric data distribution, which can be viewed as a speciﬁc zero-margin dataset. Our results do not require the assumptions in other works such as small initial loss, presumed convergence of weight direction, or overparam-eterization. We also characterize our ﬁndings in experiments. 1

Introduction
Local v.s. non-local analysis. Deep neural networks have been successfully trained with simple gradient-based methods, despite the inherent non-convexity of the objective function. Recently, a number of works proved the convergence of gradient methods to global minima for ultra-wide neural networks [1, 4, 12, 13, 20, 31]. These works essentially performed “local analysis” because in their proofs the parameters stay close to initialization during training. In the large width setting, there exists a global minimum near a random initial point, thus staying in the local region can still result in convergence to global minima. Despite the technical convenience of handling a local region, in practical training, the parameters often travel far from initialization (see, e.g., [15]). To understand the practical optimization trajectory, it is important to develop a non-local convergence analysis.
Deep linear nets with quadratic loss: “local analysis”. To understand the behavior of gradient descent (GD) for general neural nets, we need to understand GD for deep linear nets. A number of works analyzed linear networks with the square loss. Bartlett et al. [7] provided that gradient descent converges to the target matrix at a linear rate from identity initialization, while assuming the target matrix is either close to identity or positive deﬁnite. Arora et al. [2] proved linear convergence of deep linear networks if the initialization has a positive “deﬁciency margin" and is nearly balanced. Later, a few works followed a similar idea to neural tangent kernel (NTK) [20] to establish convergence analysis. Du and Hu [11] showed GD with Gaussian random initialization converges to a global minimum at a linear rate if the width of every hidden layer is large enough. Hu et al. [19] improved the width requirement to be independent of depth, by utilizing orthogonal weight initialization, but they assume each layer to have the same width. All the above works are not non-local analysis.
Deep linear nets with quadratic loss: non-local analysis. Eftekhari [14] provided non-local convergence analysis for deep linear nets with quadratic loss. When one layer has only one neuron (including scalar output case) and assumming that the input data are whitened, they proved that gradient ﬂow converges to global minimizers starting from balanced initialization. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we are interested in the classiﬁcation task with binary cross-entropy loss. To our knowledge, there was no non-local convergence analysis of gradient ﬂow for deep linear nets in such a scheme.
Deep linear nets with logit loss: ﬁnal-phase analysis. Several recent works [17, 38, 23, 21, 37] studied the exponential-type loss for deep linear networks. Speciﬁcally, Gunasekar et al. [17], Nacson et al. [38] proved the convergence to max-margin solution, but under the assumption that the weight direction and the loss have converged to global optima, which is a “ﬁnal-point analysis”. Lyu and
Li [37], Ji and Telgarsky [21, 23] also proved the convergence to max-margin solution under the assumption that the initial point has already obtained zero classiﬁcation error (weaker assumption than
[17, 38]), i.e., they analyzed the “ﬁnal phase” of training. In this work, we would like to understand the entire training dynamics, not just the “late training” period. 1.1 Our Contributions
In this paper, we analyze gradient ﬂow for deep linear networks with logit loss (i.e. binary cross-entropy loss). The main contributions of this paper are summarized as follows:
• Convergence result. We prove the global convergence of gradient ﬂow for minimizing a population logit loss with deep linear nets, under the assumption that the input data is spherically symmetric (Theorem 3). This assumption covers the standard Gaussian distribution and uniform distribution on the sphere. To our knowledge, this is the ﬁrst global analysis of gradient ﬂow on deep linear nets with logit loss, though under certain strong assumptions. We emphasize that our analysis is beyond the “lazy training” scheme.
• Convergence rate. We also establish explicit convergence rate of gradient ﬂow (Theorem 3).
Denote θ(t) as the angle between we(t) and v, where we(t) is the collection of parameters at time t following gradient ﬂow, and v is the directional global minimizer. In the ﬁrst phase, we have polynomial convergence rate of cos(θ(t)), that is, cos(θ(t)) ≥ 1 − O(cid:0)(N1t)− 2
N1 π (cid:1), where N1 + 2 is the depth of the deep linear network. In the second phase, we have linear convergence rate of cos(θ(t)), that is, cos(θ(t)) ≥ 1 − eO(−t). And the second phase begins when the induced weight norm changes from descending to ascending mentioned below.
• Weight norm change pattern. We prove that the induced weight norm goes through descending and ascending periods. If the initial induced weight norm starts with descending behavior, then after ﬁnite time, it will change to ascending and continues increasing to inﬁnity. If the initial induced weight norm begins with ascending behavior, then it would increase to inﬁnity directly.
We also verify our results in numerical experiments including the descending and ascending behavior of weight norm and the convergence rates in our setting. 1.2 Additional