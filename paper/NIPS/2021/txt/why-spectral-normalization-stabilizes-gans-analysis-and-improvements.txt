Abstract
Spectral normalization (SN) [30] is a widely-used technique for improving the stability and sample quality of Generative Adversarial Networks (GANs). However, current understanding of SN’s efﬁcacy is limited. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the suc-cessful LeCun initialization [25]. This connection helps to explain why the most popular implementation of SN for GANs [30] requires no hyper-parameter tuning, whereas stricter implementations of SN [15, 12] have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, SN preserves this property throughout training. Build-ing on this theoretical understanding, we propose a new spectral normalization technique: Bidirectional Scaled Spectral Normalization (BSSN), which incorpo-rates insights from later improvements to LeCun initialization: Xavier initialization
[13] and Kaiming initialization [17]. Theoretically, we show that BSSN gives better gradient control than SN. Empirically, we demonstrate that it outperforms
SN in sample quality and training stability on several benchmark datasets. 1

Introduction
Generative adversarial networks (GANs) are state-of-the-art deep generative models, perhaps best known for their ability to produce high-resolution, photorealistic images [14]. The objective of GANs is to produce random samples from a target data distribution, given only access to an initial set of training samples. This is achieved by learning two functions: a generator G, which maps random input noise to a generated sample, and a discriminator D, which tries to classify input samples as either real (i.e., from the training dataset) or fake (i.e., produced by the generator). In practice, these functions are implemented by deep neural networks (DNNs), and the competing generator and discriminator are trained in an alternating process known as adversarial training. Theoretically, given enough data and model capacity, GANs converge to the true underlying data distribution [14].
Although GANs have been very successful in improving the sample quality of data-driven generative models [22, 8], their adversarial training also contributes to instability. That is, small hyper-parameter changes and even randomness in the optimization can cause training to fail. Many approaches have been proposed for improving the stability of GANs, including different architectures [38, 22, 8], loss functions [2, 3, 16, 50], and various types of regularizations/normalizations [30, 9, 41]. One of the most successful proposals to date is called spectral normalization (SN) [30, 15, 12]. SN forces each layer of the discriminator to have unit spectral norm during training. This has the effect of controlling the Lipschitz constant of the discriminator, which is empirically observed to improve the stability of
GAN training [30]. Despite the successful applications of SN [8, 28, 53, 21, 52, 31, 27], to date, it remains unclear precisely why this speciﬁc normalization is so effective. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The interesting connections we ﬁnd between spectral normalizations and prior initialization techniques: (1) LeCun initialization [25] can help explain why SN avoids vanishing gradients; (2)
Motivated by newer initialization techniques [13, 17], we propose BSSN to further improve SN.
In this paper, we show that SN controls two important failure modes of GAN training: exploding gradients and vanishing gradients. These problems are well-known to cause instability in GANs
[3, 8], leading either to bad local minima or stalled training prior to convergence. We make three primary contributions: (1) Analysis of why SN avoids exploding gradients (§ 3). Poorly-chosen architectures and hyper-parameters, as well as randomness during training, can amplify the effects of large gradients on training instability, ultimately leading to generalization error in the learned discriminator. We theoretically prove that SN upper bounds gradients during GAN training, mitigating these effects. (2) Analysis of why SN avoids vanishing gradients (§ 4).
Small gradients during training are known to cause GANs (and other DNNs) to converge to bad models [25, 3]. The well-known LeCun initialization, ﬁrst proposed over two decades ago, mitigates this effect by carefully choosing the variance of the initial weights [25]. We prove theoretically that SN controls the variance of weights in a way that closely parallels LeCun initialization. Whereas LeCun initialization only controls the gradient vanishing problem at the beginning of training, we show empirically that SN preserves this property throughout training. Our analysis also explains why a strict implementation of SN [12] has poor out-of-the-box performance on GANs and requires additional tuning to avoid the vanishing gradient problem, whereas the implementation of SN in [30] requires no tuning. (3) Improving SN with the above theoretical insights (§ 5). Given this new understanding of the connections between SN and LeCun initialization, we propose Bidirectional Scaled Spectral Normal-ization (BSSN), a simple modiﬁcation of SN that combines two key insights (Fig. 1): (a) It introduces a novel bidirectional spectral normalization inspired by Xavier initialization, which improved on
LeCun initialization by controlling not only the variances of internal outputs, but also the variance of backpropagated gradients [13]. We theoretically prove that BSSN mimics Xavier initialization to give better gradient control than SN. (b) BSSN introduces a new scaling of weights inspired by Kaiming initialization, a newer initialization technique that has better performance in practice [17]. We show that BSSN achieve better sample quality and training stability than SN on several benchmark datasets, including CIFAR10, STL10, CelebA, and ImageNet.
Note that our goal in this work is not to propose the best normalization or regularization technique for training GANs; there has been substantial work in this area already with promising results
[34, 48, 20, 11]. Our goal is instead to understand why SN has been so effective, and evaluate whether these insights can inform more effective alternatives. Our theoretical and empirical results suggest that gradient control may play a signiﬁcant role in this story (though they need not be the only factor explaining SN’s success). 2