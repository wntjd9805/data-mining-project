Abstract
We consider the problem of minimizing a convex function that is evolving in time according to unknown and possibly stochastic dynamics. Such problems abound in the machine learning and signal processing literature, under the names of concept drift and stochastic tracking. We provide novel non-asymptotic convergence guar-antees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. Notably, we show that the tracking efﬁciency of the proximal stochastic gradient method depends only logarithmically on the initialization quality when equipped with a step-decay schedule. 1

Introduction
Stochastic optimization underpins much of machine learning theory and practice. Signiﬁcant progress has been made over the last two decades in the ﬁnite-time analysis of stochastic approximation algorithms; see, e.g., [1, 2, 6, 7, 8, 21, 24, 29, 30]. The predominant assumption in this line of work is that the distribution generating the data is ﬁxed throughout the run of the process. There is no shortage of problems, however, where this assumption is grossly violated for reasons beyond the learner’s control. Indeed, data often shifts and evolves over time for reasons that may be independent of the learning process.
Two examples are worth highlighting. The ﬁrst is a classical problem in signal processing related to stochastic tracking [19, 27], wherein the learning algorithm aims to track over time a moving target driven by an unknown stochastic process. The second example is the concept drift phenomenon in online learning [14, 28], wherein the true hypothesis may be changing over time, as in topic modeling or spam classiﬁcation. An important goal in online problems, and the one we adopt here, is to track as closely as possible an unknown sequence of minimizers or minimal values. The tracking error efﬁciency of stochastic algorithms in online settings is much less developed than sample complexity guarantees for static problems.
We present ﬁnite-time efﬁciency estimates in expectation and with high probability for the tracking error of the proximal stochastic gradient method under time drift. Our results concisely explain the interplay between the learning rate, the noise variance in the gradient oracle, and the strength of the time drift. The high-probability results merely assume that the gradient noise and time drift have light tails. Moreover, none of the results require the objectives to have bounded domains.
While conventional wisdom and previous work recommend the use of constant step sizes under time drift, we show in an important regime that a signiﬁcantly better step size schedule is one that is geometrically decaying to a “critical step size”. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1