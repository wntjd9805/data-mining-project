Abstract
We give relative error coresets for training linear classiﬁers with a broad class of loss functions, including the logistic loss and hinge loss. Our construction achieves (1 ± (cid:15)) relative error with ˜O(d · µy(X)2/(cid:15)2) points, where µy(X) is a natural complexity measure of the data matrix X ∈ Rn×d and label vector y ∈ {−1, 1}n, introduced in [MSSW18]. Our result is based on subsampling data points with probabilities proportional to their (cid:96)1 Lewis weights. It signiﬁcantly improves on existing theoretical bounds and performs well in practice, outperforming uniform subsampling along with other importance sampling methods. Our sampling dis-tribution does not depend on the labels, so can be used for active learning. It also does not depend on the speciﬁc loss function, so a single coreset can be used in multiple training scenarios. 1

Introduction
Coresets are an important tool in scalable machine learning. Given n data points and some objective function, we seek to select a subset of m (cid:28) n data points such that minimizing the objective function on those points (possibly where selected points are weighted non-uniformly) will yield a near minimizer over the full dataset. Coresets have been applied to problems ranging from clustering
[HPM04, FL11], to principal component analysis [CEM+15, FSS20], to linear regression [DMM06,
DDH+09, CWW19], to kernel density estimation [PT20], and beyond [AHPV05, BLK17, SS18].
We study coresets for linear classiﬁcation. Given a data matrix X ∈ Rn×d, with ith row xi and a label vector y ∈ {−1, 1}n, the goal is to compute β∗ = arg minβ∈Rd L(β), where L(β) = (cid:80)n i=1 f ((cid:104)xi, β(cid:105) · yi) for a classiﬁcation loss function f , such as the logistic loss f (z) = ln(1 + e−z) used in logistic regression or hinge loss f (z) = max(0, 1 − z) used in soft-margin SVMs.
We seek to select a subset of m (cid:28) n points xi1, . . . , xim along with a corresponding set of weights w1, . . . , wm such that, for some small (cid:15) > 0 and all β ∈ Rd, (cid:12) (cid:12) (cid:12) wj · f ((cid:104)xij , β(cid:105) · yij ) − L(β) (cid:12) (cid:12) (cid:12)
≤ (cid:15) · L(β). m (cid:88) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (1) j=1
This relative error coreset guarantee ensures that if ˜β ∈ Rd is computed to be the minimizer of the weighted loss over our m selected points, then L( ˜β) ≤ 1+(cid:15) 1−(cid:15) · L(β∗).
It is well known that common classiﬁcation loss functions such as the log and hinge losses do not admit relative error coresets with o(n) points [MSSW18]. To address this issue, Munteanu et al. introduce a natural notion of the complexity of the matrix X and label vector y, which we will also use to parameterize our results: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Deﬁnition 1 (Classiﬁcation Complexity Measure [MSSW18]). For any X ∈ Rn×d, y ∈ {−1, 1}n,
, where Dy ∈ Rn×n is a diagonal matrix with y as its diagonal, let µy(X) = supβ(cid:54)=0 and (DyXβ)+ and (DyXβ)− denote the set of positive and negative entries in DyXβ. (cid:107)(DyXβ)+(cid:107)1 (cid:107)(DyXβ)−(cid:107)1
Roughly, µy(X) is large when there is some parameter vector β ∈ Rd that produces signiﬁcant imbalance between correctly classiﬁed and misclassiﬁed points. This can occur e.g., when the data is exactly separable. However, as argued in [MSSW18], we typically expect µy(X) to be small. 1.1 Our Results
Our main result, formally stated in Corollary 9, is that sampling ˜O points according to the (cid:96)1 Lewis weights of X and reweighting appropriately, yields a relative error coreset satisfying (1) for the logistic loss, the hinge loss, and generally a broad class of ‘hinge-like’ losses. This signiﬁcantly improves the previous state-of-the-art using the same µy(X) parameterization, which was ˜O
[MSSW18]. See Table 1 for a detailed comparison with prior work. (cid:17) (cid:16) d3·µy(X)3 (cid:15)4 (cid:16) d·µy(X)2 (cid:15)2 (cid:17)
Theoretical Approach. The Lewis weights are a measure of the importance of rows in X, originally designed to sample rows in order to preserve (cid:107)Xβ(cid:107)1 for any β ∈ Rd [CP15]. They can be viewed as an (cid:96)1 generalization of the leverage scores which are used in applications where one seeks to preserve (cid:107)Xβ(cid:107)2 [CLM+15]. Like the leverage scores, the Lewis weights can be approximated very efﬁciently, in ˜O(nnz(X) + dω) time where ω ≈ 2.37 is the constant of fast matrix multiplication. They can also be approximated in streaming and online settings [BDM+20]. Our coreset constructions directly inherit these computational properties. i=1 f ((cid:104)xi, β(cid:105) · yi) concentrates only better under sampling than (cid:80)n
The (cid:96)1 Lewis weights are a natural sampling distribution for hinge-like loss functions, including the logistic loss, hinge loss, and the ReLU. These functions grow approximately linearly for positive z, but asymptote at 0 for negative z. Thus, ignoring some technical details, it can be shown that (cid:80)n i=1 |(cid:104)xi, β(cid:105) · yi| = (cid:107)DyXβ(cid:107)1.
As shown by Cohen and Peng [CP15], taking ˜O(d/(cid:15)2) samples according to the Lewis weights of
X (which are the same as those of DyX) sufﬁces to approximate (cid:107)DyXβ(cid:107)1 for all β ∈ Rd up to (1 ± (cid:15)) relative error. We show in Thm. 8 using contraction bounds for Rademacher averages that it in turn sufﬁces to approximate (cid:80)n i=1 f ((cid:104)xi, β(cid:105) · yi) up to additive error roughly (cid:15)((cid:107)X(cid:107)1 + n). We then simply show in Corollaries 6 and 9 that by setting (cid:15)(cid:48) = Θ((cid:15)/µy(X)) and applying Def. 1, this result yields a relative error coreset for a broad class of hinge-like loss functions including the ReLU, the log loss, and the hinge loss. (cid:17)
Samples (cid:16) d·µ(X)2 (cid:15)2 (cid:16) d3·µ(X)3 (cid:15)4 n·d3/2·µ(X) (cid:15)2 (cid:17)
˜O
˜O (cid:16) √
˜O
˜O (cid:16) n1−κd (cid:15)2 (cid:17) (cid:17) (cid:16) √ d (cid:15)
O (cid:17)
Error relative relative relative
Loss log, hinge
ReLU log log relative log, hinge additive (cid:15)n log
Assumptions
Distribution
Ref.
Def. 1
Def. 1
Def. 1 (cid:107)xi(cid:107)2 ≤ 1 ∀i regularization nκ(cid:107)β(cid:107)1, nκ(cid:107)β(cid:107)2, or nκ(cid:107)β(cid:107)2 2 (cid:107)β(cid:107)2, (cid:107)xi(cid:107)2 ≤ 1 ∀i (cid:96)1 Lewis
Cors. 6, 9 sqrt lev. scores
[MSSW18] sqrt lev. scores
[MSSW18] uniform
[CIM+19] deterministic
[KL19]
Table 1: Comparison to prior work. ˜O(·) hides logarithmic factors in the problem parameters. We note that the bounded norm assumption of [CIM+19] can be removed by simply scaling X, giving a dependence on the maximum row norm of X in the sample complexity. The importance sampling distributions of our work and [MSSW18] are both in fact a mixture with uniform sampling. Our work and [KL19, CIM+19] generalize to broader classes of loss functions – for simplicity here we focus just on the important logistic loss, hinge loss, and ReLU. 2
Experimental Evaluation. In Sec 5, we compare our Lewis weight-based method to the square root of leverage score method of [MSSW18], uniform sampling as studied in [CIM+19], and an oblivious sketching algorithm of [MOW21]. We study performance in minimizing both the log and hinge losses, with and without regularization. We observe that our method typically far outperforms uniform sampling, even in some cases when regularization is used. It performs comparably to the method of [MSSW18], seeming to outperform when the µy(X) complexity parameter is large. 1.2