Abstract
A key challenge for reinforcement learning is solving long-horizon planning prob-lems. Recent work has leveraged programs to guide reinforcement learning in these settings. However, these approaches impose a high manual burden on the user since they must provide a guiding program for every new task. Partially ob-served environments further complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible conﬁguration of the hidden regions of the environment. We propose a new approach, model predictive program synthesis (MPPS), that uses program synthesis to automatically generate the guiding programs. It trains a generative model to predict the unobserved portions of the world, and then synthesizes a program based on samples from this model in a way that is robust to its uncer-tainty. In our experiments, we show that our approach signiﬁcantly outperforms non-program-guided approaches on a set of challenging benchmarks, including a 2D Minecraft-inspired environment where the agent must complete a complex sequence of subtasks to achieve its goal, and achieves a similar performance as using handcrafted programs to guide the agent. Our results demonstrate that our approach can obtain the beneﬁts of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task. 1

Introduction
Reinforcement learning is a prominent technique for solving challenging planning and control problems [50, 4]. Despite signiﬁcant recent progress, solving long-horizon problems remains a signiﬁcant challenge due to the combinatorial explosion of possible strategies. One promising approach to addressing these issues is to leverage programs to guide the behavior of the agents [3, 62, 39]. The approaches in this paradigm typically involve three key elements:
• Domain-speciﬁc language (DSL): For a given domain, the user deﬁnes a set of components c that correspond to intermediate subgoals that are useful for that domain (e.g., “get wood” or “build bridge”), but leaves out how exactly to achieve these subgoals.
• Task-speciﬁc program: For every new task in the domain, the user provides a sequence of components (i.e. a program written in the DSL) that, if followed, enable the agent to achieve its goal in the task (e.g., [“get wood”; “build bridge”; “get gem”]).
• Low-level neural policy: For a given domain, the reinforcement learning algorithm learns an option [63] that implements each component (i.e., achieves the subgoal speciﬁed by that component). Typically a neural policy is learned as each option.
∗Correspondence to yicheny@csail.mit.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Given a new task in a domain, the user provides a program in the DSL that describes a high-level strategy to solve that task. The agent then executes the program by deploying the sequence of learned options that correspond to the components in that program.
A key drawback of this approach is programming overhead: for every new task (a task consists of an instantiation of an environment and a goal), the user must analyze the environment, design a strategy to achieve the goal, and encode the strategy into a program, with a poorly written program producing a suboptimal agent. Furthermore, partially observed environments signiﬁcantly complicate the programming task because the program must implement a strategy that correctly, and ideally optimally, handles every possible conﬁguration of the hidden regions of the environment.
To address this challenge, we propose a new approach, model predictive program synthesis (MPPS), that automatically synthesizes the guiding programs for program guided reinforcement learning.
MPPS works with a conditional generative model of the environment and a high level speciﬁcation of the goal of the task to automatically synthesize a program that achieves the goal, with the synthesized program robust to uncertainty in the model. Because the automatically generated agent, and not the user, reasons about how to solve each new task, MPPS signiﬁcantly reduces user burden. Given a goal speciﬁcation φ, the agent uses the following three steps to choose its actions:
• Hallucinator: First, inspired by world-models [29], the agent keeps track of a conditional generative model g over possible realizations of the unobserved portions of the environment.
• Synthesizer: Next, the agent synthesizes a program p that achieves φ assuming the hallu-cinator g is accurate. Since world predictions are stochastic in nature, it samples multiple predicted worlds and computes the program that maximizes the probability of success.
• Executor: Finally, the agent executes the options corresponding to the components in the program p = [c1; ...; ck] for a ﬁxed number of steps N .
If φ is not satisﬁed after N steps, then the above process is repeated. Since the hallucinator now has more information (because the agent has explored more of the environment), the agent now has a better chance of achieving its goal. Importantly, the agent is implicitly encouraged to explore since it must do so to discover whether the current program can successfully achieve the goal φ.
We instantiate our approach in the context of a 2D Minecraft-inspired environment [3, 57, 62], which we call “craft,” and a “box-world” environment [76]. We demonstrate that our approach signiﬁcantly outperforms non-program-guided approaches, while achieving a similar performance as using handcrafted programs to guide the agent. In addition, we demonstrate that the policy we learn can be transferred to a continuous variant of the craft environment, where the agent is replaced by a MuJoCo [66] Ant. Thus, our approach can obtain the beneﬁts of program-guided reinforcement learning without requiring the user to provide a new guiding program for every new task.2