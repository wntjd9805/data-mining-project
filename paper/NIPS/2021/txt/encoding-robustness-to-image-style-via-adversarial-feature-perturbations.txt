Abstract
Adversarial training is the industry standard for producing models that are ro-bust to small adversarial perturbations. However, machine learning practitioners need models that are robust to other kinds of changes that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by directly perturbing feature statistics, rather than image pixels, to produce models that are robust to various un-seen distributional shifts. We explore the relationship between these perturbations and distributional shifts by visualizing adversarial features. Our proposed method,
Adversarial Batch Normalization (AdvBN), is a single network layer that generates worst-case feature perturbations during training. By ﬁne-tuning neural networks on adversarial feature distributions, we observe improved robustness of networks to various unseen distributional shifts, including style variations and image corrup-tions. In addition, we show that our proposed adversarial feature perturbation can be complementary to existing image space data augmentation methods, leading to improved performance. The source code and pre-trained models are released at https://github.com/azshue/AdvBN. 1

Introduction
Figure 1: Images from ImageNet variants along with classiﬁcation scores by a pre-trained
ResNet-50. The image of column (a) is from ImageNet validation set. Dataset of column (d) is collected independently of the ImageNet dataset. Dataset of Column (f) is generated by our
Adversarial Batch Normalization module. Details on how we generate column (f) can be found in
Section 3.
Robust optimization for neural networks has been a major focus of recent research. A mainstream approach to reducing the brittleness of classiﬁers is adversarial training, which solves a min-max 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
optimization problem in which an adversary makes perturbations to images to degrade network performance, while the network adapts its parameters to resist degradation [9, 18, 25]. The result is a hardened network that is no longer brittle to small perturbations to input pixels. While adversarial training makes networks robust to adversarial perturbations, it does not address other forms of brittleness that plague vision systems. For example, shifts in image style, lighting, color mapping, and domain shifts can still severely degrade the performance of neural networks [12].
We propose adapting adversarial training to make neural networks robust to changes in image style and appearance, rather than small perturbations at the pixel level. We formulate a min-max game in which an adversary chooses adversarial feature statistics, and network parameters are then updated to resist these changes in feature space that correspond to appearance differences of input images. This game is played until the network is robust to a variety of changes in image space including texture, color, brightness, etc.
The idea of adversarial feature statistics is inspired by the observation that the mean and variance of feature maps encode style information, and thus enable the transfer of style information from a source image to a target image through normalization [15, 40]. Unlike standard approaches that rely on feature statistics from auxiliary images to deﬁne an image style, we use adversarial optimization of feature statistics to prepare classiﬁers for the worst-case style that they might encounter.
We propose training with Adversarial Batch Normalization (AdvBN) layers. Before each gradient update, the AdvBN layer performs an adversarial feature shift by re-normalizing with the most damaging mean and variance. By using this layer in a robust optimization framework, we create networks that are resistant to various domain shifts representable by shifts in feature statistics. An advantage of this method is that it does not require additional auxiliary data from new domains. We show that robust training with AdvBN layer hardens classiﬁers against changes in image appearance and style [6, 45], as well as common image corruptions [12]. Besides classiﬁcation, the effectiveness of AdvBN is also shown in the task of semantic segmentation, where it improves cross-domain generalization. 2