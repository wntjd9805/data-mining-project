Abstract
Quantile (and, more generally, KL) regret bounds, such as those achieved by
NormalHedge (Chaudhuri, Freund, and Hsu 2009) and its variants, relax the goal of competing against the best individual expert to only competing against a majority of experts on adversarial data. More recently, the semi-adversarial paradigm (Bilodeau, Negrea, and Roy 2020) provides an alternative relaxation of adversarial online learning by considering data that may be neither fully adversarial nor stochastic (i.i.d.). We achieve the minimax optimal regret in both paradigms using FTRL with separate, novel, root-logarithmic regularizers, both of which can be interpreted as yielding variants of NormalHedge. We extend existing KL regret upper bounds, which hold uniformly over target distributions, to possibly uncountable expert classes with arbitrary priors; provide the ﬁrst full-information lower bounds for quantile regret on ﬁnite expert classes (which are tight); and provide an adaptively minimax optimal algorithm for the semi-adversarial paradigm that adapts to the true, unknown constraint faster, leading to uniformly improved regret bounds over existing methods. 1

Introduction
We focus on the setting of learning with expert advice [V90; LW94], where in each round the learner selects a probability distribution over experts, observes the loss of each expert, and incurs the average loss of the experts under the learner’s selected distribution. The learner’s objective is to minimize regret against some mixture of the experts, which is the difference between their cumulative loss and the cumulative loss of the expert mixture over T rounds.
The classical “worst-case” online learning paradigm assumes that the losses are adversarial—that is, they are chosen to make the learner perform as poorly as possible—and demands that the learner competes against the best-performing expert. However, there are many real-world settings where this assumption is too pessimistic, and consequently we focus on designing algorithms with provable guarantees that adapt to easier notions of both data and performance measures. A non-exhaustive list
∗Equal contribution authors. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
of work on “easy data” includes Freund and Schapire [FS97], Cesa-Bianchi, Mansour, and Stoltz
[CMS07], van Erven, Grünwald, Koolen, and de Rooij [v+11], and Gaillard, Stoltz, and van Erven
[GSv14], all of which use variants of the Hedge algorithm [FS97] to obtain regret bounds in terms of data-dependent quantities. Ideally, such quantities are small when the data are “easy” to predict.
In this work, we focus on two paradigms beyond the classical worst-case: ﬁrst, we consider relaxing the performance measure to quantile (KL) regret, which measures the ability of an algorithm to compete against an unknown mixture of the experts that potentially performs worse than a point-mass on the single best expert, and second, we consider regret within the semi-adversarial paradigm, which deﬁnes a spectrum of constraints on the permissible data distributions between stochastic and adversarial. The concept of (cid:15)-quantile regret was introduced by Chaudhuri, Freund, and Hsu [CFH09], in which the player competes against the (cid:98)(cid:15)N (cid:99) best experts (out of N total) rather than the single best.
The authors demonstrated empirically that Hedge does poorly in this paradigm, and introduced a new algorithm NormalHedge with an upper bound on quantile regret of (cid:112)(T + (log N )2)(1 + log(1/(cid:15))).
Later algorithms improved it to (cid:112)T (1 + log(1/(cid:15))) [CV10; OP16], removing the dependency on N .
The semi-adversarial paradigm considers constraining the adversary’s choice of data distributions, which was ﬁrst motivated by Rakhlin, Sridharan, and Tewari [RST11]. Bilodeau, Negrea, and Roy
[BNR20] extended this idea, deﬁning adaptive minimax regret with respect to such constraints and providing an efﬁcient algorithm with corresponding regret bounds.
Contributions While the best known algorithms for the above two paradigms are intrinsically different, we show that the follow-the-regularized-leader (FTRL) algorithm with new root-logarithmic regularizers achieves minimax optimal performance for both quantile and semi-adversarial regret.
First, we provide the ﬁrst FTRL algorithm with minimax optimal quantile regret guarantees, and do so without using the additive normalization step of previous algorithms [CL06, Section 2.1].
We achieve root-KL bounds that hold uniformly over target distributions on (possibly uncountable) expert classes with arbitrary priors, and reduce to the optimal quantile regret for discrete uniform priors. Moreover, we prove matching lower bounds for quantile regret with ﬁnite expert classes, demonstrating the optimality of known upper bounds (including our own) for the ﬁrst time. Finally, in the semi-adversarial paradigm, we improve the dependence on the number of experts in the regret bound, obtaining uniformly improved upper bounds over previous work.
We achieve the above results through a novel local-norm analysis of FTRL with linearly decomposable regularizers on general (possibly uncountable) expert spaces. We use this analysis in conjunction with basic conditions on the ﬁrst and second derivatives to design and analyze the root-logarithmic regularizers. We believe that this approach is fundamentally different from existing ones and could lead to further advances in obtaining optimal algorithms. In fact, there exist results stating if a regret bound is achievable by some algorithm, then that same bound is nearly achievable by mirror descent with some potential function (see, e.g., Srebro, Sridharan, and Tewari [SST11, Thm 9]). However, it is not clear how to design such a function in practice. In contrast, our general FTRL bound reduces the choice of regularizer to a single univariate function, and clariﬁes how fundamental properties of this function lead to trade-offs in the regret bound.