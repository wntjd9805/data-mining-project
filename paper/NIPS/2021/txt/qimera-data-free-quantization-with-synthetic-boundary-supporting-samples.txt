Abstract
Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. How-ever, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We ﬁnd that this is often insufﬁcient to capture the distribution of the original data, especially around the de-cision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reﬂect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera. 1

Introduction
Among many neural network compression methodologies, quantization is considered a promising direction because it can be easily supported by accelerator hardwares [1] than pruning [2] and is more lightweight than knowledge distillation [3]. However, quantization generally requires some form of adjustment (e.g., ﬁne-tuning) using the original training data [4, 5, 6, 7, 8] to restore the accuracy drop due to the quantization errors. Unfortunately, access to the original training data is not always possible, especially for deployment in the ﬁeld, for many reasons such as privacy and security. For example, the data could be medical images of patients, photos of conﬁdential products, or pictures of military assets.
Therefore, data-free quantization is a natural direction to achieve a highly accurate quantized model without accessing any training data. Among many excellent prior studies [9, 10, 11, 12], generative methods [13, 14, 15] have recently been drawing much attention due to their superior performance.
Generative methods successfully generate synthetic samples that resemble the distribution of the original dataset and achieve high accuracy using information from the pretrained full-precision network, such as batch-normalization statistics [15, 13] or intermediate features [14].
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, a signiﬁcant gap still exists between data-free quantized models and quantized models ﬁne-tuned with original data. What is missing from the current generative data-free quantization schemes?
We hypothesize that the synthetic samples of conventional methods lack boundary supporting samples [16], which lie on or near the decision boundary of the full-precision model and directly affect the model performance. The generator designs are often based on conditional generative adversarial networks (CGANs) [17, 18] that take class embeddings representing class-speciﬁc latent features. Based on these embeddings as the centroid of each class distribution, generators rely on the input of random Gaussian noise vectors to gain diverse samples. However, one can easily deduce that random noises have difﬁculty reﬂecting the complex class boundaries. In addition, the weights and embeddings of the generators are trained with cross-entropy (CE) loss, further ensuring that these samples are well-separated from each other.
In this work, we propose Qimera, a method for data-free quantization employing superposed latent embeddings to create boundary supporting samples. First, we conduct a motivational experiment to conﬁrm our hypothesis that samples near the boundary can improve the quantized model performance.
Then, we propose a novel method based on inputting superposed latent embeddings into the generator to produce synthetic boundary supporting samples. In addition, we provide two auxiliary schemes for
ﬂattening the latent embedding space so that superposed embeddings could contain adequate features.
Qimera achieves signiﬁcant performance improvement over the existing techniques. The experimental results indicate that Qimera sets new state-of-the-art performance for various datasets and model settings for the data-free quantization problem. Our contributions are summarized as the following:
• We identify that boundary supporting samples form an important missing piece of the current state-of-the-art data-free compression.
• We propose using superposed latent embeddings, which enables a generator to synthesize boundary supporting samples of the full-precision model.
• We propose disentanglement mapping and extracted embedding initialization that help train a better set of embeddings for the generator.
• We conduct an extensive set of experiments, showing that the proposed scheme outperforms the existing methods. 2