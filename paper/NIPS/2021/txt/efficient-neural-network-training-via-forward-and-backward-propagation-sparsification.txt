Abstract
Sparse training is a natural idea to accelerate the training speed of deep neural net-works and save the memory usage, especially since large modern neural networks are signiﬁcantly over-parameterized. However, most of the existing methods cannot achieve this goal in practice because the chain rule based gradient (w.r.t. structure parameters) estimators adopted by previous methods require dense computation at least in the backward propagation step. This paper solves this problem by proposing an efﬁcient sparse training method with completely sparse forward and backward passes. We ﬁrst formulate the training process as a continuous minimiza-tion problem under global sparsity constraint. We then separate the optimization process into two steps, corresponding to weight update and structure parameter update. For the former step, we use the conventional chain rule, which can be sparse via exploiting the sparse structure. For the latter step, instead of using the chain rule based gradient estimators as in existing methods, we propose a variance reduced policy gradient estimator, which only requires two forward passes without backward propagation, thus achieving completely sparse training. We prove that the variance of our gradient estimator is bounded. Extensive experimental results on real-world datasets demonstrate that compared to previous methods, our algo-rithm is much more effective in accelerating the training process, up to an order of magnitude faster. 1

Introduction
In the last decade, deep neural networks (DNNs) [35, 11, 38] have proved their outstanding per-formance in various ﬁelds such as computer vision and natural language processing. However, training such large-sized networks is still very challenging, requiring huge computational power and storage. This hinders us from exploring larger networks, which are likely to have better performance.
Moreover, it is a widely-recognized property that modern neural networks are signiﬁcantly over-parameterized, which means that a fully trained network can always be sparsiﬁed dramatically by network pruning techniques [9, 8, 25, 46, 20] into a small sub-network with negligible degradation in accuracy. After pruning, the inference efﬁciency can be greatly improved. Therefore, a natural question is can we exploit this sparsity to improve the training efﬁciency?
The emerging technique called sparse network training [10] is closely related with our question, which can obtain sparse networks by training from scratch. We can divide existing methods into two categories, i.e., parametric and non-parametric, based on whether they explicitly parameterize network structures with trainable variables (termed structure parameters). Empirical results [24, 34, 44, 23] demonstrate that the sparse networks they obtain have comparable accuracy with those
∗Equal contribution
†Jointly with Google Research 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
obtained from network pruning. However, most of them narrowly aim at ﬁnding a sparse subnetwork instead of simultaneously sparsifying the computation of training by exploiting the sparse structure.
As a consequence, it is hard for them to effectively accelerate the training process in practice on general platforms, e.g., Tensorﬂow [1] and Pytorch [31]. Detailed reasons are discussed below:
• Non-parametric methods ﬁnd the sparse network by repeating a two-stage procedure that alternates between weight optimization and pruning [10, 6], or by adding a proper sparsity-inducing regularizer on the weights to the objective [22, 41]. The two-stage methods prune the networks in weight space and usually require retraining the obtained subnetwork from scratch every time when new weights are pruned, which makes training process even more time-consuming. Moreover, the computation of regularized methods is dense since the gradients of a zero-valued weights/ﬁlters are still nonzero.
• All the parametric approaches estimate the gradients based on chain rule. The gradient w.r.t. the structure parameters can be nonzero even when the corresponding channel/weight is pruned. Thus, to calculate the gradient via backward propagation, the error has to be propagated through all the neurons/channels. This means that the computation of backward propagation has to be dense. Concrete analysis can be found in Section 3.
We notice that some existing methods [4, 28] can achieve training speedup by careful implementation.
For example, the dense to sparse algorithm [28] removes some channels if the corresponding weights are quite small for a long time. However, these methods always need to work with a large model at the beginning epochs and consume huge memory and heavy computation in the early stage. Therefore, even with such careful implementations, the speedups they can achieve are still limited.
In this paper, we propose an efﬁcient channel-level parametric sparse neural network training method, which is comprised of completely sparse (See Remark 1) forward and backward propagation. We adopt channel-level sparsity since such sparsity can be efﬁciently implemented on the current training platforms to save the computational cost. In our method, we ﬁrst parameterize the network structure by associating each ﬁlter with a binary mask modeled as an independent Bernoulli random variable, which can be continuously parameterized by the probability. Next, inspired by the recent work [47], we globally control the network size during the whole training process by controlling the sum of the Bernoulli distribution parameters. Thus, we can formulate the sparse network training problem into a constrained minimization problem on both the weights and structure parameters (i.e., the probability). The main novelty and contribution of this paper lies in our efﬁcient training method called completely sparse neural network training for solving the minimization problem. Speciﬁcally, to fully exploit the sparse structure, we separate training iteration into two parts, i.e., weight update and structure parameter update. For weight update, the conventional backward propagation is used to calculate the gradient, which can be sparsiﬁed completely because the gradients of the ﬁlters with zero valued masks are also zero. For structure parameter update, we develop a new variance reduced policy gradient estimator (VR-PGE). Unlike the conventional chain rule based gradient estimators (e.g., straight through[2]), VR-PGE estimates the gradient via two forward propagations, which is completely sparse because of the sparse subnetwork. Finally, extensive empirical results demonstrate that our method can signiﬁcantly accelerate the training process of neural networks.
The main contributions of this paper can be summarized as follows:
• We develop an efﬁcient sparse neural network training algorithm with the following three appealing features: – In our algorithm, the computation in both forward and backward propagations is completely sparse, i.e., they do not need to go through any pruned channels, making the computational complexity signiﬁcantly lower than that in standard training. – During the whole training procedure, our algorithm works on small sub-networks with the target sparsity instead of follows a dense-to-sparse scheme. – Our algorithm can be implemented easily on widely-used platforms, e.g., Pytorch and
Tensorﬂow, to achieve practical speedup.
• We develop a variance reduced policy gradient estimator VR-PGE speciﬁcally for sparse neural network training, and prove that its variance is bounded.
• Experimental results demonstrate that our methods can achieve signiﬁcant speed-up in training sparse neural networks. This implies that our method can enable us to explore larger-sized neural networks in the future. 2
Remark 1. We call a sparse training algorithm completely sparse if both its forward and backward propagation do not need to go through any pruned channels. For such algorithms, the computational cost in forward and backward propagation cost can be roughly reduced to ρ2 ∗ 100%, with ρ being the ratio of remaining unpruned channels. 2