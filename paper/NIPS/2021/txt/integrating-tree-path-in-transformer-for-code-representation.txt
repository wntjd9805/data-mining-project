Abstract
Learning distributed representation of source code requires modelling its syntax and semantics. Recent state-of-the-art models leverage highly structured source code representations, such as the syntax trees and paths therein. In this paper, we investigate two representative path encoding methods shown in previous research work and integrate them into the attention module of Transformer. We draw inspiration from the ideas of positional encoding and modify them to incorporate these path encoding. Speciﬁcally, we encode both the pairwise path between tokens of source code and the path from the leaf node to the tree root for each token in the syntax tree. We explore the interaction between these two kinds of paths by integrating them into the uniﬁed Transformer framework. The detailed empirical study for path encoding methods also leads to our novel state-of-the-art representation model TPTrans, which ﬁnally outperforms strong baselines.
Extensive experiments and ablation studies on code summarization across four different languages demonstrate the effectiveness of our approaches. We release our code at https://github.com/AwdHanPeng/TPTrans. 1

Introduction
Machine learning for source code aims at building models that learn semantic embedding of programs.
The initial representation of source code relied on sequential models adopted from natural language processing, such as n-gram language model [Hindle et al., 2016, Hellendoorn and Devanbu, 2017],
Recurrent Neural Networks (RNNs) [Wei et al., 2019] and so on. However, source code is more logical than natural languages, rich in structured information such as Abstract Syntax Tree (AST).
Therefore, these previous works struggle to capture the structural complexity of source code.
Some works leverage the program AST to model source code structure and linearize the graph by traversing it. Code2seq [Alon et al., 2018] represents source code as a set of pairwise paths over
AST where each path is compressed to a vector using LSTMs [Hochreiter and Schmidhuber, 1997].
Code2seq obtains state-of-the-art for code representation using only AST information, demonstrating the effectiveness of path encoding. [Kim et al., 2020] leveraged another kind of path from the leaf node to the root of AST by traversing up its ancestors and then coupled the representation of paths with the source code.
On the other hand, several works also leverage structured graph neural networks for modelling source code directly. [Allamanis et al., 2017] used Gated Graph Neural Network (GGNN) to embed the semantically meaningful relationships in the code. [Zhou et al., 2019] proposed a novel graph neural network with composite programming representation for vulnerability identiﬁcation. However, GNNs
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Example of (a) a python code snippet and (b) the syntax tree of it. The relative path across the syntax tree between tokens for and result represents the relationship between them, showing the pattern of for loop in the code snippet. Meanwhile, the absolute path for token result starts at the leaf node and ends at the tree root, which reveals the behaviour of assignment for this token. typically rely on synchronous message passing, requiring message passing iterations to aggregate information [Allamanis et al., 2017, Fernandes et al., 2018]. Besides, primarily for computational reasons, GNNs for programs usually compute few message-passing iterations. For these reasons, the representation of GNNs tends to be local and struggle to leverage long-range interactions.
On the contrary, Transformer [Vaswani et al., 2017] allows global information ﬂow at each step but lacks the ability to model the structural complexity of source code. Recently, some works have explored introducing structural inductive bias in Transformer to access the global structural representation of source codes. [Hellendoorn et al., 2019] proposed Graph Relation Embedding
Attention Transformer (GREAT), which extends [Shaw et al., 2018] and biases Transformer with relational information from graph edge types. After that, [Zügner et al., 2021] proposed Code
Transformer based on XLNet [Yang et al., 2019], which computes pairwise distances on AST and integrate multiple relations into the attention module.
In this paper, we pursue the research line of combining Transformer with additional structure information of source code. Our ﬁrst starting point is the structural model of Code2seq [Alon et al., 2018], which obtains state-of-the-art of code summarization using only pairwise path information in AST. However, Code2seq lacks the modelling of context, which leads us to explore combining path representation with source code context. The other starting point is Code Transformer [Zügner et al., 2021], which counts node distances cross AST to capture source code structure. However, the different nodes combinations of paths contain plenty of structure information, which is overlooked by only encoding distances.
To overcome the drawback shown in previous works, we adopt the ideas of encoding path to represent source code and integrate them to bias the attention module of Transformer. Speciﬁcally, we encode the path between source code tokens across AST and the path from leaf node to tree root for each token. Then we draw inspiration from both relative and absolute positional encoding methods in the NLP ﬁeld and modify them to integrate path encodings into Transformer. These paths introduce inductive bias into the attention module of Transformer, powering it to know the structure of source codes. For clarity, we name the ﬁrst path as the relative path and the last one as the absolute path (see Fig1), since much similarities between path and position on relative or absolute encoding exist.
Intuitively, the relative path represents the structural relationship between tokens and shows speciﬁc patterns of code block such as For loop, etc. Meanwhile, the absolute path complements the structural information for each token and reveals the program behaviour on it, such as Assignment and Call, etc. Both two kinds of paths contain plenty of structural information of code syntax. Actually, lots of works have introduced these paths into many novel models [Kim et al., 2020, Alon et al., 2018, 2020]. However, to our best knowledge, the feature relationship between these two paths for code representation learning is still not completely studied. In this paper, we integrate these paths into the uniﬁed Transformer framework to analyze each effectiveness and their relationship. The detailed 2
empirical study also leads to our novel code representation model TPTrans, which means encoding the Tree Path into the Transformer. We show the effectiveness of our approaches on the code summarization task across four different languages datasets, in which the model predicts a method’s name based on its body. Our model ﬁnally outperforms solid baselines and obtains state-of-the-art on most datasets. The contributions of this paper are summarized as follows: 1. We propose TPTRANS which integrates path encoding in Transformer, powers it to know the structure of source code, and our model signiﬁcantly outperforms existing baselines. 2. We present the empirical study to investigate the relationship between two kinds of path encoding methods proposed in previous works, shedding light on this line of future work. 2