Abstract
We show a generic reduction from multiclass differentially private PAC learning to binary private PAC learning. We apply this transformation to a recently proposed binary private PAC learner to obtain a private multiclass learner with sample com-plexity that has a polynomial dependence on the multiclass Littlestone dimension and a poly-logarithmic dependence on the number of classes. This yields a doubly exponential improvement in the dependence on both parameters over learners from previous work. Our proof extends the notion of Ψ-dimension deﬁned in work of
Ben-David et al. [5] to the online setting and explores its general properties. 1

Introduction 1 Machine learning and data analytics are increasingly deployed on sensitive information about individuals. Differential privacy [10] gives a mathematically rigorous way to enable such analyses while guaranteeing the privacy of individual information. The model of differentially private PAC learning [17] captures binary classiﬁcation for sensitive data, providing a simple and broadly appli-cable abstraction for many machine learning procedures. Private PAC learning is now reasonably well-understood, with a host of general algorithmic techniques, lower bounds, and results for speciﬁc fundamental concept classes [8, 12, 3, 4, 1, 15, 2, 16].
Beyond binary classiﬁcation, many problems in machine learning are better modeled as multiclass learning problems. Here, given a training set of examples from domain X with labels from [k] =
{0, 1, . . . , k}, the goal is to learn a function h : X → [k] that approximately labels the data and generalizes to the underlying population from which it was drawn. Much less is presently known about differentially private multiclass learnability than is known about private binary classiﬁcation, though it appears that many speciﬁc tools and techniques can be adapted one at a time. In this work, we ask: Can we generically relate multiclass to binary learning so as to automatically transfer results from the binary setting to the multiclass setting?
To illustrate, there is a simple reduction from a given multiclass learning problem to a sequence of binary classiﬁcation problems. (This reduction was described by Ben-David et al. [5] for non-private 1The full version of this paper with all of the details can be found at https://arxiv.org/abs/2107.10870 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learning, but works just as well in the private setting.) Intuitively, one can learn a multi-valued label one bit at a time. That is, to learn an unknown function f : X → [k], it sufﬁces to learn the (cid:100)log2(k + 1)(cid:101) binary functions fi : X → [k], where each fi is the ith bit of f .
Theorem 1.1 (Informal). Let H be a concept class consisting of [k]-valued functions. If all of the binary classes Hi = {fi : f ∈ H} are privately learnable, then H is privately learnable.
Beyond its obvious use for enabling the use of tools for binary private PAC learning on the classes
Hi, we show that Theorem 1.1 has strong implications for relating the private learnability of H to the combinatorial properties of H itself. Our main application of this reductive perspective is an improved sample complexity upper bound for private multiclass learning in terms of online learnability. 1.1 Online vs. Private Learnability
A recent line of work has revealed an intimate connection between differentially private learnability and learnability in Littlestone’s mistake-bound model of online learning [18]. For binary classes, the latter is tightly captured by a combinatorial parameter called the Littlestone dimension; a class H is online learnable with mistake bound at most d if and only if its Littlestone dimension is at most d. The Littlestone dimension also qualitatively characterizes private learnability. If a class H has
Littlestone dimension d, then every private PAC learner for H requires at least Ω(log∗ d) samples [1].
Meanwhile, Bun et al. [7] showed that H is privately learnable using 22O(d) samples, and Ghazi et al. [13] gave an improved algorithm using ˜O(d6) samples. (Moreover, while quantitatively far apart, both the upper and lower bound are tight up to polynomial factors as functions of the Littlestone dimension alone [15].)
Jung et al. [14] recently extended this connection from binary to multiclass learnability. They gave upper and lower bounds on the sample complexity of private multiclass learnability in terms of the multiclass Littlestone dimension [9]. Speciﬁcally, they showed that if a multi-valued class H has multiclass Littlestone dimension d, then it is privately learnable using 2kO(d) samples and that every private learner requires Ω(log∗ d) samples.
Jung et al.’s upper bound [14] directly extended the deﬁnitions and arguments from Bun et al.’s [7] earlier 22O(d)
-sample algorithm for the binary case. While plausible, it is currently unknown and far from obvious whether similar adaptations can be made to the improved binary algorithm of Ghazi et al. [13]. Instead of attacking this problem directly, we show that Theorem 1.1, together with additional insights relating multiclass and binary Littlestone dimensions, allows us to generically translate sample complexity upper bounds for private learning in terms of binary Littlestone dimension into upper bounds in terms of multiclass Littlestone dimension. Instantiating this general translation using the algorithm of Ghazi et al. gives the following improved sample complexity upper bound.
Theorem 1.2 (Informal). Let H be a concept class consisting of [k]-valued functions and let d be the multiclass Littlestone dimension of H. Then H is privately learnable using ˜O(d6 log8(k + 1)) samples.
In addition to being conceptually simple and modular, our reduction from multiclass to binary learning means that potential future improvements for binary learning will also automatically give improvements for multiclass learning. For example, if one were able to prove that all binary classes of Littlestone dimension d are privately learnable with O(d) samples, this would imply that every
[k]-valued class of multiclass Litttlestone dimension d is privately learnable with ˜O(d log3(k + 1)) samples.2 1.2 Techniques
Theorem 1.1 shows that a multi-valued class H is privately learnable if all of the binary classes
Hi are privately learnable, which in turn holds as long as we can control their (binary) Littlestone dimensions. So the last remaining step in order to conclude Theorem 1.2 is to show that if H has bounded multiclass Littlestone dimension, then all of the classes Hi have bounded binary Littlestone dimension. At ﬁrst glance, this may seem to follow immediately from the fact that (multiclass) 2The nearly cubic dependence on log(k + 1) follows from the fact that the accuracy of private learners can be boosted with a sample complexity blowup that is nearly inverse linear in the target accuracy [11, 6]. 2
Littlestone dimension characterizes (multiclass) online learnability – a mistake bounded learner for a multiclass problem is, in particular, able to learn each individual output bit of the function being learned. The problem with this intuition is that the multiclass learner is given more feedback from each example, namely the entire multi-valued class label, than a binary learner for each Hi that is only given a single bit. Nevertheless, we are still able to use combinatorial methods to show that multiclass online learnability of a class H implies online learnability of all of the binary classes Hi.
Theorem 1.3. Let H be a [k]-valued concept class with multiclass Littlestone dimension d. Then every binary class Hi has Littlestone dimension at most 6d ln(k + 1).
Moreover, this result is nearly tight. In the full version of the paper, we show that for every k, d ≥ 1 there is a [k]-valued class with multiclass Littlestone dimension d such that at least one of the classes
Hi has Littlestone dimension at least Ω(d log(k + 1)).
In addition, it turns out that online learnability of the binary classes Hi implies multiclass online learnability of H. In the full version of the paper, we show that if the multiclass Littlestone dimension d of H is d, then there is at least one binary class Hi with Littlestone dimension larger than log(k+1) .
This result is also tight.
Theorem 1.3 is the main technical contribution of this work. The proof adapts techniques introduced by Ben-David et al. [5] for characterizing the sample complexity of (non-private) multiclass PAC learnability. Speciﬁcally, Ben-David et al. introduced a family of combinatorial dimensions, param-eterized by collections of maps Ψ and called Ψ-dimensions, associated to classes of multi-valued functions. One choice of Ψ corresponds to the “Natarajan dimension” [19], which was previously known to give a lower bound on the sample complexity of multiclass learnability. Another choice corresponds to the “graph dimension” [19] which was known to give an upper bound. Ben-David et al. gave conditions under which Ψ-dimensions for different choices of Ψ could be related to each other, concluding that the Natarajan and graph dimensions are always within an O(log(k + 1)) factor, and thus characterizing the sample complexity of multiclass learnability up to such a factor.
Our proof of Theorem 1.3 proceeds by extending the deﬁnition of Ψ-dimension to online learning.
We show that one choice of Ψ corresponds to the multiclass Littlestone dimension, while a different choice corresponds to the maximum Littlestone dimension of any binary class Hi. We relate the two quantities up to a logarithmic factor using a new variant of the Sauer-Shelah-Perles Lemma for the “0-cover numbers” of a class of multi-valued functions. While we were originally motivated by privacy, we believe that Theorem 1.3 and the toolkit we develop for understanding online Ψ-dimensions may be of broader interest in the study of (multiclass) online learnability.
Finally, we remark that Theorem 1.3 implies a qualitative converse to Lemma 1.1. If a multi-valued class H is privately learnable, then the lower bound of [14] implies that H has ﬁnite multiclass
Littlestone dimension. Theorem 1.3 then shows that all of the classes Hi have ﬁnite binary Littlestone dimension, which implies via [7, 13] that they are also privately learnable.
Societal Impact Our work is motivated by privacy-respecting data analysis. Our focus is on theoretical questions aimed at uncovering general principles about when private learning is feasible.
As such, it does not negatively impact the way privacy-respecting techniques are used, but rather it clariﬁes their potential. 2