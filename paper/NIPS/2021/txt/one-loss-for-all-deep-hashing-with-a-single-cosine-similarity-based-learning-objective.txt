Abstract
A deep hashing model typically has two main learning objectives: to make the learned binary hash codes discriminative and to minimize a quantization error.
With further constraints such as bit balance and code orthogonality, it is not uncom-mon for existing models to employ a large number (>4) of losses. This leads to difﬁculties in model training and subsequently impedes their effectiveness. In this work, we propose a novel deep hashing model with only a single learning objective.
Speciﬁcally, we show that maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal codes can ensure both hash code discriminativeness and quantization error minimization. Further, with this learning objective, code balancing can be achieved by simply using a Batch Normalization (BN) layer and multi-label classiﬁcation is also straightforward with label smooth-ing. The result is an one-loss deep hashing model that removes all the hassles of tuning the weights of various losses. Importantly, extensive experiments show that our model is highly effective, outperforming the state-of-the-art multi-loss hashing models on three large-scale instance retrieval benchmarks, often by signiﬁcant margins. Code is available at https://github.com/kamwoh/orthohash 1

Introduction
A key building block of a real-world large-scale image retrieval system is hashing. The objective of image hashing is to represent the content of an image using a binary code for efﬁcient storage and accurate retrieval. Recently, deep hashing methods [48, 23] have shown great improvements over conventional hashing methods [46, 15, 16, 22, 36, 37, 19]. Furthermore, deep hashing methods can be grouped by how the similarity of the learned hashing codes are measured, namely pointwise
[49, 54, 40, 12, 50], pairwise [25, 23, 5, 4], triplet-wise [45, 32], or listwise [52]. Among them, pointwise methods have a O(N ) computational complexity, whilst the complexity of the others are of at least O(N 2) for N data points. This means that for large-scale problems, only the pointwise methods are tractable [49]. They are thus the focus of most recent studies.
A deep hashing neural network naturally has multiple learning objectives. Speciﬁcally, given an image input, the network outputs a continuous code (feature vector) which is then converted into a binary
∗equal contribution.
†corresponding author (cs.chan@um.edu.my). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) CE (b) CE+BN (c) Proposed
Figure 1: We train a simple CNN model on CIFAR10 with only ﬁrst 4 classes and 2-bits. The continuous codes v are visualized before sgn. (Left) The model is trained with cross entropy (CE) only. Although it can separate the 4 classes in Euclidean space, the output is not bounded and thus indicating high quantization error and sub-optimal in the Hamming space. (Middle) By appending a batch normalization (BN) layer after v, the hash codes are now balanced. (Right) Now the model (proposed) is trained to maximize the cosine similarity between v and its corresponding binary target o. The black arrows are the binary orthogonal target, denoted as o for each class. It can be seen that the continuous codes exhibit lower intra-class variance and quantization error as compared with the CE+BN models (middle). hash code using a quantization layer (usually a sign function). There are thus two main objectives.
First, the ﬁnal model output, i.e., the binary codes must be discriminative, meaning the intra-class hamming distances are small, while the inter-class ones are big. Second, a quantization error minimization objective is needed to regularize the continuous codes. But the learning is constrained by the vanishing gradient problem caused by the quantization layer. Although the problem can be avoided by deploying some relaxation schemes [5, 23, 25], these schemes often produce sub-optimal hash codes due to the introduction of quantization error (see Figure 1). Hence, most recently deep hashing methods [41, 25, 4, 53, 50] has an explicit quantization error minimization learning objective.
Having these two main objectives/losses are still not enough. In particular, to ensure the quality of hash codes, many other losses are employed by existing methods. These include bit balance loss
[53, 49, 40], weights constraints to maximize Hamming distance [54], code orthogonality [31, 32].
Further, losses are designed to address the vanishing gradient problem caused by the sign function used to obtain binary codes from the continuous ones [41, 40, 27]. As a result, the state-of-the-art hashing models typically have a large number (>4) losses. This means difﬁculties in optimization which in turn hamper their effectiveness.
In this work, for the ﬁrst time, a deep hashing model with a single loss is developed which removes any needs for loss weight tuning and is thus much easier to optimize. As mentioned earlier, a deep hashing model needs to be trained with at least two objectives, namely binary code discriminativenss and quantization error minimization. So how could one use one loss only? The answer lies in the fact that the two objectives are closely related and can be uniﬁed into one. More concretely, we show that both objectives can be satisﬁed by maximizing the cosine similarity between the continuous codes and their corresponding binary orthogonal target, which can be formulated as a cross-entropy (CE) loss.
Our model, dubbed OrthoHash has one loss only which maximizes the cosine similarity between the L2-normalized continuous codes and binary orthogonal target to maximize inter-class Hamming distance and minimize quantization error simultaneously. We show that this single unifying loss has a number of additional beneﬁts. First, we can leverage the beneﬁt of margin [42, 10] to further improve the intra-class variance. Second, since conventional CE loss only works for single-label classiﬁcation, we can easily leverage Label Smoothing [38] to modify the CE loss to tackle multi-labels classiﬁcation. Finally, we show that code balancing can now be enforced by introducing a batch normalization [17] (BN) layer rather than requiring a different loss. Extensive experiment results suggest that on conventional category-level retrieval tasks using ImageNet100, NUS-WIDE and
MS-COCO, our model is on par with the SOTA. More importantly, on the large-scale instance-level retrieval tasks, our method achieves the new SOTA, beating the best results obtained so far on GLDv2,
ROxf and RParis by 0.6%, 9.1% and 17.1% respectively. 2
2