Abstract
Modern convolutional neural networks (CNNs) have massive identical convolu-tion blocks, and, hence, recursive sharing of parameters across these blocks has been proposed to reduce the amount of parameters. However, naive sharing of parameters poses many challenges such as limited representational power and the vanishing/exploding gradients problem of recursively shared parameters. In this paper, we present a recursive convolution block design and training method, in which a recursively shareable part, or a ﬁlter basis, is separated and learned while effectively avoiding the vanishing/exploding gradients problem during training. We show that the unwieldy vanishing/exploding gradients problem can be controlled by enforcing the elements of the ﬁlter basis orthonormal, and empirically demonstrate that the proposed orthogonality regularization improves the ﬂow of gradients dur-ing training. Experimental results on image classiﬁcation and object detection show that our approach, unlike previous parameter-sharing approaches, does not trade performance to save parameters and consistently outperforms overparameterized counterpart networks. This superior performance demonstrates that the proposed recursive convolution block design and the orthogonality regularization not only prevent performance degradation, but also consistently improve the representation capability while a signiﬁcant amount of parameters are recursively shared. 1

Introduction
Modern convolutional neural networks (CNNs) such as ResNets have massive identical convolution blocks and recent analytic studies (Jastrzebski et al., 2018) show that these blocks perform mostly iterative reﬁnement of features rather than learning new features. Inspired by these results, recursive sharing of weights has been studied as a promising direction to parameter-efﬁcient CNNs (Jastrzebski et al., 2018; Guo et al., 2019; Savarese & Maire, 2019). However, naive sharing of parameters across many convolution layers incurs several problems. First of all, recursive sharing of parameters can result in the vanishing and the exploding gradients problem, which is one of the main reasons that recurrent neural networks (RNNs) are so hard to train properly (Pascanu et al., 2013; Jastrzebski et al., 2018). Another problem is that overall representation power can be limited by iterative sharing of parameters. Due to these challenges, most compression approaches based on parameter-sharing suffer from performance degradation.
In this work, we conjecture that convolution layers or blocks can be separated into inherently shareable parts and non-shareable parts, and can be trained effectively by avoiding the vanishing and the exploding gradients problem. To achieve this, for a full convolution operator, we ﬁrst replace it with a factorized version that splits the convolution operator into two separate operators; one
∗W. Kang and D. Kim are contributed equally. This work was conducted at Incheon National University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
operator with inherently shareable ﬁlters, called a ﬁlter basis, and the other operator with non-shared
ﬁlters, called coefﬁcients. When successive convolution blocks share a common ﬁlter basis, they are positioned in the same vector subspace. However, their representation capability is retained through non-shared coefﬁcients that learn diverse features by linearly combining the shared ﬁlter basis.
By separating shareable parts from non-shareable parts, we can impose desirable properties on the shared parameters. To avoid performance degradation from recursive sharing of parameters, we propose the orthogonality regularization, in which the vanishing/exploding gradients problem is controlled by enforcing the elements of a shared ﬁlter basis orthonormal during training. We both theoretically and empirically show that the proposed orthogonality regularization improves the ﬂow of the gradients during training and reduces the redundancy in parameters effectively.
For efﬁcient CNNs such as MobileNets (Howard et al., 2017), we do not need to factorize convolution operators to uncover a shared ﬁlter basis since these networks already have factorized convolution block structures for computational efﬁciency. For such networks, our approach can be applied simply by identifying one or two convolution operators of repeating convolution blocks as a ﬁlter basis that shares weights across the repeating blocks. Other convolution operators in each convolution block become block-speciﬁc non-shared coefﬁcients.
Since our focus is not on pushing the state-of-the-art performance, we demonstrate the effectiveness of our work using widely-used models as base models on image classiﬁcation and object detection.
Without bells and whistles, simply applying the proposed convolution block design and the orthog-onality regularization saves a signiﬁcant amount of parameters while consistently outperforming over-parameterized counterpart networks. For example, our method can save up to 46.0% of param-eters of ResNets while consistently achieving lower test errors. Even in compact models, such as
MobileNetV2, our approach can achieve further 8-21% parameter savings while outperforming the original models. This superior performance demonstrates that the proposed recursive convolution blocks and orthogonality regularization enables effective learning of better feature representations while a signiﬁcant amount of parameters are shared recursively. 2