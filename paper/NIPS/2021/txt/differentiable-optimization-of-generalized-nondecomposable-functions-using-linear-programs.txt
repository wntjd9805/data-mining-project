Abstract
We propose a framework which makes it feasible to directly train deep neural networks with respect to popular families of task-speciﬁc non-decomposable per-formance measures such as AUC, multi-class AUC, F -measure and others. A feature of the optimization model that emerges from these tasks is that it involves solving a Linear Programs (LP) during training where representations learned by upstream layers characterize the constraints or the feasible set. The constraint ma-trix is not only large but the constraints are also modiﬁed at each iteration. We show how adopting a set of ingenious ideas proposed by Mangasarian for 1-norm
SVMs – which advocates for solving LPs with a generalized Newton method – provides a simple and effective solution that can be run on the GPU. In particu-lar, this strategy needs little unrolling, which makes it more efﬁcient during the backward pass. Further, even when the constraint matrix is too large to ﬁt on the
GPU memory (say large minibatch settings), we show that running the Newton method in a lower dimensional space yields accurate gradients for training, by utilizing a statistical concept called sufﬁcient dimension reduction. While a num-ber of specialized algorithms have been proposed for the models that we describe here, our module turns out to be applicable without any speciﬁc adjustments or relaxations. We describe each use case, study its properties and demonstrate the efﬁcacy of the approach over alternatives which use surrogate lower bounds and often, specialized optimization schemes. Frequently, we achieve superior compu-tational behavior and performance improvements on common datasets used in the literature. 1

Introduction
Commonly used losses such as cross-entropy used in deep neural network (DNN) models can be expressed as a sum over the per-sample losses incurred by the current estimate of the model. This allows the direct use of mature optimization routines, and is sufﬁcient for many use cases. But in applications ranging from ranking/retrieval to class imbalanced learning, the most suitable losses for the task do not admit a “decompose over samples” form. Examples include Area under the ROC curve (AUC), multi-class variants of AUC, F -score, Precision at a ﬁxed recall (P@R) and others.
Optimizing such measures in a scalable manner can pose challenges even in the shallow setting.
For AUC maximization, we now know that convex surrogate losses can be used in a linear model
Liu et al. [2018], Natole et al. [2018] in the so-called ERM framework. These ideas have been incor-porated within deep neural network models and solved using SGD type schemes in Liu et al. [2019].
Such results on stochastic and online data models have also been explored in Ataman et al. [2006],
Cortes and Mohri [2004], Gao et al. [2013]. There are also available strategies for measures other 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
than the AUC: Nan et al. [2012], Dembczynski et al. [2011] give exact algorithms for optimizing
F -score and Eban et al. [2017], Ravi et al. [2020] proposes scalable methods for non-decomposable objectives which utilizes Lagrange multipliers to construct the proxy objectives. The authors in Mo-hapatra et al. [2018] discuss using a function that upper bounds (structured) hinge-loss to optimize average precision. Recently, Fathony and Kolter [2020] presented an adversarial prediction formula-tion for such nondecomposable measures, and showed that it is indeed possible to incorporate such measures within differentiable pipelines.
Our work utilizes the simple observation that a number of these non-decomposable objectives can be expressed in the form of an integer program that can be relaxed to a linear program (LP). Our approach is based on the premise that tackling the LP form of the non-decomposable objective as a module within the DNN, one which permits forward and reverse mode differentiation and can utilize in-built support for specialized GPU hardware in modern libraries such as PyTorch, is desirable. First, as long as a suitable LP formulation for an objective is available, the module may be directly used. Second, based on which scheme is used to solve the LP, one may be able to provide guarantees for the non-decomposable objective based on simple calculations (e.g., number of constraints, primal-dual gap). The current tools do not entirely address all these requirements.
A characteristic of the LPs that arise from the nondecomposable losses mentioned above is that the constraints (including the mini-batch of samples themselves) are modiﬁed at each iteration – as a function of the updates to the representations of the data in the upstream layers. In Section 3, we provide LP formulations of widely used nondecomposable terms, which fall squarely within the ca-pabilities of our solver. In Section 4, we show that the modiﬁed Newton’s algorithm in Mangasarian
[2004] can be used for deep neural network (DNN) training in an end-to-end manner without requir-ing external solvers, where support for GPUs currently remains limited. Speciﬁcally, by exploiting self-concordance of the objective, we show that the algorithm can converge globally without line search. We then analyze the gradient properties of our approach, and some modiﬁcations to improve stability during backpropagation. Our experiments in Section 5 show that this scheme based on
Mangasarian’s parametric exterior penalty formulation of the primal LP is a computationally effec-tive and scalable strategy to solve LPs with a large number of constraints. On the practical side, we provide two ways to deal with the scaling issue when the constraint matrix is large. On the one hand, we show that sufﬁcient dimension reduction can be used in our solver to solve the problem in a lower dimension space. On the other hand, when the matrix is sparse, with our new sparse implementa-tion, we show that we can train Resnet-18 with 10(cid:2) mini-batch size (memory savings) on a 2080TI
Nvidia GPU. Our code is available, see Fig 1 for an overview of our solver for differentiating many popular nondecomposable objectives. 2