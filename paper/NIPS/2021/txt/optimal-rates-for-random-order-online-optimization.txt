Abstract
We study online convex optimization in the random order model, recently proposed by Garber et al. [8], where the loss functions may be chosen by an adversary, but are then presented to the online algorithm in a uniformly random order. Focusing on the scenario where the cumulative loss function is (strongly) convex, yet individual loss functions are smooth but might be non-convex, we give algorithms that achieve the optimal bounds and signiï¬cantly outperform the results of Garber et al. [8], completely removing the dimension dependence and improving their scaling with respect to the strong convexity parameter. Our analysis relies on novel connections between algorithmic stability and generalization for sampling without-replacement analogous to those studied in the with-replacement i.i.d. setting, as well as on a reï¬ned average stability analysis of stochastic gradient descent. 1

Introduction
Online convex optimization [25, 12] studies the iterative process of decision making as data arrives in an online fashion. The model posits a game of ğ‘‡ rounds, where in each round the learner chooses a decision ğ‘¤ğ‘¡ from a convex set ğ‘Š âŠ† â„ğ‘‘, after which she observes a loss function ğ‘“ğ‘¡ : ğ‘Š â†’ â„, and incurs loss ğ‘“ğ‘¡ (ğ‘¤ğ‘¡ ). The learnerâ€™s objective is to minimize her regret, deï¬ned as her cumulative
ğ‘“ğ‘¡ (ğ‘¤). In the prototypical loss minus that of the best decision in hindsight ğ‘¤âˆ— = arg minğ‘¤ âˆˆğ‘Š setting, the individual loss functions are assumed to be convex and adversarially chosen by an opponentâ€”commonly known as nature or the adversaryâ€”who has knowledge of the learnerâ€™s algorithm. While this setup is fundamental enough to accommodate a diverse set of applications (see, e.g., [12]), studying variants of the basic model promotes modeling ï¬‚exibility, and further broadens the set of problems to which optimization techniques may be applied. (cid:80)ğ‘‡
ğ‘¡=1
Recently, Garber et al. [8] consider relaxing the convexity assumption by requiring that only on average the loss is (strongly) convexâ€”a property the authors refer to as cumulative (strong) convexityâ€”but do not require that the losses are convex individually. It is well known (e.g., [4]) that under these assumptions, if the losses are sampled i.i.d. from some distribution, stochastic gradient descent (SGD) obtains the optimal ğ‘‚ (log ğ‘‡) regret in expectation. However, as it turns out, in the fully adversarial model the cumulative strong convexity assumption is too weak: Garber et al. [8] show that in this case there is a linear regret lower bound. Consequently, they propose the random order model, where ğ‘‡ losses are chosen adversarially but then revealed to the learner in uniformly random order. Within this model, under the relaxed convexity assumption Garber et al. [8] obtain sub-linear regret for a number of specialized settings, diï¬€ering in their assumptions on the structure of the individual loss functions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In the most general case (the one we consider in this paper), they prove online gradient descent obtains regret ğ‘‚ ((ğ‘‘ğº2/Î»3) log ğ‘‡) w.h.p. for ğº-Lipschitz Î»-cumulative-strongly convex losses. (cid:80)
It is informative to compare the random order model with the i.i.d. stochastic case, where on every round a new loss is sampled uniformly and independently from the set of losses, that is, with-replacement. By contrast, the random order model speciï¬es that on every round a new loss is sampled uniformly without-replacement, an in particular not independently. Concretely, let L be an arbitrary set of ğ‘‡ smooth and Lipschitz continuous loss functions. Set ğ¹ (ğ‘¤) (cid:66) 1
ğ‘“ âˆˆL ğ‘“ (ğ‘¤),
ğ‘‡ and assume ğ¹ is Î»-strongly-convex over ğ‘Š. The learnerâ€™s goal is to minimize her regret on the loss sequence ğ‘“1, . . . , ğ‘“ğ‘‡ obtained from a uniformly random ordering of L. As noted previously, if the losses ğ‘“ğ‘¡ were drawn i.i.d., SGD obtains the optimal ğ‘‚ (log ğ‘‡) regret even though the losses are not individually convex. In a nutshell, when losses are i.i.d., the gradients used in the SGD update are conditionally unbiased estimates of the gradient of the average (strongly convex) loss, hence the optimal regret is achieved in expectation. The diï¬ƒculty in the random order model stems from the fact that random order gradients are not conditionally unbiased; given any set of past losses ğ‘“1, . . . , ğ‘“ğ‘¡âˆ’1, the next loss is uniform over the complement L \ { ğ‘“1, . . . , ğ‘“ğ‘¡âˆ’1}, and thus âˆ‡ ğ‘“ğ‘¡ (ğ‘¤ğ‘¡ ) is biased.
To overcome this complication, Garber et al. [8] work via the uniform convergence route, and build on concentration bounds applied to Hessians of the losses. As a result, they achieve suboptimal bounds, particularly in the general case where a dimension factor is introduced by a discretization argument necessary to ensure convergence over the entire domain. Here we choose a diï¬€erent strategy, and draw connections to notions of algorithmic stability and generalization studied in statistical learning theory. Our approach introduces signiï¬cant improvements compared to prior work, and achieves regret bounds optimal up to additive factors. 1.1 Our results
We present and analyze two algorithms for random order online optimization, the ï¬rst of which obtains the optimal regret up to additive factors. Let ğ‘“1, . . . , ğ‘“ğ‘‡ be a random order sequence of ğº-Lipschitz,
Î²-smooth losses, where ğ‘“ğ‘¡ : ğ‘Š â†’ â„ for all ğ‘¡ â‰¤ ğ‘‡. Assume the domain ğ‘Š âŠ‚ â„ğ‘‘ is convex, and has diameter bounded by ğ·. Further, assume the average loss 1
ğ‘“ğ‘¡ is Î»-strongly convex. We prove;
ğ‘‡ (cid:80)ğ‘‡
ğ‘¡=1
Theorem (informal). There exists an algorithm (Algorithm 1) for random order online optimization that obtains regret of ğ‘‚ (cid:0)(ğº2/Î») log ğ‘‡ (cid:1) in expectation.
Although the above result matches the optimal result for the individually strongly convex setting (up to additive factors), Algorithm 1 requires memory linear in ğ‘‡. This disadvantage motivates another algorithm, which trades oï¬€ an extra factor of Îº = Î²/Î» in the regret for lower memory requirements.
Theorem (informal). There exists an algorithm (Algorithm 2) for random order online optimization that requires memory linear in ğ‘‘, and obtains regret of ğ‘‚ (cid:0)(Î²ğº2/Î»2) log ğ‘‡ (cid:1) in expectation.
Big-ğ‘‚ notation in both theorems hides additive factors polynomial in problem parameters Î², ğº and
ğ· (but does not hide multiplicative factors in these parameters). By comparison, Garber et al. [8] obtain a regret bound of ğ‘‚ ((ğ‘‘ğº2/Î»3) log ğ‘‡) w.h.p. for this setting. Both of our algorithms completely remove the dimension factor ğ‘‘, and reduce scaling w.r.t. the strong convexity parameter by a factor of 1/Î»2 and 1/Î» respectively. Moreover, their results require the losses to have a Lipschitz Hessian (see 8, Theorem A.3), an assumption we do not make.
In addition, we consider the case that ğ¹ is convex (but not strongly convex), and apply our above results by means of regularization. As a corollary of the ï¬rst theorem, we obtain for this setting regret that scales as (cid:101)ğ‘‚ (
ğ‘‡), matching up to logarithmic and additive factors the optimal rate for the individually convex setting. Similarly, the second theorem implies a (cid:101)ğ‘‚ (ğ‘‡ 2/3) regret algorithm which is also memory eï¬ƒcient for the convex ğ¹ case. Notably, a similar reduction applied to the results of Garber et al. [8] would yield a regret bound of (cid:101)ğ‘‚ (ğ‘‘ğ‘‡ 3/4). This highlights the signiï¬cance of our improvement to the dependence on Î».
âˆš
Finally, we note it remains an open question to determine whether vanilla without-replacement SGD achieves optimal regret in the random-order model (our results leave a multiplicative gap of order 1/Î» between the upper and lower bounds). For the case of quadratic losses, however, using our techniques combined with a concentration argument it is possible to show that SGD indeed achieves optimal
ğ‘‚ (log ğ‘‡/Î») regret bound (up to logarithmic terms). 2
1.2 Overview of techniques
Our approach builds on the observation that regret on a random order loss sequence may be expressed as the average generalization error w.r.t a without-replacement training sample. In light of this, we relate random order regret to a suitable notion of algorithmic stability, mimicking in a sense a well known argument previously employed in the context of i.i.d. sampled training sets [3, 21]. At a high level, stability measures the sensitivity of an algorithm to small changes in its training set, and is a classical approach to proving generalization bounds [3, 21]. More often than not, the particular notion used in practice is uniform stability, where sensitivity is measured w.r.t. the worst case small change in the training dataset; maxğ‘† âŠ†Z(ğ‘š) (cid:107) ğ‘“ (A(ğ‘†) âˆ’ ğ‘“ (A(ğ‘†(cid:48))))(cid:107).
Our key insight is that while we cannot hope for uniform stability as losses are not assumed to be convex individually, we may exploit strong convexity of the population loss to show SGD admits average stability; ğ”¼ğ‘†âˆ¼Z(ğ‘š) (cid:107) ğ‘“ (A(ğ‘†)) âˆ’ ğ‘“ (A(ğ‘†(cid:48)))(cid:107) â‰¤ (cid:178)(ğ‘š). In particular, we prove the gradient update is contractive in expectation; ğ”¼ (cid:107)ğ‘¥ âˆ’ Î·âˆ‡ ğ‘“ (ğ‘¥) âˆ’ (ğ‘¦ âˆ’ Î·âˆ‡ ğ‘“ (ğ‘¦))(cid:107) (cid:12) ğ”¼ (cid:107)ğ‘¥ âˆ’ ğ‘¦(cid:107) under the cumulative strong convexity assumption. In turn, this yields a stability result which implies regret that scales with 1/ğ‘¡ for the early rounds up to ğ‘¡ â‰ˆ ğ‘‡/Îº, where Îº is the condition number of the problem. In short, as the game progresses the bias of the random order gradient estimates increases, and the gradient update becomes unstable.
To overcome the loss of stability in later rounds, we devise a simple online sampling mechanism that generates i.i.d. uniform samples from a random order distribution, eï¬€ectively ensuring unbiased gradient estimates throughout all ğ‘‡ rounds, and consequently optimal regret up to additive factors.
Finally, our approach allows us to develop an analysis framework that naturally accommodates SGD based algorithms in the random order model, and in a broader sense establish stability of SGD in a new, relatively general setting. 1.3