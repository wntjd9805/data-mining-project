Abstract
We study online convex optimization in the random order model, recently proposed by Garber et al. [8], where the loss functions may be chosen by an adversary, but are then presented to the online algorithm in a uniformly random order. Focusing on the scenario where the cumulative loss function is (strongly) convex, yet individual loss functions are smooth but might be non-convex, we give algorithms that achieve the optimal bounds and signiﬁcantly outperform the results of Garber et al. [8], completely removing the dimension dependence and improving their scaling with respect to the strong convexity parameter. Our analysis relies on novel connections between algorithmic stability and generalization for sampling without-replacement analogous to those studied in the with-replacement i.i.d. setting, as well as on a reﬁned average stability analysis of stochastic gradient descent. 1

Introduction
Online convex optimization [25, 12] studies the iterative process of decision making as data arrives in an online fashion. The model posits a game of 𝑇 rounds, where in each round the learner chooses a decision 𝑤𝑡 from a convex set 𝑊 ⊆ ℝ𝑑, after which she observes a loss function 𝑓𝑡 : 𝑊 → ℝ, and incurs loss 𝑓𝑡 (𝑤𝑡 ). The learner’s objective is to minimize her regret, deﬁned as her cumulative
𝑓𝑡 (𝑤). In the prototypical loss minus that of the best decision in hindsight 𝑤∗ = arg min𝑤 ∈𝑊 setting, the individual loss functions are assumed to be convex and adversarially chosen by an opponent—commonly known as nature or the adversary—who has knowledge of the learner’s algorithm. While this setup is fundamental enough to accommodate a diverse set of applications (see, e.g., [12]), studying variants of the basic model promotes modeling ﬂexibility, and further broadens the set of problems to which optimization techniques may be applied. (cid:80)𝑇
𝑡=1
Recently, Garber et al. [8] consider relaxing the convexity assumption by requiring that only on average the loss is (strongly) convex—a property the authors refer to as cumulative (strong) convexity—but do not require that the losses are convex individually. It is well known (e.g., [4]) that under these assumptions, if the losses are sampled i.i.d. from some distribution, stochastic gradient descent (SGD) obtains the optimal 𝑂 (log 𝑇) regret in expectation. However, as it turns out, in the fully adversarial model the cumulative strong convexity assumption is too weak: Garber et al. [8] show that in this case there is a linear regret lower bound. Consequently, they propose the random order model, where 𝑇 losses are chosen adversarially but then revealed to the learner in uniformly random order. Within this model, under the relaxed convexity assumption Garber et al. [8] obtain sub-linear regret for a number of specialized settings, diﬀering in their assumptions on the structure of the individual loss functions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In the most general case (the one we consider in this paper), they prove online gradient descent obtains regret 𝑂 ((𝑑𝐺2/λ3) log 𝑇) w.h.p. for 𝐺-Lipschitz λ-cumulative-strongly convex losses. (cid:80)
It is informative to compare the random order model with the i.i.d. stochastic case, where on every round a new loss is sampled uniformly and independently from the set of losses, that is, with-replacement. By contrast, the random order model speciﬁes that on every round a new loss is sampled uniformly without-replacement, an in particular not independently. Concretely, let L be an arbitrary set of 𝑇 smooth and Lipschitz continuous loss functions. Set 𝐹 (𝑤) (cid:66) 1
𝑓 ∈L 𝑓 (𝑤),
𝑇 and assume 𝐹 is λ-strongly-convex over 𝑊. The learner’s goal is to minimize her regret on the loss sequence 𝑓1, . . . , 𝑓𝑇 obtained from a uniformly random ordering of L. As noted previously, if the losses 𝑓𝑡 were drawn i.i.d., SGD obtains the optimal 𝑂 (log 𝑇) regret even though the losses are not individually convex. In a nutshell, when losses are i.i.d., the gradients used in the SGD update are conditionally unbiased estimates of the gradient of the average (strongly convex) loss, hence the optimal regret is achieved in expectation. The diﬃculty in the random order model stems from the fact that random order gradients are not conditionally unbiased; given any set of past losses 𝑓1, . . . , 𝑓𝑡−1, the next loss is uniform over the complement L \ { 𝑓1, . . . , 𝑓𝑡−1}, and thus ∇ 𝑓𝑡 (𝑤𝑡 ) is biased.
To overcome this complication, Garber et al. [8] work via the uniform convergence route, and build on concentration bounds applied to Hessians of the losses. As a result, they achieve suboptimal bounds, particularly in the general case where a dimension factor is introduced by a discretization argument necessary to ensure convergence over the entire domain. Here we choose a diﬀerent strategy, and draw connections to notions of algorithmic stability and generalization studied in statistical learning theory. Our approach introduces signiﬁcant improvements compared to prior work, and achieves regret bounds optimal up to additive factors. 1.1 Our results
We present and analyze two algorithms for random order online optimization, the ﬁrst of which obtains the optimal regret up to additive factors. Let 𝑓1, . . . , 𝑓𝑇 be a random order sequence of 𝐺-Lipschitz,
β-smooth losses, where 𝑓𝑡 : 𝑊 → ℝ for all 𝑡 ≤ 𝑇. Assume the domain 𝑊 ⊂ ℝ𝑑 is convex, and has diameter bounded by 𝐷. Further, assume the average loss 1
𝑓𝑡 is λ-strongly convex. We prove;
𝑇 (cid:80)𝑇
𝑡=1
Theorem (informal). There exists an algorithm (Algorithm 1) for random order online optimization that obtains regret of 𝑂 (cid:0)(𝐺2/λ) log 𝑇 (cid:1) in expectation.
Although the above result matches the optimal result for the individually strongly convex setting (up to additive factors), Algorithm 1 requires memory linear in 𝑇. This disadvantage motivates another algorithm, which trades oﬀ an extra factor of κ = β/λ in the regret for lower memory requirements.
Theorem (informal). There exists an algorithm (Algorithm 2) for random order online optimization that requires memory linear in 𝑑, and obtains regret of 𝑂 (cid:0)(β𝐺2/λ2) log 𝑇 (cid:1) in expectation.
Big-𝑂 notation in both theorems hides additive factors polynomial in problem parameters β, 𝐺 and
𝐷 (but does not hide multiplicative factors in these parameters). By comparison, Garber et al. [8] obtain a regret bound of 𝑂 ((𝑑𝐺2/λ3) log 𝑇) w.h.p. for this setting. Both of our algorithms completely remove the dimension factor 𝑑, and reduce scaling w.r.t. the strong convexity parameter by a factor of 1/λ2 and 1/λ respectively. Moreover, their results require the losses to have a Lipschitz Hessian (see 8, Theorem A.3), an assumption we do not make.
In addition, we consider the case that 𝐹 is convex (but not strongly convex), and apply our above results by means of regularization. As a corollary of the ﬁrst theorem, we obtain for this setting regret that scales as (cid:101)𝑂 (
𝑇), matching up to logarithmic and additive factors the optimal rate for the individually convex setting. Similarly, the second theorem implies a (cid:101)𝑂 (𝑇 2/3) regret algorithm which is also memory eﬃcient for the convex 𝐹 case. Notably, a similar reduction applied to the results of Garber et al. [8] would yield a regret bound of (cid:101)𝑂 (𝑑𝑇 3/4). This highlights the signiﬁcance of our improvement to the dependence on λ.
√
Finally, we note it remains an open question to determine whether vanilla without-replacement SGD achieves optimal regret in the random-order model (our results leave a multiplicative gap of order 1/λ between the upper and lower bounds). For the case of quadratic losses, however, using our techniques combined with a concentration argument it is possible to show that SGD indeed achieves optimal
𝑂 (log 𝑇/λ) regret bound (up to logarithmic terms). 2
1.2 Overview of techniques
Our approach builds on the observation that regret on a random order loss sequence may be expressed as the average generalization error w.r.t a without-replacement training sample. In light of this, we relate random order regret to a suitable notion of algorithmic stability, mimicking in a sense a well known argument previously employed in the context of i.i.d. sampled training sets [3, 21]. At a high level, stability measures the sensitivity of an algorithm to small changes in its training set, and is a classical approach to proving generalization bounds [3, 21]. More often than not, the particular notion used in practice is uniform stability, where sensitivity is measured w.r.t. the worst case small change in the training dataset; max𝑆 ⊆Z(𝑚) (cid:107) 𝑓 (A(𝑆) − 𝑓 (A(𝑆(cid:48))))(cid:107).
Our key insight is that while we cannot hope for uniform stability as losses are not assumed to be convex individually, we may exploit strong convexity of the population loss to show SGD admits average stability; 𝔼𝑆∼Z(𝑚) (cid:107) 𝑓 (A(𝑆)) − 𝑓 (A(𝑆(cid:48)))(cid:107) ≤ (cid:178)(𝑚). In particular, we prove the gradient update is contractive in expectation; 𝔼 (cid:107)𝑥 − η∇ 𝑓 (𝑥) − (𝑦 − η∇ 𝑓 (𝑦))(cid:107) (cid:12) 𝔼 (cid:107)𝑥 − 𝑦(cid:107) under the cumulative strong convexity assumption. In turn, this yields a stability result which implies regret that scales with 1/𝑡 for the early rounds up to 𝑡 ≈ 𝑇/κ, where κ is the condition number of the problem. In short, as the game progresses the bias of the random order gradient estimates increases, and the gradient update becomes unstable.
To overcome the loss of stability in later rounds, we devise a simple online sampling mechanism that generates i.i.d. uniform samples from a random order distribution, eﬀectively ensuring unbiased gradient estimates throughout all 𝑇 rounds, and consequently optimal regret up to additive factors.
Finally, our approach allows us to develop an analysis framework that naturally accommodates SGD based algorithms in the random order model, and in a broader sense establish stability of SGD in a new, relatively general setting. 1.3