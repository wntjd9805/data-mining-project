Abstract
Optimal decision making requires that classiﬁers produce uncertainty estimates consistent with their empirical accuracy. However, deep neural networks are often under- or over-conﬁdent in their predictions. Consequently, methods have been developed to improve the calibration of their predictive uncertainty, both during training and post-hoc. In this work, we propose differentiable losses to improve calibration based on a soft (continuous) version of the binning operation underlying popular calibration-error estimators. When incorporated into training, these soft calibration losses achieve state-of-the-art single-model ECE across multiple datasets with less than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE (70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative decrease in accuracy relative to the cross-entropy baseline on
CIFAR-100. When incorporated post-training, the soft-binning-based calibration error objective improves upon temperature scaling, a popular recalibration method.
Overall, experiments across losses and datasets demonstrate that using calibration-sensitive procedures yield better uncertainty estimates under dataset shift than the standard practice of using a cross-entropy loss and post-hoc recalibration methods.2 1

Introduction
Despite the success of deep neural networks across a variety of domains, they are still susceptible to miscalibrated predictions. Both over- and under-conﬁdence contribute to miscalibration, and empirically, deep neural networks empirically exhibit signiﬁcant miscalibration [Guo et al., 2017].
Calibration error (CE) quantiﬁes a model’s miscalibration by measuring how much its conﬁdence, i.e. the predicted probability of correctness, diverges from its accuracy, i.e. the empirical probability of correctness. Models with low CE are critical in domains where satisfactory outcomes depend on well-modeled uncertainty, such as autonomous vehicle navigation [Bojarski et al., 2016] and medical diagnostics [Jiang et al., 2012, Caruana et al., 2015, Kocbek et al., 2020]. Calibration has also been shown to be useful for improving model fairness [Pleiss et al., 2017] and detecting out-of-distribution data [Kuleshov and Ermon, 2017, Devries and Taylor, 2018, Shao et al., 2020]. More generally, low
∗co-ﬁrst author 2Code available on GitHub: https://github.com/google/uncertainty-baselines/tree/main/experimental/caltrain 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
CE is desirable in any setting in which thresholds are applied to the predicted conﬁdence of a neural network in order to make a decision.
Methods for quantifying CE typically involve binning model predictions based on their conﬁdence.
CE is then computed empirically as a weighted average of the absolute difference in average prediction conﬁdence and average accuracy across different bins [Naeini et al., 2015]. Oftentimes these bins are selected heuristically such as equal-width (uniformly binning the score interval) and equal-mass (with equal numbers of samples per-bin) [Nixon et al., 2019].
However, these commonly used measures of CE are not trainable with gradient-based methods because the binning operation is discrete and has zero derivatives. As a result, neural network parameters are not directly trained to minimize CE, either during training or during post-hoc recalibration. In this paper, we introduce new objectives based on a differentiable binning scheme that can be used to efﬁciently and directly optimize for calibration.
Contributions. We propose estimating CE with soft (i.e., overlapping, continuous) bins rather than the conventional hard (i.e., nonoverlapping, all-or-none) bins. With this formulation, the CE estimate is differentiable, allowing us to use it as (1) a secondary (i.e., auxiliary) loss to incentivize model calibration during training, and (2) a primary loss for optimizing post-hoc recalibration methods such as temperature scaling. In the same spirit, we soften the AvUC loss [Krishnan and Tickoo, 2020], allowing us to use it as an effective secondary loss during training for non-Bayesian neural networks where the AvUC loss originally proposed for Stochastic Variational Inference (SVI) typically does not work. Even when training with the cross-entropy loss results in training set memorization (perfect train accuracy and calibration), Soft Calibration Objectives are still useful as secondary training losses for reducing test ECE using a procedure we call interleaved training.
In an extensive empirical evaluation, we compare Soft Calibration Objectives as secondary losses to existing calibration-incentivizing losses. In the process, we ﬁnd that soft-calibration losses outperform prior work on in-distribution test sets. Under distribution shift, we ﬁnd that calibration-sensitive training objectives as a whole (not always the ones we propose) result in better uncertainty estimates compared to the standard cross-entropy loss coupled with temperature scaling.
Our contributions can be summarized as follows:
• We propose simple Soft Calibration Objectives S-AvUC, SB-ECE as secondary losses which optimize for CE throughout training. We show that across datasets and choice of primary losses, the S-AvUC secondary loss results in the largest improvement in ECE as per the Cohen’s d effect-size metric (Figure 1). We also show that such composite losses obtain state-of-the-art single-model ECE in exchange for less than 1% reduction in accuracy (Figure 5) for CIFAR-10, CIFAR-100, and Imagenet.
• We improve upon temperature scaling - a popular post-hoc recalibration method - by directly optimizing the temperature parameter for soft calibration error instead of the typical log-likelihood. Our extension (TS-SB-ECE) consistently beats original temperature scaling (TS) across different datasets, loss functions and calibration error measures, and we ﬁnd that the performance is better under dataset shift (Figure 2).
• Overall, our work demonstrates a fundamental advantage of objectives which better incen-tivize calibration over the standard practice of training with cross-entropy loss and then applying post-hoc methods such as temperature scaling. Uncertainty estimates of neural networks trained using these methods generalize better in and out-of-distribution. 2