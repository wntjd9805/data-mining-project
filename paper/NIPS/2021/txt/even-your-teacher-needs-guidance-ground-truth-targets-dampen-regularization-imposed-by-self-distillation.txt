Abstract
Knowledge distillation is classically a procedure where a neural network is trained on the output of another network along with the original targets in order to transfer knowledge between the architectures. The special case of self-distillation, where the network architectures are identical, has been observed to improve generalization accuracy. In this paper, we consider an iterative variant of self-distillation in a kernel regression setting, in which successive steps incorporate both model outputs and the ground-truth targets. This allows us to provide the ﬁrst theoretical results on the importance of using the weighted ground-truth targets in self-distillation.
Our focus is on ﬁtting nonlinear functions to training data with a weighted mean square error objective function suitable for distillation, subject to (cid:96)2 regularization of the model parameters. We show that any such function obtained with self-distillation can be calculated directly as a function of the initial ﬁt, and that inﬁnite distillation steps yields the same optimization problem as the original with ampliﬁed regularization. Furthermore, we provide a closed form solution for the optimal choice of weighting parameter at each step, and show how to efﬁciently estimate this weighting parameter for deep learning and signiﬁcantly reduce the computational requirements compared to a grid search. 1

Introduction
Knowledge distillation, most commonly known from Hinton et al. (2015), is a procedure to transfer knowledge from one neural network (teacher) to another neural network (student).1 Often the student has fewer parameters than the teacher, and the procedure can be seen as a model compression technique. Originally, the distillation procedure achieves the knowledge transfer by training the student network using the original training targets, denoted as ground-truth targets, as well as a softened distribution of logits from the (already trained and ﬁxed) teacher network.2 Since the popularization of knowledge distillation by Hinton et al. (2015), the idea of knowledge distillation has been extended to a variety of settings.3 This paper will focus on the special case where the teacher and student are of identical architecture, called self-distillation, and where the aim is to improve predictive performance, rather than compressing the model.
The idea of self-distillation is to use outputs from a trained model together with the original targets as new targets for retraining the same model from scratch. We refer to this as one step of self-distillation, and one can iterate this procedure for multiple distillation steps (see Figure 1). Empirically, it has been shown that this procedure often generalizes better than the model trained merely on the 1Often knowledge distillation is also referred to under the name Teacher-Student learning. 2We will refer to the weighted outputs of the penultimate layer, i.e. pre-activation of the last layer, as logits. 3See Section 2 for a brief overview, or see Wang and Yoon (2020) for a more exhaustive survey 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
original targets, and achieves higher predictive performance on validation data, despite no additional information being provided during training (Furlanello et al., 2018; Ahn et al., 2019; Yang et al., 2018).
Figure 1: Illustration of self-distillation for two steps after the initial training, where we use the notation f (τ ) = f (
·
, ˆβ(τ )). See Section 3 for details.
Modern deep neural networks are often trained in the over-parameterized regime, where the amount of trainable parameters highly exceed the amount of training samples. Under simple ﬁrst-order methods such as gradient descent, such large networks can ﬁt any target, but in order to generalize well, such overﬁtting is usually undesirable (Zhang et al., 2017; Nakkiran et al., 2020). Thus, some type of regularization is typically imposed during training, in order to avoid overﬁtting. A common choice is to add an (cid:96)2-regularization4 term to our objective function, which has been shown to perform comparably to early-stopping gradient descent training (Yao et al., 2007). However, in the theoretical study of the over-parameterized regime, regularization is often overlooked, but recent results have shown a connection between wide neural networks and kernel ridge regression through the Neural Tangent Kernel (NTK) (Lee et al., 2019, 2020; Hu et al., 2019). We brieﬂy elaborate on this connection in Section D, which motivates our problem setup and connection to deep learning in
Section 5.
Our Contributions Through a theoretical analysis we show that
• the solution at any distillation step can easily be calculated as a function of the initial ﬁt, and inﬁnitely many steps of self-distillation (with ﬁxed distillation weight) correspond to solving the usual kernel ridge regression problem with a speciﬁc ampliﬁed regularization parameter when the distillation weight is non-zero,
• for ﬁxed distillation weights, self-distillation ampliﬁes the regularization at each distillation step, and the ground-truth targets dampen the sparsiﬁcation and regularization of the self-distilled solutions, ensuring non-zero solutions for any number of distillation steps,
• the optimal distillation weight has a closed form solution for kernel ridge regression, and can be estimated efﬁciently for neural networks compared to a grid search.
Proofs of all our results can be found in Supplementary Material A, and code to reproduce our illustrative example in Section 4.5. Experimental results in Section B can be found at github.com/Kennethborup/self_distillation. 2