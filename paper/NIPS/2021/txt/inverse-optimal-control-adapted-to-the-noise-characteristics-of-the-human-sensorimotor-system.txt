Abstract
Computational level explanations based on optimal feedback control with signal-dependent noise have been able to account for a vast array of phenomena in human sensorimotor behavior. However, commonly a cost function needs to be assumed for a task and the optimality of human behavior is evaluated by comparing observed and predicted trajectories. Here, we introduce inverse optimal control with signal-dependent noise, which allows inferring the cost function from observed behavior.
To do so, we formalize the problem as a partially observable Markov decision pro-cess and distinguish between the agent’s and the experimenter’s inference problems.
Speciﬁcally, we derive a probabilistic formulation of the evolution of states and be-lief states and an approximation to the propagation equation in the linear-quadratic
Gaussian problem with signal-dependent noise. We extend the model to the case of partial observability of state variables from the point of view of the experimenter.
We show the feasibility of the approach through validation on synthetic data and application to experimental data. Our approach enables recovering the costs and beneﬁts implicit in human sequential sensorimotor behavior, thereby reconciling normative and descriptive approaches in a computational framework. 1

Introduction
Computational level theories of behavior strive to answer the questions, why a system behaves the way it does and what the goal of the system’s computations is. Such goals can be formalized based on the reward hypothesis. In the words of Richard Sutton, the reward hypothesis assumes, that “goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal” [1]. Thus, to understand human sensorimotor behavior, it is essential to characterize its goals and purposes quantitatively in terms of costs and beneﬁts. But particularly in every-day tasks, the cost and beneﬁts underlying behavior are unknown.
Stochastic optimal control allows formulating behavioral goals in terms of a cost function for tasks involving sequential actions under action variability, uncertainty in the internal model, and delayed rewards. The solution to the optimization problem entailed in the cost function is a sequence of
∗Both authors contributed equally to this paper. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
actions, which is then compared to human movements. While early models optimized costs related to deterministic kinematics of a movement to a target [2], task goals were subsequently formalized as costs on the variance of stochastic movements’ endpoint distances to a goal target [3]. Importantly, although [3] considered open-loop control, it revealed the importance of modeling the speciﬁc variability of human movements, which increases linearly with the magnitude of the control signal
[4]. As the neuronal control signal increases, so does its variability, leading to optimal movements trading off between achieving task goals and reducing the expected impact of movement variability.
Including sensory feedback in stochastic optimal control leads to a computationally much more intricate problem, which can be formulated as a partially observable Markov decision process (POMDP) and is intractable in general. One of the few tractable cases is the linear-quadratic Gaussian (LQG) setting, where dynamics are linear, costs are quadratic, and variability is additive and Gaussian, leading to sensory inference of the state and control to be decoupled [5]. However, not only is human movement variability signal dependent, but additionally the uncertainty of sensory signals increases linearly with the magnitude of the stimulus, a phenomenon known as Weber’s law [6].
Todorov [7] extended the LQG case by introducing stochastic optimal feedback control with signal-dependent noise, which allows the speciﬁcation of noise models in line with what is known about the human sensorimotor system. This model [8] has been able to explain a broad range of phenomena in sensorimotor control [9, 10], including linear movement trajectories, smooth velocity proﬁles, speed-accuracy tradeoffs, and corrections of errors only if they inﬂuence attaining the behavioral goal. Particularly incorporating signal-dependent noise has been crucial in explaining experimental data, ranging from how corrections of movements during action execution depend on feedback and task goals [11], that movements consider sensory uncertainty and temporal delays in real-time [12], that movement plans in novel environments are reoptimized based on the learning of internal models to minimize implicit motor costs and maximize rewards [13], and many others [14, 15, 16, 17].
Explaining human behavior in these studies usually starts by hypothesizing the cost function describ-ing a task, obtaining the optimal feedback controller, and comparing simulated trajectories to those observed experimentally. This line of inquiry, therefore, utilizes similarity of trajectories to quantify the degree of optimality in human behaviors. In some cases [14], trajectories are simulated from the model to check for robustness with respect to changes in the model parameters. If our goal is to use optimal control models to infer such quantities, which often cannot be measured independently, from behavior, it would instead be desirable to invert the problem and ﬁnd those parameter settings which are consistent with the observed trajectories. While such inverse methods have been developed both in the ﬁeld of reinforcement learning to infer the rewards being optimized by an agent [18, 19, 20] as well as in optimal control for the LQG case [21, 22, 23], this is currently not possible for the noise characteristics of the human sensorimotor system.
Here, we introduce a probabilistic formulation of inverse optimal feedback control under signal-dependent noise in the tradition of rational analysis [24, 25]. Our starting point is the forward problem introduced in [8]. We formulate the inference problem faced by an agent as a POMDP and distinguish it from the inference problem of an experimenter observing the agent. We proceed by deriving the likelihood of a sequence of observed states and provide an approximation to the non-Gaussian uncertainty due to the signal-dependent noise. First, this allows recovering the cost function underlying the agent’s behavior from observed behavioral data. Second, we extend the inference of the cost function to the case in which the state variables are only partially observable to the experimenter, e.g., when only measuring the position of the agent’s movement. Third, we show through simulated and experimental data that the cost functions can indeed be recovered. Fourth, the probabilistic formulation allows recovering the agent’s belief during the experiment as well as the experimenter’s uncertainty about the inferred belief.