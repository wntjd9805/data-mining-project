Abstract
The performance of ML models degrades when the training population is different from that seen under operation. Towards assessing distributional robustness, we study the worst-case performance of a model over all subpopulations of a given size, deﬁned with respect to core attributes Z. This notion of robustness can con-sider arbitrary (continuous) attributes Z, and automatically accounts for complex intersectionality in disadvantaged groups. We develop a scalable yet principled two-stage estimation procedure that can evaluate the robustness of state-of-the-art models. We prove that our procedure enjoys several ﬁnite-sample convergence guarantees, including dimension-free convergence. Instead of overly conservative notions based on Rademacher complexities, our evaluation error depends on the dimension of Z only through the out-of-sample error in estimating the performance conditional on Z. On real datasets, we demonstrate that our method certiﬁes the robustness of a model and prevents deployment of unreliable models. 1

Introduction
The training population typically does not accurately represent what the model will encounter under operation. Model performance has been observed to substantially degrade under distribution shift [16, 28, 69, 80, 53] in speech recognition [52], automated essay scoring [4], and wildlife conservation [11]. Similar trends persist for state-of-the-art NLP and computer vision models [78, 74], even on new data constructed under a near-identical process [57, 66]. Heavily engineered commercial models are no exception [19], performing poorly on rare entities in named entity linking and examples that require abstraction and distillation in summarization tasks [38].
A particularly problematic form of distribution shift comes from embedded power structures in data collection. Data forms the infrastructure on which we build prediction models [30], and they inherit socioeconomic and political inequities against marginalized communities. For example, out of 10,000+ cancer clinical trials the National Cancer Institute funds, less than 5% of participants were non-white [21]. Typical models replicate and perpetuate such bias, and their performance drops signiﬁcantly on underrepresented groups. Speech recognition systems work poorly for Blacks [52]
∗Authors ordered alphabetically. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and those with minority accents [3]. More generally, model performance degrades across demographic attributes such as race, gender, or age, in facial recognition, video captioning, language identiﬁcation, and academic recommender systems [41, 46, 17, 72, 79, 19].
Model training typically relies on varied engineering practices. It is crucial to rigorously certify model robustness prior to deployment for these heuristic approaches to bear fruit and transform consequential applications. Ensuring that models perform uniformly well across subpopulations is simultaneously critical for reliability, fairness, satisfactory user experience, and long-term business goals. While a natural approach is to evaluate performance across a small set of groups, disadvan-taged subpopulations are hard to deﬁne a priori because of intersectionality. The most adversely affected are often determined by a complex combination of variables such as race, income, and gender [19]. For example, performance on summarization tasks varies across demographic character-istics and document speciﬁc traits such as abstractiveness, distillation, and location and dispersion of information [38].
Motivated by these challenges, we study the worst-case subpopulation performance across all subpopulations of a given size. This conservative notion of performance evaluates robustness to unanticipated distribution shifts in Z, and automatically accounts for complex intersectionality by virtue of being agnostic to demographic groupings. Formally, let Z be a set of core attributes that we wish to guarantee uniform performance over. It may include protected demographic variables such as race, gender, income, age, or task-speciﬁc information such as length of the prompt or metadata on the input; notably, it can contain any continuous or discrete variables. We let X ∈ X be the input
/ covariate, and Y ∈ Y be the label. In NLP and vision applications, X is high-dimensional and typically dim(Z) (cid:28) dim(X).
We use θ(X) to denote a ﬁxed prediction model and consider ﬂexible and abstract losses (cid:96)(θ(x); y).
Our goal is to ensure that the model θ performs well over all subpopulations deﬁned over Z. We evaluate model losses on a mixture component, which we call a subpopulation. Postulating a lower bound α ∈ (0, 1] on the demographic proportion (mixture weight), we consider the set of subpopulations of the data-generating distribution PZ
Qα := {QZ | PZ = aQZ + (1 − a)Q(cid:48)
Z for some a ≥ α, and subpopulation Q(cid:48)
Z} . (1)
The demographic proportion (mixture weight) a represents how underrepresented the subpopulation is under the data-generating distribution PZ. Before deploying the model θ, we wish to evaluate the worst-case subpopulation performance
Wα(θ) := sup
QZ ∈Qα
EZ∼QZ [E[(cid:96)(θ(X), Y ) | Z]] . (2)
The worst-case subpopulation performance (2) guarantees uniform performance over subpopula-tions (1) and has a clear interpretation that can be communicated to diverse stakeholders. The minority proportion α can often be chosen from ﬁrst principles, e.g., we wish to guarantee uniformly good performance over subpopulations comprising at least α = 20% of the collected data. Alternatively, it is often informative to study the threshold level of α(cid:63) when α (cid:55)→ Wα(θ) crosses the maximum level of acceptable loss. The threshold α(cid:63) provides a certiﬁcate of robustness on the model θ(·), guaranteeing that all subpopulations large than α(cid:63) enjoy good performance.
We develop a principled and scalable procedure for estimating the worst-case subpopulation perfor-mance (2) and the certiﬁcate of robustness α(cid:63). A key technical challenge is that for each data point, we observe the loss (cid:96)(θ(X); Y ) but never observe the conditional risk evaluated at the attribute Z
µ(Z) := E[(cid:96)(θ(X); Y ) | Z]. (3)
In Section 2, we propose a two-stage estimation approach where we compute an estimate (cid:98)h1(·) of the conditional risk µ(·). Then, we compute a plug-in estimate of the worst-case subpopulation performance under (cid:98)h1(·) using a dual reformulation of the worst-case problem (2). We show several theoretical guarantees for our estimator of the worst-case subpopulation performance (2). Our ﬁrst (cid:17), where Compn
ﬁnite-sample result (Section 3.1) shows convergence at the rate Op denotes a notion of complexity for the model class estimating the conditional risk (3). (cid:16)(cid:112)Compn(H)/n
In some applications, it may be natural to deﬁne Z using images or natural languages describing the input and use deep networks to predict the conditional risk (3). As the complexity term Compn(H) becomes prohibitively large in this case [10, 86], our second result (Section 3.2) shows data-dependent 2
dimension-free concentration of our two-stage estimator: our bound only depends on the complexity of the model class H through the out-of-sample error for estimating the conditional risk (3). This error can be made small using overparameterized deep networks, allowing us to estimate the condi-tional risk (3) using even the largest deep networks and still obtain a theoretically principled upper conﬁdence bound on the worst-case subpopulation performance. Leveraging these guarantees, we develop principled procedures for estimating the certiﬁcates of robustness α(cid:63) in Section 3.3.
In Section 4, we demonstrate the effectiveness of our procedure on real data. By evaluating model robustness under subpopulation shifts, our methods allow the selection of robust models before deployment as we illustrate using the recently proposed CLIP model [62].