Abstract
We consider Bayesian optimization of the output of a network of functions, where each function takes as input the output of its parent nodes, and where the net-work takes signiﬁcant time to evaluate. Such problems arise, for example, in reinforcement learning, engineering design, and manufacturing. While the stan-dard Bayesian optimization approach observes only the ﬁnal output, our approach delivers greater query efﬁciency by leveraging information that the former ignores: intermediate output within the network. This is achieved by modeling the nodes of the network using Gaussian processes and choosing the points to evaluate using, as our acquisition function, the expected improvement computed with respect to the implied posterior on the objective. Although the non-Gaussian nature of this posterior prevents computing our acquisition function in closed form, we show that it can be efﬁciently maximized via sample average approximation. In addition, we prove that our method is asymptotically consistent, meaning that it ﬁnds a globally optimal solution as the number of evaluations grows to inﬁnity, thus generalizing previously known convergence results for the expected improvement. Notably, this holds even though our method might not evaluate the domain densely, instead leveraging problem structure to leave regions unexplored. Finally, we show that our approach dramatically outperforms standard Bayesian optimization methods in several synthetic and real-world problems. 1

Introduction
We consider Bayesian optimization (BO) of objective functions deﬁned by a series of time-consuming-to-evaluate functions, f1, . . . , fK, arranged in a directed acyclic network, so that each function takes as input the output of its parent nodes. As we detail below, these problems arise in BO-based policy search in reinforcement learning (Lizotte et al., 2007), optimization of complex systems modeled via simulation, and calibration of time-consuming physics-based models.
To illustrate, we introduce a running example of vaccine manufacturing (Sekhon and Saluja, 2011), focusing on the portion of the manufacturing process that uses live cells to produce proteins needed in a vaccine. It begins with a cell culture, in which living cells are grown and used as “factories” to produce proteins. This process is controlled by a vector, x1, containing the temperature, pH, and CO2 content used when growing these cells. The output of this process is the quantity of the desired protein y1 = f1(x1), i.e., the yield of this step, along with other byproducts. This output is passed into a second process, puriﬁcation, which removes byproducts and is controlled by a vector x2 comprising temperature, pressure, and ﬂow rate. The yield of the desired protein from this second step is y2 = f2(x2, y1). This output enters a third step, formulation, in which we formulate the raw protein into a form that can be distributed as controlled by a third set of parameters. This determines the yield of the overall process y3 = f3(x3, y2). We wish to choose (x1, x2, x3) to maximize overall protein yield. This problem is summarized as a function network in Figure 1.
The problem described above and other similar problems can be tackled with Bayesian optimization (BO), which has been shown to perform well compared to other derivative-free global optimization 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
methods for time-consuming-to-evaluate objective functions (Snoek et al., 2012; Frazier, 2018). A standard BO algorithm would ﬁt a Gaussian process (GP) (Rasmussen and Williams, 2006) model on the objective function (y3, which depends on (x1, x2, x3)) and use it, along with an acquisition function, to sequentially choose the points to evaluate. Under this standard approach, however, evaluations of the intermediate nodes, f1, . . . , fK−1, would be ignored despite being available when computing the objective function. In the example above, this corresponds to looking only at the yield of the overall process, and not of each individual step. y3 y1 y2 f3 x3 f2 x2 f1 x1
Figure 1: Vaccine manufacturing as a function network. Protein y1 = f1(x1) is created, then puriﬁed with yield y2 = f2(x2, y1), and formulated with yield y3 = f3(x3, y2). The goal is to ﬁnd (x1, x2, x3) that maximizes y3.
In this paper, we introduce a novel BO approach that leverages function network structure for substantially more efﬁcient optimization. This approach models the individual nodes of the network using distinct GPs. This allows incorporating observations of each node’s output recursively into a non-Gaussian posterior on the network’s overall output. Our approach then chooses the points to evaluate using the expected improvement (Jones et al., 1998) computed with respect to this implied posterior on the objective function. The non-Gaussian nature of this posterior prevents the expected improvement from having a closed form. However, we show that it can still be efﬁciently maximized via sample average approximation (Kleywegt et al., 2002).
Our approach can outperform standard BO by leveraging information from internal nodes unavailable to standard methods. We brieﬂy explain one way this can happen, in the context of the example above. In vaccine manufacturing, each function fk(xk, yk−1), k = 2, 3, is bounded between 0 and yk−1 because new protein cannot be created in the puriﬁcation and formulation stages. Moreover, application experts have a prior on what values for fk(xk, yk−1)/yk−1 should be achievable if xk is set well. Thus, if we see that y3 is unexpectedly poor, information from intermediate nodes can be extremely valuable: if y1 and y2 are as expected, then this suggests the problem is with x3; if y1 is as expected but y2 is poor, then the problem is likely with x2; and if y1 is poor then the problem is likely with x1. (If there is a problem with xk, there may also be a problem with xk(cid:48), k(cid:48) > k, but we can focus on ﬁxing xk ﬁrst.) Thus, by observing intermediate nodes, we can instantly reduce the effective dimensionality of the input space by a factor of K = 3.
We show that our method is asymptotically consistent, i.e., that it discovers the global optimum given sufﬁciently many samples. Remarkably, in contrast with most BO methods, it may do so without measuring densely over the feasible domain, instead leveraging function network structure to exclude regions as unnecessary to explore. This indicates the power of function network structure to improve query efﬁciency.
We demonstrate through numerical experiments that access to additional information available in a problem formulated as a function network can dramatically accelerate optimization. We study four synthetic problems and four real-world problems: a manufacturing problem similar in spirit to the vaccine example above, an active learning problem with a robotic arm, and two problems arising in epidemiology, one calibrating an epidemic model and the other designing a testing strategy to control the spread of COVID-19. Our method signiﬁcantly outperforms competing methods that utilize less information, in some cases by ∼5% and in other cases by several orders of magnitude. 2