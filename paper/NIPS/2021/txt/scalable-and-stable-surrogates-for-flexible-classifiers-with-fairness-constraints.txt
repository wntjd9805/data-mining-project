Abstract
We investigate how fairness relaxations scale to ﬂexible classiﬁers like deep neural networks for images and text. We analyze an easy-to-use and robust way of impos-ing fairness constraints when training, and through this framework prove that some prior fairness surrogates exhibit degeneracies for non-convex models. We resolve these problems via three new surrogates: an adaptive data re-weighting, and two smooth upper-bounds that are provably more robust than some previous methods.
Our surrogates perform comparably to the state-of-the-art on low-dimensional fairness benchmarks, while achieving superior accuracy and stability for more complex computer vision and natural language processing tasks. 1

Introduction
A growing body of research has explored the ethical and societal implications of increasingly automated decision-making. One critical concern is whether particular machine learning algorithms perform differently on average for members of marginalized groups. In addition to being an ethical question, this is a legal one: Attributes such as race and gender are legally-protected categories, and an employer whose decision-making process disproportionately hurts one group can be litigated for committing adverse or disparate impact. One quantitative notion of fairness is called demographic parity, which states that a decision maker should accept (or classify as positive) a roughly equal proportion of each group [8]. An employer who hires many male applicants, but few female applicants, violates demographic parity by disproportionately allocating resources to one sensitive group.
Machine learning is also increasingly used by law enforcement, where ethical questions become civil rights questions. Facial recognition and risk assessment algorithms have been shown to have disparities in accuracy between black and white defendants [2, 7], which may lead to the arrest of an innocent person [29]. The equalized odds [27] fairness criterion requires a decision maker to have the same false negative rates, as well as the same false positive rates, for each sensitive group. Equality of opportunity requires only one of those two equalities to hold, whichever is considered more ethically important (e.g., in law enforcement, we may ensure that innocent individuals have similar treatment).
Other notions of fairness include predictive parity [16], which matches classiﬁer precision across groups, or the minimization of mutual information [34] between predictions and sensitive attributes.
Except in degenerate cases, some pairs of criteria cannot be achieved simultaneously [3], meaning a domain expert must make a moral decision about which fairness constraint they aim to satisfy.
We formalize the problem of fair classiﬁcation as learning the highest-accuracy model that satisﬁes a constraint on some fairness criterion (either demographic parity or equality of opportunity). This constrained optimization problem is mathematically difﬁcult to solve, which has lead to the creation of a variety of fair classiﬁcation algorithms, many involving relaxations of constraints like those analyzed in this paper. Prior work has mostly limited experiments to small tabular data sets like
COMPAS [2], obscuring the fact that many previous methods are either heuristic, or have guarantees that are limited to linear classiﬁers with convex losses.
∗Department of Computer Science, University of California Irvine School of Information and Computer
Science, Irvine, CA, USA 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We compare fairness relaxations via an optimization framework that explores entire trajectories of fairness-accuracy trade-offs, illuminating advantages and weaknesses of each method. We are the
ﬁrst to show (theoretically and empirically) what conditions may lead relaxation methods to fail or converge to degenerate solutions. We further show that this problem grows worse for non-convex classiﬁers like deep neural networks. To address this weakness, we promote three new fairness surrogates with promising theoretical properties: the log-sigmoid difference, log-sigmoid sum, and sigmoid relaxations. These surrogates are more stable and efﬁcient to optimize than prior work. The log-sigmoid sum and sigmoid bounds have provable fairness guarantees; the log-sigmoid difference has weaker guarantees, but excellent empirical performance.
We are the ﬁrst to empirically compare relaxation methods on deep neural network models for large-scale image and text classiﬁcation. To enable these experiments, we show how our fairness surrogates may be optimized via stochastic gradient descent, and applied to data where the sensitive attribute is not always observed. Our methods are comparable to the state-of-the-art on small tabular datasets, and show clear improvements for deep learning of ﬂexible (but fair) image and text classiﬁers. 2