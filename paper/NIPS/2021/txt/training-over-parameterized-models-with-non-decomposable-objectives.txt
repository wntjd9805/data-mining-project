Abstract
Many modern machine learning applications come with complex and nuanced design goals such as minimizing the worst-case error, satisfying a given precision or recall target, or enforcing group-fairness constraints. Popular techniques for optimizing such non-decomposable objectives reduce the problem into a sequence of cost-sensitive learning tasks, each of which is then solved by re-weighting the training loss with example-speciﬁc costs. We point out that the standard approach of re-weighting the loss to incorporate label costs can produce unsatisfactory results when used to train over-parameterized models. As a remedy, we propose new cost-sensitive losses that extend the classical idea of logit adjustment to handle more general cost matrices. Our losses are calibrated, and can be further improved with distilled labels from a teacher model. Through experiments on benchmark image datasets, we showcase the effectiveness of our approach in training ResNet models with common robust and constrained optimization objectives. 1

Introduction
The misclassiﬁcation error is the canonical performance measure in most treatments of classiﬁcation problems [92]. While appealing in its simplicity, practical machine learning applications often come with more complex and nuanced design goals. For example, these may include minimizing the worst-case error across all classes [64], satisfying a given precision or recall target [106], enforcing minimal coverage for minority classes [31], or imposing group-fairness constraints [105]. Unlike the misclassiﬁcation error, such objectives are non-decomposable, i.e., they cannot be expressed as a sum of losses over individual samples. This poses a non-trivial optimization challenge, which is typically addressed by reducing the problem into a sequence of cost-sensitive learning tasks [12, 25, 4, 16, 71].
Such reduction-based approaches have been successfully employed in many open-source libraries
[3, 2, 1] to provide drop-in replacements for standard loss functions.
In this paper, we point out that the standard approach of re-weighting the training loss to incorporate label costs can produce unsatisfactory results when used with high capacity models, which are often trained to memorize the training data. Such over-parameterized models are frequently encountered in the use of modern neural networks, and have been the subject of considerable recent study [107, 73, 66]. As a remedy, we provide new calibrated losses for cost-sensitive learning that are better equipped at training over-parameterized models to optimize non-decomposable metrics, and demonstrate their effectiveness on benchmark image classiﬁcation tasks. Our main contributions are as follows: (i) we illustrate the pitfalls of using loss re-weighting in over-parameterized settings, particularly with diagonal class weights (Section 3). (ii) we propose new logit-adjusted losses for cost-sensitive learning for both diagonal and non-diagonal gain matrices, and show that they are calibrated (Section 4). (iii) we demonstrate that our losses provide signiﬁcant gains over loss re-weighting in training ResNet models to optimize worst-case recall and to enforce coverage constraints (Section 5). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(iv) we show that the proposed approach compares favorably to post-hoc correction strategies, and can be further improved by distilling scores from a teacher model (Section 6). 1.1