Abstract
Semi-supervised learning (SSL) algorithms have had great success in recent years in limited labeled data regimes. However, the current state-of-the-art SSL algorithms are computationally expensive and entail signiﬁcant compute time and energy requirements. This can prove to be a huge limitation for many smaller companies and academic groups. Our main insight is that training on a subset of unlabeled data instead of entire unlabeled data enables the current SSL algorithms to con-verge faster, signiﬁcantly reducing computational costs. In this work, we propose
RETRIEVE1, a coreset selection framework for efﬁcient and robust semi-supervised learning. RETRIEVE selects the coreset by solving a mixed discrete-continuous bi-level optimization problem such that the selected coreset minimizes the labeled set loss. We use a one-step gradient approximation and show that the discrete optimization problem is approximately submodular, enabling simple greedy algo-rithms to obtain the coreset. We empirically demonstrate on several real-world datasets that existing SSL algorithms like VAT, Mean-Teacher, FixMatch, when used with RETRIEVE, achieve a) faster training times, b) better performance when unlabeled data consists of Out-of-Distribution (OOD) data and imbalance. More speciﬁcally, we show that with minimal accuracy degradation, RETRIEVE achieves a speedup of around 3× in the traditional SSL setting and achieves a speedup of 5× compared to state-of-the-art (SOTA) robust SSL algorithms in the case of imbalance and OOD data. RETRIEVE is available as a part of the CORDS toolkit: https://github.com/decile-team/cords. 1

Introduction
Deep learning algorithms have had great success over the past few years, often achieving human or superhuman performance in various tasks like computer vision [10], speech recognition [18], natural language processing [5], and video games [45]. One of the signiﬁcant factors attributing to the recent success of deep learning is the availability of large amounts of labeled data [55]. However, creating large labeled datasets is often time-consuming and expensive in terms of costs. Moreover, some domains like medical imaging require a domain expert for labeling, making it nearly impossible to create a large labeled set. In order to reduce the dependency on the availability of labeled data, semi-supervised learning (SSL) algorithms [7] were proposed to train models using large amounts of unlabeled data along with the available labeled data. Recent works [42, 56, 4, 53] show that semi-supervised learning algorithms can achieve similar performance to standard supervised learning using signiﬁcantly fewer labeled data instances. 1coResets for EfﬁcienT and Robust semI-supErVised lEarning 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, the current SOTA SSL al-gorithms are compute-intensive with large training times. For example, from our personal experience, train-ing a WideResNet model [60] on a
CIFAR10 [27] dataset with 4000 la-bels using the SOTA FixMatch al-gorithm [53] for 500000 iterations takes around four days on a single
RTX2080Ti GPU. This also implies increased energy consumption and an associated carbon footprint [54]. Fur-thermore, it is common to tune these
SSL algorithms over a large set of hyper-parameters, which means that the training needs to be done hundreds and sometimes thousands of times. For example, [44] performed hyperparameter tuning by running 1000 trails of Gaussian Process-based Blackbox optimization[16] for each SSL algorithm (which runs for 500000 iterations). This process implies signiﬁcantly higher experimental turnaround times, energy consumption, and CO2 emissions. Furthermore, this is not something that can be done at most universities and smaller companies. The ﬁrst problem we try to address in this work is: Can we efﬁciently train a semi-supervised learning model on coresets of unlabeled data to achieve faster convergence and reduction in training time?
Figure 1: (a )Unlabeled set with the same distribution as the labeled set, (b) Unlabeled set containing OOD instances, (c)
Unlabeled set where the class distribution is imbalanced
Despite demonstrating encouraging results on standard and clean datasets, current SSL algorithms perform poorly when OOD data or class imbalance is present in the unlabeled set [44, 8]. This performance degradation can be attributed to the fact that the current SSL algorithms assume that both the labeled set and unlabeled set are sampled from the same distribution. A visualization of OOD data and class imbalance in the unlabeled set is shown in Figure 1. Several recent works [59, 8, 17] were proposed to mitigate the effect of OOD in unlabeled data, in turn improving the performance of
SSL algorithms. However, the current SOTA robust SSL method [17] is 3X slower than the standard
SSL algorithms, further increasing the training times, energy costs, and CO2 emissions. The second problem we try to address in this work is: In the case where OOD data or class imbalance exists in the unlabeled set, can we robustly train an SSL model on coresets of unlabeled data to achieve similar performance to existing robust SSL methods while being signiﬁcantly faster? (a) Traditional SSL (b) Robust SSL
Figure 2: Comparison of RETRIEVE with VAT, FixMatch, and MT on CIFAR-10 and SVHN: We contrast the accuracy degradation with speedup compared to the base SSL or robust SSL (DS3L) approach. We observe speedups of 3× in standard SSL case with 0.7% accuracy drop and 2× speedup with no accuracy drop. In the robust SSL case, we observe 5× speedup compared to DS3L [17] while outperforming it in terms of accuracy.
To this end, we propose RETRIEVE, a coreset selection framework that enables faster convergence and robust training of SSL algorithms. RETRIEVE selects coreset of the unlabeled data resulting in minimum labeled set loss when trained upon in a semi-supervised manner. Intuitively, RETRIEVE tries to achieve faster convergence by selecting data instances from the unlabeled set whose gradients are aligned with the labeled set gradients. Furthermore, RETRIEVE also achieves distribution matching by selecting a coreset from the unlabeled set with similar gradients to the labeled set. 2
1.1 Our Contributions
The contributions are our work can be summarized as follows:
• RETRIEVE Framework: We propose a coreset selection algorithm RETRIEVE for efﬁcient and robust semi-supervised learning. RETRIEVE poses the coreset selection as a discrete-continuous bi-level optimization problem and solves it efﬁciently using an online approximation of single-step gradient updates. Essentially, RETRIEVE selects a coreset of the unlabeled set, which, when trained using the combination of the labeled set and the speciﬁc unlabeled data coreset, minimizes the model loss on the labeled dataset. We also discuss several implementation tricks to speed up the coreset selection step signiﬁcantly (c.f., Section 3.3, Section 3.4)
• RETRIEVE in Traditional SSL: We empirically demonstrate the effectiveness of RETRIEVE in conjunction with several SOTA SSL algorithms like VAT, Mean-Teacher, and FixMatch. The speedups obtained by RETRIEVE are shown in Figure 2a. Speciﬁcally, we see that RETRIEVE consistently achieves close to 3× speedup with accuracy degradation of around 0.7%. RETRIEVE also achieves more than 4.2× speedup with a slightly higher accuracy degradation. Furthermore, when RETRIEVE is trained for more iterations, RETRIEVE can match the performance of VAT while having a 2× speedup (see VAT Extended bar plot in Figure 2a). RETRIEVE also consistently outperforms simple baselines like early stopping and random sampling.
• RETRIEVE in Robust SSL: We further demonstrate the utility of RETRIEVE for robust SSL in the presence of OOD data and imbalance in the unlabeled set. We observe that with the VAT SSL algorithm, RETRIEVE outperforms SOTA robust SSL method DS3L [17] (with VAT) while being around 5× faster. RETRIEVE also signiﬁcantly outperforms just VAT and random sampling. 1.2