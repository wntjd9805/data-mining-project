Abstract
In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of M possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We ﬁrst show that a general instance of LMDPs requires at least Ω((SA)M ) episodes to even approximate the optimal policy. Then, we consider suﬃcient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With suﬃcient separation, we provide an eﬃcient algorithm with local guarantee, i.e., providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical suﬃciency assumptions common in the Predictive
State Representation (PSR) literature (e.g., [6]) and a reachability assumption, we show that the need for initialization can be removed. 1

Introduction
Partially observable Markov decision processes (POMDPs) [42] give a general framework to describe partially observable sequential decision problems. In POMDPs, the underlying dynamics satisfy the Markovian property, but the observations give only partial information on the identity of the underlying states. With the generality of this framework comes a high computational and statistical price to pay: POMDPs are hard, primarily because optimal policies depend on the entire history of the process. But for many important problems, this full generality can be overkill, and in particular, does not have a way to leverage special structure. We are interested in settings where the hidden or latent (unobserved) variables have slow dynamics or are even static in each episode. This model is important for diverse applications, from serving a user in a dynamic web application [18], to medical decision making [45], to transfer learning in diﬀerent RL tasks [8]. Yet, as we explain below, even this area remains little understood, and challenges abound.
Thus, in this work, we consider reinforcement learning (RL) for a special type of POMDP which we call a latent Markov decision process (LMDP). LMDPs consist of some (perhaps large) number M of
MDPs with joint state space S and actions A. In episodic LMDPs with ﬁnite time-horizon H, the static latent (hidden) variable that selects one of M MDPs is randomly chosen at the beginning of each episode, yet is not revealed to the agent. The agent then interacts with the chosen MDP throughout the episode (see Deﬁnition 1 for the formal description). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).