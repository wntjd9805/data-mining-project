Abstract
Normalizing ﬂows are generative models that provide tractable density estimation via an invertible transformation from a simple base distribution to a complex target distribution. However, this technique cannot directly model data supported on an unknown low-dimensional manifold, a common occurrence in real-world domains such as image data. Recent attempts to remedy this limitation have introduced geometric complications that defeat a central beneﬁt of normalizing ﬂows: exact density estimation. We recover this beneﬁt with Conformal Embedding Flows, a framework for designing ﬂows that learn manifolds with tractable densities. We argue that composing a standard ﬂow with a trainable conformal embedding is the most natural way to model manifold-supported data. To this end, we present a series of conformal building blocks and apply them in experiments with synthetic and real-world data to demonstrate that ﬂows can model manifold-supported distributions without sacriﬁcing tractable likelihoods. 1

Introduction
Deep generative modelling is the task of modelling a complex, high-dimensional data distribution from a sample set. Research has encompassed major approaches such as normalizing ﬂows (NFs) [16, 59], generative adversarial networks (GANs) [23], variational autoencoders (VAEs) [36], autoregressive models [52], energy-based models [18], score-based models [64], and diffusion models [29, 63]. NFs in particular describe a distribution by modelling a change-of-variables mapping to a known base density. This approach provides the unique combination of efﬁcient inference, efﬁcient sampling, and exact density estimation, but in practice generated images have not been as detailed or realistic as those of those of other methods [7, 10, 29, 32, 68].
One limitation of traditional NFs is the use of a base density with the same dimensionality as the data. This stands in contrast to models such as GANs and VAEs, which generate data by sampling from a low-dimensional latent prior and mapping the sample to data space. In many application domains, it is known or commonly assumed that the data of interest lives on a lower-dimensional manifold embedded in the higher-dimensional data space [21]. For example, when modelling images, data samples belong to [0, 1]n, where n is the number of pixels in each image and each pixel has a brightness in the domain [0, 1]. However, most points in this data space correspond to meaningless n. A traditional noise, whereas meaningful images of objects lie on a submanifold of dimension m
NF cannot take advantage of the lower-dimensional nature of realistic images.
⌧
There is growing research interest in injective ﬂows, which account for unknown manifold structure by incorporating a base density of lower dimensionality than the data space [6, 12, 13, 39, 41].
Flows with low-dimensional latent spaces could beneﬁt from making better use of fewer parameters, being more memory efﬁcient, and could reveal information about the intrinsic structure of the data.
Properties of the data manifold, such as its dimensionality or the semantic meaning of latent directions, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
can be of interest as well [36, 58]. However, leading injective ﬂow models still suffer from drawbacks including intractable density estimation [6] or reliance on stochastic inverses [13].
In this paper we propose Conformal Embedding Flows (CEFs), a class of ﬂows that use conformal embeddings to transform from low to high dimensions while maintaining invertibility and an efﬁciently computable density. We show how conformal embeddings can be used to learn a lower dimensional data manifold, and we combine them with powerful NF architectures for learning densities. The overall CEF paradigm permits efﬁcient density estimation, sampling, and inference. We propose several types of conformal embedding that can be implemented as composable layers of a ﬂow, including three new invertible layers: the orthogonal k k convolution, the conditional orthogonal transformation, and the special conformal transformation. Lastly, we demonstrate their efﬁcacy on synthetic and real-world data.
⇥ 2