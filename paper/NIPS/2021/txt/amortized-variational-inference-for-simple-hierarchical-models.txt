Abstract
It is difficult to use subsampling with variational inference in hierarchical models since the number of local latent variables scales with the dataset. Thus, inference in hierarchical models remains a challenge at large scale. It is helpful to use a variational family with structure matching the posterior, but optimization is still slow due to the huge number of local distributions. Instead, this paper suggests an amortized approach where shared parameters simultaneously represent all local distributions. This approach is similarly accurate as using a given joint distribution (e.g., a full-rank Gaussian) but is feasible on datasets that are several orders of magnitude larger. It is also dramatically faster than using a structured variational distribution. 1

Introduction
Hierarchical Bayesian models are a general framework where parameters of “groups” are drawn from some shared distribution, and then observed data is drawn from a distribution specified by each group’s parameters. After data is observed, the inference problem is to infer both the parameters for each group and the shared parameters. These models have proven useful in various domains [13] including hierarchical regression amd classification [12], topic models [4, 22, 3], polling [11, 24], epidemiology [23], ecology [8], psychology [37], matrix-factorization [35], and collaborative filtering
[26, 33].
A proven technique for scaling variational inference (VI) to large datasets is subsampling. The idea is that if the target model has the form p(z, y) = p(z) (cid:81) i p(yi|z) then an unbiased gradient can be estimated while only evaluating p(z) and p(yi|z) at a few i [29, 16, 20, 31, 30, 36, 15].
This paper addresses hierarchical models of the form p(θ, z, y) = p(θ) (cid:81) i p(zi, yi|θ), where only y is observed. There are two challenges. First, the number of local latent variables zi increases with the dataset, meaning the posterior distribution increases in dimensionality. Second, there is often a dependence between zi and θ which must be captured to get strong results [15, 18].
The aim of this paper is to develop a black-box variational inference scheme that can scale to large hierarchical models without losing benefits of a joint approximation. Our solution takes three steps.
First, in the true posterior, the different latent variables zi are conditionally independent given θ, which suggests using a variational family of the same form. We confirm this intuition by showing that for any joint variational family q(θ, z), one can define a corresponding "branch" family q(θ) (cid:81) i q(zi|θ) such that inference will be equally accurate (theorem 2). We call inference using such a family the
"branch" approach.
Second, we observe that if using the branch approach, the optimal local variational parameters can be computed only from θ and local data (eq. (12)). Thus, we propose to amortize the computation of the local variational parameters by learning a network to approximately solve that optimization. We 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
show that when the target distribution is symmetric over latent variables, this will be as accurate as the original joint family, assuming a sufficiently capable amortization network (claim 5).
Third, we note that in many real hierarchical models, there are many i.i.d. data generated from each local latent variable. This presents a challenge for learning an amortization network, since the full network should deal with different numbers of data points and naturally reflect the symmetry between the inputs (that is, without having to relearn the symmetry.) We propose an approach where a preliminary "feature" network processes each datum, after which they are combined with a pooling operation which forms the input for a standard network (section 6). This is closely related to the
"deep sets" [39] strategy for permutation invariance.
We validate these methods on a synthetic model where exact inference is possible, and on a user-preference model for the MovieLens dataset with 162K users who make 25M ratings of different movies. At small scale (2.5K ratings), we show similar accuracy using a dense joint Gaussian, a branch distribution, or our amortized approach. At moderate scale (180K ratings), joint inference is intractable. Branch distributions gives a meaningful answer, and the amortized approach is comparable or better. At large scale (18M ratings) the amortized approach is thousands of nats better on test-likelihoods even after branch distributions were trained for almost ten times as long as the amortized approach took to converge (fig. 6). 2 Hierarchical Branched Distributions
θ zi yij xij j ∈ {1, . . . , ni} i ∈ {1, . . . , N }
θ z2 z1 z3 y11 y12 y21 y31 y32 y33 x11 x12 x21 x31 x32 x33
Figure 1: The graphical model for the HBDs. On the left, we have plate notation for the generic HBD from eq. (3). Note, we can have an edge from θ to yij (we skip it for clarity.) On the right, we have an example model with N = 3.
We focus on two-level hierarchical distributions. A generic model of this type is given by p(θ, z, y|x) = p(θ)
N (cid:89) i=1 p(zi|θ)p(yi|θ, zi, xi), (1) where θ and z = {zi}N i=1 are covariates. As the visual representations of these models resemble branches, we refer them as hierarchical branch distributions (HBDs). i=1 are observations, and x = {xi}N i=1 are latent variables, y = {yi}N
Symmetric. We call an HBD symmetric if the conditionals are symmetric, i.e., if zi = zj, xi = xj, and yi = yj, it implies that p(zi|θ) = p(zj|θ), and p(yi|θ, zi, xi) = p(yj|θ, zj, xj). (2)
Locally i.i.d. Often local observations yi (and xi) are a collection of conditionally i.i.d observations.
Then, an HBD takes the form of p(θ, z, y|x) = p(θ)
N (cid:89) i=1 p(zi|θ) ni(cid:89) j=1 p(yij|θ, zi, xij), (3) where yi = {yij}ni j=1 and xi = {xij}ni covariates; ni ≥ 1 is the number of observations for branch i. j=1 are collections of conditionally i.i.d observations and 2
No local covariates. Some applications do not involve the covariates xi. In such cases, HBDs have a simplified form of p(θ, z, y) = p(θ)
N (cid:89) p(zi|θ)p(yi|θ, zi). (4) i=1
In this paper, we will be using eq. (1) and eq. (3) to refer HBDs—the results extend easily to case where there are no local covariates. (For instance, in section 5, we amortize using (xi, yi) as inputs.
When there are no covariates, we can amortize with just yi.) 2.1