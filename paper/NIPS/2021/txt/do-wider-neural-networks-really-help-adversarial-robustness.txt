Abstract
Adversarial training is a powerful type of defense against adversarial examples.
Previous empirical results suggest that adversarial training requires wider networks for better performances. However, it remains elusive how does neural network width affect model robustness. In this paper, we carefully examine the relationship between network width and model robustness. Speciﬁcally, we show that the model robustness is closely related to the tradeoff between natural accuracy and perturbation stability, which is controlled by the robust regularization parameter
λ. With the same λ, wider networks can achieve better natural accuracy but worse perturbation stability, leading to a potentially worse overall model robustness.
To understand the origin of this phenomenon, we further relate the perturbation stability with the network’s local Lipschitzness. By leveraging recent results on neural tangent kernels, we theoretically show that wider networks tend to have worse perturbation stability. Our analyses suggest that: 1) the common strategy of
ﬁrst ﬁne-tuning λ on small networks and then directly use it for wide model training could lead to deteriorated model robustness; 2) one needs to properly enlarge λ to unleash the robustness potential of wider models fully. Finally, we propose a new
Width Adjusted Regularization (WAR) method that adaptively enlarges λ on wide models and signiﬁcantly saves the tuning time. 1

Introduction
Researchers have found that Deep Neural Networks (DNNs) suffer badly from adversarial examples
[59]. By perturbing the original inputs with an intentionally computed, undetectable noise, one can deceive DNNs and even arbitrarily modify their predictions on purpose. To defend against adversarial examples and further improve model robustness, various defense approaches have been proposed
[48, 42, 18, 40, 67, 26, 58, 55]. Among them, adversarial training [23, 41] has been shown to be the most effective type of defenses [5]. Adversarial training can be seen as a form of data augmentation by
ﬁrst ﬁnding the adversarial examples and then training DNN models on those examples. Speciﬁcally, given a DNN classiﬁer f parameterized by θ, a general form of adversarial training with loss function
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
L can be deﬁned as: argmin
θ 1
N
N (cid:88) (cid:104) i=1
L(θ; xi, yi) (cid:123)(cid:122) (cid:125) (cid:124) natural risk
+λ · max (cid:98)xi∈B(xi,(cid:15)) (cid:124) (cid:2)L(θ; (cid:98)xi, yi) − L(θ; xi, yi)(cid:3) (cid:125) (cid:123)(cid:122) robust regularization (cid:105)
, (1.1) i=1} are training data, B(x, (cid:15)) = {(cid:98)x | (cid:107)(cid:98)x − x(cid:107)p ≤ (cid:15)} denotes the (cid:96)p norm ball where {(xi, yi)n with radius (cid:15) centered at x, and p ≥ 1, and λ > 0 is the regularization parameter. Compared with standard empirical risk minimization, the extra robust regularization term encourages the data points within B(x, (cid:15)) to be classiﬁed as the same class, i.e., encourages the predictions to be stable. The regularization parameter λ adjusts the strength of robust regularization. When λ = 1, it recovers the formulation in [41], and when λ = 0.5, it recovers the formulation in [23]. Furthermore, replacing the loss difference in robust regularization term with the KL-divergence based regularization recovers the formulation in [70].
One common belief in the practice of adversar-ial training is that, compared with the standard empirical risk minimization, adversarial training requires much wider neural networks to achieve better robustness. [41] provided an intuitive ex-planation: robust classiﬁcation requires a much more complicated decision boundary, as it needs to handle the presence of possible adversarial examples. However, it remains elusive how the network width affects model robustness. To an-swer this question, we ﬁrst examine whether the larger network width contributes to both the nat-ural risk term and the robust regularization term in (1.1). Interestingly, when tracing the value changes in (1.1) during adversarial training, we observe that the value of the robust regulariza-tion part actually gets worse on wider models, suggesting that larger network width does not lead to better stability in predictions. In Figure 1, we show the loss value comparison of two different wide models trained by TRADES [70] with λ = 6 as suggested in the original paper. We can see that the wider model (i.e., WideResNet-34-10) achieves better natural risk but incurs a larger value on robust regularization. This motivates us to ﬁnd out the cause of this phenomenon.
Figure 1: Plots of both natural risk and robust regularization in (1.1). Two 34-layer WideResNet
[69] are trained by TRADES [70] on CIFAR10
[37] with widen factor being 1 and 10. (b) Robust Regularization (a) Natural Risk
In this paper, we study the relationship between neural network width and model robustness for adversarially trained neural networks. Our contributions can be summarized as follows: 1. We show that the model robustness is closely related to both natural accuracy and perturbation stability, a new metric we proposed to characterize the strength of robust regularization. The balance between the two is controlled by the robust regularization parameter λ. With the same value of λ, the natural accuracy is improved on wider models while the perturbation stability often worsens, leading to a possible decrease in the overall model robustness. This suggests that proper tuning of λ on wide models is necessary despite being extremely time-consuming, while directly using the ﬁne-tuned λ on small networks to train wider ones, as many people did in practice
[41, 70], may lead to deteriorated model robustness. 2. Unlike previous understandings that there exists a trade-off between natural accuracy and robust accuracy, we show that the real trade-off should between natural accuracy and perturbation stability.
And the robust accuracy is actually the consequence of this trade-off. 3. To understand the origin of the lower perturbation stability of wider networks, we further relate perturbation stability with the network’s local Lipschitznesss. By leveraging recent results on neural tangent kernels [36, 3, 73, 8, 21], we show that with the same value of λ, larger network width naturally leads to worse perturbation stability, which explains our empirical ﬁndings. 4. Our analyses suggest that to unleash the potential of wider model architectures fully, one should mitigate the perturbation stability deterioration and enlarge robust regularization parameter λ for training wider models. Empirical results veriﬁed the effectiveness of this strategy on benchmark datasets. In order to alleviate the heavy burden for tuning λ on wide models, we develop the 2
Width Adjusted Regularization (WAR) method to transfer the knowledge we gain from ﬁne-tuning smaller networks into the training of wider networks and signiﬁcantly save the tuning time.
Notation. For a d-dimensional vector x = [x1, ..., xd](cid:62), we use (cid:107)x(cid:107)p = ((cid:80)d i=1 |xi|p)1/p with p ≥ 1 to denote its (cid:96)p norm. 1(·) represents the indicator function and ∀ represents the universal quantiﬁer. 2