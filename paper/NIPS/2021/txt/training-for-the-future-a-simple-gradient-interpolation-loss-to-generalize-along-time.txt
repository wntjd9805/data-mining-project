Abstract
In several real world applications, machine learning models are deployed to make predictions on data whose distribution changes gradually along time, leading to a drift between the train and test distributions. Such models are often re-trained on new data periodically, and they hence need to generalize to data not too far into the future. In this context, there is much prior work on enhancing temporal generalization, e.g. continuous transportation of past data, kernel smoothed time-sensitive parameters and more recently, adversarial learning of time-invariant features. However, these methods share several limitations, e.g, poor scalability, training instability, and dependence on unlabeled data from the future. Responding to the above limitations, we propose a simple method that starts with a model with time-sensitive parameters but regularizes its temporal complexity using a Gradient
Interpolation (GI) loss. GI allows the decision boundary to change along time and can still prevent overﬁtting to the limited training time snapshots by allowing task-speciﬁc control over changes along time. We compare our method to existing baselines on multiple real-world datasets, which show that GI outperforms more complicated generative and adversarial approaches on the one hand, and simpler gradient regularization methods on the other. 1

Introduction
Many organizations operate their machine learning pipeline as follows: collect labeled data based on day-to-day operations e.g. user-clicks for a recommendation model; periodically (say every ∆ = 48 hours) train the model on the data collected from the past; deploy the model to serve requests of the next ∆ period; repeat the cycle. Real-world examples of such usage include recommendation models for e-retails, loan approval models in banks, regression models to forecast future popularity of media content, classiﬁcation of malicious websites, and activity recognition models from sensor data.
A key motivation behind such periodic retraining is the implicit knowledge that the data distribution is not stationary, requiring a revision of the decision boundary with time. While incorporating fresh data in training does make the trained model more current, we investigate if we can improve the training even further. In particular, when training the model at a time t, we know that the model will not be deployed on data samples in the training interval, but on data from the immediate future
[t, t + ∆] which could differ from even the most recent training data. Our goal in this paper is to use this knowledge more centrally to train a model tailored to the intended deployment period.
Formal Problem Statement. We consider prediction tasks over an input space X and output space
Y where the joint distribution P (X , Y) evolves with time. The prediction model should therefore be conditioned on time as P (y|x, t) for x ∈ X , y ∈ Y, and time stamp t. During training we are provided with labeled data from T time snapshots t1 ≤ t2... ≤ tT . Call these D1, D2, . . . DT where each Ds = {(xi, yi) : i = 1 . . . ns} is assumed to be sampled from P (x, y|ts). The trained model
∗Equal Contribution
†anshulnasery@gmail.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
will only be deployed on data from a time interval in the future which we denote as tT +1, that is on examples sampled from P (x|tT +1). Note that we only evaluate performance once on time tT +1, differing from online learning where regret is computed incrementally. Following most practical setups, we assume that the temporal drift across time is not too high. Since the deployment time is in the future, we cannot assume the presence of even unlabeled target data at the time of training the model. Further, at different time snapshots, completely unrelated sets of xi may comprise a Ds.
This makes the problem much harder than standard time-series forecasting tasks, since we do not have values of the same instance at different points in time. Our goal is not to generalize to all future data, but rather to data relatively close in the future, since our motivating application is one where models are retrained periodically. The former generalization would require very different models and approaches than what we propose. We assume that P (y|x, t) is implemented as a general-purpose neural network F (x, t) that takes as input x, t and outputs either (1) a vector of logits for classiﬁcation tasks or (2) mean and other scores deﬁning a continuous distribution over predicted y for regression tasks.
A solution often used in practice is to only train the model on recent data and/or apply a graceful ageing penalty on past data. However, this may not be adequate with data hungry modern networks, especially when non-obvious seasonality makes distant data relevant. So we seek general purpose solutions that can harness all available data for making optimal decisions into the future. Any such solution ﬁrst needs to make the model time sensitive. This can be done by feeding time as an input feature, or by making all or some of the model parameters to be a function of time. However, that by itself cannot be sufﬁcient to generalize to future data if we train the model to minimize loss on the training snapshots. In fact, a sufﬁciently complex model can easily overﬁt on training time-stamps when the number of instances per snapshot is large and the number of time stamps T is small.
The ﬁeld of continuous learning [1] offers one approach to tackle this problem. However, in our case the training data can be revisited and fresh models retrained making issues like catastrophic forgetting irrelevant. More relevant to our problem formulation are recent continuous domain adaptation methods [2, 3, 4, 5, 6] which either try to predict how the training domains evolve as time passes, or learn feature representations which are invariant across time [4, 5]. However, these domain adaptation approaches assume the presence of unlabeled data in the future, which is not available in our case.
Another set of approaches aim to make network parameters a function of time [7, 8, 9] but they require external smoothness kernels as hyper-parameters for each parameter-type, which cannot be easily trained end-to-end on deep neural networks.
The key objective of our desired training algorithm is to generalize to the near future, even at the cost of performance on past data. In order to achieve this goal, the training algorithm needs to encourage the model to adjust for temporal drifts both in the data distribution and the decision boundary, and be able to generalize and extrapolate well taking into account such shifts. To this end, we aim to develop a supervised learning model which would be robust to small perturbations in time— which is fed as feature. We attempt learning functions that are smooth in time, ensuring a limited temporal drift from past to future. However, given a limited number of training time-stamps, a moderately complex model is prone to ﬁt the training data perfectly, without generalizing it well to the future. To tackle this problem, we propose a method which can learn to generalize well in time by interpolating the predictor between two training time steps using a ﬁrst order Taylor series expansion. This interpolated predictor is supervised using the available labels. While doing so, it makes optimal use of the temporal information, which allows it to extrapolate well on future time steps.
Our gradient interpolation method gives us the privilege to use a complex temporal model even if the number of timestamps is small without the fear of overﬁtting. To that end, we develop a novel neural model that captures the evolving decision boundary over time. More speciﬁcally, we design a novel time dependent activation function called leaky Temporal ReLU (TReLU), which changes the underlying activation threshold smoothly along time. Such a model along with the Gradient
Interpolation (GI) based loss function allows our method to make the optimal use of the temporal information and thus extrapolate well to future time.
Our Contributions. In this work, we propose a simple training algorithm to encourage a model to learn functions which can extrapolate well to the near future. We do this by supervising the ﬁrst order Taylor expansion of the learnt function. We also propose a method to make neural networks time aware by modifying the leaky ReLU activation function to change with different time-stamps.
We demonstrate strong empirical performance of our method by comparing against state-of-the-art methods and practical baselines on several real world datasets, and also give insights into its workings through a simple synthetic setup. 2
2