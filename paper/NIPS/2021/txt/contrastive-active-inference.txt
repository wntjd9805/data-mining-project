Abstract
Active inference is a unifying theory for perception and action resting upon the idea that the brain maintains an internal model of the world by minimizing free energy. From a behavioral perspective, active inference agents can be seen as self-evidencing beings that act to fulﬁll their optimistic predictions, namely preferred outcomes or goals. In contrast, reinforcement learning requires human-designed re-wards to accomplish any desired outcome. Although active inference could provide a more natural self-supervised objective for control, its applicability has been lim-ited because of the shortcomings in scaling the approach to complex environments.
In this work, we propose a contrastive objective for active inference that strongly reduces the computational burden in learning the agent’s generative model and planning future actions. Our method performs notably better than likelihood-based active inference in image-based tasks, while also being computationally cheaper and easier to train. We compare to reinforcement learning agents that have access to human-designed reward functions, showing that our approach closely matches their performance. Finally, we also show that contrastive methods perform signiﬁcantly better in the case of distractors in the environment and that our method is able to generalize goals to variations in the background. 1

Introduction
Deep Reinforcement Learning (RL) has led to successful results in several domains, such as robotics, video games and board games [42, 36, 2]. From a neuroscience perspective, the reward prediction error signal that drives learning in deep RL closely relates to the neural activity of dopamine neurons for reward-based learning [44, 3]. However, the reward functions used in deep RL typically require domain and task-speciﬁc design from humans, spoiling the generalization capabilities of RL agents.
Furthermore, the possibility of faulty reward functions makes the application of deep RL risky in real-world contexts, given the possible unexpected behaviors that may derive from it [10, 29, 38].
Active Inference (AIF) has recently emerged as a unifying framework for learning perception and action. In AIF, agents operate according to one absolute imperative: minimize their free energy [15].
With respect to past experience, this encourages to update an internal model of the world to maximize evidence with respect to sensory data. With regard to future actions, the inference process becomes
‘active’ and agents select behaviors that fulﬁll optimistic predictions of their model, which are represented as preferred outcomes or goals [17]. Compared to RL, the AIF framework provides a more natural way of encoding objectives for control. However, its applicability has been limited because of the shortcomings in scaling the approach to complex environments, and current implementations have focused on tasks with either low-dimensional sensory inputs and/or small sets of discrete actions [12].
Moreover, several experiments in the literature have replaced the agent’s preferred outcomes with
RL-like rewards from the environment, downplaying the AIF potential to provide self-supervised objectives [13, 34, 49]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
One of the major shortcomings in scaling AIF to environments with high-dimensional, e.g. image-based, environments comes from the necessity of building accurate models of the world, which try to reconstruct every detail in the sensory data. This complexity is also reﬂected in the control stage, when AIF agents compare future imaginary outcomes of potential actions with their goals, to select the most convenient behaviors. In particular, we advocate that fulﬁlling goals in image space can be poorly informative to build an objective for control.
In this work, we propose Contrastive Active Inference, a framework for AIF that aims to both reduce the complexity of the agent’s internal model and to propose a more suitable objective to fulﬁll preferred outcomes, by exploiting contrastive learning. Our method provides a self-supervised objective that constantly informs the agent about the distance from its goal, without needing to reconstruct the outputs of potential actions in high-dimensional image space.
The contributions of our work can be summarised as follows: (i) we propose a framework for AIF that drastically reduces the computational power required both for learning the model and planning future actions, (ii) we combine our method with value iteration methods for planning, inspired by the RL literature, to amortize the cost of planning in AIF, (iii) we compare our framework to state-of-the-art RL techniques and to a non-contrastive AIF formulation, showing that our method compares well with reward-based systems and outperforms non-contrastive AIF, (iv) we show that contrastive methods work better than reconstruction-based methods in presence of distractors in the environment, (v) we found that our contrastive objective for control allows matching desired goals, despite differences in the backgrounds. The latter ﬁnding could have important consequences for deploying AIF in real-world settings, such as robotics, where perfectly reconstructing observations from the environment and matching them with high-dimensional preferences is practically unfeasible. 2