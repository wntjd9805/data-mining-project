Abstract
Human explanation (e.g., in terms of feature importance) has been recently used to extend the communication channel between human and agent in interactive machine learning. Under this setting, human trainers provide not only the ground truth but also some form of explanation. However, this kind of human guidance was only investigated in supervised learning tasks, and it remains unclear how to best incorporate this type of human knowledge into deep reinforcement learning. In this paper, we present the ﬁrst study of using human visual explanations in human-in-the-loop reinforcement learning (HIRL). We focus on the task of learning from feedback, in which the human trainer not only gives binary evaluative "good" or
"bad" feedback for queried state-action pairs, but also provides a visual explanation by annotating relevant features in images. We propose EXPAND (EXPlanation
AugmeNted feeDback) to encourage the model to encode task-relevant features through a context-aware data augmentation that only perturbs irrelevant features in human salient information. We choose ﬁve tasks, namely Pixel-Taxi and four
Atari games, to evaluate the performance and sample efﬁciency of this approach.
We show that our method signiﬁcantly outperforms methods leveraging human explanation that are adapted from supervised learning, and Human-in-the-loop RL baselines that only utilize evaluative feedback. 1

Introduction
Deep reinforcement learning (DRL) algorithms have achieved many successes in solving problems with high-dimensional state and action spaces [27, 3]. However, DRL’s performance is limited by its sample (in)efﬁciency. It is often impractical to collect millions of training samples as required by standard DRL algorithms. One way to tame this problem is to leverage additional human guidance by following the general paradigm of Human-in-the-Loop Reinforcement Learning (HIRL) [51]. It often allows the agent to achieve better performance and higher sample efﬁciency.
One popular form of human guidance in HIRL is the binary evaluative feedback [15], in which humans provide a "good" or "bad" judgment for a queried state-action pair. This framework allows 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
non-expert humans to provide feedback, but its sample efﬁciency could be further improved by asking humans to provide stronger guidance. For example, the binary feedback does not tell the agent why it made a mistake. If humans can explain the "why" behind the evaluative feedback, then it is possible to further improve the sample efﬁciency and performance.
Taking the training of an autonomous driving agent as a motivating example, humans can "explain" the correct action of "apply-brake" by pointing out to the agent that the "STOP" sign is an essential signal. One way to convey this information is through saliency information, in which humans highlight the important (salient) regions of the visual environment state. The visual explanation will indicate which visual features matter the most for the decision in the current state. Note that requiring human trainers to provide explanations on their evaluations does not necessarily require any more expertise on their part than is needed for providing only binary feedback. In our driving agent example, the human trainers may not know things like the optimal angle of the steering wheel; however, we can expect them to be able to tell whether an observed action is good or bad, and what visual objects matter for that decision.
In fact, the use of human visual explanation has been investigated by recent works in several supervised learning tasks [39, 35, 34]. They show that the generalization of a convolutional neural network can be improved by forcing the model to output the same saliency map as human
- in other words, forces to model to make the right prediction for the right reasons. However, it remains unclear how to effectively incorporate domain knowledge in visual explanation in deep reinforcement learning.
Figure 1: Overview of EXPAND. The agent queries the human with a sampled trajectory for bi-nary evaluative feedback on the action and saliency annotation on the state. The Perturbation Module supplements the saliency explanation by perturb-ing irrelevant regions. The agent then consumes the integrated feedback and updates its parameters.
This loop continues until the agent is trained with feedback queried every Nf episodes. The domain shown for "Visual Explanation" is Pixel Taxi.
In this work, we present EXPAND, which aims to leverage - EXPlanation AugmeNted feeDback (Fig. 1) to support efﬁcient human guidance for deep reinforcement learning. This raises two im-mediate challenges (i) how to make it easy for humans to provide the visual explanations and (ii) how to amplify the sparse human feedback.
EXPAND employs a novel context-aware data augmentation method, which ampliﬁes the dif-ference between relevant and irrelevant regions in human explanation by applying multiple perturbations to irrelevant parts. To reduce the human effort needed to provide visual explanations, EXPAND uses off-the-shelf object detectors and trackers to allow humans to to give explanations in terms of salient objects in the visual state, which are then automatically converted to saliency regions to be used by the RL system. We show that EXPAND agents require fewer interactions with the environment (environment sample efﬁciency) and over 30% fewer human signals (human feedback sample efﬁciency) by leveraging human visual explanation.
We highlight the main contributions of EXPAND below:
• This is the ﬁrst work that leverages human visual explanation in human-in-the-loop rein-forcement learning tasks. We show that EXPAND is the state-of-the-art method to learn from human evaluative feedback in terms of environment sample efﬁciency and human feedback sample efﬁciency.
• We benchmark our context-aware data augmentation against standard context-agnostic data augmentation techniques. Our experiment results help shed light on the limitations of existing data augmentations and can beneﬁt future research in accelerating RL with augmented data.
• We show that human visual explanation in a sequential decision-making task can be collected in a low-effort and semi-automated way by using an off-the-shelf object detector and tracker.
We note that EXPAND agents have applicability beyond improving the efﬁciency of human-in-the-loop RL with sparse environment rewards. In particular, they can be equally useful in accepting human guidance to align the RL system to human preferences. They can be used to incorporate 2
rewards from human feedback, for example, when environment reward is not deﬁned upfront or to accommodate additional constraints that are not reﬂected in existing reward signal. EXPAND, thus, can also be useful in aligning objectives of humans in the loop to the agent’s [15, 5]. Finally,
EXPAND’s approach of allowing humans to provide their explanations and guidance in terms of objects of relevance to them, and automatically converting those to saliency regions, is an instance of the general approach for explainable and advisable AI systems that use symbols as a lingua franca for communicating with humans, independent of whether they use symbolic reasoning internally [13]. 2