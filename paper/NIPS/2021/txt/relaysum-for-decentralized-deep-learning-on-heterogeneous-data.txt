Abstract
In decentralized machine learning, workers compute model updates on their local data. Because the workers only communicate with few neighbors without cen-tral coordination, these updates propagate progressively over the network. This paradigm enables distributed training on networks without all-to-all connectivity, helping to protect data privacy as well as to reduce the communication cost of distributed training in data centers. A key challenge, primarily in decentralized deep learning, remains the handling of differences between the workers’ local data distributions. To tackle this challenge, we study the RelaySum mechanism for information propagation in decentralized learning. RelaySum uses spanning trees to distribute information exactly uniformly across all workers with ﬁnite delays depending on the distance between nodes. In contrast, the typical gossip averaging mechanism only distributes data uniformly asymptotically while using the same communication volume per step as RelaySum. We prove that RelaySGD, based on this mechanism, is independent of data heterogeneity and scales to many workers, enabling highly accurate decentralized deep learning on heterogeneous data. Our code is available at http://github.com/epfml/relaysgd. 1

Introduction
Ever-growing datasets lay at the foundation of the recent breakthroughs in machine learning. Learning algorithms therefore must be able to leverage data distributed over multiple devices, in particular for reasons of efﬁciency and data privacy. There are various paradigms for distributed learning, and they differ mainly in how the devices collaborate in communicating model updates with each other.
In the all-reduce paradigm, workers average model updates with all other workers at every training step. In federated learning [24], workers perform local updates before sending them to a central server that returns their global average to the workers. Finally, decentralized learning signiﬁcantly generalizes the two previous scenarios. Here, workers communicate their updates with only few directly-connected neighbors in a network, without the help of a server.
Decentralized learning offers strong promise for new applications, allowing any group of agents to collaboratively train a model while respecting the data locality and privacy of each contributor [25].
At the same time, it removes the single point of failure in centralized systems such as in federated learning [12], improving robustness, security, and privacy. Even from a pure efﬁciency standpoint, decentralized communication patterns can speed up training in data centers [2].
In decentralized learning, workers share their local stochastic gradient updates with the others through gossip communication [41]. They send their updates to their neighbors, which iteratively
∗Equal contribution. Corresponding authors thijs.vogels@epfl.ch and lie.he@epfl.ch. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1:
To spread information across a decentralized network, classical gossip averaging diffuses information slowly through the network. The left ﬁgure illustrates the spread of information originating from the fourth worker in a chain network. In RelaySum, the messages are relayed without reweighting, resulting in uniform delivery of the information to every worker. When multiple workers broadcast simultaneously (not pictured), RelaySum can sum their messages and use the same bandwidth as gossip averaging. propagate the updates further into the network. The workers typically use iterative gossip averaging of their models with their neighbors, using averaging weights chosen to ensure asymptotic uniform distribution of each update across the network. It will take τ rounds of communication for an update from worker i to reach a worker j that is τ hops away, and when it ﬁrst arrives, the update is exponentially weakened by repeated averaging with weights < 1. In general networks, worker j will never exactly, but only asymptotically receive its uniform share of the update. The slow distribution of updates not only slows down training, but also makes decentralized learning sensitive to heterogeneity in workers’ data distributions.
We study an alternative mechanism to gossip averaging, which we call RelaySum. RelaySum operates on spanning trees of the network, and distributes information exactly uniformly within a ﬁnite number of gossip steps equal to the diameter of the network. Rather than iteratively averaging models, each node acts as a ‘router’ that relays messages through the whole network without decaying their weight at every hop. While naive all-to-all routing requires n2 messages to be transmitted at each step, we show that on trees, only n messages (one per edge) are sufﬁcient. This is enabled by the key observation that the routers can merge messages by summation to avoid any extra communication compared to gossip averaging. RelaySum achieves this using additional memory linear in the number of edges, and by tailoring the messages sent to different neighbors. At each time step, RelaySum workers receive a uniform average of exactly one message from each worker. Those messages just originate from different time delays depending on how many hops they travelled. The difference between gossip averaging and RelaySum is illustrated in Figure 1.
The RelaySum mechanism is structurally similar to Belief Propagation algorithms for inference in graphical models. This link was made by Zhang et al. [50], who used the same mechanism for decentralized weighted average consensus in control.
We use RelaySum in the RelaySGD learning algorithm. We theoretically show that this algorithm is not affected by differences in workers’ data distributions. Compared to other algorithms that have this property [36, 31], RelaySGD does not require the selection of averaging weights, and its convergence does not depend on the spectral gap of the averaging matrix, but instead on the network diameter.
While RelaySum is formulated for trees, it can be used in any decentralized network. We use the
Spanning Tree Protocol [30] to construct spanning trees of any network in a decentralized fashion.
RelaySGD often performs better on any such spanning tree than gossip-based methods on the original graph. When the communication network can be chosen freely, the algorithm can use double binary trees [33]. While these trees have logarithmic diameter and scale to many workers, RelaySGD in this setup uses only constant memory equivalent to two extra copies of the model parameters and sends and receives only two models per iteration.
Surprisingly, in deep learning with highly heterogeneous data, prior methods that are theoretically independent of data heterogeneity [36, 31], perform worse than heuristic methods that do not have this property, but use cleverly designed time-varying communication topologies [2]. In extensive tests on image- and text classiﬁcation, RelaySGD performs better than both kinds of baselines at equal communication budget. 2
2