Abstract
Modern deep learning applications require increasingly more compute to train state-of-the-art models. To address this demand, large corporations and institutions use dedicated High-Performance Computing clusters, whose construction and maintenance are both environmentally costly and well beyond the budget of most organizations. As a result, some research directions become the exclusive domain of a few large industrial and even fewer academic actors. To alleviate this disparity, smaller groups may pool their computational resources and run collaborative experiments that beneﬁt all participants. This paradigm, known as grid- or volunteer computing, has seen successful applications in numerous scientiﬁc areas. However, using this approach for machine learning is difﬁcult due to high latency, asymmetric bandwidth, and several challenges unique to volunteer computing. In this work, we carefully analyze these constraints and propose a novel algorithmic framework designed speciﬁcally for collaborative training. We demonstrate the effectiveness of our approach for SwAV and ALBERT pretraining in realistic conditions and achieve performance comparable to traditional setups at a fraction of the cost.
Finally, we provide a detailed report of successful collaborative language model pretraining with 40 participants. 1

Introduction
The deep learning community is becoming increasingly more reliant on transfer learning. In computer vision, pretraining convolutional networks on large image collections such as ImageNet [1] is the de facto standard for a wide range of applications ranging from object detection [2] and semantic segmentation [3] to image classiﬁcation [4] and even learning perceptual similarity [5]. A growing number of natural language processing systems capitalize on language models with billions of
∗Equal contribution. Correspondence to mryabinin0@gmail.com
Detailed author contributions are listed at the end of the work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
parameters [6, 7, 8, 9, 10, 11] trained on vast unlabeled corpora. Similar trends have emerged in areas such as speech processing [12], reinforcement learning [13], and computational biology [14, 15].
Training these models is a notoriously time-consuming and challenging task: it often requires hundreds of high-end GPU servers [10, 16] and would take multiple years on a single device [17].
Most academic and independent researchers simply cannot afford to train state-of-the-art models from scratch, which slows down scientiﬁc progress and practical adoption of deep learning.
Historically, the deep learning community has addressed this problem via “model hubs” or “model zoos” — public repositories for pretrained model checkpoints [18, 19, 20, 21]. These repositories have played a signiﬁcant role in the democratization of deep learning, allowing everyone to reap the beneﬁts of large-scale training runs conducted by corporations and universities with sufﬁcient resources. However, model hubs are limited to a narrow subset of datasets and tasks that match the interests of model creators. For instance, in natural language processing, it is often difﬁcult to ﬁnd up-to-date models for more than a handful of languages [22]. In turn, computer vision hubs rarely feature models trained on drawings, satellite images, 3D renders, microscopy, or any other data that does not resemble ImageNet. As a result, many researchers in these areas can only work on problems for which there are available pretrained models rather than the problems that most need solving.
However, there might be an alternative way to obtain pretrained models: to train these models collaboratively. This approach, known as volunteer (or grid) computing, allows many independent parties to combine their computational resources and collectively perform large-scale experiments [23, 24, 25]. The raw compute performance of such collaborations often exceeds that of the fastest supercomputers [26]; however, fully utilizing it can be challenging due to several reasons. First, devices that contribute to collaborative experiments can range from GPU servers and high-end workstations to consumer-grade computers and even smartphones [27]. Second, most of these devices use household internet connection with limited bandwidth and low reliability. Third, participants in such projects often donate their hardware part-time, joining and leaving the experiment at will.
While it is theoretically possible to train neural networks on this kind of infrastructure, modern distributed training strategies are only efﬁcient in a narrow range of conditions. For instance, training with Ring All-Reduce [28] works well for identical servers but suffers signiﬁcant performance penalties from network latency or bandwidth variation [29]. Another technique known as Parameter
Server can handle heterogeneous devices at the cost of being less scalable [30]. Applying any of these strategies outside their preferred conditions may signiﬁcantly reduce the training throughput [31], which makes them difﬁcult to apply in the volatile infrastructure of volunteer computing. This issue is further complicated by the unique limitations of volunteer devices, such as network address translation (NAT), regional access restrictions, or variations in performance.
In this study, we carefully analyze the above challenges and come up with a practical solution for
Distributed Deep Learning in Open Collaborations (DeDLOC). DeDLOC is based on a novel algo-rithm that adapts to the available hardware in order to maximize the training throughput. Depending on the infrastructure, DeDLOC can recover parameter servers [30], All-Reduce SGD [32], decen-tralized SGD [33], BytePS [34], or an intermediate strategy that combines all of them. Using this algorithm, we propose a system for collaborative training designed to accommodate a large number of heterogeneous devices with uneven compute, bandwidth, reliability, and network capabilities.
The contributions of our work can be summarized as follows:
• We analyze the unique challenges of distributed training in open collaborations and propose a practical recipe for training in these conditions.
• We formulate a novel distributed training algorithm that interpolates between traditional strategies to directly maximize the training performance for the available hardware.
• We verify the effectiveness of the proposed algorithm and system design for unsupervised pretrain-ing of ALBERT-Large and SwAV under realistic conditions.
• We run collaborative training with actual volunteers, achieving competitive results to models trained on hundreds of data center GPUs. We also report insights on the collaborator activity and share the codebase for running similar experiments in the future2. 2Code and training conﬁgurations are available at github.com/yandex-research/DeDLOC 2
2