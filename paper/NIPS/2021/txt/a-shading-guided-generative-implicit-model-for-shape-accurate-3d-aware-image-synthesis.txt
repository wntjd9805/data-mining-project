Abstract
The advancement of generative radiance ﬁelds has pushed the boundary of 3D-aware image synthesis. Motivated by the observation that a 3D object should look realistic from multiple viewpoints, these methods introduce a multi-view constraint as regularization to learn valid 3D radiance ﬁelds from 2D images. Despite the progress, they often fall short of capturing accurate 3D shapes due to the shape-color ambiguity, limiting their applicability in downstream tasks. In this work, we address this ambiguity by proposing a novel shading-guided generative implicit model that is able to learn a starkly improved shape representation. Our key insight is that an accurate 3D shape should also yield a realistic rendering under different lighting conditions. This multi-lighting constraint is realized by modeling illumina-tion explicitly and performing shading with various lighting conditions. Gradients are derived by feeding the synthesized images to a discriminator. To compensate for the additional computational burden of calculating surface normals, we further devise an efﬁcient volume rendering strategy via surface tracking, reducing the training and inference time by 24% and 48%, respectively. Our experiments on mul-tiple datasets show that the proposed approach achieves photorealistic 3D-aware image synthesis while capturing accurate underlying 3D shapes. We demonstrate improved performance of our approach on 3D shape reconstruction against existing methods, and show its applicability on image relighting. Our code will be released at https://github.com/XingangPan/ShadeGAN. 1

Introduction
Advanced deep generative models, e.g., StyleGAN [1, 2] and BigGAN [3], have achieved great successes in natural image synthesis. While producing impressive results, these 2D representation-based models cannot synthesize novel views of an instance in a 3D-consistent manner. They also fall short of representing an explicit 3D object shape. To overcome such limitations, researchers have proposed new deep generative models that represent 3D scenes as neural radiance ﬁelds [4, 5]. Such 3D-aware generative models allow explicit control of viewpoint while preserving 3D consistency during image synthesis. Perhaps a more fascinating merit is that they have shown the great potential of learning 3D shapes in an unsupervised manner from just a collection of unconstrained 2D images.
If we could train a 3D-aware generative model that learns accurate 3D object shapes, it would broaden various downstream applications such as 3D shape reconstruction and image relighting.
Existing attempts for 3D-aware image synthesis [4, 5] tend to learn coarse 3D shapes that are inaccurate and noisy, as shown in Fig.1 (a). We found that such inaccuracy arises from an inevitable ambiguity inherent in the training strategy adopted by these methods. In particular, a form of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Motivation. (a) Previous methods like pi-GAN [4] resort to the "multi-view constraint", where the 3D representation is projected to different viewpoints as fake images to the discriminator.
The extracted 3D meshes are often inaccurate due to the shape-color ambiguity. (b) The proposed approach ShadeGAN further adopts a "multi-lighting constraint", which motivates the 3D represen-tation to look realistic under different lighting conditions. This constraint effectively addresses the ambiguity, giving rise to more natural and precise 3D shapes. regularization, which we refer to as "multi-view constraint", is used to enforce the 3D representation to look realistic from different viewpoints. The constraint is commonly implemented by ﬁrst projecting the generator’s outputs (e.g., radiance ﬁelds [6]) to randomly sampled viewpoints, and then feeding them to a discriminator as fake images for training. While such a constraint enables these models to synthesize images in a 3D-aware manner, it suffers from the shape-color ambiguity, i.e., small variations of shape could lead to similar RGB images that look equally plausible to the discriminator, as the color of many objects is locally smooth. Consequently, inaccurate shapes are concealed under this constraint.
In this work, we propose a novel shading-guided generative implicit model (ShadeGAN) to address the aforementioned ambiguity. In particular, ShadeGAN learns more accurate 3D shapes by explicitly modeling shading, i.e., the interaction of illumination and shape. We believe that an accurate 3D shape should look realistic not only from different viewpoints, but also under different lighting conditions, i.e., satisfying the "multi-lighting constraint". This idea shares similar intuition with photometric stereo [7], which shows that accurate surface normal could be recovered from images taken under different lighting conditions. Note that the multi-lighting constraint is feasible as real-world images used for training are often taken under various lighting conditions. To fulﬁll this constraint, ShadeGAN takes a relightable color ﬁeld as the intermediate representation, which approximates the albedo but does not necessarily satisfy viewpoint independence. The color ﬁeld is shaded under a randomly sampled lighting condition during rendering. Since image appearance via such a shading process is strongly dependent on surface normals, inaccurate 3D shape representations will be much more clearly revealed than in earlier shading-agnostic generative models. Hence, by satisfying the multi-lighting constraint, ShadeGAN is encouraged to infer more accurate 3D shapes as shown in Fig.1 (b).
The above shading process requires the calculation of the normal direction via back-propagation through the generator, and such calculation needs to be repeated dozens of times for a pixel in volume rendering [4, 5], introducing additional computational overhead. Existing efﬁcient volume rendering techniques [8, 9, 10, 11, 12] mainly target static scenes, and could not be directly applied to generative models due to their dynamic nature. Therefore, to improve the rendering speed of ShadeGAN, we formulate an efﬁcient surface tracking network to estimate the rendered object surface conditioned on the latent code. This enables us to save rendering computations by just querying points near the predicted surface, leading to 24% and 48% reduction of training and inference time without affecting the quality of rendered images.
Comprehensive experiments are conducted across multiple datasets to verify the effectiveness of
ShadeGAN. The results show that our approach is capable of synthesizing photorealistic images while capturing more accurate underlying 3D shapes than previous generative methods. The learned 2
distribution of 3D shapes enables various downstream tasks like 3D shape reconstruction, where our approach signiﬁcantly outperforms other baselines on the BFM dataset [13]. Besides, modeling the shading process enables explicit control over lighting conditions, achieving image relighting effect. Our contributions can be summarized as follows: 1) We address the shape-color ambiguity in existing 3D-aware image synthesis methods with a shading-guided generative model that satisﬁes the proposed multi-lighting constraint. In this way, ShadeGAN is able to learn more accurate 3D shapes for better image synthesis. 2) We devise an efﬁcient rendering technique via surface tracking, which signiﬁcantly saves training and inference time for volume rendering-based generative models. 3)
We show that ShadeGAN learns to disentangle shading and color that well approximates the albedo, achieving natural relighting effects in image synthesis. 2