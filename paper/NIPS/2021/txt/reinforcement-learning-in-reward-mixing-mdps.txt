Abstract
Learning a near optimal policy in a partially observable system remains an elusive challenge in contemporary reinforcement learning.
In this work, we consider episodic reinforcement learning in a reward-mixing Markov decision process (MDP).
There, a reward function is drawn from one of multiple possible reward models at the beginning of every episode, but the identity of the chosen reward model is not revealed to the agent. Hence, the latent state space, for which the dynamics are Markovian, is not given to the agent. We study the problem of learning a near optimal policy for two reward-mixing MDPs. Unlike existing approaches that rely on strong assumptions on the dynamics, we make no assumptions and study the problem in full generality. Indeed, with no further assumptions, even for two switching reward-models, the problem requires several new ideas beyond existing algorithmic and analysis techniques for eﬃcient exploration. We provide the ﬁrst polynomial-time algorithm that ﬁnds an (cid:15)-optimal policy after exploring
˜O(poly(H, (cid:15)−1)·S2A2) episodes, where H is time-horizon and S, A are the number of states and actions respectively. This is the ﬁrst eﬃcient algorithm that does not require any assumptions in partially observed environments where the observation space is smaller than the latent state space. 1

Introduction
In reinforcement learning (RL), an agent solves a sequential decision-making problem in an unknown dynamic environment to maximize the long-term reward [46]. The agent interacts with the environment by receiving feedback on its actions in the form of a state-dependent reward and observation.
One of the key challenges in RL is exploration: in the absence of auto-exploratory properties such as ergodicity of the system, the agent has to devise a clever scheme to collect data from under-explored parts of the system. In a long line of work, eﬃcient exploration in RL has been extensively studied for fully observable environments, i.e., under the framework of Markov decision process (MDP) with a number of polynomial-time algorithms proposed [34, 27, 43, 40, 18]. For instance, in tabular MDPs, i.e., environments with a ﬁnite number of states and actions, we can achieve sample-optimality or minimax regret without any assumptions on system dynamics [3, 50].
In contrast to fully observable environments, very little is known about the exploration in partially observable MDPs (POMDPs). In general, RL in POMDPs may require an exponential number of samples (without simplifying structural assumptions) [35, 31]. Therefore, it is important to consider natural sub-classes of POMDPs which admit tractable solutions. Previous studies focus on a special class of POMDPs where observation spaces are large enough such that a single observation provides 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
suﬃcient information on the underlying state [22, 35, 4, 12, 16, 30]. However, there are many applications in robotics, topic modeling and beyond, where the observations space is smaller than the latent state space. This “small observation space” setting is critical, as most prior work does not apply. Our work seeks to provide one of the ﬁrst results to tackle a class of problems in this space.
In particular, we tackle a sub-class of POMDPs inspired by latent variable or mixture problems.
Speciﬁcally, we consider reward-mixing MDPs (RM-MDPs). In RM-MDPs, the transition kernel and initial state distribution are deﬁned as in standard MDPs, while the reward function is randomly chosen from one of M -reward models at the beginning of every episode (see the formal deﬁnition 1). RM-MDPs are important in their own right. Consider, for instance, a user-interaction model in a dynamical web system, where reward models vary across users with diﬀerent characteristics [38, 23].
A similar setting has been considered in a number of recent papers [8, 6, 23, 45, 7], and most recently, and most directly related to our setting, [36].
Even for M = 2, the RM-MDP setting shares some of the key challenges of general POMDPs. This is because the best policy for the RM-MDP problem should account for not just the current state, but also an entire sequence of previous observations since some previous events might be correlated with a reward of the current action. And in the small observation space setting, prior work as in
[22, 35, 4, 12, 16, 30] cannot be applied. The work in [36] develops an algorithmic framework for solving the RM-MDP problem in this small observation setting, but requires strong assumptions without which the algorithm falls apart. We discuss this in more detail below. Indeed, no work to date, has been able to address the RM-MDP problem, even for M = 2, without signiﬁcant further assumptions. This is precisely the problem we tackle in this paper.
Main Results and Contributions We focus on the problem of learning near-optimal policies in two reward-mixing MDPs, i.e., RM-MDPs with two reward models M = 2. To the best of our knowledge, no prior work has studied sample complexity of RL in RM-MDPs even for M = 2 without any assumptions on system dynamics. Speciﬁcally, we provide the ﬁrst polynomial-time algorithm which learns an (cid:15)-optimal policy after exploring ˜O(poly(H, (cid:15)−1) · S2A2) episodes. We also show that Ω(S2A2/(cid:15)2) number of episodes is necessary, and thus our result is tight in S, A. This is the ﬁrst eﬃcient algorithm in a sub-class of partially observable domains with small observation spaces and without any assumptions on system dynamics.
On the technical side, we must overcome several new challenges that are not present in fully observable settings. One key hurdle relates to identiﬁability. In standard MDPs, an average of single observations from a single state-action pair is suﬃcient to get unbiased estimators of transition and reward models for the state-action. However, in RM-MDPs, the same quantity would only give an averaged reward model, where the average is taken over the reward model distribution. However, an average reward model alone is not suﬃcient to obtain the optimal policy which depends on a sequence of previous rewards.
Our technique appeals to the idea of uncertainty in higher-order moments, bringing inspiration from algorithms for learning a mixture of structured distributions [19, 9, 26, 15]. In such problems, the key for eﬃcient learning is to leverage information from higher-order moments. Following this spirit, we consider estimating correlations of rewards at multiple diﬀerent state-actions. A central challenge we must overcome is that in ﬁnite-horizon MDPs, it may be not possible to estimate all higher-order correlations with uniformly good accuracy. In fact, it may not be possible to estimate some correlations at all when some pairs of state-actions are not reachable in the same episode. Therefore, we cannot simply rely on existing techniques that require good estimations of all elements in higher-order moment matrices. The main technical challenge is, therefore, to show that a near-optimal policy can still be obtained from uncertain and partial correlations of state-actions. Our technical contributions are thus two-fold: (1) design of an eﬃcient exploration algorithm to estimate each correlation up to some required accuracy, and (2) new technical tools to analyze errors from diﬀerent levels of uncertainties in estimated higher-order correlations.