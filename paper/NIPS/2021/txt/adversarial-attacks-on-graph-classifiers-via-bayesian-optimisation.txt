Abstract
Graph neural networks, a popular class of models effective in a wide range of graph-based learning tasks, have been shown to be vulnerable to adversarial attacks.
While the majority of the literature focuses on such vulnerability in node-level classiﬁcation tasks, little effort has been dedicated to analysing adversarial attacks on graph-level classiﬁcation, an important problem with numerous real-life applica-tions such as biochemistry and social network analysis. The few existing methods often require unrealistic setups, such as access to internal information of the victim models, or an impractically-large number of queries. We present a novel Bayesian optimisation-based attack method for graph classiﬁcation models. Our method is black-box, query-efﬁcient and parsimonious with respect to the perturbation applied.
We empirically validate the effectiveness and ﬂexibility of the proposed method on a wide range of graph classiﬁcation tasks involving varying graph properties, constraints and modes of attack. Finally, we analyse common interpretable patterns behind the adversarial samples produced, which may shed further light on the adver-sarial robustness of graph classiﬁcation models. An open-source implementation is available at https://github.com/xingchenwan/grabnel. 1

Introduction
Graphs are a general-purpose data structure consisting of entities represented by nodes and edges which encode pairwise relationships. Graph-based machine learning models has been widely used in a variety of important applications such as semi-supervised learning, link prediction, community detection and graph classiﬁcation [3, 51, 14]. Despite the growing interest in graph-based machine learning, it has been shown that, like many other machine learning models, graph-based models are vulnerable to adversarial attacks [33, 17]. If we want to deploy such models in environments where the risk and costs associated with a model failure are high e.g. in social networks, it would be crucial to understand and assess the model stability and vulnerability by simulating adversarial attacks.
Adversarial attacks on graphs can be aimed at different learning tasks. This paper focuses on graph-level classiﬁcation, where given an input graph (potentially with node and edge attributes), we wish to learn a function that predicts a property of interest related to the graph. Graph classiﬁcation is an important task with many real-life applications, especially in bioinformatics and chemistry
[24, 25]. For example, the task may be to accurately classify if a molecule, modelled as a graph whereby nodes represent atoms and edges model bonds, inhibits HIV replication or not. Although there are a few attempts on performing adversarial attacks on graph classiﬁcation [10, 23], they all operate under unrealistic assumptions such as the need to query the target model a large number of times or access a portion of the test set to train the attacking agent. To address these limitations, we formulate the adversarial attack on graph classiﬁcation as a black-box optimisation problem and solve it with Bayesian optimisation (BO), a query-efﬁcient state-of-the-art zeroth-order black-box optimiser. Unlike existing work, our method is query-efﬁcient, parsimonious in perturbations and 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
Figure 1: The overall pipeline of GRABNEL. The key components are explained in different paragraphs of Sec 2: Surrogate model describes the construction of the BO surrogate and the feature extractor (Block 1), Sequential perturbation selection describes how base graphs and perturbed graphs as candidates of adversarial attack are selected (Block 2), and Optimisation of acquisition function describes how new query points are generated by BO via optimising acquisition (Block 3). A detailed algorithmic description for GRABNEL is also available in App. A. does not require policy training on a separate labelled dataset to effectively attack a new sample.
Another beneﬁt of our method is that it can be easily adapted to perform various modes of attacks such as deleting or rewiring edges and node injection. Furthermore, we investigate the topological properties of the successful adversarial examples found by our method and offer valuable insights on the connection between the graph topology change and the model robustness.
The main contributions of our paper are as follows. First, we introduce a novel black-box attack for graph classiﬁcation, GRABNEL1, which is both query efﬁcient and parsimonious. We believe this is the ﬁrst work on using BO for adversarial attacks on graph data. Second, we analyse the generated adversarial examples to link the vulnerability of graph-based machine learning models to the topological properties of the perturbed graph, an important step towards interpretable adversarial examples that has been overlooked by the majority of the literature. Finally, we evaluate our method on a range of real-world datasets and scenarios including detecting the spread of fake news on Twitter, which to the best of our knowledge is the ﬁrst analysis of this kind in the literature. 2 Proposed Method: GRABNEL
Problem Setup A graph G = (V, E) is deﬁned by a set of nodes V = {vi}n i=1 and edges E =
{ei}m i=1 where each edge ek = {vi, vj} connects between nodes vi and vj. The overall topology can be represented by the adjacency matrix A ∈ {0, 1}n×n where Aij = 1 if the edge {vi, vj} is present2. The attack objective in our case is to degrade the predictive performance of the trained victim graph classiﬁer fθ by ﬁnding a graph G0 perturbed from the original test graph G (ideally with the minimum amount of perturbation) such that fθ produces an incorrect class label for G.
In this paper, we consider the black-box evasion attack setting, where the adversary agent cannot access/modify the the victim model fθ (i.e. network architecture, weights θ or gradients) or its training data {(Gi, yi)}L i=1; the adversary can only interact with fθ by querying it with an input graph
G0 and observe the model output fθ(G0) as pseudo-probabilities over all classes in a C-dimensional standard simplex. Additionally, we assume that sample efﬁciency is highly valued: we aim to ﬁnd adversarial examples with the minimum number of queries to the victim model. We believe that this is a practical and difﬁcult setup that accounts for the prohibitive monetary, logistic and/or opportunity costs of repeatedly querying a (possibly huge and complicated) real-life victim model. With a high query count, the attacker may also run a higher risk of getting detected. Formally, the objective function of our BO attack agent can be formulated as a black-box maximisation problem: max
G0∈Ψ(G)
Lattack (cid:0)fθ(G0), y(cid:1) s.t. y = arg max fθ(G) (1) 1Stands for Graph Adversarial attack via BayesiaN Efﬁcient Loss-minimisation. 2We discuss the unweighted graphs for simplicity; our method may also handle other graph types. 2
where fθ is the pretrained victim model that remains ﬁxed in the evasion attack setup and y is the correct label of the original input G. Denote the output logit for the class y as fθ(G)y, the attack loss
Lattack can be deﬁned as:
Lattack (cid:16) fθ(G0), y (cid:17)
= (cid:26)maxt∈Y,t6=y log fθ(G0)t − log fθ(G0)y log fθ(G0)t − log fθ(G0)y (untargeted attack) (targeted attack on class t), (2) where fθ(·)t denotes the logit output for class t. Such an attack loss deﬁnition is commonly used both in the traditional image attack and the graph attack literature [4, 52] although our method is compatible with any choice of loss function. Furthermore, Ψ(G) refers to the set of possible G0 generated from perturbing G. In this work, we experiment with a diverse modes of attacks to show that our attack method can be generalised to different set-ups:
• creating/removing an edge: we create perturbed graphs by ﬂipping the connection of a small set of node pairs δA = {{ui, vi}}∆ i=1 of G following previous works [52, 10];
• rewiring or swapping edges: similar to [23], we select a triplet (u, v, s) where we either rewire the edge (u → v) to (u → s) (rewire), or exchange the edge weights w(u, v) and w(u, s) (swap);
• node injection: we create new nodes together with their attributes and connections in the graph.
The overall routine of our proposed GRABNEL is presented in Fig 1 (and in pseudo-code form in App
A), and we now elaborate each of its key components.
Surrogate model The success of BO hinges upon the surrogate model choice. Speciﬁcally, such a surrogate model needs to 1) be ﬂexible and expressive enough to locally learn the latent mapping from a perturbed graph G0 to its attack loss Lattack(fθ(G0), y) (note that this is different and generally easier than learning G0 → y, which is the goal of the classiﬁer fθ), 2) admit a probabilistic interpretation of uncertainty – this is key for the exploration-exploitation trade-off in BO, yet also 3) be simple enough such that the said mapping can be learned with a small number of queries to fθ to preserve sample efﬁciency. Furthermore, given the combinatorial nature of the graph search space, it also needs to 4) be capable of scaling to large graphs (e.g. in the order of 103 nodes or more) typical of common graph classiﬁcation tasks with reasonable run-time efﬁciency. Additionally, given the fact that BO has been predominantly studied in the continuous domain which is signiﬁcantly different from the present setup, the design of a appropriate surrogate is highly non-trivial. To handle this set of conﬂicting desiderata, we propose to ﬁrst use a Weisfeiler-Lehman (WL) feature extractor to extracts a vector space representation of G, followed by a sparse Bayesian linear regression which balances performance with efﬁciency and gives an probabilistic output.
With reference to Fig. 1, given a perturbation graph G0 as a proposed adversarial sample, the WL feature extractor ﬁrst extracts a vector representation φ(G0) in line with the WL subtree kernel procedure (but without the ﬁnal kernel computation) [30]. For the case where the node features are discrete, let x0(v) be the initial node feature of node v ∈ V (note that the node features can be either scalars or vectors) , we iteratively aggregate and hash the features of v with its neighbours,
{ui}deg(v) i=1
, using the original WL procedure at all nodes to transform them into discrete labels: xh+1(v) = hash (cid:16) xh(v), xh(u1), ..., xh(udeg(v)) (cid:17)
, ∀h ∈ {0, 1, . . . , H − 1}, (3) where H is the total number of WL iterations, a hyperparameter of the procedure. At each level h, we compute the feature vector φh(G0) = [c(G0, Xh1), ..., c(G0, Xh|Xh|)]>, where Xh is the set of distinct node features xh that occur in all input graphs at the current level and c(G0, xh) is the counting function that counts the number of times a particular node feature xh appears in G0. For the case with continuous node features and/or weighted edges, we instead use the modiﬁed WL procedure proprosed in [36]: xh+1(v) = (cid:16) 1 2 xh(v) + 1 deg(v) deg(v)
X i=1 w(v, ui)xh(ui) (cid:17)
, ∀h ∈ {0, 1, . . . , H − 1}, (4) where w(v, ui) denotes the (non-negative) weight of edge e{v,ui} (1 if the graph is unweighted) and we simply have feature at level h φh(G0) = vec(Xh), where Xh is the feature matrix of graph G0 at level h by collecting the features at each node Xh = and vec(·) denotes the vectorisation operator. In both cases, at the end of H WL iterations we obtain the ﬁnal feature vector
φ(G0) = concat(cid:0)φ1(G0), ..., φH (G0)(cid:1) for each training graph in [1, nG0] to form the feature matrix i xh(1), ...xh(v) h 3
Figure 2: Sequential edge selection. At each stage, the BO agent sequentially proposes candidate graphs with edge edit distance of 1 from the base graph G0(i) (which is the original unperturbed graph
G at initialisation, or a perturbed graph that led to the largest increase in loss from the previous stage otherwise) by selecting the graph that maximises the acquisition function value amongst all candidates generated via sampling/genetic algorithm (see details at Optimisation of acquisition function). This procedure repeats until either the attack succeeds (i.e. we ﬁnd a graph G0 with Lattack(fθ(G0), y) > 0) or the maximum number of B queries to fθ is exhausted. 0 1), ...φ(G0
|nG0 |)]> ∈ R|nG0 |×D to be passed to the Bayesian regressor – it is particularly
Φ = [φ(G0 worth noting that the training graphs here denote inputs to train the surrogate model of the attack agent and are typically perturbed versions of a test graph G of the victim model; they are not the graphs that are used to train the victim model itself: in an evasion attack setup, the model is considered frozen and the training inputs cannot be accessed by the attack agent any point in the pipeline. The WL iterations capture both information related to individual nodes and topological information (via neighbourhood aggregation), and have been shown to have comparable distinguishing power to some Graph Neural
Network (GNN) models [26], and hence the procedure is expressive. Alternative surrogate choices could be, for example, GNNs with the ﬁnal fully-connected layer replaced by a probabilistic linear regression layer such as the one proposed in [31]. However, in contrast to these, our extraction process G0 → φ(G0) requires no learning from data (we only need to learn the Bayesian linear regression weights) and therefore should lead to better sample efﬁciency. Alternatively, we may also use a Gaussian Process (GP) surrogate, such as the Gaussian Process with Weisfeiler-Lehman Kernel (GPWL) model proposed in [29] that directly uses a GP model together with a WL kernel. Nonetheless, while GPs are theoretically more expressive (although we empirically show in App. D.1 that in most of the cases their predictive performances are comparable), they are also much more expensive with a cubic scaling w.r.t the number of training inputs. Furthermore, GPWL is designed speciﬁcally for neural architecture search, which features small, directed graphs with discrete node features only; on the other hand, the GRABNEL surrogate covers a much wider scope of applications
When we select a large H or if there are many training inputs and/or input graph(s) have a large number of nodes/edges, there will likely be many unique WL features and the resulting feature matrix will be very high-dimensional, which would lead to high-variance regression coefﬁcients α being estimated if nG0 (number of graphs to train the surrogate of the attack agent) is comparatively few. To attain a good predictive performance in such a case, we employ Bayesian regression surrogate with the Automatic Relevance Determination (ARD) prior to learn the mapping Φ → Lattack(fθ(G0), y), which regularises weights and encourages sparsity in α [42]:
Lattack|Φ, α, σ2 n ∼ N (α>Φ, σ2 nI),
α|λ ∼ N (0, Λ), diag(Λ) = λ−1 = {λ−1 1 , ..., λ−1
D },
λi ∼ Gamma(k, θ) ∀i ∈ [1, D], (5) (6) (7) where Λ is a diagonal covariance matrix. To estimate α and noise variance σ2 n, we optimise the model marginal log-likelihood. Overall, the WL routines scales as O(Hm) and Bayesian linear regression has a linear runtime scaling w.r.t. the number of queries; these ensure the surrogate is scalable to both larger graphs and/or a large number of graphs, both of which are commonly encountered in graph classiﬁcation (See App D.6 for a detailed empirical runtime analysis).
Sequential perturbation selection In the default structural perturbation setting, given an attack budget of ∆ (i.e. we are allowed to ﬂip up to ∆ edges from G), ﬁnding exactly the set of perturbations 4
(cid:1)
δA that leads to the largest increase in Lattack entails an combinatorial optimisation over (cid:0)n2 candidates. This is a huge search space that is difﬁcult for the surrogate to learn meaningful patterns in a sample-efﬁcient way even for modestly-sized graphs. To tackle this challenge, we adopt the strategy illustrated in Fig. 2: given the query budget B (i.e. the total number of times we are allowed to query fθ for a given G), we assume B ≥ ∆ and amortise B into ∆ stages and focus on selecting one edge perturbation at each stage. While this strategy is greedy in the sense that it always commits the perturbation leading to the largest increase in loss at each stage, it is worth noting that we do not treat the previously modiﬁed edges differently, and the agent can, and does occasionally as we observe empirically, “correct” previous modiﬁcations by ﬂipping edges back: this is possible as the effect of edge selection is permutation invariant. Another beneﬁt of this strategy is that it can potentially make full use of the entire attack budget ∆ while remaining parsimonious w.r.t. the amount of perturbation introduced, as it only progresses to the next stage and modiﬁes the G further when it fails to ﬁnd a successful adversarial example in the current stage.
∆
Optimisation of acquisition function At each BO iteration, acquisition function α(·) is optimised to select the next point(s) to query the victim model fθ. However, commonly used gradient-based optimisers cannot be used on the discrete graph search space; a naïve strategy would be to randomly generate many perturbed graphs, evaluate α on all of them, and choose the maximiser(s) to query fθ next. While potentially effective on modestly-sized G especially with our sequential selection strategy, this strategy nevertheless discards any known information about the search space.
Inspired by recent advances in BO in non-continuous domains [8, 38], we optimise α via an adapted version of the Genetic algorithm (GA) in [10], which is well-suited for our purpose but is not particularly sample efﬁcient since many evolution cycles could be required for convergence. However, the latter is not a serious issue here as we only use GA for acquisition optimisation where we only query the surrogate instead of the victim model, a subroutine of BO that does not require sample efﬁciency. We outline its ingredients below:
• Initialisation: While GA typically starts with random sampling in the search space to ﬁll the initial population, in our case we are not totally ignorant about the search space as we could have already queried and observed fθ with a few different perturbed graphs G0. A smoothness assumption on the search space would be that if a G0 with an edge (u, v) ﬂipped from G led to a large Lattack, then another G0 with (u, s), s 6∈ {u, v} ﬂipped is more likely to do so too. To reﬂect this, we ﬁll the initial population by mutating the top-k queried G0s leading to the largest Lattack seen so far in the current stage, where for G0 with (u, v) ﬂipped from the base graph we 1) randomly choose an end node (u or v) and 2) change that node to another node in the graph except u or v such that the perturbed edges in all children shares one common end node with the parent.
• Evolution: After the initial population is built, we follow the standard evolution routine by evaluating the acquisition function value for each member as its ﬁtness, selecting the top-k performing members as the breeding population and repeating the mutation procedure in initialisation for a ﬁxed number of rounds. At termination, we simply query fθ with the graph(s) seen so far (i.e. computing the loss in Fig. 2) with the largest acquisition function value(s) seen during GA. 3