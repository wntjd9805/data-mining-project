Abstract
Gaussian Cox processes are widely-used point process models that use a Gaussian process to describe the Bayesian a priori uncertainty present in latent intensity functions. In this paper, we propose a novel Bayesian inference scheme for Gaus-sian Cox processes by exploiting a conceptually-intuitive path integral formula-tion. The proposed scheme does not rely on domain discretization, scales linearly with the number of observed events, has a lower complexity than the state-of-the-art variational Bayesian schemes with respect to the number of inducing points, and is applicable to a wide range of Gaussian Cox processes with various types of link functions. Our scheme is especially beneﬁcial under the multi-dimensional input setting, where the number of inducing points tends to be large. We evaluate our scheme on synthetic and real-world data, and show that it achieves comparable predictive accuracy while being tens of times faster than reference methods. 1

Introduction
Gaussian Cox processes constitute a class of doubly-stochastic point process models in which a
ﬂexible prior over a latent intensity function is established by a Gaussian process through a positive link function under which point events are generated. Gaussian Cox processes are the gold stan-dard in analyzing event data in a Bayesian manner, and have a wide ranging list of applications in neuroscience [10], ﬁnance [5], and spatio-temporal analysis [30].
The observation process, i.e., the likelihood function of the Gaussian Cox process, depends on the functional form of the latent intensity function over a compact domain, which makes Bayesian inference challenging as it imposes the intractable computation of inﬁnite-dimensional distribu-tions. Many algorithms have been proposed to deal with this difﬁculty. They include Markov Chain
Monte Carlo (MCMC) sampling, domain discretization, and variational Bayesian (VB) approxima-tion. MCMC approaches [2, 22] provide exact inferencing for a speciﬁc link function, but they have excessive computation costs. Although domain discretization [10, 43] achieves sub-linear computa-tional scaling with discretization size, it suffers from poor scaling with data size and the dimension of the domain. To overcome these limitations, state-of-the-art algorithms employ sparse VB ap-proximation with inducing points [3, 31]; they achieve improved scaling with the dimension of the domain as well as computation cost that is linear to data size. However, the computation costs of VB approaches scale at the rate of O(N L2 + L3) where L and N are the number of inducing points and data points, respectively, making them problematic in the multi-dimensional domain setting, where
L tends to be large. Also, VB approaches achieve good approximations by aggressively focusing on the unique structures of individual kernel functions or link functions, and thus are not applicable to a wide range of Gaussian Cox processes.
In this paper, we introduce a conceptually-intuitive path integral formulation of Gaussian processes, by which we derive a novel inference scheme for tackling intractable Gaussian Cox processes. The 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
formulation predicts that the maximum a posteriori (MAP) estimation of a latent intensity function coincides with the problem of ﬁnding the most likely path that maximizes the functional posterior, whose solution can be obtained by solving an explicit integral equation. The approximate predictive covariance is also acquired by solving the corresponding integral equation. We propose an efﬁcient algorithm to solve the integral equations; it offers O(N L + L2) computation scaling, that is, lower complexity than the state-of-the-art variational Bayesian schemes with respect to the number of inducing points, as well as linear computation scaling with data size. Furthermore, our inference scheme via path integral holds for any kernel function and a wide range of link functions, and thus provides a practical methodology against Gaussian Cox processes that conventional approaches have yet to tackle successfully.
In Section 2, we introduce the path integral formulation of Gaussian processes, and construct a new Bayesian inference scheme for Gaussian Cox processes. In Section 3, we provide a practical methodology to perform inferencing. In Section 4, we outline related work. In Section 5, we perform comparative evaluations against reference methods on synthetic and real-world data, and conﬁrm that our scheme achieves comparable predictive accuracy with substantial speed improvements over state-of-the-art algorithms. Finally, Section 6 states our conclusions. 2 Path Integral Formulation 2.1 Gaussian Cox Process
We assume that a latent function on a compact space, T ⊂ RD, x(t) : T → R, is generated from a
Gaussian Process (GP), denoted by GP(x(t)|µ, k(t, t′)), and observations are generated from a pro-cess under latent function x(t), where µ and k are the mean value and the kernel/covariance function for x(t), respectively. Given data set D observed within T , we consider the problem of obtaining the maximum a posteriori (MAP) estimator of x(t) that maximizes the posterior probability p(x(t)|D) =
R p(D|x(t)) GP(x(t)|µ, k)
Dx(t) p(D|x(t)) GP(x(t)|µ, k)
R
, (1)
Dx(t) in the denominator represents the integral where p(D|x(t)) is the likelihood function, and over the function or the inﬁnite-dimensional variable x(t).
The GP prior, GP(x(t)|µ, k), can be regarded as a normal distribution for an inﬁnite-dimensional variable (see Section 2.2), and thus if the likelihood function depends only on a set of latent functions on ﬁnite N training points, p(D|x(t)) = p(D|{x(tn)}N n=1), then MAP estimation for an inﬁnite-dimensional variable reduces to that for a ﬁnite N -dimensional variable by marginalizing out the variables on the points other than {x(tn)}N n=1; this inference is tractable and has been examined extensively (e.g. GP regression, GP classiﬁcation). Here, we focus on a more challenging situation where the likelihood function includes a factor that depends on an integral of a latent function over
T as follows, log p(D|x(t)) =
NX n=1
Z log λ(tn) −
T (cid:0)
λ(t)dt, λ(t) = κ (cid:1) x(t)
, (2) where D = {tn}N n=1 is the set of point events occurring in the observation region T , κ(x) : R →
R+ is a non-negative function called the link function, and λ(t) is the intensity function or the instantaneous probability of events occurring at each point in T . The probabilistic model deﬁned by (1-2) is called the Gaussian Cox process.
Bayesian inference (1) on situation (2) is often referred as “doubly-intractable", because it requires solving the integral of the stochastic function x(t) as well as computing the intractable posterior probability of an inﬁnite-dimensional variable. Below, we address the problem directly by introduc-ing path integral, a mathematical tool familiar in the ﬁeld of quantum physics, to treat probability distributions of stochastic functions properly and intuitively. 2.2 Gaussian Process Prior via Path Integral
In order to derive an explicit description of GP prior, we ﬁrst review a well-known formulation of
GP [7], and reconstruct its explicit form of probability density distribution by using a path integral representation. 2
Consider a model deﬁned in terms of a linear combination of M ﬁxed basis functions, {ϕm(t)}M such that m=1,
MX x(t) = µ + m=1
αmϕm(t) = µ + α⊤ϕ(t), t ∈ T , (3) where α ≜ (α1, · · · , αM )⊤ is the M -dimensional weight vector, ϕ(t) ≜ (ϕ1(t), · · · , ϕM (t))⊤, and µ is a mean parameter. If α is generated from an M -dimensional Gaussian with a diagonal covariance matrix of the form p(α) = N (α|0, c−1IM ), (4) the joint probability distribution for any ﬁnite (even inﬁnite) set of variables, x ≜ (x(t1), . . . , x(tJ ))⊤ for J ≤ ∞, follows the J-dimensional Gaussian given by p(x)dx = N (x|µ1, K)dx, Kjj′ = k(tj, tj′) ≜ c−1ϕ(tj)⊤ϕ(tj′), (5) where k(t, t′) is a semi-deﬁnite kernel function, c is the precision parameter, 1 ≤ j, j′ ≤ J, and 1 ≜ (1, . . . , 1)⊤. Because (5) holds for arbitrary numbers of dimensions, J, it provides an implicit representation of GP prior.
Next, based on the expression (5), we derive a more explicit representation of GP prior. For sim-plicity of explanation, we assume that the domain T is one-dimensional, t = t and T = [0, T ], and mean µ is zero. Let K be the integral operator corresponding to k(t, t′), and K∗ be the inverse operator of K,
Z
Z
K(∗)x(t) ≜ k(∗)(t, t′)x(t′)dt′,
K∗k(t, s) = k∗(t, t′)k(t′, s)dt′ = δ(t − s),
T
T (6) where δ(·) represents the Dirac delta function. Note that K∗ and k∗(t, t′) should be described by a differential operator because relation (6) indicates that k(t, s) is the Green function [15] for the operator K∗, but we do not go into detail here. Under the ﬁnite scenario (5) with [t0, tJ ] = [0, T ] and J ≫ 1, the second equation in (6) can be approximated by K∗∆K = ∆−1, where ∆jj′ = jj′ = k∗(tj, tj′), and δjj′ is the Kronecker delta. Then we can write down the (tj − tj−1)δjj′, K∗
J-dimensional Gaussian (5) by using K∗ as
|∆K∗∆| (2π)J i (∆x)⊤K∗(∆x) h exp p(x)dx =
− 1 2 dx. (7) s
By taking the limit of the division number J → ∞ (∆ → 0) for the ﬁnite-dimensional Gaus-sian measure (7), we achieve the following path integral representation of GP prior over the latent function x(t):
GP(x(t)|0, k)Dx ≜ lim
J→∞ p(x)
JY j=1 dx(tj) = where |K| ≜ limJ→∞ |K∆|, Dx ≜ limJ→∞ path integral holds,
Z
Z Z s 1
|K| exp p
Q
J j=1
Z Z h
− 1 2 i k∗(t, s)x(t)x(s)dtds
T ×T
Dx, (8) (tj −tj−1)/(2π)dx(tj), and the following h
− 1 2 exp
T ×T i k∗(t, s)x(t)x(s)dtds
Dx = p
|K|. (9)
|K| represents the Fredholm determinant or functional determinant [19] of the integral operator K, which is deﬁned by the product of its eigenvalues. See the supplementary material (§1) for the derivation. The expression for GP(x(t)|µ, k) is easily recovered by x(t) → x(t) − µ and t → t in (8).
The path integral representation (8-9) has a clear advantage over the standard one (5) in that the distribution of the latent function x(t) is written in terms of the explicit integral of x(t) over the domain T , as with the likelihood function (2), which makes it possible to apply functional analyses, calculus of variation, to the Bayesian estimation (1). To the best of our knowledge, this is the ﬁrst proposal of a path integral representation for GP prior in the machine learning community.
It should be noted that our derivation of the path integral representation is based on an intuitive view reminiscent of Feynman’s path integral formulation of the quantum ﬁeld theory [17], and thus is not mathematically rigorous. In this paper, we conﬁrm that our path integral expression is valid by using experiments rather than taking a more mathematically rigorous approach; we discuss this brieﬂy in
Section 4. 3
2.3 Maximum A Posteriori Estimation
Using the representation (8) and the likelihood function (2), we can rewrite the posterior (1) as the functional, h (cid:0) (cid:1)i p(x(t)|D)Dx = exp
−S x(t), x(t)
Dx, (10) 1 p(D) (cid:0) where S (cid:1) x(t), x(t)
Z (cid:0)
S (cid:1) x(t), x(t)
≜
T
R is an action integral, deﬁned by (cid:20) (cid:0) (x(t) − µ)x(t) + κ (cid:1) x(t)
− 1 2 (cid:0) log κ (cid:1) x(t)
NX n=1 (cid:21)
δ(t−tn) dt + 1 2 log |K|, (11)
T k∗(t, t′)(x(t′) − µ)dt′ = K∗(x − µ). (10) indicates that MAP estimation of x(t) and x(t) ≜ is equivalent to the problem of ﬁnding the most likely function or path x(t) that minimizes the action integral. Thus we now apply calculus of variations to the action integral, where the functional derivative of S(x
δ ˆx(t) δ ˆx(t) = 0.
This leads us to realize the following equation for deriving the MAP estimator ˆx(t), on the MAP estimator ˆx
δ ˆx(t) δ ˆx(t)+ δS t) is equal to zero: t), x(t)
δS (cid:0) (cid:1) (cid:0)
ˆx(t) +
Z
T (cid:0) k(t, t′) ˙κ (cid:1)
ˆx(t′) dt′ = µ +
NX n=1 k(t, tn) (cid:0)
˙κ (cid:0)
κ (cid:1)
ˆx(tn) (cid:1) ,
ˆx(tn) t ∈ T , (12) where ˙κ(x) ≜ dκ(x)/dx. See the supplementary material (§2) for the detailed derivations of (12). 2.4 Predictive Covariance
One of the advantages of GP models over non-Bayesian approaches is that they can provide predic-tive distributions. We apply a Laplace approximation to GP models (10), and ﬁnd the approximate form of the predictive covariance. (cid:0)
We now know the mode of the posterior, ˆx(t), and consider a Taylor expansion of functional action potential S (cid:0) centered on the mode such that (cid:1)
Z Z (cid:0) (cid:1)
≃ S
ˆx(t), ˆx(t)
+
σ∗(t, s)(x(t)− ˆx(t))(x(s)− ˆx(s))dtds, (13) x(t), x(t) (cid:1) x(t), x(t)
S 1 2
T×T (cid:12) (cid:12)
δx(t)δx(s) where σ∗(t, s) ≜ δ2S(x,x) x=ˆx is the second derivative of S. The ﬁrst term in the Taylor expansion vanishes due to the stationary condition. The quadratic approximation of the action integral corre-sponds to the approximation of the posterior process by a GP, and the predictive covariance or the kernel function for the posterior GP, denoted by σ(t, s), can be obtained by the functional inversion of σ∗(t, s) (see Equation (8)), which results in
σ(t, s) = h(t, s) − h(t)⊤(Z + H)−1h(s),
κ2(x)
˙κ2(x)−κ(x)¨κ(x) x=ˆx(tn), ¨κ(x) ≜ d2κ(x) (cid:12) (cid:12)
, Hnn′ ≜ h(tn, tn′), h(t) ≜ where Znn′ ≜ δnn′ (h(t, t1),. . . ,h(t, tN ))⊤, and h(t, s) is deﬁned by the following Fredholm integral equation of the second kind [36], dx2
Z (14) (cid:0) k(t, t′)¨κ (cid:1)
ˆx(t′) h(t, s) +
T h(t′, s)dt′ = k(t, s). (15)
Note that Znn′ has an inﬁnite value for k(x) = ex, which leads to the relation, σ(t, s) = h(t, s).
The full derivation of (14-15) is given in the supplementary material (§3). 2.5 Marginal Likelihood
·σ(t, t′)dt′, H ≜
Let Σ and H be the integral operators for σ(t, s) and h(t, s),
R respectively: Σ ≜
·h(t, t′)dt′. Under Laplace approximation (13), we can obtain the
T marginal likelihood or evidence of Gaussian Cox processes, p(D), by performing the following path integral (9),
R
T
Z log p(D) = log (cid:2)
−S exp (cid:0) (cid:1)(cid:3) x(t), x(t)
Dx ≃ −S (cid:0) (cid:1)
ˆx(t), ˆx(t)
+ 1 2 log |Σ |, (16) 4
Table 1: Link functions κ(x) and their derivation. quadratic exponential softplus
κ(x) x2 exp(x) log(1+exp(x))
˙κ(x)
¨κ(x)
γ( ˙κ) = ˙κ/κ 2x exp(x) (1+exp(−x))−1 2 exp(x) 4/ ˙κ 1 exp(−x)/(1−exp(−x))2 − log(1− ˙κ) where log |Σ | = log |H| − log |IN + Z−1H|, and (cid:1)
NX (cid:20) (cid:0)
S
ˆx(t), ˆx(t)
= log |K| + 1 2
Z (cid:0) (cid:1)
+
κ
ˆx(t) (cid:0)
˙κ (cid:0)
κ (ˆx(tn)−µ)
Z
ˆx(tn)
ˆx(tn) (cid:1) (cid:0) (ˆx(t)−µ) ˙κ
ˆx(t) n=1 1 2 dt − 1 2
T
The full derivation is given in the supplementary material (§4).
T (cid:1) (cid:0) (cid:1) − log κ (cid:21) (cid:1)
ˆx(tn) dt. (17) 3 How to Solve Integral Equations
To obtain the MAP estimator ˆx(t) and the predictive covariance σ(t, s), we need to solve the corre-sponding integral equations. We provide efﬁcient algorithms to solve them (also see Suppl. (§8)). 3.1 MAP Estimator
Equation (12) for the MAP estimator is a nonlinear integral equation of the second kind, sometimes called the Hammerstein integral equation [23]. We can obtain an approximation solution of the equation through the most widely used class of methods, projection methods [4], which approximate the solution or its derivation by choosing an approximation from a given ﬁnite L-dimensional linear subspace of functions, denoted by Z. We employ the collocation method [29], a variant of the projection method: (cid:0)
˙κ (cid:1)
ˆx(t)
LX
LX
ˆβlφl(t),
{ ˆβl}L l=1 = arg min
≃ (cid:18)X r(t) ≜ ˙κ l=1 hX i
βlφl(tn) k(t, tn)γ n l
−
X
Z
βl
T l
{βl}L l=1 l=1
[r(pl)]2, (cid:19) k(t, t′)φl(t′)dt′ + µ
− (18)
X l
βlφl(t), l=1 is a basis for Z, {pl ∈ T }L where r(t) is the residual derived from (12), {φl}L l=1 are the collocation points, and γ( ˙κ) represents ˙κ(x)/κ(x) being re-written as a function of ˙κ(x) (see Table 1). As the l=1, we adopt the eigenfunctions of the kernel operator K, which reduces the problematic basis {φl}L l=1 is integral term in the residual into a tractable one as the set of the eigenvalues of K. The procedure for ﬁnding the eigenfunctions is provided at the end of this section. For fair comparison with VB-based approaches of Gaussian Cox processes [3, 31], we solve the minimization problem of the sum of the squared residuals by using a popular gradient descent algorithm, Adam [27]. We can obtain the MAP estimator ˆx(t) by substituting (18) into (12): l λl
ˆx(t) = µ +
R
T k(t, t′)φl(t′)dt′ = λlφl(t), where {λl}L (cid:1)
ˆβlφl(tn) n k(t, tn)γ
ˆβlφl(t). (cid:0)P l
P
P
−
The sparsely located collocation points, {pl}L l=1, reduce the computational complexity of MAP estimation substantially, and play a similar role in the Bayesian inference scheme to that played by the inducing points in the variational framework for sparse GP regression [44]. In this paper, we adopt a regular grid over T as the collocation points in accordance with the VB-based references
[31, 3], but note that other location choices are possible. For simplicity of explanation, we hereafter call {pl}L
It should be noted here that our approach (18) with the collocation method can be applied to cases where γ( ˙κ) = ˙κ(x)/κ(x) is deﬁned properly as a function of ˙κ(x). Table 1 shows that γ( ˙κ) can be deﬁned for popular link functions which include quadratic [18, 31], exponential [12, 30, 33], and softplus [28] functions. But our approach is not applicable to link functions whose derivatives, ˙κ(x), are not monotonic with respect to x, such as sigmoidal link functions [2, 3, 22]. l=1 inducing points. 5
3.2 Predictive Covariance
Given the MAP estimator ˆx(t), the predictive covariance (14) can be obtained by solving the linear integral equation (15) about h(t, s). We apply the Gelerkin method [4], a variant of the projection method, to solve the equation, which results in
X
Z
λl 1 + λlΞl
, Ξl =
T (cid:0)
¨κ (cid:1)
ˆx(t)
φ2 l (t)dt. (19) h(t, s) ≃
ωlφl(t)φl(s), ωl = l
See the supplementary material (§5) for the detailed derivation. We estimate the integral in {Ξl}L l=1 via the Monte Carlo method, but the quadratic link function (κ(x) = x2) is an exceptional case in which the integration can be performed analytically, Ξl = 2, due to the orthogonal relation of the
R
T φl(t)φl′(t)dt = δll′. It should be noted that (19) is the Mercer expansion [32] eigenfunctions, of function h(t, s), where {ωl}L l=1 is a set of its eigenvalues. 3.3 Marginal Likelihood
We can evaluate the marginal likelihood by substituting (18) into (16-17), log p(D) =
X
+ 1 2
X (cid:0) log κ n (cid:0)
ˆx(tn)
˙κ (cid:0)
ˆx(tn)
κ (cid:1) (cid:1)
ˆx(tn) (cid:20)X
Z (cid:1)
− (cid:0)
κ
ˆx(t) (cid:1) dt − 1 2
T
X
λlβ2 l l (cid:21)
λlβlφl(tn) − (ˆx(tn)−µ) l (cid:0)
R
T κ
ˆx(t) (cid:1) n dt, is estimated by the Monte Carlo method. The last term where the second term, is associated with the functional determinant (see the supplementary material (§1)), which can be
L calculated by the product of its eigenvalues as, log l=1 log(1+λlΞl), where we approximate the kernel function in terms of the Mercer expansion up to the top L eigenvalues, k(t, s) ≃ l=1 log λl
ωl
|K|
|H| =
P
P
P
L l=1 λlφl(t)φl(s).
=
L
− 1 2 log |IN +Z−1H| − 1 2 log
|K|
|H| , (20) 3.4 Computational Complexity
P (cid:2)P
L l=1[r(pl)]2, which needs the com-The objective function to be minimized in MAP estimation is putation of O(N L + L2) for each evaluation (see Eq. (18)): the computational complexities of
{γn≜γ
P l′=1 or { l′=1 needs the computation of O(L2). When a gradient descent algorithm with automatic differentiation [6] is employed, the computational complexity of the MAP estimation per gradient descent step is equal to O(N L + L2), which is one-Lth the complexity of the VB-based approaches [31, 3]. We adopt the algorithm in Section 5. l=1 are both O(N L), and { l βlφl(tn) l βlφl(pl′)}L (cid:3) n=1 and {
}N n k(pl, tn)γn}L l λlβlφl(pl′)}L
P
P
Furthermore, in view of the shape of the objective function, we can substantially improve the compu-tational efﬁciency of the MAP estimation by using a mini-batch gradient descent (MGD) algorithm:
MGD reduces the computational complexity to O(N L), which is due to {γn}N n=1. We verify the effectiveness of MGD in the supplementary material (§10). Also, when {pl}l are located regu-larly over T and k(t, t′) is separable across the dimensions, matrix Φ deﬁned as Φll′ = φl(pl′) has the Kronecker structure (see the the supplementary material (§7)), which can further reduce the complexity to O(N L) under (batch) gradient descent. Note that the cost of the VB-based methods
[31, 3] become O(N L2) when the Kronecker structure of the gram matrix is exploited.
Once the MAP estimator or the set of coefﬁcient {βl}L l=1 is obtained, estimating the predictive co-variance (14) needs the computation of O((N + L) · min(N 2, L2) + NmcL), where Nmc is the number of samples for the Monte Carlo method. The cost O(NmcL) comes from the computa-tion of h(t, s) (19), and the rest is due to the matrix operation. Due to the fact that h(t, s) is a degenerate kernel of rank L, the N × N matrix H can be decomposed into a product of N × L
ωlφl(tn), and its transpose as H = RR⊤; The matrix opera-matrix R, deﬁned by Rnl = tion h(t)⊤(Z + RR⊤)−1h(s) costs O(LN 2 + N 3) in a direct manner, but O(L2N + L3) if the
Woodbury matrix identity is used. Because the predictive covariance computation is not part of the iterative optimization in MAP estimation, its computation cost is negligible compared to that of
MAP estimation. Also, the selection of hyper-parameters can be performed based on the marginal likelihood (20), which needs the computation of O((N + L) · min(N 2, L2) + Nmc).
√ 6
3.5 Eigenfunctions of Kernel Operator
D
R
The pairs of eigenvalues and eigenfunctions, {λl, φl}∞ l=1, for the kernel operator K are obtained
T k(t, s)φl(s)ds = λlφl(t), where the by solving a homogeneous Fredholm integral equation, set of eigenvalues is at most countable [36]. Here we assume a multiplicative kernel, k(t, s) =
Q d=1 k(d)(t(d), s(d)), and a hyper-rectangular domain with interval T (d) = [0, T (d)] in each dimen-sion d. The multidimensional integral equation can then be reduced to a set of unidimensional ones,
R (t(d))}d, via the separation of variables, which results in a
{ (t(d))
) is the multiplicative form of solution, (λl, φl(t)) = ld-th solution of the integral equation in dimension d, and l = (l1, . . . , lD). Thus the computa-tion complexity of solving the integral equation scales linearly with the number of dimensions, D.
We adopt the eigenfunctions with the top Ld eigenvalues in each dimension d as the basis for the projection method, resulting in a set of L =
T (d) k(d)(t(d), s)φ(d) ld
, where (λ(d) ld (s)ds = λ(d) ld d φ(d) ld d λ(d) ld
, φ(d) ld
φ(d) ld d Ld basis functions. (cid:0)Q
Q
Q (cid:1)
,
P
J
The integral equation for each dimension d can rarely be solved analytically, but we can approximate the solution by the Nyström method [35]: The integral term is approximated by a J-point numerical j=1 k(t, sj) ˜φl(sj)w = ˜λl ˜φl(t), where (˜λl, ˜φl) is the approximation solution, integration such that
R j=1 and w ≜
{sj}J
T 1dt/J denote the nodes and the weight, respectively, and index d is omitted for simplicity of explanation (e.g. k(d)(t(d), s(d)) → k(t, s)); the approximation solution is then obtained by using the eigenvalues el and eigenvectors vl of the J×J matrix ˜K, deﬁned by ˜Kjj′ ≜ k(sj, sj′), as (21) where ˜k(t) ≜ (k(t, s1), . . . , k(t, sJ ))⊤. Under the Nyström method, the computational complexity of ﬁnding the eigenfunctions is O(DJ 3). We set J = 1000 in this paper, which is negligible compared to the cost of the MAP estimation. w),
˜φl(t) = ˜k(t)⊤vl/(el
˜λl = elw,
√ 4