Abstract
Federated learning has emerged as an important paradigm for training machine learning models in different domains. For graph-level tasks such as graph classiﬁ-cation, graphs can also be regarded as a special type of data samples, which can be collected and stored in separate local systems. Similar to other domains, multiple local systems, each holding a small set of graphs, may beneﬁt from collaboratively training a powerful graph mining model, such as the popular graph neural net-works (GNNs). To provide more motivation towards such endeavors, we analyze real-world graphs from different domains to conﬁrm that they indeed share certain graph properties that are statistically signiﬁcant compared with random graphs.
However, we also ﬁnd that different sets of graphs, even from the same domain or same dataset, are non-IID regarding both graph structures and node features. To handle this, we propose a graph clustered federated learning (GCFL) framework that dynamically ﬁnds clusters of local systems based on the gradients of GNNs, and theoretically justify that such clusters can reduce the structure and feature heterogeneity among graphs owned by the local systems. Moreover, we observe the gradients of GNNs to be rather ﬂuctuating in GCFL which impedes high-quality clustering, and design a gradient sequence-based clustering mechanism based on dynamic time warping (GCFL+). Extensive experimental results and in-depth analysis demonstrate the effectiveness of our proposed frameworks. 1

Introduction
Federated learning (FL) as a distributed learning paradigm that trains centralized models on decentral-ized data has attracted much attention recently [28, 53, 25, 18, 17]. FL allows local systems to beneﬁt from each other while keeping their own data private. Especially, for local systems with scarce train-ing data or lack of diverse distributions, FL provides them with the potentiality to leverage the power of data from others, in order to facilitate the performance on their own local tasks. One important problem FL concerns is data distribution heterogeneity, since the decentralized data, collected by different institutes using different methods and aiming at different tasks, are highly likely to follow non-identical distributions. Prior works approach this problem from different aspects, including optimization process [25, 18], personalized FL [13, 6, 8], clustered FL [9, 15, 2], etc.
As more advanced techniques are developed for learning with graph data, using graphs to model and solve real-world problems becomes more popular. One important scenario of graph learning is graph classiﬁcation, where models such as graph kernels [44, 34, 36, 45] and graph neural networks
[21, 43, 49, 46, 47, 48] are used to predict graph-level labels based on the features and structures of graphs. One real scenario of graph classiﬁcation is molecular property prediction, which is an important task in cheminformatics and AI medicine. In the area of bioinformatics, graph classiﬁcation can be used to learn the representation of proteins and classify them into enzymes or non-enzymes.
For collaboration networks, sub-networks can be classiﬁed regarding the information of research areas, topics, genre, etc. More applicable scenarios include geographic networks, temporal networks, etc.
⇤Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Since the key idea of FL is the sharing of underlying common information, as [23] discusses that real-world graphs preserve many common properties, we become curious about the question, whether real-world graphs from heterogeneous sources (e.g., different datasets or even divergent domains) can provide useful common information among each other? To understand this question, we ﬁrst conduct preliminary data analysis to explore real-world graph properties, and try to ﬁnd clues about common patterns shared among graphs across datasets. As shown in Table 1, we analyze four typical datasets from different domains, i.e., PTC_MR (molecular structures), ENZYMES (protein structures),
IMDB-BINARY (social communities), and MSRC_21 (superpixel networks). We ﬁnd them to indeed share certain properties that are statistically signiﬁcant compared to random graphs with the same numbers of nodes and links (generated with the Erd˝os–Rényi model [7, 10]). Such observations conﬁrm the claim about common patterns underlying real-world graphs, which can largely inﬂuence the graph mining models and motivates us to consider the FL of graph classiﬁcation across datasets and even domains. More details and discussion about Table 1 can be found in Appendix A.
Table 1: Data analysis on important graph properties shared among real-world graphs across different domains. For example, large Kurtosis values [32] indicate long-tail distribution of node degrees, which is observed in ENZYMES, IMDB-BINARY, and MSRC_21; similar average shortest path lengths are observed in PTC_MR, ENZYMES, and MSRC_21, although their actual graph sizes are rather different; large CC are observed in ENZYMES, IMDB-BINARY, and MSRC_21 and large LC are observed in almost all graphs.
Property kurtosis of degree distribution avg. shortest path length largest component size (LC, %) clustering coefﬁcient (CC) real random p-value real random p-value real random p-value real random p-value
PTC_MR (molecules)
ENZYMES (proteins)
IMDB-BINARY (social)
MSRC_21 (superpixel) 2.1535 3.0106 8.9262 3.6959 2.4424 2.8243 2.2791 2.9714 0.9999 0.0027 0 0
⇠
⇠ 3.36 4.44 1.48 4.09 2.42 2.56 1.54 2.81 0 0 0 0
⇠
⇠
⇠
⇠ 100 98.24 100 100 82.68 97.69 99.93 99.43 0
⇠ 0.2054 0.0023 0
⇠ 0.0095 0.4516 0.9471 0.5147 0.1201 0.1425 0.5187 0.0655 0 0 0 0
⇠
⇠
⇠
⇠
Although common patterns exist among graph datasets, we can still observe certain heterogeneity. In fact, the detailed graph structure distributions and node feature distributions can both diverge due to various reasons. To demonstrate this, we design and evaluate a structure heterogeneity measure and a feature heterogeneity measure in different scenarios (c.f. Section 4.1). We refer to the graphs possibly with signiﬁcant heterogeneity in our cross-dataset FL setting as non-IID graphs, which concerns both structure non-IID and feature non-IID, where naïve FL algorithms like FedAvg [28] can fail and even backﬁre (c.f. Section 6.2). Moreover, as the heterogeneity varies from case to case, a dynamic
FL algorithm is needed to keep track of such heterogeneity of non-IID graphs while conducting collaborative model training.
Due to the observations that the graphs in one client can be similar to those in some clients but not the others, we get motivated by [2] and ﬁnd it intuitive to consider a clustered FL framework, which assigns local clients to multiple clusters with less data heterogeneity. To this end, we propose a novel graph-level clustered FL framework (termed GCFL) through integrating the powerful graph neural networks (GNNs) such as GIN [43] into clustered FL, where the server can dynamically cluster the clients based on the gradients of GNN without additional prior knowledge, while collaboratively training multiple GNNs as necessary for homogeneous clusters of clients. We theoretically analyze that the model parameters of GNN indeed reﬂect the structures and features of graphs, and thus using the gradients of GNN for clustering in principle can yield clusters with reduced heterogeneity of both structures and features. In addition, we conduct empirical analysis to support the motivation of clustered FL across heterogeneous graph datasets in Appendix A.
Although GCFL can theoretically achieve homogeneous clusters, during its training, we observe that the gradients transmitted at each communication round ﬂuctuate a lot (c.f. Section 5.1), which could be caused by the complicated interactions among clients regarding both structure and feature hetero-geneity, making the local gradients towards divergent directions. In the vanilla GCFL framework, the server calculates a matrix for clustering only based on the last transmitted gradients, which ignores the client’s multi-round behaviors. Therefore, we further propose an improved version of GCFL with gradient-series-based clustering (termed GCFL+).
We conduct extensive experiments with various settings to demonstrate the effectiveness of our frameworks. Moreover, we provide in-depth analysis on the capability of them on reducing both structure and feature heterogeneity of clients through clustering. Lastly, we analyze the convergence of our frameworks. The experimental results show surprisingly positive results brought by our novel setting of cross-dataset/cross-domain FL for graph classiﬁcation, where our GCFL+ framework can effectively and consistently outperform other straightforward baselines. 2
2