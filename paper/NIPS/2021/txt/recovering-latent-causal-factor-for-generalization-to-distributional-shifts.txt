Abstract
Distributional shifts between training and target domains may degrade the predic-tion accuracy of learned models, mainly because these models often learn features that possess only correlation rather than causal relation with the output. Such a correlation, which is known as “spurious correlation” statistically, is domain-dependent hence may fail to generalize to unseen domains. To avoid such a spurious correlation, we propose Latent Causal Invariance Models (LaCIM) that speciﬁes the underlying causal structure of the data and the source of distributional shifts, guiding us to pursue only causal factor for prediction. Speciﬁcally, the LaCIM introduces a pair of correlated latent factors: (a) causal factor and (b) others, while the extent of this correlation is governed by a domain variable that characterizes the distributional shifts. On the basis of this, we prove that the distribution of observed variables conditioning on latent variables is shift-invariant. Equipped with such an invariance, we prove that the causal factor can be recovered without mixing information from others, which induces the ground-truth predicting mecha-nism. We propose a Variational-Bayesian-based method to learn this invariance for prediction. The utility of our approach is veriﬁed by improved generalization to distributional shifts on various real-world data. Our code is freely available at https://github.com/wubotong/LaCIM. 1

Introduction
Current data-driven deep learning models, revolutionary in various tasks though, often exploit all types of correlations to ﬁt data well. Among such correlations, there can be spurious ones corresponding to biases (e.g., confounding bias due to the presence of a third unseen factor) inherited from the data provided. Such data-dependent spurious correlations can erode the prediction power on unseen domains with distributional shifts, which can cause serious consequences especially in safety-critical tasks such as healthcare.
Recently, there is a Renaissance of causality in machine learning, expected to pursue causal rela-tionships [59] to achieve stable generalization across domains. The so-called area of “causality” is pioneered by Structural Causal Models [51], as a mathematical formulation of this metaphysical concept grasped in the human mind. The incorporation of these human priors about cause and effect endows the model with the ability to identify the causal structure [51] which entails not only the data but also the underlying process of how they are generated. To achieve causal modeling, the old-school methods [52, 10] directly causally related the output label Y to a subset of covariates X, which is however not conceptually reasonable in applications with sensory-level data (e.g. model pixels as causal factors of the output does not make sense in image classiﬁcation [11]).
∗Corresponding author
†Work done during an internship at Microsoft Research Asia. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
For such applications, we rather adopt the manner of human visual perception [8, 9, 80] to causally relate the label Y to unobserved abstractions denoted by S, i.e., Y ← S. We further assume the existence of another non-causal latent factor (of Y ) denoted as Z, that together with S generate the input X: X ← (S, Z). Such an assumption is similarly adopted in the literature [25, 27, 35, 75, 71].
To model shifts across domains, we allow Z to be spuriously correlated with S (hence also the output), as marked by the bidirected arrow in Fig. 1 (a). Taking image classiﬁcation as an example, the S and
Z respectively refer to object-related abstractions (e.g., contour, texture) and contextual information (e.g., background, view). Due to this correlation, the model can learn contextual information into prediction, which may fail to generalize to the domain such that this correlation is broken.
We encapsulate above assumptions into the skeleton illustrated in Fig. 1 (a), in which the spurious correlation between S and Z varies across domains, as marked by the red bi-directed arrow in
Fig. 1 (b). Taking a closer inspection, such a domain-dependent spurious correlation is governed by an auxiliary domain variable D in Fig. 1 (c), which causes the domain shifts. We call the set of causal models augmented with D as Latent Causal Invariance Models (LaCIM). Here, the “Causal
Invariance” refers to P (Y |S), which together with P (X|S, Z), can be proved to be stable to the shifts across domains, under the assumptions embedded in the causal structure of LaCIM. Equipped with such an invariance, we prove that the S and the ground-truth predictor: P (Y |s(cid:63)) for x generated from (s(cid:63), z(cid:63)), are identiﬁable up to transformations that do not mix the non-causal information.
Under such an identiﬁability guarantee, we propose to learn the P (Y |S) and P (X|S, Z) by reformu-lating the Variational Auto-encoder (VAE) [37] to ﬁt the joint distribution of the input and output variables from training domains. During the test stage, we ﬁrst infer the value of S by optimizing the estimated P (X|S, Z) over latent space, followed by the learned P (Y |S) for prediction. We
ﬁrst use simulated data to verify the correctness of the identiﬁability claim. Then, to demonstrate the utility, we test our approach on real-world data, consistently achieving better generalization to the new distribution; besides, we ﬁnd that our inferred causal factor can be concentrated in highly explainable semantic regions for the task of image classiﬁcation.
We summarize our contribution as follows: Methodologically (in sec. 4.1), we propose LaCIM in which the causal assumptions of two latent factors and the distributional shifts are incorporated;
Theoretically (in theorem 4.4), we prove the identiﬁability of the causal factor and the ground-truth predicting mechanism; Algorithmically (in sec. 4.3), guided by the identiﬁability, we reformu-late Variational Bayesian method to learn P (X|S, Z), P (Y |S) for prediction; Experimentally (in sec. 5.2), our approach generalizes better to distributional shifts, compared with others. 2