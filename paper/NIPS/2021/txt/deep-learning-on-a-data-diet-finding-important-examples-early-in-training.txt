Abstract
Recent success in deep learning has partially been driven by training increasingly overparametrized networks on ever larger datasets. It is therefore natural to ask: how much of the data is superﬂuous, which examples are important for general-ization, and how do we ﬁnd them? In this work, we make the striking observation that, in standard vision datasets, simple scores averaged over several weight ini-tializations can be used to identify important examples very early in training. We propose two such scores—the Gradient Normed (GraNd) and the Error L2-Norm (EL2N) scores—and demonstrate their efﬁcacy on a range of architectures and datasets by pruning signiﬁcant fractions of training data without sacriﬁcing test accuracy. In fact, using EL2N scores calculated a few epochs into training, we can prune half of the CIFAR10 training set while slightly improving test accuracy.
Furthermore, for a given dataset, EL2N scores from one architecture or hyper-parameter conﬁguration generalize to other conﬁgurations. Compared to recent work that prunes data by discarding examples that are rarely forgotten over the course of training, our scores use only local information early in training. We also use our scores to detect noisy examples and study training dynamics through the lens of important examples—we investigate how the data distribution shapes the loss surface and identify subspaces of the model’s data representation that are relatively stable over training. 1

Introduction in part, by training over-Recently, deep learning has made remarkable progress driven, parameterized models on ever larger datasets. This trend creates new challenges: the large com-putational resources required pose a roadblock to the democratization of AI. Memory and resource constrained settings, such as on-device computing, require smaller models and datasets. Identifying important training data plays a role in online and active learning. Finally, it is of theoretical interest to understand how individual examples and sub-populations of training examples inﬂuence learning.
To address these challenges, we propose a scoring method that can be used to identify important and difﬁcult examples early in training, and prune the training dataset without large sacriﬁces in test accuracy. We also investigate how different sub-populations of the training data identiﬁed by our score affect the loss surface and training dynamics of the model.
⇤This work was carried out while the author was at ServiceNow. It was ﬁnalized at Google Brain. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recent work on pruning data [1, 2], can be placed in the broader context of identifying coresets— examples that provably guarantee a small gap in training error on the full dataset [3–7]. However, due to the nonconvex nature of deep learning, coreset techniques make conservative estimates that lead to weak theoretical guarantees and are less effective in practice.
A different approach recently proposed by Toneva et al. [8] tracks the number of times through training an example transitions from being correctly classiﬁed to misclassiﬁed, called a "forget-ting event", and ﬁnd that some examples are rarely forgotten, while others are forgotten repeatedly.
Empirically, they observed that training accuracy is not affected by the rarely forgotten training ex-amples and a large fraction of the training data can be removed without any impact on test accuracy.
However, since this method relies on collecting forgetting statistics throughout training, the forget-ting score is typically calculated in the middle of or at the end of training. Toneva et al. [8] ﬁnd that, in their example of a ResNet18 trained on CIFAR-10 for 200 epochs, the Spearman rank correlation between early and late scores is good after about 25 epochs and stabilizes after 75 epochs.
Broadly speaking, the ability to prune datasets raises a number of questions: What is the nature of examples that can be removed from the training data without hurting accuracy? How early in training can we recognize such examples? How many examples do we need and how does this depend on the data distribution? These questions may have no generic answers and so, in this work, we begin to pursue them empirically in the context of several standard vision benchmarks and standard network architectures. Answers to these questions may both (1) lead to new methodologies that could dramatically reduce training times and memory requirements, and (2) offer important insights into the training dynamics of deep neural networks, and the role of data.
Our ﬁrst ﬁnding is that very early in training (just a few epochs), partial forgetting scores identify large fractions of data that can be pruned. Analyzing this puzzling result with a one gradient step analysis of training suggests a very simple heuristic: use the loss gradient norm of individual ex-amples to identify important examples. While this approach does not work when the loss gradient norms are computed at the weights early in training of a single trajectory, we ﬁnd that, surprisingly, averaging these norms over multiple weight initializations does produce a ranking that correlates strongly with forgetting scores and allows us to prune a signiﬁcant fraction of examples early in training. Indeed, we can prune 50% of examples from CIFAR-10 without affecting accuracy, while on the more challenging CIFAR-100 dataset, we can prune 25% of examples with only a 1% drop in accuracy.
Through a series of empirical studies, we have begun to tease apart the properties of important examples and how they can depend on the data distribution. In particular, we ﬁnd that the examples with the very highest norms become superﬂuous as the amount of label noise increases. Indeed, even on clean data, we ﬁnd that in the high pruning regime, the best population excludes the very highest-scoring examples. 1.1 Contributions
• We propose to score the importance of each training example (xi, yi) by its expected loss gradient norm (GraNd score), which, up to a constant, bounds the expected change in loss for an arbitrary example (x, y) caused by removing (xi, yi).
• We show that pruning training samples with small GraNd scores at initialization allows one to train on much smaller subset of the training data without signiﬁcant loss in accuracy. While the pruning levels are comparable to those provided by other methods [1, 8], our score is the only one that is well-deﬁned at initialization and early in training.
• Our experimental ﬁndings suggest that, within the ﬁrst few epochs of training, the GraNd score is well-approximated by the norm of the error vector (EL2N score), where the error vector is the predicted class probabilities minus one-hot label encoding. In fact, we ﬁnd that the EL2N score provides an even stronger signal for data-pruning—for CIFAR10 we can prune 50% of the data, and for the harder CIFAR100, we can prune as much as 25% of the data without any loss in test accuracy.
• We study the role of examples with the highest EL2N scores, and ﬁnd that excluding a small subset of the very highest scoring examples produces a boost in performance. This boost in performance is enhanced in a corrupted label regime. 2
• We introduce a method, based on linearly connected modes, for studying the empirical risk surface in terms of the modes of subsets of data, allowing us to identify when, in training, the ﬁnal performance on subpopulations is determined. We demonstrate that the linearly con-nected mode at-convergence of empirical risk surface computed on low EL2N score examples is determined much earlier in training compared to high score examples.
• Finally, we study how an example’s EL2N score connects to the network’s training dynamics.
We do so by tracking the data-dependent NTK submatrices corresponding to the low or high score examples, and measuring the rate at which it evolves in a scale-invariant way. We ﬁnd that the NTK submatrix for the high score examples evolves faster throughout training, supporting our hypothesis that high-scoring examples are the ones driving the learning and the changes in the NTK feature space [9]. 2 Which samples are important for learning? 2.1 Preliminaries
N (xi, yi)
We consider supervised classiﬁcation, where S = i=1 denotes the training set, drawn i.i.d.
}
{
K
, with input vectors x from an unknown data distribution
}
RK be the logit outputs encoding labels. For a ﬁxed neural network architecture, let fw(x) 2
Rd. Let   be the softmax of the neural network with weights w 2W✓ function given by  (z1, . . . , zK)k = exp
. Let p(w, x) =  (f (w, x)) denote
/ zk}
{ the neural network output in the form of a probability vector. For any probability vector ˆp, let
P
`(ˆp, y) = 2
RD on input x
K k=1 y(k) log ˆp(k) denote cross-entropy loss.
Rd and one-hot vectors y
K k0=1 exp zk0 } 0, 1 2{
D 2
{
P
Let w0, w1, w2, . . . , wT be the iterates of stochastic gradient descent (SGD), where, for some se-quence of minibatches S0, S1, . . . , ST
  wt = wt
S of size M , we have 1(x, y), (1) gt
⌘ 1 ✓ 1  
  (x,y)
St 1
  2
  for gt
  1(x, y) = rwt
  1 `(p(wt
  1, x), y), and t = 1, . . . , T .
P 2.2 Gradient Norm Score and an inﬁnitesimal analysis
Fix a training set S. Due to training with SGD from a random initialization, the weight vector at time t > 0, wt, is a random variable. The expected magnitude of the loss vector is our primary focus:
Deﬁnition 2.1. The GraNd score of a training example (x, y) at time t is  t(x, y) = Ewt k
Here we describe conditions under which the GraNd score controls the contribution of a training example to the change in the training loss. In order to simplify our analysis, we approximate the training dynamics as if they were in continuous time. gt(x, y) k2 .
A key quantity in our analysis is the time derivative of the loss for a generic labeled example (x, y):
 t((x, y), St) =
)), i.e., the instantaneous rate of change in the
) = fwt ( loss on (x, y) at time t, where the gradient is computed on the minibatch St. By the chain rule, d`(ft(x),y) dt (where ft(
 
·
·
 t((x, y), St) = gt(x, y) dwt dt . wt = wt+1  
This relates to our discrete time dynamics via dwt dt ⇡
Our goal is to understand how removing a training point from minibatch St affects  t((x⇤, y⇤), St) for any (x⇤, y⇤). If a training point (x, y) is not in the minibatch St, then the effect is trivial. We thus study  t((x⇤, y⇤), S) in order to be able to rank all the training examples.
Lemma 2.2. Let S (xj, yj). Then for all (x⇤, y⇤), there exists c such that j = S
P
  2 (x0,y0)
St gt(x0, y0).
⌘
¬
\
 t((x⇤, y⇤), S) k
 t((x⇤, y⇤), S
¬ j) c k k  gt(xj, yj)
. k
  (3) (2)
Proof. For a d`(ft(x⇤),y⇤) dt given example
= d`(ft(x⇤),y⇤) dwt
= (x⇤, y⇤), dwt dt . Since the weights are updated using SGD, we have yields  t((x⇤, y⇤), S) chain rule the
  dwt dt =
⌘
  (xj ,yj )
St 2
P gt(xj, yj). Letting c = ⌘ k 3 d`(ft(x⇤),y⇤) dwt
, the result follows. k
At any given training step, given the current location wt, the contribution of a training example (x, y)2 to the decrease of loss on any other example, is bounded by Eq. (3). Since the constant c does not depend on the training example (x, y)3, we only consider the gradient norm term,
. k
The expected value of this gradient norm is exactly the GraNd score of (x, y).
In other words, examples with a small GraNd score in expectation have a bounded inﬂuence on learning how to classify the rest of the training data at a given training time4. We therefore propose to rank training examples by their GraNd scores, larger norm meaning more important for preserving  t(x). gt(x, y) k
For an arbitrary input x can be written as5 2
Rd, let  (k) t (x) = rwt f (k) t (x) denote the kth logit gradient. Then GraNd (x)
. 2 yk. When
 
 
 
 t(x, y) = E
K k=1 rf (k) `(ft(x), y)T  (k) t (4)
Under the cross entropy loss,
}k are roughly orthogonal across logits, and are of a similar size across logits and training examples x, then we can approximate GraNd by just the norm of the error vector.
Deﬁnition 2.3. The EL2N score of a training sample (x, y) is deﬁned to be E rf (k) `(ft(x), y)T = p(wt, x)(k) (x)
  y  (k) t
{
P
 
 
  p(wt, x) k
  k2.
Our experimental results suggest that this approximation becomes accurate after a few epochs of training (see Section 3). These approximations are also in agreement with the empirical results reported in [9, 10]. Fort and Ganguli [10, Sec. 5.1] demonstrate that the mean logit gradients are nearly orthogonal among classes throughout training. The authors demonstrate that per-example gradients cluster around the mean logit gradient. Fort et al. [9, Figs. 12D-14D] provide evidence that the mean logit gradient directions evolve rapidly early in training and then stabilize (as measured by the cosine distance between mean logit gradient vectors at different times in training). 2.3 Comparison to forgetting scores
Toneva et al. [8] deﬁne a “forgetting event” for a training sample to be a point in training when the classiﬁer switches from making a correct classiﬁcation decision to an incorrect one. They deﬁne an approximate forgetting score for each training example as the number of times during training when it was included in a minibatch and underwent a forgetting event. Toneva et al. demonstrate that examples with low forgetting score may be completely omitted during training without any noticeable effect on the accuracy of the learned predictor. In Fig. 1 and Appendix E.5, we make an empirical comparison of forgetting scores to our proposed GraNd and EL2N scores.
·
In Lemma 2.2, we bounded the contribution of a training example to the decrease of the loss of any other sample over a single gradient step. Due to  t(
)’s being time-dependent, it is complicated to extend the analysis to multiple steps. However, it is interesting to consider a case when  t(xi) =  (xi) for all xi in the training set, and K = 1. Then summing the bound in Eq. (3) on how much a sample (xj, yj) affects the logit output on an arbitrary point at each time t
, we
. For two examples, (x, y) and (x0, y0),  (xj) obtain a score that depends on k  (x0) such that
, we see that the example that is learned faster and maintains small error over training time will have a smaller GraNd score on average throughout training. Note that (pt(xj) upper
| bounds the number of forgetting events during training (after rescaling). In this simpliﬁed setting an example with a high number of forgetting events will also have a high GraNd score.
, if rescaled, is an upper bound on 0–1 loss, and therefore t(pt(xj) (pt(xj) k ⇡ k 1, .., T  (x) yj) yj) yj) 2{
P
P t |
 
 
  k| k k
}
|
|
| 3 Empirical Evaluation of GraNd and EL2N Scores via Data Pruning
In the previous section, we motivated GraNd and EL2N scores by quantifying the inﬂuence of a training example on the loss of an arbitrary example after one optimization step. In this section, 2Here we drop the index j since we refer to an arbitrary training point. 3Note that c depends on (x⇤, y⇤) but for a given (x⇤, y⇤), c is ﬁxed for all training inputs (x, y), allowing us to rank the training examples. 4The opposite is not necessarily true: examples with large scores may have gradients that cancel out and do not contribute much, meaning that this upper bound is loose. 5The score  t(x, y) is a function of (x, y). Thus (x, y) is non-random, and the expectation is taken over the remaining randomness (the weights at time t which depend on a random initialization, random minibatch sequence, GPU noise, etc.). 4
Figure 1: Columns correspond to three different dataset and network combinations (labeled at the top). Each legend applies to all 3 ﬁgures in its row. First row: Final test accuracy achieved by training on a subset of training data comprised of examples with maximum forgetting, EL2N and
GraNd scores computed at different times early in training. Subsets of a ﬁxed size are used: networks are trained on 50% of training data for CIFAR-10, 60% for CINIC-10 and 75% for CIFAR-100.
Second row: Final test accuracy achieved by training after different fractions of the dataset are pruned. Here we compare forgetting scores at the end of training, EL2N scores early in training (at epoch 20) and GraNd scores at initialization. In each case, examples with the lowest scores are pruned at initialization. In all experiments accuracies achieved by training on the full dataset and on a random subset of the corresponding size are used as baselines. we evaluate these scores empirically, and verify that they identify examples important for general-ization. Networks trained on subsets of the data with high scores achieve levels of test accuracy comparable to training on the full dataset and are competitive with other state of the art data pruning methods. Perhaps most remarkably, these scores are effective even when computed early in training and perform signiﬁcantly better than a random baseline, even at initialization.
Data pruning experiments. We train convolutional neural networks of varying depth–ResNet18 and ResNet50 [11]–on standard vision datasets of varying difﬁculty–CIFAR-10, CIFAR-100 [12], and CINIC-10 [13]. All scores are calculated by averaging the scores from ten independent training runs. After calculating scores and selecting a training subset, ﬁnal test accuracies are obtained by retraining networks from new random initializations on only the selected subset. Networks used for evaluating the scores are initialized with seeds that are different from those used to calculate the scores. For each experiment, we report the mean of four independent runs and represent variability across runs by shading the region which spans the 16th to 84th percentile of obtained accuracies.
See Appendix B for more implementation details and Appendix E for additional experiments.
In Fig. 1, we show the results of two sets of experiments (top and bottom) on three different network and dataset combinations. The ﬁrst experiment asks, how early in training are forgetting, GraNd and
EL2N scores effective at identifying examples important for generalization? We compare the ﬁnal test accuracy from training on subsets of ﬁxed size but pruned based on scores computed at different times early in training. The second experiment compares how GraNd scores at initialization, EL2N scores early in training and forgetting scores at the end of training negotiate the trade-off between generalization performance and training set size. The training sets are constructed by pruning differ-ent fractions of the lowest score examples. In all examples, training on the full dataset and a random subset of the corresponding size are used as baselines. We make the following observations.
Pruning at initialization. In all settings, GraNd scores can be used to select a training subset at initialization that achieves test accuracy signiﬁcantly better than random, and in some cases, com-petitive with training on all the data. This is remarkable because GraNd only contains information about the gradient norm at initializion, averaged over initializations. This suggests that the ge-5
ometry of the training distribution induced by a random network contains a surprising amount of information about the structure of the classiﬁcation problem. EL2N scores, which only contain in-formation about errors, are not consistently effective at initialization and forgetting scores, which require counting forgetting events over training, are not deﬁned at initialization.
Pruning early in training. We ﬁnd that, after only a few epochs of training, EL2N scores are extremely effective at identifying important examples for generalization. For a wide range of inter-mediate pruning levels, training on the highest scores performs on par with or better than training on the full dataset. Even at higher pruning levels, EL2N scores computed using local information early in training are competitive with forgetting scores which integrate information over the training tra-jectory. This suggests that the average error vector a few epochs into training can identify examples that the network heavily uses to shape the decision boundary throughout training.
Interestingly, at extreme levels of pruning with either EL2N or GraNd scores, we observe a sharp drop in performance. We hypothesize that this is because at high levels of pruning, using either
GraNd or EL2N scores leads to bad coverage of the data distribution. By only focusing on the highest error examples, it is likely that an entire subpopulation of signiﬁcant size that is present in the test data is now excluded from the training set. We only ﬁt a small number of very difﬁcult examples and do not keep enough of a variety of examples for training models with good test error.
A property of the data. Our results suggest that the ranking of important examples induced by
EL2N scores is a property of the dataset and not speciﬁc to a network. First, in Appendix E.2, we show that a ResNet18 and a ResNet50 trained on CIFAR-10 have similar performance curves and the same amount of data can be pruned, even though ResNet50 is a much deeper network with more parameters. Second, EL2N scores calculated on one set of network architecture and hyperparam-eter conﬁgurations can be used to prune data for training with a different network architecture or hyperparameter conﬁguration. The set of important examples generalizes across architectures and hyperparameters. See Appendix E.3 for the experiment on generalization across architectures and
Appendix E.4 for the experiment on using scores calculated during hyperparameter optimization.
Additionally, in an analysis of the sensitivity of the scoring methods to hyperparameters in Ap-pendix E.1, we observe that scores calculated on a single network do not perform as well as those averaged across networks.
We hypothesize that averaging the gradient or error norms over multiple initializations or training trajectories removes dependence on speciﬁc weights, allowing a more accurate distillation of the properties of the dataset. EL2N scores can thus be used to probe and understand how the distribution of the training data impacts dynamics (as we show in the next sections). Additionally, it can also reduce the computational burden of training neural networks; once we compute the scores, future networks can be trained on the pruned dataset.
In the following experiments, we focus on EL2N scores computed early in training, as they appear to more accurately identify important examples. 4
Identifying noise examples
In the previous section, we studied the effect of keeping the highest-scoring examples, and found that we could train on only the top 50% of exam-ples by score without a drop in accuracy (CIFAR-10). What is the nature of subpopulations of examples that allow us to reach high accuracy? One hypothe-sis is that the highest-scoring examples are the most impor-tant ones for achieving an ac-In this sec-curate classiﬁer. tion, we refute this hypothesis, and demonstrate the role of la-bel noise.
Figure 2: ResNet18 trained on a 40% subset of CIFAR-10 with clean (left) and 10% randomized labels (right). The training subset contains the lowest scoring examples after examples with scores below the offset are discarded. Scores computed at epoch 10. 6
To test whether the highest-scoring examples are most important for achieving high accuracy, we
ﬁrst sort the examples by increasing EL2N score computed after a small number of training epochs.6
Then we perform a sliding window analysis by training on a subset of examples with scores within a window from percentile f to percentile f + P percentile, always keep P % of the data but sliding up f . As this window slides to higher percentiles, performance increases, except when the window includes examples with the very highest scores Fig. 2 (left). Indeed the the optimal sliding window actually excludes approximately 500 of the highest-scoring training examples. These effects are reduced in the low pruning regime (see Appendix F.1). In Appendix C, we visualize some of the images that are excluded from each class.
Before we analyze these results, we ﬁrst place them into a wider context, where we also change the amount of noise in the underlying label distribution. We repeat the experiment outlined above, but corrupt a random K% of labels, replacing them with a random label, mirroring the protocol popu-larized by Zhang et al. [14]. Fig. 2 reveals that with increased label corruption, the optimal window shifts and excludes a higher number of examples. Therefore, the effect we see in the noiseless case appears to be magniﬁed in the presence of label noise. Appendix F.2 examines how adding label noise inﬂuences the distribution of EL2N scores of examples.
These ﬁndings have several implications. The most obvious implication is that training with only the highest-scoring samples may not be optimal, especially when there is label noise. When the population has a low Bayes error rate, using only the highest scoring samples yields optimal re-sults. However, without a validation set, one should be cautious in excluding high-score examples.
Feldman [15] discusses memorization in a noisy-label setup and gives conditions under which one should memorize in order to not misclassify singleton examples ( examples in the training data that are the sole representatives of a subpopulation). For example, if the subpopulation appears with a frequency ⌦(1/N ), memorizing such examples can improve generalization. In practice, we may not know whether our data ﬁts these conditions. However, our analysis in Fig. 2 suggests a simple and powerful method to prune data for optimal performance by optimizing just two hyperparameters of a sliding window using a validation set. 5 Optimization landscape and the training dynamics 5.1 Evolution of the data-dependent NTK
The dynamics of neural-network training in the inﬁnite-width limit are now well understood [16, 17]: for an appropriate scaling of the learning rate and initial weights, the neural network behaves like a linear model in which the data is transformed by the Neural Tangent Kernel (NTK) at initialization, which is deﬁned as the product of the Jacobians of the logits at initialization. In the limit, neural network training implements kernel regression with the ﬁxed NTK as the kernel.
In fact,
However, ﬁnite neural net-works outperform their inﬁnite-width limits [18] and have dif-ferent dynamics early in train-ing [19]. rather than being constant, the data-dependent NTK, deﬁned by
Fort et al. [9] as the Gram matrix of the logit Jacobian, evolves with high velocity in the initial phase of training.
Then, around the time of on-set of linear mode connectiv-ity, the NTK velocity stabilizes at a smaller value and remains nearly constant for the rest of the high learning rate training time.
Figure 3: Kernel velocity for different subsets of images when
ResNet18 is trained on CIFAR-10 with all true labels (left) and 10% label noise (right). Examples are sorted in ascending order by EL2N scores and each point corresponds to the kernel velocity of 100 contiguous images starting at example index. Both scores and velocities are computed at the same epoch indicated by color. 6In Appendix F.3, we repeat these experiments for the GraNd score. 7
Here we seek to understand which training samples contribute to the NTK gram matrix evolution.
To empirically approximate the velocity of a NTK submatrix corresponding to a subset of images in a scale invariant way, we follow [9]. We compute the cosine distance between two NTK gram matrices on the given subset, one computed at epoch t, and another one at epoch t+1, one epoch later (see Appendix B.3). We look at submatrices of a ﬁxed size, formed by examples with contiguous
EL2N scores. Fig. 3 shows that higher EL2N scores lead to higher velocities. This relationship is not affected by the time at which both are computed.
Interestingly, the kernel velocity drops off sharply for examples with the very highest scores when label noise is introduced. In Section 4, we showed that dropping these examples boosts the accuracy of the ﬁnal predictor. We hypothesize that, while the kernel velocity is higher for harder examples that the model is actively trying to ﬁt, the kernel velocity drops off for the very highest scoring examples that might be too difﬁcult to learn, perhaps because they are unrepresentative samples or they have have label noise. 5.2 Connections to the Linear Mode Connectivity
We now examine how the ranking of the examples by EL2N connects to the geometry of the loss surface. In particular, Frankle et al. [20] studied the effect of minibatch randomness on the training trajectory, focusing on identifying the point in training when two networks, starting from the same weights, but trained with independent minibatches, converge to the same “linearly connected” mode.
They ﬁnd that, for standard vision datasets, the onset of this “linear mode connectivity” (LMC) happens early in training.
More precisely, let w1, w2, . . . , wT be the training trajectory of a parent network, ﬁx a spawning time t⇤, and let vt⇤ , vt⇤+1, vt⇤+2, . . . , vT be an independent training trajectory (i.e., with independent minibatches), beginning at vt⇤ = wt⇤ . We call vT the child network and vt⇤ , vt⇤+1, . . . the child trajectory. The (training) error barrier between two weights w and w0, denoted err(w, w0; S), is the maximum deviation of the training error surface ˆRS(
) above the line connecting the empirical risk at w and w0. That is,
· err(w, w0; S) = sup↵
[0,1] 2
ˆRS(↵w + (1
↵) w0)
 
 
↵ ˆRS(w) (1
 
 
↵) ˆRS(w0)
. (5)
We then deﬁne the mean (training) error barrier, spawning at t⇤, at time t, for t⇤ errt⇤
 t (S), to be the expected error barrier between wt and vt on the data S. That is,
 
T , denoted t
 errt⇤ t (S) = Ewt⇤+1:t,vt⇤+1,t [err(wt, vt; S)], where the expectation is taken over the randomness in the trajectories of w and v after t⇤ due to the choice of minibatches, conditional on the initial trajectories up through time t⇤. (Note that, at the end of training t = T , the supremum in err(wT , vT ; S) is often achieved near ↵ = 1/2, and so this is a cheap approximation used in practice.) The “onset” of linear mode connectivity is the earliest spawning time t⇤ at which point errt⇤ 0, where S is the whole training set. In our work, we instead compute the error barrier on subsets of the training set, which allows us to compare the training dynamics and modes on subpopulations.
T (S) (6)
⇡
In Fig. 4, we measure the mean error barrier errt⇤ t (S0) as a function of the spawning time t⇤, in the cases where S0 are either 1) the training examples with the smallest scores, 2) the largest scores, or 3) a random subset of training examples. We ﬁnd that the error barrier falls close to zero very rapidly for examples that have low EL2N scores, and stays high for high score examples. These
ﬁndings suggest that the loss landscape derived from restricted subsets of examples with low and high EL2N behave very differently. The loss landscape derived from easy subsets of examples with low scores is quite ﬂat, in the sense that error barriers between children as a function of spawn time rapidly diminish. On the other hand, the loss landscape derived from harder subsets of examples with higher scores is rougher, with higher error barriers that persist for longer in the spawn time.
Further, this result is in agreement with the results presented in Section 5.1, showing that most of the learning happens in the high EL2N score examples. 6