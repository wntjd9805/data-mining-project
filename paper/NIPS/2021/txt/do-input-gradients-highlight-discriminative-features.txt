Abstract
Post-hoc gradient-based interpretability methods [1, 2] that provide instance-speciﬁc explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach: 1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classiﬁcation benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A) reasonably well. 2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models. 3. Finally, we theoretically prove that our empirical ﬁndings hold on a simpliﬁed version of the BlockMNIST dataset. Speciﬁcally, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-speciﬁc “signal” coordinates, thus grossly violating (A).
Our ﬁndings motivate the need to formalize and test common assumptions in inter-pretability in a falsiﬁable manner [3]. We believe that the DiffROAR framework and
BlockMNIST datasets serve as sanity checks to audit interpretability methods; code and data available at https://github.com/harshays/inputgradients. 1

Introduction
Interpretability methods that provide instance-speciﬁc explanations of model predictions are often used to identify biased predictions [4], debug trained models [5], and aid decision-making in high-stakes domains such as medical diagnosis [6, 7]. A common approach for providing instance-speciﬁc explanations is feature attribution. Feature attribution methods rank or score input coordinates, or features, in the order of their purported importance in model prediction; coordinates achieving the top-most rank or score are considered most important for prediction, whereas those with the bottom-most rank or score are considered least important.
Input gradient attributions. Ranking input coordinates based on the magnitude of input gradients is a fundamental feature attribution technique [8, 1] that undergirds well-known methods such as
SmoothGrad [2] and Integrated Gradients [9]. Given instance x and a trained model θ with prediction
ˆy on x, the input gradient attribution scheme (i) computes the input gradient xLogitθ(x, ˆy) of the logit 2 of the predicted label ˆy and (ii) ranks the input coordinates in decreasing order of their input gradient magnitude. Below we explicitly characterize the underlying intuitive assumption behind input gradient attribution methods:
∇
∗Part of the work completed after joining Google Research India 2In Appendix C, we show that our results also hold for input gradients taken w.r.t. the loss 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Experiments on BlockMNIST dataset. (a) Four representative images from class 0 & class 1 in BlockMNIST dataset; every image consists of a signal and null block that are randomly placed as the top or bottom block. The signal block, containing the MNIST digit, determines the image class.
The null block, containing the square patch, does not encode any information of the image class.
For these four images, subplots (b-e) show the input gradients of standard Resnet18, standard MLP, (cid:96)2 robust Resnet18 ((cid:15)=2) and (cid:96)2 robust MLP ((cid:15)=4) respectively. The plots clearly show that input gradients of standard BlockMNIST models highlight the signal block and the non-discriminative null block, thereby violating (A). In contrast, input gradients of adversarially robust models exclusively highlight the signal block, suppress the null block, and satisfy (A). Please see Section 5 for details.
Assumption (A): Coordinates with larger input gradient magnitude are more relevant for model prediction compared to coordinates with smaller input gradient magnitude.
Sanity-checking attribution methods. Several attribution methods [10] are based on input gradients and explicitly or implicitly assume an appropriately modiﬁed version of (A). For example, Integrated
Gradients [9] aggregate input gradients of linearly interpolated points, SmoothGrad [2] averages input gradients of points perturbed using gaussian noise, and Guided Backprop [11] modiﬁes input gradients by zeroing out negative values at every layer during backpropagation. Surprisingly, unlike vanilla input gradients, popular methods that output attributions with better visual quality fail simple sanity checks that are indeed expected out of any valid attribution method [12, 13]. On the other hand, while vanilla input gradients pass simple sanity checks, Hooker et al. [14] suggest that they produce estimates of feature importance that are no better than a random designation of feature importance.
Do input gradients satisfy assumption (A)? Since (A) is necessary for input gradients attributions to accurately reﬂect model behavior, we introduce an evaluation framework, DiffROAR, to analyze whether input gradient attributions satisfy assumption (A) on real-world datasets. While DiffROAR adopts the remove-and-retrain (ROAR) methodology [14], DiffROAR is more appropriate for testing the validity of assumption (A) because it directly compares top-ranked features against bottom-ranked features. We apply DiffROAR to evaluate input gradient attributions of MLPs & CNNs trained on multiple image classiﬁcation datasets. Consistent with the message in Hooker et al. [14], our experiments indicate that input gradients of standard models (i.e., trained on original data) can grossly violate (A) (see Section 4). Furthermore, we also observe that unlike standard models, adversarially trained models [15] that are robust to (cid:96)2 and (cid:96)∞ perturbations satisfy (A) in a consistent manner.
Probing input gradient attributions using BlockMNIST. Our empirical ﬁndings mentioned above strongly suggest that standard models grossly violate (A). However, without knowledge of ground-truth discriminative features learned by models trained on real data, conclusively testing (A) remains elusive. In fact, this is a key shortcoming of the remove-and-retrain (ROAR) framework. So, to further verify and better understand our empirical ﬁndings, we introduce an MNIST-based semi-real dataset,
BlockMNIST, that by design encodes a priori knowledge of ground-truth discriminative features.
BlockMNIST is based on the principle that for different inputs, discriminative and non-discriminative features may occur in different parts of the input. For example, in an object classiﬁcation task, the object of interest can occur in different parts of the image (e.g., top-left, center, bottom-right etc.) for different images. As shown in Figure 1(a), BlockMNIST images consist of a signal block and a null block that are randomly placed at the top or bottom. The signal block contains the MNIST digit that determines the class of the image, whereas the null block, contains a square patch with two diagonals that has no information about the label. This a priori knowledge of ground-truth discriminative 2
features in BlockMNIST data allows us to (i) validate our empirical ﬁndings vis-a-vis input gradients of standard and robust models (see ﬁg. 1) and (ii) identify feature leakage as a reason that potentially explains why input gradients violate (A) in practice. Here, feature leakage refers to the phenomenon wherein given an instance, its input gradients highlight the location of discriminative features in the given instance as well as in other instances that are present in the dataset. For example, consider the
ﬁrst BlockMNIST image in ﬁg. 1(a), in which the signal is placed in the bottom block. For this image, as shown in ﬁg. 1(b,c), input gradients of standard models incorrectly highlight the top block because there are other instances in the BlockMNIST dataset which have signal in the top block.
Rigorously demonstrating feature leakage. In order to concretely verify as well as understand feature leakage more thoroughly, we design a simpliﬁed version of BlockMNIST that is amenable to theoretical analysis. On this dataset, we ﬁrst rigorously demonstrate that input gradients of standard one-hidden-layer MLPs exhibit feature leakage in the inﬁnite-width limit and then discuss how feature leakage results in input gradient attributions that clearly violate assumption (A).
Paper organization: Section 2 discusses related work and section 3 presents our evaluation frame-work, DiffROAR, to test assumption (A). Section 4 employs DiffROAR to evaluate input gradient attributions on four image classiﬁcation datasets. Section 5 analyzes BlockMNIST data to differen-tially characterize input gradients of standard and robust models using feature leakage. Section 6 provides theoretical results on a simpliﬁed version on BlockMNIST that shed light on how feature leakage results in input gradients that violate assumption (A). Our code, along with the proposed datasets, is publicly available at https://github.com/harshays/inputgradients. 2