Abstract
Large width limits have been a recent focus of deep learning research: modulo com-putational practicalities, do wider networks outperform narrower ones? Answering this question has been challenging, as conventional networks gain representational power with width, potentially masking any negative effects. Our analysis in this paper decouples capacity and width via the generalization of neural networks to
Deep Gaussian Processes (Deep GP), a class of nonparametric hierarchical models that subsume neural nets. In doing so, we aim to understand how width affects (standard) neural networks once they have sufﬁcient capacity for a given mod-eling task. Our theoretical and empirical results on Deep GP suggest that large width can be detrimental to hierarchical models. Surprisingly, we prove that even nonparametric Deep GP converge to Gaussian processes, effectively becoming shallower without any increase in representational power. The posterior, which corresponds to a mixture of data-adaptable basis functions, becomes less data-dependent with width. Our tail analysis demonstrates that width and depth have opposite effects: depth accentuates a model’s non-Gaussianity, while width makes models increasingly Gaussian. We ﬁnd there is a “sweet spot” that maximizes test performance before the limiting GP behavior prevents adaptability, occurring at width = 1 or width = 2 for nonparametric Deep GP. These results make strong predictions about the same phenomenon in conventional neural networks trained with L2 regularization (analogous to a Gaussian prior on parameters): we show that such neural networks may need up to 500 − 1000 hidden units for sufﬁcient capacity—depending on the dataset—but further width degrades performance. 1

Introduction
Research has shown that deeper neural networks tend to be more expressive and efﬁcient than wider networks under a variety of metrics [e.g. 21, 63, 67, 70, 74, 75, 78, 83]. Nevertheless, there is resurgent interest in wide models due in part to empirical successes [e.g. 92] and theoretical analyses of limiting behavior. When randomly initialized to create a distribution over functions, neural networks converge to Gaussian processes (GP) as width increases. This result, ﬁrst proved for 2-layer networks [69], has been extended to deeper networks [56, 64], convolutional networks
[38, 71], and other architectures [50, 88]. A similar limit exists for gradient-trained networks, which behave increasingly like kernel machines under the neural tangent kernel [e.g. 6, 8, 28, 39, 52, 57, 89].
While these limits simplify analyses, there is something unsettling about reducing neural networks to kernel methods. Neal [69, p. 161] describes the GP limit as “disappointing,” noting that “inﬁnite networks do not have hidden units that represent ‘hidden features’. . . often seen [as the] interesting aspect of neural network learning.” Recent work indeed shows that learned hierarchical features can be exponentially more efﬁcient than the ﬁxed shallow representations of kernels [e.g. 4, 5, 8, 11, 13, 21, 41, 42, 60, 91]. At the same time, wider networks can more accurately model complex functions 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[44]. Thus, wide limits appear to confound opposing phenomenon: increased capacity makes them more expressive, yet the loss of hierarchical features seems to make them less expressive. This may explain the mixed empirical performance of limiting models: outperforming ﬁnite width models in some scenarios [e.g. 9, 38, 39, 58], yet falling short on more complex tasks [e.g. 8, 11, 35, 57, 81].
This paper aims to decouple these effects of large width. Our goal is to understand the inductive biases of wide networks, after a network has “sufﬁcient” capacity for a given modeling task. We ask:
If we control for the effects of increased capacity, what—if any—value remains in wide networks?
To achieve this control, we note that a typical neural network layer corresponds to a ﬁnite basis, where elementwise nonlinearities transform each hidden feature into a single basis function. In order to decouple width from capacity, one could generalize these layers so that each nonlinearity produces any number of basis functions; if each hidden feature gives rise to an inﬁnite and universal basis, then hidden layers would have inﬁnite representational capacity regardless of width. This generalization is in fact a well-studied class of hierarchical models—Deep Gaussian Processes (Deep
GP) [19, 24, 26, 27, 30, 32, 46, 79]—where standard neural net layers are replaced with vector-valued
Gaussian processes. Indeed, typical neural networks are a degenerate Deep GP subclass [1, 2, 33, 72].
We therefore have a generalization of neural networks where capacity is controlled, from which we can glean insights about conventional networks that have sufﬁcient representational power for a given modeling task. Surprisingly, despite using Gaussian processes as the primary hierarchical component, we prove that Deep GP converge to (single-layer) GP in their inﬁnite width limit (Thm. 1). Troubling implications immediately ensue: large width is strictly detrimental to Deep GP, as the limiting model collapses to a shallower version of itself. We support this theorem with an analysis of neural network and Deep GP posteriors, which become less adaptable as width increases. Speciﬁcally, we show that the posterior mean corresponds to a mixture of functions drawn from data-dependent (and thus adaptive) reproducing kernel Hilbert spaces, formalizing the above claim from Neal [69]. As width increases, this mixture collapses to the data-independent kernel of the limiting GP, implying that wider models have less feature learning. Finally, we present a novel tail analysis which indicates that width and depth have opposite effects: depth accentuates non-Gaussianity, sharpening peaks and fattening tails, whereas width increases Gaussianity (Thms. 2 and 3).
Our theoretical results hold for Deep GP and conventional (parametric) neural networks alike.
Experiments conﬁrm that—after a model achieves sufﬁcient capacity1—width can become harmful to model ﬁt and performance. For nonparametric Deep GP, a width of 1 or 2 often achieves the best performance. Neural networks—because of their parametric nature—naturally require more hidden units before achieving optimal accuracy. Nevertheless, for Bayesian neural networks and conventional (optimized) neural networks trained with L2 regularization, performance degrades after a certain width. On small datasets (N ≤ 1000) with low dimensionality, we ﬁnd that models with
≤ 16 hidden units achieve best test set performance. On larger datasets like CIFAR10, this “sweet spot” occurs later (at ≈ 500 hidden units for sufﬁciently deep models), yet performance degrades beyond this width. We note that these trends do not necessarily hold for models that do not have a probabilistic interpretation—i.e. optimized neural networks trained without (or nearly without) L2 regularization. Nevertheless, our ﬁndings suggest that narrower models have better inductive biases, and wide models perform well in spite of —not because of—large width. 2 Setup 2.1