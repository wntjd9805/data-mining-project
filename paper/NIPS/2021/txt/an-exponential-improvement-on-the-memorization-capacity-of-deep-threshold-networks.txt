Abstract (e1/δ2
It is well known that modern deep neural networks are powerful enough to mem-orize datasets even when the labels have been randomized. Recently, Vershynin (2020) settled a long standing question by Baum (1988), proving that deep thresh-old networks can memorize n points in d dimensions using
+√n) neurons and (d + √n) + n) weights, where δ is the minimum distance between the points. In this work, we improve the dependence on δ from exponential to almost ( d linear, proving that
δ + n) weights are sufﬁcient. Our construction uses Gaussian random weights only in the ﬁrst layer, while all the subsequent layers use binary or integer weights. We also prove new lower bounds by connecting memorization in neural networks to the purely geometric problem of separating n points on a sphere using hyperplanes. ( 1
δ + √n) neurons and (e1/δ2
O
O
O
O (cid:101) (cid:101) (cid:101) (cid:101) 1

Introduction
The current paradigm of training neural networks is to train the networks until they ﬁt the training dataset perfectly (Zhang et al., 2017; Arpit et al., 2017). This means that the network is able to output the exact label for every sample in the training set, a phenomenon known as interpolation. Quite interestingly, modern deep networks have been known to be powerful enough to interpolate even randomized labels (Zhang et al., 2017; Liu et al., 2020), a phenomenon that is usually referred to as memorization (Yun et al., 2019; Vershynin, 2020; Bubeck et al., 2020), where the networks can interpolate any arbitrary labeling of the dataset.
Given that memorization is a common phenomenon in modern deep learning, a reasonable question to ask is that of how big a neural network needs to be so that it can memorize a dataset of n points in d dimensions. This question has been of interest since the 60s (Cover, 1965; Baum, 1988; Mitchison
& Durbin, 1989; Sontag, 1990; Huang et al., 1991; Sartori & Antsaklis, 1991). In particular, Baum (1988) proved that a single hidden layer threshold network with
) n/d (cid:101) neurons can memorize any set of n points in d dimensions, as long as they are in general position.1
Baum asked if going deeper could increase the memorization power of threshold networks, and in particular if one could reduce the number of neurons needed for memorization to (√n). While for
O
ReLU activated networks, Yun et al. (2019) were able to prove that indeed only (√n) neurons were sufﬁcient for memorization using deeper networks, the same question remained open for threshold networks. (max(n, d)) weights and ( (cid:100)
O
O
O 1The “general position” assumption of n vectors in R linearly independent. d means that any collection of d of these n vectors are 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Reference
Baum (1988)
Huang et al. (1991)
Vershynin (2020)
# Neurons
# Weights n/d ( (cid:100) (n) (e1/δ2
O
O
O
) (cid:101) logp(n) + √n log2.5(n))
O
O
O (nd) (e1/δ2 (max(n, d))
Assumptions
General Position
None (d + √n) + n log5(n))
δ-separated, points lie on the unit sphere
Ours, Theorem 1 log2 n
δ
+ √n log2 n (d + log n) log n
δ
+ n log2 n
δ-separated
O
O
Table 1: Table comparing the upper bounds for the parameters needed for a threshold network to memorize a dataset. General position assumption in Rd means that no more than d points lie on a d 1 dimensional hyperplane. Note: logp(n) denotes a poly-log factor in n. (cid:19) (cid:19) (cid:18) (cid:18)
− (e1/δ2
Recently, Vershynin (2020) was able to answer Baum’s question in the positive by proving that indeed
+ √n) neurons were sufﬁcient to memorize a set of n points on a unit sphere separated by
O a distance at least δ. Many recent works study the memorization power of neural networks under separation assumptions, e.g., Bubeck et al. (2020); Vershynin (2020); Park et al. (2020). One reason (cid:101) why before Vershynin’s work, it was unclear whether going deeper was helpful for threshold networks was that unlike ReLU or sigmoid functions, the threshold activation function, i.e., σ(z) = 1z 0, prohibits neurons from passing ‘amplitude information’ to the next layer.
≥
Although the dependence of O(√n) on the number of neurons was achieved by the work of Vershynin, it is unclear that the exponential dependence on distance and the requirement that the points lie on a sphere is fundamental. In this work, we lift the spherical requirement and offer an exponential improvement on the dependence on δ.
Theorem 1. (Informal) There exists a threshold neural network with d
δ + n (cid:1) (cid:0) weights that can memorize any δ-separated dataset of size n in d dimensions.
O
Please see Deﬁnition 2 for the formal deﬁnition of δ-separation; informally, it means that all the (cid:101) points have bounded norm and a distance of at least δ between them. Comparing the theorem above with Vershynin’s result, we see that ﬁrstly we have reduced the dependence on δ from exponential to nearly linear; and secondly, in our upper bound for the number of weights, the δ and n terms appear in summation rather than product (ignoring the logarithmic factors). Further, note that Vershynin (2020) needs the points to lie on a sphere, whereas we only need them to have a bounded (cid:96)2 norm.
We have compared our results with the existing work in Table 1. (cid:101) (cid:1) (cid:0) 1
δ + √n neurons and
O
O
Our construction has weights, which are i.i.d Gaussian. This layer has other layers have integer weights which are bounded and are on the of order
) layers, of which only the ﬁrst layer in our construction has real
) weights. All the ( d log n
δ (log n).
) neurons and ( log n
δ
O
O
O
δ (log log n n/d (cid:100)
Baum (1988) also proved that there exists a dataset of n points such that any threshold neural network neurons in the ﬁrst layer to memorize it. However, the distance for this would need at least (1/n).2 Later, Baum & Haussler (1989) proved VC-dimension bounds which imply that dataset is neurons are necessary in a network to memorize a dataset of n points in d dimensions, min
{ and this bound is independent of the minimum distance δ. Our upper bound (Theorem 1) shows that
) neurons in the ﬁrst layer. To complement if δ = Ω(1/n), then we can memorize with only our upper bound, we introduce a new, δ-dependent lower bound below:
O
√n, n/d
} ( log n
δ
O (cid:101)
Theorem 2. (Informal) There exists a δ-separated dataset of size n
∈ threshold network that can memorize it needs
Ω 1
√δ neurons in the ﬁrst layer. (cid:0) (cid:104) (cid:105) (cid:1) d2
δ , 1
δ d 2 such that any (cid:16)
The rest of the paper is divided into 7 sections. In Section 2, we provide the related works for memorization using neural networks. We provide deﬁnitions and notation in Section 3 and our main results in Section 4. Then, we brieﬂy explain the constructions of our upper bound in Section 5. (cid:17) (cid:101) 2Here, we have rescaled the dataset to have maximum norm of any sample to be 1. This is done to make the dataset consistent with our minimum distance deﬁnition (see Assumption 2). The assumption in Park et al. (2020) also uses this kind of normalization. 2
Before exploring lower bounds for threshold networks, we provide sharp bounds on the minimum parameterization needed for any model (not necessarily a neural network) to memorize a δ-separated dataset, in Section 6. We discuss our lower bound for threshold networks in Section 7. Finally, we conclude our paper with a brief conclusion in Section 8. 2