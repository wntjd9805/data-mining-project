Abstract
Several works in implicit and explicit generative modeling empirically observed that feature-learning discriminators outperform ﬁxed-kernel discriminators in terms of the sample quality of the models. We provide separation results between probability metrics with ﬁxed-kernel and feature-learning discriminators using the function classes F2 and F1 respectively, which were developed to study overparametrized two-layer neural networks. In particular, we construct pairs of distributions over hyper-spheres that can not be discriminated by ﬁxed kernel (F2) integral probability metric (IPM) and Stein discrepancy (SD) in high dimensions, but that can be discriminated by their feature learning (F1) counterparts. To further study the separation we provide links between the F1 and F2 IPMs with sliced Wasserstein distances. Our work suggests that ﬁxed-kernel discriminators perform worse than their feature learning counterparts because their corresponding metrics are weaker. 1

Introduction
The ﬁeld of generative modeling, whose aim is to generate artiﬁcial samples of some target distri-bution given some true samples of it, is broadly divided into two types of models (Mohamed and
Lakshminarayanan, 2017): explicit generative models, which involve learning an estimate of the log-density of the target distribution which is then sampled (e.g. energy-based models), and implicit generative models, where samples are generated directly by transforming some latent variable (e.g. generative adversarial networks (Goodfellow et al., 2014), normalizing ﬂows (Rezende and Mohamed, 2015)).
Several works have observed experimentally that both in implicit and in explicit generative models, using ‘adaptive’ or ‘feature-learning’ function classes as discriminators yields better generative performance than ‘lazy’ or ‘kernel’ function classes. Within implicit models, Li et al. (2017) show that generative moment matching networks (GMMN) generate signiﬁcantly better samples for the
CIFAR-10 and MNIST datasets when using maximum mean discrepancy (MMD) with learned instead of ﬁxed features. For a related method and in a similar spirit, Santos et al. (2019) show that for image generation, ﬁxed-feature discriminators are only successful when we take an amount of features exponential in the intrinsic dimension of the dataset. Genevay et al. (2018) study implicit generative models with the Sinkhorn divergence as discriminator, and they also show that other than for simple datasets like MNIST, learning the Wasserstein cost is crucial for good performance.
As to explicit models, Grathwohl et al. (2020) train energy-based models with a Stein discrepancy based on neural networks and show improved performance with respect to kernel classes. Chang et al. (2020) show that Stein variational gradient descent (SVGD) fails in high dimensions, and that learning the kernel helps. Given the abundant experimental evidence, the aim of this work is to provide some theoretical results that showcase the advantages of feature-learning over kernel discriminators. For 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the sake of simplicity, we compare the discriminative behavior of two function classes F1 and F2, arising from inﬁnite-width two-layer neural networks with different norms penalties on its weights (Bach, 2017). F1 displays an adaptive behavior, while F2 is an RKHS which consequently has a lazy behavior. Namely, our main contributions are: (i) We construct a sequence of pairs of distributions over hyperspheres Sd−1 of increasing dimensions, such that the F2 integral probability metric (IPM) between the pair decreases exponentially in the dimension, while the F1 IPM remains high. (ii) We construct a sequence of pairs of distributions over Sd−1 such that the F2 Stein discrepancy (SD) between the pair decreases exponentially in the dimension, while the F1 SD remains high. (iii) We prove polynomial upper and lower bounds between the F1 IPM and the max-sliced Wasserstein distance for distributions over Euclidean balls. For a class ˜F2 related to F2, we prove similar upper and lower bounds between the ˜F2 IPM and the sliced Wasserstein distance for distributions over Euclidean balls.
Our ﬁndings reinforce the idea that generative models with kernel discriminators have worse per-formance because their corresponding metrics are weaker and thus unable to distinguish between different distributions, especially in high dimensions. 2