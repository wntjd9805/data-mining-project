Abstract
Researchers often face data fusion problems, where multiple data sources are available, each capturing a distinct subset of variables. While problem formulations typically take the data as given, in practice, data acquisition can be an ongoing process. In this paper, we aim to estimate any functional of a probabilistic model (e.g., a causal effect) as efﬁciently as possible, by deciding, at each time, which data source to query. We propose online moment selection (OMS), a framework in which structural assumptions are encoded as moment conditions. The optimal action at each step depends, in part, on the very moments that identify the functional of interest. Our algorithms balance exploration with choosing the best action as suggested by current estimates of the moments. We propose two selection strategies: (1) explore-then-commit (OMS-ETC) and (2) explore-then-greedy (OMS-ETG), proving that both achieve zero asymptotic regret as assessed by MSE. We instantiate our setup for average treatment effect estimation, where structural assumptions are given by a causal graph and data sources may include subsets of mediators, confounders, and instrumental variables. 1

Introduction
Statistical and causal modeling typically proceed from the assumption that we already know which variables are (and are not) observed. However, this perspective fails to address the difﬁcult data collection decisions that precede such modeling efforts. Doctors must select a set of tests to run.
Survey designers must select a slate of questions to ask. Companies must select which datasets to purchase. Whether or not we model these decisions, they pervade the practice of data science, inﬂuencing both what questions we can ask and how accurately we can answer them.
One might ask, why not collect everything? The answers are two-fold: First, data acquisition can be expensive. In a medical setting, blood tests can cost anywhere from tens to thousands of dollars.
Running every test for every patient is infeasible. Likewise, space on surveys is limited, and asking every conceivable question of every respondent is infeasible. Second, in many settings, we lack complete control over the set of variables observed. Instead, we might have access to multiple data sources, each capturing a different subset of variables. Such data fusion problems pervade economic modeling and public health, and present interesting challenges: (i) efﬁciently estimating (or even identifying) a population parameter of interest often requires intelligently combining data from multiple sources; (ii) data collection is often iterative, with tentative conclusions at each stage informing choices about what data to collect next.
In this paper, we formalize the sequential problem of deciding, at each time, which data source to query (i.e., what to observe) in order to efﬁciently estimate a target parameter. We propose online moment selection (OMS), a framework that applies the generalized method of moments (GMM) [17] both to estimate the parameter and to decide which data sources to query. This framework can be 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
applied to estimate any statistical parameter that can be identiﬁed by a set of moment conditions. For example, OMS can address (i) any (regular) maximum likelihood estimation problem [13, Page 109]; and (ii) estimating average treatment effects (ATEs) using instrumental variables (IVs), backdoor adjustment sets, mediators, and/or other identiﬁcation strategies.
Our strategy requires only that the agent has sufﬁcient structural knowledge to formulate the set of moment conditions and that each moment can be estimated using the variables returned by at least one of the data sources. Interestingly, the optimal decisions which lead to estimates with the lowest mean squared error (MSE) depend on the (unknown) model parameters. This motivates our adaptive strategy: as we collect more data, we better estimate the underlying parameters, improving our strategy for allocating our remaining budget among the available data sources.
We ﬁrst address the setting where the cost per instance is equal across data sources (Section 4). First, we show that any ﬁxed policy that differs from the oracle suffers constant asymptotic regret, as assessed by MSE. We then overcome this limitation by proposing two adaptive strategies—explore-then-commit (OMS-ETC) and explore-then-greedy (OMS-ETG)—both of which choose data sources based on the estimated asymptotic variance of the target parameter.
Under OMS-ETC, we use some fraction of the sample budget to explore randomly, using the collected data to estimate the model parameters. We then exploit the current estimated model, collecting the remaining samples according to the fraction expected to minimize our estimator’s asymptotic variance.
In OMS-ETG, we continue to update our parameter estimates after every step as we collect new data.
We prove that both policies achieve zero asymptotic regret. To overcome the non-i.i.d. nature of the sample moments, we draw upon martingale theory. To derive zero asymptotic regret, we show uniform concentration of sample moments and a ﬁnite-sample concentration inequality for the GMM estimator with dependent data. Next, we adapt OMS-ETC and OMS-ETG to handle heterogeneous costs over the data sources (Section 5) and prove that they still have zero asymptotic regret.
Finally, we validate our ﬁndings experimentally 1 (Section 6). Motivated by ATE estimation in causal models encoded as directed acyclic graphs, we generate synthetic data from a variety of causal graphs and show that the regret of our proposed methods converges to zero. Furthermore, we see that despite being asymptotically equivalent, OMS-ETG outperforms OMS-ETC in ﬁnite samples.
Finally, we demonstrate the effectiveness of our methods on two semi-synthetic datasets: the Infant
Health Development Program (IHDP) dataset [19] and a Vietnam era draft lottery dataset [2]. 2