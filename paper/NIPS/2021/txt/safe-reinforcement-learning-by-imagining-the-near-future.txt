Abstract
Safe reinforcement learning is a promising path toward applying reinforcement learning algorithms to real-world problems, where suboptimal behaviors may lead to actual negative consequences. In this work, we focus on the setting where unsafe states can be avoided by planning ahead a short time into the future. In this setting, a model-based agent with a sufﬁciently accurate model can avoid unsafe states. We devise a model-based algorithm that heavily penalizes unsafe trajectories, and derive guarantees that our algorithm can avoid unsafe states under certain assumptions. Experiments demonstrate that our algorithm can achieve competitive rewards with fewer safety violations in several continuous control tasks. 1

Introduction
Reinforcement learning (RL) enables the discovery of effective policies for sequential decision-making tasks via trial and error [Mnih et al., 2015, Gu et al., 2016, Bellemare et al., 2020]. However, in domains such as robotics, healthcare, and autonomous driving, certain kinds of mistakes pose danger to people and/or objects in the environment. Hence there is an emphasis on the safety of the policy, both at execution time and while interacting with the environment during learning. This issue, referred to as safe exploration, is considered an important problem in AI safety [Amodei et al., 2016].
In this work, we advocate a model-based approach to safety, meaning that we estimate the dynamics of the system to be controlled and use the model for planning (or more accurately, policy improvement).
The primary motivation for this is that a model-based method has the potential to anticipate safety violations before they occur. Often in real-world applications, the engineer has an idea of what states should be considered violations of safety: for example, a robot colliding rapidly with itself or surrounding objects, a car driving on the wrong side of the road, or a patient’s blood glucose levels spiking.Yet model-free algorithms typically lack the ability to incorporate such prior knowledge and must encounter some safety violations before learning to avoid them.
We begin with the premise that in practice, forward prediction for relatively few timesteps is sufﬁcient to avoid safety violations. Consider the illustrative example in Figure 1, in which an agent controls the acceleration (and thereby, speed) of a car by pressing the gas or brake (or nothing). Note that there is an upper bound on how far into the future the agent would have to plan to foresee and (if possible) avoid any collision, namely, the amount of time it takes to bring the car to a complete stop.
Assuming that the horizon required for detecting unsafe situations is not too large, we show how to construct a reward function with the property that an optimal policy will never incur a safety violation. A short prediction horizon is also beneﬁcial for model-based RL, as the well-known issue of compounding error plagues long-horizon prediction [Asadi et al., 2019]: imperfect predictions are fed back into the model as inputs (possibly outside the distribution of inputs in the training data), leading to progressively worse accuracy as the prediction horizon increases. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: An illustrative example. The agent controls the speed of a car by pressing the accelerator or brake (or neither), attempting to avoid any obstacles such as other cars or people in the road. The top car has not yet come into contact with the pedestrian, but cannot avoid the pedestrian from its current position and speed, even if it brakes immediately. The bottom car can slow down before hitting the pedestrian. If the bottom car plans several steps into the future, it could reduce its speed to avoid the
“irrecoverable” situation faced by the top car.
Our main contribution is a model-based algorithm that utilizes a reward penalty – the value of which is prescribed by our theoretical analysis – to guarantee safety (under some assumptions). Experiments indicate that the practical instantiation of our algorithm,
Safe Model-Based Policy Optimization (SMBPO), effectively reduces the number of safety viola-tions on several continuous control tasks, achieving a comparable performance with far fewer safety violations compared to several model-free safe RL algorithms. Code is made available at https://github.com/gwthomas/Safe-MBPO. 2