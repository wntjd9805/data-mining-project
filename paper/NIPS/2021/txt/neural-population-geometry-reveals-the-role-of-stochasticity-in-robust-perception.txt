Abstract
Adversarial examples are often cited by neuroscientists and machine learning re-searchers as an example of how computational models diverge from biological sensory systems. Recent work has proposed adding biologically-inspired compo-nents to visual neural networks as a way to improve their adversarial robustness.
One surprisingly effective component for reducing adversarial vulnerability is re-sponse stochasticity, like that exhibited by biological neurons. Here, using recently developed geometrical techniques from computational neuroscience, we investigate how adversarial perturbations inﬂuence the internal representations of standard, ad-versarially trained, and biologically-inspired stochastic networks. We ﬁnd distinct geometric signatures for each type of network, revealing different mechanisms for achieving robust representations. Next, we generalize these results to the auditory domain, showing that neural stochasticity also makes auditory models more robust to adversarial perturbations. Geometric analysis of the stochastic networks reveals overlap between representations of clean and adversarially perturbed stimuli, and quantitatively demonstrates that competing geometric effects of stochasticity medi-ate a tradeoff between adversarial and clean performance. Our results shed light on the strategies of robust perception utilized by adversarially trained and stochastic networks, and help explain how stochasticity may be beneﬁcial to machine and biological computation.1 1

Introduction
In recent years, artiﬁcial neural networks (ANNs) have come to dominate both visual object recog-nition and auditory recognition tasks [1, 2, 3], establishing them as leading candidate models for several domains of human perception [4, 5, 6, 7]. However, they still exhibit many non-human-like traits [8, 9]. One such failure is in the existence of adversarial perturbations – small changes to stimuli explicitly crafted to fool a model that remain imperceptible to humans [10, 11, 12] – which demonstrate the fragility of some ANNs as models of biological perception. 1See https://github.com/chung-neuroai-lab/adversarial-manifolds for accompanying code. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recently, Dapello, Marques et al. discovered that one such method, known as adversarial training
[13], not only reduces the network’s adversarial vulnerability but also yields network representations that are more similar to those in the primate primary visual cortex [14]. Motivated by this result, the authors developed VOneNets, a class of networks that simulate the primate primary visual cortex at the front of a convolutional neural network, and show improved robustness to adversarial attacks with no adversarial training. However, a number of questions remain unanswered – in particular, while both adversarially trained networks and VOneNets have improved adversarial robustness and also greater similarity to the primate primary visual cortex, it is unclear how VOneNets achieve robustness, and in particular if the mechanism of robustness is similar to that induced by adversarial training.
A key component of robustness in VOneNets is the inclusion of stochastic representations during both training and inference, a feature inspired by biological sensory neurons which exhibit trial-to-trial variability across presentations of the same stimulus [15]. The implications of this stochasticity for information processing are open questions in neuroscience [16, 17, 18, 19]. Pinpointing how this representational stochasticity contributes to robustness in VOneNets could drive further developments in the mechanisms of robust perception.
Here we use recently developed manifold analysis techniques from computational neuroscience
[20] to look beyond accuracy and investigate the internal neural population geometry [21] of stan-dard, adversarially trained, and biologically-inspired stochastic networks in response to clean and adversarially perturbed examples in both visual and auditory domains. We present several key
ﬁndings:
• Using manifold analysis, we demonstrate that standard, adversarially trained, and stochastic networks each have distinct geometric signatures in response to clean and adversarially perturbed stimuli, shedding light on varied robustness mechanisms.
• We demonstrate the generality of our ﬁndings by translating the results to a novel biologically-inspired auditory ANN, StochCochResNet50, that includes stochastic responses.
Stochasticity makes auditory networks more robust to adversarial perturbations, and the underlying neural population geometry is largely consistent with that in vision networks.
• Analysis of stochastic networks reveals a protective overlap between the representations of adversarial examples and clean stimuli. We quantitatively survey the stochasticity conditions leading to the overlap, and map a competing geometric effect mediating a trade-off between clean and adversarial performance. 2