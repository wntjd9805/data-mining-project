Abstract
We study online learning for optimal allocation when the resource to be allocated is time. An agent receives task proposals sequentially according to a Poisson process and can either accept or reject a proposed task. If she accepts the proposal, she is busy for the duration of the task and obtains a reward that depends on the task duration. If she rejects it, she remains on hold until a new task proposal arrives. We study the regret incurred by the agent, ﬁrst when she knows her reward function but does not know the distribution of the task duration, and then when she does not know her reward function, either. This natural setting bears similarities with contextual (one-armed) bandits, but with the crucial difference that the normalized reward associated to a context depends on the whole distribution of contexts. 1

Introduction
Motivation. A driver ﬁlling her shift with rides, a landlord renting an estate short-term, an inde-pendent deliveryman, a single server that can make computations online, a communication system receiving a large number of calls, etc. all face the same trade-off. There is a unique resource that can be allocated to some tasks/clients for some duration. The main constraint is that, once it is allocated, the resource becomes unavailable for the whole duration. As a consequence, if a “better” request arrived during this time, it could not be accepted and would be lost. Allocating the resource for some duration has some cost but generates some rewards – possibly both unknown and random. For instance, an estate must be cleaned up after each rental, thus generating some ﬁxed costs; on the other hand, guests might break something, which explains why these costs are both random and unknown beforehand. Similarly, the relevance of a call is unknown beforehand. Concerning duration, the shorter the request the better (if the net reward is the same). Indeed, the resource could be allocated twice in the same amount of time.
The ideal request would therefore be of short duration and large reward; this maximizes the revenue per time. A possible policy could be to wait for this kind of request, declining the other ones (too long and/or less proﬁtable). On the other hand, such a request could be very rare. So it might be more rewarding in the long run to accept any request, at the risk of “missing” the ideal one. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Some clear trade-offs arise. The ﬁrst one is between a greedy policy that accepts only the highest proﬁtable requests – at the risk of staying idle quite often – and a safe policy that accepts every request – but unfortunately also the non-proﬁtable ones. The second trade-off concerns the learning phase; indeed, at ﬁrst and because of the randomness, the actual net reward of a request is unknown and must be learned on the ﬂy. The safe policy will gather a lot of information (possibly at a high cost) while the greedy one might lose some possible valuable information for the long run (in trying to optimize the short term revenue).
We adopt the perspective of an agent seeking to optimize her earned income for some large duration.
The agent receives task proposals sequentially, following a Poisson process. When a task is proposed, the agent observes its expected duration and can then either accept or reject it. If she accepts it, she cannot receive any new proposals for the whole duration of the task. At the end of the task she observes her reward, which is a function of the duration of the task. If, on the contrary, she rejects the task, she remains on hold until she receives a new task proposal.
The agent’s policies are evaluated in terms of their expected regret, which is the difference between the cumulative rewards obtained until T under the optimal policy and under the implemented agent policy (as usual the total length could also be random or unknown (Degenne and Perchet, 2016)).
In this setting, the “optimal” policy is within the class of policies that accept – or not – tasks whose length belongs to some given acceptance set (say, larger than some threshold, or in some speciﬁc
Borel subset, depending on the regularity of the reward function).
Organization and main contributions.
In Section 2, we formally introduce the model and the problem faced by an oracle who knows the distribution of task durations as well as the reward function (quite importantly, we emphasize again that the oracle policy must be independent of the realized rewards). Using continuous-time dynamic programming principles, we construct a quasi-optimal policy in terms of accepted and rejected tasks: this translates into a single optimal threshold for the ratio of the reward to the duration, called the proﬁtability function. Any task with a proﬁtability above this threshold is accepted, and the other ones are declined. As a benchmark, we ﬁrst assume in
Section 3 that the agent knows the reward function r(·), but ignores the distribution of task durations.
The introduced techniques can be generalized, in the following sections, to further incorporate estimations of r(·). In that case, our base algorithm has a regret scaling as O(
T ); obviously, this cannot be improved without additional assumptions, ensuring minimax optimality. In Section 4, the reward function is not known to the agent anymore and the reward realizations are assumed to be noisy. To get non-trivial estimation rates, regularity – i.e., (L, β)-Hölder – of the reward function is assumed. Modifying the basic algorithm to incorporate non-parametric estimation of r yields a regret scaling as O(T 1−η ln T ) where η = β/(2β + 1). As this is the standard error rate in classiﬁcation (Tsybakov, 2006), minimax optimality (up to log-term) is achieved again. Finally, our different algorithms are empirically evaluated on simple toy examples in Section 5. Due to space constraints, all the proofs are deferred to the Appendix.
√
√