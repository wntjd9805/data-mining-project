Abstract
Theoretical analyses for graph learning methods often assume a complete obser-vation of the input graph. Such an assumption might not be useful for handling any-size graphs due to the scalability issues in practice. In this work, we develop a theoretical framework for graph classiﬁcation problems in the partial observation setting (i.e., subgraph samplings). Equipped with insights from graph limit theory, we propose a new graph classiﬁcation model that works on a randomly sampled subgraph and a novel topology to characterize the representability of the model.
Our theoretical framework contributes a theoretical validation of mini-batch learn-ing on graphs and leads to new learning-theoretic results on generalization bounds as well as size-generalizability without assumptions on the input. 1

Introduction
Going beyond regular structural inputs such as grids (images), sequences (time series, sentences), or general feature vectors is an important research direction of machine learning and computational sciences. Arguably, most interesting objects and problems in nature can be described as graphs [37].
For such reason, graph learning methods, especially Graph Neural Networks (GNN) [60], have recently proven to be a useful solution to many problems in computer vision [10, 12, 24, 63], complex network analyses [22, 30, 73], molecule modeling [17, 32, 41, 45], and physics simulations [3, 31, 54].
The signiﬁcant value of graph learning models in practice has inspired a large amount of theoretical work dedicated to exploring their representational limits and the possibilities of improving them.
Most notably, the representational capability of GNNs has been in the spotlight of recent years.
To answer the question “Can GNNs approximate all functions on graphs?”, researchers discussed universal invariant and equivariant neural networks [27, 40, 43, 51] as theoretical upper limits for neural architectures or showed the correspondence between message-passing GNNs (MP-GNNs) to the Weisfeiler-Leman (WL) algorithm [68] as practical upper limits [46, 70].
Given an extremely large graph as an input, it is often impractical to keep the whole graph in the working memory. Therefore, practical graph learning methods often utilize neighborhood samplings [22, 73] or random walks [53] to handle this scalability issue. Because existing analyses assumed a complete observation of the input graphs [27, 51, 57], it is unclear what can be learned if we combine graph learning models with random samplings. Thus, the relevant question in this scenario is “What graph functions are representable by GNNs when we can only observe random neighborhoods?.” This question adds another dimension to the discussion of GNN expressivity; even if we have a powerful GNN (in both theoretical and practical senses), what kind of graph functions can we learn if the input graphs are too large to be computed as a whole?
Contributions This study proposes a theoretical approach to address graph learning problems on large graphs by identifying a novel topology of the graph space. We discuss the graph classiﬁcation 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
problem in the main part of the paper and extend the discussion to the vertex classiﬁcation problem in Appendix C. The extension to vertex classiﬁcation can be realised by viewing it as a rooted-graph classiﬁcation problem. We ﬁrst introduce a random ball sampling GNN (RBS-GNN), which is a mathematical model of GNNs implementable in a random neighborhood computational model, and prove that the model is universal in the class of estimable functions (Theorem 4). Our main contribution is introducing randomized Benjamini–Schramm topology in the space of all graphs and identifying the estimability of the function as the uniform continuity in this topology (Theorem 7).
By applying our main theorem, we obtain the following learning-theoretic results.
• We show the equivalent of estimability and continuity (Theorems 4 and 7). This implies the continuity assumption is a sufﬁcient condition for the mini-batch learning on graphs.
• We prove that the functions representable by RBS-GNNs are generalizable by showing an upper bound of the Rademacher complexity of Lipschitz graph functions (Theorem 10).
• We identify size-generalizable functions with estimable functions (Theorem 11). Then, by recognizing the size-generalization as a domain adaptation, we provide a size-generalization error based on the Wasserstein distance (Theorem 13).
Unlike existing studies, which assumed a random graph model [26, 28] or boundedness [11, 27, 51, 58], our framework does not assume anything about the graph class; instead, we assume the continuity of the graph functions. Our results listed above are model-agnostic, i.e., we only discuss the property of the function space, regardless of how GNN models are implemented. The model-agnostic nature of our results gives a systematic view to general graph parameters learning; their generality is especially useful as there are many different GNN architectures in practice [57, 69, 78]. 2