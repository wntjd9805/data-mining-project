Abstract
Vision-Language Pre-training (VLP) aims to learn multi-modal representations from image-text pairs and serves for downstream vision-language tasks in a fine-tuning fashion. The dominant VLP models adopt a CNN-Transformer architecture, which embeds images with a CNN, and then aligns images and texts with a
Transformer. Visual relationship between visual contents plays an important role in image understanding and is crucial for inter-modal alignment learning in VLP.
However, CNNs have limitations in visual relation learning due to local receptive field’s weakness in modeling long-range dependencies. Thus the two objectives of learning visual relation and inter-modal alignment are encapsulated in the same
Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by neglecting the specialized characteristic of each objective. To tackle this challenge, we propose a fully Transformer visual embedding for VLP to better learn visual relation and further promote inter-modal alignment. Specifically, we propose a metric named Inter-Modality Flow (IMF) to measure the interaction between vision and language (i.e., inter-modality). We also design a novel masking optimization mechanism named Masked Feature Regression (MFR) in Transformer to further promote the inter-modality learning. To the best of our knowledge, this is the first study to explore the benefit of Transformer for visual feature learning in
VLP. We verify our method on a wide range of vision-language tasks, including
Image-Text Retrieval, Visual Question Answering (VQA), Visual Entailment and
Visual Reasoning. The result shows that our approach not only outperforms the state-of-the-art VLP models, but also exhibits superiority on the IMF metric. 1

Introduction
Vision-Language Pre-training (VLP) has shown great benefits for Visual-Language (VL) tasks such as Visual Question Answering (VQA), Visual Entailment, etc., in many recent works [9, 21, 31, 32, 36, 40, 42, 51]. VLP is designed to learn vision and language (VL) joint representation and alignment with a huge number of image-text pairs. It provides a powerful initialization of multi-modal representation for downstream VL tasks in a fine-tuning way. In this paper, we explore how to improve the multi-modal representation which is the key challenge in VLP from the perspective of intra-modal and inter-modal learning.
∗This work was performed when Hongwei Xue and Yupan Huang were visiting Microsoft Research as research interns. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Existing works use image features as visual tokens and learn their alignments with language tokens using a multi-modal Transformer. Three main types of image representation are commonly used in VLP: region feature, grid feature and patch projection. Most VLP models [9, 42, 40] extract region-based image features with an off-the-shelf object detector [34]. Each visual token in VLP corresponds to a pre-defined region feature. Some recent works directly learn grid features from images by CNNs to lift restrictions of bounding boxes and pre-defined object categories [22, 21].
They train CNN and multi-modal Transformer in an end-to-end fashion to enable visual features optimized for pre-training objectives. In addition to use CNNs for visual feature learning, a recent work ViLT [28] directly input image patch projections into the multi-modal Transformer to achieve the fastest inference speed with the lightest VLP architecture.
In vision-language (VL) tasks, the relation between visual concepts is crucial. For example, given a question “what is the man doing?” for an image of a surfing man, a VL model is expected to infer the relation of “surfing” from object “man” and object “surfboard”. However, existing three types of image representations fail to model the intra-vision relation. For both region feature and patch projection, each unit (i.e., bounding box or patch) is independent while global relations are not encoded in the visual embedding. For grid feature learned by CNNs, the local receptive field in the convolution layer results in local features of neighbor regions. Thus the two objectives of VLP, i.e., learning visual relation and inter-modal alignment, are encapsulated in the multi-modal Transformer network. Such design might restrict the inter-modal alignment learning in the Transformer by ignoring the specialized characteristic of each objective. On the other hand, for language, texts are highly structured and relations are explicitly provided by grammar. This inconsistent representation of different modalities distracts the multi-modal Transformer from the inter-modal alignment.
To better learn visual relation and further promote inter-modal alignment, we propose a fully Trans-former VLP model which adopts self-attention for visual feature learning. Self-attention breaks the spatial inductive bias and enables long-range global relation learning of visual features. Thus, multi-modal Transformer can be specialized for multi-modal joint learning. In self-attention mech-anism, each visual token is an approximate weighted mixture of all tokens. The higher weight indicates higher dependency. We name this way of image feature learning as visual parsing. As visual parsing provides dependencies of each visual token pair, inter-modality learning can be further promoted by masking visual tokens with high dependency, forcing the multi-modal Transformer to have more attention on the language side. To the best of our knowledge, our proposed Masked Feature
Regression (MFR) is first ranking-based masking mechanism in VLP that leverages the intrinsic properties of visual extractor. To promote the inter-modality learning, we propose the Inter-Modality
Flow (IMF) metric based on Attention Flow [1] to measure the fusion of inter-modality in VLP. IMF aims to quantify the information flow between the two modalities.
To verify the effectiveness of our approach, we conduct experiments on a wide range of vision-language tasks that are highly related to visual relation understanding and inter-modal reasoning. Our approach not only outperforms the state-of-the-art VLP models, but also shows superiority on the pro-posed IMF metric. We also conduct extensive ablation studies to demonstrate the effectiveness of our proposed self-attention visual parsing and parsing-based masking mechanism. We thoroughly probe the inter-modality learning in VLP from the perspective of information flow and data distribution.
Our probing reveals how vision and language fuse with each other.
Our contributions in this paper are summarized as follows: 1 We are the first to adopt self-attention to learn visual features for VLP, aiming to promote inter-modality learning in multi-modal Transformer. Our model outperforms existing works on a wide range of vision-language tasks. 2 We propose a novel Inter-Modality Flow (IMF) metric to measure and reveal vision and language fusion in VLP. 3 We design the first ranking-based masking mechanism for visual self-attention module to further promote inter-modality learning, verified by well-designed ablation studies. 2