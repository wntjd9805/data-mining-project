Abstract
Policies trained via Reinforcement Learning (RL) are often needlessly complex, making them difﬁcult to analyse and interpret. In a run with n time steps, a policy will make n decisions on actions to take; we conjecture that only a small subset of these decisions delivers value over selecting a simple default action. Given a trained policy, we propose a novel black-box method based on statistical fault localisation that ranks the states of the environment according to the importance of decisions made in those states. We argue that among other things, the ranked list of states can help explain and understand the policy. As the ranking method is statistical, a direct evaluation of its quality is hard. As a proxy for quality, we use the ranking to create new, simpler policies from the original ones by pruning decisions identiﬁed as unimportant (that is, replacing them by default actions) and measuring the impact on performance. Our experiments on a diverse set of standard benchmarks demonstrate that pruned policies can perform on a level comparable to the original policies. Conversely, we show that naive approaches for ranking policy decisions, e.g., ranking based on the frequency of visiting a state, do not result in high-performing pruned policies. 1

Introduction
Reinforcement learning is a powerful method for training policies that complete tasks in complex environments. The policies produced are optimised to maximise the expected reward provided by the environment. While performance is clearly an important goal, the reward typically does not capture the entire range of our preferences. By focusing solely on performance, we risk overlooking the demand for models that are easier to analyse, predict and interpret [16]. Our hypothesis is that many trained policies are needlessly complex, i.e., that there exist alternative policies that perform just as well or nearly as well but that are signiﬁcantly simpler. This tension between performance and simplicity is central to the ﬁeld of explainable AI (XAI), and machine learning as a whole [11]; our method aims to help by highlighting the most important parts of a policy.
∗The work in this paper was done while at the University of Oxford.
†The work in this paper was done prior to joining Amazon. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The starting point for our deﬁnition of “simplicity” is the assumption that there exists a way to make a “simple choice”, that is, there is a simple default action for the environment. We argue that this is the case for many environments in which RL is applied: for example, “repeat previous action” is a straightforward default action for navigation tasks.
The key contribution of this paper is a novel method for ranking policy decisions according to their importance relative to some goal. We argue that the ranked list of decisions is already helpful in explaining how the policy operates. We evaluate our ranking method by using the ranking to simplify policies without compromising performance, hence addressing one of the main hurdles for wide adoption of deep RL: the high complexity of trained policies.
We produce a ranking by scoring the states a policy visits. The rank reﬂects the impact that replacing the policy’s chosen action by the default action has on a user-selected binary outcome, such as
“obtain more than X reward”. It is intractable to compute this ranking precisely, owing to the high complexity and the stochasticity of the environment and the policy, complex causal interactions between actions and their outcomes, and the sheer size of the problem. Our work uses spectrum-based fault localisation (SBFL) techniques [20, 31], borrowed from the software testing domain, to compute an approximation of the ranking of policy decisions. SBFL is an established technique in program testing for ranking the parts of a program source code text that are most likely to contain the root cause of a bug. This ranking is computed by recording the executions of a user-provided test suite.
SBFL distinguishes passing and failing executions; failing executions are those that exhibit the bug.
Intuitively, a program location is more likely to be the root cause of the bug if it is visited in failing executions but less (or not at all) in passing ones. SBFL is a lightweight technique and its rankings are highly correlated with the location of the root cause of the bug [31]. We argue that SBFL is also a good ﬁt for analysing complex RL policies.
Our method applies to RL policies in a black-box manner, and requires no assumptions about the policy’s training or representation. We evaluate the quality of the ranking of the decisions by the proxy of creating new, simpler policies (we call them “pruned policies”) without retraining, and then calculate the reward achieved by these policies. Experiments with agents for MiniGrid (a more complex version of gridworlds) [7], CartPole [4] and a number of Atari games [4] demonstrate that pruned policies maintain high performance (similar or only slightly worse than that of the original policy) when taking the default action in the majority of the states (often 90% of the states). As pruned policies are much easier to understand than the original policies, we consider this an important step towards explainable RL. Pruning a given policy does not require re-training, and hence, our procedure is relatively lightweight. Furthermore, the ranking of states by itself provides important insight into the importance of particular decisions for the performance of the policy overall.
The code for reproducing our experiments is available on GitHub3, and further examples are provided on the project website4. 2