Abstract
Learning to execute algorithms is a fundamental problem that has been widely studied. Prior work [1] has shown that to enable systematic generalisation on graph algorithms it is critical to have access to the intermediate steps of the pro-gram/algorithm. In many reasoning tasks, where algorithmic-style reasoning is important, we only have access to the input and output examples. Thus, inspired by the success of pre-training on similar tasks or data in Natural Language Pro-cessing (NLP) and Computer Vision, we set out to study how we can transfer algorithmic reasoning knowledge. Speciﬁcally, we investigate how we can use algorithms for which we have access to the execution trace to learn to solve similar tasks for which we do not. We investigate two major classes of graph algorithms, parallel algorithms such as breadth-ﬁrst search and Bellman-Ford and sequential greedy algorithms such as Prim and Dijkstra. Due to the fundamental differences between algorithmic reasoning knowledge and feature extractors such as used in
Computer Vision or NLP, we hypothesise that standard transfer techniques will not be sufﬁcient to achieve systematic generalisation. To investigate this empirically we create a dataset including 9 algorithms and 3 different graph types. We validate this empirically and show how instead multi-task learning can be used to achieve the transfer of algorithmic reasoning knowledge. 1

Introduction
Transfer learning [2] has been responsible for signiﬁcant successes in multiple areas of machine learning, including Natural Language Processing (NLP) [3, 4] and Computer Vision (CV) [5]. Pre-training and reusing the learned weights, by freezing them as feature extractors, or ﬁne-tuning from them as an initialisation, are common approaches to transfer in these domains. This has enabled successful learning on problems where data is limited in some form.
Algorithmic reasoning on graphs [1, 6] is a fundamental problem that has been studied under the assumption that we have access to the execution traces of the algorithms we want to learn. In practice, such as many real world reasoning tasks, this may not be true. Due to the limited data, we look to transfer learning to enable us to solve algorithmic tasks without intermediate steps. Speciﬁcally, we
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
investigate how to transfer knowledge between similar graph algorithms in the setting where we have access to the execution traces for some (e.g. PRIM [7, 8]), but not others (e.g. DIJKSTRA [9]). This has not been explored before, but is important as many reasoning related ﬁelds care deeply about systematic generalisation, while only having input-output pairs to learn from. One such example is knowledge graph link prediction. There exists a reasoning process that can answer the question: what is the tail entity given the head entity and the relation. However, we do not have access to its execution trace, i.e. the step-by-step deductions of the reasoning process, but we do have input-output pair examples. We envision that the set-up studied and the direction proposed in this paper will help to learn neural networks that can solve reasoning-style problems by using other reasoning knowledge as an inductive bias. A slightly different, but related application may be when the data the graph algorithm needs to operate on is encoded in a high-level space. This is a scenario encountered in reinforcement learning and concurrent work Deac et al. [10] has already started investigating the process of pre-training an encoder with graph algorithms.
Veliˇckovi´c et al. [1] successfully trained graph neural networks (GNNs) [11, 12, 13, 14] to execute graph algorithms. Two key ingredients were access to the intermediate steps of the algorithm and algorithmic alignment [15]. Algorithmic alignment [15] refers to the concept of a computation structure—in our case neural networks—and the structure of an algorithm ‘aligning’—in this paper graph algorithms. ‘Alignment’ means there exists a mapping between substructures of the computation architecture and simple substeps of the algorithm, where the substructures can ‘easily’ compute the corresponding substep they are mapped to—for some deﬁnition of easy, usually linear.
Extending this work, we enable solving tasks without access to the intermediate steps using other similar algorithms as an inductive bias. To do this we add several new algorithms. Further, since the algorithms studied in [1] were all expressible with only linear components in the GNN, we extend their architecture with a more expressive encoder to enable algorithmic alignment with more tasks.
One assumption we make is that the algorithms are similar (e.g. both PRIM and DIJKSTRA greedily select nodes from a priority queue). This shared algorithmic knowledge should serve as an inductive bias to enable a neural network to systematically generalise, even when learning only on input-output pairs. We hypothesise that due to differences between feature extraction and algorithmic reasoning, successful approaches to transfer as used in CV and NLP will provide only minimal improvements when transferring from one base algorithm to a target algorithm. While features across images may be similar, the features of algorithms differs (e.g. shortest distance in DIJKSTRA and lightest edge to the tree in PRIM), but are processed in a conceptually similar manner. Our intuition is that this conceptual relationship may not yield weights that are near each other in the weight space, thus making transfer difﬁcult. We instead propose to use the base algorithms as inductive biases by training them with the intermediate steps along side training the target algorithm, for which we do not provide intermediate supervision. Our second hypothesis is that this will help systematic generalisation. We validate both hypotheses—transfer does not help, while multi-task learning helps—empirically.
The contributions of this paper are: 1. presenting a new benchmark for transferring algorithmic reasoning knowledge on graphs; 2. sampling the best trajectory to stabilise training when no execution traces are available; 3. show that standard transfer techniques fail to signiﬁcantly improve systematic generalisation; 4. demonstrate how systematic generalisation can instead be improved on algorithmic tasks with multi-task learning. 2