Abstract
In this paper, we consider a different data format for images: vector graphics. In contrast to raster graphics which are widely used in image recognition, vector graphics can be scaled up or down into any resolution without aliasing or infor-mation loss, due to the analytic representation of the primitives in the document.
Furthermore, vector graphics are able to give extra structural information on how low-level elements group together to form high level shapes or structures. These merits of graphic vectors have not been fully leveraged in existing methods. To explore this data format, we target on the fundamental recognition tasks: object localization and classiﬁcation. We propose an efﬁcient CNN-free pipeline that does not render the graphic into pixels (i.e. rasterization), and takes textual document of the vector graphics as input, called YOLaT (You Only Look at Text). YOLaT builds multi-graphs to model the structural and spatial information in vector graphics, and a dual-stream graph neural network is proposed to detect objects from the graph.
Our experiments show that by directly operating on vector graphics, YOLaT out-performs raster-graphic based object detection baselines in terms of both average precision and efﬁciency. Code is available at https://github.com/microsoft/YOLaT-VectorGraphicsRecognition. 1

Introduction
Raster graphics have been commonly used for image recognition due to its easy accessibility from cameras. Most existing benchmark datasets are built upon raster graphics, from ImageNet [1] for classiﬁcation to COCO [2] for object detection. However, due to its pixel-based ﬁx-sized format, raster graphics may lead to aliasing when scaling up or down by interpolation. Fields like engineering design or graphic design require a more precise way to describe visual content without aliasing when scaling (e.g., graphic designs, mechanical drafts, ﬂoorplans, diagrams, etc), so another important image format emerges, namely vector graphics.
Vector graphics achieve this powerful feature by recording how the graphics are constructed or drawn, instead of the color bitmaps represented by pixel arrays deﬁned in the raster graphics (ﬁrst row in
Figure 1). Speciﬁcally, vector graphics contain a set of primitives like lines, curves and circles, which is deﬁned with parametric equations in analytic geometry and some extra attributes. As shown in
Figure 1, such vector graphic is usually a document where every primitive is deﬁned precisely and written in a line of textual command. Due to the analytic representation, with few parameters, vector graphics can represent an object at any scale or even in inﬁnite resolution, making it potentially a lot more precise and compact image format than raster graphics. Also, instead of independent pixels,
∗This work was done when the authors were interns at MSRA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Difference between raster graphics (row 1) and vector graphics (row 2). vector graphics give higher level structural information on how low-level elements like points or curves group together to form high level shapes or structures. However, this powerful and widely used data format has been rarely investigated in previous computer vision literature.
To explore this data format, this paper focuses on the fundamental recognition tasks: object lo-calization and classiﬁcation, with wide applications in vector graphics related ﬁeld like automatic design audit, AI aided design, design graphics retrieval, etc. Existing raster graphics based methods
[3, 4, 5, 6, 7] takes pixel arrays as input and cannot be directly applied on vector graphics. There have been attempt [8] dealing with this format by rendering vector graphics into raster graphics
ﬁrst. However, rendering the vector graphics into raster graphics could result in a pixel array with super resolutions (e.g., thousands by thousands), which brings extremely large memory cost, and would be inefﬁcient or even intractable for the traditional models to process. On the other hand, rendering a lower resolution image causes substantial information loss, and the object bounding boxes obtained from a low resolution image could be imprecise when scaled back to the original resolution. Furthermore, the rendering process results in a set of independent pixels and discards the high-level structural information within the primitives. Some of this information could be critical for recognition, such as corners in a shape or contours, etc.
To address these issues, we resolve the tasks on vector graphics by introducing a model that does not need rasterization and takes the textual documents of vector graphics as input, called YOLaT(You
Only Look at the Text). Instead of rendering the vector graphics into raster graphics, we propose an efﬁcient end-to-end pipeline which predicts objects from the raw textual deﬁnitions of primitives.
YOLaT ﬁrst transforms different types of primitives into a uniﬁed format. Then it constructs an undirected multi-graph to model the structural and spatial information from the uniﬁed primitives.
Compared to rendering to raster graphics, this transformation is able to preserve more complete information. YOLaT generates object proposals directly from the vector graphics, which produces precise object bounding boxes. Finally, a dual-stream graph neural network (GNN) speciﬁcally designed for vector graphics is proposed to classify the graph contained in each proposal, with no extra regression needed for bounding box reﬁnement.
To evaluate our pipeline over vector graphics, we use two datasets. i.e., ﬂoorplans and diagrams and show the advantages of our method over the raster graphics based object detection baselines. Without pre-training, our method consistently outperforms raster graphics based object detection baselines, with signiﬁcantly higher efﬁciency in terms of the number of parameters and FLOPs. Even compared with the powerful ImageNet pretrained two-stage model, YOLaT achieves comparable performance with 25 times fewer parameters and 100 times fewer FLOPs. We also show visualizations to better demonstrate why looking at the text can capture more delicate details and predicts more accurate bounding boxes. 2