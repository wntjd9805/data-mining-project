Abstract
Safe exploration is a key to applying reinforcement learning (RL) in safety-critical systems. Existing safe exploration methods guaranteed safety under the assumption of regularity, and it has been difﬁcult to apply them to large-scale real problems.
We propose a novel algorithm, SPO-LF, that optimizes an agent’s policy while learning the relation between a locally available feature obtained by sensors and environmental reward/safety using generalized linear function approximations. We provide theoretical guarantees on its safety and optimality. We experimentally show that our algorithm is 1) more efﬁcient in terms of sample complexity and com-putational cost and 2) more applicable to large-scale problems than previous safe
RL methods with theoretical guarantees, and 3) comparably sample-efﬁcient and safer compared with existing advanced deep RL methods with safety constraints. 1

Introduction
Applying reinforcement learning (RL) to applications with unknown safety constraints is challenging as RL is inherently an exploratory process and requires agents to learn the whole environment ﬁrst.
Though there has been a surge of attempts to incorporate safety in RL [3, 6, 11, 16], it is essentially difﬁcult for most of the algorithms to guarantee safety, especially in the exploration phase. If a single mistake could lead to catastrophic failures, conventional algorithms cannot be directly applied.
To guarantee safety while learning an environment, safe exploration problems have been actively studied as a sub-ﬁeld of safe RL [17]. A mainstream safe-exploration method is to utilize a Gaussian process (GP, [24]) under problem settings in which the agent can observe only the safety function value of its current state. Since the agent cannot get any information on the neighboring states, it is necessary to assume desirable properties for the function. Speciﬁcally, Sui et al. [31] assumed that a safety function is Lipschitz continuous and has regularity that can be captured by appropriate kernel functions. However, these assumptions do not hold in many environments. For example, in the context of autonomous driving, the degree of safety varies steeply depending on the presence or absence of other vehicles and pedestrians. Also, previous work on GP-based safe exploration suffers from inconsistency between their theoretical guarantees and computational cost. Previous work has proved the completeness of the predicted safe region [34] and the optimality of the acquired policy
[36], after a large number of samples. However, the computational cost of GP is known to be large.
Hence, it is hard to achieve theoretically guaranteed performance in a large environment.
Based on the fact that robots are equipped with sensors, it is more reasonable to formulate the problem assuming that robots can obtain “feature vectors” for inferring the degree of safety of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Existing safe exploration. (b) Proposed safe exploration.
Figure 1: (a) Previous GP-based safe exploration under regularity assumptions, which fails to handle non-smooth changes. (b) Our proposed feature-based safe exploration. In this problem setting, an agent predicts safety function value while using feature vector obtained by far-sighted observations. (observable) states of their neighbors. This problem formulation is realistic for real-world robots, and it also allows us to deal with safety that changes steeply, which previous research could not handle (see Figure 1). Under this new problem setting, we aim to propose a theoretically guaranteed and empirically efﬁcient safe RL algorithm that is applicable to real, large-scale problems.