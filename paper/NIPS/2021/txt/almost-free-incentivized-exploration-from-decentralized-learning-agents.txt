Abstract
Incentivized exploration in multi-armed bandits (MAB) has witnessed increasing interests and many progresses in recent years, where a principal offers bonuses to agents to do explorations on her behalf. However, almost all existing studies are conﬁned to temporary myopic agents. In this work, we break this barrier and study incentivized exploration with multiple and long-term strategic agents, who have more complicated behaviors that often appear in real-world applications. An im-portant observation of this work is that strategic agents’ intrinsic needs of learning beneﬁt (instead of harming) the principal’s explorations by providing “free pulls”.
Moreover, it turns out that increasing the population of agents signiﬁcantly lowers the principal’s burden of incentivizing. The key and somewhat surprising insight revealed from our results is that when there are sufﬁciently many learning agents involved, the exploration process of the principal can be (almost) free. Our main results are built upon three novel components which may be of independent interest: (1) a simple yet provably effective incentive-provision strategy; (2) a carefully crafted best arm identiﬁcation algorithm for rewards aggregated under unequal conﬁdences; (3) a high-probability ﬁnite-time lower bound of UCB algorithms.
Experimental results are provided to complement the theoretical analysis. 1

Introduction
Multi-armed bandits (MAB) is a simple yet powerful model for sequential decision making with an exploration-exploitation tradeoff (Bubeck and Cesa-Bianchi, 2012; Lattimore and Szepesvári, 2020).
In standard MAB settings, one principal, who has a long-term system-level objective, takes charge of selecting and playing arms. However, such assumption does not always hold in reality. It is often the case that arm pulls are performed by multiple different agents whose individual goals are not aligned with the system, and the principal can only observe agents’ actions. One typical example is the individual buyers (agents) and the online shopping platform (the principal). Such scenarios complicate decision making and introduce signiﬁcant difﬁculties to optimize the system performance.
Incentivized exploration has been proposed to address this problem (Frazier et al., 2014; Mansour et al., 2015). Speciﬁcally, bonuses can be offered by the principal to incentivize agents to perform speciﬁc actions, e.g., to explore their originally underrated arms. This framework provides an opportunity to reconcile different interests between the principal and agents. As a concrete example, the online shopping platform can offer discounts on certain items so that individual buyers would buy them and provide feedbacks, which can be used to optimize future strategies of the platform. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
While incentivized exploration has been investigated in the existing literature, we recognize two major limitations. First, almost all of the existing works assume the participating agents to be myopic, i.e., they always choose the empirically best arm. Second, it is always assumed that at each time slot, one new agent participates in the system, i.e., agents never stay or return. In other words, prior research mainly considers how to incentivize one single temporary myopic agent. These two assumptions largely limit the applicability of incentivized exploration.
In this work, we extend the study of incentivized exploration beyond the aforementioned barriers, and investigate situations with multiple long-term strategic agents. In particular, we focus on the scenarios (see Section 3.1) where the principal wants to identify the (overall) best arm whereas the heterogeneously involved agents only care about their different individual cumulative rewards. For such scenarios, the “Observe-then-Incentivize” (OTI) mechanism is proposed and several interesting observations are obtained. First, we ﬁnd that strategic agents’ intrinsic needs of learning can actually beneﬁt principal’s exploration by providing “free pulls”. In other words, as opposed to myopic agents, the self interests of strategic agents can be exploited by the principal. Second, it turns out that increasing the number of participating agents can signiﬁcantly mitigate the principal’s burden on incentivizing, which highlights the importance of increasing the population of agents. A crux of our
ﬁndings is the following intriguing conceptual message: when there are sufﬁciently many learning agents involved, the exploration process of the principal could be (almost) free.
Behind these ﬁndings, three novel technical components play critical roles in the design and analysis of OTI, all of which may have independent values.
• First, a simple yet provably effective incentive-provision strategy is developed, which can efﬁciently regulate strategic agents’ behaviors and serves as the foundation of the algorithm analysis.
• Second, a best arm identiﬁcation algorithm is carefully crafted to tackle the varying amounts of local information from heterogeneous agents. This setting itself is novel in best arm identiﬁcation.
• A high-probability ﬁnite-time lower bound of UCB algorithms (Auer et al., 2002) is proved, which contributes to a better understanding of the celebrated UCB.
These insights and techniques are unique in incentivizing multiple long-term strategic agents, which may ﬁnd applications in related problems, and encourage future research in this direction. 2