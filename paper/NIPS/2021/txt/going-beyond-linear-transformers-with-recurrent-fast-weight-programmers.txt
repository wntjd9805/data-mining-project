Abstract
Transformers with linearised attention (“linear Transformers”) have demonstrated the practical scalability and effectiveness of outer product-based Fast Weight
Programmers (FWPs) from the ’90s. However, the original FWP formulation is more general than the one of linear Transformers: a slow neural network (NN) continually reprograms the weights of a fast NN with arbitrary architecture. In existing linear Transformers, both NNs are feedforward and consist of a single layer. Here we explore new variations by adding recurrence to the slow and fast nets. We evaluate our novel recurrent FWPs (RFWPs) on two synthetic algorithmic tasks (code execution and sequential ListOps), Wikitext-103 language models, and on the Atari 2600 2D game environment. Our models exhibit properties of
Transformers and RNNs. In the reinforcement learning setting, we report large improvements over LSTM in several Atari games. Our code is public.1 1

Introduction
The Transformer [1] has become one of the most popular neural networks (NNs) for processing sequential data. Its success on neural machine translation quickly transferred to other problems in natural language processing, such as language modelling [2, 3] or question answering [4]. Recently, it has also been applied in other domains, including image processing [5, 6] or mathematical problem solving [7, 8, 9].
Conceptually, the Transformer is a deep feedforward NN that processes all elements of a sequence in parallel: unlike in recurrent NNs (RNNs), the computations of a layer for the entire sequence can be packed into one big matrix multiplication. This scales well with the number of parallel processors.
Despite the beneﬁts of parallelisation, a major drawback of Transformers is that their computational complexity in time and space is quadratic in sequence length. Furthermore, in the auto-regressive version [1, 2] — the focus of our work — the state size increases linearly with sequence length. This makes Transformers infeasible for auto-regressive settings dealing with very long or potentially inﬁ-nite sequences, forcing practitioners to truncate temporal contexts and ignore long-term dependen-cies beyond ﬁxed-size time windows. Although recent work tries to address this issue [10, 11], this limitation makes some applications of Transformers challenging, e.g., reinforcement learning (RL) in partially observable environments [12, 13], which is still dominated by RNNs such as the Long
Short-Term Memory (LSTM; [14]) trained by policy gradients [15, 16, 17, 18].
To scale Transformers to longer sequences, recent works have proposed to linearise the softmax in the self-attention computation and reorganise the latter in a sequential way [19]. Such models include
∗Equal contribution. 1https://github.com/IDSIA/recurrent-fwp 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Katharopoulos et al.’s Linear Transformer (LT) [19], Choromanski et al.’s Performer [20] and Peng et al. [21]’s variant. They enjoy time and space complexities linear in sequence length with states of constant size. While their performance on some tasks does not fully match the one of regular
Transformers [22], several improvements have already been proposed [21, 23] (see our review in
Sec. 2.2) which makes this Transformer family a promising alternative.
Here we go one step further in advancing linear Transformer variants as powerful auto-regressive sequence processing models, adopting the perspective of “Fast Weight Programmers” (FWPs) [24, 25, 26]. Recent work emphasised that linearised Transformers are essentially equivalent to outer product-based FWPs from the ’90s ([23]; reviewed in Sec. 2). Here we explore this connection further and describe more powerful FWPs.
The original FWP [24] is a two-NN system: a slow and a fast net, each with arbitrary architectures.
The slow net learns to generate rapid context-dependent weight modiﬁcations for the fast net. In the case of existing linear Transformer variants, the slow and fast nets are simple one layer feedforward
NNs. Here we augment them with recurrent connections to obtain recurrent FWPs (RFWPs).
Recurrence enhances the model’s theoretical power [27] and can help to solve tasks that naturally require recurrence as a part of the solution.
Our experiments on the language modelling dataset Wikitext-103 [28] show that our RFWPs are competitive compared to regular Transformers. We then study various properties of the proposed models on two synthetic algorithmic tasks: code execution [29] and sequential ListOps [30]. Finally, it is straightforward to apply our models to RL problems as a drop-in replacement for LSTMs. Here our RFWPs obtain large improvements over LSTM baselines across many Atari 2600 2D game environments [31]. Although LSTM still works better in a few environments, we show that our
RFWPs generally improve by scaling them up.
The main contribution of this work is twofold: (1) from the perspective of FWPs, we study novel powerful FWPs for sequence processing, demonstrating that NNs can easily learn to control NNs that are more complex than a single feedforward layer, and (2) from the perspective of Transformer models, our RFWPs augment linear Transformers with recurrence, addressing general limitations of existing auto-regressive Transformer models. 2