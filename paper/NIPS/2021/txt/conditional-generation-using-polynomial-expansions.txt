Abstract
Generative modeling has evolved to a notable ﬁeld of machine learning. Deep polynomial neural networks (PNNs) have demonstrated impressive results in unsu-pervised image generation, where the task is to map an input vector (i.e., noise) to a synthesized image. However, the success of PNNs has not been replicated in conditional generation tasks, such as super-resolution. Existing PNNs focus on single-variable polynomial expansions which do not fare well to two-variable inputs, i.e., the noise variable and the conditional variable.
In this work, we introduce a general framework, called CoPE, that enables a polynomial expan-sion of two input variables and captures their auto- and cross-correlations. We exhibit how CoPE can be trivially augmented to accept an arbitrary number of input variables. CoPE is evaluated in ﬁve tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation) involving eight datasets. The thorough evaluation suggests that
CoPE can be useful for tackling diverse conditional generation tasks. The source code of CoPE is available at https://github.com/grigorisg9gr/ polynomial_nets_for_conditional_generation. 1

Introduction
Modelling high-dimensional distributions and generating samples from complex distributions are fundamental tasks in machine learning. Among prominent generative models, StyleGAN [Karras et al., 2019] has demonstrated unparalleled performance in unsupervised image generation. Its success can be attributed to the higher-order correlations of the input vector z captured by the generator. As
Chrysos et al. [2019] argue, StyleGAN1 is best explained as a deep polynomial neural network (PNN).
PNNs have demonstrated impressive generation results in faces, animals, cars [Karras et al., 2020b], paintings, medical images [Karras et al., 2020a]. Nevertheless, PNNs have yet to demonstrate similar performance in conditional generation tasks, such as super-resolution or image-to-image translation.
In contrast to unsupervised generators that require a single-variable input z, in conditional generation (at least) two inputs are required: i) one (or more) conditional variables c, e.g., a low-resolution image, and ii) a noise sample z. A trivial extension of PNNs for conditional generation would be to concatenate all the input variables into a fused variable. The fused variable is then the input to the single-variable polynomial expansion of PNNs. However, the concatenation reduces the ﬂexibility of the model signiﬁcantly. For instance, concatenating a noise vector and a vectorized low-resolution image results in sub-optimal super-resolution, since the spatial correlations of the input image are lost in the vectorization. Additionally, the concatenation of the vectorized conditional variable c and 1This work focuses on the generator network; any reference to StyleGAN refers to its generator. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
z leads to a huge number of parameters when we use a fully-connected layer as typically done in the input of StyleGAN, especially when c depicts an image.
In this work, we introduce a framework, called CoPE, for conditional data generation. CoPE resorts to multivariate polynomials that capture the higher-order auto- and cross-correlations between the two input variables. By imposing a tailored structure in the higher-order correlations, we obtain an intuitive, recursive formulation for CoPE. The formulation enables different constraints to be applied to each variable and its associated parameters. In CoPE, different architectures can be deﬁned simply by changing the recursive formulation. Our contributions can be summarized as follows:
• We introduce a framework, called CoPE, that expresses a high-order, multivariate polynomial for conditional data generation. We exhibit how CoPE can be applied on diverse conditional generation tasks.
• We derive two extensions to the core two-variable model: a) we augment the formulation to enable an arbitrary number of conditional input variables, b) we design different architectures that arise by changing the recursive formulation.
• CoPE is evaluated on ﬁve different tasks (class-conditional generation, inverse problems, edges-to-image translation, image-to-image translation, attribute-guided generation); overall eight datasets are used for the thorough evaluation.
The diverse experiments suggest that CoPE can be useful for a variety of conditional generation tasks, e.g., by deﬁning task-speciﬁc recursive formulations. To facilitate the reproducibility, the source code is available at https://github.com/grigorisg9gr/polynomial_nets_ for_conditional_generation. 2