Abstract
Modern approaches typically formulate semantic segmentation as a per-pixel classi-ﬁcation task, while instance-level segmentation is handled with an alternative mask classiﬁcation. Our key insight: mask classiﬁcation is sufﬁciently general to solve both semantic- and instance-level segmentation tasks in a uniﬁed manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classiﬁcation model which predicts a set of binary masks, each associated with a single global class label prediction.
Overall, the proposed mask classiﬁcation-based method simpliﬁes the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classiﬁcation baselines when the number of classes is large. Our mask classiﬁcation-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.1 1

Introduction
The goal of semantic segmentation is to partition an image into regions with different semantic categories. Starting from Fully Convolutional Networks (FCNs) work of Long et al. [28], most deep learning-based semantic segmentation approaches formulate semantic segmentation as per-pixel classiﬁcation (Figure 1 left), applying a classiﬁcation loss to each output pixel [8, 46]. Per-pixel predictions in this formulation naturally partition an image into regions of different classes.
Mask classiﬁcation is an alternative paradigm that disentangles the image partitioning and classiﬁca-tion aspects of segmentation. Instead of classifying each pixel, mask classiﬁcation-based methods predict a set of binary masks, each associated with a single class prediction (Figure 1 right). The more ﬂexible mask classiﬁcation dominates the ﬁeld of instance-level segmentation. Both Mask
R-CNN [19] and DETR [3] yield a single class prediction per segment for instance and panoptic segmentation. In contrast, per-pixel classiﬁcation assumes a static number of outputs and cannot return a variable number of predicted regions/segments, which is required for instance-level tasks.
Our key observation: mask classiﬁcation is sufﬁciently general to solve both semantic- and instance-level segmentation tasks. In fact, before FCN [28], the best performing semantic segmentation methods like O2P [4] and SDS [18] used a mask classiﬁcation formulation. Given this perspective, a natural question emerges: can a single mask classiﬁcation model simplify the landscape of effective approaches to semantic- and instance-level segmentation tasks? And can such a mask classiﬁcation model outperform existing per-pixel classiﬁcation methods for semantic segmentation?
To address both questions we propose a simple MaskFormer approach that seamlessly converts any existing per-pixel classiﬁcation model into a mask classiﬁcation. Using the set prediction mechanism proposed in DETR [3], MaskFormer employs a Transformer decoder [37] to compute a set of pairs,
∗Work partly done during an internship at Facebook AI Research. 1Project page: https://bowenc0221.github.io/maskformer 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Per-pixel classiﬁcation vs. mask classiﬁcation. (left) Semantic segmentation with per-pixel classiﬁcation applies the same classiﬁcation loss to each location. (right) Mask classiﬁcation predicts a set of binary masks and assigns a single class to each mask. Each prediction is supervised with a per-pixel binary mask loss and a classiﬁcation loss. Matching between the set of predictions and ground truth segments can be done either via bipartite matching similarly to DETR [3] or by
ﬁxed matching via direct indexing if the number of predictions and classes match, i.e., if N = K. each consisting of a class prediction and a mask embedding vector. The mask embedding vector is used to get the binary mask prediction via a dot product with the per-pixel embedding obtained from an underlying fully-convolutional network. The new model solves both semantic- and instance-level segmentation tasks in a uniﬁed manner: no changes to the model, losses, and training procedure are required. Speciﬁcally, for semantic and panoptic segmentation tasks alike, MaskFormer is supervised with the same per-pixel binary mask loss and a single classiﬁcation loss per mask. Finally, we design a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format.
We evaluate MaskFormer on ﬁve semantic segmentation datasets with various numbers of categories:
Cityscapes [13] (19 classes), Mapillary Vistas [31] (65 classes), ADE20K [49] (150 classes), COCO-Stuff-10K [2] (171 classes), and ADE20K-Full [49] (847 classes). While MaskFormer performs on par with per-pixel classiﬁcation models for Cityscapes, which has a few diverse classes, the new model demonstrates superior performance for datasets with larger vocabulary. We hypothesize that a single class prediction per mask models ﬁne-grained recognition better than per-pixel class predictions.
MaskFormer achieves the new state-of-the-art on ADE20K (55.6 mIoU) with Swin-Transformer [27] backbone, outperforming a per-pixel classiﬁcation model [27] with the same backbone by 2.1 mIoU, while being more efﬁcient (10% reduction in parameters and 40% reduction in FLOPs).
Finally, we study MaskFormer’s ability to solve instance-level tasks using two panoptic segmentation datasets: COCO [26, 22] and ADE20K [49]. MaskFormer outperforms a more complex DETR model [3] with the same backbone and the same post-processing. Moreover, MaskFormer achieves the new state-of-the-art on COCO (52.7 PQ), outperforming prior state-of-the-art [38] by 1.6 PQ.
Our experiments highlight MaskFormer’s ability to unify instance- and semantic-level segmentation. 2