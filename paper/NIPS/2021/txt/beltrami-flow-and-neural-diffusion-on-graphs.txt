Abstract
We propose a novel class of graph neural networks based on the discretised Beltrami
ﬂow, a non-Euclidean diffusion PDE. In our model, node features are supplemented with positional encodings derived from the graph topology and jointly evolved by the Beltrami ﬂow, producing simultaneously continuous feature learning and topology evolution. The resulting model generalises many popular graph neural networks and achieves state-of-the-art results on several benchmarks. 1

Introduction
The majority of graph neural networks (GNNs) are based on the message passing paradigm [30], wherein node features are learned by means of a non-linear propagation on the graph. Multiple recent works have pointed to the limitations of the message passing approach. These include; limited expressive power [80, 95, 9, 7], the related oversmoothing problem [60, 62] and the bottleneck phenomena [1, 93], which render such approaches inefﬁcient, especially in deep GNNs. Multiple alternatives have been proposed, among which are higher-order methods [54, 7] and decoupling the propagation and input graphs by modifying the topology, often referred to as graph rewiring.
Topological modiﬁcations can take different forms such as graph sampling [32], kNN [43], using the complete graph [86, 1], latent graph learning [89, 36], or multi-hop ﬁlters [92, 73]. However, there is no agreement in the literature on when and how to modify the graph, and a single principled framework for doing so.
A somewhat underappreciated fact is that GNNs are intimately related to diffusion equations [16], a connection that was exploited in the early work of Scarselli et al. [76]. Diffusion PDEs have been historically important in computer graphics [83, 11, 51, 64], computer vision [13, 18, 6], and image processing [65, 82, 90, 85, 26, 12], where they created an entire trend of variational and
PDE-based approaches. In machine learning and data science, diffusion equations underpin such popular manifold learning methods as eigenmaps [5] and diffusion maps [20], as well as the family of
PageRank algorithms [63, 14]. In deep learning, differential equations are used as models of neural networks [16, 19, 25, 94, 71, 98] and for physics-informed learning [72, 22, 75, 21, 81, 47].
Main contributions
In this paper, we propose a novel class of GNNs based on the discretised non-Euclidean diffusion PDE in joint positional and feature space, inspired by the Beltrami ﬂow [82] used two decades ago in the image processing literature for edge-preserving image denoising. We show that the discretisation of the spatial component of the Beltrami ﬂow offers a principled view on positional encoding and graph rewiring, whereas the discretisation of the temporal component can replace GNN layers with more ﬂexible adaptive numerical schemes. Based on this model, we introduce Beltrami Neural Diffusion (BLEND) that generalises a broad range of GNN architectures and shows state-of-the-art performance on many popular benchmarks. In a broader perspective, our 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
approach explores new tools from PDEs and differential geometry that are less well known in the graph ML community. 2