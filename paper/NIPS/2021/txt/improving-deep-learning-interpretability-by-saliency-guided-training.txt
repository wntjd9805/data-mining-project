Abstract
Saliency methods have been widely used to highlight important input features in model predictions. Most existing methods use backpropagation on a modiﬁed gradient function to generate saliency maps. Thus, noisy gradients can result in unfaithful feature attributions. In this paper, we tackle this issue and introduce a saliency guided training† procedure for neural networks to reduce noisy gradients used in predictions while retaining the predictive performance of the model. Our saliency guided training procedure iteratively masks features with small and poten-tially noisy gradients while maximizing the similarity of model outputs for both masked and unmasked inputs. We apply the saliency guided training procedure to various synthetic and real data sets from computer vision, natural language processing, and time series across diverse neural architectures, including Recurrent
Neural Networks, Convolutional Networks, and Transformers. Through qualitative and quantitative evaluations, we show that saliency guided training procedure sig-niﬁcantly improves model interpretability across various domains while preserving its predictive performance. 1

Introduction
Deep Neural Networks (DNNs) have been widely used in a variety of different tasks [31, 26, 43, 37]; yet interpreting complex networks remains a challenge. Reliable explanations are necessary for critical domains like medicine, neuroscience, ﬁnance, and autonomous driving [9, 34]. Explanations are also useful for model debugging [63, 35]. As a result, various interpretability methods were developed to understand DNNs [5, 49, 22, 53, 52, 32, 51]. A common approach for understanding model decisions is to identify features in the input that highly inﬂuenced the ﬁnal classiﬁcation decision [6, 63, 53, 52, 47, 36, 64]. Such approaches, known as saliency maps, often use gradient calculations to assign an importance score to individual features, reﬂecting their inﬂuences on the model prediction.
Saliency methods aim to highlight meaningful input features in model predictions to humans; however, the maps produced are often noisy (i.e., contain visual noise). To improve the faithfulness of saliency maps, explanations methods that depend on more than one or higher-order gradient calculations were developed. For example, SmoothGrad [52] reduces saliency noise by adding noise to the input multiple times and then taking the average of the resulting saliency maps for each input. Integrated gradients [53], DeepLIFT [47] and Layer-wise Relevance Propagation [5] backpropagate through a modiﬁed gradient function [3] while Singla et al. [51] studies the use of higher-order gradients in saliency maps.
∗Authors contributed equally
†Code: https://github.com/ayaabdelsalam91/saliency_guided_training 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we take a different approach to improve the interpretability of deep neural networks— instead of developing yet another saliency method, we propose a new training procedure that naturally leads to improved model explanations using current saliency methods. Our proposed training procedure, called saliency guided training, trains models that produce sparse, meaningful, and less noisy gradients without degrading model performance. This is done by iteratively masking input features with low gradient values (i.e., less important features) and then minimizing a loss function that combines (a) the KL divergence [27] between model outputs from the original and masked inputs, and (b) the appropriate loss function for the model prediction. This procedure reduces noise in model gradients without sacriﬁcing its predictive performance.
To demonstrate the effectiveness of our proposed saliency guided training approach, we consider a variety of classiﬁcation tasks for images, language, and multivariate time series across diverse neural architectures, including Convolutional Neural Networks (CNNs), Recurrent Neural Network (RNNs), and Transformers. In particular, we observe that using saliency guided training in image classiﬁcation tasks leads to a reduction in visual saliency noise and sparser saliency maps, as shown in Figure 2. Saliency guided training also improves the comprehensiveness of the produced explanations for sentiment analysis, and fact extraction tasks as shown in Table 1.
In multivariate time series classiﬁcation tasks, we observe an increase in the precision and recall of saliency maps when applying the proposed saliency guided training. Interestingly, we also ﬁnd that the saliency guided training reduces the vanishing saliency issue of RNNs [20] as shown in Figure 6. Finally, we note that although we use the vanilla gradient for masking in the saliency guided training procedure, we observe signiﬁcant improvements in the explanations produced after training by several other gradient-based saliency methods. 2