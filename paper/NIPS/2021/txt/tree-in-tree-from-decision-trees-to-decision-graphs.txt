Abstract
Decision trees have been widely used as classiﬁers in many machine learning applications thanks to their lightweight and interpretable decision process. This paper introduces Tree in Tree decision graph (TnT), a framework that extends the conventional decision tree to a more generic and powerful directed acyclic graph.
TnT constructs decision graphs by recursively growing decision trees inside the internal or leaf nodes instead of greedy training. The time complexity of TnT is linear to the number of nodes in the graph, and it can construct decision graphs on large datasets. Compared to decision trees, we show that TnT achieves better classiﬁcation performance with reduced model size, both as a stand-alone classiﬁer and as a base estimator in bagging/AdaBoost ensembles. Our proposed model is a novel, more efﬁcient, and accurate alternative to the widely-used decision trees. 1

Introduction
Decision trees (DTs) and tree ensembles are widely used in practice, particularly for applications that require few parameters [1–5], fast inference [6–8], and good interpretability [9, 10]. In a DT, the internal and leaf nodes are organized in a binary structure, with internal nodes deﬁning the routing function and leaf nodes predicting the class label. Although DTs are easy to train by recursively splitting leaf nodes, the tree structure can be suboptimal for the following reasons: (1) DTs can grow exponentially large as the depth of the tree increases. Yet, the root-leaf path can be short even for large DTs, limiting the predictive power. (2) In a DT, the nodes are not shared across different paths, reducing the efﬁciency of the model.
Decision trees are similar to neural networks (NNs) in that both models are composed of basic units.
A possible way to enhance the performance of DTs or NNs is to replace the basic units with more powerful models. For instance, “Network in Network” builds micro NNs with complex structures within local receptive ﬁelds to achieve state-of-the-art performances on image recognition tasks [11].
As for DTs, previous work replaced the axis-aligned splits with logistic regression or linear support vector machines to construct oblique trees [1, 3, 6, 12–14]. The work in [5] further incorporates convolution operations into DTs for improved performance on image recognition tasks, while [1] replaces the leaf predictors with linear regression to improve the regression performance. Unlike the greedy training algorithms used for axis-aligned trees (e.g., Classiﬁcation and Regression Trees or
CART [15]), oblique trees are generally trained by gradient-based [3, 13, 14] or alternating [1, 6] optimization algorithms.
Inspired by the concepts of Network in Network [11] and oblique trees [6, 12], we propose a novel model, Tree in Tree (TnT), to recursively replace the internal and leaf nodes with micro decision trees. In contrast to a conventional tree structure, the nodes in a TnT form a Directed Acyclic Graph (DAG) to address the aforementioned limitations and construct a more efﬁcient model. Unlike previous oblique trees that were optimized on a predeﬁned tree structure [1, 5], TnT can learn graph 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
connections from scratch. The major contributions of this work are as follows: (1) We extend decision trees to decision graphs and propose a scalable algorithm to construct large decision graphs. (2) We show that the proposed algorithm outperforms existing decision trees/graphs, either as a stand-alone classiﬁer or base estimator in an ensemble, under the same model complexity constraints. (3) Rather than relying on a predeﬁned graph/tree structure, the proposed algorithm is capable of learning graph connections from scratch (i.e., starting from a single leaf node) and offers a fully interpretable decision process. We provide a Python implementation of the proposed TnT decision graph at https://github.com/BingzhaoZhu/TnTDecisionGraph. 2