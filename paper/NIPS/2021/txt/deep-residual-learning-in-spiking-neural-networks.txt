Abstract
Deep Spiking Neural Networks (SNNs) present optimization difﬁculties for gradient-based approaches due to discrete binary activation and complex spatial-temporal dynamics. Considering the huge success of ResNet in deep learning, it would be natural to train deep SNNs with residual learning. Previous Spiking
ResNet mimics the standard residual block in ANNs and simply replaces ReLU activation layers with spiking neurons, which suffers the degradation problem and can hardly implement residual learning. In this paper, we propose the spike-element-wise (SEW) ResNet to realize residual learning in deep SNNs. We prove that the SEW ResNet can easily implement identity mapping and overcome the vanishing/exploding gradient problems of Spiking ResNet. We evaluate our SEW
ResNet on ImageNet, DVS Gesture, and CIFAR10-DVS datasets, and show that
SEW ResNet outperforms the state-of-the-art directly trained SNNs in both ac-curacy and time-steps. Moreover, SEW ResNet can achieve higher performance by simply adding more layers, providing a simple method to train deep SNNs.
To our best knowledge, this is the ﬁrst time that directly training deep SNNs with more than 100 layers becomes possible. Our codes are available at https:
//github.com/fangwei123456/Spike-Element-Wise-ResNet. 1

Introduction
Artiﬁcial Neural Networks (ANNs) have achieved great success in many tasks, including image classiﬁcation [28, 52, 55], object detection [9, 34, 44], machine translation [2], and gaming [37, 51].
One of the critical factors for ANNs’ success is deep learning [29], which uses multi-layers to learn representations of data with multiple levels of abstraction. It has been proved that deeper networks have advantages over shallower networks in computation cost and generalization ability [3]. The function represented by a deep network can require an exponential number of hidden units by a shallow network with one hidden layer [38]. In addition, the depth of the network is closely related to the network’s performance in practical tasks [52, 55, 27, 52]. Nevertheless, recent evidence [13, 53, 14] reveals that with the network depth increasing, the accuracy gets saturated and then degrades rapidly.
To solve this degradation problem, residual learning is proposed [14, 15] and the residual structure is widely exploited in “very deep” networks that achieve the leading performance [22, 59, 18, 57].
Spiking Neural Networks (SNNs) are regarded as a potential competitor of ANNs for their high biological plausibility, event-driven property, and low power consumption [45]. Recently, deep learning methods are introduced into SNNs, and deep SNNs have achieved close performance as
ANNs in some simple classiﬁcation datasets [56], but still worse than ANNs in complex tasks, e.g., classifying the ImageNet dataset [47]. To obtain higher performance SNNs, it would be natural to
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
explore deeper network structures like ResNet. Spiking ResNet [25, 60, 21, 17, 49, 12, 30, 64, 48, 42, 43], as the spiking version of ResNet, is proposed by mimicking the residual block in ANNs and replacing ReLU activation layers with spiking neurons. Spiking ResNet converted from ANN achieves state-of-the-art accuracy on nearly all datasets, while the directly trained Spiking ResNet has not been validated to solve the degradation problem.
In this paper, we show that Spiking ResNet is inapplicable to all neuron models to achieve identity mapping. Even if the identity mapping condition is met, Spiking ResNet suffers from the problems of vanishing/exploding gradient. Thus, we propose the Spike-Element-Wise (SEW) ResNet to realize residual learning in SNNs. We prove that the SEW ResNet can easily implement identity mapping and overcome the vanishing/exploding gradient problems at the same time. We evaluate Spiking ResNet and SEW ResNet on both the static ImageNet dataset and the neuromorphic DVS Gesture dataset [1],
CIFAR10-DVS dataset [32]. The experiment results are consistent with our analysis, indicating that the deeper Spiking ResNet suffers from the degradation problem — the deeper network has higher training loss than the shallower network, while SEW ResNet can achieve higher performance by simply increasing the network’s depth. Moreover, we show that SEW ResNet outperforms the state-of-the-art directly trained SNNs in both accuracy and time-steps. To the best of our knowledge, this is the ﬁrst time to explore the directly-trained deep SNNs with more than 100 layers. 2