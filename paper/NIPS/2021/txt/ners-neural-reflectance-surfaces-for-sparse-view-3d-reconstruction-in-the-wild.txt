Abstract
Recent history has seen a tremendous growth of work exploring implicit repre-sentations of geometry and radiance, popularized through Neural Radiance Fields (NeRF). Such works are fundamentally based on a (implicit) volumetric repre-sentation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-deﬁned surfaces, we introduce a surface analog of such implicit models called Neural Reﬂectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reﬂectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular “shininess.” Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such “in-the-wild” multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a ﬁrst step toward building scalable, high-quality libraries of real-world shape, materials, and illumination. The project page with code and video visualizations can be found at jasonyzhang.com/ners.
Figure 1: 3D view synthesis in the wild. From several multi-view internet images of a truck and a coarse initial mesh (top left), we recover the camera poses, 3D shape, texture, and illumination (top right). We demonstrate the scalability of our approach on a wide variety of indoor and outdoor object categories (second row). [Video]
∗ denotes equal coding. Corresponding email: jasonyzhang@cmu.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
Although we observe the surrounding world only via 2D percepts, it is undeniably 3D. The goal of recovering this underlying 3D from 2D observations has been a longstanding one in the vision community, and any computational approach aimed at this task must answer a central question about representation—how should we model the geometry and appearance of the underlying 3D structure?
An increasingly popular answer to this question is to leverage neural volumetric representations of density and radiance ﬁelds (Mildenhall et al., 2020). This allows modeling structures from rigid objects to translucent ﬂuids, while further enabling arbitrary view-dependent lighting effects.
However, it is precisely this unconstrained expressivity that makes it less robust and unsuitable for modeling 3D objects from sparse views in the wild. While these neural volumetric representations have been incredibly successful, they require hundreds of images, typically with precise camera poses, to model the full 3D structure and appearance of real-world objects. In contrast, when applied to
‘in-the-wild’ settings e.g. a sparse set of images with imprecise camera estimates from off-the-shelf systems (see Fig. 1), they are unable to infer a coherent 3D representation. We argue this is because these neural volumetric representations, by allowing arbitrary densities and lighting, are too ﬂexible.
Is there a robust alternative that captures real-world 3D structure? The vast majority of real-world objects and scenes comprise of well-deﬁned surfaces. This implies that the geometry, rather than being an unconstrained volumetric function, can be modeled as a 2D manifold embedded in euclidean 3D space—and thus encoded via a (neural) mapping from a 2D manifold to 3D. Indeed, such meshed surface manifolds form the heart of virtually all rendering engines (Foley et al., 1996). Moreover, instead of allowing arbitrary view-dependent radiance, the appearance of such surfaces can be described using (neural) bidirectional surface reﬂection functions (BRDFs), themselves developed by the computer graphics community over decades. We operationalize these insights into Neural
Reﬂectance Surfaces (NeRS), a surface-based neural representation for geometry and appearance.
NeRS represents shape using a neural displacement ﬁeld over a canonical sphere, thus constraining the geometry to be a watertight surface. This representation crucially associates a surface normal to each point, which enables modeling view-dependent lighting effects in a physically grounded manner.
Unlike volumetric representations which allow unconstrained radiance, NeRS factorizes surface appearance using a combination of diffuse color (albedo) and specularity. It does so by learning neural texture ﬁelds over the sphere to capture the albedo at each surface point, while additionally inferring an environment map and surface material properties. This combination of a surface constraint and a factored appearance allows NeRS to learn efﬁciently and robustly from a sparse set of images in the wild, while being able to capture varying geometry and complex view-dependent appearance.
Using only a coarse category-level template and approximate camera poses, NeRS can reconstruct instances from a diverse set of classes. Instead of evaluating in a synthetic setup, we introduce a dataset sourced from marketplace settings where multiple images of a varied set of real-world objects under challenging illumination are easily available. We show NeRS signiﬁcantly outperforms neural volumetric or classic mesh-based approaches in this challenging setup, and as illustrated in
Fig. 1, is able to accurately model the view-dependent appearance via its disentangled representation.
Finally, as cameras recovered in the wild are only approximate, we propose a new evaluation protocol for in-the-wild novel view synthesis in which cameras can be reﬁned during both training and evaluation. We hope that our approach and results highlight the several advantages that neural surface representations offer, and that our work serves as a stepping stone for future investigations. 2