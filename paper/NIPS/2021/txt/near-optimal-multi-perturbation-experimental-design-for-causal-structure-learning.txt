Abstract
Causal structure learning is a key problem in many domains. Causal structures can be learnt by performing experiments on the system of interest. We address the largely unexplored problem of designing a batch of experiments that each simultaneously intervene on multiple variables. While potentially more informative than the commonly considered single-variable interventions, selecting such inter-ventions is algorithmically much more challenging, due to the doubly-exponential combinatorial search space over sets of composite interventions. In this paper, we develop efﬁcient algorithms for optimizing different objective functions quantifying the informativeness of a budget-constrained batch of experiments. By establishing novel submodularity properties of these objectives, we provide approximation guarantees for our algorithms. Our algorithms empirically perform superior to both random interventions and algorithms that only select single-variable interventions. 1

Introduction
The problem of ﬁnding the causal relationships between a set of variables is ubiquitous throughout the sciences. For example, scientists are interested in reconstructing gene regulatory networks (GRNs) of biological cells [11]. Directed Acyclic Graphs (DAGs) are a natural way to represent causal structures, with a directed edge from variable X to Y representing X being a direct cause of Y [33].
Learning the causal structure of a set of variables is fundamentally difﬁcult. With only observational data, in general we can only identify the true DAG up to a set of DAGs called its Markov Equivalence
Class (MEC) [35]. Empirically, for sparse DAGs the size of the MEC grows exponentially in the number of nodes [17]. Identiﬁability can be improved by intervening on variables, meaning one perturbs a subset of the variables and then observes more samples from the system [8, 16, 40]. There exist various inference algorithms for learning causal structures from a combination of observational and interventional data [16, 36, 40, 28, 34]. Here we focus on the identiﬁcation of DAGs that have no unobserved confounding variables.
Performing experiments is often expensive, however. Thus, we are interested in learning as much about the causal structure as possible given some constraints on the interventions. In this work, we focus on the batched setting, where several interventions are performed in parallel. This is a natural setting in scientiﬁc domains like reconstructing GRNs. Existing works propose meaningful objective 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
a b c
Intervention 1
Intervention 2 d
Figure 1: a) We illustrate the MEC of a tree graph on 5 nodes. b) Two single-node interventions are required to fully identify the true DAG. c) Only one two-node intervention is required to fully identify the true DAG. d) This MEC contains 5 DAGs, each corresponding to a different root node (marked white). This is a property particular to tree MECs. functions for this batched causal structure learning problem and then give algorithms that have prov-able guarantees [3, 13]. However, these works focus on the setting where only a single random variable is perturbed per intervention. It is an open question as to whether there exist efﬁcient algorithms for the multiple-perturbation setting, where more than one variable is perturbed in each intervention.
For the task of reconstructing GRNs, it is now possible for experimenters to perturb multiple genes in a single cell [2, 7]. Figures 1 b) and c) illustrate a speciﬁc example where a two-node intervention completely identiﬁes a DAG in half as many interventions as single-node interventions. In general, it is possible for a set of q-node interventions to orient up to q-times more edges in a DAG than single-node interventions (see the supplementary material for a more general example). While multi-perturbation interventions can be more informative, designing them is algorithmically challenging because it leads to an exponentially larger search space: any algorithm must now select a set of sets.
Our main contribution is to provide efﬁcient algorithms for different objective functions with accompanying performance bounds. We demonstrate empirically on both synthetic and GRN graphs that our algorithms result in greater identiﬁability than existing approaches that do not make use of multiple perturbations [13, 40], as well as a random strategy.
We begin by introducing the notation and the objective functions considered in this work in Section 2, before reviewing related work in Section 3. In Section 4 we present our algorithms along with proofs of their performance guarantees. Finally, in Section 5 we demonstrate the superior empirical performance of our method over existing baselines on both synthetic networks and on data generated from models of real GRNs. 2