Abstract
Low-rank Tucker and CP tensor decompositions are powerful tools in data analytics.
The widely used alternating least squares (ALS) method, which solves a sequence of over-determined least squares subproblems, is costly for large and sparse tensors.
We propose a fast and accurate sketched ALS algorithm for Tucker decomposi-tion, which solves a sequence of sketched rank-constrained linear least squares subproblems. Theoretical sketch size upper bounds are provided to achieve O((cid:15)) relative error for each subproblem with two sketching techniques, TensorSketch and leverage score sampling. Experimental results show that this new ALS algorithm, combined with a new initialization scheme based on the randomized range ﬁnder, yields decomposition accuracy comparable to the standard higher-order orthogonal iteration (HOOI) algorithm. The new algorithm achieves up to 22.0% relative decomposition residual improvement compared to the state-of-the-art sketched randomized algorithm for Tucker decomposition of various synthetic and real datasets. This Tucker-ALS algorithm is further used to accelerate CP decompo-sition, by using randomized Tucker compression followed by CP decomposition of the Tucker core tensor. Experimental results show that this algorithm not only converges faster, but also yields more accurate CP decompositions. 1

Introduction
Tensor decompositions [31] are general tools for compressing, approximating, as well as extracting important features from high dimensional data, and are widely used in both scientiﬁc computing [54, 25, 26] and machine learning [4, 59, 55]. In this paper, we focus on Tucker decomposition [67] and
CANDECOMP/PARAFAC (CP) decomposition [24, 23]. The alternating least squares (ALS) method is widely used to compute both decompositions. The ALS algorithm consists of sweeps, and each sweep updates every factor matrix once in a ﬁxed order. The ALS method for Tucker decomposition, called the higher-order orthogonal iteration (HOOI) [5, 16, 31], updates one of the factor matrices along with the core tensor at a time. Similarly, each update procedure in the ALS algorithm for CP decomposition (CP-ALS) updates one of the factor matrices. For both decompositions, solutions to each optimization subproblem guarantee decrease of the decomposition residual.
In this work, we consider decomposition of order N tensors (TTT ) that are large in dimension size (s) and can be potentially sparse. We focus on the problem of computing low-rank (with target rank R (cid:28) s and R (cid:28) nnz(TTT )) decompositions for such tensors via ALS, which is often used for extracting principal component information from large-scale datasets. For Tucker decomposition,
ALS is bottlenecked by the operation called the tensor times matrix-chain (TTMc). For CP decompo-sition, ALS is bottlenecked by the operation called the matricized tensor-times Khatri-Rao product (MTTKRP). Both TTMc and MTTKRP have a per-sweep cost of Ω(nnz(TTT )R) [62]. Consequently, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the per-sweep costs of both HOOI and CP-ALS are proportional to the number of nonzeros in the tensor, which are expensive for large tensors with billions of nonzeros.
Recent works have applied different randomized techniques to accelerate both CP [32, 2, 14, 70] and Tucker decompositions [13, 3, 12, 40, 70, 65]. For Tucker decomposition, these randomized algorithms apply sketching techniques to the higher-order singular value decomposition (HOSVD).
To do so, they calculate each factor matrix by applying randomized SVD on each matricization of the input tensor. These methods calculate the core tensor via TTMc among the input tensor and all the factor matrices, which incurs a cost of O(nnz(T )R + sN −1R2) for sparse tensors and is still expensive. In addition, HOSVD generates decompositions that are generally less accurate compared to HOOI.
Becker and Malik [39] introduce a sketched ALS algorithm for Tucker decomposition, which avoids the expensive cost of TTMc. Unlike the traditional HOOI, each sweep of this ALS scheme contains
N + 1 subproblems, where only one of the factor matrices or the core tensor is updated in each subproblem. This scheme is easier to analyze theoretically, since each subproblem is an unconstrained linear least squares problem, which can be efﬁciently solved via sketching. However, the scheme produces decompositions that are generally less accurate than HOOI. 1.1 Our Contributions
In this work, we propose a new sketched ALS algorithm for Tucker decomposition. Different from
Becker and Malik [39], our ALS scheme is the same as HOOI, where one of the factor matrices along with the core tensor are updated in each subproblem. This guarantees the algorithm can reach the same accuracy as HOOI with sufﬁciently large sketch size. Experimental results show that it provides more accurate results compared to those in [39].
In this scheme, each subproblem is a sketched rank-constrained linear least squares problem, with the left-hand-side matrix with size sN −1 × RN −1 composed of orthonormal columns. To the best of our knowledge, the relative error analysis of sketching techniques for this problem have not been discussed in the literature. Existing works either only provide sketch size upper bounds for the relaxed problem [57], where rank constraint is relaxed with a nuclear norm constraint, or provide upper bounds for general constrained problems [69]. We provide tighter sketch size upper bounds to achieve O((cid:15)) relative error with two state-of-the-art sketching techniques, TensorSketch [52] and leverage score sampling [18].
With leverage score sampling, our analysis shows that with probability at least 1 − δ, the sketch size of O(cid:0)RN −1/((cid:15)2δ)(cid:1) is sufﬁcient for results with O((cid:15))-relative error. With TensorSketch, the sketch size upper bound is O(cid:0)(RN −1 · 3N −1) · (RN −1 + 1/(cid:15)2)/δ(cid:1), at least O(cid:0)3N −1(cid:1) times that for leverage score sampling. For both techniques, our bounds are at most O(1/(cid:15)) times the sketch size upper bounds for the unconstrained linear least squares problem.
The upper bounds suggest that under the same accuracy criteria, leverage score sampling potentially needs smaller sketch size for each linear least squares problem and thus can be more efﬁcient than
TensorSketch. Therefore, with the same sketch size, the accuracy with leverage score sampling can be better. However, with the standard random initializations for factor matrices, leverage score sampling can perform poorly on tensors with high coherence [10] (the orthogonal basis for the row space of each matricization of the input tensor has large row norm variability), making it less robust than
TensorSketch. To improve the robustness of leverage score sampling, we introduce an algorithm that uses the randomized range ﬁnder (RRF) [22] to initialize the factor matrices. The initialization scheme uses the composition of CountSketch and Gaussian random matrix as the RRF embedding matrix, which only requires one pass over the non-zero elements of the input tensor. Our experimental results show that the leverage score sampling based randomized algorithm combined with this RRF scheme performs well on tensors with high coherence.
For R (cid:28) s, our new sketching based algorithm for Tucker decomposition can also be used to accelerate CP decomposition. Tucker compression is performed ﬁrst, and then CP decomposition is applied to the core tensor [70, 9, 20]. Since the per-sweep costs for both sketched Tucker-ALS and sketched CP-ALS are comparable, and Tucker-ALS often needs much fewer sweeps than CP-ALS (Tucker-ALS typically converges in less than 5 sweeps based on our experiments), this Tucker + CP method can be more efﬁcient than directly applying randomized CP decomposition [32, 14] on the input tensor. 2
In summary, this paper makes the following contributions.
• We introduce a new sketched ALS algorithm for Tucker decomposition, which contains a sequence of sketched rank-constrained linear least squares subproblems. Experimental results show that the algorithm yields decomposition accuracy comparable to HOOI, and provides up to 22.0% relative decomposition residual improvement compared to the previous randomized Tucker algorithm.
• We provide theoretical upper bounds for the sketch size of both leverage score sampling and
TensorSketch, which ensure that each sketched rank-constrained linear least squares incurs O((cid:15)) relative error with high probability.
• We provide detailed comparison of TensorSketch and leverage score sampling in terms of efﬁciency and accuracy. Our theoretical analysis shows that leverage score sampling is better in terms of both metrics.
• We propose an initialization scheme based on RRF that improves the accuracy of leverage score sampling based sketching algorithm on tensors with high coherence.
• We show that CP decomposition can be more efﬁciently and accurately calculated based on the sketched Tucker + CP method, compared to directly performing sketched CP-ALS on the input tensor. 2