Abstract
Reinforcement learning has achieved great success in many applications. However, sample efﬁciency remains a key challenge, with prominent methods requiring millions (or even billions) of environment steps to train. Recently, there has been signiﬁcant progress in sample efﬁcient image-based RL algorithms; however, consistent human-level performance on the Atari game benchmark remains an elusive goal. We propose a sample efﬁcient model-based visual RL algorithm built on MuZero, which we name EfﬁcientZero. Our method achieves 190.4% mean human performance and 116.0% median performance on the Atari 100k benchmark with only two hours of real-time game experience and outperforms the state SAC in some tasks on the DMControl 100k benchmark. This is the ﬁrst time an algorithm achieves super-human performance on Atari games with such little data. EfﬁcientZero’s performance is also close to DQN’s performance at 200 million frames while we consume 500 times less data. EfﬁcientZero’s low sample complexity and high performance can bring RL closer to real-world applicability.
We implement our algorithm in an easy-to-understand manner and it is available at https://github.com/YeWR/EfficientZero. We hope it will accelerate the research of MCTS-based RL algorithms in the wider community.
Figure 1: Our proposed method EfﬁcientZero is 170% and 180% better than the previous SoTA performance in mean and median human normalized score and is the ﬁrst to outperform the average human performance on the Atari 100k benchmark. The high sample efﬁciency and performance of
EfﬁcientZero can bring RL closer to the real-world applications. 1

Introduction
Reinforcement learning has achieved great success on many challenging problems. Notable work includes DQN [24], AlphaGo [33] and OpenAI Five [5]. However, most of these works come at the cost of a large number of environmental interactions. For example, AlphaZero [34] needs to play 21 million games at training time. On the contrary, a professional human player can only play around 5 games per day, meaning it would take a human player 11,500 years to achieve the same amount of experience. The sample complexity might be less of an issue when applying RL algorithms in simulation and games. However, when it comes to real-world problems, such as robotic
∗{ywr20, liush20}@mails.tsinghua.edu.cn, gaoyangiiis@tsinghua.edu.cn
†{thanard.kurutach, pabbeel}@berkeley.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
manipulation, healthcare, and advertisement recommendation systems, achieving high performance while maintaining low sample complexity is the key to viability.
People have made a lot of progress in sample efﬁcient RL in the past years [8, 10, 35, 22, 21, 32, 18].
Among them, model-based methods have attracted a lot of attention, since both the data from real environments and the “imagined data” from the model can be used to train the policy, making these methods particularly sample-efﬁcient [8, 10]. However, most of the successes are in state-based environments. In image-based environments, some model-based methods such as MuZero [27] and Dreamer V2 [14] achieve super-human performance, but they are not sample efﬁcient; other methods such as SimPLe [18] is quite efﬁcient but achieve inferior performance (0.144 human normalized median scores). Recently, data-augmented and self-supervised methods applied to model-free methods have achieved more success in the data-efﬁcient regime [32]. However, they still fail to achieve the levels which can be expected of a human.
Therefore, for improving the sample efﬁciency as well as keeping superior performance, we ﬁnd the following three components are essential to the model-based visual RL agent: a self-supervised environment model, a mechanism to alleviate the model compounding error, and a method to correct the off-policy issue. In this work, we propose EfﬁcientZero, a model-based RL algorithm that achieves high performance with limited data. Our proposed method is built on MuZero. We make three critical changes: (1) use self-supervised learning to learn a temporally consistent environment model, (2) learn the value preﬁx in an end-to-end manner, thus helping to alleviate the compounding error in the model, (3) use the learned model to correct off-policy value targets.
As illustrated as Figure 1, our model achieves state-of-the-art performance on the widely used
Atari [4] 100k benchmark and it achieves super-human performance with only 2 hours of real-time gameplay. More speciﬁcally, our model achieves 190.4% mean human normalized performance and 116.0% median human normalized performance. As a reference, DQN [24] achieves 220% mean human normalized performance, and 96% median human normalized performance, at the cost of 500 times more data (200 million frames). To further verify the effectiveness of EfﬁcientZero, we conduct experiments on some simulated robotics environments of the DeepMind Control (DMControl) suite.
It achieves state-of-the-art performance and outperforms the state SAC which directly learns from the ground truth states. Our sample efﬁcient and high-performance algorithm opens the possibility of having more impact on many real-world problems. 2