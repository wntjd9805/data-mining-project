Abstract
We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations.
To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities’ representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset. We show that our approach outperforms alternative baselines, including shallow co-attention and full cross-modal attention.
We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective and, when combined with a word-to-region loss, achieves state of the art on recall-at-one and pointing hand accuracies. 1

Introduction
Content creators often add a voice-over narration to their videos to point out important moments and guide the viewer on where to look. While the timing of the narration in the audio track in the video gives the viewer a rough idea of when the described moment occurs, there is not explicit information on where to look. We, as viewers, naturally infer this information from the narration and often direct our attention to the spatial location of the described moment.
Inspired by this capability, we seek to have a recognition system learn to spatially localize narrated moments without strong supervision. We see a large opportunity for self-supervision as there is an abundance of online narrated video content. In this work, we primarily focus on spatially localizing transcribed narrated interactions in a video, illustrated in Figure 1 (right). Unlike prior phrase grounding work (Figure 1 left), which primarily focuses on matching a noun phrase to an object, our task involves matching entire sentences to regions containing multiple objects and actions.
Not only is this task integral to advancing machine perception of our world where information often comes in different modalities, it also has important implications in fundamental vision-and-language research such as robotics, visual question answering, video captioning, and retrieval. Our task is challenging as we do not know the correspondence between spatial regions in a video and words in the transcribed narration. Moreover, there is a loose temporal alignment of the narration with the described moment and not all words refer to spatial regions (and vice versa). Finally, narrated interactions may be complex with long-range dependencies and multiple actions. In Figure 1 (right), notice how “them” refers to the onions and that there are two actions – “cut” and “add”. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Prior work on phrase grounding focuses on localizing single objects in static images (left, example from Flickr30K [40]). In this paper, we propose a new task of spatially localizing narrated interactions in video (right). In the task of phrase grounding, the goal is to localize nouns or adjective-noun phrases in images. In contrast, interaction grounding may involve multiple objects and actions and use entire sentences. Our approach self-trains on a large corpus of narrated videos and does not require supervision. (Video credit: Woolworths [57])
Contrastive learning [38] and computing contextual features via an attention model [52] are natural tools for addressing the aforementioned alignment and long-range cross-modal dependency challenges.
While contrastive learning has been successfully applied for temporal alignment of a video clip with a narration [35], the alignment of spatial regions with narrations has not been addressed. Naively applying a joint attention model over the set of features for the two modalities, followed by optimizing a contrastive loss over the two modalities’ aggregated contextual features, results in a fundamental difﬁculty. Recall that an attention computation comprises two main steps: (i) selecting a set of features given a query feature, and (ii) aggregating the selected features via weighted averaging. By selecting and aggregating features over the two modalities during the attention computation and subsequently optimizing a contrastive loss, the model in theory may select and aggregate features from only one modality to trivially align the contextual representations.
While there are approaches for grounding natural language phrases in images without spatial supervi-sion, the best-performing approaches involve matching individual words to regions [22] or not using any paired text-image supervision at all [54]. This strategy is effective for simpler short phrases, as illustrated in Figure 1 (left). However, consider the interaction “cut the onions and add them to the tray”, shown in Figure 1 (right). This interaction is described with more complex, compositional language. In the ﬁrst two frames, there are multiple “onion" objects visible and the correct one depends on the action being applied. Unlike the task of object grounding, it is not sufﬁcient to simply co-localize all of the mentioned objects. We will show that it is difﬁcult to effectively employ a strategy that matches individual words to regions to localize such complex interactions.
To address these challenges, we make the following contributions. First, we learn to localize the spatial location of a narrated interaction from abundantly available narrated instructional videos [36]. Our approach does not require manually collecting sentence descriptions or the locations of the described interactions for training. Here, the automatically transcribed narrations allow for self-supervised learning via alignment with the video clip.
Second, we propose a new approach for directly contrasting aggregated representations from the two modalities while computing joint attention over spatial regions in a video and words in a narration.
We show that optimizing a loss over the aggregated, sentence-level representation allows for a global alignment of the described interaction with the video clip, and offers an improvement over optimizing a matching loss over words and regions. To overcome the network learning a trivial solution while directly contrasting jointly attended features from the two modalities during training, we design network attention layers that do not allow feature aggregation across the two modalities. Our strategy involves alternating network layers that compute inter- and intra-modal attention. Our inter-modal attention layer allows features from one modality to select and aggregate features from only the other modality, and not within its own modality. Our intra-modal attention layer selects and aggregates features from within the same modality. This strategy ensures that the output representations do not aggregate features across the two modalities. In combination with stacking the inter- and intra-modal attention layers, our approach attends jointly and deeply and directly contrasts the resulting contextual representations from the two modalities. 2
Finally, we introduce an evaluation dataset that provides bounding box annotations for interactions described by natural language sentences. Our dataset is built on the validation split of the YouCook2 dataset [65] and contains approximately 1000 segments of varying durations. We demonstrate our approach on our collected evaluation dataset of localized interactions and on localizing objects via the
YouCook2-BB benchmark where we show that we outperform shallow and full cross-modal attention.
We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective with our loss. Furthermore, we show that our loss is complementary with the word-to-region loss of Gupta et al. [22], and when combined with it, achieves state of the art on recall-at-one and pointing-hand accuracies. 2