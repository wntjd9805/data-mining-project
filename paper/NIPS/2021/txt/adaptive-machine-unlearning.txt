Abstract
Data deletion algorithms aim to remove the inﬂuence of deleted data points from trained models at a cheaper computational cost than fully retraining those models.
However, for sequences of deletions, most prior work in the non-convex setting gives valid guarantees only for sequences that are chosen independently of the models that are published. If people choose to delete their data as a function of the published models (because they don’t like what the models reveal about them, for example), then the update sequence is adaptive.
In this paper, we give a general reduction from deletion guarantees against adaptive sequences to deletion guarantees against non-adaptive sequences, using differential privacy and its connection to max information. Combined with ideas from prior work which give guarantees for non-adaptive deletion sequences, this leads to extremely ﬂexible algorithms able to handle arbitrary model classes and training methodologies, giving strong provable deletion guarantees for adaptive deletion sequences. We show in theory how prior work for non-convex models fails against adaptive deletion sequences, and use this intuition to design a practical attack against the
SISA algorithm of Bourtoule et al. [2021] on CIFAR-10, MNIST, Fashion-MNIST. 1

Introduction
Businesses like Facebook and Google depend on training sophisticated models on user data.
Increasingly—in part because of regulations like the European Union’s General Data Protection Act and the California Consumer Privacy Act—these organizations are receiving requests to delete the data of particular users. But what should that mean? It is straightforward to delete a customer’s data from a database and stop using it to train future models. But what about models that have already been trained using an individual’s data? These are not necessarily safe; it is known that individual training data can be exﬁltrated from models trained in standard ways via model inversion attacks
[Shokri et al., 2017, Veale et al., 2018, Fredrikson et al., 2015]. Regulators are still grappling with when a trained model should be considered to contain personal data of individuals in the training set and the potential legal implications. In 2020 draft guidance, the U.K.’s Information Commissioner’s
Ofﬁce addressed how to comply with data deletion requests as they pertain to ML models:
If the request is for rectiﬁcation or erasure of the data, this may not be possible without re-training the model...or deleting the model altogether [ICO, 2020].
Fully retraining the model every time a deletion request is received can be prohibitive in terms of both time and money—especially for large models and frequent deletion requests. The problem of data deletion (also known as machine unlearning) is to ﬁnd an algorithmic middle ground between the compliant but impractical baseline of retraining, and the potentially illegal standard of doing nothing. We iteratively update models as deletion requests come in, with the twin goals of having 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
computational cost that is substantially less than the cost of full retraining, and the guarantee that the models we produce are (almost) indistinguishable from the models that would have resulted from full retraining.
After an initial model is deployed deletion requests arrive over time as users make decisions about whether to delete their data. It is easy to see how these decisions may be adaptive with respect to the models. For example, security researchers may publish a new model inversion attack that identiﬁes a speciﬁc subset of people in the training data, thus leading to increased deletion requests for people in that subset. In this paper we give the ﬁrst machine unlearning algorithms that both have rigorous deletion guarantees against these kind of adaptive deletion sequence, and can accommodate arbitrary non-convex models like deep neural networks without requiring pretraining on non-user data. 1.1 Main Results
The deletion guarantees proven for several prior methods crucially rely on the implicit assumption that the points that are deleted are independent of the randomness used to train the models. However this assumption fails unless the sequence of deletion requests is chosen independently of the information that the model provider has made public. This is a very strong assumption, because users may wish to delete their data exactly because of what deployed models reveal about them.
We give a generic reduction. We show that if: 1. A data deletion algorithm RA for a learning algorithm A has deletion guarantees for oblivious sequences of deletion requests (as those from past work do), and 2. Information about the internal randomness of RA is revealed only in a manner that satisﬁes differential privacy, then (A, RA) also satisﬁes data deletion guarantees against an adaptive sequence of deletion requests, that can depend in arbitrary ways on the information that the model provider has made public.
In Section 3, we motivate our main result with a theoretical example which illustrates that past method’s lack of guarantees for adaptive sequences is not simply a failure of analysis, but an actual failure of these methods to satisfy deletion guarantees for adaptive deletion sequences. As an exemplar, we use a variant of SISA from Bourtoule et al. [2021] that satisﬁes perfect deletion guarantees for non-adaptive deletion sequences and exhibit adaptive deletion sequences that strongly separate the resulting distribution on models compared to the retraining baseline.
The generic reduction found in Section 4 can be used to give adaptive data deletion mechanisms for a wide variety of problems by leveraging past work on deletion algorithms for non-adaptive sequences, and a line of work on differentially private aggregation [Papernot et al., 2018, Dwork and Feldman, 2018]. Since prior deletion algorithms themselves tend to use existing learning algorithms in a black-box way, the entire pipeline is modular and easy to bolt-on to existing methods. In Section 5, we show how this can be accomplished by using a variant of the SISA framework of Bourtoule et al.
[2021] together with a differentially private aggregation method.
In Section 6, we complement our main result with a set of experimental results on CIFAR-10, MNIST, and Fashion-MNIST that demonstrate differential privacy may be useful in giving adaptive guarantees beyond the statement of our theorems. Speciﬁcally we show that small amounts of noise addition (insufﬁcient for our theorems to apply) already serve to break the adaptive deletion strategies that we use to falsify the adaptive deletion guarantees in our experiments described in Section 3 and do so at minimal expense in model accuracy. 1.2