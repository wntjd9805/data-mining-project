Abstract
From CNNs to attention mechanisms, encoding inductive biases into neural net-works has been a fruitful source of improvement in machine learning. Adding auxiliary losses to the main objective function is a general way of encoding biases that can help networks learn better representations. However, since auxiliary losses are minimized only on training data, they suffer from the same generalization gap as regular task losses. Moreover, by adding a term to the loss function, the model optimizes a different objective than the one we care about. In this work we address both problems: ﬁrst, we take inspiration from transductive learning and note that af-ter receiving an input but before making a prediction, we can ﬁne-tune our networks on any unsupervised loss. We call this process tailoring, because we customize the model to each input to ensure our prediction satisﬁes the inductive bias. Second, we formulate meta-tailoring, a nested optimization similar to that in meta-learning, and train our models to perform well on the task objective after adapting them using an unsupervised loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on a diverse set of examples. 1

Introduction
The key to successful generalization in machine learning is the encoding of useful inductive biases.
A variety of mechanisms, from parameter tying to data augmentation, have proven useful to improve the performance of models. Among these, auxiliary losses can encode a wide variety of biases, constraints, and objectives; helping networks learn better representations and generalize more broadly.
Auxiliary losses add an extra term to the task loss that is minimized over the training data.
However, they have two major problems: 1. Auxiliary losses are only minimized at training time, but not for the query points. This leads to a generalization gap between training and testing, in addition to that of the task loss. 2. By minimizing the sum of the task loss plus the auxiliary loss, we are optimizing a different objective than the one we care about (only the task loss).
In this work we propose a solution to each problem: 1. We use ideas from transductive learning to minimize unsupervised auxiliary losses at each query, thus eliminating their generalization gap. Because these losses are unsupervised, we can optimize them at any time inside the prediction function. We call this process tailoring, since we customize the model to each query. 2. We use ideas from meta-learning to learn a model that performs well on the task loss after being tailored with the unsupervised auxiliary loss; i.e. meta-tailoring. This effectively trains the model to leverage the unsupervised tailoring loss in order to minimize the task loss. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Comparison of several learning settings with ofﬂine computation in the orange boxes and online computation in the green boxes, with tailoring in blue. For meta-tailoring training,
τ (θ, tailor(x, θ�) represents the tailoring process resulting in θx. tailor, x) = argminθ�≈
L
θ L
Imagine you want to use a neural network to predict the motion of a planetary
Illustrative example system: given the positions and velocities of each planet, the network predicts their future positions and velocities. Additionally, we could encode energy and momentum conservation by adding an auxiliary loss encouraging the neural network to conserve energy and momentum for the training examples. However, this does not guarantee that the network will conserve them for test queries.
Alternatively, we can exploit that evaluating these conservations requires comparing only the input with the prediction without needing access to the true target. Therefore, we can enforce these conservations by optimizing an unsupervised objective within the prediction function. In doing so, we tailor the model to each individual query to ensure it satisﬁes energy and momentum conservation.
Taking into account this prediction-time adaptation during training leads to a two-layer optimization, where we train to make accurate predictions after encouraging the physical conservations.
Tailoring a predictor Traditionally, supervised learning is approached within the inductive learning framework, shown in the second row of Figure 1. There, an algorithm consumes a training dataset i=1, and produces a set of parameters ˆθ by minimizing a supervised of input-output pairs, ((xi, yi))n unsup(θ, xi). These loss parameters specify a hypothesis fˆθ(
) that, given a new input x, generates an output ˆy = fˆθ(x). This
· problem setting misses a substantial opportunity: before the learning algorithm sees the query point x, it has distilled the data down to the parameters ˆθ, which are frozen during inference, and so it cannot use new information about the particular x that it will be asked to make a prediction for. sup(fθ(xi), yi) and, optionally, an unsupervised auxiliary loss n i=1 L n i=1 L
�
�
Vapnik recognized an opportunity to make more accurate predictions when the query point is known, in a framework that is now known as transductive learning [50, 11], illustrated in the top row of
Figure 1. In transductive learning, a single algorithm consumes both labeled data, ((xi, yi))n i=1, and a set of input queries for which predictions are desired, (x(j))j, and produces predictions (ˆy(j))j for each query. In general, however, we do not know queries a priori, and instead, we want an inductive function that makes predictions online, as queries arrive. To obtain such an online prediction function from a transductive system, we would need to take the training data and the single unlabeled query and encapsulate the entire transductive learning procedure inside the prediction function itself.
This strategy would achieve our objective of taking x into account at prediction time but would be computationally much too slow [12].
This approach for combining induction and transduction would reuse the same training data and ob-jective for each prediction, only changing the single unlabeled query. Consequently, it would perform extremely similar computations for each prediction. Therefore, we propose to effectively reuse the shared computations and ﬁnd a “meta-hypothesis” that can then be efﬁciently adapted to each query.
As shown in the third row of Figure 1, we propose to ﬁrst run regular supervised learning to obtain parameters ˆθ. Then, given a query input x, we ﬁne-tune ˆθ on an unsupervised loss tailor to obtain cus-L 2
Algorithm 1 MAMmoTh: Model-Agnostic Meta-Tailoring sup, λsup,
Subroutine Training(f ,
Dtrain ,b) randomly initialize θ while not done do tailor, λtailor,
L
L
Sample batch of samples (xi, yi) forall (xi, yi) do
θxi = θ
−
λsup∇θ
λtailor∇θL (xi,yi) L tailor(θ, xi) fθxi sup
θ = θ return θ
−
∼ Dtrain
�
�
� (xi), yi
// Inner step with tailor loss
// Outer step with supervised loss tomized parameters θx and use them to make the ﬁnal prediction: fθx (x). We call this process tailor-ing, because we adapt the model to each particular input for a customized ﬁt. Notice that tailoring opti-mizes the loss at the query input, eliminating the generalization gap on the unsupervised auxiliary loss.
Meta-tailoring Since we will be applying tailoring at prediction time, it is natural to incorporate this adaptation during training, resulting in a two-layer optimization similar to those used in meta-learning. Because of this similarity, we call this process meta-tailoring, illustrated in the bottom row of Figure 1. Now, rather than letting ˆθ be the direct minimizer of the supervised loss, we set it to n
ˆθ arg min
θ
∈ sup(fτ (θ,
L
L tailor,xi)(xi), yi). i=1
�
L sup, instead of a proxy combination of
Here, the inner loop optimizes the unsupervised tailoring loss the supervised task loss care, tailor and the outer loop optimizes sup. Notice that now the outer process optimizes the only objective we unsup. At the same time, we learn to leverage sup and tailor in the inner loop to affect the model before making the ﬁnal prediction, both during training
L and evaluation. Adaptation is especially clear in the case of a single gradient step, as in MAML [19].
We show its translation, MAMmoTh (Model-Agnostic Meta-Tailoring), in algorithm 1.
L
L
L
L
In many settings, we want to make predictions for a large number of queries in a (mini-)batch. While
MAMmoTh adapts to every input separately, it can only be run efﬁciently in parallel in some deep learning frameworks, such as JAX [10]. Inspired by conditional normalization (CN) [18] we propose
CNGRAD, which adds element-wise afﬁne transformations to our model and only adapts the added parameters in the inner loop. This allows us to independently tailor the model for multiple inputs in parallel. We prove theoretically, in Sec. 4, and provide experimental evidence, in Sec. 5.1, that optimizing these parameters alone has enough capacity to minimize a large class of tailoring losses.
Relation between (meta-)tailoring, ﬁne-tuning transfer, and meta-learning Fine-tuning pre-trained networks is a fruitful method of transferring knowledge from large corpora to smaller related datasets [17]. This allows us to reuse features on related tasks or for different distributions of the same task. When the data we want to adapt to is unlabeled, we must use unsupervised losses. This can be useful to adapt to changes of task [16], from simulated to real data [52], or to new distributions [46].
Tailoring performs unsupervised ﬁne-tuning and is, in this sense, similar to test-time train-ing(TTT) [46] for a single sample, which adapts to distribution shifts. However, tailoring is applied to a single query; not to a data set that captures distribution shift, where batched TTT sees most of its beneﬁts. Thus, whereas regular ﬁne-tuning beneﬁts from more adaptation data, tailoring would be hindered by adapting simultaneously to more data. This is because tailoring aims at building a custom model for each query to ensure the network satisﬁes a particular inductive bias. Customizing the model to multiple samples makes it harder, not easier. We show this in Figure 2, where TTT with 6400 samples performs worse than tailoring with a single sample. Furthermore, tailoring adapts to each query one by one, not globally from training data to test data. Therefore, it also makes sense to do tailoring on training queries (i.e., meta-tailoring).
Meta-tailoring has the same two-layer optimization structure as meta-learning. More concretely, it can be understood as the extreme case of meta-learning where each single-query prediction is its own task. However, whereas meta-learning tasks use one loss and different examples for the inner sup). and outer loop, meta-tailoring tasks use one example and different losses for each loop (
We emphasize that meta-tailoring does not operate in the typical multi-task meta-learning setting.
Instead, we are leveraging techniques from meta-learning for the classical single-task setting. tailor,
L
L 3
Contributions
In summary, our contributions are: 1. Introducing tailoring, a new framework for encoding inductive biases by minimizing unsuper-vised losses at prediction time, with theoretical guarantees and broad potential applications. 2. Formulating meta-tailoring, which adjusts the outer objective to optimize only the task loss, and developing a new algorithm, CNGRAD, for efﬁcient meta-tailoring. 3. Demonstrating meta-tailoring in 3 domains: encoding hard and soft conservation laws in physics prediction problems (Sec. 5.1 and Sec. 5.2), enhancing resistance to adversarial examples by increasing local smoothness at prediction time (Sec. 5.4), and improving prediction quality both theoretically (Sec. 3.1) and empirically (Sec. 5.3) by tailoring with a contrastive loss. 2