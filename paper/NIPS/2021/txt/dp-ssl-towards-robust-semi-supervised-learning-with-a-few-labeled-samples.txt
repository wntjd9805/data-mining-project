Abstract
The scarcity of labeled data is a critical obstacle to deep learning. Semi-supervised learning (SSL) provides a promising way to leverage unlabeled data by pseudo labels. However, when the size of labeled data is very small (say a few labeled samples per class), SSL performs poorly and unstably, possibly due to the low quality of learned pseudo labels. In this paper, we propose a new SSL method called
DP-SSL that adopts an innovative data programming (DP) scheme to generate probabilistic labels for unlabeled data. Different from existing DP methods that rely on human experts to provide initial labeling functions (LFs), we develop a multiple-choice learning (MCL) based approach to automatically generate LFs from scratch in SSL style. With the noisy labels produced by the LFs, we design a label model to resolve the conﬂict and overlap among the noisy labels, and
ﬁnally infer probabilistic labels for unlabeled samples. Extensive experiments on four standard SSL benchmarks show that DP-SSL can provide reliable labels for unlabeled data and achieve better classiﬁcation performance on test sets than existing SSL methods, especially when only a small number of labeled samples are available. Concretely, for CIFAR-10 with only 40 labeled samples, DP-SSL achieves 93.82% annotation accuracy on unlabeled data and 93.46% classiﬁcation accuracy on test data, which are higher than the SOTA results. 1

Introduction
The de-facto approaches to deep learning achieve phenomenal success with the release of huge labeled datasets. However, large manually-labeled datasets are time-consuming and expensive to acquire, especially when expert labelers are required. Nowadays, many techniques are proposed to alleviate the burden of manual labeling and help to train models from scratch, such as active learning [1], crowd-labeling [2], distant supervision [3], semi [4]/weak [5]/self-supervision [6]. Among them, semi-supervised learning (SSL) is one of the most popular techniques to cope with the scarcity of labeled data. Two major strategies of SSL are pseudo labels [7] and consistency regularization [8].
Pseudo labels (also called self-training [9]) utilize a model’s predictions as the labels to train the model again, while consistency of regularization forces a model to make the same prediction under different transformations. However, when the size of labeled data is small, SSL performance degrades drastically in both accuracy and robustness. Fig. 1 shows the change of prediction error rate with the number of labeled samples of CIFAR-10. When the number of labeled samples reduces from 250 to 40, error rates of major existing SSL methods increase from 4.74% (USADTM) to 36.49% (MixMatch). One possible reason of performance deterioration is the quality degradation of learnt pseudo labels when labeled data size is small. Therefore, in this paper we address this problem by
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
developing sophisticated labeling techniques for unlabeled data to boost SSL even when the number of labeled samples is very small (e.g. a few labeled samples per class).
Recently, data programming (DP) was proposed as a new paradigm of weak supervision [10]. In DP, human experts are required to transform the decision-making process into a series of small functions (called labeling functions, abbre-viated as LFs), thus data can be labeled programmatically.
Besides, a label model is applied to determining the correct labels based on consensus from the noisy and conﬂicting labels assigned by the LFs. Such a paradigm achieves considerable success in NLP tasks [11–14]. In addition,
DP has also been applied to computer vision tasks [15, 16].
However, current DP methods require human experts to provide initial LFs, which is time-consuming and expen-sive, and it is not easy to guarantee the quality of LFs.
Furthermore, LFs speciﬁcally deﬁned for one task usually cannot be re-used for other tasks.
Figure 1: Error rate vs. #labeled samples (CIFAR-10). Results of existing meth-ods are from the original papers. When only 40 labeled samples are given, all existing SSL methods are substantially degraded and more unstably, while our method is still effective and robust.
In this paper, we propose a new SSL method called DP-SSL that is effective and robust even when the number of labeled samples is very small. In DP-SSL, an inno-vative data programming (DP) scheme is developed to generate probabilistic labels for unlabeled data. Different from existing DP methods, we develop a multiple-choice learning (MCL) based approach to automatically generate
LFs from scratch in SSL style. To remedy the over-conﬁdence problem with existing MCL methods, we assign an additional option as abstention for each LF. After that, we design a label model to resolve the conﬂict and overlap among the noisy labels generated by LFs, and infer a probabilistic label for each unlabeled sample. Finally, the probabilistic labels are used to train the end model for classifying unlabeled data. Our experiments validate the effectiveness and advantage of DP-SSL. As shown in Fig. 1, DP-SSL performs best, and only 1.76% increase of error rate when the number of labeled samples decreases from 250 to 40 in CIFAR-10.
Note that the pseudo labels used in existing SSL methods are quite different from the probabilistic labels in DP-SSL, which may explain the advantage of DP-SSL over existing SSL methods. On the one hand, pseudo labels are “hard” labels that indicate an unlabeled sample belonging to a certain class or not, while probabilistic labels are “soft” labels that indicate the class distributions of unlabeled samples. Obviously, the latter should be more ﬂexible and robust. On the other hand, pseudo labels are actually generated by a single model for all unlabeled samples, while probabilistic labels are generated from a number of diverse and specialized LFs (due to the MCL mechanism), which makes the latter more powerful in generalization as a whole.
In summary, the contributions of this paper are as follows: 1) We propose a new SSL method DP-SSL that employs an innovative data programming method to generate probabilistic labels for unlabeled data, which makes DP-SSL effective and robust even when there are only a few labeled samples per class. 2) We develop a multiple choice learning based approach to automatically generate diverse and specialized LFs from scratch for unlabeled data in SSL manner. 3) We design a label model with a novel potential and an unsupervised quality guidance regularizer to infer probabilistic labels from the noisy labels generated by LFs. 4) We conduct extensive experiments on four standard benchmarks, which show that DP-SSL outperforms the state-of-the-art methods, especially when only a small number of labeled samples are available, DP-SSL is still effective and robust. 2