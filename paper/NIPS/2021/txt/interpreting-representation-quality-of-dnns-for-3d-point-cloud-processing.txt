Abstract
In this paper, we evaluate the quality of knowledge representations encoded in deep neural networks (DNNs) for 3D point cloud processing. We propose a method to disentangle the overall model vulnerability into the sensitivity to the rotation, the translation, the scale, and local 3D structures. Besides, we also propose metrics to evaluate the spatial smoothness of encoding 3D structures, and the representation complexity of the DNN. Based on such analysis, experiments expose representation problems with classic DNNs, and explain the utility of the adversarial training. The code will be released when this paper is accepted. 1

Introduction
Deep neural networks (DNNs) have exhibited superior performance in various tasks, but the black-box nature of DNNs hampers the analysis of knowledge representations. Previous studies on explainable AI mainly focused on the following two directions. The ﬁrst is to explain the knowledge encoded in a DNN, e.g. visualizing patterns encoded by a DNN [24, 36, 50] and estimating the saliency/importance/attribution of input variables w.r.t. the network output [20, 30, 57]. The second is to evaluate the representation power of a DNN, e.g. the generalization and robustness of feature representations.
This paper focuses on the intersection of the above two directions, i.e. analyzing the quality of knowledge representations of DNNs for 3D point cloud processing. Speciﬁcally, we aim to design metrics to illustrate properties of feature representations for different point cloud regions, including various types of regional sensitivities, the spatial smoothness of encoding 3D structures, and the representation complexity of a DNN. These metrics provide new perspectives to diagnose DNNs for 3D point clouds. For example, unlike the image processing relying on color information, the processing of 3D point clouds usually exclusively uses 3D structural information for classiﬁcation.
Therefore, a well-trained DNN for 3D point cloud processing is supposed to just use scale information and 3D structures for inference, and be robust to the rotation and translation.
Regional sensitivities. We ﬁrst propose six metrics to disentangle the overall model vulnerability into the regional rotation sensitivity, the regional translation sensitivity, the regional scale sensitivity, and three types of regional structure sensitivity (sensitivity to edges, surfaces, and masses), so that we can use such sensitivity metrics to evaluate the representation quality of a DNN (as Fig. 1 (a-b) shows). Each sensitivity metric (let us take the rotation sensitivity for example) is deﬁned as the vulnerability of the regional attribution when we rotate the 3D point cloud with different angles. The regional attribution is computed as the Shapley value, which is widely used as a unique unbiased estimation of an input variable’s attribution [11, 20, 31, 37] from the perspective of game theory.
∗This work was done when Wen Shen was an intern at Shanghai Jiao Tong University.
†Quanshi Zhang is the corresponding author. This study was done under the supervision of Dr. Quanshi
Zhang. He is with the John Hopcroft Center and the MoE Key Lab of Artiﬁcial Intelligence, AI Institute, at the
Shanghai Jiao Tong University, China. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) Comparison of regional sensitivities of PointNet++ [28]. (b) Visualization of regional sensitivities of PointNet++ [28], where the heatmap is normalized in each sample. These colorbars are shown in log-scale. (c) Illustration of the regional smoothness. (d) Illustration of the representation complexity.
Based on the sensitivity metrics, we have discovered the following new insights, which have exposed problems with classic DNNs.
• Insight 1. Rotation robustness is the Achilles’ heel of most DNNs for point cloud processing. The rotation sensitivities of PointNet [26], PointNet++ [28], DGCNN [43], the non-dynamic version of
DGCNN (referred to as GCNN in this paper) [43], and PointConv [45] are much higher than other sensitivities (see Fig. 1 (a)).
• Insight 2. It is usually difﬁcult for DNNs to extract rotation-robust features from 3D points at edges and corners. These points are usually vulnerable to rotation-based attacks.
• Insight 3. What’s worse, DNNs usually cannot ignore features of such points at edges and corners.
Instead, such rotation-sensitive points usually have large regional attributions.
• Insight 4. PointNet fails to model local 3D structures, because convolutional operations in PointNet encode each point independently. Thus, PointNet has low sensitivity to local 3D structures.
Spatial smoothness. Beyond the sensitivity metrics, we also evaluate the spatial smoothness of knowledge representations of DNNs. We deﬁne the spatial smoothness of knowledge representations as the similarity among the neighboring regions’ attributions to network output. Most widely-used benchmark datasets for point cloud classiﬁcation [46, 49] only contain objects with simple 3D structures (e.g. a bed mainly composed of a few surfaces in Fig. 1 (c)), in which adjacent regions usually have similar 3D structures. Therefore, most adjacent regions are supposed to have similar attributions to the network output. We also ﬁnd that the adversarial training increases the spatial smoothness of knowledge representations.
Representation complexity. Besides regional sensitivities, we further extend the metric of multi-order interaction [53] to evaluate the representation complexity of a DNN, i.e. the maximum complexity of 3D structures that can be encoded in a DNN. The interaction of the m-th order measures the additional beneﬁts brought by collaboration between two point cloud regions i, j under the contexts of other m regions, where m reﬂects the contextual complexity of the collaboration between regions i and j. High-order interactions usually represent global and complex 3D structures, corresponding to the collaboration between point cloud regions i, j and massive other regions. Low-order interactions usually describe local and simple 3D structures, without being inﬂuenced by many contextual regions (see Fig. 1 (d)). Based on such interactions, we have discovered the following new insights.
• Insight 5. Most DNNs fail to encode high-order interactions. I.e. most DNNs mainly focus on local structures and do not extract many global and complex structures from normal samples.
• Insight 6. Rotation-sensitive regions usually activate abnormal high-order interactions (global and complex structures). The very limited global structures modeled by the DNN are usually over-ﬁtted to training samples. Thus, when the sample is rotated, the activated global and complex structures usually appear as abnormal patterns. 2
• Insight 7. Besides, in terms of model robustness, adversarial training based on adversarial samples w.r.t. using rotations/translations for attack is a typical way to improve the robustness to rotation and translation. Thus, we use the proposed sensitivity metrics to explain the inner mechanism of how and why the adversarially trained DNN is robust to rotation and translation. We ﬁnd that (1) the adversarial training shifts the attention from the global orientations and positions to the structural information, thereby boosting the sensitivity to local 3D structures. (2) The adversarial training usually increases both the quantity and the complexity of 3D structures (high-order interactions from normal samples) modeled in a DNN, which boosts the robustness to the rotation and translation. 2