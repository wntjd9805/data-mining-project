Abstract
We introduce the problem of sleeping dueling bandits with stochastic preferences and adversarial availabilities (DB-SPAA). In almost all dueling bandit applica-tions, the decision space often changes over time; eg, retail store management, online shopping, restaurant recommendation, search engine optimization, etc. Sur-prisingly, this ‘sleeping aspect’ of dueling bandits has never been studied in the literature. Like dueling bandits, the goal is to compete with the best arm by sequen-tially querying the preference feedback of item pairs. The non-triviality however results due to the non-stationary item spaces that allow any arbitrary subsets items to go unavailable every round. The goal is to ﬁnd an optimal ‘no-regret’ policy that can identify the best available item at each round, as opposed to the standard ‘ﬁxed best-arm regret objective’ of dueling bandits. We ﬁrst derive an instance-speciﬁc lower bound for DB-SPAA Ω((cid:80)K−1 (cid:80)K log T
∆(i,j) ), where K is the number of i=1 items and ∆(i, j) is the gap between items i and j. This indicates that the sleeping problem with preference feedback is inherently more difﬁcult than that for classical multi-armed bandits (MAB). We then propose two algorithms, with near optimal regret guarantees. Our results are corroborated empirically. j=i+1 1

Introduction
The problem of Dueling-Bandits has gained much attention in the machine learning community
[34, 38, 36], which is an online learning framework that generalizes the standard multiarmed bandit (MAB) [6] setting for identifying a set of ‘good’ arms from a ﬁxed decision-space (set of arms/items) by querying preference feedback of actively chosen item-pairs. More formally, in dueling bandits, the learning proceeds in rounds: At each round, the learner selects a pair of arms and observes stochastic preference feedback of the winner of the comparison (duel) between the selected arms; the objective of the learner is to minimize the regret with respect to a (or set of) ‘best’ arm(s) in hindsight. Towards this several algorithms have been proposed [2, 37, 21, 14]. Due to the inherent exploration-vs-exploitation tradeoff of the learning framework and several advantages of preference feedback [9, 35], many real-world applications can be modeled as dueling bandits, including movie recommendations, retail management, search engine optimization, job scheduling, etc.
However, in reality, the decision spaces might often change over time due to the non-availability of some items, which are considered to be ‘sleeping’. This ‘sleeping-aspect’ of online decision making problems has been widely studied in the standard multiarmed bandit (MAB) literature
[17, 24, 15, 19, 18, 11]. There the goal is to learn a ‘no-regret’ policy that maps to the ‘best awake item’ of any available (non-sleeping) subset of items, and the learner’s performance is measured with respect to the optimal policy in hindsight. This setting is famously known as Sleeping Bandits in
MAB [17, 24, 15, 11]. More discussions are given in