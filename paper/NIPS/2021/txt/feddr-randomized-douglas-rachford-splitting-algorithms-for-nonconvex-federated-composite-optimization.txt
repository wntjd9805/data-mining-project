Abstract
We develop two new algorithms, called, FedDR and asyncFedDR, for solving a fundamental nonconvex composite optimization problem in federated learning. Our algorithms rely on a novel combination between a nonconvex Douglas-Rachford splitting method, randomized block-coordinate strategies, and asynchronous im-plementation. They can also handle convex regularizers. Unlike recent methods in the literature, e.g., FedSplit and FedPD, our algorithms update only a subset of users at each communication round, and possibly in an asynchronous manner, making them more practical. These new algorithms can handle statistical and sys-tem heterogeneity, which are the two main challenges in federated learning, while achieving the best known communication complexity. In fact, our new algorithms match the communication complexity lower bound up to a constant factor under standard assumptions. Our numerical experiments illustrate the advantages of our methods over existing algorithms on synthetic and real datasets. 1

Introduction
Training machine learning models in a centralized fashion becomes more challenging and marginally inaccessible for a large number of users, especially when the size of datasets and models is growing substantially larger. Consequently, training algorithms using decentralized and distributed approaches comes in as a natural replacement. Among several approaches, federated learning (FL) has received tremendous attention in the past few years since it was ﬁrst introduced in [18, 30]. In this setting, a central server coordinates between many local users (also called agents or devices) to perform their local updates, then the global model will get updated, e.g., by averaging or aggregating local models.
Challenges. FL provides a promising solution for many machine learning applications such as learning over smartphones or across organizations, and internet of things, where privacy protection is one of the most critical requirements. However, this training mechanism faces a number of fundamental challenges, see, e.g., [31]. First, when the number of users gets substantially large, it creates communication bottleneck during model exchange process between server and users. Second, the local data stored in each local user may be different in terms of sizes and distribution which poses a challenge: data or statistical heterogeneity. Third, the variety of users with different local storage, computational power, and network connectivity participating into the system also creates a major challenge, known as system heterogeneity. This challenge also causes unstable connection 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
between server and users, where some users may be disconnected from the server or simply dropped out during training. In practice, we can expect only a subset of users to participate in each round of communication. Another challenge in FL is privacy concern. Accessing and sharing local raw data is not permitted in FL. In addition, distributed methods exchange the objective gradient of local users, and private data can be exposed from the shared model such as the objective gradients [51]. Therefore,
FL methods normally send the global model to each user at the start of each communication round, each user will perform its local update and send back only the necessary update for aggregation.
Our goal and approach. Our goal in this paper is to further and simultaneously address these funda-mental challenges by proposing two new algorithms to train the underlying common optimization model in FL. Our approach relies on a novel combination between randomized block-coordinate strategy, nonconvex Douglas-Rachford (DR) splitting, and asynchronous implementation. While each individual technique or partial combinations is not new, our combination of three as in this paper appears to be the ﬁrst in the literature. To the best of our knowledge, this is the ﬁrst work developing randomized block-coordinate DR splitting methods for nonconvex composite FL optimization models, and they are fundamentally different from some works in the convex setting, e.g., [7, 8].
Contribution. Our contribution can be summarized as follows. (a) We develop a new FL algorithm, called FedDR (Federated Douglas-Rachford), by combin-ing the well-known DR splitting technique and randomized block-coordinate strategy for the common nonconvex composite optimization problem in FL. Our algorithm can handle nonsmooth convex regularizers and allows inexact evaluation of the underlying proximal operators as in FedProx or FedPD. It also achieves the best known O (cid:0)ε−2(cid:1) communication complexity for ﬁnding a stationary point under standard assumptions (Assumptions 2.1-2.2), where ε is a given accuracy. More importantly, unlike FedSplit [33] and FedPD [49], which require full user participation to achieve convergence, our analysis does allow partial participation by selecting a subset of users to perform update at each communication round. (b) Next, we propose an asynchronous algorithm, asyncFedDR, where each user can asyn-chronously perform local update and periodically send the update to the server for proximal aggregation. We show that asyncFedDR achieves the same communication complexity
O (cid:0)ε−2(cid:1) as FedDR (up to a constant factor) under the same standard assumptions. This algorithm is expected to simultaneously address all challenges discussed above.
Let us emphasize some key points of our contribution. First, the best known O (cid:0)ε−2(cid:1) communication complexity of our methods matches the lower bound complexity up to a constant factor as shown in [49], even with inexact evaluation of the objective proximal operators. Second, our methods rely on a DR splitting technique for nonconvex optimization and can handle possibly nonsmooth convex regularizers, which allows us to deal with a larger class of applications and with constraints [47].
Furthermore, it can also handle both statistical and system heterogeneity as discussed in FedSplit
[33] and FedPD [49]. However, FedSplit only considers the convex case, and both FedSplit and
FedPD require all users to update at each communication round, making them less practical and applicable in FL. Our methods only require a subset of users or even one user to participate in each communication round as in FedAvg or FedProx. In addition, our aggregation step on the server is different from most existing works due to a proximal step on the regularizer. It is also different from
[47]. Third, as FedProx [23], we allow inexact evaluation of users’ proximal operators with any local solver (e.g., local SGD or variance reduced methods) and with adaptive accuracies. Finally, requiring synchronous aggregation at the end of each communication round may lead to slow-down in training due to the heterogeneity in computing power and communication capability of local users. It is natural to have asynchronous update from local users as in, e.g., [34, 35, 39]. Our asynchronous variant, asyncFedDR, can fairly address this challenge. Moreover, it uses a general probabilistic model recently introduced in [5], which allows us to capture the variety of asynchronous environments and architectures compared to existing methods, e.g., [39, 44].