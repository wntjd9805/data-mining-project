Abstract
Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks) without requiring labeled data to learn useful semantic represen-tations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words in text; yet predicting this known information helps in learning representations effective for downstream prediction tasks.
We posit a mechanism exploiting the statistical connections between certain reconstruction-based pretext tasks that guarantee to learn a good representation.
Formally, we quantify how the approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task by just training a linear layer on top of the learned representation. We prove the linear layer yields small approxima-tion error even for complex ground truth function class and will drastically reduce labeled sample complexity. 1

Introduction
Self-supervised learning revitalizes machine learning models in computer vision, NLP, and control problems (see reference therein [36, 38, 15, 63, 35]). Training a model with auxiliary tasks based only on input features reduces the extensive costs of data collection and semantic annotations for downstream tasks. It is also known to improve the adversarial robustness of models [29, 11, 12].
Self-supervised learning creates pseudo labels solely based on input features, and solves auxiliary prediction tasks (or pretext tasks) in a supervised manner. However, the underlying principles of self-supervised learning are mysterious since it is a-priori unclear why predicting what we already know should help. We thus raise the following question:
What conceptual connection between pretext and downstream tasks ensures good representations?
What is a good way to quantify this?
As a thought experiment, consider a simple downstream task of classifying desert, forest, and sea images. A meaningful pretext task is to predict the background color of images (known as image colorization [66]). Denote X1, X2, Y to be the input image, color channel, and the downstream label respectively. Given knowledge of the label Y , one can possibly predict the background X2 without knowing much about X1. In other words, X2 is approximately independent of X1 conditional on the label Y . Consider another task of inpainting [48] the front of a building (X2) from the rest (X1).
While knowing the label “building” (Y ) is not sufficient for successful inpainting, adding additional latent variables Z such as architectural style, location, window positions, etc. will ensure that variation in X2 given Y, Z is small. We can mathematically interpret this as X1 being approximate conditionally independent of X2 given Y, Z.
The main insight that we exploit in this work is that with approximate conditional independence (as in the above examples), a method that predicts X2 from X1 will inadvertently implicitly encode and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learn to predict Y (and Z) from X1 as an intermediate step, and then predict X2 from Y 1. Building upon this insight, we make the following contributions.
Contributions. The goal of this paper, as in statistical learning theory, is to investigate the statistical connections between the random variables of input features (in this paper (X1, X2)) and downstream labels Y , and show how specific connections can guarantee a successful learning procedure. For self-supervised learning (SSL), success is measured using the following 2 notions, 1) expressivity, i.e. does the learned representation from SSL have the ability to express the ground truth prediction function for labels Y , and 2) sample complexity, i.e. can it do so with way fewer labeled samples than what would be required without SSL.
In this work, we show such guarantees for a class of reconstruction-based SSL methods under a statistical assumption of approximate conditional independence (ACI). In particular we show that under such an assumption, the learned representation from SSL will end up having the following properties, 1) it can express the ground truth label as a linear function, thus guaranteeing expressivity, and 2) will also end up being low-rank (or low-dimensional), thus guaranteeing smaller labeled sample complexity. Note that such an expressive and sample efficient (summarized as good) representation is often not a-priori available. For instance, the original input features themselves may not be able to express the ground truth function linearly, while kernel methods with a fixed kernel, while expressive, may not be sample efficient for many problems of interest. The strategy in modern machine learning is to find such a good representation as the output of a complicated neural network. The benefit of SSL, as we formally show here, is that the complicated but good representation function can be learned using just unlabeled data, so that labeled data is just needed to learn a linear function.
The reconstruction-based SSL method (differentiated from other SSL methods in Section 1.1) we consider is strongly motivated by empirical works [66, 48, 15, 25], but is a simplification that captures the essence of the problem and is amenable to a precise theoretical analysis. We consider a two-staged pipeline, where we first learn a representation function ψ (e.g. output of a neural network) from input X1 and pretext target X2 using unlabeled data by minimizing E(X1,X2)[∥X2 − ψ(X1)∥2].
In the second stage of downstream task, we learn a linear layer on top of representation ψ using labeled samples (X1, Y ), thus restricting to learning from a significantly smaller hypothesis class of Hψ = {f : X1 → Y |f is linear in ψ}. The key non-trivial question of expressivity is now whether the ground truth predictor f ∗ ≡ E[Y |X1] can be approximated well by this class Hψ, and the question of sample complexity reduces to the understanding the sample complexity of learning
Hψ. Under appropriate statistical connections2 between input data X1, X2 and target Y , we prove both the desired properties, expressivity and low sample complexity, for the aforementioned SSL method.
Our statistical assumption based on approximate conditional independence (ACI) helps us demon-strate how solving pretext tasks created from known information can learn useful representations.
Specifically, we show that once the complicated representation function ψ is learned using an abun-dance of unlabeled data in the SSL stage, not only is ψ expressive enough, but it will also require only ˜O(k) labeled samples to solve a k-way supervised learning task under exact conditional inde-pendence (CI). In contrast, solving the downstream task without any pretraining will require a lot of labeled data to learn the representation function from scratch. Since the strong exact conditional independence assumption will likely not be satisfied in practice, our main contribution is to derive similar risk bounds when only approximate CI (ACI) is satisfied. We quantify the notion of ACI using the norm of a certain partial covariance matrix (Definition 4.1) and our risk bound scales linearly with it. We verify this and other aspects of our main Theorem 4.2 using simulations and also find that pretext task helps when CI is approximately enforced in text domain. We further demonstrate on a real-world image dataset that a pretext task-based linear model performs at least as well as many baselines. 1.1