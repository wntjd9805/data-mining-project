Abstract
Most existing imitation learning approaches assume the demonstrations are drawn from experts who are optimal, but relaxing this assumption enables us to use a wider range of data. Standard imitation learning may learn a suboptimal policy from demonstrations with varying optimality. Prior works use conﬁdence scores or rankings to capture beneﬁcial information from demonstrations with varying opti-mality, but they suffer from many limitations, e.g., manually annotated conﬁdence scores or high average optimality of demonstrations. In this paper, we propose a general framework to learn from demonstrations with varying optimality that jointly learns the conﬁdence score and a well-performing policy. Our approach,
Conﬁdence-Aware Imitation Learning (CAIL) learns a well-performing policy from conﬁdence-reweighted demonstrations, while using an outer loss to track the performance of our model and to learn the conﬁdence. We provide theoretical guar-antees on the convergence of CAIL and evaluate its performance in both simulated and real robot experiments. Our results show that CAIL signiﬁcantly outperforms other imitation learning methods from demonstrations with varying optimality. We further show that even without access to any optimal demonstrations, CAIL can still learn a successful policy, and outperforms prior work. 1

Introduction
We consider an imitation learning setting that learns a well-performing policy from a mixture of demonstrations with varying optimality ranging from random trajectories to optimal demonstrations.
As opposed to standard imitation learning, where the demonstrations come from experts and thus are optimal, this beneﬁts from a larger and more diverse source of data. Note that different from setting that the demonstrations are optimal but lack some causal factors [31], in our setting, the demonstrations can be suboptimal. However, this introduces a new set of challenges. First, one needs to select useful demonstrations beyond the optimal ones. We are interested in settings where we do not have sufﬁcient expert demonstrations in the mixture so we have to rely on learning from sub-optimal demonstrations that can still be successful at parts of the task. Second, we need to be able to ﬁlter the negative effects of useless or even malicious demonstrations, e.g., demonstrations that implicitly fail the tasks.
To address the above challenges, we propose to use a measure of conﬁdence to indicate the likelihood that a demonstration is optimal. A conﬁdence score can provide a ﬁne-grained characterization of each demonstration’s optimality. For example, it can differentiate between near-optimal demonstrations or adversarial ones. By reweighting demonstrations with a conﬁdence score, we can simultaneously learn from useful but sub-optimal demonstrations while avoiding the negative effects of malicious ones. So
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
our problem reduces to learning an accurate conﬁdence measure for demonstrations. Previous work learns the conﬁdence from manually annotated demonstrations [30], which are difﬁcult to obtain and might contain bias—For example, a conservative and careful demonstrator may assign lower conﬁdence compared to an optimistic demonstrator to the same demonstration. In this paper, we remove restrictive assumptions on the conﬁdence, and propose an approach that automatically learns the conﬁdence score for each demonstration based on evaluation of the outcome of imitation learning.
This evaluation often requires access to limited evaluation data.
We propose a new algorithm, Conﬁdence-Aware Imitation Learning (CAIL), to jointly learn a well-performing policy and the conﬁdence for every state-action pair in the demonstrations. Speciﬁcally, our method adopts a standard imitation learning algorithm and evaluates its performance to update the conﬁdence scores with an evaluation loss, which we refer to as the outer loss. In our implementation, we use a limited amount of ranked demonstrations as our evaluation data for the outer loss. We then update the policy parameters using the loss of the imitation learning algorithm over the demonstrations reweighted by the conﬁdence, which we refer to as the inner loss. Our framework can accommodate any imitation learning algorithm accompanied with an evaluation loss to assess the learned policy.
We optimize for the inner and outer loss using a bi-level optimization [5], and prove that our algorithm converges to the optimal conﬁdence assignments under mild assumptions. We further implement the framework using Adversarial Inverse Reinforcement Learning (AIRL) [14] as the underlying imitation learning algorithm along with its corresponding learning loss as our inner loss. We design a ranking loss as the outer loss, which is compatible with the AIRL model and only requires easy-to-access ranking annotations rather than the exact conﬁdence values.
The main contributions of the paper can be summarized as:
• We propose a novel framework, Conﬁdence-Aware Imitation Learning (CAIL), that jointly learns conﬁdence scores and a well-performing policy from demonstrations with varying optimality.
• We formulate our problem as a modiﬁed bi-level optimization with a pseudo-update step and
T ) (T prove that the conﬁdence learned by CAIL converges to the optimal conﬁdence in O(1/ is the number of steps) under some mild assumptions.
√
• We conduct experiments on several simulation and robot environments. Our results suggest that the learned conﬁdence can accurately characterize the optimality of demonstrations, and that the learned policy achieves higher expected return compared to other imitation learning approaches. 2