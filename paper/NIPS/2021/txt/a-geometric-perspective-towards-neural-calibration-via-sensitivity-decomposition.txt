Abstract
It is well known that vision classiﬁcation models suffer from poor calibration in the face of data distribution shifts. In this paper, we take a geometric approach to this problem. We propose Geometric Sensitivity Decomposition (GSD) which decomposes the norm of a sample feature embedding and the angular similarity to a target classiﬁer into an instance-dependent and an instance-independent com-ponent. The instance-dependent component captures the sensitive information about changes in the input while the instance-independent component represents the insensitive information serving solely to minimize the loss on the training dataset. Inspired by the decomposition, we analytically derive a simple extension to current softmax-linear models, which learns to disentangle the two components during training. On several common vision models, the disentangled model out-performs other calibration methods on standard calibration metrics in the face of out-of-distribution (OOD) data and corruption with signiﬁcantly less complexity.
Speciﬁcally, we surpass the current state of the art by 30.8% relative improvement on corrupted CIFAR100 in Expected Calibration Error. Code available at https:
//github.com/GT-RIPL/Geometric-Sensitivity-Decomposition.git. 1

Introduction
During development, deep learning models are trained and validated on data from the same distribu-tion. However, in the real world sensors degrade and weather conditions change. Similarly, subtle changes in image acquisition and processing can also lead to distribution shift of the input data. This is often known as covariate shift, and will typically decrease the performance (e.g. classiﬁcation ac-curacy). However, it has been empirically found that the model’s conﬁdence remains high even when accuracy has degraded [1]. The process of aligning conﬁdence to empirical accuracy is called model calibration. Calibrated probability provides valuable uncertainty information for decision making.
For example, knowing when a decision cannot be trusted and more data is needed is important for safety and efﬁciency in real world applications such as self-driving [2] and active learning [3].
A comprehensive comparison of calibration methods has been studied for in-distribution (IND) data [4], However, these methods lead to unsatisfactory performance under distribution shift [5]. To resolve the problem, high-quality uncertainty estimation [6, 5] is required. Principled Bayesian meth-ods [7] model uncertainty directly but are computationally heavy. Recent deterministic methods [8, 9] propose to improve a model’s sensitivity to input changes by regularizing the model’s intermediate layers. In this context, sensitivity is deﬁned as preserving distance between two different input samples through layers of the model. We would like to utlize the improved sensitivity to better detect 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Out-of-Distribution (OOD) data. However, these methods introduce added architecture changes and large combinatorics of hyperparameters.
Unlike existing works, we propose to study sensitivity from a geometric perspective. The last linear layer in a softmax-linear model can be decomposed into the multiplication of a norm and a cosine similarity term [10, 11, 12, 13]. Geometrically, the angular similarity dictates the membership of an input and the norm only affects the conﬁdence in a softmax-linear model. Counter-intuitively, the norm of a sample’s feature embedding exhibits little correlation to the hardness of the input [11].
Based on this observation, we explore two questions: 1) why is a model’s conﬁdence insensitive to distribution shift? 2) how do we improve model sensitivity and calibration?
We hypothesize that in part an insensitive norm is responsible for bad calibration especially on shifted data. We observe that the sensitivity of the angular similarity increases with training whereas the sensitivity of the norm remains low. More importantly, calibration worsens during the period when the norm increases while the angular similarity changes slowly. This shows a concrete example of the inability of the norm to adapt when accuracy has dropped. Intuitively, training on clean datasets encourages neural networks to always output increasingly large feature norm to continuously minimize the training loss. Because the probability of the prevalent class of an input is proportional to its norm, larger norms lead to smaller training loss when most training data have been classiﬁed correctly (See Sec. 3.1). This renders the norm insensitive to input differences because the model is trained to always output features with large norm on clean data. While we have put forth that the norm is poorly calibrated, we must emphasize that it can still play an important role in model calibration (See Sec. 4.1).
To encourage sensitivity, we propose to decompose the norm of a sample’s feature embedding and the angular similarity into two components: instance-dependent and instance-independent. The instance-dependent component captures the sensitive information about the input while the instance-independent component represents the insensitive information serving solely to minimize the loss on the training dataset. Inspired by the decomposition, we analytically derive a simple extension to the current softmax-linear model, which learns to disentangle the two components during training. We show that our model outperforms other deterministic methods (despite their signiﬁcant complexity) and is comparable to multi-pass methods with fewer training hyperparameters in Sec. 4.1.
In summary, our contributions are four fold:
• We study the problem of calibration geometrically and identify that the insensitive norm is responsible for bad calibration under distribution shift.
• We derive a principled but simple geometric decomposition that decomposes the norm into an instance-dependent and instance-independent component.
• Based on the decomposition, we propose a simple training and inference scheme to encour-age the norm to reﬂect distribution changes.
• We achieve state of the art results in calibration metrics in the face of corruptions while having arguably the simplest calibration method to implement. 2