Abstract
Due to the discrete nature of words, language GANs require to be optimized from rewards provided by discriminator networks, via reinforcement learning methods.
This is a much harder setting than for continuous tasks, which enjoy gradient
ﬂows from discriminators to generators, usually leading to dramatic learning instabilities. However, we claim that this can be solved by making discriminator and generator networks cooperate to produce output sequences during training.
These cooperative outputs, inherently built to obtain higher discrimination scores, not only provide denser rewards for training, but also form a more compact artiﬁcial set for discriminator training, hence improving its accuracy and stability. In this paper, we show that our SelfGAN framework, built on this cooperative principle, outperforms Teacher Forcing and obtains state-of-the-art results on two challenging tasks, Summarization and Question Generation. 1

Introduction
Natural Language Generation encompasses tasks such as Machine Translation, Summarization or
Data To Text generation. The real life applications are numerous, but require highly reliable and
ﬂuent models. Despite signiﬁcant advances, state-of-the-art models are still known to be de-generated, with outputs containing repetitions and even nonfactual information i.e. hallucination [13].
Among the culprits is a limitation of Teacher Forcing [37]: the loss is computed at a token level while the aim is to produce complete sequences. Moreover, while a single ground-truth reference is considered correct, several realizations of the same content may exist. Finally, the model is subject to
Exposure Bias [27], i.e. a mismatch between training and inference distributions – in the latter, the model has no access to ground truth for the previously generated tokens. The literature has considered this mismatch responsible for the lower quality observed when generating longer sequences [2, 16].
To overcome such Teacher Forcing limitations, a consensus has emerged: a sequence level objective should be introduced [27, 41]. A body of work has proposed to use Reinforcement Learning (RL) with standard NLG metrics like BLEU [39] or ROUGE [23]. However, NLG metrics are known to not reﬂect well human judgement [21], which explains why the resulting models tend to be qualitatively worse than their MLE baselines [3]. To move toward less biased metrics, a natural alternative is to evaluate the output with a learned discriminator. An ideal discriminator would not be biased w.r.t. to its training set, and could therefore be considered as a perfect metric that matches human consensus.
Note that discriminators are already reported to be highly accurate to distinguish human written texts from machine generated ones [31, 43]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In light of this observation, two concurrent approaches have been explored: i) at training time, using
Generative Adversarial Networks [42]; and ii) at inference time, via cooperative decoding [11]: a discriminator guides the search algorithm, such that the generator and the discriminator cooperate to select the generated tokens. These approaches pursue the same objective: producing texts more similar to what a human writes.
Both methodologies suffer from speciﬁc limitations. Cooperative decoding algorithms rely on a discriminator that re-ranks a limited set of candidates selected by the generator.1 Hence, cooperative decoding algorithms are limited by the generator ability to rank relevant tokens in a good enough position. On the other hand, language GANs are learned via reinforcement learning due to the discrete nature of text. This makes them particularly unstable to train, and usually fall short compared to
Teacher Forcing [3]. In standard Language GANs, the discriminator provides a reward for the entire sequence, which can be difﬁcult to exploit by the generator due to its sparsity [6].
In this paper, we propose SelfGAN, a framework to learn language GANs in a Self -training process where the signal from the discriminator is passed to the generator in a completely new way. We consider cooperative algorithms as a way to infuse the discriminator signal. We start from a simple observation: outputs obtained via cooperative decoding are more human-like, compared to their generator-only counterparts. Inspired by recent knowledge distillation approaches, we propose to consider cooperative outputs as targets in a Teacher Forcing training process: cooperative decoding stands as a teacher we attempt to imitate through the generator network. Just like a standard GAN, both the generator and the discriminator are trained at each step. While the generator improves, it becomes adversarial to the discriminator, which beneﬁts from the cooperative generation. The discriminator, now trained on improved sequences, also contributes to improve the cooperative generation, and so forth. Note that in SelfGANs the discriminator is only used to drive the cooperative generation and never to provide a reward signal like in standard Language GANs.
SelfGAN can be implemented with any cooperative decoding algorithm. Current cooperative ap-proaches [7, 31] rely on "myopic" algorithms like Beam Search or Sampling that generate the tokens left-to-right. The model has to always predict the next word, and can never look back and revise past choices. In some cases, despite all the candidates being judged to likely not be human by the discriminator, the model is locked in a dead-end. This behavior is quite unnatural for humans – who often proofread their texts. We refer to this phenomenon as the left-to-right curse.
To address this left-to-right curse, we introduce Coop-MCTS, a new decoding algorithm based on
Monte Carlo Tree Search (MCTS) [5, 14]. We compare Coop-MCTS to state-of-the-art cooperative decoding algorithms in two scenarios: i) inference time, as the decoding algorithm; and ii) during training, as the cooperative algorithm in SelfGAN. In both scenarios, we show that the respective resulting outputs are more likely to look like human texts and improve all the automatic metrics.
All in all, our contributions can be summarized as follows: 1. SelfGAN We propose a new training framework based on cooperative decoding, wherein the generated sequences are used as ground truth; 2. Coop-MCTS We improve cooperative decoding with a new decoding algorithm, Coop-MCTS, offering a solution to the left-to-right limitation of current search methods; 3. We show that combining SelfGAN and Coop-MCTS compare favorably to prior state-of-the-art results on two challenging tasks, Summarization and Question Generation. 2