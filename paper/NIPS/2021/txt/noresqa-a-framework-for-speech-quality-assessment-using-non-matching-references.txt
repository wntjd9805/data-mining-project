Abstract
The perceptual task of speech quality assessment (SQA) is a challenging task for machines to do. Objective SQA methods that rely on the availability of the corresponding clean reference have been the primary go-to approaches for SQA.
Clearly, these methods fail in real-world scenarios where the ground truth clean references are not available. In recent years, non-intrusive methods that train neural networks to predict ratings or scores have attracted much attention, but they suffer from several shortcomings such as lack of robustness, reliance on labeled data for training and so on. In this work, we propose a new direction for speech quality assessment. Inspired by human’s innate ability to compare and assess the quality of speech signals even when they have non-matching contents, we propose a novel framework that predicts a subjective relative quality score for the given speech signal with respect to any provided reference without using any subjective data. We show that neural networks trained using our framework produce scores that correlate well with subjective mean opinion scores (MOS) and are also competitive to methods such as DNSMOS [1], which explicitly relies on
MOS from humans for training networks. Moreover, our method also provides a natural way to embed quality-related information in neural networks, which we show is helpful for downstream tasks such as speech enhancement.

Introduction 1
Speech quality assessment is critical for designing and developing a wide range of real-world audio and speech applications, such as, Telephony, VoIP, Hearing Aids, Automatic Speech Recognition,
Speech Enhancement etc. Clearly, the gold standard for SQA is the evaluation of speech recordings by humans. However, these subjective evaluations are not scalable and can be immensely time-consuming and costly, as they often need to be repeated tens or hundreds of times for every recording. Several objective methods for SQA have been developed to address this problem,
PESQ [2], POLQA [3], VISQOL [4], HASQI [5], DPAM [6] and CDPAM [7]. These methods are intrusive or full-reference by deﬁnition as they are designed to produce a quality score or rating by comparing the corrupted speech signal to its clean reference. However, they suffer from three critical drawbacks. First, the requirement of a paired clean reference for quality assessment limits their
∗Work done during internship at Facebook Reality Labs Research 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
applicability to real-world scenarios as the paired clean reference is likely not available in those cases.
Second, these methods have acknowledged shortcomings such as sensitivity to perceptually invariant transformations [8], therefore hindering stability in more diverse tasks such as speech enhancement.
Lastly, these metrics are non-differentiable and cannot be directly leveraged as training objectives in the context of neural networks.
To address these problems, a recent trend has been to develop non-intrusive [9] methods using neural networks [1, 10–20]. In most cases, the primary approach is to train a model to predict objective (e.g., PESQ) and/or subjective (e.g., MOS) scores. However, generalization to unseen perturbations and tasks remains a concern [21], and most methods have not found wide-spread uses for SQA.
Given that matching human subjective ratings is the ultimate motivation, some recent works try to train neural networks directly on MOS scores [20, 14, 1]. DNSMOS [1], in particular, trains neural networks on a very large-scale MOS database. However, collecting such a dataset is an uphill task and requires considerable resources. For e.g., one requires uniformity with respect to hardware (headphones/speakers), listening environments etc., among hundreds and thousands of raters to ensure that ratings are consistent. Otherwise, a signiﬁcant amount of noise can creep into the dataset, making it unreliable for training. In DNSMOS [1], almost half of the recordings have a standard deviation of more than 1 (MOS ±1 ), which poses challenges in training robust models due to noisy labels.
Lastly, we would like to point out an inherent challenge of the conventional formulation of any non-intrusive metric. The problem might lie in the lack of reference itself. While training any model on subjective scores, we expect the model to implicitly learn the distribution of references that are consciously or unconsciously used by human listeners. These references can be highly varied and strongly inﬂuenced by each individual’s past experience and even the mood when participating in the evaluations. Learning such a distribution can be an extremely hard problem, especially when no speciﬁc constraints on the distribution of clean reference are provided to the model during training.
In this work, we propose a novel and alternate framework for speech quality assessment. Instead of a completely reference-free approach, our framework relies on random non-matching references (NMRs) of known qualities, and is designed to provide a relative assessment of speech quality with respect to the NMRs. The inspiration for the approach comes from human’s ability to do the same.
Given two completely random speech recordings, it is highly likely that a human would be able to compare them with respect to quality irrespective of the actual speech content. This innate ability to compare the quality of two speech recordings in an “unsupervised" setting (or non-matching conditions) holds true even when the recordings contain different speakers, languages, words, and so on. Moreover, comparative tests are relatively easier for humans than absolute rating, and relative scores tend to have lower variance and less noise.
Motivated by the above points, we propose NORESQA - NOn-matching REference based Speech
Quality Assessment. Within this framework, we propose to learn neural networks that can predict a relative quality score for a given speech recording with respect to any provided reference. A few key characteristics of NORESQA are: (i) unlike full-reference objective metrics, it remains usable in real world situations by relying on NMRs which are readily available; (ii) it addresses the problem of lack of reference in non-intrusive methods by providing an NMR, thereby providing the necessary grounding for the model to learn and predict acoustic quality differences; (iii) pairwise comparisons are expected to have lower variance and less noise than absolute ratings (e.g MOS); and (iv) any learning within this framework is “unsupervised” in the sense that we do not require any manually labeled dataset for training models.
To summarize the key contributions of the paper: (1) we propose a novel framework for speech quality assessment that relies on NMRs; (2) we propose methods to train neural networks within this framework using multi-task multi-objective learning; and (3) we evaluate our framework comprehensively through several subjective and objective evaluations as well as exploring its utility in a downstream task like speech enhancement. 2