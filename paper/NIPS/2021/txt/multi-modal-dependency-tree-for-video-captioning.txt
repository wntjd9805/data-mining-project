Abstract
Generating ﬂuent and relevant language to describe visual content is critical for the video captioning task. Many existing methods generate captions using sequence models that predict words in a left-to-right order. In this paper, we investigate a graph structured model by explicitly modeling the hierarchical structure in the sentences to further improve the ﬂuency and relevance of the generated captions. To this end, we propose a novel video captioning method that generates a sentence by
ﬁrst constructing a multi-modal dependency tree and then traversing the constructed tree, where the syntactic structure and semantic relationship in the sentence are represented by the tree topology. To take full advantage of the information from both vision and language, both the visual and textual representation features are encoded into each tree node. Different from existing dependency parsing methods that generate uni-modal dependency trees for language understanding, our method constructs multi-modal dependency trees for language generation of videos. We also propose a tree-structured reinforcement learning algorithm to effectively optimize the captioning model, where a novel reward is designed by evaluating the semantic consistency between the generated sub-trees and the ground-truth tree. Extensive experiments on several video captioning datasets demonstrate the effectiveness of the proposed method. 1

Introduction
Video captioning has received extensive attention from researchers in both computer vision and natural language processing. It is a challenging task since it requires not only understanding the visual content but also generating ﬂuent and relevant sentences. With the success of deep neural networks in natural language processing, many existing video captioning methods [1, 2] use recurrent neural network (RNN) to generate sentences, which processes sequences by updating hidden states.
Several recent studies [3–6] apply Transformer [7], which relates the words in the sequences using a self-attention mechanism, to video captioning. All these methods treat each sentence as a word sequence and generate words in a predeﬁned left-to-right order by capturing the relatively close contextual relationship between words in the sentence.
In this paper, we investigate a graph structured model for video captioning to explicitly model the hierarchical structure in the sentence and capture the long-range dependency between words. With this in mind, we propose to generate a sentence by ﬁrst constructing a multi-modal dependency tree in a top-down and depth-ﬁrst order, and then traversing the tree in a recursive manner, as shown in Figure 1. Each node of the tree is represented by a multi-modal embedding representation that integrates the information from both visual and textual modalities. During the tree construction, each
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
newly generated node receives the multi-modal embeddings from its parent and sibling nodes, and these multi-modal embeddings are used to predict the attention weights of the input features and the word. The attended visual feature and the predicted word are fused to generate the multi-modal embedding of the new node.
Figure 1: The sentence generation process of the proposed method. Ti denotes the dependency tree generated in the i-th step, and the colored box denotes the multi-modal embedding of each node.
Different from existing sequence models that tend to focus on the dependency between each word and its close preceding words, our tree model sufﬁciently captures the global dependency structure in the sentence to further improve the ﬂuency of the generated sentences. In contrast to the uni-modal dependency tree widely used in dependency parsing for language analysis, our multi-modal dependency tree effectively integrates both the visual and linguistic information, improving the relevance of the output sentence to the visual input.
To effectively optimize the captioning model, we propose a tree-structured reinforcement learning algorithm and a novel node-level reward tailor-made for the tree construction process. By evaluating the consistency between the parent-child node pairs in the constructed tree and those in the ground truth trees, the tree node-level reward enables the model to capture the topological structure of the ground-truth dependency trees. Compared to the sequence-level rewards in existing methods, our node-level reward recognizes the contribution of each node to the multi-modal dependency tree and avoids the reward ambiguity problem [8].
To evaluate the effectiveness of the proposed method, we conduct experiments on two difﬁcult video captioning datasets, the ActivityNet Captions dataset and Charades Captions dataset. We also perform experiments on two most widely-used datasets, namely the MSVD dataset and MSR-VTT dataset. Compared to MSVD and MSR-VTT, the sentences in the ActivityNet Captions dataset and
Charades Captions dataset are typically longer and describe more complex activities in the videos.
The experiments on these datasets demonstrate that our method not only generates long and complex sentences with high ﬂuency and relevancy, but is also effective for relatively simple sentences.
The main contributions of this paper are as follows:
• We propose a multi-modal dependency tree construction method for video captioning.
With the help of tree topology, our model generates more ﬂuent and relevant sentences by effectively capturing the syntactic and semantic dependencies in natural language, especially when generating long and complex sentences.
• We develop a novel tree-structured reinforcement learning algorithm that optimizes the captioning model using a node-level reward, which facilitates learning the topology of the dependency trees and alleviates the reward ambiguity problem. 2