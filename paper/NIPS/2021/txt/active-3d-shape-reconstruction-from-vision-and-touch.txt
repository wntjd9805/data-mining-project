Abstract
Humans build 3D understandings of the world through active object exploration, using jointly their senses of vision and touch. However, in 3D shape reconstruction, most recent progress has relied on static datasets of limited sensory data such as RGB images, depth maps or haptic readings, leaving the active exploration of the shape largely unexplored. In active touch sensing for 3D reconstruction, the goal is to actively select the tactile readings that maximize the improvement in shape reconstruction accuracy. However, the development of deep learning-based active touch models is largely limited by the lack of frameworks for shape exploration. In this paper, we focus on this problem and introduce a system composed of: 1) a haptic simulator leveraging high spatial resolution vision-based tactile sensors for active touching of 3D objects; 2) a mesh-based 3D shape reconstruction model that relies on tactile or visuotactile signals; and 3) a set of data-driven solutions with either tactile or visuotactile priors to guide the shape exploration. Our framework enables the development of the first fully data-driven solutions to active touch on top of learned models for object understanding. Our experiments show the benefits of such solutions in the task of 3D shape understanding where our models consistently outperform natural baselines. We provide our framework as a tool to foster future research in this direction. 1

Introduction 3D shape understanding is an active area of research, whose goal is to build 3D models of objects and environments from limited sensory data. It is commonly tackled by leveraging partial observations such as a single view RGB image [60, 67, 23, 44], multiple view RGB images [16, 25, 30, 31], depth maps [61, 74] or tactile readings [7, 49, 59, 45, 38]. Most of this research focuses on building shape reconstruction models from a fixed set of partial observations. However, this constraint is relaxed in the active sensing scenario, where additional observation can be acquired to improve the quality of the 3D reconstructions. In active vision [4], for instance, the objective can be to iteratively select camera perspectives from an object that result in the highest improvement in quality of the reconstruction [76] and only very recently the research community has started to leverage large scale datasets to learn exploration strategies that generalize to unseen objects [77, 5, 41, 51, 29, 78, 50, 6].
Human haptic exploration of objects both with and without the presence of vision has classically been analysed from a psychological perspective, where it was discovered that the developed tactile exploration strategies for object understanding were demonstrated to not only be ubiquitous, but also highly tailored to specific tasks [37, 33]. In spite of this, deep learning-based data-driven approaches to active touch for shape understanding are practically non-existent. Previous haptic exploration works consider objects
∗Correspondence to: ejsmith@fb.com and edward.smith@mail.mcgill.ca
†Equal Contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
Figure 1: An overview of our active touch exploration framework. Given a 3D model, the simulator extracts touch and vision signals that are fed to the reconstruction model. The reconstruction model predicts a 3D shape that is used as an input to a policy model that decides where to touch next. The policies are trained to select grasps which minimize the Chamfer Distance. independently, and build uncertainty estimates over point clouds, produced by densely touching the objects surface with point-based touch sensors [7, 73, 28, 18, 46]. These methods do not make use of learned object priors, and a large number of touches sampled on an object’s surface (over 100) is necessary to produce not only a prediction of the surface but also to drive exploration. However, accurate estimates of object shape have also been successfully produced with orders of magnitude fewer touch signals, by making use of high resolution tactile sensors such as [75], large datasets of static 3D shape data, and deep learning – see e.g. [56, 68]. Note that no prior work exists to learn touch exploration by leveraging large scale datasets.
Moreover, no prior work explores active touch in the presence of visual inputs either (e.g. an RGB camera).
Combining the recent emergence of both data-driven recon-struction models from vision and touch systems [56, 68], and data-driven active vision approaches, we present a novel formu-lation for active touch exploration. Our formulation is designed to easily enable the use of vision signals to guide the touch exploration. First, we define a new problem setting over active touch for 3D shape reconstruction where touch exploration strategies can be learned over shape predictions from a learned reconstruction model with strong object priors. Second, we develop a simulator which allows for fast and realistic grasping of objects, and for extracting both vision and touch signals using a robotic hand augmented with high-resolution tactile sensors [75]. Third, we present a 3D reconstruction model from vision and touch which produces mesh-based predictions and achieves impressive performance in the single view image setting, both with and without the presence of touch signals.
Fourth, we combine the simulator and reconstruction model to produce a tactile active sensing environment for training and evaluating touch exploration policies. The outline for this environment can be viewed in Figure 1. (a) Touch only exploration. (b) Touch exploration with visual prior.
Figure 2: Target objects (top rows) and predicted 3D shapes (bottom rows) after 5 grasps have been selected.
Over the provided environment, we present a series of data-driven touch exploration models that take as an input a mesh-based shape reconstruction and decide the position of the next touch. By leveraging a large scale dataset of over 25k CAD models from the ABC dataset [34] together with our environment, the data-driven touch exploration models outperform baseline policies which fail to leverage learned patterns between object shape or the distribution of object shapes and optimal actions. We demonstrate our proposed data-driven solutions perform up to 18 % better than random baselines and lead to impressive object reconstructions relative to their input modality, such as those demonstrated in Figure 2. Our framework, training and evaluation setup, and trained models are publicly available on a GitHub repository to ensure and encourage reproducible experimental comparison 3. 3https://github.com/facebookresearch/Active-3D-Vision-and-Touch 2
2