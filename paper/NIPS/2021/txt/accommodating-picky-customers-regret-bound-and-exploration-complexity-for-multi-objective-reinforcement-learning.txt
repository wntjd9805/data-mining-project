Abstract
In this paper we consider multi-objective reinforcement learning where the objec-tives are balanced using preferences. In practice, the preferences are often given in an adversarial manner, e.g., customers can be picky in many applications. We formalize this problem as an episodic learning problem on a Markov decision pro-cess, where transitions are unknown and a reward function is the inner product of a preference vector with pre-speciﬁed multi-objective reward functions. We consider two settings. In the online setting, the agent receives a (adversarial) preference every episode and proposes policies to interact with the environment. We provide a model-based algorithm that achieves a nearly minimax optimal regret bound (cid:101)O(cid:0)(cid:112)min{d, S} · H 2SAK(cid:1), where d is the number of objectives, S is the number of states, A is the number of actions, H is the length of the horizon, and K is the number of episodes. Furthermore, we consider preference-free exploration, i.e., the agent ﬁrst interacts with the environment without specifying any preference and then is able to accommodate arbitrary preference vector up to (cid:15) error. Our proposed algorithm is provably efﬁcient with a nearly optimal trajectory complexity (cid:101)O(cid:0)min{d, S} · H 3SA/(cid:15)2(cid:1). This result partly resolves an open problem raised by
Jin et al. [2020]. 1

Introduction
In single-objective reinforcement learning (RL), a scalar reward is pre-speciﬁed and an agent learns a policy to maximize the long-term cumulative reward [Sutton and Barto, 2018]. However, in many real-world applications, we need to optimize multiple objectives for the same (unknown) environment, even when these objectives are possibly contradicting [Roijers et al., 2013]. For example, in an autonomous driving application, each passenger may have a different preference of driving styles: some of the passengers prefer a very steady riding experience while other passengers enjoy the fast acceleration of the car. Therefore, traditional single-objective RL approach may fail to be applied in such scenarios. One way to tackle this issue is the multi-objective reinforcement learning (MORL) [Roijers et al., 2013, Yang et al., 2019, Natarajan and Tadepalli, 2005, Abels et al., 2018] 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
method, which models the multiple objectives by a vectorized reward, and an additional preference vector to specify the relative importance of each objective. The agent of MORL needs to ﬁnd policies to optimize the cumulative preference-weighted rewards.
If the preference vector is ﬁxed or drawn from a ﬁxed distribution, MORL is no more challenging than single-objective RL, as we can predict the objective to be optimized and apply (variants of) single-objective RL algorithms (e.g., [Azar et al., 2017]). However, more often in practice, the preference vector (under which the weighted objective needs to be optimized) is: (i) adversarially provided, (ii) or even not available in the learning phase.
Once more taking the autonomous driving application as example: (i) the intelligent system needs to adapt driving conﬁgurations to accommodate every customer, even though the next customer can have picky preference that is unpredictable; (ii) when the intelligent system is under development, the future customer cannot be known in advance, nonetheless, when the system is deployed, it has to be capable to accommodate any potential customer who rides the car. Such MORL examples are common, to name a few beyond the autonomous driving one: medical treatment must take care of every patient even in very rare health conditions; an education system should accommodate every student according to his/her own characteristics; and emergency response systems have to be responsible in all extreme cases. Due to these exclusive challenges that have not appeared in single-objective RL, new sample-efﬁcient algorithms for MORL need to be developed, as well as their theoretical grounds need to be established.
In this work, we study provable sample-efﬁcient algorithms for MORL that resolve the aforemen-tioned issues. Speciﬁcally, we consider MORL on a ﬁnite-horizon Markov decision process (MDP) with an unknown transition kernel, S states, A actions, H steps, and d reward functions that represent the d difference objectives. We investigate MORL problems in two paradigms: (i) online MORL (where the preferences are adversarially presented), and (ii) preference-free exploration (where the preferences are not available in the learning/exploration phase). The two settings and our contributions are explained respectively in the following.
Setting (i): Online MORL. We ﬁrst consider an online learning problem to capture the challenge that preference vectors could be adversarially provided in MORL. In the beginning of each episode, the MORL agent is provided (potentially adversarially) with a preference vector, and the agent interacts with the unknown environment to collect data and rewards (that is speciﬁed by the provided preference). The performance of an algorithm is measured by the regret, i.e., the difference between the rewards collected by the agent and those would be collected by a theoretically optimal agent (who could use varying policies that adapt to the preferences). This setting generalizes the classical online single-objective RL problem [Azar et al., 2017] (where d = 1).
Contribution (i). For online MORL, we provide a provably efﬁcient algorithm with a regret upper bound (cid:101)O(cid:0)(cid:112)min{d, S} · H 2SAK(cid:1)1, where K is the number of episodes for interacting with the en-vironment. Furthermore, we show an information-theoretic lower bound Ω(cid:0)(cid:112)min{d, S} · H 2SAK(cid:1) for online MORL. These bounds together show that, ignoring logarithmic factors, our algorithm resolves the online MORL problems optimally.
Setting (ii): Preference-Free Exploration. We further consider an unsupervised MORL problem to capture the issue that preferences could be hard to obtain in the training phase. The MORL agent
ﬁrst interacts with the unknown environment in the absence of preference vectors; afterwards, the agent is required to use the collected data to compute near-optimal policies for an arbitrarily speciﬁed preference vector. The performance is measured by the sample complexity, i.e., the minimum amount of trajectories that an MORL agent needs to collect during exploration in order to be near-optimal during planning. This setting extends the recent proposed reward-free exploration problem [Jin et al., 2020] to MORL.
Contribution (ii). For preference-free exploration, we show that a simple variant of the proposed online algorithm can achieve nearly optimal sample complexity. In particular, the algorithm achieves a sample complexity upper bound (cid:101)O(cid:0)min{d, S} · H 3SA/(cid:15)2(cid:1) where (cid:15) is the tolerance of the planning error; and we also show a sample complexity lower bound, Ω(cid:0)min{d, S} · H 2SA/(cid:15)2(cid:1), for any 1We use (cid:101)O(·) to hide (potential) polylogarithmic factors in O(·), i.e., (cid:101)O(n) := O(n logk n) for sufﬁciently large n and some absolute constant k > 0. 2
algorithm. These bounds suggest that our algorithm is optimal in terms of d, S, A, (cid:15) up to logarithmic factors. It is also worth noting that our results for preference-free exploration partly answer an open question raised by Jin et al. [2020]: reward-free RL is easier when the unknown reward functions enjoy low-dimensional representations (as in MORL).
Paper Layout. The remaining paper is organized as follows: the preliminaries are summarized in Section 2; then in Section 3, we formally introduce the problem of online MORL, our algorithm and its regret upper and lower bounds; then in Section 4, we turn to study the preference-free exploration problem in MORL, where we present an exploration algorithm with sample complexity analysis (with both upper bound and lower bound), and compare our results with existing results for related problems; ﬁnally, the related literature is reviewed in Section 5 and the paper is concluded by
Section 6. 2 Preliminaries
We specify a ﬁnite-horizon Markov decision process (MDP) by a tuple of (S, A, H, P, r, W). S is a ﬁnite state set where |S| = S. A is a ﬁnite action set where |A| = A. H is the length of the horizon. P(· | x, a) is a stationary, unknown transition probability to a new state for taking action a at state x. r = {r1, . . . , rH }, where rh : S × A → [0, 1]d represents a d-dimensional vector rewards function that captures the d objectives2. W := (cid:8)w ∈ [0, 1]d, (cid:107)w(cid:107)1 = 1(cid:9) speciﬁes the set of all possible preference vectors3, where each preference vector w ∈ W induces a scalar reward function by4 rh(·, ·) = (cid:104)w, rh(·, ·)(cid:105) for h = 1, . . . , H. A policy is represented by π := {π1, . . . , πH }, where each πh(·) maps a state to a distribution over the action set. Fixing a policy π, we will consider the following generalized Q-value function and generalized value function: j=h(cid:104)w, rj(xj, aj)(cid:105)(cid:12) h(x, a; w) := E(cid:2)(cid:80)H
Qπ (cid:12)xh = x, ah = a(cid:3), h (x; w) := Qπ
V π h(x, πh(x); w), where xj ∼ P (· | xj−1, aj−1) and aj ∼ πj(xj) for j > h. Note that compared with the typical
Q-value function (or the value function) used in single-objective RL, here the generalized Q-value function (or the generalized value function) takes the preference vector as an additional input, besides the state-action pair. Fixing a preference w ∈ W, the optimal policy under w is deﬁned as w := arg maxπ V π
π∗ 1 (x1; w). For simplicity, we denote h(x, a; w) := Qπ∗
Q∗ w h (x, a; w), h (x; w) := V π∗
V ∗ w h (x; w) = max a
Q∗ h(x, a; w).
The following abbreviation is adopted for simplicity:
PV π h |x,a,w := (cid:80) y∈S
P(y|x, a)V π h (y; w), and similar abbreviations will be adopted for variants of probability transition (e.g., the empirical transition probability) and variants of value function (e.g., the estimated value function). Finally, we remark that the following Bellman equations hold for the generalized Q-value functions (cid:40) (cid:40) h(x, a; w) = (cid:104)w, rh(x, a)(cid:105) + PV π
Qπ h (x; w) = Qπ
V π h(x, πh(x); w); h+1 (cid:12) (cid:12)x,a,w
, h(x, a; w) = (cid:104)w, rh(x, a)(cid:105) + PV ∗
Q∗ h (x; w) = maxa Q∗
V ∗ h(x, a; w). h+1 (cid:12) (cid:12)x,a,w
,
With the above preparations, we are ready to discuss our algorithms and theory for online MORL (Section 3) and preference-free exploration (Section 4). 3 Online MORL
Problem Setups. The online setting captures the ﬁrst difﬁculty in MORL, where the preference vectors can be adversarially provided to the MORL agent. Formally, the MORL agent interacts with 2For the sake of presentation, we discuss bounded deterministic reward functions in this work. The techniques can be readily extended to stochastic reward settings. 3The condition that w ∈ [0, 1]d is only assumed for convenience. Our results naturally generalize to preference vectors that are entry-wisely bounded by absolute constants. 4The linear scalarization method can be generalized. See more discussions in Section 3.2, Remark 2. 3
an unknown environment through Protocol 1: at the beginning of the k-th episode, an adversary selects a preference vector wk and reveals it to the agent; then starting from a ﬁxed initial state5 x1, the agent draws a trajectory from the environment by recursively taking an action and observing a new state, and collects rewards from the trajectory, where the rewards are scalarized from the vector rewards by the given preference. The agent’s goal is to ﬁnd the policies that maximize the cumulative rewards. Its performance will be measured by the following regret: suppose the MORL agent has interacted with the environment through Protocol 1 for K episodes, where the provided preferences are {wk}K k=1 correspondingly, we consider the regret of the collected rewards (in expectation) competing with the theoretically maximum collected rewards (in expectation): k=1 and the adopted policies are {πk}K regret(K) := (cid:80)K (1)
Clearly, the regret (1) is always non-negative, and a smaller regret implies a better online performance.
We would like to highlight that the regret (1) allows the theoretically optimal agent to adopt varying policies that adapt to the preferences. 1 k=1V ∗ 1 (x1; wk) − V πk (x1; wk).
Protocol 1 Environment Interaction Protocol
Require: MDP(S, A, H, P, r, W) and online agent 1: agent observes (S, A, H, r, W) 2: for episode k = 1, 2, . . . , K do 3: 4: 5: 6: 7: 8: end for agent receives an initial state xk for step h = 1, 2, . . . , H do agent chooses an action ak agent transits to a new state xk h+1 ∼ P(· | xk end for 1 = x1, and a preference wk (from an adversary) h, and collects reward (cid:104)wk, rh(xk h, ak h)(cid:105) h, ak h) to Online Single-Objective RL.
Connections
The online regret minimization problems are well investigated in the context of single-objective RL (see, e.g., [Azar et al., 2017, Jin et al., 2018]). In both online single-objective RL and our studied on-line MORL, it is typical to assume that the transition probability P is the only unknown information about the environment since estimating a stochastic reward is relatively easy (see, e.g., [Azar et al., 2017, Jin et al., 2018]). In particular, single-objective RL is a special case of MORL in the online setting, where the preference is ﬁxed during the entire learning pro-cess (i.e., wk := w for all k) — therefore an online
MORL algorithm naturally applies to single-objective
RL. However, the reverse is not true as that in MORL the preference vectors can change over time and are potentially adversarial.
Figure 1: A regret comparison of MO-UCBVI vs. the best-in-hindsight policy in a simulated ran-dom multi-objective MDP. Note that the best-in-hindsight policy is the optimal policy for single-objective RL. The plots show that the best-in-hindsight policy will incur linear regret in online
MORL, and the proposed MO-UCBVI achieves sub-linear regret as predicted by Theorem 1. See Ap-pendix A for details.
Comparison with Single-Objective Stochastic and Adversarial Reward Setting. The essential difﬁculty of MORL is further reﬂected in the regret (1). Speciﬁcally, the regret (1) compares the perfor-mance of the agent’s policies to a sequence of optimal policy under each given preference, which could vary over time. However, in online single-objective RL, either with known/stochastic re-wards [Azar et al., 2017, Jin et al., 2018] or adversarial rewards [Rosenberg and Mansour, 2019, Jin et al., 2019], the benchmark policy is supposed to be ﬁxed over time (the best policy in the hindsight).
This difference suggests that online MORL could be more challenging than online single-objective
RL, as a harder performance measurement is adopted. More speciﬁcally, when measured by regret 5Without loss of generality, we ﬁx the initial state; otherwise we may as well consider an MDP with an external initial state x0 with zero reward for all actions, and a transition P0(· | x0, a) = P0(·) for all action a.
This is equivalent to our setting by letting the horizon length H be H + 1. 4
(1), existing algorithms for single-objective RL (e.g., [Azar et al., 2017]) easily suffer a ∝ Θ(K) regret when the sequence of preferences is adversarially designed; in contrast, we will show an
√
MORL algorithm that experiences at most ∝ (cid:101)O(
K) regret under any sequence of preferences. A numerical simulation of this issue is presented in Figure 1. 3.1 A Sample-Efﬁcient Online Algorithm
In this part we introduce an algorithm for online MORL, called multi-objective upper conﬁdence bound value iteration (MO-UCBVI). A simpliﬁed verison of MO-UCBVI is presented as Algorithm 1, and a more advanced version (Algorithm 4) can be found in Appendix B. Our algorithm is inspired by UCBVI [Azar et al., 2017] that achieves minimax regret bound in online single-objective RL.
Algorithm 1 MO-UCBVI 1: initialize history H0 = ∅ 2: for episode k = 1, 2, . . . , K do 3: N k(x, a), (cid:98)Pk(y | x, a) ← Empi-Prob(Hk−1) (cid:113) min{d,S}H 2ι 4:
N k(x,a) compute bonus bk(x, a) := c · receive a preference wk
{Qk h(x, a; wk)}H receive initial state xk 1 = x1 for step h = 1, 2, . . . , H do h=1 ← UCB-Q-Value((cid:98)Pk, wk, bk) 5: 6: 7: 8: 9: 10: 11: 12: end for take action ak h = arg maxa Qk h(xk h, a; wk), and obtain a new state xk h+1 end for update history Hk = Hk−1 ∪ {xk h, ak h}H h=1
, where ι = log(HSAK/δ) and c is a constant if N k(x, a) > 0 then 13: Function Empi-Prob 14: Require: history Hk−1 15: for (x, a, y) ∈ S × A × S do 16: N k(x, a, y) := #{(x, a, y) ∈ Hk−1}, and N k(x, a) := (cid:80) 17: 18: 19: 20: end if 21: 22: end for 23: return N k(x, a) and (cid:98)Pk(y | x, a) (cid:98)Pk(y | x, a) = N k(x, a, y)/N k(x, a) (cid:98)Pk(y | x, a) = 1/S else y N k(x, a, y) 24: Function UCB-Q-Value 25: Require: empirical transition (cid:98)Pk, preference wk, and bonus bk(x, a) 26: set V k
H+1(x; wk) = 0 27: for step h = H, H − 1, . . . , 1 do for (x, a) ∈ S × A do 28: 29: h(x, a; wk) = min (cid:8)H, (cid:104)wk, rh(x, a)(cid:105) + bk(x, a) + (cid:98)Pk
Qk
V k h(x, a; wk) h (x; wk) = maxa∈A Qk end for hV k h+1 30: 31: 32: end for 33: return (cid:8)Qk h(x, a; wk)(cid:9)H h=1 (cid:12) (cid:12)x,a,wk (cid:9)
In Algorithm 1, the agent interacts with the environment according to Protocol 1, and use an optimistic policy to explore the unknown environment and collect rewards. The optimistic policy is a greedy policy with respect to an optimistic estimation to the value function, UCB-Q-Value (lines 6 and 9).
Speciﬁcally, UCB-Q-Value is constructed to maximize the cumulative reward, which is scalarized by the current preference, through dynamic programming over an empirical transition probability (line 24). The empirical transition probability is inferred from the data collected so far (lines 3 and 5
13), which might not be accurate if a state-action pair has not yet been visited for sufﬁcient times. To mitigate this inaccuracy, UCB-Q-Value utilizes an extra exploration bonus (lines 4 and 29) so that: (i)
UCB-Q-Value never under-estimates the optimal true value function, for whatever preference vector (and with high probability); and (ii) the added bonus shrinks quickly enough (as the corresponding state-action pair continuously being visited) so that the over-estimation is under control. The overall consequence is that: MO-UCBVI explores the environment via a sequence of optimistic policies, in order to collect rewards under a sequence of adversarially provided preferences; since the policies are optimistic for any preference, the incurred regret would not exceed the total amount of added bonus; and since the bonus decays fast, their sum up would be sublinear (with respect to the number of episodes played). Therefore MO-UCBVI only suffers a sublinear regret, even when the preferences are adversarially presented. The above intuition is formalized in the next part. 3.2 Theoretical Analysis
The next two theorems justify regret upper bounds for MO-UCBVI and a regret lower bound for online
MORL problems, respectively.
Theorem 1 (Regret bounds for MO-UCBVI). Suppose K is sufﬁciently large. Then for any sequence of the incoming preferences {wk}K
• the regret (1) of MO-UCBVI (see Algorithm 1) satisﬁes k=1, with probability at least 1 − δ: regret(K) ≤ O(cid:0)(cid:112)min{d, S} · H 3SAK log(HSAK/δ)(cid:1);
• and the regret (1) of a Bernstein-variant MO-UCBVI (see Algorithm 4 in Appendix B) satisﬁes regret(K) ≤ O(cid:0)(cid:113) min{d, S} · H 2SAK log2(HSAK/δ)(cid:1).
Theorem 2 (A regret lower bound for MORL). There exist some absolute constants c, K0 > 0, such that for any K > K0, any MORL algorithm that runs K episodes, there is a set of MOMDPs and a sequence of (necessarily adversarially chosen) preferences vectors such that
E[regret(K)] ≥ c · (cid:112)min {d, S} · H 2SAK, where the expectation is taken with respect to the randomness of choosing MOMDPs and the randomness of the algorithm for collecting dataset.
Remark 1. When d = 1, MORL recovers the single-objective RL setting, and Theorem 1 recovers existing nearly minimax regret bounds for single-objective RL [Azar et al., 2017, Zanette and
Brunskill, 2019]. Moreover, the lower bound in Theorem 2 implies that our upper bound in Theorem 1 is tight ignoring logarithm terms. Interestingly, the lower bound suggests MORL with d > 2 is truly harder than single objective RL (corresponding to d = 1) as the sequence of preferences can be adversarially chosen.
Remark 2. Theorem 1 (as well as our other theorems) applies to general scalarization methods besides the linear one as adopted by Algorithm 1 and many other MORL papers [Yang et al., 2019, Roijers et al., 2013, Natarajan and Tadepalli, 2005, Abels et al., 2018]. In particular, our results apply to scalarization functions rh(·, ·) = f (rh(·, ·); w) that are (1) deterministic, (2) Lipschitz continuous for w, and (3) bounded between [0, 1] (which can be relaxed). This will be clear from proofs in
Appendix B, where we treat the potentially adversarially given preferences by a covering argument and union bound, and these techniques are not dedicated to linear scalarization function and can be easily extended to more general cases.
The proof of Theorem 1 leverages standard analysis procedures for single-objective RL [Azar et al., 2017, Zanette and Brunskill, 2019], and a covering argument with an union bound to tackle the challenge of adversarial preferences. Th rigorous proof is included in Appendix B.
Specifying an adversarial process of providing preferences is the key challenge for proving Theorem 2.
To handle this issue, we use reduction techniques and utilize a lower bound that we will show shortly for preference-free exploration problems. We refer the readers to Theorem 4 and Appendix E for more details. 4 Preference-Free Exploration
Problem Setups. Preference-free exploration (PFE) captures the second difﬁculty in MORL: the preference vector might not be observable when the agent explores the environment. Speciﬁcally, PFE 6
consists of an exploration phase and a planning phase. Similarly as in the online setting, the transition probability is hidden from the MORL agent. In the exploration phase, the agent interacts with the unknown environment to collect samples, however the agent has no information about the preference vectors at this point. Afterwards PFE switches to the planning phase, where the agent is prohibited to obtain new data, and is required to compute near-optimal policy for any preference-weighted reward functions. Since this task is no longer in an online fashion, we turn to measure the performance of a
PFE algorithm by the minimum number of required trajectories (in the exploration phase) so that the algorithm can behave near-optimally in the planning phase. This is made formal as follows: a PFE algorithm is called ((cid:15), δ)-PAC (Probably Approximately Correct), if
P(cid:8)∀w ∈ W, V ∗ 1 (x1; w) − V πw 1 (x1; w) ≤ (cid:15)(cid:9) ≥ 1 − δ, where πw is the policy outputted by the PFE algorithm for input preference w, then the sample complexity of a PFE algorithm is deﬁned by the least amount of trajectories it needs to collect in the exploration phase for being ((cid:15), δ)-PAC in the planning phase.
Connections to Reward-Free Exploration. PFE problem is a natural extension to the recent proposed reward-free exploration (RFE) problem [Jin et al., 2020, Kaufmann et al., 2020, Wang et al., 2020, Ménard et al., 2020, Zhang et al., 2020b]. Both problems consist of an exploration phase and a planning phase; the difference is in the planning phase: in RFE, the agent needs to be able to compute near-optimal policies for all reward functions, while in PFE, the agent only needs to achieve that for all preference-weighted reward functions, i.e., the reward functions that can be represented as the inner product of a d-dimensional preference vectors and the d-dimensional vector rewards functions (i.e., the d objectives in MORL). A PFE problem reduces to a RFE problem if d = SA such that every reward function can be represented as a preference-weighted reward function. However, if d (cid:28) SA, it is conjectured by Jin et al. [2020] that PFE can be solved with a much smaller sample complexity than RFE. Indeed, in the following part we show an algorithm that improves a ∝ (cid:101)O(S2) dependence for RFE to ∝ (cid:101)O(min{d, S} · S) dependence for PFE, in terms of sample complexity. 4.1 A Sample-Efﬁcient Exploration Algorithm
We now present a simple variant of MO-UCBVI that is sample-efﬁcient for PFE. The algorithm is called preference-free upper conﬁdence bound exploration (PF-UCB), and is discussed separately as in the exploration phase (Algorithm 2) and in the planning phase (Algorithm 3) in below.
Algorithm 2 PF-UCB (Exploration) 1: initialize history H0 = ∅ 2: for episode k = 1, 2, . . . , K do 3: N k(x, a), (cid:98)Pk(y | x, a) ← Empi-Prob(Hk−1) 4: compute bonus ck(x, a) := H 2S 2N k(x,a) + 2bk(x, a) for bk(x, a) deﬁned in Algorithms 1 or 3 5: 6: 7: h=1 ← UCB-Q-Value((cid:98)Pk, w = 0, ck) k h(x, a)}H
{Q receive initial state xk 1 = x1 for step h = 1, 2, . . . , H do take action ak k h(xk h = arg maxa Q end for update history Hk = Hk−1 ∪ {xk h, ak h}H h=1 8: 9: 10: 11: end for h, a), and obtain a new state xk h+1
Algorithm 2 presents our PFE algorithm in the exploration phase. Indeed, Algorithm 2 is a modiﬁed
MO-UCBVI (Algorithm 1) by setting the preference to be zero (Algorithm 2, line 5), and slightly enlarging the exploration bonus (Algorithm 2, line 4). The intention of an increased exploration bonus will be made clear later when we discuss the planning phase. With a zero preference vector, the
UCB-Q-Value in Algorithm 2 will identify a trajectory along which the cumulative bonus (instead of the cumulative rewards) is maximized (with respect to the empirical transition probability). Also note that the bonus function (Algorithm 2, line 4) is negatively correlated with the number of visits to a state-action pair. Hence the greedy policy with respect to the zero-preference UCB-Q-Value tends to visit the state-actions pairs that are associated with large bonus, i.e., those that have been visited 7
Algorithm 3 PF-UCB (Planning)
Require: history HK, preference vector w 1: for k = 1, 2, . . . , K do 2: N k(x, a), (cid:98)Pk(y|x, a) ← Empi-Prob(Hk−1) (cid:113) min{d,S}H 2ι 3: compute bonus bk(x, a) := c ·
{Qk infer greedy policy πk 4: 5: 6: end for 7: return π drawn uniformly from {πk}K h=1 ← UCB-Q-Value((cid:98)Pk, w, bk) h(x) = arg maxa Qk h(·, ·; w)}H k=1 h(x, a; w)
N k(x,a) where ι = log(HSAK/δ) and c is a constant for less times. In sum, Algorithm 2 explores the unknown environment “uniformly”, without the guidance of preference vectors.
Then Algorithm 3 shows our PFE algorithm in the planning phase. Given any preference vector,
Algorithm 3 computes a sequence of optimistically estimated value functions based on the data collected from the exploration phase, and then outputs a greedy policy with respect to a randomly drawn optimistic value estimation. Note that the bonus in Algorithm 3 is set as the one in Algorithm 1, and recall that the bonus in Algorithm 2 is an enlarged one. The relatively large bonus in the exploration phase guarantees that the regret in the planning phase never exceeds that in the exploration phase. On the other hand, based on Theorem 1 for MO-UCBVI (Algorithm 1), we have already known that the exploration algorithm (Algorithm 2), a modiﬁed Algorithm 1, suffers at most (cid:101)O(
K) regret, hence the planning algorithm (Algorithm 3) experiences at most (cid:101)O(1/
K) error. The next part rigorously justiﬁes these discussions.
√
√ 4.2 Theoretic Analysis
We ﬁrst provide Theorem 3 to justify the trajectory complexity of Algorithms 2 and 3 in the PFE setting; then we present Theorem 4 that gives an information-theoretic lower bound on the trajectory complexity for any PFE algorithm.
Theorem 3 (A trajectory complexity of PF-UCB). Suppose (cid:15) > 0 is sufﬁciently small. Then for
PF-UCB (Algorithm 2) run for
K = O(cid:0)min{d, S} · H 3SAι/(cid:15)2(cid:1), where ι := log(HSA/(δ(cid:15))), episodes, PF-UCB (Algorithm 3) is ((cid:15), δ)-PAC.
Theorem 4 (A lower bound for PFE). There exist absolute constants c, (cid:15)0 > 0, such that for any 0 < (cid:15) < (cid:15)0, there exists a set of MOMDPs such that any PFE algorithm that is ((cid:15), 0.1)-PAC on them, it needs to collect at least
K ≥ Ω(cid:0)min {d, S} · H 2SA/(cid:15)2(cid:1) trajectories in expectation (with respect to the randomness of choosing MOMDPs and the exploration algorithm).
Remark 3. According to Theorems 3 and 4, the trajectory complexity of PF-UCB is optimal for d, S, A, (cid:15) ignoring logarithmic factors, but is an H factor loose compared with the lower bound. This is because the current Algorithm 2 utilizes a preference-independent, Hoeffding-type bonus since the preference vector is not available during exploration. We leave it as an open problem to further remove this gap about H.
Proof Sketch of Theorem 3. Theorem 3 is obtained in three procedures. (1) We ﬁrst observe the total regret incurred by Algorithm 2 is (cid:101)O(cid:0)(cid:112)min {d, S} H 3SAK(cid:1) according to Theorem 1. (2) Then utilizing the enlarged exploration bonus, we show that in each episode, the planning error is at most constant times of the incurred exploration error. (3) Thus the averaged planning error is at most (cid:101)O(cid:0)(cid:112)min {d, S} H 3SA/K(cid:1) as claimed. A complete proof is deferred to Appendix C.
Note that the second argument is motivated by [Zhang et al., 2020a, Wang et al., 2020]. However in their original paper, a brute-force union bound over all possible value functions are required to obtain similar effect, due to the limitation of model-free algorithm [Zhang et al., 2020a] (see Appendix F 8
for more details) or linear function approximation [Wang et al., 2020]. This will cause a loose,
∝ (cid:101)O(S2) complexity in the obtained bound. Different from their approach, we carefully manipulate a lower order term to avoid union bounding all value functions during the second argument. As a consequence we obtain a near-tight ∝ (cid:101)O(min{d, S} · S) dependence in the ﬁnal bound. We believe this observation has broader application in the analysis of similar RL problems.
Proof Sketch of Theorem 4. We next introduce the idea of constructing the hard instance that witnesses the lower bound in Theorem 4. A basic ingredient is the hard instance given by Jin et al.
[2020]. However, this hard instance is invented for RFE, where the corresponding lower bound is
K ≥ Ω(cid:0)H 2S2A/(cid:15)2(cid:1). Note this lower bound cannot match the upper bound in Theorem 3 in terms of d and S. In order to develop a dedicated lower bound for PFE, we utilize Johnson–Lindenstrauss
Lemma [Johnson and Lindenstrauss, 1984] to reﬁne the hard instance in [Jin et al., 2020], and successfully reduce a factor S to min {d, S} in their lower bound, which gives the result in Theorem 4. We believe that the idea to reﬁne RL hard instance by Johnson–Lindenstrauss Lemma is of broader interests. A rigorous proof is deferred to Appendix D.
Application in Reward-Free Exploration. By setting d = SA and allowing arbitrary reward functions, PFE problems reduce to RFE problems. Therefore as a side product, Theorem 3 implies the following results for RFE problems on stationary or non-stationary MDPs6:
Corollary 5. Suppose (cid:15) > 0 is sufﬁciently small. Consider the reward-free exploration problems on a stationary MDP. Suppose PF-UCB (Algorithm 2) is run for
K = O(cid:0)H 3S2Aι/(cid:15)2(cid:1), where ι := log(HSA/(δ(cid:15))), episodes, then PF-UCB (Algorithm 3) is ((cid:15), δ)-PAC. Moreover, if the MDP is non-stationary, the above bound will be revised to K = O(cid:0)H 4S2Aι/(cid:15)2(cid:1).
Remark 4. When applied to RFE, PF-UCB matches the rate shown in [Kaufmann et al., 2020] and improves an H factor compared with [Jin et al., 2020] (for both stationary and non-stationary MDPs), but is an H factor loose compared with the current best rates, [Ménard et al., 2020] (for non-stationary
MDPs) and [Zhang et al., 2020b] (for stationary MDPs). However we highlight that our results are superior in the context of PFE, since PF-UCB adapts with the structure of the rewards. In speciﬁc, if rewards admit a d-dimensional feature for d < S, PF-UCB only needs ∝ (cid:101)O(min{d, S} · S) samples, but the above methods must explore the whole environment with a high precision which consumes
∝ (cid:101)O(S2) samples.
Application in Task-Agnostic Exploration. PFE is also related to task-agnostic exploration (TAE) [Zhang et al., 2020a]: in PFE, the agent needs to plan for an arbitrary reward function from a ﬁxed and bounded d-dimensional space; and in TAE, the agent needs to plan for N ﬁxed reward functions. Due to the nature of the problem setups, PFE algorithms (that do not exploit the given reward basis r during exploration, e.g., ours) and TAE algorithms can be easily applied to solve the other problem through a covering argument and a union bound, and with a modiﬁcation of min{d, S} ↔ log(N ) in the obtained trajectory complexity bounds. For TAE on a non-stationary
MDP, Zhang et al. [2020a] show an algorithm which takes (cid:101)O(cid:0)log(N ) · H 5SA/(cid:15)2(cid:1) episodes for
TAE7. In comparison, when applied to TAE on a non-stationary MDP, Theorem 3 implies PF-UCB only takes (cid:101)O(cid:0)log(N ) · H 4SA/(cid:15)2(cid:1) episodes8, which improves [Zhang et al., 2020a]. 6Our considered MDP is stationary as the transition probability P is ﬁxed (across different steps). An MDP is called non-stationary, if the transition probability varies at different steps, i.e., replacing P by {Ph}H 7This bound is copied from [Zhang et al., 2020a], which is erroneously stated due to a technical issue in the proof of Lemma 2 in their original paper. The issue can be ﬁxed by a covering argument and union bound on the value functions, but then the obtained bound should be (cid:101)O(cid:0)log(N )H 5S2A/(cid:15)2(cid:1). See Appendix F for details. 8The conversion holds as follows. First set d = 1 in our algorithm to yield an algorithm for TAE with a single agnostic task, where we have min{d, S} = 1. Then one can extend this algorithm to TAE with N agnostic tasks using a union bound to have the algorithm succeed simultaneously for all N tasks, which adds a log N multiplicative factor in the sample complexity bound. In this way, we obtain a TAE algorithm with a sample complexity bound in Theorem 3 where min{d, S} is replaced with log N . h=1. 9
5