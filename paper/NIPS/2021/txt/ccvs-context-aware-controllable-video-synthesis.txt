Abstract
This presentation introduces a self-supervised learning approach to the synthesis of new video clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (e.g., a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions.
Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks. 1

Introduction
Feeding machines with extensive video content, and teaching them to create new samples on their own, may deepen their understanding of both the physical and social worlds. Video synthesis has numerous applications from content creation (e.g., deblurring, slow motion) to human-robot interaction (e.g., motion prediction). Despite the photo-realistic results of modern image synthesis models [38], video synthesis is still lagging behind due to the increased complexity of the additional temporal dimension.
An emerging trend is to use autoregressive models, for example transformer architectures [68], for their simplicity, and their ability to model long-range dependencies and learn from large volumes of data [4, 16]. First introduced for natural language processing (NLP) and then succesfully applied to visual data [18], the strength of transformers is grounded in a self-attention mechanism which considers all pairwise interactions within the data. The price to pay is a computational complexity which grows quadratically with the data size, which itself depends linearly on the temporal dimension and quadratically on the spatial resolution in the image domain. Although there have been some efforts to reduce the complexity of self-attention [11, 39, 50], using such methods directly on visual data is still limited to low resolutions and impractical without considerable computational power [9, 79].
Some recent works [20, 53] address this problem by using an autoencoder to compress the visual data, and apply the autoregressive model in the latent space. This greatly reduces the memory footprint and computational cost, yet, the greater the compression, the harder it is to faithfully reconstruct frames.
The corresponding trade-offs may undermine the practical usability of these approaches. GANs [26] mitigate this issue by “hallucinating” plausible local details in image synthesis [20]. But latent video
∗corresponding author: guillaume.le-moing@inria.fr 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
transformers [53] decode frames independently, which prevents local details from being temporally coherent. Hence, using GANs in this setting may result in flickering effects in the synthesized videos.
We follow the problem decomposition from [20, 53], but introduce a more elaborate compression strategy with CCVS (for Context-aware Controllable Video Synthesis), an approach that takes advantage of “context” frames (i.e., both input images and previously synthesized ones) to faithfully reconstruct new ones despite lossy compression. As shown in Figure 1, CCVS relies on optical flow estimation between context and new frames, within temporal skip connections, to let information be shared across timesteps. New content, which cannot be retrieved from context, is synthesized directly from latent compressed features, and adversarial training [26] is used to make up realistic details.
Indeed, information propagates in CCVS to new frames as previously synthesized ones become part of the context. Like other video synthesis architectures based on autoregressive models [53, 79],
CCVS can be used in many tasks besides future video prediction. Any data which can be expressed in the form of a fixed-size sequence of elements from a finite set (aka, tokens) can be processed by a transformer. This applies to video frames (here, via compression and quantization) and to other types of data. Hence, one can easily fuse modalities without having to build complex or task-specific architectures. This idea has been used to control image synthesis [20, 54], and we extend it to a variety of video synthesis tasks by guiding the prediction with different annotations as illustrated in
Figure 2. Code, pretrained models, and video samples synthesized by our approach are available at the url https://16lemoing.github.io/ccvs. Our main contributions are as follows: 1. an optical flow mechanism within an autoencoder to better reconstruct frames from context, 2. the use of ancillary information to control latent temporal dynamics when synthesizing videos, 3. a performance on par with or better than the state of the art, while being more memory-efficient. 2