Abstract
Despite the signiﬁcant interests and many progresses in decentralized multi-player multi-armed bandits (MP-MAB) problems in recent years, the regret gap to the natural centralized lower bound in the heterogeneous MP-MAB setting remains open. In this paper, we propose BEACON – Batched Exploration with Adaptive
COmmunicatioN – that closes this gap. BEACON accomplishes this goal with novel contributions in implicit communication and efﬁcient exploration. For the former, we propose a novel adaptive differential communication (ADC) design that signiﬁcantly improves the implicit communication efﬁciency. For the latter, a carefully crafted batched exploration scheme is developed to enable incorporation of the combinatorial upper conﬁdence bound (CUCB) principle. We then generalize the existing linear-reward MP-MAB problems, where the system reward is always the sum of individually collected rewards, to a new MP-MAB problem where the system reward is a general (nonlinear) function of individual rewards. We extend BEACON to solve this problem and prove a logarithmic regret. BEACON bridges the algorithm design and regret analysis of combinatorial MAB (CMAB) and MP-MAB, two largely disjointed areas in MAB, and the results in this paper suggest that this previously ignored connection is worth further investigation. 1

Introduction
Motivated by the application of cognitive radio (Anandkumar et al., 2010, 2011; Gai et al., 2010), the multi-player version of the multi-armed bandits problem (MP-MAB) has sparked signiﬁcant interests in recent years. MP-MAB takes player interactions into account by having multiple decentralized players simultaneously play the bandit game and interact with each other through arm collisions.
Prior MP-MAB studies mostly focus on the homogeneous variant, where the bandit model is assumed to be the same across players (Liu and Zhao, 2010; Rosenski et al., 2016; Besson and Kaufmann, 2018).
Recent attentions have shifted towards the more general MP-MAB model with player-dependent bandit instances (i.e., the heterogeneous variant) (Bistritz and Leshem, 2018, 2020; Tibrewal et al., 2019; Boursier et al., 2020). However, unlike the homogeneous variant, the current understanding of the heterogeneous setting is still limited.
• Recent advances (Boursier and Perchet, 2019; Wang et al., 2020) show that for the homogeneous setting, decentralized MP-MAB algorithms can achieve almost the same performance as centralized ones. However, state-of-the-art results in heterogeneous variants still have signiﬁcant gaps from 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the centralized performance. It remains an open problem whether a decentralized algorithm can approach the centralized performance for the heterogeneous MP-MAB variant.
• All prior MP-MAB works are conﬁned to a linear system reward function: the system reward is the sum of individual outcomes from players. However, practical system objectives are often captured by more complicated nonlinear reward functions, e.g., the minimal function (see Section 5.1).
In this paper, we make progress in the aforementioned problems for decentralized heterogeneous MP-MAB. A novel algorithm called BEACON – Batched Exploration with Adaptive COmmunicatioN – is proposed and analyzed. In particular, this work makes the following contributions.
• BEACON introduces several novel ideas to the design of implicit communication and efﬁcient exploration. For the former, a novel adaptive differential implicit communication (ADC) scheme is proposed, which can signiﬁcantly lower the implicit communication loss compared to the state of the art. For the latter, core principles from CUCB (Chen et al., 2013) are incorporated with a batched exploration design, which leads to both efﬁcient and effective explorations.
• For the linear reward function, we rigorously show that regret bounds of BEACON, both problem-dependent and problem-independent, not only improve all prior regret analyses but more importantly are capable of approaching the centralized lower bounds, thus answering the aforementioned open problem positively.
• We then propose to generalize the study of heterogeneous MP-MAB to general (nonlinear) reward functions. BEACON is extended to solve such problems and we show that it achieves a regret of
O(log(T )), where T is the time horizon. The analysis itself holds important value as it bridges the regret analysis of combinatorial MAB (CMAB) and MP-MAB.
• BEACON achieves impressive empirical results. It not only outperforms existing decentralized algorithms signiﬁcantly, but indeed has a comparable performance as the centralized benchmark, hence corroborating the theoretical analysis. Remarkably, BEACON with the linear reward function generally achieves ∼ 6× improvement over the state-of-the-art METC (Boursier et al., 2020). 2 Problem Formulation
A decentralized MP-MAB model consists of K arms and M players. As commonly assumed (Bistritz and Leshem, 2020; Boursier et al., 2020), there are more arms than players, i.e., M ≤ K, and initially the players have knowledge of K but not M . Furthermore, no explicit communications are allowed among players, which results in a decentralized system. Also, time is assumed to be slotted, and at time step t, each player m ∈ [M ] chooses and pulls an arm sm(t) ∈ [K]. The action vector of all players at time t is denoted as S(t) := [s1(t), ..., sM (t)], which is referred to as a “matching” for convenience although it is not necessarily one-to-one.
Individual Outcomes. For each player m, an outcome1 Ok,m(t) is associated with her action of pulling arm sm(t) = k at time t, which is deﬁned as
Ok,m(t) := Xk,m(t)ηk(S(t)). (1)
In Eqn. (1), Xk,m(t) is a random variable of arm utility and ηk(S(t)) is the no-collision indicator deﬁned by ηk(S) := 1{|Ck(S)| ≤ 1} with Ck(S) := {n ∈ [M ]|sn = k}. In other words, if player m is the only player choosing arm k, the outcome is Xk,m(t); if multiple players choose arm k simultaneously, a collision happens on this arm and the outcome is zero regardless of Xk,m(t).
For a certain arm-player pair, i.e., (k, m), the set of random arm utilities {Xk,m(t)}t≥1 is assumed to be sampled independently from an unknown distribution φk,m, which has a bounded support on
[0, 1] and an unknown expectation E[Xk,m(t)] = µk,m. In general, these utility distributions are player-dependent, i.e., µk,m (cid:54)= µk,n when m (cid:54)= n. Note that despite the time independence among
{Xk,m(t)}t≥1 for a certain arm-player pair (k, m), correlations can exist among the random utility variables of different arm-player pairs, i.e., among Xk,m(t) for different (k, m) pairs.
To ease the exposition, we deﬁne S = {S = [s1, ..., sM ]|sm ∈ [K], ∀m ∈ [M ]} as the set of all possible matchings S and abbreviate the arm k of player m as arm (k, m). We further denote
µ = [µk,m](k,m)∈[K]×[M ] and µS = [µsm,m]m∈[M ] for S = [s1, ..., sM ].
System Rewards. Besides players’ individual outcomes, with matching S(t) chosen at time t, a random system reward, denoted as V (S(t), t), is collected for the entire system. The most commonly-1The term “outcome” distinguishes players’ individual rewards from the later introduced system rewards. 2
studied reward function (Bistritz and Leshem, 2020; Boursier et al., 2020) is the sum of outcomes from different players (referrd to as the linear reward function), i.e., V (S(t), t) := (cid:80) m∈[M ] Osm(t),m(t).
With this linear reward function, for matching S, the expected system reward is denoted as Vµ,S :=
E[V (S, t)] = (cid:80) m∈[M ] µsm,mηsm (S) under matrix µ. As almost all of the existing MP-MAB literature focus on the linear reward function, we also focus on this case ﬁrst, but note that the problem formulation presented in this section can be extended to general (nonlinear) reward functions in Section 5.
Feedback Model. Different feedback models exist in the MP-MAB literature, and this work focuses on the collision-sensing model (Bistritz and Leshem, 2018, 2020; Boursier and Perchet, 2019;
Boursier et al., 2020). Speciﬁcally, player m can access her own outcome Osm(t),m(t) and the corresponding no-collision indicator ηsm(t)(S(t)), but neither the overall reward V (S(t), t) nor outcomes of other players. In other words, at time t, player m chooses arm sm(t) based on her own history Hm(t) = (cid:8)sm(τ ), Osm(τ ),m(τ ), ηsm(τ )(S(τ ))(cid:9)
Regret Deﬁnition. If µ is known a priori, the optimal choice is the matching that gives the highest expected reward Vµ,∗ := maxS∈S Vµ,S. We formally deﬁne the regret after T rounds of playing as 1≤τ ≤t−1.
R(T ) = T Vµ,∗ − E (cid:35)
V (S(t), t)
, (cid:34) T (cid:88) t=1 (2) where the expectation is w.r.t. the randomness of the policy and the environment.
One technical novelty worth noting is that this work considers the general case with possibly multiple optimal matchings, instead of the commonly assumed unique one (Bistritz and Leshem, 2018, 2020).
Multiple optimal matchings might be uncommon for the linear reward function, but often occur under more sophisticated reward functions that will be discussed later, e.g., the minimal function, and brings substantial difﬁculties into player coordination. In addition, the proposed BEACON design is also applicable to the homogeneous setting, i.e., ∀m ∈ [M ], µk,m = µk, with some trivial adjustments. 3 The BEACON Algorithm 3.1 Algorithm Structure and Key Ideas
After the orthogonalization procedure (Wang et al., 2020) at the beginning of the game, during which each player individually estimates the number of players M and assigns herself of a unique index m ∈ [M ], BEACON proceeds in epochs and each epoch consists of two phases: (implicit) communication and exploration.2 While similar two-phase structures have been adopted by other heterogeneous MP-MAB algorithms (Tibrewal et al., 2019; Boursier et al., 2020), those designs fail to have regrets approaching the centralized lower bound.
The challenge in approaching the centralized lower bound is not only designing more efﬁcient implicit communications and explorations, but also connecting them in a way that neither phase dominates the overall regret and both approach the centralized lower bound simultaneously. BEACON precisely achieves these goals, with several key ideas that not only are crucial to closing the regret gap but also hold individual values in MP-MAB research. First, a novel adaptive differential communication (ADC) method is proposed, which is fundamental in improving the effectiveness and efﬁciency of implicit communications. Speciﬁcally, ADC drastically reduces the communication cost from up to O(log(T )) per epoch in state-of-the-art designs (Boursier et al., 2020) to O(1) per epoch, which ensures a low communication cost. Second, CUCB principles (Chen et al., 2013) are incorporated with a batched exploration structure to ensure a low exploration loss (see Section 8 for more discussions on the relationship between CMAB and MP-MAB). CUCB principles address a critical challenge of large amount of matchings in heterogeneous MP-MAB (i.e., |S| = KM ), which hampered prior designs. The batched structure, on the other hand, is carefully embedded and optimized such that the need of communication and exploration is balanced, leading to neither dominating the overall regret. 2Details of the orthogonalization procedure are given in the supplementary material.
In addition, by
“exploration phase”, we mean the time steps in one epoch that are not used for (implicit) communications, which actually contain both exploration and exploitation. 3
3.2 Batched Exploration
To facilitate the illustration, we ﬁrst present the batched exploration scheme and also a sketch of
BEACON under an imaginary communication-enabled setting, which will be addressed in Section 3.3.
Speciﬁcally, players are assumed to be able to communicate with each other freely in this subsection. k,m > pr−1 k,m, where ˜µr k,m = (cid:98)log2(T r k,m, statistics ˜µr k,m is collected from follower m; otherwise, ˜µr k,m](k,m)∈[K]×[M ] is calculated by the leader, where ¯µr k,m for each arm k of hers. The counters are updated as pr
The batched exploration proceeds as follows. At the beginning of epoch r, each player m maintains an arm counters pr k,m)(cid:99), where
T r k,m is the number of exploration pulls on arm (k, m) up to epoch r. Then, the leader (referring to the player with index 1) collects arm statistics from followers (referring to the players other than the leader). Speciﬁcally, if pr is not updated and kept the same as ˜µr−1 (k, m)’s sample mean ˆµr k,m k,m is a to-be-speciﬁed characterization of arm k,m. With the updated information, an upper conﬁdence bound (UCB) matrix k,m+1, and
¯µr = [¯µr tr is the time step at the beginning of epoch r.
The UCB matrix ¯µr is then fed into a combinatorial optimization solver, denoted as Oracle(·), which outputs the optimal matching w.r.t. the input. Speciﬁcally, Sr = [sr
M ] ← Oracle(¯µr) = (cid:9), which can be computed with a polynomial time complexity using the arg maxS∈S
Hungarian algorithm (Munkres, 1957). We note that similar optimization solvers are also required by Boursier et al. (2020); Tibrewal et al. (2019). Inspired by the exploration choice of CUCB, this matching Sr is chosen to be explored. The leader thus assigns the matching Sr to followers, i.e., arm sr m for player m.
After the assignment, the exploration begins. One important ingredient of BEACON is that the duration of exploring the chosen matching, i.e., the adopted batch size, is determined by the smallest arm counter in it. Speciﬁcally, for Sr, we denote pr = minm∈[M ] pr m,m and the batch size is chosen sr to be 2pr . In other words, during the following 2pr time steps, players are ﬁxated to exploring the matching Sr. Then, epoch r + 1 starts, and the same procedures are iterated. 3 ln tr/2pr k,m = ˜µr 1, ..., sr k,m + (cid:8)(cid:80) sm,m
¯µr (cid:113) sm
Remarks. BEACON directly selects the matching with the largest UCB to explore. It turns out that this natural method signiﬁcantly outperforms the “matching-elimination” scheme in Boursier et al. (2020), and is critical to achieving a near-optimal exploration loss. In addition, the chosen batch size of 2pr ensures sufﬁcient but not excessive pulls w.r.t. the least pulled arm(s) in the chosen matching, which dominate the uncertainties. Furthermore, while similar batched structures have been utilized in the bandit literature (Auer et al., 2002; Hillel et al., 2013), the updating of arm counters in BEACON is carefully tailored. Last, the leader collects followers’ statistics only when arm counters increase, i.e., pr k,m. This design contributes to a low communication frequency while not affecting the exploration efﬁciency. k,m is sufﬁciently more precise than ˜µr−1 k,m, which means ˜µr k,m > pr−1 3.3 Efﬁcient Implicit Communication
Since explicit communication is prohibited in decentralized MP-MAB problems, we now discuss how to use implicit communication (Boursier and Perchet, 2019) to share information in BEACON.
Speciﬁcally, players can take predetermined turns to “communicate” by having the “receive” player sample one arm and the “send” player either pull (create collision; bit 1) or not pull (create no collision; bit 0) the same arm to transmit one-bit information. Although information sharing is enabled, such a forced-collision communication approach is inevitably costly, as collisions reduce the rewards. The challenge now is how to keep the communication loss small, ideally O(log(T )).
The batched exploration scheme plays a key role in reducing the communication loss via infrequent information updating. In other words, players only communicate statistics and decisions before each batch instead of each time step. With the aforementioned batch size, there are at most O(log(T )) epochs in horizon T . Thus, intuitively, if the communication loss per epoch can be controlled of order O(1) irrelevant of T , the overall communication loss would not be dominating. However, this requirement is challenging and none of the existing implicit communication schemes (Boursier and
Perchet, 2019; Boursier et al., 2020) can meet it, which calls for a novel communication design.
From the discussion of the exploration phases, we can see that sharing arm statistics ˜µr k,m is the most challenging part. Speciﬁcally, as opposed to sharing integers of arm indices in Sr and the batch 4
size parameter pr, statistics ˜µr k,m is often a decimal while forced-collision is fundamentally a digital communication protocol. We thus focus on the communication design for sharing statistics ˜µr k,m, and propose the adaptive differential communication (ADC) method as detailed below. Details of sharing Sr and pr can be found in supplementary material. k,m. Instead, ˜µr
The ﬁrst important idea is to let followers adap-tively quantize sample means for communica-tion. Speciﬁcally, upon communication, the arm statistics ˜µr k,m is not directly set as the collected sample mean ˆµr k,m is a quan-tized version of ˆµr k,m using (cid:100)1 + pr k,m/2(cid:101) bits.
Since ˜µr k,m is communicated only upon an in-crease of the arm counter pr k,m, this quantiza-tion length is adaptive to the arm counter (or equivalently the arm pulls), and further to the adopted conﬁdence bound in Section 3.2, i.e., (cid:113) k,m+1. However, this idea alone is k,m is of order up to not sufﬁcient because pr
O(log(T )), instead of O(1). 3 ln tr/2pr
Algorithm 1 BEACON: Leader 1: Initialization:
−1, T r k,m ← 0, ˜µr r ← 0; ∀(k, m), pr k,m ← 0 k,m ← k,1 + 1 2: Play each arm k ∈ [K] and T r+1 k,1 ← T r 3: while not reaching the time horizon do 4: 5: 6: r ← r + 1 k,m ← (cid:4)log2(T r
∀(k, m), pr
∀k ∈ [K], update sample mean ˆµr k,m)(cid:5)
ﬁrst 2pr (cid:46) Communication Phase k,1 exploratory samples from arm k k,1 with the for (k, m) ∈ [K] × [M ] do k,m > pr−1 if pr k,m then k,m ← Receive(˜δr
˜δr k,m + ˜δr k,m ← ˜µr−1
˜µr k,m k,m, m) else (cid:113) k,m+1 m, m) k,m + 1, ..., sr k,m ← ˜µr 3 ln tr/2pr k,m = ˜µr (cid:46) Exploration Phase
M ] ← Oracle( ¯µr) k,m ← ˜µr−1
˜µr k,m end if end for
∀(k, m), ¯µr
Sr = [sr
∀m ∈ [M ], Send(sr
To overcome this obstacle, the second key idea is differential communication, which signiﬁ-cantly reduces the redundancies in statistics shar-ing. Speciﬁcally, follower m ﬁrst computes the difference ˜δr k,m − ˜µr−1 k,m, and then trun-cates the bit string of ˜δr k,m upon the most signiﬁ-cant non-zero bit, e.g., 110 for 000110. She only communicates this truncated version of ˜δr k,m in the transmission of ˜µr k,m to the leader. The in-k,m and ˜µr−1 tuition is that ˜µr k,m are both concen-trated at µk,m with high probabilities, which results in a small ˜δr k,m. From an information-theoretic perspective, the conditional entropy of k,m on ˜µr−1
˜µr
As will be clear in the regret analysis, putting these two ideas together results in an effective communication design, i.e, the ADC scheme, whose expected regret is of order O(1) per epoch and O(log(T )) overall. This method itself represents an important improvement over prior implicit communication protocols in MP-MAB, whose loss is typically of order O(log(T )) per epoch and
O(log2(T )) in total with multiple optimal matchings (Boursier and Perchet, 2019; Boursier et al., 2020). Techniques similar to ADC have been utilized in areas outside of MAB, e.g., wireless communications (Goldsmith and Chua, 1998), with proven success in practice (Goldsmith, 2005). pr ← minm∈[M ] pr sr m,m
Play arm sr 1 for 2pr times
Signal followers to stop exploration
Update ∀m ∈ [M ], T r+1 k,m), is often small because they are highly correlated.3 18: 19: 20: 21: 22: end while k,m, i.e., H(˜µr sm,m ← T r k,m|˜µr−1 sm,m + 2pr 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17:
Figure 1: A sketch of epoch r in BEACON. Yellow boxes and yellow lines indicate communications, green boxes for explorations, and boxes with dotted frame for computations. 3Note that sharing the truncated version of ˜δr k,m results in another difﬁculty that its length varies for different player-arm pairs and is unknown to the leader. A specially crafted “signal-then-communicate” scheme is designed to tackle this challenge and can be found in the supplementary material. 5
The complete BEACON algorithm can now be obtained by plugging ADC into the batched exploration structure. A sketch of one BEACON epoch is illustrated in Fig. 1, and the leader’s algorithm is presented in Algorithm 1. The follower’s algorithm can be found in the supplementary material, along with the deﬁnitions of the implicit communication protocols denoted by functions Send() and Receive(). Note that the for-loops with (k, m) and ∀(k, m) in the pseudo-codes indicate the iteration over all possible arm-player pairs of [K]×[M ]. In addition, the communications of the leader to herself indicated by the pseudo-codes denote her own calculations instead of real forced-collision communications (among the leader and followers), which is a simpliﬁcation for better exposition. 4 Theoretical Analysis
With notations Sc := {S ∈ S|∃m (cid:54)= n, sm = sn} as the set of collided matchings; S∗ := {S ∈
S|Vµ,S = Vµ,∗} as the set of optimal matchings; Sb = S\(S∗ ∪ Sc) as the set of collision-free suboptimal matchings; ∆k,m min := Vµ,∗ − max{Vµ,S|S ∈ Sb, sm = k} as the minimum sub-optimality gap for collision-free matchings containing arm-player pair (k, m); ∆min := min(k,m){∆k,m min } as the minimum sub-optimality gap for all collision-free matchings, the regret of BEACON with the linear reward function is analyzed in the following theorem.
Theorem 1. With the linear reward function, the regret of BEACON is upper bounded as4
Rlinear(T ) = ˜O

 (cid:88)
+ M 2K log(T )


M log(T )
∆k,m min (3) (k,m)∈[K]×[M ] (cid:18) M 2K log(T )
∆min (cid:19)
.
= ˜O
Note that in Eqn. (3), the ﬁrst term represents the exploration regret of BEACON, and the second term the communication regret. Compared with the state-of-the-art regret result ˜O(M 3K log(T )/∆min) for METC (Boursier et al., 2020), the regret bound in Theorem 1 improves the dependence of M from M 3 to M 2. It turns out that this quadratic dependence is optimal because the same dependence exists in the centralized lower bound (hence a natural lower bound for decentralized MP-MAB) for the linear reward function, as from Kveton et al. (2015c):5
Rlinear(T ) = Ω (cid:18) M 2K
∆min (cid:19) log(T )
. (4)
By comparing Theorem 1 and Eqn. (4), it can be observed that with the linear reward function, BEA-CON achieves a regret that approaches the centralized lower bound. The efﬁciency and effectiveness of both exploration and communication phases are critical in this achievement, as we can see that both terms in Theorem 1 are non-dominating at ˜O(M 2K log(T )).
In addition to the problem-dependent bound given in Theorem 1, the following theorem establishes a problem-independent bound, which can be thought of as a worst-case characterization.
Theorem 2. With the linear reward function, it holds that
Rlinear(T ) = O (cid:16) (cid:17)
M (cid:112)KT log(T )
. 2 (cid:112)KT log(T ))
Theorem 2 not only improves the best known problem-independent bound O(M 3 (Boursier et al., 2020) in the decentralized MP-MAB literature, but also approaches the centralized
KT ) (Kveton et al., 2015c; Merlis and Mannor, 2020) up to logarithmic factors. lower bound Ω(M
√
Theorems 1 and 2 demonstrate that for the linear reward function, BEACON closes the perfor-mance gap (both problem-dependent and problem-independent) between decentralized heterogeneous
MP-MAB algorithms and their centralized counterparts. The regret bounds of various MP-MAB algorithms, including BEACON, are summarized in Table 1. 4With the notation ˜O(·), logarithmic parameters containing K are ignored. 5This lower bound holds for the cases with arbitrarily correlated arms, as considered in this work. Under additional arm independence assumptions (Combes et al., 2015), lower regrets can be achieved. 6
Remarks. We note that it is also feasible to combine the ADC protocol and METC (Boursier et al., 2020), which can address its communication inefﬁciency, especially with multiple optimal matchings. However, with ideas from CUCB, BEACON is much more efﬁcient in exploration than
“Explore-then-Commit”-type of algorithms (e.g., METC), which is the main reason we did not fully elaborate the combination of METC and ADC in this work. Theoretically, this superiority can be reﬂected in the extra multiplicative factor in the exploration loss of METC shown in Table 1.
Table 1: Regret Bounds of Decentralized MP-MAB Algorithms
Algorithm/Reference
GoT † (Bistritz and Leshem, 2020)
Decentralized MUMAB (Magesh and Veeravalli, 2019)
ESE1 (Tibrewal et al., 2019)
METC (Boursier et al., 2020)
METC (Boursier et al., 2020)
BEACON (this work, Thm. 3)
BEACON (this work, Thm. 1)
Lower bound (Kveton et al., 2015c)
Reward function
Linear
Linear
Linear
Linear
Linear
General
Linear
Known horizon
T
Assumptions
Known gap
∆min
Unique optimal matching
No
No
No
Yes
Yes
No
No
Yes
Yes
No
No
No
No
No
Yes
No
Yes
Yes
No
No
No
Linear
N/A
N/A
N/A
Regret
O (cid:0)M log1+κ(T )(cid:1)
O (cid:0)K 3 log(T )(cid:1)
O
O
O
˜O
˜O
Ω (cid:16) M 2K
∆2 (cid:16) M 3K
∆min min (cid:18)
M K (cid:17) (cid:17) log(T ) log(T ) (cid:16) M 2 log(T )
∆min (cid:17)1+ι(cid:19) (cid:16) M K∆max (f −1(∆min))2 log(T ) (cid:17) (cid:16) M 2K
∆min (cid:16) M 2K
∆min (cid:17) log(T ) (cid:17) log(T )
†: tuning parameters in GoT requires knowledge of arm utilities;
κ, ι: arbitrarily small non-zero constants. 5 Beyond Linear Reward Functions 5.1 General Reward Functions
In this section, we move away from the linear reward functions in almost all prior MP-MAB research, and extend the study to general (nonlinear) reward functions. Two exemplary nonlinear reward functions are given below, with more examples provided in the supplementary material.
• Proportional fairness: V (S, t) = (cid:80) m∈[M ] ωm ln((cid:15) + Osm,m(t)), where (cid:15) > 0 and ωm > 0 are constants. It promotes fairness among players (Mo and Walrand, 2000);
• Minimal: V (S, t) = minm∈[M ]{Osm,m(t)}, which indicates the system reward is determined by the least-rewarded player, i.e., the short board of the system;6
These reward functions all hold their value in real-world applications, but are largely ignored and cannot be effectively solved by previous approaches. The difﬁculty introduced by this extension not only lies in the complex mapping from the (unreliable) individual outcomes to system rewards, but also comes from the potential “coupling” effect among players (e.g., the minimal reward function).
To better characterize the problem, the following mild assumptions are considered.
Assumption 1. There exists an expected reward function v(·) such that Vµ,S := E[V (S, t)] = v(µS (cid:12) ηS), where ηS := [ηsm (S)]m∈[M ] and µS (cid:12) ηS := [µsm,mηsm(S)]m∈[M ].
Assumption 2 (Monotonicity). The expected reward function is monotonically non-decreasing with respect to the vector Λ = µS (cid:12) ηS, i.e., if Λ (cid:22) Λ(cid:48), we have v(Λ) ≤ v(Λ(cid:48)).
Assumption 3 (Bounded smoothness). There exists a strictly increasing (and thus invertible) function f (·) such that ∀Λ, Λ(cid:48), |v(Λ) − v(Λ(cid:48))| ≤ f ((cid:107)Λ − Λ(cid:48)(cid:107)∞).
Assumption 1 indicates that the expected reward Vµ,S of matching S is determined only by its expected individual outcomes. It is true for the linear reward function, and also generally holds if 6Differences with the max-min fairness (Bistritz et al., 2020) are elaborated in the supplementary material. 7
distributions {φk,m} are mutually independent and determined by their expectations {µk,m}, e.g.,
Bernoulli distribution. Assumptions 2 and 3 concern the monotonicity and smoothness of the expected reward function, which are natural for most practical reward functions, including the above examples.
Similar assumptions have been adopted by Chen et al. (2013, 2016b); Wang and Chen (2018). 5.2 BEACON Adaption and Performance Analysis
In Section 3.2, a combinatorial optimization solver Oracle(·) is implemented for the linear reward function. With ideas from CUCB (Chen et al., 2013), BEACON can be extended to handle a general reward function with a corresponding solver Oracle(·) that outputs the optimal (non-collision) matching w.r.t. the input matrix µ(cid:48), i.e., S(cid:48) ← Oracle(µ(cid:48)) = arg maxS∈S\Sc Vµ(cid:48),S.
With such an oracle, the following theorem provides performance guarantees of BEACON.
Theorem 3 (General reward function). Under Assumptions 1, 2, and 3, denoting ∆k,m min{Vµ,S|S ∈ Sb, sm = k} and ∆c := f (1), the regret of BEACON is upper bounded as max := Vµ,∗ −
R(T ) = ˜O

 (cid:88) (k,m)∈[K]×[M ]


= ˜O (cid:88) (k,m)∈[K]×[M ] (cid:34)
∆k,m min (f −1(∆k,m min ))2
+ (cid:90) ∆k,m max
∆k,m min log(T ) + M 2K∆c log(T )

 (cid:35) 1 (f −1(x))2 dx

∆k,m (f −1(∆k,m max log(T ) min ))2
+ M 2K∆c log(T )
 .
With a stronger smoothness assumption, we can obtain a clearer exposition of the regret.
Corollary 1. Under Assumptions 1 and 2, if there exists B > 0 such that ∀Λ, Λ(cid:48), |v(Λ) − v(Λ(cid:48))| ≤
B(cid:107)Λ − Λ(cid:48)(cid:107)∞, it holds that
R(T ) = ˜O

 (cid:88) (k,m)∈[K]×[M ]
B2
∆k,m min log(T ) + M 2KB log(T )
 .

In addition, since the combinatorial optimization problems with general reward functions can be
NP-hard, it is more practical to adopt approximate solvers rather than the exact ones (Vazirani, 2013).
To accommodate such needs, we introduce the following deﬁnition of (α, β)-approximation oracle for α, β ∈ [0, 1] as in Chen et al. (2013, 2016a,b); Wang and Chen (2017):
Deﬁnition 1. With a matrix µ(cid:48) = [µ(cid:48) outputs a matching S(cid:48), such that P[Vµ(cid:48),S(cid:48) ≥ α · Vµ(cid:48),∗] ≥ β, where Vµ(cid:48),∗ = maxS∈S Vµ(cid:48),S. k,m](k,m)∈[K]×[M ] as input, an (α, β)-approximation oracle
With only an approximate solver, it is no longer fair to compare the performance against the optimal reward. Instead, as in the CMAB literature (Chen et al., 2013, 2016a,b; Wang and Chen, 2017), an (α, β)-approximation regret is considered: Rα,β(T ) = T αβVµ,∗ − E[(cid:80)T t=1 V (S(t), t)], where the performance is compared to the αβ fraction of the optimal reward. As shown in the supplementary material, for this (α, β)-approximation regret, an upper bound similar to Theorem 3 can be obtained. 6 Experiments
In this section, BEACON is empirically evaluated with both linear and general (nonlinear) reward functions. All results are averaged over 100 experiments and the utilities follow mutually independent
Bernoulli distributions. Additional experimental details, empirical algorithm enhancements and more experimental results (e.g., with a large game), can be found in the supplementary material.
Linear Reward Function. BEACON is evaluated along with the centralized CUCB (Chen et al., 2013) and the state-of-the-art decentralized algorithm METC (Boursier et al., 2020). The decentralized
GoT algorithm (Bistritz and Leshem, 2020) is also evaluated but its regrets are over 100× larger than those of BEACON, and thus is omitted in the plots. Fig. 2(a) reports results under the same instance in Boursier et al. (2020) with K = 5, M = 5. Although this is a relatively hard instance with multiple optimal matchings and small sub-optimality gaps, BEACON still achieves a comparable performance as CUCB, and signiﬁcantly outperforms METC: an approximate 7× regret reduction at the horizon. 8
(a) Linear, cumul. regret. (b) Linear, regret histo. (c) Proportional fairness. (d) Minimal.
Figure 2: Regret comparisons. The continuous curves represent the empirical average values, and the shadowed areas represent the standard deviations. (a), (c) and (d) are evaluated with speciﬁc game instances, and (b) is the regret histogram of 100 randomly generated instances.
To validate whether this signiﬁcant gain of BEACON over METC is representative, we plot in Fig. 2(b) the histogram of regrets with 100 randomly generated instances still with M = 5, K = 5, T = 106.
Expected arm utilities are uniformly sampled from [0, 1] in each instance. It can be observed that the gain of BEACON is very robust – its average regret is approximately 6× lower than METC.
General Reward Function. Two representative nonlinear reward functions are used to evaluate
BEACON: (1) the proportional fairness function with ∀m ∈ [M ], ωm = 1, (cid:15) = 10−2; (2) the minimal function. BEACON is compared with CUCB and METC.7 Under a game instance with
M = 6, K = 8, Fig. 2(c) reports the regrets under the proportional fairness function, and Fig. 2(d) with the minimal function. From both results, it can be observed that BEACON has slightly larger (but comparable) regrets than the centralized CUCB, while signiﬁcantly outperforming METC.
To summarize, BEACON not only signiﬁcantly outperforms state-of-the-art decentralized MP-MAB algorithms, but is also capable of empirically approaching the centralized performance, which is the
ﬁrst time for a decentralized heterogeneous MP-MAB algorithm to the best of our knowledge. 7 Discussions
We brieﬂy summarize the novel theoretical contributions of this work:
• Closing the regret gap. With the linear reward function, BEACON can approach (both problem-dependent and problem-independent) centralized lower bounds. To the best of our knowledge, this is the ﬁrst time such performance gap is closed (scaling wise) for the heterogeneous MP-MAB.
• Broader applicability. BEACON can handle a broad range of general reward functions with a regret of O(log(T )), while existing algorithms mostly focus on the linear reward function and their analyses do not apply to the general reward functions. To the best of our knowledge, this is the ﬁrst time general reward functions are studied in decentralized MP-MAB.
• Fewer assumptions. BEACON achieves a strictly O(log(T ))-regret without any assumptions or prior knowledge of the game instance, while prior MP-MAB algorithms typically rely on additional assumptions or knowledge; see Table 1 for details.
In addition to these tangible contributions, this work also demonstrates the beneﬁt of incorporating
CMAB techniques in the study of MP-MAB. In this paper, both the BEACON design and its regret analysis beneﬁt from CMAB, especially CUCB (Chen et al., 2013; Kveton et al., 2015c). While these two sub-ﬁelds of MAB are largely considered disjoint, this work shows that the underlying connection is rather fundamental. This revelation may open up interesting future research directions.
For example, under the structure of BEACON, it is conceivable to introduce more advanced CMAB algorithms, e.g., ESCB (Combes et al., 2015), into the study of MP-MAB with additional assumptions on the arm dependence. In another direction, ideas from this work may also contribute to the study of
CMAB. For example, due to the batched structure, BEACON only accesses the oracle O(log(T )) times over T steps, which is more computational efﬁcient than the O(T ) times access in CUCB.
Besides contributions, there are open questions left for future studies. First, BEACON relies on a centralized combinatorial optimization solver, i.e., Oracle(·), and so do Boursier et al. (2020);
Tibrewal et al. (2019). While being a reasonable requirement, this oracle might be computational-infeasible for some applications, e.g., with Internet-of-Things (IoT) devices, especially when M and K are large. Also, while the oracle allows general analysis, it also decouples the problem into 7To make meaningful comparisons, non-trivial adjustments and enhancements have been applied to METC, which originally applies only to the linear reward function. Details are given in the supplementary material. 9
two disconnected parts: combinatorial optimization and bandits. It might be helpful to tailor the algorithm into one speciﬁc reward function, where joint designs over these two parts can be performed.
Furthermore, it would be interesting to investigate the non-cooperative setting as in Boursier and
Perchet (2020), where we believe the design ideas in this work can still be of use, especially ADC. 8