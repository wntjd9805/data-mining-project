Abstract
We consider the problem of teaching via demonstrations in sequential decision-making settings. In particular, we study how to design a personalized curriculum over demonstrations to speed up the learner’s convergence. We provide a uniﬁed curriculum strategy for two popular learner models: Maximum Causal Entropy
Inverse Reinforcement Learning (MaxEnt-IRL) and Cross-Entropy Behavioral
Cloning (CrossEnt-BC). Our uniﬁed strategy induces a ranking over demonstra-tions based on a notion of difﬁculty scores computed w.r.t. the teacher’s optimal policy and the learner’s current policy. Compared to the state of the art, our strategy doesn’t require access to the learner’s internal dynamics and still enjoys similar convergence guarantees under mild technical conditions. Furthermore, we adapt our curriculum strategy to the setting where no teacher agent is present using task-speciﬁc difﬁculty scores. Experiments on a synthetic car driving environment and navigation-based environments demonstrate the effectiveness of our curriculum strategy. 1

Introduction
Imitation learning is a paradigm in which a learner acquires a new set of skills by imitating a teacher’s behavior. The importance of imitation learning is realized in real-world applications where the desired behavior cannot be explicitly deﬁned but can be demonstrated easily. These applications include the settings involving both human-to-machine interaction [1–4], and human-to-human in-teraction [5, 6]. The two most popular approaches to imitation learning are Behavioral Cloning (BC) [7] and Inverse Reinforcement Learning (IRL) [8]. BC algorithms aim to directly match the behavior of the teacher using supervised learning methods. IRL algorithms operate in a two-step approach: ﬁrst, a reward function explaining the teacher’s behavior is inferred; then, the learner adopts a policy corresponding to the inferred reward.
In the literature, imitation learning has been extensively studied from the learner’s point of view to design efﬁcient learning algorithms [9–15]. However, much less work is done from the teacher’s point of view to reduce the number of demonstrations required to achieve the learning objective. In this paper, we focus on the problem of Teaching via Demonstrations (TvD), where a helpful teacher assists the imitation learner in converging quickly by designing a personalized curriculum [16–20].
Despite a substantial amount of work on curriculum design for reinforcement learning agents [21– 27], curriculum design for imitation learning agents is much less investigated.
Prior work on curriculum design for IRL learners has focused on two concrete settings: non-interactive and interactive. the teacher provides a near-optimal set of demonstrations as a single batch. These curriculum strategies do not incorporate
In the non-interactive setting [17, 18], 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
any feedback from the learner, hence unable to adapt the teaching to the learner’s progress. In the interactive setting [28], the teacher can leverage the learner’s progress to adaptively choose the next demonstrations to accelerate the learning process. However, the existing state-of-the-art work [28] has proposed interactive curriculum algorithms that are based on learning dynamics of a speciﬁc
IRL learner model (i.e., the learner’s gradient update rule); see further discussion in Section 1.1. In contrast, we focus on designing an interactive curriculum algorithm with theoretical guarantees that is agnostic to the learner’s dynamics. This will enable the algorithm to be applicable for a broad range of learner models, and in practical settings where the learner’s internal model is unknown (such as tutoring systems with human learners). A detailed comparison between our curriculum algorithm and the prior state-of-the-art algorithms from [18, 28] is presented in Section 1.1.
Our approach is motivated by works on curriculum design for supervised learning and reinforcement learning algorithms that use a ranking over the training examples using a difﬁculty score [29–35].
In particular, our work is inspired by theoretical results on curriculum learning for linear regression models [32]. We deﬁne difﬁculty scores for any demonstration based on the teacher’s optimal policy and the learner’s current policy. We then study the differential effect of the difﬁculty scores on the learning progress for two popular imitation learners: Maximum Causal Entropy Inverse Reinforce-ment Learning (MaxEnt-IRL) [10] and Cross-Entropy loss-based Behavioral Cloning (CrossEnt-BC) [36]. Our main contributions are as follows:1 1. Our analysis for both MaxEnt-IRL and CrossEnt-BC learners leads to a uniﬁed curriculum strat-egy, i.e., a preference ranking over demonstrations. This ranking is obtained based on the ratio between the demonstration’s likelihood under the teacher’s optimal policy and the learner’s cur-rent policy. Experiments on a synthetic car driving environment validate our curriculum strategy. 2. For the MaxEnt-IRL learner, we prove that our curriculum strategy achieves a linear convergence rate (under certain mild technical conditions), notably without requiring access to the learner’s dynamics. 3. We adapt our curriculum strategy to the learner-centric setting where a teacher agent is not present through the use of task-speciﬁc difﬁculty scores. As a proof of concept, we show that our strategy accelerates the learning process in synthetic navigation-based environments. 1.1 Comparison to Existing Approaches on Curriculum Design for Imitation Learning
In the non-interactive setting, [18] have proposed a batch teaching algorithm (SCOT) by showing that the teaching problem can be formulated as a set cover problem. In contrast, our algorithm is interac-tive in nature and hence, can leverage the learner’s progress (see experimental results in Section 5).
In the interactive setting, [28] have proposed the Omniscient algorithm (OMN) based on the iterative machine teaching (IMT) framework [37]. Their algorithm obtains strong convergence guarantees for the MaxEnt-IRL learner model; however, requires exact knowledge of the learner’s dynamics (i.e, the learner’s update rule). Our algorithm on the other hand is agnostic to the learner’s dynamics and is applicable to a broader family of learner models (see Sections 4 and 5).
Also for the interactive setting, [28] have proposed the Blackbox algorithm (BBOX) as a heuristic to apply the OMN algorithm when the learner’s dynamics are unknown—this makes the BBOX al-gorithm more widely applicable than OMN. However, this heuristic algorithm is still based on the gradient functional form of the linear MaxEnt-IRL learner model (see Footnote 2), and does not pro-vide any convergence guarantees. In contrast, our algorithm is derived independent of any speciﬁc learner model and we provide a theoretical analysis of our algorithm for different learner models (see Theorems 1, 2, and 3). Another crucial difference is that the BBOX algorithm requires access to the true reward function of the environment, which precludes it from being applied to learner-centric settings where no teacher agent is present. In comparison, our algorithm is applicable to learner-centric settings (see experimental results in Section 6). 1.2 Additional