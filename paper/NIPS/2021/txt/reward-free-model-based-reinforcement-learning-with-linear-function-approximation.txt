Abstract
We study the model-based reward-free reinforcement learning with linear function approximation for episodic Markov decision processes (MDPs). In this setting, the agent works in two phases. In the exploration phase, the agent interacts with the environment and collects samples without the reward. In the planning phase, the agent is given a specific reward function and uses samples collected from the exploration phase to learn a good policy. We propose a new provably efficient algorithm, called UCRL-RFE under the Linear Mixture MDP assumption, where the transition probability kernel of the MDP can be parameterized by a linear function over certain feature mappings defined on the triplet of state, action, and next state. We show that to obtain an ϵ-optimal policy for arbitrary reward function,
UCRL-RFE needs to sample at most (cid:101)O(H 5d2ϵ−2) episodes during the exploration phase. Here, H is the length of the episode, d is the dimension of the feature mapping. We also propose a variant of UCRL-RFE using Bernstein-type bonus and show that it needs to sample at most (cid:101)O(H 4d(H + d)ϵ−2) to achieve an ϵ-optimal policy. By constructing a special class of linear Mixture MDPs, we also prove that for any reward-free algorithm, it needs to sample at least (cid:101)Ω(H 2dϵ−2) episodes to obtain an ϵ-optimal policy. Our upper bound matches the lower bound in terms of the dependence on ϵ and the dependence on d if H ≥ d. 1

Introduction
In reinforcement learning (RL), the agent sequentially interacts with the environment and receives reward from it. In many real-world RL problems, the reward function is manually designed to encourage the desired behavior of the agent. Thus, engineers have to change the reward function time by time and train the agent to check whether it has achieved the desired behavior. In this case, RL algorithms need to be repeatedly executed with different reward functions and are therefore sample inefficient or even intractable. To tackle this challenge, Jin et al. [9] proposed a new reinforcement learning paradigm called Reward-Free Exploration (RFE), which explores the environment without using any reward function. In detail, the reward-free RL algorithm consists of two phases. The first phase is called Exploration Phase, where the algorithm explores the environment without receiving reward signals. The second phase is called Planning Phase, where the algorithm is given a specific 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
reward function and use the collected data in the first phase to learn the policy. They have shown that this exploration paradigm can learn a near-optimal policy in the planning phase given any reward function after collecting polynomial number of episodes in the exploration phase. Follow up work
[12, 14, 28] proposed improved algorithms to achieve better or nearly optimal sample complexity.
All the aforementioned works are focused on the tabular Markov decision process (MDP), where the number of states and actions are finite. In practice, the number of states and actions can be large or even infinite, and therefore function approximation is required for the sake of computational tractability and generalization. However, the understanding of function approximation for reward-free exploration, even under the simplest linear function approximation, remains underexplored, with only two notable related works [18, 27]. Specifically, Wang et al. [18] studied linear MDPs
[21, 10], where both the transition probability and the reward function admit linear representations, and proposed a reward-free RL algorithm with (cid:101)O(d3H 6ϵ−2) sample complexity, where d is the dimension of the linear representation, H is the planning horizon, and ϵ is the required accuracy.
They also proved that if the optimal state-action function is linear, then the reward-free exploration needs an exponential number of episodes in the planning horizon H to learn a ϵ-optimal policy.
Zanette et al. [27] considered a slightly larger class of MDPs with low inherent Bellman error [26], and proposed an algorithm with (cid:101)O(d3H 5ϵ−2) sample complexity. However, both works assume the reward function is a linear function over some feature mapping. Moreover, the lower bound proved in [18] is for a very large class of MDPs where the optimal state-action function is linear, thus it is too conservative and cannot tell the information-theoretic limits of reward-free exploration for linear
MDPs or related models.
In this paper, we seek a better understanding of the statistical efficiency for reward-free RL with linear function approximation. We propose two reward-free model-based RL algorithms for the finite-horizon episodic linear mixture/kernel MDP [16, 7, 3, 30], where the transition probability kernel is a linear mixture model. In detail, our contributions are highlighted as follows:
• We propose a new exploration-driven reward function and its corresponding pseudo value function for linear mixture MDPs, which will encourage the algorithm to explore the state-action pair with more uncertainty on the transition probability.
• We propose a UCRL-RFE algorithm which guides the agent to explore the state space using the exploration-driven reward function and pseudo value functions. We prove an (cid:101)O(H 5d2ϵ−2) sample complexity for UCRL-RFE to achieve an ϵ-optimal policy for any reward function for time-homogeneous MDP.
• We further propose a UCRL-RFE+ algorithm which uses a Bernstein-type exploration bonus.
UCRL-RFE+ can reduce the error caused by the exploration-driven reward function during the exploration phase. With a novel analysis based on total variance, we prove an (cid:101)O(H 4d(H + d)ϵ−2) sample complexity for UCRL-RFE+, which improves that of UCRL-RFE by a factor of min{H, d}.
• By constructing a special class of linear mixture MDPs, we show that any reward-free algorithm needs to sample at least (cid:101)Ω(H 2dϵ−2) episodes to achieve an ϵ-optimal policy for any reward function.
This lower bound matches the upper bound of UCRL-RFE+ in terms of the dependence on the accuracy ϵ and feature dimension d when H ≥ d.
Notation. Scalars and constants are denoted by lower and upper case letters, respectively. Vectors are denoted by lower case bold face letters x, and matrices by upper case bold face letters A. We denote by [k] the set {1, 2, · · · , k} for positive integers k. For two non-negative sequence {an}, {bn}, an = O(bn) means that there exists a positive constant C such that an ≤ Cbn, and we use (cid:101)O(·) to hide the log factor in O(·); an = Ω(bn) means that there exists a positive constant C such that an ≥ Cbn, and we use (cid:101)Ω(·) to hide the log factor. an = o(bn) means that limn→∞ an/bn = 0.
We denote by S, A as the cardinality of the state set S and action set A separately. For a vector x ∈ Rd and corresponding matrix A ∈ Rd×d, we define ∥x∥2
A = x⊤Ax. We denote [x](0,H) := max{min{x, H}, 0}. For vector x ∈ Rd, we denote by [x]i the i-th element of x. 2