Abstract
Modern neural networks are often quite wide, causing large memory and com-putation costs. It is thus of great interest to train a narrower network. However, training narrow neural nets remains a challenging task. We ask two theoretical questions: Can narrow networks have as strong expressivity as wide ones? If so, does the loss function exhibit a benign optimization landscape? In this work, we provide partially afﬁrmative answers to both questions for 1-hidden-layer networks with fewer than n (sample size) neurons when the activation is smooth. First, we prove that as long as the width m ≥ 2n/d (where d is the input dimension), its expressivity is strong, i.e., there exists at least one global minimizer with zero training loss. Second, we identify a nice local region with no local-min or saddle points. Nevertheless, it is not clear whether gradient descent can stay in this nice re-gion. Third, we consider a constrained optimization formulation where the feasible region is the nice local region, and prove that every KKT point is a nearly global minimizer. It is expected that projected gradient methods converge to KKT points under mild technical conditions, but we leave the rigorous convergence analysis to future work. Thorough numerical results show that projected gradient methods on this constrained formulation signiﬁcantly outperform SGD for training narrow neural nets. 1

Introduction
Modern neural networks are huge (e.g. [8, 74]). Reducing the size of neural nets is appealing for many reasons: ﬁrst, small networks are more suitable for embedded systems and portable devices; second, using smaller networks can reduce power consumption, contributing to “green computing”.
∗Equal contribution. These authors are listed in alphabetical order.
†Corresponding author: Ruoyu Sun. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
There are many ways to reduce network size, such as quantization, sparciﬁcation and reducing the width (e.g. [20, 77]). In this work, we focus on reducing the width (training narrow nets).
Reducing the network width often leads to signiﬁcantly worse performance 3. What is the possible cause? From the theoretical perspective, there are three possible causes: worse generalization power, worse trainability (how effective a network can be optimized), and weaker expressivity (how complex the function a network can represent; see Deﬁnition 1). Our simulation shows that the training error deteriorates signiﬁcantly as the width shrinks, which implies that the trainability and/or expressivity are important causes of the worse performance (see Section 5.1 for more evidence ). We do not discuss generalization power for now, and leave it to future work.
So how is the training error related to expressivity and trainability? The training error is the sum of two parts (see, e.g., [64]): the expressive error (which is the best a given network can do; also the global minimal error) and the optimization error (which is the gap between training error and the global minimal error; occurs because the algorithm may not ﬁnd global-min). The two errors are of different nature, and thus need to be discussed separately.
It is understandable that narrower networks might have weaker expressive power. What about optimization? There is also evidence that smaller width causes optimization difﬁculty. A number of recent works show that increasing the width of neural networks helps create a benign empirical loss landscape ([19, 35, 59]), while narrow networks (width m < sample size n) suffer from bad landscape ([2, 60, 66, 72, 79]). Therefore, if we want to improve the performance of narrow networks, it is likely that both expressiveness and trainability need to be improved.
The above discussion leads to the following two questions: (Q1) Can a narrow network have as strong expressivity as a wide one? (Q2) If so, can a local search method ﬁnd a (near) globally optimal solution?
The key challenges in answering these questions are listed below:
• It is not clear whether a narrow network has strong expressivity or not. Many existing works focus on verifying the relationship between zero-training-error solutions and stationary points, but they neglect the (non)existence of such solutions (e.g. [71], [63]). For narrow networks, the (non)-existence of zero-training-error-solution is not clear.
• Even if zero-training-error solutions do exist, it is still not clear how to reach those solutions because the landscape of a narrow neural network can be highly non-convex.
• Even assuming that we can identify a region that contains zero-training-error solutions and has a good landscape, it is potentially difﬁcult to keep the iterates inside such a good region.
One may think of imposing an explicit constraint, but this approach might introduce bad local minimizers on the boundary [6].
In this work, we (partially) answer (Q1) and (Q2) for a 1-hidden-layer nets with fewer than n neurons.
Our main contributions are as follows:
• Expressiveness and nice local landscape. We prove that, as long as the width m is larger than 2n/d (where n is the sample size and d is the input dimension), then the expressivity of the 1-hidden-layer net is strong, i.e., w.p.1. there exists at least one global-min with zero empirical loss. In addition, such a solution is surrounded by a good local landscape with no local-min or saddles. Note that our results do not exclude the possibility that there are sub-optimal local minimizers on the global landscape.
• Every KKT point is an approximated global minimizer. For the original unconstrained optimization problem, the nice local landscape does not guarantee the global statement of “every stationary point is a global minimizer”. We propose a constrained optimization problem that restricts the hidden weights to be close to the identiﬁed nice region. We show 3This can be veriﬁed on our empirical studies in Section 5. Another evidence is that structure pruning (reducing the number of channels in convolutional neural nets (CNN)) is known to achieve worse performance than unstructured pruning; this is an undesirable situation since many practitioners prefer structure pruning (due to hardware reasons). 2
that every Karush–Kuhn–Tucker (KKT) point is an approximated global minimizer of the unconstrained training problem 4.
• In real-data experiments, our proposed training regime can signiﬁcantly outperforms SGD for training narrow networks. We also perform ablation studies to show that the new elements proposed in our method are useful. 2