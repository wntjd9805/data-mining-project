Abstract
When machine learning systems meet real world applications, accuracy is only one of several requirements. In this paper, we assay a complementary perspective originating from the increasing availability of pre-trained and regularly improv-ing state-of-the-art models. While new improved models develop at a fast pace, downstream tasks vary more slowly or stay constant. Assume that we have a large unlabelled data set for which we want to maintain accurate predictions. Whenever a new and presumably better ML models becomes available, we encounter two prob-lems: (i) given a limited budget, which data points should be re-evaluated using the new model?; and (ii) if the new predictions differ from the current ones, should we update? Problem (i) is about compute cost, which matters for very large data sets and models. Problem (ii) is about maintaining consistency of the predictions, which can be highly relevant for downstream applications; our demand is to avoid negative
ﬂips, i.e., changing correct to incorrect predictions. In this paper, we formalize the Prediction Update Problem and present an efﬁcient probabilistic approach as answer to the above questions. In extensive experiments on standard classiﬁcation benchmark data sets, we show that our method outperforms alternative strategies along key metrics for backward-compatible prediction updates. 1

Introduction
The machine learning (ML) community develops new models at a fast pace: for example, just in the past year, the state-of-the-art on ImageNet has changed at least ﬁve times [9, 10, 31, 50, 51].
As reproducibility has increasingly been scrutinized [33, 34, 42], it is now common practice to release pre-trained models upon publication. In this work we take the perspective of an owner of an unlabelled data set who is interested in keeping the best possible predictions at all times. When a new pre-trained model is released, we face what we refer to as the Prediction Update Problem: (i) decide which points in the data set to re-evaluate with the new model, and (ii) integrate the new, possibly contradicting, predictions. For this task, we postulate the following three desiderata: 1. The prediction updates should improve overall accuracy. 2. The prediction updates should avoid introducing new errors. 3. The prediction updates should be as cheap as possible since the target data set could be huge.
∗Work done while FT and JvK were interning at Amazon.
†Correspondence to: frederik.traeuble@tuebingen.mpg.de and pgehler@amazon.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We consider the setting in which the target data set for which we wish to maintain predictions is fully unlabelled (i.e., the ground-truth labels are unknown) and may come from a different distribution than the one on which models have been pre-trained, but with overlap in the label space. This is a transductive or semi-supervised problem, but, due to computational constraints, we avoid any model
ﬁtting or ﬁne-tuning and rely solely on the predictions of the pre-trained models that are released over time. Typically, these models exhibit increased performance on their labelled training domain (e.g., the ImageNet validation or test set) as evidence for being good candidates for re-evaluation.
Clearly, one goal of updating the predictions stored for the target data set is to improve overall performance, e.g., top-k accuracy for classiﬁcation. At the same time, the stored predictions may form an intermediate step in a larger ML pipeline or are accessible to users. This is the reason for our second desideratum: we would like to be backward-compatible, i.e., new predictions should not ﬂip previously correct predictions (negative ﬂips). Finally, we aim to reduce computational cost during inference and to avoid evaluating the entire data set which may be prohibitive in practice and unnecessary if we are already somewhat certain about a prediction.
In this paper, we motivate and formalize the Prediction Update Problem and describe its relation to various relevant research areas like ensemble learning, domain adaption, active learning, and others.
We propose a probabilistic approach that maintains a posterior distribution over the unknown true la-bels by combining all previous model re-evaluations. Based on these uncertainty estimates, we devise an efﬁcient selection strategy which only chooses those examples with highest posterior label entropy for re-evaluation in order to reduce computational cost. Furthermore, we consider different prediction-update strategies to decide whether to change the stored predictions, taking asymmetric costs for neg-ative and positive ﬂips into account. Using the task of image classiﬁcation as a case study, we perform extensive experiments on common benchmarks (ImageNet, CIFAR10, and ObjectNet) and demon-strate that our approach achieves competitive accuracy and introduces much fewer negative ﬂips across a range of computational budgets, thus showing that our three desiderata are not necessarily at odds.
Contributions We highlight the following contributions:
• We introduce the Prediction Update Problem which addresses some common, but previously unaddressed challenges faced in real world ML systems (§ 2).
• We propose a probabilistic, model-agnostic approach for the Prediction Update Problem, based on Bayesian belief estimates of the true label combined with an efﬁcient selection and different prediction-update strategies (§ 3).
• We contextualise this understudied problem setting as well as our method with related work (§ 4 & § 5) and discuss several extensions and limitations (§ 4).
• We demonstrate that our approach successfully outperforms alternative approaches and accomplishes all our desiderata in experiments across multiple common benchmark datasets (CIFAR-10, ImageNet, and ObjectNet) and practically relevant scenarios (§ 6).3 1.1 Backward-Compatible Prediction Systems
In real world ML applications, empirical performance is only one of several requirements. When humans interact with automatic predictions, they will start to build mental models of how these models operate and whether and when their predictions can be trusted. This is described as Human-AI teams by [1] who argue to “make the human factor a ﬁrst-class consideration of AI updates”.
An example from [1] is autopilot functionality in cars for which drivers will build expectations in which driving situations the autopilot is safe to engage. It is important not to violate these assumptions when updating the models over the air. AI assisted medical decision processes are another example of a high stake application where medical professionals need to understand when systems can be trusted.
Consider the example of automatically tagging images in a user’s photo collection. Those tags are used for example in photo search. As models progress, the overall accuracy on all uploaded images may increase, but for any single user the experience can deteriorate if previously correct searches now show wrong results. Even worse, if errors ﬂuctuate over the user’s photo collection as the result of prediction-updates, the user’s trust will be eroded. This “cost” is asymmetric and the negative experience may outweigh the beneﬁt of better predictions on other images. 3All software and assets we use are open source and under MIT, Apache or Creative Commons Licenses. 2
Figure 1: Overview of our proposed Bayesian approach to the Prediction Update Problem. Starting from a uniform prior, we maintain a posterior distribution p(y = k|ˆy0:t) (middle) over the unknown true label y of an unlabelled sample which takes the predictions ˆy0:t from new ML models C t (top) arriving over time t = 0, ..., T into account. Given a limited compute budget Bt, we re-evaluate those samples with highest posterior label entropy St at each time step, e.g., the example shown is ﬁrst selected in time step t = 3 after the initial annotation at t = 0. We then consider different strategies for deciding whether to update the stored prediction (bottom) based on our changed beliefs. Note that the non-probabilistic baselines “Replace” (always update to last prediction) and “Majority Vote” (resolve ties by using the last prediction) incorrectly update the stored prediction from “truck” to “deer” in step t = 4. Our strategies (MB, MBME, CR-10) which rely on the estimated label posterior, on the other hand, avoid such a negative ﬂip, which is one of our key goals.
In contrast to carefully curated and labelled ML benchmarks, many real-world data sets are magnitudes larger (up to billions of samples) and entirely unlabelled. Having no feedback which predictions are correct is a common scenario: consider any type of private data such as health data, photo collections, or personal information. Because the data is private, we can neither train on it, nor collect feedback, nor observe the effect of predictions. On the other hand, such data is valuable to an individual: she has an interest to keep it up to date with the best possible predictions. Since it is of little consolation to her if an update of the model improves predictions on average but on her data it gets worse, the update costs are asymmetric. Service providers often rely on models pre-trained on a different data set, and the desire to be backward-compatible arises naturally in this setting [1, 39, 43, 53].
This is an understudied problem where progress will have large impact. ML systems are becoming pervasive, and their accuracy will continue to increase. Being able to seamlessly transfer them to existing data will be crucial for real-world ML systems. 2 The Prediction Updates Problem Setting
X . The ground-truth labels yn ∈ Y = {1, ..., K} distributed according to Ptarg
Target Data Set We are given a large, unlabelled target data set Dtarg = {xn}N n=1 comprising
N independent and identically distributed (i.i.d.) observations xn ∈ X ⊆ Rd drawn from a target distribution Ptarg
Y |X are not observed. Note that we are particularly interested in a scenario where N may be extremely large.
Models Over time t = 0, 1, ..., T we successively gain access to classiﬁers C 0, C 1, . . . , C T : X →
Y which have been trained on a labelled data set Dsrc from a potentially different source distribution
Psrc
X,Y over X × Y. For simplicity, we assume that the observation space X and label space Y are shared. We consider both the standard scenario where the models {C t}T t=0 are trained on a labelled set from the same domain (Psrc = Ptarg); and the transfer scenario where we deploy a model trained on a labelled ML benchmark to a different data set (Psrc (cid:54)= Ptarg). We assume that {C t}T t=0 are improving in performance on the training data set. Therefore, denoting by At the estimated accuracy of C t on Psrc
X,Y , we have At ≤ At+1 ∀t. As motivating example, consider an object recognition task in the wild and let Ct be the winning entry of the ImageNet competition in year t. 3
Y |X = Psrc
Labelling To relate the source and target distributions and to justify applying {C t}T t=0 to our target data set Dtarg, we make the commonly used covariate shift assumption Ptarg
Y |X, i.e. the conditional label distribution is shared across source and target distributions [40, 45].4 We denote the n = C t(xn) and the stored prediction for xn after time step t by lt predicted label by C t for xn by ˆyt n. n := ˆy0
The target data set is then initially fully labelled by C 0, i.e., l0 n.
Objective As new classiﬁers {C t}t≥1 become available, our main objective is to maintain the best estimates {lt n=1 on our target data set at all times and improve overall accuracy, while, at the same time, maintaining backward compatibility by minimising the number of negative ﬂips, i.e., the number of previously correctly stored predictions that are incorrectly changed. The key challenge is that no ground truth labels for our target data set are available, so that we have no feedback on which predictions are correct and which are wrong. For each test sample xn and each time step t ≥ 1, we thus need to decide whether or not to update the previously stored prediction lt−1 based on the current and previous model predictions ˆyt
, respectively. n}N n n and ˆy0:t−1 n
Limited Evaluation Budget Re-evaluating all samples (so-called backﬁlling) can be very costly and requires signiﬁcant resources. Since we consider N to be very large, we also consider a limited budget of at most Bt ≤ N sample re-evaluations for step t. We thus additionally need to decide how to allocate this budget and select a subset of samples to be re-evaluated by C t at every step. 3 Our Method
Having speciﬁed the setting, we next describe our proposed method for the Prediction Update
Problem. We start by providing a Bayesian approach for maintaining and updating our beliefs about the unknown true labels as new predictions become available (§ 3.1), followed by describing strategies for selecting candidate samples for re-evaluation (§ 3.2) and for updating the stored predictions based on our changed beliefs (§ 3.3). Our framework is summarised in Figure 1. 3.1 Bayesian Approach
Since the true labels {yn}N n=1 are unknown to us, we treat them as random quantities over which we maintain uncertainty estimates. We then perform Bayesian reasoning to update our beliefs as n from newly-available classiﬁers C t arrives over time new evidence in the form of predictions ˆyt
In standard Bayesian notation, the true labels yn thus take the role of unknown t = 1, ..., T . parameters θ and the predictions ˆyt n of data x. Since Dtarg is sampled i.i.d., we reason about each label yn independently of the others, i.e., the following is the same for all n.
Prior Lacking label information on the target data set, we choose a uniform prior over Y for all yn, i.e., p(yn = k) = 1/K, ∀k ∈ Y. If (estimates of) the class probabilities on Dtarg are available, we may instead use these as a more informative prior.
Likelihood Next, we need to specify a likelihood function p(ˆy0:T predictions ˆy0:T
Assumption 1 (Conditionally independent classiﬁers). The different classiﬁers’ predictions ˆy0:T conditionally independent given the true label yn, i.e., the likelihood factorises as n |yn = k) for the observed model given a value k of the true label yn. We make the following simplifying assumption. are n n p(ˆy0:T n |yn = k) = (cid:81)T t=0 p(ˆyt n|yn = k). (1)
In a standard Bayesian setting, this corresponds to the assumption of conditionally independent observations given the parameters; we refer to § 4 for further discussion. The main advantage of
Assumption 1 is that the factors p(ˆyt n|yn = k) on the RHS of (1) have a natural interpretation: these are the (normalised) confusion matrices πt of the classiﬁers C t, i.e., we denote by
πt(i, k) := p(ˆyt = i|y = k), the probability that C t predicts class i given that the true label is k, which is the same for all n; see below and § 4 for more details on how we estimate πt in practice. 4In the context of image classiﬁcation, this means that images of the same object under different environmental conditions (i.e., different PX) will always share the same label (i.e., same PY |X). Note that such covariate shift may also lead to changes in PY (label/target shift) and/or PX|Y (conditional shift) [44, 55]. 4
Posterior At every time step t ≥ 0, we can then compute our posterior belief about the true label yn given model predictions ˆy0:t n according to Bayes rule, n ) = πt(ˆyt p(yn = k|ˆy0:t (cid:80) n,k)p(yn=k|ˆy0:t−1 n
) i∈Y πt(ˆyt n,i)p(yn=i|ˆy0:t−1 n
) (2) where we have used Assumption 1 to write p(ˆyt posterior at step t − 1 acts as prior for step t, so we do not have to store all previous predictions.
Estimating Confusion Matrices In practice, πt are generally not known and we instead use their (maximum likelihood) estimates ˆπt from the source distribution. If the number of classes K is large compared to the amount of labelled source data,5 we only estimate the diagonal elements ˆπt kk (i.e., the class-speciﬁc accuracies) and set the K(K − 1) off-diagonal elements to be constant, n|yn = k) = πt(ˆyt n|yn = k, ˆy0:t−1 n, k). The
) = p(ˆyt n
ˆπt(i, k) = 1−ˆπt(k,k)
K−1
∀i (cid:54)= k, so that (cid:80)K i=1 ˆπt(i, k) = 1 ∀k ∈ Y. We refer to § 4 for further discussion on the estimation of πt. 3.2 Selecting Candidates for Re-evaluation
Given the label posteriors computed according to (2), we compute the Shannon entropies [38] n = − (cid:80)
St k∈Y p(yn = k|ˆy0:t n ) log p(yn = k|ˆy0:t n ), which provide a simple measure of uncertainty in the true label yn after step t. We then select and re-evaluate the Bt samples with highest posterior label entropy St n to update our beliefs. 3.3 Prediction-Update Strategies based on our new beliefs. We consider three such prediction-update strategies.
Finally, we need a strategy for deciding whether and how to update the previously stored prediction lt−1 n
MaxBelief (MB) The simplest approach is to always update based on the maximum a posteriori belief, i.e., lt n ). We refer to this strategy as MaxBelief (MB). n = argmaxk∈Y p(yn = k|ˆy0:t n := ˆlt
MaxBeliefMinEntropy (MBME) A slightly more sophisticated approach is to also take the change in posterior entropy into account and only update when it has decreased: lt n := (cid:40)ˆlt n lt−1 n n < St−1 n , if St otherwise.
We refer to this strategy as MaxBeliefMinEntropy (MBME).
CostRatio (CR) So far, we have not taken the assumed larger penalty for negative ﬂips into account.
We therefore now develop a third approach based on asymmetric ﬂip costs. We denote the cost of a negative ﬂip (NF) by cNF > 0 and that of a positive ﬂip (PF) by cPF < 0.
We need to decide whether to update the previously stored prediction lt−1 based on our updated beliefs p(yn = k|ˆy0:t n = argmaxk∈Y p(yn = k|ˆy0:t n ).
If ˆlt n = lt−1 n . We then need to reason about the (estimated) positive and negative ﬂip probabilities when changing the stored to ˆlt prediction from lt−1 is not), and, vice versa, a negative ﬂip occurs if lt−1 n ). Denote the MAP label estimate after step t by ˆlt there is no reason to change the stored prediction. Suppose that ˆlt n. A positive ﬂip (PF) occurs if ˆlt n is the correct label (and hence lt−1 is correct (and hence ˆlt n (cid:54)= lt−1 n n n n n is not): n n) = p(yn = ˆlt n|ˆy0:t n ), n (lt−1
ˆpNF n → ˆlt n) = p(yn = lt−1 n |ˆy0:t n ).
If neither lt−1 cost. The estimated cost of changing the stored prediction from lt−1 n are the correct label, the ﬂip is inconsequential which we assume incurs zero to ˆlt n n is thus: n n (lt−1
ˆpPF n → ˆlt nor ˆlt
ˆc(lt−1 n → ˆlt n) = cNF ˆpNF n (lt−1 n → ˆlt n) + cPF ˆpPF n (lt−1 n → ˆlt n). 5For example, on ImageNet we have K = 1000 which would require estimating 1 million parameters. 5
We only want to change the prediction if ˆc(lt−1 leading to the following update rule: n (lt−1
ˆpPF n (lt−1
ˆpNF n →ˆlt n) n →ˆlt n) n → ˆlt
= p(yn=ˆlt p(yn=lt−1 n) < 0, i.e., n|ˆy0:t n ) n |ˆy0:t n )
> − cNF cPF (3) lt n :=

ˆlt n,

ˆlt n,
 lt−1 n
ˆlt n = lt−1 n , if
ˆlt n ∧ ˆc(lt−1 n (cid:54)= lt−1 if otherwise. n → ˆlt n) < 0,
Note that (3) has an intuitive interpretation: we only want to update the currently stored prediction (thus potentially risking a negative ﬂip) if our belief in a different label is larger than that in the current one by a factor exceeding |cNF/cPF|. We therefore refer to this strategy as CostRatio (CR). 4 Discussion: Extensions and Limitations
We discuss current limitations of our method and propose extensions to address them in future work.
Soft vs. Hard Labels Our approach presented in § 3 assumes deterministic classiﬁers which output hard labels, i.e., only the most likely class. This allows for maximum ﬂexibility and a wide range of classiﬁer models that can be used in conjunction with this method. However, our Bayesian framework can easily be adapted to also allow for probabilistic classiﬁers which output soft labels, i.e., vectors of class probabilities. We included some additional exploratory experiments utilizing softmax prob-abilities in Appendix A.11. Since deep neural networks are known to have unreliable uncertainty es-timates [14, 25, 26, 47], we deliberately choose to work with hard labels. If, however, well-calibrated probabilistic classiﬁers are available (and can be scaled to huge data sets), taking this additional infor-mation into account will likely lead to more accurate posterior estimates and thus better performance.
Assumption of Conditionally-Independent Classiﬁers Since the models {C t} are typically trained and developed on the same data and may even build on insights from prior architectures, our assumption of conditionally independent predictions on Dtarg does likely not hold exactly in practice.
It should therefore rather be understood as an approximation that enables tractable posterior inference.
Our experiments (§ 6) suggest that it is a useful approximation that yields competitive performance.
Properly incorporating estimated model correlations may yield further improvements. We refer the interested reader to Appendix A.1 for a more detailed discussion.
Confusion Matrix Estimates Unless labelled data from Ptarg is available, the confusion matrices
{πt} need to be estimated from Psrc. This is only an approximation because they may change as a result of Psrc
X , and taking such shifts into account could yield more accurate posterior estimates. For this, one may use ideas from the ﬁeld of unsupervised domain adaptation [12, 28, 45].
One could use an importance-weighting approach [40] to give more weight to points which are representative of Ptarg
X,Y . As an example, in further experiments in
Appendix A.4 we studied estimating the off-diagonal elements using Laplace smoothing [13, 35],
X when estimating πt from Psrc
X (cid:54)= Ptarg
Other Selection Strategies Consider an ambiguous image that could be either a zucchini or a cucumber [4]. Such a sample would have large label entropy and could thus potentially be selected for re-evaluation again and again. To overcome this hurdle, one could decompose label uncertainty into epistemic (reducible) and aleatoric (irreducible) uncertainty [8, 19] and only re-evaluate samples with high aleatoric uncertainty, i.e., those with high expected information gain [24]. Such considerations also play a role in the ﬁeld of active learning [37, 54]
Growing Data Set Size Our method is not constrained to ﬁxed data set sizes and can accommodate for the addition of new data. New samples can be added at any time using an uniform prior over labels. Given their high initial entropy, they would then be naturally selected for (re-)evaluation ﬁrst.
Adaptive Budgets Currently, we consider a ﬁxed local budget of Bt re-evaluations at every time step. A possible extension would be to allow for a global budget of Btotal evaluations spread over all time-steps, i.e., to devise a strategy for deciding whether to (a) keep re-evaluating or (b) save budget for the next better model, potentially using techniques from reinforcement learning [46].
On the Cost of “Neutral” Flips For simplicity, we have assumed that “neutral” ﬂips (i.e., changing a label estimate from an incorrect to a different incorrect one) bear no cost. However, as motivated 6
in § 1, it is well conceivable that even such neutral ﬂips have a cost due to the potential to disrupt downstream robustness. If this is the case, it can easily be incorporated into our CR update strategy. 5