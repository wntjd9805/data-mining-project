Abstract
The three-dimensional reconstruction of multiple interacting humans given a monocular image is crucial for the general task of scene understanding, as cap-turing the subtleties of interaction is often the very reason for taking a picture.
Current 3D human reconstruction methods either treat each person independently, ignoring most of the context, or reconstruct people jointly, but cannot recover inter-actions correctly when people are in close proximity. In this work, we introduce
REMIPS, a model for 3D Reconstruction of Multiple Interacting People under
Weak Supervision. REMIPS can reconstruct a variable number of people directly from monocular images. At the core of our methodology stands a novel transformer network that combines unordered person tokens (one for each detected human) with positional-encoded tokens from image features patches. We introduce a novel uniﬁed model for self- and interpenetration-collisions based on a mesh approxi-mation computed by applying decimation operators. We rely on self-supervised losses for ﬂexibility and generalisation in-the-wild and incorporate self-contact and interaction-contact losses directly into the learning process. With REMIPS, we report state-of-the-art quantitative results on common benchmarks even in cases where no 3D supervision is used. Additionally, qualitative visual results show that our reconstructions are plausible in terms of pose and shape and coherent for challenging images, collected in-the-wild, where people are often interacting. 1

Introduction
Reconstructing three-dimensional models of multiple interacting people from images is an important computer vision task with applications in behavior analysis, automatic video analysis of sport events, or collaborative augmented reality applications. As the demands on the depth of analysis increase, beyond qualitative pose, one often seeks to know whether people are in contact or not, what is the nature of that contact, and how long it lasts. As such questions become important, it is clear that very coarse estimates of pose or even shape are no longer sufﬁcient. One needs accurate models of shape and contact with predictable response over time. This is difﬁcult due to the various degree of occlusion that occurs, the depth ambiguities particularly given only monocular images (but more generally given that occlusion and self-occlusion for people are frequent), the high degree of variability of even valid human poses, their different scales and underlying spatial image support, ranging from the large body parts like torso or thighs – informed by larger image regions – to hands, with parts typically accounted for by smaller spatial support, comparatively. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Most of the volumetric human pose and shape recovery methods focus on a single person and require detecting the humans in the image and then running the inference model on each positive response.
This approach has achieved good results and scales linearly in the number of people in the scene.
There are multi people reconstruction methods which either predict 3D pose only and defer the shape reconstruction step to a later optimization step[1] or rely on an orthographic camera model
[2]. However, it is nontrivial to obtain a realistic 3D scene placement for all the reconstructed human models such that their 3D spatial relations are plausible and the alignment with the image evidence is good. All aforementioned methods train using full 3D supervision coming from mocap datasets.
However, this often lacks scenarios of interactions or ﬁne grained self-contact. Recent work such as
[3, 4] designed datasets for interaction analysis, with additional interaction signature annotations, and showed that models able to represent those produce more realistic reconstructions. However, they only work in an optimization framework and reconstruct only pairs of people in close interaction.
Ideally one would like to be able to precisely model contacts, rely on attention models to identify the important element of interaction, have a framework that can accommodate more than two people and learn such models from data, using only weak supervision.
Our main contributions can be summarized as proposing the following: (1) fast, accurate and uniﬁed 3d self collision and interpenetration models for multiple people; (2) a novel vision transformer architecture to predict 3D pose and shape for multiple people; (3) weakly supervised models which do not require 3D annotations during training; (4) state of the art results on challenging datasets, with favorable performance compared to competing predictive or optimization-based methods.
In this paper, we propose REMIPS, a hybrid convolutional-transformer where the output of a human detector is combined with high-level convolutional image features. They are processed by a series of transformer encoder layers that iteratively reﬁne the 3D pose and shape reconstruction estimates for all detected humans in the scene, irrespective of their number. Decoupling the detection component has the advantage of beneﬁting from innovations in that space without necessarily paying a performance cost (detection models are already quite fast, close to real time, and providing accurate results).
Similar to [5], we use a full perspective camera model, allowing us to infer translations for all humans in the same camera coordinate system. We propose novel interpenetration and self-collision losses that are amenable to a deep-learning-based training process and work in a weakly-supervised regime. 2