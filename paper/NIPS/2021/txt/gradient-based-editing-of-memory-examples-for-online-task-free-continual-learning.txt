Abstract
We explore task-free continual learning (CL), in which a model is trained to avoid catastrophic forgetting in the absence of explicit task boundaries or identities.
Among many efforts on task-free CL, a notable family of approaches are memory-based that store and replay a subset of training examples. However, the utility of stored seen examples may diminish over time since CL models are continually updated. Here, we propose Gradient based Memory EDiting (GMED), a framework for editing stored examples in continuous input space via gradient updates, in order to create more “challenging” examples for replay. GMED-edited examples remain similar to their unedited forms, but can yield increased loss in the upcoming model updates, thereby making the future replays more effective in overcoming catastrophic forgetting. By construction, GMED can be seamlessly applied in conjunction with other memory-based CL algorithms to bring further improvement.
Experiments validate the effectiveness of GMED, and our best method signiﬁcantly outperforms baselines and previous state-of-the-art on ﬁve out of six datasets1. 1

Introduction
Learning from a continuous stream of data – referred to as continual learning (CL) or lifelong learning – has recently seen a surge in interest, and many works have proposed ways to mitigate CL models’ catastrophic forgetting of previously learned knowledge [20, 32, 33]. Here, we study online task-free CL [3], where task identiﬁers and boundaries are absent from the data stream. This setting reﬂects many real-world data streams [6, 25] and offers a challenging testbed for online CL research.
Memory-based methods, a prominent class of approaches used for task-free continual learning, store a small number of training examples (from the data stream) in a memory and replay them at the later training iterations [33, 34]. Existing methods operate over the original examples in the data-stream and focus on identifying samples to populate the memory [4, 9] and ﬁnding samples in the memory to be replayed [2]. However, for continually updating models, using stored-seen examples in their original form, may lead to diminishing utility over time — i.e., model may gradually memorize the stored examples after runs of replay, as the memory refreshes slowly. An alternate approach is to use generative models to create samples that suffers more from forgetting such as in GEN-MIR in [2].
In practice, training the generator network with limited data is challenging and leads to low-quality generated examples. Further, in the online learning setup, the generative model itself suffers from forgetting. As such, generative models perform worse than their memory counter-parts.
In this paper, we present a novel memory-based CL framework, Gradient based Memory EDiting (GMED), which looks to directly “edit” (via a small gradient update) examples stored in the replay memory. These edited examples are stored (replacing their unedited counterparts), replayed, and further edited, thereby making the future replays more effective in overcoming catastrophic forgetting. 1Code can be found at https://github.com/INK-USC/GMED. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Since no explicit generative model is involved, GMED approach retains the advantages of memory-based methods and is straightforward to train only inducing a small computation overhead.
The main consideration in allowing “editing” via a gradient update is the choice of the optimization objective. In light of recent work on designing alternative replay strategies [2, 7, 40], we hypothesize that “interfering” examples (i.e., past examples that suffer from increased loss) should be prioritized for replay. For a particular stored example, GMED ﬁnds a small update over the example (“edit”) such that the resulting edited example yields the most increase in loss when replayed. GMED additionally penalizes the loss increase in the edited example to enforce the proximity of the edited example to the original sample, so that the edited examples stay in-distribution. As a result, replaying these edited examples is more effective in overcoming catastrophic forgetting. Since GMED focuses only on editing the stored examples, by construction, GMED is modular, i.e., it can be seamlessly integrated with other state-of-the-art memory-based replay methods [2, 5, 26].
We demonstrate the effectiveness of GMED with a comprehensive set of experiments over six benchmark datasets. In general, combining GMED with existing memory-based approaches results in consistent and statistically signiﬁcant improvements with our single best method establishing a new state-of-art performance on ﬁve datasets. Our ablative investigations reveal that the gains realized by
GMED are signiﬁcantly larger than those obtained from regularization effects in random perturbation, and can be accumulated upon data augmentation to further improve performance.
To summarize, our contributions are two-fold: (i) we introduce GMED, a modular framework for task-free online continual learning, to edit stored examples and make them more effective in alleviating catastrophic forgetting (ii) we perform intensive set of experiments to test the performance of GMED under various datasets, parameter setups (e.g., memory size) and competiting baseline objectives. 2