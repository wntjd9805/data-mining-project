Abstract
Bayesian neural networks are theoretically well-understood only in the inﬁnite-width limit, where Gaussian priors over network weights yield Gaussian priors over network outputs. Recent work has suggested that ﬁnite Bayesian networks may outperform their inﬁnite counterparts, but their non-Gaussian function space priors have been characterized only though perturbative approaches. Here, we derive exact solutions for the function space priors for individual input examples of a class of ﬁnite fully-connected feedforward Bayesian neural networks. For deep linear networks, the prior has a simple expression in terms of the Meijer G-function. The prior of a ﬁnite ReLU network is a mixture of the priors of linear networks of smaller widths, corresponding to different numbers of active units in each layer.
Our results unify previous descriptions of ﬁnite network priors in terms of their tail decay and large-width behavior. 1

Introduction
Modern Bayesian neural networks (BNNs) ubiquitously employ isotropic Gaussian priors over their weights [1–22]. Despite their simplicity, these weight priors induce richly complex priors over the network’s outputs [1, 4–22]. These function space priors are well-understood only in the limit of inﬁnite hidden layer width, in which they become Gaussian [4–8]. However, these inﬁnite networks cannot ﬂexibly adapt to represent the structure of data during inference, an ability that is key to the empirical successes of deep learning, Bayesian or otherwise [2, 3, 9–14, 16–20, 23, 24]. As a result, elucidating how ﬁnite-width networks differ from their inﬁnite-width cousins is an important objective for theoretical study.
Progress towards this goal has been made through systematic study of the leading asymptotic cor-rections to the inﬁnite-width prior [12, 14–17], including approaches emphasizing the physical framework of effective ﬁeld theory [13, 14]. However, the applicability of these perturbative ap-proaches to narrow networks, particularly those with extremely narrow bottleneck layers [9], remains unclear. In this paper, we present an alternative treatment of a simple class of BNNs, drawing inspiration from the study of exactly solvable models in physics [25–27]. Our primary contributions are as follows: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
• We derive exact formulas for the priors over the output preactivations of ﬁnite fully-connected feedforward linear or ReLU BNNs without bias terms induced by Gaussian priors over their weights (§3). We only consider the prior for a single input example, not the joint prior over the outputs for multiple input examples, as it can capture many ﬁnite-width effects [9, 12]. Our result for the prior of a linear network is given in terms of the Meijer
G-function, which is an extremely general but well-studied special function [28–32]. The prior of a ReLU network is a mixture of the priors of linear networks of narrower widths, corresponding to different numbers of active ReLUs in each layer.
• We leverage our exact formulas to provide a simple characterization of ﬁnite-width network priors (§4). The fact that the priors of ﬁnite-width networks become heavy-tailed with increasing depth and decreasing width [21, 22], as well as the asymptotic expansions for the priors at large hidden layer widths [12, 15], follow as corollaries of our main results.
Moreover, we show that the perturbative ﬁnite-width corrections do not capture the heavy-tailed nature of the true prior.
To the best of our knowledge, our results constitute the ﬁrst exact solutions for the priors over the outputs of ﬁnite deep BNNs. As one might expect from knowledge of even the simplest interacting systems in physics [25–27], these solutions display many intricate, non-Gaussian properties, despite the fact that they are obtained for a somewhat simpliﬁed setting. 2 Preliminaries
In this section, we deﬁne our notation and problem setting. We use subscripts to index layer-dependent quantities. We denote the standard (cid:96)2 inner product of two vectors a, b ∈ Rn by a · b. Depending on context, we use (cid:107) · (cid:107) to denote the (cid:96)2 norm on vectors or the Frobenius norm on matrices.
We consider a fully-connected feedforward neural network f : Rn0 → Rnd with d layers and no bias terms, deﬁned recursively in terms of its preactivations h(cid:96) as h0 = x, h(cid:96) = W(cid:96)φ(cid:96)−1(h(cid:96)−1) (1) (2) (3) where n(cid:96) is the width of the (cid:96)-th layer (i.e., h(cid:96) ∈ Rn(cid:96)) and the activation functions φ(cid:96) act elementwise
[2, 3]. Without loss of generality, we take the input activation function φ0 to be the identity. We consider linear and ReLU networks, with φ(cid:96)(x) = x or φ(cid:96)(x) = max{0, x} for (cid:96) = 1, . . . , d − 1, respectively. As we focus on the output preactivations hd, we do not impose any assumptions on the output activation function φd. ((cid:96) = 1, . . . , d), f = φd(hd),
We take the prior over the weight matrices to be an isotropic Gaussian distribution [1–14, 16–22], with
[W(cid:96)]ij ∼ i.i.d.
N (0, σ2 (cid:96) ) (4) for layer-dependent variances σ2 (cid:96) . Depending on how one chooses σ(cid:96)—in particular, how it scales with the network width—this setup can account for most commonly-used neural network parameterizations
[24]. In particular, one usually takes σ2 (cid:96) [2–8, 12, 24].
This weight prior induces a conditional Gaussian prior over the preactivations at the (cid:96)-th layer [4–8]: (cid:96) /n(cid:96)−1 for some width-independent ς 2 (cid:96) = ς 2 h(cid:96) | h(cid:96)−1 ∼ N (0, σ2 (cid:96) (cid:107)φ(cid:96)−1(h(cid:96)−1)(cid:107)2In(cid:96) ), (5) where the prior for the ﬁrst hidden layer is conditioned on the input x, which we henceforth assume to be non-zero. Thus, the joint prior of the preactivations at all layers of the network for a given input x is of the form p(h1, . . . , hd | x) = p(hd | hd−1)p(hd−1 | hd−2) · · · p(h1 | x). (6)
To perform single-sample inference of the network outputs with a likelihood function pl(y | hd) for some target output y = y(x), one must compute the posterior p(hd | x, y) = pl(y | hd, x)pd(hd | x) p(y | x)
, (7) 2
where p(y | x) = (cid:82) dhd pl(y | hd, x)pd(hd | x) [1, 4–13, 18–20]. Before computing the posterior, it is therefore necessary to marginalize out the hidden layer preactivations h1, . . . , hd−1 to obtain the prior density of the output preactivation pd(hd | x). Moreover, in this framework, all information about the network’s inductive bias is encoded in the prior pd(hd | x), as the likelihood is independent of the network architecture and the prior over the weights. This marginalization has previously been studied perturbatively in limiting cases [4–9, 12, 14]; here we perform it exactly for any width.
To integrate out the hidden layer preactivations, it is convenient to work with the characteristic function ϕd(qd | x) corresponding to the density pd(hd | x). Adopting a convention for the Fourier transform such that pd(hd | x) = (cid:90) dqd (2π)nd exp(iqd · hd)ϕd(qd | x), (8) it follows from (5) that this characteristic function is given as
ϕd(qd | x) = (cid:90) d−1 (cid:89) (cid:96)=1 dq(cid:96) dh(cid:96) (2π)n(cid:96) exp (cid:32)d−1 (cid:88) (cid:96)=1 iq(cid:96) · h(cid:96) − 1 2 d (cid:88) (cid:96)=1 (cid:96) (cid:107)q(cid:96)(cid:107)2(cid:107)φ(cid:96)−1(h(cid:96)−1)(cid:107)2
σ2
. (9) (cid:33)
We immediately observe that the characteristic function is radial, i.e., ϕd(qd | x) = ϕd((cid:107)qd(cid:107) | x). As the inverse Fourier transform of a radial function is radial [33], this implies that the preactivation prior is radial, i.e., pd(hd | x) = pd((cid:107)hd(cid:107) | x). Moreover, as the prior at any given layer is separable over the neurons of that layer, we can see that pd(hd | x) has the property that the marginal prior distribution of some subset of k of the outputs of a network with nd > k outputs is identical to the full prior distribution of a network with k outputs. As detailed in Appendix A, these properties enable us to exploit the relationship between the Fourier transforms of radial functions and the Hankel transform, which underlies our calculational approach. 3 Exact priors of ﬁnite deep networks
Here, we present our main results for the priors of ﬁnite deep linear and ReLU networks, deferring their detailed derivations to Appendices A and B of the Supplemental Material. 3.1 Two-layer linear networks
As a warm-up, we ﬁrst consider a linear network with a single hidden layer. In this case, we can easily evaluate the integral (9) to obtain the characteristic function
ϕ2(q2 | x) = (1 + κ2 2(cid:107)q2(cid:107)2)−n1/2, (10) where we deﬁne the quantity κ2 ≡ σ1σ2(cid:107)x(cid:107) for brevity. We can now directly evaluate the required
Hankel transform to obtain the prior density (see Appendix A.1), yielding p2(h2 | x) = 1 (4πκ2 2)n2/2 2
Γ(n1/2) (cid:18) (cid:107)h2(cid:107) 2κ2 (cid:19)(n1−n2)/2
K(n1−n2)/2 (cid:18) (cid:107)h2(cid:107)
κ2 (cid:19)
, (11) where Γ is the Euler gamma function and Kν(z) is the modiﬁed Bessel function of the second kind of order ν [28–30].
Interestingly, we recognize this result as the distribution of the sum of n1/2 independent n2-dimensional multivariate Laplace random variables with covariance matrix 2κ2 2In2 [34]. Moreover, we can see from the characteristic function (10) that we recover the expected Gaussian behavior at inﬁnite width provided that κ2 2 ∝ 1/n1 [4–8], as one would expect from the interpretation of this prior as a sum of i.i.d. random vectors. To our knowledge, this simple correspondence has not been previously noted in the literature, though it provides a succinct explanation of the slight heavy-tailedness of this prior distribution noted by Vladimirova et al. [21, 22]. The fact that the function space prior is heavy-tailed at ﬁnite width is a particularly important non-Gaussian feature.
These results are plotted in Figure 1. 3
Figure 1: Priors of deep linear networks of depths d = 2, 3, and 4. In each panel, the prior density is plotted only for positive values of the output preactivation hd, as it is symmetric about zero. For each depth, all hidden layers are of the same width n, which is indicated by line color. The black line indicates the Gaussian inﬁnite-width limit discussed in §4.3. Thick lines show the exact priors, while thin jagged lines show experimental estimates from 108 examples. Further details on the numerical methods used to generate these ﬁgures are provided in Appendix E. 3.2 General deep linear networks
We now consider a general deep linear network. Deferring the details of our derivation to Appendix
A, we ﬁnd that the characteristic function and density of the function space preactivation prior for such a network can be expressed in terms of the Meijer G-function [28, 29]. The Meijer G-function is an extremely general special function, of which most classical special functions are special cases.
Despite its great generality, it is quite well-studied, and provides a powerful tool in the study of integral transforms [28–31]. Its standard deﬁnition, introduced by Erdélyi [29], is as follows: Let 0 ≤ m ≤ q and 0 ≤ n ≤ p be integers, and let a1, . . . , ap and b1, . . . , bq be real or complex parameters such that none of ak − bj are positive integers when 1 ≤ k ≤ n and 1 ≤ j ≤ m. Then, the Meijer G-function is deﬁned via the Mellin-Barnes integral
Gm,n p,q (cid:18) z (cid:12) (cid:12) (cid:12) (cid:12) a1, . . . , ap b1, . . . , bq (cid:19)
= 1 2πi (cid:90)
C ds zs (cid:81)m j=1 Γ(bj − s) (cid:81)n j=m+1 Γ(1 − bj + s) (cid:81)p (cid:81)q k=1 Γ(1 − aj + s) k=n+1 Γ(ak + s)
, (12) where empty products are interpreted as unity and the integration path C separates the poles of
Γ(bj − s) from those of Γ(1 − ak + s) [28, 29]. Expressing the density or characteristic function of a radial distribution in terms of the Meijer G-function is useful because one can then immediately read off its Mellin spectrum and absolute moments [28, 29]. Moreover, one can exploit integral identities for the Meijer G-function to compute other expectations and transformations of the density [28–31].
With this deﬁnition, the characteristic function and density of the prior of a deep linear network are given as d (qd | x) = γdG1,d−1
ϕlin d−1,1 (cid:18) 2d−2κ2 d(cid:107)qd(cid:107)2 (cid:12) (cid:12) (cid:12) (cid:12) 1 − n1/2, . . . , 1 − nd−1/2 0 (cid:19) (13) and plin d (hd | x) =
γd (2dπκ2 d)nd/2
Gd,0 0,d (cid:18) (cid:107)hd(cid:107)2 2dκ2 d (cid:12) (cid:12) (cid:12) (cid:12)
− 0, (n1 − nd)/2, . . . , (nd−1 − nd)/2 (cid:19)
, (14) respectively, where we deﬁne the quantities
κd ≡ σ1 · · · σd(cid:107)x(cid:107) and γd ≡ d−1 (cid:89) (cid:96)=1 1
Γ(n(cid:96)/2) (15) for brevity. Here, the horizontal dash in the upper row of arguments to Gd,0 0,d indicates the absence of ‘upper’ arguments to the G-function, denoted by a1, . . . , ap in (12), because p = 0. For d = 2, 4
Figure 2: Priors of depth d = 4 linear networks with narrow bottlenecks. The left panel shows a diagram of a depth d = 4 network with two wide hidden layers of widths n1 and n3 separated by a narrow bottleneck of width n2 = 2. The right panel shows prior densities for networks of this structure with n1 = n3 = 100 and variable bottleneck widths n2, which is indicated by line color.
The prior density is plotted only for positive values of the output preactivation hd, as it is symmetric about zero. The black line indicates the Gaussian limit in which the widths of all three hidden layers are taken to inﬁnity, as discussed in §4.3. Further details on the numerical methods used to generate this ﬁgure are provided in Appendix E. we can use G-function identities to recover our earlier results (10) and (11) for a two-layer network (see Appendix A) [29]. We plot the exact density for networks of depths d = 2, 3, and 4 and various widths along with densities estimated from numerical sampling in Figure 1, illustrating that our exact result displays the expected perfect agreement with experiment (see Appendix E for details of our numerical methods).
For any depth, the density (14) has the intriguing property that its functional form depends only on the difference between the hidden layer widths and the output dimensionality. This suggests that the priors of networks with large input and output dimensionalities but narrow intermediate bottlenecks—as would be the case for an autoencoder—will differ noticeably from those of networks with only a few outputs. However, it is challenging to visualize a distribution over more than two variables. We therefore plot the marginal prior over a single component of the output of a network with a bottleneck layer of varying width in Figure 2. Qualitatively, the prior for a network with a narrow bottleneck layer sandwiched between two wide hidden layers is more similar to that of a uniformly narrow network than that of a wide network without a bottleneck. These observations are consistent with previous arguments that wide networks with narrow bottlenecks may possess interesting priors [9, 35]. 3.3 Deep ReLU networks
Finally, we consider ReLU networks. For this purpose, we adopt a more verbose notation in which the dependence of the prior on width is explicitly indicated, writing plin d (hd; κd; n1, . . . , nd−1, nd) for the prior density (14) of a linear network with the speciﬁed hidden layer widths. Similarly, we write pReLU (hd; κd; n1, . . . , nd−1, nd) for the prior density of the corresponding ReLU network. As shown in Appendix B, we ﬁnd that d pReLU d (hd; κd; n1, . . . , nd) (cid:18)
= 1 − (2n1 − 1)(2n2 − 1) · · · (2nd−1 − 1) 2n1+···+nd−1 nd−1 n1(cid:88) (cid:88) (cid:19)
· · · (cid:18)n1 k1
· · · (cid:18)nd−1 kd−1
+ 1 2n1+···+nd−1 (cid:19)
δ(hd) k1=1 kd−1=1 (cid:19) plin d (hd; κd; k1, . . . , kd−1, nd), (16) where δ(hd) is the nd-dimensional Dirac distribution. We prove this result by induction on network depth d, using the characteristic function corresponding to this density. The base case d = 2 follows by direct integration and the binomial theorem, and the inductive step uses the fact that the linear 5
Figure 3: The prior of a deep ReLU network. (a) Schematic depiction of the ReLU prior as a mixture of the priors of linear networks of different widths (16). Grey nodes indicate ‘inactive’ units, while the linear network of active units is shown by the orange nodes. (b) ReLU prior densities for networks of depths d = 2, 3, and 4 and varying width. Here, we choose κd such that the variance of the preactivations matches that of the linear networks shown in Figure 1. In each panel, the prior density is plotted only for positive values of the output preactivation hd, as it is symmetric about zero. For each depth, all hidden layers are of the same width n, which is indicated by line color. The black line indicates the Gaussian inﬁnite-width limit discussed in §4.3. Thick lines show the exact priors, while thin jagged lines show experimental estimates from 108 examples. Further details on the numerical methods used to generate these ﬁgures are provided in Appendix E. network prior (14) is radial and has marginals equal to the priors of linear networks with fewer outputs. This result has a simple interpretation: the prior for a ReLU network is a mixture of priors of linear networks corresponding to different numbers of active ReLU units in each hidden layer, along with a Dirac distribution representing the cases in which no output units are active. As we did for linear networks, we plot the exact density along with numerical estimates in Figure 3, showing perfect agreement. 4 Properties of these priors
Having obtained exact expressions for the priors of deep linear or ReLU networks, we brieﬂy characterize their properties, and how those properties relate to prior analyses of ﬁnite network priors. 4.1 Moments
We ﬁrst consider the moments of the output preactivation. As the prior distributions are zero-centered and isotropic, it is clear that all odd raw moments vanish. However, the moments of the norm of the output preactivation are non-vanishing. In particular, using basic properties of the Meijer G-function
[28, 29], we can easily read off the moments for a linear network as
Elin(cid:107)hd(cid:107)m = 2dm/2κm d (cid:17)m/2 d (cid:89) (cid:96)=1 (cid:16) n(cid:96) 2 6 (m ≥ 0), (17)
where ab = Γ(a + b)/Γ(a) is the rising factorial [28]. This result takes a particularly simple form for the even moments m = 2k, in which case (n/2)k = 2−k (cid:81)k−1 j=0 (n + 2j). Most simply, for m = 2, we have Elin(cid:107)hd(cid:107)2 = κ2
Similarly, for ReLU networks, we have dn1 · · · nd.
EReLU(cid:107)hd(cid:107)m = 2dm/2κm d (cid:16) nd 2 (cid:17)m/2 d−1 (cid:89) (cid:34) (cid:96)=1 1 2n(cid:96) n(cid:96)(cid:88) k(cid:96)=1 (cid:18)n(cid:96) k(cid:96) (cid:19) (cid:18) k(cid:96) 2 (cid:19)m/2(cid:35)
. (18)
Each term in the product over (cid:96) expands in terms of generalized hypergeometric functions evaluated at unity [28]. As for linear networks, this expression has a particularly simple form for even moments, particularly if m = 2, for which EReLU(cid:107)hd(cid:107)2 = 21−dκ2 dn1 · · · nd. Therefore, for identical weight variances, the variance of the output preactivation of a ReLU network is 21−d times that of a linear network of the same width and depth. However, one can compensate for this variance reduction by simply doubling the variances of the priors over the hidden layer weights.
Using the property that the marginal prior distribution of a single component of the output is identical to the prior of a single-output network, these results give the marginal absolute moments of the prior of a linear or ReLU network. Moreover, these results can also be used to obtain joint moments of different components by exploiting the fact that the prior is radial. By symmetry, the odd moments vanish, and the even moments are given up to combinatorial factors by the corresponding moments of any individual component of the preactivation. For example, the covariance of two components of the output preactivation is Ehd,ihd,j = (Eh2 d,1)δij for all i, j = 1, . . . , nd. 4.2 Tail bounds
Vladimirova et al. [21, 22] have shown that the marginal prior distributions of the preactivations of deep networks with ReLU-like activation functions and ﬁxed, ﬁnite widths become increasingly heavy-tailed with depth. This behavior contrasts sharply with the thin-tailed Gaussian prior of inﬁnite-width networks [4–8]. In particular, Vladimirova et al. [21, 22] showed that the prior distributions are sub-Weibull with optimal tail parameter θ = d/2, meaning that they satisfy
P(|hd,j| ≥ ρ) ≤ C exp(−ρ1/θ) (19) for each neuron j ∈ {1, . . . , nd}, all ρ > 0, and some constant C > 0 if θ ≥ d/2, but not if θ < d/2.
A sub-Gaussian distribution is sub-Weibull with optimal tail parameter at most 1/2; distributions with larger tail parameters have increasingly heavy tails. As shown in Appendix C, we can use the results of §4.1 to give a straightforward derivation of this result, showing that the norm (cid:107)hd(cid:107) of the output preactivation for either linear or ReLU networks is sub-Weibull with optimal tail parameter d/2. Due to the aforementioned fact that the marginal prior for a single output of a multi-output network is identical to the prior for a single-output network, this implies (19). 4.3 Asymptotic behavior
Most previous studies of the priors of deep Bayesian networks have focused on their asymptotic behavior for large hidden layer widths. Provided that one takes
κd = (n1 · · · nd−1)−1/2κd (20) for κd independent of the hidden layer widths such that the preactivation variance remains ﬁnite, the prior tends to a Gaussian as n1, · · · , nd−1 → ∞ for ﬁxed d, n0, and nd [4–10, 12, 24]. This behavior is qualitatively apparent in Figures 1 and 3. Here, we exploit our exact results to study this asymptotic regime. An ideal approach would be to study the asymptotic behavior of the characteristic function (13) and apply Lévy’s continuity theorem [36] to obtain the Gaussian limit, but we are not aware of suitable doubly-scaled asymptotic expansions for the Meijer G-function [28, 29]. Instead, we use a multivariate Edgeworth series to obtain an asymptotic expansion of the density [37]. As 7
Figure 4: The large-width Edgeworth approximation for the prior density is thin-tailed. From left to right, the panels show the priors of linear networks of depths d = 2, 3, and 4 of varying widths.
In each panel, solid lines show the exact prior density (14), while dashed lines show the asymptotic
Edgeworth approximation (21). The exact prior density is computed numerically as described in
Appendix E. detailed in Appendix D, we ﬁnd that the prior of a linear network has an Edgeworth series of the form plin d (hd | x) ≈ 1 (2πκ2 d)nd/2 (cid:34) 1 4 1 +
× (cid:18) exp (cid:32)d−1 (cid:88) (cid:96)=1 1 n(cid:96) (cid:19)
− (cid:107)hd(cid:107)2 2κ2 d (cid:33) (cid:18) (cid:107)hd(cid:107)4
κ4 d
− 2(nd + 2) (cid:107)hd(cid:107)2
κ2 d (cid:19)
+ nd(nd + 2)
+ O (cid:19)(cid:35)
. (cid:18) 1 n2 (21)
The Edgeworth expansion of the prior of a ReLU network is of the same form, with the factor of 1/4 scaling the ﬁnite-width correction being replaced by 5/4, and the variance κ2 d re-scaled by 21−d.
Heuristically, this result makes sense given that the binomial sums in (16) will be dominated by k(cid:96) ≈ n(cid:96)/2 in the large-width limit.
These results succinctly reproduce the leading ﬁnite-width corrections formally written down by
Antognini [15] and recursively computed by Yaida [12]. However, importantly, this approxi-mate distribution is sub-Gaussian: it cannot capture the depth-dependent heaviness of the tails of the true ﬁnite-width prior described in §4.2. More generally, one can see that the heavier-than-Gaussian tails of the ﬁnite width prior are an essentially non-perturbative effect. At any ﬁnite order of the Edgeworth expansion, the approximate density for a network of any ﬁxed depth is of the form (2πκ2 d)[1 + f ((cid:107)hd(cid:107)2/κ2 d)], where f is a polynomial satisfying (cid:82) dh exp(−(cid:107)h(cid:107)2/2)f ((cid:107)h(cid:107)2) = 0 [37]. Such a density is sub-Gaussian. In Figure 4, we illustrate the discrepancy between the thin tails of the Edgeworth expansion and the heavier tails of the exact prior.
Even at the relatively modest depths shown, the increasing discrepancy between the tail behavior of the approximate prior and the true tail behavior with increasing depth is clearly visible. We emphasize that low-order Edgeworth expansions will capture some qualitative features of the ﬁnite-width prior, but not all. It is therefore important to consider approximation accuracy on a case-by-case basis depending on what features of ﬁnite BNNs one aims to study. d)−nd/2 exp(−(cid:107)hd(cid:107)2/2κ2 5