Abstract
In this paper, we consider the problem of iterative machine teaching, where a teacher provides examples sequentially based on the current iterative learner. In contrast to previous methods that have to scan over the entire pool and select teaching examples from it in each iteration, we propose a label synthesis teaching framework where the teacher randomly selects input teaching examples (e.g., images) and then synthesizes suitable outputs (e.g., labels) for them. We show that this framework can avoid costly example selection while still provably achieving exponential teachability. We propose multiple novel teaching algorithms in this framework. Finally, we empirically demonstrate the value of our framework. 1

Introduction
Machine teaching [103, 106] studies the problem of constructing a minimal dataset for a target concept such that a learner can learn the concept based on this dataset. Machine teaching has diverse applications ranging from crowd sourcing [71, 72, 100, 101] to model robustness [2, 3, 54, 63].
Machine teaching also has nice connections with curriculum learning [7] and coresets [1, 28].
Based on the learner, machine teaching can be performed in either batch or sequential fashion.
The majority of prior work studies batch machine teaching [42, 56, 102, 103], where a teacher con-structs a minimal batch set of training samples and provides it to a student in one shot without further interactions. Then the student keeps learn-ing from this batch dataset for the target concept.
The size of such a minimal batch set is called teaching dimension [22]. Differently, sequential machine teaching [38, 43, 44, 61] bridges the gap between machine teaching and practical learning algorithms by studying the sequential (i.e., itera-tive) learner such as neural networks. A typical example is iterative machine teaching (IMT) [43, 44] where the teacher guides a learner to a target concept by interacting with the learner (e.g., feeding training samples) in every iteration. The minimum number of such iterations is called the iterative teaching dimension. One of the largest challenges in sequential teaching is how to effectively and efﬁciently provide teaching examples to the iterative learner. Usually we are mostly interested in the pool-based teaching in IMT since it well matches the setting of modern machine learning. However, exploring all the possible teaching trajectories is computationally prohibitive. For example, there are (cid:0)m possible teaching trajectories (n is the number of iterations) if we select k samples per iteration k from a pool of size m. Due to such a huge search space, the selection of teaching examples is a combinatorial problem that is inherently difﬁcult to solve. IMT [43] performs the teaching sample selection with a greedy policy, but it could be substantially sub-optimal in certain cases [38] and its computational complexity also scales linearly with the size of the dataset.
Figure 1: Comparison of vanilla iterative machine teaching and label synthesis teaching. The red dotted frames indicate the teacher’s efforts. (cid:1)n 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We propose a general teaching framework called LAbel Synthesis Teaching (LAST), which in its standard version avoids the problems of sample selection – though we will later discuss how sample selection can be combined with our approach. In the standard version of LAST, teaching examples are randomly sampled (similar to SGD) in the pool and the teacher synthesizes their labels in order to quickly guide the learner to the desired model. A brief comparison between IMT and LAST is given in Fig. 1. LAST restricts the teaching to the label space and bypasses the selection of teaching samples. Therefore, LAST avoids the high complexity of selecting teaching samples in a large pool.
Intuition for why LAST is able to achieve promising teaching performance comes from the empirical success of knowledge distillation [26] and label smoothing [73]. Knowledge distillation shows that training neural networks with soft labels from a pretrained model can generally improve generalization.
Label smoothing demonstrates that the ground truth labels are not necessarily the optimal supervision for training a model. Instead, smoothing the label with uniform distribution can calibrate the model and lead to better generalization. Both methods can be viewed as providing an alternative label (instead of the ground truth) to the learner in order to improve its generalizability. Moreover, [77, 78] show that there exists privileged information beyond the ground truth labels that can signiﬁcantly improve the convergence rate of learning algorithms. Therefore, now we can safely argue that the ground truth labels are not always optimal learning signals. Motivated by these work, we aim to construct a teacher model that can adaptively synthesize suitable labels for a learner (with the hope to implicitly encode priviledged information) in order to improve the learner’s convergence.
Speciﬁcally, we study LAST primarily under the omniscient scenario where the teacher knows everything about the learner (e.g., the optimal learner parameters). To perform omniscient teaching, we consider a greedy teacher and a parameterized teacher. We show that greedy teaching can achieve exponential teachability (ET) [43] without selecting teaching examples. Additionally, we touch upon the black-box teaching scenario where the teacher knows less about the learner (e.g., the optimal learner parameters are unavailable), and discuss how to perform LAST in this case.
LAST provides a uniﬁed view for understanding soft label methods, e.g., knowledge distillation [26, 53], label smoothing [11, 59, 73], and self-training [104]. All these methods can be interpreted as modifying the labels to achieve desirable learning behavior and outcome. With LAST, we can connect iterative machine teaching to many classic learning algorithms and shed novel light on them. 2