Abstract
Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant “positive" pairs higher than the irrelevant “negative" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efﬁcient and compares favorably with more complex, state-of-the-art approaches. 1

Introduction
Learning to represent objects as dense vectors, often called embeddings, has proved to be crucial in large scale information retrieval tasks from multiple domains including recommendation systems [39], vision [18] and natural language processing [13, 22]. A popular paradigm for such learning tasks involves training two separate neural networks (often called two-towers or dual-encoders), each representing a query and a document. Given positive and negative (query, document) pairs, the learning task trains the two networks by minimizing a loss function, usually softmax cross-entropy, to encourage positive pairs to have higher similarity scores and negative pairs to have lower scores.
While it is easy to sample positive pairs of examples through user feedback such as impressions or clicks, it is more challenging to sample good negative pairs from a pool of potentially millions or even billions of documents. A large number of negative pairs is often required to ensure high quality of the ﬁnal model, which makes the training process expensive.
A number of strategies have been proposed in the literature to address the problem of sampling good negative pairs from a large corpus. The most common approach is to use in-batch negatives, which treats random, non-positive pairs in a minibatch as negatives [15, 22]. This approach is computationally efﬁcient and works in a streaming setting, but the pool of negative examples is limited to the minibatch. Towards the later stages of the training, the in-batch negatives become less informative (i.e., have low gradients) since they are sampled randomly without paying attention 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to which negatives are hard for a given query [22]. Another popular approach is to maintain an asynchronous retrieval index of the full dataset for negative sampling [13, 38]. Negatives from the full dataset can be extracted based on approximate retrieval techniques such as ScaNN [11], Faiss [20] or
SPTAG [7]. However, it requires coordinating with a separate process for re-indexing and re-building the retrieval index, which is not only computationally expensive and hard to maintain but also suffers from the problem of stale index.
In this work, we theoretically and experimentally analyze training dual encoders using a large cache of negative elements. The cached elements are stale, as they may be generated from a prior iteration’s parameters. Our contributions are the following.
Main Contributions:
• We propose an approach to train retrieval models with cross-entropy loss using a large negative cache. We utilize Gumbel-Max sampling on the cached embeddings to efﬁciently sample the negatives.
• We analyze the convergence of our algorithm in terms of the refresh rate of the cache.
We show that even for a small refresh rate we can obtain a ﬁrst-order convergence rate comparable to that of getting exact gradients using the entire dataset.
• We develop a streaming version of our approach. Our streaming algorithm allows us to scale to very large datasets and avoids needing to maintain an up-to-date index for nearest neighbor search (or maximum inner product search). We analyze the bias induced by our method and how it affects the convergence.
• We experimentally validate our method using the MS MARCO and TREC 2019 passage retrieval tasks. We show that our approach can be efﬁciently implemented and achieves statistical performance comparable to state-of-the-art benchmarks with a computationally simpler approach that requires only a fraction of the memory. 2