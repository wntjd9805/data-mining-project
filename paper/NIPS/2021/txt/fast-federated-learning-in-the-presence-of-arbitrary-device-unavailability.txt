Abstract
Federated Learning (FL) coordinates with numerous heterogeneous devices to collaboratively train a shared model while preserving user privacy. Despite its multiple advantages, FL faces new challenges. One challenge arises when devices
In drop out of the training process beyond the control of the central server. this case, the convergence of popular FL algorithms such as FedAvg is severely inﬂuenced by the straggling devices. To tackle this challenge, we study federated learning algorithms under arbitrary device unavailability and propose an algorithm named Memory-augmented Impatient Federated Averaging (MIFA). Our algorithm efﬁciently avoids excessive latency induced by inactive devices, and corrects the gradient bias using the memorized latest updates from the devices. We prove that
MIFA achieves minimax optimal convergence rates on non-i.i.d. data for both strongly convex and non-convex smooth functions. We also provide an explicit characterization of the improvement over baseline algorithms through a case study, and validate the results by numerical experiments on real-world datasets. 1

Introduction
Federated learning is a machine learning setting in which a central server coordinates with a large number of devices to collectively train a shared model [28, 34, 20, 33, 25, 26]. Practical advantages of this training scheme are mainly twofold. First, each device keeps the private data locally and hence preserves its data privacy. Second, federated learning can make use of idle computing resources and lower computation costs. Although federated learning successfully scales up with data sizes and accelerates training via more affordable computing power [43, 38, 36], the collaborative setup leads to new challenges due to large variations among individual computing devices. Our work aims to formulate and investigate the impact of device variations on FL from an optimization perspective.
In FL, a device can differ from its peers in multiple aspects [17, 25]. First, the data distribution and local task can be different among devices. To address the data variation, non-i.i.d. objective models were proposed and analyzed by [26, 18, 19, 42, 25]. We follow this line of work and formulate our optimization objective as a sum of stochastic functions on individual devices (See Eqn. (1)).
∗Equal contribution.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A second variation among devices is caused by different computing and communication speeds. One natural way to formulate the variation in computation speeds is to allow asynchronous updates and model the updates as delayed responses. Lots of novel research has studied the problem with different delay models, e.g., [35, 4, 27, 11, 45, 1, 5, 13]. However, the delayed setup assumes that all devices make roughly the same number of (delayed) responses in the end. This behavior may deviate largely from the FL practice, where each device, e.g., personal cell phones, can have very different active duration when participating in the FL training, and hence make different numbers of responses. For this reason, our work aims to address this third discrepancy among devices caused by individual availability patterns.
The third device heterogeneity caused by different availability patterns is less studied in optimization for federated learning problems. In this model, instead of making a delayed response, devices can abort the training halfway, e.g., due to battery level, incoming calls, etc, and fail to return their responses upon the central server’s requests [28, 7, 17]. To handle missing responses, researchers propose algorithms where the central server may collect responses from only a fraction of the devices and make updates [18, 28, 42, 25, 26, 17, 30, 14].
Previous works on collecting responses from a fraction of devices can be divided into two categories.
When the response distribution is known, one could collect only the fastest responses and re-weight according to their response probability [17, 26, 30]. This model can be restrictive, as in practice, the exact distribution may not be available and may evolve. Another line of work assumes that the server can arbitrarily decide and sample a set of devices to collect responses accordingly in every communication round [18, 28, 42, 25, 14]. This model does not require knowing the response possibility. However, the response time can be very long if the selected subset contains unavailable devices.
In this work, we address the above limitations by studying federated learning in the presence of arbitrary device unavailability. Within this practical setup, we propose an algorithm that automatically accommodates for the underlying unavailability and allows patterns of the device unavailability to be non-stationary and even adversarial. Furthermore, our algorithm can achieve optimal convergence rates in the presence of device inactivity and automatically reduce to best-known rates if all devices are active. Our contributions are summarized as follows.
• We investigate the federated learning problem with a practical formulation of device partici-pation, which does not require each device to be online according to an (either known or unknown) distribution.
• We propose the Memory-augmented Impatient Federated Averaging (MIFA) algorithm that is agnostic to the availability pattern. It efﬁciently avoids excessive latency induced by inactive devices, successfully exploits the information about the descent direction in stale and noisy gradients, and corrects the gradient bias using the memorized latest updates.
• We prove that MIFA achieves minimax optimal convergence rates O (cid:0) ¯τT +1 (cid:1) for smooth, strongly convex functions, and O for smooth, non-convex functions, and es-tablish matching lower bounds. Here, N, K and T stand for the number of devices, local updates and communication rounds respectively. ¯τT and ¯ν characterize how actively devices participate in training (see formal deﬁnitions in Sections 3, 5 and 6). MIFA also achieves optimal convergence rates in the ideal case when all devices are active. (cid:16)(cid:113) ¯ν+1
N KT
N KT (cid:17)
• We provide an explicit characterization of the improvement over baseline algorithms through a case study and empirically verify our results on real-world datasets. 2