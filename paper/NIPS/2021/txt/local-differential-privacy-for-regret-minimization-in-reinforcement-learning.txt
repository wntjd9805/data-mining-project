Abstract
Reinforcement learning algorithms are widely used in domains where it is desirable to provide a personalized service. In these domains it is common that user data contains sensitive information that needs to be protected from third parties. Moti-vated by this, we study privacy in the context of ﬁnite-horizon Markov Decision
Processes (MDPs) by requiring information to be obfuscated on the user side.
We formulate this notion of privacy for RL by leveraging the local differential privacy (LDP) framework. We establish a lower bound for regret minimization in
ﬁnite-horizon MDPs with LDP guarantees which shows that guaranteeing privacy has a multiplicative effect on the regret. This result shows that while LDP is an appealing notion of privacy, it makes the learning problem signiﬁcantly more complex. Finally, we present an optimistic algorithm that simultaneously satisﬁes
ε-LDP requirements, and achieves √K/ε regret in any ﬁnite-horizon MDP after
K episodes, matching the lower bound dependency on the number of episodes K. 1

Introduction
The practical successes of Reinforcement Learning (RL) algorithms have led to them becoming ubiquitous in many settings such as digital marketing, healthcare and ﬁnance, where it is desirable to provide a personalized service [e.g., 1, 2]. However, users are becoming increasingly wary of the amount of personal information that these services require. This is particularly pertinent in many of the aforementioned domains where the data obtained by the RL algorithm are highly sensitive.
For example, in healthcare, the state encodes personal information such as gender, age, vital signs, etc. In advertising, it is normal for states to include browser history, geolocalized information, etc. Unfortunately, [3] has shown that, unless sufﬁcient precautions are taken, the RL agent leaks information about the environment (i.e., states containing sensitive information). That is to say, observing the policy computed by the RL algorithm is sufﬁcient to infer information about the data (e.g., states and rewards) used to compute the policy (scenario (cid:172)). This puts users’ privacy at jeopardy.
Users therefore want to keep their sensitive information private, not only to an observer but also to the service provider itself (i.e., the RL agent). In response, many services are adapting to provide stronger protection of user privacy and personal data, for example by guaranteeing privacy directly on the user side (scenario (cid:173)). This often means that user data (i.e., trajectories of states, actions, rewards) are privatized before being observed by the RL agent. In this paper, we study the effect that this has on the learning problem in RL. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Differential privacy (DP) [4] is a standard mechanism for preserving data privacy, both on the al-gorithm and the user side. The (ε, δ)-DP deﬁnition guarantees that it is statistically hard to infer information about the data used to train a model by observing its predictions, thus addressing scenario (cid:172). In online learning, (ε, δ)-DP has been studied in the multi-armed bandit framework [e.g., 5, 6].
However, [7] showed that DP is incompatible with regret minimization in the contextual bandit problems. This led to considering weaker or different notions of privacy [e.g., 7, 8]. Recently, [9] transferred some of these techniques to RL, presenting the ﬁrst private algorithm for regret mini-mization in ﬁnite-horizon problems. In [9], they considered a relaxed deﬁnition of DP called joint differential privacy (JDP) and showed that, under JDP constraints, the regret only increases by an additive term which is logarithmic in the number of episodes. Similarly to DP, in the JDP setting the privacy burden lies with the learning algorithm which directly observes user states and trajectories containing sensitive data. In particular, this means that the data itself is not private and could poten-tially be used –for example by the owner of the application– to train other algorithms with no privacy guarantees. An alternative and stronger deﬁnition of privacy is Local Differential Privacy (LDP) [10].
This requires that the user’s data is protected at collection time before the learning agent has access to it. This covers scenario (cid:173) and implies that the learner is DP. Intuitively, in RL, LDP ensures that each sample (states and rewards associated to an user) is already private when observed by the learning agent, while JDP requires computation on the entire set of samples to be DP. Recently, [11] showed that, in contrast to DP, LDP is compatible with regret minimization in contextual bandits.1
LDP is thus a stronger deﬁnition of privacy, simpler to understand and more user friendly. These characteristics make LDP more suited for real-world applications. However, as we show in this paper, guaranteeing LDP in RL makes the learning problem more challenging. (cid:0) (cid:0)
− eε
{
H√SAK/ min 1, 1
}
H√SAK + SAH log(KH)/ε (cid:1)
Contributions. In this paper, we study LDP for regret minimization in ﬁnite horizon reinforcement learning problems with S states, A actions, and a horizon of H.2 Our contributions are as follows. 1)
, showing LDP is
We provide a regret lower bound for (ε, δ)-LDP of Ω inherently harder than JDP, where the lower-bound is only Ω
[9]. 2) We propose the ﬁrst LDP algorithm for regret minimization in RL. We use a general privacy-preserving mechanism to perturb information associated to each trajectory and derive LDP-OBI, an optimistic model-based (ε, δ)-LDP algorithm with regret guarantees. 3) We present multiple privacy-preserving mechanisms that are compatible with LDP-OBI and show that their regret is
O(√K/ε) up to some mechanism dependent terms depending on S, A, H. 4) We perform numerical simulations to evaluate the impact of LDP on the learning process. For comparison, we build a
Thompson sampling algorithm [e.g., 12] for which we provide LDP guarantees but no regret bound. (cid:101)