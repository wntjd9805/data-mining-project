Abstract
α )-competitive for α-polyhedral functions and 1 + 4
In this paper, we revisit the problem of smoothed online learning, in which the online learner suffers both a hitting cost and a switching cost, and target two per-formance metrics: competitive ratio and dynamic regret with switching cost. To bound the competitive ratio, we assume the hitting cost is known to the learner in each round, and investigate the simple idea of balancing the two costs by an optimization problem. Surprisingly, we ﬁnd that minimizing the hitting cost alone is max(1, 2
λ -competitive for
λ-quadratic growth functions, both of which improve state-of-the-art results signi-ﬁcantly. Moreover, when the hitting cost is both convex and λ-quadratic growth, we reduce the competitive ratio to 1 + 2√ by minimizing the weighted sum of the
λ hitting cost and the switching cost. To bound the dynamic regret with switching cost, we follow the standard setting of online convex optimization, in which the hitting cost is convex but hidden from the learner before making predictions. We modify Ader, an existing algorithm designed for dynamic regret, slightly to take into account the switching cost when measuring the performance. The proposed al-gorithm, named as Smoothed Ader, attains an optimal O((cid:112)T (1 + PT )) bound for dynamic regret with switching cost, where PT is the path-length of the comparator sequence. Furthermore, if the hitting cost is accessible in the beginning of each round, we obtain a similar guarantee without the bounded gradient condition, and establish an Ω((cid:112)T (1 + PT )) lower bound to conﬁrm the optimality. 1

Introduction
Online learning is the process of making a sequence of predictions given knowledge of the answer to previous tasks and possibly additional information [Shalev-Shwartz, 2011]. While the traditional online learning aims to make the prediction as accurate as possible, in this paper, we study smoothed online learning (SOL), where the online learner incurs a switching cost for changing its predictions between rounds [Cesa-Bianchi et al., 2013]. SOL has received lots of attention recently because in many real-world applications, a change of action usually brings some additional cost. Examples include the dynamic right-sizing for data centers [Lin et al., 2011], geographical load balancing [Lin et al., 2012], real-time electricity pricing [Kim and Giannakis, 2014], video streaming [Joseph and de Veciana, 2012], spatiotemporal sequence prediction [Kim et al., 2015], multi-timescale control
[Goel et al., 2017], and thermal management [Zanini et al., 2010].
Speciﬁcally, SOL is performed in a sequence of consecutive rounds, where at round t the learner is asked to select a point xt from the decision set X , and suffers a hitting cost ft(xt). Depending on the performance metric, the learner may be allowed to observe ft(·) when making decisions, which is different from the traditional online learning in which ft(·) is revealed to the learner after submitting the decision [Cesa-Bianchi and Lugosi, 2006]. Additionally, the learner also incurs a switching cost m(xt, xt−1) for changing decisions between successive rounds. The switching cost m(xt, xt−1) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
could be any distance function, such as the (cid:96)2-norm distance (cid:107)xt − xt−1(cid:107) and the squared (cid:96)2-norm distance (cid:107)xt − xt−1(cid:107)2/2 [Goel et al., 2019]. In the literature, there are two performance metrics for
SOL: competitive ratio and dynamic regret with switching cost.
Competitive ratio is popular in the community of online algorithms [Borodin and El-Yaniv, 1998].
It is deﬁned as the worst-case ratio of the total cost incurred by the online learner and the ofﬂine optimal cost: (cid:80)T t=1 (cid:0)ft(xt) + m(xt, xt−1)(cid:1) minu0,u1,...,uT ∈X (cid:80)T t=1 (cid:0)ft(ut) + m(ut, ut−1)(cid:1) . (1)
When focusing on the competitive ratio, the learner can observe ft(·) before picking xt. The problem is still nontrivial due to the coupling created by the switching cost. On the other hand, dynamic regret with switching cost is a generalization of dynamic regret—a popular performance metric in the community of online learning [Zinkevich, 2003]. It is deﬁned as the difference between the total cost incurred by the online learner and that of an arbitrary comparator sequence u0, u1, . . . , uT ∈ X :
T (cid:88) t=1 (cid:0)ft(xt) + m(xt, xt−1)(cid:1) −
T (cid:88) t=1 (cid:0)ft(ut) + m(ut, ut−1)(cid:1). (2)
Different from previous work [Chen et al., 2018, Goel et al., 2019], we did not introduce the minimization operation over u0, u1, . . . , uT in (2). The reason is that we want to bound (2) by certain regularities of the comparator sequence, such as the path-length
PT (u0, u1, . . . , uT ) =
T (cid:88) t=1 (cid:107)ut − ut−1(cid:107). (3)
When focusing on (2), ft(·) is generally hidden from the learner before submitting xt. The conditions for bounding the two metrics are very different, so we study competitive ratio and dynamic regret with switching cost separately. To bound the two metrics simultaneously, we refer to Andrew et al.
[2013] and Daniely and Mansour [2019], especially the meta-algorithm in the latter work.
This paper follows the line of research stemmed from online balanced descent (OBD) [Chen et al., 2018, Goel and Wierman, 2019]. The key idea of OBD is to ﬁnd an appropriate balance between the hitting cost and the switching cost through iterative projections. It has been shown that OBD and its variants are able to exploit the analytical properties of the hitting cost (e.g., polyhedral, strongly convex) to derive dimension-free competitive ratio. At this point, it would be natural to ask why not use the greedy algorithm, which minimizes the weighted sum of the hitting cost and the switching cost in each round, i.e., min x∈X ft(x) + γm(x, xt−1) (4) to balance the two costs, where γ ≥ 0 is the trade-off parameter. We note that the greedy algorithm is usually treated as the baseline in competitive analysis [Borodin and El-Yaniv, 1998], but its usage for smoothed online learning is quite limited. One result is given by Goel et al. [2019], who demonstrate that the greedy algorithm as a special case of Regularized OBD (R-OBD), is optimal for strongly convex functions. Besides, Lin et al. [2020] have analyzed the greedy algorithm with γ = 0, named as the naive approach below, for polyhedral functions and quadratic growth functions.
In this paper, we make the following contributions towards understanding the greedy algorithm.
• For α-polyhedral functions, the competitive ratio of the naive approach is max(1, 2
α ), which
α competitive ratio of OBD [Chen et al., 2018]
α ratio proved by Lin et al. [2020, Lemma 1]. When α > 2, the ratio becomes is a signiﬁcant improvement over the 3 + 8 and the 1 + 2 1, indicating that the naive approach is optimal in this scenario.
• For λ-quadratic growth functions, the competitive ratio of the naive algorithm is 1 + 4
λ , which matches the lower bound of this algorithm [Goel et al., 2019, Theorem 5], and is better than the max(1 + 6
λ , 4) ratio obtained by Lin et al. [2020, Lemma 1].
• If the hitting cost is both convex and λ-quadratic growth, the greedy algorithm with γ > 0 competitive ratio, which demonstrates the advantage of taking the switching ratio is on the same order as Greedy OBD [Goel et al., attains a 1 + 2√
λ cost into considerations. Our 1 + 2√
λ 2019, Theorem 3] but with much smaller constants. 2
• Our analysis of the naive approach and the greedy algorithm is very simple. In contrast, both OBD and Greedy OBD rely on intricate geometric arguments.
While both OBD and R-OBD are equipped with sublinear dynamic regret with switching cost, they are unsatisfactory in the following aspects:
• The regret of OBD depends on an upper bound of the path-length instead of the path-length itself [Chen et al., 2018, Corollary 11], making it nonadaptive.
• The regret of R-OBD is adaptive but it uses the squared (cid:96)2-norm to measure the switching cost, which may not be suitable for general convex functions [Goel et al., 2019].1
• Both OBD and R-OBD observe ft(·) before selecting xt, which violates the convention of online learning.
To avoid the above limitations, we demonstrate that a small change of Ader [Zhang et al., 2018a], which is an existing algorithm designed for dynamic regret, is sufﬁcient to minimize the dynamic regret with switching cost under the setting of online convex optimization [Shalev-Shwartz, 2011].
Ader runs multiple online gradient descent (OGD) [Zinkevich, 2003] with different step sizes as expert-algorithms, and uses Hedge [Freund and Schapire, 1997] as the meta-algorithm to aggre-gate predictions from experts. The only modiﬁcation is to incorporate the switching cost into the loss of Hedge. The proposed algorithm, named as Smoothed Ader (SAder), attains the optimal
O((cid:112)T (1 + PT )) dynamic regret, where PT is the path-length deﬁned in (3). Thus, our regret bound is adaptive because it automatically becomes small when the comparators change slowly. Finally, we also investigate the case that the hitting cost is available before predictions, and establish a similar result without the bounded gradient condition. To this end, we design a lookahead version of SAder, which chooses the greedy algorithm in (4) as the expert and utilizes the cost of the current round in Hedge. To show the optimality of this algorithm, we further establish an Ω((cid:112)T (1 + PT )) lower bound under the lookahead setting. 2