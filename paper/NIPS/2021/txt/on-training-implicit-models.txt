Abstract
This paper focuses on training implicit models of inﬁnite layers. Speciﬁcally, previous works employ implicit differentiation and solve the exact gradient for the backward propagation. However, is it necessary to compute such an exact but expensive gradient for training? In this work, we propose a novel gradient estimate for implicit models, named phantom gradient, that 1) forgoes the costly computation of the exact gradient; and 2) provides an update direction empirically preferable to the implicit model training. We theoretically analyze the condition under which an ascent direction of the loss landscape could be found, and provide two speciﬁc instantiations of the phantom gradient based on the damped unrolling and Neumann series. Experiments on large-scale tasks demonstrate that these lightweight phantom gradients signiﬁcantly accelerate the backward passes in training implicit models by roughly 1.7×, and even boost the performance over approaches based on the exact gradient on ImageNet. 1

Introduction
Conventional neural networks are typically constructed by explicitly stacking multiple linear and non-linear operators in a feed-forward manner. Recently, the implicitly-deﬁned models [1, 2, 3, 4, 5] have attracted increasing attentions and are able to match the state-of-the-art results by explicit models on several vision [3, 6], language [2] and graph [4] tasks. These works treat the evolution of the intermediate hidden states as a certain form of dynamics, such as ﬁxed-point equations
[2, 3] or ordinary differential equations (ODEs) [1, 7], which represents inﬁnite latent states. The forward passes of implicit models are therefore formulated as solving the underlying dynamics, by either black-box ODE solvers [1, 7] or root-ﬁnding algorithms [2, 3]. As for the backward passes, however, directly differentiating through the forward pass trajectories could induce a heavy memory overhead [8, 9]. To this end, researchers have developed memory-efﬁcient backpropagation via implicit differentiation, such as solving a Jacobian-based linear ﬁxed-point equation for the backward pass of deep equilibrium models (DEQs) [2], which eventually makes the backpropagation trajectories independent of the forward passes. This technique allows one to train implicit models with essentially constant memory consumption, as we only need to store the ﬁnal output and the layer itself without saving any intermediate states. However, in order to estimate the exact gradient by implicit differentiation, implicit models have to rely on expensive black-box solvers for backward passes, e.g., ODE solvers or root-solving algorithms. These black-box solver usually makes the gradient computation very costly in practice, even taking weeks to train state-of-the-art implicit models on ImageNet [10] with 8 GPUs.
This work investigates fast approximate gradients for training implicit models. We found that a ﬁrst-order oracle that produces good gradient estimates is enough to efﬁciently and effectively train implicit
*Equal contribution
†Corresponding author, zlin@pku.edu.cn 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
models, circumventing laboriously computing the exact gradient as in prior arts [2, 3, 4, 11, 12]. We develop a framework in which a balanced trade-off is made between the precision and conditioning of the gradient estimate. Speciﬁcally, we provide the general condition under which the phantom gradient can provide an ascent direction of the loss landscape. We further propose two instantiations of phantom gradients in the context of DEQ models, which are based on the the damped ﬁxed-point unrolling and the Neumann series, respectively. Importantly, we show that our proposed instantiations satisfy the theoretical condition, and that the stochastic gradient descent (SGD) algorithm based on the phantom gradient enjoys a sound convergence property as long as the relevant hyperparameters, e.g., the damping factor, are wisely selected. Note that our method only affects, and thus accelerates, the backward formulation of the implicit models, leaving the forward pass formulation (i.e., the root-solving process) and the inference behavior unchanged so that our method is applicable to a wide range of implicit models, forward solvers, and inference strategies. We conduct an extensive set of synthetic, ablation, and large-scale experiments to both analyze the theoretical properties of the phantom gradient and validate its speedup and performances on various tasks, such as ImageNet [10] classiﬁcation and Wikitext-103 [13] language modeling.
Overall, our results suggest that: 1) the phantom gradient estimates an ascent direction; 2) it is applicable to large-scale tasks and is capable of achieving a strong performance which is comparable with or even better than that of the exact gradient; and 3) it signiﬁcantly shortens the total training time needed for implicit models roughly by a factor of 1.4 ∼ 1.7×, and even accelerates the backward passes by astonishingly 12× on ImageNet. We believe that our results provide strong evidence for effectively training implicit models with the lightweight phantom gradient. 2 Method 2.1
Inspection of Implicit Differentiation
In this work, we primarily focus on the formulation of implicit models based on root-solving, represented by the DEQ models [2]. The table of notations is arranged in Appendix A. Speciﬁcally, given an equilibrium module F, the output of the implicit model is characterized by the solution h∗ to the following ﬁxed-point equation: h∗ = F(h∗, z), (1) where z ∈ Rdu+dθ is the union of the module’s input u ∈ Rdu and parameters θ ∈ Rdθ , i.e., z(cid:62) =
[u(cid:62), θ(cid:62)]. Here, u is usually a projection of the original data point x ∈ Rdx, e.g., u = M(x). In this section, we assume F is a contraction mapping w.r.t. h so that its Lipschitz constant Lh w.r.t. h is less than one, i.e., Lh < 1, a setting that has been analyzed in recent works [14, 15]1.
To differentiate through the ﬁxed point by Eq. (1), we need to calculate the gradient of h∗ w.r.t. the input z. By Implicit Function Theorem (IFT), we have
∂h∗
∂z
=
∂F
∂z (cid:12) (cid:12) (cid:12) (cid:12)h∗ (cid:18)
I −
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)h∗ (cid:19)−1
. (2)
Here, (∂a/∂b)ij = ∂aj/∂bi. The equilibrium point h∗ of Eq. (1) is then passed to a post-processing function G to obtain a prediction ˆy = G(h∗). In the generic learning scenario, the training objective is the following expected loss:
R(θ) = E(x,y)∼P [L( ˆy(x; θ), y)] , (3) where y is the groundtruth corresponding to the training example x, and P is the data distribution.
Here, we omit the parameters of G, because given the output h∗ of the implicit module F, training the post-processing part G is the same as training explicit neural networks. The most crucial component is the gradient of the loss function L w.r.t. the input vector z(cid:62) = [u(cid:62), θ(cid:62)], which is used to train both the implicit module F and the input projection module M. Using Eq. (2) with the condition h = h∗, we have
∂L
∂u
=
∂F
∂u (cid:18)
I −
∂F
∂h (cid:19)−1 ∂L
∂h
,
∂L
∂θ
=
∂F
∂θ (cid:18)
I −
∂F
∂h (cid:19)−1 ∂L
∂h
. (4)
The gradients in Eq. (4) are in the same form w.r.t. u and θ. Without loss of generality, we only discuss the gradient w.r.t. θ in the following sections. 1Note that the contraction condition could be largely relaxed to the one that the spectral radius of ∂F/∂h∗ on the given data is less than 1, i.e., ρ(∂F/∂h∗) < 1, as indicated by the well-posedness condition in [5]. 2
2.2 Motivation
The most intriguing part lies in the Jacobian-inverse term, i.e., (I − ∂F/∂h)−1. Computing the inverse term by brute force is intractable due to the O(n3) complexity. Previous implicit models
[2] approach this by solving a linear system involving a Jacobian-vector product iteratively via a gradient solver, introducing over 30 Broyden [16] iterations in the backward pass. However, the scale of the Jacobian matrix can exceed 106 × 106 in the real scenarios, leading to a prohibitive cost in computing the exact gradient. For example, training a small-scale state-of-the-art implicit model on ImageNet can consume weeks using 8 GPUs while training explicit models usually takes days, demonstrating that pursuing the exact gradient severely slows down the training process of implicit models compared with explicit models.
Secondly, because of the inversion operation, we cast doubt on the conditioning of the gradient and the stability of training process from the numerical aspect. The Jacobian-inverse can be numerically unstable when encountering the ill-conditioning issue. The conditioning problem might further undermine the training stability, as studied in the recent work [17].
Plus, the inexact gradient [9, 18, 19, 20, 21, 22] is widely applied in the previous learning protocol, like linear propagation [23] and synthetic gradient [24]. Here, the Jacobian-inverse is used to calculate the exact gradient which is not always optimal for model training. Moreover, previous research has used a moderate gradient noise as a regularization approach [25], which has been shown to play a central role in escaping poor local minima and improving generalization ability [26, 27, 28].
The concerns and observations motivate us to rethink the possibility of replacing the Jacobian-inverse term in the standard implicit differentiation with a cheaper and more stable counterpart. We believe that an exact gradient estimate is not always required, especially for a black-box layer like those in the implicit models. Hence this work designs an inexact, theoretically sound, and practically efﬁcient gradient for training implicit models under various settings. We name the proposed gradient estimate as the phantom gradient.
Suppose the Jacobian ∂h∗/∂θ is replaced with a matrix A, and the corresponding phantom gradient is deﬁned as (cid:100)∂L
∂θ
:= A
∂L
∂h
. (5)
Next, we give the general condition on A so that the phantom gradient can be guaranteed valid for optimization (Sec. 2.3), and provide two concrete instantiations of A based on either damped
ﬁxed-point unrolling or the Neumann series (Sec. 2.4). The proofs of all our theoretical results are presented in Appendix C. 2.3 General Condition on the Phantom Gradient
Previous research on theoretical properties for the inexact gradient include several aspects, such as the gradient direction [22], the unbiasedness of the estimator [29], and the convergence theory of the stochastic algorithm [19, 30]. The following theorem formulates a sufﬁcient condition that the phantom gradient gives an ascent direction of the loss landscape.
Theorem 1. Suppose the exact gradient and the phantom gradient are given by Eq. (4) and (5), respectively. Let σmax and σmin be the maximal and minimal singular value of ∂F/∂θ. If
∂F
∂h
∂F
∂θ
I − (6)
A
σ2 min
σmax (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
−
< (cid:18) (cid:19)
, then the phantom gradient provides an ascent direction of the function L, i.e., (cid:43) (cid:42) (cid:100)∂L
∂θ
,
∂L
∂θ
> 0. (7)
Remark 1. Suppose only the (I − ∂F/∂h)−1 term is replaced with a matrix D, namely, A = (∂F/∂θ) D. Then, the condition in (6) can be reduced into (cid:18) (cid:13) (cid:13) (cid:13) (cid:13)
D
I − (cid:19)
∂F
∂h
− I (cid:13) (cid:13) (cid:13) (cid:13)
< 1
κ2 , (8) where κ is the condition number of ∂F/∂θ. (See Appendix C.1 for the derivation.) 3
2.4
Instantiations of the Phantom Gradient
In this section, we present two practical instantiations of the phantom gradient. We also verify that the general condition in Theorem 1 can be satisﬁed if the hyperparameters in our instantiations are wisely selected.
Suppose we hope to differentiate through implicit dynamics, e.g., either a root-solving process or an optimization problem. In the context of hyperparameter optimization (HO), previous solutions include differentiating through the unrolled steps of the dynamics [19] or employing the Neumann series of the Jacobian-inverse term [9]. In our case, if we solve the root of Eq. (1) via the ﬁxed-point iteration: then by differentiating through the unrolled steps of Eq. (9), we have ht+1 = F(ht, z), t = 0, 1, · · · , T − 1,
∂hT
∂θ
=
T −1 (cid:88) t=0
∂F
∂θ (cid:12) (cid:12) (cid:12) (cid:12)ht
T −1 (cid:89) s=t+1
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)hs
.
Besides, the Neumann series of the Jacobian-inverse (I − ∂F/∂h)−1 is
I +
∂F
∂h
+ (cid:18) ∂F
∂h (cid:19)2 (cid:19)3
+ (cid:18) ∂F
∂h
+ · · · . (9) (10) (11)
Notably, computing the Jacobian ∂h∗/∂θ using the Neumann series in (11) is equivalent to differen-tiating through the unrolled steps of Eq. (9) at the exact equilibrium point h∗ and taking the limit of inﬁnite steps [9].
Without altering the root of Eq. (1), we consider a damped variant of the ﬁxed-point iteration: ht+1 = Fλ(ht, z) = λF(ht, z) + (1 − λ)ht, t = 0, 1, · · · , T − 1. (12)
Differentiating through the unrolled steps of Eq. (12), Eq. (10) is adapted as (cid:33) (cid:32)
∂hT
∂θ
= λ
T −1 (cid:88) t=0
∂F
∂θ (cid:12) (cid:12) (cid:12) (cid:12)ht
T −1 (cid:89) s=t+1
λ
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)hs
+ (1 − λ) I
. (13)
The Neumann series of (I − ∂F/∂h)−1 is correspondingly adapted as
λ (cid:0)I + B + B2 + B3 + · · · (cid:1) , where B = λ
∂F
∂h
+ (1 − λ)I. (14)
The next theorem shows that under mild conditions, the Jacobian from the damped unrolling in
Eq. (13) converges to the exact Jacobian and the Neumann series in (14) converges to the Jacobian-inverse (I − ∂F/∂h)−1 as well.
Theorem 2. Suppose the Jacobian ∂F/∂h is a contraction mapping. Then, (i) the Neumann series in (14) converges to the Jacobian-inverse (I − ∂F/∂h)−1; and (ii) if the function F is continuously differentiable w.r.t. both h and θ, the sequence in Eq. (13) converges to the exact Jacobian ∂h∗/∂θ as T → ∞, i.e., lim
T →∞
∂hT
∂θ
=
∂F
∂θ (cid:12) (cid:12) (cid:12) (cid:12)h∗ (cid:18)
I −
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)h∗ (cid:19)−1
. (15)
However, as discussed in Sec. 2.2, it is unnecessary to compute the exact gradient with inﬁnite terms. In the following context, we introduce two instantiations of the phantom gradient based on the
ﬁnite-term truncation of Eq. (13) or (14).
Unrolling-based Phantom Gradient (UPG).
In the unrolling form, the matrix A is deﬁned as
Aunr k,λ = λ k−1 (cid:88) t=0
∂F
∂θ (cid:12) (cid:12) (cid:12) (cid:12)ht (cid:32) k−1 (cid:89) s=t+1
λ
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)hs (cid:33)
+ (1 − λ) I
. (16) 4
Neumann-series-based Phantom Gradient (NPG). as
In the Neumann form, the matrix A is deﬁned
Aneu k,λ = λ
∂F
∂θ (cid:12) (cid:12) (cid:12) (cid:12)h∗ (cid:0)I + B + B2 + · · · + Bk−1(cid:1) , where B = λ
∂F
∂h (cid:12) (cid:12) (cid:12) (cid:12)h∗
+ (1 − λ)I. (17)
Note that both the initial point of the ﬁxed-point iteration (i.e., h0 in Eq. (16)) and the point at which the Neumann series is evaluated (i.e., h∗ in (17)) are the solution of the root-ﬁnding solver. (See
Appendix B for implementation of the phantom gradient.)
According to Theorem 2, the matrix A deﬁned by either Eq. (16) or (17) converges to the exact
Jacobian ∂h∗/∂θ as k → ∞ for any λ ∈ (0, 1]. Therefore, by Theorem 2, the condition in (6) can be satisﬁed if a sufﬁciently large step k is selected, since (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:19)−1(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (1 + Lh)
∂F
∂h
∂F
∂h
∂F
∂θ
∂F
∂θ
A − (18)
I −
I − (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
A
− (cid:19) (cid:18) (cid:18)
.
Next, we characterize the impact of the two hyperparameters, i.e., k and λ, on the precision and conditioning of A. Take the NPG (Eq. (17)) as an example. (i) On the precision of the phantom gradient,
• a large k makes the gradient estimate more accurate, as higher-order terms of the
Neumann series are included, while
• a small λ slows down the convergence of the Neumann series because the norm (cid:107)B(cid:107) increases as λ decreases. (ii) On the conditioning of the phantom gradient,
• a large k impairs the conditioning of A since the condition number of Bk grows exponentially as k increases, while
• a small λ helps maintain a small condition number of A because the singular values of
∂F/∂h are “smoothed” by the identity matrix.
In a word, a large k is preferable for a more accurate A, while a small λ contributes to the well-conditioning of A. Practically, these hyperparameters should be selected in consideration of a balanced trade-off between the precision and conditioning of A. See Sec. 3 for experimental results. 2.5 Convergence Theory
In this section, we provide the convergence guarantee of the SGD algorithm using the phantom gradient. We prove that under mild conditions, if the approximation error of the phantom gradient is sufﬁciently small, the SGD algorithm converges to an (cid:15)-approximate stationary point in the expectation sense. We will discuss the feasibility of our assumptions in Appendix C.3.
Theorem 3. Suppose the loss function R in Eq. (3) is (cid:96)-smooth, lower-bounded, and has bounded gradient almost surely in the training process. Besides, assume the gradient in Eq. (4) is an unbiased estimator of ∇R(θ) with a bounded covariance. If the phantom gradient in Eq. (5) is an (cid:15)-approximation to the gradient in Eq. (4), i.e., (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) almost surely, (cid:100)∂L
∂θ
∂L
∂θ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤ (cid:15), (19)
− then using Eq. (5) as a stochastic ﬁrst-order oracle with a step size of ηn = O(1/ with gradient descent, it follows after N iterations that (cid:34) (cid:80)N n=1 ηn(cid:107)∇R(θn)(cid:107)2 n=1 ηn
Remark 2. Consider the condition in (19): (cid:80)N
E (cid:35) (cid:18)
≤ O (cid:15) + (cid:19)
. log N
√
N (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:100)∂L
∂θ
−
∂L
∂θ (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
≤
A − (cid:18)
I −
∂F
∂θ
∂F
∂h (cid:19)−1(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
∂L
∂h (cid:13) (cid:13) (cid:13) (cid:13)
.
√ n) to update θ (20) (21)
Suppose the gradient ∂L/∂h is almost surely bounded. By Theorem 2, the condition in (19) can be guaranteed as long as a sufﬁciently large k is selected. 5
λ = 0.5
λ = 1.0
λ = 0.5
λ = 1.0 (a) Neumann-series-based phantom gradient. (b) Unrolling-based phantom gradient.
Figure 1: Cosine similarity between the phantom gradient and the exact gradient in the synthetic setting. 3 Experiments
In this section, we aim to answer the following questions via empirical results: (1) How precise is the phantom gradient? (2) What is the difference between the unrolling-based and the Neumann-series-based phantom gradients? (3) How is the phantom gradient inﬂuenced by the hyperparameters k and λ? (4) How about the computational cost of the phantom gradient compared with implicit differentiation? (5) Can the phantom gradient work at large-scale settings for various tasks?
We have provided some theoretical analysis and intuitions to (1), (2), and (3) in Sec. 2.4. Now we answer (1) and (2) and demonstrate the performance curves under different hyperparameters k and λ on CIFAR-10 [31]. Besides, we also study other factors that have potential inﬂuences on the training process of the state-of-the-art implicit models [2, 3] like pretraining. For (4) and (5), we conduct experiments on large-scale datasets to highlight the ultra-fast speed and competitive performances, including image classiﬁcation on ImageNet [10] and language modeling on Wikitext-103 [13].
We start by introducing two experiment settings. We adopt a single-layer neural network with spectral normalization [32] as the function F and ﬁxed-point iterations as the equilibrium solver, which is the synthetic setting. Moreover, on the CIFAR-10 dataset, we use the MDEQ-Tiny model [3] (170K parameters) as the backbone model, denoted as the ablation setting. Additional implementation details and experimental results are presented in Appendix D2.
Precision of the Phantom Gradient. The precision of the phantom gradient is measured by its angle against the exact gradient, indicated by the cosine similarity between the two. We discuss its precision in both the synthetic setting and the ablation setting. The former is under the static and randomly generated weights, while the latter provides characterization of the training dynamics.
In the synthetic setting, the function F is restricted to be a contraction mapping. Speciﬁcally, we directly set the Lipschitz constant of F as Lh = 0.9, and use 100 ﬁxed-point iterations to solve the root h∗ of Eq. (1) until the relative error satisﬁes (cid:107)h − F(h, z)(cid:107)/(cid:107)h(cid:107) < 10−5. Here, the exact gradient is estimated by backpropagation through the ﬁxed-point iterations, and cross-validated by implicit differentiation solved with 20 iterations of the Broyden’s method [16]. In our experiment, the cosine similarity between these two gradient estimates consistently succeeds 0.9999, indicating the gradient estimate is quite accurate when the relative error of forward solver is minor. The cosine similarity between phantom gradients and exact gradients is shown in Fig. 1. It shows that the cosine similarity tends to increase as k grows and that a small λ tends to slow down the convergence of the phantom gradient, allowing it to explore in a wider range regarding the angle against the exact gradient.
In the ablation setting, the precision of the phantom gradient during the training process is shown in
Fig. 2. The model is trained by implicit differentiation under the ofﬁcial schedule3. It shows that the phantom gradient still provides an ascent direction in the real training process, as indicated by the considerable cosine similarity against the exact gradient. Interestingly, the cosine similarity slightly decays as the training progresses, which implies a possibility to construct an adaptive gradient solver for implicit models. 2All training sources of this work are available at https://github.com/Gsunshine/phantom_grad. 3Code available at https://github.com/locuslab/mdeq. 6
(a) Neumann-series-based phantom gradient. (b) Unrolling-based phantom gradient.
Figure 2: Cosine similarity between the phantom gradient and the exact gradient in the real scenario.
The horizontal axis corresponds to the cosine similarity, and the vertical axis to the training step.
To Pretrain, or not to Pretrain? To better understand the components of the implicit models’ training schedule, we ﬁrst illustrate a detailed ablation study of the baseline model in the ablation setting. The average accuracy with standard deviation is reported in Tab. 1.
The MDEQ model employs a pretraining stage in which the model F is unrolled as a recurrent network. We study the impact of the pretrain-ing stage, the Dropout [33] operation, and the
It can be seen that the optimizer separately. unrolled pretraining stabilizes the training of the MDEQ model. Removing the pretraining stage leads to a severe performance drop and apparent training instability among different tri-als because the solver cannot obtain an accurate
ﬁxed point h∗ when the model is not adequately trained. This ablation study also suggests that the MDEQ model is a strong baseline for our method to compare with.
Table 1: Ablation settings on CIFAR-10.
Method
Implicit Differentiation w/o Pretraining w/o Dropout
Adam → SGD
SGD w/o Pretraining
UPG (A5,0.5, w/o Dropout)
NPG (A5,0.5, w/o Dropout)
UPG (A9,0.5, w/ Dropout)
Acc. (%) 85.0 ± 0.2 82.3 ± 1.3 83.7 ± 0.1 84.5 ± 0.3 82.9 ± 1.5 85.8 ± 0.5 85.6 ± 0.5 86.1 ± 0.5
However, pretraining is not always indispensable for training implicit models. It introduces an extra hyperparameter, i.e., how many steps should be involved in the pretraining stage. Next, we discuss how the UPG could circumvent this issue.
Trade-offs between Unrolling and Neumann. For an exact ﬁxed point h∗, i.e., h∗ = F(h∗, z), there is no difference between UPG and NPG. However, when the numerical error exists in solving h∗, i.e., (cid:107)h∗ − F(h∗, z)(cid:107) > 0, these two instatiations of the phantom gradient can behave differently.
Complexity comparison. Mem
Table 2: means the memory cost, and K and k denote the solver’s steps and the unrolling/Neumann steps, respectively. Here, K (cid:29) k ≈ 1.
We note that a particular beneﬁt of the UPG is its ability to automatically switch between the pretrain-ing and training stages for implicit models. When the model is not sufﬁciently trained and the solver converges poorly (see [3]), the UPG deﬁnes a for-ward computation graph that is essentially equiva-lent to a shallow weight-tied network to reﬁne the coarse equilibrium states. In this stage, the phan-tom gradient serves as a backpropagation through time (BPTT) algorithm and hence behaves as in the pretraining stage. Then, as training progresses, the solver becomes more stable and converges to the ﬁxed point h∗ better. This makes the UPG behave more like the NPG. Therefore, the unrolled pretraining is gradually transited into the regular training phase based on implicit differentiation, and the hyperparameter tuning of pretraining steps can be waived. We argue that such an ability to adaptively switch training stages is benign to the implicit models’ training protocol, which is also supported by the performance gain in Tab. 1.
Method
Implicit O(K) O(1)
O(k)
UPG
O(1)
NPG
Time Mem Peak Mem
O(k)
O(k)
O(1)
O(k)
O(k)
Although the UPG requires higher memory overhead than implicit differentiation or the NPG, it does not surpass the peak memory usage in the entire training protocol by implicit differentiation 7
Figure 3: Ablation studies on (a) the hyperparameters λ and k, and (b) two forms of phantom gradient. (a) Impact of λ and k (b) NPG v.s. UPG due to the pretraining stage. In the ablation setting, the MDEQ model employs a 10-layer unrolling for pretraining, which actually consumes double memory compared with a 5-step unrolling scheme, e.g., A5,0.5 in Tab. 1. In Tab. 2, we also demonstrate the time and memory complexity for implicit differentiation and the two forms of phantom gradient.
In addition, leaving the context of approximate and exact gradient aside, we also develop insights into understanding the subtle differences between UPG and NPG in terms of the state-dependent and state-free gradient. Actually, the UPG is state-dependent, which means it corresponds to the
“exact” gradient of a computational sub-graph. Both the NPG and the gradient solved by implicit differentiation, however, do not exactly match the gradient of any forward computation graph unless the numerical error is entirely eliminated in both the forward and the backward passes for implicit differentiation or the one-step gradient [34, 21, 30, 22] is used for the NPG, i.e., k = 1. Interestingly, we observe that models trained on the state-dependent gradient demonstrate an additional training stability regarding the Jacobian spectral radius, compared with those trained on the state-free counterpart. We empirically note that it can be seen as certain form of implicit Jacobian regularization for implicit models as a supplement to the explicit counterpart [17], indicated by the stable estimated Jacobian spectral radius during training, i.e., ρ(∂Fλ/∂h) ≈ 1.
The experimental results also cohere with our insight. The performance curves in Fig. 3 demonstrate the inﬂuence of λ and k and further validate that the UPG is more robust to a wide range of steps k than the NPG. In fact, when the Jacobian spectral radius ρ(∂F/∂h) increases freely without proper regularization, the exploding high-order terms in the NPG could exert a negative impact on the overall direction of the phantom gradient, leading to performance degradation when a large k is selected (see
Fig. 3(b)). We also observe a surging of the estimated Jacobian spectral radius as well as the gradient explosion issue in the NPG experiments. On the contrary, the UPG can circumvent these problems thanks to its implicit Jacobian regularization.
Table 3: Experiments using DEQ [2] and MDEQ [3] on vision and language tasks. Metrics stand for accuracy(%)↑ for image classiﬁcation on CIFAR-10 and ImageNet, and perplexity↓ for language modeling on Wikitext-103. JR stands for Jacobian Regularization [17]. † indicates additional steps in the forward equilibrium solver.
Datasets
CIFAR-10
CIFAR-10
ImageNet
ImageNet
Model
MDEQ
MDEQ
MDEQ
MDEQ
Wikitext-103 DEQ (PostLN)
Wikitext-103 DEQ (PostLN) UPG A5,0.8
JR + Implicit
Wikitext-103 DEQ (PreLN)
JR + UPG A5,0.8
Wikitext-103 DEQ (PreLN)
JR + UPG A5,0.8
Wikitext-103 DEQ (PreLN)
Method
Implicit
UPG A5,0.5
Implicit
UPG A5,0.6
Implicit 8
Params 10M 10M 18M 18M
Metrics 93.8 ± 0.17 95.0 ± 0.16 75.3 75.7 98M 98M 98M 98M 98M 24.0 25.7 24.5 24.4 24.0†
Speed 1.0× 1.4× 1.0× 1.7× 1.0× 1.7× 1.7× 2.2× 1.7×
Phantom Gradient at Scale. We conduct large-scale experiments to verify the advantages of the phantom gradient on vision, graph, and language benchmarks. We adopt the UPG in the large-scale experiments. The results are illustrated in Tab. 3 and Tab. 4. Our method matches or surpasses the implicit differentiation training protocol on the state-of-the-art implicit models with a visible reduction on the training time. When only considering the backward pass, the acceleration for MDEQ can be remarkably 12× on ImageNet classiﬁcation.
Table 4: Experiments using IGNN [4] on graph tasks. Metrics stand for accuracy(%)↑ for graph classiﬁcation on COX2 and PROTEINS, Micro-F1(%)↑ for node classiﬁcation on PPI.
Datasets
COX2
COX2
COX2
COX2
PROTEINS
PROTEINS
PROTEINS
PROTEINS
PPI
PPI
PPI
PPI
Model Method
IGNN Implicit
IGNN UPG A5,0.5
IGNN UPG A5,0.8
IGNN UPG A5,1.0
IGNN Implicit
IGNN UPG A5,0.5
IGNN UPG A5,0.8
IGNN UPG A5,1.0
IGNN Implicit
IGNN UPG A5,0.5
IGNN UPG A5,0.8
IGNN UPG A5,1.0
Params Metrics (%) 84.1 ± 2.9 83.9 ± 3.0 83.9 ± 2.7 83.0 ± 2.9 78.6 ± 4.1 78.4 ± 4.2 78.6 ± 4.2 78.8 ± 4.2 38K 38K 38K 38K 34K 34K 34K 34K 4.7M 97.6 4.7M 98.2 4.7M 97.4 4.7M 96.2 4