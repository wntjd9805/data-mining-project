Abstract
Learning from datasets without interaction with environments (Ofﬂine Learning) is an essential step to apply Reinforcement Learning (RL) algorithms in real-world scenarios. However, compared with the single-agent counterpart, ofﬂine multi-agent RL introduces more agents with the larger state and action space, which is more challenging but attracts little attention. We demonstrate current ofﬂine RL algorithms are ineffective in multi-agent systems due to the accumulated extrapola-tion error. In this paper, we propose a novel ofﬂine RL algorithm, named Implicit
Constraint Q-learning (ICQ), which effectively alleviates the extrapolation error by only trusting the state-action pairs given in the dataset for value estimation.
Moreover, we extend ICQ to multi-agent tasks by decomposing the joint-policy under the implicit constraint. Experimental results demonstrate that the extrapo-lation error is successfully controlled within a reasonable range and insensitive to the number of agents. We further show that ICQ achieves the state-of-the-art performance in the challenging multi-agent ofﬂine tasks (StarCraft II). Our code is public online at https://github.com/YiqinYang/ICQ. 1

Introduction
Recently, reinforcement learning (RL), an active learning process, has achieved massive success in various domains ranging from strategy games [59] to recommendation systems [8]. However, applying RL to real-world scenarios poses practical challenges: interaction with the real world, such as autonomous driving, is usually expensive or risky. To solve these issues, ofﬂine RL is an excellent choice to deal with practical problems [3, 24, 35, 42, 15, 28, 4, 23, 54, 12], aiming at learning from a
ﬁxed dataset without interaction with environments.
The greatest obstacle of ofﬂine RL is the distribution shift issue [16], which leads to extrapolation error, a phenomenon in which unseen state-action pairs are erroneously estimated. Unlike the online setting, the inaccurate estimated values of unseen pairs cannot be corrected by interacting with the environment. Therefore, most off-policy RL algorithms fail in the ofﬂine tasks due to intractable over-generalization. Modern ofﬂine methods (e.g., Batch-Constrained deep Q-learning (BCQ) [16]) aim to enforce the learned policy to be close to the behavior policy or suppress the Q-value directly. These methods have achieved massive success in challenging single-agent ofﬂine tasks like D4RL [14].
However, many decision processes in real-world scenarios belong to multi-agent systems, such as intelligent transportation systems [2], sensor networks [37], and power grids [7]. Compared with the single-agent counterpart, the multi-agent system has a much larger action space, which grows
†Equal Contribution.
‡Equal Corresponding. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) BCQ. (b) ICQ.
Figure 1: The comparison between ICQ and BCQ for the target Q-value estimation. The spots denote states, and the connections between spots indicate actions. The red solid-lines denote seen pairs, and the gray dotted-lines are unseen pairs. (a) BCQ estimates Q-value in a deﬁned similar action set (orange) while unseen pairs still exist in the set with low probability. (b) ICQ only adopts seen pairs (orange) in the training set for Q-value estimation. exponentially with the increasing of the agent number. When coming into the ofﬂine scenario, the unseen state-action pairs will grow exponentially as the number of agents increases, accumulating the extrapolation error quickly. The current ofﬂine algorithms are unsuccessful in multi-agent tasks even though they adopt the modern value-decomposition structure [26, 48, 25]. As shown in Figure 2, our results indicate that BCQ, a state-of-the-art ofﬂine algorithm, has divergent Q-estimates in a simple multi-agent MDP environment (e.g., BCQ (4 agents)). The extrapolation error for value estimation is accumulated quickly as the number of agents increases, signiﬁcantly impairing the performance.
Based on these analyses, we propose the Implicit Constraint Q-learning (ICQ) algorithm, which effectively alleviates the extrapolation error as no unseen pairs are involved in estimating Q-value.
Motivated by an implicit constraint optimization problem, ICQ adopts a SARSA-like approach [49] to evaluate Q-values and then converts the policy learning into a supervised regression problem. By decomposing the joint-policy under the implicit constraint, we extend ICQ to the multi-agent tasks successfully. To the best of our knowledge, our work is the ﬁrst study analyzing and addressing the extrapolation error in multi-agent reinforcement learning.
We evaluate our algorithm on the challenging multi-agent ofﬂine tasks based on StarCraft II [40], where a large number of agents cooperatively complete a task. Experimental results show that ICQ can control the extrapolation error within a reasonable range under any number of agents and learn from complex multi-agent datasets. Further, we evaluate the single-agent version of ICQ in D4RL, a standard single-agent ofﬂine benchmark. The results demonstrate the generality of ICQ for a wide range of task scenarios, from single-agent to multi-agent, from discrete to continuous control. 2