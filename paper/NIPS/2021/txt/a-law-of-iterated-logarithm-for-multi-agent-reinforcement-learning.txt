Abstract
In Multi-Agent Reinforcement Learning (MARL), multiple agents interact with a common environment, as also with each other, for solving a shared problem in sequential decision-making. It has wide-ranging applications in gaming, robotics, finance, etc. In this work, we derive a novel law of iterated logarithm for a family of distributed nonlinear stochastic approximation schemes that is useful in MARL.
In particular, our result describes the convergence rate on almost every sample path where the algorithm converges. This result is the first of its kind in the distributed setup and provides deeper insights than the existing ones, which only discuss con-vergence rates in the expected or the CLT sense. Importantly, our result holds under significantly weaker assumptions: neither the gossip matrix needs to be doubly stochastic nor the stepsizes square summable. As an application, we show that, for the stepsize n−γ with γ ∈ (0, 1), the distributed TD(0) algorithm with n−γ ln n) a.s.; for linear function approximation has a convergence rate of O( n−1 ln ln n) a.s. These decay rates do not the 1/n type stepsize, the same is O( depend on the graph depicting the interactions among the different agents.
√
√ 1

Introduction
Can a machine train itself in the same way an infant learns to sit up, crawl, and walk? That is, can a device interact with the environment and figure out the action sequence required to complete a given task? The study of algorithms that enable such decision-making is what the field of Reinforcement
Learning (RL) is all about [40]. In contrast, the mathematics needed to analyze such schemes is what forms the focus in Stochastic Approximation (SA) theory [2, 4]. More generally, SA refers to an iterative scheme that helps find zeroes or optimal points of a function, for which only noisy evaluations are possible. In this work, we analyze a family of Distributed Stochastic Approximation (DSA) algorithms [24] that is useful in Multi-Agent Reinforcement Learning (MARL) [22, 51].
In the MARL framework, we have multiple agents or learners that continually engage with a shared environment: the agents pick local actions, and the environment responds by transitioning to a new state and giving each agent a different local reward. Additionally, the agents also gossip about local computations with each other. The goal of the agents is to cooperatively find action policies that maximize the collective rewards obtained over time. Algorithms useful in this endeavor have found empirical success in domains as diverse as gaming [28], robotics [29], autonomous driving
[35], communication networks [21], power grids [34], and economics [19]. However, theoretical analyses of such schemes are still very minimal, and this is what this paper aims to address.
For the purpose of analysis, MARL methods are often viewed as special cases of DSA algorithms.
The archetypical form of a DSA scheme with m distributed nodes can be described as follows. Let G 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
be a directed graph representing the connections between these nodes, and W ≡ (Wij) ∈ [0, 1]m×m a matrix whose ij-th entry denotes the strength of the edge j → i. It is assumed that W is compatible with G, i.e., Wij > 0 only if j → i ∈ G. Then, at agent i, the above scheme (written as row vectors) has the update rule xn+1(i) = (cid:88) j∈Ni
Wijxn(j) + αn[hi(xn) + Mn+1(i)], n ≥ 0, (1) where xn ∈ Rm×d is the joint estimate of the solution at time n, its j-th row, i.e., xn(j) denotes1 the estimate obtained at agent j, Ni represents the set of in-neighbors of node i in G, αn is the stepsize, hi : Rm×n → Rd is the driving function at agent i, and Mn+1(i) ∈ Rd is the noise in its evaluation at time n. This update rule has two parts: a weighted average of the estimates obtained by gossip and a refinement based on local computations. Clearly, the joint update rule of all the agents is xn+1 = W xn + αn[h(xn) + Mn+1], (2) where Mn+1 is the m × d matrix whose i-th row is Mn+1(i), and h is the function that maps x ∈ Rm×d to the m × d matrix whose i-th row is hi(x).
Two important points about the above framework are as follows: i.) we allow hi to be a function of all of xn and not just of xn(i), as is commonly assumed, and ii.) the computations at different nodes in the above setup run synchronously on a common clock.