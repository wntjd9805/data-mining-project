Abstract
Checklists are simple decision aids that are often used to promote safety and reliability in clinical applications. In this paper, we present a method to learn checklists for clinical decision support. We represent predictive checklists as discrete linear classiﬁers with binary features and unit weights. We then learn globally optimal predictive checklists from data by solving an integer programming problem. Our method allows users to customize checklists to obey complex constraints, including constraints to enforce group fairness and to binarize real-valued features at training time. In addition, it pairs models with an optimality gap that can inform model development and determine the feasibility of learning sufﬁciently accurate checklists on a given dataset. We pair our method with specialized techniques that speed up its ability to train a predictive checklist that performs well and has a small optimality gap. We benchmark the performance of our method on seven clinical classiﬁcation problems, and demonstrate its practical beneﬁts by training a short-form checklist for PTSD screening. Our results show that our method can ﬁt simple predictive checklists that perform well and that can easily be customized to obey a rich class of custom constraints. 1

Introduction
Checklists are simple tools that are widely used to assist humans when carrying out important tasks or making important decisions [8, 13, 18, 35, 44, 50, 60, 61, 64, 65, 66]. These tools are often used as predictive models in modern healthcare applications. In such settings, a checklist is a set of Boolean conditions that predicts a condition of interest – e.g., a list of symptoms ﬂag a patient for a critical illness when M out N symptoms are checked. These kinds of “predictive checklists” are often used for clinical decision support because they are easy to use and easy to understand [33, 54]. In contrast to other kinds of predictive models, clinicians can easily scrutinize a checklist, and make an informed decision as to whether they will adopt it. Once they have decided to use a checklist, they can integrate the model into their clinical workﬂow without extensive training or technology (e.g., as a printed sheet [52]).
Considering these beneﬁts, one of the key challenges in using predictive checklists in healthcare applications is ﬁnding a reliable way to create them [33]. Most predictive checklists in medicine are either hand-crafted by panels of experts [30, 41], or built by combining statistical techniques and heuristics [e.g., logistic regression, stepwise feature selection, and rounding 39]. These approaches
* Equal supervision. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
make it difﬁcult to develop checklists that are sufﬁciently accurate – as panel or pipeline will effectively need to specify a model that performs well under stringent assumptions on model form.
Given the simplicity of the model class, it is entirely possible that some datasets may never admit a checklist that is sufﬁciently accurate to deploy in a clinical setting – as even checklists that are accurate at a population-level may perform poorly on a minority population [57, 70].
In this paper, we introduce a machine learning method to learn checklists from data. Our method is designed to streamline the creation of predictive checklists in a way that overcomes speciﬁc challenges of model development in modern healthcare applications. Our method solves an integer programming problem to return the most accurate checklist that obeys user-speciﬁed constraints on model form and/or model performance. This approach is computationally challenging, but provides speciﬁc functionality that simpliﬁes and streamlines model development. First, it learns the most accurate checklist by optimizing exact measures of model performance (i.e., accuracy rather than a convex surrogate measure). Second, it seeks to improve the performance of checklists by adaptively binarizing features at training time. Third, it allows practitioners to train checklists that obey custom requirements on model form or on prediction, by allowing them to encode these requirements as constraints in the optimization problem. Finally, it provides practitioners with an optimality gap, which informs them when a sufﬁciently accurate checklist does not exist.
The main contributions of this paper are: 1. We present a machine learning method to learn checklists from data. Our method allows practi-tioners to customize models to obey a wide range of real-world constraints. In addition, it pairs checklists with an optimality gap that can inform practitioners in model development. 2. We develop specialized techniques that improve the ability of our approach to train a checklist that performs well, and to pair this model with a small optimally gap. One of these techniques can be used to train checklists heuristically. 3. We conduct a broad empirical study of predictive checklists on clinical classiﬁcation datasets [24, 32, 37, 51, 58]. Our results show that predictive checklists can perform as well as state-of-the-art classiﬁers on some datasets, and that our method can provide practitioners with an optimality gap to ﬂag when this is not the case. We highlight the ability of our method to handle real-world requirements through applications where we enforce group fairness constraints on a mortality prediction task, and where we build a short-form checklist to screen for PTSD. 4. We provide a Python package to train and customize predictive checklists with open-source and commercial solvers, including CBC [29] and CPLEX [20] (see https://github.com/
MLforHealth/predictive_checklists). 2