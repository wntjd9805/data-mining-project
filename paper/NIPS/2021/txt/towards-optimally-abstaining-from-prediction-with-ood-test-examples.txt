Abstract
A common challenge across all areas of machine learning is that training data is not distributed like test data, due to natural shifts, “blind spots,” or adversarial examples; such test examples are referred to as out-of-distribution (OOD) test examples. We consider a model where one may abstain from predicting, at a ﬁxed cost. In particular, our transductive abstention algorithm takes labeled training examples and unlabeled test examples as input, and provides predictions with optimal prediction loss guarantees. The loss bounds match standard generalization bounds when test examples are i.i.d. from the training distribution, but add an additional term that is the cost of abstaining times the statistical distance between the train and test distribution (or the fraction of adversarial examples). For linear regression, we give a polynomial-time algorithm based on Celis-Dennis-Tapia optimization algorithms. For binary classiﬁcation, we show how to efﬁciently implement it using a proper agnostic learner (i.e., an Empirical Risk Minimizer) for the class of interest. Our work builds on a recent abstention algorithm of
Goldwasser, Kalais, and Montasser [10] for transductive binary classiﬁcation. 1

Introduction
For learning of a class of functions F of bounded complexity, statistical learning theory guarantees low error if test examples are distributed like training examples. Thus abstention is not necessary for standard realizable prediction. However, when the test distribution Q is not the same as the distribution of training examples P , abstaining from prediction may be beneﬁcial if the cost of abstaining α is substantially less than the cost of an error. This is particularly important when there are “blind spots” where Q(x) > P (x) = 0. Such blind spots may occur because of natural distribution shifts, or indeed may be due to adversarial attacks. Collectively, test examples that deviate from the training distribution are referred to as out of distribution (OOD) test examples. Such an extreme covariate shift scenario was analyzed for binary classiﬁcation in recent work by Goldwasser,
Kalais, and Montasser [10] (henceforth GKKM).
Abstaining from predicting may be useful because it is well-known to be impossible to guarantee accuracy on test examples from arbitrary Q (cid:54)= P (abstention is unnecessary under the common assumption that Q(x)/P (x) is upper-bounded) [7]. Whether one is classifying images or predicting the probability of success of a medical procedure, training data may miss important regions of test examples. As an example, Fang et al. [9] observed noticeable signs of COVID-19 in many lung scans; any model trained on data pre-2019 would not have any such instances in its training dataset. In the case of image classiﬁcation, a classiﬁer may be trained on publicly available data but may be also used on people’s private images or even adversarial spam/phishing images [21]. In settings such as medical diagnoses or content moderation, abstention may be much less costly than a misclassiﬁcation.
In practice, if a model abstains from making a prediction, it could be followed up by a more elaborate (and costly) model or direct human intervention. The extra costs of human intervention, or as may be the case in a medical setting, a more expensive test, needs to be traded off with the potential costs of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
misclassiﬁcation. For a regression example, a model may be used to predict the success probability of a medical treatment across a population, yet examples from people that are considered high-risk for the treatment may be absent from the training data.
We begin by discussing binary classiﬁcation, and then move to regression. In particular, for any distribution P over examples x ∈ X and any true classiﬁer f : X → {0, 1} in F of VC-dimension d, the so-called Fundamental Theorem of Statistical Learning (FTSL) guarantees w.h.p. ˜O(d/n) error rate1 on future examples from P using any classiﬁer h ∈ F that agrees with f on n noiseless labeled examples [see, e.g., 20].
In Chow’s original abstention model [6], a selective classiﬁer is allowed to either make a prediction ˆy at a loss of (cid:96)(y, ˆy) ≥ 0 or abstain from predicting at a ﬁxed small loss α > 0. Following GKKM, we consider a transductive abstention algorithm that takes as input n unlabeled test examples and n labeled training examples and predicts on a subset of the n test labels, abstaining on the rest. The goal is to minimize the average loss on the test set. (Unfortunately, the natural idea to abstain on test points whose labels are not uniquely determined by the training data can lead to abstaining on all test points even when P = Q.) GKKM give a algorithm with guarantees that naturally extend the
FTSL to Q (cid:54)= P albeit at an additional cost. The word transductive refers to the prediction model where one wishes to classify a given test set rather a standard classiﬁer that generalizes to future examples, though the two models are in fact equivalent in terms of expected error as we discuss. The term covariate shift is appropriate here as it describes settings in which Q (cid:54)= P but both train and test labels are consistent with the same f ∈ F . Without abstention, both transductive learning and covariate shift have been extensively studied [see,e.g., 1].
The principle behind our approach is illustrated through an example of Figure 1. Suppose P is the distribution Q restricted to a set S ⊂ X which contains, say, 90% of the test set, so that 10% of the unlabeled test examples are in blind spots. Say we have learned a standard classiﬁer h ∈ F from the n labeled training examples. Hypothetically, if we knew S, then we could predict h(xi) for test xi ∈ S and abstain from predicting on xi /∈ S. If abstaining costs α, then the FTSL would naturally guarantee test loss ≤ 0.1α + ˜O(d/n), because one abstains on 10% of the test examples and the remaining test examples are distributed just like P . The difﬁculty is that S may be too complex to learn. To circumvent this, we suggest a conceptually simple but theoretically powerful approach: choose the set of points to abstain on so as to minimize the worst-case loss over any true function f ∈ F that is also consistent with the training data. In particular, given a predictor h and a set of test points not abstained on, (ignoring efﬁciency) one could compute the worst-case loss over all f ∈ F that are consistent with the labeled training examples. This approach achieves optimal worst-case guarantees and, in particular inherits the natural loss guarantee one attains from abstaining outside of
S discussed above. Converting this theoretical insight into an efﬁcient algorithm is the focus of this paper, and different algorithms are needed for the case of classiﬁcation and regression.
In the case of known P, Q, one may say there is a known unknown: the region
Interpretation. of large Q(x)/P (x). In our model, however, even though this region is an unknown unknown, we achieve essentially the same bounds as if we knew P and Q. Thus, perhaps surprisingly, there is little additional loss for not knowing P and Q. GKKM give related guarantees, but which suffer from not knowing P, Q. In particular, even when P = Q their guarantees are ˜O((cid:112)d/n) compared to the ˜O(d/n) of FTSL, and they give a lower-bound showing Ω((cid:112)d/n) is inherent in their “PQ” learning model. In the PQ model, the error rate on Q (mistakes that are not abstained on) and the abstention rate on future examples from P are separately bounded. In Appendix C, we show that the
Chow model is stronger than PQ-learning in the sense that our guarantees imply PQ-bounds similar to theirs, but the reverse does not hold. Hence, the algorithms and guarantees in this paper extend the
FTSL in both the Chow and PQ models. We also note that abstaining on predictions can either help reduce inaccuracies on marginal groups or be a source of unfairness towards a person or group of people, and it should not serve as an excuse not to collect representative data. 1.1 Classiﬁcation results
For classiﬁcation, suppose Y = {0, 1}, (cid:96)(y, ˆy) := |y − ˆy| is the 0-1 loss (more general losses are con-sidered in the body), and F is a family of functions of VC-dimension d. There is also a deterministic 1The ˜O notation hides logarithmic factors. 2
Figure 1: A simple illustration of our approach when the training distribution P is the test distribution
Q restricted to a (blue) set S. We use classiﬁer h ﬁt on the labeled training data. We choose the subset of test examples to abstain on so as to minimize the worst-case loss over possible f ’s consistent with the training data. This gives a better worst-case loss than if we knew S and did the obvious thing of abstaining on all test x /∈ S. i (cid:96)(yi, g(xi))
Empirical Risk Minimization (ERM) oracle that computes ERM(x, y) ∈ arg ming∈F on any dataset x ∈ X n, y ∈ Y n (even noisy). While it is NP-hard to efﬁciently compute ERM for many simple classes like disjunctions, previous reductions to ERM have proven useful with off-the-shelf classiﬁers (e.g., neural networks) albeit without theoretical guarantees.
The inputs to the learner are labeled training examples ¯x ∈ X n, ¯y = f (¯x) ∈ Y n, and unlabeled test examples x ∈ X n. For transductive learning, the goal is to predict labels for the n test examples x.
A transductive abstention algorithm is also given the predictor h := ERM(¯x, ¯y), which has 0 training error, and selects a vector a ∈ [0, 1]n for the probability of abstaining on test examples x. Its loss is deﬁned to be, (cid:80) (cid:96)x(f, h, a) := 1 n n (cid:88) i=1 ai α + (1 − ai)(cid:96)(f (xi), h(xi)). (1)
Our Min-Max Abstention (MMA) reduction, mentioned above, attempts to minimize the maximum test loss among classiﬁers consistent with the training data. In particular, it (approximately) solves the following convex optimization problem for a: min a∈[0,1]n max g∈V (cid:96)x(g, h, a). (2)
Here, V := {g ∈ F : ∀i g(¯xi) = f (¯xi)} is the version space of classiﬁers consistent with the training labels; thus h, f ∈ V . In other words, MMA minimizes the worst-case test loss it could possibly incur based on the labeled training and unlabeled test data. A key insight is that (2) has no unknowns, so MMA will achieve a max in (2) as low as if it knew P, Q and even f .
To solve (2), one must be able to maximize loss over V . Fortunately, GKKM showed how to solve that using a simple subroutine, which we call FLIP, that calls ERM.
Theorem 1 (Classiﬁcation). For Y = {0, 1}, any n, d ∈ N, any F of VC dimension d, any f ∈ F , and any distributions P, Q over X,
E
¯x∼P n,x∼Qn
[(cid:96)x(f, h, ˆa)] ≤ α|P − Q|T V + 2d lg 3n n
, where h = ERM(¯x, f (¯x)) and ˆa = MMA(¯x, f (¯x), x, h, FLIP) ∈ [0, 1]n can be computed in time poly(n) using the ERM oracle for F .
The total variation distance |P − Q|TV, also called the statistical distance, is a natural measure of non-overlap that ranges from 0 when P = Q to 1 when P and Q have disjoint supports. The
α|P − Q|TV term arises because the learner may need to abstain where P and Q do not overlap, and the other term derives directly from the same bound one gets in the case where P = Q (so
|P − Q|TV = 0). These bounds are stated in terms of expected loss, because unfortunately as we show in Lemma 8, high probability bounds require Ω(α/ n) loss because of the variance in how Q samples are distributed. All proofs, unless otherwise stated, are deferred to Appendix F.
√ 3
We generalize Theorem 1 in multiple ways. First, we point out a stronger bound in terms of a divergence Dk(P (cid:107)Q), k ≥ 1, that measures the excess of Q(x) over k · P (x). It generalizes the total variation distance |P − Q|TV between distributions P, Q:
Dk(P (cid:107)Q) := (cid:88) x max(cid:0)Q(x) − k · P (x), 0(cid:1) ∈ [0, 1]
|P − Q|TV := D1(P (cid:107)Q) = 1 2 (cid:88) x
|P (x) − Q(x)| (3) (4)
Note that if Q(x) ≤ k · P (x) for all x, then Dk(P (cid:107)Q) = 0.
Second, we show this implies generalization by using a transductive abstaining algorithm to bound the expected loss with respect to future examples from Q. That is, to go alongside classiﬁer h : X → {0, 1}, we can output abstainer A : X → [0, 1] that gives a probability of abstaining on each test example. Here, the generalization loss is, (cid:96)Q(f, h, A) := E x∼Q (cid:2)α A(x) + (1 − A(x))(cid:96)(cid:0)f (x), h(x)(cid:1)(cid:3) .
These two generalizations are summarized by the following theorem, which we state for classiﬁcation but also has a regression analog.
Theorem 2 (Generalization for classiﬁcation). Fix Y = {0, 1} and F of VC dimension d. For any f ∈ F , and any distributions P, Q over X,
E
¯x∼P n,x∼Qn
[(cid:96)Q(f, h, A)] ≤ min k≥1
αDk(P (cid:107)Q) + 2dk lg 3n n
≤ α|P − Q|T V + 2d lg 3n n
. where h = ERM(¯x, f (¯x)) and abstainer A : X → [0, 1] can be computed in time poly(n) using
ERM and is deﬁned by A(x(cid:48)) := MMA(cid:0)¯x, f (¯x), (x(cid:48), x2, . . . , xn), h, FLIP(cid:1).
These guarantees use the same algorithm—to predict on a new test example x(cid:48), it simply runs MMA with a modiﬁed test set where we have replaced the ﬁrst test example by x(cid:48) and returns the probability of abstaining on it.
Third, as in GKKM, guarantees hold with respect to a “white-box” adversarial model in which there is only a training distribution P and an adversary who may corrupt any number of test examples (but they are still labeled by f ). More speciﬁcally, natural train and test sets ¯x, z ∼ P n are drawn, and an adversary may form an arbitrary test set x ∈ X n. We achieve guarantees as low as if one knew exactly which examples were corrupted and abstained on those:
Theorem 3 (Adversarial classiﬁcation). For any f ∈ F with d = VC(F ), any n ∈ N, δ ≥ 0 and any distribution P over X, with probability ≥ 1 − δ over ¯x, z ∼ P n, the following holds simultaneously for all x ∈ X n:
α n (cid:96)x(f, h, ˆa) ≤
|{i : xi (cid:54)= zi}| + where h = ERM(¯x, f (¯x)) and ˆa = MMA(¯x, f (¯x), x, h, FLIP) ∈ [0, 1]n can be computed in time poly(n) using ERM.
In their adversarial setting, GKKM again has ˜O((cid:112)d/n) bounds. Our presentation actually begins in this adversarial framework as it is in some sense most general–adversarial robustness implies robustness to covariate shift.
. 2d lg 2n + lg 1/δ n 1.2 Linear regression results
For selective linear regression, the classic problem with transductive abstention, we give a more involved but fully polynomial-time algorithm. (GKKM did not discuss regression.) Here, Y =
[−1, 1], X = Bd(1) is taken to be the unit ball in d dimensions. There is now a joint distribution
ν over X × Y such that: f (x) := E
[y|x] = w · x for some vector w with (cid:107)w(cid:107) ≤ 1. We write
ν (¯x, ¯y) ∼ νn to indicate that the n labeled training examples are drawn from ν. The loss for selective regression (cid:96)x(h, f, a) is still deﬁned as in (1) except that (cid:96)(f (x), h(x)) := |f (x) − h(x)|2. Note that we are considering loss with respect to f rather than y which means that it may approach 0 for identical train and test distributions. Indeed, the additional loss term we will face due to the covariate shift is again α|P − Q|TV, where P and Q are the marginal distributions over X for ν and the test distribution, respectively. 4
Theorem 4 (Linear regression). Let Y = [−1, 1], n, d ∈ N, δ > 0, and X = Bd(1). Let P, Q be distributions over X and ν be a distribution over X × Y with marginal P over X. Let f (x) :=
E (x,y)∼ν
[y|x] = w · x for some w ∈ Bd(1). Then,
E (¯x,¯y)∼νn,x∼Qn
[(cid:96)x(f, h, ˆa)] ≤ α|P − Q|TV +
κ log n
√ n
. where κ is a constant, h = ERM(¯x, ¯y) and ˆa = MMA(¯x, ¯y, x, h, CDT) ∈ [0, 1]n can be computed in time poly(n, d).
A technical challenge is that, as a subroutine, we need to ﬁnd a linear model maximizing squared error, which is a non-convex problem. Fortunately, this step can be formulated as a Celis-Dennis-Tapia (CDT) problem, for which efﬁcient algorithms are known [5, 3]. Our information theoretic results generalize to more general regression in a straightforward fashion, however the algorithm for maximizing loss is non-convex and we only know how to implement it efﬁciently for linear regression.
Contributions and organization. The main contributions of this work are: (1) introducing and analyzing an approach for optimally abstaining in classiﬁcation and regression in the transductive setting, and (2) giving an efﬁcient abstention algorithm for linear regression and an efﬁcient reduction for binary classiﬁcation (optimal in our model). After reviewing related work, we formulate the problem in a manner that applies to both types of prediction: binary classiﬁcation and regression.
Section 2 gives information-theoretic bounds for selective prediction. Section 3 covers the general
MMA reduction. We then focus on classiﬁcation, where Section 4 gives bounds and an efﬁcient reduction to ERM. Finally, Section 5 gives bounds and a polynomial-time abstention algorithm for linear regression. 1.3