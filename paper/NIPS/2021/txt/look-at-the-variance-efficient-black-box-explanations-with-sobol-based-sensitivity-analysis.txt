Abstract
We describe a novel attribution method which is grounded in Sensitivity Analysis and uses Sobol indices. Beyond modeling the individual contributions of image regions, Sobol indices provide an efﬁcient way to capture higher-order interactions between image regions and their contributions to a neural network’s prediction through the lens of variance. We describe an approach that makes the computation of these indices efﬁcient for high-dimensional problems by using perturbation masks coupled with efﬁcient estimators to handle the high dimensionality of images. Importantly, we show that the proposed method leads to favorable scores on standard benchmarks for vision (and language models) while drastically reducing the computing time compared to other black-box methods – even surpassing the accuracy of state-of-the-art white-box methods which require access to internal representations. Our code is freely available: github.com/fel-thomas/
Sobol-Attribution-Method. 1

Introduction
Deep neural networks are now being deployed in numerous domains including medicine, trans-portation, security or ﬁnances with broad societal implications. Yet, these networks have become nearly inscrutable, and for most real-world applications, these systems are used to make critical decisions – often without any explanation. In recent years, numerous explainability methods have been proposed [1, 2, 3, 4, 5, 6, 7, 8]. In addition to helping improve people’s trust in these systems, these methods have helped identify and correct biases in datasets [4, 5, 9]. This has, in turn, helped improve these systems’ robustness and accelerate their broad deployment. An important limitation of standard explainability methods is that they require access to the system’s internal states including hidden layer activations or input gradients [1, 5, 6, 10]. As a result, these so-called white-box methods cannot be applied in the most general situations for which the internal states of network are not publicly accessible. For instance, it is common for companies to use neural networks provided by third parties (e.g., through web APIs or specialized hardware). However, only a handful of so-called black-box methods have been proposed to address this challenge with limited successes [2, 4, 8].
It is thus critical to develop more general methods that can reliably interpret and characterize the underlying decision processes of a wider array of models.
Common approaches to explaining a model’s prediction consists of attributing a score for each input dimension such as image pixels for computer vision systems or individual words for natural language processing. Shown in Fig. 1 is an example image and associated importance map for an image categorization system, whereby scores for individual pixels are displayed as heatmaps where
* Equal contribution †Work done before April 2021 and joining Tesla 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (Left) Sobol Attribution Method overview. Our method aims to explain the prediction of a black-box model for a given image. We ﬁrst sample a set of real-valued masks M drawn from a
Quasi-Monte Carlo (QMC) sequence. We apply these masks to the input image through a perturbation function Φ (here the Inpainting function) to form perturbed inputs X that we forward to the black box f to obtain prediction scores. Using the masks M and the associated prediction scores, we ﬁnally produce an explanation STi which characterizes the importance of each region by estimating the total order Sobol indices. While STi encompasses the effects of ﬁrst and all higher-order non-linear interactions between pixel regions, we can also produce the ﬁrst-order Sobol indices Si that reﬂect the importance of a region in isolation (e.g., the eyes of the cats). (Right) Sample explanations for
ResNet50V2. Comparing explanations produced with Si and STi helps highlight the importance of individual image regions in isolation vs. jointly (e.g., the lynx tips are important but conditioned on the presence of the presence of an eye). hotter locations correspond to pixels that contribute the most to the system’s ﬁnal prediction. In the context of black-box models, a core challenge is to derive these heatmaps using only the output predictions available through the network’s forward pass. A simple approach consists in applying a given perturbation at a speciﬁc location on the input image to then measure how the corresponding prediction is affected. In the case of image models, pixel intensities are simply set to a default value corresponding to a pure black or gray value; in the case of language models, individual words are removed entirely from the text [11, 12]. However, evaluating the impact of these perturbations one dimension at a time fails to identify all of the non-linear interactions between input variables that are known to prevail in a complex system such as a deep neural network. However, estimating the combined effect of perturbations across multiple locations quickly becomes combinatorially intractable. Methods have been recently proposed to try to address some of these issues by grouping dimensions together, such as by grouping pixels within a neighborhood of the image (superpixel) [2] or sampling perturbation masks that affect multiple regions of the input [4, 8]. A ﬁrst limitation of these approaches includes the use of Monte Carlo sampling methods which require a high number of forward passes – making these approaches computationally expensive. A second limitation is that they rely on relatively simple perturbations such as ﬂipping pixels on or off [2, 4, 8]. This severely constrains the space of perturbations considered and limits the efﬁciency with which the space of perturbations can be explored.
We address these limitations by introducing an attribution method that leverages variance-based sensitivity analysis techniques, and more speciﬁcally Sobol indices [13]. These methods were initially introduced to help identify the input variables that have the most inﬂuence on the variance of the output of a non-linear mathematical system [14]. These were traditionally used in physics and economics to estimate the part of the variance induced by a group of variables on a system’s output [15, 16, 17]. Our main contribution is a general framework to explain predictions from black-box models by adapting Sobol indices to be used in conjunction with perturbation masks. One of the originalities of our attribution method is that it is designed to support standard perturbations and real-valued intensity perturbations that can generate a continuous range of perturbations. Our second contribution is a tractable method for the calculation of Sobol indices. This is done by ﬁrst sampling the perturbation masks following Quasi-Monte Carlo sequences, which efﬁciently covers the space of perturbations. This can be done best by leveraging the most efﬁcient estimator borrowed from the sensitivity analysis literature [18, 19, 20, 21]. As a result, the proposed method can be 2
efﬁciently applied to high-dimensional inputs such as images. It produces on par with or better explanations than the state-of-the-art with at least half the number of forward passes. Another beneﬁt of our method is that it allows to characterize not only the main effect of image regions but also higher-order interactions by decomposing the variance of the system’s prediction using the Sobol indices. We run extensive experiments to demonstrate the beneﬁts of the proposed method on several recent complementary benchmarks including the “Pointing Game” and “Deletion”. Our primary focus is image classiﬁcation, but our method is general and we include results on text classiﬁcation benchmarks as well. 2