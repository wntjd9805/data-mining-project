Abstract
We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment.
The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models. 1

Introduction
Designing experiments to maximize the information gathered about an underlying process is a key challenge in science and engineering. Most such experiments are naturally adaptive—we can design later iterations on the basis of data already collected, refining our understanding of the process with each step [36, 45, 51]. For example, suppose that a chemical contaminant has accidentally been released and is rapidly spreading; we need to quickly discover its unknown source. To this end, we measure the contaminant concentration level at locations ξ1, . . . , ξT (our experimental designs), obtaining observations y1, . . . , yT . Provided we can perform the necessary computations sufficiently quickly, we can design each ξt using data from steps 1, . . . , t − 1 to narrow in on the source.
Bayesian optimal experimental design (BOED) [7, 32] is a principled model-based framework for choosing designs optimally; it has been successfully adopted in a diverse range of scientific fields
[52, 58, 60]. In BOED, the unknown quantity of interest (e.g. contaminant location) is encapsulated by a parameter θ, and our initial information about it by a prior p(θ). A simulator, or likelihood, model y|θ, ξ describes the relationship between θ, our controllable design ξ, and the experimental outcome y. To select designs optimally, the guiding principle is information maximization—we select the design that maximizes the expected (Shannon) information gained about θ from the data y, or, equivalently, that maximizes the mutual information between θ and y.
This naturally extends to adaptive settings by considering the conditional expected information gain given previously collected data. The traditional approach, depicted in Figure 1a, is to fit a posterior p(θ|ξ1:t−1, y1:t−1) after each iteration, and then select ξt in a myopic fashion using the one-step mutual information (see, e.g., [51] for a review). Unfortunately, this approach necessitates significant computation at each t and does not lend itself to selecting optimal designs quickly and adaptively.
Recently, Foster et al. [17] proposed an exciting alternative approach, called Deep Adaptive Design (DAD), that is based on learning design policies. DAD provides a way to avoid significant computation 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Live experiment
Design
T iterations
Infer
Observe e n i fl f
O e n i fl f
O deploy
Policy
Critic
Design
T iterations
Infer data
Observe
L i v e e x p e r i m e n t (a) Traditional BOED: costly computa-tions (design optimisation and parameter inference) are required at each iteration. (b) Policy-based BOED using iDAD: a design policy and critic are learnt before the live experiment. The policy enables quick and adaptive experiments, the critic assists likelihood-free inference.
Figure 1: Overview of adaptive BOED approaches applicable to implicit models. at deployment-time by, prior to the experiment itself, learning a design policy network that takes past design-outcome pairs and near–instantaneously returns the design for the next stage of the experiment. The required training is done using simulated experimental histories, without the need to estimate any posterior or marginal distributions. DAD further only needs a single policy network to be trained for multiple experiments, further allowing for amortization of the adaptive design process.
Unfortunately, DAD requires conditionally independent experiments and only works for the restricted class of models that have an explicit likelihood model we can simulate from, evaluate the density of, and calculate derivatives for, substantially reducing its applicability.
To address this shortfall, we instead consider a far more general class of models where we require only the ability to simulate y|θ, ξ and compute the derivative ∂y/∂ξ, e.g. via automatic differentiation [5].
Such models are ubiquitous in scientific modelling and include differentiable implicit models [19], for which the likelihood density p(y|θ, ξ) is intractable. Examples include mixed effects models [15, 18], various models from chemistry and epidemiology [1], the Lotka Volterra model used in ecology [19], and models specified via stochastic differential equations (such as the SIR model [10]).
To perform rapid adaptive experimentation with this large class of models, we introduce implicit
Deep Adaptive Design (iDAD), a method for learning adaptive design policy networks using only simulated outcomes (see Figure 1b). To achieve this, we introduce likelihood-free lower bounds on the total information gained from a sequence of experiments, which iDAD utilizes to learn a deep policy network. This policy network amortizes the cost of experimental design for implicit models and can be run in milliseconds at deployment-time. To train it, we show how the InfoNCE [57] and NWJ [37] bounds, popularized in representation learning, can be applied to the policy-based experimental design setting. The optimization of both of these bounds involves simultaneously learning an auxiliary critic network, bringing an important added benefit: it can be used to perform likelihood-free posterior inference of the parameters given the data acquired from the experiment.
We also relax DAD’s requirement for experiments to be conditionally independent, allowing its application in complex settings like time series data, and, through innovative architecture adaptations, also provide improvements in the conditionally independent setting as well. This further expands the model space for policy-based BOED, and leads to additional performance improvements.
Critically, iDAD forms the first method in the literature that can practically perform real-time adaptive
BOED with implicit models: previous approaches are either not fast enough to run in real-time for non-trivial models, or require explicit likelihood models. We illustrate the applicability of iDAD on a range of experimental design problems, highlighting its benefits over existing baselines, even finding that it often outperforms costly non-amortized approaches. Code for iDAD is publicly available at https://github.com/desi-ivanova/idad. 2