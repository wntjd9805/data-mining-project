Abstract 3D object detection is a key module in safety-critical robotics applications such as autonomous driving. For such applications, we care the most about how the detections impact the ego-agent’s behavior and safety (the egocentric perspective).
Intuitively, we seek more accurate descriptions of object geometry when it’s more likely to interfere with the ego-agent’s motion trajectory. However, current detec-tion metrics, based on box Intersection-over-Union (IoU), are object-centric and are not designed to capture the spatio-temporal relationship between objects and the ego-agent. To address this issue, we propose a new egocentric measure to evaluate 3D object detection: Support Distance Error (SDE). Our analysis based on SDE reveals that the egocentric detection quality is bounded by the coarse geometry of the bounding boxes. Given the insight that SDE can be improved by more accu-rate geometry descriptions, we propose to represent objects as amodal contours, speciﬁcally amodal star-shaped polygons, and devise a simple model, StarPoly, to predict such contours. Our experiments on the large-scale Waymo Open Dataset show that SDE better reﬂects the impact of detection quality on the ego-agent’s safety compared to IoU; and the estimated contours from StarPoly consistently improve the egocentric detection quality over recent 3D object detectors. 1

Introduction 3D object detection is a key problem in robotics, including popular applications such as autonomous driving. Common evaluation metrics for this problem, e.g. mean Average Precision (mAP) based on box Intersection-over-Union (IoU), follow an object-centric approach, where errors on different objects are computed and aggregated without taking their spatiotemporal relationships with the ego-agent into account. While these metrics provide a good proxy for downstream performance in general scene understanding applications, they have limitations for egocentric applications, e.g. autonomous driving, where detections are used to assist navigation of the ego-agent. In these applications, detecting potential collisions on the ego-agent’s trajectory is critical. Accordingly, evaluation metrics should focus more on the objects closer to the planned trajectory and to the parts/boundaries of those objects that are closer to the trajectory.
Recent works have introduced a few modiﬁcations to evaluation protocols to address these issues, e.g., breaking down the metrics into different distance buckets [53] or using learned planning models to reﬂect detection quality [34]. However, they are either very coarse [53] or rely on optimized neural networks [34], making it difﬁcult to interpret and compare results in different settings. In this
∗Correspondence to bydeng@waymo.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
paper, we take a novel approach to 3D object detection from an egocentric perspective. We start by reviewing the ﬁrst principle: the detection quality relevant to the ego-agent’s planned trajectory, both at the moment and in the future, has the most profound impact on the ability to facilitate navigation.
This leads us to transform detection predictions into two types of distance estimates relative to the ego-agent’s trajectory — lateral distance and longitudinal distance (Fig. 1). The errors on these two distances form our support distance error (SDE) concept, where the components can either be aggregated as the max distance estimation error or used independently, for different purposes.
Compared to IoU, SDE (as a shape metric) is conditioned on the spatio-temporal relationship between the object and the ego-agent. Even a small mistake in detection near the ego-agent’s planned trajectory can incur a high SDE (as in
Fig. 2 left, object 3). Additionally, SDE can be ex-tended to evaluate the impact of detections to the ego-agent’s future plans (for cases where an ob-ject comes close to the planned trajectory later in time). This is not feasible for IoU, which is invari-ant to the ego-agent position or trajectory(shown in Fig. 2).
Figure 1: Lateral distance and longitudinal distance.
These two types of support distance measure how far an object’s shape boundary is to the observer (ego-agent) in both the direction along the observer velocity (longitudinal) and perpendicular to it (lateral).
Using SDE to analyze a state-of-the-art detec-tor [44], we observe a signiﬁcant error discrep-ancy between using a rectangular-shaped box ap-proximation and the actual object’s boundary, suggesting the need for a better representation to describe the ﬁne-grained geometry of objects. To this end, we propose a simple lightweight reﬁne-ment to box-based detectors named StarPoly. Based on a detection box, StarPoly predicts an amodal contour around the object, as a star-shaped polygon.
Moreover, we incorporate SDE into the standard average precision (AP) metric and derive an
SDE-based AP (SDE-AP) for conveniently evaluating existing detectors. In order to make an even more egocentric AP metric, we further add inverse distance weighting to the examples, obtaining
SDE-APD (D for distance weighted). With the proposed metrics, we observe different behaviors among several popular detectors [41, 44, 67, 20] compared to what IoU-AP would reveal. For example, PointPillars [20] excels on SDE-AP in the near range in spite of its less competitive overall performance. Finally, we show that StarPoly consistently improves upon the box representation of shape based on our egocentric metric, SDE-APD. 2