Abstract
Most prior methods for learning navigation policies require access to simulation environments, as they need online policy interaction and rely on ground-truth maps for rewards. However, building simulators is expensive (requires manual effort for each and every scene) and creates challenges in transferring learned policies to robotic platforms in the real-world, due to the sim-to-real domain gap. In this paper, we pose a simple question: Do we really need active interaction, ground-truth maps or even reinforcement-learning (RL) in order to solve the image-goal navigation task? We propose a self-supervised approach to learn to navigate from only passive videos of roaming. Our approach, No RL, No Simulator (NRNS), is simple and scalable, yet highly effective. NRNS outperforms RL-based formulations by a signiﬁcant margin. We present NRNS as a strong baseline for any future image-based navigation tasks that use RL or Simulation. 1

Introduction
In recent years, we have seen signiﬁcant advances in learning-based approaches for indoor naviga-tion [1, 2]. Impressive performance gains have been obtained for a range of tasks, from non-semantic point-goal navigation [3] to semantic tasks such as image-goal [4] and object-goal navigation [5, 6], via methods that use reinforcement learning (RL). The effectiveness of RL for these tasks can be attributed in part to the emergence of powerful new simulators such as Habitat [7], Matterport [8] and AI2Thor [9]. These simulators have helped scale learning to billions of frames by providing large-scale active interaction data and ground-truth maps for designing reward functions. But do we actually need simulation and RL to learn to navigate? Is there an alternative way to formulate the navigation problem, such that no ground-truth maps or active interaction are required? These are valuable questions to explore because learning navigation in simulation constrains the approach to a limited set of environments, since the creation of 3D assets remains costly and time-consuming.
In this paper, we propose a self-supervised approach to learning how to navigate from passive egocentric videos. Our novel method is simple and scalable (no simulator required for training), and at the same time highly effective, as it outperforms RL-based formulations by a signiﬁcant margin.
To introduce our approach, let us ﬁrst examine the role of RL and simulation in standard navigation learning methods. In the standard RL formulation, an agent gets a reward upon reaching the goal, followed by a credit assignment stage to determine the most useful state-action pairs. But do we actually need the reinforce function for action credit assignment? Going a step further, do we even need to learn a policy explicitly? In navigation, we argue that the state space itself is highly structured via a distance function, and the structure itself could be leveraged for credit assignment. Simply put, states that help reduce the distance to the goal are better – and therefore predicted distance can be
∗Correspondence: meerahahn@gatech.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Left: Using passive videos we learn to predict distances for navigation. Our distance function learns the priors of the layouts of indoor buildings to estimate distances to goal location.
Right: Image-Goal Navigation Task [10]. Our model uses distance function to predict distances of unexplored nodes and uses greedy policy to choose the shortest-distance node. used either as the value function or as a proxy for it. In fact, RL formulations frequently use ‘distance reduced to goal’ in reward shaping. The key property of our approach is that we learn a generalizable distance estimator directly from passive videos, and as a result we do not require any interaction.
We demonstrate that an effective distance estimator can be learned directly from visual trajectories, without the need for an RL policy to map visual observations to the action space, thereby obviating the need for extensive interaction in a simulator and hand-designed rewards. However passive videos do not provide learning opportunities for obstacle avoidance since they rarely, if ever, consist of cameras bumping into walls. We forgo the need for active interaction to reason about collisions as we show that obstacle avoidance is only required locally and simple depth maps are sufﬁcient to prune invalid actions and locations for navigation. More broadly, our approach can be considered as closely related to model-based control, which is an alternative paradigm to RL-based policy learning, with the key insight that components of the model and cost functions can be learned from passive data.
No RL, No Simulator Approach (NRNS): Our NRNS algorithm can be described as follows.
During training we learn two functions from passive videos: (a) a geodesic distance estimator: given the state features and goal image, this function predicts the geodesic distance of the unexplored frontiers of the graph to the goal location. This enables a greedy policy in which we select the node with the least distance; (b) a target prediction model: Given the goal image and the image from the agent’s current location, this function predicts if the goal is within sight and can be reached without collisions, along with the exact location of the goal. The key is that both the distance model and the target prediction model can be learned from passive RGBD videos, with SLAM used to estimate relative poses. We believe our simple NRNS approach should act as a strong baseline for any future approaches that use RL and simulation. We show that NRNS outperforms end-to-end RL, even when
RL is trained using 5x more data and 10x more compute. Furthermore, unlike RL methods which need to be trained in simulation because they require substantial numbers of interactions, NRNS can be trained directly on real-world videos alone, and therefore does not suffer from the sim-to-real gap. 2