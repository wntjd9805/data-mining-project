Abstract
We study the theoretical convergence properties of random-search methods when optimizing non-convex objective functions without having access to derivatives.
We prove that standard random-search methods that do not rely on second-order information converge to a second-order stationary point. However, they suffer from an exponential complexity in terms of the input dimension of the problem.
In order to address this issue, we propose a novel variant of random search that exploits negative curvature by only relying on function evaluations. We prove that this approach converges to a second-order stationary point at a much faster rate than vanilla methods: namely, the complexity in terms of the number of function evaluations is only linear in the problem dimension. We test our algorithm empirically and ﬁnd good agreements with our theoretical results. 1

Introduction
We consider solving the non-convex optimization problem minx∈Rd f (x), where f (·) is differentiable but its derivatives are not directly accessible, or can only be approximated at a high computational cost. This setting recently gained attention in machine learning, in areas such as black-box adversarial attacks [11], reinforcement learning [51], meta-learning [7], online learning [9], and conditional optimization [57].
We focus our attention on a popular class of derivative-free methods known as random direct-search methods of directional type 1. These methods optimize f (·) by evaluating the objective function over a number of (ﬁxed or randomized) directions, to ensure descent using a sufﬁciently small stepsize.
Direct-search algorithms date to the 1960’s, including e.g. [44, 46]. More recent variants include deterministic direct search [15], random direct search (e.g. [54], or the Stochastic Three Points (STP) method [6]), which randomly sample a direction and accept a step in this direction if it decreases the function f (·). As discussed in [40], direct-search methods have remained popular over the years for a number of reasons, including their good performance and known global convergence guarantees [54], as well as their straightforward implementation that makes them suitable for many problems. We refer the reader to [40, 15] for a survey.
In machine learning, objective functions of interest are often non-convex, which poses additional challenges due to the presence of saddle points and potentially suboptimal local minima [33].
Instead of aiming for a global minimizer, one often seeks a second-order stationary point (SOSP): i.e. a solution where the gradient vanishes and the Hessian is positive deﬁnite. Indeed, as shown by [14, 36, 22, 21], many machine learning problems have no spurious local minimizers, yet have many saddle points which yield suboptimal solutions and are often hard to escape from [19]. While convergence to SOSPs and saddle escape times have been extensively studied in the context of
∗Alphabetical ordering, all authors contributed equally. 1In this manuscript, we will use the terms “random direct-search” and “random search” interchangeably. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
gradient-based methods [32, 16, 10, 56], prior analyses for (random) direct search have mainly focused on convergence to ﬁrst-order stationary points [6, 54]. One exception is the Approximate
Hessian Direct Search (AHDS) method [26], that explicitly computes the Hessian of the objective to obtain second-order worst-case guarantees. However, computing or storing a full Hessian is prohibitively expensive in high dimensions.
Towards a better understanding of the complexity of ﬁnding second-order stationary points with random search methods, we make the following contributions:
• We study the complexity of a simple random search similar to STP (Algorithm 1) to reach SOSPs.
We ﬁnd that the (worst-case) complexity requires a number of function evaluations that scales exponentially in terms of the problem dimension d. As we will see, the exponential scaling is not an artefact of the analysis. This is intuitive, indeed, if we are at a saddle point where we just have one direction of negative curvature, ﬁnding good alignment with a random direction becomes exponentially difﬁcult as the dimension of the space increases.
• To solve this issue, we design a variant of random search (RSPI, Algorithm 2) that, instead of randomly sampling directions from the sphere, relies on an approximate derivative-free power iteration routine to extract negative curvature direction candidates (unlike [26] that requires an approximation to the full Hessian). This approach is inspired from recent work on gradient-based methods for ﬁnding SOSPs [10, 56] and effectively decouples negative curvature estimation from progress in the large gradient setting. As a result, RSPI does not suffer from the exponential scaling of the vanilla random search approach : we show that the overall complexity of ﬁnding a SOSP in terms of function evaluations becomes linear in the problem dimension.
• Finally, we verify our results empirically and compare our novel algorithm to standard random-search methods. Our results show improvements of RSPI both in terms of algorithm iterations and (crucially) wall-clock time. 2