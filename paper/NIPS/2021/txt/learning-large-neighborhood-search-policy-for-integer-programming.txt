Abstract
We propose a deep reinforcement learning (RL) method to learn large neighborhood search (LNS) policy for integer programming (IP). The RL policy is trained as the destroy operator to select a subset of variables at each step, which is reoptimized by an IP solver as the repair operator. However, the combinatorial number of variable subsets prevents direct application of typical RL algorithms. To tackle this challenge, we represent all subsets by factorizing them into binary decisions on each variable. We then design a neural network to learn policies for each variable in parallel, trained by a customized actor-critic algorithm. We evaluate the proposed method on four representative IP problems. Results show that it can find better solutions than SCIP in much less time, and significantly outperform other
LNS baselines with the same runtime. Moreover, these advantages notably persist when the policies generalize to larger problems. Further experiments with Gurobi also reveal that our method can outperform this state-of-the-art commercial solver within the same time limit. 1

Introduction
Combinatorial optimization problems (COPs) have been widely studied in computer science and operations research, which cover numerous real-world tasks in many fields such as communication, transportation and manufacturing [1]. Most COPs are very difficult to solve efficiently due to their
NP-hardness. The performance of classic methods, including exact and heuristic algorithms [2], is generally limited by hand-crafted policies that are costly to design, since considerable trial-and-error and domain knowledge are needed. On the other hand, it is common in practice that similar instances with shared structure are frequently solved, and differ only in data that normally follows a distribution
[3]. This provides a chance for machine learning to automatically generate heuristics or policies. In doing so, the learned alternatives are expected to save massive manual work in algorithm design, and raise the performance of the algorithm on a class of problems.
Recently, a number of works apply deep (reinforcement) learning to automatically design heuristic algorithms, either in constructive or improving fashion. Different from construction heuristics that sequentially extend partial solutions to complete ones [4, 5, 6, 7, 8, 9, 10, 11], learning improvement heuristics can often deliver high solution quality by iteratively reoptimizing an initial solution using
∗Wen Song is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
local operations [12, 13, 14]. In this line, some methods are developed under the Large Neighborhood
Search (LNS) framework [15, 16], which is a powerful improving paradigm to find near-optimal solutions for COPs.
However, the above methods are restricted to specific problem types, and cannot generalize to those from different domains. This motivates the studies of learning to directly solve Integer Programs (IPs), which is very powerful and flexible in modelling a wide range of COPs. The standard approach to solve IPs is branch-and-bound (B&B) [17], which lies at the core of common solvers such as
SCIP, Gurobi, and CPLEX. Thus, most of existing methods improve the performance of a solver on a distribution of instances, by training models for critical search decisions in B&B such as variable and node selection [18, 19, 20]. Nevertheless, these methods are generally limited to small instances and require sufficient interface access to the internal solving process of the solvers.
This paper mainly tackle the issue that how to improve a solver from externals such that it can find high-quality solutions more quickly? In specific, we propose a high-level, learning based LNS method to solve general IP problems. Based on deep reinforcement learning (RL), we train a policy network as the destroy operator in LNS, which decides a subset of variables in the current solution for reoptimization. Then we use a solver as the repair operator, which solves sub-IPs to reoptimize the destroyed variables. Despite being heuristic, our method can effectively handle the large-scale IP by solving a series of smaller sub-IPs. Moreover, complex interface to the solver’s internal logic is not required. However, the above RL task is challenging, mainly because the action space, i.e., number of variable subsets at each LNS step, is exponentially large. To resolve this issue, we represent all the subsets by factorizing them into binary decisions on each variable, i.e., whether a variable should be destroyed. In doing so, we make it possible to learn a policy to select any subset from large discrete action spaces (at least 21000 candidates in our experiments). To this end, we design a Graph Neural
Network (GNN) based policy network that enables learning policies for each variable in parallel, and train it by a customized actor-critic algorithm.
A recent work [21] also attempts to learn LNS policy to solve IP problems, and we generalize this framework to enable learning more flexible and powerful LNS algorithms. One limitation of [21] is that it hypothesizes a constant cardinality of the destroyed variable subset at each LNS step, which is a predefined hyperparameter. However, the number and choice of optimized variables at each step should be adaptive according to instance information and solving status, which is achieved in our method. In doing so, the LNS policies trained by our method significantly outperform those trained by the method in [21].
We evaluate our method on four NP-hard benchmark problems with SCIP as the repair solver.
Extensive results show that our method generally delivers better solutions than SCIP with mere 1/2 or 1/5 of runtime, and significantly outperforms LNS baselines with the same runtime. These advantages notably persist when the trained policies are directly applied to much larger problems. We also apply our LNS framework to Gurobi, which shows superiority over the solver itself and other baselines. 2