Abstract
Among the many ways of quantifying uncertainty in a regression setting, specifying the full quantile function is attractive, as quantiles are amenable to interpretation and evaluation. A model that predicts the true conditional quantiles for each input, at all quantile levels, presents a correct and efﬁcient representation of the underly-ing uncertainty. To achieve this, many current quantile-based methods focus on optimizing the pinball loss. However, this loss restricts the scope of applicable regression models, limits the ability to target many desirable properties (e.g. cali-bration, sharpness, centered intervals), and may produce poor conditional quantiles.
In this work, we develop new quantile methods that address these shortcomings.
In particular, we propose methods that can apply to any class of regression model, select an explicit balance between calibration and sharpness, optimize for calibra-tion of centered intervals, and produce more accurate conditional quantiles. We provide a thorough experimental evaluation of our methods, which includes a high dimensional uncertainty quantiﬁcation task in nuclear fusion. 1

Introduction
Uncertainty quantiﬁcation (UQ) in machine learning typically refers to the task of quantifying the conﬁdence of a given prediction. This measure of certainty can be crucial in a variety of downstream applications, including Bayesian optimization [36, 45, 54], model-based reinforcement learning
[64, 42, 10, 18], and in high-stakes predictions where mistakes incur large costs [52, 61].
While the common goal of UQ is to describe predictive distributions over outputs for given inputs, the representation of the distributional prediction varies across methods. For example, some methods assume a parametric distribution and return parameter estimates [65, 14, 37], while others return density function estimates, as is common in Bayesian methods [41, 38, 24, 3, 33, 50]. Alternatively, many methods represent predictive uncertainty with quantile estimates [15, 53, 58, 49, 27].
Quantiles provide an attractive representation for uncertainty because they can be used to model complex distributions without parametric assumptions, are interpretable with units in the target output space, allow for easy construction of prediction intervals, and can be used to efﬁciently sample from the predictive distribution via inverse transform sampling [22, 32]. Learning the quantile for a single
Code is available at https://github.com/YoungseogChung/calibrated-quantile-uq. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
quantile level is a well studied problem in quantile regression (QR) [30, 32], which typically involves optimizing the so-called pinball loss, a tilted transformation of the absolute value function. Given a target y, a prediction ˆy, and quantile level τ (0, 1), the pinball loss ρτ is deﬁned as
∈
ρτ (y, ˆy) = (ˆy y)(I
− y
{
≤
ˆy
} −
τ ). (1)
By training for all quantiles simultaneously, recent works have made concrete steps in incorporating
QR methods to form competitive UQ methods which output the full predictive distribution [51, 58].
In this work, we highlight some limitations of the pinball loss and propose several methods to address these shortcomings. Speciﬁcally, we explore the following:
• Model agnostic QR. Optimizing the pinball loss often restricts the choice of model family for which we can provide UQ. We propose an algorithm to learn all quantiles simultaneously by utilizing methods from conditional density estimation. This algorithm is agnostic to model class and can be applied to any regression model.
• Explicitly balancing calibration and sharpness. While the pinball loss, as a proper scoring rule, targets both calibration and sharpness, the balance between these two quantities is made implicitly, which may result in a poor optimization objective. We propose a tunable loss function that targets calibration and sharpness separately, and allows the end-user to set an explicit balance.
• Centered intervals. In practice, we often desire uncertainty predictions made with centered intervals, which are not targeted via the pinball loss. We propose an alternative loss function that is better suited for this goal.
• Encouraging individual calibration. Perfect quantile forecasts will satisfy individual calibration (Eq. 2), which is a much stricter condition than the more-commonly used notion of average calibration (Eq. 3). We introduce a training procedure that aims to improve quantile predictions beyond average calibration, and demonstrate its efﬁcacy via adversarial group calibration (Eq. 4).
We proceed by ﬁrst describing methods of assessing the quality of predictive UQ and the pitfalls of optimizing the pinball loss in Section 2. Drawing motivation from this, we then present our proposed methods in Section 3. In Section 4, we demonstrate our methods experimentally, where we model predictive uncertainty on benchmark datasets, and on a high-dimensional, real-world uncertainty estimation task in the area of nuclear fusion. 2 Preliminaries and