Abstract
This paper investigates the problem of computing the equilibrium of competitive games, which is often modeled as a constrained saddle-point optimization problem with probability simplex constraints. Despite recent efforts in understanding the last-iterate convergence of extragradient methods in the unconstrained setting, the theoretical underpinnings of these methods in the constrained settings, espe-cially those using multiplicative updates, remain highly inadequate, even when the objective function is bilinear. Motivated by the algorithmic role of entropy regularization in single-agent reinforcement learning and game theory, we develop provably efficient extragradient methods to find the quantal response equilibrium (QRE)—which are solutions to zero-sum two-player matrix games with entropy regularization—at a linear rate. The proposed algorithms can be implemented in a decentralized manner, where each player executes symmetric and multiplicative updates iteratively using its own payoff without observing the opponent’s actions directly. In addition, by controlling the knob of entropy regularization, the pro-posed algorithms can locate an approximate Nash equilibrium of the unregularized matrix game at a sublinear rate without assuming the Nash equilibrium to be unique. Our methods also lead to efficient policy extragradient algorithms for solving entropy-regularized zero-sum Markov games at a linear rate. All of our convergence rates are nearly dimension-free, which are independent of the size of the state and action spaces up to logarithm factors, highlighting the positive role of entropy regularization for accelerating convergence. 1

Introduction
Finding the equilibrium of competitive games, which can be viewed as constrained saddle-point optimization problems with probability simplex constraints, lies at the heart of modern machine learn-ing and decision making paradigms such as Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), competitive reinforcement learning (RL) (Littman, 1994), game theory (Shapley, 1953), adversarial training (Mertikopoulos et al., 2018b), to name a few.
In this paper, we study one of the most basic forms of competitive games, namely two-player zero-sum games, in both the matrix setting and the Markov setting. Our goal is to find the equilibrium policies of both players in an independent and decentralized manner (Daskalakis et al., 2020; Wei et al., 2021a) with guaranteed last-iterate convergence. Namely, each player will execute symmetric and independent updates iteratively using its own payoff without observing the opponent’s actions directly, and the final policies of the iterative process should be a close approximation to the equilibrium up to any prescribed precision. This kind of algorithms is more advantageous and versatile especially in federated environments, as it requires neither prior coordination between the players like two-timescale algorithms, nor a central controller to collect and disseminate the policies of all the players, which are often unavailable due to privacy constraints. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1 Last-iterate convergence in competitive games
In recent years, there have been significant progresses in understanding the last-iterate convergence of simple iterative algorithms for unconstrained saddle-point optimization, where one is interested in bounding the sub-optimality of the last iterate of the algorithm, rather than say, the ergodic iterate — which is the average of all the iterations — that are commonly studied in the earlier literature. This shift of focus is motivated, for example, by the infeasibility of averaging large machine learning models in training GANs (Goodfellow et al., 2014). While vanilla Gradient Descent / Ascent (GDA) may diverge or cycle even for bilinear matrix games (Daskalakis et al., 2018), quite remarkably, small modifications lead to guaranteed last-iterate convergence to the equilibrium in a non-asymptotic fashion. A flurry of algorithms is proposed, including Optimistic Gradient Descent Ascent (OGDA) (Rakhlin and Sridharan, 2013; Daskalakis and Panageas, 2018b; Wei et al., 2021b), predictive updates (Yadav et al., 2017), implicit updates (Liang and Stokes, 2019), and more. Several unified analyses of these algorithms have been carried out (see, e.g. Mokhtari et al. (2020a); Liang and Stokes (2019) and references therein), where these methods in principle all make clever extrapolation of the local curvature in a predictive manner to accelerate convergence. With slight abuse of terminology, in this paper, we refer to this ensemble of algorithms as extragradient methods (Korpelevich, 1976; Tseng, 1995; Mertikopoulos et al., 2018a; Harker and Pang, 1990).
However, saddle-point optimization in the constrained setting, which includes competitive games as a special case, remains largely under-explored even for bilinear matrix games. While it is possible to reformulate constrained bilinear games to unconstrained ones using softmax parameterization of the probability simplex, this approach falls short of preserving the bilinear structure and convex-concave properties in the original problem, which are crucial to the convergence of gradient methods.
Therefore, there is a strong necessity of understanding and developing improved extragradient methods in the constrained setting. Daskalakis and Panageas (2018a) proposed the optimistic variant of the multiplicative weight updates (MWU) method (Arora et al., 2012) – which is extremely natural and popular for optimizing over probability simplexes – called Optimistic Multiplicative Weight
Updates (OMWU), and established the asymptotic last-iterate convergence of OMWU for matrix games. Very recently, Wei et al. (2021b) established non-asymptotic last-iterate convergences of
OMWU. However, these last-iterate convergence results require the Nash equilibrium to be unique, and cannot be applied to problems with multiple Nash equilibria. 1.2 Our contributions
Motivated by the algorithmic role of entropy regularization in single-agent RL (Neu et al., 2017;
Geist et al., 2019; Cen et al., 2020) as well as its wide use in game theory to account for imperfect and noisy information (McKelvey and Palfrey, 1995; Savas et al., 2019), we initiate the design and analysis of extragradient algorithms using multiplicative updates for finding the quantal response equilibrium (QRE), which are solutions to competitive games with entropy regularization (McKelvey and Palfrey, 1995). While finding QRE is of interest in its own right, by controlling the knob of entropy regularization, the QRE provides a close approximation to the Nash equilibrium (NE), and in turn acts as a smoothing scheme for finding the NE. Our contributions are summarized below.
Near dimension-free last-iterate convergence to QRE of entropy-regularized matrix games. We
• propose two policy extragradient algorithms to solve entropy-regularized matrix games, namely the
Predictive Update (PU) and OMWU methods, where both players execute symmetric and multiplica-tive updates without knowing the entire payoff matrix nor the opponent’s actions. Encouragingly, we show that the last iterate of the proposed algorithms converges to the unique QRE at a linear rate that is almost independent of the size of the action spaces. Roughly speaking, to find an ϵ-optimal iterations,
QRE in terms of Kullback-Leibler (KL) divergence, it takes no more than (cid:101)O (cid:16) 1
ητ log (cid:0) 1 (cid:1)(cid:17)
ϵ
A
∥
∥
) hides logarithmic dependencies. Here, τ is the regularization parameter, and η is the where (cid:101)O(
· learning rate of both players. Maximizing the learning rate, the iteration complexity is bounded by is the ℓ∞ norm of the payoff matrix A. (cid:101)O ((1 +
∞/τ ) log(1/ϵ)), where
Ai,j
A
∥
∞ = maxi,j
∥
|
|
Last-iterate convergence to ϵ-NE of unregularized matrix games without uniqueness assumption.
•
The QRE provides an accurate approximation to the NE by setting the entropy regularization τ sufficiently small, therefore our result directly translates to finding a NE with last-iterate convergence guarantee. Roughly speaking, to find an ϵ-NE (Zhang et al., 2020, Definition 2.1), it takes no more iterations with optimized learning rates, which is again independent of the size than (cid:101)O 1 + ∥A∥∞ (cid:17) (cid:16)
ϵ 2
Equilibrium type
ϵ-QRE
ϵ-NE
Method
PU & OMWU (this work)
OMWU (Daskalakis and Panageas, 2018a)
OMWU (Wei et al., 2021b)
PU & OMWU (this work)
Convergence rate
Dimension-free
Require unique NE linear asymptotic sublinear + linear sublinear yes no no yes n/a yes yes no
Table 1: Comparisons of last-iterate convergence of the proposed entropy-regularized PU and
OMWU methods with prior results for finding ϵ-QRE or ϵ-NE of competitive matrix games. We note that the convergence rates of unregularized OMWU established in Wei et al. (2021b) are problem-dependent, and scale at least polynomially on the size of the action spaces. Desirable features in the last two columns are highlighted in blue. of the action spaces up to logarithmic factors. Unlike prior literature (Daskalakis and Panageas, 2018a; Wei et al., 2021b), our last-iterate convergence guarantee does not require the NE to be unique.
Extensions to two-player zero-sum Markov games. By connecting value iteration with matrix
• games, we propose a policy extragradient method for solving infinite-horizon discounted entropy-regularized zero-sum Markov games, which finds an ϵ-optimal minimax soft Q-function—in terms of
ℓ∞ error—in at most (cid:101)O (0, 1) is the discount factor. iterations, where γ
τ (1−γ)2 log2 (cid:0) 1 (cid:1)(cid:17) (cid:16) 1
ϵ
∈
To the best of our knowledge, our paper is the first that develops policy extragradient algorithms for solving entropy-regularized competitive games with multiplicative updates and dimension-free linear last-iterate convergence, and demonstrates entropy regularization as a smoothing technique to find ϵ-NE without the uniqueness assumption. Table 1 provides detailed comparisons of the proposed methods with prior arts for solving matrix games. Our results highlight the positive role of entropy regularization for accelerating convergence and safeguarding against imperfect information in competitive games. We defer the complete proof of our results to Cen et al. (2021). 1.3