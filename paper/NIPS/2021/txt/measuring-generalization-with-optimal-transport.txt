Abstract
Understanding the generalization of deep neural networks is one of the most im-portant tasks in deep learning. Although much progress has been made, theoretical error bounds still often behave disparately from empirical observations. In this work, we develop margin-based generalization bounds, where the margins are nor-malized with optimal transport costs between independent random subsets sampled from the training distribution. In particular, the optimal transport cost can be inter-preted as a generalization of variance which captures the structural properties of the learned feature space. Our bounds robustly predict the generalization error, given training data and network parameters, on large scale datasets. Theoretically, we demonstrate that the concentration and separation of features play crucial roles in generalization, supporting empirical results in the literature. The code is available at https://github.com/chingyaoc/kV-Margin. 1

Introduction
Motivated by the remarkable empirical success of deep learning, there has been signiﬁcant effort in statistical learning theory toward deriving generalization error bounds for deep learning, i.e complexity measures that predict the gap between training and test errors. Recently, substantial progress has been made, e.g., [3, 4, 7, 13, 21, 43, 56]. Nevertheless, many of the current approaches lead to generalization bounds that are often vacuous or not consistent with empirical observations
[14, 25, 38].
In particular, Jiang et al. [25] present a large scale study of generalization in deep networks and show that many existing approaches, e.g., norm-based bounds [4, 42, 43], are not predictive of generalization in practice. Recently, the Predicting Generalization in Deep Learning (PGDL) competition described in [26] sought complexity measures that are predictive of generalization error given training data and network parameters. To achieve a high score, the predictive measure of generalization had to be robust to different hyperparameters, network architectures, and datasets.
The participants [32, 40, 48] achieved encouraging improvement over the classic measures such as
VC-dimension [54] and weight norm [4]. Unfortunately, despite the good empirical results, these proposed approaches are not yet supported by rigorous theoretical bounds.
In this work, we attempt to decrease this gap between theory and practice with margin bounds based on optimal transport. In particular, we show that the expected optimal transport cost of matching two independent random subsets of the training distribution is a natural alternative to Rademacher complexity. Interestingly, this optimal transport cost can be interpreted as the k-variance [49], a generalized notion of variance that captures the structural properties of the data distribution. Applied to latent space, it captures important properties of the learned feature distribution. The resulting k-variance normalized margin bounds can be easily estimated and correlate well with the generalization error on the PGDL datasets [26]. In addition, our formulation naturally encompasses the gradient 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
normalized margin proposed by Elsayed et al. [15], further relating our bounds to the decision boundary of neural networks and their robustness.
Theoretically, our bounds reveal that the concentration and separation of learned features are important factors for the generalization of multiclass classiﬁcation. In particular, the downstream classiﬁer generalizes well if (1) the features within a class are well clustered, and (2) the classes are separable in the feature space in the Wasserstein sense.
In short, this work makes the following contributions:
• We develop new margin bounds based on k-variance [49], a generalized notion of variance based on optimal transport, which better captures the structural properties of the feature distribution;
• We propose k-variance normalized margins that predict generalization error well on the PGDL challenge data;
• We provide a theoretical analysis to shed light on the role of feature distributions in generalization, based on our k-variance normalized margin bounds. 2