Abstract
Deep ensembles excel in large-scale image classiﬁcation tasks both in terms of prediction accuracy and calibration. Despite being simple to train, the computation and memory cost of deep ensembles limits their practicability. While some recent works propose to distill an ensemble model into a single model to reduce such costs, there is still a performance gap between the ensemble and distilled models.
We propose a simple approach for reducing this gap, i.e., making the distilled performance close to the full ensemble. Our key assumption is that a distilled model should absorb as much function diversity inside the ensemble as possible. We ﬁrst empirically show that the typical distillation procedure does not effectively transfer such diversity, especially for complex models that achieve near-zero training error.
To ﬁx this, we propose a perturbation strategy for distillation that reveals diversity by seeking inputs for which ensemble member outputs disagree. We empirically show that a model distilled with such perturbed samples indeed exhibits enhanced diversity, leading to improved performance. 1

Introduction
Deep Ensemble (DE) [Lakshminarayanan et al., 2017], a simple method to ensemble the same model trained multiple times with different random initializations, is considered to be a competitive method for various tasks involving deep neural networks both in terms of prediction accuracy and uncertainty estimation. Several works have tried to reveal the secret behind DE’s effectiveness. As stated by
Duvenaud et al. [2016] and Wilson and Izmailov [2020], DE can be considered as an approximate
Bayesian Model Average (BMA) procedure. Fort et al. [2019] studied the loss landscape of DEs and showed that the effectiveness comes from the diverse modes reached by ensemble members, making it well suited for approximating BMA. It is quite frustrating that most sophisticated approximate
Bayesian inference algorithms, especially the ones based on variational approximations, are not as effective as DEs in terms of exploring various modes in parameter spaces.
Despite being simple to train, DE incurs signiﬁcant overhead in inference time and memory usage. It is therefore natural to develop a way to reduce such costs. An example of such a method is Knowledge
Distillation (KD) [Hinton et al., 2015], which transfers knowledge from a large teacher network to a relatively smaller student network. The student network is trained with a loss that encourages it to copy the outputs of the teacher networks evaluated at the training data. With KD, there have been several works that learn a single student network by distilling from DE teacher networks. A naïve approach would be to directly distill ensembled outputs of DE teachers to the single student network.
A better way proven to be more effective is to set up a student network having multiple subnetworks (multiple heads [Tran et al., 2020] or rank-one factors [Mariet et al., 2020]) and distill the outputs of each ensemble member to each subnetwork in a one-to-one fashion. Nevertheless, the empirical performance of such distilled networks is still far inferior to DE teachers.
∗ Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: It shows randomly selected train examples from CIFAR-10 and their corresponding predictions given by 8 ResNet-32 models. In the second row, the vertical axis denotes the ensemble members, and the horizontal axis denotes the class index.
There may be many reasons why the students are not doing as well as DE teachers. We argue that a critical limitation of current distillation schemes is that they do not effectively transfer the diversity of DE teacher predictions to students. Consider an ensemble of deep neural networks trained for an image classiﬁcation task. Given sufﬁcient network capacity, each ensemble member is likely to achieve near-zero training error, meaning that the resulting outputs of ensemble members evaluated at the training set will be nearly identical, as shown in Fig. 1. In such a situation, reusing the training data during distillation will not encourage the student to produce diverse predictions. This is quite critical, considering various results establishing that the effectiveness of DE comes from averaging out its diverse predictions.
To this end, we propose a method that ampliﬁes the diversity of students learning from DE teachers.
Our idea is simple: instead of using the same training set, use a perturbed training set for which the predictions of ensemble members are likely to be diversiﬁed. To implement this idea, we employ
Output Diversiﬁed Sampling (ODS) [Tashiro et al., 2020], a sampling scheme that ﬁnds small input perturbations that result in signiﬁcant changes in outputs. Empirically, we conﬁrm that the inputs perturbed with ODS result in large deviations in the DE teacher outputs. We further justify our method by analyzing the role of ODS perturbation. Speciﬁcally, we show that distilling using training data perturbed with ODS can be interpreted as an approximate Jacobian matching procedure where ODS improves the sample efﬁciency of the approximate Jacobian matching. Intuitively, by approximately matching Jacobians of teachers and students, we are transferring the outputs of teacher networks evaluated not only on the training data points but also on nearby points.
Using standard image classiﬁcation benchmarks, we empirically validate that our distillation method promotes diversities in student network predictions, leading to improved performance, especially in terms of uncertainty estimation. 2