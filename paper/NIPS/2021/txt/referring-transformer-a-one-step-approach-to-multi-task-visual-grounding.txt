Abstract
As an important step towards visual reasoning, visual grounding (e.g., phrase localization, referring expression comprehension / segmentation) has been widely explored. Previous approaches to referring expression comprehension (REC) or segmentation (RES) either suffer from limited performance, due to a two-stage setup, or require the designing of complex task-speciﬁc one-stage architectures.
In this paper, we propose a simple one-stage multi-task framework for visual grounding tasks. Speciﬁcally, we leverage a transformer architecture, where two modalities are fused in a visual-lingual encoder. In the decoder, the model learns to generate contextualized lingual queries which are then decoded and used to directly regress the bounding box and produce a segmentation mask for the corresponding referred regions. With this simple but highly contextualized model, we outperform state-of-the-art methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model beneﬁts greatly from contextualized information and multi-task training. 1

Introduction
Multi-modal grounding1 tasks (e.g., phrase localization [1, 3, 9, 41, 48], referring expression compre-hension [17, 19, 24, 26, 29, 30, 37, 51, 52, 55, 56] and segmentation [6, 18, 20, 21, 29, 38, 53, 56]) aim to generalize traditional object detection and segmentation to localization of regions (rectangular or at a pixel level) in images that correspond to free-form linguistic expressions. These tasks have emerged as core problems in vision and ML due to the breadth of applications that can make use of such techniques, spanning image captioning, visual question answering, visual reasoning and others.
The majority of multi-modal grounding architectures, to date, take the form of two-stage approaches, inspired by Faster RCNN [44] and others, which ﬁrst generate a set of image region proposals and then associate/ground one, or more, of these regions to a phrase by considering how well the content matches the query phrase. Context among the regions and multiple query phrases, which often come parsed from a single sentence, has also been considered in various ways (e.g., using LSTM stacks
[9], graph neural networks [1] and others). More recent variants leverage pre-trained multi-modal
Transformers (e.g., ViLBERT [33, 34]) to ﬁne-tune to the grounding tasks. Such models have an added beneﬁt of being able to learn sophisticated cross-modal feature representations from external 1Grounding and referring expression comprehension have been used interchangeably in the literature. While the two terms are indeed trying to characterize the same task of associating a lingual phrase with an image region, there is a subtle difference in that referring expressions tend to be unique and hence need to be grounded to a single region, e.g., “person in a red coat next to a bus". The grounding task, as originally deﬁned in [41], is more general where a lingual phrase may be ambiguous and therefore grounded to multiple regions, e.g., “a person". 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
large-scale data, which further improve the performance. However, a signiﬁcant limitation of all such two-stage methods is their inability to condition the proposal mechanism on the query phrase itself, which inherently limits the upper bound of performance (see Table 3 in [51]).
To address these limitations, more recently, a number of one-stage approaches have been introduced
[22, 36, 51, 52]. Most of these take inspiration from Yolo [43] and the variants, and rely on more integrated visual-linguistic fusion and a dense anchoring mechanism to directly predict the grounding regions. While this alleviates the need for a proposal stage, it instead requires somewhat ad hoc anchor deﬁnitions, often obtained by clustering of labeled regions, and also limits ability to contextualize grounding decisions as each query phrase is effectively processed independently. Finally, little attention in the literature has been given to leveraging relationship among the REC and RES tasks.
In this work we propose an end-to-end one-stage architecture, inspired by the recent DETR [2] detection framework, which is capable of simultaneous language grounding at both a bounding-box and segmentation level, without requiring dense anchor deﬁnitions. This model also enables contextualized reasoning by taking into account the entire image, all referring query phrases of interest and (optionally) lingual context (e.g., a sentence from which referring phrases are parsed).
Speciﬁcally, we leverage a transfomer architecture, with a visual-lingual encoder, to encode image and lingual context, and a two-headed (detection and segmentation) custom contextualized tranformer decoder. The contextualized decoder takes as input learned contextualized phrase queries and decodes them directly to bounding boxes and segmentation masks. Implicit 1-to-1 correspondence between input referring phrases and resulting outputs also enables a more direct formulation of the loss without requiring Hungarian matching. With this simple model we outperform state-of-the-art methods by a large margin on both REC and RES tasks. We also show that a simple pre-training schedule (on an external dataset) further improves the performance. Extensive experiments and ablations illustrate that our model beneﬁt greatly from the contextualized information and the multi-task training.
Contributions. Our contributions are: (1) We propose a simple and general one-stage transformer-based architecture for referring expression comprehension and segmentation. The core of this model is the novel transformer decoder that leverages contextualized phrase queries and is able to directly decode those, subject to contextualized image embeddings, into corresponding image regions and segments; (2) Our approach is unique in enabling simultaneous REC and RES using a single trained model (the only other method capable of this is [36]); showing that such multi-task learning leads to improvements on both tasks; (3) As with other transformer-based architectures, we show that pre-training can further improve the performance and both vanila and pre-trained models outperform state-of-the-art on both tasks by signiﬁcant margins (up to 8.5% on RefCOCO dataset for REC and 19.4% for RES). We also thoroughly validate our design in detailed ablations. 2