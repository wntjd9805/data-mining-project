Abstract
Motivated by applications to resource-limited and safety-critical domains, we study selective classiﬁcation in the online learning model, wherein a predictor may abstain from classifying an instance. For example, this may model an adaptive decision to invoke more resources on this instance. Two salient aspects of the setting we consider are that the data may be non-realisable, due to which abstention may be a valid long-term action, and that feedback is only received when the learner abstains, which models the fact that reliable labels are only available when the resource intensive processing is invoked.
Within this framework, we explore strategies that make few mistakes, while not abstaining too many times more than the best-in-hindsight error-free classiﬁer from a given class. That is, the one that makes no mistakes, while abstaining the fewest number of times. We construct simple versioning-based schemes for any µ ∈ (0, 1], that make most T µ mistakes while incurring ˜O(T 1−µ) excess abstention against adaptive adversaries. We further show that this dependence on
T is tight, and provide illustrative experiments on realistic datasets. 1

Introduction
Consider a low-power or battery-limited edge device, such as a sensor or a smart-speaker that receives a stream of classiﬁcation requests. Due to the resource limitations, such a device cannot implement modern models that are needed for accurate decisions. Instead the device has access (e.g. via an internet connection) to an accurate but resource-intensive model implemented on a cloud server, and may send queries to the cloud server in order to retain accuracy. Of course, this incurs costs such as latency and battery drain due to communication. The ideal operation of such a device should thus be to learn a rule that classiﬁes ‘easy’ instances locally, while sending harder ones to the cloud, thus maintaining accuracy whilst minimising the net resource consumption [Xu+14; NS17].
Selective classiﬁcation [Cho57; Cho70] is a classical paradigm of relevance to such settings. The setup allows a predictor to abstain from classifying some instances (without incurring a mistake). This abstention models adaptive decisions to invoke more resource-intensive methods on subtle cases, like in the above example. The solution concept is relevant widely - for instance, it is relevant to adaptively recommending further (and costly) tests rather than offering a diagnosis in a medical scenario, or to recommending a human review instead of an alarm-or-not decision in security contexts. Two aspects of such settings are of particular interest to us. Firstly, the cheaper methods are typically not sufﬁcient to realise the true labels, due to which abstention may be a long-term necessity. Secondly, a-priori reliable labels can only be obtained by invoking the resource intensive option, and thus feedback on whether a non-abstaining decision was correct is unavailable. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We propose online selective classiﬁcation, with an emphasis on ensuring very few mistakes, to account for the need for very accurate decisions. Concretely, an adversary sequentially produces contexts and labels (Xt, Yt), and the learner uses the Xts to produce a decision (cid:98)Yt that may either be one of K classes, or an abstention, which we represent as ⊥. Feedback in the form of Yt is provided if and only if (cid:98)Yt = ⊥, and the learner incurs a mistake if (cid:98)Yt was non-abstaining and did not equal Yt.
With the emphasis on controlling the total number of mistakes, we study regrets achievable when compared to the behaviour of the best-in-hindsight error-free selective classiﬁer from a given class -that is, one that makes no mistakes, while abstaining the fewest number of times. Notice that our situation is non-realisable, and therefore this competitor may abstain in the long-run. The two metrics of importance here are the number of mistakes the learner makes, and its excess abstention over this competitor. An effective learner must control both abstention and mistakes, and it is not enough to make one small, e.g. a learner that makes a lot of mistakes but incurs a very negative excess abstention is no good. This simultaneous control of two regrets raises particular challenges.
We construct a simple scheme that, when competing against ﬁnite classes, simultaneously guarantees
O(T µ) mistakes and O(T 1−µ) excess abstentions against adaptive adversaries (for any µ ∈ [0, 1]), and show that these rates are Pareto-tight [OR94]. We further show that against stochastic adversaries, the same rates can be attained with improved dependence of the regret bounds on the size of the class, and we also describe schemes that enjoy similar improvements against adaptive adversaries, but at the cost of the T -dependence of the regret bounds. The main schemes randomly abstain at a given rate in order to gain information, and otherwise play (cid:98)Yt consistent with the ‘version space’ of classiﬁers that have not been observed to make mistakes. For the adversarial case, the analysis of the scheme relies on a new ‘adversarial uniform law of large numbers’(ALLN) to argue that such methods cannot incur too many mistakes. This ALLN uses a self-normalised martingale concentration bound, and further yields an adaptive continuous approximation guarantee for the Bernoulli-sampling sketch in the sense of Ben-Eliezer & Yogev [BY20; Alo+21]. The theoretical exploration is complemented by illustrative experiments that implement our scheme on two benchmark datasets. 1.1