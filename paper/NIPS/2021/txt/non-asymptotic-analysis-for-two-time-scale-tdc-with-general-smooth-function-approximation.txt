Abstract
Temporal-difference learning with gradient correction (TDC) is a two time-scale algorithm for policy evaluation in reinforcement learning. This algorithm was initially proposed with linear function approximation, and was later extended to the one with general smooth function approximation. The asymptotic convergence for the on-policy setting with general smooth function approximation was established in [Bhatnagar et al., 2009], however, the non-asymptotic convergence analysis remains unsolved due to challenges in the non-linear and two-time-scale update structure, non-convex objective function and the projection onto a time-varying tangent plane. In this paper, we develop novel techniques to address the above chal-lenges and explicitly characterize the non-asymptotic error bound for the general off-policy setting with i.i.d. or Markovian samples, and show that it converges as
T ) (up to a factor of O(log T )). Our approach can be applied to a fast as O(1/ wide range of value-based reinforcement learning algorithms with general smooth function approximation.
√ 1

Introduction
In reinforcement learning (RL), an agent interacts with a stochastic environment in order to maximize the total reward [Sutton and Barto, 2018]. Towards this goal, it is often needed to evaluate how good a policy performs, and more speciﬁcally, to learn its value function. Temporal difference (TD) learning algorithm is one of the most popular policy evaluation approaches. However, when applied with function approximation approach and/or under the off-policy setting, the TD learning algorithm may diverge [Baird, 1995, Tsitsiklis and Van Roy, 1997]. To address this issue, a family of gradient-based
TD (GTD) algorithms, e.g., GTD, GTD2, temporal-difference learning with gradient correction (TDC) and Greedy-GQ, were developed for the case with linear function approximation [Maei, 2011,
Sutton et al., 2009b, Maei et al., 2010, Sutton et al., 2009a,b]. These algorithms were later extended to the case with general smooth function approximation in [Bhatnagar et al., 2009], where asymptotic convergence guarantee was established for the on-policy setting with i.i.d. samples.
Despite the success of the GTD methods in practice, previous theoretical studies only showed that these algorithms converge asymptotically, and did not suggest how fast these algorithms converge and how the accuracy of the solution depends on various parameters of the algorithms. Not until 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
recently have the non-asymptotic error bounds for these algorithms been investigated, e.g., [Dalal et al., 2020, Karmakar and Bhatnagar, 2018, Wang and Zou, 2020, Xu et al., 2019, Kaledin et al., 2020, Dalal et al., 2018, Wang et al., 2017], which mainly focus on the case with linear function approximation. These results thus cannot be directly applied to more practical applications with general smooth function approximation, e.g., neural networks, which have greater representation power, do not need to construct feature mapping, and are widely used in practice.
In this paper, we develop a non-asymptotic analysis for the TDC algorithm with general smooth function approximation (which we refer to as non-linear TDC) for both i.i.d. and Markovian samples.
Technically, the analysis in this paper is not a straightforward extension of previous studies on those
GTD algorithms with linear function approximation. First of all, different from existing studies with linear function approximation whose objective functions are convex and the updates are linear, the objective function of the non-linear TDC algorithm is non-convex, and the two time-scale updates are non-linear functions of the parameters. Second, the objective function of the non-linear TDC algorithm, the mean-square projected Bellman error (MSPBE), involves a projection onto a time-varying tangent plane which depends on the sample trajectory, whereas for GTD algorithms with linear function approximation, this projection is time-invariant. Third, due to the two time-scale structure of the algorithm and the Markovian noise, novel techniques to deal with the stochastic bias and the tracking error need to be developed. 1.1 Challenges and Contributions
In this section, we summarize the technical challenges and our contributions.
Analysis for two time-scale non-linear updates and non-convex objective. Unlike many existing results on two time-scale stochastic approximation, e.g., [Konda et al., 2004, Gupta et al., 2019,
Kaledin et al., 2020] and the studies of linear GTD algorithms in [Xu et al., 2019, Ma et al., 2020,
Wang et al., 2017, Dalal et al., 2020], the objective function of the non-linear TDC is non-convex, and its two time-scale updates are non-linear. Therefore, existing studies on linear two time-scale algorithms cannot be directly applied. Moreover, the convergence to global optimum cannot be guaranteed for the non-linear TDC algorithm, and therefore, we study the convergence to stationary points. In this paper, we develop a novel non-asymptotic analysis of the non-linear TDC algorithm, which solves RL problems from a non-convex optimization perspective. We note that our analysis is not a straightforward extension of analyses of non-convex optimization, as the update rule here is two time-scale and the noise is Markovian. The framework we develop in this paper can be applied to analyze a wide range of value-based RL algorithms with general smooth function approximation.
Time-varying projection. For the MSPBE, a projection of the Bellman error onto the parameterized function class is involved. However, unlike linear function approximation, the projection onto a general smooth class of functions usually does not have a closed-form solution. Thus, a projection onto the tangent plane at the current parameter is used instead, which incurs a time-varying projection that depends on the current parameter and thus the sample trajectory. This brings in additional challenges in the bias and variance analysis due to such dependency. We develop a novel approach to decouple such a dependency and characterize the bias by exploiting the uniform ergodicity of the underlying MDP and the smoothness of the parameterized function. The new challenges posed by the time-varying projection and the dependence between the projection and the sample trajectory are not special to the non-linear TDC investigated in this paper, and they exist in a wide range of value-based algorithms with general smooth function approximation, where our techniques can be applied.
A tight tracking error analysis. Due to the two time-scale structure of the update rule, the tracking error, which measures how fast the fast time-scale tracks its own limit, needs to be explicitly bounded.
Unlike the studies on two time-scale linear stochastic approximation [Dalal et al., 2020, Kaledin et al., 2020, Konda et al., 2004], where a linear transformation can asymptotically decouple the dependence between the fast and slow time-scale updates, it is non-trivial to construct such a transformation for non-linear updates. To develop a tight bound on the tracking error, we develop a novel technique that bounds the tracking error as a function of the gradient of the MSPBE. This leads to a tighter bound on the tracking error compared to many existing works on two time-scale analysis, e.g., [Wu et al., 2020, Hong et al., 2020]. Although we do not decouple the fast and slow time-scale updates, we still obtain a desired convergence rate of O(1/
T ) (up to a factor of log T ), which matches with the complexity of stochastic gradient descent for non-convex problems [Ghadimi and Lan, 2013].
√ 2
1.2