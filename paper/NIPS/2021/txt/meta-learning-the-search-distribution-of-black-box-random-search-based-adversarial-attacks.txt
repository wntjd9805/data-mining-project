Abstract
Adversarial attacks based on randomized search schemes have obtained state-of-the-art results in black-box robustness evaluation recently. However, as we demonstrate in this work, their efﬁciency in different query budget regimes depends on manual design and heuristic tuning of the underlying proposal distributions. We study how this issue can be addressed by adapting the proposal distribution online based on the information obtained during the attack. We consider Square Attack, which is a state-of-the-art score-based black-box attack, and demonstrate how its performance can be improved by a learned controller that adjusts the parameters of the proposal distribution online during the attack. We train the controller using gradient-based end-to-end training on a CIFAR10 model with white box access. We demonstrate that plugging the learned controller into the attack consistently improves its black-box robustness estimate in different query regimes by up to 20% for a wide range of different models with black-box access. We further show that the learned adaptation principle transfers well to the other data distributions such as CIFAR100 or ImageNet and to the targeted attack setting1. 1

Introduction
It was demonstrated that despite their impressive performance in various tasks, neural networks are susceptible to small imperceptible perturbations in the input called adversarial examples [4]. This is a concerning issue for real-world deployment of deep learning approaches, especially in safety-critical domains such as autonomous driving [5]. But apart from practical concerns, adversarial examples serve as a tool to better understand the true nature of artiﬁcial neural networks and capture their inherent properties and differences from their biological counterparts [6].
Evaluating adversarial robustness is usually formulated as a constrained optimization problem [4].
However, ﬁnding the exact solution is typically intractable [7, 8]. Therefore, in practice one often resorts to approximate methods called adversarial attacks that try to ﬁnd adversarial examples within a small number of iterations. A number of different adversarial attacks were proposed [8–10] that can be distinguished into white- and black-box attacks. In white-box attacks, one assumes full access to the model architecture and weights [8]. However, as white-box attacks typically rely on the gradient, gradient obfuscation, which does not eliminate the existence of adversarial examples but makes it signiﬁcantly harder to ﬁnd them gradient-based, can be a severe obstacle for certain attacks [11]. 1The code is available at https://github.com/boschresearch/meta-rs 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) T = 500 queries (b) T = 5000 queries
Figure 1: (Left) The schedules for SA [1] (which scales accordingly for different query budgets) and
AA [2] compared to Meta Square Attack (MSA) proposed in this work. MSA adapts the update size during the course of the attack for each image. We illustrate the mean and the percentile range for a given set of attacked images. (Right) The maximization of the loss for the model of Ding et al. [3] on 100 CIFAR10 images. MSA outperforms SA and AA signiﬁcantly in terms of the achieved loss.
Therefore, black-box attacks that use only limited information from the model such as the class scores or the ﬁnal decision often provide an additional perspective on the true robustness of the model
[12–21] and are thus recommended for reliable robustness evaluation [2, 22].
A very promising direction in the ﬁeld of black-box adversarial attacks are randomized search schemes for crafting adversarial examples [1, 23, 24]. Combining random search with speciﬁc update proposal distributions allows to achieve state-of-the-art black-box efﬁciency for different threat models such as (cid:96)∞ and (cid:96)2 [1], (cid:96)1 [25], (cid:96)0, adversarial patches, and adversarial frames [24].
Despite the conceptual simplicity of these methods, the main disadvantage of random search based methods is that the construction of a suitable proposal distribution requires signiﬁcant manual design and is crucial for competitive performance (see also Figure 1).
In this work, we propose a method that allows to circumvent the ﬁne-tuning and reduce the amount of manual design in random search based attacks. We use gradient-based meta-learning to automatically optimize controllers for schedules and proposal distribution on models with white-box access. After meta-training, the controllers can be plugged into a random search attack substituting manually designed schedules and proposal distributions. Importantly, once meta-trained, the controllers do not require any gradient access and can thus be used in a fully black-box setting and without being affected by gradient obfuscation. We consider the proposed methodology for the case of Square
Attack for the (cid:96)∞ and (cid:96)2 threat models [1]. We meta-train controllers for update size and color on an adversarially trained [26] ResNet18 [27] model with white-box access and apply them to many different models from the RobustBench model zoo [28] that we treat as black-boxes. Since query efﬁciency is of crucial importance in black-box adversarial attacks, we also study the method for different query regimes ranging from several hundreds to several thousands. Depending on the query regime and the attacked model we obtain up to 20% improvement with respect to the baseline schedules proposed by Andriushchenko et al. [1] and Croce and Hein [2]. 2
In short, we make the following contributions:
• We frame adversarial attack optimization as a meta-learning problem (Section 3.2).
• We formalize the gradient-based meta-learning for the state-of-the-art Square Attack and propose Meta Square Attack (Section 3.3).
• We meta-train Meta Square Attack (MSA) on a CIFAR10 [48] model with white-box access and show that MSA improves robust accuracy by up to 5.6% on a vast range of CI-FAR10 models with black-box access with respect to the hand-designed search distributions proposed in previous work [1, 2] for the (cid:96)∞ and (cid:96)2 threat models (Section 4).
• We show that Meta Square Attack generalizes well to different datasets and to the targeted attack setting. It achieves up to 20% better robust accuracy compared to the state-of-the art baseline [1] for attacking models on CIFAR100 and ImageNet (Section 4.2). 2