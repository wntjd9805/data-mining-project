Abstract
Gradient-based meta-learning and hyperparameter optimization have seen signifi-cant progress recently, enabling practical end-to-end training of neural networks together with many hyperparameters. Nevertheless, existing approaches are rel-atively expensive as they need to compute second-order derivatives and store a longer computational graph. This cost prevents scaling them to larger network architectures. We present EvoGrad, a new approach to meta-learning that draws upon evolutionary techniques to more efficiently compute hypergradients. Evo-Grad estimates hypergradient with respect to hyperparameters without calculating second-order gradients, or storing a longer computational graph, leading to signifi-cant improvements in efficiency. We evaluate EvoGrad on three substantial recent meta-learning applications, namely cross-domain few-shot learning with feature-wise transformations, noisy label learning with Meta-Weight-Net and low-resource cross-lingual learning with meta representation transformation. The results show that EvoGrad significantly improves efficiency and enables scaling meta-learning to bigger architectures such as from ResNet10 to ResNet34. 1

Introduction
Gradient-based meta-learning and hyperparameter optimization have been of long-standing interest in neural networks and machine learning [11, 21, 4]. Hyperparameters (aka meta-parameters) can take diverse forms, especially under the guise of meta-learning, where there has recently been an explosion of successful applications addressing diverse learning challenges [9]. For example to name just a few: training optimizer initial condition in support of few-shot learning [7, 1, 15]; training instance-wise weights for cleaning noisy datasets [31, 26]; training loss functions in support of generalisation [14] and learning speed; and training stochastic regularizers in support of cross-domain robustness [33].
Most of these applications share the property that meta-parameters impact validation loss only indirectly through their effect on model parameters, and so computing validation loss gradients with respect to meta-parameters usually leads to the need to compute second-order derivatives, and store longer computational graphs for backpropagation. This eventually becomes a bottleneck to execution time, and – more severely – to scaling the size of the underlying models, given the practical limitation of GPU memory. There has been steady progress in the development of diverse practical algorithms for computing validation loss with respect to meta-parameters [20, 19, 21]. Nevertheless they mostly share some form of the aforementioned limitations. In particular, the majority of recent successful practical applications [31, 33, 14, 2, 5, 17, 30] essentially use some variant of the T1 − T2 algorithm
[20] to estimate the gradient ∂ℓV
∂λ of validation loss w.r.t. hyperparameters. This approach computes
∂2ℓT the gradient online at each step of updating the base model θ, and estimates it as ∂ℓV
∂θ∂λ , for training loss ℓT . As with many alternative estimators, this requires second-order derivatives, and extending the computational graph. Besides the additional computation cost, this limits the size of the
∂λ ≈ ∂ℓV
∂θ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
base model that can be used in a given GPU, since the memory cost of meta-learning is now multiple times the size of vanilla backpropagation. This in turn prevents the application of meta-learning to problems where large state-of-the-art model architectures are required.
To address this issue, we draw inspiration from evolutionary optimization methods [27] to develop
EvoGrad, a meta-gradient algorithm that requires no higher-order derivatives and as such is sig-nificantly faster and lighter than the standard approaches. In particular, we take the novel view of estimating meta-gradients via a putative inner-loop evolutionary update to the base model. As this requires no gradients itself, the meta-gradient can then be computed using first-order gradients alone, and without extending the computational graph – leading to efficient hyperparameter updates.
Meanwhile for efficient and accurate base model learning, the real inner-loop update can separately be carried out by conventional gradient descent.
Our EvoGrad is a general meta-optimizer applicable to many meta-learning applications, among which we choose three to demonstrate its impact: the LFT model [33] observes that a properly tuned stochastic regularizer can significantly improve cross-domain few-shot learning performance. We show that by training those regularizer parameters with EvoGrad, rather than the standard second-order approach, we can obtain the same improvement in accuracy with significant reduction in time and memory cost. This allows us to scale LFT from the original ResNet10 to ResNet34 within a 12GB
GPU. Second, the Meta-Weight-Net (MWN) [31] model deals with label noise by meta-learning an auxiliary network that re-weights instance-wise losses to down-weight noisy instances and improve validation loss. We also show that EvoGrad can replicate MWN results with significant cost savings.
Third, we demonstrate the benefits of EvoGrad on an application from NLP, in addition to the ones from computer vision: low-resource cross-lingual learning using MetaXL approach [35].
To summarize, our main contributions are: (1) We introduce EvoGrad, a novel method for gradient-based meta-learning and hyperparameter optimization that is simple to implement and efficient in time and memory requirements. (2) We evaluate EvoGrad on a variety of illustrative and substantial meta-learning problems, where we demonstrate significant compute and memory benefits compared to standard second-order approaches. (3) In particular, we illustrate that EvoGrad allows us to scale meta-learning to bigger models than was previously possible on a given GPU size, thus bringing meta-learning closer to the state-of-the-art frontier of real applications. We provide source code for
EvoGrad at https://github.com/ondrejbohdal/evograd. 2