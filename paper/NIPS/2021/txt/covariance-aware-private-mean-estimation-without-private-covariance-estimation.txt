Abstract
We present two sample-efﬁcient differentially private mean estimators for d-dimensional (sub)Gaussian distributions with unknown covariance. Informally, given n (cid:38) d/α2 samples from such a distribution with mean µ and covariance Σ, our estimators output ˜µ such that (cid:107)˜µ − µ(cid:107)Σ ≤ α, where (cid:107) · (cid:107)Σ is the Mahalanobis distance. All previous estimators with the same guarantee either require strong a priori bounds on the covariance matrix or require Ω(d3/2) samples.
Each of our estimators is based on a simple, general approach to designing differ-entially private mechanisms, but with novel technical steps to make the estimator private and sample-efﬁcient. Our ﬁrst estimator samples a point with approximately maximum Tukey depth using the exponential mechanism, but restricted to the set of points of large Tukey depth. Proving that this mechanism is private requires a novel analysis. Our second estimator perturbs the empirical mean of the data set with noise calibrated to the empirical covariance, without releasing the covariance itself. Its sample complexity guarantees hold more generally for subgaussian dis-tributions, albeit with a slightly worse dependence on the privacy parameter. For both estimators, careful preprocessing of the data is required to satisfy differential privacy. 1

Introduction
Although the goal of statistics and machine learning is to infer properties of a population, there is a growing awareness that many statistical estimators and trained models reveal a concerning amount of information about their data set, which leads to signiﬁcant concerns about the privacy of the individuals who have contributed sensitive information to that data set. These privacy violations have been demonstrated repeatedly via reconstruction attacks [21, 30, 27, 46], membership-inference attacks [37, 52, 10, 32, 53, 59], and instances of unwanted memorization of training data [13, 14, 34, 7]. In order to realize the beneﬁts of analyzing sensitive data sets, it is crucial to develop statistical 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
estimators and machine learning algorithms that make accurate inferences about the population but also protect the privacy of the individuals who contribute data.
In this work we study statistical estimators that satisfy a condition called differential privacy [29], which has become the standard criterion for individual privacy in statistics and machine learning. In-formally, a differentially private algorithm guarantees that no attacker, regardless of their background knowledge or resources, can infer much more about any individual than they could have learned had that individual never contributed to the data set [45]. A long body of work shows that differential privacy is compatible with a wide range of tasks in statistics and machine learning, and it is now seeing deployment at companies like Google [33, 6, 57], Apple [3], Facebook [55] and LinkedIn [51], as well as statistical agencies like the U.S. Census Bureau [1, 36].