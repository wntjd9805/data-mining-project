Abstract
Recent theoretical work studies sample-efﬁcient reinforcement learning (RL) ex-tensively in two settings: learning interactively in the environment (online RL), or learning from an ofﬂine dataset (ofﬂine RL). However, existing algorithms and theories for learning near-optimal policies in these two settings are rather different and disconnected. Towards bridging this gap, this paper initiates the theoretical study of policy ﬁnetuning, that is, online RL where the learner has additional access to a “reference policy” µ close to the optimal policy π(cid:63) in a certain sense. We con-sider the policy ﬁnetuning problem in episodic Markov Decision Processes (MDPs) with S states, A actions, and horizon length H. We ﬁrst design a sharp ofﬂine reduction algorithm—which simply executes µ and runs ofﬂine policy optimization on the collected dataset—that ﬁnds an ε near-optimal policy within (cid:101)O(H 3SC (cid:63)/ε2) episodes, where C (cid:63) is the single-policy concentrability coefﬁcient between µ and
π(cid:63). This ofﬂine result is the ﬁrst that matches the sample complexity lower bound in this setting, and resolves a recent open question in ofﬂine RL. We then establish an Ω(H 3S min{C (cid:63), A}/ε2) sample complexity lower bound for any policy ﬁne-tuning algorithm, including those that can adaptively explore the environment. This implies that—perhaps surprisingly—the optimal policy ﬁnetuning algorithm is either ofﬂine reduction or a purely online RL algorithm that does not use µ. Finally, we design a new hybrid ofﬂine/online algorithm for policy ﬁnetuning that achieves better sample complexity than both vanilla ofﬂine reduction and purely online RL algorithms, in a relaxed setting where µ only satisﬁes concentrability partially up to a certain time step. Overall, our results offer a quantitative understanding on the beneﬁt of a good reference policy, and make a step towards bridging ofﬂine and online RL. 1

Introduction
Reinforcement learning (RL)—where agents learn to play sequentially in an environment to maximize a cumulative reward function—has achieved great recent success in many artiﬁcial intelligence challenges such as video games playing [38, 52], large-scale strategy games (e.g. GO) [44, 45], robotic manipulation [3, 32], behavior learning in social scenarios [8], and more. In many such challenging domains, achieving human-like or superhuman performance requires training the RL agent with millions of samples (steps of acting or game playing) or more. Understanding and improving the sample efﬁciency of RL algorithms has been a central topic of research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Sample-efﬁcient RL has been studied in a rich body of theoretical work in two main settings: online
RL, in which the learner has interactive access to the environment and can execute any policy; and ofﬂine RL, in which the learner only has access to an “ofﬂine” dataset collected by executing some (one or many) policies within the environment, and is not allowed to further access the environment. These two settings share some common learning goals such as the sample complexity (number of episodes of playing) for ﬁnding the optimal policy. However, existing algorithms and theories in the online and ofﬂine setting seem rather different and disconnected—In online RL, state-of-the-art sample-efﬁcient algorithms typically explore the entire environment, e.g. by using optimism to encourage visitation to unseen states and actions [9, 27, 19, 41, 21, 5, 22, 12, 23, 53]. In contrast, ofﬂine RL does not allow interactive exploration, and sample-efﬁcient policy optimization algorithms typically focus on optimizing an unbiased (or downward biased) estimator of the value function [39, 48, 4, 40, 10, 56, 35, 58, 25, 42]. It is therefore of interest to ask whether these two types of algorithms and theories can be connected in any way.
Further, on the empirical end, insights and patterns from ofﬂine RL often help as well in designing online RL algorithms and improving the sample efﬁciency in the real world. For example, there are online RL algorithms that alternate between data collection steps using a ﬁxed policy, and policy improvement steps by learning on the collected dataset [20]. The replay buffer in value-based algorithms can also be seen as a local form of ofﬂine (off-policy) policy optimization and are often be used in conjunction with optimistic exploration techniques [38, 18, 49]. The prevalence of these algorithms also offers practical motivations for us to look for a more uniﬁed understanding of online and ofﬂine RL in theory. These reasonings motivate us to ask the following question:
Can we bridge sample-efﬁcient ofﬂine and online RL from a theoretical perspective?
This paper proposes policy ﬁnetuning, a new RL setting that investigates the beneﬁt of a good initial policy in reinforcement learning, and encapsulates challenges of both online and ofﬂine RL. In the policy ﬁnetuning problem, the learner is given interactive access to the environment and asked to learn a near-optimal policy, but in addition has access to a reference policy µ that is good in certain aspects. This setting offers great ﬂexibility for the algorithm design: For example, the algorithm is allowed to either simply collect data from µ and run any ofﬂine policy optimization algorithm on the collected dataset. It is also allowed to play any other policy interactively, including those that adaptively explores the environment. The policy ﬁnetuning problem offers a common playground for both ofﬂine and online types of algorithms, and has a uniﬁed performance metric (sample complexity for ﬁnding the near-optimal policy) for comparing their performance.
We study the policy ﬁnetuning problem theoretically in ﬁnite-horizon Markov Decision Processes (MDPs) with H time steps, S states, and A actions. We summarize our contributions as follows.
• We begin by considering ofﬂine reduction algorithms which simply collect data using the reference policy µ and run an ofﬂine policy optimization algorithm on the collected dataset. This setting equivalent to ofﬂine RL with behavior policy µ, and thus our result translates to a same result for ofﬂine RL as well.
We design an algorithm PEVI-ADV that is able to ﬁnd an ε-optimal policy (for small ε) within (cid:101)O(H 3SC (cid:63)/ε2) episodes of play, where C (cid:63) is the single-policy concentrability coefﬁcient between µ and some optimal policy π(cid:63) (Section 3). This improves over the best existing ofﬂine result by an H 2 factor in the same setting and matches the lower bound (up to log factors), thereby resolving the recent open question of [42] on tight ofﬂine RL under single-policy concentrability.
• Under the same assumption on µ, we establish an Ω(H 3S min {C (cid:63), A}/ε2) sample complexity lower bound for any policy ﬁnetuning algorithm, including those that adaptively explores the environment (Section 4). This implies that the optimal policy ﬁnetuning algorithm is either ofﬂine reduction via PEVI-ADV, or a “purely” online RL algorithm from scratch (such as UCBVI), depending on whether C (cid:63) ≤ A. This comes rather surprising, as it rules out possibilities of combining online exploration and knowledge of µ to further improve the sample complexity over the aforementioned two baselines.
• Finally, we consider policy ﬁnetuning in a more challenging setting where µ only satisﬁes con-centrability up to a certain time step. We design a “hybrid ofﬂine/online” algorithm HOOVI that combines online exploration and ofﬂine data collection, and show that it achieves better sample complexity than both vanilla ofﬂine reduction and purely online algorithms in certain cases (Section 5). This gives a positive example on when such hybrid algorithm designs are beneﬁcial. 2
1.1