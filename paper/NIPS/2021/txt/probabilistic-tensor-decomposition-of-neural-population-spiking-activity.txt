Abstract
The ﬁring of neural populations is coordinated across cells, in time, and across experimental conditions or repeated experimental trials, and so a full understanding of the computational signiﬁcance of neural responses must be based on a separation of these different contributions to structured activity. Tensor decomposition is an approach to untangling the inﬂuence of multiple factors in data that is common in many ﬁelds. However, despite some recent interest in neuroscience, wider appli-cability of the approach is hampered by the lack of a full probabilistic treatment allowing principled inference of a decomposition from non-Gaussian spike-count data. Here, we extend the Pólya-Gamma (PG) augmentation, previously used in sampling-based Bayesian inference, to implement scalable variational inference in non-conjugate spike-count models. Using this new approach, we develop tech-niques related to automatic relevance determination to infer the most appropriate tensor rank, as well as to incorporate priors based on known brain anatomy such as the segregation of cell response properties by brain area. We apply the model to neural recordings taken under conditions of visual-vestibular sensory integration, revealing how the encoding of self- and visual-motion signals is modulated by the sensory information available to the animal. 1

Introduction
Large-scale neural population recording offers a unique window through which to study brain computations that involve the coordinated action of many neurons. Although such rich data sets are increasingly common, the information they contain can only be put to full use once we are able to separate the underlying factors that contribute to the neural activity in a simple and interpretable way.
Rich data sets often ﬁt naturally within a multidimensional array or tensor, and a variety of tensor decomposition techniques are available to identify factorial contributions that shape such data. But whereas tensor decompositions have long been used in ﬁelds such as chemometrics [1] or computer vision [2], until recently their use in neuroscience had been limited to the study of continuous traces like EEG [3], fMRI signals [4] or LFP [5]. Encouragingly, in the last few years, tensor methods have proven useful to segregate the inﬂuence of time dynamics, trial to trial variability [6, 7] or 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Probabilistic tensor decomposition model. (A) Graphical model. (B) The Offset tensor V is constrained to vary along a limited set of dimensions. (C) W is a low rank tensor. experimental condition [8] in neural spike-count data, even though the models employed have not yet provided a full probabilistic treatment appropriate to counts.
Canonical Polyadic (CP) decomposition [9] is a widely used technique in which a tensor is decom-posed into a sum of rank-1 elements. When applied to noisy data, it is usually cast as a least-squares problem and therefore implicitly assumes that noise is normally distributed. Spike counts violate this assumption but alternated-gradient-based optimization methods can be adapted to non-Gaussian likelihood functions. Hong et al. [10] introduced a general framework for Generalized CP (GCP) decomposition that can be applied to Poisson or negative binomial (NB) distributed datasets but it has not been used in neuroscience to our knowledge. Moreover, practical methods have largely focused on point estimates of the decomposition rather than a full probabilistic treatment. As such, they do not provide principled ways to incorporate prior knowledge about the data (such as the (co)location of recorded neurons in the brain), to automatically determine the rank of the observed tensor (although it is a generalization of the matrix rank, tensor rank is not as well behaved and understood [9]), nor to estimate posterior conﬁdence in results.
One approach to the Bayesian treatment of count models is based on augmentation using Pólya-Gamma (PG) variables [11]. This solution enables Gibbs sampling in models which combine
Gaussian latent structure with logistic, Poisson or negative binomial observations, including factor models [12] and generalised-linear regression [11], and it has been used successfully in neuroscience settings [13]. While PG augmentation has also been considered for tensor decomposition [14], these authors recognised the computational challenges posed by Gibbs sampling in typical size problems, and so focused primarily on point estimation while incorporating a limited range of priors.
Here, we introduce a variational approach to PG-augmented models with negative-binomial obser-vations, which enables a relatively efﬁcient Variational Bayesian (VB) treatment. Our approach is based on a principled approximation of the PG cross entropy, which is especially well suited for neural datasets in which conditional Fano factors (FF) (once the contribution of population-wide shared inﬂuences is discounted) are often close to one [15, 16]. The fully probabilistic formulation is able to handle missing observations, extending to situations where data from multiple animals or recording sessions are to be combined (sometimes referred to as “stitching” [17]). VB also makes it possible to combine Automatic Relevance Determination (ARD) with knowledge about expected group structures based on anatomical or histological information about the recorded neurons (brain area, morphology, protein expression etc.). Finally, we also augment standard CP decomposition models using a constrained offset tensor, which allows us to study modulation of neural activity around baseline values, improving readability and interpretability of the CP-factors.
The paper is organised as follows: In section 2 we review background material on tensor decom-position and PG augmentation schemes. In section 3, we derive a VB algorithm for approximate inference of the tensor factors and model parameters. Last, in section 4, we analyse synthetic data and neural recordings in mice performing passive multisensory integration [18] and show that our method can estimate the population-level effects of temporal dynamics and experimental condition in a fully probabilistic manner. We show improvements in variance explained, deviance and, last but not least, decomposition robustness when compared to standard CP and GCP baselines. 2
2