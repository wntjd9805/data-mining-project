Abstract
Efﬁciently approximating local curvature information of the loss function is a key tool for optimization and compression of deep neural networks. Yet, most existing methods to approximate second-order information have high computational or storage costs, which limits their practicality.
In this work, we investigate matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessian by the empirical Fisher matrix. We propose two new algorithms: the ﬁrst is tailored towards network compression and can compute the IHVP for dimension d, if the Hessian is given as a sum of m rank-one matrices, using O(dm2) precomputation, O(dm) cost for computing the IHVP, and query cost O(m) for any single element of the inverse
Hessian. The second algorithm targets an optimization setting, where we wish to compute the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as required for preconditioned
SGD. We give an algorithm with cost O(dm + m2) for computing the IHVP and
O(dm + m3) for adding or removing any gradient from the sliding window. These two algorithms yield state-of-the-art results for network pruning and optimization with lower computational overhead relative to existing second-order methods.
Implementations are available at [9] and [17]. 1

Introduction
Given the recent success and increasing impact of deep learning, there has been signiﬁcant work on improving the fundamental technical tools underpinning its progress. One such tool is the ability to estimate the local geometry of the loss function for deep models, which often comes in the form of estimates for second-order (Hessian) information. Such information is critical in several settings, such as neural network optimization and pruning.
Directly using Hessian information in the context of deep learning is infeasible: for example, just storing the Hessian matrix for the standard ResNet50 model [14] would occupy 2.5 Petabytes. These constraints have inspired signiﬁcant work on efﬁcient numerical approximations of the Hessian for deep neural networks, such as the line of work on the K-FAC approximation [19, 31, 5, 49], or efﬁcient block-wise approximations [47, 37]. One of the classic approaches, which we focus on in this paper, is the empirical Fisher [13, 3, 4] approximation to the Hessian, written as:
H (cid:39) (cid:98)F = 1
N
N (cid:88)
∇(cid:96)i · ∇(cid:96)(cid:62) i , (1) i=1 where N is the number of samples, ∇(cid:96)i denotes the gradient w.r.t. the ith sample at the given point, and · is the outer product of individual gradients, which we view as column vectors.
The empirical Fisher approximation is fairly standard, e.g. [13, 3, 4], and has been recognized to be useful in a variety of practical settings where exact estimation of the Hessian or of the true Fisher information matrix is not feasible. At the same time, there is still active research in the community 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
on the conditions for its applicability [29, 21, 37, 40]. A useful property of this approximation is that it also allows to estimate the inverse of the Hessian, which is essential in many applications.
Speciﬁcally, the fact that the empirical Fisher can be written as a sum of rank-one matrices allows the use of the Woodbury-Sherman-Morrison inversion formula [46] to exactly compute its inverse, by recursively integrating terms corresponding to each gradient into the inverse. (Please see Equation 2 for an exact derivation.) This approach was independently proposed by [13, 3], for pruning and optimization, respectively, where it was validated on small networks, with hundreds of weights.
The idea was adapted to deep neural networks (DNNs) by [37], through approximation of the inverse in small diagonal blocks. The authors show improved approximation quality for the Hessian inverse relative to a simple diagonal approximation, and that this leads to state-of-the-art pruning results in terms of accuracy. Yet, this approach is limited by the block-wise approximation: for block size B and dimension d, it requires Θ(Bdm) time to recursively build the block-wise Fisher approximation using m gradients, and Θ(Bd) time and memory for computing the Inverse-Hessian-Vector-Products (IHVPs) necessary for estimating pruning statistics. Clearly, the ideal B = d case is still intractable at scale, and generally it is still unknown whether efﬁcient algorithms are possible for computing
IHVPs in this context.
Contribution. We address this question by introducing two efﬁcient algorithms for computing
IHVPs under the empirical Fisher approximation, with computational and storage costs that are linear in the dimension d of the model, without the need for block-wise approximations, assuming that the number of samples m in the approximation is constant. Concretely, we provide exact matrix-free algorithms to compute products of the form (cid:98)F−1v, where (cid:98)F−1 is the inverse empirical Fisher, and v is an arbitrary vector. We show that these algorithms can be implemented efﬁciently, and that they can match or improve the state-of-the-art results for both neural network pruning and optimization.
The Static Algorithm. Our ﬁrst algorithm assumes a static scenario, which is standard in neural network pruning: we are given a fully-trained model θ(cid:63), for which we wish to estimate IHVPs and diagonal elements of the inverse Hessian, in order to determine the “optimal” pruning update using e.g. the Optimal Brain Surgeon (OBS) framework [24, 13]. For this, we ﬁrst compute m gradients at
θ(cid:63), which we will use to estimate IHVPs via the empirical Fisher and compute pruning statistics.
The main idea is that, since we only wish to compute products between the inverse and an arbitrary vector (IHVPs), we can rewrite the Woodbury recursion such that we work exclusively with individual vectors and scalars, and never with full d × d or d × B matrices. Given model dimension d and m gradients, each deﬁning a rank-one component of the empirical Fisher, the algorithm uses O(dm2) pre-computation time, and will have O(dm) cost for exactly computing the IHVP. Further, we can specialize the algorithm to directly query elements of the Hessian inverse, at a cost of O(m) time per element. This provides efﬁcient, linear-in-d implementations for all operations required by the OBS pruning framework. Finally, we note that the static algorithm can also be applied in a block-wise manner without any change in the total compute and memory costs.
The Dynamic Algorithm. Our main contribution is in extending this idea to preconditioned SGD optimization, i.e. to precondition stochastic gradients by our estimate of the inverse Hessian. We start from the classic idea of bootstrapping the approximation by leveraging previous gradients: the preconditioner at time t is built from gradients obtained during a “sliding window” over the last
∆ ≥ 1 optimization steps. This requires a dynamic representation, allowing addition and removal of gradients without full recomputation of second-order statistics.
We show that this can be achieved, with approximately O(dm) time and space complex-ity. The key idea is that, for any ordered set (∇(cid:96)j)m j=1 of m gradients, and any vector v, it is possible to represent the corresponding IHVP estimate (cid:98)F−1 m v as a linear combination of terms corresponding to individual gradients ∇(cid:96)j and v, of the form (cid:98)F−1 m v = λ−1v − (cid:80)m j ∇(cid:96)j, where λ > 0 is a dampening constant. j=1 cm j can be computed just via dot products ∇(cid:96)(cid:62)
Crucially, we ensure that the coefﬁcients cm i ∇(cid:96)j, where i ≤ j in the ordering, and ∇(cid:96)(cid:62) j v. Then, to replace a given gradient ∇(cid:96)i from this representation, we just have to compute m scalar products with the new gradient vector (as well as update some intermediate information). Hence, the entire update operation has computational cost O(dm + m3) for replacing any gradient in the sliding window, and O(dm + m2) for computing the IHVP.
Implementation and Experiments. We provide efﬁcient vectorized implementations for the above algorithms, called M-FAC, for Matrix-Free Approximate Curvature. Speciﬁcally, M-FAC consists of 2
Pytorch [34] implementations of a pruning and optimization library. Our implementation introduces several additional optimizations, in particular GPU acceleration via custom CUDA kernels, and minimizes the cost of memory transfer between the GPU and main memory via memory paging.
For pruning, our implementation provides order-of-magnitude improvements over the block-wise approximation of [37] for classic benchmarks such as pruning ResNet50 and MobileNet on the
ImageNet dataset. This allows us to obtain more accurate sparse models by exploring higher parameter settings and increasing the total number of pruning steps, while remaining practical in terms of memory and compute even for larger models. What is more, our preconditioned SGD (even without momentum) can be competitive in terms of validation accuracy with state-of-the-art optimizers on models of moderate size, including compact vision architectures and Transformer language models [42]. Its computational overheads are of 5%–55% relative to vanilla SGD on standard CNN architectures. 2 Preliminaries and