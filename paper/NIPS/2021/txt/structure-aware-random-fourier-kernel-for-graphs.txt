Abstract
Gaussian Processes (GPs) deﬁne distributions over functions and their generaliza-tion capabilities depend heavily on the choice of kernels. In this paper, we propose a novel structure-aware random Fourier (SRF) kernel for GPs that brings several beneﬁts when modeling graph-structured data. First, SRF kernel is deﬁned with a spectral distribution based on the Fourier duality given by the Bochner’s theo-rem, transforming the kernel learning problem to a distribution inference problem.
Second, SRF kernel admits a random Fourier feature formulation that makes the kernel scalable for optimization. Third, SRF kernel enables to leverage geometric structures by taking subgraphs as inputs. To effectively optimize GPs with SRF ker-nel, we develop a variational EM algorithm, which alternates between an inference procedure (E-step) and a learning procedure (M-step). Experimental results on ﬁve real-world datasets show that our model can achieve state-of-the-art performance in two typical graph learning tasks, i.e., object classiﬁcation and link prediction. 1

Introduction
Gaussian Process (GP) is a typical Bayesian non-parametric method for function learning. Choosing appropriate kernels profoundly improves the generalization performance of GPs on various learning tasks [1, 2]. Stationary kernels, such as Radial Basis Function (RBF) kernel, are common choices of
GP kernels and have been successfully applied in many regression and classiﬁcation problems [3, 4, 5].
However, due to their restricted smoothness assumption and the limited expressiveness, stationary kernels may not be capable for complex data structures, such as graph-structured data [6], where both object features and geometric structures are essential.
To improve GP’s performance on data with complex structures, some expressive kernels have been proposed. Two typical expressive kernels are: compositional kernels [7, 8, 9], which are obtained based on the kernel composition rules [10], and deep kernels [11, 12], which are deﬁned as the stack of a deep neural network and a base kernel. Alternatively, the spectral kernels are deﬁned in the spectral domain [2, 13, 14]. Nonetheless, when modeling graph-structured data, prior kernels face several challenges. First of all, the ﬂexibility and expressiveness of these kernels are limited by the simple base kernels upon which they are built, and the optimization of base kernels is computationally expensive [15]. Second, these kernels are usually feature-based, which indicates that they capture the similarities between graph objects based solely on object features. It may be inefﬁcient for them to
∗Corresponding Author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
capture the structural smoothness in graph data, i.e., neighboring objects tend to share similar function values [6]. However, capturing such structural smoothness is critical to the success of many graph learning methods [16, 17, 18]. In another line of work, structure-based Laplacian kernels [19, 20, 21] have been proposed to model functions over graphs. They are obtained by applying transformation functions to the graph Laplacian matrices. However, they rarely utilize the object features and may be suboptimal to model complicated functions over large-scale graphs.
To tackle the aforementioned challenges, we propose a Structure-aware Random Fourier (SRF) kernel, which is both feature-based and structure-aware. To improve the kernel expressiveness, inspired by the Bochner’s theorem [22], we deﬁne SRF kernel with a spectral distribution based on the Fourier duality between the kernel and the spectral distribution. This deﬁnition brings some beneﬁts: (1) the SRF kernel converts the kernel learning problem into a distribution inference problem, and the resulting SRF kernel will be fairly expressive if the dual spectral distribution is adequately complex; (2) the SRF kernel admits a random Fourier feature formulation for scalable learning, which leads to a linear computational complexity with respect to the number of data points. Compared with feature-based kernels and Laplacian kernels, SRF kernel enables to leverage both object features and local geometric structures to measure similarities by taking subgraphs as inputs. Moreover, to effectively optimize the GPs with proposed SRF, abbreviated as GPSRF, we develop a variational
Expectation-Maximization (EM) method [23], which alternates between an inference procedure (E-step) for posterior inference and a learning procedure (M-step) for parameter learning. It’s empirically veriﬁed that GP with SRF kernel can achieve superior performance than GPs with feature-based kernels or Laplacian kernels when modeling graph-structured data.
Our contributions can be summarized as follows: (1) We propose a novel SRF kernel for modeling functions over graphs, which enables to leverage both geometric structures and object features. (2)
Based on the SRF kernel, we propose a GP model, GPSRF, for graph learning tasks. (3) To effectively optimize GPSRF, we develop a variational Expectation-Maximization algorithm. (4) Experimental results on ﬁve real-world graph datasets show that GPSRF can outperform strong baselines in two typical graph learning tasks, i.e., object classiﬁcation and link prediction. 2