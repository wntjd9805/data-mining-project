Abstract
We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by ﬁnding a better architecture through a series of ablations. For condi-tional image synthesis, we further improve sample quality with classiﬁer guidance: a simple, compute-efﬁcient method for trading off diversity for ﬁdelity using gradi-ents from a classiﬁer. We achieve an FID of 2.97 on ImageNet 128 128, 4.59 on
ImageNet 256 512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we ﬁnd that classiﬁer guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 512. 256 256 and 3.85 on ImageNet 512 256, and 7.72 on ImageNet 512
⇥
⇥
⇥
⇥
⇥ 1

Introduction
Figure 1: Selected samples from our best ImageNet 512
⇥ 512 model (FID 3.85)
Over the past few years, generative models have gained the ability to generate human-like natural language [9], high-quality synthetic images [8, 34, 57] and highly diverse human speech and music
[70, 17]. These models can be used in a variety of ways, such as generating images from text prompts
[78, 56] or learning useful feature representations [18, 10]. While these models are already capable
⇤Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
of producing realistic images and sound, there is still much room for improvement beyond the current state-of-the-art, and better generative models could have wide-ranging impacts on graphic design, games, music production, and countless other ﬁelds.
GANs [25] currently hold the state-of-the-art on most image generation tasks [8, 74, 34] as measured by sample quality metrics such as FID [29], Inception Score [61] and Precision [38]. However, some of these metrics do not fully capture diversity, and it has been shown that GANs capture less diversity than state-of-the-art likelihood-based models [57, 49, 48]. Furthermore, GANs are often difﬁcult to train, collapsing without carefully selected hyperparameters and regularizers [8, 47, 7]. While GANs hold the state-of-the-art, their drawbacks make them difﬁcult to scale and apply to new domains.
As a result, much work has been done to achieve GAN-like sample quality with likelihood-based models [22, 57, 31, 48, 12]. While these models capture more diversity and are typically easier to scale and train than GANs, they still fall short in terms of visual ﬁdelity. Furthermore, except for
VAEs, sampling from these models is slower than GANs in terms of wall-clock time.
Diffusion models are a class of likelihood-based models which have recently been shown to produce high-quality images [63, 66, 31, 49] while offering desirable properties such as distribution coverage, a stationary training objective, and easy scalability. These models generate samples by gradually removing noise from a signal, and their training objective can be expressed as a reweighted variational lower-bound [31]. This class of models already holds the state-of-the-art [67] on CIFAR-10 [37], but still lags behind GANs on difﬁcult generation datasets like LSUN and ImageNet. We hypothesize that this gap exists for at least two reasons: ﬁrst, that the model architectures used by recent GAN literature have been heavily explored and reﬁned; second, that GANs are able to trade off diversity for ﬁdelity, producing high quality samples but not covering the whole distribution. We aim to bring these beneﬁts to diffusion models, ﬁrst by improving model architecture and then by devising a scheme for trading off diversity for ﬁdelity.
The rest of the paper is organized as follows. In Section 2, we give a brief background of diffusion models based on Ho et al. [31] and the improvements from Nichol and Dhariwal [49] and Song et al. [64], and we describe our evaluation setup. In Section 3, we introduce simple architecture improvements that give a substantial boost to FID. In Section 4, we describe a method for using gradients from a classiﬁer to guide a diffusion model during sampling. Finally, in Section 5 we show that models with our improved architecture achieve state-of-the-art on unconditional image synthesis tasks, and with classiﬁer guidance achieve state-of-the-art on conditional image synthesis. 2