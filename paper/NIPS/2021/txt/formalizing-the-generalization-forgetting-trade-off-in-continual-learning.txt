Abstract
We formulate the continual learning problem via dynamic programming and model the trade-off between catastrophic forgetting and generalization as a two-player sequential game. In this approach, player 1 maximizes the cost due to lack of generalization whereas player 2 minimizes the cost due to increased catastrophic forgetting. We show theoretically and experimentally that a balance point between the two players exists for each task and that this point is stable (once the balance is achieved, the two players stay at the balance point). Next, we introduce balanced continual learning (BCL), which is designed to attain balance between generaliza-tion and forgetting, and we empirically demonstrate that BCL is comparable to or better than the state of the art. 1

Introduction
In continual learning (CL), we incrementally adapt a model to learn tasks (deﬁned according to the problem at hand) observed sequentially. CL has two main objectives: maintain long-term memory (remember previous tasks) and navigate new experiences continually (quickly adapt to new tasks). An important characterization of these objectives is provided by the stability-plasticity dilemma [9], where the primary challenge is to balance network stability (preserve past knowledge; minimize catastrophic forgetting) and plasticity (rapidly learn from new experiences; generalize quickly). This balance provides a natural objective for CL: balance forgetting and generalization.
Traditional CL methods either minimize catastrophic forgetting or improve quick generalization but do not model both. For example, common solutions to the catastrophic forgetting issue include (1) representation-driven approaches [49, 25], (2) regularization approaches [27, 2, 34, 16, 48, 48, 24, 37, 10, 43], and (3) memory/experience replay [31, 32, 11, 12, 17]. Solutions to the generalization problem include representation-learning approaches (matching nets [45], prototypical networks [42], and metalearning approaches [18, 19, 8, 47]). More recently, several approaches [35, 16, 46, 23, 48, 15] have been introduced that combine methods designed for quick generalization with frameworks designed to minimize forgetting.
The aforementioned CL approaches naively minimize a loss function (combination of forgetting and generalization loss) but do not explicitly account for the trade-off in their optimization setup. The
ﬁrst work to formalize this trade-off was presented in meta-experience replay (MER) [38], where the forgetting-generalization trade-off was posed as a gradient alignment problem. Although MER provides a promising methodology for CL, the balance between forgetting and generalization is enforced with several hyperparameters. Therefore, two key challenges arise: (1) lack of theoretical tools that study the existence (under what conditions does a balance point between generalization and forgetting exists?) and stability (can this balance be realistically achieved?) of a balance point 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and (2) lack of a systematic approach to achieve the balance point. We address these challenges in this paper.
We describe a framework where we ﬁrst formulate CL as a sequential decision-making problem and seek to minimize a cost function summed over the complete lifetime of the model. At any time k, given that the future tasks are not available, the calculation of the cost function becomes intractable.
To circumvent this issue, we use Bellman’s principle of optimality [4] and recast the CL problem to model the catastrophic forgetting cost on the previous tasks and generalization cost on the new task. We show that equivalent performance on an inﬁnite number of tasks is not practical (Lemma 1 and Corollary 1) and that tasks must be prioritized. To achieve a balance between forgetting and
Figure 1: (left) Exemplary CL problem: the lifetime of the model can be split into three intervals. At k = 1 we seek to recognize lions; at k = 2 we seek to recognize both lions and cats; and at k = 3 we seek to recognize cats, lions, and dogs. (right) Illustration of the proposed method: our methodology comprises an interplay between two players. The ﬁrst player maximizes generalization by simulating maximum discrepancy between two tasks. The second player minimizes forgetting by adapting to maximum discrepancy . generalization, we pose the trade-off as a saddle point problem where we designate one player for maximizing the generalization cost (player 1) and another for minimizing the forgetting cost (player 2). We prove mathematically that there exists at least one saddle point between generalization and forgetting for each new task (Theorem 1). Furthermore, we show that this saddle point can be attained asymptotically (Theorem 2) when player strategies are chosen as gradient ascent-descent. We then introduce balanced continual learning (BCL), a new algorithm to achieve this saddle point. In our algorithm (see Fig. 1 for a description of BCL), the generalization cost is computed by training and evaluating the model on given new task data. The catastrophic forgetting cost is computed by evaluating the model on the task memory (previous tasks). We ﬁrst maximize the generalization cost and then minimize the catastrophic forgetting cost to achieve the balance. We compare our approach with other methods such as elastic weight consolidation (EWC) [27], online EWC [40], and MER [38] on continual learning benchmark data sets [21] to show that BCL is better than or comparable to the state-of-the-art methods. Moreover, we also show in simulation that our theoretical framework is appropriate for understanding the continual learning problem. The contributions of this paper are (1) a theoretical framework to study the CL problem, (2) BCL, a method to attain balance between forgetting and generalization, and (3) advancement of the state of the art in CL. 2 Problem Formulation
We use R to denote the set of real numbers and N to denote the set of natural numbers. We use (cid:107).(cid:107) to denote the Euclidean norm for vectors and the Frobenius norm for matrices, while using bold symbols to illustrate matrices and vectors. We deﬁne an interval [0, K), K ∈ N and let p(Q) be the distribution over all the tasks observed in this interval. For any k ∈ [0, K), we deﬁne a parametric model g(.) with yk = g(xk; θk), where θk is a vector comprising all parameters of the model with xk ∈ X k. Let n be the number of samples and m be the number of dimensions. Suppose a task at k|k ∈ [0, K) is observed and denoted as Qk : Qk ∼ p(Q), where Qk = {X k, (cid:96)k} is a tuple with X k ∈ Rn×m being the input data and (cid:96)k quantiﬁes the loss incurred by X k using the model g for the task at k. We denote a sequence of θk as uk:K = {θτ ∈ Ωθ, k ≤ τ ≤ K}, with Ωθ being the compact (feasible) set for the parameters. We denote the optimal value with a superscript (∗); for instance, we use θ(∗) to denote the optimal value of θk at task k. In this paper we use balance k 2
point, equilibrium point, and saddle point to refer to the point of balance between generalization and forgetting. We interchange between these terms whenever convenient for the discussion. We will use
∇(j)i to denote the gradient of i with respect to j and ∆i to denote the ﬁrst difference in discrete time.
An exemplary CL problem is described in Fig. 1 where we address a total of K = 3 tasks. To particularize the idea in Fig. 1, we deﬁne the cost (combination of catastrophic cost and generalization cost) at any instant k as Jk(θk) = γk(cid:96)k + (cid:80)k−1
τ =0 γτ (cid:96)τ , where (cid:96)τ is computed on task Qτ with γτ describing the contribution of Qτ to this sum.
To solve the problem at k, we seek θk to minimize Jk(θk). Similarly, to solve the problem in the complete interval [0, K], we seek a θk to minimize Jk(θk) for each k ∈ [0, K]. In other words we seek to obtain θk for each task such that the cost Jk(θk) is minimized. Therefore, the optimization problem for the overall CL problem (overarching goal of CL) is provided as the minimization of the cumulative cost Vk(uk:K) = (cid:80)K
τ =k βτ Jτ (θτ ) such that V (∗)
, is given as k
V (∗) k = minuk:K Vk(uk:K), with 0 ≤ βτ ≤ 1 being the contribution of Jτ and uk:K being a weight sequence of length K − k. (1)
Within this formulation, two parameters determine the contributions of tasks: γτ , the contribution of each task in the past, and βτ , the contribution of tasks in the future. To successfully solve the optimization problem, Vk(uk:K) must be bounded and differentiable, typically ensured by the choice of γτ , βτ . Lemma 1 (full statement and proof in Appendix A) states that equivalent performance cannot be guaranteed for an inﬁnite number of tasks. Furthermore, Corollary 1 (full statement and proof in Appendix A) demonstrates that if the task contributions are prioritized, the differentiability and boundedness of Jτ (θτ ) can be ensured. A similar result was proved in [28], where a CL problem with inﬁnite memory was shown to be NP-hard from a set theoretic perspective. These results (both ours and in [28]) demonstrate that a CL methodology cannot provide perfect performance on a large number of tasks and that tasks must be prioritized.
Despite these invaluable insights, the data corresponding to future tasks (interval [k, K]) is not available, and therefore Vk(uk:K) cannot be evaluated. The optimization problem in Eq. (1) naively minimizes the cost (due to both previous tasks and new tasks) and does not provide any explicit modeling of the trade-off between forgetting and generalization. Furthermore, uk:K, the solution to Eq. (1) is a sequence of parameters, and it is not feasible to maintain uk:K for a large number of tasks. Because of these three issues, the problem is theoretically intractable in its current form.
We will ﬁrst recast the problem using tools from dynamic programming [29], speciﬁcally Bellman’s principle of optimality, and derive a difference equation that summarizes the complete dynamics for the CL problem. Then, we will formulate a two-player differential game where we seek a saddle point solution to balance generalization and forgetting. 3 Dynamics of Continual Learning
Let V (∗) k = minuk:K to k) is provided as (cid:80)K
τ =k βτ Jτ (θτ ); the dynamics of CL (the behavior of optimal cost with respect
∆V (∗) k = −minθk∈Ωθ (cid:2)βkJk(θk) + (cid:0)(cid:104)∇θk V (∗) k
, ∆θk(cid:105) + (cid:104)∇xk V (∗) k
, ∆xk(cid:105)(cid:1)(cid:3). (2) k
The derivation is presented in Appendix A (refer to Proposition 1). Note that V (∗) is the minima for k the overarching CL problem in Eq. (2)and ∆V (∗) represents the change in V (∗) upon introduction of a new task (we hitherto refer to this as perturbations). Zero perturbations (∆V (∗) k = 0) implies that the introduction of a new task does not impact our current solution; that is, the optimal solution on all previous tasks is optimal on the new task as well. Therefore, the smaller the perturbations, the better the performance of a model on all tasks, thus providing our main objective: minimize the perturbations (∆V (∗) is quantiﬁed by three terms: the cost contribution from all the previous tasks and the new task Jk(θk); the change in the optimal cost due to the change in the parameters (cid:104)∇θk V (∗)
, ∆θk(cid:105); and the change in the optimal cost due to the change in the input (introduction of new task) (cid:104)∇xk V (∗)
). In Eq. 2, ∆V (∗)
, ∆xk(cid:105). k k k k k 3
The ﬁrst issue with the cumulative CL problem (Eq. (1)) can be attributed to the need for information from the future. In Eq. (2), all information from the future is approximated by using the data from the new and the previous tasks. Therefore, the solution of the CL problem can directly be obtained by solving Eq. (2) using all the available data. Thus, minθk∈Ω k ≈ 0 for β > 0, with H(∆xk, θk) = βkJk(θk) + (cid:104)∇θk V (∗)
, ∆xk(cid:105). Essentially, minimizing H(∆xk, θk) would minimize the perturbations introduced by any new task k. (cid:2)H(∆xk, θk)(cid:3) yields ∆V (∗)
, ∆θk(cid:105) + (cid:104)∇xk V (∗) k k
In Eq. (2), the ﬁrst and the third term quantify generalization and the second term quantiﬁes forgetting.
A model exhibits generalization when it successfully adapts to a new task (minimizes the ﬁrst and the third term in Eq. (2)). The degree of generalization depends on the discrepancy between the previous tasks and the new task (numerical value of the third term in Eq. (2)) and the worst-case discrepancy prompts maximum generalization. Quantiﬁcation of generalization is provided by ∆xk that summarizes the discrepancy between subsequent tasks. However, ∆xk = xk+1 − xk, and xk+1 is unknown at k. Therefore, we simulate worst-case discrepancy by iteratively updating ∆xk through gradient ascent in order to maximize H(∆xk, θk); thus maximizing generalization. However, large discrepancy increases forgetting, and worst-case discrepancy yields maximum forgetting. Therefore, once maximum generalization is simulated, minimizing forgetting (update θk by gradient descent) under maximum generalization provides the balance.
To formalize our idea, let us indicate the iteration index at k by i and write ∆xk as ∆x(i)
θ(i) k with H(∆xk, θk) as H(∆x(i) as H whenever convenient). Next, we write k ) (for simplicity of notation, we will denote H(∆x(i) k and θk as k , θ(i) k ) k , θ(i) (cid:2)βkJk(θ(i) k ) + (cid:104)∇θ(i) k
V (∗) k
, ∆θ(i) k (cid:105) + (cid:104)∇x(i) k
V (∗) k
, ∆x(i) k (cid:105)(cid:3) (cid:20)
H(∆x(i) k , θ(i) k ) (cid:21) min k ∈Ωθ
θ(i)
= min
θ(i) k ∈Ωθ
V (∗) k
≤ min
θ(i) k ∈Ωθ (cid:2)βkJk(θ(i) k ) + (cid:104)∇θ(i) k
, ∆θ(i) k (cid:105) + max
∆x(i) k ∼p(Q) (cid:104)∇x(i) k
V (∗) k
, ∆x(i) k (cid:105)(cid:3)
≤ min
θ(i) k ∈Ωθ max k ∼p(Q)
∆x(i) (cid:2)H(∆x(i) k , θ(i) k )(cid:3). (3), we seek the solution pair (∆x(∗)
In Eq.
H (maximizing player, player 1) while θ(∗)
) are the feasible sets for ∆x(i) (Ωθ, Ω∆x(∗) k , θ(∗) (∆x(∗) (3) k maximizes k minimizes H (minimizing player, player 2) where k and θ(i) respectively. The solution is attained, and k ) is said to be the equilibrium point when it satisﬁes the following condition: k ) ∈ (Ωθ, Ω∆x(∗)
), where ∆x(∗) k , θ(∗) k k k
H(∆x(∗) k , θ(i) k ) ≥ H(∆x(∗) k , θ(∗) k ) ≥ H(∆x(i) k , θ(∗) k ). (4) 3.1 Theoretical Analysis
With our formulation, two key questions arise:
Does our problem setup have an equilibrium point satisfying Eq. (4)? and how can one at-tain this equilibrium point? We answer these questions with Theorems 1 and 2, respectively.
Full statements and proofs are provided in Ap-pendix A. the theory, we to
To illustrate refer the
Fig. 2, where the initial values for two players are characterized by the pair
{θ(i) k (blue circle), ∆x(i) and the cost value at {θ(i) k } is indicated by H(∆x(i) k ) (the grey circle on the cost curve (the dark blue curve)).
Our prooﬁng strategy is as follows. First, we
ﬁx θ(.) k ∈ Ωθ and construct a neighborhood k (red circle)} k , ∆x(i) k , θ(i)
Figure 2: Illustration of the proofs. ∆x (player 1) is the horizontal axis, and the vertical axis indicates
θ (player 2) where the curve indicates H. If we start from the red circle for player 1 (player 2 is ﬁxed at the blue circle), H is increasing (goes from a grey circle to a red asterisk) with player 1 reaching the red asterisk. Next, start from the blue circle (θ is at the red asterisk), the cost decreases. 4
k . Second, we let ∆x(.)
Mk = {Ωx, θ(.) k }. Within this neighborhood we prove in Lemmas 2 and 4 that if we search for ∆x(i) k through gradient ascent, we can converge to a local maximizer, and H is maximizing with respect to ∆x(i) k ∈ Ωx be ﬁxed, and we search for θ(i) k through gradient descent. Under this condition, we demonstrate two ideas in
Lemmas 3 and 5: (1) we show that H is minimizing in the neighborhood Nk : Nk = {Ωθ, ∆x(.) k }; and (2) we converge to the local minimizer in the neighborhood Nk. Third, in the union of the two neighborhoods Mk ∪ Nk, (proven to be nonempty according to Lemma 6), we show that there exists at least one local equilibrium point (Theorem 1); that is, there is at least one balance point.
Theorem 1 (Existence of an Equilibrium Point). For any k ∈ [0, K], let θ(∗) of H according to Lemma 5 and deﬁne M(∗) maximizer of H according to Lemma 4 and deﬁne N (∗) be nonempty according to Lemma. 6, then (∆x(∗) k , θ(∗) k ∈ Ωθ, be the minimizer k ∈ Ωx, be the k ∪ N (∗) is a local equilibrium point. k = {∆x(∗) k ) ∈ M(∗) k , Ωθ}. Further, let M(∗) k }. Similarly, let ∆x(∗) k = {Ωx, θ(∗) k ∪ N (∗) k k
We next show that this equilibrium point is stable (Theorem 2) under a sequential play. Speciﬁcally, we show that when player 1 plays ﬁrst and player 2 plays second, we asymptotically reach a saddle point pair (∆x(∗) k ) for H. At this saddle point, both players have no incentive to move, and the game converges. k , θ(∗)
∈ Ωθ be the initial values for ∆x(i) k
Theorem 2 (Stability of
Ωx and θ(i) k
Mk = {Ωx, Ωθ} with H(∆x(i)
α(i) k × (∇∆x(i)
H(∆x(.)
∇θ(i) a consequence of Lemmas 2 and 3, (∆x(∗) the Equilibrium Point). For any k ∈ [0, K], ∆x(i)
∈ k
Deﬁne k = k × k ). Let the existence of an equilibrium point be given by Theorem 1, then, as k ) ∈ Mk is a stable equilibrium point for H given . and θ(i) k
Let ∆x(i+1) k
− θ(i) k ) given by Proposition 2. k , θ(.)
− ∆x(i) k = −α(i) k , θ(i) k ))/(cid:107)∇∆x(i) k )(cid:107)2) and θ(i+1) respectively. k , θ(∗)
H(∆x(i)
H(∆x(i) k , θ(i) k , θ(.) k k k k
In this game, the interplay between these two opposing players (representative of generalization and forgetting, respectively) introduces the dynamics required to play the game. Furthermore, the results presented in this section are local to the task. In other words, we prove that we can achieve a balance between generalization and forgetting for each task k (neighborhoods are task dependent, and we achieve a local solution given a task k). Furthermore, our game is sequential; that is, there is k ) plays ﬁrst, and the follower (θ(i) a leader (player 1) and a follower (player 2). The leader (∆x(i) k ) plays second with complete knowledge of the leader’s play. The game is directed by ∆x(i) k , and any changes in the task (reﬂected in ∆x(i) k ) will shift the input and thus the equilibrium point.
Consequently, the equilibrium point varies with respect to a task, and one will need to attain a new equilibrium point for each shift in a task. Without complete knowledge of the tasks (not available in a
CL scenario), only a local result is possible. This highlights one of the key limitations of this work.
Ideally, we would like a balance between forgetting and generalization that is independent of tasks.
However, this would require learning a trajectory of the equilibrium point (How does the equilibrium point change with the change in the tasks?) and is beyond the scope of this paper. One work that attempts to do this is [39], where the authors learn a parameter per task. For a large number of tasks, however, such an approach is computationally prohibitive.
These results are valid only under certain assumptions: (1) the Frobenius norm of the gradient is bounded, always positive; (2) the cost function is bounded and differentiable; and (3) the learning rate goes to zero as i tends to inﬁnity. The ﬁrst assumption is reasonable in practice, and gradient clipping or perturbation strategies can be used to ensure it. The boundedness of the cost (second assumption) can be ensured by prioritizing the contributions of the task (Lemma 1 and Corollary 1).
The third assumption assumes a decaying learning rate. Learning rate decay is a common strategy and is employed widely. Therefore, all assumptions are practical and reasonable. 3.2 Balanced Continual Learning
Equipped with the theory, we develop a new CL method to achieve a balance between forgetting and generalization. By Proposition 2, the cost function can be upper bounded as H(∆x(i) k ) ≤ k , θ(i) 5
, k (cid:124) (cid:123)(cid:122)
Player 1
) − Jk(θ(i) k ) − Jk(θ(i) k ) + (Jk(θ(i+ζ) k )) + (Jk+ζ(θ(i) k , θ(i) k )] k )(cid:107)2 indicates ζ updates on player 2. k )), where Jk+ζ indicates ζ updates k ∇∆xk E[H(∆x(i)
α(i) k , θ(i) (cid:107)∇∆xk H(∆x(i)
βkJk(θ(i) k on player 1 and θ(i+ζ)
The strategies for the two players ∆xk, θk are chosen in
Eq. (5) with E being the expected value operator. We can approximate the required terms in our update rule (player strategies) using data samples (batches). Note that the ap-proximation is performed largely through one-sided ﬁnite difference, which may introduce an error and is another po-tential drawback. The pseudo code of the BCL is shown in
Algorithm 1. We deﬁne a new task array DN (k) and a task memory array DP (k) ⊂ ∪k−1
τ =0Qτ (samples from all previous tasks). For each batch bN ∈ DN (k), we sample bP from DP (k), combine to create bP N (k) = bP (k) ∪ bN (k), and perform a sequential play. Speciﬁcally, for each task the ﬁrst player initializes xP N k = bP N (k) and performs ζ updates on xP N through gradient ascent. The second player, with complete knowledge of the ﬁrst player’s strat-k egy, chooses the best play to reduce H(∆x(i) k ). To estimate player 2’s play, we must estimate different terms in H(∆x(i) k , θ(i) k ). This procedure involves three steps. First, we use the ﬁrst player’s k )). Second, to approximate (Jk(θ(i+ζ) k ) − Jk(θ(i) play and approximate (Jk+ζ(θ(i) k )) : (a), we copy ˆθ into ˆθB (a temporary network) and perform ζ updates on ˆθB; and (b) we compute
Jk(θ(i+ζ) k )). Third, equipped with these ap-k proximations, we compute H(∆x(i) k ) and obtain the play for the second player. Both these players perform the steps repetitively for each piece of information (batch of data). Once all the data from the new task is exhausted, we move to the next task.
) using ˆθB(k + ζ) and evaluate (Jk(θ(i+ζ) k × ∇θk E[H(∆x(∗)
) − Jk(θ(i)
) − Jk(θ(i)
−α(i) (cid:124) k ))]
, (cid:125) k , θ(i) k , θ(i) k , θ(i) (cid:123)(cid:122)
Player 2 (5) (cid:125) k k 3.3