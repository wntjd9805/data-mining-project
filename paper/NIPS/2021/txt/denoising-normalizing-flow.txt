Abstract
Normalizing ﬂows (NF) are expressive as well as tractable density estimation methods whenever the support of the density is diffeomorphic to the entire data-space. However, real-world data sets typically live on (or very close to) low-dimensional manifolds thereby challenging the applicability of standard NF on real-world problems. Here we propose a novel method - called Denoising Normalizing
Flow (DNF) - that estimates the density on the low-dimensional manifold while learning the manifold as well. The DNF works in 3 steps. First, it inﬂates the manifold - making it diffeomorphic to the entire data-space. Secondly, it learns an
NF on the inﬂated manifold and ﬁnally it learns a denoising mapping - similarly to denoising autoencoders. The DNF relies on a single cost function and does not require to alternate between a density estimation phase and a manifold learning phase - as it is the case with other recent methods. Furthermore, we show that the DNF can learn meaningful low-dimensional representations from naturalistic images as well as generate high-quality samples. 1

Introduction
Given samples from the data-density p(x), key objectives in probabilistic Machine Learning are 1. estimating p(x) (density estimation), 2. generating new data points from p(x) (sampling), and 3.
ﬁnding low-dimensional representations of the data (inference). The three main methods used to perform these tasks are Normalizing Flows (NFs) [34], Generative Adversarial Networks (GANs)
[17], and Variational Autoencoders (VAEs) [26]. Among those methods, only the VAE does check all desired objectives by default. However, it does so by sacriﬁcing the sample quality (compared to
GANs), and only learning a lower bound on p(x) (rather than the exact value as NFs do). Finding new ways to meet these key objectives, based on either the known main methods or new ones, is an active and important research area.
How can we infer low-dimensional representations using NFs? The standard NF requires the data-density p(x) to have a support diffeomorphic to the entire data-space RD. However, the manifold hypothesis conjectures that the data-manifold M lies close to a d-dimensional manifold embedded in
RD, d < D. Thus, unfortunately, a standard NF cannot be used to infer the latent space. Recently,
Brehmer et al. proposed to overcome this limitation by ﬁrst projecting the data into a d-dimensional space using the ﬁrst d−components of a standard NF, and then using another NF to learn the latent distribution π(u) [10]. For their method to work they propose different learning schemes, separating the manifold learning from the density estimation.
In this paper, we propose an easy and new way to learn low-dimensional representations with NFs.
Our idea is based on the theoretical work derived in [20] where it was shown that by inﬂating the data-manifold with Gaussian noise ε ∼ N (0, σ2ID), i.e. ˜x := x + ε, the data-density p(x) can be well approximated by learning the inﬂated distribution qσ(˜x). More concretely, the main result in [20] states sufﬁcient conditions on the choice of noise qσ(˜x|x) and type of manifold such that 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
qσ(x) = p(x)qσ(x|x) holds. Here, by adding a penalty term to the usual KL-divergence used to learn qσ(˜x), we ensure that the ﬁrst d-components of the corresponding ﬂow are noise insensitive and thus encode the manifold. This penalty term is essentially the objective function of a Denoising autoencoder (DAE), and thus we call our method Denoising Normalizing Flow (DNF).
In summary, our contributions are the following: 1. We propose a new method, the Denoising Normalizing Flow, which combines two previously well-known methods (DAE and NF), and is able to
• approximate p(x),
• sample new data x ∼ p(x) as a non-linear transformation of u ∼ N (u; 0, Id),
• infer low-dimensional latent variables u ∼ p(u|x) given x ∼ p(x). 2. We demonstrate on naturalistic data that our method learns meaningful latent representations without sacriﬁcing the sample quality.
Notations: We adapt the notation used in [10]. To further simplify it and avoid clutter, we denote the
Gram matrix of g evaluated at g−1(x) as
Gg(x) := Jg(g−1(x))T Jg(g−1(x)) where Jg(g−1(x))T is the transpose of the Jacobian of g : Rd → RD, d ≤ D, evaluated at g−1(x). (1) 2 Problem statement (cid:90)
In the following, we are going to show why standard NFs are not suited to infer low-dimensional representations of the given data. We end the section with the research question we are going to study in Section 3.
General setting: In generative modeling, it is assumed that the data x ∈ M ⊂ RD is generated by a non-linear transformation g of some latent variables u ∈ U ⊂ Rd, i.e. x = g(u) where u ∼ π(u).
Typically, d < D and the latent distribution π(u) is assumed to be Gaussian, π(u) = N (u; 0, Id).
Hence, the latent random variable u generates x, and the data-density p(x) evaluated at x ∈ RD is given by p(x) =
π(u)δ(x − g(u))du, (2)
U where δ denotes the Dirac function, see [3]. If d = D and g is a diffeomorphism, we have for x ∈ M that (3)
Then, the target density p(x) can be learned, in principle, exactly using an NF [21]. In general, an NF is an embedding mapping U ⊂ Rd to M ⊂ RD, see [32] or [27] for some recent reviews. Denoting this mapping as gθ : U → M and its parameters as θ, the induced density on M is given by p(x) = | det Gg(x)|− 1 2 π(g−1(x)). pθ(x) = | det Ggθ (x)|− 1 2 pu(g−1
θ (x)), (4) where pu(u) is a known reference density (usually set to be standard Gaussian). The parameters θ are updated such that the KL-divergence between p(x) and pθ(x),
DKL(p(x)||pθ(x)) = −Ex∼p(x)[log pθ(x)] + const. (5) 2 log | det Ggθ (x)| efﬁciently. is minimized. Thus, to learn θ efﬁciently, one needs to evaluate T1(x) := log pu(g−1
T2(x) := 1
Topological constraints: The evaluation of T1(x) is efﬁcient since we are free to choose the reference measure pu, and g−1
θ (x) is the forward pass of a neural network constructed to be bijective [13, 21].
Therefore, the majority of the NF literature focuses on designing clever ﬂow architectures to be able to calculate T2(x) efﬁciently without sacriﬁcing the ﬂow’s expressiveness (i.e. the size of the space of embeddings able to learn). However, so far these architectures are constructed for d = D since in this case, the Jacobian of gθ is a square matrix, and thus T2(x) becomes
θ (x)) and 1 2 log | det Ggθ (x)| = log | det Jgθ (g−1
θ (x))|. (6) 2
Hence, calculating the Gram determinant of gθ efﬁciently amounts to calculating the determinant of the Jacobian of gθ efﬁciently. Popular choices are to construct gθ such that Jgθ is lower triangular, as in this case, the determinant is simply the product of diagonal elements. Unfortunately, for d < D,
Jgθ is not a square matrix, and the full Gram determinant needs to be calculated. This makes NFs unsuitable for ﬁnding low-dimensional representations u of high-dimensional data points for large d as the computational complexity to calculate det Ggθ is O(d2D) + O(d3).
Research question: As mentioned in [10], ﬁnding ways to design gθ such that T2(x) can be efﬁciently calculated for d < D is an interesting research question. Here, we address it from a different angle.
Let M be a d−dimensional manifold embedded in RD through g. Can we construct a mapping gθ : Rd → M to 1. generate x ∼ p(x) in 2 steps: (a) generate u ∼ N (u; 0, Id) and (b) set x = gθ(u)? 2. infer u ∈ Rd such that x = gθ(u)? 3. approximate det Gg(x) efﬁciently? 3 Denoising Normalizing Flow
We answer the research question based on the theoretical work developed in [20]. First, we brieﬂy review this work, and then introduce the DNF.
Preliminaries: In Section 2, we discussed why classical NFs are not suited to infer low-dimensional representations of the given data. Also, if one is only interested in the value of p(x), standard NF cannot be used. Intuitively, a standard NF fψ with parameters ψ simply squeezes or expands a volume element where the net change is given by its Jacobian determinant. A volume element of a d−dimensional manifold is d−dimensional and thus has D−dimensional Lebesgue measure 0. Thus, we are asking fψ to expand a d−dimensional volume to a D−dimensional one which will lead to a degeneration of | det Gfψ (x)| and manifest in numerical instabilities. Therefore, [20] inﬂated the manifold by adding Gaussian noise to the data points.1 This inﬂated manifold is D−dimensional and thus a usual ﬂow can be used to learn the corresponding density. Their main result states sufﬁcient conditions on the choice of noise and type of manifold M, such that the learned inﬂated distribution can be deﬂated, and p(x) is exactly retrieved.
More precisely, given a random variable x ∼ p(x) with probability measure PX and taking values in a d−dimensional manifold M embedded in RD, if we add some noise ε to it, the resulting new random variable ˜x = x + ε has the following density: qσ(˜x) = (cid:90)
M qσ(˜x|x)dPX (x). (7)
In [20], it was shown that if (a) the noise is only added in the (D − d)−dimensional normal space Nx in x, (b) the noise magnitude σ is sufﬁciently small, and (c) the manifold M is sufﬁciently smooth and disentangled, the resulting inﬂated distribution evaluated at ˜x = x takes the following product form: qσ(x) = p(x)qσ(x|x), where qσ(x|x) is the normalization constant of the noise density. More concretely, (a) and (b) need to ensure that x is almost surely uniquely determined by ˜x as the orthogonal projection of ˜x on M. For this projection to be well-deﬁned, a sufﬁcient condition is that the manifolds reach number2 is ﬁnite
[6]. A manifold where almost every point ˜x ∼ q(˜x) in the inﬂated set has a unique projection on
M was called Q−normally reachable in [20], where Q denotes the collection of noise distributions qσ(˜x|x). Their main Theorem proves that for any Q−normally reachable manifold equation (8) holds.
Therefore, if qσ(˜x) can be learned exactly using a standard NF, the on-manifold density p(x) can be retrieved exactly.
It was also shown that for the case where d (cid:28) D (as it is generally assumed for high-resolution images), full Gaussian noise is an excellent approximation for a Gaussian in the normal space. (8) 1Adding noise to circumvent the aforementioned degeneracy problem was also proposed in [24]. 2Informally, this reach condition ensures that a manifold is learnable through samples. 3
Main idea: We use a standard ﬂow fψ to learn qσ(˜x) such that the corresponding density has the product form of equation (8), and the ﬁrst d−components u of the ﬂow’s output f −1
ψ (˜x) are noise-insensitive whereas the remaining (D − d)−components v remain noise-sensitive. Thus, intuitively, we want the ﬁrst d−components to denoise the inﬂated data.
DNF: Let fψ : RD → RD be a standard ﬂow with reference measure pz(z). We denote the ﬁrst d−components of the ﬂows output as u, and the remaining ones as v, i.e. (u, v)T = f −1
ψ (˜x). More formally,
ψ (˜x)), v = v(˜x) = Projv(f −1 u = u(˜x) = Proju(f −1 (9) with Proju(z) = (z1, . . . , zd) and Projv(z) = (zd+1, . . . , zD) for z ∈ RD. As reference measure pz, we choose pz((u, v)T ) = pu(u)pv(v) with pv(v) modelling the noise-sensitive part, and pu(u) the noise-insensitive part. In particular , if qσ(˜x|x) is a (D − d)−dimensional Gaussian distribution with covariance σ2ID−d, we set pv(v) = N (v; 0, σ2ID−d).
For pu(u) to model the noise-insensitive part, we want the image fψ(u, 0) to be in the manifold.
Therefore, we embed u back in RD by padding the missing coordinates with 0,
ψ (˜x))
Pad(u) = (u, 0, . . . , 0 (cid:124) (cid:123)(cid:122) (cid:125) (D−d)-times
)T , (10) such that the operation Pad(Proju((u, v))) = (u, 0)T ignores the noise-sensitive part v in the latent space. This operator allows us to deﬁne a denoising function rψ(˜x) as rψ(˜x) := (fψ ◦ Pad ◦ Proju ◦ f −1
ψ )(˜x) and we regularize ψ by minimizing
C(ψ) := Ex∼p(x)E˜x∼qσ(˜x|x)||x − rψ(˜x)||2 where || · || denotes the L2 norm. (11) (12)
We have not speciﬁed the reference measure pu(u). To facilitate the disentanglement of noise-insensitivity and noise-sensitivity in the u and v variables, we transform u with yet another ﬂow hφ with paramters φ and reference measure pu(cid:48) (e.g. standard Normal).
Now, our sampling procedure looks as follows: 1. sample u(cid:48) ∼ pu(cid:48)(u(cid:48)), 2. apply hφ to obtain a sample u from pu(u), i.e. u = hφ(u(cid:48)), and 3. set x = fψ(Pad(u)). Denoting θ = (ψ, φ), our model to learn qσ(˜x) is qθ(˜x) =| det Gfψ (˜x)|− 1
=| det Gfψ (˜x)|− 1 2 pu(u(˜x))pv(v(˜x)) 2 | det Ghφ(u(˜x))|− 1 2 pu(cid:48)(h−1
φ (u(˜x)))pv(v(˜x)). (13)
Note that the reconstruction loss, equation (12), is essentially the objective function for the Denoising
Autoencoder introduced in [1]. Therefore, we call our method Denoising Normalizing Flow (DNF) and it is trained on
LDNF(θ) :=DKL(qσ(˜x)||qθ(˜x)) + λC(ψ) (14) where λ > 0 is the penalty hyperparameter and is trading the density estimation with the manifold learning. A graphical description of the DNF model is given in Figure 1 (a), and an algorithmic description in the DNF Algorithm below.3
Answer to research question: If LDNF(θ) = 0, the reconstruction error expressed in equation (12) is 0. Thus, the generative story of the DNF is exactly the one described in point 1. of our research question with gθ := fψ ◦ Pad ◦ hφ. The inverse, h−1
ψ , can be used to infer u(cid:48) s.t. point 2. holds. Finally, to show the third claim, we additionally assume that U (which is the domain of the manifold generating function g) is diffeomorphic to Rd, s.t. without loss of generality we set
π(u) = N (u; 0, Id). Then, we exploit equation (8) and calculate |det Gg(x)| efﬁciently with the help of fψ and hφ, see Proposition 1 and its proof in the supplementary.
φ ◦ Proju ◦ f −1 3We show the algorithm for the general scenario where the manifold is unknown and thus noise cannot be added to the normal space. We also ignore terms independent of θ in the calculation of LDNF, and denote this loss function as L(cid:48)
DNF. 4
DNF Algorithm: Training of Denoising Normalizing Flow for qσ(˜x|x) = N (˜x; x, σ2ID). For sim-plicity, we show a stochastic gradient descent with a constant learning rate. Alternative optimization methods and learning rate schedules can be easily adapted.
Require: Manifold dimension d, Learning rate α, penalty parameter λ, inﬂation variance σ2, batch size n, number of epochs E.
Initialize: Parameters ψ and φ for ﬂows fψ and hφ. while θ = (ψ, φ) has not converged do for e = 1 to E do for i = 1 to n do
ψ (˜xi)
Sample: xi ∼ p(x)
Inﬂate: ˜xi = xi + εi, where εi ∼ N (0, σ2ID) (ui, vi) ← f −1
ˆxi ← fψ(ui, 0) i ← h−1 u(cid:48)
φ (ui) end for
L(cid:48) i) − log | det Jhφ(u(cid:48)
DNF ← 1 n (cid:80)n i log pu(cid:48)(u(cid:48)
+λ||xi − ˆxi||2
DNF
θ ← θ − α∇θL(cid:48) end for end while
# sample data
# add noise
# project on u ∈ Rd and v ∈ RD−d
# reconstruct x
# transform u i)| + log(pv(vi)) − log | det Jfψ (ui, vi)|
# calculate logqθ(˜x) and add reconstruction error
# update model parameters
Proposition 1 Let M be a d−dimensional manifold embedded in RD through g : Rd → M. Let x ∼ p(x) be generated by g(u), where u ∼ N (u; 0, Id), i.e. x = g(u). Assume that we can learn the inﬂated distribution qσ(˜x) using an NF and that for ˜x = x it holds that qσ(x) = p(x)qσ(x|x). If
LDNF(θ) = 0, then
|det Gg(x)| = | det Gfψ (x)|| det Ghφ(x)|. (15) 4