Abstract
We consider the task of minimizing the sum of smooth and strongly convex func-tions stored in a decentralized manner across the nodes of a communication network whose links are allowed to change in time. We solve two fundamental problems for this task. First, we establish the first lower bounds on the number of decentralized communication rounds and the number of local computations required to find an
ϵ-accurate solution. Second, we design two optimal algorithms that attain these lower bounds: (i) a variant of the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a multi-consensus subroutine, which is optimal in the case when access to the dual gradients is assumed, and (ii) a novel algorithm, called
ADOM+, which is optimal in the case when access to the primal gradients is as-sumed. We corroborate the theoretical efficiency of these algorithms by performing an experimental comparison with existing state-of-the-art methods. 1

Introduction
In this work we are solving the decentralized optimization problem n (cid:80) i=1 where each function fi : Rd → Rd is stored on a compute node i ∈ {1, . . . , n}. We assume that the nodes are connected through a communication network. Each node can perform local computations based on its local state and data, and can directly communicate with its neighbors only. Further, we assume the functions fi to be smooth and strongly convex. min x∈Rd fi(x), (1)
Such decentralized optimization problems arise in many applications, including estimation by sensor networks (Rabbat and Nowak, 2004), network resource allocation (Beck et al., 2014), cooperative control (Giselsson et al., 2013), distributed spectrum sensing (Bazerque and Giannakis, 2009) and power system control (Gan et al., 2012). Moreover, problems of this form draw attention of the machine learning community (Scaman et al., 2017), since they cover training of supervised machine
∗King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.
†Moscow Institute of Physics and Technology, Moscow, Russia
‡Institute for System Programming of the Russian Academy of Sciences, Research Center for Trusted
Artificial Intelligence, Moscow, Russia 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learning models through empirical risk minimization from the data stored across the nodes of a net-work as a special case. Finally, while the current federated learning (Koneˇcný et al., 2016; McMahan et al., 2017) systems rely on a star network topology, with a trusted server performing aggregation and coordination placed at the center of the network, advances in decentralized optimization could be useful in new-generation federated learning formulations that would rely on fully decentralized computation (Li et al., 2020). 1.1 Time-varying Networks
In this work, we focus on the practically highly relevant and theoretically challenging situation when the links in the communication network are allowed to change over time. Such time-varying networks (Zadeh, 1961; Kolar et al., 2010) are ubiquitous in many complex systems and practical applications.
In sensor networks, for example, changes in the link structure occur when the sensors are in motion, and due to other disturbances in the wireless signal connecting pairs of nodes. We envisage that a similar regime will be supported in future-generation federated learning systems (Koneˇcný et al., 2016; McMahan et al., 2017), where the communication pattern among pairs of mobile devices or mobile devices and edge servers will be dictated by their physical proximity, which naturally changes over time. 1.2 Contributions
In this work we present the following key contributions: 1. Lower bounds. We establish the first lower bounds on decentralized communication and local computation complexities for solving problem (1) over time-varying networks. Our results are summarized in Table 1, and detailed in Section 3 (see Theorems 2 and 3 therein). 2. Optimal algorithms. Further, we prove that these bounds are tight by providing two new optimal algorithms4 which match these lower bounds: (i) a variant of the recently proposed algorithm ADOM (Kovalev et al., 2021) enhanced via a multi-consensus subroutine, and (ii) a novel algorithm, called ADOM+ (Algorithm 1), also featuring multi-consensus.
The former method is optimal in the case when access to the dual gradients is assumed, and the latter one is optimal in the case when access to the primal gradients is assumed. See
Sections 4 and 5 for details. To the best of our knowledge, ADOM with multi-consensus is the first dual based optimal decentralized algorithm for time-varying networks 3. Experiments. Through illustrative numerical experiments (see Section 6, and the extra experiments contained in the appendix) we demonstrate that our methods are implementable, and that they perform competitively when compared to existing baseline methods APM-C (Rogozin et al., 2020; Li et al., 2018) and Acc-GT (Li and Lin, 2021).