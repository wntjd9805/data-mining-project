Abstract
Teacher-student models provide a framework in which the typical-case performance of high-dimensional supervised learning can be described in closed form. The assumptions of Gaussian i.i.d. input data underlying the canonical teacher-student model may, however, be perceived as too restrictive to capture the behaviour of realistic data sets. In this paper, we introduce a Gaussian covariate generalisation of the model where the teacher and student can act on different spaces, generated with ﬁxed, but generic feature maps. While still solvable in a closed form, this generalization is able to capture the learning curves for a broad range of realistic data sets, thus redeeming the potential of the teacher-student framework. Our contribution is then two-fold: ﬁrst, we prove a rigorous formula for the asymptotic training loss and generalisation error. Second, we present a number of situations where the learning curve of the model captures the one of a realistic data set learned with kernel regression and classiﬁcation, with out-of-the-box feature maps such as random projections or scattering transforms, or with pre-learned ones -such as the features learned by training multi-layer neural networks. We discuss both the power and the limitations of the framework. 1

Introduction
Teacher-student models are a popular framework to study the high-dimensional asymptotic perform-ance of learning problems with synthetic data, and have been the subject of intense investigations spanning three decades [1–7]. In the wake of understanding the limitations of classical statistical learning approaches [8–10], this direction is witnessing a renewal of interest [10–15]. However, this framework is often assuming the input data to be Gaussian i.i.d., which is arguably too simplistic to be able to capture properties of realistic data. In this paper, we redeem this line of work by deﬁning a
Gaussian covariate model where the teacher and student act on different Gaussian correlated spaces with arbitrary covariance. We derive a rigorous asymptotic solution of this model generalizing the formulas found in the above mentioned classical works.
We then put forward a theory, supported by universality arguments and numerical experiments, that this model captures learning curves, i.e. the dependence of the training and test errors on the number of samples, for a generic class of feature maps applied to realistic datasets. These maps can be deterministic, random, or even learnt from the data. This analysis thus gives a uniﬁed framework to describe the learning curves of, for example, kernel regression and classiﬁcation, the analysis of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
feature maps – random projections [16], neural tangent kernels [17], scattering transforms [18] – as well as the analysis of transfer learning performance on data generated by generative adversarial networks [19]. We also discuss limits of applicability of our results, by showing concrete situations where the learning curves of the Gaussian covariate model differ from the actual ones.
Model deﬁnition — The Gaussian covariate teacher-student model is deﬁned via two vectors u ∈ Rp and v ∈ Rd, with correlation matrices Ψ ∈ Rp×p, Ω ∈ Rd×d and Φ ∈ Rp×d, from which we draw n independent samples: (cid:20)uµ vµ (cid:20) Ψ Φ
Φ(cid:62) Ω
∈ Rp+d ∼ i.i.d.
µ = 1, · · · , n. (cid:21)(cid:19) (1)
N 0, (cid:18) (cid:21)
,
The labels yµ are generated by a teacher function that is only using the vectors uµ: (cid:18) 1
√ p where f0 : R → R is a function that may include randomness such as, for instance, an additive
Gaussian noise, and θ0 ∈ Rp is a vector of teacher-weights with ﬁnite norm which can be either random or deterministic. Learning is performed by the student with weights w via empirical risk minimization that has access only to the features vµ: yµ = f0 0 uµ
θ(cid:62) (2) (cid:19)
,
ˆw = arg min w∈Rd (cid:34) n (cid:88)
µ=1 g (cid:18) w(cid:62)vµ
√ d (cid:19)
, yµ (cid:35)
+ r(w)
, (3) where r and g are proper, convex, lower-semicontinuous functions of w ∈ Rd (e.g. g can be a logistic or a square loss and r a (cid:96)p (p = 1, 2) regularization). The key quantities we want to compute in this model are the averaged training and generalisation errors for the estimator w, (cid:19) (cid:19)(cid:19)(cid:21) (cid:19) (cid:18) (cid:20)
Etrain.(w) ≡
, yµ and Egen.(w) ≡ E
ˆg
ˆf (cid:18) v(cid:62) neww
√ d
, f0 (cid:18) u(cid:62) newθ0√ p
. 1 n n (cid:88)
µ=1 g (cid:18) w(cid:62)vµ
√ d (4) where g is the loss function in eq. (3), ˆf is a prediction function (e.g. ˆf = sign for a classiﬁcation task), ˆg is a performance measure (e.g. ˆg(ˆy, y) = (ˆy − y)2 for regression or ˆg(ˆy, y) = P(ˆy (cid:54)= y) for classiﬁcation) and (unew, vnew) is a fresh sample from the joint distribution of u and v.
Our two main technical contributions are: (C1) In Theorems 1 & 2, we give a rigorous closed-form characterisation of the properties of the estimator ˆw for the Gaussian covariate model (1), and the corresponding training and generalisation errors in the high-dimensional limit. We prove our result using Gaussian comparison inequalities [20]; (C2) We show how the same expression can be obtained using the replica method from statistical physics [21]. This is of additional interest given the wide range of applications of the replica approach in machine learning and computer science [22]. In particular, this allows to put on a rigorous basis many results previously derived with the replica method.
Towards realistic data — In the second part of our paper, we argue that the above Gaussian covariate model (1) is generic enough to capture the learning behaviour of a broad range of realistic
µ=1 denote a data set with n independent samples on X ⊂ RD. Based on this input, data. Let {xµ}n the features u, v are given by (potentially) elaborated transformations of x, i.e. (5) for given centred feature maps ϕt : X → Rp and ϕs : X → Rd, see Fig. 1. Uncentered features can be taken into account by shifting the covariances, but we focus on the centred case to lighten notation. and v = ϕs(x) ∈ Rd u = ϕt(x) ∈ Rp
The Gaussian covariate model (1) is exact in the case where x are Gaussian variables and the feature maps (ϕs, ϕs) preserve the Gaussianity, for example linear features. In particular, this is the case for u = v = x, which is the widely-studied vanilla teacher-student model [24]. The interest of the model (1) is that it also captures a range of cases in which the feature maps ϕt and ϕs are deterministic, or even learnt from the data. The covariance matrices Ψ, Φ, and Ω then represent different aspects of the data-generative process and learning model. The student (3) then corresponds to the last layer of the learning model. These observation can be distilled into the following conjecture: 2
Figure 1: Left: Given a data set {xµ}n
µ=1, teacher u = ϕt(x) and student maps v = ϕt(x), we assume [u, v] to be jointly Gaussian random variables and apply the results of the Gaussian covariate model (1). Right: Illustration on real data, here ridge regression on even vs odd MNIST digits, with regularisation λ = 10−2. Full line is theory, points are simulations. We show the performance with no feature map (blue), random feature map with σ = erf & Gaussian projection (orange), the scattering transform with parameters J = 3, L = 8 [18] (green), and of the limiting kernel of the random map [23] (red). The covariance Ω is empirically estimated from the full data set, while the other quantities appearing in the Theorem 1 are expressed directly as a function of the labels, see
Section 3.4. Simulations are averaged over 10 independent runs.
Conjecture 1. (Gaussian equivalent model) For a wide class of data distributions {xµ}n
µ=1, and features maps u = ϕt(x), v = ϕs(x), the generalisation and training errors of estimator (3) are asymptotically captured by the equivalent Gaussian model (1), where [u, v] are jointly Gaussian variables, and thus by the closed-form expressions of Theorem 1.
The second part of our main contributions are: (C3) In Sec. 3.3 we show that the theoretical predictions from (C1) captures the learning curves in non-trivial cases, e.g. when input data are generated using a trained generative adversarial network, while extracting both the feature maps from a neural network trained on real data. (C4) In Sec. 3.4, we show empirically that for ridge regression the asymptotic formula of Theorem 1 can be applied directly to real data sets, even though the Gaussian hypothesis is not satisﬁed. This universality-like property is a consequence of Theorem 3 and is illustrated in Fig. 1 (right) where the real learning curve of several features maps learning the odd-versus-even digit task on MNIST is compared to the theoretical prediction.