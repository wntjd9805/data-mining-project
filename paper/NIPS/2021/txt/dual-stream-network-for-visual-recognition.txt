Abstract
Transformers with remarkable global representation capacities achieve competitive results for visual tasks, but fail to consider high-level local pattern information in input images. In this paper, we present a generic Dual-stream Network (DS-Net) to fully explore the representation capacity of local and global pattern features for image classification. Our DS-Net can simultaneously calculate fine-grained and integrated features and efficiently fuse them. Specifically, we propose an
Intra-scale Propagation module to process two different resolutions in each block and an Inter-Scale Alignment module to perform information interaction across features at dual scales. Besides, we also design a Dual-stream FPN (DS-FPN) to further enhance contextual information for downstream dense predictions. Without bells and whistles, the proposed DS-Net outperforms DeiT-Small by 2.4% in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art performance over other Vision Transformers and ResNets. For object detection and instance segmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5
% in terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art scheme, which significantly demonstrates its potential to be a general backbone in vision tasks. The code will be released soon. 1

Introduction
In recent years, convolutional neural networks (CNNs) have dominated various vision tasks including but not limited to image recognition [22, 18, 38, 39, 5, 19, 41, 20, 52], object detection [37, 2, 26, 36, 24, 51, 30, 21, 11] and segmentation [17, 28], thanks to their unprecedented representation capacity.
However, limited receptive fields of convolutions inevitably neglect the global patterns in images, which might be crucial during inference. For example, it is more likely a chair than a elephant nearby a table. Such object-level information cannot be fully explored by local convolutions, that hampers further improvement of CNNs. Motivated by the success of Transformer architecture [43] in
Natural Language Processing (NLP) [9, 34, 1] and Multi-modality Fusion [29, 40, 12], researchers are trying to apply Transformer to vision tasks and have obtained promising results. In [10, 55, 48, 4, 7, 45, 46, 27, 3, 54, 14, 15, 44, 53], many novel architectures and optimizing strategies are proposed and achieve comparable or even better performance than CNNs, emphasizing the significance of extracting global features in vision tasks.
Inspired by the verified efficacy of CNN and Transformer, many concurrent works, such as Cont-Net [47] a CvT [46], attempt to introduce convolutions to vision transformers in different manners, hoping to combine both advantages. In CvT [46], linear projection of self-attention block in the
Transformer module are replaced with convolutional projection. In contrast, ContNet[47] performs 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
convolution on different token maps to build mutual connections, which is similar to Swin Trans-former [27].
Figure 1: Comparison of attention maps and feature maps of DeiT and our proposed DS-Net, both of which are obtained from the last block. (a) and (b) are attention weights of the blue points in the images. To achieve clearer illustration, the figures are acquired by overlaying heatmaps to the input images.
However, some drawbacks of existing models still remain to be solved. Firstly, works mentioned above either perform convolution and attention mechanisms sequentially, or just replace linear projection with convolutional projection in attention mechanisms, which may not be the most ideal design. Additionally, the conflicting properties of such two operations, convolution for local patterns but attention for global patterns, might cause ambiguity during training, which prevents their merits from merging to the maximum extent. Furthermore, self-attention can capture long-range information via the built-in all-pair interaction in theory, but it is quite possible that attentions might be confused and disturbed by neighboring details in high resolution feature maps, fail to build up object-level global patterns(see Fig. 1(a)). Finally, the computation cost of self-attention is unaffordable due to the quadratic computation complexity of sequence length. Although PVT [45] and APNB [56] downsample the key-query features to improve the efficiency of self-attention operator, they both abandon fine-graind local details of the image, which greatly impairs their performance.
In this paper, we address the these issues by introducing a Dual-stream Network (DS-Net). Instead of single stream architecture as previous works, our DS-Net adopts Dual-stream Blocks (DS-Blocks), which generates two feature maps with different resolutions, and retains both local and global information of the image via two parallel branches. We propose a Intra-scale Propagation module here to process two feature maps. Specifically, high-resolution features are used to extract fine-grained local patterns with depth-wise convolution, while low-resolution features are expected to summarize long-range global patterns. Considering low-resolution features themselves contain more integrated information, it would be much more easier for self-attention mechanism to capture object-level patterns rather than overwhelmed by trivial details (see Fig. 1(b)). Such dual-stream architecture disentangles local and global representations, which helps maximize both their merits, and thus generates better representations compared to DeiT baseline (see (c) and (e) in Fig. 1). Besides, low-resolution feature maps for self-attention dramatically reduce the memory cost and computation complexity. After parallelly processing dual streams, we present Inter-scale Alignment module based on co-attention mechanism at the end of DS-Blocks. This is because local details and global patterns capture different perspectives of the image, which are misaligned not only in pixel positions but also in semantics. Hence, this module is designed for modeling complex cross-scale relations and adaptively fuse local and global patterns.
Besides, we apply our DS-Blocks to Feature Pyramid Networks for further feature refinement, named
DS-FPN. In this way, multi-level features are capable of extracting contextual information from both local and global views, improving performance of downstream tasks. This demonstrates that our
Dual Stream design could be utilized as a plug-in building block not only for image recognition, but for many other vision tasks.
The contributions of this work are concluded as follows: 1. We present a novel Dual-Stream Network, named DS-Net, which retains both local and global features in DS-Block. The independent propagation maximize the advantages of convolution and self-attention, thus eliminating the conflicts during training. 2. We propose Intra-scale Propagation and Inter-Scale Alignment mechanism to achieve effec-tive information flows within and between features of different resolutions, thus generating better representations of the image. 2
3. We introduce Dual-stream Feature Pyramid Network (DS-FPN) to enhance contextual information for downstream dense tasks and achieves better performance with little extra costs. 4. Without bells and whistles, the proposed DS-Net outperforms DeiT baseline by significant margins in terms of top-1 accuracy on ImageNet-1k and achieves state-of-the-art perfor-mance over other Vision Transformers and CNN-based networks on image classification and downstream tasks, including object detection and instance segmentation. 2