Abstract
In this paper, we introduce spatiotemporal joint ﬁlter decomposition to decouple spatial and temporal learning, while preserving spatiotemporal dependency in a video. A 3D convolutional ﬁlter is now jointly decomposed over a set of spatial and temporal ﬁlter atoms respectively. In this way, a 3D convolutional layer becomes three: a temporal atom layer, a spatial atom layer, and a joint coefﬁcient layer, all three remaining convolutional. One obvious arithmetic manipulation allowed in our joint decomposition is to swap spatial or temporal atoms with a set of atoms that have the same number but different sizes, while keeping the remaining unchanged.
For example, as shown later, we can now achieve tempo-invariance by simply dilating temporal atoms only. To illustrate this useful atom-swapping property, we further demonstrate how such a decomposition permits the direct learning of 3D CNNs with full-size videos through iterations of two consecutive sub-stages of learning: In the temporal stage, full-temporal downsampled-spatial data are used to learn temporal atoms and joint coefﬁcients while ﬁxing spatial atoms.
In the spatial stage, full-spatial downsampled-temporal data are used for spatial atoms and joint coefﬁcients while ﬁxing temporal atoms. We show empirically on multiple action recognition datasets that, the decoupled spatiotemporal learning signiﬁcantly reduces the model memory footprints, and allows deep 3D CNNs to model high-spatial long-temporal dependency with limited computational resources while delivering comparable performance. 1

Introduction
Convolutional Neural Networks (CNNs) have been used intensively in the ﬁeld of video understanding.
Particularly, networks with 3D convolutional layers capture spatiotemporal correlation and achieve great success in applications like action recognition [1, 30]. However, joint spatiotemporal modeling of videos usually requires signiﬁcant training time, computation, and memory, which becomes one of the main obstacles in video understanding. In practice, we almost always need to ﬁrst downsample video data spatially or temporally or both to meet real-world constraints of computational resources and training time. For instance, a 64-frame video with the spatial resolution 224×224 is downsampled 2 times temporally for training Non-Local Networks [30], and 8 times temporally for the R(2+1)D models [26]. As in most cases, for spatiotemporal modeling of high-resolution videos, 3D CNNs cannot be ﬁt into most modern GPUs due to the huge model memory footprints. However, it has been observed that the full spatial and temporal resolution are essential to achieve superior performance in many video understanding tasks [24, 27, 30].
In this paper, we propose spatiotemporal joint ﬁlter decomposition to decouple spatial and temporal learning while preserving spatiotemporal dependency in a video. As shown in Figure 1, a 3D convolu-tional ﬁlter is jointly decomposed over a group of spatial atoms and temporal atoms. The two groups of atoms can reconstruct the 3D ﬁlter together with the joint coefﬁcients through tensor multiplication. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A single 3D convolutional layer is now divided into three convolutional layers: a temporal-atom layer to focus on time, a spatial-atom layer for space, and a joint coefﬁcient layer to model the spatiotemporal dependency. Thus, spatial and temporal atoms can now be optimized separately.
Different from methods that decorrelate the spa-tial and temporal modeling, the proposed decom-position can still capture spatiotemporal corre-lations in the joint coefﬁcients. Moreover, our approach can also signiﬁcantly reduce model parameters and computation complexity by lim-iting the number of atoms.
Figure 1: Spatiotemporal jointly decomposed convo-lutional ﬁlters (STDCF) over spatial atoms {ψi}M i=1, temporal atoms {φj}N j=1, and joint coefﬁcients α, with
M = 3, N = 2. It divides one spatiotemporal convo-lutional layer into three convolutional layers: a spatial-atom layer, a temporal-atom layer, and a joint coefﬁ-cient layer which mixes information from spatial-atom and temporal-atom convolutional to capture joint depen-dency.
One obvious arithmetic property of our joint decomposition is to allow spatial or temporal atoms to be swapped with a set of atoms with the same number but different sizes while keeping the remaining unchanged. For example, we can now achieve tempo-invariance by simply dilating temporal atoms only. To exploit this useful atom-swapping property, we further show how the proposed spatiotemporal jointly decomposed convolutional ﬁlter (STDCF) permits the direct learning of 3D CNNs with full-size videos.
We start with a pedagogical learning decoupling to train a 3D CNN iteratively with full-temporal downsampled-spatial data ﬁrst, and then with full-spatial downsampled-temporal data. Note that, such simple-minded decoupled learning is less capable of modeling spatiotemporal dependency. A regular 3D CNN model trained from the two-stage strategy above may capture rich temporal features in the ﬁrst stage, however, such features will be severely degraded by the downsampled-temporal data in the second stage.
Exploiting the atom-swapping property of the proposed spatiotemporal joint decomposition, we can instead decouple spatial and temporal learning into iterations of two consecutive sub-stages of learning: temporal-focus stage (stage-t) and spatial-focus stage (stage-s). In stage-t, temporal atoms and joint coefﬁcients are learned from full-temporal downsampled-spatial videos by keeping spatial atoms ﬁxed, while in stage-s, spatial atoms and coefﬁcients are updated from full-spatial downsampled-temporal data by keeping temporal atoms ﬁxed. Note that, in stage-s, spatial atoms are learned from full spatial-resolution, while temporal atoms are frozen to prevent degradation from temporal down-sampling, similar for atoms update in stage-t. The proposed two-stage 3D CNN training strategy can model a full temporal and spatial resolution video only from downsampled data while preserving spatiotemporal dependency. Empirically, we show on multiple action recognition datasets, e.g., KTH [19], Kinetics-400 [1] and Something-Somethingv1 [10], that, the STDCF model trained with the proposed strategy signiﬁcantly reduces memory usage, while producing comparable results with the state-of-the-art models trained with full-size videos. 2 Method
In this section, we start by introducing the proposed spatiotemporal jointly decomposed convolutional
ﬁlter, STDCF, along with its two properties of capturing spatiotemporal dependency and enabling atom swapping. Then, we decouple spatiotemporal learning into iterative two sub-stage learning.
Next, we introduce a simple but effective design, enabled by the atom swapping property, to allow the model to learn from videos with different spatial and temporal resolutions in different sub-stages. 2.1 Spatiotemporal Joint Filter Decomposition
A regular spatiotemporal convolutional ﬁlter in deep video models consists of a group of 3D tensors with shape t × k × k for capturing local joint dependency on space and time, where t and k are temporal and spatial ﬁlter sizes. The 3D structure not only leads to a dramatic increase in parameters compared to 2D CNNs, but also couples the spatial and temporal learning as each tensor attends to both dimensions simultaneously. To decouple spatial and temporal learning, as well as to reduce model parameters, we propose to jointly decompose a 3D ﬁlter over spatial and temporal atoms, as shown in Figure 1. Our idea originates from spatial ﬁlter decomposition as proposed in [17], and we extend it to 3D ﬁlters by taking the time dimension into consideration. 2
Figure 2: Translate-Rotate-MNIST reconstruction results.
As proposed in [17], a 2D spatial ﬁlter W ∈ RCout×Cin×k×k (Cout, Cin are input, output channels, and k is spatial kernel size), can be decomposed as a linear combination of M spatial atoms i=1, ψi ∈ Rk×k, with coefﬁcients α ∈ RM ×Cout×Cin. The spatial convolution then becomes
{ψi}M two: A spatial-atom convolution with ψi, a 1 × 1 convolution with joint coefﬁcients α. This decomposition not only reduces the parameters and computation cost but also imposes low-rank ﬁlter structures.
Then, for a 3D spatiotemporal ﬁlter W ∈ RCout×Cin×t×k×k (t is the temporal kernel size), as shown in Figure 1, we decompose it over spatial atoms Ψ = {ψi}M j=1,
φj ∈ Rt, with joint coefﬁcients α ∈ RCout×Cin×M ×N as W = (cid:80)M
In this way, the 3D convolution becomes three, i=1 and temporal atoms Φ = {φj}N j=1 αi,jψiφj. (cid:80)N i=1
Spatial-atom convolution: Zi(λ(cid:48), t) = (cid:88)
I(λ(cid:48), t, u + u(cid:48))ψi(u(cid:48)),
Temporal-atom convolution: Yij(λ(cid:48)) = u(cid:48) (cid:88)
φj(t(cid:48))Zi(λ(cid:48), t + t(cid:48)), 1 × 1 joint coefﬁcient convolution: J(λ, t, u) = t(cid:48) (cid:88) (cid:88)
αi,j (λ(cid:48),λ)Yij(λ(cid:48)),
λ(cid:48) i,j (1) where I, J denotes the input and output, u ∈ R2, t ∈ R, λ, λ(cid:48) are spatial and temporal indices, λ, λ(cid:48) are output, input channel indices, and Zi, Yij are intermediate outputs of spatial-atom layer and temporal-atom layer. 2.1.1 Spatiotemporal Dependency
Note that a popular alternative way of 3D ﬁlter decomposition is to apply rank-1 decomposition to obtain a 2D spatial ﬁlter and a 1D temporal ﬁlter, as proposed in [18, 26, 37]. Speciﬁcally, a 3D ﬁlter
W ∈ RCin×Cout×t×k×k is decomposed into Ws ∈ RCin×Cout×1×k×k and Wt ∈ RCout×Cout×t×1×1 as W = Ws × Wk. In this way, 3D convolution becomes two: spatial sub-convolution with Ws, and temporal sub-convolution with Wt.
However, such a rank-1 decomposition neglects joint spatiotemporal dependency. The spatial sub-convolution only focuses on spatial modeling, while the temporal sub-convolution merely attends to temporal modeling. The spatial and temporal features are extracted independently by omitting spatiotemporal correlation. Instead, in our STDCF, joint coefﬁcients α encode spatiotemporal dependency while keeping learning in both dimensions independent. We derive below that STDCF has more capacity than rank-1 decomposition. Recall our joint decomposition of 3D ﬁlter W ,
W(λ,λ(cid:48)) = (cid:88) i,j
αi,j (λ,λ(cid:48))ψi(u)φj(t), αi,j (λ,λ(cid:48)) ∈ R, (2)
λ ∈ [C], λ(cid:48) ∈ [C (cid:48)], i ∈ [M ], j ∈ [N ], and αi,j
{αi,j (λ,λ(cid:48))}i,j is a M -by-N matrix, and generally is full rank. (λ,λ(cid:48)) are freely trainable. For each ﬁxed pair of (λ, λ(cid:48)),
In the rank-1 decomposition, spatial ﬁlters are Ws = W (λ(cid:48)(cid:48),λ(cid:48))(u), temporal ﬁlters are Wt =
W (λ,λ(cid:48)(cid:48))(t), λ(cid:48)(cid:48) ∈ [C (cid:48)(cid:48)]. We can write W (λ(cid:48)(cid:48),λ(cid:48)) as combination of atoms ψi and φj and W (λ,λ(cid:48)(cid:48)) s t 3
Figure 3: Tempo discrepancy experiments on KTH. respectively,
W (λ(cid:48)(cid:48),λ(cid:48)) s
= (cid:88) i (λ(cid:48)(cid:48),λ(cid:48))ψi, W (λ,λ(cid:48)(cid:48))
µi t
= (cid:88) j
νj (λ,λ(cid:48)(cid:48))φj.
Tracking the degree of freedom reveals that rank-1 decomposition is more restrictive than our STDCF.
Let C = C (cid:48) = C (cid:48)(cid:48), α in STDCF has M N C 2 many variables if all free. In comparison, µ has M C 2 many variables, and ν has N C 2 many, thus the rank-1 decomposed convolution formulation has only (M + N )C 2 many free variables in total. This quantiﬁes the loss of expressiveness from STDCF to rank-1 decomposition. A more comprehensive analysis is provided in Appendix A.
To empirically illustrate STDCF is more expressive than rank-1 decomposition, we design a toy example on the translate-rotate-MNIST (TR-MNIST) dataset. It consists of short clips of moving digits, which contain complex spatiotemporal correlations. Adopting a 3-layer 3D CNN, we conduct video reconstruction experiments and compare the qualitative and quantitative results between STDCF and rank-1 decomposition. Details of experimental settings are provided in Appendix B. As shown in Figure 2, given a sequence of moving-rotating digits, STDCF reconstructs in better quality than rank-1. As for qualitative results, we measure mean PSNR on the test set. STDCF achieves a mean
PSNR of 24.60, whereas the result of rank-1 decomposition is 21.50, indicating STDCF possesses more capacity to capture spatiotemporal correlations. 2.1.2 Atom Swapping Property
The proposed joint decomposition further enables us to manipulate the atoms by changing their spatial or temporal sizes, while keeping their numbers and the rest of components ﬁxed. In this way, we can handle variances in a speciﬁc domain by adapting only the corresponding atoms, while keeping the knowledge about spatiotemporal correlation and another domain intact. Rather, we would have to modify the whole 3D ﬁlters unwieldy. We call this kind of atom manipulations Atom Swapping. We illustrate next its potential usage with a toy tempo-invariance example.
In the temporal domain of videos, a common variance is tempo changes τ , which characterizes the moving speed of objects in a video. We create a toy example based on KTH [19] to illustrate this in Figure 3(a), where the tempo discrepancy happens between training clips (τtrain = 1) and test clips (τtest = 2). This leads to test clips proceeding faster than training clips. Without atom swapping, the representation of the model can suffer from severe distortions on the testing set, as shown in Figure 3(b). To tackle this, we can swap temporal atoms with the ones having the corresponding dilation. As shown in Figure 3(b), the model with swapped temporal atoms almost restores the representation compared to the baseline. We provide quantitative results for dilated atoms in Section 3.1. 2.2 Decoupled Spatiotemporal Learning
By exploiting the atom swapping property above, we will illustrate next how the proposed STDCF enables direct learning of 3D CNNs with full-size videos. Training deep video models usually utilizes large-scale video datasets with samples in the size of T × H × W . When T or H, W are large enough, model training will require a signiﬁcantly large amount of memory. The proposed STDCF jointly decomposes 3D ﬁlters as well as 3D convolution, and thus decouples spatial and temporal learning, while preserving dependency across both dimensions. As shown in Figure 4, decoupled learning is performed as iterations of two consecutive sub-stages of learning to model full-size videos but from down-sampled data only, to signiﬁcantly reduce the memory usage in training.
Temporal-Focus Learning (stage-t). In this stage, we focus on modeling the temporal dimension using full-temporal downsampled-spatial videos by a factor β. The original temporal sampling rate is adopted here to update temporal atoms {φj}N j=1to best encode the temporal knowledge. The 4
Figure 4: Iterative two-stage spatiotemporal learning. By decomposing 3D ﬁlter W , we convert spatiotemporal learning from full spatial and temporal resolution videos, which is usually infeasible due to memory issues, to iterations of temporal-focus learning from full-temporal downsampled-spatial videos, and spatial-focus learning from full-spatial downsampled-temporal videos that dramatically reduces the memory footprints. The superscripts of atoms and coefﬁcients denote the current iteration (e) and the sub-stage of learning (t for stage-t, and s for stage-s). Note that our joint decomposition allows us to swap atoms with different dilation to handle resolution changes, e.g. {ψe,t j } has dilation 1 for 2x downsampled-temporal data. j } has dilation 2 to deal with full-temporal data, whereas {ψe,s joint coefﬁcients α are also updated to capture the spatiotemporal dependency. As we downsample training video spatially by a factor β to meet the memory limit, we freeze spatial atoms {ψi}M i=1to preserve knowledge learned on full spatial-resolution videos in the spatial-focus learning stage next.
Spatial-Focus Learning (stage-s). Similarly, in this stage, we focus on modeling the spatial dimen-sion using full-spatial downsampled-temporal videos by a factor of γ. The full spatial resolution is used to update spatial atoms {ψi}M i=1to best encode the spatial knowledge, while temporal atoms
{φj}N j=1are frozen to preserve knowledge learned on full frame-rate videos in stage-t above. The joint coefﬁcient α is updated about the spatiotemporal dependency.
Iterative Two-Stage Learning. The above two stages are iterated to model full temporal and spatial resolution videos from down-sampled ones, with far fewer memory footprints, as elaborated in
Algorithm 1. Intuitively, more temporal knowledge gained in stage-t will further help spatial learning in the subsequent stage-s and reﬁne spatiotemporal dependency in joint coefﬁcients. Such iterative two-stage learning (ITSL) is empirically validated in Section 3.1.
We provide an interpretation of our iterative two-stage learning from the view of alternating opti-mization. After being jointly decomposed over spatial and temporal atoms, a 3D ﬁlter W becomes
W = ΨαΦT , where we use matrix notation here for simplicity, spatial atoms Ψ ∈ Rk2×M , and temporal atoms Φ ∈ Rt×N , and the columns of Ψ (Φ) consist of the M (N ) spatial (temporal) atoms.
The atoms Ψ and Φ, and the joint coefﬁcients α are all trainable, and in our proposed algorithm, we train the triplet in an alternative fashion. In the stage-s, we ﬁx temporal atoms Φ, and optimize
Ψ and α jointly, which is equivalent to training in a spatial CNN; In the stage-t, we ﬁx the spatial atoms Ψ, and optimize Φ and α jointly, and this is equivalent to training in a temporal CNN. As a result, the optimization in stage-s prepares an improved model for the stage-t, and vice versa, and the alternative optimization converges suppose each step can make progress. Combined with our dilated atoms discussed next, in each step we adjust the sampling rate of the data accordingly so as to most efﬁciently use the memory. The alternative stages store the model reﬁnement in the shared α, which is passed to the next stage, and α is independent of geometrical dilation or ﬁlter decomposition in space or time. 2.3 Dilated Atoms
In ITSL, a severe problem lies in the resolution variation in different sub-stages. Considering the spatial dimension as an example, videos with downsampled spatial resolution, e.g. H 2 are used in stage-t, whereas in stage-s and ﬁnal testing, full spatial size H × W videos are used; similar for different temporal resolution across sub-stages. We observe this resolution discrepancy in learning and testing will lead to signiﬁcant performance degradation, which poses a great challenge to our proposed learning strategy. For instance, a model learned on stage-t will perform poorly on data in stage-s due to the mismatched resolutions, e.g. T v.s. T 2 . 2 × W 5
Algorithm 1 Iterative Two-Stage Learning
Require:
, φ0,s j
, α0,s ←− ψi, φj, α
Full-size video training dataset D = {(xi, yi)}N
Initialize the STDCF model M with spatial atoms {ψi}M
Classiﬁcation loss function L. Iterations E.
ψ0,s i for e = 1, 2, ..., E do stage-t Training:
Acquire full-temporal downsampled-spatial data: Ds = {(x(cid:48) j , αe,t ←− ∇ML(M(Ds|ψe−1,s
φe,t stage-s Training:
Acquire full-spatial downsampled-temporal data: Dt = {(x(cid:48)(cid:48)
ψe,s i end for
Return M(ψE,s i ←− ∇ML(M(Dt|ψe,t j , αe,t), φe,s
, αe−1,s), ψe,t j ←− φe,t
, φe−1,s j
, αE,s)
, αe,s
, φe,t j i i
, φE,s j i i=1, xi of size T × H × W i=1, temporal atoms {φj}N j=1, joint coefﬁcient α i=1, x(cid:48) i of size T × H
β × W
β i, yi)}N i ←− ψe−1,s i i , yi)}N i=1, x(cid:48)(cid:48) i of size T
γ × H × W
As discussed in Section 2.1.2, we can handle this problem using dilated atoms. To be speciﬁc, model learned in stage-t from videos of size T × H i=1 dilated by factor
β when learned in stage-s and tested on full spatial-resolution videos. Similarly, temporal atoms
{φj}N j=1 will be dilated by γ when the model is trained in stage-t and tested, and dilated by 1 in stage-s.
β , has its spatial atoms {ψi}M
β × W 2.4 Parameter and Computation Reduction
Regular 3D ﬁlters have a signiﬁcant increase in parameters and computation ﬂops compared to 2D
ﬁlters, which contributes to the huge demand on memory and computation power. The proposed
STDCF reduces the parameters and computation complexity of 3D ﬁlters signiﬁcantly.
To be speciﬁc, a regular 2D ﬁlter is of size Cin × Cout × k × k has CinCoutk2 parameters, while regular 3D ﬁlter has CinCouttk2 parameters with the additional time dimension. In STDCF, the joint coefﬁcient α has CinCoutM N parameters, and spatial and temporal atoms {ψi}M j=1 has
M k2, N t parameters, respectively. Thus, the reduction rate in number of parameters compared to the original 3D ﬁlter is N t+M k2+CinCoutM N
. In practice, CinCoutM N is much larger than N t + M k2,
CinCouttk2 k2 than regular 3D ﬁlters, and M N which gives the approximate reduction of parameters rate of N k2 compared to regular 2D ﬁlters. If t = k = 3, and we can choose M = 3, N = 2, giving a reduction rate of 2 3 compared to non-decomposed 2D ﬁlters.
As for the computation cost, regular 2D convolution with input with shape of Cin × W × W needs CoutCinW 2(1 + 2k2) ≈ 2CoutCinW 2k2 FLOPs, and regular spatiotemporal convolution with inputs in size of Cin × T × W × W costs CoutCinW 2T (1 + 2k2t) ≈ 2CoutCinW 2T k2t
FLOPs. In STDCF, the computation is split into three sub-layers: (1) spatial-atom layer with ψi costs 2M k2CinT W 2 FLOPs, (2) temporalatom layer with φj requires 2N tM CinT W 2 FLOPs, (3) coefﬁcient layer with α on every spatiotemporal location costs 2CoutCinM N W 2T FLOPs totally.
Together, the computation cost is 2CinM T W 2(k2 + N t + N Cout) FLOPs. Since N Cout is usually 9 compared to non-decomposed 3D ﬁlters, and 2 i=1, {φj}N t · M
Table 1: Dilated atoms experiment. Dte in-dicate the dilation of spatial/temporal atoms in the testing/learning phase. s , Dtr t
Table 2: Accuracies and GPU memory footprints of different 3D layers on KTH dataset under regular full-size videos learning and the proposed iterative two-stage learning.
Train size
Test size
Dilation Dte 16x30x40 16x60x80 s = 1 Dte 53.08
Acc. s = 2 Dtr 75.34 16x60x80 8x60x80 t = 1 Dtr 66.49 t = 2 77.11 full-size learning
ITSL (E = 3)
Layer type Memory
Acc.
Memory
Acc.
Reg. 3D 459.7M 81.26 229.5M (50.07% ↓) 76.94 (4.32 ↓)
Rank-1 3D 582.5M 82.55 290.5M (50.13% ↓) 79.10 (3.45 ↓)
STDCF 459.4M 85.61 228.9M (50.17% ↓) 84.93 (0.68 ↓) 6
much larger than k2 + N t, the cost is around 2CinCoutT W 2M N FLOPs, showing that the reduction rate in computation FLOPs is again N k2 compared to regular 3D convolution. t · M 3 Experiments
In this section, we validate the proposed approach on multiple action recognition datasets, KTH [19],
Kinetics-400 [1], and Something-Somethingv1 [10]. 3.1
Illustrative Experiments
We illustrate the basic idea of the proposed decomposed learning process on the KTH action recog-nition dataset [19]. The KTH dataset consists of 600 videos at 25 fps of 6 action classes from 25 people. The videos are further sampled as short clips of size 16 × 80 × 60, which are used in the full-size learning setting. The training set contains 4667 clips, and the validation set has 4551 clips.
Models are learned on the training set and report accuracy on the validation set.
We adopt a simple three-layer 3D CNN model with regular 3D convolution (Reg. 3D) as the baseline, and further replace each convolutional layer with STDCF and Rank-1 3D decomposed convolutional layer (Rank-1 3D) in Section 2.1.1, as shown in Table B. For training models with full-size data, we set batch-size as 16, learning rate as 0.001, and train all models for 40 epochs, with learning rate reduced to 0.0001 at the 20th epoch. For ITSL, we set total iterations E = 3, β = γ = 2, i.e. we downsample clips spatially to the size of 16 × 30 × 40 in stage-t, and temporally to 8 × 60 × 80 in stage-s. To counter the discrepancy in resolution, we dilate temporal atoms by 2 in stage-t, spatial atoms by 2 in stage-s, and both atoms by 2 for testing on full-size clips. We also evaluate the Reg. 3D and Rank-1 3D model under ITSL data setting. For Reg. 3D, we train its 3D ﬁlters in both stage-t and stage-s with downsampled data, and for Rank-1 3D, we train its Wt with full-temporal downsampled-spatial videos, Ws with full-spatial downsampled-temporal videos. Dilated convolutions are applied to these two models with the same dilation setting as STDCF. The training setting aforementioned is adopted to each stage-t and stage-s, and all models trained with ITSL are tested with full-size clips.
We illustrate the effectiveness of the proposed dilated atoms in Table 1. A STDCF model is learned and tested on data with resolution discrepancy. The poor performance caused by the spatial discrepancy can be improved by dilating spatial atoms during testing. Same for the temporal dimension, dilating temporal atoms can counter the degradation caused by variation in temporal resolution.
Then, as shown in Table 2, we report accura-cies on validation clips and the maximum train-ing GPU memory usage with the above settings.
Speciﬁcally, in ITSL experiments, we report the max memory usage in one iteration, which oc-curs in stage-s. Under the full-size learning set-ting, STDCF gives better performance over the
Reg. 3D and the Rank-1 3D, showing its effec-tiveness in capturing spatiotemporal dependency.
In the ITSL setting, the GPU memory used is sig-niﬁcantly reduced for all models (around 50%).
However, the performance of Reg. 3D model drops by a large margin because of the down-sampled training clips as well as the coupled spatial and temporal learning. The Rank-1 3D model decouples spatial and temporal learning but still shows a large accuracy drop as it can not fully leverage the iterative learning. On the other hand, STDCF with ITSL gives the closest accuracy to the one obtained in full-size learning by a small difference of 0.68%, validating that it not only decouples spatial and temporal learning but also reﬁnes spatiotemporal modeling iteratively.
Figure 5: Accuracies of different models after every iteration of two-stage spatiotemporal learning. The num-ber in the parenthesis is the relative performance gain w.r.t. the model trained after stage-t in the ﬁrst iteration.
We further plot accuracies of different models at each learning iteration, as shown in Figure 5. Simi-larly, due to coupled spatial and temporal learning, Reg. 3D model only gains 0.22 in performance in stage-s compared to stage-t in the ﬁrst learning iteration, and its performance does not increase signiﬁcantly (by 1.17, 0.7) in the two learning iterations afterward. As for the Rank-1 3D model, though it shows a large increase from Iter_stage-t to Iter_stage-s in Iter1, it does not produce a substantial gain in accuracy in Iter2, Iter3. In comparison, the proposed STDCF encodes spatial and 7
Table 3: Results on Kinetics-400. We only compare with methods with the same setting as ours.
Model
#Params Memory
Top-1
Top-5 GFLOPs
R(2+1)D [26]
SlowFast [8]
TSM [15]
TIN [21]
STDCF-R50-t-3
STDCF-R50-s-3
STDCF-R50 63.7M 34.6M 24.3M 24.3M 18.5M 5.0G 9.1G 7.1G 6.2G 1.9G 3.8G 7.9G 73.1 75.6 74.1 70.9 73.6 74.0 74.5 89.8 92.3 90.8 89.8 90.3 90.6 91.2 53.2 65.7 33.0 34.0 52.2 temporal knowledge to the corresponding atoms in stage-t and stage-s separately, and thus shows a signiﬁcant leap in accuracy from Iter_stage-t to Iter_stage-s. Besides, as the joint coefﬁcient α captures richer spatiotemporal dependency with the model trained iteratively, the STDCF model shows a consistent and substantial performance gain in the next two learning iterations (by 1.87, 1.85), and an accuracy comparable to full-size learning in the end. 3.2 Experiments on Kinetics-400 3.2.1 Experimental Setup
Kinetics-400 [1] consists of around 240k training videos, 19k validation videos for total 400 action classes. All experiments are conducted with RGB modality and evaluated on validation sets. Follow-ing the settings in [8], input frames are sampled temporally from 64 consecutive frames at an interval of 8, which is the full-temporal size. Spatially, each frame is randomly rescaled so that its short side is in [256, 320], and then randomly ﬂipped, cropped into 224 × 224, which is the full-spatial size in our ITSL setting. As for ITSL setting, we set E = 3, β = γ = 2, so STDCF-R50 is learned with videos of the size 8 × 112 × 112 in stage-t, and with videos of the size 4 × 224 × 224 in stage-s.
We adopt 3D ResNet-50 (3D-R50) as the backbone, which inﬂates 3 × 3 kernels in 2D ResNet-50
[11] to 3 × 3 × 3. We further substitutes all those spatiotemporal ﬁlters with our STDCF, where we set M = 6, N = 2, and obtain STDCF-R50. The architecture of STDCF-R50 is shown in Table C.
STDCF-R50 is learned from scratch for 256 epochs in stage-t of the ﬁrst iteration (e = 1) with the
SGD optimizer (momentum of 0.9) of learning rate of 0.03, and weight decay of 0.0001. In stage-s and stage-t of e = 2, 3, STDCF-R50 is learned with the same hyper-parameters as in stage-t of the
ﬁrst iteration except for learning rate reduced to 0.0005, and updated for 50 epochs. For inference, we adopt the 30-crop setting as in [8] to conduct the inference with full-size videos. Speciﬁcally, we uniformly sample 10 clips from a video along its temporal dimension. Then, we downsample each frame spatially so that its short side size is 256, and take three crops of 256 × 256 covering all spatial information. We dilate both temporal atoms and spatial atoms by 2 in inference for STDCF-R50-t and STDCF-R50-s. 3.2.2 Results
We ﬁrstly compare STDCF-R50 learned in stage-t, STDCF-R50-t-3, with the one in stage-s that has updated spatial atoms, STDCF-R50-s-3 of the ﬁnal iteration (E = 3). As shown in Table 3, the performance gain in STDCF-R50-s-3 w.r.t. STDCF-R50-t-3 indicates the temporal knowledge and spatial knowledge are learned separately without mutual interference, validating the effectiveness of the proposed ITSL. We further compare STDCF-R50-s-3 with the model learned from full-size data, STDCF-R50. The only 0.5% performance gap shows the proposed model learned with iterative learning strategy can achieve nearly the same performance of the full-size learned model with a half
GPU memory usage reduction. The accuracies of models in all iterations are provided in Table D.
Then, we compare our models with other state-of-the-art models. Note that for a fair comparison, we only compare with methods using networks of the same depth, and the same data setting as ours.
STDCF-R50 learned with ITSL shows comparable results with other methods, with signiﬁcantly fewer GPU memory footprints in the learning phase. Moreover, we evaluate the number of parameters and computation FLOPs per clip in the inference phase. Compared to other methods, STDCF models show a signiﬁcant reduction in the number of parameters and comparable FLOPs. 8
Table 4: Results on Something-Somethingv1
Method
Memory
Pretrain
# frames Val Top-1 Val Top-5 2D-R50
TSN [29]
TRN [40]
I3D [1]
I3D+GCN [31]
TIN [21]
TSM [15]
STDCF-R50-t-2
STDCF-R50-s-2
STDCF-R50 5.9G
----7.5G 7.1G 1.9G 3.8G 7.8G
ImageNet
ImageNet
ImageNet
Kinetics
Kinetics
Kinetics
ImageNet
Kinetics
-Kinetics 8 16 8 32 32 8 8 8 4 8 22.3 19.7 34.4 41.6 43.3 45.8 45.6 44.8 45.1 45.9 48.2 46.6
-72.2 75.1 75.1 74.2 74.3 74.7 75.2 3.3 Experiments on Something-Somethingv1
Something-Somethingv1 [10] is a more challenging dataset, as the activity can only be inferred by modeling spatiotemporal joint dependency. It contains 86k training videos and 12k validation videos for 174 classes. We apply the same model STDCF-R50 with pretrained on Kinetics-400. The model learned directly from full-size videos is ﬁnetuned for 50 epochs with batchsize of 64, learning rate of 0.001 which drops by 0.1 at the 30th epoch. For the ITSL setting, we set E = 2, β = γ = 2. Models learned with ITSL are all ﬁnetuned for 20 epochs with learning rate of 0.0002 which drops by 0.1 at the 15th epoch.
As shown in Table 4, our full-size learned model, STDCF-R50, achieves competitive results with other methods. Moreover, our model learned with ITSL can nearly recover the full model’s performance, while reducing the GPU memory usage by half. The results of two iterations of ITSL are illustrated in Table E. 3.4 Visualization
KTH visualization. To better understand the proposed decoupled spatiotemporal learning, we visualize the feature maps of the last convolutional layer in KTH experiment, as shown in Figure 6. Note that we sum all feature maps along the channel index, and adds it back to original frames.
We compare the model from the ﬁrst iteration stage-t (Iter-1 stage-t) and the one from the second iteration stage-s (Iter-2 stage-s). In both Figure 6a and Figure 6b, the Iter-1 stage-t model fail to (a) (a) (b)
Figure 6: Feature maps added back to original video frames to show where the network attends to. In (a),
Iter-1 stage-t model does not focus on the right arm region while Iter-2 stage-s model does. In (b), Iter-1 stage-t model does not focus on right and left arms regions while Iter-2 stage-s model does. (b)
Figure 7: Activation maps of stage-t and stage-s mod-els that highlights region contributing to the prediction.
In (a), both stage-t and stage-s predict right, but the model ﬁne-tuned in stage-s focuses more on the ﬁnger with cream. In (b), stage-s model corrects the wrong prediction in stage-t from applauding to barbecuing by attending to meats and sausages. 9
attend to the discriminative part of frames, i.e., right arm in Figure 6a, two arms in Figure 6b, whereas the Iter-2 stage-s does by learning ﬁner spatial knowledge.
Kinetics visualization. We further use Grad-CAM [20] to visualize the activation maps of STDCF-R50-t and STDCF-R50-s on Kinetics that highlight region contribution to the prediction, as shown in Figure 7. The top row of frames in Figure 7a illustrates the activation maps of STDCF-R50-t, and the bottom row of frames are for STDCF-R50-s. The frames belong to class ’applying the cream’. It shows that while the stage-t model captures moving regions and gets the correct prediction, the learning in stage-s reﬁnes the activations on space to the most discriminative areas. In Figure 7b, activations for stage-t model on the top row attends to wrong regions and makes the wrong prediction. By ﬁne-tuning in stage-s, the model focuses on the correct area and corrects its prediction to ’barbecuing’. 4