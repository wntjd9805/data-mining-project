Abstract
Articulation-centric 2D/3D pose supervision forms the core training objective in most existing 3D human pose estimation techniques. Except for synthetic source environments, acquiring such rich supervision for each real target domain at de-ployment is highly inconvenient. However, we realize that standard foreground silhouette estimation techniques (on static camera feeds) remain unaffected by domain-shifts. Motivated by this, we propose a novel target adaptation framework that relies only on silhouette supervision to adapt a source-trained model-based regressor. However, in the absence of any auxiliary cue (multi-view, depth, or 2D pose), an isolated silhouette loss fails to provide a reliable pose-speciﬁc gradient and requires to be employed in tandem with a topology-centric loss. To this end, we develop a series of convolution-friendly spatial transformations in order to disentangle a topological-skeleton representation from the raw silhouette. Such a design paves the way to devise a Chamfer-inspired spatial topological-alignment loss via distance ﬁeld computation, while effectively avoiding any gradient hinder-ing spatial-to-pointset mapping. Experimental results demonstrate our superiority against prior-arts in self-adapting a source trained model to diverse unlabeled target domains, such as a) in-the-wild datasets, b) low-resolution image domains, and c) adversarially perturbed image domains (via UAP). 1

Introduction
Human pose recovery has garnered a lot of research interest in the vision community. It ﬁnds extensive use in a wide range of applications such as, augmented reality, virtual shopping, human-robot interaction, etc. Recent advances in human pose recovery is largely attributed to the availability of 3D pose supervision from large-scale datasets [19, 42, 57]. Although, the models achieve superior performance on in-studio benchmarks, they usually suffer from poor cross-dataset performance.
Training on synthetic and in-studio datasets, which lack diversity in subject appearance, lighting, background variation, among more, can inherently induce a domain-bias [36, 27] thereby restricting generalizability. This calls for the question: how to bridge performance gaps across diverse data domains?
Some works use weaker forms of supervision such as 2D pose keypoints, either directly [23, 43] from the manually annotated large-scale datasets [1, 21] or indirectly [5, 26] via an off-the-shelf 2D pose detector network. Further, multi-view [51, 20] cues have also been used to learn a reliable 2D-to-3D mapping. However, acquiring such additional supervision often comes at a cost, such as laborious manual skeletal-joint annotation or cumbersome calibrated multi-camera setups. This severely limits the deployability of a model in a novel, in-the-wild target environment. Images in a target deployment scenario can vary from those used in training setup in several ways, from simple changes in lighting or weather, to large domain shifts such as thermal or near-infrared (NIR) imagery.
It is highly impractical to gather pose annotations for every novel deployment scenario.
∗Equal contribution. | Webpage: https://sites.google.com/view/align-topo-human 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The proposed self-supervised adaptation relies only on silhouette supervision (unaffected by domain-shift) to adapt the source trained pose regressor to novel target deployment scenarios. Topology extraction from raw foreground silhouettes is one of the key design aspects of our framework.
Motivated by these observations, we aim to realize a self-adaptive monocular 3D human pose recovery framework that does not rely on any strong ground-truth (2D/3D pose) or auxiliary target supervision (multi-view, or depth). The proposed adaptation framework (Fig. 1) aims to transfer the task knowledge from a labeled source domain to an unlabeled target (Fig. 1B), while relying only on foreground (FG) silhouettes. We observe that the traditional FG segmentation techniques are mostly unaffected by input domain shifts (Fig. 1C) and thus the FG masks are usually robust and easy to obtain in diverse target scenarios. Consider a deployment scenario where a static camera is capturing a moving object. Here, one can rely on classical vision based background subtraction techniques to obtain a FG silhouette while being considerably robust to a wide range of domain shifts (including gray-scale, thermal, NIR imaging, etc). Certain deep learning based class-agnostic
FG segmentation methods [61, 39] (motion and salient-object segmentation methods) also exhibit reasonable robustness to domain-shifts. Hence, an adaptation framework that can effectively use silhouette supervision would open up the scope for adaptation to diverse deployment scenarios.
Prior human mesh recovery works [26, 20, 23, 24] have demonstrated considerable performance gains by training with articulation-centric annotations such as 3D and 2D pose supervision. At the same time, silhouettes have found a relegated use, mostly towards enforcing auxiliary shape-centric objectives [9, 47]. Although these weaker objectives employ silhouettes, they do not offer strong supervisory signal in isolation, hence are not self-sufﬁcient. In this paper, we aim to realize a stronger, self-sufﬁcient and articulation-centric supervision from raw silhouettes. The key challenge is that the articulation information is usually lost in direct pixel-level losses, as raw silhouettes mostly provide 2D shape information. Our key insight is to extract a skeleton-like 2D topology information from silhouettes (see Fig. 1D), which can provide a stronger articulation-centric supervision. We further propose a topological alignment objective that is inspired from 3D Chamfer distance [11] and can directly work on binary images. We leverage this objective to drive our target adaptation training using just silhouettes, and demonstrate the applicability of our framework across various domains.
We make the following main contributions:
• We develop a series of convolution-friendly spatial transformations in order to disentangle a topological-skeleton representation from the raw silhouette.
• We devise a Chamfer-inspired spatial topology alignment loss via distance ﬁeld computation, while effectively avoiding any gradient hindering spatial-to-pointset mapping.
• Our self-supervised adaptation framework achieves state-of-the-art performance on multiple chal-lenging domains, such as low-resolution domain (LR), universal adversary perturbed domain (UAP), in-studio Human3.6M [19], and in-the-wild 3DPW [58]. 2