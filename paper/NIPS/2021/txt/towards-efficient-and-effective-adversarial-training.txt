Abstract
The vulnerability of Deep Neural Networks to adversarial attacks has spurred immense interest towards improving their robustness. However, present state-of-the-art adversarial defenses involve the use of 10-step adversaries during training, which renders them computationally infeasible for application to large-scale datasets.
While the recent single-step defenses show promising direction, their robustness is not on par with multi-step training methods.
In this work, we bridge this performance gap by introducing a novel Nuclear-Norm regularizer on network predictions to enforce function smoothing in the vicinity of data samples. While prior works consider each data sample independently, the proposed regularizer uses the joint statistics of adversarial samples across a training minibatch to enhance optimization during both attack generation and training, obtaining state-of-the-art results amongst efﬁcient defenses. We achieve further gains by incorporating exponential averaging of network weights over training iterations. We ﬁnally introduce a Hybrid training approach that combines the effectiveness of a two-step variant of the proposed defense with the efﬁciency of a single-step defense.
We demonstrate superior results when compared to multi-step defenses such as
TRADES and PGD-AT as well, at a signiﬁcantly lower computational cost. 1

Introduction
The past decade has witnessed the rise of Deep Learning based systems and solutions in many applications related to Computer Vision, Natural Language Processing and Speech Processing.
Despite the remarkable success of Deep Networks, they are known to be susceptible to crafted imperceptible noise called Adversarial Attacks [27], which can have disastrous implications when deployed in critical applications such as self-driving cars, medical diagnosis and surveillance systems.
The robustness of Deep Networks to Adversarial Attacks has been of prime importance, since this relates to improving their worst-case performance [6].
Early attempts of improving robustness to adversarial attacks could be broadly classiﬁed into input pre-processing based defenses [4, 32, 25, 13] and adversarial training methods [11, 20]. While the input pre-processing based defenses were computationally cheap, they primarily involved masking of gradients to prevent the generation of strong attacks. Such methods were later shown to be ineffective against attacks constructed using expectation over the randomized components or smooth approx-imations of non-differentiable components [3]. Amongst the latter methods, the most successful early defense which stood the test of time was the Projected Gradient Descent (PGD) adversarial training method proposed by Madry et al. [20]. This involved minimization of cross-entropy loss on the worst-case perturbations generated using multiple iterations of constrained optimization, leading to a signiﬁcantly higher computational cost when compared to standard training.
∗Equal contribution.
Correspondence to: Gaurang Sriramanan <gaurangs@umd.edu>, Sravanti Addepalli <sravantia@iisc.ac.in> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
FGSM (Fast Gradient Sign Method) based adversarial training [11] alleviates the computational cost by utilizing single-step adversarial samples for training. However, this method is contingent on the local linearity assumption of the loss surface, which is often compromised during the process of
FGSM training, leading to generation of weaker adversaries as training progresses. This leads to the generation of models which exhibit a false sense of robustness towards single-step attacks and are susceptible to stronger multi-step attacks. Although there have been several recent works [29, 30, 26] to circumvent gradient masking and improve the worst-case accuracy of single-step adversarial training methods, the robustness thus achieved has been suboptimal compared to multi-step defenses.
In this work, we improve the effectiveness of single-step adversarial training by introducing a novel
Nuclear-Norm regularizer to impose local smoothness in the vicinity of data samples. The proposed
Nuclear-Norm Adversarial Training (NuAT) utilizes batch statistics to limit the oscillation of function values in the required dimensions, thereby preventing the over-smoothing of loss surface uniformly.
We summarize our contributions of our work below:
• We propose single-step Nuclear Norm Adversarial Training (NuAT), and a two step variant of the same (NuAT2), coupled with a novel cyclic-step learning rate schedule, to achieve state-of-the-art results amongst efﬁcient training methods, and results comparable to the effective multi-step training methods.
• We demonstrate improved performance by using exponential averaging of the network weights coupled with cyclic learning rate schedule (NuAT-WA).
• We propose a two-step variant NuAT2-WA, that utilizes stable gradients from the weight-averaged model for stable initialization in the ﬁrst attack step, yielding improved results.
• We improve the stability and performance of the single-step defense (NuAT) further at a marginal increase in computational cost, by introducing a hybrid approach (NuAT-H) that switches adaptively between single-step and two-step optimization for attack generation.
The organization of this paper is as follows: The preliminaries on notation and threat model are laid out in Section-2, followed by a brief note on related works in Section-3 and a discussion on challenges in single-step adversarial training in Section-4. We present a detailed description of our proposed method in Section-5, followed by experimental results to support our claims in Section-6.
We conclude the paper with our analysis and future directions in Section-7.
Our code and pre-trained models are available here: https://github.com/val-iisc/NuAT. 2 Preliminaries
In this paper, we denote x to be a d-dimensional image from an N -class dataset D. Further, we denote its corresponding ground-truth label as a one-hot vector y. Let fθ represent a Deep Neural Network with parameters θ, that maps an input image x to its pre-softmax output fθ(x). The cross-entropy loss corresponding to the network prediction on a sample (x, y) is denoted as (cid:96)CE(fθ(x), y).
For a minibatch B = {(xi, yi)}M i=1, we denote X as the image matrix whose ith row consists of
ﬂattened pixel intensities of the image xi, and Y as the corresponding ground-truth array. Thus, X is a matrix of size (M × d), and Y is a matrix of size (M × N ). Let (cid:96)CE(fθ(X), Y ) denote the sum of cross-entropy losses over all data samples in the minibatch B. Further, for a matrix A, let ||A||∗ denote the Nuclear Norm, that is the sum of the singular values of A.
Adversarial Threat Model: In this paper, we consider the robustness of Deep Networks against (cid:96)∞ constrained adversaries. Thus under an ε-constraint, given a clean image x, an adversarially perturbed counterpart (cid:101)x can differ by at most ε at any given pixel location. Further, a network fθ is said to be ε-robust on a clean sample x with label y, if fθ((cid:101)x) = y, for all perturbations (cid:101)x such that
||x − (cid:101)x||∞ ≤ ε. We note that the proposed approach can be extended to other threat models as well. 3