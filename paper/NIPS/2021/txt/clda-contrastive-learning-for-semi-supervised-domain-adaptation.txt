Abstract
Unsupervised Domain Adaptation (UDA) aims to align the labeled source distri-bution with the unlabeled target distribution to obtain domain invariant predictive models. However, the application of well-known UDA approaches does not gener-alize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where few labeled samples from the target domain are available. This paper proposes a simple
Contrastive Learning framework for semi-supervised Domain Adaptation (CLDA) that attempts to bridge the intra-domain gap between the labeled and unlabeled target distributions and the inter-domain gap between source and unlabeled target distribution in SSDA. We suggest employing class-wise contrastive learning to reduce the inter-domain gap and instance-level contrastive alignment between the original(input image) and strongly augmented unlabeled target images to mini-mize the intra-domain discrepancy. We have empirically shown that both of these modules complement each other to achieve superior performance. Experiments on three well-known domain adaptation benchmark datasets, namely DomainNet,
Ofﬁce-Home, and Ofﬁce31, demonstrate the effectiveness of our approach. CLDA achieves state-of-the-art results on all the above datasets. 1

Introduction
Deep Convolutional networks [30, 52] have shown impressive performance in various computer vision tasks, e.g., image classiﬁcation [19, 22] and action recognition [48, 23, 57, 32]. However, there is an inherent problem of generalizability with deep-learning models, i.e., models trained on one dataset(source domain) does not perform well on another domain. This loss of generalization is due to the presence of domain shift [11, 55] across the dataset. Recent works [46, 29] have shown that the presence of few labeled data from the target domain can signiﬁcantly boost the performance of the convolutional neural network(CNN) based models. This observation led to the formulation of
Semi-Supervised Domain Adaption (SSDA), which is a variant of Unsupervised Domain Adaptation where we have access to a few labeled samples from the target domain.
Unsupervised domain adaptation methods [42, 12, 36, 51, 35] try to transfer knowledge from the label rich source domain to the unlabeled target domain. Many such existing domain adaptation approaches [42, 12, 51] align the features of the source distribution with the target distribution without considering the category of the samples. These class-agnostic methods fail to generate discriminative features when aligning global distributions. Recently, owing to the success of contrastive approaches
[6, 18, 39], in self-representation learning, some recent works [26, 28] have turned to instance-based contrastive approaches to reduce discrepancies across domains.
[46] reveals that the direct application of the well-known UDA approaches in Semi-Supervised
Domain Adaptation yields sub-optimal performance. [29] has shown that supervision from labeled source and target samples can only ensure the partial cross-domain feature alignment. This creates 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Conceptual description of CLDA approach. (a) Intial distribution of samples from both domain .(b) Instance Contrastive Alignment ensures unlabeled target samples move into the low entropy area forming robust clusters (c) Inter-Domain Contrastive Alignment minimizes the distance between the clusters of same class from both domain (d) The clusters of both domain are well aligned and samples are far away from decision boundary. aligned and unaligned sub-distributions of the target domain, causing intra-domain discrepancy apart from inter-domain discrepancy in SSDA.
In this work, we propose CLDA, a simple single-stage novel contrastive learning framework to address the aforementioned problem. Our framework contains two signiﬁcant components to learn domain agnostic representation. First, Inter-Domain Contrastive Alignment reduces the discrepancy between centroids of the same class from the source and the target domain while increasing the distance between the class centroids of different classes from both source and target domain. This ensures clusters of the same class from both domains are near each other in latent space than the clusters of the other classes from both domains.
Second, inspired by the success of self-representation learning in semi-supervised settings [17, 6, 49], we propose to use Instance Contrastive Alignment to reduce the intra-domain discrepancy. In this, we
ﬁrst generate the augmented views of the unlabeled target images using image augmentation methods.
Alignment of the features of the original and augmented images of the unlabeled samples from the target domain ensures that they are closer to each other in latent space. The alignment between two variants of the same image ensures that the classiﬁer boundary lies in the low-density regions assuring that the feature representations of two variants of the unlabeled target images are similar, which helps to generate better clusters for the target domain.
In summary, our key contributions are as follows. 1) We propose a novel, simple single-stage training framework for Semi-supervised Domain Adaptation. 2)We propose using alignment at class centroids and instance levels to reduce inter and intra domain discrepancies present in SSDA. 3)We evaluate the effectiveness of different augmentation approaches, for instance-based contrastive alignment in the SSDA setting. 4)We evaluate our approach over three well-known Domain Adaptation datasets (DomainNet, Ofﬁce-Home, and Ofﬁce31) to gain insights. Our approach achieves the state of the art results across multiple datasets showing its effectiveness. We perform extensive ablation experiments highlighting the role of different components of our framework. 2