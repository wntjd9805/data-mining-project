Abstract
Gradient compression is a widely-established remedy to tackle the communication bottleneck in distributed training of large deep neural networks (DNNs). Under the error-feedback framework, Top-k sparsiﬁcation, sometimes with k as little as 0.1% of the gradient size, enables training to the same model quality as the uncompressed case for a similar iteration count. From the optimization perspective, we ﬁnd that Top-k is the communication-optimal sparsiﬁer given a per-iteration k element budget. We argue that to further the beneﬁts of gradient sparsiﬁcation, especially for DNNs, a different perspective is necessary — one that moves from per-iteration optimality to consider optimality for the entire training.
We identify that the total error — the sum of the compression errors for all it-erations — encapsulates sparsiﬁcation throughout training. Then, we propose a communication complexity model that minimizes the total error under a commu-nication budget for the entire training. We ﬁnd that the hard-threshold sparsiﬁer, a variant of the Top-k sparsiﬁer with k determined by a constant hard-threshold, is the optimal sparsiﬁer for this model. Motivated by this, we provide convex and non-convex convergence analyses for the hard-threshold sparsiﬁer with error-feedback. We show that hard-threshold has the same asymptotic convergence and linear speedup property as SGD in both the case, and unlike with Top-k sparsiﬁer, has no impact due to data-heterogeneity. Our diverse experiments on various DNNs and a logistic regression model demonstrate that the hard-threshold sparsiﬁer is more communication-efﬁcient than Top-k. Code is available at https://github.com/sands-lab/rethinking-sparsification. 1

Introduction
With the emergence of huge DNNs consisting of hundreds of millions to billions of parameters [12, 50], distributed data-parallel training [66] is an increasingly important workload. As the training process typically spans several compute nodes (or workers) that periodically exchange the local gradient vectors at each iteration of the optimizer (e.g., SGD), communication among nodes remains in many cases the main performance bottleneck [32, 40, 46].
Lossy gradient compression techniques are becoming a common approach to rein in communication efﬁciency [62]. In particular, sparsiﬁcation, which sends only a subset of gradient coordinates (e.g.,
Top-k [4, 8] sends the k largest gradient coordinates by magnitude in each iteration), may signiﬁcantly reduce data volumes and thus speed up training. However, due to its lossy nature, compression raises a complex trade-off between training performance and accuracy. For instance, Agarwal et al. [3] note that training ResNet-18 on CIFAR-100 using sparsiﬁcation speeds up training signiﬁcantly (3.6
),
× but it also degrades ﬁnal accuracy by 1.5%. On the other hand, Lin et al. [37] reports a 500 data
× 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b) (c)
Figure 1: Convergence of Top-k and Hard-threshold for a logistic regression model on gisette LIBSVM dataset with 20 workers: (a) Functional suboptimality vs. epochs; (b) functional suboptimality vs. bits communicated; (c) error norm vs. epochs. Hard-threshold converges as fast as the baseline, no compression
SGD and much faster than Top-k because of a smaller total-error than Top-k. reduction via sparsiﬁcation under deep gradient compression (DGC) for ResNet-50 on ImageNet while preserving the same ﬁnal accuracy when adopting a carefully-tuned warmup phase.
The vast literature on gradient compression largely considers a ﬁxed communication budget per iteration while leaving it up to practitioners to grapple with specifying an additional hyper-parameter that determines the degree of compression before training begins. Meanwhile, recent adaptive Top-k sparsiﬁers [3, 65] empirically demonstrate that tuning the degree of compression in different phases of DNN training yields a more communication-efﬁcient scheme than a ﬁxed compression scheme (e.g., a static k for Top-k). However, these works lack a theoretical framework proving that adaptive compression enjoys better convergence guarantees than the ﬁxed compression scheme.
This raises a fundamental question: Given a ﬁxed communication budget, is there a provably better communication scheme than ﬁxed per-iteration compressed communication? We ﬁrst observe that
Top-k is the communication-optimal sparsiﬁer for a ﬁxed per-iteration communication budget (§4.3).
Then, our insight is that by adopting a different perspective that accounts for the effect of sparsiﬁcation throughout training, a more efﬁcient communication scheme is possible under a revised notion of optimality that considers an overall communication budget (instead of a per-iteration budget).
We consider sparsiﬁcation by using the error-feedback (EF) mechanism [8, 53], a delayed gradient component update strategy that is instrumental for the convergence of the state-of-the-art sparsiﬁers
[10]. Let et denote the error arising due to sparsiﬁcation at iteration t. In EF, this error is added to the gradient update at iteration t + 1. We identify that the term affecting the non-convex convergence in
EF-SGD is the total-error: et 2 [33, 54]. (cid:107) t(cid:107) (cid:80) 2 at each iteration. et
Directly minimizing the total-error is not possible; thus, Top-k minimizes (cid:107) 2 and devise a communication scheme that
We argue that it is possible to focus on the sum of (cid:107) achieves a smaller total-error than any ﬁxed communication sparsiﬁer. We demonstrate that to achieve this change of perspective; it is sufﬁcient to consider a practical yet straightforward mechanism that is a natural counterpart of Top-k: the hard-threshold sparsiﬁer, which communicates the gradient coordinates with magnitude greater than or equal to a ﬁxed given threshold, λ 0, in each iteration.
Although the two sparsiﬁers are in an equivalence relation (a given λ corresponds to a k), under the total-error minimization perspective, we adopt a ﬁxed threshold, λ, which implies a variable k at every iteration. et (cid:107)
≥ (cid:107)
To illustrate intuitively why this change of perspective yields beneﬁts, consider the following example.
Figure 1 shows an experiment in the distributed setting where 20 workers train a 6,000-parameter logistic regression model on the gisette LIBSVM dataset [14] by using the Top-k and hard-threshold sparsiﬁers, conﬁgured to send the same data volume.1 The loss function is strongly convex and has a unique minimizer, x(cid:63), therefore, a unique optimum, f (x(cid:63)). We see that hard-threshold less data, whereas Top-k has converges at the same speed as SGD while communicating a signiﬁcantly slower convergence speed. We attribute this to the fact that Top-k has a large error accumulation in the initial 500 iterations, while the error magnitude for hard-threshold is less than 0.04 throughout training (cf. Figure 1c). Our results with DNN training also reﬂect this insight (§6). 600
∼
× 1We train for 10 epochs and set k = 0.17% for Top-k, and λ = 4.2 for hard-threshold. 2
Moreover, the hard-threshold sparsiﬁer has computational beneﬁts over Top-k sparsiﬁer, as hard-threshold’s underlying ﬁltering operation requires d comparisons in each iteration, where d is the
In contrast, Top-k is a compute-intensive sparsiﬁer (e.g., on CPU, the number of parameters. (d log2 k) [48]). For GPUs, several optimized implementations are computational complexity is proposed but they rely on the data distribution and are efﬁcient only for a small k [48]. For instance, d) where
PyTorch uses Radix select algorithm [5] which has a computational complexity of b is the number of bits to represent gradient values and r is the radix size [42]. b/r
O
O (cid:100) (cid:101) (
Finally, while the hard-threshold sparsiﬁer already exists in the literature [20, 55], we are the ﬁrst to formally study it and theoretically demonstrate its beneﬁts as an adaptive counterpart of Top-k.
Moreover, our argument in favor of hard-threshold precisely falsiﬁes the claim by Dryden et al. [18] that stopped its widespread adoption — a hard-threshold may lead to a degenerate situation when the EF in gradient compression builds up.
This paper makes the following contributions:
Communication complexity model (§4). We propose a communication complexity model that captures the effects of compression in the entire optimization process. We allow for variable communication in each iteration by only imposing a total communication budget. We show that the hard-threshold sparsiﬁer is the communication-optimal sparsiﬁer in this model.
Absolute compressors (§5). We identify that the hard-threshold sparsiﬁer, along with other existing compressors [16, 46], belongs to the class of absolute compressors, which have an absolute bound on the error. Absolute compressors have not been formally studied before with EF. We show that absolute compressors with EF converge for both strongly convex and non-convex loss functions. In both cases, similar to the δ-contraction operators [33], absolute compressors enjoy the same asymptotic convergence with linear speedup (with respect to the number of workers) as no-compression SGD.
However, δ-contraction operators have a worse dependence on δ in the distributed setting with heterogeneous data, while absolute compressors do not have such an anomaly.
Experiments (§6). We conduct diverse experiments on both strongly convex and non-convex (for
DNNs) loss functions to substantiate our claims. Our DNN experiments include computer vision, language modeling, and recommendation tasks, and our strongly convex experiment is on logistic regression. We ﬁnd that the hard-threshold sparsiﬁer is consistently more communication-efﬁcient than the Top-k sparsiﬁer given the same communication budget. 2