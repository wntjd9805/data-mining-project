Abstract
We present neural radiance ﬁelds for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance ﬁeld with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject – including both body and clothing – and to regularize the radiance ﬁeld to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities signiﬁcantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape.

Introduction 1
Enabling free-viewpoint video of a human in motion, based on a sparse set of views, is extremely challenging, but has many applications. Our work is motivated by a breadth of transformative 3D use cases, including immersive visualization of photographs, virtual clothing and try-on, ﬁtness, as well as AR and VR for improved communication or collaboration. However, so far static scenes and rigid objects have been the primary subject of research. In pursuing realistic novel-view synthesis two schools of thought have been established: 1) 3D reconstruction methods aim to recover the geometry of the observed scene as accurately as possible, with novel views generated using classical rendering pipelines [30, 42]. 2) Image-based rendering techniques [7, 10, 40] and very recently neural radiance
ﬁelds [29] primarily aim at image production quality without explicitly constructing an accurate 3D geometric model. While these techniques explicitly or implicitly reconstruct the scene geometry sometimes, this is however not guaranteed to accurately resemble the true scene geometry. We argue that novel-view rendering and reconstruction are two sides of the same coin, and that reliable viewpoint generalization, especially given relatively few input views, would require good quality for both. To this end, we propose a uniﬁed model in order to support both robust reconstruction and photo-realistic rendering. Dynamic scenes, especially those capturing a human in motion, add considerable complexity to the problem: while static scenes can be observed from many views by a camera moving through the scene, any conﬁguration of a dynamic scene is typically observed only from sparse views. Moreover, the scene geometry and its appearance may change considerably over time. To cope with few views, some methods integrate scene knowledge over time by warping observations into a common reference frame [35, 37]. At test time, the information is warped back to the desired state and rendered from a novel view. Extrapolating to unseen motion, however, remains challenging. For scenes capturing people, this means that only poses seen during training can be rendered at test time. For some applications, however, rendering the subject over a broad range of motions, or in novel poses, is desirable. To make generalisation over poses and views possible, we rely on additional problem domain knowledge in the form of a human body model, imGHUM [4]. imGHUM is an implicit signed distance function (SDF) conditioned on generative shape and pose 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Nt
M
Nc x (s, d)
θ imGHUM (β, θt, Tt)
∆d
Residual SDF
NeRF c
σ v
T
Figure 1: Overview of H-NeRF. Given a set of images of a human performer collected from a sparse set of calibrated cameras, we train a geometry-aware neural radiance ﬁeld by co-learning a deformable signed distance ﬁeld. First, we estimate the body shape β and track the articulated pose θ, T using the implicit statistical human body model imGHUM (orange). imGHUM encodes all 3D spatial points x across frames with a 4D point descriptor (s, d) referencing a canonical frame. Using the foreground mask M, we co-train a residual SDF and a NeRF network in that canonical space, in order to integrate all image observations into a consistent implicit 3D geometry ∆d and its view-depended (v) radiance representation (c, σ). The trainable residual SDF and NeRF networks (in blue) are conditioned on the body pose θ and the root transformation T to model pose-dependent geometric and appearance variations. Our framework supports both accurate 3D geometric reconstruction and free-viewpoint rendering, and generalizes well to novel views, shapes, and poses. codes learned from a large corpus of dynamic human scans. In this work, imGHUM is used as the common reference frame for a neural radiance ﬁeld and as a structured prior for robust reconstruction.
Additionally, since imGHUM can represent a broad distribution of statistically valid human poses and shapes, we can render the reconstructed subject in novel poses and even with modiﬁed body shapes. In summary, our system supports photo-realistic free-view point temporal rendering of a human subject given only sparse camera observations. By conditioning on an implicit human body model, we can render considerably different viewpoints, body poses, and body shapes compared to those observed in training. Our carefully designed losses ensure not only good image quality but also plausible temporal reconstructions that can be used for the free-viewpoint visualisation of human performance capture. 2