Abstract
Neural networks and Gaussian processes are complementary in their strengths and weaknesses. Having a better understanding of their relationship comes with the promise to make each method beneﬁt from the strengths of the other. In this work, we establish an equivalence between the forward passes of neural networks and (deep) sparse Gaussian process models. The theory we develop is based on inter-preting activation functions as interdomain inducing features through a rigorous analysis of the interplay between activation functions and kernels. This results in models that can either be seen as neural networks with improved uncertainty predic-tion or deep Gaussian processes with increased prediction accuracy. These claims are supported by experimental results on regression and classiﬁcation datasets. 1

Introduction
Neural networks (NNs) [1] and Gaussian processes (GPs) [2] are well-established frameworks for solving regression or classiﬁcation problems, with complementary strengths and weaknesses. NNs work well when given very large datasets, and are computationally scalable enough to handle them.
GPs, on the other hand, are challenging to scale to large datasets, but provide robust solutions with uncertainty estimates in low-data regimes where NNs struggle. Ideally, we would have a single model that provides the best of both approaches: the ability to handle low and high dimensional inputs, and to make robust uncertainty-aware predictions from the small to big data regimes.
Damianou and Lawrence [3] introduced the Deep Gaussian process (DGP) as a promising attempt to obtain such a model. DGPs replicate the structure of deep NNs by stacking multiple GPs as layers, with the goal of gaining the beneﬁts of depth while retaining high quality uncertainty. Delivering this potential in practice requires an efﬁcient and accurate approximate Bayesian training procedure, which is highly challenging to develop. Signiﬁcant progress has been made in recent years, which has led to methods outperform both GPs and NNs in various medium-dimensional tasks [4, 5]. In addition, some methods [5, 6] train DGPs in ways that closely resemble backpropagation in NNs, which has also greatly improved efﬁciency compared to early methods [3]. However, despite recent progress, DGPs are still cumbersome to train compared to NNs. The similarity between the training procedures sheds light on a possible reason: current DGP models are forced to choose activation functions that are known to behave poorly in NNs (e.g., radial basis functions).
In this work we aim to further unify DGP and NN training, so practices that are known to work for NNs can be applied in DGPs. We do this by developing a DGP for which propagating through
⇤Work done while at Secondmind. Correspondence to vd309@cam.ac.uk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the mean of each layer is identical to the forward pass of a typical NN. This link provides practical advantages for both models. The training of DGPs can be improved by taking best practices from
NNs, to the point where a DGP can even be initialised from a NN trained in its usual way. Conversely,
NN solutions can be endowed with better uncertainty estimates by continued training with the DGP training objective. 2