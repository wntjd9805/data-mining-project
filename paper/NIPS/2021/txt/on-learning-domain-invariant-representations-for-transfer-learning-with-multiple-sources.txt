Abstract
Domain adaptation (DA) beneﬁts from the rigorous theoretical works that study its insightful characteristics and various aspects, e.g., learning domain-invariant representations and its trade-off. However, it seems not the case for the multiple source DA and domain generalization (DG) settings which are remarkably more complicated and sophisticated due to the involvement of multiple source domains and potential unavailability of target domain during training. In this paper, we develop novel upper-bounds for the target general loss which appeal to us to deﬁne two kinds of domain-invariant representations. We further study the pros and cons as well as the trade-offs of enforcing learning each domain-invariant representation.
Finally, we conduct experiments to inspect the trade-off of these representations for offering practical hints regarding how to use them in practice and explore other interesting properties of our developed theory. 1

Introduction
Although annotated data has been shown to be really precious and valuable to deep learning (DL), in many real-world applications, annotating a sufﬁcient amount of data for training qualiﬁed DL models is prohibitively labour-expensive, time-consuming, and error-prone. Transfer learning is a vital solution for the lack of labeled data. Additionally, in many situations, we are only able to collect limited number of annotated data from multiple source domains. Therefore, it is desirable to train a qualiﬁed DL model primarily based on multiple source domains and possibly together with a target domain. Depending on the availability of the target domain during training, we encounter either multiple source domain adaptation (MSDA) or domain generalization (DG) problem.
Domain adaptation (DA) is a speciﬁc case of MSDA when we need to transfer from a single source domain to an another target domain available during training. For the DA setting, the pioneering work [3] and other following work [43, 52, 53, 21, 18, 54, 22] are abundant to study its insightful characterizations and aspects, notably what domain-invariant (DI) representations are, how to learn this kind of representations, and the trade-off of enforcing learning DI representations.
Those well-grounded studies lay the foundation for developing impressive practical DA methods
[14, 47, 44, 52, 7, 51, 36].
Due to the appearance of multiple source domains and possibly unavailability of target domain, establishing theoretical foundation and characterizing DI representations for the MSDA and especially
DG settings are signiﬁcantly more challenging. A number of state-of-the-art works in MSDA and
DG [56, 39, 19, 11, 41, 27, 28, 34, 15, 55, 49, 35, 37] have implicitly exploited and used DI representations in some sense to achieve impressive performances, e.g., [28, 28, 11] minimize representation divergence during training, [41] decomposes representation to obtain invariant feature,
[55] maximizes domain prediction entropy to learn invariance. Despite the successes, the notion
DI representations in MSDA and DG is not fully-understood, and rigorous studies to theoretically 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
characterize DI representations for these settings are still very crucial and imminent. In this paper, we provide theoretical answers to the following questions: (1) what are DI representations in MSDA and DG, and (2) what one should expect when learning them. Overall, our contributions in this work can be summarized as:
• In Section 2.2, we ﬁrst develop two upper-bounds for the general target loss in the MSDA and DG settings, whose proofs can be found in Appendix A. We then base on these bounds to characterize and deﬁne two kinds of DI representations: general DI representations and compressed DI representations.
• We further develop theory to inspect the pros and cons of two kinds of DI representations in
Section 2.3, aiming to shed light on how to use those representations in practice. Particularly, two types of DI representations optimize different types of divergence on feature space, hence serving different purposes. Appendix B contains proofs regarding these characteristics.
• Finally, we study in Section 2.4 the trade-off of two kinds of DI representations which theoretically answers the question whether and how the target performance is hurt when enforcing learning each kind of representation. We refer reader to Appendix C for its proof.
• We conduct experiments to investigate the trade-off of two kinds of representations for giving practical hints regarding how to use those representations in practice as well as exploring other interesting properties of our developed theory.
It is worth noting that although MSDA has been investigated in some works [18, 54], our proposed method is the ﬁrst work that rigorously formulates and provides theoretical analysis for representation learning in MSDA and DG. Speciﬁcally, our bounds developed in Theorems 1 and 3 interweaving both input and latent spaces are novel and beneﬁt theoretical analyses in deep learning. Our theory is developed in a general setting of multi-class classiﬁcation with a sufﬁciently general loss, which can be viewed as a non-trivial generalization of existing works in DA [30, 3, 53, 31, 54], each of which considers binary classiﬁcation with speciﬁc loss functions. Moreover, our theoretical bounds developed in Theorem 8 is the ﬁrst theoretical analysis of the trade-off of learning DI representations in MSDA and DG. Particularly, by considering the MSDA setting with single source and target domains, we achieve the same trade-off nature discovered in [53], but again our setting is more general than binary classiﬁcation setting in that work. 2 Our Main Theory 2.1 Notations
Let X be a data space on which we endow a data distribution P with a corresponding density function p(x). We consider the multi-class classiﬁcation problem with the label set Y = [C], where C is the number of classes and [C] := {1, . . . , C}. Denote Y∆ := (cid:8)α ∈ RC : (cid:107)α(cid:107)1 = 1 ∧ α ≥ 0(cid:9) as the
C−simplex label space, let f : X (cid:55)→ Y∆ be a probabilistic labeling function returning a C-tuple f (x) = [f (x, i)]C i=1, whose element f (x, i) = p (y = i | x) is the probability to assign a data sample x ∼ P to the class i (i.e., i ∈ {1, ..., C}). Moreover, a domain is denoted compactly as pair of data distribution and labeling function D := (P, f ). We note that given a data sample x ∼ P, its categorical label y ∈ Y is sampled as y ∼ Cat (f (x)) which a categorical distribution over f (x) ∈ Y∆. (cid:16) ˆf (x) , y (cid:17) with ˆf (x) ∈ Y∆ and y ∈ Y speciﬁes
Let l : Y∆ × Y (cid:55)→ R be a loss function, where l the loss (e.g., cross-entropy, Hinge, L1, or L2 loss) to assign a data sample x to the class y by the hypothesis ˆf . Moreover, given a prediction probability ˆf (x) w.r.t. the ground-truth prediction f (x), (cid:104) f (x, y). This l we deﬁne the loss (cid:96) means (cid:96) (·, ·) is convex w.r.t. the second argument. We further deﬁne the general loss caused by using a classiﬁer ˆf : X (cid:55)→ Y∆ to predict D ≡ (P, f ) as (cid:17) (cid:17) (cid:16) ˆf (x) , f (x) (cid:16) ˆf (x) , y (cid:16) ˆf (x) , y
= Ey∼f (x)
= (cid:80)C y=1 l (cid:17)(cid:105) (cid:17)(cid:105) (cid:17) (cid:17) (cid:104) (cid:16) ˆf , f, P
L (cid:16) ˆf , D
= L
:= Ex∼P (cid:16) ˆf (x), f (x) (cid:96)
.
We inspect the multiple source setting in which we are given multiple source domains {DS,i}K i=1 over the common data space X , each of which consists of data distribution and its own labeling 2
function DS,i := (cid:0)PS,i, f S,i(cid:1). Based on these source domains, we need to work out a learner or classiﬁer that requires to evaluate on a target domain DT := (cid:0)PT , f T (cid:1). Depending on the knownness or unknownness of a target domain during training, we experience either multiple source DA (MSDA)
[31, 29, 42, 18] or domain generalization (DG) [24, 28, 27, 34] setting.
One typical approach in MSDA and DG is to combine the source domains together [14, 28, 27, 34, 1, 11, 46] to learn DI representations with hope to generalize well to a target domain. When combining the source domains, we obtain a mixture of multiple source distributions denoted as Dπ = (cid:80)K i=1 can be conveniently set to πi = Ni (cid:80)K i=1 πiDS,i, where the mixing coefﬁcients π = [πi]K with Ni being the training size of the i−th source domain. j=1 Nj
In term of representation learning, input is mapped into a latent space Z by a feature map g :
X (cid:55)→ Z and then a classiﬁer ˆh : Z (cid:55)→ Y∆ is trained based on the representations g (X ). Let f : X (cid:55)→ Y∆ be the original labeling function. To facilitate the theory developed for latent space, we introduce representation distribution being the pushed-forward distribution Pg := g#P, and the labeling function h : Z (cid:55)→ Y∆ induced by g as h(z) =
[21]. Going back to our multiple source setting, the source mixture becomes Dπ g = (cid:0)PT g , hS,i(cid:1), and the target domain is DT g = (cid:0)PS,i
DS,i
Finally, in our theory development, we use Hellinger divergence between two distributions deﬁned as dx, whose squared d1/2 = (cid:112)D1/2 is a proper metric. (cid:82) g−1(z) f (x)p(x)dx (cid:82) g−1(z) p(x)dx i πiDS,i (cid:90) (cid:16)(cid:112)p1(x) − (cid:112)p2(x) g , where each source domain is g = (cid:80) g , hT (cid:1). (cid:0)P1, P2(cid:1) = 2
D1/2 (cid:17)2 2.2 Two Types of Domain-Invariant Representations
Hinted by the theoretical bounds developed in [3], DI representations learning, in which feature extractor g maps source and target data distributions to a common distribution on the latent space, is well-grounded for the DA setting. However, this task becomes signiﬁcantly challenging for the MSDA and DG settings due to multiple source domains and potential unknownness of target domain. Similar to the case of DA, it is desirable to develop theoretical bounds that directly motivate deﬁnitions of DI representations for the MSDA and DG settings, as being done in the next theorem.
Theorem 1. Consider a mixture of source domains Dπ = (cid:80)K i=1 πiDS,i and the target domain DT .
Let (cid:96) be any loss function upper-bounded by a positive constant L. For any hypothesis ˆf : X (cid:55)→ Y∆ where ˆf = ˆh ◦ g with g : X → Z and ˆh : Z → Y∆, the target loss on input space is upper bounded (cid:16) ˆf , DT (cid:17)
L
≤
K (cid:88) i=1 (cid:16) ˆf , DS,i(cid:17)
πiL
+ L max i∈[K]
EPS,i (cid:2)(cid:107)∆pi(y|x)(cid:107)1
√ (cid:3) + L 2 d1/2 (cid:0)PT g , Pπ g (cid:1) (1) where ∆pi(y|x) := (cid:2)(cid:12) space between source domain DS,i, the target domain DT , and [K] := {1, 2, ..., K}. (cid:12)f T (x, y) − f S,i(x, y)(cid:12) (cid:3)C y=1 is the absolute of single point label shift on input (cid:12) (cid:0)PT g , Pπ g (cid:1), (ii) the label shift: maxi∈[K] EPS,i
The bound in Equation 1 implies that the target loss in the input or latent space depends on three terms: (cid:3), (cid:2)(cid:107)∆pi(y|x)(cid:107)1 (i) representation discrepancy: d1/2 (cid:16) ˆf , DS,i(cid:17) and (iii) the general source loss: (cid:80)K
. To minimize the target loss in the left side, we need to minimize the three aforementioned terms. First, the label shift term is a natural characteristics of domains, hence almost impossible to tackle. Secondly, the representation discrepancy term can be explicitly tackled for the MSDA setting, while almost impossible for the DG setting. Finally, the general source loss term is convenient to tackle, where its minimization results in a feature extractor g and a classiﬁer ˆh. i=1 πiL
Contrary to previous works in DA and MSDA [3, 31, 54, 7] that consider both losses and data discrepancy on data space, our bound connects losses on data space to discrepancy on representation space. Therefore, our theory provides a natural way to analyse representation learning, especially feature alignment in deep learning practice. Note that although DANN [14] explains their feature alignment method using theory developed by Ben-david et al. [3], it is not rigorous. In particular, while application of the theory to representation space yield a representation discrepancy term, the loss terms are also on that feature space, and hence minimizing these losses is not the learning goal. 3
Finally, our setting is much more general, which extends to multilabel, stochastic labeling setting, and any bounded loss function.
From the upper bound, we turn to ﬁrst type of DI representations for the MSDA and DG settings.
Here, the objective of g is to map samples onto representation space in a way that the common classiﬁer ˆh can effectively and correctly classiﬁes them, regardless of which domain the data comes from.
Deﬁnition 2. Consider a class G of feature maps and a class H of hypotheses. Let {DS,i}K set of source domains. i) (DG with unknown target data) A feature map g∗ ∈ G is said to be a DG general domain-invariant (DI) feature map if g∗ is the solution of the optimization problem (OP): (cid:16)ˆh, DS,i
. Moreover, the latent representations z = g∗ (x) induced ming∈G minˆh∈H by g∗ is called general DI representations for the DG setting. ii) (MSDA with known target data) A feature map g∗ ∈ G is said to be a MSDA general DI feature (cid:17) (cid:16)ˆh, DS,i map if g∗ is the solution of the optimization problem (OP): ming∈G minˆh∈H (cid:1)). Moreover, the latent representations z = which satisﬁes PT g∗ (x) induced by g∗ is called general DI representations for the MSDA setting. g∗ (i.e., ming∈G d1/2 g∗ = Pπ i=1 be a i=1 πiL i=1 πiL g , Pπ g (cid:0)PT (cid:80)K (cid:80)K (cid:17) g g
The deﬁnition of general DI representations for the DG setting in Deﬁnition 2 is transparent in light of (cid:16) ˆf , DS,i(cid:17)
Theorem 1, wherein we aim to ﬁnd g and ˆh to minimize the general source loss (cid:80)K due to the unknownness of PT . Meanwhile, regarding the general DI representations for the g∗ and ˆh to minimize the general source
MSDA setting, we aim to ﬁnd g∗ satisfying PT loss (cid:80)K
. Practically, to ﬁnd general representations for MSDA, we solve (cid:16) ˆf , DS,i(cid:17) g∗ = Pπ i=1 πiL i=1 πiL min g∈G min
ˆh∈H (cid:40) K (cid:88) i=1 (cid:16)ˆh, DS,i g (cid:17)
πiL
+ λD (cid:0)PT g , Pπ g (cid:1) (cid:41)
, where λ > 0 is a trade-off parameter and D can be any divergence (e.g., Jensen Shannon (JS) divergence [16], f -divergence [38], MMD distance [27], or WS distance [42]).
In addition, the general DI representations have been exploited in some works [57, 26] from the practical perspective and previously discussed in [2] from the theoretical perspective. Despite the similar deﬁnition to our work, general DI representation in [2] is not motivated from minimization of a target loss bound.
As pointed out in the next section, learning general DI representations increases the span of latent (cid:1) for a general representations g (x) on the latent space which might help to reduce d1/2 target domain. On the other hand, many other works [27, 28, 15, 49, 11] have also explored the possibility of enhancing generalization by ﬁnding common representation among source distributions.
The following theorem motivates the latter, which is the second kind of DI representations.
Theorem 3. Consider a mixture of source domains Dπ = (cid:80)K i=1 πiDS,i and the target domain DT .
Let (cid:96) be any loss function upper-bounded by a positive constant L. For any hypothesis ˆf : X (cid:55)→ Y∆ where ˆf = ˆh ◦ g with g : X → Z and ˆh : Z → Y∆, the target loss on input space is upper bounded g , Pπ g (cid:0)PT (cid:16) ˆf , DT (cid:17)
L
≤
K (cid:88) i=1 (cid:16) ˆf , DS,i(cid:17)
πiL
+ L max i∈[K]
EPS,i (cid:104) (cid:107)∆pi(y|x)(cid:107)1 (cid:105)
K (cid:88)
K (cid:88)
+ i=1 j=1
√
L 2πj
K (cid:16) g , PS,i
PT g (cid:17)
+ d1/2
K (cid:88)
K (cid:88) i=1 j=1
√
L 2πj
K (cid:16) d1/2 g , PS,j
PS,i g (cid:17)
. (2)
Evidently, Theorem 3 suggests another kind of representation learning, where source representation distributions are aligned in order to lower the target loss’s bound as concretely deﬁned as follows.
Deﬁnition 4. Consider a class G of feature maps and a class H of hypotheses. Let (cid:8)DS,i(cid:9)K set of source domains. i=1 be a 4
g i=1 πiL i) (DG with unknown target data) A feature map g∗ ∈ G is a DG compressed DI represen-tations for source domains {DS,i}K i=1 if g∗ is the solution of the optimization problem (OP): (cid:17) (cid:16)ˆh, DS,i (cid:80)K which satisﬁes PS,1 (i.e., the pushed ming∈G minˆh∈H forward distributions of all source domains are identical). The latent representations z = g∗ (x) is then called compressed DI representations for the DG setting. ii) (MSDA with known target data) A feature map g∗ ∈ G is an MSDA compressed DI repre-sentations for source domains {DS,i}K i=1 if g∗ is the solution of the optimization problem (OP): (cid:80)K which satisﬁes PS,1 g∗ (i.e., the ming∈G minˆh∈H pushed forward distributions of all source and target domains are identical). The latent representa-tions z = g∗ (x) is then called compressed DI representations for the MSDA setting. g∗ = . . . = PS,K g∗ = . . . = PS,K g∗ g∗ = PS,2 g∗ = PS,2 (cid:16)ˆh, DS,i g∗ = PT i=1 πiL (cid:17) g 2.3 Characteristics of Domain-Invariant Representations
In the previous section, two kinds of DI representations are introduced, where each of them originates from minimization of different terms in the upper bound of target loss. In what follows, we examine and discuss their beneﬁts and drawbacks. We start with the novel development of hypothesis-aware divergence for multiple distributions , which is necessary for our theory developed later. 2.3.1 Hypothesis-Aware Divergence for Multiple Distributions
Let consider multiple distributions Q1, ..., QC on the same space, whose density functions are q1, ..., qC. We are given a mixture of these distributions Qα = (cid:80)C i=1 αiQi with a mixing coefﬁcient
α ∈ Y∆ and desire to measure a divergence between Q1, ..., QC. One possible solution is employing a hypothesis from an inﬁnite capacity hypothesis class H to identify which distribution the data come from. In particular, given z ∼ Qα, the function ˆh (z, i) , i ∈ [C] outputs the probability that x ∼ Qi. Note that we use the notation ˆh in this particular subsection to denote a domain classiﬁer, while everywhere else in the paper ˆh denotes label classiﬁer. Let l
∈ R be the loss when classifying z using ˆh provided the ground-truth label i ∈ [C]. Our motivation is that if the distributions Q1, ..., QC are distant, it is easier to distinguish samples from them, hence (cid:17) (cid:16)ˆh the minimum classiﬁcation loss minˆh∈H Lα is much lower than in the case of clutching
Q1, ..., QC. Therefore, we develop the following theorem to connect the loss optimization problem with an f-divergence among Q1, ..., QC. More discussions about hypothesis-aware divergence can be found in Appendix B of this paper and in [13].
Theorem 5. Assuming the hypothesis class H has inﬁnite capacity, we deﬁne the hypothesis-aware divergence for multiple distributions as (cid:17) (cid:16)ˆh (z) , i
Q1:C
Dα (Q1, ..., QC) = − min
ˆh∈H
Lα
Q1:C (cid:17) (cid:16)ˆh
+ Cl,α, (3) where Cl,α depends only on the form of loss function l and value of α. This divergence is a proper f-divergence among Q1, ..., QC in the sense that Dα (Q1, ..., QC) ≥ 0, ∀Q1, ..., QC and α ∈ Y∆, and Dα (Q1, ..., QC) = 0 if Q1 = ... = QC. 2.3.2 General Domain-Invariant Representation
As previously deﬁned, the general DI feature map g∗ (cf. Deﬁnition 2) is the one that minimizes the total source loss g∗ = arg min g∈G min
ˆh∈H
K (cid:88) i=1 (cid:16)ˆh, hS,i, Pi g (cid:17)
πiL
= argming∈Gminˆh∈H (cid:16)ˆh, DS,i g (cid:17)
.
πiL (4)
K (cid:88) i=1
From the result of Theorem 5, we expect that the minimal loss minˆh∈H L inversely proportional to the divergence between the class-conditionals PS,i,c g result to the multi-source setting, we further deﬁne QS,c
PS,i,c g class c conditional distributions of the source domains on the latent space, where αc = (cid:80)K
. To generalize this as the mixture of the j=1 πjγj,c
:= (cid:80)K i=1 should be
πiγi,c
αc g g (cid:16)ˆh, hS,i, PS,i (cid:17) 5
are the mixing coefﬁcients, and γi,c = PS,i (y = c) are label marginals. Then, the inner loop of the objective function in Eq. 4 can be viewed as training the optimal hypothesis ˆh ∈ H to distinguish the samples from QS,c g , c ∈ [C] for a given feature map g. Therefore, by linking to the multi-divergence concept developed in Section 2.3.1, we achieve the following theorem.
Theorem 6. Assume that H has inﬁnite capacity, we have the following statements. 1. Dα (cid:0)Qs,1 deﬁned as above. 2. Finding the general DI feature map g∗ via the OP in Eq. 4 is equivalent to solving
+ const, where α = [αc]c∈[C] is (cid:1) = − minˆh∈H (cid:16)ˆh, hS,i, PS,i g , ..., Qs,C i=1 πiL (cid:80)K (cid:17) g g g∗ = argmax g∈G
Dα (cid:0)Qs,1 g , ..., Qs,C g (cid:1) . (5) g g , ..., Qs,C (cid:1). Hence, the span of source mixture Pπ
Theorem 6, especially its second claim, discloses that learning general DI representations max-imally expands the coverage of latent representations of the source domains by maximizing
Dα (cid:0)Qs,1 is also increased.
We believe this demonstrates one of the beneﬁts of general DI representations because it implicitly enhance the chance to match a general unseen target domains in the DG setting by possibly reducing (cid:1) (cf. Theorem 1). Additionally, the source-target representation discrepancy term d1/2 (cid:1) explicitly, expanding source representation’s in MSDA where we wish to minimize d1/2 coverage is also useful. c=1 αcQs,c g = (cid:80)C g , Pπ g g , Pπ g (cid:0)PT (cid:0)PT g 2.3.3 Compressed Domain-Invariant Representations
It is well-known that generalization gap between the true loss and its empirical estimation affects generalization capability of model [48, 45]. One way to close this gap is increasing sampling density. However, as hinted by our analysis, enforcing general DI representation learning tends to maximize the cross-domain class divergence, hence implicitly increasing the diversity of latent representations from different source domains. This renders learning a classiﬁer ˆh on top of those source representations harder due to scattering samples on representation space, leading to higher generalization gap between empirical and general losses. In contrast, compressed DI representations help making the task of learning ˆh easier with a lower generalization gap by decreasing the diversity of latent representations from the different source domains, via enforcing PS,1 g∗ = . . . = PS,K g∗ .
We now develop rigorous theory to examine this observation.
Let the training set be S = {(zi, yi)}N i=1, consisting of independent pairs (zi, yi) drawn from the source mixture, i.e., the sampling process starts with sampling domain index k ∼ Cat (π), then sampling z ∼ PS,k (i.e., x ∼ PS,k and z = g (x)), and ﬁnally assigning a label with y ∼ hS,k (z) (i.e., hS,k (z) is a distribution over [C]). The empirical loss is deﬁned as g∗ = PS,2 g (cid:16)ˆh, S (cid:17)
L
= 1 k=1 Nk (cid:80)K
K (cid:88)
Nk(cid:88) k=1 i=1 (cid:17) (cid:16)ˆh (zi) , hS,k (zi)
, (cid:96) (6) where Nk, k ∈ [C] is the number of samples drawn from the k-th source domain and N = (cid:80)K (cid:17) (cid:16)ˆh, S is an unbiased estimation of the general loss L (cid:16)ˆh, DS,k
Here, L
.
To quantify the quality of the estimation, we investigate the upper bound of the generalization gap (cid:12) (cid:12) (cid:12)L
Theorem 7. For any conﬁdent level δ ∈ [0, 1] over the choice of S, the estimation of loss is in the (cid:15)-range of the true loss (cid:17)(cid:12) (cid:12) (cid:12) with the conﬁdence level 1 − δ. (cid:16)ˆh, Dπ (cid:16)ˆh, Dπ k=1 πkL
= (cid:80)K (cid:16)ˆh, S
− L (cid:17) (cid:17) g g g k=1 Nk. (cid:17)
Pr (cid:16)(cid:12) (cid:12) (cid:12)L (cid:16)ˆh, S (cid:17)
− L (cid:16)ˆh, Dπ g (cid:17) (cid:17)(cid:12) (cid:12) (cid:12) ≤ (cid:15)
≥ 1 − δ, where (cid:15) = (cid:15) (δ) = (cid:0) A
δ (cid:1)1/2 is a function of δ for which A is proportional to 1
N (cid:32) K (cid:88)
K (cid:88) i=1 j=1
√
πi
K (cid:16) ˆf , DS,j(cid:17)
L
+ L
K (cid:88)
√ i=1
πi max k∈[K]
EPS,k (cid:104)(cid:13) (cid:13)∆pk,i (y|x) (cid:13) (cid:105) (cid:13) (cid:13) (cid:13)1
+
L
K
K (cid:88)
K (cid:88)
√ i=1 j=1 2πi d1/2 6 (cid:16) g , PS,j
PS,i g (cid:33)2 (cid:17)
Theorem 7 reveals one beneﬁt of learning compressed DI representations, that is, when enforcing (cid:1), which compressed DI representation learning, we minimize L
K tends to reduce the generalization gap (cid:15) = (cid:15) (δ) for a given conﬁdence level 1 − δ. Therefore, compressed DI representations allow us to minimize population loss L more efﬁciently via minimizing empirical loss. (cid:16)ˆh, Dπ g , PS,j g 2πi d1/2 (cid:0)PS,i (cid:80)K (cid:80)K j=1 i=1
√ (cid:17) g 2.4 Trade-off of Learning Domain Invariant Representation g (cid:55)−→ Z
Similar to the theoretical ﬁnding in Zhao et al. [53] developed for DA, we theoretically ﬁnd that compression does come with a cost for MSDA and DG. We investigate the representation trade-off, typically how compressed DI representation affects classiﬁcation loss. Speciﬁcally, we consider a
ˆh(cid:55)−→ Y∆, where X is the common data space, Z is the latent space data processing chain X induced by a feature extractor g, and ˆh is a hypothesis on top of the latent space. We deﬁne Pπ
Y and PT
Y , we sample k ∼ Cat (π), x ∼ PS,k, and y ∼ f S,k (x), while similar to draw y ∼ PT
Y . Our theoretical bounds developed regarding the trade-off of learning DI representations are relevant to d1/2
Theorem 8. Consider a feature extractor g and a hypothesis ˆh, the Hellinger distance between two label marginal distributions Pπ
Y as two distribution over Y in which to draw y ∼ Pπ
Y can be upper-bounded as:
Y and PT
Y , PT (cid:0)Pπ (cid:1).
Y 1. d1/2 (cid:0)Pπ
Y , PT
Y (cid:1) ≤ (cid:104)(cid:80)K k=1 πkL (cid:16)ˆh ◦ g, f S,k, PS,k(cid:17)(cid:105)1/2
+ d1/2 (cid:0)PT g , Pπ g (cid:1) + L (cid:16)ˆh ◦ g, f T , PT (cid:17)1/2 2. d1/2 (cid:0)Pπ (cid:80)K i=1 (cid:80)K j=1
Y (cid:1) ≤
Y , PT
√
πj
K d1/2 (cid:104)(cid:80)K i=1 πiL (cid:0)PT g , PS,i g (cid:1) + L (cid:16)ˆh ◦ g, f S,i, PS,i(cid:17)(cid:105)1/2 (cid:16)ˆh ◦ g, f T , PT (cid:17)1/2
.
+ (cid:80)K i=1 (cid:80)K j=1
√
πj
K d1/2 (cid:0)PS,i g , PS,j g (cid:1) +
Here we note that the general loss L is deﬁned based on the Hellinger loss (cid:96) which is deﬁne as (cid:96)( ˆf (x), f (x)) = D1/2( ˆf (x), f (x)) = 2 (cid:80)C found in Appendix C). i=1 (cid:18)(cid:113)
ˆf (x, i) − (cid:112)f (x, i) (cid:19)2 (more discussion can be
Remark. Compared to the trade-off bound in the work of Zhao et al. [53], our context is more general, concerning MSDA and DG problems with multiple source domains and multi-class probabilistic labeling functions, rather than single source DA with binary-class and deterministic setting. Moreover, the Hellinger distance is more universal, in the sense that it does not depend on the choice of classiﬁer family H and loss function (cid:96) as in the case of H-divergence in [53]. (cid:0)PT g , Pπ g (cid:1) + L
Y and PT
We base on the ﬁrst inequality of Theorem 8 to analyze the trade-off of learning general DI repre-sentations. The ﬁrst term on the left hand side is the source mixture’s loss, which is controllable and tends to be small when enforcing learning general DI representations. With that in mind, (cid:1) is high), the sum if two label marginal distributions Pπ (cid:16)ˆh ◦ g, f T , PT (cid:17)1/2 d1/2 tends to be high. This leads to 2 possibilities. The ﬁrst (cid:1) has small value, e.g., it is minimized scenario is when the representation discrepancy d1/2 in MSDA setting, or it happens to be small by pure chance in DG setting. In this case, the lower bound of target loss L is high, possibly hurting model’s generalization ability. On (cid:1) is large for some reasons, the lower bound of target (cid:0)PT g , Pπ g (cid:16)ˆh ◦ g, f T , PT (cid:17) the other hand, if the discrepancy d1/2 loss will be small, but its upper-bound is higher, as indicated Theorem 1.
Y are distant (i.e., d1/2
Y , PT g , Pπ g (cid:0)PT (cid:0)Pπ
Y
Based on the marginal distributions Pπ (i.e., representations
DI (cid:104)(cid:80)K
Y and PT both (cid:16)ˆh ◦ g, f S,i, PS,i(cid:17)(cid:105)1/2 (cid:0)PT i=1 πiL (cid:80)K (cid:80)K
√
πj
K d1/2 g , PS,i g i=1 j=1 second inequality of Theorem 8, we observe two label
Y are distant while enforcing learning compressed source-source discrepancy source
+ (cid:80)K g , PS,j g i=1 feature (cid:1) are low), and
√
πj
K d1/2 loss (cid:80)K the sum (cid:0)PS,i that j=1 if (cid:1) + L (cid:0)h ◦ g, f T , PT (cid:1)1/2 is high. For the MSDA setting, the 7
j=1 (cid:0)PT g , PS,i g
√
πj (cid:80)K
K d1/2 (cid:16)ˆh ◦ g, f T , PT (cid:17) (cid:1) is trained to get smaller, meaning that the lower bound discrepancy (cid:80)K i=1 is high, hurting the target performance. Similarly, for the DG of target loss L (cid:1) for setting, if the trained feature extractor g occasionally reduces (cid:80)K i=1 some unseen target domain, it certainly increases the target loss L (cid:0)h ◦ g, f T , PT (cid:1). In contrast, if for (cid:1) is high by some reasons, by some target domains, the discrepancy (cid:80)K j=1 i=1 linking to the upper bound in Theorem 3, the target general loss has a high upper-bound, hence is possibly high.
√
πj
K d1/2
√
πj
K d1/2 g , PS,i g g , PS,i g (cid:0)PT (cid:0)PT (cid:80)K (cid:80)K j=1
This trade-off between representation discrepancy and target loss suggests a sweet spot for just-right feature alignment. In that case, the target loss is most likely to be small. 3 Experiment
To investigate the trade-off of learning DI representations, we conduct domain generalization exper-iments on the colored MNIST dataset (CC BY-SA 3.0) [2, 23]. In particular, the task is to predict binary label Y of colored input images X generated from binary digit feature Zd and color feature
Zc. We refer readers to Appendix D for more information of this dataset.
We conduct 7 source domains by setting color-label correlation P(Zc = 1|Y = 1) = P(Zc = 0|Y = 0) = θS,i where θs,i ∼ U ni ([0.6, 1]) for i = 1, ..., 12, while two target domains are created with θT,i ∈ {0.05, 0.7} for i = 1, 2. Here we note that colored images in the target domain with
θT,2 = 0.7 are more similar to those in the source domains, while colored images in the target domain with θT,1 = 0.05 are less similar.
We wish to study the characteristics and trade-off between two kinds of DI representations when predicting on various target domains. Speciﬁcally, we apply adversarial learning [16] similar to
[14], in which a min-max game is played between domain discriminator ˆhd trying to distinguish the source domain given representation, while the feature extractor (generator) g tries to fool the domain discriminator. Simultaneously, a classiﬁer is used to classify label based on the representation. Let
Lgen and Ldisc be the label classiﬁcation and domain discrimination losses, the training objective becomes: (cid:18) (cid:19) min g min
ˆh
Lgen + λ max
ˆhd
Ldisc
, where the source compression strength λ > 0 controls the compression extent of learned represen-tation. More speciﬁcally, general DI representation is obtained with λ = 0, while larger λ leads to more compressed DI representation. Finally, our implementation is based on DomainBed [17] repository, and all training details as well as further MSDA experiment and DG experiment on real datasets are included in Appendix D.1 3.1 Trade-off of Two Kinds of Domain-Invariant Representations
Figure 1: Phase transition of representations with increased λ. (a) General DI representations. (b)
Just-right compressed DI representations. (c) Overly-compressed DI representations. 1Our code could be found at: https://github.com/VinAIResearch/DIRep 8
We consider target domain which is similar to source domain in this experiment. To govern the trade-off between two kinds of DI representations, we gradually increase the source compression strength
λ with noting that λ = 0 means only focusing on learning general DI representations. According to our theory and as shown in Figure 1, learning only general DI representations (λ = 0) maximizes the cross-domain class divergence by encouraging the classes of source domains more separate arbitrarily, while by increasing λ, we enforce compressing the latent representations of source domains together.
Therefore, for an appropriate λ, the class separation from general DI representations and the source compression from compressed DI representations (i.e., just-right compressed DI representations), are balanced as in the case (b) of Figure 1, while for overly high λ, source compression from compressed
DI representations dominates and compromises the class separation from general DI representations (i.e., overly-compressed DI representations).
Figure 2a shows the source validation and target accuracies when increasing λ (i.e., encouraging the source compression). It can be observed that both source validation accuracy and target accuracy have the same pattern: increasing when setting appropriate λ for just-right compressed DI representations and compromising when setting overly high values λ for overly-compressed DI representations.
Figure 2b shows in detail the variation of the source validation accuracy for each speciﬁc λ. In practice, we should encourage learning two kinds of DI representations simultaneously by ﬁnding an appropriate trade-off to balance them for working out just-right compressed DI representations. 3.2 Generalization Capacity on Various Target Domains (a) Accuracy vs λ (b) Accuracy vs iteration
Figure 2: (2a) Source validation accuracy and target accuracy for close target domain (see Section 3.2) over compression strength. (2b) Validation accuracy over training step for different values of λ. (a) Far target domain with θ = 0.05. (b) Close target domain with θ = 0.7.
Figure 3: Accuracy of models trained with different λ on two test domains. For the far domain with
θ = 0.05, the model corresponding with general DI representations performs better than that with more compact DI representations. For close domain with θ = 0.7, the models with compression generalize signiﬁcantly better than those with general DI representations.
In this experiment, we inspect the generalization capacity of the models trained on the source domains with different compression strengths λ to different target domains (i.e., close and far ones). As shown in Figure 3, the target accuracy is lower for the far target domain than for the close one regardless of the source compression strengths λ. This observation aligns with our upper-bounds developed in Theorems 1 and 3 in which larger representation discrepancy increases the upper-bounds of the general target loss, hence more likely hurting the target performance.
Another interesting observation is that for the far target domain, the target accuracy for λ = 0 peaks.
This can be partly explained as the general DI representations help to increase the span of source latent representations for more chance to match a far target domain (cf. Section 2.3.2). Speciﬁcally, the (cid:1) could be small and the upper bound in Theorem 1 is lower. source-target discrepancy d1/2
In contrast, for the close target domain, the compressed DI representations help to really improve g , Pπ g (cid:0)PT 9
the target performance, while over-compression degrades the target performance, similar to result on source domains. This is because the target domain is naturally close to the source domains, the source and target latent representations are already mixed up, but by encouraging the compressed DI representations, we aim to learning more elegant representations for improving target performance. 4