Abstract
Boosting is an algorithmic approach which is based on the idea of combining weak and moderately inaccurate hypotheses to a strong and accurate one. In this work we study multiclass boosting with a possibly large number of classes or categories.
Multiclass boosting can be formulated in various ways. Here, we focus on an especially natural formulation in which the weak hypotheses are assumed to belong to an “easy-to-learn” base class, and the weak learner is an agnostic PAC learner for that class with respect to the standard classiﬁcation loss. This is in contrast with other, more complicated losses as have often been considered in the past. The goal of the overall boosting algorithm is then to learn a combination of weak hypotheses by repeatedly calling the weak learner.
We study the resources required for boosting, especially how they depend on the number of classes k, for both the booster and weak learner. We ﬁnd that the boosting algorithm itself only requires O(log k) samples, as we show by analyzing a variant of AdaBoost for our setting. In stark contrast, assuming typical limits on the number of weak-learner calls, we prove that the number of samples required by a weak learner is at least polynomial in k, exponentially more than the number of samples needed by the booster. Alternatively, we prove that the weak learner’s accuracy parameter must be smaller than an inverse polynomial in k, showing that the returned weak hypotheses must be nearly the best in their class when k is large.
We also prove a trade-off between number of oracle calls and the resources required of the weak learner, meaning that the fewer calls to the weak learner the more that is demanded on each call. 1

Introduction
Boosting [16] is a fundamental primitive in machine learning, most widely studied and applied in the context of binary supervised learning. Yet many important problems require classiﬁcation into a great many target classes. For example, in image object recognition and building language models, the number of classes scales as the number of possible objects, or the dictionary size, respectively. It is thus of practical as well as theoretical interest to generalize boosting to the multiclass setting. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In contrast with binary classiﬁcation, there are several versions of multiclass boosting since there is more than one way to extend the weak learnability assumption. With only two labels, we simply require that the weak learner ﬁnd a hypothesis with error slightly better than 1/2, that is, a bit better than random guessing. So when the number of labels k is more than 2, perhaps the most natural extension requires that the weak learner outputs hypotheses with classiﬁcation error slightly better than a random guess among the k labels, that is, with error a little better than 1 − 1/k. However, this turns out to be too weak for boosting to be possible. Instead requiring classiﬁcation error less than 1/2 is sufﬁcient for boosting, but far stronger than mere random guessing. Other approaches are based on reducing the multiclass problem to multiple binary problems, or constructing loss functions to be minimized by the weak learner that are signiﬁcantly different than the simple classiﬁcation error.
See [13, 16] and references therein for a detailed discussion.
A Weak Learning Assumption. In this work we return to the classical formulation of boosting in which the weak learner aims to minimize ordinary classiﬁcation loss, though in a way that sidesteps some of the hurdles just discussed. Speciﬁcally, we assume the weak learner is an agnostic PAC learner for some base class, meaning that it can ﬁnd the approximately best hypothesis in that class.
This requirement differs subtly, but importantly, from the kind we have been discussing in which, say, the weak hypotheses must have error not greater than some value; instead, the criterion is relative to this particular base class. Further, we take this class to consist of simple hypotheses, in line with the common supposition that weak hypotheses are rules of thumb from an “easy-to-learn” class [16, 18].
Thus, the weakness of the weak learner is manifested in the simplicity of the hypotheses in the base class, and the goal of the overall boosting algorithm is to learn target concepts outside the base class by aggregating the weak hypotheses provided by the weak learner. A similar setting was recently considered by [3] who studied binary-labeled classiﬁcation.
Which target concepts can be approximated by boosting weak hypotheses from a given base-class? In this work we assume that the target concept can be represented by weighted plurality votes over the base class. To motivate this assumption, note that: (i) Most boosting algorithms (in both binary and multiclass settings) output a weighted vote of weak hypotheses [16, 10, 3, 8, 1]. Thus, if one focuses on such algorithms, it becomes a necessary condition that the target concept can be approximated arbitrarily well by such votes, and hence yields our assumption. (ii) In addition, the work by [16] conducted a thorough study of a variety of natural weak learning assumptions for multiclass boosting, and showed that each of those assumptions implies the plurality vote assumption. (iii) Finally, weighted votes are extremely expressive: for example, in the binary-labelled case, the work by [3] showed that under extremely mild assumptions on the base-class, one can approximate arbitrarily complex concepts by weighted votes. Relatedly, [19] proved that assuming a non-trivial base-class, one can obtain Bayes-consistent learning algorithms by boosting using weighted votes. 1.1 Contributions
We study the resources required for boosting, especially how they depend on k, the number of classes.
For the booster, these include the number of examples needed for learning and the number of oracle calls to the weak learner. For the weak learner, we measure the required accuracy, that is, how close the returned hypothesis must be to the best in the base class, or alternatively, the number of unweighted samples the boosting algorithm feeds the weak learner in each round.
A Multiclass Boosting Algorithm. Our starting point (and ﬁrst contribution) is a natural variant of
AdaBoost (Algorithm 1), which demonstrates that it is possible to replace previously considered weak learners by one which aims to minimize the classiﬁcation loss. Indeed, we prove that Algorithm 1 is able to learn any task which satisﬁes the general multiclass weak learning assumption of [13], which they proved to be necessary and sufﬁcient (see Section 2).
However, a closer inspection of our algorithm reveals a surprising phenomenon: while the sample complexity of the booster exhibits a standard logarithmic dependence on k (the number of labels), the sample-complexity of the weak learner depends quadratically on k, which is exponentially worse.
In more detail, our algorithm requires that at each round t, the weak learner returns a hypothesis ht whose excess loss relative to the best hypothesis in the base class is less than 1/k, which means, since k is assumed to be large, that ht is nearly optimal. Stated differently, using standard statistical arguments, in each round t, our boosting algorithm provides the weak learner with O(k2) unweighted training examples (ignoring parameters other than k). This demonstrates a striking discrepancy 2
between the amount of information (in the form of labeled examples) the boosting algorithm uses from the source distribution and between the amount of information exchanged between the boosting algorithm and the weak learner.
Thus, the central question of study in this paper is whether a large dependence on the number of labels k is inherent in the interaction between the weak learner and the boosting algorithm. This question has a direct and signiﬁcant relationship with the overall complexity of multiclass boosting.
Indeed, each call to the weak learner amounts to solving a learning problem over the base class whose complexity is controlled by the prescribed excess loss. Thus, when the number of labels is large, it would be highly beneﬁcial to improve the dependence exhibited by our Algorithm 1.
An Impossibility Result.
Interestingly (and somewhat disappointingly), we show that the discrep-ancy between the sample complexities of the booster and weak learner is inevitable: in Section 5 we prove that, assuming typical limits on the number of weak-learner calls, the number of samples required by the weak learner is at least polynomial in k, which is exponentially more than the number of samples needed by the booster. Alternatively, we prove that the weak learner’s accuracy parameter must be smaller than an inverse polynomial in k, meaning that the returned weak hypotheses must be nearly the best in the base class when k is large. This lower bound follows from a more general trade-off that we prove between the number of oracle calls (to the weak learner) and the sample complexity of the weak learner, meaning that the fewer calls to the weak learner the more that is demanded on each call. Speciﬁcally, we show that at least one of these quantities must be Ω( k).
√
From a technical perspective, the main challenge in proving this trade-off is the construction of the base class; speciﬁcally, the requirement that the base class is easy-to-learn rules out naive randomized constructions ( e.g. as in the oracle-complexity lower bound in [16]), and requires a subtler treatment.
Summarizing, for the model of multiclass boosting that we study, we prove that there is a striking discrepancy between the amount of information (in the form of labeled examples) the boosting algorithm requires from the source distribution and the amount of information exchanged in the channel between the weak learner and the booster. 1.2