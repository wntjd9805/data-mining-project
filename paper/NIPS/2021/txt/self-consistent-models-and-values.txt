Abstract
Learned models of the environment provide reinforcement learning (RL) agents with ﬂexible ways of making predictions about the environment. In particular, models enable planning, i.e. using more computation to improve value functions or policies, without requiring additional environment interactions. In this work, we investigate a way of augmenting model-based RL, by additionally encouraging a learned model and value function to be jointly self-consistent. Our approach differs from classic planning methods such as Dyna, which only update values to be consistent with the model. We propose multiple self-consistency updates, evaluate these in both tabular and function approximation settings, and ﬁnd that, with appropriate choices, self-consistency helps both policy evaluation and control. 1

Introduction
Models of the environment provide reinforcement learning (RL) agents with ﬂexible ways of making predictions about the environment. They have been used to great effect in planning for action selection
[45, 29, 49], and for learning policies or value functions more efﬁciently [51]. Learning models can also assist representation learning, serving as an auxiliary task, even if not used for planning [25, 48].
Traditionally, models are trained to be consistent with experience gathered in the environment. For instance, an agent may learn maximum likelihood estimates of the reward function and state-transition probabilities, based on the observed rewards and state transitions. Alternatively, an agent may learn a model that only predicts behaviourally-relevant quantities like rewards, values, and policies [47].
In this work, we study a possible way to augment model-learning, by additionally encouraging a learned model ˆm and value function ˆv to be jointly self-consistent, in the sense of jointly satisfying the Bellman equation with respect to ˆm and ˆv for the agent’s policy π. Typical methods for using models in learning, like Dyna [51], treat the model as a ﬁxed best estimate of the environment, and only update the value to be consistent with the model. Self-consistency, by contrast, jointly updates the model and value to be consistent with each other. This may allow information to ﬂow more
ﬂexibly between the learned reward function, transition model, and approximate value. Since the true model and value are self-consistent, this type of update may also serve as a useful regulariser.
We investigate self-consistency both in a tabular setting and at scale, in the context of deep RL. There are many ways to formulate a model-value update based on the principle of self-consistency, but we ﬁnd that naive updates may be useless, or even detrimental. However, one variant based on a semi-gradient temporal difference objective can accelerate value learning and policy optimisation.
We evaluate different search-control strategies (i.e. the choice of states and actions used in the self-consistency update), and show that self-consistency can improve sample efﬁciency in environments such as Atari, Sokoban and Go. We conclude with experiments designed to shed light on the mechanisms by which our proposed self-consistency update aids learning. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
2