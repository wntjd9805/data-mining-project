Abstract
Clustering is a central topic in unsupervised learning and its online formulation has received a lot of attention in recent years. In this paper, we study the classic facility location problem in the presence of multiple machine-learned advice. We design an algorithm with provable performance guarantees such that, if the advice is good, it outperforms the best-known online algorithms for the problem, and if it is bad it still matches their performance. We complement our theoretical analysis with an in-depth study of the performance of our algorithm, showing its effectiveness on synthetic and real-world data sets. 1

Introduction
Clustering is a central topic in unsupervised learning [Jain and Dubes, 1988, Aggarwal and Reddy, 2014, Gan et al., 2020] In the past few years, its online version has gained a lot of attention [Zhang et al., 2004, Liberty et al., 2016, Lattanzi and Vassilvitskii, 2017, Cohen-Addad et al., 2021]. In this paper we are interested in the fundamental question of whether machine-learned advice, especially when coming from multiple sources, can boost the performance of clustering algorithms in the online setting. In particular, we are interested in designing online algorithms with access to multiple advice that exhibit strong theoretical guarantees together with a good experimental performance.
To explore the interplay between online algorithms and multiple advice we focus on facility location, a classic clustering problem with a long and rich history in computer science and operations research
[Korte and Vygen, 2018], which can also be seen as the Lagrangian relaxation of k−median clustering.
In the online setting of k−median clustering, it is easy to see that if the requirement of having k clusters is a hard constraint then the cost of solutions is bound to be arbitrarily far from the optimum, which makes the straightforward online formulation rather uninteresting algorithmically. There are two standard ways in which the constraint can be relaxed: bi-criteria solutions and the Lagrangian relaxation. Facility location captures the latter.
In the online version of facility location, points arrive in sequence and, as soon as a point arrives, an irrevocable decision must be made: we either assign it to an existing opened facility or we open a new one to serve it. Each facility, therefore, induces a cluster, consisting of the points assigned to it. The cost of this sequence of decisions is given by the sum of the service costs (i.e. the distance between a point and its facility) and the total facility cost (opening each facility has a cost, which in this paper we assume to be uniform). The goal is to ﬁnd as cheap a clustering as possible. This online version 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
was ﬁrst introduced by Meyerson [2001], who gave an elegant O(log(n))-approximation algorithm, and studied extensively thereafter [Fotakis, 2003, 2005, 2011, Cygan et al., 2018, Cohen-Addad et al., 2019, Ahmed et al., 2020, Guo et al., 2020]. One of the main reasons for this heightened attention is that even in dynamic settings the resulting clustering is very stable. This is particularly useful in real-world systems where clusters are served directly to users or when they are used in a downstream machine learning model, a situation where modifying the clustering would incur too high a cost.
In real-world applications, one typically has side information that could help solve the online clustering problem. This natural scenario is a recurrent theme in machine learning that focuses on how to take advantage of expert advice. In the context of online algorithms, Lykouris and Vassilvitskii
[2018] and Mahdian et al. [2012] introduced a formal framework for online problems within which mathematically rigorous performance guarantees can be given in terms of the quality of the advice. In particular, they showed how to incorporate the advice in a robust way: the advice is exploited when it is good and disregarded when it is bad. All this happens automatically: the robust algorithm does not need to know in advance which is which. Remarkably, when the advice is bad the performance of the best online algorithm is matched. The approach combines mathematical rigor with practical effectiveness, adding the desirable dimension of robustness. The solution we develop for facility location is in the same spirit, with a focus on multiple advice which adds an important new dimension.
Thus, the goal of this paper is to show how to take advantage of multiple advice within this rigorous algorithmic framework in the case of online facility location.
In our set-up the advice comes in the form of a family of sets, each suggesting a list of facilities to open. As we show with our experiments, multiple advice of this form can be easily and efﬁciently produced on the basis of past data. The main technical result of this paper is the following. Suppose we have a family of sets S1, . . . , Sk which intuitively represent sets of suggested facilities to open, and let S be their union. We develop an online randomized algorithm, called TAKEHEED, that computes a solution whose expected cost is
O (log(|S|) · OPT(S)) , where OPT(S) is the best solution that can be obtained using only facilities in S. Notice that the
OPT(S) factor is no worse than any of the suggested sets in isolation since OPT(S) ≤ mini OPT(Si).
In fact, it can be much better. For instance, every set could contain just a few good points and a lot of noise. Or, one of the sets could be good but its identity unknown. And yet, TAKEHEED separates the wheat from the chaff and delivers a near-optimal solution. This result (which is the main technical step toward the solution) can be seen as a solution to the version of facility location in which the facilities are constrained to be in a speciﬁc set, as opposed to the entire input metric space. The above result is essentially the best possible. In Section 4, we prove that the log |S| factor is near-optimal.
We also show that the best-known algorithms (see Meyerson [2001], Fotakis [2003, 2005]) cannot match the same bound, for they can only attain an Ω(|S|) approximation factor in general. This is a strong indication that techniques different from those employed by those state-of-the-art algorithms are required for this problem, providing a compelling motivation for our approach which is based on the theory of Hierarchically Separated Trees (HST’s) introduced by Bartal [1996] in a seminal paper.
On the practical side, our experimental analysis with real and synthetic data shows that it is easy and inexpensive to compute multiple advice of the form described, allowing TAKEHEED to outperform state-of-the-art algorithms such as those in Meyerson [2001], Fotakis [2003], Anagnostopoulos et al.
[2004]. Furthermore, we also show that we can make our algorithm robust to erroneous advice: when
S is far from any optimal set of facilities for our input, OPT(S) can be much larger than the actual ofﬂine optimum OPT. However, by combining TAKEHEED with known results of Mahdian et al.
[2012] we can get a robust version of TAKEHEED called WARY, whose cost is (cid:18) (cid:26)
O min log(|S|) · OPT(S), log(n) log log(n) (cid:27)(cid:19)
· OPT
.
Note that the second term matches the best possible bound, in view of the lower bound of Fotakis
[2003]. Our experiments conﬁrm the good behavior in practice of this robust version. Indeed, it outperforms other state-of-the-art robust solutions.
One of the appealing features of TAKEHEED is that it leverages multiple advice in a simple way.
The algorithm however could suffer from an “adversarial” attack of sorts. If very large bad sets of suggested facilities are given as part of the advice, the theoretical guarantee, which depends on the cardinality of the union of the advice sets, becomes uninteresting. To deal with this we design 2
BUCKETHEED, an algorithm that is resilient to the presence of such large, bad advice. If the best advice set is Si∗ , the expected cost of the solution produced by this algorithm is
O (log log(n) · (log(k) + log(|Si∗ |)) · OPT(Si∗ ).
This result is mainly of theoretical interest. Indeed, one of the strengths of TAKEHEED is that in practice small advice sets are easy to obtain and the term log(|S|) is going to be quite small, making the overall approach very actionable.