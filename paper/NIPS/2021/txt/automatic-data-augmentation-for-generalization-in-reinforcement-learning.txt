Abstract
Deep reinforcement learning (RL) agents often fail to generalize beyond their training environments. To alleviate this problem, recent work has proposed the use of data augmentation. However, different tasks tend to beneﬁt from different types of augmentations and selecting the right one typically requires expert knowledge.
In this paper, we introduce three approaches for automatically ﬁnding an effective augmentation for any RL task. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmen-tation theoretically sound for actor-critic algorithms. Our method achieves a new state-of-the-art1on the Procgen benchmark and outperforms popular RL algorithms on DeepMind Control tasks with distractors. In addition, our agent learns policies and representations which are more robust to changes in the environment that are irrelevant for solving the task, such as the background. Our code is available at https://github.com/rraileanu/auto-drac. 1

Introduction
Generalization to new environments remains a major challenge in deep reinforcement learning (RL). Current methods fail to generalize to unseen environments even when trained on similar settings [19, 51, 71, 11, 21, 12, 60]. This indicates that standard RL agents memorize speciﬁc trajectories rather than learning transferable skills. Several strategies have been proposed to alleviate this problem, such as the use of regularization [19, 71, 11, 28], data augmentation [11, 44, 69, 38, 41], or representation learning [72, 74]. In this work, we focus on the use of data augmentation in RL. We identify key differences between supervised learning and reinforcement learning which need to be taken into account when using data augmentation in RL.
More speciﬁcally, we show that a naive application of data augmentation can lead to both theoretical and practical problems with standard RL algorithms, such as unprincipled objective estimates and poor performance. As a solution, we propose Data-regularized Actor-Critic or DrAC, a new algorithm that enables the use of data augmentation with actor-critic algorithms in a theoretically sound way. Speciﬁcally, we introduce two regularization terms which constrain the agent’s policy and value function to be invariant to various state transformations. Empirically, this approach allows the agent to learn useful behaviors (outperforming strong RL baselines) in settings in which a naive use of data augmentation completely fails or converges to a sub-optimal policy. While we use Proximal
Policy Optimization (PPO, Schulman et al. [56]) to describe and validate our approach, the method 1in June 2020, at the time of making this work publicly available on arXiv. it has since been surpassed. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
can be easily integrated with any actor-critic algorithm with a discrete stochastic policy such as
A3C [49], SAC [24], or IMPALA [17].
Figure 1: Overview of UCB-DrAC. A UCB bandit selects an image transformation (e.g. random-conv) and applies it to the observations. The augmented and original observations are passed to a regularized actor-critic agent (i.e. DrAC) which uses them to learn a policy and value function which are invariant to this transformation.
The current use of data augmentation in RL either relies on expert knowledge to pick an appropriate augmentation [11, 44, 38] or separately evaluates a large number of transformations to ﬁnd the best one [69, 41]. In this paper, we propose three methods for automatically ﬁnding a useful augmentation for a given RL task. The ﬁrst two learn to select the best augmentation from a ﬁxed set, using either a variant of the upper conﬁdence bound algorithm (UCB, Auer [2]) or meta-learning (RL2,
Wang et al. [66]). We refer to these methods as UCB-DrAC and RL2-DrAC, respectively. The third method, Meta-DrAC, directly meta-learns the weights of a convolutional network, without access to predeﬁned transformations (MAML, Finn et al. [20]). Figure 1 gives an overview of UCB-DrAC.
We evaluate these approaches on the Procgen generalization benchmark [12] which consists of 16 procedurally generated environments with visual observations. Our results show that UCB-DrAC is the most effective among these at ﬁnding a good augmentation, and is comparable or better than using DrAC with the best augmentation from a given set. UCB-DrAC also outperforms baselines speciﬁcally designed to improve generalization in RL [28, 44, 41] on both train and test. In addition, we show that our agent learns policies and representations that are more invariant to changes in the environment which do not alter the reward or transition function (i.e. they are inconsequential for control), such as the background theme.
To summarize, our work makes the following contributions: (i) we introduce a principled way of using data augmentation with actor-critic algorithms, (ii) we propose a practical approach for automatically selecting an effective augmentation in RL settings, (iii) we show that the use of data augmentation leads to policies and representations that better capture task invariances, and (iv) we demonstrate state-of-the-art results on the Procgen benchmark and outperform popular RL methods on four DeepMind Control tasks with natural and synthetic distractors. 2