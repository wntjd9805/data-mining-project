Abstract
Two-stage methods have dominated Human-Object Interaction (HOI) detection for several years. Recently, one-stage HOI detection methods have become popular. In this paper, we aim to explore the essential pros and cons of two-stage and one-stage methods. With this as the goal, we ﬁnd that conventional two-stage methods mainly suffer from positioning positive interactive human-object pairs, while one-stage methods are challenging to make an appropriate trade-off on multi-task learning, i.e., object detection, and interaction classiﬁcation. Therefore, a core problem is how to take the essence and discard the dregs from the conventional two types of methods. To this end, we propose a novel one-stage framework with disentangling human-object detection and interaction classiﬁcation in a cascade manner. In detail, we ﬁrst design a human-object pair generator based on a state-of-the-art one-stage
HOI detector by removing the interaction classiﬁcation module or head and then design a relatively isolated interaction classiﬁer to classify each human-object pair.
Two cascade decoders in our proposed framework can focus on one speciﬁc task, detection or interaction classiﬁcation. In terms of the speciﬁc implementation, we adopt a transformer-based HOI detector as our base model. The newly introduced disentangling paradigm outperforms existing methods by a large margin, with a signiﬁcant relative mAP gain of 9.32% on HICO-Det. The source codes are available at https://github.com/YueLiao/CDN. 1

Introduction
The goal of Human-Object Interaction (HOI) detection [2, 20, 6, 18, 7, 8, 17, 3] is to make a machine detailedly understand human activities from a static image. Human activities in this task are abstracted as a set of <human, object, action> HOI triplets. Thus, an HOI detector is required to locate human-object pairs and classify their corresponding action simultaneously. Based on this deﬁnition, we can summarize conventional HOI detection methods into two paradigms, i.e., two-stage methods, and one-stage methods. These two paradigms have made signiﬁcant progress with the development of deep learning, but both paradigms still have their shortcomings due to their structural design.
This paper aims to present a detailed analysis of methods under these two paradigms and propose a solution to mine the beneﬁts of two-stage and one-stage methods.
We ﬁrst take a closer look at the conventional two-stage and one-stage HOI detectors. Conventional two-stage methods [6, 2, 18, 5] are mostly with a serial architecture. As shown in Figure 1 (a), two-stage methods detect humans and objects ﬁrst and then feeds the human-object pairs, which are generated by matching humans and objects one by one, into an interaction classiﬁer. The serial architecture suffers from locating the interactive human-object pairs under the interference of a large number of negative pairs only based on local region features. Otherwise, the efﬁciency of
∗Equal contribution
†Corresponding author (liusi@buaa.edu.cn) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) Two-stage framework, (b) one-stage end-to-end framework, (c) one-stage framework with parallel architecture, and (d) our one-stage framework with a cascade disentangling head. two-stage methods is also limited by the serial architecture. To alleviate these problems, one-stage methods [20, 13, 39, 3, 28, 14] are proposed to detect the HOI triplets directly and break HOI detection as multi-task learning, i.e., human-object detection and interaction classiﬁcation, which is shown in Figure 1(b). Therefore, one-stage methods can easily focus on the interactive human-object pairs and effectively extract corresponding features in an end-to-end manner. However, it is difﬁcult for a single model to make a good trade-off on multi-task learning since human-object detection and interaction classiﬁcation are two very different tasks, which requires the model to focus on different visual features. As shown in Figure 1(c), though some previous methods [20, 3] design two parallel branches to detect instances and predict interaction respectively, the interaction classiﬁcation branch still needs to regress additional offsets to associate humans and objects. Thus the interaction branch is also required to make a trade-off between interaction classiﬁcation and human and object positioning.
Therefore, the intuitive idea is to take the essence and discard the dregs from the two paradigms. To attain this, we propose a novel end-to-end one-stage framework with disentangling human-object detection and interaction classiﬁcation in a cascade manner, namely Cascade Disentangling Net-work (CDN). The original intention of our framework is to keep the advantages of conventional one-stage methods, directly and accurately locating the interactive human-object pairs, and bring the advantages of two-stage methods into our one-stage framework, disentangling human-object detection and interaction classiﬁcation. As shown in Figure 1(d), in our proposed framework, we design a human-object pair decoder based on the one-stage paradigm by removing the interaction classiﬁcation function, namely HO-PD, and following an isolated interaction classiﬁer. To instantiate our idea with an end-to-end manner, we design the HO-PD based on the previous state-of-the-art one-stage transformer-based HOI detector, HOI-Trans [39] and QPIC [28], where we remove the interaction classiﬁcation head for each query and make it focus on human-object pairs detection.
Otherwise, we design an independent HOI decoder for interaction classiﬁcation to make the interac-tion classiﬁcation unaffected by human-object detection. Therefore, there exists a core problem, i.e., how to link the human-object pair and the corresponding action class. To address this problem, we initialize the query embedding of the HOI decoder with the output of the last layer of the HO-PD.
In this case, the HOI decoder is able to learn the corresponding action category under the guidance of the query embedding and free out from the human-object detection task. Moreover, we design a decoupling dynamic re-weighting manner to handle the long-tailed problems in HOI detection.
Our contributions can be summarized threefold: (1) We conduct a detailed analysis of two conven-tional HOI detection paradigms, i.e., two-stage and one-stage. (2) We propose a novel one-stage framework with a cascade disentangling decoder to combine the advantages of two-stage and one-stage methods. (3) Our method outperforms previous state-of-the-art methods by a large margin on the HOI detection task, especially achieves a 25.35% performance gain on rare classes of HICO-Det. 2 Analysis of Two-stage and One-stage HOI detectors
We ﬁrst introduce a uniﬁed formulation for the HOI detection problem. Given a human-centric image
I, the model T (·) is required to predict a set of HOI triplets S = {(bh i , ai), i ∈ {1, 2, · · · , K}}, where bh i and ai denotes a human bounding-box, an object bounding-box and their corresponding action category, respectively. i , bo i , bo
Two-stage HOI detector. Two-stage detectors can be regarded as an instance-driven manner, detect-ing instances ﬁrst and predicting interaction based on the detected instances. The two-stage detector divides T (·) into two stages, i.e., detection Td(·) and interaction classiﬁcation Tc(·). In the ﬁrst stage, 2
we suppose that Td(·) produces M human bounding-boxes and N object bounding-boxes. Here the
‘object’ is a universal object which includes human as one class. Therefore, Td(·) generates M × N human-object pairs. In general, the number of true-positive interactive human-object pairs, denoted as
K (cid:48), is much smaller than M × N . However, in the second stage, Tc(·) needs to scan all M × N pairs one by one and predict an action category with its corresponding conﬁdence score. In this case, Tc(·) is required to inference M × N times to ﬁnd K (cid:48) interactive pairs from M × N pairs. We argue that this manner causes three problems. Firstly, these models produce a more additional computational cost, whose time complexity is O(M × N ) (cid:29) O(K (cid:48)). Secondly, the imbalance between positive and negative samples makes the model easily overﬁt to negative samples. Thus the model tends to assign a ‘no-interaction’ class for human-object pairs with very high conﬁdence, suppressing the true-positive samples. Thirdly, the accuracy of interaction classiﬁcation is inﬂuenced by the non-end-to-end pipeline. Because the interaction classiﬁcation is mostly based on the region features extracted by Td(·), while the core of Td(·) is to regress bounding-boxes and its extracted features pay more attention to the edge of regions, thereby such features are not good options for interaction classiﬁcation, which needs more context. However, it is an excellent property for two-stage methods that disentangling detection and interaction classiﬁcation makes each stage focus on its task and produce good results in each stage.
One-stage HOI detector. As for one-stage methods, they detect all HOI triplets S directly and simultaneously with an end-to-end framework. Such paradigm has greatly relieved the three problems of two-stage methods, especially for efﬁciency, where the time complexity is reduced to O(K (cid:48)).
Most one-stage methods are interaction-driven, which directly locate the interaction point [20] or interactive human-object pairs [39], thereby reducing negative sample interference. However, coupling human-object detection and interaction classiﬁcation limit their performance because it is hard to generate a uniﬁed feature representation for two very different tasks. Though the parallel one-stage methods break HOI detection into two parallel branches, their interaction branch still suffers from multi-task learning. Speciﬁcally, the optimization target of interaction branch is P(eh, eo, a|V ), where eh and eo are associative embeddings, e.g., offset, to match interaction with human and object respectively. Therefore, even if detection is organized as an independent branch, the interaction branch must position humans and objects for the association. The set-based detectors couple detection and interaction completely, whose optimization function is P(bh, bo, a|V ).
Next, we introduce a simple one-stage framework with disentangling human-object detection and interaction classiﬁcation, namely CDN, to mine the beneﬁts of two-stage and one-stage HOI detectors.
Our CDN disentangles the original set-based one-stage optimization function into two cascade decoders. Firstly, we predict human-object pair by P(bh, bo|V ). Secondly, we apply an isolated decoder to predict the action category by P(a|V , bh, bo). More details are in the following. 3 Method
In this section, we will present a detailed introduction to the pipeline of our proposed CDN. In section 3.1, we present an overview of our framework and brieﬂy introduce the pipeline. In section 3.2, we introduce the visual feature extractor. The cascade disentangling HOI decoder is introduced in section 3.3. Section 3.4 introduces a novel dynamic re-weighting mechanism that mitigates the long-tailed problem. The detailed training process and post-processing are discussed in section 3.5. 3.1 Overview
The architecture of our proposed CDN is illustrated in Figure 2. Our CDN is organized in a cascade manner with a visual feature extractor. Given an image, we ﬁrst follow transformer-based detection methods [1, 39] to apply a CNN followed by a transformer encoder architecture to extract visual features into a sequence. Then we detect HOI triplets in two cascade decoders. Firstly, we apply the Human-Object Pair Decoder (HO-PD) to predict a set of human-object bounding-boxes pairs based on a set of learnable queries. Next, taking the output of the last layer of HO-PD as queries, an isolated interaction decoder is utilized to predict the action category for each query. Finally, the HOI triplets are formed by the output of the above two cascade decoders. 3.2 Visual Feature Extractor
We deﬁne the visual feature extractor by combining a CNN and a transformer encoder. Fed with
, Db). an input image I with shape (H, W, C), the CNN generates a feature map of shape (H
, W (cid:48) (cid:48) 3
Figure 2: The framework of our CDN. It is comprised of three components:Visual Feature Extractor,
Human-Object Pair Decoder (HO-PD) and Interaction Decoder. We ﬁrst apply a CNN-transformer combined architecture to extract sequenced visual features Xs. Then, we divide HOI detection into two cascade transformer-based decoders. Firstly, we regress the human-object bounding-box pairs based on Xs and a set of random-initialized queries Qd by HO-PD. The interactive score is from a binary classiﬁcation to determine whether the human-object pair is an interactive pair or not.
Secondly, we predict one or many action categories for each predicted human-object pairs, where we take the output of HO-PD Qout to initialize the interaction queries Qc and aggregate information with Xs. Finally, the HOI triplets are formed by the output of the cascade decoders. d
Then, Db is reduced to Dc by a projection convolution layer with a kernel size 1 × 1. Next, a ﬂatten operator is used to generating the ﬂatten feature Xv ∈ R(H
)×Dc by collapsing the spatial dimensions into one dimension. This ﬂatten feature is then fed into a transformer encoder and the
)×Dc, which distinguishes the relative position in the sequence position encoding Epos ∈ R(H
Xs ∈ R(H
)×Dc. Thanks to the multi-head self-attention mechanism, the transformer encoder produces a feature map with richer contextual information by summarizing global information. The output of the encoder is denoted as global memory with a dimension of Dc.
×W
×W
×W (cid:48) (cid:48) (cid:48) (cid:48) (cid:48) (cid:48) 3.3 Cascade Disentangling HOI Decoder
The cascade disentangling HOI decoder consists of two decoders: Human-Object Pair Decoder (HO-PD) and interaction decoder. Both decoders share the same architecture, a transformer-based decoder, with independent weights. In this subsection, we ﬁrst introduce the general architecture of the decoder and then elaborate on the two decoders in detail, respectively.
Transformer-based decoder. We follow the transformer-based object detector DETR [1] to design the basic architecture in our cascade disentangling HOI decoder. We apply N transformer decoder layers for each decoder and equip each decoder layer with several FFN heads for intermediate supervision. Speciﬁcally, each decoder layer is comprised of a self-attention module and a multi-head co-attention module. During feed-forward, fed into a set of learnable queries Q ∈ RNq×Cq , each decoder layer ﬁrst applies a self-attention module on all queries and then conducts a multi-head co-attention operation between queries and the sequenced visual features, and outputs a set of updated queries. For the FFN heads, each head is comprised of one or several MLP branches, and each branch is for a speciﬁc task, e.g., regression, or classiﬁcation. All queries share the same FFN heads.
Therefore, each decoder can be simply represented as:
P = f (Q, Xs, Epos). (1)
Besides, the number of queries Nq is determined by the number of positive samples of an image.
HO-PD. Firstly, we design the HO-PD to predict a set of human-object pairs from the sequenced visual features. To this end, we ﬁrst randomly initialize a set of learnable queries Qd ∈ RNd×Cq as
HO queries. Then we apply a transformer-based decoder, which takes HO queries Qd and sequenced visual features as input and applies three FFN heads for each query to predict human bounding-box, object bounding-box, and object class, which form a human-object pair. We also utilize an additional interactive score head to simply determine whether the human-object pair is an interactive pair or not by a binary classiﬁcation. In this case, P is instantiated as Pho, which is consist of a set of 4
human-object pairs {(bh i , bo i ), i ∈ {1, 2, · · · , Nd}}. Thus, HO-PD can be denoted as:
In addition, we keep the output queries of the last layer of HO-PD as Qout d for the following step.
Pho = fd(Qd, Xs, Epos). (2)
Interaction Decoder. Secondly, we propose the interaction decoder to classify the human-object queries and assign one or several action categories for each human-object query. To classify each human-object query one-to-one, we initialize Qc with the output of HO-PD Qout d . In this way,
Qout can provide prior knowledge to guide Qc to learn the corresponding action categories for each d human-object query. The other components and inputs are the same as HO-PD, which conducts self-attention among queries and co-attention with Xs and Epos. The ﬁnal output is a set of action categories Pcls : {ai, i ∈ {1, 2, · · · , Nd}}. Therefore, this process can be formulated as:
Pcls = fcls(Qout d , Xs, Epos). (3)
In our proposed cascade disentangling HOI decoder, the task of HOI detection is disentangled into two relatively independent steps: human-object pairs detection and interaction classiﬁcation. Therefore, each step can aggregate more related features to concentrate on its corresponding task. 3.4 Decoupling Dynamic Re-weighting
The HOI datasets usually have long-tail class distribution for both object class and action class. To alleviate the long-tail problem, we design a dynamic re-weighting mechanism for further improve-ments with a decoupling training strategy. In detail, we ﬁrst train the whole model with regular losses.
Then, we freeze the parameters of the visual feature extractor and only train the cascade disentangling decoders with a relatively small learning rate and the designed dynamic re-weighted losses.
During decoupling training, at each iteration, we apply two similar queues to accumulate number of each object class or action class. The queues are used as memory banks to accumulate training samples and truncate the accumulation with length LQ as sliding windows. In detail, Qo with length
Lo i for each object class i ∈ {1, 2, · · · , Co}, and Qa with length
La i for each action category i ∈ {1, 2, · · · , Ca}. The dynamic re-weighting coefﬁcients wdynamic are presented as follow:
Q to accumulate object number N o
Q to accumulate interaction number N a wa i (cid:12) (cid:12)i∈{1,2,··· ,Ca} = (cid:16) (cid:80)Ca i=1 Ni
Ni (cid:17)pa
, wa bg = (cid:16) (cid:80)Ca i=1 Ni
N a bg (cid:17)pa wo i (cid:12) (cid:12)i∈{1,2,··· ,Co} = (cid:16) (cid:80)Co i=1 Ni
Ni (cid:17)po
, wo bg = (cid:16) (cid:80)Co i=1 Ni
N o bg (cid:17)po
,
, (4) (5) where Ni is the number of accumulated positive samples of category i by the queues Qo and Qa, Nbg is the number of accumulated background samples, C is the number of categories, and exponent p is a hyper-parameter that adapts the magnitude of mitigation. Speciﬁcally, the weight of background class, wbg, is designed to balance the positives and negatives. For the stability of the dynamic re-weighted training, the weight coefﬁcients are initialized as wstatic with those calculated by 4 and 5 using the static number of object and action categories. The ﬁnal dynamic weights are given as w = γwstatic + (1 − γ)wdynamic, where γ is a smooth factor, given as min(0.999LQ, 0.9). The factor γ transits w from wstatic to wdynamic with the increasing of LQ. Finally, the weights are used to the classiﬁcation losses in a traditional way by multiplying each coefﬁcient to each class and then calculating the summation. 3.5 Training and Post-processing
In this section, we introduce the training and inference processes in detail. Especially, we will introduce a novel Pair-wise Non-Maximal Suppression (PNMS) strategy in the inference process.
Training. Following the set-based training process of HOI-Trans [39] and QPIC [28], we ﬁrst match each ground-truth with its best-matching prediction by the bipartite matching with the Hungarian algorithm. Then the loss is produced between the matched predictions and the corresponding ground truths for the ﬁnal back-propagation. During matching, we consider the predictions of two cascade decoders together. The loss of CDN follows QPIC which is composed by ﬁve parts: the box regression 5
loss Lb, the intersection-over-union loss LGIoU [26], the interactive score loss Lp, the object class loss Lo c . The target loss is the weighted sum of these parts as: c, and the action category loss La (cid:88)
L = (λbLk b + λGIoU Lk
GIoU ) + λpLp + λoLo c + λaLa c , (6) k∈(h,o) where λb, λGIoU , λp, λo and λa are the hyper-parameters for adjusting the weights of each loss.
Inference. The inference process is to composite the output of instance-related FFNs and the interaction-related FFN to form HOI triplets. By our cascade disentangling decoder architecture, the instance queries and the interaction queries are one-to-one corresponding, therefore, the ﬁve components <human bounding box, object bounding box, object class, interactive score, action class> can be homologous in each of the Nd dimensions per FFN head. Formally, we generate the i-th i cp i co output prediction as <bh i , where ca i and co i is the interactive score from the interactive FFN head for the query vector being an human-object pair.
PNMS. After sorting choi in descending order and generating the top K HOI triplets, we design a pair-wise non-maximal suppression (PNMS) method to further ﬁlter out human-object pairs from pair-wise bounding boxes overlapping perspective. For two HOI triplets m and n, the pair-wise overlap PIoU is calculated as: i = ca i are the scores of interaction and object classiﬁcation, respectively, and cp (k)>. The HOI triplet score choi i , argmaxkchoi is given by choi i , bo i i i m, bh n) m, bh n) where the operators I and U compute the intersection and union areas between the two boxes of m and n, respectively. α and β are the balancing parameters between humans and objects. (cid:17)α(cid:16) I(bo
U (bo m, bo n) m, bo n) (cid:16) I(bh
U (bh
P IoU (m, n) = (7) (cid:17)β
, 4 Experiments
In this section, we conduct comprehensive experiments to demonstrate the superiority of our designed
CDN. In section 4.1, we brieﬂy introduce the experimental benchmarks. Section 4.2 presents implementation details. Next, It is a detailed experimental comparison and analysis of two-stage and one-stage methods in section 4.3. In section 4.4, we compare our methods with the previous state-of-the-art methods. The ablation studies and components analysis are included in 4.5. 4.1 Datasets and Evaluation Metrics
Datasets. We carry out experiments on two widely-used HOI detection benchmarks: HICO-Det [2] and V-COCO [8]. We follow the standard evaluation scheme. HICO-Det consists of 47, 776 Creative
Common images from Flickr (38, 118 for training and 9, 658 for test) with more than 150K human-object pairs. It contains the same 80 object categories as MS-COCO [21] and 117 action categories.
The objects and actions form 600 classes of HOI triplets. V-COCO is derived from MS-COCO dataset, which consists of 5, 400 images in the trainval subset and 4, 946 images in the test subset. It has 29 action categories (25 HOIs and 4 body motions) and 80 object categories. For both datasets, one person can interact with multiple objects in different ways at the same time.
Evaluation Metrics. Following the standard evaluation [2], we use the mean average precision (mAP) as the evaluation metric. For one positively predicted HOI triplet <human, object, action>, it needs to contain accurate human and object locations (box IoU with reference to GT box is greater than 0.5) and correct object and action categories. Speciﬁcally, for HICO-Det, besides the full set of 600 HOI classes, we also report the mAP over a rare set of 138 HOI classes that have less than 10 training instances and a non-rare set of the other 462 HOI classes. For V-COCO, we report the role mAP for two scenarios: scenario 1 includes the cases even without any objects (for the four action categories of body motions), and scenario 2 ignores these cases. 4.2
Implementation Details
We implement three variant architectures of CDN: CDN-S, CDN-B, and CDN-L, where ‘S’, ‘B’, and
‘L’ denote small, base, and large, respectively. For CDN-S and CDN-B, we adopt ResNet-50 with a 6-layer transformer encoder as the visual feature extractor. For the cascade decoders, CDN-S is equipped with both 3-layer transformers, while CDN-B has a 6-layer transformer for each decoder.
CDN-L only replaces the ResNet-50 with ResNet-101 in CDN-B. The reduced dimension size Dc 6
is set to 256. The number of queries Nd is set to 64 for HICO-Det and 100 for V-COCO since the average number of positives for variant human-object pairs per image of HICO-Det is smaller than
V-COCO. The human and object box FFNs have 3 linear layers with ReLU, while the object and action category FFNs have one linear layer. The code is provided in supplemental material.
During training, we initialize the network with the parameters of DETR [1] trained with the MS-COCO dataset. We set the weight coefﬁcients λb, λGIoU , λp, λo and λa to 2.5, 1, 1, 1 and 1, respectively, which are exactly same with QPIC [28]. We optimize the network by AdamW [23] with the weight decay 10−4. We ﬁrst train the whole model for 90 epochs with a learning rate of 10−4 decreased by 10 times at the 60th epoch. Then, during the decoupling training process, we ﬁne-tune the cascade disentangling decoders together with the box, object, and action FFNs for 10 epochs with a learning rate of 10−5. We use both object and action dynamic re-weighting for HICO-Det and only action dynamic re-weighting for V-COCO. The re-weighting parameter p is set to 0.7 for both object and action. The length LQ of training sample queue Q for both object and action is set to 2 × Ns, where Ns is the sample number of the training set. All experiments are conducted on the 8 Tesla
V100 GPUs and CUDA10.2, with a batch size of 16.
For validation, we select 100 detection results with the highest scores and then adopt PNMS to further
ﬁlter results. The threshold, α, and β of PNMS are set to 0.7, 1, and 0.5, respectively. 4.3 Experiment Analysis of Two-stage and One-stage Methods
In this part, we introduce a detailed experimental analysis of conventional two-stage and one-stage methods and our proposed CDN from the following three aspects.
Human-object Pair Generation. We ﬁrst explore the quality of the human-object pairs generation between two-stage and one-stage methods. To attain this, we conduct a detailed experiment based on a representative two-stage method iCAN [6]. We ﬁrst implement a PyTorch version iCAN as the baseline model, denoted as iCAN∗, which only applies human and object appearance with a
COCO-pretrained Faster-RCNN detector [25]. For a fair comparison, we ﬁrst ﬁne-tune DETR on
HICO-Det for 100 epochs only with the instance detection annotation based on COCO-pretrained weights. Then we combine the detected human and object bounding-boxes, whose conﬁdences are greater than a threshold, one by one to generate human-object pairs denoted as iCAN† in Table 1.
We train our CDN only with HO-PD for 100 epochs and get the human-object pairs from the output directly. Then, we graft the human-object pairs to the baseline model to extract box features and utilize the same interaction classiﬁer in the second stage of iCAN∗. In this way, we degrade the number of pairs from M × N to K (cid:48), which means time complexity is reduced from O(M × N ) to
O(K (cid:48)). Primarily, HO-PD signiﬁcantly promotes mAP from 15.37 to 24.05, as shown in Table 1.
This indicates that one-stage methods are much superior in human-object pair generation.
Strategy iCAN∗ iCAN †
Full 14.16 15.37
HO-PD+iCAN∗ 24.05 29.07 30.96
CDN-S base
QPIC [28]
Rare Non-Rare 12.26 13.23 18.32 21.85 27.02 14.73 16.01 25.76 31.23 32.14
Table 1: Analysis of Two-stage and One-stage Methods. ∗ denotes our implemented
PyTorch version iCAN [6] baseline model. † denotes replacing instance detection boxes given by a HICO-Det ﬁne-tuned DETR
‘HO-detector to extract box features.
PD+iCAN∗’ denotes replacing original one-by-one generated human-object pairs with our HO-PD generated. ‘CDN-S base’ de-notes CDN-S w/o re-weighting and PNMS strategies.
Figure 3: Visualization of Feature Maps for
Queries. Visual attended features for query with top-1 score extracted from the last layer of the de-coder of (a) QPIC, (b) HO-PD in CDN, and (c) interaction decoder in CDN. Zoom in for details.
Interaction Classiﬁcation. We aim to study the interaction classiﬁcation between conventional multi-task one-stage methods and our disentangled one-stage detector. We can regard QPIC [28] as a 7
Detector
Backbone
Extra
Full
Default
Rare
Non-Rare
Full
Rare
Non-Rare
Know Object
Method
Two-stage Method:
InteractNet [7]
GPNN [24] iCAN [6]
No-Frills [9]
PMFNet [30]
CHGNet [31]
DRG [5]
VCL [12]
IP-Net [32]
VSGNet [29]
FCMNet [22]
ACP [15]
PD-Net [35]
DJ-RN [16]
IDN [17]
One-stage Method:
UnionDet [13]
DIRV [4]
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
COCO
PPDM-Hourglass [20] HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HICO-DET
HOI-Trans [39]
GG-Net [37]
ATL [11]
HOTR [14]
AS-Net [3]
QPIC [28]
CDN-S
CDN-B
CDN-L
ResNet-50-FPN
Res-DCN-152
ResNet-50
ResNet-152
ResNet-50-FPN
ResNet-50
ResNet-50-FPN
ResNet-50
Hourglass-104
ResNet-152
ResNet-50
ResNet-152
ResNet-152-FPN
ResNet-50
ResNet-50
ResNet-50-FPN
EfﬁcientDet-d3
Hourglass-104
ResNet-50
Hourglass-104
ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-50
ResNet-101 (cid:55) (cid:55) (cid:55)
P
P (cid:55)
T (cid:55) (cid:55) (cid:55) (cid:55)
T
T
P (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) 9.94 13.11 14.84 17.18 17.46 17.57 19.26 19.43 19.56 19.80 20.41 20.59 20.81 21.34 23.36 17.58 21.78 21.94 23.46 23.47 23.81 25.10 28.87 29.07 31.44 31.78 32.07 7.16 9.34 10.45 12.17 15.65 16.85 17.74 16.55 12.79 16.05 17.34 15.92 15.90 18.53 22.47 11.72 16.38 13.97 16.91 16.48 17.43 17.34 24.25 21.85 27.39 27.55 27.19 10.77 14.23 16.15 18.68 18.00 17.78 19.71 20.29 21.58 20.91 21.56 21.98 22.28 22.18 23.63 19.33 23.39 24.32 25.41 25.60 25.72 27.42 30.25 31.23 32.64 33.05 33.53
--16.26
-20.34 21.00 23.40 22.00 22.05
-22.04
-24.78 23.69 26.43 19.76 25.52 24.81 26.15 27.36 27.38
-31.74 31.68 34.09 34.53 34.79
--11.33
-17.47 20.74 21.75 19.09 15.77
-18.97
-18.88 20.64 25.01 14.68 20.84 17.09 19.24 20.23 22.09
-27.07 24.14 29.63 29.73 29.48
--17.73
-21.20 21.08 23.89 22.87 23.92
-23.12
-26.54 24.60 26.85 21.27 26.92 27.12 28.22 29.48 28.96
-33.14 33.93 35.42 35.96 36.38
Table 2: Performance comparison on the HICO-Det test set. The ‘P’, ‘T’ represent human pose information and the language feature, respectively. multi-task version of our CDN. Table 1 shows that our ‘CDN-S base’ (w/o re-weighting and PNMS strategies) has achieved mAP 30.96 with 6.50% relative mAP gain compared to QPIC. Especially, our ‘CDN-S base’ signiﬁcantly outperforms QPIC for rare classes with a 23.66% improvement. The performance of rare classes can partly reﬂect the accuracy of interaction classiﬁcation.
Feature Learning. This part discusses the differences in feature learning between the conventional one-stage method, QPIC, and our CDN from a qualitative view. As shown in Figure 3, we visualized the feature maps extracted from the last layer of the decoder of QPIC, the HO-PD, and the interaction decoder in our CDN. We can see that HO-PD and QPIC attend very similar regions, e.g., the boundaries of humans and objects and the human-object contact areas, which are beneﬁcial for locating the interactive human-object pairs. However, the interaction decoder concentrates on human-pose and the regions that contribute to understanding human actions. As for the speciﬁc case, for example, for ‘hold cake’, HO-PD in CDN attends to the boundaries of the cake while the interaction decoder in CDN concentrates on the interaction context, i.e., the human’s hands holding the cake.
Thus, it shows that CDN disentangles the human-object detection and interaction classiﬁcation. For
‘ride horse’, HO-PD in CDN emphasizes the overall feature of the human and the horse, and the highlight parts are the edges of the human and horse. For the interaction decoder in CDN, the highlighted part emphasizes the interaction context, i.e., the human carries the rope when riding a horse. Finally, QPIC somehow combines the two highlights, but both are not obvious. 4.4 Comparison to State-of-the-Art
We conduct experiments on HICO-Det and V-COCO benchmarks to verify the effectiveness of our proposed methods. For HICO-Det dataset as shown in Table 2, comparing to the previous state-of-the-art two-stage method FCMNet [22] with ResNet-50 as backbone, our CDN-B signiﬁcantly promotes mAP from 20.41 to 31.78, with a relative gain of 55.71%. Even compared with PD-Net [36] which adopts extra language feature and DJ-RN [16] which utilizes extra human pose features, CDN-B achieves 52.71% and 48.92% relative mAP gains, respectively. When comparing to the one-stage method AS-Net [3] and QPIC [28] which also adopt transformer-based detector architecture, CDN-B attains 10.08% and 9.32% point relative mAP gains, respectively. Table 3 shows the comparisons on V-COCO dataset. CDN-B achieves 62.29 AProle on Scenario 1 and 64.42 AProle on Scenario 2, which signiﬁcantly outperform previous state-of-the-art method QPIC with relative 5.94% and 5.61% 8
Method
Two-stage Method:
InteractNet [7]
GPNN [24] iCAN [6]
TIN [18]
VCL [12]
DRG [5]
IP-Net [32]
VSGNet [29]
PMFNet [30]
PD-Net [35]
CHGNet [31]
FCMNet [22]
ACP [15]
IDN [17]
One-stage Method:
UnionDet [13]
HOI-Trans [39]
AS-Net [3]
GG-Net [37]
HOTR [14]
DIRV [4]
QPIC [28]
CDN-S
CDN-B
CDN-L
Extra APS1 role APS2 role (cid:55) (cid:55) (cid:55) (cid:55) (cid:55)
T (cid:55) (cid:55)
P
T (cid:55) (cid:55)
T (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) 40.0 44.0 45.3 47.8 48.3 51.0 51.0 51.8 52.0 52.6 52.7 53.1 53.23 53.3
--52.4 54.2
---57.0
-----60.3 56.2 47.5
-52.9
-53.9
-54.7 64.4 55.2
-56.1 58.8 61.0 61.68 63.77 62.29 64.42 63.91 65.89
Table 3: Performance comparison on the V-COCO test set. The ‘P’, ‘T’ rep-resent the human pose information and the language feature, respectively.
Strategy
QPIC [28] base
+ re-weighting
+ PNMS
Full 29.07 31.06 31.38 31.78
Rare 21.85 26.68 27.36 27.55
Non-Rare 31.23 32.36 32.58 33.05 (a) Strategies: Analysis of improvements by various training strategies.
Strategy base decouple static dynamic dynamic dynamic
LQ
---2 × Ns 1 × Ns 2 × Ns p
--0.7 0.8 0.7 0.7
Full 31.06 30.90 31.25 31.33 31.34 31.38
Rare 26.68 26.09 27.12 27.45 27.48 27.36
Non-Rare 32.36 32.33 32.49 32.49 32.49 32.58 (b) Dynamic re-weighting: Analysis of decouple training with dynamic re-weighted losses, i.e., different queue length LQ, coefﬁ-cient p and dynamic or static.
α
-1 1 1 1 1
β
-1 1 0.7 0.5 0.5 thres
-0.8 0.7 0.7 0.8 0.7
Full 31.38 31.66 31.75 31.77 31.75 31.78
Rare 27.36 27.46 27.50 27.54 27.51 27.55
Non-Rare 32.58 32.91 33.03 33.03 33.02 33.05 (c) PNMS: The effects of different settings of PNMS coef-ﬁcients, i.e., α, β, and thres denotes threshold.
Table 4: Ablation studies of our proposed method on the HICO-Det test set. We carry out all experiments based on the base model (CDN-B). gains, respectively. As for efﬁciency analysis, CDN-S has almost the same number of parameters and
ﬂops compared to QPIC, but CDN-S achieves mAP 31.44 on HICO-Det, 8.15% higher than QPIC. 4.5 Ablation Study
In this subsection, we analyse the effectiveness of the proposed strategies and components in detail.
All experiments are eventuated on the HICO-Det dataset. The performance of each strategy is evaluated in Table 4a. The ﬁve hyper-parameters of the training loss in 7 follow QPIC [28]. The ablation study of the two hyper-parameters in the re-weighting is shown in Table 4b, and that of the three hyper-parameters in the PNMS is shown in Table 4c. We carry out all experiments based on the model CDN-B with ResNet-50 as backbone.
Strategies. As shown Table 4a, our pure model without any additional post-processing operation, namely base model, achieves mAP 31.06, promoting 1.99 compared with QPIC [28]. Especially, the base model signiﬁcantly promotes mAP for rare classes from 21.85 to 26.68 compared to QPIC. It indicates the superiority of the architecture of disentangling human-object detection and interaction classiﬁcation. The re-weighted training further promotes mAP to 31.38, with a gain of 0.32, and the gain mainly lies in rare classes. Finally, the PNMS further improves mAP to 31.78.
Dynamic re-weighting. In this part, we conduct experiments to evaluate the components in the dynamic re-weighted training strategy based on the base model as shown in Table 4b. If we only decouple training without re-weighting, the model achieves mAP 30.90, which is lower than the base model. Therefore, it shows that the performance gain does not come from a longer training process.
Adding static weights wstatic to losses promotes mAP to 31.25. The dynamic re-weighting method improves the re-weighting effect since it captures the real-time weight of each class for each real-time sample during training. Thus it can sufﬁciently dig information from every single sample to achieve the best overall performance. Our method obtains best result mAP 31.38 when LQ = 2 × Ns and p = 0.7.
PNMS. On the basis of the model after re-weighted training, we compare the variance by different parameter settings of the PNMS strategy, which is shown in Table 4c. We ﬁx the human box balance 9
factor α to 1. Then we tune the object box balance factor β and the threshold of the PIoU to ﬁlter pair-wise boxes. We achieve best performance mAP 31.78 when β = 0.5 and thres = 0.7. The fact that β is smaller than α, indicates that the overall performance is more sensitive to human boxes compared with object boxes in our framework. 5