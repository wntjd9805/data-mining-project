Abstract
There is an increasing need for effective active learning algorithms that are com-patible with deep neural networks. This paper motivates and revisits a classic,
Fisher-based active selection objective, and proposes BAIT, a practical, tractable, and high-performing algorithm that makes it viable for use with neural models.
BAIT draws inspiration from the theoretical analysis of maximum likelihood esti-mators (MLE) for parametric models. It selects batches of samples by optimizing a bound on the MLE error in terms of the Fisher information, which we show can be implemented efﬁciently at scale by exploiting linear-algebraic structure especially amenable to execution on modern hardware. Our experiments demonstrate that
BAIT outperforms the previous state of the art on both classiﬁcation and regression problems, and is ﬂexible enough to be used with a variety of model architectures.

Introduction 1
The active learning paradigm considers a sequential, supervised learning scenario in which unlabeled samples are abundant but label acquisition is costly. At each round of active learning, the agent ﬁts its parameters using available labeled data before selecting a batch of unlabeled samples to be labeled and integrated into its training set. A well-chosen batch of samples is one that is maximally informative to the learner, such that it can obtain the best hypothesis possible given a ﬁxed labeling budget.
Active learning is well established as an area of machine learning research due to the ubiquity of impor-tant real world problems that ﬁt the sample-abundant, label-expensive setting; commonly cited applica-tions range from medical diagnostics [1, 2] to image labeling [3]. Mitigating large sample complexity requirements is particularly relevant for deep neural networks, which have in recent years achieved impressive success on a wide array of tasks but often require considerable amounts of labeled data.
Shifting the focus of active learning to deep neural networks highlights several important problems.
For one, most foundational active learning work assumes a convex setting, which is clearly violated by massive nonlinear neural networks. Many of these approaches are computationally expensive, and it is not clear how to adapt them for real-world use [4]. Further, because neural network training is generally expensive, practical active learning algorithms must be able to work in the batch regime, querying B samples at each round of active learning instead of a single point at a time [5].
Despite a long history of active learning research, these constraints draw attention to a need for practical, principled batch active learning algorithms for neural networks. Current state-of-the-art methods, like Batch Active Learning by Diverse Gradient Embeddings (BADGE), perform robustly in experiments, but explanations for its behavior are fairly limited [6]. This drawback makes it unclear how to scale some active learning algorithms into regimes that deviate somewhat from the setting for which they were designed—BADGE, for example, cannot be run on regression problems, and as we show in this paper, performs poorly when used in conjunction with a convex model. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This article adopts a probabilistic perspective of neural active learning. We view neural networks as specifying a conditional probability distribution p(y
,
X where ✓ are the network parameters. This perspective provides theoretical inspiration from the convex regime with which to examine and design neural active learning algorithms. From this viewpoint we motivate and revisit a classic, Fisher-based objective for idealized active selection. We argue that approximately minimizing this objective can be done tractably in the neural regime, despite their overparametrized structure and shifting internal representation. Accordingly, this work helps bridge the divide between algorithms that are performant but not well understood by theory, and those that are theoretically transparent but not computationally tractable. x, ✓) over label space given example
Y
|
Experimentally, BAIT offers improved performance over baselines in deep classiﬁcation problems, a trend that is robust to experimental conditions like batch size, model architecture, and dataset.
Crucially, BAIT is general purpose, and can be easily extended to regression settings, where many other algorithms cannot. It further performs well on both regression and classiﬁcation with convex models, a paradigm in which other algorithms often struggle.
In summary, this paper
•
•
• puts neural active learning on ﬁrm probabilistic grounding, giving a new, rigorous perspective on the functionality of previously proposed algorithms. provides in-depth empirics that elucidate differences between neural and convex regimes, and discusses simplifying assumptions that are sometimes reasonable in the neural case. proposes a practical, unifying, high-performing active learning algorithm that leverages these insights in a computationally tractable manner. 2