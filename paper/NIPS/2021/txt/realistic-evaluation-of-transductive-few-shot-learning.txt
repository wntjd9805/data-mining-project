Abstract
Transductive inference is widely used in few-shot learning, as it leverages the statis-tics of the unlabeled query set of a few-shot task, typically yielding substantially bet-ter performances than its inductive counterpart. The current few-shot benchmarks use perfectly class-balanced tasks at inference. We argue that such an artiﬁcial reg-ularity is unrealistic, as it assumes that the marginal label probability of the testing samples is known and ﬁxed to the uniform distribution. In fact, in realistic scenar-ios, the unlabeled query sets come with arbitrary and unknown label marginals. We introduce and study the effect of arbitrary class distributions within the query sets of few-shot tasks at inference, removing the class-balance artefact. Speciﬁcally, we model the marginal probabilities of the classes as Dirichlet-distributed random variables, which yields a principled and realistic sampling within the simplex. This leverages the current few-shot benchmarks, building testing tasks with arbitrary class distributions. We evaluate experimentally state-of-the-art transductive meth-ods over 3 widely used data sets, and observe, surprisingly, substantial performance drops, even below inductive methods in some cases. Furthermore, we propose a generalization of the mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations. Empirically, we show that our transductive α-divergence optimization outperforms state-of-the-art methods across several data sets, models and few-shot settings. Our code is publicly available at https://github.com/oveilleux/Realistic_Transductive_Few_Shot. 1

Introduction
Deep learning models are widely dominating the ﬁeld. However, their outstanding performances are often built upon training on large-scale labeled data sets, and the models are seriously challenged when dealing with novel classes that were not seen during training. Few-shot learning [1, 2, 3] tackles this challenge, and has recently triggered substantial interest within the community. In standard few-shot settings, a model is initially trained on large-scale data containing labeled examples from a set of base classes. Then, supervision for a new set of classes, which are different from those seen in the base training, is restricted to just one or a few labeled samples per class. Model generalization is evaluated over few-shot tasks. Each task includes a query set containing unlabeled samples for evaluation, and is supervised by a support set containing a few labeled samples per new class.
The recent few-shot classiﬁcation literature is abundant and widely dominated by convoluted meta-learning and episodic-training strategies. To simulate generalization challenges at test times, such strategies build sequences of artiﬁcially balanced few-shot tasks (or episodes) during base training, each containing both query and support samples. Widely adopted methods within this paradigm include: Prototypical networks [4], which optimizes the log-posteriors of the query points within each base-training episode; Matching networks [3], which expresses the predictions of query points
∗Equal contributions, corresponding authors: {olivier.veilleux.2, malik.boudiaf.1}@ens.etsmtl.ca 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
as linear functions of the support labels, while deploying episodic training and memory architectures;
MAML (Model-Agnostic Meta-Learning) [5], which encourages a model to be “easy” to ﬁne-tune; and the meta-learner in [6], which prescribes optimization as a model for few-shot learning. These popular methods have recently triggered a large body of few-shot learning literature, for instance,
[7, 8, 9, 10, 11, 12, 13], to list a few.
Recently, a large body of works investigated transductive inference for few-shot tasks, e.g., [11, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23], among many others, showing substantial improvements in performances over inductive inference2. Also, as discussed in [24], most meta-learning approches rely critically on transductive batch normalization (TBN) to achieve competitive performances, for instance, the methods in [5, 25, 26], among others. Adopted initially in the widely used MAML
[5], TBN performs normalization using the statistics of the query set of a given few-shot task, and yields signiﬁcant increases in performances [24]. Therefore, due to the popularity of MAML, several meta-learning techniques have used TBN. The transductive setting is appealing for few-shot learning, and the outstanding performances observed recently resonate well with a well-known fact in classical transductive inference [27, 28, 29]: On small labeled data sets, transductive inference outperforms its inductive counterpart. In few-shot learning, transductive inference has access to exactly the same training and testing data as its inductive counterpart3. The difference is that it classiﬁes all the unlabeled query samples of each single few-shot task jointly, rather than one sample at a time.
The current few-shot benchmarks use perfectly class-balanced tasks at inference: For each task used at testing, all the classes have exactly the same number of samples, i.e., the marginal probability of the classes is assumed to be known and ﬁxed to the uniform distribution across all tasks. This may not reﬂect realistic scenarios, in which testing tasks might come with arbitrary class proportions.
For instance, the unlabeled query set of a task could be highly imbalanced. In fact, using perfectly balanced query sets for benchmarking the models assumes exact knowledge of the marginal distri-butions of the true labels of the testing points, but such labels are unknown. This is, undeniably, an unrealistic assumption and an important limitation of the current few-shot classiﬁcation benchmarks and datasets. Furthermore, this suggests that the recent progress in performances might be, in part, due to class-balancing priors (or biases) that are encoded in state-of-the-art transductive models. Such priors might be implicit, e.g., through carefully designed episodic-training schemes and specialized architectures, or explicit, e.g., in the design of transductive loss functions and constraints. For instance, the best performing methods in [23, 31] use explicit label-marginal terms or constraints, which strongly enforce perfect class balance within the query set of each task. In practice, those class-balance priors and assumptions may limit the applicability of the existing few-shot benchmarks and methods. In fact, our experiments show that, over few-shot tasks with random class balance, the performances of state-of-the-art methods may decrease by margins. This motivates re-considering the existing benchmarks and re-thinking the relevance of class-balance biases in state-of-the-art methods.
Contributions We introduce and study the effect of arbitrary class distributions within the query sets of few-shot tasks at inference. Speciﬁcally, we relax the assumption of perfectly balanced query sets and model the marginal probabilities of the classes as Dirichlet-distributed random variables. We devise a principled procedure for sampling simplex vectors from the Dirichlet distribution, which is widely used in Bayesian statistics for modeling categorical events. This leverages the current few-shot benchmarks by generating testing tasks with arbitrary class distributions, thereby reﬂecting realistic scenarios. We evaluate experimentally state-of-the-art transductive few-shot methods over 3 widely used datasets, and observe that the performances decrease by important margins, albeit at various degrees, when dealing with arbitrary class distributions. In some cases, the performances drop even below the inductive baselines, which are not affected by class-distribution variations (as they do not use the query-set statistics). Furthermore, we propose a generalization of the transductive mutual-information loss, based on α-divergences, which can handle effectively class-distribution variations.
Empirically, we show that our transductive α-divergence optimization outperforms state-of-the-art few-shot methods across different data sets, models and few-shot settings. 2The best-performing state-of-the-art few-shot methods in the transductive-inference setting have achieved performances that are up to 10% higher than their inductive counterparts; see [23], for instance. 3Each single few-shot task is treated independently of the other tasks in the transductive-inference setting.
Hence, the setting does not use additional unlabeled data, unlike semi-supervised few-shot learning [30]. 2
2 Standard few-shot settings
Base training Assume that we have access to a fully labelled base dataset Dbase = {xi, yi}Nbase
, where xi ∈ Xbase are data points in an input space Xbase, yi ∈ {0, 1}|Ybase| the one-hot labels, and
Ybase the set of base classes. Base training learns a feature extractor fφ : X → Z, with φ its learnable parameters and Z a (lower-dimensional) feature space. The vast majority of the literature adopts episodic training at this stage, which consists in formatting Dbase as a series of tasks (=episodes) in order to mimic the testing stage, and train a meta-learner to produce, through a differentiable process, predictions for the query set. However, it has been repeatedly demonstrated over the last couple years that a standard supervised training followed by standard transfer learning strategies actually outperforms most meta-learning based approaches [32, 33, 34, 20, 23]. Therefore, we adopt a standard cross-entropy training in this work. i=1
Testing The model is evaluated on a set of few-shot tasks, each formed with samples from
Dtest = {xi, yi}Ntest i=1 , where yi ∈ {0, 1}|Ytest| such that Ybase ∩ Ytest = ∅. Each task is composed of a labelled support set S = {xi, yi}i∈IS and an unlabelled query set Q = {xi}i∈IQ, both containing instances only from K distinct classes randomly sampled from Ytest, with K < |Ytest|. Leveraging a feature extractor fφ pre-trained on the base data, the objective is to learn, for each few-shot task, a classiﬁer fW : Z → ∆K, with W the learnable parameters and ∆K = {y ∈ [0, 1]K / (cid:80) k yk = 1} the (K − 1)-simplex. To simplify the equations for the rest of the paper, we use the following notations for the posterior predictions of each i ∈ IS ∪ IQ and for the class marginals within Q: pik = fW (fφ(xi))k = P(Y = k|X = xi; W , φ) and (cid:98)pk = 1
|IQ| (cid:88) i∈IQ pik = P(YQ = k; W , φ), where X and Y are the random variables associated with the raw features and labels, respectively;
XQ and YQ means restriction of the random variable to set Q. The end goal is to predict the classes of the unlabeled samples in Q for each few-shot task, independently of the other tasks. A large body of works followed a transductive-prediction setting, e.g., [11, 14, 12, 15, 16, 17, 18, 19, 20, 21, 22, 23], among many others. Transductive inference performs a joint prediction for all the unlabeled query samples of each single few-shot task, thereby leveraging the query-set statistics. On the current benchmarks, tranductive inference often outperforms substantially its inductive counterpart (i.e., classifying one sample at a time for a given task). Note that our method is agnostic to the speciﬁc choice of classiﬁerfW , whose parameters are learned at inference. In the experimental evaluations 2 (cid:107)wk − zi(cid:107)2), with W := (w1, . . . , wK), of our method, similarly to [23], we used pik ∝ exp(− τ zi = fφ(xi) (cid:107)fφ(xi)(cid:107)2
, τ is a temperature parameter and base-training parameters φ are ﬁxed4.
In standard K-way few-shot settings, the support and
Perfectly balanced vs imbalanced tasks query sets of each task T are formed using the following procedure: (i) Randomly sample K classes YT ⊂ Ytest; (ii) For each class k ∈ YT , randomly sample nS k support examples, such that k = |S|/K; and (iii) For each class k ∈ YT , randomly sample nQ nS k query examples, such that nQ k = |Q|/K. Such a setting is undeniably artiﬁcial as we assume S and Q have the same perfectly balanced class distribution. Several recent works [35, 36, 37, 38] studied class imbalance exclusively on the support set S. This makes sense as, in realistic scenarios, some classes might have more labelled samples than others. However, even these works rely on the assumption that query set Q is perfectly balanced. We argue that such an assumption is not realistic, as one typically has even less control over the class distribution of Q than it has over that of S. For the labeled support S, the class distribution is at least fully known and standard strategies from imbalanced supervised learning could be applied [38]. This does not hold for Q, for which we need to make class predictions at testing time and whose class distribution is unknown. In fact, generating perfectly balanced tasks at test times for benchmarking the models assumes that one has access to the unknown class distributions of the query points, which requires access to their unknown labels. More importantly, artiﬁcial balancing of Q is implicitly or explicitly encoded in several transductive methods, which use the query set statistics to make class predictions, as will be discussed in section 4. 4φ is either ﬁxed, e.g., [23], or ﬁne-tuned during inference, e.g., [15]. There is, however, evidence in the literature that freezing φ yields better performances [23, 32, 34, 33], while reducing the inference time. 3
Figure 1: Dirichlet density function for K = 3, with different choices of parameter vector a. 3 Dirichlet-distributed class marginals for few-shot query sets
Standard few-shot settings assume that pk, the proportion of the query samples belonging to a class k within a few-shot task, is deterministic (ﬁxed) and known priori: pk = nQ k /|Q| = 1/K, for all k and all few-shot tasks. We propose to relax this unrealistic assumption, and to use the Dirichlet distribution to model the proportions (or marginal probabilities) of the classes in few-shot query sets as random variables. Dirichlet distributions are widely used in Bayesian statistics to model K-way categorical events5. The domain of the Dirichlet distribution is the set of K-dimensional discrete distributions, i.e., the set of vectors in (K − 1)-simplex ∆K = {p ∈ [0, 1]K | (cid:80) k pk = 1}. Let Pk denotes a random variable associated with class probability pk, and P the random simplex vector given by P = (P1, . . . , PK). We assume that P follows a Dirichlet distribution with parameter vector a = (a1, . . . , aK) ∈ RK: P ∼ Dir(a). The Dirichlet distribution has the following density function: fDir(p; a) = 1 for p = (p1, . . . , pK) ∈ ∆K, with B denoting the multivariate Beta function, which could be expressed with the Gamma function6: B(a) = k=1 pak−1 (cid:81)K
B(a) k
. (cid:81)K
Γ((cid:80)K k=1 Γ(ak) k=1 αk)
Figure 1 illustrates the Dirichlet density for K = 3, with a 2-simplex support represented with an equilateral triangle, whose vertices are probability vectors (1, 0, 0), (0, 1, 0) and (0, 0, 1). We show the density for a = a1K, with 1K the K-dimensional vector whose all components are equal to 1 and concentration parameter a equal to 0.5, 2, 5 and 50. Note that the limiting case a → +∞ corresponds to the standard settings with perfectly balanced tasks, where only uniform distribution, i.e., the point in the middle of the simplex, could occur as marginal distribution of the classes.
The following result, well-known in the literature of random variate generation [39], suggests that one could generate samples from the multivariate Dirichlet distribution via simple and standard univariate
Gamma generators.
Theorem 3.1. ([39, p. 594]) Let N1, . . . , NK be K independent Gamma-distributed random vari-ables with parameters ak: Nk ∼ Gamma(ak), i.e., the probability density of Nk is univariate Gamma7, with shape parameter ak. Let Pk = Nk
, k = 1, . . . , K. Then, P = (P1, . . . , PK) is Dirichlet distributed: P ∼ Dir(a), with a = (a1, . . . , aK). k=1 Nk (cid:80)K
A proof based on the Jacobian of random-variable transformations Pk =
, k = 1, . . . , K, could be found in [39], p. 594. This result prescribes the following simple procedure for sampling random simplex vectors (p1, . . . , pK) from the multivariate Dirichlet distribution with parameters a = (a1, . . . , aK): First, we draw K independent random samples (n1, . . . , nK) from Gamma distributions, with each nk drawn from univariate density fGamma(n; ak); To do so, one could use standard random generators for the univariate Gamma density; see Chapter 9 in [39]. Then, we set pk = nk k , the number of samples of class k within query (cid:80)K k=1 nk set Q, as follows: nQ k is the closest integer to pk|Q| such that (cid:80)
. This enables to generate randomly nQ k = |Q|. k nQ (cid:80)K
Nk k=1 Nk 5Note that the Dirichlet distribution is the conjugate prior of the categorical and multinomial distributions. 6The Gamma function is given by: Γ(a) = (cid:82) ∞ ta−1 exp(−t)dt for a > 0. Note that Γ(a) = (a − 1)! 0 when a is a strictly positive integer. 7Univariate Gamma density is given by: fGamma(n; ak) = nak −1 exp(−n)
Γ(ak)
, n ∈ R. 4
4 On the class-balance bias of the best-performing few-shot methods
As brieﬂy evoked in section 2, the strict balancing of the classes in both S and Q represents a strong inductive bias, which few-shot methods can either meta-learn during training or leverage at inference. In this section, we explicitly show how such a class-balance prior is encoded in the two best-performing transductive methods in the literature [23, 31], one based on mutual-information maximization [23] and the other on optimal transport [31].
Class-balance bias of optimal transport Recently, the transductive method in [31], referred to as
PT-MAP, achieved the best performances reported in the literature on several popular benchmarks, to the best of our knowledge. However, the method explicitly embeds a class-balance prior. Formally, the objective is to ﬁnd, for each few-shot task, an optimal mapping matrix M ∈ R|Q|×K
, which could be viewed as a joint probability distribution over XQ × YQ. At inference, a hard constraint
M ∈ {M : M 1K = r, 1|Q|M = c} for some r and c is enforced through the use of the
Sinkhorn-Knopp algorithm. In other words, the columns and rows of M are constrained to sum to pre-deﬁned vectors r ∈ R|Q| and c ∈ RK. Setting c = 1 1K as done in [31] ensures that M
K deﬁnes a valid joint distribution, but also crucially encodes the strong prior that all the classes within the query sets are equally likely. Such a hard constraint is detrimental to the performance in more realistic scenarios where the class distributions of the query sets could be arbitrary, and not necessarily uniform. Unsurprisingly, PT-MAP undergoes a substantial performance drop in the realistic scenario with Dirichlet-distributed class proportions, with a consistent decrease in accuracy between 18 and 20 % on all benchmarks, in the 5-ways case.
+
Class-balance bias of transductive mutual-information maximization Let us now have a closer look at the mutual-information maximization in [23]. Following the notations introduced in section 2, the transductive loss minimized in [23] for a given few-shot task reads:
LTIM = CE − I(XQ; YQ) = CE − (cid:124) 1
|IQ| (cid:88)
K (cid:88) pik log(pik)
+λ
K (cid:88) (cid:98)pk log (cid:98)pk
, (1) i∈Q k=1 (cid:123)(cid:122)
H(YQ|XQ) (cid:125) k=1 (cid:124) (cid:123)(cid:122)
−H(YQ) (cid:125) i∈S (cid:80) (cid:80)K where I(XQ; YQ) = −H(YQ|XQ) + λH(YQ) is a weighted mutual information between the query samples and their unknown labels (the mutual information corresponds to λ = 1), and
CE := − 1 k=1 yik log(pik) is a supervised cross-entropy term deﬁned over the support
|IS | samples. Let us now focus our attention on the label-marginal entropy term, H(YQ). As mentioned in
[23], this term is of signiﬁcant importance as it prevents trivial, single-class solutions stemming from minimizing only conditional entropy H(YQ|XQ). However, we argue that this term also encourages class-balanced solutions. In fact, we can write it as an explicit KL divergence, which penalizes deviation of the label marginals within a query set from the uniform distribution:
H(YQ) = −
K (cid:88) k=1 (cid:98)pk log ((cid:98)pk) = log(K) − DKL( (cid:98)p(cid:107)uK). (2)
Therefore, minimizing marginal entropy H(YQ) is equivalent to minimizing the KL divergence 1K. between the predicted marginal distribution (cid:98)p = ((cid:98)p1, . . . , (cid:98)pK) and uniform distribution uK = 1
This KL penalty could harm the performances whenever the class distribution of the few-shot task is no longer uniform. In line with this analysis, and unsurprisingly, we observe in section 6 that the original model in [23] also undergoes a dramatic performance drop, up to 20%. While naively removing this marginal-entropy term leads to even worse performances, we observe that simply down-weighting it, i.e., decreasing λ in Eq. (1), can drastically alleviate the problem, in contrast to the case of optimal transport where the class-balance constraint is enforced in a hard manner.
K 5 Generalizing mutual information with α-divergences
In this section, we propose a non-trivial, but simple generalization of the mutual-information loss in (1), based on α-divergences, which can tolerate more effectively class-distribution variations. We identiﬁed in section 4 a class-balance bias encoded in the marginal Shannon entropy term. Ideally, 5
Figure 2: (Left) α-entropy as a function of p = σ(l). (Right) Gradient of α-entropy w.r.t to the logit l ∈ R as a function of p = σ(l). Best viewed in color. we would like to extend this Shannon-entropy term in a way that allows for more ﬂexibility: Our purpose is to control how far the predicted label-marginal distribution, (cid:98)p, could depart from the uniform distribution without being heavily penalized. 5.1