Abstract
Implicit neural representations are a promising new avenue of representing gen-eral signals by learning a continuous function that, parameterized as a neural net-work, maps the domain of a signal to its codomain; the mapping from spatial coordinates of an image to its pixel values, for example. Being capable of convey-ing ﬁne details in a high dimensional signal, unboundedly of its domain, implicit neural representations ensure many advantages over conventional discrete repre-sentations. However, the current approach is difﬁcult to scale for a large number of signals or a data set, since learning a neural representation—which is parameter heavy by itself—for each signal individually requires a lot of memory and com-putations. To address this issue, we propose to leverage a meta-learning approach in combination with network compression under a sparsity constraint, such that it renders a well-initialized sparse parameterization that evolves quickly to represent a set of unseen signals in the subsequent training. We empirically demonstrate that meta-learned sparse neural representations achieve a much smaller loss than dense meta-learned models with the same number of parameters, when trained to ﬁt each signal using the same number of optimization steps. 1

Introduction
An explosively growing line of research on implicit neural representations (INRs)—also known as coordinate-based neural representations—studies a new paradigm of signal representation: Instead of storing the signal values corresponding to the coordinate grid (e.g., pixels or voxels), we train a neural network with continuous activation functions (e.g., ReLU, sinusoids) to approximate the coordinate-to-value mapping [31, 26, 3]. As the activation functions are continuous, the trained
INRs give a continuous representation of the signal. The continuity of INRs brings several practical beneﬁts over the conventional discrete representations, such as providing an out-of-the-box method for superresolution or inpainting tasks [2, 37, 24, 36], or having its number of parameters not strictly scaling with the spatial dimension and/or spatial resolution of the signal [31, 26, 3, 8, 1, 34, 32].
Furthermore, as INRs take a form of trainable models, they are readily amenable to the idea of
“learning a prior” from a set of signals, which can be utilized for various purposes including image generation [2, 37, 36] or view synthesis [40, 36, 35, 27].
Despite many advantages, this network-as-a-representation approach is difﬁcult to scale to handle a large set of signals, as having a parameter-heavy neural network trained for each signal requires a lot
⇤Equal contributions
Code: https://github.com/jaeho-lee/MetaSparseINR 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) INR (b) The standard pruning pipeline for handling a single signal. (c) The proposed Meta-SparseINR procedure for handling multiple signals.
Figure 1: Illustrations of (a) an implicit neural representation, (b) the standard pruning algorithm [9] that prunes and retrains the model for each signal considered, and (c) the proposed Meta-SparseINR procedure to ﬁnd a sparse initial INR, which can be trained further to ﬁt each signal. of memory and computations. Existing approaches to address this problem can be roughly divided into three categories. The ﬁrst category of works utilizes a neural network structure shared across the signals, which takes a latent code vector2 as an input and modiﬁes the INR output accordingly (e.g., [36, 2, 31]). While these methods only need to store a latent vector for each signal (along with shared model parameters), they have a limited ability to adapt to signals falling outside the learned latent space; this may lead to a degraded representation capability on new signals. The second line of works adopt meta-learning approach to train an initial INR from which each signal can be trained to ﬁt within a small number of optimization steps [35, 40]. Unfortunately, these methods do not reduce the memory required to store the neural representations for each signal. The work belonging to the third category [4] reduces the memory required to store the INR model parameters by uniformly quantizing the weights of each INR. The method, however, does not reduce the number of optimization steps needed to train for each signal, or utilize any information from other signals to improve the quantization. In this sense, all three approaches have their own limitations in providing both memory- and compute-efﬁcient method to train INRs for both seen and unseen signals.
To address these limitations, we establish a new framework based on the neural network pruning, i.e., removing parameters from the neural representation by ﬁxing them to zero (e.g., [29]). Under this framework, the objective is to train a set of sparse INRs that represent a diverse set of signals (either previously seen or unseen) in a compute-efﬁcient manner. To achieve this goal, we make the following contributions:
• We formulate this problem as ﬁnding a sparse initial INR, which can be trained to ﬁt each signal within a small number of optimization steps. By considering such formulation, we eliminate the need to prune the INRs separately for every single signal, each of which typically requires a computationally heavy prune-retrain cycle (Fig. 1b). We give a detailed formulation in Section 3.
• To solve this newly formulated problem, we propose a pruning algorithm called Meta-SparseINR, which is the ﬁrst pruning algorithm designed for the INR setups (up to our knowledge). Meta-SparseINR alternately applies (1) a meta-learning step to learn the INR that can be efﬁciently trained to ﬁt each signal, and (2) a pruning step to removes a fraction of surviving parameters from the INR using the meta-trained weight magnitudes as a saliency score (Fig. 1c). See Section 4 for a complete description of the algorithm.
In Section 5, we validate the performance of the proposed Meta-SparseINR scheme on three nat-ural/synthetic image datasets (CelebA [21], Imagenette [14], SDF [40]) using a widely used INR 2a low dimensional vector generated by forwarding each signal through some encoder, 2
architecture (SIREN [36])3. In particular, we show that, when trained to ﬁt seen/unseen signals with a small number of optimization steps, Meta-SparseINR consistently achieves a better PSNR than the baseline methods using similar number of parameters, which include the meta-trained dense INRs with narrower width [40] and random pruning. Furthermore, in Section 5.3, we provide results on an exploratory experiment which sheds a light on the potential limitations of pruning-at-initialization schemes for ﬁtting multiple signals under the INR context. 2