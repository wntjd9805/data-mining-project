Abstract
√
√
We study the problem of sparse tensor principal component analysis: given a tensor
YYY = WWW + λx⊗p with WWW ∈ ⊗pRn having i.i.d. Gaussian entries, the goal is to recover the k-sparse unit vector x ∈ Rn. The model captures both sparse PCA (in its Wigner form) and tensor PCA. For the highly sparse regime of k ≤ n, we present a family of algorithms that smoothly interpolates between a simple polynomial-time algorithm and the exponential-time exhaustive search algorithm.
For any 1 ≤ t ≤ k, our algorithms recovers the sparse vector for signal-to-t · (k/t)p/2) in time (cid:101)O(np+t), capturing the state-of-the-art noise ratio λ ≥ (cid:101)O( guarantees for the matrix settings (in both the polynomial-time and sub-exponential time regimes). Our results naturally extend to the case of r distinct k-sparse signals with disjoint supports, with guarantees that are independent of the number of spikes. Even in the restricted case of sparse PCA, known algorithms only recover the sparse vectors for λ ≥ (cid:101)O(k · r) while our algorithms require λ ≥ (cid:101)O(k). Finally, by analyzing the low-degree likelihood ratio, we complement these algorithmic results with rigorous evidence illustrating the trade-offs between signal-to-noise ratio and running time. This lower bound captures the known lower bounds for both sparse PCA and tensor PCA. In this general model, we observe a more intricate three-way trade-off between the number of samples n, the sparsity k, and the tensor power p. 1

Introduction
Sparse tensor principal component analysis is a statistical primitive generalizing both sparse PCA2 and tensor PCA3. We are given multi-linear measurements in the form of a tensor
YYY = WWW + λx⊗p ∈ ⊗pRn (SSTM) for a Gaussian noise tensor WWW ∈ ⊗pRn containing i.i.d. N (0, 1) entries4 and signal-to-noise ratio
λ > 0. Our goal is to estimate the “structured” unit vector x ∈ Rn. The structure we enforce on x is sparsity: |supp(x)| ≤ k. The model can be extended to include multiple spikes in a natural way: YYY = WWW + (cid:80)r q=1 λqX(q) for
X(q) = x(q,1) ⊗ · · · ⊗ x(q,p) ∈ ⊗pRn. In this introduction, we focus on the simplest single spike setting of SSTM. (q), and even general order-p tensors: YYY = WWW + (cid:80)r q=1 λqx⊗p
It is easy to see that sparse PCA corresponds to the setting with tensor order p = 2. On the other hand, tensor PCA is captured by effectively removing the sparsity constraint: |supp(x)| ≤ n. In
∗Part of the work was done while the author was in ETH Zürich. 2Often in the literature, the terms sparse PCA and spiked covariance model refer to the sparse spiked Wishart model. However, here we consider the sparse spiked Wigner matrix model. 3Tensor PCA is also known as the spiked Wigner tensor model, or simply the spiked tensor model. 4Throughout the paper, we will write random variables in boldface. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
recent years, two parallel lines of work focused respectively on sparse PCA [JL09, AW08, BR13a,
DM16, HKP+17, DKWB19, HSV20, dKNS20] and tensor PCA [MR14, HSS15, MSS16, HKP+17,
KWB19, AMMN19], however no result captures both settings. The appeal of the sparse spiked tensor model (henceforth SSTM) is that it allows one to study the computational and statistical aspects of these other fundamental statistical primitives in a uniﬁed framework, understanding the computational phenomena at play from a more general perspective.
In this work, we investigate SSTM from both algorithmic and computational hardness perspectives.
Our algorithm improves over known tensor algorithms whenever the signal vector is highly sparse.
We also present a lower bound against low-degree polynomials which extends the known lower bounds for both sparse PCA and tensor PCA, leading to a more intricate understanding of how all three parameters (n, k and p) interact. 1.1