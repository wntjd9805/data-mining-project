Abstract
Ofﬂine (or batch) reinforcement learning (RL) algorithms seek to learn an optimal policy from a ﬁxed dataset without active data collection. Based on the composition of the ofﬂine dataset, two main methods are used: imitation learning which is suitable for expert datasets, and vanilla ofﬂine RL which often requires uniform coverage datasets. From a practical standpoint, datasets often deviate from these two extremes and the exact data composition is usually unknown. To bridge this gap, we present a new ofﬂine RL framework that smoothly interpolates between the two extremes of data composition, hence unifying imitation learning and vanilla ofﬂine RL. The new framework is centered around a weak version of the concentrability coefﬁcient that measures the deviation of the behavior policy from the expert policy alone. Under this new framework, we ask: can one develop an algorithm that achieves a minimax optimal rate adaptive to unknown data composition? To address this question, we consider a lower conﬁdence bound (LCB) algorithm developed based on pessimism in the face of uncertainty in ofﬂine
RL. We study ﬁnite-sample properties of LCB as well as information-theoretic limits in multi-armed bandits, contextual bandits, and Markov decision processes (MDPs). Our analysis reveals surprising facts about optimality rates. In particular, in both contextual bandits and RL, LCB achieves a faster rate of 1/N for nearly-expert datasets compared to the usual rate of 1/
N in ofﬂine RL, where N is the batch dataset sample size. In contextual bandits, we prove that LCB is adaptively optimal for the entire data composition range, achieving a smooth transition from imitation learning to ofﬂine RL. We further show that LCB is almost adaptively optimal in tabular MDPs.
√ 1

Introduction
Reinforcement learning (RL) algorithms have recently achieved tremendous empirical success including beating Go champions [45, 46] and surpassing professionals in Atari games [30, 31], to name a few. Most success stories, however, are in the realm of online RL in which active data collection is necessary. This online paradigm falls short of leveraging previously-collected datasets and dealing with scenarios where online exploration is not possible [10]. To tackle these issues, ofﬂine 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dataset composition expert data imitation learning uniform coverage data vanilla ofﬂine RL
Figure 1: Dataset composition range for ofﬂine RL problems. (or batch) reinforcement learning [24, 26] arises in which the agent aims at achieving competence by exploiting a batch dataset without access to online exploration. This paradigm is useful in a diverse array of application domains such as healthcare [51, 15, 35], autonomous driving [57, 4, 36], and recommendation systems [47, 12, 49].
The key component of ofﬂine RL is a pre-collected dataset from an unknown stochastic environment.
Broadly speaking, there exist two types of data composition for which ofﬂine RL algorithms have shown promising empirical and theoretical success; see Figure 1 for an illustration.
Expert data. One end of the spectrum includes datasets collected by following an expert policy. For such datasets, imitation learning algorithms (e.g., behavior cloning [40]) are shown to be effective in achieving a small sub-optimality w.r.t. the expert policy. Recently, Rajaraman et al. [39] showed that the behavior cloning algorithm achieves the minimal sub-optimality of 1/N in episodic Markov decision processes (MDPs), where N is the sample size in the expert dataset.
Uniform coverage data. On the other end of the spectrum lies the datasets with uniform coverage, which aim to cover all states and actions, even the states never visited or actions never taken by satisfactory policies. Most vanilla ofﬂine RL algorithms are only suited in this region and are shown to diverge—both empirically [11, 22] and theoretically [2, 7]—for narrower datasets [10, 20], such as those collected via human demonstrations or hand-crafted policies. In this regime, a widely-adopted requirement is the bounded uniform concentrability coefﬁcient which assumes that the ratio of the state-action occupancy density of any policy and the data distribution is bounded uniformly over all states and actions [32, 9, 6, 52]. Another common assumption is uniformly lower bounded data distribution on all states and actions [43, 1], which ensures all states and actions are visited with sufﬁcient probabilities. Algorithms developed for this regime are demonstrated to achieve a 1/
N sub-optimality competing with the optimal policy [53, 16, 50].
√ 1.1 Motivating questions
Both of these two ends impose strong assumptions on the dataset: at one extreme, we hope for a solely expert-driven dataset; at the other extreme, we require the dataset to cover every, even sub-optimal, actions. In practice, there are numerous scenarios where the dataset deviates from these two extremes, which has motivated new ofﬂine RL benchmark datasets with different data compositions [10, 20].
With this need in mind, the ﬁrst and foremost question is regarding ofﬂine RL formulations:
Question 1 (Formulation) Can we propose an ofﬂine RL framework that accommodates the entire data composition range?
We answer this question afﬁrmatively by proposing a new formulation for ofﬂine RL that smoothly interpolates between two regimes: expert data and data with uniform coverage. More speciﬁcally, we characterize the data composition in terms of the ratio between the state-action occupancy density of an optimal policy d(cid:63)(s, a)1 and that of the data distribution µ(s, a), i.e., we deﬁne C (cid:63) to be the smallest constant that satisﬁes d(cid:63)(s, a)/µ(s, a) ≤ C (cid:63) for all s ∈ S and a ∈ A; see Deﬁnition 1 for a precise characterization.
In words, C (cid:63) can be viewed as a measure of the deviation between the data distribution and the distribution induced by the optimal policy. C (cid:63) = 1 describes the expert datasets as by deﬁnition, the behavior policy is identical to the optimal policy. In contrast, when C (cid:63) > 1, the dataset is no longer purely expert-driven: it could contain “spurious” samples—states and actions that are not visited by the optimal policy. As another example, when the data distribution is lower bounded by µmin over all states and actions, C (cid:63) is upper bounded by µ−1 min. 1Our developments can accommodate arbitrary competing policies, however, we restrict ourselves to the optimal policy for ease of presentation. 2
Assuming a ﬁnite C (cid:63) is the weakest concentrability requirement [42, 13, 52] that is currently enjoyed only by some online algorithms such as CPI [18]. C (cid:63) imposes a much weaker assumption in contrast to other concentrability requirements which involve taking a maximum over all policies; see [42] for a hierarchy of different concentrability deﬁnitions. We would like to immediately point out that existing works on ofﬂine RL either do not specify the dependency of sub-optimality on data coverage
[17, 55], or do not have a batch data coverage assumption that accommodates the entire data spectrum
[54, 19]. For instance, Yin et al. [54] requires a uniformly lower bounded data distribution that traces an optimal policy, which implies that optimal actions should be included in states not visited by the optimal policy. Furthermore, this characterization of data coverage does not recover imitation learning: even if the behavior policy is exactly equal to the optimal policy, data distribution lower bound can be arbitrarily small. Further discussion of related work is presented in Appendix A.
With this formulation in mind, a natural next step is designing ofﬂine RL algorithms that can handle various data compositions, i.e., for all C (cid:63) ≥ 1. Recently, efforts have been made toward reducing the ofﬂine dataset requirements based on a shared intuition: the agent should act conservatively and avoid states and actions less covered in the ofﬂine dataset. Based on this intuition, a variety of model-based [55, 19, 56] and model-free [22, 33, 11, 34, 25, 37, 44, 14, 28, 23, 3] ofﬂine RL algorithms are proposed that achieve promising empirical results. However, it is observed empirically that existing model-free methods perform better when the dataset is nearly expert-driven whereas existing model-based methods perform better when the dataset is randomly-collected [55, 5, 56].
It remains unclear whether a single algorithm exists that performs well regardless of data composition— an important challenge from a practical perspective [21, 10, 20]. More importantly, the knowledge of the dataset composition may not be available a priori to assist in selecting the right algorithm.
In practice, imitation learning often succeeds with very few samples in contrast to ofﬂine RL [41].
Unifying ofﬂine RL and imitation learning via a single algorithm is thus beneﬁcial, as it can result in tremendous sample savings, in case the dataset has good coverage on an expert policy. This motivates the second question on the algorithm design:
Question 2 (Adaptive algorithm design) Can we design algorithms that can achieve minimal sub-optimality when facing different dataset compositions (i.e., different C (cid:63))? Furthermore, can this be achieved in an adaptive manner, i.e., without knowing C (cid:63) beforehand?
To answer the second question, we analyze a pessimistic variant of a value-based method in which we ﬁrst form a lower conﬁdence bound (LCB) for the value function of a policy using the batch data and then seek to ﬁnd a policy that maximizes the LCB. The idea of pessimism has appeared in the literature of risk minimization [48, 8]. A similar algorithm design has recently appeared in [17]. It turns out that such a simple algorithm—fully agnostic to the data composition—achieves almost optimal performance in multi-armed bandits and MDPs, and optimally solves the ofﬂine learning problem in contextual bandits.
Results summary. Figure 2 summarizes our theoretical ﬁndings. For multi-armed bandits, we prove that LCB achieves a (cid:112)C (cid:63)/N sub-optimality for any C (cid:63) ≥ 1. Yet, we prove lower bounds showing that LCB cannot be adaptively optimal for any C (cid:63) ≥ 1 if the knowledge of C (cid:63) is not available. For contextual bandits with at least two contexts, we prove that LCB enjoys a rate of (cid:112)(C (cid:63) − 1)/N + 1/N , which translates to a fast rate of 1/N when C (cid:63) ≈ 1 akin to the performance
N as C (cid:63) increases. This rate of behavioral cloning and smoothly transitions from 1/N to 1/ matches the information theoretic limit, showing adaptive optimality of LCB in contextual bandits.
Establishing the C (cid:63) − 1 dependency requires a novel analysis based on a careful policy sub-optimality decomposition and directly analyzing the probability of taking wrong actions. For MDPs, we similarly show that LCB achieves a fast rate of 1/N for C (cid:63) ≈ 1 and a rate of (cid:112)C (cid:63)/N for larger values of C (cid:63).
We conjecture that LCB upper bound also has the form (cid:112)(C (cid:63) − 1)/N . We verify this conjecture in a simple example and show that establishing the C (cid:63) − 1 dependency in MDPs requires a delicate analysis that accounts for the value gap between optimal and sub-optimal actions.
√ 2