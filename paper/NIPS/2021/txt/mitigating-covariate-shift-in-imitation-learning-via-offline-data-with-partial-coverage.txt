Abstract
This paper studies ofﬂine Imitation Learning (IL) where an agent learns to imitate an expert demonstrator without additional online environment interactions. In-stead, the learner is presented with a static ofﬂine dataset of state-action-next state transition triples from a potentially less proﬁcient behavior policy. We introduce
Model-based IL from Ofﬂine data (MILO): an algorithmic framework that utilizes the static dataset to solve the ofﬂine IL problem efﬁciently both in theory and in practice. In theory, even if the behavior policy is highly sub-optimal compared to the expert, we show that as long as the data from the behavior policy provides sufﬁcient coverage on the expert state-action traces (and with no necessity for a global coverage over the entire state-action space), MILO can provably combat the covariate shift issue in IL. Complementing our theory results, we also demon-strate that a practical implementation of our approach mitigates covariate shift on benchmark MuJoCo continuous control tasks. We demonstrate that with behavior policies whose performances are less than half of that of the expert, MILO still successfully imitates with an extremely low number of expert state-action pairs while traditional ofﬂine IL methods such as behavior cloning (BC) fail completely.
Source code is provided at https://github.com/jdchang1/milo. 1

Introduction
Covariate shift is a core issue in Imitation Learning (IL). Traditional IL methods like behavior cloning (BC) [49], while simple, suffer from covariate shift, learning a policy that can make arbitrary mistakes in parts of the state space not covered by the expert dataset. This leads to compounding errors in the agent’s performance [57], hurting the generalization capabilities in practice.
Prior works have presented several means to combat this phenomenon in IL. One line of thought utilizes an interactive expert, i.e. an expert that can be queried at an arbitrary state encountered during the training procedure. Interactive IL algorithms such as DAgger [59], LOLS [15], DART [40], and
AggreVaTe(D) [58; 66] utilize a reduction to no-regret online learning and demonstrate that under certain conditions, they can successfully learn a policy that imitates the expert. These interactive IL algorithms, however, cannot provably avoid covariate shift if the expert is not recoverable. That is,
∗Equal contribution
†Work done outside Amazon 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
Figure 1: (Left) Frames at timesteps 200, 400, 600, 800, and 1000 for Humanoid-v2 from policies trained with BC on 100 state-action pairs from the expert (blue), BC on 1M ofﬂine samples plus 100 expert samples (yellow), and our algorithm MILO (red). The expert has a performance of 3248 and the behavior policy used to collect the ofﬂine dataset has performance of 1505 ± 473 (≈ 46% of the expert’s). (Right) Expert performance normalized scores averaged across 5 seeds.
Aπe (s, a) = Θ(H) where πe is the expert, Aπ is the usual (dis)advantage function,3 and H is the planning horizon [52; 4, Chapter 15]. A second line of work that avoids covariate shift utilizes either a known transition dynamics model [2; 86] or uses real world interactions [27; 10; 65; 38; 56; 33].
Prior works have shown that with known transition dynamics or real world interactions, agents can provably avoid covariate shift in both tabular and general MDPs [4; 52] even without a recoverable expert. While these results offer strong theoretical guarantees and empirical performance, online interactions are often costly and prohibitive for real world applications where active trial-and-error exploration in the environment could be unsafe or impossible. A third perspective towards addressing this issue is to assume that the expert visits the entire state space [62], where the expert effectively informs the learner what actions to take in every state. Unfortunately, such a full coverage expert distribution might be rare and holds only for special MDPs and expert policies (for e.g. an expert that induces ergodicity in the MDP).
In this work, we consider a new perspective towards handling the covariate shift issue in IL. In particular, we investigate a pure ofﬂine learning setting where the learner has access to neither the expert nor the environment for additional interactions. The learner, instead, has access to a small pre-collected dataset of state-action pairs sampled from the expert and a large batch ofﬂine dataset of state-action-next state transition triples sampled from a behavior policy that could be highly sub-optimal (see Figure 1 where BC on the ofﬂine data results in a low-quality policy). Unlike prior works that require online interactions, our proposed method, MILO performs high ﬁdelity imitation in an ofﬂine, data-driven manner. Moreover, different from interactive IL, we do not require the expert to be present during learning, signiﬁcantly relieving the expert’s burden. Finally, in contrast to the prior work [62] that assumes the expert distribution covers the entire state-action space (i.e., maxπ maxs,a dπ(s, a)/dπe (s, a) < ∞ where dπ denotes the state-action distribution of policy π), we require the ofﬂine dataset to provide partial coverage, i.e., it only needs to cover the expert’s state-actions (i.e., maxs,a dπe (s, a)/ρ(s, a) < ∞ where ρ is the ofﬂine distribution of some behavior policy).4
In summary, we list our main contributions below: 1. We propose Model based Imitation Learning from Ofﬂine data, MILO: a model-based framework that leverages ofﬂine batch data with only partial coverage (see Section 4.1 for deﬁnition) to overcome covariate shift in IL. 3in this work, we use cost instead of reward, thus we call Aπ the disadvantage function. 4In our analysis, we reﬁne the density ratio dπe (s, a)/ρ(s, a) via the concept of relative conditional number which allows us to extend it to large MDPs where the ratio is inﬁnite but the relative condition number is ﬁnite. 2
2. Our analysis is modular and covers common models such as discrete MDPs and linear models.
Notably, our new result on non-parametric models (e.g. Gaussian Processes) with relative condition number is new even considering all existing results in ofﬂine RL (see Remark 4,7,11). 3. The practical instantiation of our general framework leverages neural network model ensembles, and demonstrates its efﬁcacy on benchmark MuJoCo continuous control problems. Speciﬁ-cally, even under low-quality behavior policies, our approach can successfully imitate using an extremely small number of expert samples while algorithms like BC completely fail (Figure 1). 1.1