Abstract
This paper presents two Bayesian optimization (BO) algorithms with theoretical performance guarantee to maximize the conditional value-at-risk (CVaR) of a black-box function: CV-UCB and CV-TS which are based on the well-established principle of optimism in the face of uncertainty and Thompson sampling, respec-tively. To achieve this, we develop an upper conﬁdence bound of CVaR and prove the no-regret guarantee of CV-UCB by utilizing an interesting connection between
CVaR and value-at-risk (VaR). For CV-TS, though it is straightforwardly performed with Thompson sampling, bounding its Bayesian regret is non-trivial because it re-quires a tail expectation bound for the distribution of CVaR of a black-box function, which has not been shown in the literature. The performances of both CV-UCB and CV-TS are empirically evaluated in optimizing CVaR of synthetic benchmark functions and simulated real-world optimization problems. 1

Introduction
A wide range of applications from Auto-ML [15] to chemistry [6] and drug design [3] require optimizing a black-box objective function (i.e., its closed-form expression, gradient, and convexity are unknown) through observing noisy function evaluations. To resolve this problem, an efﬁcient class of algorithms, called Bayesian optimization (BO) [2, 20], has seen rapid development lately.
One of the recent developments of BO considers searching for the optimization variable x∗ that maximizes the objective function ρ[f (x, W)] [4]. Different from the classical BO, there exists an environmental random variable W that cannot be controlled, which happens frequently in real-world problems. For example, to maximize the crop yield (i.e., f (x, W)) by controlling the amount of fertilizer (i.e., x), there are various uncontrollable weather conditions such as the temperature, lighting, and rainfall (i.e., W) [13]. Thus, even though an amount of fertilizer x increases the crop yield most of the time, there exists a risk/chance that under certain weather conditions (realizations of
W), the same amount of fertilizer leads to a low crop yield (an undesirable realization of the random variable f (x, W)). This issue renders the optimization of f (x, W) futile, so the work of [4] proposes to optimize a risk measure, denoted as ρ, of f (x, W) such as value-at-risk (VaR) and conditional
VaR (CVaR). However, its approach suffers from two main shortcomings: the computational cost of a nested optimization procedure and the lack of theoretical performance guarantee. Although these drawbacks are addressed in the work of [13], its proposed algorithm only works for optimizing VaR of a black-box function. Hence, an efﬁcient algorithm with theoretical performance guarantee for optimizing CVaR of a black-box function remains an open question.
Nonetheless, optimizing CVaR of a black-box function is desirable, especially when the effect of optimizing CVaR cannot be obtained by optimizing VaR. A prominent property of CVaR is its sensitivity to values at the extreme tails of the random variable f (x, W), which VaR does not exhibit [19]. Let us consider two portfolio allocation schemes: one scheme has an unacceptable 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
worst-case value of the return and the other does not. It is possible that the two schemes have the same
VaR of the return because VaR is insensitive to the extreme tails of the return (e.g., the worst-case value of the return). On the other hand, CVaR is able to distinguish the risks of the two schemes. This advantage of CVaR is further clariﬁed in Remark 1.
In this paper, we adopt the settings of [4, 13]: the distribution of W is given and we can control/select the realization w of W in the optimization procedure. In practice, the distribution of W can be estimated from data (e.g., weather historical data in the crop yield optimization example) and we can perform BO in a laboratory or using computer simulation where W is controlled [4, 13]. After the optimization, the optimal x∗ can be used in the real-world environment with the uncontrollable W.
Our main contribution is to propose two computationally efﬁcient algorithms (Sec. 3) with theoretical performance guarantee (Sec. 4) for optimizing CVaR of a black-box function.
First, we propose CVaR optimization with upper conﬁdence bound (CV-UCB) that is based on the well-established principle of optimism in the face of uncertainty. By exploiting the connection between CVaR and VaR, we are able to develop a conﬁdence bound of CVaR (Sec. 3.1.1) and prove the no-regret guarantee of CV-UCB (Sec. 4.1).
Second, we propose CVaR optimization with Thompson sampling (CV-TS) which can be naturally extended to handle batch queries (i.e., gathering observations at a batch of inputs in each BO iteration).
The capability of gathering observations in a batch simultaneously is often available and preferable in laboratory settings and computer simulations. Though CV-TS can be simply performed with the popular Thompson sampling (or posterior sampling) [18] (Sec. 3.1.2), bounding its Bayesian regret is challenging as the distribution of CVaR in our problem has not been investigated before and its support is unbounded. Fortunately, we are able to relate the problem of bounding the tail expectation of CVaR to that of the function evaluation f (x, w) (Sec. 4.2). The latter follows a
Gaussian distribution with known tail expectation, unlike that of CVaR.
We empirically evaluate the performance of both CV-UCB and CV-TS in optimizing CVaR of synthetic benchmark functions, an optimization problem of the residuary resistance per unit weight of displacement of a yacht, a portfolio optimization problem, and a simulated robot task (Sec. 5).