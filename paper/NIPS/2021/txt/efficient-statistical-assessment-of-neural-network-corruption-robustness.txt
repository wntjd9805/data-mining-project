Abstract
We quantify the robustness of a trained network to input uncertainties with a stochastic simulation inspired by the ﬁeld of Statistical Reliability Engineering.
The robustness assessment is cast as a statistical hypothesis test: the network is deemed as locally robust if the estimated probability of failure is lower than a critical level. The procedure is based on an Importance Splitting simulation generating samples of rare events. We derive theoretical guarantees that are non-asymptotic w.r.t. sample size. Experiments tackling large scale networks outline the efﬁciency of our method making a low number of calls to the network function. 1

Introduction
Despite state-of-the-art performances on many Computer Vision and NLP tasks, Deep Neural Net-works (DNNs) have been shown to be sensitive to both adversarial and random perturbations [Gilmer et al., 2019, Franceschi et al., 2018]. Concerns about their safety and reliability have come forth as their applications move to critical ﬁelds, such as the defense sector or self-driving vehicles.
Certiﬁcation A posteriori certiﬁcation aims at verifying the correct behavior of a trained network f : Rn → Rm. This expected property is usually deﬁned locally (a.k.a. instance-wise property): the network performs correctly in the neighborhood V(xo) ⊂ Rn of a particular input xo ∈ Rn. Let us denote ι(·|xo) : Rn → {0, 1} the function indicating a violation of the expected property. The network is locally correct if ι(x|xo) = 0 for any x ∈ V(xo).
In classiﬁcation, the property takes the name of robustness and reads as: the output of the network remains unchanged over the neighborhood V(xo). It certiﬁes that the network is robust against inputs corrupted by uncertainties of limited support or adversarial perturbations of constrained distortion.
The certiﬁcation mechanism has two desired features as deﬁned in [Singh et al., 2018]:
• Soundness: it does not certify the network when the property does not hold.
• Completeness: it does certify the network whenever the property holds. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Corruption robustness assessment Adversarial robustness corresponds to a worst-case analysis whereas corruption robustness considers random perturbations of the inputs. The key ingredient is the introduction of a statistical model π0 of epistemic uncertainties occurring along the acquisition chain of the input. For instance, Franceschi et al. [2018] take Gaussian and uniform distributions over the lp ball Bp,(cid:15)(xo) of radius (cid:15) centered on xo.
This recent trend goes with a quantitative assessment gauging to what extent a given property holds or does not hold. For instance, Webb et al. [2019] estimate the probability p that a property is violated under a given statistical model of the inputs. This approach makes no assumption about the network under scrutiny as it is used as a black box. This grants the scalability to tackle deep networks. The main difﬁculty lies in the efﬁciency, i.e. the computational power needed to estimate weak probabilities. Their lack of soundness stems from the inability to determine if the probability p of violation is exactly zero or too small to be estimated.
Section 2 presents a brief overview of robustness assessment procedures outlining the assumptions made about the network and their limitations.
This work presents a scalable and efﬁcient procedure assessing corruption robustness under a large panel of statistical models. It provides completeness and theoretical guarantees on the lack of soundness. 2