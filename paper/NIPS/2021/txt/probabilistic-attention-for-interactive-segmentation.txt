Abstract
We provide a probabilistic interpretation of attention and show that the standard dot-product attention in transformers is a special case of Maximum A Posteriori (MAP) inference. The proposed approach suggests the use of Expectation Maximization algorithms for online adaptation of key and value model parameters. This approach is useful for cases in which external agents, e.g., annotators, provide inference-time information about the correct values of some tokens, e.g., the semantic category of some pixels, and we need for this new information to propagate to other tokens in a principled manner. We illustrate the approach on an interactive semantic segmentation task in which annotators and models collaborate online to improve annotation efﬁciency. Using standard benchmarks, we observe that key adaptation boosts model performance (∼ 10% mIoU) in the low feedback regime and value propagation improves model responsiveness in the high feedback regime. A Py-Torch layer implementation of our probabilistic attention model is available here: https://github.com/apple/ml-probabilistic-attention. 1

Introduction
Attention was ﬁrst introduced as a computational primitive for natural language processing [53] and has since been widely adopted [17, 63, 13, 14] as a replacement for recurrent primitives such as LSTMs [26]. More recently it has been making inroads into computer vision [43, 69, 57, 64, 5, 19, 49] as a replacement for the long accepted convolution as the main computational primitive.
Self-attention based architectures have demonstrated state-of-the-art results in fundamental vision problems including image classiﬁcation [5, 43, 69, 19, 49], object detection [8, 71, 57], image and video semantic segmentation [55, 29, 57, 41] and tracking [66] to state a few.
There are a few different perspectives on the reasons for success of self-attention in computer vision and its superiority over convolution. This includes a view that the self-attention mechanism allows modeling spatially varying dynamic convolution ﬁlters [32] and at the same time enabling parameter independent scaling of receptive ﬁelds [52]. Another includes their ability to capture global context through long range interactions especially when full attention is feasible [49] at reduced spatial resolution maps or using an approximation of full attention with axial [55] or criss-cross attention
[29]. A recent work [45] introduces modern Hopﬁeld networks with continuous states where the update mechanism is shown to be equivalent to the update mechanism of standard dot-product attention [53]. They show that such a network has the capacity to store exponentially many patterns and retrieve them with high ﬁdelity. In this work, we provide a novel interpretation of attention as a probabilistic generative model for queries and values. Speciﬁcally we hypothesize the existence of a bank of probabilistic memory units, each of which maintains a joint probability distribution over queries and values parameterized through keys. A query/value pair is generated by ﬁrst sampling a unit (from a prior over units) followed by sampling the pair from the unit speciﬁc joint distribution.
This is equivalent to generating the queries and values through a probabilistic mixture model over the units. A particular form for unit joint likelihoods expressed as Gaussians for both the query and value marginals, assuming their independence conditioned on a unit, turns out to be equivalent to traditional 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dot product attention under a few constraints. As shown in Section 3.3, maximum likelihood (ML) inference for the corresponding value given a query is equivalent to standard dot-product attention.
Our probabilistic interpretation provides a systematic framework for online update of mixture model parameters based on a set of observed queries. It also allows propagation of correct values provided by an external agent for some of the units to all other units. Using Bayesian inference in the constrained case, we derive update rules for online unsupervised adaptation (Section 3.5) of query/key likelihood parameters based on a set of observed queries. We also derive update equations for online value propagation (Section 3.6) across units based on ﬁxed externally speciﬁed values for a subset of units. The latter is speciﬁcally useful for interactive segmentation where a correction provided by an annotator has to be propagated globally to make the process more efﬁcient. We use probabilistic attention in place of standard attention in deep architectures for interactive segmentation both within the backbone and at the network head as a classiﬁer. Speciﬁcally we use probabilistic attention updates in the BoTNet50 [49] architecture and show that adapting keys to incoming queries leads to better model performance in the low annotator feedback regime. Using value propagation within a probabilistic attention layer at the head of the segmentation network leads to a more responsive model through effective feedback propagation in the high feedback regime. We also use both key adaptation and value propagation together and demonstrate the complementary effects of the two in both the low and high annotator feedback regimes. 2