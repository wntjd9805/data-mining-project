Abstract
We are motivated by the problem of providing strong generalization guarantees in the context of meta-learning. Existing generalization bounds are either challenging to evaluate or provide vacuous guarantees in even relatively simple settings. We derive a probably approximately correct (PAC) bound for gradient-based meta-learning using two different generalization frameworks in order to deal with the qualitatively different challenges of generalization at the “base” and “meta” levels.
We employ bounds for uniformly stable algorithms at the base level and bounds from the PAC-Bayes framework at the meta level. The result of this approach is a novel PAC bound that is tighter when the base learner adapts quickly, which is precisely the goal of meta-learning. We show that our bound provides a tighter guarantee than other bounds on a toy non-convex problem on the unit sphere and a text-based classiﬁcation example. We also present a practical regularization scheme motivated by the bound in settings where the bound is loose and demonstrate improved performance over baseline techniques. 1

Introduction
A major challenge with current machine learning systems is the need to acquire large amounts of training data in order to learn a new task. Over the past few decades, meta-learning [62, 70] has emerged as a promising avenue for addressing this challenge. Meta-learning relies on the intuition that a new task often bears signiﬁcant similarity to previous tasks; hence, a learner can learn to perform a new task very quickly by exploiting data from previously-encountered related tasks. The meta-learning problem formulation thus assumes access to datasets from a variety of tasks during meta-training. The goal of the meta learner is then to learn inductive biases from these tasks in order to train a base learner to achieve few-shot generalization on a new task.
Over the past few years, there has been tremendous progress in practical algorithms for meta-learning (see, e.g., [61, 55, 25, 32]). Techniques such as model-agnostic meta-learning (MAML)
[25] have demonstrated the ability to perform few-shot learning in a variety of supervised learning and reinforcement learning domains. However, our theoretical understanding of these techniques lags signiﬁcantly behind successes on the empirical front. In particular, the problem of deriving generalization bounds for meta-learning techniques remains an outstanding challenge. Current methods for obtaining generalization guarantees for meta-learning [5, 34, 77] either (i) produce bounds that are extremely challenging to compute or (ii) produce vacuous or near-vacuous bounds in even highly simpliﬁed settings (see Section 5 for numerical examples). Indeed, we note that existing work on generalization theory for meta-learning techniques do not explicitly report numerical values for generalization bounds. This is in contrast to the state of generalization theory in the supervised learning setting, where recent techniques demonstrate the ability to obtain non-vacuous generalization guarantees on benchmark problems (e.g. visual classiﬁcation problems [24, 78, 54]).
The generalization challenge in meta-learning is similar to, but distinct from, the supervised learning case. In particular, any generalization bound for meta-learning must account for two levels of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
generalization. First, one must account for generalization at the base level, i.e., the ability of the base learner to perform well on new data from a given task. This is particularly important in the few-shot learning setting. Second, one must account for generalization at the meta level, i.e., the ability of the meta learner to generalize to new tasks not encountered during meta-training. Moreover, the generalization performance at the two levels is coupled since the meta learner is responsible for learning inductive biases that the base learner can exploit for future tasks.
The key technical insight of this work is to bound the generalization error at the two levels (base and meta) using two different generalization theory frameworks that each are particularly well-suited for addressing the speciﬁc challenges of generalization. At the base level, we utilize the fact that a learning algorithm that exhibits uniform stability [14, 15] also generalizes well in expectation (see
Section 4.1 for a formal statement). Intuitively, uniform stability quantiﬁes the sensitivity of the output of a learning algorithm to changes in the training dataset. As demonstrated by [29], limiting the number of training epochs of a gradient-based learning algorithm leads to uniform stability. In other words, a gradient-based algorithm that learns quickly is stable. Since the goal of meta-learning is precisely to train the base learner to learn quickly, we posit that generalization bounds based on stability are particularly well-suited to bounding the generalization error at the base level. At the meta level, we employ a generalization bound based on Probably Approximately Correct (PAC)-Bayes theory. Originally developed two decades ago [43, 38], there has been a recent resurgence of interest in PAC-Bayes due to its ability to provide strong generalization guarantees for neural networks
[24, 8, 54]. Intuitively, the challenge of generalization at the meta level (i.e., generalizing to new tasks) is similar to the challenge of generalizing to new data in the standard supervised learning setting.
In both cases, one must prevent over-ﬁtting to the particular tasks/data that have been seen during meta-training/training. Thus, the strong empirical performance of PAC-Bayes theory in supervised learning problems makes it a promising candidate for bounding the generalization error at the meta level.
Contributions. The primary contributions of this work are the following. First, we leverage the insights above in order to develop a novel generalization bound for gradient-based meta-learning using uniform stability and PAC-Bayes theory (Theorem 3). Second, we develop a regularization scheme for MAML [25] that explicitly minimizes the derived bound (Algorithm 1). We refer to the resulting approach as PAC-BUS since it combines PAC-Bayes and Uniform Stability to derive generalization guarantees for meta-learning. Third, we demonstrate our approach on two meta-learning problems: (i) a toy non-convex classiﬁcation problem on the unit-ball (Section 5.1), and (ii) the Mini-Wiki benchmark introduced in [34] (Section 5.2). Even in these relatively small-scale settings, we demonstrate that recently-developed generalization frameworks for meta-learning provide either near-vacuous or loose bounds, while PAC-BUS provides signiﬁcantly stronger bounds. Fourth, we demonstrate our approach in larger-scale settings where it remains challenging to obtain non-vacuous bounds (for our approach as well as others). Here, we propose a practical regularization scheme which re-weights the terms in the rigorously-derived PAC-BUS upper bound (PAC-BUS(H);
Algorithm 3 in the appendix). Recent work [77] introduces a challenging variant of the Omniglot benchmark [35] which highlights and tackles challenges with memorization in meta-learning. We show that PAC-BUS(H) is able to prevent memorization on this variant (Section 5.3). 2 Problem formulation
Z
⇠
Pt induces an (unknown) distribution Pz is shared between tasks, but the distribution Pz
Samples, tasks, and datasets. Formally, consider the setting where we have an unknown meta distribution Pt over tasks (roughly, “tasks” correspond to different, but potentially related, learning problems). A sampled task t t over sample space
. We assume that all sampling is independent and identically distributed (i.i.d.). Note that the
Z sample space t may be different. We then sample within-task samples z z1, z2, . . . , zm}⇠ t. We assume that each sample z has a single corresponding label o(z), where the function o is an oracle which outputs the correct label of z. At meta-training time, we assume access to l datasets, which we call
S =
. Each dataset Si in S is drawn by ﬁrst selecting a task ti from Pt, and then
S1, S2, . . . , Sl}
{ drawing Si ⇠
.
Hypotheses and losses. Let h denote a hypothesis and L(h, z) be the loss incurred by hypothesis h on sample z. The loss is computed by comparing h(z) with the true label o(z). For simplicity, we assume that there is no noise on the labels; we can thus assume that all loss functions have access to t and within-task datasets S =
P m ti z
|
P m z
|
Pz
⇠
{
|
|
| 2
the label oracle function o and thus the loss depends only on hypothesis h and sample z. We note that this assumption is not required for our analysis and is made for the ease of exposition. Overloading the notation, we let L(h, Pz t L(h, z) and
S
|i=1 L(h, zi).
| t) := Ez
Pz
L(h, S) := 1
S
|
|
|
⇠
| b 2
Meta-learning. As with model-agnostic meta-learning (MAML) [25], we let meta parameters
Rn✓ correspond to an initialization of the base learner’s hypothesis. Let h✓ be the ✓-initialized
✓ hypothesis. Generally, the initialization ✓ is learned from the multiple datasets we have access to at meta-training time. In this work, we will learn a distribution P✓ over initializations so that we can use bounds from the PAC-Bayes framework. At test time, a new task t
Pt is sampled and we are
P m t. The base learner uses an algorithm A (e.g., gradient descent), provided with a new dataset S z
| the dataset S, and the initialization ✓
P✓ in order to ﬁne-tune the hypothesis and perform well on
⇠ future samples drawn from Pz t. We denote the base learner’s updated hypothesis by hA(✓,S). More formally, our goal is to learn a distribution P✓ with the following objective:
P
⇠
⇠
| min
P✓ L (P✓, Pt) := min
P✓
E
Pt
⇠ t
S
E
P m t z
⇠
|
E
P✓
⇠
✓
L(hA(✓,S), Pz t).
| (1)
We are particularly interested in the the few-shot learning case, where the number of samples which the base learner can use to adapt is small. A common technique to improve test performance in the few-shot learning case is to allow for validation data at meta-training time. Thus, in addition to a generalization guarantee on meta-learning without validation data, we will derive a bound when allowing for the use of validation data Sva ⇠ 3