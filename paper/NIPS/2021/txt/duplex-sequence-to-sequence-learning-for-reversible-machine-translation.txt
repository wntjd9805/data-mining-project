Abstract
Sequence-to-sequence learning naturally has two directions. How to effectively utilize supervision signals from both directions? Existing approaches either require two separate models, or a multitask-learned model but with inferior performance.
In this paper, we propose REDER (REversible Duplex TransformER), a parameter-efﬁcient model and apply it to machine translation. Either end of REDER can si-multaneously input and output a distinct language. Thus REDER enables reversible machine translation by simply ﬂipping the input and output ends. Experiments verify that REDER achieves the ﬁrst success of reversible machine translation, which helps outperform its multitask-trained baselines up to 1.3 BLEU. 1 1

Introduction
Neural sequence-to-sequence (seq2seq) learning [Sutskever et al., 2014] has been extensively used in various applications of natural language processing. Standard seq2seq neural networks usually employ the encoder-decoder framework, which includes an encoder to acquire the representations from the source side, and a decoder to yield the target side outputs from the encoded source represen-tation [Bahdanau et al., 2015, Gehring et al., 2017, Vaswani et al., 2017].
Generally, given paired training data is target side of the corresponding seq2seq task, supervision signals are always bidirectional. Thus we can learn not only the mapping from source to target (fθxy :
) but also the mapping from target to source (fθyx :
). This is very common in many applications. For example, given parallel corpus, we can obtain machine translation models from both Chinese to English and English to Chinese, or transfer between different stylized texts [Yang et al., 2018, He et al., 2019]. is the source side and
Dxy =
Y (cid:55)→ X
X (cid:55)→ Y
, where
X × Y
X
Y
Typical seq2seq learning utilizes the bidirectional supervisions by splitting the bidirectional supervi-sions into two unidirectional ones and trains two individual seq2seq models on them, respectively.
However, such splitting ignores the internal consistencies between the bidirectional supervisions.
Thus how to make better use of the bidirectional supervisions remains an open problem. One potential solution is multitask learning [Johnson et al., 2017], which jointly leverages the bidirectional supervi-sions within one model and expects the two unidirectional supervisions could boost each other (Figure 1(b)). But it does not work well in our case due to the parameter interference problem [Arivazhagan et al., 2019, Zhang et al., 2021]. For instance, in the setting of multitask Chinese-to-English and
English-to-Chinese bidirectional translations, the encoder/decoder of seq2seq networks is trained to simultaneously understand/generate both Chinese and English, which always results in performance
∗Work was done when Zaixiang Zheng was a ﬁnal-year PhD candidate at Nanjing University and an intern (now FTE) at ByteDance AI Lab; and when Lei Li was also at ByteDance AI Lab. 1Code is available at https://github.com/zhengzx-nlp/REDER. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
drop due to the divergent nature of the two languages. Another branch of solutions lies in cycle training [Sennrich et al., 2016, He et al., 2016, Zheng et al., 2020], which still deploys two individual models for separately learning, but explicitly regularize their outputs by the cycle consistency of seq2seq problems. Such approaches usually need to introduce extra monolingual data in machine translation.
In this paper, we propose to explore an alternative approach for utilizing bidirectional supervisions called duplex sequence-to-sequence learning. We argue that current seq2seq networks do not beneﬁt from multitask training because the seq2seq net-works are simplex from the view of telecommunica-tions2. Speciﬁcally, the data stream only ﬂows from encoder to decoder in the current seq2seq learn-ing, and such simplex property makes the multitask learning suffer from the parameter interfere prob-lem. Instead, if we have duplex neural networks, where the data stream can ﬂow from both ends, each of which only specializes in one language when learning from bidirectional supervisions (Fig-ure 1(c)), thus potentially alleviating the parameter interference problem. As a result, the bidirectional translation can be achieved as a reversible and uni-ﬁed process. In addition, we could still incorporate cycle training in the duplex networks using only one single model.
Figure 1: Illustration of different sequence-to-sequence neural models in regards to modeling direction and generation formulations.
Deﬁnition 1. A sequence-to-sequence neural network with parameter θ is duplex if it satisﬁes the following: its network has two ends, i.e., a source end and a target end; both source and target ends can take input and output sequences; its network deﬁnes a forward mapping function f →θ
, that satisﬁes the following 1; besides, it satisﬁes the following cycle consistency: reversibility: f ←θ = (f →θ )− x
∀
, and a reverse mapping function f ←θ 1 and f →θ = (f ←θ )−
: f ←θ (f →θ (x)) = x and
: f →θ (f ←θ (y)) = y.
X (cid:55)→ Y
Y (cid:55)→ X
∈ X
∈ Y y
∀
:
:
Based on the idea of duplex network, we propose REDER3, the REversible Duplex TransformER, and apply it to machine translation. Note that, building duplex seq2seq networks is non-trivial: a) vanilla encoder-decoder network is irreversible. The output end of the decoder cannot take in input signals to exhibit the encoding functionality and vice versa; b) the topologies of the encoder and the decoder are heterogeneous, i.e., the decoder works autoregressively, while the encoder is non-autoregressive.
To this end, we therefore design REDER without explicit encoder and decoder division, in which we introduce the reversible Transformer layer [Gomez et al., 2017] and fully non-autoregressive modeling to solve the above two problems respectively. As a result, REDER works in the duplex fashion, which could better exploit the bidirectional supervisions for achieving better downstream task performance.
Experimental results show that the duplex idea indeed works: Overall, REDER achieves BLEU scores of 27.50 and 31.25 on standard WMT14 EN-DE and DE-EN benchmarks, respectively, which are top results among non-autoregressive machine translation models. REDER achieves signiﬁcant gains (+1.3 BLEU) compared to its simplex baseline, whereas multitask learning does hurt the translation performance of simplex models both in the autoregressive (-0.5 BLEU) and non-autoregressive settings (-1.3 BLEU). Although REDER adopts fully non-autoregressive modeling to realize the duplex networks, the gap of BLEU between REDER and autoregressive Transformer is negligible, meanwhile REDER can directly translate between two directions and enjoys 5.5
× inference speedup as a bonus of non-autoregressive modeling. To our best knowledge, REDER is the
ﬁrst duplex seq2seq network, which enables the ﬁrst feasible reversible machine translation system. 2In telecommunications and computer networking, the simplex communication means the communication channel is unidirectional while the duplex communication is bidirectional. 3The model’s name is a palindrome, which implies the model works from both ends. 2
2 REDER for Reversible Machine Translation
In this section, we introduce how to design a duplex neural seq2seq model, the REDER (REversible
Duplex TransformER), that satisﬁes Deﬁnition 1, and its application in machine translation that realizes the ﬁrst neural reversible machine translation system. 2.1 Challenges of Reversible Machine Translation
Reversible natural language processing [Franck, 1992, Strzalkowski, 1993] and its applications in machine translation [van Noord, 1990] were proposed for the purpose of building machine models that understand and generate natural languages as a reversible, uniﬁed process. Such process resembles the mechanism of the ability that allows human beings to communicate with each other via natural languages [Franck, 1992]. Despite the success of neural machine translation with deep learning, designing neural architectures for reversible machine translation yet remains under-studied and has the following challenges:
Reversibility. Typical encoder-decoder networks and their neural components, such as Transformer layers, are irreversible, i.e., one cannot just obtain its inverse function by ﬂipping the same encoder-decoder network. To meet our expectation, an inverse function of the network should be derived from the network itself.
Homogeneity. Intuitively, a pair of forward and reverse translation directions should resemble a homogeneous process of understanding and generation. However, typical encoder-decoder networks certainly do not meet such computational homogeneity due to extra cross attention layers in the decoder; and also because of the discrepancy that the decoder works autoregressively but the encoder does non-autoregressively. To meet our expectations, the separation of encoder and decoder should be no more exist in the desired network. 2.2 The Architecture of REDER
To solve the above challenges, we include two corresponding solutions in REDER to address the reversibility and homogeneity issues respectively, i.e., the Reversible Duplex Transformer layers, and the symmetric network architecture without the encoder-decoder framework.
Figure 2 shows the overall architecture of
REDER. As illustrated, REDER has two ends: the source end (left) and the target end (right). θ is the model parameter, shared by both directions. The architecture of REDER is composed of a series of identical Re-versible Duplex Transformer layers. When performing the source-to-target mapping f →θ , a source sentence x (blue circles) 1) ﬁrst transforms to its embedding e(x) and en-ters the source end; 2) then goes through the entire stacked layers and evolves to ﬁ-nal representations HL which predicts prob-abilities; 3) ﬁnally its target translation (or-ange circles) will be generated from the target ends. The generation process is fully non-autoregressive.
Likewise, the target-to-source mapping f ←θ is achieved by reversely executing the architec-ture of REDER from target end to source end.
We will dive into the details of the key com-ponents of REDER in the following parts.
Figure 2: The proposed REDER for duplex sequence-to-sequence generation. The bottom two diagrams show the computation of the regular and reverse forms of a reversible layer. Notice that, to make the whole model symmetric, we reverse the 1-th to L/2-th lay-ers, such that the overall computational operations of forward and reverse of REDER are homogeneous.
Reversibility: Reversible Duplex Transformer layers. We adopt the idea of the reversible residual network (RevNet, Gomez et al., 2017, Kitaev et al., 2020) in the design of the reversible duplex
Transformer layer. The bottom of Figure 2 illustrates the forward and reverse computations of a layer.
Each layer is composed of a multi-head self-attention (SAN) and a feed-forward network (SAN) with 3
a special reversible design to ensure duplex behavior, where the input and output representations of such a layer are split by 2 halves, i.e., [H(1)
]. Formally, the regular form of
− the l-th layer
] and [H(1)
− 1; H(2) 1; H(2) l l l l
The reverse form of can be computed by subtracting (instead of adding) the residuals:
Fl performs as follow:
Fl(Hl
Hl = where H(1) l = H(1) l
− 1
− l
F 1 =
Hl where H(2)
−
− l
[H(1) l 1) 1 + SAN(H(2)
⇔
; H(2) l
] = 1), H(2)
Fl([H(1) l = H(2) 1; H(2) 1]), 1 + FFN(H(1)
−
− l l l l l
).
−
−
− 1
− l (Hl)
F 1 = H(2) l −
[H(1) l
⇔
−
FFN(H(1) l 1; H(2) l 1 ([H(1) l
− l 1] =
F
− 1 = H(1)
), H(1)
− l l −
; H(2)
]), l
SAN(H(2)
− l 1).
For better modeling the reordering between source and target languages, we employ relative self-attention [Shaw et al., 2018] instead of the original one [Vaswani et al., 2017].
Homogeneity: Symmetric network architecture without encoder-decoder framework. To meet our need to ensure homogeneous network computations for forward and reverse directional tasks, we therefore choose to discard the encoder-decoder paradigm.
Symmetric network. To achieve homogeneous computations, one solution is to make our network symmetric, as depicted at the top of Figure 2. Speciﬁcally, we let the 1-th to L/2-th layers be the reverse form, whereas the latter (L/2 + 1)-th to L-th be the regular form: f →θ (x) (cid:44) f ←θ (y) (cid:44) 1 1
−
− 1 ◦ · · · ◦ F
F
FL ◦ · · · ◦ FL/2+1 ◦ F
L/2 ◦ FL/2+1 ◦ · · · ◦ FL(x), 1 1 (y),
−
−
L/2 ◦ · · · ◦ F 1 where means a layer is connected to the next layer. Thereby the forward and reverse computations of REDER become homogeneous: the forward computational operation series reads as a palindrome string and so does the reverse series, where s and f denote SAN and FFN.
◦ fs fssf sf (cid:104)
· · ·
· · · (cid:105)
Fully non-autoregressive modeling. Note that without encoder-decoder division, REDER works in a fully non-autoregressive fashion in both reading and generating sequences. Speciﬁcally, given an input sequence x, H0,i = [H(1) 0,i ] = [e(xi); e(xi)], is the i-th element of REDER’s input, which is the concatenation of two copies of the embedding of xi. Once a forward computation is done, the concatenation of the output of the model [H(1)
L,i] serves as the representations of target translation. And then, a softmax operation is performed to measure the similarity between the model output [H(1)
L,i] and the concatenated embedding of ground-truth reference [e(yi), e(yi)], to obtain the prediction probability:
L,i; H(2)
L,i; H(2) 0,i ; H(2) x; θ) = softmax([e(yi); e(yi)](cid:62)[H(1)
L,i; H(2)
L,i]/2). p(yi|
We can likewise derive the procedure of f ←θ for the target-to-source direction. Due to the conditional independence assumption among target tokens introduced by non-autoregressive generation, the log-likelihood of a translation becomes: log p(y
| x; θ) = (cid:88) i log pθ(yi| x)
Modeling variable-length input and output. Encoder-decoder models can easily model variable-length input and output of most seq2seq problems. However, discarding encoder-decoder separation imposes a new challenge: the width of all the layers of the network is depending on the length of the input, thus it is very difﬁcult to allow variable-length input and output, especially when the input is shorter than the output. We resort to the Connectionist Temporal Classiﬁcation (CTC) [Graves et al., 2006] to solve this problem, a latent alignment approach with superior performance and the ﬂexibility of variable length prediction. Given the conditional independence assumption, CTC is capable of efﬁciently ﬁnding all valid alignments a which derives from the target y by allowing consecutive repetitions and inserting blank tokens, and marginalizes log-likelihood: log pctc(y
| x; θ) = log (cid:88) a
Γ(y)
∈ pθ(a
| x), 4
1(a) is the collapse function that recovers the target sequence by collapsing consecutive where Γ− repeated tokens, and then removing all blank tokens. Note that CTC requires that the length of source input should not be smaller than the target output, which is not the case in machine translation. To deal with this, we follow previous useful practice by upsampling the source tokens by 2 times [Saharia et al., 2020, Libovický and Helcl, 2018], and ﬁlter those examples when the target lengths are still larger than the one of upsampled source sentences.
Remark. Reversibility in REDER can be assured in the continuous representation level, where
REDER can recover from output representations (last layer) to input embeddings (ﬁrst layer), which is also the motivation and basis of the auxiliary learning signal, i.e.
Lfba, in the next section.
Reversibility might not hold in the discrete token level, because of the existence of irreversible operations, e.g. the argmax operation discretizes probabilities to tokens and the CTC collapse process. But REDER still shows decent reconstruction capability in practice, as visually depicted in the experiment section. 2.3 Training
Given a parallel corpus and a single model θ, REDER can be jointly supervised by source-to-target and target-to-source translation for f →θ and f ←θ , respectively. Thus both translation directions can be achieved in one REDER model. We refer this to bidirectional training, which is opposite to unidirectional training, where each translation direction needs a separate model. Moreover, the reversibility of REDER enables appealing potentials to exploit consistency/agreement between forward and reverse directions. We introduce two auxiliary learning signals as follows.
Layer-wise Forward-Backward Agreement Since REDER is fully reversible, which consists of a series of computationally inverse of intermediate layers, an interesting question arises: given the desired output (i.e., the target sentence), is it possible to derive the desired intermediate hidden representation by the backward target-to-source computation, and encourage the forward source-to-target intermediate hidden representations as close as possible to these “optimal” representations?
Given a source sentence x, the inner representations of each layer in the forward direction are:
−→
H1 = F1(e(x)),
−→
H2 = F2(
−→
H1) . . .
−→
HL = FL(
−→
HL−1), and given its corresponding target sequence y as the optimal desired output4, the inner representations of each layer in the reverse direction are:
←−
←−
HL−1 = F −1
HL = F −1
←−
HL) . . .
←−
H1 = F −1
L (
L (e(y)),
←−
H2),
L−1( where −→Hl and ←−Hl represent the representations of l-th layer in forward and reverse models, respec-tively. As we consider these reverse inner layer representations as “optimal”, we try to minimize the cosine distance between the forward and backward corresponding inner layer representations:
Lfba(x y; θ) =
| 1
L
L (cid:88) l=1 1
− cos(
−→
Hl, sg(
←−
Hl)), where sg(
) denotes the stop-gradient operation.
·
Cycle Consistency The symmetry of a pair of seq2seq tasks enables the use of cycle consistency [He et al., 2016, Cheng et al., 2016a]. Given a source sentence x, we ﬁrst obtain the forward prediction, and then we use the REDER to reconstruct this prediction to the source language:
¯y = f →θ (x),
¯x = f ←θ ( ¯y).
Finally, we maximize the consistency or agreement between the original one x and reconstructed one
¯x. Thus, the loss function reads
Lcc(x; θ) = distancecc(x, f ←θ (f →θ (x))) = distancecc(x, ¯x).
We expect it can provide an auxiliary signal that a valid prediction should be loyal to reconstruct its source input. Here we use cross-entropy between the probabilistic prediction of the reverse model as distance to measure the consistency. 4for CTC-based model where the model predictions are the alignments, we instead extract the token sequence of the best alignment a∗, predicted by the model, associated with the ground-truth y as the optimal desired output. 5
A potential danger of both of the above auxiliary objectives is learning a degenerate solution, where it would probably cheat this task by simply learning an identity mapping. We solve this problem by setting a two-stage training scheme for them, where we ﬁrst train REDER without using any auxiliary losses until a predeﬁned number of updates, and then activate the additional losses and continue training the model until convergence.
Final Objective Given a parallel dataset
ﬁnal objective of REDER is to minimize
Dxy =
{ (x(n), y(n) n = 1...N
|
} of i.i.d observations, the (θ;
L
Dxy) =
N (cid:88) (cid:16) n=1 log pctc(y(n) x(n); θ)
−
|
+ λfbaLfba(x(n) y(n); θ)
| y(n); θ) + λfbaLfba(y(n)
| log pctc(x(n) (cid:123)(cid:122) layer-wise forward-backward agreements x(n); θ) (cid:125)
− (cid:124)
|
+ λccLcc(x(n); θ) + λccLcc(y(n); θ) (cid:125) (cid:123)(cid:122) cycle consistencies (cid:124) (cid:17) where λfba and λcc are coefﬁcients of the auxiliary losses. 3