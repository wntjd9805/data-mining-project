Abstract
We investigate the problem of active learning in the streaming setting in non-parametric regimes, where the labels are stochastically generated from a class of functions on which we make no assumptions whatsoever. We rely on recently proposed Neural Tangent Kernel (NTK) approximation tools to construct a suitable neural embedding that determines the feature space the algorithm operates on and the learned model computed atop. Since the shape of the label requesting threshold is tightly related to the complexity of the function to be learned, which is a-priori unknown, we also derive a version of the algorithm which is agnostic to any prior knowledge. This algorithm relies on a regret balancing scheme to solve the resulting online model selection problem, and is computationally efﬁcient. We prove joint guarantees on the cumulative regret and number of requested labels which depend on the complexity of the labeling function at hand. In the linear case, these guarantees recover known minimax results of the generalization error as a function of the label complexity in a standard statistical learning setting. 1

Introduction
Supervised learning is a fundamental paradigm in machine learning and is at the core of modern breakthroughs in deep learning [29]. A machine learning system trained via supervised learning requires access to labeled data collected via recruiting human experts, crowdsourcing, or running expensive experiments. Furthermore, as the complexity of current deep learning architectures grows, their requirement for labeled data increases signiﬁcantly. The area of active learning aims to reduce this data requirement by studying the design of algorithms that can learn and generalize from a small carefully chosen subset of the training data [13, 40].
The two common formulations of active learning are pool based active learning, and sequential (or streaming) active learning. In the pool based setting [30], the learning algorithm has access to a large unlabeled set of data points, and the algorithm can ask for a subset of the data to be labeled. In contrast, in the sequential setting, data points arrive in a streaming manner, either adversarially or drawn i.i.d. from a distribution, and the algorithm must decide whether to query the label of a given point or not [14].
From a theoretical perspective, active learning has typically been studied under models inspired by the probably approximately correct (PAC) model of learning [41]. Here one assumes that there is a pre-speciﬁed class H of functions such that the target function mapping examples to their labels 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
either lies in H or has a good approximation inside the class. Given access to unlabeled samples generated i.i.d. from the distribution, the goal is to query for a small number of labels and produce a hypothesis of low error.
In the parametric setting, namely, when the class of functions H has ﬁnite VC-dimension (or ﬁnite disagreement coefﬁcient) [21], the rate of convergence of active learning, i.e., the rate of decay of the regret as a function of the number of label queries (N ), is of the form ν N −1/2 + e−
N , where ν is the population loss of the best function in class H. This simple ﬁnding shows that active learning behaves like passive learning when ν > 0, while very fast rates can only be achieved under low noise (ν ≈ 0) conditions. This has been worked out in, e.g., [19, 15, 5, 4, 6, 38].
√
While the parametric setting comes with methodological advantages, the above shows that in order to unleash the true power of active learning, two properties are desirable: (1) A better interplay between the input distribution and the label noise and, (2) a departure from the parametric setting leading us to consider wider classes of functions (so as to reduce the population loss ν to close to 0). To address the above, there has also been considerable theoretical work in recent years on non-parametric active learning [10, 33, 31]. However, these approaches suffer from the curse of dimensionality and do not lead to computationally efﬁcient algorithms. A popular approach that has been explored empirically in recent works is to use Deep Neural Networks (DNNs) to perform active learning (e.g., [37, 26, 39, 3, 44]). While these works empirically demonstrate the power of the DNN-based approach to active learning, they do not come with provable guarantees. The above discussion raises the following question: Is provable and computationally efﬁcient active learning possible in non-parametric settings?
We answer the above question in the afﬁrmative by providing the ﬁrst, to the best of our knowledge, computationally efﬁcient algorithm for active learning based on Deep Neural Networks. Similar to non-parametric active learning, we avoid ﬁxing a function class a-priori. However, in order to achieve computational efﬁciency, we instead propose to use over-parameterized DNNs, where the amount of over-parameterization depends on the input data at hand. We work in the sequential setting, and propose a simple active learning algorithm that forms an uncertainty estimate for the current data point based on the output of a DNN, followed by a gradient descent step to update the network parameters if the data point is queried. We show that under standard low-noise assumptions [32] our proposed algorithm achieves fast rates of convergence.
In order to analyze our algorithm, we use tools from the theory of Neural Tangent Kernel (NTK) approximation [24, 2, 18] that allows us to analyze the dynamics of gradient descent by considering a linearization of the network around random initialization. Since we study the non-parametric regime, the convergence rates of our algorithm depend on a data-dependent complexity term that is expected to be small in practical settings, but could be very large in worst-case scenarios. Furthermore, the algorithm itself needs an estimate of complexity term in order to form accurate uncertainty estimates.
We show that one can automatically adapt to the magnitude of the unknown complexity term by designing a novel model selection algorithm inspired by recent works in model selection in multi-armed bandit settings [36, 35]. Yet, several new insights are needed to ensure that the model selection algorithm can simultaneously achieve low generalization error without spending a signiﬁcant amount of budget on label queries. 2 Preliminaries and Notation
Let X denote the input space, Y the output space, and D an unknown distribution over X × Y.
We denote the corresponding random variables by x and y. We also denote by DX the marginal distribution of D over X , and by DY|x0 the conditional distribution of random variable y given x = x0. Moreover, given a function f (sometimes called a hypothesis or a model) mapping X to Y, the conditional population loss (often referred to as conditional risk) of f is denoted by L(f | x), and deﬁned as L(f | x) = Ey∼DY|x [(cid:96)(f (x), y) | x], where (cid:96) : Y × Y → [0, 1] is a loss function. For ease of presentation, we restrict to a binary classiﬁcation setting with 0-1 loss, whence Y = {−1, +1}, and (cid:96)(a, y) = 11{a (cid:54)= y} ∈ {0, 1}, 11{·} being the indicator function of the predicate at argument.
When clear from the surrounding context, we will omit subscripts like “y ∼ DY|x" from probabilities and expectations. 2
We investigate a non-parametric setting of active learning where the conditional distribution of y given x is deﬁned through an unknown function h : X 2 → [0, 1] such that
P(y = 1 | x) = h((x, 0))
P(y = −1 | x) = h((0, x)) , (1) where 0 ∈ X , (x1, x2) denotes the concatenation (or pairing) of the two instances x1 and x2 (so that (x, 0) and (0, x) are in X 2) and, for all x ∈ X we have h((x, 0)) + h((0, x)) = 1. We make no explicit assumptions on h, other than its well-behavedness w.r.t. the data {xt}T t=1 at hand through the formalism of Neural Tangent Kernels (NTK) – see below. As a simple example, in the linear case, X is the d-dimensional unit ball, h(·, ·) is parametrized by an unknown unit vector θ ∈ Rd, and h((x1, x2)) = 1+(cid:104)(θ,−θ),(x1,x2)(cid:105)
, where (cid:104)·, ·(cid:105) is the usual dot product in Rd.
, so that h((x, 0)) = 1+(cid:104)θ,x(cid:105) and h((0, x)) = 1−(cid:104)θ,x(cid:105) 2 2 2
We consider a streaming setting of active learning where, at each round t ∈ [T ] = {1, . . . , T }, a pair (xt, yt) ∈ X × Y is drawn i.i.d. from D. The learning algorithm receives as input only xt, and is compelled to both issue a prediction at for yt and, at the same time, decide on-the-ﬂy whether or not to observe yt. These decisions can only be based on past observations. Let Et denote the conditional expectation E[· |(x1, y1) . . . , (xt−1, yt−1), xt], and we introduce the shorthand xt,a = (cid:26)(xt, 0) (0, xt) if a = 1 if a = −1 .
Notice that with this notation E[(cid:96)(a, yt) | xt] = 1 − h(xt,a), for all a ∈ Y. We quantify the accuracy of the learner’s predictions through its (pseudo) regret, deﬁned as
RT =
T (cid:88) (cid:16) t=1
Et[(cid:96)(at, yt) | xt] − E[(cid:96)(a∗ t , yt) | xt] (cid:17)
=
T (cid:88) t=1 (cid:0)h(xt,a∗ t
) − h(xt,at)(cid:1) , t is the Bayesian-optimal classiﬁer on instance xt, that is, a∗ where a∗ t = arg maxa∈Y h(xt,a).
Additionally, we are interested in bounding the number of labels NT the algorithm decides to request.
Our goal is to simultaneously bound RT and NT with high probability over the generation of the sample {(xt, yt)}t=1,...,T .
Throughout this work, we consider the following common low-noise condition on the marginal distribution DX (Mammen-Tsybakov low noise condition [32]): There exist absolute constants c > 0, and α ≥ 0 such that for all (cid:15) ∈ (0, 1/2) we have
P(cid:0)|h((x, 0)) −
In particular, α = ∞ gives the so-called hard margin condition P(cid:0)|h((x, 0)) − 1 2 | < (cid:15)(cid:1) = 0. while, at the opposite extreme, exponent α = 0 (and c = 1) results in no assumptions whatsoever on DX .
For simplicity, we shall assume throughout that the above low-noise condition holds for1 c = 1.
| < (cid:15)(cid:1) ≤ c (cid:15)α . 1 2
Our techniques are inspired by the recent work [45] from which we also borrow some notation. We are learning the class of functions {h} by means of fully connected neural networks
√ f (x, θ) = mWnσ(...σ(W1x)) , where σ is a ReLU activation function σ(x) = max{0, x}, m is the width of the network and n ≥ 2 is its depth. In the above, θ ∈ Rp collectively denotes the set of weights {W1, W2, . . . , Wn} of the network, where p = m + 2md + m2(n − 2) is their number, and the input x at training time should be thought of as some xt,a ∈ X 2.
With any depth-n network and data points {xt,a}t=1,...,T, a=±1 we associate a depth-n NTK matrix as follows [24]. First, rename {xt,a}t=1,...,T, a=±1 as {x(i)}i=1,...,2T . Then deﬁne matrices (cid:101)H (1) = (cid:104)
H (1) i,j (cid:105)2T ×2T i,j=1
Σ(1) = (cid:104)
Σ(1) i,j (cid:105)2T ×2T i,j=1 with
H (1) i,j = Σ(1) i,j = (cid:104)x(i), x(j)(cid:105) , and then, for any k ≤ n and i, j = 1, . . . , 2T , introduce the bivariate covariance matrix (cid:34)
A(k) i,j = (cid:35)
Σ(k) i,i Σ(k) i,j Σ(k)
Σ(k) j,j i,j 1A more general formulation requires the above to hold only for (cid:15) ≤ (cid:15)0, where (cid:15)0 ∈ (0, 1/2) is a third parameter. We shall omit this extra parameter from our presentation. 3
by which we recursively deﬁne
Σ(k+1) i,j
= 2E (u,v)∼N (0,A(k) i,j )[σ(u)σ(v)] and (cid:101)H (k+1) i,j
= 2 (cid:101)H (k) i,j
E (u,v)∼N (0,A(k) i,j )[ 11{u ≥ 0} 11{v ≥ 0}] + Σ(k+1) i,j
.
The 2T × 2T -dimensional matrix H = 1 2 ( (cid:101)H (n) + Σ(n)) is called the Neural Tangent Kernel (NTK) matrix of depth n (and inﬁnite width) over the set of points {xt,a}t=1,...,T, a=±1. The reader is referred to [24] for more details on NTK.
In order to avoid heavy notation, we assume ||xt|| = 1 for all t. Matrix H is positive semi-deﬁnite by construction but, as is customary in the NTK literature (e.g., [2, 9, 17]), we assume it is actually positive deﬁnite (hence invertible) with smallest eigenvalue λ0 > 0. This is a mild assumption that can be shown to hold if no two vectors xt are aligned to each other.
We measure the complexity of the function h at hand in a way similar to [45]. Using the same rearrangement of {xt,a}t=1,...,T, a=±1 into {x(i)}i=1,...,2T as above, let h be the 2T -dimensional (column) vector whose i-th component is h(x(i)). Then, we deﬁne the complexity ST,n(h) of h over h(cid:62)H −1h . Notice that this notion of
{xt,a}t=1,...,T, a=±1 w.r.t. an NTK of depth n as ST,n(h) = (data-dependent) complexity is consistent with the theoretical ﬁndings of [2], who showed that for a two-layer network the bound on the generalization performance is dominated by y(cid:62)H −1y, where y is the vector of labels. Hence if y is aligned with the top eigenvectors of H the learning problem becomes easier. In our case, vector h plays the role of vector y. Also observe that S2
T,n(h) can in general be as big as linear in T (in which case learning becomes hopeless with our machinery). In the special case where h belongs to the RKHS induced by the NTK, one can upper bound ST,n(h) by the norm of h in the RKHS.
√
The complexity term ST,n(h) is typically unknown to the learning algorithm, and it plays a central role in both regret and label complexity guarantees. Hence the algorithm needs to learn this value as well during its online functioning. Apparently, this aspect of the problem has been completely overlooked by [45] (as well as by earlier references on contextual bandits in RKHS, like [12]), where a (tight) upper bound on ST,n(h) is assumed to be available in advance. We will cast the above as a model selection problem in active learning, where we adapt and largely generalize to active learning the regret balancing technique from [36, 35].
In what follows, we use the short-hand g(x; θ) = ∇θf (x, θ) and, for a vector g ∈ Rp and matrix (cid:112)g(cid:62)Zg as ||g||Z, so that ST,n(h) = ||h||H−1.
Z ∈ Rp×p, we often write 2.1