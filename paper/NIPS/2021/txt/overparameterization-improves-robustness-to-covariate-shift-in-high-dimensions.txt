Abstract
A signiﬁcant obstacle in the development of robust machine learning models is covariate shift, a form of distribution shift that occurs when the input distributions of the training and test sets differ while the conditional label distributions remain the same. Despite the prevalence of covariate shift in real-world applications, a theoretical understanding in the context of modern machine learning has remained lacking. In this work, we examine the exact high-dimensional asymptotics of ran-dom feature regression under covariate shift and present a precise characterization of the limiting test error, bias, and variance in this setting. Our results motivate a natural partial order over covariate shifts that provides a sufﬁcient condition for determining when the shift will harm (or even help) test performance. We
ﬁnd that overparameterized models exhibit enhanced robustness to covariate shift, providing one of the ﬁrst theoretical explanations for this ubiquitous empirical phe-nomenon. Additionally, our analysis reveals an exact linear relationship between the in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent observation. 1

Introduction
Theoretical justiﬁcation for almost all machine learning methods relies upon the equality of the distributions from which the training and test data are drawn. Nevertheless, in many real-world applications, this equality is violated—naturally-occurring distribution shift between the training data and the data encountered during deployment is the rule, not the exception [31]. Even non-adversarial changes in distributions can uncover the surprising fragility of modern machine learning models
[55, 56, 43, 26, 12, 50]. Such shifts are distinct from adversarial examples, which require explicit poisoning attacks [22]; rather, they can result from mild corruptions, ranging from changes of camera angle or blur [26], to subtle, unintended changes in data acquisition procedures [56]. Moreover, this fragility limits the application of deep learning in certain safety-critical areas [31].
Empirical studies of distribution shift have observed several intriguing phenomena, including linear trends between model performance on shifted and unshifted test distributions [56, 26, 31], dramatic degradation in calibration [50], and surprising spurious inductive biases [12]. Theoretical under-standing of why such patterns occur across a variety of real-world domains is scant. Even basic questions such as what makes a certain distribution shift likely to hurt (or help) a model’s performance,
⇤Equal contribution.
†Work performed while the author was at Google. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and by how much, are not understood. One reason that these phenomena have eluded theoretical understanding is that there is often a strong coupling between model and distribution, implying that the effect of a given shift cannot usually be understood in a model-agnostic way. Another reason that satisfactory explanations have remained lacking is that the go-to formalism for studying general-ization in classical models, namely uniform convergence theory (see e.g. [66]), may be insufﬁcient to explain the behavior of modern deep learning methods (even in the absence of distribution shift)
[47, 68]. Indeed, classical measures of model complexity, such as various norms of the parameters, have been found to lead to ambiguous conclusions [49].
In this paper, we follow a different approach: instead of focusing on worst-case bounds for generic distributions, we study average-case behavior for narrowly speciﬁed distributions. While this change in perspective sacriﬁces generality, it allows us to derive more precise predictions, which we believe are necessary to fully capture the relevant phenomenology. We study a speciﬁc type of distribution shift called covariate shift, in which the distributions of the training and test covariates differ, while the conditional distribution of the labels given the covariates remains ﬁxed. Using random matrix theory, we perform an asymptotically exact computation of the generalization error of random feature regression under covariate shift. The random feature model provides a useful testbed to (1) investigate the interplay between various factors such as model complexity, label noise, bias, variance, and covariate shift; (2) rigorously deﬁne a model-agnostic notion for the strength of covariate shift; and (3) provide a theoretical explanation for the linear relationships recently observed between in-distribution and out-of-distribution generalization performance [55, 56, 27]. 1.1 Contributions
Our primary contributions are to: 1. Provide a model-agnostic partial order over covariate shifts that is sufﬁcient to determine when a shift will increase or decrease the test error in random feature regression (see Def. 4.1); 2. Compute the test error, bias, and variance of random feature regression for general multivariate
Gaussian covariates under covariate shift in the high-dimensional limit (see Sec. 5.1); 3. Prove that overparameterization enhances robustness to covariate shift, and that the error, bias, and variance are nonincreasing functions of the number of excess parameters (see Sec. 5.3); 4. Deduce an exact linear relationship between in-distribution and out-of-distribution generalization performance, offering an explanation for this surprising recent empirical observation (see Sec. 5.4). 1.2