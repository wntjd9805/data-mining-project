Abstract
Cooperative multi-agent reinforcement learning (MARL) has received increasing attention in recent years and has found many scientiﬁc and engineering applica-tions. However, a key challenge arising from many cooperative MARL algorithm designs (e.g., the actor-critic framework) is the policy evaluation problem, which can only be conducted in a decentralized fashion. In this paper, we focus on decentralized MARL policy evaluation with nonlinear function approximation, which is often seen in deep MARL. We ﬁrst show that the empirical decentral-ized MARL policy evaluation problem can be reformulated as a decentralized nonconvex-strongly-concave minimax saddle point problem. We then develop a decentralized gradient-based descent ascent algorithm called GT-GDA that enjoys a convergence rate of O(1/T ). To further reduce the sample complexity, we pro-pose two decentralized stochastic optimization algorithms called GT-SRVR and
GT-SRVRI, which enhance GT-GDA by variance reduction techniques. We show that all algorithms all enjoy an O(1/T ) convergence rate to a stationary point of the reformulated minimax problem. Moreover, the fast convergence rates of GT-SRVR and GT-SRVRI imply O((cid:15)−2) communication complexity and O(m n(cid:15)−2) sam-ple complexity, where m is the number of agents and n is the length of trajectories.
To our knowledge, this paper is the ﬁrst work that achieves O((cid:15)−2) in both sample and communication complexities in decentralized policy evaluation for cooperative
MARL. Our extensive experiments also corroborate the theoretical results of our proposed decentralized policy evaluation algorithms.
√ 1

Introduction
In recent years, multi-agent reinforcement learning (MARL) has found important applications in many scientiﬁc and engineering ﬁelds, such as robotic network [45, 22, 41], sensor network [6, 40, 43], and power network [5, 11, 15, 16], just to name a few. In MARL, multiple agents observe the current joint state over a network, perform their own actions based on the current state, and transition to
∗Xin Zhang and Zhuqing Liu contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the next joint state. Each agent can only observe its local reward, which is a function of the joint state and actions. One important paradigm in MARL is the cooperative MARL, where the agents share a common goal to ﬁnd an optimal global policy to achieve the maximum global accumulative reward [63, 62, 13, 37]. As in classical reinforcement learning (RL) problems, a key component in cooperative MARL algorithms is the policy evaluation (PE) problem, whose goal is to evaluate the expected long-term accumulative reward for a given global policy. In cooperative MARL, agents share a common value function based on the joint states. As a result, the parameterization of the value function is the same across all agents. Examples of cooperative MARL include but are not limited to trafﬁc light control [56], autonomous driving [21], ﬁnancial trading [26]), etc. In cooperative MARL, the PE problem emerges as a key step for the agents to ﬁnd an optimal global policy in MARL tasks
[3, 23]. For example, in the actor-critic algorithmic framework for MARL, the actors conduct the policy improvement step, while the critics perform the policy evaluation step and estimates the value function. The overall actor-critic algorithm tries to ﬁnd an optimal policy by iterating between the policy evaluation and improvement steps. Thus, developing efﬁcient policy evaluation algorithms is critical to the success of RL algorithms based on the actor-critic framework.
However, developing efﬁcient PE algorithms for MARL is highly non-trivial. On one hand, the global accumulative reward is not directly observable in an MARL system. As a result, the PE problem of MARL can only be solved in a decentralized fahsion. On the other hand, modern MARL tasks have been increasingly complex and often not directly computable. As a result, MARL often use highly nonlinear parametric models (e.g., deep neural network (DNN)) for policy approximation.
In this paper, we focus on PE based on nonlinear function approximations due to the following reasons: 1) linear approximation schemes are based on their pre-deﬁned basis space, which may not be able to approximate the non-linear value function with high accuracy; 2) non-linear neural network approximation can handle the cases where the states space that is mixed with continuous and (inﬁnite) discrete state values; and 3) nonlinear neural network approximation usually have a better generalization performance than linear approximation [61], [20], [60]. However, it has been shown that the convergence performance of RL algorithms with nonlinear function approximations is not guaranteed [49].
In light of the growing importance of MARL, in this paper, we focus on addressing the above challenges. The key contributions of this paper are summarized as follows:
• To our knowledge, this work is the ﬁrst to investigate the decentralized PE (DPE) problem for
MARL with nonlinear function approximations. Via the Fenchel’s duality and local reward decomposition, we ﬁrst reformulate the DPE problem of MARL as a decentralized non-convex-strongly-concave minimax saddle point problem. To solve the minimax problem in a decentralized fashion, we propose a gradient-tracking-based gradient descent-ascent (GT-GDA) algorithm. We show that GT-GDA enjoys a convergence rate of O(1/T ), which leads to an O(mn(cid:15)−2) sample complexity and an O((cid:15)−2) communication complexity, where m is the number of agents, n is the data size, and (cid:15) is the convergence accuracy.
• To further reduce the sample complexity, we develop two variance-reduced algorithms, namely gradient-tracking stochastic recursive variance reduction (GT-SRVR) algorithm and its variant with incremental batch size (GT-SRVRI). We show that both algorithms achieve the same commu-n(cid:15)−2). nication complexity O((cid:15)−2) as GT-GDA, but requiring a lower sample complexity O(m
√
• It is worth noting that, in our theoretical analysis, we relax the commonly-used compactness conditions of the feasible set with some mild assumptions on objectives. Thus, the solutions found by our algorithms are exactly the stationary points for the original policy evaluation problem. This result may be of independent interest for general RL problems.
The rest of the paper is organized as follows. In Section 2, we ﬁrst provide the preliminaries of the
DPE problem of MARL and discuss related works. In Section 3, we ﬁrst introduce the GT-GDA algorithm, and then propose two stochastic variance reduced algorithms, namely GT-SRVR and
GT-SRVRI. We present their theoretical properties in Section 4. Section 5 provides numerical results to verify our theoretical ﬁndings, and Section 6 concludes this paper. 2 Problem formulation and related work
In this section, we ﬁrst introduce the DPE problem formulation in in Section 2.1. Then in Section 2.2, we review the recent developments of PE algorithms and compare them with our work. In Sec-2
tion 2.3, we highlight the challenges in designing efﬁcient DPE algorithms with nonlinear function approximation and the signiﬁcance of our work. 2.1 Problem formulation of decentralized policy evaluation for MARL (cid:80)m ss(cid:48), {Ri(s, a)}m
Consider a multi-agent network system G = (N , L), where N and L denote the sets of agents and edges, respectively, with |N | = m. In the system, the agents cooperatively perform a learning task.
The agents can communicate with each other through edges in L. An MARL problem is formulated based on the multi-agent Markov decision process (MDP) framework, which is characterized by a quintuple (S, A, P a i=1, ζ), where S and A are the state and action spaces, respectively; s ∈ S and a ∈ A are joint state and action; P a ss(cid:48) is the transition probability from state s to state s(cid:48) after taking action a; Ri(s, a) is the local reward received by agent i after taking action a in state s; and ζ ∈ (0, 1) is a discount factor. Both the joint state s and action a are available to all agents, while the local reward Ri is private to agent i. In a multi-agent system, the global reward function is deﬁned as the average of the local rewards 1 i=1 Ri(s, a). Moreover, a joint policy π speciﬁes sequential m decision rules for all agents. Policy π(a|s) is the conditional probability of taking joint action a given state s. The goal of PE is to estimate the value function of a given policy π, which is deﬁned as i=1 Ri(st, at)|s0, π(cid:3) , the long-term discounted accumulative reward: V π(s0) = E (cid:2) 1 where the expectation is taken over all possible state-action trajectories and initial states.
To determine V π(·), one of the most effective methods is the temporal-difference (TD) learning algo-rithm, which focuses on solving the Bellman equation for V π(·): V(s) = T πV(s) (cid:44) 1 i (s)+ m
ζ (cid:80) s(cid:48)∈S P π ss(cid:48) =
Ea∼π(·|s)P a ss(cid:48) is unknown in MARL and the size of the state space S could be inﬁnite. To address this challenge, a widely adopted approach is to approximate V π(·) by a function Vθ(·) parameterized by θ ∈ Rp. According to the formulation in [38, 50, 10, 9, 30], the Bellman equation can be solved by minimizing the following mean-squared projected bell-man error (MSPBE): MSPBE(θ) (cid:44) 1
, where Kθ = 2
Es∼dπ [∇θVθ(s)∇θVθ(s)(cid:62)] ∈ Rp×p and dπ is the stationary distribution of the MDP under policy π.
A−1 = maxy∈Rp 2(cid:104)x, y(cid:105) − y(cid:62)Ay, we can reformulate the MSPBE
From the Fenchel’s duality (cid:107)x(cid:107)2 minimization problem as the following primal-dual minimax problem: ss(cid:48)V(s(cid:48)), where T π denotes the Bellman operator, Rπ ss(cid:48) However, P π i=1 Rπ i (s) = Ea∼π(·|s)R(s, a) andP π (cid:1)∇θVθ(s)(cid:62)(cid:3)(cid:13) 2 (cid:13)
K−1
θ (cid:2)(cid:0)T πVθ(s) − Vθ t=0 ζ t (cid:80)m (cid:13) (cid:13)Es∼dπ (cid:80)∞ (cid:80)m m min
θ∈Rp max
ω∈Rp
L(θ, ω) (cid:44) E(cid:2)(cid:104)δ · ∇θVθ(s), ω(cid:105) − 1 2
ω(cid:62)[∇θVθ(s)∇θVθ(s)(cid:62)]ω(cid:3), (1) where the expectation is taken over s ∼ dπ(·), a ∼ π(·|s), s(cid:48) ∼ P a
ζVθ(s(cid:48)) − Vθ(s).
D = (cid:8)(st, at, {Ri(st, at)}n i=1, st+1)(cid:9)n sample average, we have the following empirical minimax problem: i=1 Ri(s, a) +
In practice, we only have access to a ﬁnite dataset with n-step trajectories t=0. By replacing the unknown expectation with the ﬁnite s·, and δ = 1 m (cid:80)m min
θ∈Rp max
ω∈Rp
F (θ, ω) = 1 n n (cid:88) t=1 (cid:104)δt · ∇θVθ(st), ω(cid:105) − 1 2
ω(cid:62) (cid:98)Kθω, (2) (cid:80)n (cid:80)m n i=1 Ri(st, at) + ζVθ(st+1) − Vθ(st) and (cid:98)Kθ (cid:44) 1 where δt (cid:44) 1 t=1 ∇θVθ(st)∇θVθ(st)(cid:62). m
In this paper, we assume that both Kθ and its empirical estimate ˆKθ are positive deﬁnite for all
θ. Deﬁne J(θ) (cid:44) F (θ, ω∗) = maxω∈Rp F (θ, ω), where ω∗ = arg maxω∈Rp F (θ, ω). J(θ) can be viewed as the ﬁnite empirical version of MSPBE. Here, we aim to minimize J(θ) by ﬁnding a stationary point of F (θ, ω). Recall that in MARL, the local reward is only observable for each indi-vidual agent. Thus, it is hard to obtain the global reward 1 i=1 Ri(st, at) and δt in a multi-agent m network. To address this challenge, we deﬁne δi,t = Ri(st, at) + ζVθ(st+1) − Vθ(st) and decom-(cid:80)m pose the minimax problem in (2) as follows: minθ∈Rp maxω∈Rp F (θ, ω) = 1 i=1 Fi(θ, ω) = m t=1 fit(θ, ω), where fit(θ, ω) (cid:44) (cid:104)δi,t ·∇θVθ(st), ω(cid:105)− 1 1 2 ω(cid:62)[∇θVθ(st) ∇θVθ(st)(cid:62)]ω. mn
We call this step as local reward decomposition. In cooperative MARL, a key challenge is that the
PE problem in (2) has to be solved in a decentralized fashion, which is due to the fact that i) the locally observed rewards are private and cannot be shared with the other agents/central server; ii) it is difﬁcult to set up a central sever in many MARL applications while decentralized setting is more
ﬂexible (e.g.,wireless network [59], UAV network [7]); and iii) the central server is vulnerable to cyber-attacks and would be a signiﬁcant communication bottleneck [57], [27]. To solve Problem (2) (cid:80)m (cid:80)m (cid:80)n i=1 3
in a decentralized fashion, we can rewrite it in the following equivalent form: min
{θi}m i=1 max
{ωi}m i=1 1 m m (cid:88)
Fi(θi, ωi) = m (cid:88) n (cid:88) fit(θi, ωi), 1 mn t=1 subject to θi = θj, ωi = ωj, ∀(i, j) ∈ L, i=1 i=1 (3) where θi and ωi are the local copies of the original primal-dual parameters at agent i. In (3), the equality constraint ensures that the local copies at all nodes are equal to each other, so the formulation is also referred to as the “consensus form.”
Clearly, Problems (2) and (3) are equivalent. For a ﬁxed θ, each local function Fi(θi, ·) is a strongly concave function of ω. For a ﬁxed ω, Fi(·, ω) is a non-convex function of θ. Thus, Problem (3) is a decentralized non-convex-strongly-concave minimax consensus optimization problem. In this paper, we adopt two complexity metrics that are widely used in the decentralized optimization literature (e.g., [47]) to measure the efﬁciency of an algorithm:
Deﬁnition 1 (Sample Complexity). The sample complexity is deﬁned as the total number of incre-mental ﬁrst-order oracle (IFO) calls required across all nodes until algorithm converges, where one
IFO call evaluates a pair of (fit(θ, ω), ∇fit(θ, ω)) at node i.
Deﬁnition 2 (Communication Complexity). The communication complexity is deﬁned as the total rounds of communications required until algorithm converges, where each node can send and receive a p-dimensional vector with its neighboring nodes in one communication round. 2.2