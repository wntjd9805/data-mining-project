Abstract
A good visual representation is an inference map from observations (images) to features (vectors) that faithfully reﬂects the hidden modularized generative factors (semantics). In this paper, we formulate the notion of “good” representation from a group-theoretic view using Higgins’ deﬁnition of disentangled representation [40], and show that existing Self-Supervised Learning (SSL) only disentangles simple augmentation features such as rotation and colorization, thus unable to modularize the remaining semantics. To break the limitation, we propose an iterative SSL algorithm: Iterative Partition-based Invariant Risk Minimization (IP-IRM), which successfully grounds the abstract semantics and the group acting on them into concrete contrastive learning. At each iteration, IP-IRM ﬁrst partitions the training samples into two subsets that correspond to an entangled group element. Then, it minimizes a subset-invariant contrastive loss, where the invariance guarantees to disentangle the group element. We prove that IP-IRM converges to a fully disentangled representation and show its effectiveness on various benchmarks.
Codes are available at https://github.com/Wangt-CN/IP-IRM. 1

Introduction
Deep learning is all about learning feature representa-tions [5]. Compared to the conventional end-to-end super-vised learning, Self-Supervised Learning (SSL) ﬁrst learns a generic feature representation (e.g., a network backbone) by training with unsupervised pretext tasks such as the pre-vailing contrastive objective [36, 16], and then the above stage-1 feature is expected to serve various stage-2 appli-cations with proper ﬁne-tuning. SSL for visual represen-tation is so fascinating that it is the ﬁrst time that we can obtain “good” visual features for free, just like the trending pre-training in NLP community [26, 8]. However, most
SSL works only care how much stage-2 performance an
SSL feature can improve, but overlook what feature SSL is learning, why it can be learned, what cannot be learned, what the gap between SSL and Supervised Learning (SL) is, and when SSL can surpass SL?
U
I
X
U
I
X
The crux of answering those questions is to formally un-derstand what a feature representation is and what a good one is. We postulate the classic world model of visual generation and feature representation [1, 69] as in Figure 1. Let U be a set of (unseen) semantics, e.g., attributes such as “digit” and “color”. There
Figure 1: Disentangled representation is an equivariant map between the semantic space
U and the vector space X , which is decom-posed into “color” and “digit”. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: (a) The heat map visualizes feature dimensions related to augmentations (aug. related) and unrelated to augmentations (aug. unrelated), whose respective classiﬁcation accuracy is shown in the bar chart below.
Dashed bar denotes the accuracy using full feature dimensions. Experiment was performed on STL10 [22] with representation learnt with SimCLR [16] and our IP-IRM. (b) Visualization of CNN activations [77] of 4 ﬁlters on layer 29 and 18 of VGG [75] trained on ImageNet100 [81]. The ﬁlters were chosen by ﬁrst clustering the aug. unrelated ﬁlters with k-means (k = 4) and then selecting the ﬁlters corresponding to the cluster centers. is a set of independent and causal mechanisms [66] ϕ : U → I, generating images from semantics, e.g., writing a digit “0” when thinking of “0” [74]. A visual representation is the inference process
φ : I → X that maps image pixels to vector space features, e.g., a neural network. We deﬁne semantic representation as the functional composition f : U → I → X . In this paper, we are only interested in the parameterization of the inference process for feature extraction, but not the generation process, i.e., we assume ∀I ∈ I, ∃u ∈ U, such that I = ϕ(u) is ﬁxed as the observation of each image sample. Therefore, we consider semantic and visual representations the same as feature representation, or simply representation, and we slightly abuse φ(I) := f (cid:0)ϕ−1(I)(cid:1), i.e., φ and f share the same trainable parameters. We call the vector x = φ(I) as feature, where x ∈ X .
We propose to use Higgins’ deﬁnition of disentangled representation [40] to deﬁne what is “good”.
Deﬁnition 1. (Disentangled Representation) Let G be the group acting on U, i.e., g · u ∈ U × U transforms u ∈ U, e.g., a “turn green” group element changing the semantic from “red” to “green”.
Suppose there is a direct product decomposition1 G = g1 × . . . × gm and U = U1 × . . . × Um, where gi acts on Ui respectively. A feature representation is disentangled if there exists a group G acting on
X such that: 1. Equivariant: ∀g ∈ G, ∀u ∈ U, f (g · u) = g · f (u), e.g., the feature of the changed semantic: “red” to “green” in U, is equivalent to directly change the color vector in X from “red” to “green”. 2. Decomposable: there is a decomposition X = X1 × . . . × Xm, such that each Xi is ﬁxed by the action of all gj, j (cid:54)= i and affected only by gi, e.g., changing the “color” semantic in U does not affect the “digit” vector in X .
Compared to the previous deﬁnition of feature representation which is a static mapping, the disentan-gled representation in Deﬁnition 1 is dynamic as it explicitly incorporate group representation [35], which is a homomorphism from group to group actions on a space, e.g., G → X × X , and it is common to use the feature space X as a shorthand—this is where our title stands.
Deﬁnition 1 deﬁnes “good” features in the common views: 1) Robustness: a good feature should be invariant to the change of environmental semantics, such as external interventions [45, 87] or domain shifts [32]. By the above deﬁnition, a change is always retained in a subspace Xi, while others are not affected. Hence, the subsequent classiﬁer will focus on the invariant features and ignore the ever-changing Xi. 2) Zero-shot Generalization: even if a new combination of semantics is unseen in training, each semantic has been learned as features. So, the metrics of each Xi trained by seen samples remain valid for unseen samples [95].
Are the existing SSL methods learning disentangled representations? No. We show in Section 4 that they can only disentangle representations according to the hand-crafted augmentations, e.g., color jitter and rotation. For example, in Figure 2 (a), even if we only use the augmentation-related feature, 1Note that gi can also denote a cyclic subgroup Gi such as rotation [0◦ : 1◦ : 360◦], or a countable one but treated as cyclic such as translation [(0, 0) : (1, 1) : (width, height)] and color [0 : 1 : 255]. 2
the classiﬁcation accuracy of a standard SSL (SimCLR [16]) does not lose much as compared to the full feature use. Figure 2 (b) visualizes that the CNN features in each layer are indeed entangled (e.g., tyre, motor, and background in the motorcycle image). In contrast, our approach IP-IRM, to be introduced below, disentangles more useful features beyond augmentations.
In this paper, we propose Iterative Partition-based Invariant Risk Minimization (IP-IRM [­ai"p@:m]) that guarantees to learn disentangled representations in an SSL fashion. We present the algorithm in Section 3, followed by the theoretical justiﬁcations in Section 4. In a nutshell, at each iteration,
IP-IRM ﬁrst partitions the training data into two disjoint subsets, each of which is an orbit of the already disentangled group, and the cross-orbit group corresponds to an entangled group element gi.
Then, we adopt the Invariant Risk Minimization (IRM) [2] to implement a partition-based SSL, which disentangles the representation Xi w.r.t. gi. Iterating the above two steps eventually converges to a fully disentangled representation w.r.t. (cid:81)m i=1 gi. In Section 5, we show promising experimental results on various feature disentanglement and SSL benchmarks. 2