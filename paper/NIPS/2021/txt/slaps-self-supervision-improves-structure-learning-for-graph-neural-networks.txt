Abstract
Graph neural networks (GNNs) work well when the graph structure is provided.
However, this structure may not always be available in real-world applications.
One solution to this problem is to infer a task-speciﬁc latent structure and then apply a GNN to the inferred graph. Unfortunately, the space of possible graph structures grows super-exponentially with the number of nodes and so the task-speciﬁc supervision may be insufﬁcient for learning both the structure and the GNN parameters. In this work, we propose the Simultaneous Learning of Adjacency and
GNN Parameters with Self-supervision, or SLAPS, a method that provides more supervision for inferring a graph structure through self-supervision. A compre-hensive experimental study demonstrates that SLAPS scales to large graphs with hundreds of thousands of nodes and outperforms several models that have been proposed to learn a task-speciﬁc graph structure on established benchmarks. 1

Introduction
Graph representation learning has grown rapidly and found applications in domains where a natural graph of the data points is available [4, 25]. Graph neural networks (GNNs) [40] have been a key component to the success of the research in this area. Speciﬁcally, GNNs have shown promising results for semi-supervised classiﬁcation when the available graph structure exhibits a high degree of homophily (i.e. connected nodes often belong to the same class) [57].
We study the applicability of GNNs to (semi-supervised) classiﬁcation problems where a graph structure is not readily available. The existing approaches for this problem either ﬁx a similarity graph between the nodes or learn the GNN parameters and a graph structure simultaneously (see