Abstract
Whittle index policy is a powerful tool to obtain asymptotically optimal solutions for the notoriously intractable problem of restless bandits. However, ﬁnding the
Whittle indices remains a difﬁcult problem for many practical restless bandits with convoluted transition kernels. This paper proposes NeurWIN, a neural Whittle index network that seeks to learn the Whittle indices for any restless bandits by leveraging mathematical properties of the Whittle indices. We show that a neural network that produces the Whittle index is also one that produces the optimal control for a set of Markov decision problems. This property motivates using deep reinforcement learning for the training of NeurWIN. We demonstrate the utility of NeurWIN by evaluating its performance for three recently studied restless bandit problems. Our experiment results show that the performance of NeurWIN is signiﬁcantly better than other RL algorithms. 1

Introduction
Many sequential decision problems can be modeled as multi-armed bandit problems. A bandit problem models each potential decision as an arm. In each round, we play M arms out of a total of N arms by choosing the corresponding decisions. We then receive a reward from the played arms. The goal is to maximize the expected long-term total discounted reward. Consider, for example, displaying advertisements on an online platform with the goal to maximize the long-term discounted click-through rates. This can be modeled as a bandit problem where each arm is a piece of advertisement and we choose which advertisements to be displayed every time a particular user visits the platform. It should be noted that the reward, i.e., click-through rate, of an arm is not stationary, but depends on our actions in the past. For example, a user that just clicked on a particular advertisement may be much less likely to click on the same advertisement in the near future. Such a problem is a classic case of the restless bandit problem, where the reward distribution of an arm depends on its state, which changes over time based on our past actions.
The restless bandit problem is notoriously intractable [20]. Most recent efforts, such as recovering bandits [21], rotting bandits [23], and Brownian bandits [24], only study some special instances of the restless bandit problem. The fundamental challenge of the restless bandit problem lies in the explosion of state space, as the state of the entire system is the Cartesian product of the states of individual arms. A powerful tool traditionally used to solve the RMABs’ decision-making problem 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is the Whittle index policy [30]. In a nutshell, the Whittle index policy calculates a Whittle index for each arm based on the arm’s current state, where the index corresponds to the amount of cost that we are willing to pay to play the arm, and then plays the arm with the highest index. When the indexability condition is satisﬁed, it has been shown that the Whittle index policy is asymptotically optimal in a wide range of settings.
In this paper, we present Neural Whittle Index Network (NeurWIN), a principled machine learning approach that ﬁnds the Whittle indices for virtually all restless bandit problems. We note that the
Whittle index is an artiﬁcial construct that cannot be directly measured. Finding the Whittle index is typically intractable. As a result, the Whittle indices of many practical problems remain unknown except for a few special cases.
We are able to circumvent the challenges of ﬁnding the Whittle indices by leveraging an important mathematical property of the Whittle index: Consider an alternative problem where there is only one arm and we decide whether to play the arm in each time instance. In this problem, we need to pay a constant cost of λ every time we play the arm. The goal is to maximize the long-term discounted net reward, deﬁned as the difference between the rewards we obtain from the arm and the costs we pay to play it. Then, the optimal policy is to play the arm whenever the Whittle index becomes larger than λ.
Based on this property, a neural network that produces the Whittle index can be viewed as one that
ﬁnds the optimal policy for the alternative problem for any λ.
Using this observation, we propose a deep reinforcement learning method to train NeurWIN. To demonstrate the power of NeurWIN, we employ NeurWIN for three recently studied restless bandit problems, namely, recovering bandit [21], wireless scheduling [1], and stochastic deadline scheduling
[34]. We compare NeurWIN against ﬁve other reinforcement learning algorithms and the application-speciﬁc baseline policies in the respective restless bandit problems. Experiment results show that the index policy using our NeurWIN signiﬁcantly outperforms other reinforcement learning algorithms.
Moreover, for problems where the Whittle indices are known, NeurWIN has virtually the same performance as the corresponding Whittle index policy, showing that NeurWIN indeed learns a precise approximation to the Whittle indices.
The rest of the paper is organized as follows: Section 2 reviews related literature. Section 3 provides formal deﬁnitions of the Whittle index and our problem statement. Section 4 introduces our training algorithm for NeurWIN. Section 5 demonstrates the utility of NeurWIN by evaluating its performance under three recently studied restless bandit problems. Finally, Section 6 concludes the paper. 2