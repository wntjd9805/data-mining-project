Abstract
When a deep learning model is deployed in the wild, it can encounter test data drawn from distributions different from the training data distribution and suffer drop in performance. For safe deployment, it is essential to estimate the accuracy of the pre-trained model on the test data. However, the labels for the test inputs are usually not immediately available in practice, and obtaining them can be expen-sive. This observation leads to two challenging tasks: (1) unsupervised accuracy estimation, which aims to estimate the accuracy of a pre-trained classiﬁer on a set of unlabeled test inputs; (2) error detection, which aims to identify mis-classiﬁed test inputs. In this paper, we propose a principled and practically effective frame-work that simultaneously addresses the two tasks. The proposed framework it-eratively learns an ensemble of models to identify mis-classiﬁed data points and performs self-training to improve the ensemble with the identiﬁed points. Theo-retical analysis demonstrates that our framework enjoys provable guarantees for both accuracy estimation and error detection under mild conditions readily satis-ﬁed by practical deep learning models. Along with the framework, we proposed and experimented with two instantiations and achieved state-of-the-art results on 59 tasks. For example, on iWildCam, one instantiation reduces the estimation error for unsupervised accuracy estimation by at least 70% and improves the F1 score for error detection by at least 4.7% compared to existing methods.
∗Part of the work done while interning at Google.
Our code is available at: https://github.com/jfc43/self-training-ensembles. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
Data distribution in the real world may be wildly different from the training dataset for various reasons such as covariate shift due to domain divergence, corruption of images due to weather con-ditions, or out-of-distribution test inputs. When facing these issues, a deployed deep learning model can have unexpected performance drop on the test data. This performance degradation can be miti-gated by test data annotation, which may be costly. Hence, estimating the accuracy of a pre-trained model on the unlabeled test data provides an alternative to avoid the cost when it is not necessary.
Furthermore, it is beneﬁcial to estimate the correctness of the predictions on individual points. This leads to an even more challenging task of error detection, which aims to identify points in the un-labeled test set that are mis-classiﬁed by the pre-trained model. Such a ﬁner-grained estimation can facilitate a further improvement of the pre-trained model (e.g., manually label those mis-classiﬁed data points and retrain the model on them).
While there have been previous attempts to address accuracy estimation or the broader problem of error detection, their successes usually rely on some conditions or assumptions that may not hold in practice. For example, a natural approach is to use conﬁdence based metrics to measure the performance of the pre-trained model, e.g., as in [9]. If the model is well-calibrated on the test data, then the average conﬁdence approximates its accuracy. However, it has been observed that many machine learning systems, in particular modern neural networks, are poorly calibrated, especially on test data with distribution shift [14, 30]. Another method is to learn a regression function that takes statistics about the model and the test data as input, and predicts the performance on the test data [9, 35]. This requires training on labeled data from various data distributions, which is very expensive or even impractical. Furthermore, the performance predictor trained on the labeled data may not generalize to unknown data distributions. Recent work by [4] proposes to learn a “check” model using domain-invariant representation and use it as a proxy for the unknown true test labels to estimate the performance via error detection. It relies on the success of the domain-invariant representation methods to obtain a highly accurate check model on the test data. Hence, the check model performance suffers when domain-invariant representation is not accurate in circumstances such as test data having outlier feature vectors or different class probabilities than the training data.
In this paper, we propose a principled and practically effective framework for the challenging tasks of accuracy estimation and error detection (Section 4). The framework makes a novel use of the self-training technique on ensembles for these tasks. It ﬁrst learns an ensemble of models to iden-tify some mis-classiﬁed points. Then it assigns pseudo-labels to these points and uses self-training with these pseudo-labeled data to identify more mis-classiﬁed points. We also provide provable guarantees for the framework (Section 5). Our analysis shows that it provably outputs an accurate estimate of the accuracy as well as the mis-classiﬁed points, under mild practical conditions: the ensemble make small errors on the test inputs correctly classiﬁed by the model f , mostly disagree with f on the current identiﬁed mis-classiﬁed inputs, and are correct or have diverse predictions on the remaining inputs. These conditions can be readily satisﬁed by deep learning model ensembles.
Furthermore, they have no explicit assumptions on the test distribution and thus the framework can be instantiated by incorporating properly designed ensemble methods for different settings (Sec-tion 6). Experimental results on 59 tasks over ﬁve dataset categories including image classiﬁcation and sentiment classiﬁcation datasets show that our method achieves state-of-the-art on both accu-racy estimation and error detection (Section 7). The experiments also provide positive support for our analysis, verifying the conditions and implications. 2