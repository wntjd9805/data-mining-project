Abstract
Recent work in scientiﬁc machine learning has developed so-called physics-informed neural network (PINN) models. The typical approach is to incorporate physical domain knowledge as soft constraints on an empirical loss function and use existing machine learning methodologies to train the model. We demonstrate that, while existing PINN methodologies can learn good models for relatively trivial problems, they can easily fail to learn relevant physical phenomena for even slightly more complex problems. In particular, we analyze several distinct situations of widespread physical interest, including learning differential equations with convection, reaction, and diffusion operators. We provide evidence that the soft regularization in PINNs, which involves PDE-based differential operators, can introduce a number of subtle problems, including making the problem more ill-conditioned. Importantly, we show that these possible failure modes are not due to the lack of expressivity in the NN architecture, but that the PINN’s setup makes the loss landscape very hard to optimize. We then describe two promising solutions to address these failure modes. The ﬁrst approach is to use curriculum regularization, where the PINN’s loss term starts from a simple PDE regularization, and becomes progressively more complex as the NN gets trained. The second approach is to pose the problem as a sequence-to-sequence learning task, rather than learning to predict the entire space-time at once. Extensive testing shows that we can achieve up to 1-2 orders of magnitude lower error with these methods as compared to regular PINN training. 1

Introduction
Partial differential equations (PDEs) are commonly used to describe different phenomena in science and engineering. These PDEs are often derived by starting from governing ﬁrst principles (e.g., conservation of mass or energy). It is typically not possible to ﬁnd analytical solutions to these PDEs for many real-world settings. Thus, many different numerical methods (e.g., the ﬁnite element method
[44], pseudo-spectral methods [9], etc.) have been introduced to approximate their solutions/behavior.
However, these PDEs can be quite complex for several settings (e.g., turbulence simulations), and numerical integration techniques, which typically update and improve a candidate solution iteratively until convergence, are often quite computationally expensive. Motivated by this—as well as the increasing quantities of data available in many scientiﬁc and engineering applications—there has been recent interest in developing machine learning (ML) approaches to ﬁnd the solution of the underlying PDEs (and/or work in tandem with numerical solutions). As a result, the area of Scientiﬁc
Machine Learning (SciML)—which aims to couple traditional scientiﬁc mechanistic modeling
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(typically, differential equations) with data-driven ML methodologies (most recently, neural network training)—has emerged. In this vein, there have been a number of ML approaches to incorporate scientiﬁc knowledge into such problems while keeping the automatic, data-driven estimates of the solution [2, 17, 33, 39].
A recent line of work involves Physics-Informed Neural Network (PINN) models, which aim to incorporate physical domain knowledge as soft constraints on an empirical loss function, that is then optimized using existing ML training methodologies. To some degree, PINNs are an example of “grafting together” domain-driven models and data-driven methodologies. However, there are important subtleties with this, and we identify several possible failure modes with a naive approach.
We then illustrate possible directions for addressing these failure modes.
Rd,