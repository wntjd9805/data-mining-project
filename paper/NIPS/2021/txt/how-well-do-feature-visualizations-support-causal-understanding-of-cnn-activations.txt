Abstract
A precise understanding of why units in an artiﬁcial network respond to certain stimuli would constitute a big step towards explainable artiﬁcial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated — an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog’s head) changes a unit’s activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit’s activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. [40] indeed help humans on this task (68 ± 4 % accuracy; baseline performance without any visualizations is 60 ± 3 %).
However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance (66±3 % to 67±3 % accuracy). Taken together, we propose an objective psychophysical task to quantify the beneﬁt of unit-level interpretability methods for humans, and ﬁnd no evidence that a widely-used feature visualization method provides humans with better “causal understanding” of unit activations than simple alternative visualizations. 1

Introduction
It is hard to trust a black-box algorithm, and it is hard to deploy an algorithm if one does not trust its output. Many of today’s best-performing machine learning models, deep convolutional neural networks (CNNs), are also among the most mysterious ones with regards to their internal information processing. CNNs typically consist of dozens of layers with hundreds or thousands of units that distributively process and aggregate information until they reach their ﬁnal decision at the topmost layer. Shedding light onto the inner workings of deep convolutional neural networks has been a long-standing quest that has so far produced more questions than answers.
One of the most popular tools for explaining the behavior of individual network units is to visualize unit responses via activation maximization [16, 33, 38, 35, 39, 36, 54, 15]. The idea is to start with an image (typically random noise) and iteratively change pixel values to maximize the activation 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: How useful are feature visualizations to interpret the effects of interventions? A: “Causal” synthetic feature visualizations. B: Human experiment. Given strongly activating reference images (e.g. synthetic or natural), a human participant chooses which out of two manipulated images activates a unit more. Note that this trial is made up — real trials are often more difﬁcult. C: Core result. While participants are above chance for all visualization types, synthetic images only provide a substantial advantage over no references and not over other alternatives such as natural references. of a particular network unit via gradient ascent. The resulting synthetic images, called feature visualizations, often show interpretable structures, and are believed to isolate and highlight exactly those features that “cause” a unit’s response [40, 50]. Some of the synthetic feature visualizations appear quite intuitive and precise. As shown in Fig. 1A, they might facilitate distinguishing whether, for example, a unit responds to just an eye or a whole dog’s face.
However, other aspects cast a more critical light on feature visualization’s “causality”: Generating these synthetic images typically involves regularization mechanisms [36, 33, 38, 35], which may inﬂuence how faithfully they visualize what “causes” a network unit’s activation. Furthermore, to obtain a complete description of a mathematical function, one generally needs more information than just knowing its extrema. In view of this, it is an open question how well a unit can be characterized by simply visualizing the arguments of its maxima. Finally, a crucial unknown factor is whether humans are able to obtain a causal understanding of CNN activations from these synthetic visualizations.
Given these points, we develop a psychophysical experiment to test whether feature visualizations by Olah et al. [40] indeed allow humans to gain a causal understanding of a unit’s behavior. Our task is based on the reasoning that being able to predict the effect of an intervention is at the heart of causal understanding. Understanding the causal relation between variables implies an understanding of how changes in one variable affect another one [45]. In our proposed experiment, this means that participants can predict the effect of an intervention — in form of an image manipulation — if they know the causal relation between image features and a unit’s activations. Our experiment tests whether synthetic feature visualizations indeed provide information about such causal relations.
Speciﬁcally, we ask humans which of two manipulated images activates a CNN unit more strongly.
The interventions we test are obtained by placing an occlusion patch at two different locations in an image. Taken together, this experiment probes the purported explanation method’s advantage of causality in a counterfactual-inspired prediction set-up [14].
Besides feature visualizations, other visualization methods have been used to gain an understanding of the inner workings of CNNs. In this experiment, we additionally test alternatives based on natural dataset examples and compare them with feature visualizations. This is particularly interesting because dataset examples are often assumed to provide less “causal” information about a unit’s response as they might contain misleading correlations [40]. To continue the example above, dog eyes usually co-occur with dog faces; thus, separating the inﬂuence of one image feature from the other one using natural exemplars might be challenging.
Our data shows that:
• Synthetic feature visualizations provide humans with some helpful information about the most important patch in an image — but not much more information than no visualizations at all.
• Dataset samples as well as other combinations and types of visualizations are similarly helpful.
• How easily the most important patch is identiﬁable depends on the unit, the images as well as the relative activation strength attributed to the patch. 2
2