Abstract
Continuous deep learning architectures enable learning of ﬂexible probabilistic models for predictive modeling as neural ordinary differential equations (ODEs), and for generative modeling as continuous normalizing ﬂows. In this work, we design a framework to decipher the internal dynamics of these continuous depth models by pruning their network architectures. Our empirical results suggest that pruning improves generalization for neural ODEs in generative modeling. We empirically show that the improvement is because pruning helps avoid mode-collapse and ﬂatten the loss surface. Moreover, pruning ﬁnds efﬁcient neural ODE representations with up to 98% less parameters compared to the original network, without loss of accuracy. We hope our results will invigorate further research into the performance-size trade-offs of modern continuous-depth models. 1

Introduction
The continuous analog of normalizing ﬂows (CNFs) (Chen et al., 2018) efﬁciently (Grath-wohl et al., 2019) maps a latent space to data by ordinary differential equations (ODEs), re-laxing the strong constraints over discrete nor-malizing ﬂows (Dinh et al., 2016, Durkan et al., 2019, Huang et al., 2020, Kingma and Dhari-wal, 2018, Papamakarios et al., 2017, Rezende and Mohamed, 2015). CNFs enable learning
ﬂexible models by unconstrained neural net-works. While recent works investigated ways to improve CNFs’ efﬁciency (Finlay et al., 2020,
Grathwohl et al., 2019, Li et al., 2020), regu-larize the ﬂows (Onken et al., 2020, Yang and
Karniadakis, 2020), or solving their shortcomings such as crossing trajectories (Dupont et al., 2019,
Massaroli et al., 2020), less is understood about their inner dynamics during and post training.
Figure 1: Pruning Neural ODEs improves their generalization with at least 1 order of magnitude less parameters. CIFAR-10 density estimation.
Values and methods are described in Table 3.
In this paper, we set out to use standard pruning algorithms to investigate generalization properties of sparse neural ODEs and continuous normalizing ﬂows. In particular, we investigate how the inner dynamics and the modeling performance of a continuous ﬂow varies if we methodologically prune its neural network architecture. Reducing unnecessary weights of a neural network (pruning) (Han et al., 2015b, Hassibi and Stork, 1993, LeCun et al., 1990, Li et al., 2016) without loss of accuracy results in smaller network size (Hinton et al., 2015, Liebenwein et al., 2021a,b), computational efﬁciency (Luo et al., 2017, Molchanov et al., 2019, Yang et al., 2017), faster inference (Frankle and Carbin, 2019), and enhanced interpretability (Baykal et al., 2019a,b, Lechner et al., 2020a, Liebenwein et al., 2020). Here, our main objective is to better understand CNFs’ dynamics in density estimation tasks as we increase network sparsity and to show that pruning can improve generalization in neural ODEs.
∗denotes authors with equal contributions. Code: https://github.com/lucaslie/torchprune 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: Pruning enhances generaliza-tion of continuous-depth models. Struc-tured pruning (green), unstructured prun-ing (blue). More details in Section 4.
Pruning improves generalization in neural ODEs.
Our results consistently suggest that a certain ratio of pruning of fully connected neural ODEs leads to lower empirical risk in density estimation tasks, thus obtaining better generalization. We validate this observation on a large series of experiments with increasing dimensionality.
See an example here in Figure 2.
Pruning ﬂattens the loss surface of neural ODEs. Ad-ditionally, we conduct a Hessian-based empirical investi-gation on the objective function of the ﬂows-under-test in density estimation tasks to better understand why pruning results in better generalization. We ﬁnd that for Neural
ODEs, pruning decreases the value of the Hessian’s eigen-values, and as a result, ﬂattens the loss which leads to better generalization, c.f., Keskar et al. (2017) (Figure 3).
Pruning helps avoiding mode-collapse in generative modeling. In a series of multi-modal density estimation tasks, we observe that densely connected CNFs often get stuck in a sharp local minimum (See Figure 3) and as a result, cannot properly distinguish different modes of data.
This phenomena is known as mode-collapse. Once we sparsify the ﬂows, the quality of the density estimation task increases signiﬁcantly and consequently mode-collapse does not occur.
Pruning ﬁnds minimal and efﬁcient neural ODE rep-resentations. Our framework ﬁnds highly optimized and efﬁcient neural ODE architectures via pruning. In many instances, we can reduce the parameter count by 70-98% (6x-50x compression rate). Notably, one cannot directly train such sparse and efﬁcient continuous-depth models from scratch.
Figure 3: Flat minima result in better generalization compared to sharp min-ima. Pruning neural ODEs ﬂattens the loss around local minima. Figure is re-produced from Keskar et al. (2017). 2