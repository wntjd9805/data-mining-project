Abstract
Learning efﬁciently from small amounts of data has long been the focus of model-based reinforcement learning, both for the online case when interacting with the environment and the ofﬂine case when learning from a ﬁxed dataset. However, to date no single uniﬁed algorithm has demonstrated state-of-the-art results in both settings. In this work, we describe the Reanalyse algorithm which uses model-based policy and value improvement operators to compute new improved training targets on existing data points, allowing efﬁcient learning for data budgets varying by several orders of magnitude. We further show that Reanalyse can also be used to learn entirely from demonstrations without any environment interactions, as in the case of ofﬂine Reinforcement Learning (ofﬂine RL). Combining Reanalyse with the
MuZero algorithm, we introduce MuZero Unplugged, a single uniﬁed algorithm for any data budget, including ofﬂine RL. In contrast to previous work, our algorithm does not require any special adaptations for the off-policy or ofﬂine RL settings.
MuZero Unplugged sets new state-of-the-art results in the RL Unplugged ofﬂine
RL benchmark as well as in the online RL benchmark of Atari in the standard 200 million frame setting. 1

Introduction
Ofﬂine reinforcement learning holds the promise of learning useful policies from many existing real-world datasets in a wide range of important problems such as robotics, healthcare or education (Levine et al., 2020). Learning effectively from ofﬂine data is crucial for such tasks where interaction with the environment is costly or comes with safety concerns, but a large amount of logged and other ofﬂine data is often available.
A wide variety of effective reinforcement learning (RL) algorithms for the online case have been described, achieving impressive results in video games (Mnih et al., 2015), robotic control (Akkaya et al., 2019) and many other problems. However, applying these online RL algorithms to ofﬂine data often remains challenging due to off-policy issues, with the best results in ofﬂine RL so far obtained by specialised ofﬂine algorithms (Kumar et al., 2020; Wang et al., 2020; Agarwal et al., 2020). At the same time, model-based reinforcement learning (RL) has long focused on learning efﬁciently from little data, even going as far as learning completely within a model of the environment (Hafner et al., 2018) - an approach ideally suited for ofﬂine RL.
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
So far, these developments have been relatively independent, with no uniﬁed algorithm that could achieve state-of-the art results in both the online and ofﬂine settings.
In this paper, we describe the Reanalyse algorithm, a simple yet effective technique for policy and value improvement at any data budget, including the fully ofﬂine case. A preliminary version of
Reanalyse was brieﬂy introduced in the context of MuZero (Schrittwieser et al., 2020), but limited to data efﬁciency improvements in the discrete action case. Here, we delve deeper into the algorithm and push its capabilities much further – ultimately to the point where most or all of the data is reanalysed.
Starting with the possible uses of Reanalyse, we show how it can be used for data efﬁcient learning and ofﬂine RL, leading to MuZero Unplugged. We demonstrate its effectiveness for the online case through results on Atari and for the ofﬂine case through results on the RL Unplugged benchmark for
Atari and DM Control. 2