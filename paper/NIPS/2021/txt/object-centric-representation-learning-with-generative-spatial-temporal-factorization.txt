Abstract
Learning object-centric scene representations is essential for attaining structural understanding and abstraction of complex scenes. Yet, as current approaches for unsupervised object-centric representation learning are built upon either a stationary observer assumption or a static scene assumption, they often: i) suf-fer single-view spatial ambiguities, or ii) infer incorrectly or inaccurately object representations from dynamic scenes. To address this, we propose Dynamics-aware Multi-Object Network (DyMON), a method that broadens the scope of multi-view object-centric representation learning to dynamic scenes. We train Dy-MON on multi-view-dynamic-scene data and show that DyMON learns—without supervision—to factorize the entangled effects of observer motions and scene object dynamics from a sequence of observations, and constructs scene object spatial representations suitable for rendering at arbitrary times (querying across time) and from arbitrary viewpoints (querying across space). We also show that the factorized scene representations (w.r.t. objects) support querying about a single object by space and time independently. 1

Introduction
Object-centric representation learning promises improved interpretability, generalization, and data-efﬁcient learning on various downstream tasks like reasoning (e.g. [18, 39]) and planning (e.g. [30, 2, 41]). It aims at discovering compositional structures around objects from the raw sensory input data, i.e. a binding problem [12], where the segregation (i.e. factorization) is the major challenge (e.g. [9, 4]), especially in cases of no supervision. In the context of visual data, most existing focus has been on single-view settings, i.e. decomposing and representing 3D scenes based on a single 2D image [1, 10, 26] or a ﬁxed-view video [23]. These methods often suffer from single-view spatial ambiguities and thus show several failures or inaccuracies in representing 3D scene properties. It was demonstrated by Nanbo et al. [31] that such ambiguities could be effectively resolved by multi-view information aggregation. However, current multi-view models are built upon a foundational static-scene assumption. As a result, they: 1) require static-scene data for training and 2) cannot handle well dynamic scenes where the spatial structures evolve over time. This greatly harms a model’s potentials in real-world applications.
In this work, we target an unexplored problem—unsupervised object-centric latent representation learning in multi-view-dynamic-scene scenarios. Despite the importance of the problem to spatial-35th Conference on Neural Information Processing Systems (NeurIPS 2021).
temporal understanding of 3D scenes, solving it presents several technical challenges. Consider one particularly interesting scenario where both an observer (e.g. a camera) and the objects in the scene are moving at the same time. To aggregate 3D object information from two consecutive observations, an agent needs not only to handle the cross-view object correspondence problem [31] but also to reason about the independent effects of the scene dynamics and observer motions. One can consider the aggregation as a process of answering two questions: “how much has an object really changed in the 3D space” and “what previous spatial unclarity can be clariﬁed by the current view”. In this paper, we refer to the relationship between the scene spatial structures and the viewpoints as the temporal entanglement because the temporal dependence of them complicates the identiﬁcation of the independent generative mechanism [35].
We introduce DyMON (Dynamics-aware Multi-Object Network), a uniﬁed unsupervised framework for multi-view object-centric representation learning. Instead of making a strong assumption of static scenes as that in previous multi-view methods, we only make two weak assumptions about the training scenes: i) observation sequences are taken at a high frame rate, and ii) there exists a signiﬁcant difference between the speed of the observer and the objects (see Sec. 3). Under these two assumptions, in a short period, we can transition a multi-view-dynamic-scene problem to a multi-view-static-scene problem if an observer moves faster than a scene evolves, or to a single-view-dynamic-scene problem if a scene evolves faster than an observer moves. These local approximations allow DyMON to learn independently the generative relationships between scenes and observations, and viewpoints and observations during training, which further enable DyMON to address the problem of scene spatial-temporal factorization, i.e. solving the observer-scene temporal entanglement and scene object decomposition, at test time.
Through the experiments we demonstrate that: (i) DyMON represents the ﬁrst unsupervised multi-view object-centric representation learning work in the context of dynamic-scene settings that can train and perform object-oriented inference on multi-view-dynamic-scene data (see Sec. 5). (ii)
DyMON recovers the independent generative mechanism of an observer and scene objects from observations and permits querying predictions of scene appearances and segmentations across both space and time (see Sec. 5.1). (iii) As DyMON learns scene representations that are factorized in terms of objects, DyMON allows single-object manipulation along both the space (i.e. viewpoint) and time axis—e.g. replays dynamics of a single object without interferring the others (see Sec. 5.1). 2