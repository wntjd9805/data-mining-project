Abstract log d nε
We study differentially private stochastic optimization in convex and non-convex settings. For the convex case, we focus on the family of non-smooth generalized linear losses (GLLs). Our algorithm for the ℓ2 setting achieves optimal excess population risk in near-linear time, while the best known differentially private algorithms for general convex losses run in super-linear time. Our algorithm for the
ℓ1 setting has nearly-optimal excess population risk ˜O
, and circumvents the dimension dependent lower bound of [AFKT21] for general non-smooth convex losses. In the differentially private non-convex setting, we provide several new algorithms for approximating stationary points of the population risk. For the
ℓ1-case with smooth losses and polyhedral constraint, we provide the ﬁrst nearly dimension independent rate, ˜O in linear time. For the constrained ℓ2-case with smooth losses, we obtain a linear-time algorithm with rate ˜O
. (cid:0)
Finally, for the ℓ2-case we provide the ﬁrst method for non-smooth weakly convex (cid:0) (cid:1) stochastic optimization with rate ˜O n1/4 + d which matches the best existing non-private algorithm when d = O(√n). We also extend all our results above for the non-convex ℓ2 setting to the ℓp setting, where 1 < p 2, with only polylogarithmic (in the dimension) overhead in the rates. n1/3 + d 2/3 d log (nε)1/3 1/6 (nε)1/3 1/5 (nε)2/5 (cid:0)q
≤ (cid:1) (cid:0) (cid:1) (cid:1) 1 1 1

Introduction
Stochastic optimization (SO) is a fundamental and pervasive problem in machine learning, statistics and operations research. Here, the goal is to minimize the expectation of a loss function (often referred to as the population risk), given only access to a sample of i.i.d. draws from a distribution. When such a sample entails privacy concerns, differential privacy (DP) becomes an important algorithmic desideratum.
Consequently, differentially private stochastic optimization (DP-SO) has been actively investigated for over a decade. Despite major progress in this area, some crucial problems remain with existing 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Loss
ℓp-Setting
Convex GLL (Nonsmooth) p = 1 p = 2
Rate log d nε q max
√d nε , 1
√n
Nonconvex Smooth p = 1 (cid:16) 2/3 d log (nε)1/3 (cid:17) 1 < p 2
≤ 2/3
κ n1/3 + κ2/3 1/5 d˜κ n2ε2 1/6
Linear Time?
Thm.
Nearly 6 5 8 10
Weakly Convex (Nonsmooth) n1/4 + κ4/3
Table 1: Accuracy bounds and running time for our algorithms. Here, n is sample size, d is dimension, 1(p < 2). We omit the
ε, δ are the privacy parameters, κ = min 1 , log d dependence on factors of order polylog(n, 1/δ). Bounds shown for unit ℓp ball as a feasible set. (cid:16) and ˜κ = 1 + log d d˜κ n2ε2 16
≤
≤ (cid:17) p 1
} 2
{ (cid:0) (cid:1)
−
κ p 1
· 5/4 methods. One major problem is the lack of linear-time1 algorithms for nonsmooth DP-SO (even in the convex case), whereas its non-private counterpart has minimax optimal-risk algorithms which make a single pass over the data [NY83]. A second challenge arises in DP-SCO for non-Euclidean settings; i.e., when the diameter of the feasible set, and Lipschitzness and/or smoothness of losses are measured w.r.t. a non-Euclidean norm (e.g., ℓp norm). In particular, in the ℓ1-setting there is a stark contrast between the polylogarithmic dependence on the dimension in the risk achievable for the smooth case and the necessary polynomial dependence on the dimension in the non-smooth case
[AFKT21].
Finally, our understanding of DP-SO in the non-convex case is still quite limited. In the non-convex domain, there are only a few prior results, all of which have several limitations. First, all existing works either assume that the optimization problem is unconstrained or only consider the empirical version of the problem known as differentially private empirical risk minimization (DP-ERM).
Obtaining population guarantees based on the empirical risk potentially limits the applicability of the existing methods either in terms of accuracy or in terms of computational efﬁciency. In particular, all existing methods require super-linear running time w.r.t. the dataset size. Second, most of the existing works consider only the Euclidean setting.2 Finally, none of the prior works have studied non-convex DP-SO when the loss is non-smooth.
The goal of this work is to provide faster and more accurate methods for DP-SO. Some of the settings we investigate are also novel in the DP literature. 1.1 Our Results
We enumerate the different settings we investigate in DP-SO, together with our main contributions.
Convex generalized linear losses. Our ﬁrst case of the study is non-smooth DP-SCO in the case of generalized linear losses (GLL). This model encompasses a broad class of problems, particularly those which arise in supervised learning, making it a very important particular case. Here, our contributions are two-fold. First, in the ℓ2-setting, we provide the ﬁrst nearly linear-time algorithm that attains the optimal excess risk. The fastest existing methods with similar risk work for general convex losses, but they run in superlinear time w.r.t. sample size [AFKT21, KLL21]. Our second contribution here is a nearly-dimension independent excess risk bound in the ℓ1-setting3 for convex non-smooth GLL. This result circumvents a general DP-SCO excess risk lower bound in the non-smooth ℓ1-setting which shows polynomial dependence on the dimension [AFKT21], and it matches the minimax risk in the non-private case when ε = Θ(1) [ABRW12]. 1In this work, complexity is measured by the number of gradient evaluations, omitting other operations. This is in line with the oracle complexity model in optimization [NY83]. 2One exception is [WX19] who study the ℓ1 setting in the context of DP-ERM under a fairly strong assumption (see