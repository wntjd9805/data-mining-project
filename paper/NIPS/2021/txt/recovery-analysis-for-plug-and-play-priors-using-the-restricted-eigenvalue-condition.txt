Abstract
The plug-and-play priors (PnP) and regularization by denoising (RED) methods have become widely used for solving inverse problems by leveraging pre-trained deep denoisers as image priors. While the empirical imaging performance and the theoretical convergence properties of these algorithms have been widely investi-gated, their recovery properties have not previously been theoretically analyzed.
We address this gap by showing how to establish theoretical recovery guarantees for PnP/RED by assuming that the solution of these methods lies near the ﬁxed-points of a deep neural network. We also present numerical results comparing the recovery performance of PnP/RED in compressive sensing against that of recent compressive sensing algorithms based on generative models. Our numerical results suggest that PnP with a pre-trained artifact removal network provides signiﬁcantly better results compared to the existing state-of-the-art methods. 1

Introduction
×
∈
∈
Rm
Rn from noisy measurements
Many imaging problems—such as denoising, inpainting, and super-resolution—can be formulated as an inverse problem involving the recovery of an image x∗ y = Ax∗ + e , n is the measurement operator and e (1)
Rm is the noise. Compressed sensing where A (CS) [1, 2] is a related class of inverse problems that seek to recover a sparse vector x∗ from m < n measurements. The sparse recovery is possible under certain assumptions on the measurement matrix, such as the restricted isometry property (RIP) [1] or the restricted eigenvalue condition (REC) [3, 4]. While traditional CS recovery relies on sparsity-promoting priors, recent work on compressed sensing using generative models (CSGM) [5] has broadened this perspective to priors speciﬁed through pre-trained generative models. CSGM has prompted a large amount of follow-up work on the design and theoretical analysis of algorithms that can leverage generative models as priors for image recovery [6–9].
∈
Plug-and-play priors (PnP) [10, 11] and regularization by denoising (RED) [12] are two methods related to CSGM that can also leverage pre-trained deep models as priors for inverse problems.
However, unlike CSGM, the regularization in PnP/RED is not based on restricting the solution to the range of a generative model, but rather on denoising the iterates with an existing additive white
Gaussian noise (AWGN) removal method. The effectiveness of PnP/RED has been shown in a number of inverse problems [13–18], which has prompted researchers to investigate the theoretical properties and interpretations of PnP/RED algorithms [19–30]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite the rich literature on both PnP/RED and CSGM, the conceptual relationship between these two classes of methods has never been formally investigated. In particular, while PnP/RED algorithms enjoy computational advantages over CSGM by not requiring nonconvex projections onto the range of a generative model, they lack theoretical recovery guarantees available for CSGM. In this paper, we address this gap by presenting the ﬁrst recovery analysis of PnP/RED under the assumptions of CSGM.
We show that if a measurement matrix satisﬁes a variant of REC from [5] over the range of a denoiser, then the distance of the PnP solutions to the true x∗ can be explicitly characterized. We also present conditions under which the solutions of both PnP and RED coincide, providing sufﬁcient conditions for the exact recovery of x∗ using both methodologies. Our results highlight that the regularization in PnP/RED is achieved by giving preference to images near the ﬁxed points of pre-trained deep neural networks. Besides new theory, this paper also presents numerical results directly comparing the recovery performance of PnP/RED against the recent algorithms in compressed sensing from random projections and subsampled Fourier measurements. These numerical results lead to new insights highlighting the excellent recovery performance of both PnP and RED, as well as the beneﬁt of using priors speciﬁed as pre-trained artifact removal (AR) operators rather than AWGN denoisers.
All proofs and some technical details that have been omitted for space appear in the Supplement, which also provides more background and simulations. The code for our numerical evaluation is available at: https://github.com/wustl-cig/pnp-recovery. 2