Abstract
We present a novel approach for tracking multiple people in video. Unlike past approaches which employ 2D representations, we focus on using 3D representations of people, located in three-dimensional space. To this end, we develop a method,
Human Mesh and Appearance Recovery (HMAR) which in addition to extracting the 3D geometry of the person as a SMPL mesh, also extracts appearance as a texture map on the triangles of the mesh. This serves as a 3D representation for appearance that is robust to viewpoint and pose changes. Given a video clip, we
ﬁrst detect bounding boxes corresponding to people, and for each one, we extract 3D appearance, pose, and location information using HMAR. These embedding vectors are then sent to a transformer, which performs spatio-temporal aggregation of the representations over the duration of the sequence. The similarity of the resulting representations is used to solve for associations that assigns each person to a tracklet. We evaluate our approach on the Posetrack, MuPoTs and AVA datasets.
We ﬁnd that 3D representations are more effective than 2D representations for tracking in these settings, and we obtain state-of-the-art performance. Code and results are available at: https://brjathu.github.io/T3DP 1

Introduction
Humans are three-dimensional beings, who live and move in a three-dimensional space. However, monocular people tracking algorithms from the computer vision community, e.g., (4; 7; 12; 28; 36; 41; 46) are typically based on 2D representations of people in 2D space. This made a lot of sense in the 1990s when people tracking ﬁrst came to the fore as (1) the application context was often video surveillance, where people are at low resolutions (2) there were not reliable algorithms for lifting people from 2D to 3D. But today neither of these limitations hold. Applications of video analysis can use HD video (1080p or better), and we have excellent technology e.g., (17) for 3Dfying people.
Figure 1 illustrates why tracking people might be easier in 3D than in 2D. Even when two people overlap and occlude each other in 2D, their spatial location is typically separable in 3D. While 2D appearance features are sensitive to viewpoint and body pose, 3D appearance features are less affected by viewpoint and pose changes. These properties enable robust tracking through people-to-people occlusion and even shot changes, a common phenomena in edited media and online videos.
Tracking people in 3D also opens up many downstream tasks such as predicting 3D human motion from video (18; 21), predicting their behavior (11; 48), and imitating human behavior from video (31).
These tasks involve videos of people other than pedestrians, such as videos of people performing sports, dance, daily activities (10), or even edited media such as movies or TV shows (13; 30).
Our input is a monocular video with detected bounding boxes. On each bounding box, we develop an approach that extracts both 3D geometry and appearance information of the person. Speciﬁcally, we extend HMR (17), a method that predicts 3D human mesh from monocular images, to also extract the appearance information of the person by predicting a ﬂow ﬁeld that estimates the texture map, which is a viewpoint and pose invariant space. We refer to this model as Human Mesh and
Appearance Recovery (HMAR). From HMAR output, we extract 3D appearance, 3D pose, joints, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The beneﬁts of tracking people in 3D. Left: Using the 3D location of people makes it easier to separate them than using their 2D location or 2D bounding boxes. Right: The appearance of people, when computed in a 3D representation (texture map) is less affected by changes in viewpoint and pose. and the estimate of their 3D location as separate features. We then aggregate this per-bounding box 3D information across time and space via a transformer (38) that takes these separate 3D cues as input. The transformer acts as a spatio-temporal diffusion mechanism that can propagate information across similar features by means of attention. Finally, we associate the diffused features across time to assign identities to each bounding box, which produces the ﬁnal tracklets. The overview of our approach is shown in Figure 2.
We evaluate our algorithm on three different datasets: PoseTrack (1), MuPots (27) and AVA (13).
AVA is a particularly interesting dataset as it is derived from movies, which include shot changes.
These exhibit much greater variety of behavior than videos in the traditional tracking challenges such as MOT. In addition, people are larger (in terms of number of image pixels), which is essential for the 3D approach to work. We ﬁnd the following: 1. 3D representations are more effective than 2D representations, 2. Ablation studies show that using 3D information in both appearance and location is helpful, 3. We outperform current state of the art on these datasets. 2