Abstract
The problem of ﬁnding the unique low dimensional decomposition of a given matrix has been a fundamental and recurrent problem in many areas.
In this paper, we study the problem of seeking a unique decomposition of a low rank matrix Y ∈ Rp×n that admits a sparse representation. Speciﬁcally, we consider Y = AX ∈ Rp×n where the matrix A ∈ Rp×r has full column rank, with r < min{n, p}, and the matrix X ∈ Rr×n is element-wise sparse.
We prove that this sparse decomposition of Y can be uniquely identiﬁed by recovering ground-truth A column by column, up to some intrinsic signed permutation. Our approach relies on solving a nonconvex optimization problem constrained over the unit sphere. Our geometric analysis for the nonconvex optimization landscape shows that any strict local solution is close to the ground truth solution, and can be recovered by a simple data-driven initialization followed with any second order descent algorithm. At last, we corroborate these theoretical results with numerical experiments.

Introduction 1
The problem of matrix decomposition has been a popular and fundamental topic under extensive investigations across several disciplines, including signal processing, machine learning, natural language processing [10, 11, 31, 46, 32, 8]. From the decomposition, one can construct eﬃcient representation of the original data matrix. However, for any matrix
Y ∈ Rp×n that can be factorized as a product of two matrices A ∈ Rp×r and X ∈ Rr×n, there exist inﬁnitely many decompositions, simply because one can use any r × r invertible matrix Q to construct A′ = AQ and X ′ = Q−1X such that Y = AX = A′X ′, while
A′ 6= A and X ′ 6= X. Thus, in various applications, additional structures and priors are be-ing exploited to ﬁnd a preferred representation [22, 15]. For example, principal component analysis (PCA) aims to ﬁnd orthogonal representations which retain as much variations in
Y as possible [17, 23], whereas independent component analysis (ICA) targets the repre-sentations of statistically independent non-Gaussian signals [26].
In this paper, we are interested in ﬁnding a unique sparse low-dimensional representation of Y . To this end, we study the decomposition of a low rank matrix Y ∈ Rp×n that satisﬁes
Y = AX, (1.1) where A ∈ Rp×r is an unknown deterministic matrix, with r < min{n, p}, and X ∈ Rr×n is an unknown sparse matrix.
Formulation (1.1) is an important model problem in many applications. As columns of
Y are viewed as linear combinations of columns of A with X being the sparse coeﬃcient, (1.1) can be used to form overlapping clusters of the n columns of Y via the support of
X with columns of A being viewed as r cluster centers [12, 7]. When we form a p × r low-dimensional representation of Y via sparse combinations, this greatly enhance the in-terpretability of the resulting representations [28, 21, 4], in the same spirit as the sparse
PCA, but (1.1) generalizes to the factorization of non-orthogonal matrices. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To motivate our approach, we ﬁrst consider the simple case that A has orthonormal columns, namely, AT A = Ir 1. Then it is easy to see that the sparse coeﬃcient matrix X is recovered by multiplying Y on the left by AT ,
The problem of ﬁnding such orthonormal matrix A boils down to successively ﬁnding a unit-norm direction q that renders qT Y as sparse as possible [34, 39, 35],
AT Y = AT AX = X. (1.2) min q qT Y sparsity s. t. kqk2 = 1. (1.3)
However, the natural choice of sparsity penalty, either ℓ0 or ℓ1, leads to trivial and meaning-less solutions, as there always exists q in the null space of AT such that qT Y = 0. (cid:13) (cid:13) (cid:13) (cid:13)
To avoid the null space of AT , we instead choose to ﬁnd the unit direction q that maximizes the ℓ4 norm of qT Y as max q qT Y s. t. 4 kqk2 = 1. (1.4) (cid:13) (cid:13) (cid:13) (cid:13)
The above formulation is based on the key observation that the objective value is maximized when q coincides with one column of A (see, Section 2, for details) while the objective value is zero when q lies in the null space of AT . The ℓ4 norm objective function and its variants have been adopted as a sparsity regularizer in a line of recent works [30, 44, 43, 35, 42].
However, even with this new objective function, the null space of AT persists as a challenge for solving the optimization problem: they form a ﬂat region of saddle points.
This paper characterizes the nonconvex optimization landscape of (1.4) and proposes a guaranteed procedure that avoids the ﬂat region and provably recovers the global solution to (1.4), which corresponds to one column of A. More speciﬁcally, we demonstrate that, despite the non-convexity, (1.4) still possesses benign geometric property in the sense that any strict local solution with large objective value is globally optimal and recovers one col-umn of A, up to its sign. See, Theorem 3.1 in Section 3.1 for the population level result and
Theorem 3.4 for the ﬁnite sample result.
We further extend these results to the general case when A only has full column rank in
Theorem 3.6 of Section 3.2. To recover a general A with full column rank, our procedure
ﬁrst resorts to a preconditioning procedure of Y proposed in Section 2.3 and then solves a optimization problem similar to (1.4). From our analysis of the optimization landscape, the intriguing problem boils down to developing algorithms to recover the nontrivial local solutions by avoiding regions with small objective values. We thus propose a simple initial-ization scheme in Section 4.1 and prove in Theorem 4.3 that such initialization, proceeded with any second order descent algorithm [20, 27], suﬃces to ﬁnd the global solution, up to some statistical error. Our theoretical analysis provides the explicit convergence rate of the statistical error and characterizes its dependence on various dimensions, such as p, r and n, as well as the sparsity of X.
Numerical simulation results are provided in Section 5. Due to the space limitation, we defer all the proof along with our conclusions and discussion of several future directions of our work to Appendix.
Notations Throughout this paper, we use bold lowercase letters, like a, to represent vec-tors and bold uppercase letters, like A, to represent matrices. For matrix X, Xij denotes the entry at the i-th row and j-th column of X, with Xi· and X·j denoting the i-th row and j-th column of X, respectively. Oftentimes, we write X·j = Xj for simplicity. We use grad and Hess to represent the Riemannian gradient and Hessian. For any vector v ∈ Rd, we use kvkq to denote its ℓq norm, for 1 ≤ q ≤ ∞. The notation v◦q stands for {vq i }i. For matrices, we use k·kF and k·kop to denote the Frobenius norm and the operator norm, respectively.
For any positive integer d, we write [d] = {1, 2, . . . , d}. The unit sphere in d-dimensional real space Rd is written as Sd−1. For two sequences an and bn, we write an À bn if there exists some constant C > 0 such that an ≤ Cbn for all n. Both uppercase C and lowercase c are reserved to represent numerical constants, whose values may vary line by line. 1Ir is the identity matrix of size r × r. 2
(cid:13) (cid:13) (cid:13) (cid:13) 1.1