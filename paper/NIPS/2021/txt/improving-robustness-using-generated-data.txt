Abstract
Recent work argues that robust training requires substantially larger datasets than those required for standard classiﬁcation. On CIFAR-10 and CIFAR-100, this translates into a sizable robust-accuracy gap between models trained solely on data from the original training set and those trained with additional data extracted from the “80 Million Tiny Images” dataset (80M-TI). In this paper, we explore how generative models trained solely on the original training set can be leveraged to artiﬁcially increase the size of the original training set and improve adversarial robustness to (cid:96)p norm-bounded perturbations. We identify the sufﬁcient conditions under which incorporating additional generated data can improve robustness, and demonstrate that it is possible to signiﬁcantly reduce the robust-accuracy gap to models trained with additional real data. Surprisingly, we show that even the addition of non-realistic random data (generated by Gaussian sampling) can improve robustness. We evaluate our approach on CIFAR-10, CIFAR-100, SVHN and TINYIMAGENET against (cid:96)∞ and (cid:96)2 norm-bounded perturbations of size (cid:15) = 8/255 and (cid:15) = 128/255, respectively. We show large absolute improvements in robust accuracy compared to previous state-of-the-art methods. Against (cid:96)∞ norm-bounded perturbations of size (cid:15) = 8/255, our models achieve 66.10% and 33.49% robust accuracy on CIFAR-10 and CIFAR-100, respectively (improving upon the state-of-the-art by +8.96% and +3.29%). Against (cid:96)2 norm-bounded perturbations of size (cid:15) = 128/255, our model achieves 78.31% on CIFAR-10 (+3.81%). These results beat most prior works that use external data. 1

Introduction
Neural networks are being deployed in a wide variety of applications ranging from ranking content on the web [15] to autonomous driving [5] via medical diagnostics [22]. It has become increasingly important to ensure that deployed models are robust and generalize to various input perturbations.
Unfortunately, the addition of imperceptible adversarial perturbations can cause neural networks to make incorrect predictions [9, 10, 27, 44, 64]. There has been a lot of work on understanding and generating adversarial perturbations [1, 4, 10, 64], and on building defenses that are robust to such perturbations [27, 49, 60, 82]. We note that while robustness and invariance to input perturbations is crucial to the deployment of machine learning models in various applications, it can also have broader negative impacts to society such as hindering privacy [63] or increasing bias [68].
The adversarial training procedure proposed by Madry et al. [49] feeds adversarially perturbed examples back into the training data. It is widely regarded as one of the most successful method to train robust deep neural networks [30], and it has been augmented in different ways – with changes in the attack procedure [25], loss function [50, 82] or model architecture [76, 85]. We highlight the works by Carmon et al. [11], Najaﬁ et al. [51], Uesato et al. [72], Zhai et al. [80] who simultaneously proposed the use of additional unlabeled external data. While the addition of external data helped boost robust accuracy by a large margin, progress in the setting without additional data has slowed 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Robust accuracy of models against AU-TOATTACK [16] on CIFAR-10 with (cid:96)∞ perturbations of size 8/255 displayed in publication order. Our method explores how generated data can be used to improve robust accuracy by +8.96% without us-ing any additional external data. This constitutes the largest jump in robust accuracy in this setting.
Our best model reaches a robust accuracy of 66.10% against AA+MT [30].
Figure 2: Overview of our approach. Our method initially trains a generative model and a non-robust classiﬁer. The non-robust clas-siﬁer is used to provide pseudo-labels to the generated data. Finally, generated and origi-nal training data are combined to train a robust classiﬁer. (see Fig. 1). On CIFAR-10 [42] against (cid:96)∞ perturbations of size (cid:15) = 8/255, the best known model obtains a robust accuracy of 65.87% when using additional data. The same model obtains a robust accuracy of 57.14% without this data [30]. As a result, we ask ourselves whether it is possible to leverage the information contained in the original training set to a greater extent. This manuscript challenges the status-quo. To the contrary of standard training where it is widely believed that generative models lack diversity and that the samples they produce cannot be used to train better classiﬁers [59], we demonstrate both theoretically and experimentally that these generated samples can be used to improve robustness (using the approach described in Fig. 2 and Sec. 3.3). We make the following contributions:
• We demonstrate in Sec. 3.2 that it is possible to use low-quality random inputs (sampled from a conditional Gaussian ﬁt of the training data) to improve robust accuracy on CIFAR-10 against (cid:96)∞ perturbations of size (cid:15) = 8/255 (+0.93% on a WRN-28-10) and provide a justiﬁcation and sufﬁcient conditions in Sec. 4.
• We leverage higher quality generated inputs (i.e., inputs generated by generative models solely trained on the original data), and study four recent generative models: the Denoising Diffusion
Probabilistic Model (DDPM) [36], StyleGAN2 [40], BigGAN [7] and the Very Deep Variational
Auto-Encoder (VDVAE) [14] (Sec. 5). We show that DDPM samples cover most closely the real data distribution (as measured by the distance to the test set in the Inception feature space).
• Using images generated by the DDPM allows us to reach a robust accuracy of 66.10% on
CIFAR-10 against (cid:96)∞ perturbations of size (cid:15) = 8/255 (an improvement of +8.96% upon the state-of-the-art). Notably, our best CIFAR-10 models beat all techniques that use additional data (see
Sec. 6) and constitutes one of the largest improvements ever made in the setting without additional data. As a consequence, we demonstrate that it is possible to avoid the use of 80M-TI [65] which has been withdrawn due to presence of offensive images.1 2