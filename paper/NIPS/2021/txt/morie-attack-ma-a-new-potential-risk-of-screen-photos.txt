Abstract
Images, captured by a camera, play a critical role in training Deep Neural Net-works (DNNs). Usually, we assume the images acquired by cameras are consistent with the ones perceived by human eyes. However, due to the different physical mechanisms between human-vision and computer-vision systems, the ﬁnal per-ceived images could be very different in some cases, for example shooting on digital monitors. In this paper, we ﬁnd a special phenomenon in digital image processing, the moiré effect, that could cause unnoticed security threats to DNNs.
Based on it, we propose a Moiré Attack (MA) that generates the physical-world moiré pattern adding to the images by mimicking the shooting process of digi-tal devices. Extensive experiments demonstrate that our proposed digital Moiré
Attack (MA) is a perfect camouﬂage for attackers to tamper with DNNs with a high success rate (100.0% for untargeted and 97.0% for targeted attack with the noise budget (cid:15) = 4), high transferability rate across different models, and high robustness under various defenses. Furthermore, MA owns great stealthiness be-cause the moiré effect is unavoidable due to the camera’s inner physical structure, which therefore hardly attracts the awareness of humans. Our code is available at https://github.com/Dantong88/Moire_Attack. 1

Introduction
Deep Neural Networks (DNNs) present huge potential in solving varies of vision tasks such as image classiﬁcation [19], instance segmentation [18], and object detection [32]. While such algorithms bring huge productivity and greatly facilitate the daily life of humans, security risks also arise. It was
ﬁrst revealed by Szegedy et al. [38] that small and imperceptible to human eyes perturbation on the inputs can totally fool a DNN model and dramatically alter the results [14]. Since then, large amounts of studies [14, 24, 22, 23, 10, 29, 11] have focused on crafting adversarial examples to mislead DNNs to make wrong predictions which is called adversarial attack.
In the application of DNNs, an assumption are usually assumed that image samples used in the test phase should follow the same distribution of the training data. However, in real-world applications, the image samples are usually captured by various sensors (cameras, scanners, etc.) in different conditions (light, angles, etc.). Such differences are reasonable to human eyes while may confuse a seemed robust DNN. Especially, under some situations, the strange distortions beyond the commonly used data augmentation methods may severely decrease its accuracy. One of such non-typical distortion we focus on in this paper is moiré pattern, an artifact caused by interference of overlapping lines, grids
∗Work was done during an internship at Peking University.
†Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and patterns. Moiré effect commonly occurs in the image when the frequency of the details in a scene exceeds the sensors’ resolution [36]. Despite both human observers and image-capture sensors view the same image, the human perceives a normal image while colorful strange waves (moiré effect) are generated as a by-product by the sensors when the camera interacts with the frequent details of the objects (as shown in Figure 1 (a)). The moiré effect is a typical example showing such a difference between the human eyes and camera sensors. To be speciﬁc, in terms of human eyes, three different types of cone cells respond differently to light of different wavelengths, the received different color signals of the cone cells allow the brain to perceive a continuous range of colors. However, the color vision perceived by the digital sensor called color ﬁlter array (CFA) receives color discretely. The
CFA is composed of many tiny color ﬁlters arranged periodically, thus the camera samples the color signals at either discrete intervals or locations. Moreover, the human observer does not even know the existence of moiré patterns for robotic observers.
Figure 1: (a) Moiré pattern in the physical life; (b) Potential security risk brought by moiré pattern.
Moiré pattern disturbs the normal shooting of images and has drawn the attention of many photog-raphers and electronic screen manufacturers. Especially, The NTIRE challenge on example-based demoiréing is organized every year to remove moiré pattern in images3. Simultaneously, we are wondering: 1) whether the above-mentioned difference between human eyes and image-capture sensors can be a potential physical risk to DNNs? 2) if it can be maliciously utilized by attackers to compromise the DNN-based applications? For example, objects with frequent stripes or lines (Figure 1 (a)) may produce moiré when recorded by camera and then lead to a wrong classiﬁcation. Another more common real-world scenario is illustrated in Figure 1 (b): an autonomous car is required to make real-time predictions of camera-captured images. Since the LCD monitors are now broadly existing on the streets (trafﬁc signs, advertisements, etc.), the image captured by the sensors is highly possible to include a moiré pattern, which may trigger the subsequent false predictions of the detection and recognition system of the self-driving car, incurring a catastrophe.
In this work, we ﬁnd that DNNs are vulnerable to the moiré pattern and can be easily fooled to make the false prediction (the corresponding experiments are shown in Section 3.1). This demonstrates the effectiveness of the moiré pattern as a potential physical-world attack. Then we propose a Moiré
Attack (MA) by mimicking the shooting process of the camera when taking images on the LCD monitor. From the perspective of attackers, the moiré pattern can be a perfect camouﬂage of the adversarial perturbations. Especially, the proposed Moiré Attack is more controllable and enables the attackers to deliver the targeted attack. While from the perspective of humans or supervisors, few of them pay attention to the moiré pattern because of its unavoidability in the physical world. In summary, the contributions of our work are listed as follows:
• We ﬁrst discover and analyze the potential security threat of the moiré artifacts to DNNs.
Due to the mechanism of the camera’s inner property to generate images, moiré pattern is unavoidable, which make the attack stealthy.
• We propose a Moiré Attack (MA) by mimicking the shooting process of the camera sensors, which gives the attackers more freedom and controllability to tamper with DNNs.
• We perform comprehensive experiments to evaluate the proposed MA, which has high attack success rate and transferability across the different victim models. In addition, it possesses a high robustness that is resistant to many extant image transformation methods. 3More information about the challenge is here: http://www.vision.ee.ethz.ch/ntire20/ 2
2