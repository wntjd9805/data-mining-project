Abstract
Most conventional Neural Architecture Search (NAS) approaches are limited in that they only generate architectures without searching for the optimal parameters.
While some NAS methods handle this issue by utilizing a supernet trained on a large-scale dataset such as ImageNet, they may be suboptimal if the target tasks are highly dissimilar from the dataset the supernet is trained on. To address such limitations, we introduce a novel problem of Neural Network Search (NNS), whose goal is to search for the optimal pretrained network for a novel dataset and constraints (e.g. number of parameters), from a model zoo. Then, we propose a novel framework to tackle the problem, namely Task-Adaptive Neural Network
Search (TANS). Given a model-zoo that consists of network pretrained on diverse datasets, we use a novel amortized meta-learning framework to learn a cross-modal latent space with contrastive loss, to maximize the similarity between a dataset and a high-performing network on it, and minimize the similarity between irrelevant dataset-network pairs. We validate the effectiveness and efﬁciency of our method on ten real-world datasets, against existing NAS/AutoML baselines. The results show that our method instantly retrieves networks that outperform models obtained with the baselines with signiﬁcantly fewer training steps to reach the target performance, thus minimizing the total cost of obtaining a task-optimal network. Our code and the model-zoo are available at https://github.com/wyjeong/TANS. 1

Introduction
Neural Architecture Search (NAS) aims to automate the design process of network architectures by searching for high-performing architectures with RL [76, 77], evolutionary algorithms [43, 11], parameter sharing [6, 42], or surrogate schemes [38], to overcome the excessive cost of trial-and-error approaches with the manual design of neural architectures [47, 23, 27]. Despite their success, existing
NAS methods suffer from several limitations, which hinder their applicability to practical scenarios.
First of all, the search for the optimal architectures usually requires a large amount of computation, which can take multiple GPU hours or even days to ﬁnish. This excessive computation cost makes it difﬁcult to efﬁciently obtain an optimal architecture for a novel dataset. Secondly, most NAS approaches only search for optimal architectures, without the consideration of their parameter values.
Thus, they require extra computations and time for training on the new task, in addition to the architecture search cost, which is already excessively high.
For this reason, supernet-based methods [8, 37] that search for a sub-network (subnet) from a network pretrained on large-scale data, are attracting more popularity as it eliminates the need for additional
⇤Equal contribution.
†This work was done while the author was interning at AITRICS. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Conventional Neural Architecture Search
Task-Adaptive Network Search (Ours)
Supernet Pretrained on 
Large-Scale Dataset
Additional Training Phases  for Neural Architecture Search d l r o w
-l a e
R s t e s a t a
D d l r o w
-l a e
R s e r u t c e t i h c r
A
…
Amortized Meta-Learning  for Network Retrieval
Query
Accuracy
Latency
…
FLOPs
Data Samples
Model-Zoo Construction  with Optimal Pair Selection
… y
E m b e d d i n g   T o p o l o g a m e t e r r n e d   P a r
L e a
&   s
Instant
Retrieval
Rapid
Fine-tuning
Cross-Modal Latent Space 
Best-fitted 
Trained Networks
Final 
Networks
Figure 1: Comparison between conventional NAS and our method: Conventional supernet-based NAS approaches (Left) sample subnets from a ﬁxed supernet trained on a single dataset. TANS (Right) can dynamically select the best-ﬁtted neural networks that are trained on diverse datasets, adaptively for each query dataset. training. However, this approach may be suboptimal when we want to ﬁnd the subnet for a dataset that is largely different from the source dataset the supernet is trained on (e.g. medical images or defect detection for semiconductors). This is a common limitation of existing NAS approaches, although the problem did not receive much attention due to the consideration of only a few datasets in the NAS benchmarks (See Figure 1, left). However, in real-world scenarios, NAS approaches should search over diverse datasets with heterogeneous distributions, and thus it is important to task-adaptively search for the architecture and parameter values for a given dataset. Recently, MetaD2A [31] has utilized meta-learning to learn common knowledge for NAS across tasks, to rapidly adapt to unseen tasks. However it does not consider parameters for the searched architecture, and thus still requires additional training on unseen datasets.
Given such limitations of NAS and meta-NAS methods, we introduce a novel problem of Neural
Network Search (NNS), whose goal is to search for the optimal pretrained networks for a given dataset and conditions (e.g. number of parameters). To tackle the problem, we propose a novel and extremely efﬁcient task-adaptive neural network retrieval framework that searches for the optimal neural network with both the architecture and the parameters for a given task, based on cross-modal retrieval. In other words, instead of searching for an optimal architecture from scratch or taking a sub-network from a single super-net, we retrieve the most optimal network for a given dataset in a task-adaptive manner (See Figure 1, right), by searching through the model zoo that contains neural networks pretrained on diverse datasets. We ﬁrst start with the construction of the model zoo, by pretraining state-of-the-art architectures on diverse real-world datasets.
Then, we train our retrieval model via amortized meta-learning of a cross-modal latent space with a contrastive learning objective.
Speciﬁcally, we encode each dataset with a set encoder and obtain functional and topological embeddings of a network, such that a dataset is embedded closer to the network that performs well on it while minimizing the similarity between irrelevant dataset-network pairs. The learning process is further guided by a performance predictor, which predicts the model’s performance on a given dataset.
The proposed Task-Adaptive Network Search (TANS) largely outper-forms conventional NAS/AutoML methods (See Figure 2), while signiﬁcantly reducing the search time. This is because the retrieval of a trained network can be done instantly without any additional architecture search cost, and retrieving a task-relevant network will further reduce the ﬁne-tuning cost. To evaluate the proposed TANS, we ﬁrst demonstrate the sample-efﬁciency of our model zoo construction method, over construction with random sampling of dataset-network pairs. Then, we show that the TANS can adaptively retrieve the best-ﬁtted models for an unseen dataset. Finally, we show that our method signiﬁcantly outperforms baseline NAS/AutoML methods on real-world datasets (Figure 2), with incomparably smaller computational cost to reach the target performance.
In sum, our main contributions are as follows:
Figure 2: Comparison with NAS (orange) & AutoML (blue) base-lines on 5 Real-world Datasets
• We consider a novel problem of Neural Network Search, whose goal is to search for the optimal network for a given task, including both the architecture and the parameters.
• We propose a novel cross-modal retrieval framework to retrieve a pretrained network from the model zoo for a given task via amortized meta-learning with constrastive objective.
• We propose an efﬁcient model-zoo construction method to construct an effective database of dataset-architecture pairs, considering both the model performance and task diversity.
• We train and validate TANS on a newly collected large-scale database, on which our method outperforms all NAS/AutoML baselines with almost no architecture search cost and signiﬁcantly fewer ﬁne-tuning steps. 2    
2