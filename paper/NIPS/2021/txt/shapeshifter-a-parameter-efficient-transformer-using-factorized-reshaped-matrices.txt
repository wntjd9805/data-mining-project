Abstract
Language models employ a very large number of trainable parameters. Despite being highly overparameterized, these networks often achieve good out-of-sample test performance on the original task and easily ﬁne-tune to related tasks. Recent observations involving, for example, intrinsic dimension of the objective landscape and the lottery ticket hypothesis, indicate that often training actively involves only a small fraction of the parameter space. Thus, a question remains how large a parameter space needs to be in the ﬁrst place — the evidence from recent work on model compression, parameter sharing, factorized representations, and knowledge distillation increasingly shows that models can be made much smaller and still per-form well. Here, we focus on factorized representations of matrices that underpin dense, embedding, and self-attention layers. We use low-rank factorized represen-tation of a reshaped and rearranged original matrix to achieve space efﬁcient and expressive linear layers. We prove that stacking such low-rank layers increases their expressiveness, providing theoretical understanding for their effectiveness in deep networks. In Transformer models, our approach leads to more than ten-fold reduction in the number of total trainable parameters, including embedding, attention, and feed-forward layers, with little degradation in on-task performance.
The approach operates out-of-the-box, replacing each parameter matrix with its compact equivalent while maintaining the architecture of the network. 1

Introduction
Natural language models involve large number of parameters. A single encoder-decoder Transformer
[1] in its base variant has about 44 million parameters, not counting the word embedding matrix, which adds another 10 million or more, depending on the chosen vocabulary or tokenization scheme.
Base variant of encoder-only BERT [2], including the embedding, has about 108 million parameters.
GPT-3 [3] has about 175 billion parameters, and the largest of the Switch Transformer [4] models has 1.5 trillion. This explosion in the model size has led to increased interest in approaches for reducing the number of parameters in the model.
Models with high-dimensional parameter space have much lower intrinsic dimension [5], that is, training trajectory can be successfully restricted to a random, smaller-dimensional subspace, even
∗Work performed while at Virginia Commonwealth University.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
though training a small-parameter architecture is often less successful. These observations have been recently extended to the ﬁne-tuning trajectories of language models [6]. Lottery ticket hypothesis
[7, 8], recently demonstrated to hold also for language model ﬁne-tuning [9, 10], shows that smaller subnetworks can be selected from a large model, re-trained in isolation, and perform as well as the large model; what those subnetworks are is not known a prior, in absence of the trained large model, though. Some approaches for reducing model size build on this observation to train a smaller model based on an existing large model, for example a 66 million parameter DistillBERT [11] student has been distilled from 108 million parameter BERT-base [2] teacher with little loss in quality. Alternatively, distillation has been used, for example in the context of machine translation, to increase the quality of the models while maintaining their size [12, 13]. Other approaches train a reduced-parameter model de novo, without relying on an already trained large model. For example,
DeLighT [14] uses an alternative parameterization of the multi-headed self-attention based on group linear transform to reduce a 62 million parameter Transformer to 22 million.
One simple way to reduce model size involves factorized matrix representations. ALBERT [15] employs a rank r decomposition of a d × nvocab embedding matrix storing d-dimensional embedding vectors for each of the nvocab tokens by using a stack of two linear layers, d × r on top of r × nvocab.
Similar low-rank decomposition is also used implicitly in the multi-headed self-attention in the generic
Transformer [1] with hidden dimension d and nheads self-attention heads. In each Transformer head, a r = d/nheads-rank factorized representation involving d × d/nheads key (K) and query (Q) matrices are used, with the pairwise self-attention scores for sequence x calculated using xT K T Qx, instead of xT W x involving a full general attention d × d trainable matrix W as originally considered in trainable-attention encoder-decoder LSTM models [16]. In both cases, the models are trained from a random initialization of the factorized parameter space, instead of attempting to ﬁnd the lowest-error factorized representation of an already trained original model.
We explore here a Transformer model that uses an alternative way to decompose a matrix into two smaller matrices. Instead of standard low-rank factorization as above, it involves reshaping and reordering matrix dimensions prior to the decomposition, and is equivalent to a sum of Kronecker products with an efﬁcient implementation. The compact model based on the smaller matrices is trained de novo, and matrices in the compact model are not aimed to be approximations of the matrices in the original model. For non-square model matrices, the approach allows for increased reduction in parameters for the same decomposition rank. For square matrices, the beneﬁts come from increased expressiveness of the decomposition for the same rank, allowing for reducing the rank needed to preserve model accuracy, and thus reducing the model size. Our main contribution is proving that stacking multiple linear layers decomposed this way increases the expressiveness of the network, unlike stacking multiple low-rank layers factorized in the standard way, which can only map into a subspace of dimensionality equal to the rank. Empirically, we show that the decomposition can reduce the size of a simple encoder-decoder Transformer to as little as 4 million parameters, including 2 million for the model and 2 million for the embeddings. The technique can be employed automatically to any matrix, including embedding, dense, attention, and output layers, without requiring any modiﬁcation of the model architecture. 2