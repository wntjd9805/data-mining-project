Abstract
This paper introduces a concept of layer aggregation to describe how information from previous layers can be reused to better extract features at the current layer.
While DenseNet is a typical example of the layer aggregation mechanism, its redundancy has been commonly criticized in the literature. This motivates us to propose a very light-weighted module, called recurrent layer aggregation (RLA), by making use of the sequential structure of layers in a deep CNN. Our RLA module is compatible with many mainstream deep CNNs, including ResNets, Xception and MobileNetV2, and its effectiveness is verified by our extensive experiments on image classification, object detection and instance segmentation tasks. Specifically, improvements can be uniformly observed on CIFAR, ImageNet and MS COCO datasets, and the corresponding RLA-Nets can surprisingly boost the performances by 2-3% on the object detection task. This evidences the power of our RLA module in helping main CNNs better learn structural information in images. 1

Introduction
Convolutional neural networks (CNNs) have achieved notable success in computer vision tasks, crediting to their ability to extract high-level features from input images. Due to rapid growth in the depth of CNNs in recent years, the problem of how to pass information efficiently through layers often arises when designing deep architectures. Residual connections [20, 21], or skip connections, are now cornerstone components that act as information pathways to give layers direct access to previous ones and make training feasible with hundreds of layers. In a deep feature hierarchy, higher-level features learned by the network are built upon simpler ones, but not necessarily on the layer right before it [17]. This conjecture can be supported by the stochastic depth training method [27], where layers randomly gain access to previous ones. Furthermore, it is discovered that entire layers can be removed from ResNets without impacting the performance. This observation incubated the dense connectivity in DenseNets [28, 29], where layers in a stage have direct access to all previous layers through fully connected skip connections.
We introduce a concept of layer aggregation to systematically study network designs on feature reuse, and a typical example is DenseNet. However, within a densely-connected stage, the number of network parameters grows quadratically with respect to the number of layers. These highly redundant parameters may extract similar features multiple times [7], and limit the number of channels to store new information. This motivates us to propose a very light-weighted recurrent layer aggregation (RLA) module with much fewer parameters. Specifically, we use recurrent connections to replace dense connectivity and achieve a parameter count independent of the network depth. The RLA 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Schematic diagram of a CNN with recurrent layer aggregation for image classification. module preserves the layer aggregation functionality, and it can be added to existing CNNs for better feature extraction.
Our RLA module is compatible with many CNNs used today. A schematic diagram is provided in
Figure 1, where x represents hidden layers in a deep CNN and h represents hidden states in RLA modules. LeCun et al. [33] pointed out that unfolded RNNs can be seen as very deep feedforward networks where all the hidden layers share the same weights. When examined alone, an RLA module behaves similar to an unfolded RNN with layers of a deep CNN as its inputs. But applying RLA modules to CNNs is more than a simple combination of RNN and CNN (e.g., [41]). As the key idea behind layer aggregation is to provide the current layer with a version of layer history, information exchange between main CNN and RLA modules is a must. This results in a connection from the hidden unit of the RNN back to its inputs, which is hardly observable in other deep architectures.
Moreover, we do not use global average pooling when passing information from the main CNN to
RLA, so that the historical information is enriched and contains spatial information.
Despite its recurrent design, a convolutional-RNN-based RLA module can be easily implemented using standard convolutions with parameter sharing. Empirically, RLA modules are computationally light-weighted and impose only a slight increase in model parameters and computational cost. In terms of functionality, it serves purposes beyond channel attention and can be applied on top of channel attention modules. We perform extensive experiments across a variety of tasks using different deep CNN architectures, including ResNets [20, 21], ECANets [54], Xception [10] and MobileNetV2
[43]. Our experimental results show that RLA consistently improves model performances on CIFAR,
ImageNet and MS COCO datasets. On MS COCO, our RLA module can achieve 2-3% gains in terms of average precision, which significantly outperforms other state-of-the-art networks with remarkable performances.
The main contributions of our work are below: (1) We introduce a definition of layer aggregation for analyzing CNN architectures. (2) We propose a novel recurrent layer aggregation module, with motivation from the layer aggregation mechanism and time series analysis. (3) We investigate detailed
RLA module designs through ablation study and provide guidelines on applying RLA modules. (4)
We show the effectiveness of RLA across a broad range of tasks on benchmark datasets using several popular deep CNN architectures. 2