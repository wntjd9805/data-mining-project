Abstract
The convergence of stochastic gradient descent is highly dependent on the step-size, especially on non-convex problems such as neural network training. Step decay step-size schedules (constant and then cut) are widely used in practice because of their excellent convergence and generalization qualities, but their theoretical properties are not yet well understood. We provide convergence results for step decay in the non-convex regime, ensuring that the gradient norm vanishes at an
O(ln T /
T ) rate. We also provide near-optimal (and sometimes provably tight) convergence guarantees for general, possibly non-smooth, convex and strongly con-vex problems. The practical efﬁciency of the step decay step-size is demonstrated in several large-scale deep neural network training tasks.
√ 1

Introduction
We focus on stochastic programming problems on the form min x∈X f (x) := Eξ∼Ξ[f (x; ξ)]. (1)
Here, ξ is a random variable drawn from some source distribution Ξ over an arbitrary probability space and X is a closed, convex subset of Rd. This problem is often encountered in machine learning applications, such as training of deep neural networks. Depending on the speciﬁcs of the application, the function f can either be non-convex, convex or strongly convex; it can be smooth or non-smooth; and it may also have additional structure that can be exploited.
Despite the many advances in the ﬁeld of stochastic programming, the stochastic gradient descent (SGD) method [37, 32] remains important and is arguably still the most popular method for solving (1). The SGD method updates the decision vector x using the following recursion xt+1 = ΠX (xt − ηtˆgt) (2) where ˆgt is an unbiased estimate of the gradient (or subgradient) at xt and ΠX is the Euclidean projection onto X . The step-size parameter (learning-rate) ηt > 0 is critical to control the rate at which the model learns and to guarantee that the SGD iterates converge to an optimizer of (1). Setting the step-size too large will result in iterates which never converge; and setting it too small leads to 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
slow convergence and may even cause the iterates to get stuck at bad local minima. As long as the iterates do not diverge, a large constant step-size promotes fast convergence but only to a (large) neighborhood of the optimal solution. To increase the accuracy, we have to decrease the step-size.
√
The traditional approach is to decrease the step-size in every iteration, typically as η0/t or η0/ t.
Both these step-size schedules have been studied extensively and guarantee a non-asymptotic con-vergence of SGD [31, 26, 36, 19, 38, 15]. However, from a practical perspective, these step-size policies often perform poorly, since they begin to decrease too early. For non-convex problems, such as those which arise in training of deep neural networks, the most successful and popular step-size policy in practice is the step decay step-size [25, 21, 22]. This step-size policy starts with a relatively large constant step-size and then cuts the step-size by a ﬁxed number (called decay factor) at after a given number of epochs. Not only does this step-size result in a faster initial convergence, but it also guarantees that the SGD iterates eventually converge to an exact solution.
The step decay step-size is the default choice in many deep learning libraries, such as TensorFlow [4] and PyTorch [35]; both use a decay rate of 0.1 and user-deﬁned milestones when the step-size is decreased. Although some recent studies attempt to monitor the optimization process to trigger the milestones when certain conditions are met [27, 50], such approaches are difﬁcult to analyze. Another common approach is to simply divide the targeted number of iterations into N intervals of equal length, and trigger a milestone at the end of each such interval. However, the theoretical properties of this simpler step-size policy is also not yet well understood. To make the problem more manageable theoretically, some related step-size policies that let the length of each segment increase linearly [6] or exponentially [19, 8, 49] are analyzed. On the contrary, we consider the more practical case when the horizon T of the step-decay step-size is divided into several equal parts.
If the number of iterations or horizon T is known apriori, reference [11] analyzes a step decay step-size with a decay rate of 1/2 applied every T / log2 T iterations and establishes a near-optimal
O(log2 T /T ) convergence rate for vanilla SGD on least-squares problems. However, to our knowl-edge, there are no convergence guarantees in the literature under more general conditions, e.g., for general strongly-convex, general convex, or non-convex problems. Motivated by this, in this paper, we study the non-asymptotic convergence of SGD with the step decay step-size in more general settings. 1.1 Main Contributions
This work establishes novel convergence guarantees for SGD with the step decay step-size on non-convex, convex and strongly convex optimization problems. We make the following contributions:
• We propose a non-uniform probability rule Pt ∝ 1/ηt for selecting the output in the smooth non-convex setting. Based on this rule, we (i) establish a near-optimal O(ln T /
T ) rate for
SGD with step decay step-size; (ii) improve the results for exponential decay step-size [28]; (iii) remove the ln T factor in the best known convergence rate for the 1/ t step-size.
√
√
• For strongly convex problems, we establish the following error bounds for the last iterate under step-decay: (i) O(ln T /T ) for smooth problem, which we also prove to be tight; (ii)
O(ln2 T /T ) without the smoothness assumption.
• For the general convex case, we prove that the step decay step-size at the last iterate can achieve a near-optimal convergence rate (up to a ln T factor). 1.2