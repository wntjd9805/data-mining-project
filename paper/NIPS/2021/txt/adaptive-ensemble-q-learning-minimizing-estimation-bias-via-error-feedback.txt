Abstract
The ensemble method is a promising way to mitigate the overestimation issue in
Q-learning, where multiple function approximators are used to estimate the action values. It is known that the estimation bias hinges heavily on the ensemble size (i.e., the number of Q-function approximators used in the target), and that determining the ‘right’ ensemble size is highly nontrivial, because of the time-varying nature of the function approximation errors during the learning process. To tackle this challenge, we ﬁrst derive an upper bound and a lower bound on the estimation bias, based on which the ensemble size is adapted to drive the bias to be nearly zero, thereby coping with the impact of the time-varying approximation errors accordingly. Motivated by the theoretic ﬁndings, we advocate that the ensemble method can be combined with Model Identiﬁcation Adaptive Control (MIAC) for effective ensemble size adaptation. Speciﬁcally, we devise Adaptive Ensemble
Q-learning (AdaEQ), a generalized ensemble method with two key steps: (a) approximation error characterization which serves as the feedback for ﬂexibly controlling the ensemble size, and (b) ensemble size adaptation tailored towards minimizing the estimation bias. Extensive experiments are carried out to show that
AdaEQ can improve the learning performance than the existing methods for the
MuJoCo benchmark. 1

Introduction
Thanks to recent advances in function approximation methods using deep neural networks [20],
Q-learning [35] has been widely used to solve reinforcement learning (RL) problems in a variety of applications, e.g., robotic control [23, 13], path planning [15, 24] and production scheduling
[34, 21]. Despite the great success, it is well recognized that Q-learning may suffer from the notorious overestimation bias [29, 33, 32, 10, 37], which would signiﬁcantly impede the learning efﬁciency.
Recent work [9, 11] indicates that this problem also persists in the actor-critic setting. To address this issue, the ensemble method [16, 1, 26, 7] has emerged as a promising solution in which multiple
Q-function approximators are used to get better estimation of the action values. Needless to say, the ensemble size, i.e., the number of Q-function approximators used in the target, has intrinsic impact on Q-learning. Notably, it is shown in [6, 17] that while a large ensemble size could completely remove the overestimation bias, it may go to the other extreme and result in underestimation bias and unstable training, which is clearly not desirable. Therefore, instead of simply increasing the ensemble 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
size to mitigate the overestimation issue, a fundamental question to ask is:“ Is it possible to determine the right ensemble size on the ﬂy so as to minimize the estimation bias?”
Some existing ensemble methods [2, 19, 17] adopt a trial-and-error strategy to search for the en-semble size, which would be time-consuming and require a lot of human engineering for dif-ferent RL tasks. The approximation error of the Q-function during the learning process plays a nontrivial role in the selection of the ensemble size, since it directly impacts the Q-target es-timation accuracy. This however remains not well understood.
In particular, the fact that the approximation error is time-varying, due to the iterative nature of Q-learning [36, 5], gives rise to the question that whether a ﬁxed ensemble size should be used in the learning process. To answer this question, we show in Section 2.2 that using a ﬁxed ensemble size is likely to lead to either overestimation or underestimation bias, and the bias may shift between overestimation and underestimation because of the time-varying approximation error, calling for an adaptive ensemble size so as to drive the bias close to zero based on the underlying learning dynamics.
Thus motivated, in this work we study effective ensemble size adaptation to minimize the estimation bias that hinges heavily on the time-varying approximation errors during the learning process. To this end, we ﬁrst characterize the relationship among the ensemble size, the function approximation errors, and the estimation bias, by deriv-ing an upper bound and a lower bound on the estimation bias. Our ﬁndings reveal that the ensemble size should be selected adaptively in a way to cope with the impact of the time-varying approximation errors. Building upon the theoretic results, we cast the estimation bias minimization as an adaptive control problem where the approximation error during the learning process is treated as the control object, and the ensemble size is adapted based on the feedback of the control output, i.e., the value of the approximation error from the last iteration. The key idea in this approach is inspired from the classic Model Identiﬁcation
Adaptive Control (MIAC) framework [3, 25], where at each step the current system identiﬁcation of the control object is fed back to adjust the controller, and consequently a new control signal is devised following the updated control law.
Figure 1: A sketch of the adaptive en-semble Q-learning (AdaEQ).
One main contribution of this work lies in the development of AdaEQ, a generalized ensemble method for the ensemble size adaptation, aiming to minimize the estimation bias during the learning process.
Speciﬁcally, the approximation error in each iteration is quantiﬁed by comparing the difference between the Q-estimates and the Monte Carlo return using the current learned policy over a testing trajectory [29, 17]. Inspired by MIAC, the approximation error serves as the feedback to adapt the ensemble size. Besides, we introduce a ‘tolerance’ parameter in the adaptation mechanism to balance the control tendency towards positive or negative bias during the learning process. In this way, AdaEQ can encompass other existing ensemble methods as special cases, including Maxmin [17], by properly setting this hyperparameter. A salient feature of the feedback-adaptation mechanism is that it can be used effectively in conjunction with both standard Q-learning [22] and actor-critic methods [28, 11].
Experimental results on the continuous-control MuJoCo benchmark [30] show that AdaEQ is robust to the initial ensemble size in different environments, and achieves higher average return, thanks to keeping the estimation bias close to zero, when compared to the state-of-the-art ensemble methods such as REDQ [6] and Average-DQN [2].