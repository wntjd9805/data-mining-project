Abstract
Spiking neural networks (SNNs) are brain-inspired models that enable energy-efﬁcient implementation on neuromorphic hardware. However, the supervised training of SNNs remains a hard problem due to the discontinuity of the spiking neuron model. Most existing methods imitate the backpropagation framework and feedforward architectures for artiﬁcial neural networks, and use surrogate derivatives or compute gradients with respect to the spiking time to deal with the problem. These approaches either accumulate approximation errors or only propa-gate information limitedly through existing spikes, and usually require information propagation along time steps with large memory costs and biological implausibility.
In this work, we consider feedback spiking neural networks, which are more brain-like, and propose a novel training method that does not rely on the exact reverse of the forward computation. First, we show that the average ﬁring rates of SNNs with feedback connections would gradually evolve to an equilibrium state along time, which follows a ﬁxed-point equation. Then by viewing the forward computation of feedback SNNs as a black-box solver for this equation, and leveraging the implicit differentiation on the equation, we can compute the gradient for parameters without considering the exact forward procedure. In this way, the forward and backward procedures are decoupled and therefore the problem of non-differentiable spiking functions is avoided. We also brieﬂy discuss the biological plausibility of implicit differentiation, which only requires computing another equilibrium. Extensive experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and CIFAR-100 demonstrate the superior performance of our method for feedback models with fewer neurons and parameters in a small number of time steps. Our code is available at https://github.com/pkuxmq/IDE-FSNN. 1

Introduction
Spiking neural networks (SNNs) have gained increasing attention recently due to their inherent energy-efﬁcient computation [21, 38, 41, 33, 8]. Inspired by the neurons in the human brain, biologically plausible SNNs transmit spikes between neurons, enabling event-based computation which can be carried out on neuromorphic chips with less energy consumption [1, 7, 30, 33]. Meanwhile, SNNs are computationally more powerful than artiﬁcial neural networks (ANNs) theoretically and are therefore regarded as the third generation of neural network models [24].
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite the advantages, directly supervised training of SNNs remains a hard problem, which hampers the real applications of SNNs compared with popular ANNs. The main obstacle lies in the complex spiking neuron model. While backpropagation [35] works well for ANNs, it suffers from the discontinuity of spike generation which is non-differentiable in SNN training. Most recent successful
SNN training methods still imitate the backpropagation through time (BPTT) [39] framework by error propagation through the computational graph unfolded along time steps, and they deal with the spiking function by applying surrogate derivatives to approximate the gradients [41, 5, 14, 38, 42, 27, 16, 47], or by computing the gradients with respect to the spiking time only on the spiking neurons [6, 46, 16].
However, these methods either accumulate approximation error along time steps, or suffer from the
“dead neuron” problem [38], i.e. learning would not occur when no neuron spikes. At the same time, BPTT requires memorizing intermediate variables at all time steps and backpropagating along them, which is memory-costing and biologically implausible. So it is necessary to consider training methods other than backpropagation along computational graphs that ﬁt SNNs better.
On the other hand, most recent SNN models simply imitate the feedforward architectures of ANNs [41, 38, 42, 46, 47], which ignores the ubiquitous feedback connections in the human brain. Feedback (recurrent) circuits are critical to human’s vision system for object recognition [15]. Meanwhile, [18] shows that shallow ANNs with recurrence achieve higher functional ﬁdelity of human brains and similarly high performance on large-scale vision recognition tasks, compared with deep ANNs. So incorporating feedback connections enables neural networks to be shallower, more efﬁcient, and more brain-like. As for SNNs, feedback was popular in early models like Liquid State Machine [25], which leverages a recurrent reservoir layer with weights ﬁxed or trained by unsupervised methods. And compared with the uneconomical cost for ANNs to incorporate feedback connections by unfolding along time, SNNs naturally compute with multiple time steps, which inherently supports feedback connections. Most recent SNN models imitate feedforward architectures because they were once lacking effective training methods and thus they borrow everything from successful ANNs. We focus on another direction, i.e. feedback SNN, which is a natural choice for visual tasks as well.
In this work, we consider the training of feedback spiking neural networks (FSNN), and propose a novel method based on the Implicit Differentiation on the Equilibrium state (IDE). Inspired by recent advances in implicit models [3, 4], which treat weight-tied ANNs as solving a ﬁxed-point equilibrium equation and propose alternative implicit models deﬁned by the equation, we derive that when the average inputs converge to an equilibrium, the average ﬁring rates of FSNNs would gradually evolve to an equilibrium state along time, which follows a ﬁxed-point equation as well. Then we view the forward computation of FSNN as a black-box solver for the ﬁxed-point equation, and borrow the idea of implicit differentiation from implicit models [3, 4] to calculate the gradients, which only relies on the equation rather than the exact forward procedure. In this way, gradient calculation is agnostic to the spiking function in SNN, thus avoiding the common difﬁculties in SNN training.
While implicit differentiation may seem too abstract to be computed in the brain, we brieﬂy discuss the biological plausibility and show that it only requires computing another equilibrium along the inverse connections of neurons. Besides, we incorporate the multi-layer structure into the feedback model for better representation ability. Our contributions include: 1. We are the ﬁrst to theoretically derive the equilibrium states with a ﬁxed-point equation for the average ﬁring rates of FSNNs with the (leaky) integrate and ﬁre model under both continuous and discrete views. According to this, the forward computation of FSNNs can be interpreted as solving a ﬁxed-point equation. 2. We propose a novel training method for FSNNs based on the implicit differentiation on the equilibrium state, which is decoupled from the forward computational graph and avoids
SNN training problems, e.g. non-differentiability and large memory costs. We also discuss the biological plausibility and demonstrate the connection to the Hebbian learning rule. 3. We conduct extensive experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and
CIFAR-100, which demonstrate the superior results of our methods with fewer neurons and parameters in a small number of time steps for both static images and neuromorphic inputs.
Especially, our directly trained model can outperform the state-of-the-art SNN performance on the complex CIFAR-100 dataset with only 30 time steps. 2