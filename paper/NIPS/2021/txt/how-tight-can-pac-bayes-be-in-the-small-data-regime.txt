Abstract
In this paper, we investigate the question: Given a small number of datapoints, for example N = 30, how tight can PAC-Bayes and test set bounds be made? For such small datasets, test set bounds adversely affect generalisation performance by withholding data from the training procedure. In this setting, PAC-Bayes bounds are especially attractive, due to their ability to use all the data to simultaneously learn a posterior and bound its generalisation risk. We focus on the case of i.i.d. data with a bounded loss and consider the generic PAC-Bayes theorem of Germain et al. While their theorem is known to recover many existing PAC-Bayes bounds, it is unclear what the tightest bound derivable from their framework is. For a
ﬁxed learning algorithm and dataset, we show that the tightest possible bound coincides with a bound considered by Catoni; and, in the more natural case of distributions over datasets, we establish a lower bound on the best bound achievable in expectation. Interestingly, this lower bound recovers the Chernoff test set bound if the posterior is equal to the prior. Moreover, to illustrate how tight these bounds can be, we study synthetic one-dimensional classiﬁcation tasks in which it is feasible to meta-learn both the prior and the form of the bound to numerically optimise for the tightest bounds possible. We ﬁnd that in this simple, controlled scenario, PAC-Bayes bounds are competitive with comparable, commonly used
Chernoff test set bounds. However, the sharpest test set bounds still lead to better guarantees on the generalisation error than the PAC-Bayes bounds we consider. 1

Introduction
Generalisation bounds are of both practical and theoretical importance. Practically, tight bounds provide certiﬁcates that algorithms will perform well on unseen data. Theoretically, the bounds and underlying proof techniques can help explain the phenomenon of learning. Among the tightest known bounds are PAC-Bayes (McAllester, 1999) and test set bounds (Langford, 2002). In this paper, we investigate their numerical tightness when applied to small datasets (N 30–60 datapoints).
The comparison between PAC-Bayes and test set bounds is particularly interesting in this setting as one cannot discard data to compute a test set bound without signiﬁcantly harming post-training performance due to a reduced training set size. PAC-Bayes on the other hand provides valid bounds while using all of the data for learning, since it provides bounds that hold uniformly. The small
≈
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
data setting can also be quite different from the big data setting, as lower-order terms in PAC-Bayes bounds have a non-negligible contribution, and the detailed structure of the bound becomes important.
Fortunately, we do not have to study each PAC-Bayes bound separately: remarkably, Germain et al. (2009) showed that a wide range of bounds can be obtained as special cases of a single generic
PAC-Bayes theorem that captures the central ideas of many PAC-Bayes proofs (see also Bégin et al. (2016)). This theorem has a free parameter: it holds for any convex function, ∆. By choosing
∆ appropriately, one can recover the well-known bounds of Langford and Seeger (2001), Catoni (2007) and other bounds. We focus on two questions related to this set-up. First, what is the tightest bound achievable by any convex function ∆? An answer would characterise the limits of the generic
PAC-Bayes theorem, and thereby of a wide range of bounds, by telling us how much improvement could be obtained before new ideas or assumptions are needed. Second, since test set bounds are the de facto standard for larger datasets, but PAC-Bayes has beneﬁts when N is small, we ask: in the small data regime, can PAC-Bayes be tighter than test set bounds?
In Section 3, Theorem 4, we show that in the (artiﬁcial) case when ∆ can be chosen depending on the dataset (without taking a union bound), the tightest version of the generic PAC-Bayes theorem is obtained by one of the Catoni bounds (Catoni, 2007). In the more realistic case when ∆ must be chosen before sampling the dataset, we do not fully characterise the tightest bound, but in
Corollary 3 we lower bound the tightest bound achievable (in expectation) with any ∆. We also provide numerical evidence in Figure 2 that suggests this lower bound can in some cases be attained, by ﬂexibly parameterising a convex function ∆ with a constrained neural network. Interestingly, this lower bound coincides with removing a lower-order term from the Langford and Seeger (2001) bound (something that Langford (2002) conjectured was possible), and relaxes to the well-known
Chernoff test set bound (see Theorem 2 below) when the PAC-Bayes posterior is equal to the prior.
In Section 4, we investigate the tightness of PAC-Bayes and test set bounds in synthetic 1D classiﬁcation. The goal of this experiment is to ﬁnd out how tight the bounds could be made in principle. We use meta-learning to adapt all aspects of the bounds and learning algorithms, producing meta-learners that are trained to optimise the value of the bounds on this task distribution.
We ﬁnd that, in this setting, PAC-Bayes can be competitive with the Chernoff test set bound, but is outperformed by the binomial tail test set bound, of which the Chernoff bound is a relaxation. This suggests that, for standard PAC-Bayes to be quantitatively competitive with the best test set bounds on small datasets, a new proof technique leading to bounds that gracefully relax to the binomial tail bound is required. Code to reproduce all experiments can be found at https://github.com/cambridge-mlg/pac-bayes-tightness-small-data. 2