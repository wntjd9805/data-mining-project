Abstract
In complex systems, we often observe complex global behavior emerge from a collection of agents interacting with each other in their environment, with each individual agent acting only on locally available information, without knowing the full picture. Such systems have inspired development of artiﬁcial intelligence algorithms in areas such as swarm optimization and cellular automata. Motivated by the emergence of collective behavior from complex cellular systems, we build systems that feed each sensory input from the environment into distinct, but identi-cal neural networks, each with no ﬁxed relationship with one another. We show that these sensory networks can be trained to integrate information received locally, and through communication via an attention mechanism, can collectively produce a globally coherent policy. Moreover, the system can still perform its task even if the ordering of its inputs is randomly permuted several times during an episode. These permutation invariant systems also display useful robustness and generalization properties that are broadly applicable. Interactive demo and videos of our results: https://attentionneuron.github.io/

Introduction 1
Sensory substitution refers to the brain’s ability to use one sensory modality (e.g., touch) to supply environmental information normally gathered by another sense (e.g., vision). Numerous studies have demonstrated that humans can adapt to changes in sensory inputs, even when they are fed into the wrong channels [5, 6, 25, 64]. But difﬁcult adaptations–such as learning to “see” by interpreting visual information emitted from a grid of electrodes placed on one’s tongue [6], or learning to ride a
“backwards” bicycle [64]–require months of training to attain mastery. Can we do better, and create artiﬁcial systems that can rapidly adapt to sensory substitutions, without the need to be retrained?
Figure 1: Comparison of visual input intended for the game player, and what our system receives.
We partition the visual input from CarRacing (Left) and Atari Pong (right) into a 2D grid of small patches, and randomly permute their ordering. Each sensory neuron in the system receives a stream of visual input at a particular permuted patch location, and through coordination, must complete the task at hand, even if the visual ordering is randomly permuted again several times during an episode.
†Equal Contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Modern deep learning systems are generally unable to adapt to a sudden reordering of sensory inputs, unless the model is retrained, or if the user manually corrects the ordering of the inputs for the model. However, techniques from continual meta-learning, such as adaptive weights [3, 36, 65],
Hebbian-learning [52, 53, 57], and model-based [2, 20, 37, 38] approaches can help the model adapt to such changes, and remain a promising active area of research.
In this work, we investigate agents that are explicitly designed to deal with sudden random reordering of their sensory inputs while performing a task. Motivated by recent developments in self-organizing neural networks [27, 55, 61] related to cellular automata [14, 17, 18, 58, 78], in our experiments, we feed each sensory input (which could be an individual state from a continuous control environment, or a patch of pixels from a visual environment) into an individual neural network module that integrates information from only this particular sensory input channel over time. While receiving information locally, each of these individual sensory neural network modules also continually broadcasts an output message. Inspired by the Set Transformer [48, 74] architecture, an attention mechanism combines these messages to form a global latent code which is then converted into the agent’s action space. The attention mechanism can be viewed as a form of adaptive weights of a neural network, and in this context, allows for an arbitrary number of sensory inputs that can be processed in any random order.
In our experiments, we ﬁnd that each individual sensory neural network module, despite receiving only localized information, can still collectively produce a globally coherent policy, and that such a system can be trained to perform tasks in several popular reinforcement learning (RL) environments.
Furthermore, our system can utilize a varying number of sensory input channels in any randomly permuted order, even when the order is shufﬂed again several times during an episode.
Permutation invariant systems have several advantages over traditional ﬁxed-input systems. We ﬁnd that encouraging a system to learn a coherent representation of a permutation invariant observation space leads to policies that are more robust and generalizes better to unseen situations. We show that, without additional training, our system continues to function even when we inject additional input channels containing noise or redundant information. In visual environments, we show that our system can be trained to perform a task even if it is given only a small fraction of randomly chosen patches from the screen, and at test time, if given more patches, the system can take advantage of the additional information to perform better. We also demonstrate that our system can generalize to visual environments with novel background images, despite training on a single ﬁxed background.
Lastly, to make training more practical, we propose a behavioral cloning scheme to convert policies trained with existing methods into a permutation invariant policy with desirable properties. 2