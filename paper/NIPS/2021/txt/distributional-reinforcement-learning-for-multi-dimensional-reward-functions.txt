Abstract
A growing trend for value-based reinforcement learning (RL) algorithms is to capture more information than scalar value functions in the value network. One of the most well-known methods in this branch is distributional RL, which models return distribution instead of scalar value. In another line of work, hybrid reward architectures (HRA) in RL have studied to model source-speciﬁc value functions for each source of reward, which is also shown to be beneﬁcial in performance.
To fully inherit the beneﬁts of distributional RL and hybrid reward architectures, we introduce Multi-Dimensional Distributional DQN (MD3QN), which extends distributional RL to model the joint return distribution from multiple reward sources.
As a by-product of joint distribution modeling, MD3QN can capture not only the randomness in returns for each source of reward, but also the rich reward correlation between the randomness of different sources. We prove the convergence for the joint distributional Bellman operator and build our empirical algorithm by minimizing the Maximum Mean Discrepancy between joint return distribution and its Bellman target. In experiments, our method accurately models the joint return distribution in environments with richly correlated reward functions, and outperforms previous RL methods utilizing multi-dimensional reward functions in the control setting. 1

Introduction
Making value network capture more information than scalar value functions is a growing trend in value-based reinforcement learning, which helps the agent gain more knowledge about the environ-ment and has great potentials to improve the sample efﬁciency of RL agents. In the early stage of deep reinforcement learning, DQN (Mnih et al., 2013) uses the scalar output of the neural network to represent value functions. As value-based RL algorithms evolve, distributional RL algorithms start to use neural networks to approximate return distributions for each state-action pair, and take action based on the expectation of return distributions. By capturing the randomness in return as auxiliary tasks, distributional RL agents can gain more knowledge about the environment and learn better representations to avoid state aliasing (Bellemare et al., 2017). Distributional RL algorithms including C51 (Bellemare et al., 2017), QR-DQN (Dabney et al., 2018b), IQN (Dabney et al., 2018a),
∗Equal contribution.
†Work done during an internship at Microsoft Research Asia.
‡Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
FQF (Yang et al., 2019) and MMDQN (Nguyen et al., 2020) achieve substantial performance gain compared to DQN.
In another line of work, HRA (Van Seijen et al., 2017) and RD2 (Lin et al., 2020) consider the setting where multiple sources of reward exist in the environment and modify the value network to model source-speciﬁc value function for each source of reward. In these works, the outputs of value networks can be interpreted as multiple source-speciﬁc value functions, and the agent takes action based on the sum of all source-speciﬁc value functions. Similar to return distribution, estimating the source-speciﬁc value functions can be seen as auxiliary tasks, which serve as additional supervision for how the total reward is composed and enable the agent to learn better representations. Several previous works support that it is beneﬁcial to model source-speciﬁc value functions (Boutilier et al., 1995; Sutton et al., 2011a; Van Seijen et al., 2017; Lin et al., 2020).
Towards providing more supervision signals and enabling agents to gain more knowledge about the environment, we propose to capture the correlated randomness in source-speciﬁc returns. Speciﬁcally, we consider the source-speciﬁc returns from all sources of rewards as a multi-dimensional random variable, and capture its joint distribution to model the randomness of returns from different sources.
This provides an informative learning target for our agent. The framework is general and can be extended to capture the correlated randomness of other types of random variables than rewards.
For example, we can capture the correlation between achieving different goals in goal-conditioned reinforcement learning (Schaul et al., 2015), or visiting different states in successor representa-tion (Kulkarni et al., 2016). In this paper, we focus on the method for learning joint return distribution of given source-speciﬁc rewards and leave the extension to more general settings for future work.
Following existing works on distributional RL, we study the convergence of the Bellman operator and propose an empirical algorithm to approximate the Bellman operator. First, we deﬁne the joint distributional Bellman operator, and prove its convergence under the Wasserstein metric. To derive an empirical algorithm, our proposed method (MD3QN) approximates the joint distributional Bellman operator by minimizing the Maximum Mean Discrepancy (MMD) loss over joint return distributions and its Bellman target. MMD holds desirable properties that, it is a metric over joint distribution and its square can be unbiasedly optimized with batch samples. This enables our algorithm to approximate the Bellman operator accurately when the number of samples goes to inﬁnity and the loss is minimized to zero. In experiments on Atari games and other environments with pixel inputs, our method accurately models the multi-dimensional joint distribution from multiple sources of reward. Moreover, our algorithm outperforms previous work HRA which also separately models multiple sources of reward on Atari game environments.
Our contributions can be summarized as follows:
• We propose a distributional RL algorithm, MD3QN, that extends distributional RL algo-rithms to model the joint return distribution from multiple sources of reward.
• We establish convergence results for the joint distributional Bellman operator, and our proposed algorithm MD3QN approximates this Bellman operator by minimizing MMD loss over joint return distribution and its Bellman target.
• Empirically, our method outperforms previous RL algorithms utilizing multiple sources of reward, and accurately models the joint return distribution from all sources of rewards. 2