Abstract
We study online control of time-varying linear systems with unknown dynamics in the nonstochastic control model. At a high level, we demonstrate that this setting is qualitatively harder than that of either unknown time-invariant or known time-varying dynamics, and complement our negative results with algorithmic upper bounds in regimes where sublinear regret is possible. More speciﬁcally, we study regret bounds with respect to common classes of policies: Disturbance Action (SLS), Disturbance Response (Youla), and linear feedback policies. While these three classes are essentially equivalent for LTI systems, we demonstrate that these equivalences break down for time-varying systems.
We prove a lower bound that no algorithm can obtain sublinear regret with respect to the ﬁrst two classes unless a certain measure of system variability also scales sublinearly in the horizon. Furthermore, we show that ofﬂine planning over the state linear feedback policies is NP-hard, suggesting hardness of the online learning problem.
On the positive side, we give an efﬁcient algorithm that attains a sublinear regret bound against the class of Disturbance Response policies up to the aforementioned system variability term. In fact, our algorithm enjoys sublinear adaptive regret bounds, which is a strictly stronger metric than standard regret and is more ap-propriate for time-varying systems. We sketch extensions to Disturbance Action policies and partial observation, and propose an inefﬁcient algorithm for regret against linear state feedback policies. 1

Introduction
The control of linear time-invariant (LTI) dynamical systems is well-studied and understood. This includes classical methods from optimal control such as LQR and LQG, as well as robust H∞ control. Recent advances study regret minimization and statistical complexity for online linear control, in both stochastic and adversarial perturbation models. Despite this progress, rigorous mathematical guarantees for nonlinear control remain elusive: nonlinear control is both statistically and computationally intractable in general.
In the face of these limitations, recent research has begun to study the rich continuum of settings which lie between LTI systems and generic nonlinear ones. The hope is to provide efﬁcient and robust algorithms to solve the most general control problems that are tractable, and at the same time, to characterize precisely at which degree of nonlinearity no further progress can be made. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
This paper studies the control of linear, time-varying (LTV) dynamical systems as one such point along this continuum. This is because the ﬁrst-order Taylor approximation to the dynamics of any smooth nonlinear system about a given trajectory is an LTV system. These approximations are widely popular because they allow for efﬁcient planning, as demonstrated by the success of iLQR and iLQG methods for nonlinear receding horizon control. We study online control of discrete-time LTV systems, with dynamics and time-varying costs xt+1 = Atxt + Btut + wt, ct(xt, ut) : (x, u) → R. (1.1)
Above, xt is the state of the system, ut the control input, wt the disturbances, and At, Bt the system matrices. Our results extend naturally to partial-state observation, where the controller observes linear projections of the state yt = Ctxt. We focus on the challenges introduced when the system matrices
At, Bt and perturbations wt are not known to the learner in advance, and can only be determined by live interaction with the changing systems.
In this setting, we ﬁnd that the overall change in system dynamics across time characterizes the difﬁculty of controlling the unknown LTV system. We deﬁne a measure, called system variability, which quantiﬁes this. We show both statistical and computational lower bounds as well as algorithmic upper bounds in terms of the system variabilility. Surprisingly, system variability does not impede the complexity of control when the dynamics are known [16]. 1.1 Contributions
We consider the recently popularized nonstochastic model of online control, and study regret bounds with respect to common classes of policies: Disturbance Action (DAC/SLS [44]), Disturbance
Response (DRC/Youla [46]), and linear feedback policies. Planning over the third class of feedback policies in LTI systems admits efﬁcient convex relaxations via the the ﬁrst two parametrizations, DAC and DRC. This insight has been the cornerstone of both robust [49, 44] and online [3, 39] control.
Separation of parametrizations. For linear time-varying systems, however, we ﬁnd that equiva-lences between linear feedback, DAC and DRC fail to hold: we show that there are cases where any one of the three parametrizations exhibits strictly better control performance than the other two.
Regret against convex parametrizations. Our ﬁrst set of results pertain to DAC and DRC parametrizations, which are convex and admit efﬁcient optimization. We demonstrate that no algorithm can obtain sublinear regret with respect to these classes when faced with unknown, LTV dynamics unless a certain measure of system variability also scales sublinearly in the horizon. This is true even under full observation, controllable dynamics, and ﬁxed control cost. This ﬁnding is in direct contrast to recent work which shows sublinear regret is attainable over LTV system dynamics if they are known [16].
We give an efﬁcient algorithm that attains sublinear regret against these policy classes up to an additive penalty for the aforementioned system variability term found in our lower bound. When the system variability is sufﬁciently small, our algorithm recovers state-of-the-art results for unknown
LTI system dynamics up to logarithmic factors.
In fact, our algorithm enjoys sublinear adaptive regret [21], a strictly stronger metric than standard regret which is more appropriate for time-varying systems. We also show that the stronger notion of adaptivity called strongly adaptive regret [11] is out of reach in the partial information setting.
Regret against state feedback. Finally, we consider the class of state feedback policies, which are linear feedback with memory length one. We show that full-information optimization over state feedback policies is computationally hard. This suggests that obtaining sublinear regret relative to these policies may be computationally prohibitive, though does not entirely rule out the possibility of improper learning. However, improper learning cannot be done via the DRC or DAC relaxations in light of our policy class separation results. Finally, we include an inefﬁcient algorithm which attains sublinear (albeit nonparametric-rate) regret against state feedback control policies.
Paper Structure
Discussion of relevant literature and relation to our work can be found in Section 1.2. In Section 2, we formally introduce the setting of LTV nonstochastic control, the policy classes we study and our key result regarding their non-equivalence in the LTV setting (Theorem 2.1). Motivated by this 2
non-equivalence, the remainder of the paper is split into the study of convex policies (Section 3) and of state feedback policies (Section 4). In Section 3, we show that regret against the DAC and DRC classes cannot be sublinear unless the metric system variability (Deﬁnition 3.1) itself is sublinear (Theorem 3.1), and also propose Algorithm 2 whose adaptive regret scales at the rate of our lower bound plus a T 2/3 term (Theorem 3.4). On the other hand, in Section 4 we show sublinear regret against state feedback policies is technically possible (Theorem 4.1) with a computationally inefﬁcient algorithm, but also provide a computational lower bound (Theorem 4.2) for planning which reveals signiﬁcant difﬁculties imposed by the LTV dynamics in this scenario as well. Finally, in Section 5 we pose several future directions, concerning both questions in LTV control, as well as the extension to nonlinear control. 1.2