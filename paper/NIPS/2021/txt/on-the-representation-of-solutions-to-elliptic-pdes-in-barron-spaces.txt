Abstract
Numerical solutions to high-dimensional partial differential equations (PDEs) based on neural networks have seen exciting developments. This paper derives complexity estimates of the solutions of d-dimensional second-order elliptic PDEs in the Barron space, that is a set of functions admitting the integral of certain parametric ridge function against a probability measure on the parameters. We prove under some appropriate assumptions that if the coefﬁcients and the source term of the elliptic PDE lie in Barron spaces, then the solution of the PDE is (cid:15)-close with respect to the H 1 norm to a Barron function. Moreover, we prove dimension-explicit bounds for the Barron norm of this approximate solution, depending at most polynomially on the dimension d of the PDE. As a direct consequence of the complexity estimates, the solution of the PDE can be approximated on any bounded domain by a two-layer neural network with respect to the H 1 norm with a dimension-explicit convergence rate. 1

Introduction
Inspired by the tremendous success of deep learning in diverse machine learning tasks including image classiﬁcation, natural language processing, and artiﬁcial intelligence, there has been growing interest in exploring scientiﬁc and engineering applications of deep learning [36, 32, 34, 26, 47].
As partial differential equations (PDEs) play a fundamental role in almost all branches of sciences and engineering, numerical solutions to PDE problems based on neural networks have become an important research direction in scientiﬁc machine learning [25, 6, 23, 17, 7, 10, 22]. Among the various directions, numerical solutions to high-dimensional PDEs – the unknown function depending on many variables – are perhaps the most exciting possibility, as solving such PDEs has been a long-35th Conference on Neural Information Processing Systems (NeurIPS 2021).
standing challenge and breakthrough would lead to tremendous progress in ﬁelds such as many-body physics [4, 11, 18], multiple agent control [35, 17], just to name a few.
Numerical solutions to low-dimensional PDEs, such as Navier-Stokes equation in ﬂuid dynamics, has become a standard practice after decades of work. However, the computational cost of the conventional numerical methods for PDEs grows exponentially with the dimension, as a manifestation of the curse of dimensionality (CoD). Given a target accuracy (cid:15), conventional methods, such as ﬁnite element or ﬁnite difference, would need a mesh size of O((cid:15)), and thus degree of freedom on the order of O((cid:15)−d), where d is the dimension of the problem. Such complexity severely limits the numerical solutions to PDEs in high dimension, such as the many-body Schrödinger equations from quantum mechanics and the high-dimensional Hamilton-Jacobi-Bellman equations from control theory. Neural networks, in particular deep neural networks, provide a promising way to overcome the CoD in representing functions in high dimension. It is thus a natural idea to parametrize the solution ansatz to a PDE as neural networks and to employ variational search for the optimal parameters. Various neural network methods [25, 6, 7, 10, 33, 17, 41, 46, 16, 5] for PDEs have been proposed recently and some of them have demonstrated great empirical success in solving PDEs of hundreds and thousands of dimensions [7, 10, 17], much beyond the capability of conventional approaches. Question remains though on theoretical analysis of such neural-network based methods for solving high-dimensional
PDEs. While there have been some recent progress on approaches including physics-informed neural networks [37, 31, 38] and the deep Ritz method [28, 27], many questions still remain open. Among them, a fundamental question is
Whether the solution of a high-dimensional PDE can be efﬁciently approximated by a neural network, and if so, how to quantify the complexity of the neural network representation with respect to the increasing dimension?
Our contributions The focus of the current study takes a functional-analytic approach to this question. Namely, we identify a function class suitable for neural network approximations and prove that the solutions to a class of PDEs can be well approximated by functions in this class. More speciﬁcally, the PDE we consider is a family of second-order elliptic PDEs of the form
Lu = −∇ · (A∇u) + cu = f on Rd. (1.1)
We choose to work with the Barron class of functions deﬁned in [8] (see also [1]), which is a class of functions admitting the integral of certain parametric ridge function against a probability measure on the parameters; see Deﬁnition 2.2 for a precise description. This Barron space is inspired by the pioneering work by Barron [2], where he proved that a class of functions whose Fourier transform has the ﬁrst order moment can be approximated by two-layer networks without CoD. The main result of our work, stated informally, is the following; a more precise statement can be found in Section 2.3.
If the coefﬁcients A, c and the source term f of the second-Main Theorem (informal version) order elliptic PDE (1.1) are all Barron functions, then the solution u∗ can be approximated by another
Barron function u such that (cid:107)u − u∗(cid:107)H 1 ≤ (cid:15), where the Barron norm of u is upper bounded by
O((d/(cid:15))C log(1/(cid:15))). Moreover, if the Barron space is deﬁned by the cosine activation function, then the upper bound on the Barron norm can be improved to O(dC log 1/(cid:15)).
√
We note that while the better rate is only obtained for the cosine activation function, such periodic activation function has indeed been found effective in certain PDE related tasks, see e.g., [42].
Since the Barron functions can be approximated on a ﬁnite domain Ω w.r.t. H 1 norm by two-layer neural networks with a rate O(1/ k) where k is the network width (see Theorem 2.5), the theorem above directly implies that there exists a two-layer network uk with the number of widths k = O((d/(cid:15))C log(1/(cid:15))), or k = O(dC log 1/(cid:15)) if the activation function is cosine, such that (cid:107)uk − u∗(cid:107)H 1(Ω) ≤ (cid:15). Therefore in our setting the solution can be approximated by a two-layer neural network without CoD, namely the complexity depends at most polynomially on the dimension d for ﬁxed (cid:15). Alternatively, we can rewrite the rates as O((1/(cid:15))C(log d+log 1/(cid:15))) and O((1/(cid:15))C log d) to contrast with that of conventional grid-based numerical methods for PDEs, which scales as
O((1/(cid:15))d). We observe that the dependence on d is replaced with log d in the complexity bound for neural network approximations.
We emphasize that such approximation result does not follow directly from the universal approxima-tion property of neural networks for Barron functions since it is not a priori known that the solution to 2
the PDE is a Barron function. In fact, directly imposing regularity or complexity assumption on the solution itself is unreasonable since the solution is unknown and its ﬁne properties are generally inac-cessible. Our main contribution is to establish the fact that the solution can be indeed approximated by a Barron function, under the assumption that coefﬁcients and the right hand term of the PDE are
Barron. From a mathematical point of view, our main theorem is in the same spirit as regularity estimates of PDEs, which are of crucial importance in the study of PDEs. While such regularity estimates are well developed in low dimension, the extension to results in high dimension is highly non-trivial and is the main focus of our work.