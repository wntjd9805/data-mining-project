Abstract
The integration of algorithmic components into neural architectures has gained increased attention recently, as it allows training neural networks with new forms of supervision such as ordering constraints or silhouettes instead of using ground truth labels. Many approaches in the ﬁeld focus on the continuous relaxation of a speciﬁc task and show promising results in this context. But the focus on single tasks also limits the applicability of the proposed concepts to a narrow range of applications.
In this work, we build on those ideas to propose an approach that allows to integrate algorithms into end-to-end trainable neural network architectures based on a general approximation of discrete conditions. To this end, we relax these conditions in control structures such as conditional statements, loops, and indexing, so that resulting algorithms are smoothly differentiable. To obtain meaningful gradients, each relevant variable is perturbed via logistic distributions and the expectation value under this perturbation is approximated. We evaluate the proposed continuous relaxation model on four challenging tasks and show that it can keep up with relaxations speciﬁcally designed for each individual task. 1

Introduction
Artiﬁcial Neural Networks have shown their ability to solve various problems, ranging from classical tasks in computer science such as machine translation [1] and object detection [2] to many other topics in science such as, e.g., protein folding [3]. Simultaneously, classical algorithms exist, which typically solve predeﬁned tasks based on a given input and a predeﬁned control structure such as, e.g., sorting, shortest-path computation and many more. Recently, research has started to combine both elements by integrating algorithmic concepts into neural network architectures. Those approaches allow training neural networks with alternative supervision strategies, such as learning 3D representations via a differentiable renderer [4], [5] or training neural networks with ordering information [6], [7]. We unify these alternative supervision strategies, which integrate algorithms into the training objective, as algorithmic supervision:
Deﬁnition 1 (Algorithmic Supervision). In algorithmic supervision, an algorithm is applied to the predictions of a model and the outputs of the algorithm are supervised. In contrast, in direct supervision, the predictions of a model are directly supervised. This is illustrated in Figure 1.
In general, to allow for end-to-end training of neural architectures with integrated algorithms, the challenge is to estimate the gradients of a respective algorithm, e.g., by a differentiable approximation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Input
Model
Output
Input
Model (supervised) meaningful embedding
Algorithm
Output (supervised)
Figure 1: Direct supervision (on the left) in comparison to algorithmic supervision (on the right).
Here, most solutions are tailored to speciﬁc problems like, e.g., differentiable sorting or rendering.
But also more general frameworks have recently been proposed, which estimate gradients for combi-natorial optimizers. Examples for such approaches are the differentiation of black box combinatorial solvers as proposed by Vlastelica et al. [8] and the differentiation of optimizers by stochastically perturbing their input as proposed by Berthet et al. [9]. Both approaches focus on the problem that it is necessary to estimate the gradients of an algorithm to allow for an end-to-end training of neural architectures including it. To address this issue, Vlastelica et al. [8] estimate the gradients of an optimizer by a one-step linearization method that takes the gradients with respect to the optimizer’s output into account, which allows to integrate any off-the-shelf combinatorial optimizer. Berthet et al.
[9] estimate the gradients by perturbing the input to a discrete solver by random noise.
In this context, we propose an approach for making algorithms differentiable and estimating their gradients. Speciﬁcally, we propose continuous relaxations of different algorithmic concepts such as comparators, conditional statements, bounded and unbounded loops, and indexing. For this, we perturb those variables in a discrete algorithm by logistic distributions, for which we want to compute a gradient. This allows us to estimate the expected value of an algorithm’s output sampling-free and in closed form, e.g., compared to methods approximating the distributions via Monte-Carlo sampling methods (e.g., [9]). To keep the computation feasible, we approximate the expectation value by merging computation paths after each conditional block in sequences of conditional blocks. For nested conditional blocks, we compute the exact expectation value without merging conditional cases.
This trade-off allows merging paths in regular intervals, and thus alleviates the need to keep track of all possible path combinations. As we model perturbations of variables when they are accessed, all distributions are independent, which contrasts the case of modeling input perturbations, where all computation paths would have to be handled separately.
To demonstrate the practical aspects, we apply the proposed approach in the context of four tasks, that make use of algorithmic supervision to train a neural network, namely sorting supervision [6],
[7], [10], shortest-path supervision [8], [9], silhouette supervision (differentiable rendering) [4],
[5], [11], and ﬁnally Levenshtein distance supervision. While the ﬁrst three setups are based on existing benchmarks in the ﬁeld, we introduce the fourth experiment to show the application of this idea to a new algorithmic task in the ﬁeld of differentiable dynamic programming. In the latter, the Levenshtein distance between two concatenated character sequences from the EMNIST data set [12] is supervised while the individual letters remain unsupervised. We show that the proposed system is able to outperform current state-of-the-art methods on sorting supervision, while it performs comparable on shortest path supervision and silhouette supervision. 2