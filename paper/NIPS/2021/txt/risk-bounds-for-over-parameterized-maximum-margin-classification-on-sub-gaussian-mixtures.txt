Abstract
Modern machine learning systems such as deep neural networks are often highly over-parameterized so that they can ﬁt the noisy training data exactly, yet they can still achieve small test errors in practice. In this paper, we study this “benign overﬁtting” phenomenon of the maximum margin classiﬁer for linear classiﬁcation problems. Speciﬁcally, we consider data generated from sub-Gaussian mixtures, and provide a tight risk bound for the maximum margin linear classiﬁer in the over-parameterized setting. Our results precisely characterize the condition under which benign overﬁtting can occur in linear classiﬁcation problems, and improve on previous work. They also have direct implications for over-parameterized logistic regression. 1

Introduction
In modern machine learning, complex models such as deep neural networks have become increasingly popular. These complicated models are capable of ﬁtting noisy training data sets, while at the same time achieving small test errors. In fact, this benign overﬁtting phenomenon is not a unique feature of deep learning. Even for kernel methods and linear models, [5] demonstrated that interpolators on the noisy training data can still perform near optimally on the test data. A series of recent works
[4, 20, 11, 2] theoretically studied how over-parameterization can achieve small population risk.
In particular in [2] the authors considered the setting where the data are generated from a ground-truth linear model with noise, and established a tight population risk bound for the minimum norm linear interpolator with a matching lower bound. More recently, [23] further studied benign overﬁtting in ridge regression, and established non-asymptotic generalization bounds for over-parametrized ridge regression. They showed that those bounds are tight for a range of regularization parameter values.
Notably, these results cover arbitrary covariance structure of the data, and give a nice characterization of how the spectrum of the data covariance matrix affects the population risk in the over-parameterized regime.
Very recently, benign overﬁtting has also been studied in the setting of linear classiﬁcation [6, 19, 25].
Speciﬁcally, [19] studied the setting where the data inputs are Gaussian and the labels are generated 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
from a ground truth linear model with label ﬂipping noise, and showed equivalence between the hard-margin support vector machine (SVM) solution and the minimum norm interpolator to study benign overﬁtting. [6, 25] studied the benign overﬁtting phenomenon in sub-Gaussian/Gaussian mixture models and established population risk bounds for the maximum margin classiﬁer. [6] leveraged the implicit bias of gradient descent for logistic regression [22] to establish the risk bound. [25] established an equivalence result between classiﬁcation and regression for isotropic Gaussian mixture models. While these results have offered valuable insights into the benign overﬁtting phenomenon for (sub-)Gaussian mixture classiﬁcation, they still have certain limitations. Unlike the results in the regression setting where the eigenvalues of the data covariance matrix play a key role, the current results for Gaussian/sub-Gaussian mixture models do not show the impact of the spectrum of the data covariance matrix on the risk.
In this paper, we study the benign overﬁtting phenomenon in a general sub-Gaussian mixture model that covers both the isotropic and anisotropic settings, where the d-dimensional features from two classes have the same covariance matrix Σ but have different means µ and −µ respectively. We consider the over-parameterized setting where d is larger than the sample size n, and prove a risk bound for the maximum margin classiﬁer. We show that under certain conditions on eigenvalues of Σ, the mean vector µ and the sample size n, the maximum margin classiﬁer for this problem is identical to the minimum norm interpolator. We then utilize this result to establish a tight population risk bound of the maximum margin classiﬁer. Our result reveals how the eigenvalues of the covariance matrix Σ affect the benign property of the classiﬁcation problem, and is tighter and more general than existing results on sub-Gaussian/Gaussian mixture models. The contributions of this paper are as follows:
• We establish a tight population risk bound for the maximum margin classiﬁer. Our bound works for both the isotropic and anisotropic settings, which is more general than existing results in [6, 25].
When reducing our bound to the setting studied in [6], our result gives a bound exp(−Ω(n(cid:107)µ(cid:107)4 2/d)), where n is the training sample size. Our bound is tighter than the risk bound exp(−Ω((cid:107)µ(cid:107)4 2/d)) in
[6] by a factor of n in the exponent. Our result also gives a tighter risk bound than that in [25]1 in the so-called “low SNR setting”: our result suggests that (cid:107)µ(cid:107)4 2 = ω(d/n) sufﬁces to ensure an o(1) population risk, while [25] requires (cid:107)µ(cid:107)4 2 = ω((d/n)3/2).
• We establish population risk lower bounds achieved by the maximum margin classiﬁer under two different settings. In both settings, the lower bounds match our population risk upper bound up to some absolute constants. This suggests that our population risk bound is tight.
• Our analysis reveals that for a class of high-dimensional anisotropic sub-Gaussian mixture models, the maximum margin linear classiﬁer on the training data can achieve small population risk under mild assumptions on the sample size n and mean vector µ. Speciﬁcally, suppose that the eigenvalues of Σ are {λk = k−α}d k=1 for some parameter α ∈ [0, 1), and treat the sample size n as a constant. Then our result shows that to achieve o(1) population risk, the following conditions on (cid:107)µ(cid:107)2 sufﬁce: (cid:107)µ(cid:107)2 =



ω(d1/4−α/2),
ω((log(d))1/4),
ω(1). if α ∈ [0, 1/2), if α = 1/2, if α ∈ (1/2, 1).
More speciﬁcally, when α = 1/2, the condition on the mean vector µ only has a logarithmic dependency on the dimension d, and when α ∈ (1/2, 1), the condition on µ for benign overﬁtting is dimension free.
• Our proof of the population risk bound introduces some tight intermediate results, which may be of independent interest. Speciﬁcally, our proof utilizes the polarization identity to establish equivalence between the maximum margin classiﬁer and the minimum norm interpolator. This is, to the best of our knowledge, the ﬁrst equivalence result between classiﬁcation and regression for anisotropic sub-Gaussian mixture models.
Additional