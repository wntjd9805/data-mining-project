Abstract
Pre-trained language models have been successful on text classiﬁcation tasks, but are prone to learning spurious correlations from biased datasets, and are thus vul-nerable when making inferences in a new domain. Prior work reveals such spurious patterns via post-hoc explanation algorithms which compute the importance of input features. Further, the model is regularized to align the importance scores with human knowledge, so that the unintended model behaviors are eliminated.
However, such a regularization technique lacks ﬂexibility and coverage, since only importance scores towards a pre-deﬁned list of features are adjusted, while more complex human knowledge such as feature interaction and pattern generalization can hardly be incorporated. In this work, we propose to reﬁne a learned language model for a target domain by collecting human-provided compositional explana-tions regarding observed biases. By parsing these explanations into executable logic rules, the human-speciﬁed reﬁnement advice from a small set of explanations can be generalized to more training examples. We additionally introduce a regular-ization term allowing adjustments for both importance and interaction of features to better rectify model behavior. We demonstrate the effectiveness of the proposed approach on two text classiﬁcation tasks by showing improved performance in target domain as well as improved model fairness after reﬁnement1. 1

Introduction
With recent advances in model architectures and pre-training techniques, neural language models [4, 31, 25] have achieved impressive results on a broad set of natural language processing (NLP) tasks, such as sentiment analysis and hate speech detection [3, 45]. However, when a source model (ﬁne-tuned on some upstream dataset) is applied to a target domain with a different data distribution, the model may suffer from poor performance due to some spurious feature patterns learned from the upstream dataset [33, 44, 6]. Moreover, some spurious patterns may cause unintended biases in the downstream tasks, resulting in fairness and trust concerns about the model [21].
Prior work suggests that humans can identify such spurious patterns through examining the visualized
“heat-map” (Fig. 1) produced by a post-hoc model explanation algorithm [13]. As a prominent example, feature attribution methods [42, 14, 19] interpret model prediction on an instance by assigning an importance (attribution) score to each input feature (or token, in the context of NLP tasks), which helps uncover an overemphasis or understatement of a speciﬁc feature (e.g., “Sweden” is overemphasized as indication of hate speech, as in Fig. 1). To alleviate these spurious patterns, recent attempts study model regularization methods that update models in a differentiable and incremental fashion, which looks to align feature attribution scores with the “intended” scores manually speciﬁed by human annotators [38, 35, 30]. For example, attribution scores on overemphasised, unintended tokens are decreased (to close to zero) through updating the model weights [21]. 1Code and data are available at https://github.com/INK-USC/expl-reﬁnement. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite these initial successes, existing model regu-larization methods have limited capacity in conveying complex human feedback regarding spurious patterns, and limited regularization strength as the regulariza-tion term is enforced on only instances associated with human feedback (while the vast amount of unlabeled data is not leveraged) [38, 36, 30, 47]. To pinpoint a spurious pattern precisely, an annotator needs to de-scribe it by composing multiple assertions regarding feature attribution and feature interaction (e.g., “to be a failure” modiﬁes “Sweden” in Fig. 1). However, previous work consider only the former [36, 30] and omit the latter when characterizing spurious patterns.
Moreover, reﬁning these data-hungry language models often requires large amount of labeled data. To extend the coverage of regularization, one must match (generalize) one human feedback to multiple (unlabeled) instances in the target domain – i.e., identify instances that potentially suffer from the same spurious patterns, and then regularize the model with a larger set of instances.
Figure 1: An illustration of post-hoc model ex-planation heat-map for hate speech detection.
A trained hate speech classiﬁer mis-classiﬁes the sentence as non-hateful. After observing the heat-map, human annotators may suggest that “Swe-den” be adjusted to neutral, and “failure” should contribute more to predicting hate speech.
To this end, we introduce Reﬁning Language Model with Compositional Explanation (REMOTE), a framework that alleviates spurious patterns of a trained model and addresses the aforementioned limitations, by soliciting complex and compositional explanations from human and reﬁning the model with broadened coverage during regularization (see Fig. 2 for an overview). Firstly, human annotators are shown the post-hoc explanations of the source model’s predictions on target domain data (Fig. 1). They are asked to describe the spurious patterns they ﬁnd and their suggestions to adjust the importance scores and interactions of features. Secondly, we extract executable ﬁrst-order logic rules from these human-provided compositional explanations. The execution of the logic rules are decomposed to several atomic operations. We devise softened versions of these operations so the logic rules provide noisy labels and reﬁnement advice to a larger number of instances in the target domain. Lastly, we update model weights according to the suggestions in the explanations, using the enlarged set of instances obtained in the previous step.
We highlight two major contributions of the REMOTE framework. First, to the best of our knowledge,
REMOTE is the ﬁrst work that studies gathering feature-level supervision from complex human explanations. Prior work [15, 47] has explored producing pseudo labels from explanations (i.e., what is the correct label?), while we focus on more concrete feature-level supervision (i.e., why is this label correct?) with the goal of reducing spurious patterns. Second, we quantify and regularize the interaction between features, in addition to feature attributions used in prior work. This greatly improves the expressiveness of human explanations by supporting more complex rationale that involves more than one feature.
We validate our approach on three pairs of datasets in hate speech classiﬁcation and sentiment analysis.
Compared with direct transfer (evaluate the source model on target domain data) and other baselines (distillation and weight regularization), we observe notable performance improvements after reﬁning the model with our proposed framework. In addition, we demonstrate that REMOTE can reduce unintended biases on group identiﬁers in hate speech detection. 2