Abstract
Gaussian Process models are a rich distribution over functions with inductive biases controlled by a kernel function. Learning occurs through optimisation of the kernel hyperparameters using the marginal likelihood as the objective. This work proposes nested sampling as a means of marginalising kernel hyperparameters, because it is a technique that is well-suited to exploring complex, multi-modal distributions. We benchmark against Hamiltonian Monte Carlo on time-series and two-dimensional regression tasks, ﬁnding that a principled approach to quantifying hyperparameter uncertainty substantially improves the quality of prediction intervals. 1

Introduction
Gaussian processes (GPs) represent a powerful non-parametric and probabilistic framework for performing regression and classiﬁcation. An important step in using GP models is the speciﬁcation of a covariance function and in turn setting the parameters of the chosen covariance function. Both these steps are jointly referred to as the model selection problem [Rasmussen and Williams, 2006].
The parameters of the covariance function are called hyperparameters as the latent function values take the place of parameters in a GP.
The bulk of the GP literature addresses the model selection problem through maximisation of the GP marginal likelihood p(y|θ). This approach called ML-II2 typically involves using gradient based opti-misation methods to yield point estimates of hyperparameters. The posterior predictive distribution is then evaluated at these optimised hyperparameter point estimates. Despite its popularity this classical approach to training GPs suffers from two issues: 1) Using point estimates of hyperparameters yield overconﬁdent predictions, by failing to account for hyperparameter uncertainty, and 2) Non-convexity of the marginal likelihood surface can lead to poor estimates located at local minima. Further, the presence of multiple modes can affect the interpretability of kernel hyperparameters. This work proposes a principled treatment of model hyperparameters and assesses its impact on the quality of prediction uncertainty. Marginalising over hyperparameters can also be seen as a robust approach to the model selection question in GPs.
The form of the kernel function inﬂuences the geometry of the marginal likelihood surface. For instance, periodic kernels give rise to multiple local minima as functions with different periodicities can be compatible with the data. Expressive kernels which are derived by adding/multiplying together primitive kernels to encode different types of inductive biases typically have many hyperparameters, exacerbating the local minima problem.
∗Equal contribution 2where ‘ML’ stands for marginal likelihood and ‘II’ denotes the second level of the hierarchy pertaining to hyperparameters 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The spectral mixture (SM) kernel proposed in Wilson and Adams [2013] is an expressive class of kernels derived from the spectral density reparameterisation of the kernel using Bochner’s Therorem
[Bochner, 1959]. The SM kernel has prior support over all stationary kernels which means it can recover sophisticated structure provided sufﬁcient spectral components are used. Several previous works [Kom Samo and Roberts, 2015, Remes et al., 2017, 2018, Benton et al., 2019] have attempted to further enhance the ﬂexibility of spectral mixture kernels, such as the introduction of a time-dependent spectrum. However, we postulate that the key limitation in the SM kernel’s performance lies not in its stationarity or expressivity, but in the optimisation procedure. The form of the SM kernel gives rise to many modes in the marginal likelihood, making optimisation especially challenging.
It therefore presents an excellent opportunity to test out nested sampling’s capabilities, as it is an inference technique which is highly effective at navigating multimodal distributions. We note that aside from being successfully applied to graphical models by Murray et al. [2006], nested sampling has been largely overlooked in the machine learning literature.
Below we provide a summary of our main contributions:
• Highlight some of the failure modes of ML-II training. We provide insights into the effectiveness of ML-II training in weak and strong data regimes.
• Present a viable set of priors for the hyperparameters of the spectral mixture kernel.
• Propose the relevance of Nested Sampling (NS) as an effective means of sampling from the hyperparameter posterior [see also Faria et al., 2016, Aksulu et al., 2020], particularly in the presence of a multimodal likelihood.
• We demonstrate that in several time series modelling tasks (where we predict the future given past observations), incorporating hyperparameter uncertainty yields superior prediction intervals. 2