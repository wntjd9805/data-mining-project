Abstract
Dealing with severe class imbalance poses a major challenge for many real-world applications, especially when the accurate classiﬁcation and generalization of minority classes are of primary interest. In computer vision and NLP, learning from datasets with long-tail behavior is a recurring theme, especially for naturally occurring labels. Existing solutions mostly appeal to sampling or weighting ad-justments to alleviate the extreme imbalance, or impose inductive bias to prioritize generalizable associations. Here we take a novel perspective to promote sample efﬁciency and model generalization based on the invariance principles of causality.
Our contribution posits a meta-distributional scenario, where the causal generating mechanism for label-conditional features is invariant across different labels. Such causal assumption enables efﬁcient knowledge transfer from the dominant classes to their under-represented counterparts, even if their feature distributions show ap-parent disparities. This allows us to leverage a causal data augmentation procedure to enlarge the representation of minority classes. Our development is orthogonal to the existing imbalanced data learning techniques thus can be seamlessly integrated.
The proposed approach is validated on an extensive set of synthetic and real-world tasks against state-of-the-art solutions. 1

Introduction
Learning with imbalanced datasets is a common yet still very challenging scenario in many machine learning applications. Typical scenarios include: (i) rare events, where the event prevalence is extremely low while their implications are of high cost, e.g., severe risks that people seek to avert
[56]; (ii) emerging objects in a dynamic environment, which call for quick adaptation of an agent to identify new cases with only a handful target examples and plentiful past experience [31]. A typical scenario in natural datasets is that the occurrence of different objects follows a power law distribution.
And in many situations, the accurate identiﬁcation of those rarer instances bears more signiﬁcant social-economic values, e.g., fraud detection [24], driving safety [27], nature conservation [50], social fairness [17], and public health [63, 92].
Notably, severe class imbalance and lack of minority labels are the two major difﬁculties in this setting, which render standard learning strategies unsuitable [57]. Without explicit statistical adjustments, the imbalance induces bias towards the majority classes. On the other hand, the lack of minority representations prevents the identiﬁcation of stable correlations that generalize in predictive settings.
In addition, due to technological advancements, more and higher dimensional data are routinely collected for analysis. This also inadvertently exacerbates the issue of minority modeling as there is an excess of predictors relative to the limited occurrence of minority samples.
∗Equal Contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Energy-based causal representation transfer alleviates tail imbalance for natural image classiﬁcation. The shaded region denotes label frequency in the iNaturalist data, with some representative images shown. Solid curves are for label-conditional F1 scores (higher is better) for the proposed energy-based causal representation transfer (ECRT), LDAM (state of the art) and the standard ERM. ECRT consistently outperforms the others, especially in the low-sample regime.
Various research efforts have been directed to address the above issues, with class re-balancing being the most popular heuristic. The two most prominent directions in the category include statistical resampling and reweighting. Resampling alters the exposure frequency during training, i.e., more for the minorities and less so for the majorities [29]. Alternatively, reweighting directly amends the relative importance of each sample based on their class [22], sampling frequency [7] and associated cost for misclassiﬁcation [30]. Recent developments also have considered class-sensitive and data-adaptive losses to more ﬂexibly offset the imbalance [10, 60, 65, 55, 80, 51]. While being intuitive and working reasonably well in practice, an important is that these approaches offer no protection against the over-ﬁtting of minority instances (see Figure 1)[91], a fundamental obstacle towards better generalization.
To circumvent this key limitation, inductive biases are often solicited to impose strong constraints that suppress spurious correlations that hinder out-of-sample generalization. A classical strategy is few-shot learning [31, 89, 40], where the majority examples are used to train meta-predictors and transferable features, leaving only a few parameters to tune for the minority data. In anomaly detection, methods such as one-class classiﬁcation instead regard minority classes as outliers that do not always associate with stable, recognizable patterns [67, 75].
Despite their relatively strong assumptions, these methods capitalize on their superior ability in generalizing in the low sample regime in empirical settings.
More recently, establishing causal invariance has emerged as a new, powerful learning principle for better generalization ability even under apparent distribution shifts [73]. In contrast to standard empirical risk minimization (ERM) schemes, where the generalization to similar data distributions is considered, causally-inspired learning instead embraces robustness against potential perturbations
[5]. This is achieved via only attending to causally relevant features and associations postulated to be invariant under different settings [70]. Speciﬁcally, contributions from spurious, unstable features are effectively blocked or attenuated. Interestingly, compromise of performance can be expected in those models [74], as a direct consequence of discarding useful (but non-causal) correlations in exchange for better causal generalization. Recently, [48] proposed non-parametric causal disentanglement of data representation to identify invariant relations.
This work explores the advancement of imbalanced data learning via adopting causal perspectives, with the insight that the key algorithm can be reformulated as an energy-based contrastive learning procedure to drastically improve efﬁciency and ﬂexibility. In recognition of the limitations dis-cussed above, we present energy-based causal representation transfer (ECRT): a novel imbalanced learning scheme that brings together ideas from causality, contrastive learning, energy modeling, data-augmentation and weakly-supervised learning, to address the identiﬁed weakness of existing solutions. Our key contributions are: (i) a causal representation encoder informed by an invariant generative mechanism based on generalized contrastive learning; (ii) integrated data-augmentation and source representation regularization techniques exploiting feature independence to enrich mi-2
nority representations that better balance the trade-off between utility and invariance; (iii) a key novelty is the derivation of an energy-based contrastive learning algorithm that greatly enhance model parallelism for large-label settings and extends generalized contrastive learning; and (iv) insightful discussions on the justiﬁcations for the use of the proposed approach. Our claims are supported by strong experimental evidence. 2 Preliminaries
Notation and problem deﬁnition. We use x ∈ Rp to denote the input data and z ∈ Rd for the predictive features extracted from x. Let y ∈ {1, . . . , M } be the class label. The number of training samples and those with label m are denoted as n and nm, respectively. We use E[·] to denote the expectation (average) of an empirical distribution, ai to denote a vector associated with the i-th sample, and [a]b to denote the b-th entry of vector a. For simplicity, we assume class M is the minority class, i.e., nm (cid:29) nM , for m ∈ {1, . . . , M − 1}. Throughout, we refer to X , Z and S as data, feature, and source spaces, respectively, as shown in Figure 2, and deﬁned below. Our goal is to accurately predict the label of minority instances with very limited training examples of it.
Generalization to multiple minority categories is straightforward.
Generalized contrastive learning and ICA The proposed approach is based on a generalized form of independent component analysis (ICA), which addresses the inverse problem of signal disentanglement [46]. Speciﬁcally, ICA decorrelates features Z of the observed signal X into a source signal representation S = fψ(Z), where fψ(z) is a smooth and invertible mapping known as the de-mixing function, and while assuming that the components of S are statistically independent, i.e., with density q(s) = (cid:81) j qj([s]j). Notationally, we call [s]j the j-th independent component (IC) of Z. While nonlinear ICA (NICA) is generally infeasible [19, 47], [48] has recently proposed a setting in which the identiﬁcation of NICA can be achieved, by requiring an additional auxiliary label y. Speciﬁcally, NICA assumes that source signals are conditionally independent given y, i.e., q(s|y) = (cid:81) j qj([s]j|y), then fψ(z) can be identiﬁed using generalized contrastive learning (GCL), whose implementation is detailed below.
GCL solves a generalized regression problem in which a critic function predicts whether la-bel y and representation z are correctly paired, i.e., congruent. We call (yi, zi) a congruent pair and (yj, zi) an incongruent pair if i (cid:54)= j.
Speciﬁcally, the critic is deﬁned as rν(y, z) = (cid:80)d
ν (·, ·) is a neural net-work with parameters ν, whose inputs are the label y and the a-th coordinate of s = fψ(z), denoted as [s]a. Then, GCL optimizes the fol-lowing objective:
ν (y, [s]a), where ra a=1 ra
Figure 2: Illustration of data space X , feature space
Z (predictive but entangled) and source space
S (independent, or disentangled) representations identiﬁed by ECRT for MNIST. arg min fψ,rν
Ei[h(−rν(yi, zi))] + Ej(cid:54)=i[h(rν(yj, zi))]
, (cid:125) (cid:123)(cid:122) (cid:124)
LGCL(fψ,rν ) (1) where h(r) = log(1 + exp(r)) is the softplus function. (1) seeks to optimize fψ(·) and rν(·, ·) by maximizing the discriminative power to tell apart congruent and incongruent pairs.
In fact, an interesting result showed by [48] revealed that maximizing the ability of the critic function rν(·, ·) for correctly identifying matching pairs (z, y) leads to the identiﬁcation (up to univariate transformation) of fψ(·), such that the components of s are conditionally independent given y. See the Supplementary Material (SM) for a formal exposition. 3 Energy-based Causal Representation Transfer
In this section, we describe the construction of energy-based causal representation transfer (ECRT): a causally informed data transformation and augmentation procedure to improve learning with imbalanced datasets. Our model assumes a shared causal data-generation procedure, which can be accurately identiﬁed by learning with the majority classes under assumed class-conditional representation independence. We obtain decorrelated representations that facilitate data augmentation and efﬁcient learning with minority classes. 3
Figure 3: Source space estimation module of ECRT. We use GCL to identify the demixing function s = fψ(z) via telling apart congruent & incongruent pairs.
Figure 4: Non-parametric source space augmentation based on shufﬂing.
The proposed model consists of the following components: (i) a feature encoder module z = eθ(x); (ii) two classiﬁcation modules, hφ(cid:48)(z) and hφ(s), for predicting label y from features z and sources s, respectively; (iii) a nonlinear ICA module for the de-mixing function s = fψ(z); (iv) a critic function rν(y, z) for GCL; and (v) a data augmentation module. Further, (θ, φ(cid:48), φ, ψ, ν) denote the parameters of all the neural-network-based modules. Algorithm 1 outlines a general workﬂow, and below, we elaborate on our assumptions and detail the implementation of ECRT. 3.1 Model assumptions
To enable knowledge transfer across classes, we make the following assump-tions:
Assumption 3.1. Let z be a sufﬁcient statistic (features) of x for predicting la-bel y, all class conditional feature distri-butions p(z|y) share a common ICA de-mixing function fψ(z).
Algorithm 1 Energy-based Causal Representation Transfer. 1. Pre-train encoder and predictor: eθ, hφ(cid:48) ← arg min{E[(cid:96)(h(e(x)), y)]} 2. NICA estimation with ﬁxed z = eθ(x): fψ, rν ← GCL({(z, y)}) % Equation (1) 3. Source space augmentation with ﬁxed s = fψ(z): (˜s, y) ← AUG({s|y = M }) % Equation (2) 4. Minority predictor modeling with augmented source: hφ ← arg min{E[(cid:96)(h(s, y = M )]+λE[(cid:96)(h(˜s, y = M )]}
This implies there exists a smooth invertible function fψ : Z → S, and a set of IC distributions m=1, that are linked to the conditional feature distributions p(z|y = m) via Sm =
{q(s|y = m)}M fψ(Zm), where Sm ∼ q(s|y = m) and Zm ∼ p(z|y = m). The subscript m in Sm and Zm indicates y = m, in a slight abuse of notation.
Importantly, f −1
ψ (s) : S → Z is the invariant causal mechanism underlying the features of observed data. This speciﬁcation, which is consistent with Assumption 3.1 enables the identiﬁcation of the shared generating process, namely, the de-mixing function fψ(z) that connects the likely dissimilar conditionals {p(z|y = m)}M m=1, as demonstrated in the context of NICA by [48] m=1 via the source conditionals {q(s|y = m)}M
A shortcoming of Assumption 3.1 is that the hypothesis supporting it is somewhat strong, untestable and may not hold in practice. However, we argue that the structural constraints imposed on the model by the assumption via the source conditionals q(s|y = m) and the invertibility of fψ(z), restrict the search space of the otherwise over-ﬂexible model space powered by neural networks. Further, the causal mechanism implied by f −1
ψ (s) enables an effective knowledge transfer mechanism across classes via q(s|y = m). 3.2 Energy-based Causal Representation Transfer
Encoder pre-training. To implement ECRT, we ﬁrst ﬁnd a good (reduced) feature representation of x highly predictive of label y. This can be achieved via supervised representation learning (see Figure 3), which optimizes an encoder and predictor pair (eθ(x), hφ(cid:48)(z)) to minimize the label prediction risk L(φ(cid:48)) (cid:44) E[(cid:96)(hφ(cid:48)(eθ(x)), y)], where (cid:96)(·, ·) is a suitable loss function, e.g., cross-entropy, hinge loss, etc. To avoid capturing spurious (non-generalizable) features that overﬁt the minority class, we advocate training only with majority samples at this stage; assuming that M > 2. Alternatively, one could also consider unsupervised feature extraction schemes, such as auto-encoders. Further, we also recommend using statistical adjustments such as importance weighting, to reduce the impact of data imbalance. 4
De-mixing representation with GCL. After obtaining a good feature representation Z = eθ(X), we proceed to learn the de-mixing function fψ(z), such that the coordinates of the source representa-tion S = fψ(Z) are (approximately) independent given the label y. This can be done by optimizing the GCL objective in (1) with respect to the feature representation and label pairings (y, z), adopting the masked auto-regressive ﬂow (MAF) [69] to model the smooth, invertible transformation fψ(z), which allows efﬁcient parallelization of the autoregressive architecture via causal masking [14]. The procedure is outlined in Figure 3.
Augmenting the minority. Inspired by [83], we artiﬁcially augment the minority feature representa-tions z via random permutations in the source space S, as shown in Figure 4. Provided the assumed conditional independence of the sources, the features Zm corresponding to label y = m are generated by Zm = f −1
ψ (Sm), where each dimension in the source representation [Sm]j ∼ q([s]j|y = m) are independently and implicitly sampled as described below. Speciﬁcally, using the (estimated) de-mixing function fψ(z), we can obtain an approximate empirical source distribution for each y = m, i.e., Sm (cid:44) {si = fψ(eθ(xi))|yi = m} = {si}nm i=1, where Sm is a collection of nm samples of q(s|y = m). Then, we can draw new artiﬁcial samples ˜sm ∼ q(s|y = m) by randomly permuting the coordinates within elements Sm independently via
]1, [sm o2
]2, · · · , [sm od
˜sm o = ([sm (2) o1 where o = (o1, · · · , od) is a random permutation of (1, . . . , nm). Note we have used ˜sm o to emphasize that the source point is artiﬁcially created via permutation o. We call this procedure nonparametric augmentation because it does not make distributional assumptions for q(s|y = m). However, below we will discuss its limitations and consider an alternative where a parametric form is assumed. While it is tempting to reﬁne the predictor hφ(cid:48)(z) with artiﬁcial features augmented via ˜zm = f −1
ψ (˜sm), in
Section 3.3 we will argue that it stands to beneﬁt more from training a new predictor directly based on source representations, i.e., hφ(s), without the need for inverting fψ(z).
]d),
Model reﬁnement. Now we can leverage the augmented data to reﬁne the prediction model. For minority class y = M , we optimize the following objective
LAUG(φ(cid:48)) = L(φ(cid:48)) + λ(E
˜ZM [(cid:96)(hφ(cid:48)(˜zM ), M )] − E (3) where L(φ(cid:48)) is the loss used for pre-training. Conceptually, (3) replaces a portion of the minority samples with augmentations. The trade-off parameter λ ∈ [0, 1] encodes the relative conﬁdence for trusting the artiﬁcially generated representations ˜Z obtained from SM for the minority label y = M . Further, at this stage we found it’s beneﬁcial to ﬁx the encoder module to prevent the de-mixing function to accommodate the changes in the encoder which in practice may cause instability during training.
ZM [(cid:96)(hφ(cid:48)(z), M )]),
M
Challenges with naïve implementation. We identify three major issues with naïvely implemented
ECRT, to be addressed in the section below: (i) Representation conﬂict: since the GCL solution is not unique, we do observe naïve GCL training drifts among viable source representations whose per-formance differ considerably, causing stability concerns; (ii) Costly augmentation: MAF inversions dominate the computation load during training, which becomes prohibitive in high dimensions; and (iii) Gridding artifact: a small minority sample size leads to pronounced augmentation bias when sampling nonparametrically via (2), manifested as a rectangular-shaped grid (see Figure 5). 3.3
Improving causal representation transfer
Energy-based GCL. Our key insight to improve GCL comes from the fact that Equation (1) is essentially learning the density ratio between the joint and product of marginals, i.e., p(x,y) p(x)p(y) . This immediately reminds us the recent literature on contrastive mutual information (MI) estimators, such as InfoNCE [71]. In such works, a variational lower bound of MI is derived, and the algorithm optimizes a critic function using the positive samples from the joint distribution, and the negative samples from the product of the marginals. At their optimal value, these critics recover the density ratio or a transformation of it. Our development is based on the recent work of [37], using an energy-perspective to improve contrastive learning. Speciﬁcally, we will be using a variant of the celebrated
Donsker-Varadhan (DV) estimator [28], and applied Fenchel duality trick to compute a solution
[32, 81, 23]. Speciﬁcally, the Fenchel-Donsker-Varadhan (FDV) estimator takes the following form:
IFDV (cid:44) ˆI K
DV({xi, yi}) + (cid:80) (cid:80) j exp[(gθ(xi, yj) − gθ(xi, yi))/τ ] j exp[(ˆgθ(xi, yj) − ˆgθ(xi, yi))/τ ]
+ 1, (4) 5
and interest is our critic of gθ(xi, yi) k(cid:48)=1 exp(g(x1, y(cid:48)
ˆI K g(x1, y1) −
DV({xk, yk}) where log((cid:80)K k))/K) is the Donsker-Varadhan (DV) estimator [28] for the MI.
Using the same parameterization used in GCL recovers the same causal identiﬁcation property (see Appendix for details). Compared to the original GCL formulation, we are now using multiple negative samples instead of one, which greatly boosts learning efﬁciency [38]. And this can be efﬁciently implemented with the bilinear critic trick [15, 13, 37] so that all in-batch data can be used as negatives. In our context, it greatly boosts training efﬁciency when dealing with a large number of different classes. See Algorithm S1 in Appendix.
=
Regularizing the data likelihood. Recall GCL solutions can only be identiﬁed up to an invertible transformation of each dimension [48], and the predictive performance of different valid GCL solutions can vary signiﬁcantly. Empirically, we observe that a naïve implementation of GCL often leads to source representations that are densely packed (see Figure S1 in the SM). This is undesirable, when decoding back to the feature space and making useful predictions, the neural network predictor will need to be expansive, i.e., requiring a large Lipschitz constant, thereby sacriﬁcing optimization stability and model generalization according to existing learning theory [20, 87].
To encourage source representations that are less condensed, we consider a simple, intuitive strategy consisting of regularizing the source representation with the log-likelihood in feature space. This likelihood can be easily obtained with MAF using (cid:96)FLOW(fψ) deﬁned in the SM. Following common practice, we set source prior p(s) to the standard Gaussian, and optimize the following likelihood-regularized GCL objective:
˜LGCL(fψ, rν) = LGCL(fψ, rν) + ρLFLOW(fψ), (5) where ρ > 0 is the regularization strength. Naturally, this regularization will encourage the global source representation identiﬁed by GCL to be more consistent with a Gaussian-shaped distribution.
An alternative interpretation for the likelihood-regularized objective in (5) is that it can be understood as a relax-ation to the conditional independence (Assumption 3.1). To see this, recall the likelihood objective obtained from the invertible neural networks (INN) alone attempts to map the source rep-resentations to be unconditionally in-dependent, as opposed to the con-ditional independence assumed by
NICA. The regularized formulation (5) provides a safe “fall-back” mode in case Assumption 3.1 is violated. This also motivates us to consider an important variant: making multiple class-dependent source priors, i.e., pm(s) for each class label m ∈ {1, · · · , M } in (5), whose parameters (i.e., mean and variance) are jointly learned with other model parameters. Compared to the fully non-parametric objective (1), this strategy further encourages the source representations to be independent given the class labels, and it enables parametric data augmentation, i.e., sampling from the parametric label priors instead of permuting the indices. We refer to this variant as ECRT-MP, where MP stands for multiple priors. And similarly, ECRT-1P refers to the case when a single prior is used. We have found that ECRT-MP performs better in most cases, and consequently, ECRT means ECRT-MP by default.
Figure 5: Comparison of different source augmentations over-laid on the ground-truth distribution (gray dots). A severe gridding artifact is observed in the low-sample regime for the nonparametric scheme, whereas the parametric augmentation closely matches the oracle in distribution.
Modeling in the source space. Rather than modeling the predictor hφ(cid:48)(z) in the feature space Z, we advocate instead for building the predictor directly in the source space S, i.e., modeling with hφ(s). This practice enjoys several beneﬁts: (i) Easy & robust augmentation: many designs of high-dimensional ﬂows are asymmetric computationally, and inverting a MAF is not only d times more costly than a forward pass, it is also numerically unstable at the boundary. Direct modeling in the source space circumvents the difﬁculties associated with MAF inversions during data augmentation; (ii) Feature whitening: the source representation identiﬁed by GCL is component-wise independent, and literature documents abundant empirical evidence that similar de-correlation based pre-processing, commonly known as whitening, beneﬁts learning [45, 6, 49].
Parametric augmentation. When the number of minority observations is scarce, the above non-parametric indices-shufﬂing augmentation suffers from the gridding artifact (Figure 5). This artifact ampliﬁes the augmentation bias in the low-sample regime. To overcome this limitation, we empirically observe that the estimated class-conditional source distributions are usually Gaussian-like after the 6
likelihood regularization (especially so when label conditional priors are used). In these situations, a parametric augmentation that draws synthetic source samples from a Gaussian distribution matched to the empirical mean and variance of minority source representations is more efﬁcient. 3.4
Insights and remarks
To better appreciate the gains and limitations expected from ECRT, we compile a few complementary arguments below, through the lens of very different perspectives.
Why causal augmentation works. It is helpful to understand the gains from ECRT’s causal augmen-tation beyond the heuristic that permuting the ICs provides more training samples for the minority class. [83] considered a similar causal augmentation procedure for few-shot learning, and provided two major theoretical arguments: (i) the risk estimator based on the augmented examples is the uniformly minimum variance unbiased estimator given the accurate estimation of fψ (see Theorem 1,
[83]); and (ii) with high probability, the generalization gap can be bounded by the approximation error of fψ (see Theorem 2, [83]). In the SM, we give arguments that our causal augments give the
‘best’ label-conditional distribution estimate. 2 ), a superior rate of O(n−η) where η ∈ [ 1
Speedup from shared embedding. While for typical supervised learning tasks the generalization bound scale as O(n− 1 2 , 1] is possible, if there exists abundant data for an alternative, yet related task that shares the same feature embedding (see Theorem 3, [72]). Note n refers to the size of labeled data directly related to the strong task of interest, in our case, prediction of minority labels. Our ECRT employs GCL to identify one such common embedding, i.e., the source space, using the majority examples, and consequently, improves predictions on the minority class.
Representation whitening. Our ECRT causally disentangles representation [78, 84] via de-correlating the representations conditionally. Extensive empirical evidence has pointed to the fact that such representation de-correlation, more commonly known as data whitening [52], is expected to con-siderably improve learning efﬁciency [18]. This beneﬁt has been attributed to the better conditioning of the Fisher information matrix for gradient-based optimization [25], thus rectifying second-order curvatures to accelerate convergence. Our source space modeling explicitly separates the task of representation disentanglement, and in turn, helps the prediction network to focus on its primary goal.
Potential limitations. The setting considered by ECRT is restrictive in that it precludes the learning of useful, yet non-transferable features predictive of the minority labels. For instance, there might be a feature unique to the minority class. However, since the de-mixer fψ(z) is only trained on the majority domains absent of this feature, it can not be accounted for by the ECRT model. This is a key limitation of causally inspired models, in that they are often too conservative for only retaining the invariant features, promoting cross-domain generalization at the cost of within-domain performance degradation [74]. 4