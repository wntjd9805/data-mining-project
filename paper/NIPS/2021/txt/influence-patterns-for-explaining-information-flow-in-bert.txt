Abstract
While “attention is all you need” may be proving true, we do not know why: attention-based transformer models such as BERT are superior but how information
ﬂows from input tokens to output predictions are unclear. We introduce inﬂuence patterns, abstractions of sets of paths through a transformer model. Patterns quantify and localize the ﬂow of information to paths passing through a sequence of model nodes. Experimentally, we ﬁnd that signiﬁcant portion of information ﬂow in BERT goes through skip connections instead of attention heads. We further show that consistency of patterns across instances is an indicator of BERT’s performance.
Finally, we demonstrate that patterns account for far more model performance than previous attention-based and layer-based methods. 1

Introduction
Previous works show that transformer models such as BERT [7] encode various linguistic concepts [24, 40, 13], some of which can be associated with internal components of each layer, such as internal embeddings or attention weights [24, 40, 15, 33]. However, exactly how information ﬂows through a transformer from input tokens to the output predictions remains an open question. Recent attempts to answer this question include using attention-based methods, where attention weights are used as indicators of ﬂow of information [5, 2, 49], or layer-based approaches[15, 14, 33], which identify important network units in each layer.
In this paper, we examine the information ﬂow question through an alternative lens of gradient-based attribution methods. We introduce inﬂuence patterns, abstractions of sets of gradient-based paths through a transformer’s entire computational graph. We also introduce a greedy search procedure for efﬁciently and effectively ﬁnding patterns representative of concept-critical information ﬂow. Figure 1 provides an example of an inﬂuence pattern in BERT. We conduct an extensive empirical study of inﬂuence patterns for several NLP tasks: Subject-Verb Agreement (SVA), Reﬂexive Anaphora (RA), and Sentiment Analysis (SA). Our ﬁndings are summarized below.
• A signiﬁcant portion of information ﬂows in BERT go through skip connections and not attention heads, indicating that attention weights[2] alone are not sufﬁcient to characterize information ﬂow. In our experiment, we show that on average, important information ﬂow through skip connections 3 times more often than attentions.
• By visualizing the extracted patterns, we show how information ﬂow of words interact inside the model and BERT may use grammatically incorrect cues to make predictions.
• The consistency of inﬂuence patterns across instances of a task reﬂects BERT’s performance for that task.
∗Correspondence to kaijil@andrew.cmu.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
.
.
. x0
"the" x1
"funny" x2
"boy" x3
"[MASK]"
.
.
.
.
.
. h0 0 h0 1 h0 h0 2h0 h0 h0 2 2 2 2 h0 3
.
.
.
.
.
. h1 0 h1 1 h1 h1 h1 h1 h1 2 2 2 2 2 h1 3
.
.
.
.
.
. h2 0 h2 1 h2 h2 h2 h2 h2 2 2 2 2 2 h2 3
.
.
.
.
.
. h3 0 h3 1 h3 2 h3 h3 h3 h3 h3 3 3 3 3 3
.
.
.
.
.
. y0 y1 y2 y3
.
.
.
.
.
. h1 0 h1 1 h1 h1 h1 h1 h1 h1 2 2 2 2 2 2 h1 3
.
.
.
QoI(y3) a1,1 2 a1,2 2 a1,3 2
.
.
. a1,A 2 s1 2 h2 h2 h2 h2 h2 h2 2 2 2 2 2 2
Figure 1: BERT architecture (left) and details of a transformer layer (right) for an instance of the
SVA task, which evaluates whether the model chooses the correct verb form is over are for [MASK] to agree with the subject. An example of a pattern is highlighted with red nodes. x and h are input and internal embeddings, a attention heads, y output logits, QoI the function for computing quantity of interest.
• Through ablation experiments, we ﬁnd that inﬂuence patterns account for information
ﬂows in BERT on average 74% and 25% more accurately than prior attention-based and layer-based explanation methods[2, 23, 9], respectively. 2