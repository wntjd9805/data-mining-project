Abstract
Neural networks can represent and accurately reconstruct radiance ﬁelds for static 3D scenes (e.g., NeRF). Several works extend these to dynamic scenes captured with monocular video, with promising performance. However, the monocular setting is known to be an under-constrained problem, and so methods rely on data-driven priors for reconstructing dynamic content. We replace these priors with measurements from a time-of-ﬂight (ToF) camera, and introduce a neural repre-sentation based on an image formation model for continuous-wave ToF cameras.
Instead of working with processed depth maps, we model the raw ToF sensor mea-surements to improve reconstruction quality and avoid issues with low reﬂectance regions, multi-path interference, and a sensor’s limited unambiguous depth range.
We show that this approach improves robustness of dynamic scene reconstruction to erroneous calibration and large motions, and discuss the beneﬁts and limitations of integrating RGB+ToF sensors that are now available on modern smartphones.

Introduction 1
Novel-view synthesis (NVS) is a long-standing problem in computer graphics and computer vision, where the objective is to photorealistically render images of a scene from novel viewpoints. Given a number of images taken from different viewpoints, it is possible to infer both the geometry and appearance of a scene, and then use this information to synthesize images at novel camera poses.
One of the challenges associated with NVS is that it requires a diverse set of images from different perspectives to accurately represent the scene. This might involve moving a single camera around a static environment [4, 16, 31, 32, 36], or using a large multi-camera system to capture dynamic events from different perspectives [2, 7, 24, 38, 44, 56]. Techniques for dynamic NVS from a monocular video sequence have also demonstrated compelling results, though they suffer from various visual artifacts due to the ill-posed nature of this problem [26, 37, 42, 50, 52]. This requires introducing priors, often deep learned, on the dynamic scene’s depth and motion.
In parallel, mobile devices now have camera systems with both color and depth sensors, including
Microsoft’s Kinect and HoloLens devices, and the front and rear RGBD camera systems in the iPhone and iPad Pro. Depth sensors can use stereo or structured light, or increasingly the more accurate time-of-ﬂight principle for measurements. Although depth sensing technology is more common than ever, many NVS techniques currently do not exploit this additional source of visual information.
To improve NVS performance, we propose TöRF1, an implicit neural representation for scene appearance that leverages both color and time-of-ﬂight (ToF) images, as depicted in Figure 1. This
∗Correspondence should be addressed to Benjamin Attal: battal@andrew.cmu.edu. 1TöRF = ToF + NeRF. Pronounced just like ‘NeRF’ but starts with a ‘T’. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Acquisition process (b) Color images (c) Phasor images (d) Novel Image
Figure 1: Illustration of time-of-ﬂight radiance ﬁelds. (a) We move a handheld imaging system around a dynamic scene, capturing (b) color images and (c) raw phasor images from a continuous-wave time-of-ﬂight (C-ToF) camera. (d) Then, we optimize for a continuous neural radiance ﬁeld of the scene that predicts the captured color and phasor images. This allows novel view synthesis. reduces the number of images required for static NVS problem settings, compared with just using a color camera. Further, the additional depth information makes the monocular dynamic NVS problem more tractable, as it directly encodes information about the geometry of the scene. Most importantly, rather than using depth directly, we show that using ‘raw’ ToF data—in the form of phasor images [12] that are normally used to derive depth—is more accurate as it allows the optimization to correctly handle geometry that exceeds the sensor’s unambiguous range, objects with low reﬂectance, and regions affected by multi-path interference, leading to better dynamic scene view synthesis. The contributions of our work include:
• A physically-based neural volume rendering model for raw continuous-wave ToF images;
• A method to optimize a neural radiance ﬁeld of dynamic scenes with information from color and continuous-wave ToF sensors;
• Quantitative and qualitative evaluation on synthetic and real scenes showing better view synthesis than NeRF [32] for few input views, and than two dynamic scene baselines [26, 52]. 2