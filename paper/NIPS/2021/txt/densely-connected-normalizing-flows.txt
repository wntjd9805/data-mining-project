Abstract
Normalizing ﬂows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood evaluation and efﬁcient sampling. However, their effective capacity is often insufﬁ-cient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as cross-unit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nyström self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show signiﬁcant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets.1 1

Introduction
One of the main tasks of modern artiﬁcial intelligence is to generate images, audio waveforms, and natural-language symbols. To achieve the desired goal, the current state of the art uses deep compositions of non-linear transformations [1, 2] known as deep generative models [3, 4, 5, 6, 7].
Formally, deep generative models estimate an unknown data distribution pD given by a set of i.i.d. samples D = {x1, ..., xn}. The data distribution is approximated with a model distribution pθ deﬁned by the architecture of the model and a set of parameters θ. While the architecture is usually handcrafted, the set of parameters θ is obtained by optimizing the likelihood across the training distribution pD:
θ∗ = argmin
Ex∼pD [− ln pθ(x)].
θ∈Θ (1)
Properties of the model (e.g. efﬁcient sampling, ability to evaluate likelihood etc.) directly depend on the deﬁnition of pθ(x), or decision to avoid it. Early approaches consider unnormalized distribution
[3] which usually requires MCMC-based sample generation [8, 9, 10] with long mixing times. Alter-natively, the distribution can be autoregressively factorized [7, 11], which allows likelihood estimation and powerful but slow sample generation. VAEs [4] use a factorized variational approximation of the latent representation, which allows to learn an autoencoder by optimizing a lower bound of the likelihood. Diffussion models [12, 13, 14] learn to reverse a diffusion process, which is a ﬁxed
Markov chain that gradually adds noise to the data in the opposite direction of sampling until the signal is destroyed. Generative adversarial networks [5] mimic the dataset samples by competing in a minimax game. This allows to efﬁciently produce high quality samples [15], which however often do not span the entire training distribution support [16]. Additionally, the inability to "invert" the generation process in any meaningful way implies inability to evaluate the likelihood.
Contrary to previous approaches, normalizing ﬂows [6, 17, 18] model the likelihood using a bijective mapping to a predeﬁned latent distribution p(z), typically a multivariate Gaussian. Given the bijection 1Code available at: https://github.com/matejgrcic/DenseFlow 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
fθ, the likelihood is deﬁned using the change of variables formula: pθ(x) = p(z) (cid:12) (cid:12) (cid:12) (cid:12) det
∂z
∂x (cid:12) (cid:12) (cid:12) (cid:12)
, z = fθ(x). (2)
This approach requires computation of the Jacobian determinant (det ∂z
∂x ). Therefore, during the construction of bijective transformations, a great emphasis is placed on tractable determinant compu-tation and efﬁcient inverse computation [18, 19]. Due to these constraints, invertible transformations require more parameters to achieve a similar capacity compared to standard NN building blocks [20].
Still, modeling pθ(x) using bijective formulation enables exact likelihood evaluation and efﬁcient sample generation, which makes this approach convenient for various downstream tasks [21, 22, 23].
The bijective formulation (2) implies that the input and the latent representation have the same dimensionality. Typically, convolutional units of normalizing-ﬂow approaches [18] internally inﬂate the dimensionality of the input, extract useful features, and then compress them back to the original dimensionality. Unfortunately, the capacity of such transformations is limited by input dimensionality
[24]. This issue can be addressed by expressing the model as a sequence of bijective transformations
[18]. However, increasing the depth alone is a suboptimal approach to improve capacity of a deep model [25]. Recent works propose to widen the ﬂow by increasing the input dimensionality [24, 26].
We propose an effective development of that idea which further improves the performance while relaxing computational requirements.
We increase the expressiveness of normalizing ﬂows by incremental augmentation of intermediate latent representations with Gaussian noise. The proposed cross-unit coupling applies an afﬁne transformation to the noise, where the scaling and translation are computed from a set of previ-ous intermediate representations. In addition, we improve intra-module coupling by proposing a transformation which fuses the global spatial context with local correlations. The proposed image-oriented architecture improves expressiveness and computational efﬁciency. Our models set the new state-of-the-art result in likelihood evaluation on ImageNet32 and ImageNet64. 2 Densely connected normalizing ﬂows
We present a recursive view on normalizing ﬂows and propose improvements based on incremental augmentation of latent representations, and densely connected coupling modules paired with self-attention. The improved framework is then used to develop an image-oriented architecture, which we evaluate in the experimental section. 2.1 Normalizing ﬂows with cross-unit coupling
Normalizing ﬂows (NF) achieve their expressiveness by stacking multiple invertible transformations
[18]. We illustrate this with the scheme (3) where each two consecutive latent variables zi−1 and zi are connected via a dedicated ﬂow unit fi. Each ﬂow unit fi is a bijective transformation with parameters θi which we omit to keep notation uncluttered. The variable z0 is typically the input x drawn from the data distribution pD(x). f3←→ · · · f1←→ z1 zK ∼ N (0, I). fK←→ zK, f2←→ z2 fi−1←→ zi fi←→ · · · (3) z0
Following the change of variables formula, log likelihoods of consecutive random variables zi and zi+1 can be related through the Jacobian of the corresponding transformation Jfi+1[18]: ln p(zi) = ln p(zi+1) + ln | det Jfi+1|.
This relation can be seen as a recursion. The term ln p(zi+1) can be recursively replaced either with another instance of (4) or evaluated under the latent distribution, which marks the termination step.
This setup is characteristic for most contemporary architectures [17, 18, 19, 27]. (4)
The standard NF formulation can be expanded by augmenting the input by a noise variable ei
[24, 26]. The noise ei subjects to some known distribution p∗(ei), e.g. a multivariate Gaussian.
We further improve this approach by incrementally concatenating noise to each intermediate latent representation zi. A tractable formulation of this idea can be obtained by computing the lower bound of the likelihood p(zi) through Monte Carlo sampling of ei: ln p(zi) ≥ Eei∼p∗(e) [ln p(zi, ei) − ln p∗(ei)] . (5) 2
The learned joint distribution p(zi, ei) approximates the product of the target distributions p∗(zi) and p∗(ei), which is explained in more detail in Appendix D. We transform the introduced noise ei with element-wise afﬁne transformation. Parameters of this transformation are computed by a learned non-linear transformation gi(z<i) of previous representations z<i = [z0, ..., zi−1]. The resulting layer hi can be deﬁned as: z(aug) i = hi(zi, ei, z<i) = [zi, σ (cid:12) ei + µ], (µ, σ) = gi(z<i). (6)
Square brackets [·, ·] denote concatenation along the features dimension. In order to compute the likelihood for (zi, ei), we need the determinant of the jacobian
∂z(aug) i
∂[zi, ei]
= (cid:20)I 0 diag(σ) 0 (cid:21)
.
Now we can express p(zi, ei) in terms of p(z(aug) i
) according to (4): ln p(zi, ei) = ln p(z(aug) i
) + ln | det diag(σ)|.
We join equations (5) and (8) into a single step: ln p(zi) ≥ Eei∼p∗(ei)[ln p(z(aug) i
) − ln p∗(ei) + ln | det diag(σ)|]. (7) (8) (9)
We refer the transformation hi as cross-unit coupling since it acts as an afﬁne coupling layer [17] over a group of previous invertible units. The latent part of the input tensor is propagated without change, while the noise part is linearly transformed. The noise transformation can be viewed as reparametrization of the distribution from which we sample the noise [4]. Note that we can conveniently recover zi from z(aug) by removing the noise dimensions. This step is performed during model sampling. i i
Fig. 1 compares the standard normalizing ﬂow (a) normalizing ﬂow with input augmentation [24] (b) and the proposed densely connected incremental augmentation with cross-unit coupling (c). Each
ﬂow unit f DF consists of several invertible modules mi,j and cross-unit coupling hi. The main novelty of our architecture is that each ﬂow unit f DF i+1 increases the dimensionality with respect to its predecessor f DF
. Cross-unit coupling hi augments the latent variable zi with afﬁnely transformed noise ei. Parameters of the afﬁne noise transformation are obtained by an arbitrary function gi which accepts all previous variables z<i. Note that reversing the direction does not require evaluating gi since we are only interested in the value of zi. For further clariﬁcation, we show the likelihood computation for the extended framework. i
Figure 1: Standard normalizing ﬂow [17, 18] (a), normalizing ﬂow with augmented input [24] (b), and the proposed incremental augmentation with cross-unit coupling (c). Unlike (b) which adds noise only to the input, (c) adds noise to the output of every unit except the last. 3
to z2, respectively. Let h1 be the cross-unit coupling from z1 to z(aug)
Example 1 (Likelihood computation) Let m1 and m2 be the bijective mappings from z0 to z1 and z(aug) 1 = [z1, σ (cid:12)e1 +µ]. 1
Assume σ and µ are computed by any non-invertible neural network g1. The network accepts z0 as the input. We calculate log likelihood of the input z0 according to the following sequence of equations: [transformation, cross-unit coupling, transformation, termination].
, z(aug) 1 ln p(z0) = ln p(z1) + ln | det Jf1 |, ln p(z1) ≥ Ee1∼p∗(e1)[ln p(z(aug)
) − ln p(e1) + ln | det diag(σ)|], 1 ln p(z(aug)
) = ln p(z2) + ln | det Jf2 |, 1 ln p(z2) = ln N (z2; 0, I). (σ, µ) = g1(z0), (10) (11) (12) (13)
We approximate the expectation using MC sampling with a single sample during training and a few hundreds of samples during evaluation to reduce the variance of the likelihood. Note however that our architecture generates samples with a single pass since the inverse does not require MC sampling.
We repeatedly apply the cross-unit coupling hi throughout the architecture to achieve incremental augmentation of intermediate latent representations. Consequently, the data distribution is modeled in a latent space of higher dimensionality than the input space [24, 26]. This enables better alignment of the ﬁnal latent representation with the NF prior. We materialize the proposed expansion of the normalizing ﬂow framework by developing an image-oriented architecture which we call DenseFlow. 2.2
Image-oriented invertible module
We propose a glow-like invertible module (also known as step of ﬂow [19]) consisting of activation normalization, 1 × 1 convolution and intra-module afﬁne coupling layer.
The attribute "intra-module" emphasizes distinction with respect to cross-unit coupling. Different than in the origi-nal glow design, our coupling network leverages advanced transformations based on dense connectivity and fast self-attention. All three layers are designed to capture complex data dependencies while keeping tractable Jacobians and efﬁcient inverse computation. For completeness, we start by reviewing elements of the original glow module [19].
ActNorm [19] is an invertible substitute for batch normaliza-tion [30]. It performs afﬁne transformation with per-channel scale and bias parameters: yi,j = s (cid:12) xi,j + b. (14)
Scale and bias are calculated as the variance and mean of the initial minibatch.
Invertible 1 × 1 Convolution is a generalization of chan-nel permutation [19]. Convolutions with 1 × 1 kernel are not invertible by construction. Instead, a combination of orthogonal initialization and the loss function keeps the ker-nel inverse numerically stable. The normalizing ﬂow loss maximizes ln | det Jf | which is equivalent to maximizing (cid:80) i ln |λi|, where λi are eigenvalues of the Jacobian. Main-taining a relatively large amplitude of the eigenvalues en-sures a stable inversion. The Jacobian of this transformation can be efﬁciently computed by LU-decomposition [19].
Afﬁne Coupling [18] splits the input tensor x channel-wise into two halves x1 and x2. The ﬁrst half is propagated without changes, while the second half is linearly trans-formed (15). The parameters of the linear transformation are calculated from the ﬁrst half. Finally, the two results are concatenated as shown in Fig. 2.
Figure 2: A glow-like module mi,j consist of ActNorm, 1x1 convolution and intra-module afﬁne coupling. The proposed intra-module coupling fuses the global context recovered by fast self-attention [28] and local correla-tions extracted by densely connected convolutions [29]. y1 = x1, y2 = s (cid:12) x2 + t, (s, t) = coupling_net(x1). (15) 4
Parameters s and t are calculated using a trainable network which is typically implemented as a residual block [18]. However, this setting can only capture local correlations. Motivated by recent advances in discriminative architectures [29, 31, 32], we design our coupling network to fuse both global context and local correlations as shown in Fig. 2: First, we project the input into a low-dimensional manifold. Next, we feed the projected tensor to a densely-connected block [29] and self-attention module [31, 33]. The densely connected block captures the local correlations
[34], while the self-attention module captures the global spatial context. Outputs of these two branches are concatenated and blended through a BN-ReLU-Conv unit. As usual, the obtained output parameterizes the afﬁne coupling transformation (15). Differences between the proposed coupling network and other network designs are detailed in related work.
It is well known that full-ﬂedged self-attention layers have a very large computational complexity.
This is especially true in the case of normalizing ﬂows which require many coupling layers and large latent dimensionalities. We alleviate this issue by approximating the keys and queries with their low-rank approximations according to the Nystrom method [28]. 2.3 Multi-scale architecture
We propose an image-oriented architecture which extends multi-scale Glow [19] with incremental augmentation through cross-unit coupling. Each DenseFlow block consists of several DenseFlow units and resolves a portion of the latent representation according to a decoupled normal distribution
[18]. Each DenseFlow unit f DF consists of N glow-like modules (mi = mi,N ◦ · · · ◦ mi,1) and cross-unit coupling (hi). Recall that each invertible module mi,j contains the afﬁne coupling network from Fig. 2 as described Section 2.2. i
The input to each DenseFlow unit is the output of the previous unit augmented with the noise and transformed in the cross-unit coupling fashion. The number of introduced noise channels is deﬁned as the growth-rate hyperparameter. Generally, the number of invertible modules in latter
DenseFlow units should increase due to enlarged latent representation. We stack M DenseFlow units to form a DenseFlow block. The last invertible unit in the block does not have the corresponding cross-unit coupling. We stack multiple DenseFlow blocks to form a normalizing ﬂow with large capacity. We decrease the spatial resolution and compress the latent representation by introducing a squeeze-and-drop modules [18] between each two blocks. A squeeze-and-drop module applies space-to-channel reshaping and resolves half of the dimensions according to the prior distribution.
We denote the developed architecture as DenseF low-L-k, where L is the total number of invertible modules while k denotes the growth rate. The developed architecture uses two independent levels of skip connections. The ﬁrst level (intra-module) is formed of skip connections inside every coupling network. The second level (cross-unit) connects DenseFlow units at the top level of the architecture.
Fig. 3 shows the ﬁnal architecture of the proposed model. Gray squares represent DenseFlow units.
Cross-unit coupling is represented with blue dots and dashed skip connections. Finally, squeeze-and-drop operations between successive DenseFlow blocks are represented by dotted squares. The proposed DenseFlow design applies invertible but less powerful transformations (e.g. convolution 1 × 1) on tensors of larger dimensionality. On the other hand, powerful non-invertible transformations
Figure 3: The proposed DenseFlow architecture. DenseFlow blocks consist of DenseFlow units (f DF
) and a Squeeze-and-Drop module [18]. DenseFlow units are densely connected through cross-unit coupling (hi). Each DenseFlow unit includes multiple invertible modules (mi,j) from Fig. 2. i 5
such as coupling networks perform most of their operations on lower-dimensional tensors. This leads to resource-efﬁcient training and inference. 3 Experiments
Our experiments compare the proposed DenseFlow architecture with the state of the art. Quantitative experiments measure the accuracy of density estimation and quality of generated samples, analyze the computational complexity of model training, as well as ablate the proposed contributions. Qualitative experiments present generated samples. 3.1 Density estimation
We study the accuracy of density estimation on CIFAR-10 [35], ImageNet [36] resized to 32 × 32 and 64 × 64 pixels and CelebA [37]. Tab. 1 compares generative performance of various contemporary models. Models are grouped into four categories based on factorization of the probability density.
Among these, autoregressive models have been achieving the best performance. Image Transformer
[38] has been the best on ImageNet32, while Routing transformer [39] has been the best on Ima-geNet64. The ﬁfth category contains hybrid architectures which combine multiple approaches into a single model. Hybrid models have succeeded to outperform many factorization-speciﬁc architectures.
The bottom row of the table presents the proposed DenseFlow architecture. We use the same
DenseFlow-74-10 model in all experiments except ablations in order to illustrate the general applica-bility of our concepts. The ﬁrst block of DenseFlow-74-10 uses 6 units with 5 glow-like modules in each DenseFlow unit, the second block uses 4 units with 6 modules, while the third block uses a single unit with 20 modules. We use the growth rate of 10 in all units. Each intra-module coupling starts with a projection to 48 channels. Subsequently, it includes a dense block with 7 densely connected layers, and the Nyström self-attention module with a single head. Since the natural images are discretized, we apply variational dequantization [27] to obtain continuous data which is suitable for normalizing ﬂows.
On CIFAR-10, DenseFlow reaches the best recognition performance among normalizing ﬂows, which equals to 2.98 bpd. Models trained on ImageNet32 and ImageNet64 achieve state-of-the-art density estimation corresponding to 3.63 and 3.35 bpd respectively. The obtained recognition performance is signiﬁcantly better than the previous state of the art (3.77 and 3.43 bpd). Finally, our model achieves competetive results on the CelebA dataset, which corresponds to 1.99 bpd. The likelihood is computed using 1000 MC samples for CIFAR-10 and 200 samples for CelebA and ImageNet. The reported results are averaged over three runs with different random seeds. One MC sample is enough for accurate log-likelihood estimation since the per-example standard deviation is already about 0.01 bpd and a validation dataset size N additionally divides it by
N . The reported results are averaged over seven runs with different random seeds. Training details are available in Appendix C.
√ 3.2 Computational complexity
Deep generative models require an extraordinary amount of time and computation to reach state-of-the-art performance. Moreover, contemporary architectures have scaling issues. For example, VFlow
[24] requires 16 GPUs and two months to be trained on the ImageNet32 dataset, while the NVAE
[56] requires 24 GPUs and about 3 days. This limits downstream applications of developed models and slows down the rate of innovation in the ﬁeld. In contrast, the proposed DenseFlow design places a great emphasis on the efﬁciency and scalability.
Tab. 2 compares the time and memory consumption of the proposed model with respect to competing architectures. We compare our model with VFlow [24] and NVAE [56] due to similar generative performance on CIFAR-10 and CelebA, respectively. We note that RTX 3090 and Tesla V100 deliver similar performance, while RTX2080Ti has a slightly lower performance compared to the previous two. However, since we model relatively small images, GPU utilization is limited by I/O performance.
In our experiments, training the model for one epoch on any of the aforementioned GPUs had similar duration. Therefore, we can still make a fair comparison. Please note that we are unable to include approaches based on transformers [38, 39, 58] since they do not report the computational effort for model training. 6
Table 1: Likelihood evaluation (in bits/dim) on standard datasets.
Variational
Autoencoders
Diffusion models
Autoregressive
Models
Normalizing
Flows
Hybrid
Architectures
Method
Conv Draw [40]
DVAE++ [41]
IAF-VAE [42]
BIVA [43]
CR-NVAE [44]
DDPM [13]
UDM (RVE) + ST [45]
Imp. DDPM [46]
VDM [47]
Gated PixelCNN [48]
PixelRNN [7]
PixelCNN++ [11]
Image Transformer [38]
PixelSNAIL [49]
SPN [50]
Routing transformer [39]
Real NVP [18]
GLOW [19]
Wavelet Flow [51]
Residual Flow [52] i-DenseNet [53]
Flow++ [27]
ANF [26]
VFlow [24] mAR-SCF [54]
MaCow [55]
SurVAE Flow [34]
NVAE [56]
PixelVAE++ [57]
δ-VAE [58]
DenseFlow-74-10 (ours)
CIFAR-10 32x32 3.58 3.38 3.11 3.08 2.51 3.70 3.04 2.94 2.65 3.03 3.00 2.92 2.90 2.85
-2.95 3.49 3.35
-3.28 3.25 3.08 3.05 2.98 3.22 3.16 3.08 2.91 2.90 2.83 2.98
ImageNet CelebA ImageNet 64x64
---2.48 1.86
-1.93
-----2.61
---3.02
----------2.03
--1.99 64x64 4.10
------3.53 3.40 3.57 3.63
---3.53 3.43 3.98 3.81 3.78 3.78
-3.69 3.66 3.66 3.80 3.69 3.70
---3.35 32x32 4.40
--3.96
----3.72 3.83 3.86
-3.77 3.80 3.85
-4.28 4.09 4.08 4.01 3.98 3.86 3.92 3.83 3.99
-4.00 3.92
-3.77 3.63
Table 2: Comparative analysis of the computational budget for training contemporary methods.
DenseFlow decreases the training complexity by an order of magnitude.
Dataset
CIFAR-10
ImageNet32
CelebA
Model
VFlow [24]
NVAE [56]
DenseFlow-74-10
VFlow [24]
NVAE [56]
DenseFlow-74-10
VFlow [24]
NVAE [56]
DenseFlow-74-10
Params 38M 257M 130M 38M
-130M
-153M 130M
GPU type
RTX 2080Ti
Tesla V100
RTX 3090
Tesla V100
Tesla V100
Tesla V100 n/a
Tesla V100
Tesla V100
GPUs Duration (h) BPD
∼500 2.98 2.91 55 2.98 250
∼1440 3.83 3.92 70 3.63 310
-n/a 2.03 92 1.99 224 16 8 1 16 24 1 n/a 8 1 3.3
Image generation
Normalizing ﬂows can efﬁciently generate samples. The generation is performed in two steps. We ﬁrst sample from the latent distribution and then transform the obtained latent tensor through the inverse mapping. Fig. 4 shows unconditionally generated images with the model trained on ImageNet64.
Fig. 5 shows generated images using the model trained on CelebA. In this case, we modify the latent distribution by temperature scaling with factor 0.8 [38, 19, 56]. Generated images show diverse hairstyles, skin tones and backgrounds. More generated samples can be found in Appendix G. The 7
developed DenseFlow-74-10 model generates minibatch of 128 CIFAR-10 samples for 0.96 sec. The result is averaged over 10 runs on RTX 3090.
Figure 4: Samples from DenseFlow-74-10 trained on ImageNet 64 × 64.
Figure 5: Samples from DenseFlow-74-10 trained on CelebA. 3.4 Visual quality
The ability to generate high ﬁdelity samples is crucial for real-world applications of generative models.
We measure the quality of generated samples using the FID score [59]. The FID score requires a large corpus of generated samples in order to provide an unbiased estimate. Hence, we generate 50k samples for CIFAR-10, and CelebA, and 200k samples for ImageNet. The samples are generated using the model described in Sec. 3.1. The generated ImageNet32 samples achieve a FID score of 38.8, the CelebA samples achieve 17.1 and CIFAR-10 samples achieve 34.9 when compared to the corresponding training dataset. When compared with corresponding validation datasets, we achieve 37.1 on CIFAR10 and 38.5 on ImageNet32.
Tab. 3 shows a comparison with FID scores of other generative models. Our model outperforms contemporary autoregressive models [7, 60] and the majority of normalizing ﬂows [61, 52, 19]. Our
FID score is comparable with the ﬁrst generation of GANs. Similar to other NF models, the achieved
FID score is still an order of magnitude higher than current state of the art [62]. The results for
PixelCNN, DCGAN, and WGAN-GP are taken from [60]. 3.5 Ablations
Tab. 4 explores the contributions of incremental augmentation and dense connectivity in cross-unit and intra-module coupling transforms. We decompose cross-unit coupling into incremental augmentation of the ﬂow dimensionality (column 1) and afﬁne noise transformation (column 2). Column 3 ablates the proposed intra-module coupling network based on fusion of fast self-attention and a densely connected convolutional block with the original Glow coupling [19].
The bottom row of the table corresponds to a DenseFlow-45-6 model. The ﬁrst DenseFlow block has 5 DenseFlow units with 3 invertible modules per unit. The second DenseFlow block has 3 units with 5 modules, while the ﬁnal block has 15 modules in a single unit. We use the growth rate of 6. The top row of the table corresponds to the standard normalized ﬂow [18, 19] with three blocks and 15 modules per block. Consequently, all models have the same number of invertible glow-like modules.
All models are trained on CIFAR-10 for 300 epochs and then ﬁne-tuned for 10 epochs. We use the same training hyperparameters for all models. The proposed cross-unit coupling improves the density estimation from 3.42 bpd (row 1) to 3.37 bpd (row 3) starting from a model with the standard glow modules. When a model is equipped with our intra-module coupling, cross-unit coupling leads to improvement from 3.14 bpd (row 4) to 3.07 bpd (row 6). Hence, the proposed cross-unit coupling 8
Table 3: Evaluation of FID score on CIFAR-10.
Autoregressive
Models
Normalizing
Flows
GANs
Diffusion models
Hybrid
Architectures
Model
PixelCNN [7, 60]
PixelIQN [60] i-ResNet [61]
Glow [19]
Residual ﬂow [52]
ANF [26]
DCGAN [15, 60]
WGAN-GP [63, 60]
DA-StyleGAN V2 [62]
VDM [47]
DDPM [13]
UDM (RVE) + ST [45]
SurVAE-ﬂow [34] mAR-SCF [54]
VAEBM [64]
DenseFlow-74-10 (ours)
FID ↓ 65.93 49.46 65.01 46.90 46.37 30.60 37.11 36.40 5.79 4.00 3.17 2.33 49.03 33.06 12.19 34.90 improves the density estimation in all experiments. Both components of cross-unit coupling are important. Models with preconditioned noise outperform models with simple white noise (row 2 vs row 3, and row 5 vs row 6). A comparison of rows 1-3 with rows 4-6 reveals that the proposed intra-module coupling network also yields signiﬁcant improvements. We have performed two further ablation experiments with the same model. Densely connected cross-coupling contributes 0.01 bpd in comparison to preconditioning noise with respect to the previous representation only. Self-attention module contributes 0.01 bpd with respect to the model with only DenseBlock coupling on ImageNet 32 × 32.
Table 4: Ablations on the CIFAR-10 dataset with DenseFlow-45-6.
Latent variable augmentation (cid:55) (cid:51) (cid:51) (cid:55) (cid:51) (cid:51)
Pre-conditioned noise (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:51)
Intra-module coupling with two-way fusion (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51)
# 1 2 3 4 5 6
BPD 3.42 3.40 3.37 3.14 3.08 3.07 4