Abstract
Machine learning is vulnerable to a wide variety of attacks. It is now well under-stood that by changing the underlying data distribution, an adversary can poison the model trained with it or introduce backdoors. In this paper we present a novel class of training-time attacks that require no changes to the underlying dataset or model architecture, but instead only change the order in which data are supplied to the model. In particular, we ﬁnd that the attacker can either prevent the model from learning, or poison it to learn behaviours speciﬁed by the attacker. Furthermore, we ﬁnd that even a single adversarially-ordered epoch can be enough to slow down model learning, or even to reset all of the learning progress. Indeed, the attacks presented here are not speciﬁc to the model or dataset, but rather target the stochas-tic nature of modern learning procedures. We extensively evaluate our attacks on computer vision and natural language benchmarks to ﬁnd that the adversary can disrupt model training and even introduce backdoors. 1

Introduction
The data-driven nature of modern machine learning (ML) training routines puts pressure on data supply pipelines, which become increasingly more complex. It is common to ﬁnd separate disks or whole content distribution networks dedicated to servicing massive datasets. Training is often distributed across multiple workers. This emergent complexity gives a perfect opportunity for an attacker to disrupt ML training, while remaining covert. In the case of stochastic gradient descent (SGD), it assumes uniform random sampling of items from the training dataset, yet in practice this randomness is rarely tested or enforced. Here, we focus on adversarial data sampling.
It is now well known that malicious actors can poison data and introduce backdoors, forcing ML models to behave differently in the presence of triggers [12]. While such attacks have been shown to pose a real threat, they have so far required the attacker to perturb the dataset used for training.
We now show that by simply changing the order in which batches or data points are supplied to a model during training, an attacker can affect model behaviour. More precisely, we show that it is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Attack
Batch Reorder
Batch Reshufﬂe
Batch Replace
Adversarial initialisation [11]
BadNets [12]
Dynamic triggers [28]
Poisoned frogs [32]
Dataset knowledge Model knowledge Model speciﬁc Changing dataset Adding data Adding perturbations (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:55) (cid:51) (cid:55) (cid:51) (cid:51) (cid:55) (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51)
Table 1: Taxonomy of training time attacks. In green, we highlight our attacks. possible to perform integrity and availability attacks without adding or modifying any data points. For integrity, an attacker can reduce model accuracy or arbitrarily control its predictions in the presence of particular triggers. For availability, an attacker can increase the amount of time it takes for the model to train, or even reset the learning progress, forcing the model parameters into a meaningless state.
We present three different types of attacks that exploit Batch Reordering, Reshufﬂing and Replacing – naming them BRRR attacks. We show that an attacker can signiﬁcantly change model performance by (i) changing the order in which batches are supplied to models during training; (ii) changing the order in which individual data points are supplied to models during training; and (iii) replacing datapoints from batches with other points from the dataset to promote speciﬁc data biases. Furthermore, we introduce Batch-Order Poison (BOP) and Batch-Order Backdoor (BOB), the ﬁrst techniques that enable poisoning and backdooring of neural networks using only clean data and clean labels; an attacker can control the parameter update of a model by appropriate choice of benign datapoints.
Importantly, BRRR attacks require no underlying model access or knowledge of the dataset. Instead, they focus on the stochasticity of gradient descent, disrupting how well individual batches approximate the true distribution that a model is trying to learn.
To summarise, we make the following contributions in this paper:
• We present a novel class of attacks on ML models that target the data batching procedure used during training, affecting their integrity and availability. We present a theoretical analysis explaining how and why these attacks work, showing that they target fundamental assumptions of stochastic learning, and are therefore model and dataset agnostic1.
• We evaluate these attacks on a set of common computer vision and language benchmarks, using a range of different hyper-parameter conﬁgurations, and ﬁnd that an attacker can slow the progress of training, as well as reset it, with just a single epoch of intervention.
• We show that data order can poison models and introduce backdoors, even in a blackbox setup. For a whitebox setup, we ﬁnd that the adversary can introduce backdoors almost as well as if they used perturbed data. While a baseline CIFAR10 VGG16 model that uses perturbed data gets 99% trigger accuracy, the whitebox BOB attacker gets 91% ± 13 and the blackbox BOB attacker achieves 68% ± 19. 2