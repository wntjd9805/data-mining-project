Abstract
Generalization is a central challenge for the deployment of reinforcement learning (RL) systems in the real world. In this paper, we show that the sequential structure of the RL problem necessitates new approaches to generalization beyond the well-studied techniques used in supervised learning. While supervised learning methods can generalize effectively without explicitly accounting for epistemic uncertainty, we show that, perhaps surprisingly, this is not the case in RL. We show that generalization to unseen test conditions from a limited number of training conditions induces implicit partial observability, effectively turning even fully-observed MDPs into POMDPs. Informed by this observation, we recast the problem of generalization in RL as solving the induced partially observed Markov decision process, which we call the epistemic POMDP. We demonstrate the failure modes of algorithms that do not appropriately handle this partial observability, and suggest a simple ensemble-based technique for approximately solving the partially observed problem. Empirically, we demonstrate that our simple algorithm derived from the epistemic POMDP achieves signiﬁcant gains in generalization over current methods on the Procgen benchmark suite. 1

Introduction
Generalization is a central challenge in machine learning. However, much of the research on reinforcement learning (RL) has been concerned with the problem of optimization: how to master a speciﬁc task through online or logged interaction. Generalization to new test-time contexts has received comparatively less attention, although several works have observed empirically [1, 2, 3, 4] that generalization to new situations poses a signiﬁcant challenge to RL policies learned from a
ﬁxed training set of situations. In standard supervised learning, it is known that in the absence of distribution shift and with appropriate inductive biases, optimizing for performance on the training set (i.e., empirical risk minimization) translates into good generalization performance. It is tempting to suppose that the generalization challenges in RL can be solved in the same manner as empirical risk minimization in supervised learning: when provided a training set of contexts, learn the optimal policy within these contexts and then use that policy in new contexts at test-time.
Perhaps surprisingly, we show that such “empirical risk minimization” approaches can be sub-optimal for generalizing to new contexts in RL, even when these new contexts are drawn from the same distribution as the training contexts. As an anecdotal example of why this sub-optimality arises, imagine a robotic zookeeper for feeding otters that must be trained on some set of zoos. When placed
∗Equal contribution. 1 UC Berkeley, 2 Princeton University, 3 Facebook AI Research. Correspond to: dibya@berkeley.edu, jrahme@math.princeton.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Visualization of the robotic zookeeper example. Standard RL algorithms learn the classiﬁer strategy, since it is optimal in every training zoo, but this strategy is sub-optimal for generalization because peeking generalizes better than the classiﬁer at test-time. This failure occurs due to the following disconnect: while the task is fully-observed since the image uniquely speciﬁes the location of the otter habitat, to an agent that has limited training data, the location is implicitly partially observed at test-time because of the agent’s epistemic uncertainty about the parameters of the image classiﬁer. in a new zoo, the robot must ﬁnd and enter the otter enclosure. It can use one of two strategies: either peek through all the habitat windows looking for otters, which succeeds with 95% probability in all zoos, or to follow an image of a hand-drawn map of the zoo that unambiguously identiﬁes the otter enclosure, which will succeed as long as the agent is able to successfully parse the image. In every training zoo, the otters can be found more reliably using the image of the map, and so an agent trained to seek the optimal policy in the training zoos would learn a classiﬁer to predict the identity of the otter enclosure from the map, and enter the predicted enclosure. This classiﬁcation strategy is optimal on the training environments because the agent can learn to perfectly classify the training zoo maps, but it is sub-optimal for generalization, because the learned classiﬁer will never be able to perfectly classify every new zoo map at test-time. Note that this task is not partially observed, because the map provides full state information even for a memoryless policy. However, if the learned map classiﬁer succeeds on anything less than 95% of new zoos at test-time, the strategy of peeking through the windows, although always sub-optimal in the training environments, turns out to be a more reliable strategy for ﬁnding the otter habitat in a new zoo, and results in higher expected returns at test-time.
Although with enough training zoos, the zookeeper can learn a policy by solving the map classiﬁcation problem, to generalize optimally when given a limited number of zoos requires a more intricate policy that is not learned by standard RL methods. How can we more generally describe the set of behaviors needed for a policy to generalize from a ﬁnite number of training contexts in the RL setting? We make the observation that, even in fully-observable domains, the agent’s epistemic uncertainty renders the environment implicitly partially observed at test-time. In the zookeeper example, although the hand-drawn map provides the exact location of the otter enclosure (and so the enclosure’s location is technically fully observed), the agent cannot identify the true parameters of the map classiﬁer from the small set of maps seen at training time, and so the location of the otters is implicitly obfuscated from the agent. We formalize this observation, and show that generalizing optimally at test-time corresponds to solving a partially-observed Markov decision process that we call an epistemic POMDP, induced by the agent’s epistemic uncertainty about the test environment.
That uncertainty about MDP parameters can be modelled as a POMDP is well-studied in Bayesian RL when training and testing on a single task in an online setting, primarily in the context of exploration
[5, 6, 7, 8]. However, as we will discuss, this POMDP interpretation has signiﬁcant consequences for the generalization problem in RL, where an agent cannot collect more data online, and must instead learn a policy from a ﬁxed set of training contexts that generalizes to new contexts at test-time. We show that standard RL methods that do not explicitly account for this implicit partial observability can be arbitrarily sub-optimal for test-time generalization in theory and in practice. The epistemic
POMDP underscores the difﬁculty of the generalization problem in RL, as compared to supervised learning, and provides an avenue for understanding how we should approach generalization under the sequential nature and non-uniform reward structure of the RL setting. Maximizing expected return in an approximation of the epistemic POMDP emerges as a principled approach to learning policies that generalize well, and we propose LEEP, an algorithm that uses an ensemble of policies to approximately learn the Bayes-optimal policy for maximizing test-time performance.
The primary contribution of this paper is to use Bayesian RL techniques to reframe generalization in RL as the problem of solving a partially observed Markov decision process, which we call the epistemic POMDP. The epistemic POMDP highlights the difﬁculty of generalizing well in RL, as compared to supervised learning. We demonstrate the practical failure modes of standard RL methods, which do not reason about this partial observability, and show that maximizing test-time performance may require algorithms to explicitly consider the agent’s epistemic uncertainty during training. Our 2
work highlights the importance of not only ﬁnding ways to help neural networks in RL generalize better, but also on learning policies that degrade gracefully when the underlying neural network eventually does fail to generalize. Empirically, we demonstrate that LEEP, which maximizes return in an approximation to the epistemic POMDP, achieves signiﬁcant gains in test-time performance over standard RL methods on several ProcGen benchmark tasks. 2