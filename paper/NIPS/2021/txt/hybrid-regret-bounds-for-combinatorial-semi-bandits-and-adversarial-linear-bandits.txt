Abstract
This study aims to develop bandit algorithms that automatically exploit tendencies of certain environments to improve performance, without any prior knowledge regarding the environments. We ﬁrst propose an algorithm for combinatorial semi-bandits with a hybrid regret bound that includes two main features: a best-of-three-worlds guarantee and multiple data-dependent regret bounds. The former means that the algorithm will work nearly optimally in all environments in an adversarial setting, a stochastic setting, or a stochastic setting with adversarial corruptions. The latter implies that, even if the environment is far from exhibiting stochastic behavior, the algorithm will perform better as long as the environment is "easy" in terms of certain metrics. The metrics w.r.t. the easiness referred to in this paper include cumulative loss for optimal actions, total quadratic variation of losses, and path-length of a loss sequence. We also show hybrid data-dependent regret bounds for adversarial linear bandits, which include a ﬁrst path-length regret bound that is tight up to logarithmic factors. 1

Introduction
In this work, we consider two fundamental problem settings w.r.t. online decision problems: combi-natorial semi-bandits [42, 66, 4] and linear bandits [13, 17, 30]. In both problem settings, a player is given an action set A ∈ Rd, a compact subset of a d-dimensional vector space. In each round t at, where (cid:96)t ∈ Rd is a loss vector t, the player chooses an action at ∈ A and then incurs loss (cid:96)(cid:62) chosen by the environment. The action set in the combinatorial semi-bandit is assumed to be a subset of {0, 1}d, each element of which corresponds to a subset of [d] = {1, 2, . . . , d}. After choosing at ∈ A, the player can observe (cid:96)ti for all i such that ati = 1 in semi-bandits. Linear bandits are problems with even more limited feedback, ones in which the learner can only observe the incurred t at. For combinatorial semi-bandits problems with (cid:96)t ∈ [0, 1]d, there is a known algorithm with loss (cid:96)(cid:62)
√ an O( t a| ≤ 1 holds for any a ∈ A, algorithms with ˜O(d
T )-regret (where factors in log T and log d are ignored) have been developed [13, 17, 30]. These algorithms are optimal in terms of worst-case analysis. In fact, matching lower bounds of Ω(
T ) for linear bandits [22] are known. mdT )-regret bound [4], where m = maxa∈A (cid:107)a(cid:107)1. For linear bandits such that |(cid:96)(cid:62) mdT ) for combinatorial semi-bandits [3, 11] and of Ω(d
√
√
√
The worst-case optimal algorithms, however, tend to be too conservative in actual practice. This is because true worst-case environments are quite rare in real-world applications. Rather, the environments may have structures that are convenient for the learner, and it is desirable that the algorithm takes advantage of such structures to improve performance. To exploit such structures, two main categories of approaches have been studied: adapting to (nearly) stochastic environments and developing data-dependent regret bounds.
An example of the ﬁrst category is in reference to best-of-both-worlds (BOBW) algorithms [12, 62, 5, 61, 68, 70], which means that they work well for both adversarial and stochastic settings. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
√
These algorithms enjoy ˜O(
T )-regret bounds in an adversarial setting and, simultaneously, achieve
O( (log T )c
∆ )-regret in a stochastic setting with i.i.d. losses, where c ≥ 1 is a constant and ∆ represents the suboptimality gap deﬁned by mina∈A\a∗ E[(cid:96)(cid:62) t (a − a∗)] for an optimal action a∗ ∈ A. As shown in Table 1, for combinatorial semi-bandits, Zimmert et al. [71] provide a BOBW algorithm, both bounds of which are tight up to constant factors as matching lower bounds are known for adversarial settings [3, 11] as well as for stochastic settings [42].
Studies on the adversarial robustness of stochastic bandit algorithms [48, 27, 70] can be considered to provide another approach in the ﬁrst category, in which an adversary can corrupt stochastically-generated losses subject to the constraint that the total amount of the corruption is at most a parameter
C ≥ 0, referred to as the corruption level. This model includes both adversarial and stochastic settings and is closely related to studies on BOBW algorithms. In fact, the special cases of C = 0 and
C = Ω(T ) correspond to stochastic and adversarial settings, respectively. For the fundamental multi-armed bandit problem, Zimmert and Seldin [70] have proposed the Tsallis-INF algorithm, which achieves BOBW with tight regret bounds up to small constant factors, and which simultaneously is very robust w.r.t. corruptions; the degradation in the regret is only O(min{C,
}). Such algorithms are called best-of-three-worlds (BOTW) algorithms. As shown in Table 1, the semi-bandit algorithm by Zimmert et al. [71] is also a BOTW algorithm (Sto. +Adv. refers to the stochastic setting with C-adversarial corruptions).1 For linear bandits, Lee et al. [45] have recently developed a BOTW algorithm as well. (cid:113) CK log T
∆
In studies on data-dependent regret bounds [55, 68, 29, 57, 15, 14, 34] of the other category, we deﬁne measures of the difﬁculty of problem instances, which we refer to as difﬁculty indicators, and aim to develop algorithms so that the regret will be smaller for smaller instance-difﬁculty indicators.
Examples of difﬁculty indicators dealt with in this paper include L∗: the cumulative losses for an optimal action, Qq: the total quadratic variation of losses, and Vq: the path-length of losses, deﬁnitions of which are given in Table 3. Tables 1 and 2 show data-dependent regret bounds, including difﬁculty indicators for, respectively, combinatorial semi-bandits and adversarial linear bandits. For example, dL∗ log T )-regret bound given by Wei and Luo [68] achieves the semi-bandit algorithm with an O( dmT )-bound when L∗ = o(mT ), i.e., when much smaller regret than the worst-case optimal O( there exists an action for which the cumulative losses are much smaller than mT .
√
√
Given the various algorithms above, a new challenge arises: how, in practice, can we choose an appropriate algorithm? If the environment is expected to behave in an almost i.i.d. stochastic manner, either BOBW or BOTW algorithms would work well. If the environment is far from exhibiting stochastic behavior but is "easy" in terms of some difﬁculty indicator, algorithms with a corresponding data-dependent regret bound may work better. In practice, however, it is hard to tell what kind of environment we are working in until we have actually tried out the algorithm. 1.1 Contributions of this work
This work addresses the above-mentioned issue of algorithm selection by developing hybrid al-gorithms. Its main contribution is to develop a semi-bandit algorithm (Algorithm 1) that enjoys multiple data-dependent regret bounds as well. This can be seen as an an extension of the work on the multi-armed bandit problem by Ito [34] to combinatorial semi-bandits. In addition to this, for linear bandits, we provide a hybrid data-dependent regret bound.
√
For combinatorial semi-bandits problems, we propose Algorithm 1 with the regret bounds shown in
Table 1. More explicit statements are provided in Theorem 1, Corollary 1 (for the adversarial setting), and Corollary 2 (for the stochastic setting with/without adversarial corruption). As can be seen in the table, the regret bound for the adversarial setting encompasses three different data-dependent dL∗ log T )-bounds imply the nearly worst-case optimal bound of regret bounds. Note that O( dmT log T ), as L∗ ≤ mT always follows from the model deﬁnition. The new regret bound is of
O(
O(R +
RCm) for the corrupted stochastic setting, where R stands for the bound for the stochastic setting. Note that the regret bounds for the stochastic settings (with corruption) can be improved for some special cases, such as for size-invariant ((cid:107)a(cid:107)1 = m for all a ∈ A) or for a matroid constraint (A forms bases of a matroid), as described in Corollary 2. We would also like to stress that the
√
√ 1This bound is not explicitly stated in their paper, but can be shown via a straightforward modiﬁcation of the proof. A proof of this is given in Appendix B of the supplementary material. 2
Table 1: Regret bounds for combinatorial semi-bandits.
Reference
Regime
Audibert et al. [4]
Kveton et al. [42]
Neu [54]
Adv.
Sto.
Adv.
Wei and Luo [68] (Sec. 3.1) Adv.
Wei and Luo [68] (Sec. 4.1) Adv.
Wei and Luo [68] (Sec. 4.2) Adv.
Zimmert et al. [71]
Adv.
Sto.
Regret bound
√
O( dmT ) (cid:16) dm log T
∆ (cid:17)
O
O(m(cid:112)dL∗ log(d/m)) dQ2 log T )
O(
√
O(
O(
√
√
√ dV1 log T ) dL∗ log T )
O(
O dmT ) (cid:16) dm log T
∆ (cid:17) (cid:18) dm log T
∆ +
Sto. + Adv. O (cid:113) Cdm2 log T
∆ (cid:19)
O((cid:112)d min{L∗, Q2, V1} log T )
O (cid:16) dm log T
∆ (cid:17) (cid:18) dm log T
∆ + (cid:113) Cdm2 log T
∆ (cid:19)
[This work] (Algorithm 1)
Adv.
Sto.
Sto. + Adv. O
Table 2: Regret bounds for adversarial linear bandits. We assume that (cid:107)a(cid:107)p ≤ 1 for all A and (cid:107)(cid:96)t(cid:107)q ≤ 1 for all t, for some p, q ∈ [1, ∞] such that 1/p + 1/q = 1. ˜O(·) ignores factors of (log T )O(1) and (log d)O(1).
Reference
Bubeck et al. [13]
Hazan and Kale [29]
Bubeck et al. [15] (Cor. 4, 8)
Bubeck et al. [15] (Cor. 6)
Ito et al. [36]
[This work]
√
√
√
Regret bound
˜O(d
T )
˜O(d
ϑQ2)
˜O(d
ϑV2)
˜O(d3/2(cid:112)ϑVq)
˜O(d(cid:112)min{L∗, Qq})
˜O(d(cid:112)min{L∗, Qq, Vq})
Table 3: Deﬁnitions of parameters. number of rounds dimension of A
T d m maxa∈A (cid:107)a(cid:107)1
ϑ
C
L∗ mina∗ (cid:80)T (cid:80)T
Qq (cid:16)¯(cid:96) = 1
T (cid:80)T −1 self-concordant parameter corruption level t a∗ t=1 (cid:96)(cid:62) t=1 (cid:107)(cid:96)t − ¯(cid:96)(cid:107)2 q (cid:80)T t=1 (cid:96)t
Vq t=1 (cid:107)(cid:96)t − (cid:96)t+1(cid:107)q
∆ mina∈A\{a∗} E[(cid:96)(cid:62) (cid:17) t (a − a∗)] proposed algorithm is parameter-free, i.e., it does not require any prior information w.r.t. parameters
∆, L∗, Q2, V1, and C.
The proposed semi-bandit algorithm is based on a follow-the-regularized-leader (FTRL) frame-work [70, 71], combined with an optimistic prediction for the losses [58, 57, 68]. More precisely, it uses a mixture regularizer [14, 55, 71, 24] consisting of the log-barrier in variables xi and the
Shannon entropy in the complement (1 − xi) of xi with entry-wisely adaptive learning rates, by which BOTW is achieved. The regret analysis for the stochastic setting (with corruption) is based on self-bounding inequalities for the regret, similarly to what is seen in the analyses by Zimmert et al.
[71], Zimmert and Seldin [70]. In addition to this, by choosing optimistic predictors with simple gradient descent methods, we can achieve multiple data-dependent regret bounds as well. The most relevant algorithms are given by Wei and Luo [68] and Zimmert et al. [71]. The proposed algorithm differs from that of Wei and Luo [68] in that the former employs follow-the-regularized-leader rather than online mirror descent methods and uses a mixture regularizer. The major differences with the work by Zimmert et al. [71] are that Algorithm 1 uses a log-barrier rather than Tsallis entropy, and that it takes advantages of optimistic predictors. As far as we have managed to determine, it appears difﬁcult to combine Tsallis-entropy-based algorithms with optimistic predictors, as we discuss in
Subsection 4.2. Our work overcomes this difﬁculty by using a log-barrier regularizer, rather than
Tsallis entropy, in exchange for additional O( log T )-factors in worst-case regret bounds.
√ 3
Remark 1. In the previously mentioned study by Wei and Luo [68], the regret bounds for com-binatorial semi-bandits are not given explicitly. However, as stated just after Corollary 3 in their paper ("but they can be straightforwardly generalized to the semi-bandit case"), we can obtain the regret bounds in Table 1 via a simple calculation. To be more precise, their regret bounds in Table 1 dependent on Q2 and V1 can be reﬁned by replacing them with Q2(I ∗) := (cid:80)T i∈I ∗ ((cid:96)ti − ¯(cid:96)i)2 and V2(I ∗) := (cid:80)T −1 i = 1} for an optimal action a∗. This means that Algorithm 1 is not necessarily superior to their algorithms.
On the other hand, it should be noted that their algorithms require prior knowledge w.r.t. Q2(I ∗) and
V2(I ∗) to achieve corresponding regret bounds. i∈I ∗ |(cid:96)ti − (cid:96)t+1,i|, respectively, where we deﬁne I ∗ = {i ∈ [d] | a∗ (cid:80) (cid:80) t=1 t=1
For linear bandits problems, we provide the hybrid data-dependent regret bounds shown in Table 2 and in Theorem 3, which holds for p ∈ [2, ∞] and q ∈ [1, 2] such that 1/p + 1/q = 1, under the assumption of (cid:107)a(cid:107)p ≤ 1 for all a ∈ A and (cid:107)(cid:96)t(cid:107)q ≤ 1 for all t. The parameter ϑ ≥ 1 is associated with a self-concordant barrier over the convex hull of A. It is known that any convex set has a self-concordant barrier with ϑ = O(d) [53], which is tight up to a constant factor. Substituting
ϑ = d and noting L∗ ≤ T , we can see that the new regret bound includes previous bounds. Further, for the special case of (p, q) = (∞, 1), the new path-length regret bound of ˜O(d
V1) is tight up to a logarithmic factor in T , as a matching lower bound of O(d
V1) is known [22, 15]. To our knowledge, this is the ﬁrst (nearly) tight path-length regret bound for linear bandits. The regret
L∗) and ˜O(d(cid:112)Qq) are also nearly tight, as has been noted in the literature [36]. bounds of ˜O(d
The approach for the new regret bound is quite simple: we combine regret bounds dependent on optimistic predictors [57, 36] and the algorithm of tracking the best linear predictor [31, 16].
√
√
√ 2 Problem settings
This section introduces the problem settings of combinatorial semi-bandits and linear bandits. In both settings, a player is given, before the game starts, an action set A ∈ Rd and the total number
T of rounds. In each round t ∈ [T ], the player chooses an action at ∈ A, while the environment chooses a loss vector (cid:96)t ∈ Rd. After choosing the action, the player gets feedback on the loss, which will depend on the problem settings. Player performance is measured in terms of regret RT deﬁned as follows:
RT (a∗) = E (cid:34) T (cid:88) (cid:35) t (at − a∗) (cid:96)(cid:62) t=1
, RT = max a∗∈A
RT (a∗), (1) where the expectation is taken w.r.t. the randomness of (cid:96)t and the algorithm’s internal randomness. 2.1 Combinatorial semi-bandits
This subsection provides settings of action sets and feedback information in combinatorial semi-bandits. The action set A is a subset of binary vectors {0, 1}d, each element of which can be interpreted as a subset of [d]. Denote m = maxa∈A (cid:107)a(cid:107)1. For each chosen action at =
[at1, at2, . . . , atd](cid:62) ∈ A, we denote It = {i ∈ [d] | ati = 1}. We further assume that (cid:96)t ∈ [0, 1]d, similarly to what is seen in existing work [71, 40, 42, 66, 54].
In combinatorial semi-bandits, the player can get entry-wise bandit feedback. More precisely, after choosing an action at, which corresponds to a subset It of [d], the player can observe (cid:96)ti for each i ∈ It, while (cid:96)ti for i ∈ Jt := [d] \ It will not be revealed.
In addition to general action set A ∈ {0, 1}d, this paper analyzes two special cases of settings. One is size-invariant semi-bandits, in which all actions a ∈ A have the same size (cid:107)a(cid:107)1 = m. The other one is matroid semi-bandits [40, 66], in which an action set A corresponds to the bases of a matroid.
As all bases of an arbitrary matroid have the same size, matroid semi-bandits are special cases of size-invariant semi-bandits, which implies
{general semi-bandits} ⊇ {size-invariant semi-bandits} ⊇ {matroid semi-bandits}.
An important example of matroid semi-bandits is the problem over m-set, which is deﬁned as
A = {a ∈ {0, 1} | (cid:107)a(cid:107)1 = m}. The problem with full-combinatorial set A = {0, 1}n can also be reduced to a special case of matroid semi-bandits with (d, m) = (2n, n). Zimmert et al. [71] have provided improved regret bounds for such special cases of m-sets and full-combinatorial sets. 4
2.2 Linear bandits
In linear bandits, the action set A is assumed to be an arbitrary closed and bounded subset of Rd. The special cases in which A consists of binary vectors in {0, 1}d are called combinatorial bandits [17].
Similarly to what has been done in existing work [15, 29], we assume that there exists p, q ∈ [1, ∞] for which 1/p + 1/q = 1, (cid:107)a(cid:107)p ≤ 1 and (cid:107)(cid:96)t(cid:107)q ≤ 1 hold for all a ∈ A and (cid:96)t. By rescaling A and
{(cid:96)t} as needed, any problem with bounded A and {(cid:96)t} can be transformed into a problem satisfying this assumption.
The available feedback in linear bandits is even more limited than in combinatorial semi-bandits.
After choosing an action at ∈ A, the player can only observe the incurred loss (cid:96)(cid:62) t at. In the special case of combinatorial bandits, the player can only observe the sum of losses (cid:80) (cid:96)ti for the chosen i∈It subset, unlike in combinatorial semi-bandits in which (cid:96)ti is revealed for each i ∈ It. 2.3 Assumptions regarding environments
The scope of this work includes the following three different settings in terms of the environments’ determining losses (cid:96)t:
{adversarial regimes} ⊇ {stochastic regimes with adversarial corruptions} ⊇ {stochastic regiems}.
In a stochastic regime, the loss vectors (cid:96)t are supposed to follow an unknown
Stochastic regimes distribution D, i.i.d. for t = 1, 2, . . . , T . Denote µ = E(cid:96)∼D[(cid:96)] and set a∗ ∈ arg mina∈A µ(cid:62)a.
The regret can then be expressed as RT = E[(cid:80)T t=1 µ(cid:62)(at − a∗)]. It is known that the optimal regret in this regime can be characterized by the suboptimality gap parameter ∆ deﬁned as ∆ = mina∈A\a∗ µ(cid:62)a−µ(cid:62)a∗. Note that no prior information on the distribution D, including the parameter
∆, is given to the player. When we consider stochastic regime, we assume that ∆ > 0, which implies that the optimal action a∗ ∈ arg mina∈A µ(cid:62)a is assumed to be unique.
In an adversarial regime, no stochastic models on (cid:96)t are assumed, but the loss
Adversarial regimes (cid:96)t may be chosen in an adversarial manner. More precisely, the environment can choose (cid:96)t depending on the actions and losses {((cid:96)j, aj)}t−1 j=1 chosen up until the (t − 1)-th round.
Stochastic regimes with adversarial corruptions A stochastic regime with adversarial corrup-tions is a regime intermediate between stochastic regimes and adversarial regimes. In such a regime, a temporary loss (cid:96)(cid:48) t is drawn from an unknown distribution D, and then the environment may corrupt it to determine (cid:96)t in each round, subject to the constraint (cid:80)T t(cid:107)∞ ≤ C, where C ≥ 0 is a parameter called the corruption level and corresponds to the total amount of corruptions. In this paper, we suppose that the corruptions on (cid:96)t depend on (cid:96)(cid:48) j=1, and that they do not depend on at, similarly to what is seen in existing models [48, 27, 70, 8]. t and historical data {((cid:96)(cid:48) j, (cid:96)j, aj)}t−1 t=1 (cid:107)(cid:96)t − (cid:96)(cid:48)
The special cases of the stochastic regime with adversarial corruptions in which C ≥ Ω(T ) and
C = 0 coincide, respectively, with the adversarial regime and the stochastic regime. This paper supposes that the player is not given parameter C in advance, i.e., it aims to adapt to any environment with an arbitrary corruption level C. 3