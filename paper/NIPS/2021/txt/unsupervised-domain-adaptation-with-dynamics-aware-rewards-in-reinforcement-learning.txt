Abstract
Unsupervised reinforcement learning aims to acquire skills without prior goal representations, where an agent automatically explores an open-ended environment to represent goals and learn the goal-conditioned policy. However, this procedure is often time-consuming, limiting the rollout in some potentially expensive target environments. The intuitive approach of training in another interaction-rich envi-ronment disrupts the reproducibility of trained skills in the target environment due to the dynamics shifts and thus inhibits direct transferring. Assuming free access to a source environment, we propose an unsupervised domain adaptation method to identify and acquire skills across dynamics. Particularly, we introduce a KL regularized objective to encourage emergence of skills, rewarding the agent for both discovering skills and aligning its behaviors respecting dynamics shifts. This suggests that both dynamics (source and target) shape the reward to facilitate the learning of adaptive skills. We also conduct empirical experiments to demonstrate that our method can effectively learn skills that can be smoothly deployed in target. 1

Introduction
Recently, the machine learning community has devoted attention to unsupervised reinforcement learn-ing (RL) to acquire useful skills, ie, the problem of automatic discovery of a goal-conditioned policy and its corresponding goal space [8]. As shown in Figure 1 (left), the standard training procedure of learning skills in an unsupervised way follows: (1) representing goals, consisting of automatically generating the goal distribution p(g) and the corresponding goal-achievement reward function rg; (2) learning the goal-conditioned policy πθ with the acquired p(g) and rg. Leveraging fully autonomous interaction with the environment, the agent sets up goals, builds the goal-achievement reward function, and extrapolates the goal-conditioned policy in parallel by adopting off-the-shelf RL methods [40, 19].
While we can obtain skills without any prior goal representations (p(g) and rg) in an unsupervised way, a major drawback of this approach is that it requires a large amount of rollout steps to represent goals and learn the policy itself, together. This procedure is often impractical in some target environments (eg, the robot in real world), where online interactions are time-consuming and potentially expensive.
That said, there often exist environments that resemble in structure (dynamics) yet provide more accessible rollouts (eg, unlimited in simulators). For problems with such source environments available, training the policy in a source environment signiﬁcantly reduces the cost associated with interaction in the target environment. Critically, we can train a policy in one environment and deploy it in another by utilizing their structural similarity and the excess of interaction. Considering the navigation in a room, we can learn arbitrary skills through the active exploration in a source simulated
∗Work was done at Westlake University. † Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The training procedures of (left) the standard unsupervised RL in a single target environment, and (right) the unsupervised domain adaptation in RL with a pair of source and target environments. p(g): the goal distribution; rg: the goal-achievement reward function; πθ: the goal-conditioned policy. room (with different layout or friction) before the deployment in the target room. However, it is reasonable to suspect that the learned skills overﬁt the training environment, the dynamics of which, dictating the goal distribution and reward function, implicitly shape goal representation and guide policy acquisition. Such deployment would then make learned skills struggle to adapt to new, unseen environments and produce a large drop in performance in target due to the dynamics shifts, as shown in Figure 2 (top). In this paper, we overcome the limitations (of limited rollout in target and dynamics shifts) associated with the (source, target) environments pair through unsupervised domain adaptation.
In practice, while performing a full unsupervised RL method in target that represents goals and captures all of them for learning the entire goal-conditioned policy (Figure 1 left) can be extremely challenging with the limited rollout steps, learning a model for only (partially) representing goals is much easier. This gives rise to learning the policy in source and taking the limited rollouts in target into account only for identifying the goal representations, which further shape the policy. As shown in Figure 1 (right), we represent goals in both environments while optimizing the policy only in the source environment, alleviating the excessive need for rollout steps in the target environment.
Furthermore, we introduce a KL regularization to address the challenge of dynamics shifts. This objective allows us to incorporate a reward mod-iﬁcation into the goal-achievement reward func-tion in the standard unsupervised RL, aligning the trajectory induced in the target environment against that induced in the source by the same policy. Importantly, it enables useful inductive biases towards the target dynamics: it allows the agent to speciﬁcally pursue skills that are competent in the target dynamics, and penalizes the agent for exploration in the source where the dynamics signiﬁcantly differ. As shown in
Figure 2 (bottom), the difference in dynamics (a wall in the target while no wall in the source) will pose a penalty when the agent attempts to go through an area in the source wherein the tar-get stands a wall. Thus, skills learned in source with such modiﬁcation are adaptive to the target.
Figure 2: Skills learned in the source environment, each represented by a distinct color, are deployed in the source and target respectively. Top plots de-pict states visited by the standard unsupervised RL method, where skills fail to run in the target envi-ronment. Bottom plots depict trajectories induced by policy πθ trained with our DARS, resulting in successful deployment in the target environment.
We name our method unsupervised domain adaptation with dynamics-aware rewards (DARS), sug-gesting that source and target dynamics both shape rg: (1) we employ a latent-conditioned probing policy in the source to represent goals [31], making the goal-achievement reward source-oriented, and (2) we adopt two classiﬁers [11] to provide reward modiﬁcation derived from the KL regularization.
This means that the repertoires of skills are well shaped by the dynamics of both the source and target. Formally, we further analyze the conditions under which our DARS produces a near-optimal goal-conditioned policy for the target environment. Empirically, we demonstrate that our objective can obtain dynamics-aware rewards, enabling the goal-conditioned policy learned in a source to perform well in the target environment in various settings (stable and unstable settings, and sim2real). 2 Preliminaries
Multi-goal Reinforcement Learning: We formalize the multi-goal reinforcement learning (RL) as a goal-conditioned Markov Decision Process (MDP) deﬁned by the tuple MG = {S, A, P, RG, γ, ρ0}, 2
where S denotes the state space and A denotes the action space. P : S × A × S → R≥0 is the transition probability density. RG (cid:44) {G, rg, p(g)}, where G denotes the space of goals, rg denotes the corresponding goal-achievement reward function rg : G × S × A × S → R, and p(g) denotes the given goal distribution. γ is the discount factor and ρ0 is the initial state distribution. Given a g ∼ p(g), the γ-discounted return R(g, τ ) of a goal-oriented trajectory τ = (s0, a0, s1, . . . , sT ) is
ΣT −1 t=0 γtrg(st, at, st+1). Building on the universal value function approximators (UVFA, Schaul et al.
[38]), the standard multi-goal RL seeks to learn a unique goal-conditioned policy πθ : A×S ×G → R to maximize the objective EP,ρ0,πθ,p(g)[R(g, τ )], where θ denotes the parameter of the policy.
Unsupervised Reinforcement Learning: In unsupervised RL, the agent is set in an open-ended environment without any pre-deﬁned goals or related reward functions. The agent aims to acquire a repertoire of skills. Following Colas et al. [8], we deﬁne skills as the association of goals and the goal-conditioned policy to reach them. The unsupervised skill acquisition problem can now be modeled by a goal-free MDP M = {S, A, P, γ, ρ0} that only characterizes the agent, its environment and their possible interactions. As shown in Figure 1 (left), the agent needs to autonomously interact with the environment and (1) learn goal representations (eg, discovering the goal distribution p(g) and learning the corresponding reward rg), and (2) learn the goal-conditioned policy πθ as in multi-goal RL.
Here we deﬁne a universal (information theoretic) objective for learning the goal-conditioned policy
πθ in unsupervised RL, maximizing the mutual information IP,ρ0,πθ (g; τ ) between the goal g and the trajectory τ induced by policy πθ running in the environment M (with P and ρ0), max IP,ρ0,πθ (g; τ ) = H(g) − H(g|τ ) = H(g) + EP,ρ0,πθ,p(g)[log p(g|τ )].
For representing goals, the speciﬁc manifold of the goal space could be a set of latent variables (eg, one-hot indicators) or perceptually-speciﬁc goals (eg, the joint torques of ant). In the absence of any prior knowledge about p(g), the maximum of H(g) will be achieved by ﬁxing the distribution p(g) to be uniform over all g ∈ G. The second term EP,ρ0,πθ,p(g)[log p(g|τ )] in Equation 1 is analogous to the objective in the standard multi-goal RL, where the return R(g, τ ) can be seen as the embodiment of log p(g|τ ). The objective speciﬁcally for learning rg in p(g|τ ) is normally optimized by lens of the generative loss [33] or the contrastive loss [42]. With the learned goal distribution p(g) and reward rg, it is straightforward to learn the goal-conditioned policy πθ using standard RL algorithms [40, 19].
In general, optimizations iteratively alternate for representing goals (including both goal-distribution p(g) and reward function rg) and learning the goal-conditioned policy πθ, as shown in Figure 1 (left). (1) 3 Unsupervised Domain Adaptation with Dynamics-Aware Rewards 3.1 Problem Formulation
Our work addresses domain adaptation in unsupervised RL, raising expectations that an agent trained without prior goal representations (p(g) and rg) in one environment can perform purposeful tasks in another. Following Wulfmeier et al. [54], we also focus on the domain adaptation of the dynamics, as opposed to states. In this work, we consider two environments characterized by MDPs MS (the source environment) and MT (the target environment), the dynamics of which are PS and PT respectively. Both MDPs share the same state and action spaces S, A, discount factor γ and initial state distribution ρ0, while differing in the transition distributions PS , PT . Since the agent does not directly receive RG from either environment, we adopt the information theoretic IP,ρ0,πθ (g; τ ) to acquire skills, equivalently learning a goal-conditioned policy πθ that achieves distinguishable trajectory by maximizing this objective. For brevity, we now omit the ρ0 term discussed in Section 2.
In our setup, agents can freely interact with the source MS . However, it has limited access to rollouts in the target MT with which are insufﬁcient to train a policy. To ensure that all potential trajectories in the target MT can be attempted in the source environment, we make the following assumption:
Assumption 1. There is no transition that is possible in the target environment MT but impossible in the source environment MS : PT (st+1|st, at) > 0 =⇒ PS (st+1|st, at) > 0. 3.2 Domain Adaptation in Unsupervised RL
We aim to acquire skills trained in the source environment MS , which can be deployed in the target environment MT . To facilitate the unsupervised learning of skills for the target environment MT 3
Figure 3: Graphical models of (a) the standard unsupervised RL, and DARS with goals (b) directly in-putted, (c1) relabeled with latent variable ω, and (c2) relabeled with state induced by probing policy. (with transition dynamics PT ), we maximize the mutual information between the goal g and the trajectory τ induced by the goal-conditioned policy πθ over dynamics PT , as shown in Figure 3 (a):
IPT ,πθ (g; τ ). (2)
However, since interaction with the target environment MT is restricted, acquiring the goal-conditioned policy πθ by optimizing the mutual information above is intractable. We instead maximize the mutual information in the source environment IPS ,πθ (g; τ ) modiﬁed by a KL di-vergence of trajectories induced by the goal-conditioned policy πθ in both environments (Figure 3 b):
IPS ,πθ (g; τ ) − βDKL (cid:16) pPS ,πθ (g, τ )(cid:107)pPT ,πθ (g, τ ) (cid:17)
, (3) where β > 0 is the regularization coefﬁcient, pPS ,πθ (g, τ ) and pPT ,πθ (g, τ ) denote the joint distribu-tions of the goal g and the trajectory τ induced by policy πθ in source MS and target MT respectively.
Intuitively, maximizing the mutual information term rewards distinguishable pairs of trajectories and goals, while minimizing the KL divergence term penalizes producing a trajectory that cannot be followed in the target environment. In other words, the KL term aligns the probability distributions of the mutual-information-maximizing trajectories under the two environment dynamics PS and
PT . This indicates that the dynamics of both environments (PS and PT ) shape the goal-conditioned policy πθ (even though trained in the source PS ), allowing πθ to adapt to the shifts in dynamics.
Building on the KL regularized objective in Equation 3, we introduce how to effectively represent goals: generating the goal distribution and acquiring the (partial) reward function. Here we assume the difference between environments in their dynamics negligibly affects the goal distribution2.
Therefore, we follow GPIM [31] and train a latent-conditioned probing policy πµ. The probing policy
πµ explores the source environment and represents goals for the source to train the goal-conditioned policy πθ with. Speciﬁcally, the probing policy πµ is conditioned on a latent variable ω ∼ p(ω)3 and aims to generate diverse trajectories that are further relabeled as goals for the goal-conditioned πθ.
Such goals can take the form of the latent variable ω itself (Figure 3 c1) or the ﬁnal state of a trajectory (Figure 3 c2). We jointly optimize the previous objective in Equation 3 with the mutual information between ω and the trajectory ˜τ induced by πµ in source, and arrive at the following overall objective: max J (µ, θ) (cid:44) IPS ,πµ(ω; ˜τ ) + IPS ,πθ (g; τ ) − βDKL (cid:16) pPS ,πθ (g, τ )(cid:107)pPT ,πθ (g, τ ) (cid:17)
, (4) where the context between p(g) and p(ω) are speciﬁed by the graphic model in Figure 3 (c1 or c2).
Note that this objective (Equation 4) explicitly decouples the goal representing (with πµ) and the policy learning (wrt πθ), providing a foundation for the theoretical guarantee in Section 3.4. 3.3 Optimization with Dynamics-Aware Rewards
Similar to Goyal et al. [16], we take advantage of the data processing inequality (DPI [3]) which implies IPS ,πθ (g; τ ) ≥ IPS ,πθ (ω; τ ) from the graphical models in Figure 3 (c1, c2). Consequently, maximizing IPS ,πθ (g; τ ) can be achieved by maximizing the information of ω encoded progressively to πθ. We therefore obtain the lower bound of Equation 4:
J (µ, θ) ≥ IPS ,πµ(ω; ˜τ ) + IPS ,πθ (ω; τ ) − βDKL (cid:16) (cid:17) pPS ,πθ (g, τ )(cid:107)pPT ,πθ (g, τ )
. (5) 2See Appendix D for the extension when MS and MT have different goal distributions. 3Following DIAYN [10] and DADS [43], we set p(ω) as a ﬁxed prior. 4
For the ﬁrst term IPS ,πµ (ω; ˜τ ) and the second term IPS ,πθ (ω; τ ), we derive the state-conditioned
Markovian rewards following Jabri et al. [24]:
IP,π(ω; τ ) ≥ 1
T
T −1 (cid:88) t=0 (H (ω) − H (ω|st+1)) = H (ω) + EpP ,π(ω,st+1) [log p(ω|st+1)]
≥ H (ω) + EpP ,π(ω,st+1) [log qφ(ω|st+1)] , (6) (7) where pP ,π(ω, st+1) = p(ω)pP ,π(st+1|ω), and pP ,π(st+1|ω) refers to the state distribution (at time step t + 1) induced by policy π conditioned on ω under the environment dynamics P; the lower bound in Equation 7 derives from training a discriminator network qφ due to the non-negativity of
KL divergence, Epπ(st+1) [DKL(p(ω|st+1)||qφ(ω|st+1))] ≥ 0. Intuitively, the new bound rewards the discriminator qφ for summarizing agent’s behavior with ω as well as encouraging a variety of states.
With the bound above, we construct the lower bound of the mutual information terms in Equation 5, taking the same discriminator qφ:
FI (cid:44) IPS ,πµ(ω; ˜τ ) + IPS ,πθ (ω; τ ) ≥ 2H (ω) + Epjoint [log qφ(ω|˜st+1) + log qφ(ω|st+1)] , where pjoint denotes the joint distribution of ω, states ˜st+1 and st+1. The states ˜st+1 and st+1 are induced by the probing policy πµ conditioned on the latent variable ω and the policy πθ conditioned on the relabeled goals respectively, both in the source environment (Figure 3 c1, c2). (8)
Now, we are ready to characterize the KL term in Equation 5. Note that only the transition probabilities terms (PS and PT ) differ since agent follows the same policy πθ in the two environments. This con-veniently leads to the expansion of the KL divergence term as a sum of differences in log likelihoods of the transition dynamics: expansion pP ,πθ (g, τ ) = p(g)ρ0(s0) (cid:81)T −1 t=0 [P(st+1|st, at)πθ(at|st, g)], where P ∈ {PS , PT }, gives rise to the following simpliﬁcation of the KL term in Equation 5:
βDKL (cid:16) (cid:17) pPS ,πθ (g, τ )(cid:107)pPT ,πθ (g, τ )
= EPS ,πθ [β∆r(st, at, st+1)] , (9) where the reward modiﬁcation ∆r(st, at, st+1) (cid:44) log PS (st+1|st, at) − log PT (st+1|st, at).
Combining the lower bound of the mutual informa-tion terms (Equation 8) and the KL divergence term pursuing the aligned trajectories in two environments (Equation 9), we optimize J (µ, θ) by maximizing the following lower bound: 2H (ω) + Epjoint [log qφ(ω|˜st+1) + log qφ(ω|st+1)]
− EPS ,πθ [β∆r(st, at, st+1)] . (10)
Overall, as shown in Figure 4, DARS rewards the goal-conditioned policy πθ with the dynamics-aware re-wards (associating log qφ with β∆r), where (1) log qφ is shaped by the source dynamics PS , and (2) β∆r is derived from the difference of the two dynamics (PS and PT ). This indicates that the learned goal-conditioned policy πθ is shaped by both source and target environments, holding the promise of acquiring adaptive skills for the target environment by training mostly in the source environment.
Figure 4: Framework of DARS: the latent-conditioned probing policy πµ provides p(g) and qφ for learning goal-conditioned πθ, as-sociated with the reward modiﬁcation β∆r. 3.4 Optimality Analysis
Here we discuss the condition under which our method produces near-optimal skills for the target environment. We ﬁrst mildly require that the most suitable policy for the target environment MT does not produce drastically different trajectories in the source environment MS :
Assumption 2. Let π∗ = arg maxπ IPT ,π(g; τ ) be the policy that maximizes the (non-kl-regularized) objective in the target environment (Equation 2). Then the joint distributions of the goal and its trajectories differ in both environments by no more than a small number (cid:15)/β > 0:
DKL (cid:16) (cid:17) pPS ,π∗ (g, τ )||pPT ,π∗ (g, τ )
≤ (cid:15)
β
. (11) 5
Algorithm 1 DARS 1: Input: source and target MDPs MS and MT ; ratio R of experience from source vs. target. 2: Output: goal-reaching policy πθ. 3: Initialize parameters µ, θ, φ and ψ. 4: Initialize buffers ˜BS , BS and BT . 5: for iter = 0, . . . , MAX_ITER do 6: 7:
Sample latent variable: ω ∼ p(ω).
Collect probing data in source:
˜BS ← ˜BS ∪ ROLLOUT(πµ, MS , ω).
Update discriminator qφ: φ ← Update(φ, ˜BS )
Set reward function for the probing policy πµ:
˜r = log qφ(ω|˜st+1).
Train probing policy πµ: µ ← SAC(µ, ˜BS , ˜r). 8: 9: 10: 11: 12: 13: 14: 15: 16: 17:
Relabel goals: # According to Figure 3 (c1, c2) g ← Relabel(ω, ˜τ ).
Collect source data:
BS ← BS ∪ ROLLOUT(πθ, MS , g, ω). if iter mod R = 0 then
Collect target data:
BT ← BT ∪ ROLLOUT(πθ, MT , g). end if
Update classiﬁers qψ for computing ∆r:
ψ ← Update(ψ, BS , BT ). (Equations 12, 13)
Set reward function for πθ: rg ← log qφ(ω|st+1) − β∆r(st, at, st+1).
Train policy πθ: θ ← SAC(θ, BS , rg). 18: 19: end for (cid:0)pP ,π(g, τ )(cid:107)p∗
Given a desired joint distribution p∗(g, τ ) (inferred from a potential goal representation), our problem can be reformulated as ﬁnding a closest match [29, 28]. Consequently, we quantify the optimality of (g, τ )(cid:1), the discrepancy between its joint distribution a policy π by measuring DKL and the desired one. With a potential goal representation, we prove that its joint distributions with the trajectories induced by our policy and the optimal one satisfy the following theoretical guarantee.
Theorem 1. Let π∗
DARS be the optimal policy that maximizes the KL regularized objective in the source environment (Equation 3), let π∗ be the policy that maximizes the (non-regularized) objective in the target environment (Equation 2), let p∗ (g, τ ) be the desired joint distribution of trajectory and
PT goal in the target (with the potential goal representations), and assume that π∗ satisﬁes Assumption 2.
Then the following holds:
P (cid:16)
DKL pPT ,π∗
DARS (g, τ )(cid:107)p∗
PT (cid:17) (g, τ )
≤ DKL (cid:16) pPT ,π∗ (g, τ )(cid:107)p∗
PT (cid:17) (g, τ )
+ 2 (cid:114) 2(cid:15)
β
Lmax, where Lmax refers to the worst case absolute difference between log likelihoods of the desired joint distribution and that induced by a policy.
Please see Appendix C for more details and the proof of the theorem. Note that Theorem 1 requires a potential goal representation, which can be precisely provided by the probing policy πµ in Equation 4. 3.5
Implementation
As shown in Algorithm 1, we alternately train the probing policy πµ and the goal-conditioned policy
πθ by optimizing the objective in Equation 10 with respect to µ, φ, θ and ∆r. In the ﬁrst phase, we update πµ with reward ˜r = log qφ(ω|˜st+1). This is compatible with most RL methods and we refer to
SAC here. We additionally optimize discriminator qφ with SGD to maximizing Eω,˜st+1 [qφ(ω|˜st+1)] at the same time. Similarly, πθ is updated with rg = log qφ(ω|st+1) − β∆r by SAC in the second phase, where πθ also collects (limited) data in the target environment to approximate ∆r by training
ψ and state-action-state qsas two classiﬁers qψ (wrt state-action qsa
ψ ) as in [11] according to Bayes’ rule: (cid:2)log qsas
ψ (target|st, at)(cid:3) . qsa
ψ (source|st,at)
ψ (target|st,at) . qsa
ψ (source|st, at, st+1)(cid:3) + EBT
ψ (source|st, at)(cid:3) + EBT qsas
ψ (source|st,at,st+1)
ψ (target|st,at,st+1) − log qsas
Then, we have ∆r(st, at, st+1) = log
ψ (target|st, at, st+1)(cid:3) , max EBS max EBS (cid:2)log qsas (cid:2)log qsa (cid:2)log qsa (12) (13) 3.6 Connections to Prior Work
Unsupervised RL: Two representative unsupervised RL approaches acquire (diverse) skills by maximizing empowerment [10, 43] or minimizing surprise [4]. Liu et al. [31] also employs a latent-conditioned policy to explore the environment and relabels goals along with the corresponding reward, 6
Figure 5: We evaluate our method in 10 (source, target) transition tasks, where the shifts in dynamics are either external (the map pairs and the attacked series) or internal (the broken series) to the robot. which can be considered as a special case of DARS with identical source and target environments.
However, none of these methods can produce skills tailored to new environments with dynamics shifts.
Off-Dynamics RL: Eysenbach et al. [11] proposes domain adaptation with rewards from classiﬁers (DARC), adopting the control as inference framework [29] to maximize −DKL(pPS ,πθ (τ )(cid:107)p∗ (τ )), but this objective cannot be directly applied to the unsupervised setting. While we adopt the same classiﬁer to provide the reward modiﬁcation, one major distinction of our work is that we do not require a given goal distribution p(g) or a prior reward function rg. Moreover, assuming an extrinsic goal-reaching reward in the source environment (ie, the potential p∗ (τ )), our pro-PS posed DARS can be simpliﬁed to a decoupled objective: maximizing −DKL(pPS ,πθ (τ )(cid:107)p∗ (τ )) −
βDKL(pPS ,πθ (τ )(cid:107)pPT ,πθ (τ )). Particularly, DARC can be considered as a special case of our decou-pled objective with the restriction — a prior goal speciﬁed by its corresponding reward and β = 1. In
Appendix E, we show that the stronger pressure (β > 1) for the KL term to align the trajectories puts extra reward signals for the policy πθ to be ∆r oriented while still being sufﬁcient to acquire skills.
PS
PT 4