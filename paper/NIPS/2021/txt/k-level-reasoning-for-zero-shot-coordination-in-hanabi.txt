Abstract
The standard problem setting in cooperative multi-agent settings is self-play (SP), where the goal is to train a team of agents that works well together. However, optimal SP policies commonly contain arbitrary conventions (“handshakes”) and are not compatible with other, independently trained agents or humans. This latter desiderata was recently formalized by [18] as the zero-shot coordination (ZSC) setting and partially addressed with their Other-Play (OP) algorithm, which showed improved ZSC and human-AI performance in the card game Hanabi. OP assumes access to the symmetries of the environment and prevents agents from breaking these in a mutually incompatible way during training. However, as the authors point out, discovering symmetries for a given environment is a computationally hard problem. Instead, we show that through a simple adaption of k-level reasoning (KLR) [7], synchronously training all levels, we can obtain competitive ZSC and ad-hoc teamplay performance in Hanabi, including when paired with a human-like proxy bot. We also introduce a new method, synchronous-k-level reasoning with a best response (SyKLRBR), which further improves performance on our synchronous KLR by co-training a best response. 1

Introduction
Research into multi-agent reinforcement learning (MARL) has recently seen a ﬂurry of activity, ranging from large-scale multiplayer zero-sum settings such as StarCraft [35] to partially observable, fully cooperative settings, such as Hanabi [1]. The latter (cooperative) setting is of particular interest, as it covers human-AI coordination, one of the longstanding goals of AI research [10, 6]. However, most work in the cooperative setting—typically modeled as a Dec-POMDPs—has approached the problem in the self-play (SP) setting, where the only goal is to ﬁnd a team of agents that works well together. Unfortunately, optimal SP policies in Dec-POMDPs commonly communicate information through arbitrary handshakes (or conventions), which fail to generalize to other, independently trained, AI agents or humans at test time.
To address this, the zero-shot coordination setting [18] was recently introduced, where the goal is to ﬁnd training strategies that allow independently trained agents to coordinate at test time. The main idea of this line of work is to develop learning algorithms that can use the structure of the Dec-POMDP itself to independently ﬁnd mutually compatible policies, a necessary step towards human-AI coordination. Related coordination problems have also been studied by different communities, in
∗Work done while at Facebook AI Research 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
Figure 1: Visualization of various hierarchical training schemas, including sequential KLR, syn-chronous KLR, synchronous CH, and our new SyKLRBR for 4 levels. Thicker arrows indicate a greater proportion of games played with the level. Additionally, red boxes indicate an actively trained agent, while grey boxes indicate a ﬁxed agent. Typically π0 is a uniform random agent. particular behavioural game theory. One of the best-known approaches in this area is the cognitive-hierarchies (CH) framework [4], in which a hierarchy of agents is trained. For this method, an agent at level-k models other agents as coming from a distribution up to level k − 1 and best-responds accordingly. The CH framework has been shown to model human behavior in games for which equilibrium theory does not match empirical data [4]; thus, in principle, the CH framework could be leveraged to facilitate human-AI coordination in complex settings. A speciﬁc instance of CH that is relevant to our work is K-level reasoning (KLR) [7], wherein the level-k agent models the other agents as level-(k − 1). However, KLR, like many of the ideas developed in these works, has not been successfully scaled to large scale coordination problems [18].
In this paper we show that k-level reasoning can indeed be scaled to large partially observable coordination problems, like Hanabi. We identify two key innovations that both increase training speed and improve the performance of the method. First, rather than training the different levels of the hierarchy sequentially, as would be suggested by a literal interpretation of the method (as was done as a baseline in [18]), we instead develop a synchronous version, where all levels are trained in parallel (see ﬁgure 1). The obvious advantage is that the wall-clock time can be reduced from linear in the number of levels to constant, taking advantage of parallel training. The more surprising ﬁnding is that synchronous training also acts as a regularizer on the policies, stabilizing training.
The second innovation is that in parallel we also train a best response (BR) to the entire KLR hierarchy, with more weight being placed on the highest two levels. This constitutes a hybrid approach between CH and KLR, and the resulting BR is our ﬁnal test time policy. Our method, synchronous-k-level reasoning with a best response (SyKLRBR), obtains high scores when evaluating independently trained agents in cross-play (XP). Importantly, this method also improves ad-hoc teamplay performance, indicating a robust policy that plays well with various conventions.
Lastly, we evaluate our SyKLRBR agents paired with a proxy human policy and establish new state-of-the-art performance, beating recent strong algorithms that, in contrast to our approach, require additional information beyond the game-provided observations [18, 17].
Our results show that indeed KLR can be adapted to address large scale coordination problems, in particular those in which the main challenge is to prevent information exchange through arbitrary conventions. Our analysis shows that synchronous training regularizes the training process and prevents level-k from overﬁtting to the now changing policy at level-k − 1. In contrast, in sequential training each agent overﬁts to the static agent at the level below, leading to arbitrary handshakes and brittle conventions. Furthermore, training a best response to the entire hierarchy improves the
ﬁnal ZSC performance and robustness in ad-hoc settings. This is intuitive since the BR can carry out on-the-ﬂy adaptation in the ZSC setting.
Our results show that the exact graph-structure used, which were similarly studied in [13], and the type of training regime (synchronous vs sequential) can have a major impact on the ﬁnal outcome when adapting ideas from the cognitive hierarchy literature to the deep MARL setting. We hope our ﬁndings will encourage other practitioners to seek inspiration in the game theory literature and to scale those ideas to high dimensional problems, even when there is precedent of unsuccessful attempts in prior work. 2
2