Abstract
In recent years Recurrent Neural Networks (RNNs) were successfully used to model the way neural activity drives task-related behavior in animals, operating un-der the implicit assumption that the obtained solutions are universal. Observations in both neuroscience and machine learning challenge this assumption. Animals can approach a given task with a variety of strategies, and training machine learning algorithms introduces the phenomenon of underspeciﬁcation. These observations imply that every task is associated with a space of solutions. To date, the structure of this space is not understood, limiting the approach of comparing RNNs with neural data. Here, we characterize the space of solutions associated with various tasks. We ﬁrst study a simple two-neuron network on a task that leads to multiple solutions. We trace the nature of the ﬁnal solution back to the network’s initial connectivity and identify discrete dynamical regimes that underlie this diversity.
We then examine three neuroscience-inspired tasks: Delayed discrimination, In-terval discrimination, and Time reproduction. For each task, we ﬁnd a rich set of solutions. One layer of variability can be found directly in the neural activity of the networks. An additional layer is uncovered by testing the trained networks’ ability to extrapolate, as a perturbation to a system often reveals hidden structure.
Furthermore, we relate extrapolation patterns to speciﬁc dynamical objects and effective algorithms found by the networks. We introduce a tool to derive the reduced dynamics of networks by generating a compact directed graph describing the essence of the dynamics with regards to behavioral inputs and outputs. Using this representation, we can partition the solutions to each task into a handful of types and show that neural features can partially predict them. Taken together, our results shed light on the concept of the space of solutions and its uses both in
Machine learning and in Neuroscience. 1

Introduction
Modern machine learning operates in an over-parameterized regime, implying that many different parameter-sets can achieve low error on a given training set (1). This observation implies that for every task, there exists a space of solutions that can implement it. What are the properties of such a solution space? Are networks able to learn solutions that capture the intended underlying 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
phenomena or do they reach artiﬁcial shortcuts that do not generalize well? What biases networks to prefer one solution over the other? These questions remain largely unanswered. A parallel phenomenon occurs in Neuroscience. When animals are instructed to perform a task in a controlled environment, they exhibit both neural and behavioral variability, which stem from different task-strategies (2; 3; 4; 5; 6; 7; 8; 9; 10; 11). In Computational Neuroscience trained Recurrent Neural
Networks (RNNs) are used as a tool to explain functions and mechanisms that are observed in brain dynamics (12). In fact, various recent studies have matched the activity of trained RNNs to that of experimental recording (13; 14; 15; 16; 17; 18; 19). In light of the variability that undoubtedly exists on both sides of the comparison, these results seem puzzling. In this work, we present multiple tasks for which trained RNNs produce a rich space of qualitatively different solutions. We argue that to properly use artiﬁcial networks, and RNNs in particular, as models of neural circuits that support a given task, it is necessary to chart the space of solutions that arises from training.
Here, we ﬁrst apply this approach to a simple two-neuron network and demonstrate how distinct solutions arise. We then study three tasks inspired by the neuroscience literature: interval reproduction (20), delayed discrimination (21), and interval discrimination (22). We show that different networks with identical hyperparameters ﬁnd qualitatively different solutions. We ﬁnd one layer of variability within the neural activity in response to stimuli from the training set. Since by design the output of all networks is identical during training, this layer is akin to multiple realizability (23). Next, we expose an additional layer of variability when we challenge the networks with inputs that are outside the distribution of the training set and systematically characterize the responses. Furthermore, we manage to show that the diversity revealed with these challenging inputs corresponds to qualitatively different computations performed by the network. To chart the space of solutions, we introduce a tool that reduces the dynamics of a network into a graph that captures the essence of the computation performed. Applying it to all networks partitions the space into a handful of possible reduced dynamics. Additionally, these classes can be partially predicted using experimentally accessible neural activity obtained only in response to trained stimuli. 2