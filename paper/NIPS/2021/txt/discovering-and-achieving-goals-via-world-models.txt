Abstract
How can artiﬁcial agents learn to solve many diverse tasks in complex visual environments without any supervision? We decompose this question into two challenges: discovering new goals and learning to reliably achieve them. Our proposed agent, Latent Explorer Achiever (LEXA), addresses both challenges by learning a world model from image inputs and using it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks speciﬁed as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal reaching, both on prior benchmarks and on a new challenging benchmark with 40 test tasks spanning across four robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. 1

Introduction
How can we build an agent that learns to solve hundreds of tasks in complex visual environ-ments, such as rearranging objects with a robot arm or completing chores in a kitchen? While traditional reinforcement learning (RL) has been successful for individual tasks, it requires a sub-stantial amount of human effort for every new task. Specifying task rewards requires domain knowledge, access to object positions, is time-consuming, and prone to human errors. More-over, traditional RL would require environment interaction to explore and practice in the environ-ment for every new task. Instead, we approach learning hundreds of tasks through the paradigm of unsupervised goal-conditioned RL, where the agent learns many diverse skills in the environ-ment in the complete absence of supervision, to later solve tasks via user-speciﬁed goal images immediately without further training [2, 26, 40].
Challenges Exploring the environment and learning to solve many different tasks is sub-stantially more challenging than traditional RL with a dense reward function or learning from
Figure 1: LEXA learns a world model without any supervision, and leverages it to train two policies in imagination. The explorer ﬁnds new images and the achiever learns to reliably reach them. Once trained, the achiever reaches user-speciﬁed goals zero-shot without further training at test time.
∗ Equal contribution. Ordering determined at random. Project page: https://orybkin.github.io/lexa/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: We benchmark LEXA across four visual control environments. A representative sample of the test-time goals is shown here. RoboYoga features complex locomotion and precise control of high-dimensional agents, RoboBins manipulation with multiple objects, and RoboKitchen a variety of diverse tasks that require complex control strategies such as opening a cabinet. expert demonstrations. Existing methods are limited to simple tasks, such as picking or pushing a puck
[13, 32, 37] or controlling simple 2D robots [50]. The key challenge in improving the performance of unsupervised RL is exploration. In particular, previous approaches explore by either revisiting previously seen rare goals [14, 18, 55] or sampling goals from a generative model [32, 37]. However, in both these approaches, the policy as well as the generative model are trained on previously visited states from the replay buffer, and hence the sampled goals are either within or near the frontier of agent’s experience. Ideally, we would like the agent to discover goals much beyond its frontier for efﬁcient exploration, but how does an agent generate goals that it is yet to encounter? This is an open question not just for AI but for cognitive science too [42].
Approach To rectify this issue, we leverage a learned world model to train a separate explorer and achiever policy in imagination. Instead of randomly sampling or generating goals, our explorer policy discovers distant goals by ﬁrst planning a sequence of actions optimized in imagination of the world model to ﬁnd novel states with high expected information gain [30, 43, 44]. It then executes those imagined actions in the environment to discover interesting states without the need to generate them.
Note these actions are likely to lead the agent to states which are several steps outside the frontier because otherwise the model wouldn’t have had high uncertainty or information gain. Finally, these discovered states are used as diverse targets for the achiever to practice. We train the achiever from on-policy imagination rollouts within the world model and without relying on experience relabeling, therefore leveraging foresight over hindsight. After this unsupervised training phase, the achiever solves tasks speciﬁed as goal images zero-shot without any additional learning at deployment. Unlike in the conventional RL paradigm [31, 47], our method is trained once and then used to achieve several tasks at test time without any supervision during training or testing.
Contributions We introduce Latent Explorer Achiever (LEXA), an unsupervised goal reaching agent that trains an explorer and an achiever within a shared world model. At training, LEXA unlocks diverse data for goal reaching in environments where exploration is nontrivial. At test time, the achiever solves challenging locomotion and manipulation tasks provided as user-speciﬁed goal images. Our contributions are summarized as follows:
• We propose to learn separate explorer and achiever policies as an approach to overcome the exploration problem of unsupervised goal-conditioned RL.
• We show that forward-looking exploration by planning with a learned world model substantially outperforms previous strategies for goal exploration.
• To evaluate on challenging tasks, we introduce a new goal reaching benchmark with a total of 40 diverse goal images across 4 different robot locomotion and manipulation environments.
• LEXA outperforms prior methods, being the ﬁrst to show success in the Kitchen robotic manipu-lation environment, and achieves goal images where multiple objects need to be moved. 2
Figure 3: Latent Explorer Achiever (LEXA) learns a general world model that is used to train an explorer and a goal achiever policy. The explorer (left) is trained on imagined latent state rollouts of the world model st:T to maximize the disagreement objective re t = Var(s(cid:48)). The goal achiever (right) is conditioned on a goal g and is also trained on imagined rollouts to minimize a distance function d(st, eg). Goals are sampled randomly from replay buffer images. For training a temporal distance, we use the imagined rollouts of the achiever and predict the number of time steps between each two states. By combining forward-looking exploration and data-efﬁcient training of the achiever, LEXA provides a simple and powerful solution for unsupervised reinforcement learning. 2 Latent Explorer Achiever (LEXA)
Our aim is to build an agent that can achieve arbitrary user-speciﬁed goals after learning in the environment without any supervision. This presents two challenges - collecting trajectories that contain diverse goals and learning to achieve these goals when speciﬁed as a goal image. We introduce a simple solution based on a world model and imagination training that addresses both challenges.
The world model represents the agent’s current knowledge about the environment and is used for training two policies, the explorer and the achiever. To explore novel situations, we construct an estimate of which states the world model is still uncertain about. To achieve goals, we train the goal-conditioned achiever in imagination, using the images found so far as unsupervised goals. At test time, the achiever is deployed to reach user-speciﬁed goals. The training procedure is in Algorithm 1. 2.1 World Model
To efﬁciently predict potential outcomes of future actions in environments with high-dimensional image inputs, we leverage a Recurrent State Space Model (RSSM) [23] that learns to predict forward using compact model states that facilitate planning [7, 51]. In contrast to predicting forward in image space, the model states enable efﬁcient parallel planning with a large batch size and can reduce accumulating errors [39]. The world model consists of the following components:
Encoder:
Dynamics: et = encφ(xt) pφ(st | st−1, at−1)
Posterior:
Image decoder: qφ(st | st−1, at−1, et) pφ(xt | st) (1)
The model states st contain a deterministic component ht and a stochastic component zt with diagonal-covariance Gaussian distribution. ht is the recurrent state of a Gated Recurrent Unit (GRU) [11]. The encoder and decoder are convolutional neural networks (CNNs) and the remaining components are multi-layer perceptrons (MLPs). The world model is trained end-to-end by optimizing the evidence lower bound (ELBO) via stochastic backpropagation [28, 38] with the Adam optimizer [27]. 2.2 Explorer
To efﬁciently explore, we seek out surprising states imagined by the world model [6, 41, 43, 44, 46], as opposed to retrospectively exploring by revisiting previously novel states [4, 5, 8, 34]. As the world model can predict model states that correspond to unseen situations in the environment, the imagined trajectories contain more novel goals, compared to model-free exploration that is limited to the replay buffer. To collect informative novel trajectories in the environment, we train an exploration 3
Algorithm 1: Latent Explorer Achiever (LEXA) 1: initialize: World model M, Replay buffer D, Explorer πe(at | zt), Achiever πg(at | zt, g) 2: while exploring do 3:
Train M on D
Train πe in imagination of M to maximize exploration rewards (cid:80)
Train πg in imagination of M to maximize (cid:80) (Optional) Train d(zi, zj) to predict distances j − i on the imagination data from last step.
Deploy πe in the environment to explore and grow D.
Deploy πg in the environment to achieve a goal image g ∼ D to grow D. t (zt, g) for images g ∼ D. 4: 5: 6: 7: t re t . t rg 8: 9: end while 10: while evaluating do 11: 12: 13: end while given: Evaluation goal g
Deploy πg in the world to reach g. policy πe from the model states st in imagination of the world model to maximize an exploration reward:
πe(at | st)
Explorer: (2)
To explore the most informative model states, we estimate the epistemic uncertainty as a disagreement of an ensemble of transition functions. We train an ensemble of 1-step models to predict the next model state from the current model state. The ensemble model is trained alongside the world model on model states produced by the encoder qφ. Because the ensemble models are initialized at random, they will differ, especially for inputs that they have not been trained on [29, 36]:
Explorer Value: ve(st)
Ensemble: f (st, θk) = ˆzk t+1 for k = 1..K (3)
Leveraging the ensemble, we estimate the epistemic uncertainty as the ensemble disagreement. The exploration reward is the variance of the ensemble predictions averaged across dimension of the model state, which approximates the expected information gain [3, 43]: re t (st)
.
= 1
N (cid:88) n
Var{k} (cid:2)f (st, θk)(cid:3) n (4)
The explorer πe maximizes the sum of future exploration rewards re t using the Dreamer algorithm
[24], which considers long-term rewards into the future by maximizing λ-returns under a learned value function. As a result, the explorer is trained to seek out situations are as informative as possible from imagined latent trajectories of the world model, and is periodically deployed in the environment to add novel trajectories to the replay buffer, so the world model and goal achiever policy can improve. 2.3 Achiever
To leverage the knowledge obtained by exploration for learning to reach goals, we train a goal achiever policy πg that receives a model state and a goal as input. Our aim is to train a general policy that is capable of reaching many diverse goals. To achieve this in a data-efﬁcient way, it is crucial that environment trajectories that were collected with one goal in mind are reused to also learn how to reach other goals. While prior work addressed this by goal relabeling which makes off-policy policy optimization a necessity [2], we instead leverage past trajectories via the world model trained on them that lets us generate an unlimited amount of new imagined trajectories for training the goal achiever on-policy in imagination. This simpliﬁes policy optimization and can improve stability, while still sharing all collected experience across many goals.
πg(at | st, eg)
Achiever:
Achiever Value: (5)
To train the goal achiever, we sample a goal image xg from the replay buffer and compute its embedding eg = encφ(xg). The achiever aims to maximize an unsupervised goal-reaching reward rg(st, eg). We discuss different choices for this reward in Section 2.4. We again use the Dreamer algorithm [24] for training, where now the value function also receives the goal embedding as input.
In addition to imagination training, it can also be important to perform practice trials with the goal achiever in the true environment, so that any model inaccuracies along the goal reaching trajectories vg(st, eg) 4
Figure 4: Successful LEXA trajectories. When given a goal image from the test set, LEXA’s achiever is used in the environment to reach that image. On RoboKitchen, LEXA manipulates up to three different objects together from a single goal image (kettle, light switch, and cabinet). On RoboBins,
LEXA performs temporally extended tasks such as picking and placing two objects in a row. may be corrected. To perform practice trials, we sample a goal from the replay buffer and execute the goal achiever policy for that goal in the environment. These trials are interleaved with exploration episodes collected by the exploration policy in equal proportion. We note that the goal achiever learning is entirely unsupervised because the practice goals are simply images the agent encountered through exploration or during previous practice trails. 2.4 Latent Distances
Training the achiever policy requires us to deﬁne a goal achievement reward rg(st, eg) that measures how close the latent state st should be considered to the goal eg. One simple measure is the cosine distance in the latent space obtained by inputting image observations into the world-model. However, such a distance function brings visually similar states together even if they could be farther apart in temporal manner as measured by actions needed to reach from one to other. This bias makes this suitable only to scenarios where most of pixels in the observations are directly controllable, e.g., trying to arrange robot’s body in certain shape, such as RoboYoga poses in Figure 2. However, many environments contain agent as well as the world, such as manipulation involves interacting with objects that are not directly controllable. The cosine distance would try matching the entire goal image, and thus places a large weight on both matching the robot and object positions with the desired goal. Since the robot position is directly controllable it is much easier to match, but this metric overly focuses on it, yielding poor policies that ignore objects. We address this is by using the number of timesteps it takes to move from one image to another as a distance measure [25, 26]. This ignores large changes in robot position, since these can be completed in very few steps, and will instead focus more on the objects. This temporal cost function can be learned purely in imagination rollouts from our world model allowing as much data as needed without taking any steps in the real world.
Cosine Distance To use cosine distance with LEXA, for a latent state st, and a goal embedding eg, we use the latent inference network q to infer sg, and deﬁne the reward as the cosine similarity [54]: rg t (st, eg) (cid:88)
.
= i stisgi, where st = st/(cid:107)st(cid:107)2, sg = sg/(cid:107)sg(cid:107)2, (6) i.e. the cosine of the angle between the two vectors st, sg in the N −dimensional latent space.
Temporal Distance To use temporal distances with LEXA, we train a neural network d to predict the number of time steps between two embeddings. We train it by sampling pairs of states st, st+k from an imagined rollout of the achiever and predicting the distance k. We implement the temporal distance in terms of predicted image embeddings ˆet+k in order to remove extra recurrent information:
Predicted embedding: emb(st) = ˆet ≈ et Temporal distance: dω(ˆet, ˆet+k) ≈ k/H, (7) where H is the maximum distance equal to the imagination horizon. Training distance function only on imagination data from the same trajectory would cause it to predict poor distance to far away states coming from other trajectories, such as images that are impossible to reach during one episode. In order to incorporate learning signal from such far-away goals, we include them by sampling images 5
Figure 5: Coincidental goal success achieved during the unsupervised exploration phase. The forward-looking explorer policy of LEXA results in substantially better coverage compared to SkewFit, a popular method for goal based exploration. from a different trajectory. We annotate these negative samples with the maximum possible distance, so that the agent always prefers images that were seen in the same trajectory. rg t (st, eg) = −dω(ˆet, eg), where (8)
The learned distance function depends on the training data policy. However, as the policy becomes more competent, the distance estimates will be closer to the optimal number of time steps to reach a particular goal, and the policy converges to the optimal solution [25]. LEXA always uses the latest data to train the distance function using imagination, ensuring that the convergence is fast. eg = encφ(xg)
ˆet = emb(st), 3 Experiments
Our evaluation focuses on the following scientiﬁc questions: 1. Does LEXA outperform prior work on previous benchmarks and a new challenging benchmark? 2. How does forward-looking exploration of goals compare to previous goal exploration strategies? 3. How does the distance function affect the ability to reach goals in different types of environments? 4. Can we train one general LEXA to control different robots across visually distinct environments? 5. What components of LEXA are important for performance?
We evaluate LEXA on prior benchmarks used by SkewFit [37], DISCERN [50], and Plan2Explore
[43] in Section 3.3. Since these benchmarks are largely saturated, we also introduce a new challenging benchmark shown in Figure 2. We evaluate LEXA on this benchmark is Section 3.2. 3.1 Experimental setup
As not many prior methods have shown success on reaching diverse goals from image inputs, we perform an apples-to-apples comparison by implementing the baselines using the same world model and policy optimization as our method:
• SkewFit
SkewFit [37] uses model-free hindsight experience replay and explores by sampling goals from the latent space of a variational autoencoder [28, 38]. Being one of the state-of-the-art agents, we use the original implementation that does not use a world model or explorer policy.
• DDL Dynamic Distance Learning [25] trains a temporal distance function similar to our method.
Following the original algorithm, DDL uses greedy exploration and trains the distance function on the replay buffer instead of in imagination.
• DIAYN Diversity is All You Need [15] learns a latent skill space and uses mutual information between skills and reached states as the objective. We augment DIAYN with our explorer policy and train a learned skill predictor to obtain a skill for a given test image [12].
• GCSL Goal-Conditioned Supervised Learning [20] trains the goal policy on replay buffer goals and mimics the actions that previously led to the goal. We also augment GCSL with our explorer policy, as we found no learning success without it.
Our new benchmark deﬁnes goal images for a diverse set of four existing environments as follows: 6
Figure 6: Evaluation of goal reaching agents on our four benchmarks. A single agent is trained from images without rewards and then evaluated on reaching goal images from the test set (see Figure 1).
Both LEXA agents solve many of the tasks and signiﬁcantly outperform prior work. SkewFit and
DLL struggle with exploration, while DIAYN and GCSL use our explorer but still are not able to learn a good downstream policy. Refer table 1 for ﬁnal success percentage (averaged across tasks) for each method and benchmark domain.
• RoboYoga We use the walker and quadruped domains of the DeepMind Control Suite [48] to deﬁne the RoboYoga benchmark, consisting of 12 goal images that correspond to different body poses for each of the two environments, such as lying down, standing up, and balancing.
• RoboBins
Based on MetaWorld [53], we create a scene with a Sawyer robotic arm, two bins, and two blocks of different colors. The goal images specify tasks that include reaching, manipulating only one block, and manipulating both blocks.
• RoboKitchen
The last benchmark involves the challenging kitchen environment from [22], where a franka robot can interact with various objects including a burner, light switch, sliding cabinet, hinge cabinet, microwave, or kettle. The goal images we include describe tasks that require interacting with only one object, as well as interacting with two objects. 3.2 Performance on New Benchmark
We show the results on our main benchmark in Figure 6 and include heatmaps that show per-task success on each of the evaluation tasks from the benchmarks in the Appendix. Further, we report success averaged across tasks for each domain at the end of training in Table 1. We visualize example successful trajectory executions for tasks that require manipulating multiple objects in Fig. 4.
RoboYoga The environments in this benchmark are directly controllable since they contain no other objects except the robot. We recall that for such settings we expect the cosine distance to be effective, as perceptual distance is quite accurate. Training is thus faster compared to using learned temporal distances, where the metric is learned from scratch. From Table 1 and Figure 6 we see that this is indeed the case for these environments (Walker and Quadruped), as LEXA with the cosine metric outperforms all prior approaches. Furthermore with temporal distances LEXA makes better progress compared to prior work on a much larger number of goals as can be seen from the per-task performance (Figures ??, ??), even though average success over goals looks similar to that of DDL.
RoboBins This environment involves interaction with block objects, and thus is not directly control-lable, and so we expect LEXA to perform better with the temporal distance metric. From Table 1 and
Method
DDL
DIAYN
GCSL
SkewFit
LEXA + Temporal (Ours)
LEXA + Cosine (Ours)
Kitchen RoboBins Quadruped Walker 0.00 0.00 0.00 0.23 37.50 6.02 35.42 13.69 7.94 15.77 69.44 45.83 22.50 13.81 15.83 5.52 31.39 56.11 40.00 0.28 1.11 0.01 36.72 73.06
Table 1: Performance on our new challenging benchmark, spanning across the four domains shown in Figure 2. The number are goal success rates, averaged over test goals within each environment. 7
Figure 7: Success rates on RoboBin. In line with the prior literature, previous methods are successful at reaching and sometimes pushing. LEXA pushes the state-of-the-art by picking and placing multiple objects to reach challenging goal images. Analogous heat maps for the other domains are included in the appendix.
Figure 6, we see that LEXA gets higher average success than all prior approaches. Further from the per-task performance in 7, LEXA with the temporal distance metric is the only approach that makes progress on all goals in the benchmark. The main difference in performance between using temporal and cosine distance can be seen in the tasks involving two blocks, which are the most complex tasks in this environment (the last 3 columns of the per-task plot). The best performing prior method is
DDL which solves reaching, and can perform simple pushing tasks. This method performs poorly due to poor exploration, as shown in Figure 5. We see that while other prior methods make some progress on reaching, they fail on harder tasks.
RoboKitchen This benchmark involves diverse objects that require different manipulation behavior.
From Table 1 and Figure 6 and ?? we ﬁnd that LEXA with temporal distance is able to learn multiple
RoboKitchen tasks, some of which require sequentially completing 2 tasks in the environment. All prior methods barely make progress due to the challenging nature of this benchmark, and furthermore using the cosine distance function makes very limited progress. The gap in performance between using the two distance functions is much larger in this environment compared to RoboBins since there are many more objects and they are not as clearly visible as the blocks.
Single Agent Across All Environments
In the previous sections we have shown that our approach can achieve diverse goals in different environments. However, we trained a new agent for every new environment, which doesn’t scale well to large numbers of environments. Thus we investigate if we can train a train a single agent across four environments in the benchmark. From Figure ?? we see that our approach with learned temporal distance is able to make progress on tasks from RoboKitchen, RoboBins Reaching, RoboBins Pick &
Place and Walker, while the best prior method on the single-environment tasks (DDL) mainly solves walker tasks and reaching from RoboBin. 3.3 Performance on Prior benchmarks
To further verify the results obtained on our bench-mark, we evaluate LEXA on previously used bench-marks. We observe that LEXA signiﬁcantly outper-forms prior work on these benchmarks, and is often close to the optimal policy. Additional details are provided in ??????.
Table 2: Goal distance for SkewFit goals [37].
Method
Pusher
Pickup
RIG [32]
RIG + HER [2]
Skew-Fit [37]
LEXA + Temporal 3.7cm 7.7cm 3.5cm 7.5cm 4.9cm 1.8cm 2.3cm 1.4cm
SkewFit Benchmark SkewFit [37] introduces a robotic manipulation benchmark for unsupervised methods with simple tasks like planar pushing or picking. We evaluate on this benchmark in Table 2.
Baseline results are taken from [37]. LEXA signiﬁcantly outperforms prior work on these tasks.
Pushing and picking up blocks from images is largely solved and future work can focus on harder benchmarks such as those introduced in our paper. 8
Table 3: Success for DISCERN goals [50].
DISCERN 76.5% 21.3% 21.8% 75.7% 49.6% 87.1%
LEXA 84.0% 35.9% 40.9% 79.1% 83.2% 100.0%
Task
Cup
Cartpole
Finger
Pendulum
Pointmass
Reacher
DISCERN Benchmark We attempted to replicate the tasks described in [50] that are based on simple two-dimensional robots [48]. While the original tasks are not released, we followed the procedure for gener-ating the goals described in the paper. Despite follow-ing the exact procedure, we were not able to obtain similar goals to the ones used in the original paper.
Nevertheless, we show the goal completion percent-age results obtained with our reproduced evaluation compared to DISCERN results from the original pa-per. LEXA results were obtained with early stopping.
In Table 3 we see that our agent solves many of the tasks in this benchmark.
Plan2Explore Benchmark We provide a compar-ison on the standard reward-based DM control tasks
[48] in Table 4. To compare on this benchmark, we create goal images that correspond to the reward func-tions. This setup is arguably harder for our agent, but is much more practical. Note our agent never ob-serves the reward function and only observes the goal at test time. Plan2Explore adapts to new tasks but it needs the reward function to be known at test time, while DrQV2 is an oracle agent that observes the reward at training time. Baseline results are taken from [43, 52]. LEXA results were obtained with early stopping. LEXA outperforms Plan2Explore on most tasks and even performs comparably to state of the art oracle agents (DrQ, DrQv2, Dreamer) that use true task rewards during training.
Walker Stand
Hopper Stand
Cartpole Balance
Cartpole Bal. Sparse
Pendulum Swing Up
Cup Catch
Reacher Hard 957 840 886 996 788 969 937
Task
Zero-Shot (cid:51)
LEXA P2E DrQv2 (cid:51)* (cid:55) 331 841 950 860 792 962 66 968 957 989 983 837 909 970
Table 4: Zero-shot return on P2E tasks [43]. 3.4 Analysis
Prior work Most work we compared against strug-gles with exploration, such as SkewFit and DLL meth-ods. DIAYN is augmented with our explorer, but still fails to leverage the exploration data to learn a diverse set of skills. GCSL struggles to ﬁt the ex-ploration data and produces behavior that does not solve the task, perhaps because the exploration data is too diverse. We observed that all baselines make progress on the simple reaching, but struggle with other tasks. We have experimented with several ver-sions and improvements to the baselines and report the best obtained performance.
Ablation of different components We ablated components of LEXA on the RoboBins environment in Figure 8. Using a separate explorer policy crucial as without it the agent does not discover the more in-teresting tasks. Without negative sampling the agent learns slower, perhaps because the distance function doesn’t produce reasonable outputs when queried on images that are more than horizon length apart. Train-ing the distance function with real data converges to slightly lower success than using imagination data, since real data is sampled in an off-policy manner due to its limited quantity.
Exploration performance Due to importance of exploration, we further examine the diversity of the data collected during training. We log the instances where the agent coincidentally solves an evaluation task during exploration, for the RoboKitchen and RoboBins environments. In Figure 5, we see that our method encounters harder tasks involving multiple objects much more often.
Figure 8: Ablations on RoboBins. A separate explorer is crucial for most tasks. Training temporal distance on negative samples speeds up learning, and both negative sampling and training in imagination as opposed to real data are important for the hardest tasks. 9
4