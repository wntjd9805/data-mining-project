Abstract
Federated learning (FL) has gain growing interests for its capability of learning from distributed data sources collectively without the need of accessing the raw data samples across different sources. So far FL research has mostly focused on improving the performance, how the algorithmic disparity will be impacted for the model learned from FL and the impact of algorithmic disparity on the utility inconsistency are largely unexplored. In this paper, we propose an FL framework to jointly consider performance consistency and algorithmic fairness across different local clients (data sources). We derive our framework from a constrained multi-objective optimization perspective, in which we learn a model satisfying fairness constraints on all clients with consistent performance. Speciﬁcally, we treat the algorithm prediction loss at each local client as an objective and maximize the worst-performing client with fairness constraints through optimizing a surrogate maximum function with all objectives involved. A gradient-based procedure is employed to achieve the Pareto optimality of this optimization problem. Theoretical analysis is provided to prove that our method can converge to a Pareto solution that achieves the min-max performance with fairness constraints on all clients.
Comprehensive experiments on synthetic and real-world datasets demonstrate the superiority that our approach over baselines and its effectiveness in achieving both fairness and consistency across all local clients. 1

Introduction
Federated learning (FL) [1] refers to the paradigm of learning from fragmented data without sacriﬁcing privacy. FL has aroused broad interests from diverse disciplines including high-stakes scenarios such as loan approvals, criminal justice, healthcare, etc [2]. An increasing concern is whether these FL systems induce disparity in local clients in these cases. For example, ProPublica reported that an algorithm used across the US for predicting a defendant’s risk of future crime produced higher scores to African-Americans than Caucasians on average [3]. This has caused severe concerns from the public on the real deployment of data mining models and made algorithmic fairness an important research theme in recent years.
Existing works on algorithmic fairness in machine learning have mostly focused on individual learning scenarios. There has not been much research on how FL will impact the model fairness at 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
different local clients1. Recently, Du et al. [6] proposed a fairness-aware method, which considered the global fairness of the model learned through a kernel re-weighting mechanism. However, such a mechanism can not guarantee to achieve fairness at local clients in FL scenario, since different clients will have different distributions across protected groups. For example, if we are building a mortality prediction model for COVID-19 patients within a hospital system [7], where each individual hospital can be viewed as a local client. Different hospitals will have different patient populations with distinct demographic compositions including race or gender. In this case, the model fairness at each hospital is important because that’s where the model will be deployed, and it is unlikely that global model fairness can lead to local model fairness.
Due to the potential trade-off between algorithmic fairness and model utility, one aiming to mitigate the algorithmic disparity on local clients can exacerbate the inconsistency of the model performance (i.e., the model performance is different at different local clients). There have been researches [4, 5] trying to address the inconsistency without considering algorithmic fairness. In particular, Mohri et al. [5] proposed an agnostic federated learning (AFL) algorithm that maximizes the performance of the worst performing client. Li et al. [4] proposed a q-Fair Federated Learning (q-FFL) approach to weigh different local clients differently by taking the q-th power of the local empirical loss when constructing the optimization objective of the global model.
In this paper, we consider the problem of enforcing both algorithmic fairness and performance consistency across all local clients in FL. Speciﬁcally, suppose we have N local clients, and ui represents the model utility for client i, and gi is the model disparity quantiﬁed by some computational fairness metric (e.g., demographic parity [8] or equal opportunity [9]). Following the idea of AFL, we can maximize the utility of the worst-performed client to achieve performance consistency. We also propose to assign each client a "fairness budget" to ensure certain level of local fairness, i.e.,
, N ) with (cid:15)i being a pre-speciﬁed fairness budget for client i. Therefore, we gi ≤ can formulate our problem as a constrained multi-objective optimization framework as shown in
Figure 1(a), where each local model utility can be viewed as an optimization objective. i = 1, 2, (cid:15)i(
· · ·
∀
Since models with fairness and min-max performance may be not unique, we also require the model to be Pareto optimal. A model is Pareto optimal if and only if the utility of any client cannot be further optimized without degrading some others. A Pareto optimal solution of this problem cannot be achieved by existing linear scalarization methods in federated learning (e.g., federated average, or FedAve in [10]), as the non-i.i.d data distributions across different clients can cause a non-convex Pareto Front of utilities (all Pareto solutions form Pareto Front). Therefore, we propose
FCFL, a new federated learning framework to obtain a fair and consistent model for all local clients.
Speciﬁcally, we ﬁrst utilize a surrogate maximum function (SMF) that considers the N utilities involved simultaneously instead of the single worst, and then optimize the model to achieve Pareto optimality by controlling the gradient direction without hurting client utilities. Theoretical analysis proves that our method can converge to a fairness-constrained Pareto min-max model and experiments on both synthetic and real-world data sets show that FCFL can achieve a Pareto min-max utility distribution with fairness guarantees in each client. The source codes of FCFL are made publicly available at https://github.com/cuis15/FCFL. 2