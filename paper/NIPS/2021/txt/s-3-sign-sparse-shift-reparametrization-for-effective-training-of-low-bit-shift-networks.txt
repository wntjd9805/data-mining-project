Abstract
Shift neural networks reduce computation complexity by removing expensive multiplication operations and quantizing continuous weights into low-bit discrete values, which are fast and energy-efﬁcient compared to conventional neural net-works. However, existing shift networks are sensitive to the weight initialization and yield a degraded performance caused by vanishing gradient and weight sign freezing problem. To address these issues, we propose S3 re-parameterization, a novel technique for training low-bit shift networks. Our method decomposes a discrete parameter in a sign-sparse-shift 3-fold manner. This way, it efﬁciently learns a low-bit network with weight dynamics similar to full-precision networks and insensitive to weight initialization. Our proposed training method pushes the boundaries of shift neural networks and shows 3-bit shift networks compete with their full-precision counterparts in terms of top-1 accuracy on ImageNet. 1

Introduction
While deep neural networks (DNNs) have achieved widely success in various tasks, the training and inference of DNNs usually cost prohibitive resources due to the fact that they are often over-parametrized and composed of computational costly multiplication operations for both forward and backward propagation [28]. To enable the application feasibility of DNNs in resource-constrained scenarios, substantial efforts have been made to reduce the computation complexity of neural networks while preserving their accuracy. Among different proposed approaches for this purpose, low-bit neural networks with binary weights [6, 22] or ternary weights [15] are recently designed to replace the expensive operations like multiplication with cheaper ones, e.g., replacing multiplication with sign ﬂip operation during inference.
We focus on low-bit shift neural networks [11, 9, 30] that replace multiplication with the bit-shift operation. Following the conventional linear algebra notations, we denote scalars by x, vectors in bold small letters x, and matrices by bold capital letters X. Traditional neural networks require computing the inner product w(cid:62)x = (cid:80) i wixi, where w is the weight vector and x is the feature vector. In comparison, shift neural networks limit the values of the weight vectors to a set of discrete values wshift ∈ {0} ∪ {±2p}. In this case, multiplication can be replaced with bit-shift operations, as multiplying wi = 2p is equavilent to shifting p places to the left (if p is positive) or to the right (if p is negative). The bit-shift operation alone only covers discrete neural networks with positive weights, therefore, most shift neural networks further add a sign ﬂip operation to the shift to allow wi = {±2p}. Inference with bit-shift operations is highly hardware friendly and can save up to 196× energy cost on FPGA and 24× on ASIC compare to their multiplication counterparts [28].
Several works aim to further improve the memory efﬁciency of shift neural networks by reducing the bit-width of the weights. Zhou et al. [30] propose to ﬁne-tune pre-trained full-precision weights 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
with a progressive power-of-two weight quantization strategy. They suggest to keep a portion of weights unquantized during ﬁne-tuning to improve model performance. DeepShift [9] is the state-of-the-art technique for training low-bit shift networks from random initialization. Initialization with pre-trained full-precision weights provides a signiﬁcant performance gain for DeepShift. All latest techniques heavily rely on the initialization of full-precision pre-trained weights, which implies some optimization difﬁculties exist in the current training diagram of low-bit shift networks. However, although extremely memory efﬁcient and hardware friendly, low-bit shift networks suffer from a signiﬁcant accuracy drop on large datasets compared to their full-precision counterpart, and the performance of them is sensitive to weight initialization.
To address these problems, in this work, we analyze the current training diagram of low-bit shift neural networks, and ﬁnd that the accuracy degradation and the weight initialization sensitivity of the low-bit shift networks are caused by the design ﬂaw of the weight quantizer during training. Speciﬁcally, training shift neural networks is different from training full-precision networks. The gradient-based optimization algorithms are designed for optimizing continuous variables, but the weights of shift neural networks are discrete values. To learn the discrete weights with a gradient-based approach, it is necessary to exploit weight re-parameterization. One of the most widely adopted discrete weight re-parameterization technique is using a quantizer function to map a continuous parameter w ∈ R to a discrete weight w ∈ {v1, v2, · · · , vk} with k possible values. As the weight quantizers are generally non-differentiable functions, a gradient approximator, such as straight-through estimator
[3], is utilized to approximate their derivatives during training. Our analysis shows the quantizer leads to a severe gradient vanishing and weight sign freezing.
In this paper, we design an effective training strategy to combat the gradient vanishing and weight sign freezing problem in low-bit shift networks. We ﬁrst impose a dense weight priori during training, which equals maximizing the (cid:96)0 norm of the discrete weights. However, optimizing (cid:96)0 norm during training is a combinatorial problem, instead we propose S3 re-parameterization and regularize the sparsity parameter. S3 is a new technique which re-parameterizes the discrete weights of shift networks in a sign-sparse-shift, a 3-dimensional augmented parameter space to disentangle the roles of quantization values, and hopefully train more effectively thanks to more orthogonal axes in the augmented space. Although our method introduces extra parameters during training, its memory occupation under low-bit is slightly higher than the full-precision network since the additional memory overhead depends on the weight size. In contrast, the activation size dominates the training memory occupation, especially using a large batch size.
We evaluate our approach on the ImageNet dataset. While all previous methods require at least 5-bit weight representation to achieve the same performance as the full-precision neural networks on large datasets such as ImageNet, our experimental results show that our proposed method surpasses all previous methods, pushes this boundary further to 3-bits. Besides, our approach requires no complicated weight initialization or training strategy. Moreover, we deﬁne two indices of weight dynamics, named weight sign variation rate (WSVR) and weight low-value rate (WLVR), and compare the trend of these two indices during the training process of different methods. Experiments show that the weight dynamics of the ternary weights trained with our proposed technique better align with the weight dynamics of full-precision weights than trained with a traditional quantizers.
This desirable dynamics is caused by our re-parametrization, and is the key to efﬁcient training. We hope that this opens the possibility for methods similar as S3 to be used in training low-bit networks. 2