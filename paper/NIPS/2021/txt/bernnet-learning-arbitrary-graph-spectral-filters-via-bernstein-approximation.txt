Abstract
Many representative graph neural networks, e.g., GPR-GNN and ChebNet, ap-proximate graph convolutions with graph spectral ﬁlters. However, existing work either applies predeﬁned ﬁlter weights or learns them without necessary constraints, which may lead to oversimpliﬁed or ill-posed ﬁlters. To overcome these issues, we propose BernNet, a novel graph neural network with theoretical support that provides a simple but effective scheme for designing and learning arbitrary graph spectral ﬁlters. In particular, for any ﬁlter over the normalized Laplacian spectrum of a graph, our BernNet estimates it by an order-K Bernstein polynomial approxi-mation and designs its spectral property by setting the coefﬁcients of the Bernstein basis. Moreover, we can learn the coefﬁcients (and the corresponding ﬁlter weights) based on observed graphs and their associated signals and thus achieve the BernNet specialized for the data. Our experiments demonstrate that BernNet can learn arbitrary spectral ﬁlters, including complicated band-rejection and comb ﬁlters, and it achieves superior performance in real-world graph modeling tasks. Code is available at https://github.com/ivam-he/BernNet. 1

Introduction
Graph neural networks (GNNs) have received extensive attention from researchers due to their excellent performance on various graph learning tasks such as social analysis [24, 17, 29], drug discovery [12, 25], trafﬁc forecasting [18, 3, 6], recommendation system [38, 32] and computer vision [39, 4]. Recent studies suggest that many popular GNNs operate as polynomial graph spectral
ﬁlters [7, 13, 5, 16, 2, 35]. Speciﬁcally, we denote an undirected graph with node set V and edge
Rn on the graph, set E as G = (V, E), whose adjacency matrix is A. Given a signal x = [x] is the number of nodes, we can formulate its graph spectral ﬁltering operation as where n = 1/2 is the symmetric normalized
Laplacian matrix of G, and D is the diagonal degree matrix of A. Another equivalent polynomial (cid:80) 1/2 is the normalized adjacency matrix
ﬁltering operation is and ck’s are the ﬁlter weights.
−
K 1/2AD− k=0 ckPkx, where P = D−
K k=0 wkLkx, wk’s are the ﬁlter weights, L = I 1/2AD−
D−
∈
V
|
| (cid:80)
*Zhewei Wei and Hongteng Xu are the corresponding authors. Work partially done at Gaoling School of
Artiﬁcial Intelligence, Beijing Key Laboratory of Big Data Management and Analysis Methods, MOE Key Lab of Data Engineering and Knowledge Engineering, Renmin University of China, and Pazhou Lab, Guangzhou, 510330, China. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: An illustration of the proposed BernNet.
We can broadly categorize the GNNs applying the above ﬁltering operation into two classes, de-pending on whether they design the ﬁlter weights or learn them based on observed graphs. Some representative models in these two classes are shown below.
• The GNNs driven by designing ﬁlters: GCN [13] uses a simpliﬁed ﬁrst-order Chebyshev polynomial, which is proven to be a low-pass ﬁlter [1, 31, 34, 41]. APPNP [14] utilizes
Personalized PageRank (PPR) to set the ﬁlter weights and achieves a low-pass ﬁlter as well [15, 41]. GNN-LF/HF [41] designs ﬁlter weights from the perspective of graph optimization functions, which can simulate high- and low-pass ﬁlters.
• The GNNs driven by learning ﬁlters: ChebNet [7] approximates the ﬁltering operation with Chebyshev polynomials, and learns a ﬁlter via trainable weights of the Chebyshev basis. GPR-GNN [5] learns a polynomial ﬁlter by directly performing gradient descent on the ﬁlter weights, which can derive high- or low-pass ﬁlters. ARMA [2] learns a rational
ﬁlter via the family of Auto-Regressive Moving Average ﬁlters [21].
Although the above GNNs achieve some encouraging results in various graph modeling tasks, they still suffer from two major drawbacks. Firstly, most existing methods focus on designing or learning simple ﬁlters (e.g., low- and/or high-pass ﬁlters), while real-world applications often require much more complex ﬁlters such as band-rejection and comb ﬁlters. To the best of our knowledge, none of the existing work supports designing arbitrary interpretable spectral ﬁlters. The GNNs driven by learning ﬁlters can learn arbitrary ﬁlters in theory, but they cannot intuitively show what ﬁlters they have learned. In other words, their interpretability is poor. For example, GPR-GNN [5] learns the ﬁlter weights wk’s but only proves a small subset of the learnt weight sequences corresponds to low- or high-pass ﬁlters. Secondly, the GNNs often design their ﬁlters empirically or learn the
ﬁlter weights without any necessary constraints. As a result, their ﬁlter weights often have poor controllability. For example, GNN-LF/HF [41] designs its ﬁlters with a complex and non-intuitive polynomial with difﬁcult-to-tune hyperparameters. The multi-layer GCN/SGC [13, 31] leads to
“ill-posed” ﬁlters (i.e., those deriving negative spectral responses). Additionally, the ﬁlters learned by
GPR-GNN [5] or ChebNet [7] have a chance to be ill-posed as well.
To overcome the above issues, we propose a novel graph neural network called BernNet, which provides an effective algorithmic framework for designing and learning arbitrary graph spectral
ﬁlters. As illustrated in Figure 1, for an arbitrary spectral ﬁlter h : [0, 2]
[0, 1] over the spectrum of the symmetric normalized Laplacian L, our BernNet approximates h by a K-order Bernstein
K k=0 θkbK
K polynomial approximation, i.e., h(λ) = k=0
K k=0 work as the model parameter, which can be interpreted as of the Bernstein basis h(2k/K), k = 0, . . . , K (i.e., the ﬁlter values uniformly sampled from [0, 2]). By designing or learning the θk’s, we can obtain various spectral ﬁlters, whose ﬁltering operation can be formulated kLkx, where x is the graph signal. We further demonstrate the as rationality of our BernNet from the perspective of graph optimization — any valid polynomial ﬁlers, i.e., those polynomial functions mapping [0, 2] to [0, 1], can always be expressed by our BernNet, and accordingly, the ﬁlters learned by our BernNet are always valid. Finally, we conduct experiments to demonstrate that 1) BernNet can learn arbitrary graph spectral ﬁlters (e.g., band-rejection, comb, low-band-pass, etc.), and 2) BernNet achieves superior performance on real-world datasets. k (λ). The non-negative coefﬁcients bK k (λ)
}
{
K k=0 θk
θk}
{
L)K 1 2K (2I (cid:80) (cid:80)
K k (cid:55)→
− (cid:0) (cid:1)
− 2
2 BernNet 2.1 Bernstein approximation of spectral ﬁlters
[0, 1], let L = UΛUT denote the eigendecomposition
Given an arbitrary ﬁlter function h : [0, 2] of the symmetric normalized Laplacian matrix L, where U is the matrix of eigenvectors and Λ = diag[λ1, ..., λn] is the diagonal matrix of eigenvalues. We use (cid:55)→ h(L)x = Uh(Λ)UT x = Udiag[h(λ1), ..., h(λn)]UT x to denote a spectral ﬁlter on graph signal x. The key of our work is approximate h(L) (or, equivalently, h(λ)). For this purpose, we leverage the Bernstein basis and Bernstein polynomial approximation deﬁned below.
Deﬁnition 2.1 ( [10]). tion f (t) on t
[0, 1], the Bernstein polynomial approximation (of order K) for f is deﬁned as (Bernstein polynomial approximation) Given an arbitrary continuous func-(1)
∈
K pK(t) := bK k (t) =
θk ·
K f k
K
K k
· (1
− t)K ktk.
− (2) (cid:88)k=0 k (t) = (cid:18) (cid:19) (cid:18) (cid:88)k=0 ktk is the k-th Bernstein base, and θk = f ( k
− (cid:19)
K ) is the
Here, for k = 0, ..., K, bK function value at k/K, which works as the coefﬁcient of bK (cid:1)
Lemma 2.1 ( [10]). Given an arbitrary continuous function f (t) on t
Bernstein approximation of f (t) as deﬁned in Equation (2). We have pK(t) k (t). t)K (1
−
∈ (cid:0)
K k
λ (cid:55)→
K k
→ 2 and f (t) = h(2t), so that the Bernstein k (t) = 2 )k for k = 1, ..., K. Consequently, we can approximate h(λ) by kλk, and Lemma 2.1
[0, 1], we let t = λ
For the ﬁlter function h : [0, 2] polynomial approximation becomes applicable, where θk = f (k/K) = h(2k/K) and bK k ( λ bK 2 ) = (1
−
K
λ pK(λ/2) = k=0 θk (cid:0) 2
. ensures that pK(λ/2) (cid:0)
→ ∞ n
Replacing i=1, we approximate the spectral ﬁlter h(L) in Equation (1)
} as Udiag[pK(λ1/2), ..., pK(λn/2)]UT and derive the proposed BernNet. In particular, given a graph signal x, the convolutional operator of our BernNet is deﬁned as follows: k( λ 2 )K (1
− h(λ) as K (cid:0) (cid:1)
→ (cid:80) n i=1 with h(λi)
} pK(λi/2)
{
K k=0 θk 2 )K
λ)K 1 2K (cid:80)
K k
K k (2
=
−
{ (cid:0) (cid:1) (cid:1) (cid:1)
−
−
−
λ k k
[0, 1], let pK(t) denote the f (t) as K
.
→ ∞ z = Udiag[pK(λ1/2), ..., pK(λn/2)]UT x =
BernNet
K (cid:88)k=0
θk 1 2K
K k (cid:18) (cid:19) (2I
−
L)K kLkx
− (3) (cid:124) where each coefﬁcient θk can be either set to h(2k/K) to approximate a predetermined ﬁlter h, (cid:125) or learnt from the graph structure and signal in an end-to-end fashion. As a natural extension of
Lemma 2.1, our BernNet owns the following proposition.
Proposition 2.1. For an arbitrary continuous ﬁlter function h : [0, 2] h(L)x as K h(2k/K), k = 0, . . . , K, the z in Equation (3) satisﬁes z
[0, 1], by setting θk = (cid:123)(cid:122)
→
→
.
→ ∞
Proof. According to the above derivation, we have pK(λ/2) =
λ)K
− kλk, and Lemma 2.1 ensures that pK(λ/2) (cid:80) (cid:0)
→
−
K k=0 θk k k
λ (1
K 2 )K
=
− k h(λ) as θk = h(2k/K) (cid:1)
−
λ 2 (cid:0) (cid:1)
K k=0 θk 1 2K
K k (2 and K (cid:80)
Consequently, we have
.
→ ∞ (cid:0) (cid:1) z = Udiag[pK(λ1/2), ..., pK(λn/2)]UT x as θk = h(2k/K) and K
.
→ ∞
Udiag[h(λ1), ..., h(λn)]UT x = h(L)
→ 2.2 Realizing existing ﬁlters with BernNet.
As shown in Proposition 2.1, our BernNet can approximate arbitrary continuous spectral ﬁlters with sufﬁcient precision. Below we give some representative examples of how our BernNet exactly realizes existing ﬁlters that are commonly used in GNNs. 3
Table 1: Realizing commonly used ﬁlters with BernNet.
Filter types
Filter h(λ)
All-pass
Linear low-pass
Linear high-pass
Impulse low-pass
Impulse high-pass
Impulse band-pass
λ/2 1 1
−
λ/2
δ0(λ)
δ2(λ)
δ1(λ) k/K
θk for k = 0, . . . , K
θk = 1
θk = 1
−
θk = k/K
θ0 = 1 and other θk = 0
θK = 1 and other θk = 0
θK/2 = 1 and other θk = 0
λ/2
Bernstein approximation pK( λ 1 1
−
λ/2 (1
− (λ/2)K
K (1
K/2
λ/2)K/2(λ/2)K/2 2 ) BernNet
I
I
− 1 2 L 1 2K (2I 1 2K LK 1 2K
λ/2)K
K
K/2 1 2 L
−
−
L)K (2I
−
L)K/2LK/2 (cid:0) (cid:1) (cid:0) (cid:1)
• All-pass ﬁlter h(λ) = 1. We set θk = 1 for k = 0, . . . , K, and the approximation 2 ) = 1 is exactly the same with h(λ). Accordingly, our BernNet becomes an identity pK( λ matrix, which realizes the all-pass ﬁlter perfectly.
• Linear low-pass ﬁlter h(λ) = 1 2 ) = 1
λ/2. The BernNet becomes pK( λ which achieves the linear low-pass ﬁlter exactly. Note that I (cid:0) same as the graph convolutional network (GCN) before renormalization [13]. (2I
− 2 L = 1 1 (cid:80) (cid:1)
−
− k/K for k = 0, . . . , K and obtain
− 1 1
L)K kLk = I 2 L, 2K 2 (I + P) is also the
K k
−
−
−
λ/2. We set θk = 1
K k)
− k=0
K (K
• Linear high-pass ﬁlter h(λ) = λ/2. Similarly, we can set θk = k/K for k = 0, . . . , K to get a perfect approximation pK( λ 2 ) = λ 2 , and the BernNet becomes 1 2 L.
Note that even for those non-continuous spectral ﬁlters, e.g., the impulse low/high/band-pass ﬁlters, our BernNet can also provide good approximations (with sufﬁcient large K).
• Impulse low-pass ﬁlter h(λ) = δ0(λ).† We set θ0 = 1 and θk = 0 for k 2 ) = (1 pK( λ linear low-pass ﬁlter.
−
λ 2 )K. Accordingly, the BernNet becomes 1 2K (2I
−
= 0, and
L)K, deriving an K-layer
• Impulse high-pass ﬁlter h(λ) = δ2(λ). We set θK = 1 and θk = 0 for k
= K, and pK( λ 2 ) = ( λ 2 )K. The BernNet becomes 1 2K LK, i.e., an K-layer linear high-pass ﬁlter.
• Impulse band-pass ﬁlter h(λ) = δ1(λ). Similarly, we set θK/2 = 1 and θk = 0 for k (2I
K/2, and pK( λ
−
L)K/2LK/2, which can be explained as stacking a K/2-layer linear low-pass ﬁlter and a
K/2-layer linear high-pass ﬁlter. Obviously, K should be an even number in this case.
λ/2)K/2(λ/2)K/2. The BernNet becomes 1 2K 2 ) =
K
K/2
K
K/2 (1
−
= (cid:1) (cid:0) (cid:1) (cid:0)
Table 1 summarizes the design of the BernNet for the ﬁlters above. We can ﬁnd that an appealing advantage of our BernNet is that its coefﬁcients are highly correlated with the spectral property of the 2k
K by using target ﬁlter. In particular, we can determine to pass or reject the spectral signal with λ a large or small θk because each Bernstein base bK
K . This property provides useful guidance when designing ﬁlters, which enhances the interpretability of our
BernNet. k (λ) corresponds to a “bump” located at 2k
≈ 2.3 Learning complex ﬁlters with BernNet
{
θk}
Besides designing the above typical ﬁlters, our BernNet can express more complex ﬁlters, such as band-pass, band-rejection, comb, low-band-pass ﬁlters, etc. Moreover, given the graph signals before and after applying such ﬁlters (i.e., the x’s and the corresponding z’s), our BernNet can learn their approximations in an end-to-end manner. Speciﬁcally, given the pairs
, we learn the
}
K coefﬁcients k=0 of the BernNet by gradient descent. More implementation details can be found at the experimental section below. Figure 2 illustrates the four complex ﬁlters and the approximations 0.5)2)I(0.5,1)(λ) + we learned (The low-band pass ﬁlter is h(λ) = I[0,0.5](λ) + exp (
−
Ω, otherwise IΩ(λ) = 0). In general, exp ( our BernNet can learn a smoothed approximation of these complex ﬁlters, and the approximation precision improves with the increase of the order K. Note that although the BernNet cannot pinpoint the exact peaks of the comb ﬁlter or drop to 0 for the valleys of comb or low-band-pass ﬁlters due to the limitation of K, it still signiﬁcantly outperforms other GNNs for learning such complex ﬁlters. 1.5)2)I[1,2](λ), where IΩ(λ) = 1 when λ 100(λ 50(λ x, z
−
−
−
∈
{
†The impulse function δx(λ) = 1 if λ = x, otherwise δx(λ) = 0 4 (cid:54) (cid:54) (cid:54)
(a) Band-pass (d) Low-band-pass
Figure 2: Illustrations of four complex ﬁlters and their approximations learnt by BernNet. (b) Band-rejection (c) Comb 3 BernNet in the Lens of Graph Optimization
In this section, we motivate BernNet from the perspective of graph optimization. In particular, we show that any polynomial ﬁlter that attempts to approximate a valid ﬁlter has to take the form of
BernNet. 3.1 A generalized graph optimization problem
Given a n-dimensional graph signal x, we consider a generalized graph optimization problem min z f (z) = (1
−
α)zT γ(L)z + α z (cid:107)
− 2 2 x (cid:107) (4)
∈
∈
[0, 1) is a trade-off parameter, z
Rn denotes the propagated representation of the input where α graph signal x, and γ(L) denotes an energy function of L, determining the rate of propagation [28].
Generally, γ(
) operates on the spectral of L, and we have γ(L) = Udiag[γ(λ1), ..., γ(λn)]UT .
·
We can model the polynomial ﬁltering operation of existing GNNs with the optimal solution of
Equation (4). For example, if we set γ(L) = L, then the optimization function (4) becomes 2 2, a well-known convex graph optimization function proposed by f (z) = (1 x (cid:107)
Zhou et al. [40]. f (z) takes the minimum when the derivative ∂f (z) x) = 0,
∂z = 2(1 which solves to
α)zT Lz + α
α)Lz + 2α (z
−
−
−
− z (cid:107) z∗ = α (I (1
−
−
α)(I
−
L))− 1 x =
α(1
−
α)k (I
−
L)k x =
∞ (cid:88)k=0
α(1
−
α)kPkx.
∞ (cid:88)k=0
α)kPkx, we obtain the polynomial ﬁltering operation for
By taking a sufﬁx sum
APPNP [14]. Zhu et al. [41] further show that GCN [13], DAGNN [19], and JKNet [36] can be interpreted by the optimization function (4) with γ(L) = L. (cid:80)
−
K k=0 α(1
The generalized form of Equation (4) allows us to simulate more complex polynomial ﬁltering operation. For example, let α = 0.5 and γ(L) = etL
I, a heat kernel with t as the temperature parameter. Then f (z) takes the minimum when the derivative ∂f (z) x = 0, which solves to
∂z = z + z etL
−
−
−
I z∗ = e− tLx = e− t(I
−
P)x =
∞ e− (cid:0)
Pkx. t tk k! (cid:1) (cid:88)k=0
K
By taking a sufﬁx sum k=0 e− kernal based GNN such as GDC [15] and GraphHeat [34]. t tk k! Pkx, we obtain the polynomial ﬁltering operation for the heat (cid:80) 3.2 Non-negative constraint on polynomial ﬁlters
A natural question is that, does an arbitrary energy function γ(L) correspond to a valid or ill-posed
K k=0 wkLkx correspond to the spectral ﬁlter? Conversely, does any polynomial ﬁltering operation optimal solution of the optimization function (4) for some energy function γ(L)?
As it turns out, there is a “minimum requirement” for the energy function γ(L); γ(L) has to be positive semideﬁnite.
In particular, if γ(L) is not positive semideﬁnite, then the optimization (cid:80) 5
function f (z) is not convex, and the solution to ∂f (z)
Furthermore, without the positive semideﬁnite constraint on γ(L), f (z) may goes to to be a multiple of the eigenvector corresponding to the negative eigenvalue.
∂z = 0 may corresponds to a saddle point. as we set z
−∞
≤ 0 = 1 for λ
α)
·
α+(1
∈
K k=0 wkλk also satisﬁes 0
−
Non-negative polynomial ﬁlters. Given a positive semideﬁnite energy function γ(L), we now
K k=0 wkLkx should look like. consider how the corresponding polynomial ﬁltering operation
Recall that we assume γ(L) = Udiag[γ(λ1), ..., γ(λn)]UT . By the positive semideﬁnite constraint, we have γ(λ)
[0, 2]. Since the objective function f (z) is convex, it takes the minimum when ∂f (z) x) = 0. Accordingly, the optimum z∗ can be derived as
α)γ(L)z + 2α (z 0 for λ (cid:80)
≥
∈
∂z = 2(1
−
α (αI + (1
α)γ(L))−
−
α
Let h(λ) =
α)γ(λ) denote the exact spectral ﬁlter, and g(λ) = mial approximation of h(λ) (e.g. the sufﬁx sum of h(λ)’s taylor expansion). Since γ(λ)
λ
α + (1 (cid:21)
−
K k=0 wkλk denote a polyno-0 when
[0, 2]. Consequently, it is natural to assume
[0, 2], we have 0
α)γ(λn)
α)γ(λ1)
α + (1 h(λ)
α+(1 (cid:80)
−
≥ (cid:20)
−
α
α
, ...,
α
UT x. (5)
− 1 x = Udiag
∈
≤ the polynomial ﬁlter g(λ) =
Constraint 3.1. Assuming the energy function γ(L) is positive semideﬁnite, a polynomial ﬁlter
K k=0 wkλk approximating the optimal solution to Equation (4) has to satisfy g(λ) = g(λ) (cid:80)
≤
≤ 1. (cid:80) g(λ) = 0
≤
K k=0 wkλk 1,
λ
∀
∈
≤
[0, 2]. (6) (cid:88)
While Constraint 3.1 seems to be simple and intuitive, some of the existing GNN may not satisﬁes this
L) x, which corresponds to a polynomial ﬁlter constraint. For example, GCN uses z = Px = (I
λ that takes negative value when λ > 1, violating Constraint 3.1. As shown in [31], the g(λ) = 1 1/2 shrinks the spectral and thus reliefs 1/2 (I + A) (I + D)− renormalization trick ˜P = (I + D)− the problem. However, g(λ) may still take negative value as the maximum eigenvalue of ˜L = I
˜P is still larger than 1.
−
−
− 3.3 Non-negative polynomials and Bernstein basis
Constraint 3.1 motivates us to design polynomial ﬁlters g(λ) = when λ
∈
K wk| k=0 | set wk ≥ (cid:80) low-pass ﬁlters. 1 1 part is trivial, as we can always rescale each wk by a factor of
≤ (cid:80) 0 part, however, requires more elaboration. Note that we can not simply 0 for each k = 0 . . . , K, since it is shown in [5] that such polynomials only correspond to
[0, 2]. The g(λ) 2k. The g(λ)
K k=0 wkλk such that 0 g(λ)
≤
≤
≥
As it turns out, the Bernstein basis has the following nice property: a polynomial that is non-negative on a certain interval can always be expressed as a non-negative linear combination of Bernstein basis.
Speciﬁcally, we have the following lemma.
Lemma 3.1 ([23]). Assume a polynomial p(x) = there exists a sequence of non-negative coefﬁcients θk, k = 0, . . . , K, such that
K k=0 θkxk satisﬁes p(x)
≥ 0 for x
[0, 1]. Then
∈
K (cid:80)
K p(x) =
θkbK k (x) =
θk
K k (1
− x)K kxk
− (cid:88)k=0
Lemma 3.1 suggests that to approximate a valid ﬁlter, the polynomial ﬁlter g(λ) has to be a non-negative linear combination of Bernstein basis. Speciﬁcally, by setting x = λ/2, the ﬁlter g(λ) that satisﬁes g(λ)
[0, 2] can be expressed as 0 for λ (cid:88)k=0 (cid:18) (cid:19)
≥
∈ g(λ) := p
λ 2 (cid:18) (cid:19)
=
θk 1 2K
K k (cid:18) (2 (cid:19)
−
λ)K kλk.
−
K (cid:88)k=0
Consequently, any valid polynomial ﬁlter that approximate the optimal solution of (4) with positive semideﬁnite energy function γ(L) has to take the following form: z =
L)K any valid polynomial ﬁlers, i.e., the g : [0, 2] accordingly, the ﬁlters learned by our BernNet are always valid.
− kLkx. This observation motivates our BernNet from the perspective of graph optimization —
[0, 1], can always be expressed by BernNet, and
K k=0 θk 1 2K (2I (cid:80)
K k (cid:55)→ (cid:0) (cid:1)
− 6
(a) Original (b) Low-pass (c) High-pass (d) Band-pass (e) Band-rejection (f) Comb
Figure 3: A input image and the ﬁltering results.
Table 2: Average sum of squared error and R2 score in parentheses.
Low-pass exp(−10λ2)
High-pass 1 − exp(−10λ2)
Band-pass exp(−10(λ − 1)2)
Band-rejection 1 − exp(−10(λ − 1)2)
Comb
| sin(πλ)| 3.4799(.9872)
GCN
GAT 2.3574(.9905)
GPR-GNN 0.4169(.9984) 1.8478(.9932)
ARMA 0.8220(.9973)
ChebNet 0.0314(.9999)
BernNet 67.6635(.2364) 21.9618(.7529) 0.0943(.9986) 1.8632(.9793) 0.7867(.9903) 0.0113(.9999) 25.8755(.1148) 14.4326(.4823) 3.5121(.8551) 7.6922(.7098) 2.2722(.9104) 0.0411(.9984) 21.0747(.9438) 12.6384(.9652) 3.7917(.9905) 8.2732(.9782) 2.5296(.9934) 0.9313(.9973) 50.5120(.2977) 23.1813(.6957) 4.6549(.9311) 15.1214(.7975) 4.0735(.9447) 0.9982(.9868) 4