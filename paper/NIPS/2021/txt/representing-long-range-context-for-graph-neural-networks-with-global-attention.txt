Abstract
Graph neural networks are powerful architectures for structured datasets. How-ever, current methods struggle to represent long-range dependencies. Scaling the depth or width of GNNs is insufﬁcient to broaden receptive ﬁelds as larger
GNNs encounter optimization instabilities such as vanishing gradients and rep-resentation oversmoothing, while pooling-based approaches have yet to become as universally useful as in computer vision. In this work, we propose the use of
Transformer-based self-attention to learn long-range pairwise relationships, with a novel “readout” mechanism to obtain a global graph embedding. Inspired by recent computer vision results that ﬁnd position-invariant attention performant in learning long-range relationships, our method, which we call GraphTrans, applies a permutation-invariant Transformer module after a standard GNN module. This simple architecture leads to state-of-the-art results on several graph classiﬁcation tasks, outperforming methods that explicitly encode graph structure. Our results suggest that purely-learning-based approaches without graph structure may be suitable for learning high-level, long-range relationships on graphs. Code for
GraphTrans is available at https://github.com/ucbrise/graphtrans. 1

Introduction
Graph neural networks (GNNs) enable deep networks to process structured inputs such as molecules or social networks. GNNs learn mappings that compute representations at graph nodes and/or edges from the structure of and features in their neighborhoods. This neighborhood-local aggregation leverages the relational inductive bias encoded by the graph’s connectivity [3]. Similar to convolutional neural networks (CNNs), GNNs can aggregate information from beyond local neighborhoods by stacking layers, effectively broadening the GNN receptive ﬁeld.
However, GNN performance drops dramatically when its depth increases [21]. This limitation has hurt the performance of GNNs on whole-graph classiﬁcation and regression tasks, where we want to predict a target value describing the whole graph that may rely on long-range dependencies that may not be captured by a GNN with a limited receptive ﬁeld [35]. Consider for example a large graph where node A must attend to a distant node B which is K-hops away. If our GNN layer aggregates only over a node’s one-hop neighborhood, then a K-layer GNN is required. However, the width of the receptive ﬁeld of this GNN will grow exponentially, diluting the signal from node B. That is, simply expanding the receptive ﬁeld to a K-hop neighborhood may not capture these long-range dependencies either [40]. Often, “too deep” GNNs lead to node representations that collapse to be equivalent over the entire graph, a phenomenon sometimes called oversmoothing or oversquashing
[21, 5, 2]. Therefore, the maximum context size for common GNN architectures is effectively limited. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Architecture of GraphTrans. A standard GNN submodule learns local, short-range structure, then a global Transformer submodule learns global, long-range relationships.
Several proposed methods combat the oversmoothing problem via intermediate pooling operations similar to those found in today’s CNNs. Graph pooling operations gradually coarsen the graph in progressive GNN layers, usually by collapsing neighborhoods into single nodes [9, 37, 20, etc.].
In theory, hierarchical coarsening should allow better long-range learning, both by reducing the distance information has to travel and by ﬁltering out unimportant nodes. However, no graph pooling operation has been found that is as universally applicable as CNN pooling. State-of-the-art results are often obtained with models using no intermediate graph coarsening [27], and some results suggest neighborhood-local coarsening may be unnecessary or counterproductive [23].
In this work, we take a different approach at graph pooling and learning long-range dependencies in
GNNs. Like hierarchical pooling, our method is also inspired by methods for computer vision: we replace some of the atomic operations that explicitly encode relevant relational inductive biases (i.e., convolutions or spatial pooling in CNNs, neighborhood coarsening in GNNs) with purely learned operations like attention [11, 4, 7].
Our method, which we call Graph Transformer (GraphTrans, see Fig. 1), adds a Transformer subnetwork on top of a standard GNN layer stack. This Transformer subnetwork explicitly computes all pairwise node interactions in a position-agnostic fashion. This approach is intuitive as it retains the
GNN as a specialized architecture to learn local representations of the structure of a node’s immediate neighborhood while leveraging the Transformer as a powerful global reasoning module. This parallels recent computer vision architectures, where authors have found hard relational inductive biases important for learning short-range patterns but less useful or even counterproductive in modeling long-range dependencies [25]. As the Transformer without a positional encoding is permutation-invariant, we ﬁnd it is a natural ﬁt for graphs. Moreover, GraphTrans does not require any specialized modules or architectures and can be implemented in any framework atop any existing GNN backbone.
We evaluate GraphTrans on a variety of popular graph classiﬁcation datasets. We ﬁnd signiﬁcant improvements in accuracy on OpenGraphBenchmark [15] where we achieve state-of-the-art results on two graph classiﬁcation tasks. Moreover, we ﬁnd substantial improvements on the molecular dataset NCI1. Surprisingly, we ﬁnd our simple model outperforms complex baselines for long-range modeling in graphs via hierarchical clustering such as self-attention pooling [20].
Our contributions are as follows:
• We show that long-range reasoning via Transformers improve graph neural network (GNN) ac-curacy. Our results suggest that modelling all pairwise node-node interactions in the graph is particularly important for large graph classiﬁcation tasks.
• We introduce a novel GNN “readout module.” Inspired by text-classiﬁcation applications of
Transformers, we use a special “<CLS>” token whose output embedding aggregates all pairwise interactions into a single classiﬁcation vector. We ﬁnd that this approach outperforms both non-learned readout methods like global pooling as well as learned aggregation methods like graph-speciﬁc pooling methods [37, 20] and “virtual node” approaches.
• Using our novel architecture GraphTrans, we obtain state-of-the-art results on several OpenGraph-Benchmark [15] datasets and the NCI biomolecular datasets [30]. 2
2