Abstract
Medical report generation, which aims to automatically generate a long and coher-ent report of a given medical image, has been receiving growing research interests.
Existing approaches mainly adopt a supervised manner and heavily rely on cou-pled image-report pairs. However, in the medical domain, building a large-scale image-report paired dataset is both time-consuming and expensive. To relax the dependency on paired data, we propose an unsupervised model Knowledge Graph
Auto-Encoder (KGAE) which accepts independent sets of images and reports in training. KGAE consists of a pre-constructed knowledge graph, a knowledge-driven encoder and a knowledge-driven decoder. The knowledge graph works as the shared latent space to bridge the visual and textual domains; The knowledge-driven encoder projects medical images and reports to the corresponding coordinates in this latent space and the knowledge-driven decoder generates a medical report given a coordinate in this space. Since the knowledge-driven encoder and decoder can be trained with independent sets of images and reports, KGAE is unsupervised.
The experiments show that the unsupervised KGAE generates desirable medical reports without using any image-report training pairs. Moreover, KGAE can also work in both semi-supervised and supervised settings, and accept paired images and reports in training. By further ﬁne-tuning with image-report pairs, KGAE consistently outperforms the current state-of-the-art models on two datasets. 1

Introduction
Medical images, such as radiology and pathology images, and their corresponding reports are widely used for clinical diagnosis and treatment [8, 11]. A medical report is usually a paragraph of multiple sentences which describes both the normal and abnormal ﬁndings in the medical image. In clinical practice, writing a report can be time-consuming and tedious for experienced radiologists, and error-prone for inexperienced radiologists [4]. Therefore, given the large volume of medical images, automatically generating reports can improve current clinical practice in diagnostic radiology and assist radiologists in clinical decision-making [15, 25]. Speciﬁcally, it can relieve radiologists from such heavy workload and alert radiologists of the abnormalities to avoid misdiagnosis and missed diagnosis. Therefore, automatic medical report generation attracts remarkable attention in both artiﬁcial intelligence and clinical medicine.
Recently, inspired by the great success of neural machine translation [2, 38, 49, 50, 48], image captioning [44, 39, 46, 28, 30, 29] and medical imaging analysis [47, 51, 52], the data-driven deep
∗Corresponding authors. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of our Knowledge Graph Auto-Encoder, which consists of a pre-constructed knowledge graph, a knowledge-driven encoder and a knowledge-driven decoder. The Green and Red lines denote the data ﬂow in the training process and testing process of report generation, respectively. neural models, particularly those based on the encoder-decoder frameworks [15, 16, 25, 22, 41, 45, 53, 54, 6], have achieved great success in advancing the state-of-the-art of medical report generation.
However, these models are trained in a supervised learning manner and heavily rely on labeled paired image-report datasets [9, 17], which are not easy to acquire in the real world. Speciﬁcally, the medical-related data can only be manually labeled by professional radiologists, and also involves privacy issues. Therefore, the medical report generation datasets are particularly labor-intensive and expensive to obtain. As a result, the scales of existing widely-used datasets for medical report generation models [16, 25, 22, 23], i.e., MIMIC-CXR (0.22M samples) [17] and IU X-ray (4K samples) [9], are relatively small compared to image recognition datasets, e.g., ImageNet (14M samples) [10], and image captioning datasets, e.g., Conceptual Captions (3.3M samples) [36]. In addition, the MIMIC-CXR and IU X-Ray datasets only include Chest X-Ray images, for other types of medical images (MRI, Dermoscopy, Retinal, etc.) of other body parts (brain, skin, eye, etc.), the image-report pairs could be much less or even unavailable. Therefore, to relax the reliance on the paired data sets, making use of all available data, like independent image or report sets, is becoming increasingly important.
In this paper, we propose an unsupervised model Knowledge Graph Auto-Encoder (KGAE), which utilizes independent sets of images and reports in training (the image and report set are separate and have no overlap). KGAE consists of a pre-constructed knowledge graph, a knowledge-driven encoder and a knowledge-driven decoder. As shown in Figure 1, the knowledge graph works as the shared latent space of images and reports. The knowledge-driven encoder can take either the image I or the report R as queries and project them to corresponding coordinates GI and GR in the latent space. In this manner, since GI and GR share the same latent space, we can use positions in latent space to measure the relationship between images and reports which narrows the gap between visual and textual domains. In brief, to bridge the gap between vision and language domains without training on the pairs of images and reports, we adopt the knowledge graph to create a latent space and propose a knowledge-driven encoder, which includes a common mapping function to project images and reports to the same latent space. As a result, our encoder can extract the image and report knowledge representations, i.e., the knowledge related to the image and report, they (image, report knowledge) share the common latent space, which allows our model to bridge the gap between vision and language domains without the training on the pairs of image and report. Next, we introduce the knowledge-driven decoder to exploit GI and GR to generate the report. In the training stage, we estimate the parameters of the decoder by reconstructing the input report R based on GR, i.e.,
R → GR → R auto-encoding pipeline; In the prediction stage, we directly input GI into the trained decoder to generate the report. In this way, our approach can produce desirable reports without any labeled image-report pairs.
Overall, the contributions of this paper are as follows:
• In this paper, we make the ﬁrst attempt to conduct unsupervised medical report generation where the image-report pairs are not available. To this end, we propose the Knowledge Graph
Auto-Encoder (KGAE). By leveraging a pre-constructed knowledge graph, we introduce the knowledge-driven encoder and decoder which are trained with independent sets of 2
images and reports. According to the experimental results, the unsupervised KGAE can even outperform several supervised approaches.
• In addition to the unsupervised mode, KGAE can also be applied in a semi-supervised or supervised manner. Under the semi-supervised setting, by using only 60% of paired dataset,
KGAE is able to achieve competitive results with current state-of-art models; Under the supervised setting, by training on fully paired datasets as in existing works, KGAE can set new state-of-the-art performances on the IU X-ray and MIMIC-CXR, respectively.
• The analysis, both quantitative and qualitative, as well as a human evaluation conducted by professional radiologists, further proves the effectiveness of our approach. 2