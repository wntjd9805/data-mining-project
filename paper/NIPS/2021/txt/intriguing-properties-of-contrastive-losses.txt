Abstract
We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we ﬁnd that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We ﬁnd that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as “color distribution” vs “object class”. We construct datasets with explicit and controllable competing features and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques 1. 1

Introduction
Contrastive learning [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] has recently achieved great successes in learning visual representations without supervision. As shown in [13, 14], contrastive learning can learn representations that rival supervised learning, and signiﬁcantly improve the state-of-the-art in semi-supervised learning on ImageNet. One successful use case of contrastive loss for self-supervised learning is to make augmented views of the same example agree [1, 2, 13]. A widely used contrastive loss to encourage agreement is based on cross entropy [15, 3, 4, 13]. Given an augmented view of an example, the contrastive prediction task aims to classify a set of candidates into the positive example (i.e. the other augmented view of the same example) and negative ones via the cross entropy loss.
In this work, to understand the effectiveness and limitation of existing contrastive learning methods, we study three intriguing aspects. First, we propose a generalization of the standard contrastive loss, and systematically study their performance differences. Second, we study if the instance-based contrastive learning, for which the contrastive loss operates on global representation of an input image, can learn well on images with multiple objects present, and whether or not it leads to meaningful local features. Finally, we systematically study the feature suppression phenomenon in contrastive learning. The suppression effect occurs among competing features shared across augmented views.
For example, with random cropping as the augmentation, “color distribution” and “object class” are often competing features as they are likely shared between two augmented views. The suppression effect among competing features can signiﬁcantly degenerate the representation quality, or even 1Code and visualization at https://contrastive-learning.github.io/intriguing. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
completely disable the learning of certain features, as shown in our experiments. Existing methods critically rely on hand-crafted data augmentation to favor certain sets of competing features than others.
Our main ﬁndings and contributions are summarized below.
• We propose a generalized contrastive loss, and show that differences between contrastive losses are small with a deep projection head.
• We show that the instance-based objective widely used in existing contrastive learning methods can learn on images with multiple objects, and also learn meaningful local features despite operating on global image representation.
• We construct three datasets with explicit and controllable competing features to systematically study the feature suppression effect in contrastive learning.
• We show that a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. This poses open challenges to existing contrastive learning. 2 Generalized contrastive loss and differences among its instantiations
The common contrastive loss used in most recent work is based on cross entropy [15, 3, 4]. Following the notation in [13], the contrastive loss can be deﬁned between two augmented views (i, j) of the same example for a mini-batch of size of n, and can be written as the following.
LNT-Xent = − 1 n (cid:88) log i,j∈MB (cid:80)2n k=1 exp(sim(zi, zj)/τ ) 1[k(cid:54)=i] exp(sim(zi, zk)/τ ) (1) where zi, zj are hidden representations of two augmented views of the same example; sim(u, v) = uT v/((cid:107)u(cid:107)(cid:107)v(cid:107)) is the cosine similarity between two vectors; τ is a temperature scalar and MB is a randomly sampled mini-batch consisting of augmented pairs of images. In [13], a MLP projection head is introduced between intermediate layer h (e.g. output of ResNet encoder) and ﬁnal output z.
It is shown that the projection head is very beneﬁcial and h is a much better feature representation than z.
In this work, we generalize the standard contrastive loss to the following form.
Lgeneralized contrastive = Lalignment + λLdistribution (2)
Both terms are deﬁned on hidden representations. Lalignment encourages representations of augmented views to be consistent, while Ldistribution encourages representations (or a random subset of them) to match a prior distribution (of high entropy). It is not difﬁcult to see that the standard contrastive loss in Eq. 1 is a special case as it can be re-written as follows (scaled by a constant τ ).
τ LNT-Xent = − 1 n (cid:88) i,j sim(zi, zj)
+ (cid:124) (cid:123)(cid:122)
Lalignment (cid:125) (cid:88) log i 2n (cid:88) k=1
τ n (cid:124) 1[k(cid:54)=i] exp(sim(zi, zk)/τ ) (cid:123)(cid:122)
Ldistribution (cid:125) (3)
This form of factorization in Eq. 3 has been proposed in [16], where the second LogSumExp term is referred to as uniformity since it encourages representation to uniformly distributed in the hypersphere.
Different from [16], here we generalize the hypersphere uniform distribution and study a wider set of prior distributions for their effectiveness in learning representations.
SWD for supporting diverse prior distributions. One issue of using more diverse set of priors is we cannot rely on LogSumExp for matching the distribution. To this end, we resort to the theory of optimal transport, via Sliced Wasserstein Distance (SWD) [17, 18, 19]. For two sets of equal-sized samples from two 1-D distributions, the optimal transport can be obtained by computing two 2
Algorithm 1 Sliced Wasserstein Distance (SWD) loss. input: activation vectors H ∈ Rb×d, a prior distribution (e.g. Gaussian) sampler S draw prior vectors P ∈ Rb×d using S generate random orthogonal matrix W ∈ Rd×d(cid:48) make projections: H ⊥ = HW ; P ⊥ = P W initialize SWD loss (cid:96) = 0 for j ∈ {1, 2, · · · , d(cid:48)} do (cid:96) = (cid:96) + (cid:107)sort(H ⊥
:,j) − sort(P ⊥
:,j)(cid:107)2 end for return (cid:96)/(dd(cid:48))
Table 1: Instantiations of the generalized contrastive loss, i.e. Lalignment + λLdistribution, that we use in this work. ˜z denotes (cid:96)2-normalized z ∈ Rd, and is only used for uniform hypersphere prior.
Lalign
Prior distribution i,j (cid:107) ˜zi − ˜zj(cid:107)2 Uniform hypersphere i,j (cid:107) ˜zi − ˜zj(cid:107)2 Uniform hypersphere i,j (cid:107)zi − zj(cid:107)2
Uniform hypercube i,j (cid:107)zi − zj(cid:107)2
Normal distribution (cid:80) (cid:80) (cid:80) (cid:80) 1 nd 1 nd 1 nd 1 nd (cid:80) 1 n
Ldistribution j exp( ˜zi i log (cid:80)
SWD( ˜Z, Z prior)
T ˜zj/τ )
SWD(Z, Z prior)
SWD(Z, Z prior) permutations that order the values of both sets of samples respectively. The 1-D Wasserstein distance can then be computed with (cid:96)2 distance between the ordered values. For n-D distributions, we ﬁrst project the samples to n randomly-generated orthogonal 1-D subspaces, and then compute the sum of 1-D Wasserstein distance across all 1-D subspaces. By adjusting the network weights to minimize the
SWD, we are able to reduce the mismatch between the distribution of hidden vectors and a known prior distribution. The detailed algorithm can be found in Algorithm 1. With SWD loss, we are able to use a wider set of priors, and Table 1 summarizes instantiations of the generalized contrastive loss with different prior distributions and distribution matching loss.
Connection with mutual information. The connection between the standard contrastive loss and mutual information has been shown before [3, 20], where the contrastive loss (a.k.a. InfoNCE loss [3]) is shown to be a lower bound of the mutual information. To connect the generalized contrastive loss to mutual information, we start by the deﬁnition of mutual information between two latent variables
U, V , which is I(U ; V ) = H(U ) − H(U |V ). Comparing this factorization of mutual information with generalized contrastive loss, it is not difﬁcult to see that: 1) the alignment term Lalignment is directly related to H(U |V ) which aims to reduce uncertainty of the other views given one view of the example; and 2) the distribution matching term Ldistribution can be considered as a proxy to
H(u) for maximizing the entropy in the representation. It is perhaps worth noting that different from mutual information, the generalized contrastive loss (Eq. 2) allows a tunable weight (λ) between the alignment and distribution matching term. The weighting scalar λ is (inversely) related to the temperature τ (details in Appendix A.2).
Comparing different instantiations of generalized contrastive loss. Here we ask: Is it essential to use a uniform hypersphere prior for the effectiveness of contrastive loss? How much difference does it make when distinct generalized contrastive losses are used? To answer this question, we conduct experiments following SimCLR settings [13, 14], and use the linear evaluation protocol.
Detailed experimental setup can be found in Appendix A.1.
Figure 1 shows linear evaluation results of models trained with different losses under different training epochs. On CIFAR-10, we see little difference in terms of linear evaluation for variants of the generalized contrastive losses, especially when trained longer than 200 epochs. As for ImageNet, there are some discrepancies between different losses, but they disappear when a deeper 3-layer non-linear projection head is used. 3
(a) CIFAR-10 (2 layers) (b) ImageNet (2 layers) (c) ImageNet (3 layers)
Figure 1: Linear evaluation accuracy of ResNet-50 trained with different losses on CIFAR-10 and
ImageNet datasets. Numbers of projection head layers are in parentheses. Differences between variants of generalized contrastive loss are small with a deep projection head. Decoupled NT-Xent loss is introduced in A.2. Numerical results can be found in Appendix A.3.
Furthermore, we ﬁnd that deep pro-jection head not only reduces the differences among different gener-alized contrastive losses, but has a similar effect for batch size. With proper learning rate scaling across batch sizes (e.g. square root scal-ing with LARS optimizer [21]), the impact of batch size on representa-tion quality is small. Table 2 demon-strate this phenomenon for the stan-dard contrastive loss, and more re-sults on other losses can be found in
Appendix A.3.
Table 2: Linear eval accuracy of ResNet-50 on ImageNet.
Projection head Batch size 2 layers 3 layers 4 layers 512 1024 2048 512 1024 2048 512 1024 2048
Epoch 100 65.4 65.6 65.3 66.6 66.8 66.8 66.8 67.0 67.0 200 67.3 67.6 67.6 68.4 68.9 69.1 68.8 69.0 69.3 400 68.7 68.8 69.0 70.0 70.1 70.4 70.0 70.4 70.4 800 69.3 69.8 70.1 71.0 70.9 71.3 70.7 70.9 71.3 3
Instance-based objective can learn on images with multiple objects and learn good local features
Most existing contrastive learning methods [13, 10, 22, 4] deﬁne their objectives at the instance level where each image is encoded into a single vector representation (e.g. representations of two random crops of the same image instance are treated as a positive pair). In other words, the objective operates on a global representation of its input rather than on some local regions (of its input). We pose two questions regarding instance-based global objective: 1) when there is only a single (dominant) object in the image, the objective seems reasonable as it encourages the model to learn features relevant to object class, but when there are multiple objects present in the image, can instance-based objective still learn well? 2) Since the instance-based objective uses a global summary of its input, can it still learn good local features (e.g. parts of an object, or multiple objects in the same scheme)? To answer these questions, we use SimCLR as representative for the instance-based objective. 3.1 SimCLR can learn on images with multiple objects
Commonly used self-supervised learning datasets, such as MNIST, CIFAR-10, ImageNet, are object centered, i.e. the image is mainly occupied by a single (dominant) object. To experiment with multiple objects in a controllable setting, we propose a new dataset setting by composing multiple digits as follows.
MultiDigits dataset. We place MNIST digits (28 × 28 size) on a shared canvas (112 × 112 size).
We vary the number of digits placed on the canvas. One factor that could interfere with learning of multiple digits is overlapping digits, therefore we use two placement strategies: random vs in-grid (Figure 2). Random placement of digits incurs no constraint on where digits can be placed on the 4
(a) 4 digits, random placement. (b) 16 digits, random placement. (c) 4 digits, in-grid placement. (d) 16 digits, in-grid placement.
Figure 2: MultiDigit dataset. More digits lead to more overlapping in random placement. canvas, whereas in-grid placement puts each digit in one of the 4 × 4 grid cells the canvas is divided into, and no two digits can fall in the same cell. In-grid placement ensures no overlapping of digits.
We ﬁrst pretrain a ResNet-18 with SimCLR or supervised learning with the same augmentation policy (random cropping and resize) on MultiDigits dataset. To access the representation quality, we then train linear classiﬁers for images with a single digit of size 28 × 28 on the canvas. Similarly during evaluation, we place only one digit of size 28 × 28 on the canvas.
As shown in Table 3, representations learned using supervised loss maintains its quality when up to 8 digits are placed in the image. After that the representation becomes worse as the canvas gets more crowded. Notably, representations learned using SimCLR display a similar phenomenon. Regardless of placement strategy, top-1 accuracy stays at the same level up to 8 digits, demonstrating that
SimCLR can learn from images with multiple objects. In addition, the increased performance gap between the two placement strategies with increased number of digits shows that object overlapping makes it harder for contrastive losses to learn from multiple objects.
Table 3: Top-1 linear evaluation accuracy (%) for pretrained ResNet-18 on the MultiDigits dataset.
We vary the number of digits placed on the canvas during training from 1 to 16. During evaluation only 1 digit is present. As a baseline, a network with random weights gives 18% top-1 accuracy.
Placing of digits
Number of digits (size 28 × 28)
Supervised
SimCLR
Random
In-grid
Random
In-grid 1 99.5 99.5 98.9 98.3 2 99.5 99.6 98.9 98.6 4 99.3 99.5 99.0 99.1 8 99.4 99.3 98.9 99.2 12 98.9 98.6 98.2 99.1 16 98.3 92.4 96.4 98.3 3.2 SimCLR learns local features that exhibit hierarchical properties
To understand the local features learned by SimCLR, we apply K-means on intermediate features of the pretrained ResNet with SimCLR, and see how local regions of an image are grouped together.
For good representations, we expect that regions of similar objects or object parts should be grouped together.
Speciﬁcally, we take a pretrained Resnet-50 2× on ImageNet, and run inference on images (from
ImageNet validation set and COCO [23]) of size 448×448. We run K-means with various numbers of clusters on the l2-normalized hidden features from middle layers of the network (e.g. block group 2,3,4 of the ResNet). We also compare SimCLR learned features with supervised learned features, as well as the raw pixel (RGB) features extracted from each 14 × 14 patch.
Figure 3a shows that as the number of clusters increases, the learned representations tend to group image regions based on parts of the object (i.e. facial components of the dog). This phenomenon appears in both SimCLR and supervised learned features, but not with raw pixel features, indicating meaningful local features learned by SimCLR and supervised learning. In Figure 3b, we compare
ResNet intermediate features at different layers, and it suggests that earlier layers contain more edge-related features, while later layers contain more object/part features. 5
(a) Features from different methods. (b) SimCLR features at different ResNet layers.
Figure 3: Visualizing features on a ImageNet validation image with K-means clustering. Each row denotes a type of local features used, and each column denotes the number of K-means clusters. Later layers of SimCLR/supervised ResNet tend to group by object parts. More visualization examples can be found in https://contrastive-learning.github.io/intriguing. (a) (b)
Figure 4: Visualizing features on two images from COCO. Each row denotes a type of local features (SimCLR, Supervised, and raw pixels; both SimCLR and Supervised are trained on ImageNet), and each column denotes the number of K-means clusters. Region grouping by SimCLR/supervised features tend to overlap with object class.
Region grouping results on two COCO images for SimCLR and supervised learning (trained on
ImageNet) are shown in Figure 4. Again, region grouping by local features tend to overlap with object class, indicating good local features learned. 4 Feature suppression limits the potential of contrastive learning
Contrastive learning requires good design of data augmentation to work well. As shown in [13], without color augmentations that randomly shift color distribution (while maintaining information regarding object class), the quality of learned representations are signiﬁcantly worse. In other words, the presence of “color distribution” features suppresses their competing feature of “object class”, and is addressed by color augmentation. However, there may be scenarios where the known augmentations cannot fully address this feature suppression effect, and it can thus limit the potential of contrastive learning. Here we quantitatively study the feature suppression phenomenon by constructing datasets with explicit and controllable competing features, and see how well contrastive learning method could learn. 6
(a) ImageNet images overlaid with MNIST digits. The left most column is original image, and others are augmented views via random crop and color distortion. MNIST digits and ImageNet classes are competing features. We vary the number of unique MNIST digits to control the competing features. (b) Two MNIST digits randomly placed on a shared canvas (of size 112 × 112). The two digits can have the same size (upper row) or different sizes (lower row), and digits of different sizes can be considered as competing features. We ﬁx the size of one digit and vary the other. (c) Images (of RGB channels) are concatenated with additional channels of random integer sampled from range of [1, log2(n)]. The integer, shared between two views, is replicated for spatial dimension and represented as n binary channels. RGB channels and random bits are competing features.
Figure 5: Probing datasets with explicit and controllable competing features. 4.1 Datasets with explicit and controllable competing features
To construct datasets with controllable competing features, we leverage two strategies: channel addition that adds different feature information in a shared canvas, and channel concatenation that expand the RGB channels to include additional features. With these strategies, we construct three datasets below.
DigitOnImageNet dataset. We overlay MNIST digits on ImageNet images via channel addi-tion/summation (Figure 5a). For each ImageNet image, we assign a unique MNIST digit and replicate it in nine ﬁxed locations before the standard SimCLR augmentations [13] are applied to create augmented views. Therefore the original ImageNet images and added MNIST digits are competing features. Although it is difﬁcult to quantify information in MNIST digits, we can manually control the number of unique MNIST digits used. Ideally, we want the model to learn both set of features so that it could perform well for both MNIST digit and ImageNet object recognition.
MultiDigits dataset (varying the size of one digit). This dataset is modiﬁed from MultiDigits introduced above. Here we only consider two digits, and vary the size of one of them (Figure 5b). In this work, we place two digits on a canvas of size 112 × 112. We ﬁx the size of one of the digits to be 20 × 20 while varying the other from 20 × 20 to 80 × 80. Digits of different sizes can be considered as competing features. Ideally, we want the model to learn features for digits of all sizes appeared during training.
RandBit dataset. We concatenate a real image with an image of a random integer in the channel dimension (Figure 5c). The random integer is randomly sampled from range of [1, log2(n)] where n is a parameter to control. It is replicated across spatial dimension (i.e. all pixel location shares the same value), and it is also represented as n binary bits/channels instead of an integer or ﬂoating 7
number to make it easily learnable. Furthermore, unlike RGB channels, these additional channels of random bits will not be altered by augmentation, so they are identical for both augmented views of the same image. The RGB channels and the added channels of random bits are competing features, and this construction allows us to control the amount of information in the added competing feature, which is n bits. Also, we know that the mutual information between two views given this construction is at least log2(n). 4.2 Easy-to-learn features (MNIST digit) suppress the learning of other features (ImageNet object class) (a) Supervised learning (b) Unsupervised contrastive learning (a) Supervised learning accuracy on ImageNet classiﬁcation. (b) Linear evaluation of
Figure 6: learned features for both MNIST classiﬁcation and ImageNet classiﬁcation on the DigitOnImageNet dataset. Batch size of 1024 and 2-layer projection head is used. Different batch sizes and projection head layers have negligible inﬂuence on the trade-off between ImageNet vs MNIST accuracy.
On DigitOnImageNet datasets, we vary the number of unique MNIST digits used in the training set, and all MNIST digits are used in the validation/test set. As a baseline, we train supervised ResNet-50 on the created datasets with ImageNet labels, and the number of unique MNIST digits has little impact on the top-1 ImageNet classiﬁcation accuracy (Figure 6a).
We then train SimCLR on the datasets with different temperatures. As shown in Figure 6b, when we increase the number of unique MNIST digits, the linear evaluation performance of the learned features for MNIST classes increases accordingly, while the accuracy for ImageNet classes decreases dramatically. The trade-off between digit recognition ability and object recognition ability shows that simple features suppress the learning of difﬁcult features, when both are shared between two augmented views. Different batch sizes and projection head depths have negligible inﬂuence to the outcome we observe here. Therefore, it is difﬁcult to learn both of the competing features using existing contrastive losses (e.g. SimCLR). 4.3 The presence of dominant object suppresses the learning of features of smaller objects
On the MultiDigits dataset, as mentioned, we ﬁx one digit to be size of 20 × 20 while varying the other from 20 × 20 to 80 × 80, on a canvas of 112 × 112. We ﬁrst pretrain a ResNet-18 with SimCLR or supervised learning with the same augmentation policy (random cropping and resize) and batch size of 1024. To access the representation quality, we then train linear classiﬁers for each of the digit sizes that appeared during pretraining. For training of the linear classiﬁer, we only place a single digit at a time on the canvas of the same size as during pretraining.
The results are summarized in Table 4. For supervised learning, the learned representations for the smaller digit do not change much as the other digit increases its size, and the model perform well for both small and large digits (accuracy > 99%). However, for SimCLR, the learned representations of the smaller digit degenerate signiﬁcantly when the size of the other digit increases, almost to the level of a random untrained network. The dominant object can be learned very well (accuracy > 99%) while suppressing the learning of the smaller object. Although tuning temperature has some effects on reducing the feature suppression, the trend stays unchanged. 8
Table 4: Top-1 linear evaluation accuracy (%) for pretrained ResNet-18 on the MultiDigits dataset.
We ﬁx the size of 1st digit while increasing the size of the 2nd digit. For SimCLR, results are presented for two temperatures. Accuracies suffered from a signiﬁcant drop when increasing 2nd digit size are red colored.
Supervised
SimCLR (τ = 0.05)
SimCLR (τ = 0.2)
Random net (untrained) 1st digit 2nd digit 1st digit 2nd digit 1st digit 2nd digit 1st digit 2nd digit 20 × 20 99.1 99.1 97.8 97.8 98.7 98.7 16.5 16.5 2nd digit size (1st digit is kept the same size of 20 × 20) 70 × 70 50 × 50 30 × 30 60 × 60 40 × 40 99.2 99.5 97.6 97.9 98.8 99.2 16.7 19.1 99.2 99.5 96.2 97.8 98.3 99.2 16.6 21.9 99.2 99.6 96.5 98.3 87.5 99.0 16.6 24.1 99.1 99.5 88.5 98.2 24.9 99.1 16.6 26.5 99.1 99.5 74.5 97.7 19.8 98.9 16.9 28.1 80 × 80 99.0 99.6 39.9 98.2 20.3 99.4 16.5 29.0 4.4 Extra channels with a few bits of easy-to-learn mutual information suppress the learning of all features in RGB channels
In the RandBit datasets, we add additional channels (identical across pixels) of random bits to MNIST and ImageNet. As mentioned above, SimCLR augmentation is only applied to RGB channels so extra added channels will be shared among two view. (a) Contrastive loss. (b) Generative loss.
Figure 7: Linear evaluation of learned features when a few bits of competing features added (on
MNIST). Adding a few bits completely disables contrastive learning (across various batch size or losses). Interestingly, it has little effects on a generative model (VAE). The detrimental effects are just as strong for larger datasets such as CIFAR-10 and ImageNet (Appendix B.1).
Figure 7 shows the linear evaluation accuracy of models trained on MNIST (with additional random bits added). We observe that the linear evaluation accuracy quickly drops with a few bits of competing feature added. This detrimental effect on the representation quality persists on bigger datasets like
CIFAR-10 and ImageNet as well, and cannot be avoided by using different contrastive losses, batch sizes, or memory mechanism based on momentum contrast (details in Appendix B.1). We believe the fact that just a few bits of easy-to-learn features can completely disable the good representation learning is related to the saturation of the distribution matching loss. As shown in Appendix B.2, the linear increase in bits requires an exponential increase in batch size, which is not sustainable as the required batch size can quickly go beyond the size of the dataset size. In practice, we rely on using data augmentation to remove those uninformative easy-to-learn features so that contrastive learning can learn useful representations. Interestingly, the extra bits do not affect a generative model, variational autoencoder [24, 25], nearly as much, despite other settings such as model size are held the same, prompting a potential direction of addressing the issue. 5