Abstract
The capabilities of natural language models trained on large-scale data have in-creased immensely over the past few years. Open source libraries such as Hugging-Face have made these models easily available and accessible. While prior research has identiﬁed biases in large language models, this paper considers biases contained in the most popular versions of these models when applied ‘out-of-the-box’ for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Speciﬁcally, we conduct an in-depth analysis of GPT-2, which is the most downloaded text generation model on
HuggingFace, with over half a million downloads per month. We assess biases re-lated to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political afﬁliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and ﬁnd: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by ﬁtting 262 logistic models; (iii) For most occupations, GPT-2 reﬂects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn - whether they should reﬂect or correct for existing inequalities. 1

Introduction
The advent of deep learning and massive growth in training data have led to natural language models surpassing humans on numerous benchmarks [1, 22, 39, 40]. However, as Bender et al. [7] states, these models can exacerbate existing biases in data and perpetuate stereotypical associations to the harm of marginalized communities. Simultaneously, pre-trained models have become readily accessible via open source libraries such as HuggingFace, allowing non-experts to apply these tools in their own applications. These developments in generative language models substantiate a need to understand the potential for biases towards protected classes, such as gender and ethnicity.
This paper considers potential biases present in the most popular and most downloaded versions of large-scale, open sourced text generation models applied ‘out-of-the-box’. Despite the release of newer and larger models often redirecting researchers’ attention, there exist important research gaps in existing models. Bearing in mind that the potential negative total impact from biased models is correlated with number of downloads of that model, this paper tests the biases in the small GPT-2 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
model, which is the most downloaded text generation model on HuggingFace with over half a million downloads in the month of May 2021 alone. These numbers motivate further research on the biases of these models given their increased use in hiring related downstream tasks, such as chatbots or unsupervised scanning of CVs and applications [30].
Within this context, specifying which biases to analyze is crucial; Blodgett et al. [9] ﬁnd that a majority of NLP papers investigating bias are unclear in their articulations of bias. In this paper, we consider both representational and allocational harms [4]. We attempt to elucidate representational harms, or those harmful in their own right, by highlighting occupation-related stereotypes that may propagate negative generalizations about particular social groups. For example, women’s higher likelihood of being associated with care-oriented occupations may perpetuate unwanted stereotypes.
Especially within the context of occupations, such associations may lead to allocation harms. Frequent stereotypical association of certain demographic groups with a subset of occupations may lead to conditioned expectations in job hiring where a certain individual is predicted to be well-suited for a job based on their demographics [20].
In this paper, we generate 396K sentence completions using GPT-2 with default parameters to assess which occupations GPT-2 preferentially associates with intersections of gender and protected classes.
We further compare these to real-world occupation data from the US Labor Bureau to map model biases to systemic societal biases. This paper provides the following contributions: (a) a detailed data collection protocol for studying intersectional biases in generative language models; (b) the analysis of biases present in GPT-2 for gender intersected with ethnicity, religion, sexuality, political afﬁliation, and continent name origin; and (c) a comparison of GPT-2’s predictions with ground truth occupation distribution as observed in US labor market data.1 2