Abstract
Unsupervised domain adaptation (UDA) enables cross-domain learning without target domain labels by transferring knowledge from a labeled source domain whose distribution differs from that of the target. However, UDA is not always successful and several accounts of ‘negative transfer’ have been reported in the literature. In this work, we prove a simple lower bound on the target domain error that complements the existing upper bound. Our bound shows the insufﬁciency of minimizing source domain error and marginal distribution mismatch for a guaranteed reduction in the target domain error, due to the possible increase of induced labeling function mismatch. This insufﬁciency is further illustrated through simple distributions for which the same UDA approach succeeds, fails, and may succeed or fail with an equal chance. Motivated from this, we propose novel data poisoning attacks to fool UDA methods into learning representations that produce large target domain errors. We evaluate the effect of these attacks on popular UDA methods using benchmark datasets where they have been previously shown to be successful. Our results show that poisoning can signiﬁcantly decrease the target domain accuracy, dropping it to almost 0% in some cases, with the addition of only 10% poisoned data in the source domain. The failure of these UDA methods demonstrates their limitations at guaranteeing cross-domain generalization consistent with our lower bound. Thus, evaluating UDA methods in adversarial settings such as data poisoning provides a better sense of their robustness to data distributions unfavorable for UDA. 1

Introduction
The problem of domain adaptation (DA) arises when the training and the test data distributions are different, violating the common assumption of supervised learning. In this paper, we focus on unsupervised DA (UDA), which is a special case of DA when no labeled information from the target domain is available. This setting is useful for applications where obtaining large-scale well-curated datasets is both time-consuming and costly. The seminal works [1, 2] proved an upper bound on a classiﬁer’s target domain error in the UDA setting leading to several algorithms for learning in this setting. Many of these algorithms rely on learning a domain invariant representation by minimizing the error on the source domain and a divergence measure between the marginal feature distributions of the source and target domains. Popular divergence measures include total variation distance, Jensen-Shannon divergence [9, 32, 37], Wasserstein distance [29, 15, 7], and maximum mean discrepancy
[19, 17, 20]. The success of these algorithms is argued in terms of minimization of the upper bound proposed in [1] along with an improvement in the target domain accuracy on benchmark UDA tasks.
Despite this success on benchmark datasets, some works [9, 16, 36, 33] have presented evidence of the failure of these methods in different scenarios. Recent works have explained this apparent failure of UDA methods by proposing new upper bounds [36, 6, 14, 34] on the target domain error while 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
others have demonstrated the cause of failure using experiments showing that learning a domain invariant representation that minimizes the source domain error can cause an increase in the error of the ideal joint hypothesis [16]. To provably explain this apparent failure of learning in the UDA setting we propose a lower bound on the target domain error. Our lower bound provides a necessary condition for successful learning in the UDA setting, complementing the existing upper bound of
[1] and is dependent on the difference between the labeling functions of source and target domain data induced by the representation map. For cases where the induced labeling functions match on the source and the target domain data (i.e., favorable case), the success of UDA is explained using the upper bounds proposed by previous works [1, 36, 14]. For a representation that aligns the source and the target domain data and minimizes the error on the source but induces labeling functions that don’t agree on the source and the target domain data (i.e., unfavorable case), our lower bound explains the failure of UDA. Our analysis brings to light yet another case (i.e., ambiguous case) of data distributions where success and failure of UDA are equally likely. This happens due to the lack of label information from the target domain. This case opens doors for adversarial attacks against
UDA methods since a small amount of misinformation about the target domain labels can lead the
UDA methods into producing a representation similar to the unfavorable case, incurring a signiﬁcant increase in the target domain error.
Motivated from this analysis of UDA methods under different data distributions, we evaluate the extent to which the performance of current UDA methods can suffer in presence of a small amount of adversarially crafted data. For this purpose, we propose novel data poisoning attacks, using mislabeled and clean-label points. We evaluate the effect of our poisoning attacks on popular UDA methods using benchmark datasets, where they were previously shown to be very effective. We
ﬁnd that our poisoning attacks cause UDA methods to either align incorrect classes from the two domains or prevent correct classes from being very close in the representation space. Both of these lead to the failure of UDA methods at reducing target domain error. With just 10% poison data in the source domain, target domain accuracy for current UDA methods is signiﬁcantly reduced, dropping to almost 0% in some cases. This dramatic failure of UDA methods demonstrates their limits and suggests that the future UDA methods must be evaluated in adversarial settings along with evaluation on benchmark datasets to truly gauge their effectiveness at learning under the UDA setting.
Our main contributions are summarized as follows:
• We prove a lower bound on the target domain error that provides a necessary condition for suc-cessful learning in the UDA setting. Our bound shows the failure of learning a domain invariant representation while minimizing the source domain error at guaranteeing target generalization.
• We present example data distributions where UDA succeeds, fails, and where success and failure are equally likely. This sensitivity of UDA methods to the data distribution brings to light a new vulnerability of UDA methods to adversarial attacks such as data poisoning.
• To concretely understand the extent of this vulnerability of UDA methods, we propose novel data poisoning attacks using clean-label and mislabeled data. Our results show a dramatic failure of current UDA methods at target generalization in presence of poisoned data. Thus, our poisoning attacks can provide better insights into the robustness of UDA methods than those obtained from performance evaluation on benchmark datasets. 2