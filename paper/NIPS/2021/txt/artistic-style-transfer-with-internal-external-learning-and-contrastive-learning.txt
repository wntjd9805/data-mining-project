Abstract
Although existing artistic style transfer methods have achieved signiﬁcant im-provement with deep neural networks, they still suffer from artifacts such as disharmonious colors and repetitive patterns. Motivated by this, we propose an internal-external style transfer method with two contrastive losses. Speciﬁcally, we utilize internal statistics of a single style image to determine the colors and texture patterns of the stylized image, and in the meantime, we leverage the ex-ternal information of the large-scale style dataset to learn the human-aware style information, which makes the color distributions and texture patterns in the stylized image more reasonable and harmonious. In addition, we argue that existing style transfer methods only consider the content-to-stylization and style-to-stylization relations, neglecting the stylization-to-stylization relations. To address this issue, we introduce two contrastive losses, which pull the multiple stylization embeddings closer to each other when they share the same content or style, but push far away otherwise. We conduct extensive experiments, showing that our proposed method can not only produce visually more harmonious and satisfying artistic images, but also promote the stability and consistency of rendered video clips. 1

Introduction
Artistic style transfer is a long-standing research topic that seeks to render a photograph with a given artwork style. Ever since Gatys et al. [10] for the ﬁrst time proposed a neural method, which leverages a pre-trained Deep Convolutional Neural Network (DCNN) to separate and recombine contents and styles of arbitrary images, an unprecedented booming [20, 26, 15, 30, 36, 51, 48] in style transfer has been witnessed.
Despite the recent progress, there still exists a large gap between real artworks and synthesized stylizations. As shown in Figure 1, the stylized images usually contain some disharmonious colors and repetitive patterns, which makes them easily distinguishable from real artworks. We argue that this is because existing style transfer methods often conﬁne themselves to the internal style statistics of a single artistic image. In some other tasks (for example, image-to-image translation
[17, 60, 16, 25, 8, 18]), the style is usually learned from a collection of images, which inspires us to leverage the external information reserved in the large-scale style dataset to improve the stylization results in style transfer. Why is the external information so important for style transfer? Our analyses are as follows:
Although different images in the style dataset vary greatly in ﬁne details, they share a key commonality: they are all human-created artworks, whose brushstrokes, color distributions, texture patterns, tones,
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Stylization examples. The ﬁrst and second columns show the style and content images, respectively. The other seven columns show the stylized images produced by our method, Gatys et al.
[10], AdaIN [15], WCT [30], Avatar-Net [41], LST [28], and SANet [36]. etc., are more consistent with human perception. Namely, they contain some human-aware style information that is lacked in synthesized stylizations. A natural idea is to utilize such human-aware style information to improve stylization results. To this end, we employ an internal-external learning scheme during training, which takes both internal learning and external learning into consideration.
To be more speciﬁc, on the one hand, we follow previous methods [10, 20, 46, 54, 58], utilizing internal statistics of a single artwork to determine the colors and texture patterns of the stylized image. On the other hand, we employ Generative Adversarial Nets (GANs) [11, 39, 2, 56, 3] to externally learn the human-aware style information from the large-scale style dataset, which is then used to make the color distributions and texture patterns in the stylized image more reasonable and harmonious, signiﬁcantly bridging the gap between human-created artworks and AI-created artworks.
In addition, there is another problem with existing style transfer methods: they usually employ a content loss and a style loss to enforce the content-to-stylization relations and style-to-stylization relations, respectively, while neglect the stylization-to-stylization relations, which are also important for style transfer. What are stylization-to-stylization relations? Intuitively, stylized images rendered with the same style image should have closer relations in style than those rendered with different style images. Similarly, stylized images based on the same content image should have closer relations in content than those based on different content images. Inspired by this, in this paper we introduce two contrastive losses: content contrastive loss and style contrastive loss that can pull the multiple stylization embeddings closer to each other when they share the same content or style, but push far away otherwise. To the best of our knowledge, this is the ﬁrst work that successfully leverages the power of contrastive learning [6, 12, 21, 38] in the style transfer scenario.
Our extensive experiments show that the proposed method can not only produce visually more harmonious and plausible artistic images, but also promote the stability and consistency of rendered video clips.
To summarize, the main contributions of this work are threefold:
• We propose a novel internal-external style transfer method which takes both internal learning and external learning into consideration, signiﬁcantly bridging the gap between human-created and AI-created artworks.
• We for the ﬁrst time introduce contrastive learning to style transfer, yielding more satisfying stylization results with the learned stylization-to-stylization relations.
• We demonstrate the effectiveness and superiority of our approach by extensive comparisons with several state-of-the-art artistic style transfer methods. 2