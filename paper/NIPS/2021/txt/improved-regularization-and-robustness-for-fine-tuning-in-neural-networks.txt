Abstract
A widely used algorithm for transfer learning is ﬁne-tuning, where a pre-trained model is ﬁne-tuned on a target task with a small amount of labeled data. When the capacity of the pre-trained model is much larger than the size of the target data set,
ﬁne-tuning is prone to overﬁtting and “memorizing” the training labels. Hence, an important question is to regularize ﬁne-tuning and ensure its robustness to noise.
To address this question, we begin by analyzing the generalization properties of
ﬁne-tuning. We present a PAC-Bayes generalization bound that depends on the distance traveled in each layer during ﬁne-tuning and the noise stability of the
ﬁne-tuned model. We empirically measure these quantities. Based on the analysis, we propose regularized self-labeling—the interpolation between regularization and self-labeling methods, including (i) layer-wise regularization to constrain the distance traveled in each layer; (ii) self label-correction and label-reweighting to correct mislabeled data points (that the model is conﬁdent) and reweight less conﬁdent data points. We validate our approach on an extensive collection of image and text data sets using multiple pre-trained model architectures. Our approach improves baseline methods by 1.76% (on average) for seven image classiﬁcation tasks and 0.75% for a few-shot classiﬁcation task. When the target data set includes noisy labels, our approach outperforms baseline methods by 3.56% on average in two noisy settings. 1

Introduction
Learning from limited labeled data is a fundamental problem in many real-world applications (Ratner et al., 2016, 2017). A common approach to address this problem is ﬁne-tuning a large model that has been pre-trained on publicly available labeled data (He et al., 2019). Since ﬁne-tuning is typically applied to a target task with limited labels, this algorithm is prone to overﬁtting or “memorization” issues (Tan et al., 2018). These issues worsen when the target task contains noisy labels (Zhang et al., 2016). In this paper, we analyze regularization methods for ﬁne-tuning from both theoretical and empirical perspectives. Based on the analysis, we propose a regularized self-labeling approach that improves the generalization and robustness properties of ﬁne-tuning.
Previous works (Li et al., 2018a,b) have proposed regularization methods to constrain the distance between a ﬁne-tuned model and the pre-trained model in the Euclidean norm. Li et al. (2020) provides extensive study to show that the performance of ﬁne-tuning and the beneﬁt of adding regularization depend on the hyperparameter choices. Salman et al. (2020) empirically ﬁnd that performing adversarial training during the pre-training phase helps learn pre-trained models that transfer better to downstream tasks. The work of Gouk et al. (2021) generalizes the above ideas to various norm choices and ﬁnds that projected gradient descent methods perform well for implementing distance-based regularization. Additionally, they derive generalization bounds for ﬁne-tuning using Rademacher complexity. These works focus on settings where there is no label noise in the target data set. When 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
label noise is present, for example, due to applying weak supervision techniques (Ratner et al., 2016), an important question is to design methods that are robust to such noise. The problem of learning from noisy labels has a rich history of study in supervised learning (Natarajan et al., 2013). In contrast, little is known in the transfer learning setting. These considerations motivate us to analyze the generalization and robustness properties of ﬁne-tuning.
In Section 4.1, we begin by conducting a PAC-Bayesian analysis of regularized ﬁne-tuning. This is inspired by recent works that have found PAC-Bayesian analysis correlates with empirical perfor-mance better than Rademacher complexity (Jiang et al., 2020). We identify two critical measures for analyzing the generalization performance of ﬁne-tuning. The ﬁrst measure is the `2 norm of the distance between the pre-trained model (initialization) and the ﬁne-tuned model. The second measure is the perturbed loss of the ﬁne-tuned model, i.e. its loss after the model weights get perturbed by random noise. First, we observe that the ﬁne-tuned weights remain closed to the pre-trained model.
Moreover, the top layers travel much further away from the pre-trained model than the bottom layers.
Second, we ﬁnd that ﬁne-tuning from a pre-trained model implies better noise stability than training from a randomly initialized model. In Section 4.2, we evaluate regularized ﬁne-tuning for target tasks with noisy labels. We ﬁnd that ﬁne-tuning is prone to “memorizing the noisy labels”, and regularization helps alleviate such memorization behavior. Moreover, we observe that the neural network has not yet overﬁtted to the noisy labels during the early phase of ﬁne-tuning. Thus, its prediction could be used to relabel the noisy labels.
We propose an algorithm that incorporates layer-wise regularization and self-labeling for im-proved regularization and robustness based on our results. Figure 1 illustrates the two compo-nents. First, we encode layer-wise distance con-straints to regularize the model weights at dif-ferent levels. Compared to (vanilla) ﬁne-tuning, our algorithm reduces the gap between the train-ing and test accuracy, thus alleviating overﬁtting.
Second, we add a self-labeling mechanism that corrects and reweights “noisy labels” based on the neural network’s predictions. Figure 1 shows that our algorithm effectively hinders the model from learning the incorrect labels by relabeling them to correct ones.
Figure 1: Red: Layer-wise regularization closes generalization gap. Magenta: Self-labeling rela-bels noisy data points to their correct label.
In Section 5, we evaluate our proposed algorithm for both transfer learning and few-shot classiﬁcation tasks with image and text data sets. First, using ResNet-101 (He et al., 2016) as the pre-trained model, our algorithm outperforms previous ﬁne-tuning methods on seven image classiﬁcation tasks by 1.76% on average and 3.56% when their labels are noisy. Second, we ﬁnd qualitatively similar results for applying our approach to medical image classiﬁcation tasks (ChestX-ray14 (Wang et al., 2017; Rajpurkar et al., 2017)) and vision transformers (Dosovitskiy et al., 2020). Finally, we extend our approach to few-shot learning and sentence classiﬁcation. For these related but different tasks and data modalities, we ﬁnd an improvement of 0.75% and 0.46% over previous methods, respectively.
In summary, our contributions are threefold. First, we provide a PAC-Bayesian analysis of regularized
ﬁne-tuning. Our result implies empirical measures that explain the generalization performance of regularized ﬁne-tuning. Second, we present a regularized self-labeling approach to enhance the generalization and robustness properties of ﬁne-tuning. Third, we validate our approach on an extensive collection of classiﬁcation tasks and pre-trained model architectures. 2