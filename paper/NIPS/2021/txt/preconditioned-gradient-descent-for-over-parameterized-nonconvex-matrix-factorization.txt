Abstract
In practical instances of nonconvex matrix factorization, the rank of the true solution r⋆ is often unknown, so the rank r of the model can be overspecified as r > r⋆.
This over-parameterized regime of matrix factorization significantly slows down the convergence of local search algorithms, from a linear rate with r = r⋆ to a sublinear rate when r > r⋆. We propose an inexpensive preconditioner for the matrix sensing variant of nonconvex matrix factorization that restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Classical gradient descent in a neighborhood of the solution slows down due to the need for the model matrix factor to become singular. Our key result is that this singularity can be corrected by ℓ2 regularization with a specific range of values for the damping parameter. In fact, a good damping parameter can be inexpensively estimated from the current iterate. The resulting algorithm, which we call preconditioned gradient descent or PrecGD, is stable under noise, and converges linearly to an information theoretically optimal error bound. Our numerical experiments find that
PrecGD works equally well in restoring the linear convergence of other variants of nonconvex matrix factorization in the over-parameterized regime. 1

Introduction
Numerous problems in machine learning can be reduced to the matrix factorization problem of recovering a low-rank positive semidefinite matrix M ⋆ ⪰ 0, given a small number of potentially noisy observations [1–7]. In every case, the most common approach is to formulate an n×n candidate matrix M = XX T in factored form, and to minimize a nonconvex empirical loss f (X) over its n × r low-rank factor X. But in most real applications of nonconvex matrix factorization, the rank of the ground truth r⋆ = rank(M ⋆) is unknown. It is reasonable to choose the rank r of the model XX T conservatively, setting it to be potentially larger than r⋆, given that the ground truth can be exactly recovered so long as r ≥ r⋆. In practice, this will often lead to an over-parameterized regime, in which r > r⋆, and we have specified more degrees of freedom in our model XX T than exists in the underlying ground truth M ⋆.
Zhuo et al. [8] recently pointed out that nonconvex matrix factorization becomes substantially less efficient in the over-parameterized regime. For the prototypical instance of matrix factorization known as matrix sensing (see Section 3 below for details) it is well-known that, if r = r⋆, then (classic) gradient descent or GD
Xk+1 = Xk − α∇f (Xk) (GD) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to an ϵ-accurate iterate in O(κ log(1/ϵ)) iterations, where κ = converges at a linear rate,
λ1(M ⋆)/λr∗ (M ⋆) is the condition number of the ground truth [9, 10]. But in the case that r > r⋆,
Zhuo et al. [8] proved that gradient descent slows down to a sublinear convergence rate, now requir-ing poly(1/ϵ) iterations to yield a comparable ϵ-accurate solution. This is a dramatic, exponential slow-down: whereas 10 digits of accuracy can be expected in a just few hundred iterations when r = r⋆, tens of thousands of iterations might produce just 1-2 accurate digits once r > r⋆. The slow-down occurs even if r is just off by one, as in r = r⋆ + 1.
It is helpful to understand this pheonomenon by viewing over-parameterization as a special, extreme case of ill-conditioning, where the condition number of the ground truth, κ, is taken to infinity. In this limit, the classic linear rate O(κ log(1/ϵ)) breaks down, and in reality, the convergence rate deterioriates to sublinear.
In this paper, we present an inexpensive preconditioner for gradient descent. The resulting algorithm, which we call PrecGD, corrects for both ill-conditioning and over-parameterization at the same time, without viewing them as distinct concepts. We prove, for the matrix sensing variant of nonconvex matrix factorization, that the preconditioner restores the convergence rate of gradient descent back to linear, even in the over-parameterized case, while also making it agnostic to possible ill-conditioning in the ground truth. Moreover, PrecGD maintains a similar per-iteration cost to regular gradient descent, is stable under noise, and converges linearly to an information theoretically optimal error bound.
We also perform numerical experiments on other variants of nonconvex matrix factorization, with different choices of the empirical loss function f . In particular, we consider different ℓp norms with 1 ≤ p < 2, in order to gauge the effectiveness of PrecGD for increasingly nonsmooth loss functions.
Our numerical experiments find that, if regular gradient descent is capable of converging quickly when the rank is known r = r⋆, then PrecGD restores this rapid converging behavior when r > r⋆.
PrecGD is able to overcome ill-conditioning in the ground truth, and converge reliably without exhibiting sporadic behavior. 2 Proposed Algorithm: Preconditioned Gradient Descent
Our preconditioner is inspired by a recent work of Tong et al. [11] on matrix sensing with an ill-conditioned ground truth M ⋆. Over-parameterization can be viewed as the limit of this regime, in which λr(M ⋆), the r-th largest eigenvalue of M ⋆, is allowed to approach all the way to zero. For finite but potentially very small values of λr(M ⋆) > 0, Tong et al. [11] suggests the following iterations, which they named scaled gradient descent or ScaledGD:
Xk+1 = Xk − α∇f (Xk)(X T k Xk)−1. (ScaledGD)
They prove that the scaling allows the iteration to make a large, constant amount of progress at every iteration, independent of the value of λr(M ⋆) > 0. However, applying this same scheme to the over-parameterized case with λr(M ⋆) = 0 results in an inconsistent, sporadic behavior.
The issues encountered by both regular GD and ScaledGD with over-parameterization r > r⋆ can be explained by the fact that our iterate Xk must necessarily become singular as our rank-r model XkX T k converges towards the rank-r⋆ ground truth M ⋆. For GD, this singularity causes the per-iteration progress itself to decay, so that more and more iterations are required for each fixed amount of progress. ScaledGD corrects for this decay in per-iteration progress by suitably rescaling the search direction. However, the rescaling itself requires inverting a near-singular matrix, which causes algorithm to take on sporadic values.
A classical remedy to issues posed by singular matrices is ℓ2 regularization, in which the singular matrix is made “less singular” by adding a small identity perturbation. Applying this idea to ScaledGD yields the following iterations
Xk+1 = Xk − α∇f (Xk)(X T (PrecGD) where ηk ≥ 0 is the damping parameter specific to the k-th iteration. There are several interpretations to this scheme, but the most helpful is to view η as a parameter that allows us to interpolate between
ScaledGD (with η = 0) and regular GD (in the limit η → ∞). In this paper, we prove for matrix sensing that, if the k-th damping parameter ηk is chosen within a constant factor of the error k Xk + ηkIr)−1,
Clb∥XkX T k − M ⋆∥F ≤ ηk ≤ Cub∥XkX T k − M ⋆∥F , where Clb, Cub > 0 are abs. const. (1) 2
Figure 1: PrecGD converges linearly in the overparameterized regime. Convergence of regular gradient descent (GD), ScaledGD and PrecGD for noiseless matrix sensing (with data taken from [12, 13]) from the same initial points and using the same learning rate α = 2 × 10−2. (Left r = r∗) Set n = 4 and r∗ = r = 2. All three methods convergence at a linear rate, though GD converges at a slower rate due to ill-conditioning in the ground truth. (Right r > r∗) With n = 4, r = 4 and r∗ = 2, over-parameterization causes gradient descent to slow down to a sublinear rate. ScaledGD also behaves sporadically. Only PrecGD converges linearly to the ground truth. then the resulting iterations are guaranteed to converge linearly, at a rate that is independent of both over-parameterization and ill-conditioning in the ground truth M ⋆. With noisy measurements, setting
ηk to satisfy (1) will allow the iterations to converge to an error bound that is well-known to be minimax optimal up to logarithmic factors [14].
We refer to the resulting iterations (with a properly chosen ηk) as preconditioned gradient descent, or
PrecGD for short. For matrix sensing with noiseless measurements, an optimal ηk that satisfies the condition (1) is obtained for free by setting ηk = (cid:112)f (Xk). In the case of noisy measurements, we show that a good choice of ηk is available based on an approximation of the noise variance. 3