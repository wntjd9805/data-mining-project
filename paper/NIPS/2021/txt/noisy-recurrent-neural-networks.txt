Abstract
We provide a general framework for studying recurrent neural networks (RNNs) trained by injecting noise into hidden states. Speciﬁcally, we consider RNNs that can be viewed as discretizations of stochastic differential equations driven by input data. This framework allows us to study the implicit regularization effect of general noise injection schemes by deriving an approximate explicit regularizer in the small noise regime. We ﬁnd that, under reasonable assumptions, this implicit regularization promotes ﬂatter minima; it biases towards models with more stable dynamics; and, in classiﬁcation tasks, it favors models with larger classiﬁcation margin. Sufﬁcient conditions for global stability are obtained, highlighting the phenomenon of stochastic stabilization, where noise injection can improve stability during training. Our theory is supported by empirical results which demonstrate that the RNNs have improved robustness with respect to various input perturbations. 1

Introduction
Viewing recurrent neural networks (RNNs) as discretizations of ordinary differential equations (ODEs) driven by input data has recently gained attention [7, 27, 16, 49]. The “formulate in continuous time, and then discretize” approach [38] motivates novel architecture designs before experimentation, and it provides a useful interpretation as a dynamical system. This, in turn, has led to gains in reliability and robustness to data perturbations.
Recent efforts have shown how adding noise can also improve stability during training, and con-sequently improve robustness [35]. In this work, we consider discretizations of the corresponding stochastic differential equations (SDEs) obtained from ODE formulations of RNNs through the addition of a diffusion (noise) term. We refer to these as Noisy RNNs (NRNNs). By dropping the noisy elements at inference time, NRNNs become a stochastic learning strategy which, as we shall prove, has a number of important beneﬁts. In particular, stochastic learning strategies (including dropout) are often used as natural regularizers, favoring solutions in regions of the loss landscape with desirable properties (often improved generalization and/or robustness). This mechanism is commonly referred to as implicit regularization [40, 39, 50], differing from explicit regularization where the loss is explicitly modiﬁed. For neural network (NN) models, implicit regularization towards wider minima is conjectured to be a prominent ingredient in the success of stochastic optimization [67, 28].
Indeed, implicit regularization has been linked to increases in classiﬁcation margins [47], which can lead to improved generalization performance [51]. A common approach to identify and study implicit regularization is to approximate the implicit regularization by an appropriate explicit regularizer 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[40, 39, 1, 6, 21]. Doing so, we will see that NRNNs favor wide minima (like SGD); more stable dynamics; and classiﬁers with a large classiﬁcation margin, keeping generalization error small.
SDEs have also seen recent appearances in neural SDEs [59, 24], stochastic generalizations of neural
ODEs [9] which can be seen as an analogue of NRNNs for non-sequential data, with a similar relationship to NRNNs as feedforward NNs do to RNNs. They have been shown to be robust in practice [35]. Analogously, we shall show that the NRNN framework leads to more reliable and robust RNN classiﬁers, whose promise is demonstrated by experiments on benchmark data sets.
Contributions. For the class of NRNNs (formulated ﬁrst as a continuous-time model, which is then discretized), the following are our main contributions:
• we identify the form of the implicit regularization for NRNNs through a corresponding (data-dependent) explicit regularizer in the small noise regime (see Theorem 1);
• we focus on its effect in classiﬁcation tasks, providing bounds for the classiﬁcation margin for the deterministic RNN classiﬁers (see Theorem 2); in particular, Theorem 2 reveals that stable RNN dynamics can lead to large classiﬁcation margin;
• we show that noise injection can also lead to improved stability (see Theorem 3) via a Lyapunov stability analysis of continuous-time NRNNs;
• we demonstrate via empirical experiments on benchmark data sets that NRNN classiﬁers to data perturbations when compared to other recurrent models, while
Research code is provided here: are more robust retaining state-of-the-art performance for clean data. https://github.com/erichson/NoisyRNN.
Notation. We use (cid:107)v(cid:107) := (cid:107)v(cid:107)2 to denote the Euclidean norm of the vector v, and (cid:107)A(cid:107)2 and (cid:107)A(cid:107)F to denote the spectral norm and Frobenius norm of the matrix A, respectively. The ith element of a vector v is denoted by vi or [v]i, and the (i, j)-entry of a matrix A by Aij or [A]ij. For a vector v = (v1, . . . , vd), diag(v) denotes the diagonalization of v with diag(v)ii = vi. I denotes the identity matrix (with dimension clear from context), while superscript T denotes transposition. For a matrix M , M sym = (M + M T )/2 denotes its symmetric part, λmin(M ) and λmax(M ) denote its minimum and maximum eigenvalue respectively, σmax(M ) denotes its maximum singular value, and T r(M ) denotes its trace. For a function f : Rn → Rm such that each of its ﬁrst-order partial derivatives (with respect to x) exist, ∂f
∂x ∈ Rm×n is the Jacobian matrix of f . For a scalar-valued function g : Rn → R, ∇hg is the gradient of g with respect to the variable h ∈ Rn and Hhg is the
Hessian of g with respect to h. 2