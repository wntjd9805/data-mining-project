Abstract
Vision transformers (ViT) have demonstrated impressive performance across nu-merous machine vision tasks. These models are based on multi-head self-attention mechanisms that can ﬂexibly attend to a sequence of image patches to encode con-textual cues. An important question is how such ﬂexibility (in attending image-wide context conditioned on a given patch) can facilitate handling nuisances in natural images e.g., severe occlusions, domain shifts, spatial permutations, adversarial and natural perturbations. We systematically study this question via an extensive set of experiments encompassing three ViT families and provide comparisons with a high-performing convolutional neural network (CNN). We show and analyze the following intriguing properties of ViT: (a) Transformers are highly robust to severe occlusions, perturbations and domain shifts, e.g., retain as high as 60% top-1 accuracy on ImageNet even after randomly occluding 80% of the image content. (b) The robustness towards occlusions is not due to texture bias, instead we show that ViTs are signiﬁcantly less biased towards local textures, compared to
CNNs. When properly trained to encode shape-based features, ViTs demonstrate shape recognition capability comparable to that of human visual system, previously unmatched in the literature. (c) Using ViTs to encode shape representation leads to an interesting consequence of accurate semantic segmentation without pixel-level supervision. (d) Off-the-shelf features from a single ViT model can be combined to create a feature ensemble, leading to high accuracy rates across a range of classiﬁcation datasets in both traditional and few-shot learning paradigms. We show effective features of ViTs are due to ﬂexible and dynamic receptive ﬁelds possible via self-attention mechanisms. Code: https://git.io/Js15X. 1

Introduction
As visual transformers (ViT) attract more interest [1], it becomes highly pertinent to study character-istics of their learned representations. Speciﬁcally, from the perspective of safety-critical applications such as autonomous cars, robots and healthcare; the learned representations need to be robust and generalizable. In this paper, we compare the performance of transformers with convolutional neural networks (CNNs) for handling nuisances (e.g., occlusions, distributional shifts, adversarial and natural perturbations) and generalization across different data distributions. Our in-depth analysis is based on three transformer families, ViT [2], DeiT [3] and T2T [4] across ﬁfteen vision datasets. For brevity, we refer to all the transformer families as ViT, unless otherwise mentioned.
We are intrigued by the fundamental differences in the operation of convolution and self-attention, that have not been extensively explored in the context of robustness and generalization. While convolutions excel at learning local interactions between elements in the input domain (e.g., edges and contour information), self-attention has been shown to effectively learn global interactions (e.g., 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: We show intriguing properties of ViT including impressive robustness to (a) severe occlusions, (b) distributional shifts (e.g., stylization to remove texture cues), (c) adversarial perturbations, and (d) patch permutations. Furthermore, our ViT models trained to focus on shape cues can segment foregrounds without any pixel-level supervision (e). Finally, off-the-shelf features from ViT models generalize better than CNNs (f). relations between distant object parts) [5, 6]. Given a query embedding, self-attention ﬁnds its interactions with the other embeddings in the sequence, thereby conditioning on the local content while modeling global relationships [7]. In contrast, convolutions are content-independent as the same
ﬁlter weights are applied to all inputs regardless of their distinct nature. Given the content-dependent long-range interaction modeling capabilities, our analysis shows that ViTs can ﬂexibly adjust their receptive ﬁeld to cope with nuisances in data and enhance expressivity of the representations.
Our systematic experiments and novel design choices lead to the following interesting ﬁndings:
• ViTs demonstrate strong robustness against severe occlusions for foreground objects, non-salient background regions and random patch locations, when compared with state-of-the-art CNNs.
For instance, with a signiﬁcant random occlusion of up to 80%, DeiT [3] can maintain top-1 60% where CNN has zero accuracy, on ImageNet [8] val. set. accuracy up to
• When presented with texture and shape of the same object, CNN models often make decisions based on texture [9]. In contrast, ViTs perform better than CNNs and comparable to humans on shape recognition. This highlights robustness of ViTs to deal with signiﬁcant distribution shifts e.g., recognizing object shapes in less textured data such as paintings.
⇠
• Compared to CNNs, ViTs show better robustness against other nuisance factors such as spatial patch-level permutations, adversarial perturbations and common natural corruptions (e.g., noise, blur, contrast and pixelation artefacts). However, similar to CNNs [10], a shape-focused training process renders them vulnerable against adversarial attacks and common corruptions.
• Apart from their promising robustness properties, off-the-shelf ViT features from ImageNet pretrained models generalize exceptionally well to new domains e.g., few-shot learning, ﬁne-grained recognition, scene categorization and long-tail classiﬁcation settings.
In addition to our extensive experimental analysis and new ﬁndings, we introduce several novel design choices to highlight the strong potential of ViTs. To this end, we propose an architectural modiﬁcation to DeiT to encode shape-information via a dedicated token that demonstrates how seemingly contradictory cues can be modeled with distinct tokens within the same architecture, leading to favorable implications such as automated segmentation without pixel-level supervision.
Moreover, our off-the-shelf feature transfer approach utilizes an ensemble of representations derived from a single architecture to obtain state-of-the-art generalization with a pre-trained ViT (Fig. 1). 2