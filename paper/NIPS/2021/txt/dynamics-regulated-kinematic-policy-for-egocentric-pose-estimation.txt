Abstract
We propose a method for object-aware 3D egocentric pose estimation that tightly integrates kinematics modeling, dynamics modeling, and scene object information.
Unlike prior kinematics or dynamics-based approaches where the two components are used disjointly, we synergize the two approaches via dynamics-regulated train-ing. At each timestep, a kinematic model is used to provide a target pose using video evidence and simulation state. Then, a prelearned dynamics model attempts to mimic the kinematic pose in a physics simulator. By comparing the pose in-structed by the kinematic model against the pose generated by the dynamics model, we can use their misalignment to further improve the kinematic model. By factor-ing in the 6DoF pose of objects (e.g., chairs, boxes) in the scene, we demonstrate for the ﬁrst time, the ability to estimate physically-plausible 3D human-object interactions using a single wearable camera. We evaluate our egocentric pose estimation method in both controlled laboratory settings and real-world scenarios. 1

Introduction
From a video captured by a single head-mounted wearable camera (e.g., smartglasses, action camera, body camera), we aim to infer the wearer’s global 3D full-body pose and interaction with objects in the scene, as illustrated in Fig. 1. This is important for applications like virtual and augmented reality, sports analysis, and wearable medical monitoring, where third-person views are often unavailable and proprioception algorithms are needed for understanding the actions of the camera wearer. However, this task is challenging since the wearer’s body is often unseen from a ﬁrst-person view and the
⇤Work done at Carnegie Mellon University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
body motion needs to be inferred solely based on the videos captured by the front-facing camera.
Furthermore, egocentric videos usually capture the camera wearer interacting with objects in the scene, which adds additional complexity in recovering a pose sequence that agrees with the scene context. Despite these challenges, we show that it is possible to infer accurate human motion and human-object interaction from a single head-worn front-facing camera.
Egocentric pose estimation can be solved using two different paradigms: (1) a kinematics perspective and (2) a dynamics perspective. Kinematics-based approaches study motion without regard to the underlying forces (e.g., gravity, joint torque) and cannot faithfully emulate human-object interaction without modeling proper contact and forces. They can achieve accurate pose estimates by directly outputting joint angles but can also produce results that violate physical constraints (e.g. foot skating and ground penetration). Dynamics-based approaches, or physics-based approaches, study motions that result from forces. They map directly from visual input to control signals of a human proxy (humanoid) inside a physics simulator and recover 3D poses through simulation. These approaches have the crucial advantage that they output physically-plausible human motion and human-object interaction (i.e., pushing an object will move it according to the rules of physics). However, since no joint torque is captured in human motion datasets, physics-based humanoid controllers are hard to learn, generalize poorly, and are actively being researched [36, 57, 51, 52].
In this work, we argue that a hybrid approach merging the kinematics and dynamics perspectives is needed. Leveraging a large human motion database [29], we learn a task-agnostic dynamics-based humanoid controller to mimic broad human behaviors, ranging from every day motion to dancing and kickboxing. The controller is general-purpose and can be viewed as providing low-level motor skills of a human. After the controller is learned, we train an object-aware kinematic policy to specify the target poses for the controller to mimic. One approach is to let the kinematic model produce target motion only based on the visual input [58, 51, 56]. This approach uses the physics simulation as a post-processing step: the kinematic model computes the target motion separately from the simulation and may output unreasonable target poses. We propose to synchronize the two aspects by designing a kinematic policy that guides the controller and receives timely feedback through comparing its target pose and the resulting simulation state. Our model thus serves as a high-level motion planning module that adapts intelligently based on the current simulation state. In addition, since our kinematic policy only outputs poses and does not model joint torque, it can receive direct supervision from motion capture (MoCap) data. While poses from MoCap can provide an initial-guess of target motion, our model can search for better solutions through trial and error. This learning process, dubbed dynamics-regulated training, jointly optimizes our model via supervised learning and reinforcement learning, and signiﬁcantly improves its robustness to real-world use cases.
In summary, our contributions are as follows: (1) we are the ﬁrst to tackle the challenging task of estimating physically-plausible 3D poses and human-object interactions from a single front-facing camera; (2) we learn a general-purpose humanoid controller from a large MoCap dataset and can perform a broad range of motions inside a physics simulation; (3) we propose a dynamics-regulated training procedure that synergizes kinematics, dynamics, and scene context for egocentric vision; (4) experiments on a controlled motion capture laboratory dataset and a real-world dataset demonstrate that our model outperforms other state-of-the-art methods on pose-based and physics-based metrics, while generalizing to videos taken in real-world scenarios. 2