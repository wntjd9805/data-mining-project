Abstract
Graph neural networks (GNNs) have limited expressive power, failing to represent many graph classes correctly. While more expressive graph representation learning (GRL) alternatives can distinguish some of these classes, they are signiﬁcantly harder to implement, may not scale well, and have not been shown to outperform well-tuned
GNNs in real-world tasks. Thus, devising simple, scalable, and expressive GRL architectures that also achieve real-world improvements remains an open challenge.
In this work, we show the extent to which graph reconstruction—reconstructing a graph from its subgraphs—can mitigate the theoretical and practical problems currently faced by GRL architectures. First, we leverage graph reconstruction to build two new classes of expressive graph representations. Secondly, we show how graph reconstruction boosts the expressive power of any GNN architecture while being a (provably) powerful inductive bias for invariances to vertex removals. Empirically, we show how reconstruction can boost GNN’s expressive power—while maintaining its invariance to permutations of the vertices—by solving seven graph property tasks not solvable by the original GNN. Further, we demonstrate how it boosts state-of-the-art
GNN’s performance across nine real-world benchmark datasets. 1

Introduction
Supervised machine learning for graph-structured data, i.e., graph classiﬁcation and regression, is ubiquitous across application domains ranging from chemistry and bioinformatics [8, 94] to image [89], and social network analysis [33]. Consequently, machine learning on graphs is an active research area with numerous proposed approaches—notably GNNs [21, 42, 45] being the most representative case of
GRL methods.
Arguably, GRL’s most interesting results arise from a cross-over between graph theory and representation learning. For instance, the representational limits of GNNs are upper-bounded by a simple heuristic for the graph isomorphism problem [78, 105], the 1-dimensional Weisfeiler-Leman algorithm (1-WL) [44, 73, 75, 101, 102], which might miss crucial structural information in the data [5]. Further works show how GNNs cannot approximate graph properties such as diameter, radius, girth, and subgraph counts [25, 38], inspiring architectures [6, 66, 77, 78] based on the more powerful κ-dimensional
Weisfeiler-Leman algorithm (κ-WL) [44].1 On the other hand, despite the limited expressiveness of
GNNs, they still can overﬁt the training data, offering limited generalization performance [105]. Hence, devising GRL architectures that are simultaneously sufﬁciently expressive and avoid overﬁtting remains an open problem. 1We opt for using κ instead of k, i.e., κ-WL instead of k-WL, to not confuse the reader with the hyperparameter k of our models. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
An under-explored connection between graph theory and GRL is graph reconstruction, which studies graphs and graph properties uniquely determined by their subgraphs.
In this direction, both the pioneering work of Shawe-Taylor [88] and the more recent work of Bouritsas et al. [18], show that assuming the reconstruction conjecture (see Conjecture 1) holds, their models are most-expressive representations (universal approximators) of graphs. Unfortunately, Shawe-Taylor’s computational graph grows exponentially with the number of vertices, and Bouritsas et al.’s full representation power requires performing multiple graph isomorphism tests on potentially large graphs (with n − 1 vertices).
Moreover, these methods were not inspired by the more general subject of graph reconstruction; instead, they rely on the reconstruction conjecture to prove their architecture’s expressive powers.
Contributions. In this work, we directly connect graph reconstruction to GRL. We ﬁrst show how the k-reconstruction of graphs—reconstruction from induced k-vertex subgraphs—induces a natural class of expressive GRL architectures for supervised learning with graphs, denoted k-Reconstruction
Neural Networks. We then show how several existing works have their expressive power limited by k-reconstruction. Further, we show how the reconstruction conjecture’s insights lead to a provably most expressive representation of graphs. Unlike Shawe-Taylor [88] and Bouritsas et al. [18], which, for graph tasks, require ﬁxed-size unattributed graphs and multiple (large) graph isomorphism tests, respectively, our method represents bounded-size graphs with vertex attributes and does not rely on isomorphism tests.
To make our models scalable, we propose k-Reconstruction GNNs, a general tool for boosting the expressive power and performance of GNNs with graph reconstruction. Theoretically, we characterize their expressive power showing that k-Reconstruction GNNs can distinguish graph classes that the 1-WL and 2-WL cannot, such as cycle graphs and strongly regular graphs, respectively. Further, to explain gains in real-world tasks, we show how reconstruction can act as a lower-variance risk estimator when the graph-generating distribution is invariant to vertex removals. Empirically, we show that reconstruction enhances GNNs’ expressive power, making them solve multiple synthetic graph property tasks in the literature not solvable by the original GNN. On real-world datasets, we show that the increase in expressive power coupled with the lower-variance risk estimator boosts GNNs’ performance up to 25%. Our combined theoretical and empirical results make another important connection between graph theory and GRL. 1.1