Abstract
In the paper, we study the underdamped Langevin diffusion (ULD) with strongly-convex potential consisting of finite summation of N smooth components, and 2 propose an efficient discretization method, which requires O(N + d 3 ) gradient evaluations to achieve ε-error (in (cid:112)E∥·∥2 2 distance) for approximating d-dimensional ULD. Moreover, we prove a lower bound of gradient complexity as 3 ), which indicates that our method is optimal in dependence of
Ω(N + d
N , ε, and d. In particular, we apply our method to sample the strongly-log-concave distribution and obtain gradient complexity better than all existing gradient based sampling algorithms. Experimental results on both synthetic and real-world data show that our new method consistently outperforms the existing ULD approaches. 2 3 /ε 2 3 /ε 1 3 N 1 3 N 2 1

Introduction
Sampling is an important research problem in statistics learning with many applications such as
Bayesian inference [1], multi-arm bandit optimization [2], and reinforcement learning [3]. One of the fundamental problems in these applications is to sample from a high-dimensional strongly-log-concave distribution. Recently, several Markov chain Monte Carlo (MCMC) based methods were proposed to solve this problem based on underdamped Langevin diffusion (ULD). This con-tinuous diffusion process converges to the target distribution exponentially fast. Thus, the methods approximating a ULD process could be used to sample from the target distribution within certain accuracy.
Multiple discretization methods have been proposed for approximating ULD. Among them, the
Euler-Maruyama discretization [4] is the simplest one but generates the largest error. Recently the left point method (LPM) 1 [5] was introduced to fix the gradient term in ULD to be the gradient at k-th iteration, and then integrate the new linear stochastic differential equation (SDE) with a small time-interval. Subsequently, Shen and Lee [6] proposed randomized midpoint method (RMM) with smaller error. There are also discretization schemes based on splitting [7] or Runge-Kutta method [8, 9]. More recently, Cao et al. [10] derived an information-based complexity lower bound for simulating a d-dimensional ULD. Under the assumption that the full gradient oracle ∇f (x) is evaluated at most n times, they show a lower bound for worst-case error by perturbation analysis, which matches the discretization error upper bound of RMM in the dependence of d and n.
Although the ULD-MCMC methods with full gradient oracle are largely understood, many real-world applications involve summation form of potential function and large-scale data, which leads to the need of stochastic gradient methods. The vanilla stochastic gradient methods have been used to replace full gradient [5]. Albeit the computational cost for each iteration is reduced, the variance of 1Although this method is mostly just denoted as ULD-MCMC, we adopt the name LPM to distinguish it from other discretization methods. The name comes from the fact that gradient is evaluated at the left point of the time interval. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Summary of gradient complexity of sampling methods, which is defined as number of gradient evaluation of ∇fi(x) needed to sample from m-strongly-log-concave distributions up to (cid:112) d/m accuracy in 2-Wasserstein distance where ε ≤ 1 is the target accuracy, d is the dimension.
ε
ULA, LPM, RMM, and ALUM are full gradient methods, therefore, gradient complexities for them are sample size N times the iteration complexities. Only dependence on d, ε, and N are shown below.
The dependence of batch size b is made clear in Table 3. The dependence of condition number κ is discussed in Section 7.
Algorithms
Gradient complexities
Unadjusted Langevin Algorithm (ULA) [15, 16]
LPM [17]
RMM [6]
ALUM (Ours)
Stochastic Gradient LPM (SG-LPM) [5]
SVRG-LPM [11]2
CV-ULD [18]3
SVRG-ALUM (Ours)
SAGA-ALUM (Ours) (cid:101)O(N ε−2) (cid:101)O(N ε−1) (cid:101)O(N ε− 2 3 ) (cid:101)O(N ε− 2 3 ) (cid:101)O(ε−2) (cid:101)O(N + ε−1 + N (cid:101)O(N + ε−3) (cid:101)O(N + N (cid:101)O(N + N 3 ε− 2 3 ) 3 ε− 2 3 ) 2 2 2 3 ε− 2 3 ) stochastic gradient is much larger than the discretization error and therefore degenerates the overall performance. Previous works [11, 12] used stochastic variance reduced gradient (SVRG) [13] and
SAGA [14] instead, but the gradient complexities of these methods are still worse than the full gradient RMM in terms of dependence on accuracy ε. Thus, there exists a natural question:
What is the optimal ULD-MCMC method with sum-decomposable potential?
In this paper, we focus on optimal dependence of dimension d, components number N and accuracy
ε in gradient complexity for estimating a ULD process. We answer this question by two parts. We first provide a novel ULD-MCMC method and derive the corresponding complexity upper bound in
Sections 4 and 5. After that, we analyze the worse case error and show that the lower bound matches the upper bound in Section 6. The major contributions of our paper can be summarized as follows. 1. We propose a new full gradient ULD-MCMC method, called as AcceLerated ULD method (ALUM), whose discretization error has the same order dependence on dimension d, step size h as RMM. Although RMM already has optimal asymptotic complexity in full gradient setting,
ALUM is still of practical interest. Compared with RMM, which uses two gradient evaluations at each iteration, ALUM uses gradient less frequently and only requires one gradient at each iteration to achieve constant speedup. 2. We further propose VR-ALUM methods, including SVRG-ALUM and SAGA-ALUM, which utilize the unbiased variance reduction techniques in ALUM under sum-decomposable setting.
We show that these methods achieve better gradient complexity than all existing gradient based
MCMC approaches. These gradient complexities for sampling from a strongly-log-concave distribution are compared in Table 1. 3. We derive an information-based lower bound on worst-case error for estimating a ULD process with only gradient oracle and weighted Brownian motion oracle. We show that in order to 1 achieve ε approximation accuracy, Ω(N + d 3 ) single component gradient evaluations 3 N are needed. This lower bound matches the upper bound for VR-ALUM in terms of dependence of dimension d, sample size N , and accuracy ε. Therefore, our VR-ALUM methods are indeed optimal for estimating a ULD process under sum-decomposable setting. 3 ε− 2 2 2In Zou et al. [11], the authors call their method SVR-HMC. However, their method is not based on
Hamiltonian Monte Carlo (HMC), but based on ULD. Their method is just applying SVRG to replace full gradient in LPM. 3Further explanation and comparison is shown in Appendix A.2. 2
2