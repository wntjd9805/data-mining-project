Abstract
Transformers, the de-facto standard for language modeling, have been recently applied for vision tasks. This paper introduces sparse queries for vision trans-formers to exploit the intrinsic spatial redundancy of natural images and save computational costs. Speciﬁcally, we propose a Dynamic Grained Encoder for vision transformers, which can adaptively assign a suitable number of queries to each spatial region. Thus it achieves a ﬁne-grained representation in discriminative regions while keeping high efﬁciency. Besides, the dynamic grained encoder is compatible with most vision transformer frameworks. Without bells and whistles, our encoder allows the state-of-the-art vision transformers to reduce computa-tional complexity by 40%-60% while maintaining comparable performance on image classiﬁcation. Extensive experiments on object detection and segmenta-tion further demonstrate the generalizability of our approach. Code is available at https://github.com/StevenGrove/vtpack. 1

Introduction
Following the evolution of network architectures in natural language processing (NLP), Vision
Transformers [1–5] have recently attracted increasing research attention and demonstrated promising results on several vision tasks, such as image classiﬁcation, object detection, and other pixel-level tasks. Vision transformers are notable for modeling long-range dependencies and introducing less inductive bias, considered to be a solid alternative to CNNs for vision tasks.
One of the eminent obstacles for vision transformers is the high computational cost. Vision tasks typically require high-resolution image features to obtain detail and structure representation, which is critical for pixel-level tasks [6–10]. However, since the encoders in vision transformers need to establish pairwise relationships, high-resolution features could impose unacceptable computational and memory costs. Therefore, similar to the efﬁcient transformers [11–13] in NLP, many variants [2– 4] of vision transformers are proposed to perform sparse self-attentions with dense queries and sparse key-value pairs based on ﬁxed pattern or heuristic rules.
In this paper, we notice that different from natural language, natural images involve much spatial redundancy, especially in ﬂat or low-texture regions [14–18]. This could enable the image features to have a low resolution in some regions while maintaining similar representational capabilities.
To verify the spatial redundancy in vision transformers, we give an empirical analysis for DeiT [19] on ImageNet [20] classiﬁcation dataset (the details refer to Sec. 3.1). It demonstrates the existence
∗Equal contribution. This work was done in Megvii Research.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The overall diagram of the proposed dynamic grained encoder. x is the input sequence, and y is the output sequence. The dynamic grained router automatically split a 2D feature into mixed-grained patches with a different number of tokens in a patch. Each patch is then ﬂattened as a sparse query by an average pooling operator. The vanilla encoder block can be a standard transformer encoder or other efﬁcient variants. Besides, the dash lines are only used in the training phase. of spatial redundancy in queries, and the complexity can be dramatically reduced by downsampling some highly redundant regions while maintaining comparable performance. These properties allow the queries to use mixed granularity to achieve a balance between effectiveness and efﬁciency, i.e., more tokens in more discriminative regions while fewer tokens in less informative regions. However, the distribution of spatial redundancy varies greatly among different input images, making it difﬁcult for a static method to handle complex and variable features.
We thus attempt to explore a new perspective: introducing dynamic network mechanism into vision transformers to reduce the spatial redundancy of image features. As shown in Fig. 1, we propose a Dynamic Grained Encoder (DGE) to replace the vanilla encoder in vision transformers. It could assign a suitable number of queries for each region by using a dynamic grained router, e.g., the foreground regions of the cat head in Fig. 1 are assigned more queries than the background regions.
Concretely, a reshaped 2D feature is ﬁrst divided into regions using a ﬁxed window. For each region, the number of patches is decided by a data-dependent routing process, and each patch is average pooled to obtain a 1D token. All the tokens are then concatenated into a sequence as the queries. Since our method focuses on the sparsity of queries, it is compatible with many efﬁcient transformer encoders [2, 3, 11–13], making our approach available as a generic plugin in most vision transformers [1–3, 19, 21]. Furthermore, the output of the encoder is restored to the input resolution by an un-pooling operation and compensates for detailed information with the input feature.
To demonstrate the effectiveness, we conduct extensive experiments on three typical vision transform-ers, i.e., DeiT [19], PVT [3] and DPVT, where DPVT is a new framework based on the deformable attention [2]. In the image classiﬁcation task, our dynamic grained encoder allows these models to reduce computational complexity by 40%-60% while maintaining comparable performance. On the other hand, with lower computational complexity, the accuracy can be improved by up to 4.4% on
ImageNet val set. In addition, the experiments on object detection and segmentation show the strong robustness and generalization of our method. 2