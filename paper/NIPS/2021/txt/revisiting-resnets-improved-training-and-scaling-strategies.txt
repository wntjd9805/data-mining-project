Abstract
Novel computer vision architectures monopolize the spotlight, but the impact of the model architecture is often conﬂated with simultaneous changes to training method-ology and scaling strategies. Our work revisits the canonical ResNet [13] and studies these three aspects in an effort to disentangle them. Perhaps surprisingly, we ﬁnd that training and scaling strategies may matter more than architectural changes, and further, that the resulting ResNets match recent state-of-the-art mod-els. We show that the best performing scaling strategy depends on the training regime and offer two new scaling strategies: (1) scale model depth in regimes where overﬁtting can occur (width scaling is preferable otherwise); (2) increase image resolution more slowly than previously recommended [55]. Using improved training and scaling strategies, we design a family of ResNet architectures, ResNet-RS, which are 1.7x - 2.7x faster than EfﬁcientNets on TPUs, while achieving similar accuracies on ImageNet. In a large-scale semi-supervised learning setup,
ResNet-RS achieves 86.2% top-1 ImageNet accuracy, while being 4.7x faster than
EfﬁcientNet-NoisyStudent. The training techniques improve transfer performance on a suite of downstream tasks (rivaling state-of-the-art self-supervised algorithms) and extend to video classiﬁcation on Kinetics-400. We recommend practitioners use these simple revised ResNets as baselines for future research. 1

Introduction
The performance of a vision model is a product of the architecture, training methods and scaling strategy. Novel architectures underlie many advances, but are often simultaneously introduced with other critical – and less publicized – changes in the details of the training methodology and hyper-parameters. Additionally, new architectures enhanced by modern training methods are sometimes compared to older architectures with dated training methods (e.g. ResNet-50 with ImageNet Top-1 accuracy of 76.5% [13]). Our work addresses these issues and empirically studies the impact of training methods and scaling strategies on the popular ResNet architecture [13].
We survey the modern training and regularization techniques widely in use today and apply them to
ResNets (Figure 1). In the process, we encounter interactions between training methods and show a beneﬁt of reducing weight decay values when used in tandem with other regularization techniques.
An additive study of training methods in Table 1 reveals the signiﬁcant impact of these decisions: a
Correspondence to Irwan Bello and Barret Zoph {ibello,barretzoph}@google.com. Code and check-points available in TensorFlow: https://github.com/tensorflow/models/tree/master/ official/vision/beta https://github.com/tensorflow/tpu/tree/master/ models/official/resnet/resnet_rs and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Improving ResNets to state-of-the-art performance. We improve on the canonical ResNet [13] with modern training methods (as also used in EfﬁcientNets [55]), minor architectural changes and improved scaling strategies. The resulting models, ResNet-RS, outperform EfﬁcientNets on the speed-accuracy Pareto curve with speed-ups ranging from 1.7x - 2.7x on TPUs and 2.1x - 3.3x on GPUs. ResNet (•) is a ResNet-200 trained at 256×256 resolution. Training times reported on TPUs. canonical ResNet-200 with 79.0% top-1 ImageNet accuracy is improved to 82.2% (+3.2%) through improved training methods alone. This is increased further to 83.4% by two small and commonly used architectural improvements: ResNet-D [15] and Squeeze-and-Excitation [21]. Figure 1 traces this reﬁnement over the starting ResNet in a speed-accuracy Pareto curve.
We offer new perspectives and practical advice on scaling vision architectures. While prior works extrapolate scaling rules from small models [55] or from short training duration [39], we design scaling strategies by exhaustively training models across a variety of scales for the full training duration (e.g. 350 epochs instead of 10 epochs). In doing so, we uncover strong dependencies between the best performing scaling strategy and the training regime (e.g. number of epochs, model size, dataset size). These dependencies are missed in any of these smaller regimes, leading to sub-optimal scaling decisions. Our analysis leads to new scaling strategies summarized as (1) scale the model depth when overﬁtting can occur (scaling the width is preferable otherwise) and (2) scale the image resolution more slowly than prior works [55].
Using the improved training and scaling strategies, we design a family of re-scaled ResNets, ResNet-RS, across model various scales (Figure 1). ResNet-RS models use less memory during training and are 1.7x - 2.7x faster on TPUs (2.1x - 3.3x faster on GPUs) than the popular EfﬁcientNets on the speed-accuracy Pareto curve. In a large-scale semi-supervised learning setup, ResNet-RS obtains a 4.7x training speed-up on TPUs (5.5x on GPUs) over EfﬁcientNet-B5 when co-trained on
ImageNet [30] and an additional 130M pseudo-labeled images.
Finally, we conclude with a suite of experiments testing the generality of the improved training and scaling strategies. We ﬁrst demonstrate that our scaling strategy improves the speed-accuracy Pareto curve of EfﬁcientNet. Next, we show that the improved training strategies yield representations that rival or outperform those from self-supervised algorithms (SimCLR and SimCLRv2 [4, 5]) on a suite of downstream tasks. The improved training strategies also extend to video classiﬁcation, yielding an improvement from 73.4% to 77.4% (+4.0%) on the Kinetics-400 dataset.
Through combining lightweight architectural changes (used since 2018) and improved training and scaling strategies, we discover the ResNet architecture sets a state-of-the-art baseline for vision research. This ﬁnding highlights the importance of teasing apart each of these factors in order to understand what architectures perform better than others. We summarize our contributions:
• An empirical study of regularization techniques and their interplay, which leads to a training strategy that achieves strong performance (e.g. +3.2% top-1 ImageNet accuracy, +4.0% top-1
Kinetics-400 accuracy) without having to change the model architecture. 2
• An empirical study of scaling which uncovers strong dependencies between training and the best performing scaling strategy. We propose a simple scaling strategy: (1) scale depth when overﬁtting can occur (scaling width can be preferable otherwise) and (2) scale the image resolution more slowly than prior works [55]. This scaling strategy improves the speed-accuracy Pareto curve of both ResNets and EfﬁcientNets.
• ResNet-RS: a Pareto curve of ResNet architectures that are 1.7x - 2.7x faster than EfﬁcientNets on TPUs (2.1x - 3.3x on GPUs) by applying the training and scaling strategies. Semi-supervised training of ResNet-RS with an additional 130M pseudo-labeled images achieves 86.2% top-1
ImageNet accuracy, while being 4.7x faster on TPUs (5.5x on GPUs) than the corresponding
EfﬁcientNet-NoisyStudent [57].
• Empirically show that representations obtained from supervised learning using modern train-ing techniques rival or outperform state-of-the-art self-supervised representations (SimCLR [4],
SimCLRv2 [5]) on suite of downstream computer vision tasks. 2 Characterizing Improvements on ImageNet
Since the breakthrough of AlexNet [30] on ImageNet [45], a wide variety of improvements have been proposed to further advance image recognition performance. These improvements broadly arise along four orthogonal axes: (a) architecture, (b) training/regularization methodology, (c) scaling strategy and (d) using additional training data. (a) Architecture. The works that perhaps receive the most attention are novel architectures. Notable proposals since AlexNet include VGG [49], ResNet [13], Inception [52, 53], and ResNeXt [58].
Automated search strategies for designing architectures have further pushed the state-of-the-art [67, 41, 55]. There have also been efforts in going beyond standard ConvNets for image classiﬁcation, by adapting self-attention [56] to the visual domain [2, 40, 20, 47, 8, 1]. (b) Training and Regularization Methods.
ImageNet progress has simultaneously been boosted by innovations in training (e.g. improved learning rate schedules [34, 12]) and regularization methods, such as dropout [50], label smoothing [53], stochastic depth [22], dropblock [11] and data augmenta-tion [61, 59, 6, 7]. Regularization methods have become especially useful to prevent overﬁtting when training ever-increasingly larger models [23] on limited data (e.g. 1.2M ImageNet images). (c) Scaling Strategies.
Increasing the model dimensions (width, depth and resolution) has been another successful axis to improve quality [44, 17]. ResNet architectures are typically scaled up by adding layers (depth): ResNets-18 to ResNet-200 and beyond [14, 62]. Wide ResNets [60] and MobileNets [19] instead scale the width. Increasing image resolutions consistently improves performance: EfﬁcientNet uses 600 image resolutions [55] while both ResNeSt [62] and TResNet [43] use 400+ image resolutions for their largest model. In an attempt to systematize these heuristics,
EfﬁcientNet proposed the compound scaling rule, which jointly scales network depth, width and image resolution using a constant scaling factor. However, Section 7.1 shows this scaling strategy is sub-optimal for not only ResNets, but EfﬁcientNets as well. (d) Additional Training Data. Finally, ImageNet accuracy is commonly improved by training on additional sources of data (either labeled, weakly labeled, or unlabeled). Pre-training on large-scale datasets [51, 35, 27] has signiﬁcantly pushed the state-of-the-art, with ViT [8] and NFNets [3] recently achieving 88.6% and 89.2% ImageNet accuracy respectively. Using pseudo-labels on additional unlabeled images [57, 37] in a semi-supervised learning fashion has also been a fruitful avenue for improving accuracy. We present semi-supervised learning results in Section 7.2. 3