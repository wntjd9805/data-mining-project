Abstract
Model smoothing is of central importance for obtaining a reliable teacher model in the student-teacher framework, where the teacher generates surrogate supervision signals to train the student. A popular model smoothing method is the Temporal
Moving Average (TMA), which continuously averages the teacher parameters with the up-to-date student parameters. In this paper, we propose “Spatial Ensemble”, a novel model smoothing mechanism in parallel with TMA. Spatial Ensemble randomly picks up a small fragment of the student model to directly replace the corresponding fragment of the teacher model. Consequentially, it stitches different fragments of historical student models into a unity, yielding the “Spatial Ensemble” effect. Spatial Ensemble obtains comparable student-teacher learning performance by itself and demonstrates valuable complementarity with temporal moving average.
Their integration, named Spatial-Temporal Smoothing, brings general (sometimes signiﬁcant) improvement to the student-teacher learning framework on a variety of state-of-the-art methods. For example, based on the self-supervised method
BYOL, it yields +0.9% top-1 accuracy improvement on ImageNet, while based on the semi-supervised approach FixMatch, it increases the top-1 accuracy by around +6% on CIFAR-10 when only few training labels are available. Codes and models are available at: https://github.com/tengteng95/Spatial_
Ensemble. 1

Introduction
Recent years have witnessed the success of the student-teacher framework in self-supervised [1–5] and semi-supervised [6–8] learning tasks. In this framework, the teacher is responsible for generating surrogate supervision signals to guide the learning of the student on unlabeled data, for example, category IDs for classiﬁcation, feature patterns for contrastive learning, etc.
Many important works under this framework share a common model smoothing technique, i.e., the
Temporal Moving Average (TMA), to generate the teacher from the student model. For example,
Mean Teacher [6] for semi-supervision averages historical student weights as the teacher. MoCo [1] for self-supervision employs a slowly progressing encoder (teacher), driven by a momentum update with the query encoder (student). Another state-of-the-art method for self-supervision BYOL [2] employs a similar strategy to update the target (teacher) network with a slow-moving average of the online (student) network. We thus recognize that model smoothing with TMA is of central importance for obtaining a reliable teacher for these works.
In this paper, we rethink the mechanism of TMA and summarize its importance as two-fold. First,
TMA enables the teacher to absorb parameters from different student editions and thus beneﬁts from
∗corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of Temporal Moving Average and Spatial Ensemble. the model ensemble. Second, TMA constrains the variance of the teachers to be small to avoid inconsistent labels produced during two adjacent updates. An abrupt change to the pseudo labels will cripple training and possibly leads to optimization divergence. This is particularly important for self-supervised learning where annotations are unavailable.
We propose a novel model smoothing mechanism named “Spatial Ensemble”. Compared with
TMA, Spatial Ensemble facilitates similar model smoothing beneﬁts with a different mechanism, as illustrated in Figure 1. In TMA, the teacher absorbs each historical student as a whole. A temporally near student is absorbed into the teacher with a larger weighting factor due to the Exponential Moving
Average effect. In contrast, in Spatial Ensemble, the teacher randomly picks up one or more small fragments of the student to replace the corresponding fragments of the teacher during each update.
All the fragments are weighted equally in the teacher model. In this way, Spatial Ensemble stitches different fragments of previous student models into a unity, yielding the “Spatial Ensemble” effect. In spite of their fundamental differences, Spatial Ensemble facilitates similar beneﬁts of model ensemble and smoothed update. The smoothed update is maintained by replacing only a small portion of parameters at each iteration. On the challenging self-supervised learning task, which is very sensitive to the quality of surrogate supervision signals, we observe that Spatial Ensemble achieves comparable performance with temporal moving average. It shows that our Spatial Ensemble can serve as a basic model smoothing technique for the student-teacher framework.
An important advantage of Spatial Ensemble lays in its complementarity to Temporal Moving
Average. To explore their complementary beneﬁts, we integrate both the temporal smoothing and spatial smoothing mechanism into a uniﬁed method named Spatial-Temporal Smoothing (STS). STS does not directly absorb a whole student, or simply replace a fragment. Instead, it updates a fragment of the teacher model with a temporal moving average of the corresponding fragment of the student.
This “mix-up” model smoothing beneﬁts from the mutual complementarity of temporal moving average and Spatial Ensemble. Experiments on both self-supervised and semi-supervised tasks show that using STS for model smoothing obtains general (sometimes signiﬁcant) improvement over a majority of state-of-the-art methods.
We summarize our contribution as follows: 1. We propose a novel model smoothing mechanism named Spatial Ensemble. Compared with the popular temporal moving average, Spatial Ensemble achieves a comparable model smoothing effect with a different mechanism. 2. We integrate the spatial and temporal smoothing mechanism into a uniﬁed method named Spatial-Temporal Smoothing. STS beneﬁts from the mutual complementarity of two mechanisms and consistently improves student-teacher methods on self- and semi-supervised learning tasks. 3. We notice two recognizable advantages of the model learned through STS, including strong robustness against various data corruptions, and superior transferring capability to the object detection task. 2