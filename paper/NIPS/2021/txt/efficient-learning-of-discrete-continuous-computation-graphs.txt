Abstract
Numerous models for supervised and reinforcement learning beneﬁt from com-binations of discrete and continuous model components. End-to-end learnable discrete-continuous models are compositional, tend to generalize better, and are more interpretable. A popular approach to building discrete-continuous compu-tation graphs is that of integrating discrete probability distributions into neural networks using stochastic softmax tricks. Prior work has mainly focused on com-putation graphs with a single discrete component on each of the graph’s execution paths. We analyze the behavior of more complex stochastic computations graphs with multiple sequential discrete components. We show that it is challenging to optimize the parameters of these models, mainly due to small gradients and local minima. We then propose two new strategies to overcome these challenges. First, we show that increasing the scale parameter of the Gumbel noise perturbations dur-ing training improves the learning behavior. Second, we propose dropout residual connections speciﬁcally tailored to stochastic, discrete-continuous computation graphs. With an extensive set of experiments, we show that we can train complex discrete-continuous models which one cannot train with standard stochastic soft-max tricks. We also show that complex discrete-stochastic models generalize better than their continuous counterparts on several benchmark datasets. 1

Introduction
Neuro-symbolic learning systems aim to combine discrete and continuous operations. The ma-jority of recent research has focused on integrating neural network components into probabilistic logics [28, 20], that is, making logic-based reasoning approaches more amenable to noisy and high-dimensional input data. On the other end of the spectrum are the data-driven approaches, which aim to learn a modular and discrete program structure in an end-to-end neural network based system. The broader vision followed by proponents of these approaches are systems capable of assembling the required modular operations to solve a variety of tasks with heterogeneous input data. Reinforcement learning [33], neuro-symbolic program synthesis [25], and neural module networks [1] are instances of such data-integrated discrete-continuous learning systems.
We focus on learning systems comprised of both symbolic and continuous operations where the symbolic operations are modeled as discrete probability distributions. The resulting systems can be described by their stochastic computation graphs, a formalism recently introduced to unify several related approaches [30]. Figure 1 illustrates two instances of such stochastic computation graphs.
For a given high-dimensional input such as a set of images or a list of symbols, a computation graph, consisting of stochastic (discrete) and continuous (neural) nodes, is created to solve a particular task.
Both, the mechanism to assemble the graphs (if not already provided) and the various operations are 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Two instances of discrete-continuous computation graphs. Top: The MNIST addition task aims to learn a neuro-symbolic program end-to-end that sums two or more digits only based on raw image inputs. The input images are mapped to the parameters of a categorical probability distribution (orange neural network). Samples from said distribution are mapped to learned vector representations of digits (green neural network). The addition module ADD takes the vector representations of two digits and learns to compute the sum of the corresponding digits. Bottom: The ListOps task aims to learn the solution to an arithmetic expression. Given the syntactic input and the numerical solution, we learn the latent parse tree, which consists of discrete and continuous components modeling the operations MAX, MIN, and MED. More details are provided in the experimental section. learned end-to-end. Discrete nodes in the computation graph are Gibbs distributions modeled at a particular temperature which can also be used to model the argmax operation at zero temperature.
The majority of prior work has focused on graphs with a single discrete probability distribution along each execution path. Examples are the discrete variational autoencoder [14], learning to explain [4], and other applications of stochastic softmax tricks [26]. We aim to analyze and improve the training behavior of complex computation graphs, that is, graphs with more than one discrete probability distribution in its execution paths. More concretely, we focus on computation graphs where the stochastic nodes are categorical random variables modeled with the Gumbel-softmax trick [14, 19]. In
Section 2, we show both analytically and empirically that it is challenging to optimize the parameters of these models, mainly due to insufﬁcient gradient signals often caused by local minima and saturation. We propose two new methods for mitigating the causes of poor optimization behavior.
First, we show in Section 2.1 that increasing the scale parameter β of the Gumbel noise perturbations improves the models’ learning behavior. Increasing β increases the probability of escaping local minima during training. Second, we propose dropout residual connections for discrete-continuous computation graphs in Section 2.2. By randomly skipping some discrete distributions, we provide more informative gradients throughout the full computation graph. We show empirically for several complex discrete-continuous models that the proposed methods are required for training. We also show that the resulting discrete-continuous models generalize better and signiﬁcantly outperform state of the art approaches on several benchmark datasets. 2 Efﬁcient Learning of Discrete-Continuous Computation Graphs
Standard neural networks compose basic differentiable functions. These networks, therefore, can be fully described by a directed acyclic graph (the computation graph) that determines the operations executed during the forward and backward passes. Schulman et al. [30] proposed stochastic computa-tion graphs as an extension of neural networks that combine deterministic and stochastic operations – a node in the computation graph can be either a differentiable function or a probability distribution.
A stochastic node X is typically a random variable with a parameterized probability distribution pθ(x) with parameters θ. Suppose that f is a smooth function (such as the loss function of a learning problem), then the gradient of f at the stochastic node is ∇θEpθ (x)[f (x)]. Kingma and Welling [15] proposed the reparameterization trick to overcome the problem of computing gradients with respect to the parameters of a distribution. The idea is to ﬁnd a function g and distribution ρ such that one 2
Figure 2: The values of the parameters, their gradients, and the softmax probabilities for various values of β (the scale parameter) for a constant Gumbel-Softmax parameter τ . Early during training, a lower scale parameter β (relative to τ ) works well and has less variance. Once the probabilities saturate, however, we can only continue to obtain a sufﬁcient gradient signal for larger values of β. can replace x ∼ pθ(x) by x = g(z, θ) with z ∼ ρ(z). If this is possible, then one can write
∇θEx∼pθ (x)[f (x)] = Ez∼ρ(z)[∇θf (g(z, θ))] ≈ 1
S
S (cid:88) i=1
∇θf (g(zi, θ)) with zi ∼ ρ(z). (1)
With this paper we address the problem of training stochastic computation graphs where the stochas-tic nodes are based on categorical (discrete) distributions approximated using stochastic softmax tricks [14, 26]. More speciﬁcally, we consider discrete-continuous components modeling a categorical variable X with k possible values z1, ..., zk. Each of the zi is the one-hot encoding of the category i.
We consider discrete-continuous functions1 f : Rn → Rn with v = f (u) deﬁned as follows
θ = gw(u) p(zi; θ) = exp(θi) j=1 exp(θj) (cid:80)k z ∼ p(z; θ) v = hw(cid:48)(z) (2a) (2b) (2c) (2d)
Figure 3: continuous component (function f ).
Illustration of generic discrete-We assume that the functions g and h (parameterized by w and w(cid:48)) are expressed using differentiable neural network components. In the majority of cases, h is deﬁned as z(cid:124)w(cid:48) for a learnable matrix w(cid:48), mapping object zi to its learned vector representation. Figure 3 illustrates a generic discrete-continuous component.
Let Gumbel(0, β) be the Gumbel distribution with location 0 and scale β. Using the Gumbel-max trick, we can sample z ∼ p(z; θ) as follows: z := zi with i = arg max j∈{1,...,k}
[θj + (cid:15)j] where (cid:15)j ∼ Gumbel(0, 1). (3)
The Gumbel-softmax trick is a relaxation of the Gumbel-max trick (relaxing the argmax into a softmax with a scaling parameter λ) that allows one to perform standard backpropagation: zi = exp ((θi + (cid:15)i)/τ ) j=1 exp ((θj + (cid:15)j)/τ ) (cid:80)k where (cid:15)i ∼ Gumbel(0, 1). (4)
Hence, instead of sampling a discrete z, the Gumbel-Softmax trick computes a relaxed z in its place.
We are concerned with the analysis of the behavior of the Gumbel-softmax trick in more complex stochastic computation graphs. In these computation graphs, multiple sequential Gumbel-softmax components occur on a single execution path. Throughout the remainder of the paper, we assume that during training, we use the Softmax-trick as outlined above, while at test time, we sample discretely using the Gumbel-max trick. Depending on the use case, we also sometimes compute the argmax at test time instead of samples from the distribution.
Let us ﬁrst take a look at ∂v/∂u, that is, the partial derivative of the output of the discrete-continuous function f with respect to its input. Using the chain rule (and with a slight abuse of notation to 1For the sake of simplicity we assume the same input and output dimensions. 3
Figure 4: A comparison of the learning behavior of different annealing schemes for the MNIST addition task, averaged over 8 trained models. Left: The base model (Base), with constant parameters
τ = 8 and β = 1, cannot achieve an accuracy of more than 0.7. Increasing β during training (TM) achieves better results than annealing τ (TauAnn). Middle: Two training runs for TM and TauAnn, respectively, where an accuracy of 0.9 was reached the latest. The accuracy curves of TM jump to the next plateau earlier and more consistently. The dotted grey line depicts the moment when a run of
TM achieves an accuracy of ∼ 1.0. Right: For the same run, zj and the absolute value of ∂L/∂θj are plotted. The downstream gradient signal for θj is largely neutralized by a small value of zj until the corresponding category is used and its average probability abruptly reaches 0.1. simplify the presentation), we can write ∂v/∂u = (∂v/∂z)(∂z/∂θ)(∂θ/∂u). By assumption
∂v/∂z and ∂θ/∂u exist and are efﬁciently computable. Using the derivative of the softmax function, we have
∂zi
∂θj
= (cid:40) 1
τ zi(1 − zi)
τ zizj
− 1 if i = j if i (cid:54)= j. (5)
We have empirically observed that during training, there is a tendency of the models to not utilize all existing categories of the distribution and to wrongfully map different input representations to the same category. For instance, for the MNIST addition problem, all encoded images of two digits are mapped to the same category of the distribution. In these cases, the gradients of the parameters of the unused categories are vanishingly small. In order to analyze this behavior more closely, we derive an upper bound on the gradient of a parameter θj of the categorical distribution with respect to a loss function L. First, we have that (cid:13) (cid:18) ∂z (cid:13) (cid:13)
∂θj (cid:13) (cid:0)(1 − zj)2 + 2
τ 2 z2 j , 1
τ 2 z2 (cid:19)(cid:13) 2 (cid:13) (cid:13) (cid:13)
∂zi
∂θj (cid:1) ≤ (cid:88) (cid:88) z2 i (6) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
=
= 2 j
F i i(cid:54)=j where (cid:107)·(cid:107)F is the Frobenius norm. The full derivation of this inequality can be found in the Appendix.
Since the Frobenius norm is a sub-multiplicative matrix norm, we can write
√ (cid:13) (cid:13) (cid:13) (cid:13)
∂L
∂θj (cid:13) (cid:13) (cid:13) (cid:13)F
= (cid:13) (cid:13) (cid:13) (cid:13)
∂L
∂z
∂z
∂θj (cid:13) (cid:13) (cid:13) (cid:13)F
≤ (cid:13) (cid:18) ∂L (cid:13) (cid:13)
∂z (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F (cid:13) (cid:18) ∂z (cid:13) (cid:13)
∂θj (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F 2 (cid:13) (cid:18) ∂L (cid:13) (cid:13)
∂z (cid:13) (cid:19)(cid:13) (cid:13) (cid:13) (cid:13)F
≤
τ zj (7)
|grad(θj)| = where the last inequality follows from eq. (6). Hence, a small value of θj (and consequently zj) leads to a small gradient for θj. As a consequence, such models are more prone to fall into poor minima during training. Figure 4 illustrates an example of such a situation, which we encountered in practice. The small value of a speciﬁc zj leads to a small gradient at θj (right) as well as to suboptimal plateauing of the accuracy curve (middle). In other words, the downstream gradients information for
θj is neutralized by a small zj. Note that this issue only occurs when L is more complex and not the categorical-cross entropy loss.
Another related problem is saturated probabilities in sequentially connected probabilistic components.
Similar to the problem of sigmoid activation units in deep neural networks, which have been replaced by ReLUs and similar modern activation functions, saturated probabilities lead to vanishing gradients.
Indeed, we observe in several experiments that the Gumbel-softmax distributions saturate and, as a result, that sufﬁcient gradient information cannot reach the parameters of upstream neural network components.
We propose two strategies to mitigate the vanishing gradient behavior in complex discrete-continuous computation graphs. First, we analyze the interplay between the temperature parameter τ and the scale parameter β of the Gumbel distribution. We make the subtle difference between these parameters explicit, which may also be of interest for improving stochastic softmax tricks [26]. By increasing 4
Figure 5: Two annealing schemes for the Gumbel-softmax trick for θ = (2, 0.5, 1). Top: The standard procedure of annealing the softmax temperature τ as introduced in [19] (see eq. (4)). Bottom: Starting with β = 0 and increasing it during training, the samples are increasingly more discrete but, compared to the standard annealing approach (τ → 0), more uniformly distributed over the corners of the probability simplex.
β relative to τ while keeping τ ﬁxed, we increase the probability of a gradient ﬂow in the case of saturation. We show that the larger β, the more uniformly distributed (over the classes) are the discrete samples from the distribution, increasing the chance to escape poor minima. Second, we introduce DROPRES connections allowing us to lower-bound the gradients in expectation and leading to more direct gradient ﬂow to parameters of upstream model components. 2.1 TEMPMATCH: Temperature Matching
We explore the behavior of two interdependent parameters: the Gumbel-softmax temperature τ and the
Gumbel scale parameter β. First, we have the temperature parameter τ from the Gumbel-softmax trick (see eq. (4)). The purpose of this parameter is to make the output of the softmax function more or less discrete, that is, more or less concentrated on one of the categories. Second, and this is a new insight, we can adjust the scale parameter β of the Gumbel distribution. For scale parameter β, we sample the noise perturbation iid as (cid:15)i ∼ Gumbel(0, β). If we use the Gumbel-max trick of eq. (3) we implicitly generate samples from the distribution p(zi; θ) = exp(θi/β)/ (cid:80)k j=1 exp(θj/β). Increasing the scale parameter β, therefore, makes the Gumbel-max samples more uniform and less concentrated.
When using the Gumbel-softmax trick instead of the Gumbel-max trick, we obtain samples zi that are more uniformly distributed over the categories and more discrete. Indeed, in the limit of β → ∞, we obtain discrete samples uniformly distributed over the categories, independent of the logits. For
β → 0, we obtain the standard softmax function. Figure 5 illustrates the impact of the two parameters on the Gumbel-softmax distribution. Now, the problem of insufﬁcient gradients caused by local minima can be mitigated by increasing the scale parameter β relative to the temperature parameter τ .
The annealing schedule we follow is the inverse exponential annealing βt = τ (1 − e−tγ) for some
γ > 0. Increasing β increases the probability of generating samples whose maximum probabilities are more uniformly distributed over the categories. This has two desired effects. First, samples drawn during training have a higher probability of counteracting poor minima caused at an earlier stage of the training. Figure 4 (left) shows that higher values for β allow the model to ﬁnd its way out of poor minima. A higher value of β makes the model more likely to utilize categories with small θs, allowing the model to obtain improved minima (Figure 4 (middle)). Second, gradients propagate to parameters of the upstream components even after downstream components are saturating. To illustrate the second effect, we conducted the toy experiment depicted in Figure 2. The model here computes Softmax(θ + (cid:15)) with parameters θ(cid:124) = (θ1, θ2) and (cid:15) ∼ Gumbel(0, β). The learning problem is deﬁned through a cross-entropy loss between the output probabilities and a constant target vector (1.0, 0.0)(cid:124). We observe that early during training, lower values for β work well and exhibit less variance than higher values. However, once the probabilities and, therefore, the gradients start to saturate, a higher value for β enables the continued training of the model. While the example is artiﬁcial, it is supposed to show that larger values of the scale parameter β, sustain gradients for upstream components even if downstream components have saturated. 5
Figure 6: The inﬂuence of dropout residuals on the mean absolute gradients for ListOps. The values on the x-axis are the GNN message-passing iterations: the higher the number, the closer to the loss function is the corresponding discrete-continuous component. Adding dropout residual connections mitigates the vanishing gradient problem for components with greater distance to the loss.
In summary, increasing the Gumbel scale parameter β has positive effects on the training dynamics.
We propose an exponential increasing scheme relative to τ . We show empirically that for more realistic learning problems choosing a higher value for β is beneﬁcial, especially later during training. 2.2 DROPRES: Residual Dropout Connections
Residual (or shortcut) connections are crucial when training deep neural networks [11]. A standard residual connection for the discrete-continuous components we consider here would be achieved by replacing Equation (2d) with v = u + hw(cid:48)(z). By creating a direct connection to the continuous input of the discrete distributions, the optimizing problem simpliﬁes. Unfortunately, what sets our problem apart from other end-to-end learnable neural networks is that we want to completely remove the residual connections at test time to obtain pure discrete representations and, therefore, the desired generalization properties. To mitigate the problem of overﬁtting to the existence of residual connections while at the same time reducing vanishing gradients in expectation, we propose
DROPRES connections. These are residual connections sampled from a Bernoulli distribution with parameter α. We replace Equation (2d) by v = (cid:26) u + hw(cid:48)(z) with probability 1 − α hw(cid:48)(z) with probability α, (8) that is, we drop out the residual connection with probability α. With probability (1 − α) we obtain a shortcut connection between the neural components, effectively bypassing the Gumbel-softmax.
With probability α the training path goes exclusively through the Gumbel-softmax distribution. The expectation of the gradients, taken over a Bernoulli random variable with parameter α, is now
E (cid:21) (cid:20) ∂v
∂u
= (1 − α)1 +
∂v
∂z
∂z
∂θ
∂θ
∂u
. (9)
We propose a simple linear scheme to modify α from 0 → 1 during training. Hence, the model obtains a stronger gradient signal for upstream models in expectation in the early phase of training.
To illustrate the impact of dropout residual connections, we analyzed the gradients of message passing steps for the ListOps problem (see experiments) in a discrete-continuous computation graph of the type depicted in Figure 1 (bottom). As we can observe in Figure 6, if we do not use dropout residual connections, the greater the distance of a discrete-continuous operation to the loss function (the lower the value on the x-axis), the smaller is the mean absolute value of the gradients reaching it. This illustrates the vanishing gradient problem. When using dropout residual connections, on the other hand, the mean absolute values of the gradients do not vanish proportional to their distance to the loss and are more evenly distributed.
To summarize, optimizing the parameters of models comprised of discrete distributions and continu-ous neural network components is challenging, mainly due to local minima and vanishing gradients.
By setting the Gumbel scale parameter β = 0 as well as the dropout probability of the residual connections α = 0 at the beginning of training, we obtain a continuous and deterministic relaxation of the target model. Increasing the DROPRES parameter α over time makes the model increasingly use the discrete stochastic nodes. Increasing the Gumbel scale parameter β allows the model to escape local minima and small gradients. 6
3