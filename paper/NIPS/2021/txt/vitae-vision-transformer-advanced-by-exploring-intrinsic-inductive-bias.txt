Abstract
Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly.
In this paper, we propose a new Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network.
Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at code. 1

Introduction
Transformers [79, 17, 40, 14, 46, 61] have shown a domination trend in NLP studies owing to their strong ability in modeling long-range dependen-cies by the self-attention mechanism [67, 81, 51].
Such success and good properties of transform-ers has inspired following many works that apply them in various computer vision tasks [19, 100, 97, 80, 7]. Among them, ViT [19] is the pioneer-ing pure transformer model that embeds images into a sequence of visual tokens and models the global dependencies among them with stacked transformer blocks. Although it achieves promis-ing performance on image classiﬁcation, it re-quires large-scale training data and a longer train-ing schedule. One important reason is that ViT
∗Equal Contribution. Interns at JD Explore Academy.
Figure 1: Comparison of data and training efﬁ-ciency of T2T-ViT-7 and ViTAE-T on ImageNet. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
lacks intrinsic inductive bias (IB) in modeling local visual structures (e.g., edges and corners) and dealing with objects at various scales like convolutions. Alternatively, ViT has to learn such IB implicitly from large-scale data.
Unlike vision transformers, Convolution Neural Networks (CNNs) naturally equip with the intrinsic
IBs of scale-invariance and locality and still serve as prevalent backbones in vision tasks [26, 70, 62, 8, 96]. The success of CNNs inspires us to explore intrinsic IBs in vision transformers. We start by analyzing the above two IBs of CNNs, i.e., locality and scale-invariance. Convolution that computes local correlation among neighbor pixels is good at extracting local features such as edges and corners. Consequently, CNNs can provide plentiful low-level features at the shallow layers [94], which are then aggregated into high-level features progressively by a bulk of sequential convolutions
[32, 68, 71]. Moreover, CNNs have a hierarchy structure to extract multi-scale features at different layers [68, 38, 26]. Besides, intra-layer convolutions can also learn features at different scales by varying their kernel sizes and dilation rates [25, 70, 8, 45, 96]. Consequently, scale-invariant feature representation can be obtained via intra- or inter-layer feature fusion. Nevertheless, CNNs are not well suited to model long-range dependencies2, which is the key advantage of transformers. An interesting question comes up: Can we improve vision transformers by leveraging the good properties of CNNs? Recently, DeiT [76] explores the idea of distilling knowledge from CNNs to transformers to facilitate training and improve the performance. However, it requires an off-the-shelf CNN model as the teacher and consumes extra training cost.
Different from DeiT, we explicitly introduce intrinsic IBs into vision transformers by re-designing the network structures in this paper. Current vision transformers always obtain tokens with single-scale context [19, 93, 80, 86, 47, 69, 77] and learn to adapt to objects at different scales from data.
For example, T2T-ViT [93] improves ViT by delicately generating tokens in a soft split manner.
Speciﬁcally, it uses a series of Tokens-to-Token transformation layers to aggregate single-scale neighboring contextual information and progressively structurizes the image to tokens. Motivated by the success of CNNs in dealing with scale variance, we explore a similar design in transformers, i.e., intra-layer convolutions with different receptive ﬁelds [70, 91], to embed multi-scale context into tokens. Such a design allows tokens to carry useful features of objects at various scales, thereby naturally having the intrinsic scale-invariance IB and explicitly facilitating transformers to learn scale-invariant features more efﬁciently from data. On the other hand, low-level local features are fundamental elements to generate high-level discriminative features. Although transformers can also learn such features at shallow layers from data, they are not skilled as convolutions by design.
Recently, [89, 43, 21] stack convolutions and attention layers sequentially and demonstrate that locality is a reasonable compensation of global dependency. However, this serial structure ignores the global context during locality modeling (and vice versa). To avoid such a dilemma, we follow the
“divide-and-conquer” idea and propose to model locality and long-range dependencies in parallel and then fuse the features to account for both. In this way, we empower transformers to learn local and long-range features within each block more effectively.
Technically, we propose a new Vision Transformers Advanced by Exploring Intrinsic Inductive Bias (ViTAE), which is a combination of two types of basic cells, i.e., reduction cell (RC) and normal cell (NC). RCs are used to downsample and embed the input images into tokens with rich multi-scale context while NCs aim to jointly model locality and global dependencies in the token sequence.
Moreover, these two types of cells share a simple basic structure, i.e., paralleled attention module and convolutional layers followed by a feed-forward network (FFN). It is noteworthy that RC has an extra pyramid reduction module with atrous convolutions of different dilation rates to embed multi-scale context into tokens. Following the setting in [93], we stack three reduction cells to reduce the spatial resolution by 1/16 and a series of NCs to learn discriminative features from data. ViTAE outperforms representative vision transformers in terms of data efﬁciency and training efﬁciency (see Figure 1), as well as classiﬁcation accuracy and generalization on downstream tasks.
Our contributions are threefold. First, we explore two types of intrinsic IB in transformers, i.e., scale invariance and locality, and demonstrate the effectiveness of this idea in improving the feature learning ability of transformers. Second, we design a new transformer architecture named ViTAE based on two new reduction and normal cells to intrinsically incorporate the above two IBs. The proposed ViTAE embeds multi-scale context into tokens and learns both local and long-range features 2Despite projection in transformer can be viewed as 1 × 1 convolution [9], the term of convolution here refers to those with larger kernels, e.g., 3 × 3, which are widely used in typical CNNs to extract spatial features. 2
effectively. Third, ViTAE outperforms representative vision transformers regarding classiﬁcation accuracy, data efﬁciency, training efﬁciency, and generalization on downstream tasks. ViTAE achieves 75.3% and 82.0% top-1 accuracy on ImageNet with 4.8M and 23.6M parameters, respectively. 2