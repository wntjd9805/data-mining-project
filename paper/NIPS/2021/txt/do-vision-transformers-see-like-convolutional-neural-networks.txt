Abstract
Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classiﬁcation tasks.
This raises a central question: how are Vision Transformers solving these tasks?
Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classiﬁcation benchmarks, we ﬁnd striking differences between the two ar-chitectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, ﬁnding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connec-tions, which strongly propagate features from lower to higher layers. We study the ramiﬁcations for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classiﬁcation meth-ods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer. 1

Introduction
Over the past several years, the successes of deep learning on visual tasks has critically relied on convolutional neural networks [20, 16]. This is largely due to the powerful inductive bias of spatial equivariance encoded by convolutional layers, which have been key to learning general purpose visual representations for easy transfer and strong performance. Remarkably however, recent work has demonstrated that Transformer neural networks are capable of equal or superior performance on image classiﬁcation tasks at large scale [14]. These Vision Transformers (ViT) operate almost identically to Transformers used in language [13], using self-attention, rather than convolution, to aggregate information across locations. This is in contrast with a large body of prior work, which has focused on more explicitly incorporating image-speciﬁc inductive biases [30, 9, 4]
This breakthrough highlights a fundamental question: how are Vision Transformers solving these image based tasks? Do they act like convolutions, learning the same inductive biases from scratch?
Or are they developing novel task representations? What is the role of scale in learning these representations? And are there ramiﬁcations for downstream tasks? In this paper, we study these questions, uncovering key representational differences between ViTs and CNNs, the ways in which these difference arise, and effects on classiﬁcation and transfer learning. Speciﬁcally, our contributions are: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
• We investigate the internal representation structure of ViTs and CNNs, ﬁnding striking differences between the two models, such as ViT having more uniform representations, with greater similarity between lower and higher layers.
• Analyzing how local/global spatial information is utilised, we ﬁnd ViT incorporates more global information than ResNet at lower layers, leading to quantitatively different features.
• Nevertheless, we ﬁnd that incorporating local information at lower layers remains vital, with large-scale pre-training data helping early attention layers learn to do this
• We study the uniform internal structure of ViT, ﬁnding that skip connections in ViT are even more inﬂuential than in ResNets, having strong effects on performance and representation similarity.
• Motivated by potential future uses in object detection, we examine how well input spatial informa-tion is preserved, ﬁnding connections between spatial localization and methods of classiﬁcation.
• We study the effects of dataset scale on transfer learning, with a linear probes study revealing its importance for high quality intermediate representations. 2