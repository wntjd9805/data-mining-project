Abstract
Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective.
We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change.
Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space.
We study the identiﬁability of the latent representation based on pairs of views of the observations and prove sufﬁcient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and dis-criminative settings. We ﬁnd numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice. 1

Introduction
Learning good representations of high-dimensional observations from large amounts of unlabelled data is widely recognised as an important step for more capable and data-efﬁcient learning systems [10, 72]. Over the last decade, self-supervised learning (SSL) has emerged as the dominant paradigm for such unsupervised representation learning [1, 20, 21, 34, 41, 47, 48, 90, 91, 115, 122, 125, 126]. The main idea behind SSL is to extract a supervisory signal from unlabelled observations by leveraging known structure of the data, which allows for the application of supervised learning techniques. A common approach is to directly predict some part of the observation from another part (e.g., future from past, or original from corruption), thus forcing the model to learn a meaningful representation in the process. While this technique has shown remarkable success in natural language processing [13, 23, 30, 81, 84, 86, 95, 99] and speech recognition [5, 6, 100, 104], where a ﬁnite dictionary allows one to output a distribution over the missing part, such predictive SSL methods are not easily applied to continuous or high-dimensional domains such as vision. Here, a common approach is to learn a joint embedding of similar observations or views such that their representation is close [7, 12, 22, 44].
Different views can come, for example, from different modalities (text & speech; video & audio) or time points. As still images lack such multi-modality or temporal structure, recent advances in representation learning have relied on generating similar views by means of data augmentation.
⇤Joint ﬁrst author. †Joint senior author. Correspondence to: jvk@tue.mpg.de
Code available at: https://www.github.com/ysharma1126/ssl_identiﬁability 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In order to be useful, data augmentation is thought to require the transformations applied to generate additional views to be generally chosen to preserve the semantic characteristics of an observation, while changing other “nuisance” aspects. While this intuitively makes sense and has shown remark-able empirical results, the success of data augmentation techniques in practice is still not very well understood from a theoretical perspective—despite some efforts [17, 19, 28]. In the present work, we seek to better understand the empirical success of SSL with data augmentation by formulating the generative process as a latent variable model (LVM) and studying identiﬁability of the representation, i.e., under which conditions the ground truth latent factors can provably be inferred from the data [77].