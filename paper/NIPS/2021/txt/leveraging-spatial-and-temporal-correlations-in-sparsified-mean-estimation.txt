Abstract
We study the problem of estimating at a central server the mean of a set of vectors distributed across several nodes (one vector per node). When the vectors are high-dimensional, the communication cost of sending entire vectors may be prohibitive, and it may be imperative for them to use sparsiﬁcation techniques. While most existing work on sparsiﬁed mean estimation is agnostic to the characteristics of the data vectors, in many practical applications such as federated learning, there may be spatial correlations (similarities in the vectors sent by different nodes) or temporal correlations (similarities in the data sent by a single node over different iterations of the algorithm) in the data vectors. We leverage these correlations by simply modifying the decoding method used by the server to estimate the mean. We provide an analysis of the resulting estimation error as well as experiments for PCA,
K-Means and Logistic Regression, which show that our estimators consistently outperform more sophisticated and expensive sparsiﬁcation methods. 1

Introduction
Estimating the mean of a set of vectors collected from a distributed system of nodes is a subroutine that is at the core of many distributed data science applications. For instance, a fusion server performing inference may seek to estimate the mean of environmental measurements made by geographically distributed sensor nodes. While inference may be a one-shot task, model training is often inherently iterative in nature. For instance, model training using a federated learning framework [19, 35] is divided into communication rounds, where in each round, the central server has to estimate the mean of model updates or gradients sent by edge clients. Other examples of such iterative methods include principal component analysis (PCA) using the power iteration methods and K-Means.
In most such distributed learning and optimization applications, the nodes sending the data are highly resource-constrained and bandwidth-limited edge devices. Hence, sending high-dimensional vectors can be prohibitively expensive and slow. A simple yet effective solution to reduce the communication 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
cost is to send a sparsiﬁed vector containing only a subset of the elements of the original vector
[30]. The Rand-k sparsiﬁcation scheme chooses a subset of k elements uniformly at random, while the Top-k scheme selects the k highest magnitude indices. Other more advanced schemes such as
[34, 36] optimize the weights assigned by a node to different elements of its vector when sparsifying it and sending it to the central server.
A common thread in the existing techniques [36, 34, 21] is that they are agnostic to the fact that in many practical applications, the vectors sent by the nodes are correlated across different nodes and over consecutive rounds of iterative algorithms that use mean estimation as a subroutine. For example, in a sensor data fusion applications, sensor nodes in close promixity will make similar environmental measurements and their resulting data vectors will be correlated. In a federated learning application, model updates sent by a node tend to be similar across consecutive communication rounds. The focus of this work is to take into account these spatial and temporal correlations when combining sparsiﬁed vectors sent by nodes and estimating their mean. 1.1 Main Contributions
In this work, we carefully design the aggregation method used by the central server to estimate the mean of sparsiﬁed vectors received from different nodes. This approach does not introduce additional memory requirement or computation at the nodes. As a result, our proposed mean estimators are compatible with any sparsiﬁcation method used by the nodes. For the purpose of this paper and the theoretical analysis of the estimation error, we consider random sparsiﬁcation at the nodes and focus on unbiased mean estimation. However, other sparsiﬁcation techniques including Top-k sparsiﬁcation and the methods proposed in [34, 36] can also be used by the nodes, albeit at the cost of additional local computation. Finally, while our goal is to leverage temporal and spatial correlations, we present methods that do not explicitly need information about the correlation; the estimators only consider the fact that such correlation exists. By following this design philosophy of requiring little or no side information and additional local computation, this paper makes the following contributions: 1. Leveraging Spatial Correlation: In random sparsiﬁcation, a randomly chosen subset of k out of d elements of the vector is sent by a node to the central server. The standard approach is for the server to decode via simple averaging of the vectors assuming the missing values to be zero, and scaling with an appropriate constant to get an unbiased mean estimate. Instead, in Section 3, we propose a family of estimators called Rand-k-Spatial that adjust the weights assigned to different vectors according to a parameter that represents the amount of spatial correlation. When the correlation parameter is not known, we propose an average-case estimator that works well in practice. 2. Leveraging Temporal Correlation: If the vectors sent by a node are similar across iterations, previously sent elements of the vectors can be used to ﬁll in the missing elements of a sparsiﬁed vector. In Section 4, we build on this idea of leveraging these temporal correlations and propose the
Rand-k-Temporal estimator. While our estimator requires additional memory at the central server, it does not add or change any computation or storage at the nodes, which are more likely to be resource-constrained.
Along with designing these estimators and theoretically analyzing their mean squared error, in
Section 5 we present experiments showing that the estimator error can be drastically reduced when there are spatial and temporal correlations. 1.2