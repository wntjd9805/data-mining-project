Abstract
Rule sets are highly interpretable logical models in which the predicates for decision are expressed in disjunctive normal form (DNF, OR-of-ANDs), or, equivalently, the overall model comprises an unordered collection of if-then decision rules. In this paper, we consider a submodular optimization based approach for learning rule sets. The learning problem is framed as a subset selection task in which a subset of all possible rules needs to be selected to form an accurate and interpretable rule set.
We employ an objective function that exhibits submodularity and thus is amenable to submodular optimization techniques. To overcome the difﬁculty arose from dealing with the exponential-sized ground set of rules, the subproblem of searching a rule is casted as another subset selection task that asks for a subset of features.
We show it is possible to write the induced objective function for the subproblem as a difference of two submodular (DS) functions to make it approximately solvable by DS optimization algorithms. Overall, the proposed approach is simple, scal-able, and likely to be beneﬁted from further research on submodular optimization.
Experiments on real datasets demonstrate the effectiveness of our method. 1

Introduction
Interpretability is becoming one of the key considerations when deploying machine learning models to high-stake decision-making scenarios. Black box models, such as random forests and deep neural networks, may achieve impressive prediction accuracy in practice, but it is often difﬁcult to understand the mechanisms about how their predictions are made. Moreover, it has been widely known that machine learning models are susceptible to spurious correlations, which makes them not robust to distribution shifts or adversarial attacks and leads to misleading predictions. Black box models are particularly problematic here because they are hard to audit and to diagnose.
The development of inherently interpretable models is a longstanding attempt towards interpretable and trustable machine learning [42, 43]. In this paper, we consider decision rule sets for binary classiﬁcation, in which if an example is tested true for a Boolean condition expressed in disjunctive normal form (DNF), then a predeﬁned label is predicted for it. For instance, a rule set for determining whether a patient with community-acquired pneumonia should be hospitalized may be "IF (IsChild =
True AND OxygenSaturation < 0.9) OR (IsChild = True AND SOB = True) OR (IsAdult = True AND
CURB65 > 2) THEN Yes". Rule sets are particularly suited for tabular data that contain mixed-type features and exhibit complex high-order feature interactions. When compared to other rule-based models such as decision trees and decision lists, the simpler combinatorial structure of rule sets makes them easier to interpret and to learn from data.
The learning of rule sets has attracted interest from various research communities over the past few decades. Two interplaying subset selection tasks constitute the major challenges in rule set learning.
First, the construction of a rule requires choosing a subset of all features. Then a subset of all possible 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
rules has to be selected to form a rule set. In early algorithms from machine learning community, such as FOIL [41], CN2 [12] and RIPPER [13], usually a greedy sequential covering strategy is utilized, in which at each stage a rule is generated from uncovered examples using heuristics and newly covered examples are removed. Associative classiﬁcation techniques [33, 50] developed by data mining community take a different two-stage strategy, in which a large set of rules are ﬁrst generated via association rule mining and then a rule set is constructed by ranking and pruning. Both these early approaches lack a global objective that guides the generation of rules and optimizes the interpretability of produced rule set.
This paper presents a new submodular optimization perspective on nearly optimal rule set learning.
We employ an objective function that simultaneously optimizes accuracy and interpretability. We show that given a ground set of rules, this objective is a regularized submodular function [27], for which algorithms with strong approximation guarantees have been developed recently [23]. Instead of using a pre-mined pool of rules as the ground set, we take an on-the-ﬂy approach in which the ground set consists of all possible rules and elements are picked from it one at a time through solving an optimization subproblem. When the subproblem is always solved to optimality, our learning algorithm enjoys the algorithmic guarantees for regularized submodular maximization. For most real cases the global optimum of the subproblem is impractical to reach, therefore we propose an efﬁcient approximate algorithm based on iterative local combinatorial search.
Our contributions are summarized as follows: (i) We formulate interpretable rule set learning as a regularized submodular maximization problem which is amenable to approximate algorithm with strong guarantees. (ii) We discover an intuitive difference of submodular (DS) decomposition for the induced rule construction subobjective, based on which an iterative reﬁnement algorithm is proposed to solve the subproblem approximately for large datasets. (iii) We conduct a comprehensive experimental evaluation, demonstrating the proposed approach is competitive against state-of-the-arts in both predictive performance and interpretability.
The remainder of this paper is organized as follows: