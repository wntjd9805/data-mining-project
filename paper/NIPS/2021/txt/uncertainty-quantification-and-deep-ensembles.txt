Abstract
Deep Learning methods are known to suffer from calibration issues: they typically produce over-conﬁdent estimates. These problems are exacerbated in the low data regime. Although the calibration of probabilistic models is well studied, calibrat-ing extremely over-parametrized models in the low-data regime presents unique challenges. We show that deep-ensembles do not necessarily lead to improved calibration properties. In fact, we show that standard ensembling methods, when used in conjunction with modern techniques such as mixup regularization, can lead to less calibrated models. This text examines the interplay between three of the most simple and commonly used approaches to leverage deep learning when data is scarce: data-augmentation, ensembling, and post-processing calibration methods. Although standard ensembling techniques certainly help boost accuracy, we demonstrate that the calibration of deep ensembles relies on subtle trade-offs.
We also ﬁnd that calibration methods such as temperature scaling need to be slightly tweaked when used with deep-ensembles and, crucially, need to be executed af-ter the averaging process. Our simulations indicate that this simple strategy can halve the Expected Calibration Error (ECE) on a range of benchmark classiﬁcation problems compared to standard deep-ensembles in the low data regime. 1

Introduction
Overparametrized deep models can memorize datasets with labels entirely randomized [48]. It is consequently not entirely clear why such extremely ﬂexible models are able to generalize well on unseen data and trained with algorithms as simple as stochastic gradient descent, although a lot of progress on these questions have recently been reported [8, 19, 2, 31, 39, 10].
The high capacity of neural network models, and their ability to easily overﬁt complex datasets, makes them especially vulnerable to calibration issues. In many situations, standard deep-learning approaches are known to produce probabilistic forecasts that are over-conﬁdent [16]. In this text, we consider the regime where the size of the training sets is very small, which typically ampliﬁes these issues. This can lead to problematic behaviors when deep neural networks are deployed in scenarios where a proper quantiﬁcation of the uncertainty is necessary. Indeed, a host of methods [22, 30, 40, 12, 37] have been proposed to mitigate these calibration issues, even though no gold standard has so far emerged. Many different forms of regularization techniques [35, 48, 50] have been shown to reduce overﬁtting in deep neural networks. Importantly, practical implementations and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
approximations of Bayesian methodologies [30, 44, 3, 14, 27, 38, 28] have demonstrated their worth in several settings. However, some of these techniques are not entirely straightforward to implement in practice. Ensembling approaches such as drop-outs [12] have been widely adopted, largely due to their ease of implementation. Recently, [1] provides a study on different ensembling techniques and describes pitfalls of certain metric for in-domain uncertainty quantiﬁcation. Also subsequent to our work, several articles also studied the interaction between data-augmentation and calibration issues.
Importantly, the CAMixup approach is proposed as a promising solution in [42]. Furthermore, [47] analyzes the under-conﬁdence of ensembles due to augmentations from a theoretical perspective. In this text, we investigate the practical use of Deep-Ensembles [22, 4, 25, 41, 9, 16], a straightforward approach that leads to state-of-the-art performances in most regimes. Although deep-ensembles can be difﬁcult to implement when training datasets are large (but calibration issues are less pronounced in this regime), the focus of this text is the data-scarce setting where the computational burden associated with deep-ensembles is not a signiﬁcant problem.
Contributions: We study the interaction between three of the most simple and widely used methods for adopting deep-learning to the low-data regime: ensembling, temperature scaling, and mixup data augmentation.
• Despite the widely-held belief that model averaging improves calibration properties, we show that, in general, standard ensembling practices do not lead to better-calibrated models.
Instead, we show that averaging the predictions of a set of neural networks generally leads to less conﬁdent predictions: that is generally only beneﬁcial in the oft-encountered regime when each network is overconﬁdent. Although our results are based on Deep Ensembles, our empirical analysis extends to any class of model averaging, including sampling-based
Bayesian Deep Learning methods.
• We empirically demonstrate that networks trained with the mixup data-augmentation scheme, a widespread practice in computer vision, are typically under-conﬁdent. Consequently, subtle interactions between ensembling techniques and modern data-augmentation pipelines have to be considered for proper uncertainty quantiﬁcation. The typical distributional shift induced by the mixup data-augmentation strategy inﬂuences the calibration properties of the resulting trained neural networks. In these settings, a standard ensembling approach typically worsens the calibration issues.
• Post-processing techniques such as temperature scaling are sometimes regarded as compet-ing methods when comparing the performance of many modern model-averaging techniques.
Instead, to mitigate the under-conﬁdence of model averaging, temperature scaling should be used in conjunction with deep-ensembling methods. More importantly, the order in which the aggregation and the calibration procedures are carried out greatly inﬂuences the resulting uncertainty quantiﬁcation. These ﬁndings lead us to formulate the straight-forward Pool-Then-Calibrate strategy for post-processing deep-ensembles: (1) in a ﬁrst stage, separately train deep models (2) in a second stage, ﬁt a single temperature parameter by minimizing a proper scoring rule (eg. cross-entropy) on a validation set. In the low data regime, this simple procedure can halve the Expected Calibration Error (ECE) on a range of benchmark classiﬁcation problems when compared to standard deep-ensembles.
Although straightforward to implement, to the best of our knowledge this strategy has not been investigated in the literature prior to our work. 2