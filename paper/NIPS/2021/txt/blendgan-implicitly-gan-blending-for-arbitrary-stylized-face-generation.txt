Abstract
Generative Adversarial Networks (GANs) have made a dramatic leap in high-ﬁdelity image synthesis and stylized face generation. Recently, a layer-swapping mechanism has been developed to improve the stylization performance. However, this method is incapable of ﬁtting arbitrary styles in a single model and requires hundreds of style-consistent training images for each style. To address the above issues, we propose BlendGAN for arbitrary stylized face generation by leveraging a ﬂexible blending strategy and a generic artistic dataset. Speciﬁcally, we ﬁrst train a self-supervised style encoder on the generic artistic dataset to extract the representations of arbitrary styles. In addition, a weighted blending module (WBM) is proposed to blend face and style representations implicitly and control the arbitrary stylization effect. By doing so, BlendGAN can gracefully ﬁt arbitrary styles in a uniﬁed model while avoiding case-by-case preparation of style-consistent training images. To this end, we also present a novel large-scale artistic face dataset
AAHQ. Extensive experiments demonstrate that BlendGAN outperforms state-of-the-art methods in terms of visual quality and style diversity for both latent-guided and reference-guided stylized face synthesis. Our project webpage is https://onion-liu.github.io/BlendGAN/ 1

Introduction
Generative adversarial networks (GANs) [1] have achieved impressive performance in various tasks such as image generation [2, 3, 4, 5, 6, 7, 8, 9], image super-resolution [10, 11], and image translation
[12, 13, 14, 15, 16, 17, 18, 19]. In recent years, GAN has also been widely used for face stylization such as portrait drawing [20], caricature [21, 22], manga [23], and anime [24]. With the deepening of
GAN research, the community has further paid attention to the quality of generated images and the disentanglement ability of latent space.
To achieve high-quality generation and latent disentanglement, StyleGAN [8] introduces the adap-tative instance normalization layer (AdaIN) [25] and proposes a new generator architecture, which
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of some reference-guided synthesis results. Our framework can generate stylized face images with high quality. achieves pretty good performance in generating face images. StyleGAN2 [9] explores the causes of droplet-like artifacts and further improves the quality of generated images by redesigning generator normalization. StyleGAN2-ada [26] proposes an adaptive discriminator augmentation mechanism which reduces the required number of images to several thousands. In the experiment of StyleGAN2-ada, the model is trained on MetFaces dataset [26] to generate artistic faces, however, the style of image is uncontrollable, and the corresponding original face cannot be obtained.
Recently, a layer-swapping mechanism [27] has been proposed to generate high-quality natural and stylized face image pairs, and equipped with an additional encoder for unsupervised image translation
[28]. In particular, these methods ﬁrst ﬁnetune StyleGAN from a pretrained model with target-style faces and then swap some convolutional layers with the original model. Given randomly sampled latent codes, the original model generates natural face images, while the layer-swapped model outputs stylized face images correspondingly. Though effective at large, these methods have to train models in a case-by-case ﬂavor, thus a single model can only generate images with a speciﬁc style. Besides, these methods still require hundreds of target-style images for ﬁnetuning, and one has to carefully select the number of training iterations, as well as the swapped layers.
In this paper, we present BlendGAN for arbitrary stylized face generation.2 BlendGAN resorts to a
ﬂexible blending strategy and a generic artistic dataset to ﬁt arbitrary styles without relying on style-consistent training images for each style. In particular, we analyze a stylized face image as composed of two latent parts: a face code (controlling the face attributes) and a style code (controlling the artistic appearance). Firstly, a self-supervised style encoder is trained via an instance discrimination objective to extract the style representation from artistic face images. Secondly, a weighted blending module (WBM) is proposed to blend the face and style latent codes into a ﬁnal code which is then fed into a generator. By controlling the indicator in WBM, we are able to decide which parts of the face and style latent codes to be blended thus controlling the stylization effect. By combining the 2Our framework can also cooperate with GAN inversion [29, 30, 31, 32, 33] or StyleGAN distillation [34] methods to enable end-to-end style transfer or image translation. We will show the results in the appendix. 2
Figure 2: Overview of the proposed framework. The style encoder Estyle extracts the style latent code zs of a reference style image. The face latent code zf is randomly sampled from the standard Gaussian distribution. Two MLPs transform face and style latent codes into their W spaces separately, then they are combined by the weighted blending module (WBM) and fed into generator G to synthesise natural and stylized face images. Three discriminators are used in our method. The face discriminator Df ace distinguishes between real and fake natural-face images, the style discriminator Dstyle distinguishes between real and fake stylized-face images, and the style latent discriminator Dstyle_latent predicts whether the stylized-face image is consistent with the style latent code zs. style encoder and the generator with WBM, our framework can generate natural and stylized face image pairs with either a randomly sampled style or a reference style (see Figure 1).
As for the generic artistic data, we present a novel large-scale dataset of high-quality artistic face images, Artstation-Artistic-face-HQ (AAHQ), which covers a wide variety of painting styles, color tones, and face attributes. Experiments show that compared to state-of-the-art methods, our framework can generate stylized face images with higher visual quality and style diversity for both latent-guided and reference-guided synthesis. 2