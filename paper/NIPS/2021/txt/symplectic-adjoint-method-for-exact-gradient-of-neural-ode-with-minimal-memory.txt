Abstract
A neural network model of a differential equation, namely neural ODE, has enabled the learning of continuous-time dynamical systems and probabilistic distributions with high accuracy. The neural ODE uses the same network repeatedly during a numerical integration. The memory consumption of the backpropagation algorithm is proportional to the number of uses times the network size. This is true even if a checkpointing scheme divides the computation graph into sub-graphs. Otherwise, the adjoint method obtains a gradient by a numerical integration backward in time.
Although this method consumes memory only for a single network use, it requires high computational cost to suppress numerical errors. This study proposes the symplectic adjoint method, which is an adjoint method solved by a symplectic integrator. The symplectic adjoint method obtains the exact gradient (up to rounding error) with memory proportional to the number of uses plus the network size. The experimental results demonstrate that the symplectic adjoint method consumes much less memory than the naive backpropagation algorithm and checkpointing schemes, performs faster than the adjoint method, and is more robust to rounding errors. 1

Introduction
Deep neural networks offer remarkable methods for various tasks, such as image recognition [18] and natural language processing [4]. These methods employ a residual architecture [21, 34], in which the output xn+1 of the n-th operation is deﬁned as the sum of a subroutine fn and the input xn as xn+1 = fn(xn) + xn. The residual architecture can be regarded as a numerical integration applied to an ordinary differential equation (ODE) [30]. Accordingly, a neural network model of the differential equation dx/dt = f (x), namely, neural ODE, was proposed in [2]. Given an initial condition x(0) = x as an input, the neural ODE solves an initial value problem by numerical integration, obtaining the ﬁnal value as an output y = x(T ). The neural ODE can model continuous-time dynamics such as irregularly sampled time series [24], stable dynamical systems [37, 42], and physical phenomena associated with geometric structures [3, 13, 31]. Further, because the neural
ODE approximates a diffeomorphism [43], it can model probabilistic distributions of real-world data by a change of variables [12, 23, 25, 45].
For an accurate integration, the neural ODE must employ a small step size and a high-order numerical integrator composed of many internal stages. A neural network f is used at each stage of each 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison of the proposed method with existing methods
Methods
Gradient Calculation
Exact
Checkpoints
Memory Consumption Computational Cost
NODE [2] adjoint method
NODE [2] backpropagation baseline scheme backpropagation backpropagation
ACA [46]
MALI [47]∗ backpropagation no yes yes yes yes checkpoint backprop.
M
—
M
MN
M
L
M (N+2 ˜N)sL
M N sL
N sL sL sL 2M N sL 3M N sL 3M N sL 4M N sL xN
— x0
{xn}N −1 n=0 xN n=0, {Xn,i}s proposed∗∗
∗Available only for the asynchronous leapfrog integrator. ∗∗Available for any Runge–Kutta methods. symplectic adjoint method yes {xn}N−1 i=1 MN+s
L 4M N sL time step. Thus, the backpropagation algorithm consumes exorbitant memory to retain the whole computation graph [39, 2, 10, 46]. The neural ODE employs the adjoint method to reduce memory consumption—this method obtains a gradient by a backward integration along with the state x, without consuming memory for retaining the computation graph over time [8, 17, 41, 44]. However, this method incurs high computational costs to suppress numerical errors. Several previous works employed a checkpointing scheme [10, 46, 47]. This scheme only sparsely retains the state x as checkpoints and recalculates a computation graph from each checkpoint to obtain the gradient.
However, this scheme still consumes a signiﬁcant amount of memory to retain the computation graph between checkpoints.
To address the above limitations, this study proposes the symplectic adjoint method. The main advantages of the proposed method are presented as follows.
Exact Gradient and Fast Computation: In discrete time, the adjoint method suffers from numerical errors or needs a smaller step size. The proposed method uses a specially designed integrator that obtains the exact gradient in discrete time. It works with the same step size as the forward integration and is thus faster than the adjoint method in practice.
Minimal Memory Consumption: Excepting the adjoint method, existing methods apply the back-propagation algorithm to the computation graph of the whole or a subset of numerical integra-tion [10, 46, 47]. The memory consumption is proportional to the number of steps/stages in the graph times the neural network size. Conversely, the proposed method applies the algorithm only to each use of the neural network, and thus the memory consumption is only proportional to the number of steps/stages plus the network size.
Robust to Rounding Error: The backpropagation algorithm accumulates the gradient from each use of the neural network and tends to suffer from rounding errors. Conversely, the proposed method obtains the gradient from each step as a numerical integration and is thus more robust to rounding errors. 2