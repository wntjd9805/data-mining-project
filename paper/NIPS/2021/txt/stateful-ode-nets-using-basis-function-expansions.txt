Abstract
The recently-introduced class of ordinary differential equation networks (ODE-Nets) establishes a fruitful connection between deep learning and dynamical sys-tems. In this work, we reconsider formulations of the weights as continuous-in-depth functions using linear combinations of basis functions which enables us to leverage parameter transformations such as function projections. In turn, this view allows us to formulate a novel stateful ODE-Block that handles stateful layers.
The beneﬁts of this new ODE-Block are twofold: ﬁrst, it enables incorporating meaningful continuous-in-depth batch normalization layers to achieve state-of-the-art performance; second, it enables compressing the weights through a change of basis, without retraining, while maintaining near state-of-the-art performance and reducing both inference time and memory footprint. Performance is demon-strated by applying our stateful ODE-Block to (a) image classiﬁcation tasks using convolutional units and (b) sentence-tagging tasks using transformer encoder units. 1

Introduction
The interpretation of neural networks (NNs) as discretizations of differential equations [7, 17, 31, 43] has recently unlocked a fruitful link between deep learning and dynamical systems. The strengths of so-called ordinary differential equation networks (ODE-Nets) are that they are well suited for modeling time series [11, 39] and smooth density estimation [14]. They are also able to learn representations that preserve the topology of the input space [9] (which may be seen as a “feature” or as a “bug”). Further, they can be designed to be highly memory efﬁcient [13, 51]. However, one major drawback of current ODE-Nets is that the predictive accuracy for tasks such as image classiﬁcation is often inferior as compared to other state-of-the-art NN architectures. The reason for the poor performance is two-fold: (a) ODE-Nets have shallow parameterizations (albeit long computational graphs), and (b) ODE-Nets do not include a mechanism for handling layers with internal state (i.e., stateful layers, or stateful modules), and thus cannot leverage batch normalization layers which are standard in image classiﬁcation problems. That is, in modern deep learning environments the running mean and variance statistics of a batch normalization module are not trainable parameters, but are instead part of the module’s state, which does not ﬁt into the ODE framework.
Further, traditional techniques such as stochastic depth [21], layer dropout [12] and adaptive depth [10, 28], which are useful for regularization and compressing traditional NNs, do not necessarily apply in a meaningful manner to ODE-Nets. That is, these methods do not provide a systematic scheme to derive smaller networks from a single deep ODE-Net source model. This limitation prohibits quick and rigorous adaptation of current ODE-Nets across different computational environments.
∗Equal contributions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Sketch of a continuous-in-depth transformer-encoder. The model architecture consists of a sparse embedding layer followed by an OdeBlock that integrates the encoder to feed into a classiﬁcation layer to determine parts of speech. The model graph is generated by nesting the residual
R on the right into a time-integration scheme in the OdeBlock. The weights for each call to R are determined by evaluating the basis expansion. Internal hidden states evolve continuously during the forward pass. This is illustrated by a smoothly varying attention matrix.
Hence, there is a need for a general but effective way of reducing the number of parameters of deep
ODE-Nets to reduce both inference time and the memory footprint.
To address these limitations, we propose a novel stateful ODE-Net model that is built upon the theoretical underpinnings of numerical analysis. Following recent work by [34, 37], we express the weights of an ODE-Net as continuous-in-depth functions using linear combinations of basis functions. However, unlike prior works, we consider parameter transformations through a change of basis functions, i.e., adding basis functions, decreasing the number of basis functions, and function projections. In turn, we are able to (a) design deep ODE-Nets that leverage meaningful continuous-in-depth batch normalization layers to boost the predictive accuracy of ODE-Nets, e.g., we achieve 94.4% test accuracy on CIFAR-10, and 79.9% on CIFAR-100; and (b) we introduce a methodology for compressing ODE-Nets, e.g, we are able to reduce the number of parameters by a factor of two, without retraining or revisiting data, while nearly maintaining the accuracy of the source model.
In the following, we refer to our model as Stateful ODE-Net. Here are our main contributions.
• Stateful ODE-Block. We introduce a novel stateful ODE-Block that enables the integration of stateful network layers (Sec. 4.1). To do so, we view continuous-in-depth weight functions through the lens of basis function expansions and leverage basis transformations. An example of our continuous-in-depth model is shown in Figure 1.
• Stateful Normalization Layer. We introduce stateful modules using function projections. This enables us to introduce continuous-in-depth batch normalization layers for ODE-Nets (Sec. 4.2).
In our experiments, we show that such stateful normalization layers are crucial for ODE-Nets to achieve state-of-the-art performance on mainstream image classiﬁcation problems (Sec. 7).
• A Posteriori Compression Methodology. We introduce a methodology to compress ODE-Nets without retraining or revisiting any data, based on parameter interpolation and projection (Sec. 5).
That is, we can systematically derive smaller networks from a single deep ODE-Net source model through a change of basis. We demonstrate the accuracy and compression performance for various image classiﬁcation tasks using both shallow and deep ODE-Nets (Sec. 7).
• Advantages of Higher-order Integrators for Compression. We examine the effects of training continuous-in-depth weight functions through their discretizations (Sec. 6). Our key insight in
Theorem 1 is that higher-order integrators introduce additional implicit regularization which is crucial for obtaining good compression performance in practice. Proofs are provided in Appendix A. 2