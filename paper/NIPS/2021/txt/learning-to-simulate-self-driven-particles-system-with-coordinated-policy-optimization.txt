Abstract
Self-Driven Particles (SDP) describe a category of multi-agent systems common in everyday life, such as ﬂocking birds and trafﬁc ﬂows. In a SDP system, each agent pursues its own goal and constantly changes its cooperative or competitive behaviors with its nearby agents. Manually designing the controllers for such
SDP system is time-consuming, while the resulting emergent behaviors are often not realistic nor generalizable. Thus the realistic simulation of SDP systems remains challenging. Reinforcement learning provides an appealing alternative for automating the development of the controller for SDP. However, previous multi-agent reinforcement learning (MARL) methods deﬁne the agents to be teammates or enemies before hand, which fail to capture the essence of SDP where the role of each agent varies to be cooperative or competitive even within one episode.
To simulate SDP with MARL, a key challenge is to coordinate agents’ behaviors while still maximizing individual objectives. Taking trafﬁc simulation as the testing bed, in this work we develop a novel MARL method called Coordinated Policy
Optimization (CoPO), which incorporates social psychology principle to learn neural controller for SDP. Experiments show that the proposed method can achieve superior performance compared to MARL baselines in various metrics. Noticeably the trained vehicles exhibit complex and diverse social behaviors that improve performance and safety of the population as a whole. Demo video and source code are available at: https://decisionforce.github.io/CoPO/. 1

Introduction
Self-Driven Particles (SDP) describe a wide range of multi-agent systems (MAS) in nature and human society. In SDP, individual agent pursues its own goal and interacts with each other following simple local alignment, and then the population exhibits complex collective behaviors [47]. We commonly see the phenomena of collective behaviors, such as the ﬂocking birds [4], molecular motors [6], human crowd [17, 16], and the trafﬁc system [20]. To understand and simulate such phenomena, researchers have developed a number of SDP models. For example, simple-rule based models [7] or
Hydrodynamic equations based models [2, 19] can simulate the SDP very well in an unconstrained environment with random movement and resemble complex behaviors such as schooling ﬁsh [12] and marching locusts [5]. However, in a more structured environment such as a particular trafﬁc scene where the interactions of agents are time-varying and the environment is non-stationary, it is difﬁcult to design manual controllers or use rules to recover the underlying collective behaviors.
When the interactive environment is available, reinforcement learning becomes a promising approach to learn the controllers for actuating the SDP. Recently, many multi-agent reinforcement learning (MARL) methods have been developed to play competitive multi-player games, such as Hide and Seek [1], Football [23], Go and other board games [34], and StarCraft [33]. However, it is challenging to apply the existing MARL to simulate SDP systems. One essential issue is that each 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The ﬁve environments used in our evaluation. We highlight two cooperative and competitive events in the vehicle interactions. constituent agent in a SDP system is self-interested and the relationship between agents is constantly changing. As illustrated in Fig. 1A, the vehicles in the trafﬁc system demonstrate social behaviors that are either cooperative or competitive depending on the situation. Meanwhile, the cooperation and the competition naturally emerge as the outcomes of the self-driven instinct and the multi-agent interaction. Thus it is difﬁcult to generate those social behaviors through a top-down design.
Furthermore, most of the MARL methods assume that the role of each agent is pre-determined and
ﬁxed within one episode. In cooperative MARL methods [32, 41, 9, 8], all agents need to cooperate to maximize a joint scalar reward. However, the credit decomposition problem centered in such setting is not the major issue of simulating SDP systems, because each agent in SDP has its individual objective and the credit decomposition is no longer needed. On the other hand, directly applying independent learning [43, 35] will lead to a group of agents with unreasonable behaviors that are aggressive or egocentric, since each agent is trained to maximize its own reward.
In order to simulate SDP systems, the coordination of the self-interested constituents is the major issue. The coordination of agents in MARL has been previously studied in the context of mixed cooperative and competitive tasks [25, 38]. However, seldom works focus on the simulation of SDP systems. Taking the microscopic trafﬁc simulation as an example, existing works focus on learning high-level controller to indirectly actuate vehicles [48], or using independent learner to train policies executing in the environments engaging a limited set of agents on simple scenes [31, 56], without considering the coordination problem. Instead, we aim to control all the vehicles in the scene by operating on the low-level continuous control space. This allows a higher degree of freedom to learn diverse behaviors but poses a challenging continuous control problem. In this work we propose a novel MARL method called Coordinated Policy Optimization (CoPO) to facilitate the coordination of agents at both local and global levels. To evaluate the proposed method, as illustrated in Fig. 1, we construct ﬁve typical trafﬁc environments as the testing bed. These environments contain rich interactions between agents and complex road structures. Besides, we develop three task-agnostic metrics to characterize different aspects of the learned populations. We show that the proposed CoPO method can achieve superior performance compared to the baselines. Noticeably, collective social behaviors that resemble real-world patterns emerge in the trained population. 2