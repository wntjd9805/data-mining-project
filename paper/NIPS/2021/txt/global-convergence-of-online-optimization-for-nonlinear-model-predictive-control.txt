Abstract
We study a real-time iteration (RTI) scheme for solving online optimization problem appeared in nonlinear optimal control. The proposed RTI scheme modiﬁes the existing RTI-based model predictive control (MPC) algorithm, by selecting the stepsize of each Newton step at each sampling time using a differentiable exact augmented Lagrangian. The scheme can adaptively select the penalty parameters of augmented Lagrangian on the ﬂy, which are shown to be stabilized after certain time periods. We prove under generic assumptions that, by involving stepsize selection instead of always using a full Newton step (like what most of the existing RTIs do), the scheme converges globally: for any initial point, the KKT residuals of the subproblems converge to zero. A key step is to show that augmented
Lagrangian keeps decreasing as horizon moves forward. We demonstrate the global convergence behavior of the proposed RTI scheme in a numerical experiment. 1

Introduction
We consider the following time-varying nonlinear optimal control problem
P(¯x0) : min x,u
N −1 (cid:88) k=0 gk(xk, uk) + gN (xN ), s.t. xk+1 = fk(xk, uk), k ∈ [N − 1], (1) x0 = ¯x0, where xk ∈ Rnx are the state variables, uk ∈ Rnu are the control variables, gk : Rnx × Rnu → R+ (gN : Rnx → R+) are the nonnegative cost functions, fk : Rnx × Rnu → Rnx are the dynamical constraint functions, ¯x0 is the given initial state, and N is the horizon length. We suppose that gk, fk are twice continuously differentiable and, without loss of generality, that the origin is a steady state, i.e. fk(0, 0) = 0, and is also a stationary point with zero loss, i.e. gk(0, 0) = 0 and ∇gk(0, 0) = 0.
The well-known linear quadratic regulator (LQR) satisﬁes this setup and the same setup is commonly used in the literature [8]. Problem (1) is also called dynamic program in control community, which has a close relation to reinforcement learning (RL) (see [6]). The main difference is that the dynamic fk in our paper is known, which is the case in many industrial applications and model-based RL studies.
In modern applications such as energy and autonomous control, N tends to be extremely large or even inﬁnity, which stimulates the interest of solving (1) in real time. Let fk(xk, uk) = fk(xk, uk; dk) where dk is data at stage k, then a more realistic setting is to collect dk as a data stream. That is, instead of knowing f0 all the way to fN −1 and solving (1) ofﬂine, we only know fk sequentially and have to solve (1) online. Model predictive control (MPC), also known as receding horizon 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
control, is an important feedback control technique that is used in online optimization of control problems. Given the current system state, MPC looks ahead multiple stages on the prediction horizon, solves the subproblem on the prediction horizon to optimality, and provides the ﬁrst optimal control action to the system as the feedback. Then the system evolves to a new state based on the control action and the procedure is repeated. However, the classical nonlinear MPC is being regarded as a theoretical concept rather than a practical strategy. It is not as widely used as linear MPC, and is mostly investigated in slow dynamics. This is due to the intensive computations of solving the control subproblems when we have nonlinear models. Fortunately, the design of real-time iteration (RTI) and its predecessors [26, 37] have enabled RTI-based nonlinear MPC to be applied on fast dynamics and achieve good performance [48, 4, 14, 38, 15, 24, 18, 22, 17, 20, 16, 19, 39, 1].
Instead of using optimal feedback control, RTI schemes compute a cheap approximation of control action and provide back to the system as fast as possible. The existing RTI schemes perform a single, full Newton step for each subproblem, and the output iterate of the previous subproblem is used to warmly initialize the current subproblem [8, 41, 42]. Thus, RTI schemes signiﬁcantly reduce the computations of each subproblem, which makes them succeed in modern industrial applications. RTI schemes exploit the fact that two successive subproblems are closely related though different, so that the iteration can achieve the convergence “on the ﬂy” (i.e. the error goes to zero as horizon moves forward). However, the RTI-based MPC is challenging to analyze. The main difﬁculty lies in the fact that the prediction horizon is shifted each time. Each subproblem is appended by a new stage that has never been scanned before and may introduce perturbation into the system. Different RTI-based MPC schemes have been studied [9, 12, 23], with an improved local analysis established in [31] recently.
Our paper complements the literature on RTI-based MPC by investigating its global convergence. We emphasize that by global convergence we mean the convergence to stationary points (i.e. KKT points) for any initialization, instead of the convergence to global optimality. The latter is not achievable for nonlinear problems without strong assumptions [34]. Instead of using a full Newton step, we design an adaptive scheme to involve the stepsize selection procedure. In fact, the stepsize selection has been suggested as a way to improve the global behavior of RTI in shrinking (not moving) horizons in an old paper [25]. The authors implemented a (cid:96)1 penalized merit function1 with a watchdog line search. However, the global convergence theory remains open. Different from the suggestion of [25], we use a differentiable exact augmented Lagrangian as the merit function. This function mainly has two differences to the (cid:96)1 penalized merit function: (i) it depends on both primal and dual variables, so that dual variables are updated using the same stepsize as the primal variables ((cid:96)1 merit function only updates the primal variables); (ii) a simple backtracking line search can effectively overcome Maratos effect [29] and accept the unit stepsize locally ((cid:96)1 merit function requires a more time-consuming line search method such as watchdog or second-order correction). We conduct experiments to show the superiority of augmented Lagrangian over (cid:96)1 penalized function, especially for time-varying problems. Moreover, our algorithm adaptively selects the penalty parameters of augmented Lagrangian on the ﬂy. When the unit stepsize is accepted, the algorithm simply boils down to the standard RTI-based MPC, and the existing local theories [8, 31] apply seamlessly. We notice that the same merit function has been employed in a recent work [32] to achieve the global convergence for solving (1) in ofﬂine fashion, with a horizon decomposition strategy. In that work, the authors showed the unit stepsize is accepted even with an inexact Newton direction. Our paper is in online regime and does not investigate the local behavior.