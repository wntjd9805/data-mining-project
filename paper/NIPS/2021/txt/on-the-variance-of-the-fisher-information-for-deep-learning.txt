Abstract
In the realm of deep learning, the Fisher information matrix (FIM) gives novel insights and useful tools to characterize the loss landscape, perform second-order optimization, and build geometric learning theories. The exact FIM is either un-available in closed form or too expensive to compute. In practice, it is almost always estimated based on empirical samples. We investigate two such estimators based on two equivalent representations of the FIM — both unbiased and consis-tent. Their estimation quality is naturally gauged by their variance given in closed form. We analyze how the parametric structure of a deep neural network can af-fect the variance. The meaning of this variance measure and its upper bounds are then discussed in the context of deep learning. 1

Introduction
The Fisher information is one of the most fundamental concepts in statistical machine learning.
Intuitively, it measures the amount of information carried by a single random observation when the underlying model varies along certain directions in the parameter space: if such a variation does not change the underlying model, then a corresponding observation contains zero (Fisher) information and is non-informative regarding the varied parameter. Parameter estimation is impossible in this case. Otherwise, if the variation signiﬁcantly changes the model and has large information, then an observation is informative and the parameter estimation can be more efﬁcient as compared to parameters with small Fisher information.
In machine learning, this basic concept is useful for deﬁning intrinsic structures of the parameter space, measuring model complexity, and performing gradient-based optimization.
Given a statistical model that is speciﬁed by a parametric form p(z | θ) and a continuous domain
θ ∈ M, the Fisher information matrix (FIM) is a 2D tensor varying with θ ∈ M, given by
I(θ) = Ep(z | θ)
, (1) (cid:18) (cid:19)
∂ℓ
∂θ
∂ℓ
∂θ⊤ where Ep(z | θ)(·), or simply Ep(·) if the model p is clear from the context, denotes the expectation w.r.t. p(z | θ), and ℓ := log p(z | θ) is the log-likelihood function. All vectors are column vectors throughout this paper. Under weak conditions (see Lemma 5.3 in Lehmann and Casella [16] for the univariate case), the FIM has the equivalent expression I(θ) = Ep(z | θ)
. Given
N i.i.d. observations z1, . . . , zN , these two equivalent expressions of the FIM lead to two different estimators
−∂2ℓ/∂θ∂θ⊤ (cid:0) (cid:1)
ˆI1(θ) = (cid:19) (cid:18)
NX i=1 1
N
∂ℓi
∂θ
∂ℓi
∂θ⊤ and
ˆI2(θ) = (cid:18)
NX i=1 1
N (cid:19)
,
− ∂2ℓi
∂θ∂θ⊤ (2) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where ℓi := log p(zi | θ) is the log-likelihood of the i’th observation zi. The notations ˆI1(θ) and
ˆI2(θ) are abused for simplicity as they depend on both θ and the random observations zi.
These estimators are universal and independent to the parametric form p(z | θ). They are expressed in terms of the 1st- or 2nd-order derivatives of the log-likelihood. Usually, we already have these derivatives to perform gradient-based learning. Therefore, we can save computational cost and reuse these derivatives to estimate the Fisher information, which in turn can be useful, e.g., to perform natural gradient optimization [1, 25]. Estimating the FIM is especially meaningful for deep learning, where the computational overhead of the exact FIM can be signiﬁcant.
It is straightforward from the law of large numbers and the central limit theorem that both estimators in Eq. (2) are unbiased and consistent. This is formally stated as follows.
Proposition 1. (cid:16)
Ep(z | θ) (cid:17)
ˆI1(θ) (cid:16)(cid:13) (cid:13) (cid:13)ˆI1(θ) − I(θ)
= Ep(z | θ) (cid:16) (cid:13) (cid:13) (cid:13)
= I(θ). (cid:17)
ˆI2(θ) (cid:13) (cid:13) (cid:13)ˆI2(θ) − I(θ)
+
F (cid:17)
> ϵ
= 0, (cid:13) (cid:13) (cid:13)
F
∀ϵ > 0, lim
N→∞
Prob where Prob(·) denotes the probability of the parameter statement being true and ∥ · ∥F is the Frobe-nius norm of a tensor (with ∥ · ∥2 as the regular vector L2-norm)
The Fisher information can be zero for non-regular models or inﬁnite [7]. However, these properties may not be preserved by the empirical estimators.
How far can ˆI1(θ) and ˆI2(θ) deviate from the “true FIM” I(θ), and how fast can they converge to
I(θ) as the number of observations increases? To answer these questions, it is natural to think of the variance of ˆI1(θ) and ˆI2(θ). For example, an estimator with a large variance means the estimation does not accurately reﬂect I(θ); and any procedure depending on the FIM consequently suffers from the estimation error. Through studying the variance, we can control the estimation quality and reliably perform subsequent measurements or algorithms based on the FIM.
Towards this direction, we made the following contributions that will unfold in the following Sec-tions 2 to 4:
• We review and rediscover two equivalent expression of the FIM in the context of deep feed-forward networks (Section 2);
• We give in closed form the variance (extending to meaningful upper bounds) and discuss the convergence rate of the estimators ˆI1(θ) and ˆI2(θ) (Section 3);
• We analyze how the 1st- and 2nd-order derivatives of the neural network can affect the estimation of the FIM (Section 4).
We discuss related work in Section 5 and conclude in Section 6. 2 Feed-forward Networks with Exponential Family Output
This section realizes the concept of Fisher information in a feed-forward network with exponentially family output and explains why its estimators are useful in theory and practice.
Consider supervised learning with a neural network. The underlying statistical model is p(z | θ) = p(x)p(y | x, θ), where z = (x, y), the random variable x represents features, and y is the target variable. The marginal distribution p(x) is parameter-free, usually ﬁxed as the empirical distribution i=1, where δ(·) is the Dirac delta. In this i=1 δ(x − xi) w.r.t. a set of observations {xi}M p(x) = 1
M paper, we consider w.l.o.g. M = 1 as the FIM w.r.t. observations {xi}M i=1 is simply the average over
FIMs of each individual observation. All results generalize to multiple observations by taking the empirical average. The predictor p(y | x, θ) is a neural network with parameters θ = {Wl−1}L
P
M l=1 2
and exponential family output units, given by p(y | x) = exp (cid:0) (cid:1) t⊤(y)hL − F (hL)
¯hL−1,
,
¯hl−1), (l = 1, . . . , L − 1) hL = WL−1 hl = σ(Wl−1
¯hl = (h⊤ l , 1)⊤, h0 = x,
R (3) exp(t⊤(y)h)dy is the where t(y) is the sufﬁcient statistics of the prediction model, F (h) = log log-partition function, and σ : ℜ → ℜ is an element-wise non-linear activation function. Moreover,
Wl is a nl+1 × (nl + 1) matrix, representing the neural network parameters (weights and biases) in the l’th layer, where nl := dim(hl) denotes the size of layer l. We use W − for the nl+1 × nl weight matrix without the bias terms, obtained by removing the last column of Wl. hl is a learned representation of the input x. All intermediate variables hl are extended to include a constant scalar 1 in ¯hl, so that a linear layer can simply be expressed as Wl
¯hl. The last layer’s output hL with dimensionality nL speciﬁes the natural parameter of the exponential family.
We need the following Lemma which gives the FIM w.r.t. hL, which is a nL × nL matrix in simple closed form for commonly used probability distributions.
Lemma 2. For the neural network model speciﬁed in Eq. (3), l
I(hL) = Cov(t(y)) =
∂η
∂hL
, where Cov(·) denotes the covariance matrix w.r.t. p(y | x, θ), η := η(hL) := ∂F/∂hL is the expectation parameters, and the vector-vector-derivative ∂η/∂hL denotes the Jacobian matrix of the mapping hL → η.
The derivatives of the log-likelihood ℓ(θ) := log p(x, y | θ) characterize its landscape and are es-sential to compute the FIM. By Eq. (3), the score function (gradient of ℓ) is (cid:19)⊤ (cid:18)
∂ℓ
∂θ
=
∂hL
∂θ (t(y) − η(hL)) = (ta − ηa). (4)
∂ha
L
∂θ
P
In this paper, we mix the usual Σ-notation of summation with the Einstein notation: in the same term, an index appearing in both upper- and lower-positions indicates a sum over this index. For example, taha = a taha. Hence, in our equations, upper- and lower-indexes have the same meaning: both ha and ha mean the a’th element of h. For convenience and consistency, we take quantities w.r.t.
θ as upper indexed and other quantities as lower indexed, i.e., I ij(θ) versus Iij(hL). This mixed representation of sums helps to simplify our expressions without causing confusion. From Eq. (4) and Lemma 2, the Hessian of ℓ is given by
− ∂ha
L
∂θ
∂2ℓ
∂θ∂θ⊤ = (ta − ηa)
∂ηa
∂θ⊤ = (ta − ηa)
∂2ha
L
∂θ∂θ⊤
∂2ha
L
∂θ∂θ⊤
− ∂ha
L
∂θ
∂hb
L
∂θ⊤ .
Iab(hL) (5)
Similar to the case of a general statistical model, the FIM is equivalent to the expectation of the
Hessian of −ℓ as long as the activation function is smooth enough.
Theorem 3. Consider the neural network model in Eq. (3). For any activation function σ ∈ C 2(ℜ) (both σ′(z) and σ′′(z) exist and are continuous), we have I(θ) = Ep
− ∂2ℓ (cid:16) (cid:17)
.
∂θ∂θ⊤
Remark 3.1. ReLU networks do not have this equivalent expression as ReLU(z) is not differen-tiable at z = 0.
Through the deﬁnition of the FIM, or alternatively its equivalent formula in Theorem 3, we arrive at the same expression
I(θ) = (cid:18) (cid:19)⊤
∂hL
∂θ
I(hL)
∂hL
∂θ⊤ =
∂ha
L
∂θ
Iab(hL)
∂hb
L
∂θ⊤ . (6)
Equation (6) takes the form of a generalized Gauss-Newton matrix [18]. This general expression of the FIM has been known in the literature [24, 25]. Under weak conditions, I(θ) is a pullback 3
metric [30] of I(hL) in Lemma 2 associated with the mapping θ → hL. To compute I(θ) in closed form, one need to ﬁrst compute the Jacobian matrix of size nL × dim(θ) then perform the matrix multiplication in Eq. (6). The naive algorithm to evaluate Eq. (6) has a computational complexity of
L dim(θ)+nL dim2(θ)), where the term O(nL dim2(θ)) is dominant as dim(θ) ≫ nL in deep
O(n2 architectures. Once the parameter θ is updated, the FIM has to be recomputed. This is infeasible in practice for large networks where dim(θ) can be millions or billions.
The two estimators ˆI1(θ) and ˆI2(θ) in Eq. (2) provide a computationally inexpensive way to esti-mate the FIM. Given θ and x, one can draw i.i.d. samples y1, . . . , yN ∼ p(y | x, θ). Both ∂ℓi/∂θ and ∂2ℓi/∂θ∂θ⊤ can be evaluated directly through auto-differentiation (AD) that is highly opti-mized for modern GPUs. For ˆI1(θ), we already have ∂ℓi/∂θ to perform gradient descent. For ˆI2(θ), efﬁcient methods to compute the Hessian are implemented in AD frameworks such as PyTorch [26].
Using these derivatives, the computational cost only scales with the number N of samples but does not scale with nL.
We rarely need the full FIM of size dim(θ) × dim(θ). Most of the time, only its diagonal blocks are needed, where each block corresponds to a subset of parameters, e.g., the neural network weights of a particular layer. Therefore the computation of both estimators can be further reduced.
If p(y | x, θ) has the parametric form in Eq. (3), from Eqs. (4) and (5), the FIM estimators become
ˆI1(θ) =
∂ha
L
∂θ
· 1
N
ˆI2(θ) =
ηa − 1
N
NX i=1
NX
! ta(yi) i=1 (ta(yi) − ηa)(tb(yi) − ηb) · ∂hb
∂θ⊤ ,
L
∂2ha
L
∂θ∂θ⊤ +
∂ha
L
∂θ
Iab(hL)
∂hb
L
∂θ⊤ . (7) (8)
Recall that the notation of ˆI1(θ) and ˆI2(θ) is abused as they depend on x and y1 · · · yN . Notably, in
Eq. (7), ˆI1(θ) is expressed in terms of the Jacobian matrix of the mapping θ → hL and the empirical variance of the minimal sufﬁcient statistic t(yi) of the output exponential family. In Eq. (8), ˆI2(θ) depends on both the Jacobian and Hessian of θ → hL and the empirical average of t(yi). The second term on the right-hand side (RHS) of Eq. (8) is exactly the FIM, and therefore the ﬁrst term serves as a bias term. Eqs. (7) and (8) are only for the case with exponential family output. If the output units belong to non-exponential families, e.g., a statistical mixture model, one falls back to the general formulae, i.e., Eq. (2) for the FIM.
As an application of the Fisher information, the Cramér-Rao lower bound (CRLB) states that any unbiased estimator ˆθ of the parameters θ satisﬁes Cov( ˆθ) ⪰ [I(θ)]−1. For example, in Lemma 2, the FIM is w.r.t. the output of the neural network. As such, I(hL) can be used to study the estimation covariance of hL based on random samples y1 · · · yN drawn from p(y | x, θ). Similarly for Eq. (6), we can consider unbiased estimators of the weights of the neural network. In any case, to apply the CRLB, one needs an accurate estimation of I(θ). If the scale of I(θ) is relatively small when compared to its covariance, its estimation ˆI(θ) is more likely to be a small positive value (or even worse, zero or negative). The empirical computation of the CRLB is not meaningful in this case. 3 The Variance of the FIM Estimators
Based on the deep learning architecture speciﬁed in Eq. (3), we measure the quality of the two estimators ˆI1(θ) and ˆI2(θ) given by their variances. Given the same sample size N , a smaller variance is preferred as the estimator is more accurate and likely to be closer to the true FIM I(θ).
We study how the structure of the exponential family has an impact on the variance. 3.1 Variance in closed form
We ﬁrst consider ˆI1(θ) and ˆI2(θ) in Eq. (2) as real matrices of dimension dim(θ) × dim(θ). As
ˆI1(θ) is a square matrix, the corresponding covariance is a 4D tensor of dimen-sion dim(θ) × dim(θ) × dim(θ) × dim(θ), representing the covariance between the two elements h
Cov (cid:17)iijkl
ˆI1(θ) (cid:16) 4  
1 (θ) and ˆI kl
ˆI ij which we denote as Var(ˆI1(θ)). Thus, 1 (θ). The element-wise variance of ˆI1(θ) is a matrix with the same size of ˆI1(θ),
Var(ˆI1(θ))ij = (cid:16) h
Cov (cid:17)iijij
ˆI1(θ)
.
Similarly, the covariance and element-wise variance of ˆI2(θ) are denoted as
Var(ˆI2(θ))ij, respectively.
As the samples y1, . . . , yN are i.i.d., we have (cid:19) (cid:18)
Cov(ˆI1(θ)) =
Cov 1
N
∂ℓ
∂θ
∂ℓ
∂θ⊤ and Cov(ˆI2(θ)) =
Cov 1
N (cid:18) (cid:19)
.
∂2ℓ
∂θ∂θ⊤ h (cid:16)
Cov
ˆI2(θ) (cid:17)iijkl
Both Cov(ˆI1(θ)) and Cov(ˆI2(θ)) have an order of O(1/N ). For the neural network model in
Eq. (3), we further have those covariance tensors in closed form.
Theorem 4. (cid:16) h (cid:18) (cid:19) (cid:17)iijkl
Cov
ˆI1(θ)
· Cov
∂ℓ
∂θi
∂ℓ
∂θj
,
∂ℓ
∂θk
∂ℓ
∂θl (9) and (10)
· ∂iha
L(x)∂jhb
L(x)∂khc
L(x)∂lhd
L(x) · (Kabcd(t) − Iab(hL) · Icd(hL)) ,
=
= 1
N 1
N where the 4D tensor
Kabcd(t) := E [(ta − ηa(hL(x)))(tb − ηb(hL(x)))(tc − ηc(hL(x)))(td − ηd(hL(x)))] is the 4th (unscaled) central moment 1 of t(y) and ∂ihL(x) := ∂hL(x)/∂θi.
Remark 4.1. The 4D tensor (Kabcd(t) − Iab(hL) · Icd(hL)) is the covariance of the random ma-trix
∂ℓ
∂hL
∂ℓ
∂h⊤
L
= (t(y) − η)(t(y) − η)⊤, where y ∼ p(y | hL). This random matrix is an estimator of I(hL), i.e. the FIM w.r.t. the nat-ural parameters hL. Theorem 4 describes how the covariance tensor adapts w.r.t. the coordinate transformation hL → θ.
Notably, as t(y) is the sufﬁcient statistics of an exponential family, the derivatives of the log-partition function F (h) w.r.t. the natural parameters h are equivalent to the cumulants of t(y). The cumulants correspond to the coefﬁcients of the Taylor expansion of the logarithm of the moment generating function [20]. Importantly, the cumulants of order 3 and below are equivalent to the cen-tral moments (see e.g. Lemma 2). However, this is not the case for the 4th central moment which must be expressed as a combination of the 2nd and 4th cumulants, as stated in the following Lemma.
Lemma 5.
Kabcd(t) = κabcd + Iab(hL) · Icd(hL) + Iac(hL) · Ibd(hL) + Iad(hL) · Ibc(hL), where
∂4F (h)
∂ha∂hb∂hc∂hd
Remark 5.1. In the 1D case, the 4th central moment simpliﬁes to K(t) = F ′′′′(hL) + 3(F ′′(hL))2.
κabcd := h=hL(x)
.
For the second estimator ˆI2(θ), the covariance only depends on the 2nd central moment of t(y).
Theorem 6. h (cid:16)
Cov
ˆI2(θ) (cid:17)iijkl (cid:18)
= 1
N
· Cov
− ∂2ℓ
∂θi∂θj
, − ∂2ℓ
∂θk∂θl
= 1
N
· ∂2 ijhα
L(x)∂2 klhβ
L(x)Iαβ(hL), where ∂2 ijhL(x) := ∂2hL(x)/∂θi∂θj 2. 1The kurtosis of a random variable is deﬁned by its 4th standardized (both centered and normalized) moment.
Here, K(·) denotes the 4th central moment but not the kurtosis. 2In this paper, the derivatives are by default taken w.r.t. θ. Therefore, ∂i := ∂
∂θi 5 and ∂2 ij := ∂2
∂θi∂θj
. (cid:12) (cid:12) (cid:12) (cid:12) (cid:19)
Remark 6.1. By Lemma 2, the matrix Iαβ(hL) is the covariance of the sufﬁcient statistic t(y).
Hence, the covariance of ˆI2(θ) scales with the covariance of t(y). If t(y) tends to be deterministic, then the covariance of ˆI2(θ) shrinks towards 0 and its estimation of the FIM becomes accurate.
The covariance in Theorems 4 and 6 has two different components: (cid:192) the derivatives of the deep neural network; and ` the central (unscaled) moments of t(y). The 4D tensor Kabcd(t) and the 2D
FIM Iαβ(hL) correspond to the 4th and 2nd central moments of t(y), respectively. Intuitively, the larger the scale of the Jacobian or the Hessian of the neural network mapping θ → hL and/or the larger the central moments of the exponential family, the lower the accuracy when estimating the
FIM. 3.2 Variance Bounds
We aim to derive meaningful upper bounds of the covariances presented in Theorems 4 and 6. Using the Cauchy-Schwarz inequality, we can “decouple” the derivatives of the neural network mapping and the central moments of the exponential family into different terms. This provides various bounds on the scale of covariance quantities.
Theorem 7. (cid:16) (cid:13) (cid:13) (cid:13)Cov (cid:17)(cid:13) (cid:13) (cid:13)
≤ 1
N where ⊗ is the tensor-product: (I(hL) ⊗ I(hL))abcd := Iab(hL) · Icd(hL).
· ∥K(t) − I(hL) ⊗ I(hL)∥F ,
∂hL
∂θ
ˆI1(θ)
F
F
· (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 4 (cid:13) (cid:13) (cid:16) (cid:13) (cid:13) (cid:13)Cov (cid:17)(cid:13) (cid:13) (cid:13)
F
ˆI1(θ) measures how much the estimator ˆI1(θ) deviates from I(θ). Theo-The scale rem 7 says that this deviation is bounded by the scale of the Jacobian matrix ∂hL/∂θ as well as the scale of (K(t) − I(hL) ⊗ I(hL)). Recall from Remark 4.1 the latter measures the variance when estimating the FIM I(hL) of the exponential family. Theorem 7 allows us to study these two different factors separately. Similarly, we have an upper bound on the scale of the covariance of
ˆI2(θ).
Theorem 8. (cid:16) (cid:13) (cid:13) (cid:13)Cov (cid:17)(cid:13) (cid:13) (cid:13)
ˆI2(θ) (cid:13) (cid:13) (cid:13) (cid:13)
≤ 1
N
·
F
∂2hL(x)
∂θ∂θ⊤ (cid:13) (cid:13) 2 (cid:13) (cid:13)
F
· ∥I(hL)∥F .
On the RHS, the Hessian ∂2hL(x)/∂θ∂θ⊤ is a 3D tensor of shape nL × dim(θ) × dim(θ). There-fore, the variance of ˆI2(θ) is bounded by the scale of the Hessian, as well as the scale of the FIM
I(hL) of the output exponential family.
We consider an upper bound to further simplify related terms in Theorems 7 and 8.
Lemma 9.
∥K(t) − I(hL) ⊗ I(hL)∥
F
≤
∥I(hL)∥
F
≤ (cid:16)p (cid:17)
Kaaaa(t) + Iaa(hL)
!2
, nLX a=1
Iaa(hL).
√ 2 nLX a=1
Remark 9.1. Using Lemma 9, it is straightforward to bound the scale of the covariance tensors with the size of the Jacobian/Hessian, as well as the central moments Kaaaa(t) and Iaa(hL). These bounds are meaningful but omitted for brevity.
Remark 9.2. By Lemma 9, ∥K(t) − I(hL) ⊗ I(hL)∥F is in the order of O(n2 in the order of O(nL).
L) and ∥I(hL)∥F is
The scale of the tensors K(t) − I(hL) ⊗ I(hL) and I(hL) is bounded by the diagonal elements of
K(t) and I(hL), or the element-wise central moments of t(y). Understanding the scale of these 1D central moments helps to understand the scale of the moment terms in our key statements. 6  
Table 1: Cumulants of univariate exponential family distributions, given by derivatives of the log-partition function. p, µ and λ denote the mean of the Bernoulli, normal, and Poisson distributions, respectively. † The normal distribution has unit standard deviation (σ = 1).
DIST.
F (h)
BERNOULLI
NORMAL†
POISSON log(1 + exp(h)) h2/2 exp(h) h log p/1−p
µ log λ
∂2F (h) p(1 − p) 1
λ
∂4F (h) p(1 − p)(6p2 − 6p + 1) 0
λ (a) Bernoulli. (b) Normal (σ = 1). (c) Poisson.
Figure 1: The scale of K(t)−Var2(t) and Var(t) for the exponential family distributions in Table 1.
Table 1 presents some 1D exponential families and their cumulants. Figure 1 displays K(t)−Var2(t) and Var(t) against the mean of these distributions. Based on Fig. 1a, if the neural network has
Bernoulli output units, then the scale of Kaaaa(t) − (Iaa(hL))2 is smaller than Iaa(hL) regardless of hL. Notably, when p = 0.5, the variance of the ﬁrst estimator ˆI1(θ) is 0 — regardless of hL.
For normal distribution output units (corresponding to the mean squared error loss) in Fig. 1b, both central moment quantities are constant. For Poisson output units in Fig. 1c, Iaa(hL) increases linearly with the average number of events λ, while Kaaaa(t) − (Iaa(hL))2 increases quadratically.
Thus, the upper bound of ∥Cov(ˆI1(θ))∥F increases faster than the upper bound of ∥Cov(ˆI2(θ))∥F as hL enlarges. In this case, one may prefer ˆI2(θ) rather than ˆI1(θ) and/or control the scale of hL.
In general, hL is desired to be in certain regions in the parameter space of the exponential family to control the estimation variance of the FIM. Techniques to achieve this include regularization on the scale of hL; temperature scaling [12]; or normalization layers [4, 28]. Of course, they could inversely increase the scale of the derivatives of the neural network, which can be controlled by imposing additional constraints, i.e., Lipschitz requirements. 3.3 Positive Deﬁniteness
By deﬁnition, the FIM of any statistical model is positive semideﬁnite (p.s.d.). The ﬁrst estimator
ˆI1(θ) is naturally on the p.s.d. manifold (space of p.s.d. matrices). On the other hand, ˆI2(θ) can
“fall off” the p.s.d. manifold. It is important to examine the likelihood for ˆI2(θ) having a negative spectrum and the corresponding scale, so that any algorithm (e.g. natural gradient) relying of the
FIM being p.s.d. can be adapted.
Eq. (8) can be re-expressed as the sum of a p.s.d. matrix and a linear combination of nL symmetric matrices. We provide the likelihood for ˆI2(θ) staying on the p.s.d. manifold given conditions on the spectrum of the Hessian.
Theorem 10. Let λmin(·), λmax(·), and ρ(·) denote the smallest eigenvalue, the largest eigen-value, and the spectral radius (largest absolute value of the spectrum), respectively. Let ρ :=
L )). If λmin(I(θ)) > 0, then with probability at least (ρ(∂2h1
L), · · · ρ(∂2hnL 1 − nL · ∥ρ∥2 2
N · λ2
· λmax(I(hL)) min(I(θ))
, the estimator ˆI2(θ) with N samples is a p.s.d. matrix.
The bound becomes uninformative as the output layer size nL increases, as the spectrum of the
Hessian of hL scales up, or as the spectrum of the FIM I(hL) enlarges. On the other hand, as the 7
minimal eigenvalue of the FIM I(θ) increases, Theorem 10 can give meaningful lower bounds. In particular, with sample rate O(N −1), estimator ˆI2(θ) will be a p.s.d. matrix. In practice for over-parametrized networks, λmin(I(θ)) is close to or equals 0 and Theorem 10 is not meaningful. In any case, we need to consider the scale of the negative spectrum of ˆI2(θ).
Theorem 11. (cid:16) (cid:17)
ˆI2(θ)
λmin
≥ −ρ(∂2ha
L(x)) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)ηa − 1
N (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . ta(yi)
NX i=1
Theorem 11 guarantees that in the worst case, the scale of the negative spectrum of ˆI2(θ) is con-N trolled. By Lemma 2, Var(ηa − 1
I aa(hL). Therefore, as N increases or i=1 ta(yi)) = 1
N
N
I aa(hL) decreases, the negative spectrum of ˆI2(θ) will shrink. Further analysis on the spectrum of ˆI1(θ) and ˆI2(θ) can utilize the geometric structure of the p.s.d. manifold. This is left for future work.
P 3.4 Convergence Rate
The rate of convergence for each of the estimators is of particular interest when considering their practical viability. Through a generalized Chebyshev inequality [8], we can get a simple Frobenius norm convergence rate.
Lemma 12. Let 0 < ε < 1. Then (cid:13) (cid:13) (cid:13)ˆI1(θ) − I(θ) (cid:13) (cid:13) (cid:13)
F
≤ 1√
εN holds with probability at least 1 − ε; and (cid:13) (cid:13) (cid:13)ˆI2(θ) − I(θ) (cid:13) (cid:13) (cid:13)
F
≤ 1√
εN hold with probability at least 1 − ε.
·
· v u u t dim(θ)X
Var i,j=1 v u u t dim(θ)X i,j=1
Var (cid:18) (cid:18) (cid:19)
∂ℓ
∂θi
∂ℓ
∂θj (cid:19)
− ∂2ℓ
∂θi∂θj
Each of these convergence rates only depends on the element-wise variance of the estimator terms in
Eq. (9). Moreover, each of the estimators has a convergence rate of O(N −1/2). The rate’s constants are determined by the variance of the estimators given by Theorems 4 and 6, which are inﬂuenced by the derivatives of the neural network and the moments of the output exponential family. 4 Effect of Neural Network Derivatives
The derivatives of the deep learning network can affect the estimation variance of the FIM. By
Theorem 4, the variance of the ﬁrst estimator ˆI1(θ) scales with the Jacobian of the neural network mapping θ → hL(x). By Theorem 6, the variance of ˆI2(θ) scales with the Hessian of θ → hL(x).
The larger the scale of the Jacobian or the Hessian, the larger the estimation variance. In this section, we examine these derivatives in more detail.
We give the closed form gradient of the log-likelihood ℓ and the last layer’s output hL w.r.t. the neural network parameters.
Lemma 13.
∂ℓ
∂Wl l (t(y) − η(hL)) ,
∂ℓ
∂hl+1
= DlB⊤
∂ha
L
∂Wl
∂ℓ
∂hl
= B⊤ l+1ea
= Dl
¯h⊤ l ,
¯h⊤ l , where ea is the ath standard basis vector, Bl and Dl are recursively deﬁned by
BL = I, Bl = Bl+1DlW −
DL−1 = I, Dl = diag
, l
σ′(Wl (cid:0) (cid:1)
¯hl)
,
I is the identity matrix, and diag(·) means a diagonal matrix with given diagonal entries. 8
By Lemma 13, we can estimate the FIM w.r.t. the hidden representations hl through
ˆI1(hl) = 1
N
NX i=1
∂ℓi
∂hl
∂ℓi
∂h⊤ l
= B⊤ l 1
N
NX i=1 (t(yi) − η(hL)) (t(yi) − η(hL))
!
⊤
Bl. (11)
As Bl is recursively evaluated from the last layer to previous layers, the FIM can also be recur-sively estimated based on ˆI1(θ). It is similar to back-propagation, except that the FIMs are back-propagated instead of gradients of the network. This is similar to the backpropagated metric [23].
To investigate how the ﬁrst estimator ˆI1(θ) is affected by the loss landscape, we bound the Frobenius norm of the parameter-output Jacobian ∂hL/∂θ.
Lemma 14. If the activation function has bounded gradient and ∀z ∈ ℜ, |σ′(z)| ≤ 1, then (cid:13) (cid:13) (cid:13) (cid:13)
∂hL
∂Wl (cid:13) (cid:13) (cid:13) (cid:13)
F
= ∥Bl+1Dl∥F · ∥¯hl∥2 ≤
L−1Y i=l+1
∥W − i
∥F · ∥¯hl∥2, (12) where ∂hL/∂Wl = is a 3D tensor. (cid:2)
∂h1
L/∂Wl, · · · , ∂hnL
L /∂Wl (cid:3) is the derivative of a vector w.r.t. a matrix that
Given Lemma 14, we see that the gradient ∂hL/∂Wl scales with both the neural network weights
Wi and the gradient of the activation function Dl. Common activation functions have both bounded outputs and 1st-order derivatives; or at least are locally Lipschitz, i.e., sigmoid and ReLU activa-tion functions. During training, regularizing the scale of the neural network weights is a sufﬁcient condition for bounding the variance of ˆI1(θ).
An alternative bound can be established which depends on the maximum singular values of the weight matrices.
Lemma 15. Suppose that the activation function has bounded gradient ∀z ∈ ℜ, |σ′(z)| ≤ 1. Then (cid:13) (cid:13) (cid:13) (cid:13)
∂hL
∂Wl (cid:13) (cid:13) (cid:13) (cid:13)
≤
L−1Y 2σ i=l+1
! smax(W − i )
· ∥¯hl∥2, (13) where smax(·) denotes the maximum singular value and ∥T ∥2σ denotes the tensor spectral norm for a 3D tensor T , deﬁned by
∥T ∥2σ = max {⟨T , α ⊗ β ⊗ γ⟩ : ∥α∥2 = ∥β∥2 = ∥γ∥2 = 1} .
Therefore, regularizing smax(W − prove the estimation accuracy of the FIM. i ), or the spectral norm of the weight matrices, also helps to im-We further reveal the relationship between the loss landscape and the FIM estimators. For a given target ˜y, the log-likelihood is denoted as ˜l := log p( ˜y | x, θ). Furthermore, let us deﬁne ∆ˆI1(θ) := (∂˜l/∂θ)(∂˜l/∂θ⊤) − ˆI1(θ) and ∆ˆI2(θ) := −∂2˜l/∂θ∂θ⊤ − ˆI2(θ). By Eqs. (4) and (5),
#
"
∆ˆI1(θ) =
∂ha
L
∂θ
" (ta( ˜y) − ηa)(tb( ˜y) − ηb) − 1
N
#
NX i=1
∆ˆI2(θ) = 1
N ta(yi) − ta( ˜y)
∂2ha
L
∂θ∂θ⊤ .
NX i=1 (ta(yi) − ηa)(tb(yi) − ηb)
∂hb
L
∂θ⊤ ,
Hence, the difference between ˆI1(θ) (resp. ˆI2(θ)) and the squared gradient (resp. Hessian) of the loss −˜ℓ depends on how yi differs from ˜y. If the network θ is trained, then the random samples yi ∼ p(y | x, θ) are close to the given target ˜y. In this case, ˆI1(θ) corresponds to the squared gradient, and ˆI2(θ) corresponds to the Hessian. This is not true for untrained neural networks with random weights. 9    
5