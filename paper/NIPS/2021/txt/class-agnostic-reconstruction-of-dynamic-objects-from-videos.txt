Abstract
We introduce REDO, a class-agnostic framework to REconstruct the Dynamic
Objects from RGBD or calibrated videos. Compared to prior work, our problem setting is more realistic yet more challenging for three reasons: 1) due to occlusion or camera settings an object of interest may never be entirely visible, but we aim to reconstruct the complete shape; 2) we aim to handle different object dynamics including rigid motion, non-rigid motion, and articulation; 3) we aim to reconstruct different categories of objects with one uniﬁed framework. To address these challenges, we develop two novel modules. First, we introduce a canonical 4D implicit function which is pixel-aligned with aggregated temporal visual cues.
Second, we develop a 4D transformation module which captures object dynamics to support temporal propagation and aggregation. We study the efﬁcacy of REDO in extensive experiments on synthetic RGBD video datasets SAIL-VOS 3D and
DeformingThings4D++, and on real-world video data 3DPW. We ﬁnd REDO outperforms state-of-the-art dynamic reconstruction methods by a margin. In ablation studies we validate each developed component. 1

Introduction 4D (3D space + time) reconstruction of both the geometry and dynamics of different objects is a long-standing research problem, and is crucial for numerous applications across domains from robotics to augmented/virtual reality (AR/VR). However, complete and accurate 4D reconstruction from videos remains a great challenge for mainly three reasons: 1) partial visibility of objects due to occlusion or camera settings (e.g., out-of-view parts, non-observable surfaces); 2) complexity of the dynamics including rigid-motion (e.g., translation and rotation), non-rigid motion (deformation caused by external forces), and articulation; and 3) variability within and across object categories.
Existing work addresses the above challenges by assuming complete visibility through a multi-view setting [33, 4], or by recovering only the observable surface rather than the complete shape of an object [57], or by ignoring rigid object motion and recovering only the articulation [59], or by building shape templates or priors speciﬁc to a particular object category like humans [47]. However, these assumptions also limit applicability of models to unconstrained videos in the wild, where these challenges are either infeasible or only met when taking special care during a video capture.
In contrast, we aim to study the more challenging unconstrained 4D reconstruction setting where objects may never be entirely visible. Speciﬁcally, we deal with visual inputs that suffer from: 1) occlusion: the moving occluder and self-articulation cause occlusion to change across time; 2) cropped view: the camera view is limited and often fails to capture the complete and consistent appearance across time; 3) front-view only: due to limited camera motion, the back side of the objects are often not captured at all in the entire video. Moreover, we focus on different dynamic object-types
⇤Indicates equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Person (synthetic) 
Articulation, rigid motion
Car/Motorcycle (synthetic)
Non-rigid motion, rigid motion
Animal (synthetic)
Articulation, rigid motion
Person (real-world)
Articulation, rigid motion
Figure 1: We present REDO, a 4D reconstruction framework that predicts the geometry and dynamics of various objects for a given video clip. Despite objects being either occluded (e.g., car/motorcycle) or partially-observed, REDO recovers relatively complete and temporally smooth results. with complex motion patterns. These objects could either move in 3D space, or be deformed due to external forces, or articulate themselves. Importantly, we aim for a class-agnostic reconstruction framework which can recover the accurate shape at each time-step.
To achieve this we develop REDO. As illustrated in Fig. 1, REDO predicts the shape of different objects (e.g., human, animals, car) and models their dynamics (e.g., articulation, non-rigid motion, rigid motion) given input video clips. Besides the RGB frames, REDO takes as input the depth map, masks of the objects of interest, and camera matrices. In practice these inputs are realistic as depth-sensors are increasingly prevalent [19, 78] and as segmentation models [44, 17] are increasingly accurate and readily available, e.g., on mobile devices. If this data isn’t accessible, off-the-shelf tools are applicable (e.g., SfM [71], instance segmentation [26], video depth estimation [51]).
To address the partial visibility challenge introduced by occlusion or camera settings, REDO predicts a temporally coherent appearance in a canonical space. To ensure that the same model is able to reconstruct very different object types in a uniﬁed manner we introduce a pixel-aligned 4D implicit representation, which encourages the predicted shapes to closely align with the 2D visual inputs (§ 3.1). The visible parts from different frames of the video clip are aggregated to reconstruct the same object (§ 3.2). During inference, the reconstructed object in canonical space is propagated to other frames to ensure a temporally coherent prediction (§ 3.3).
REDO achieves state-of-the-art results on various benchmarks (§ 4). We ﬁrst conduct experiments on two synthetic RGBD video datasets: SAIL-VOS 3D [27] and DeformingThings4D++ [42].
REDO improves over prior 4D reconstruction work [59, 70] by a great margin (+5.9 mIoU, -0.085 mCham., -0.22 mACD on SAIL-VOS 3D and +2.2 mIoU, -0.063 mCham., -0.047 mACD on DeformingThings4D++ over OFlow [59]). We then test on the real-world calibrated video dataset 3DPW [86]. We ﬁnd that REDO generalizes well and consistently outperforms prior 4D reconstruction methods (+10.1 mIoU, -0.124 mCham., -0.061 mACD over OFlow). We provide a comprehensive analysis to validate the effectiveness of each of the introduced components. 2