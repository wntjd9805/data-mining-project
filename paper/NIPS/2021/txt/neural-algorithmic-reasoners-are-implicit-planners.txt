Abstract
Implicit planning has emerged as an elegant technique for combining learned mod-els of the world with end-to-end model-free reinforcement learning. We study the class of implicit planners inspired by value iteration, an algorithm that is guaran-teed to yield perfect policies in fully-speciﬁed tabular environments. We ﬁnd that prior approaches either assume that the environment is provided in such a tabular form—which is highly restrictive—or infer “local neighbourhoods” of states to run value iteration over—for which we discover an algorithmic bottleneck effect.
This effect is caused by explicitly running the planning algorithm based on scalar predictions in every state, which can be harmful to data efﬁciency if such scalars are improperly predicted. We propose eXecuted Latent Value Iteration Networks (XLVINs), which alleviate the above limitations. Our method performs all planning computations in a high-dimensional latent space, breaking the algorithmic bottle-neck. It maintains alignment with value iteration by carefully leveraging neural graph-algorithmic reasoning and contrastive self-supervised learning. Across eight low-data settings—including classical control, navigation and Atari—XLVINs provide signiﬁcant improvements to data efﬁciency against value iteration-based implicit planners, as well as relevant model-free baselines. Lastly, we empirically verify that XLVINs can closely align with value iteration. 1

Introduction
Planning is an important aspect of reinforcement learning (RL) algorithms, and planning algorithms are usually characterised by explicit modelling of the environment. Recently, several approaches explore implicit planning [40, 30, 33, 37, 29, 17, 16]. Such approaches propose inductive biases in the policy function to enable planning to emerge, while training the policy in a model-free manner.
Accordingly, implicit planners combine the effectiveness of large-scale neural network training with the data efﬁciency promises of planning, making them a very attractive research direction.
Many popular implicit planners attempt to align with the computations of the value iteration (VI) algorithm within a policy network [40, 30, 29, 13, 26]. As VI is a differentiable algorithm, guaranteed to ﬁnd the optimal policy, it can combine with gradient-based optimisation and provides useful theoretical guarantees. We also recognise the potential of VI-inspired deep RL, hence it is our primary topic of study here. However, applying VI assumes that the underlying RL environment (a) is tabular, and that its (b) transition and (c) reward distributions are both fully known and provided upfront.
Such assumptions are unfortunately unrealistic for most environments of importance to RL research.
Very often the dynamics of the environment will not be even partially known, and the state space may
⇤Work performed while the author was at DeepMind. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
either be continuous (e.g. for control tasks) or very high-dimensional (e.g. for pixel-space observation in Atari), making a tabular representation hard to realise from a storage complexity perspective.
Accordingly, VI-based implicit planners often offer representation learning based solutions for allevi-ating some of the above limitations. Impactful early work [40, 29, 26] showed that, in tabular settings with known transition dynamics, the reward distribution and VI computations can be approximated by a (graph) convolutional network. While highly insightful, this line of work still does not allow for
RL in generic non-tabular environments with unobserved dynamics. Conversely, approaches such as
ATreeC [13] and VPN [30] lift the remaining two requirements, by using a latent transition model to construct a “local environment” around the current state. They then use learned models to predict scalar rewards and values in every node of this environment, applying VI-style algorithms directly.
While such approaches apparently allow for seamless VI-based implicit planning, we discover that the prediction of scalar signals represents an algorithmic bottleneck: if the neural network has observed insufﬁcient data to properly estimate these scalars, the predictions of the VI algorithm will be equally suboptimal. This is limiting in low-data regimes, and can be seen as unfavourable, particularly given that one of the main premises of implicit planning is improved data efﬁciency.
In this paper, we propose the eXecuted Latent Value Iteration Network (XLVIN), an implicit planning policy network which embodies the computation of VI while addressing all of the limitations mentioned previously. We retain the favourable properties of prior methods while simultaneously performing VI in a high-dimensional latent space, removing the requirement of predicting scalars and hence breaking the algorithmic bottleneck. We enable this high-dimensional VI execution by leveraging the latest advances in neural algorithmic reasoning [43]. This emerging area of research seeks to emulate iterations of classical algorithms (such as VI) directly within neural networks. As a result, we are able to seamlessly run XLVINs with minimal conﬁguration changes on a wide variety of discrete-action environments, including pixel-based ones (such as Atari), fully continuous-state control and navigation. Empirically, the XLVIN agent proves favourable in low-data environments against relevant model-free baselines as well as the ATreeC family of models.
Our contributions are thus three-fold: (a) we provide a detailed overview of the prior art in value iteration-based implicit planning, and discover an algorithmic bottleneck in impactful prior work; (b) we propose the XLVIN implicit planner, which breaks the algorithmic bottleneck while retaining the favourable properties of prior work; (c) we demonstrate a successful application of neural algorithmic reasoning within reinforcement learning, both in terms of quantitative analysis of XLVIN’s data efﬁciency in low-data environments, and qualitative alignment to VI. 2