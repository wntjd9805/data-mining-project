Abstract
Ofﬂine reinforcement learning (RL) deﬁnes the task of learning from a ﬁxed batch of data. Due to errors in value estimation from out-of-distribution actions, most ofﬂine RL algorithms take the approach of constraining or regularizing the policy with the actions contained in the dataset. Built on pre-existing RL algorithms, modiﬁcations to make an RL algorithm work ofﬂine comes at the cost of additional complexity. Ofﬂine RL algorithms introduce new hyperparameters and often leverage secondary components such as generative models, while adjusting the underlying RL algorithm. In this paper we aim to make a deep RL algorithm work while making minimal changes. We ﬁnd that we can match the performance of state-of-the-art ofﬂine RL algorithms by simply adding a behavior cloning term to the policy update of an online RL algorithm and normalizing the data. The resulting algorithm is a simple to implement and tune baseline, while more than halving the overall run time by removing the additional computational overheads of previous methods. 1

Introduction
Traditionally, reinforcement learning (RL) is thought of as a paradigm for online learning, where the interaction between the RL agent and its environment is of fundamental concern for how the agent learns. In ofﬂine RL (historically known as batch RL), the agent learns from a ﬁxed-sized dataset, collected by some arbitrary and possibly unknown process [Lange et al., 2012]. Eliminating the need to interact with the environment is noteworthy as data collection can often be expensive, risky, or otherwise challenging, particularly in real-world applications. Consequently, ofﬂine RL enables the use of previously logged data or leveraging an expert, such as a human operator, without any of the risk associated with an untrained RL agent.
Unfortunately, the main beneﬁt of ofﬂine RL, the lack of environment interaction, is also what makes it a challenging task. While most off-policy RL algorithms are applicable in the ofﬂine setting, they tend to under-perform due to “extrapolation error”: an error in policy evaluation, where agents tend to poorly estimate the value of state-action pairs not contained in the dataset. This in turn affects policy improvement, where agents learn to prefer out-of-distribution actions whose value has been overestimated, resulting in poor performance [Fujimoto et al., 2019b]. The solution class for this problem revolves around the idea that the learned policy should be kept close to the data-generating process (or behavior policy), and has been given a variety of names (such as batch-constrained [Fujimoto et al., 2019b], KL-control [Jaques et al., 2019], behavior-regularized [Wu et al., 2019], or policy constraint [Levine et al., 2020]) depending on how this “closeness” is chosen to be implemented.
While there are many proposed approaches to ofﬂine RL, we remark that few are truly “simple”, and even the algorithms which claim to work with minor additions to an underlying online RL 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
algorithm make a signiﬁcant number of implementation-level adjustments. In other cases, there are unmentioned hyperparameters, or secondary components, such as generative models, which make ofﬂine RL algorithms difﬁcult to reproduce, and even more challenging to tune. Additionally, such mixture of details slow down the run times of the algorithms, and make causal attributions of performance gains and transfers of techniques across algorithms difﬁcult, as in the case for many online RL algorithms [Henderson et al., 2017, Tucker et al., 2018, Engstrom et al., 2020,
Andrychowicz et al., 2021, Furuta et al., 2021]. This motivates the need for more minimalist approaches in ofﬂine RL.
In this paper, we ask: can we make a deep RL algorithm work ofﬂine with minimal changes? We ﬁnd that we can match the performance of state-of-the-art ofﬂine RL algorithms with a single adjustment to the policy update step of the TD3 algorithm [Fujimoto et al., 2018]. TD3’s policy π is updated with the deterministic policy gradient [Silver et al., 2014]:
π = argmax
π
E(s,a)∼D[Q(s, π(s))]. (1)
Our proposed change, TD3+BC, is to simply add a behavior cloning term to regularize the policy:
π = argmax
π
E(s,a)∼D (cid:104)
λQ(s, π(s)) − (π(s) − a)2(cid:105)
, (2) with a single hyperparameter λ to control the strength of the regularizer. This modiﬁcation can be made by adjusting only a single line of code. Additionally, we remark that normalizing the states over the dataset, such that they have mean 0 and standard deviation 1, improves the stability of the learned policy. Importantly, these are the only changes made to the underlying deep RL algorithm.
To accommodate reproduciblity, all of our code is open-sourced1.
We evaluate our minimal changes to the TD3 algorithm on the D4RL benchmark of continuous control tasks [Fu et al., 2020]. We ﬁnd that our algorithm compares favorably against many ofﬂine
RL algorithms, while being signiﬁcantly easier to implement and more than halving the required computation cost. The surprising effectiveness of our minimalist approach suggests that in the context of ofﬂine RL, simpler approaches have been left underexplored in favor of more elaborate algorithmic contributions. 2