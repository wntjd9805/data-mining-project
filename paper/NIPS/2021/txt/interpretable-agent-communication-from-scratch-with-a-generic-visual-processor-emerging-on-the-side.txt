Abstract
As deep networks begin to be deployed as autonomous agents, the issue of how they can communicate with each other becomes important. Here, we train two deep nets from scratch to perform large-scale referent identiﬁcation through un-supervised emergent communication. We show that the partially interpretable emergent protocol allows the nets to successfully communicate even about object classes they did not see at training time. The visual representations induced as a by-product of our training regime, moreover, when re-used as generic visual features, show comparable quality to a recent self-supervised learning model. Our results provide concrete evidence of the viability of (interpretable) emergent deep net communication in a more realistic scenario than previously considered, as well as establishing an intriguing link between this ﬁeld and self-supervised visual learning. 1 1

Introduction
As deep networks become more effective at solving specialized tasks, there has been interest in letting them develop a language-like communication protocol so that they can ﬂexibly interact to address joint tasks [1]. One line of work within this tradition has focused on what is arguably the most basic function of language, namely to point out, or refer to, objects through discrete symbols. Such ability would for example allow deep-net-controlled agents, such as self-driving cars, to inform each other about the presence and nature of potentially dangerous objects, besides being a basic requirement to support more advanced capabilities (e.g., denoting relations between objects).
While discreteness is not a necessary prerequisite for agent communication [2, 3], practical and ethical problems might arise if communication is incomprehensible to humans. A discrete code analogous to language is certainly easier to decode for us, helping us to understand the agents’ decisions, and ultimately contributing to the larger goal of explainable AI [4].
In this paper, we study emergent discrete referential communication between two deep network agents that are trained from scratch on the task. We observe that the referential discrimination task played by the networks is closely related to pretext contrastive objectives used in self-supervised visual representation learning [5–8]. We exploit this insight to develop a robust end-to-end variant of a communication game. Our experiments conﬁrm that, in our setup: i) the nets develop a set of discrete symbols allowing them to successfully discriminate objects in natural images, including novel ones that were not shown during training; ii) these symbols denote partially interpretable categories, so that their emergence can be seen as a ﬁrst step towards fully unsupervised image annotation; iii) the visual representations induced as a by-product can be used as high-quality general-purpose features, 1Code: https://github.com/facebookresearch/EGG/tree/master/egg/zoo/emcom_as_ssl. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
whose performance in various object classiﬁcation tasks is not lagging much behind that of features induced by a popular self-supervised representation method speciﬁcally designed for this task. 2