Abstract
The partially observable Markov decision process (POMDP) provides a general framework for modeling an agent’s decision process with state uncertainty, and online planning plays a pivotal role in solving it. A belief is a distribution of states representing state uncertainty. Methods for large-scale POMDP problems rely on the same idea of sampling both states and observations. That is, instead of exact belief updating, a collection of sampled states is used to approximate the belief; instead of considering all possible observations, only a set of sampled observations are considered. Inspired by this, we take one step further and propose an online planning algorithm, Adaptive Online Packing-guided Search (AdaOPS), to better approximate beliefs with adaptive particle ﬁlter technique and balance estimation bias and variance by fusing similar observation branches. Theoretically, our algorithm is guaranteed to ﬁnd an (cid:15)-optimal policy with a high probability given enough planning time under some mild assumptions. We evaluate our algorithm on several tricky POMDP domains, and it outperforms the state-of-the-art in all of them. Codes are available at https://github.com/LAMDA-POMDP/AdaOPS.jl. 1

Introduction
POMDPs generalize the MDPs by considering the state uncertainty [1]. In MDPs, the agent knows its state exactly. In POMDPs, though an MDP determines the underlying dynamics, the agent cannot access its actual state at each timestep and, instead, receives an observation serving as a clue. In order to make decisions in POMDPs, the state uncertainty must be handled. A popular method for tackling the uncertainty is to maintain a belief, a distribution on the state space, where the probability of a state indicates how possible the agent believes it is. However, an exact belief updating requires O(|S|2) computations, which is unaffordable in immense state space. A practical way is to approximate the belief with a collection of weighted particles (or samples) and update it with particle ﬁltering [2].
Finding an optimal solution for ﬁnite-horizon POMDPs or a near-optimal solution for discounted inﬁnite-horizon POMDPs is proved to be PSPACE-complete [3, 4]. Therefore, online planning, which computes a real-time solution, plays a crucial role in solving POMDPs, following for two reasons. The ﬁrst is that online planning searches for a solution for not all possible beliefs but the current belief alone, relieving the computational load signiﬁcantly. Second, online planning can take advantage of the approximate ofﬂine solution and other heuristics to expedite the tree search process.
Online planning algorithms have made substantial progress in solving large-scale POMDP problems
[5–9]. These methods approximate the belief with a collection of sampled states instead of updating
∗Corresponding Author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
it exactly and consider a set of sampled observations instead of all possible observations. However, the estimation derived by simple belief approximation methods that have been applied, such as direct sampling and sequential importance sampling [10], have a variance increasing exponentially with searching depth. Besides, naive sampling for observations, either the direct sampling as in [7] or importance sampling as in [9], yields a large variance when the sample size is small. Inspired by this, we take one step further and propose an online planning algorithm, Adaptive Online Packing-guided
Search (AdaOPS), to better approximate beliefs and better balance estimation bias and variance. It features two innovations, adaptive particle ﬁltering, and belief packing.
The adaptive particle ﬁlter is designed for online planning. During the belief updating, it resamples adaptively and adjusts the sample size on the ﬂy according to the dispersion of the belief distribution, which allows achieving high approximation accuracy with only a handful of particles.
AdaOPS forms a belief packing by merging similar beliefs. Only beliefs in the packing are explored, hence the name “packing-guided search”. Belief packing is meant to balance estimation bias and variance. With the use of beliefs in their exact form, it is guaranteed that similar beliefs yield a slight difference in their optimal values [11]. We further extend this conclusion to cases where beliefs are approximated by a collection of weighted particles and prove that by merging similar beliefs, our algorithm can converge to the (cid:15)-optimal policy with high probability under some mild assumptions. 2