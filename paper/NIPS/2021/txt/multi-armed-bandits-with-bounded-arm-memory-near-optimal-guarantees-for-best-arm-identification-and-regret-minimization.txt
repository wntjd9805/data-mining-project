Abstract
We study the Stochastic Multi-armed Bandit problem under bounded arm-memory.
In this setting, the arms arrive in a stream, and the number of arms that can be stored in the memory at any time, is bounded. The decision-maker can only pull arms that are present in the memory. We address the problem from the perspective of two standard objectives: 1) regret minimization, and 2) best-arm identiﬁcation.
For regret minimization, we settle an important open question by showing an almost tight guarantee. We show Ω(T 2/3) cumulative regret in expectation for single-pass algorithms for arm-memory size of (n − 1), where n is the number of arms. For best-arm identiﬁcation, we provide an (ε, δ)-PAC algorithm with arm-memory size of O(log∗ n) and O( n
δ )) optimal sample complexity.
ε2 · log( 1 1

Introduction
The Stochastic Multi-armed Bandit (MAB) problem is a classical framework used to capture decision-making in uncertain environments. In this model, a decision-maker is faced with n choices (called arms) and has to sequentially choose one of the n arms (referred to as pulling an arm). Based on the pulled arm, the decision-maker gets a reward drawn from a corresponding reward distribution unknown to the decision-maker. Starting with the seminal work of Robbins [26], MAB has been extensively studied with one of the following two goals: regret minimization and best-arm identiﬁca-tion. See, e.g. [7] for a textbook treatment of the area. In addition to being theoretically interesting,
MAB also ﬁnds numerous practical applications in diverse areas, including online advertising [31], crowd-sourcing [32], and clinical trials [9]. Hence, the study of MAB and its variants is of paramount interest in multiple ﬁelds, including online learning and reinforcement learning.
In many applications of the MAB problem, the number of arms (set of advertisements, crowd-workers, etc.) could be huge. Thus, the MAB problem with a large set of arms has received signiﬁcant attention in recent years. Starting with the formative work of Berry et al. [6] and Herschkorn et al. [16] which studied the MAB problem with inﬁnitely many arms, to the more recent work of Kleinberg et al. [20] which studies the MAB problem where the set of arms form a metric space, this variant has remained an active and prominent area of research. Large number of arms may enforce a constraint that the algorithm may not be able to store all the arms in the memory simultaneously. Additionally, as is common in these applications, the arms could arrive online, i.e., the algorithm may not have access to the entire set of arms at the beginning.
The streaming model, ﬁrst formalized in the pioneering work of Alon et al. [3], has been developed to handle data streams where a huge amount of data arrives online and an algorithm has access
∗A part of this work was done when the author was an undergraduate summer intern at IISc, Bangalore. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to only a limited amount of memory. We refer the readers to [23, 2] for a detailed survey of streaming algorithms. With the ever-increasing amount of data available to any machine learning (ML) algorithm, several salient areas of ML are being studied under streaming constraints; e.g., active learning [35, 29], lifelong learning [33], and identifying correlations in multivariate data [11] (see the recent survey by Gomes et al. [15] for further discussions).
In this work, we study the classical stochastic MAB problem under streaming constraints. This model has recently been studied in [27, 21, 4]. In particular, we study a setting where we are given a set of n arms arriving one-by-one, and an algorithm can store only a ﬁxed number of arms m < n in the memory. The set of arms stored in the memory is known as arm-memory and m denotes the upper bound on the number of arms that can be stored in the memory at any time step. In this model, at any time step an arm not in the current arm-memory can not be pulled. Further, for any arm currently not in the arm-memory, we assume we do not know anything about the reward statistics corresponding to that arm until that arm is read into arm-memory and subsequently pulled. Note that the classical
MAB algorithms such as UCB1 [5] and Thompson Sampling [30] for regret minimization, and
Median-Elimination [13] for best-arm identiﬁcation are not suited for this streaming setting as they require all arms to be present in the arm-memory at the start of the algorithm. Our goal is to study the interplay between arm-memory size vs. expected regret and arm-memory size vs. sample complexity, respectively.
An algorithm for our setting works as follows: at any time step t, the algorithm can only pull an arm that is currently in arm-memory and update its reward statistics. Then, the algorithm may choose to discard some of the arms that are currently in the arm-memory and read new arms into the arm-memory before proceeding to the next time-step to pull an arm from the arm-memory. In streaming terminology, algorithms that can not read back an arm into the arm-memory that was previously discarded from the arm-memory, are called single-pass or one-pass algorithms. On the other hand, algorithms that are allowed to read back an arm into the arm-memory that was previously discarded from the arm-memory, are called multi-pass algorithms. We note that our algorithms are designed for the more challenging single-pass setting, which has also been the focus of other prominent areas in machine learning, for example, Rai et al. [25] focus on designing one-pass SVMs when the data arrives in a stream, whereas Carvalho and Cohen [8] design single-pass algorithms for online learning. 1.1 Our Contribution
We study MAB under bounded arm-memory, where n arms arrive in a stream and at most m < n arms can be stored in the arm-memory at any time. We study the trade-off between arm-memory size vs. expected regret, and arm-memory size vs. sample complexity.
Regret minimization: Our ﬁrst result, in Section 3, settles an open question stated in both [21] and [27] pertaining to the lower bound on the expected cumulative regret in this model. Using information-theoretic machinery related to KL-divergence, we show that any single-pass algorithm in this model will incur an expected regret of Ω(cid:0)n1/3 · T 2/3/m7/3(cid:1). Interestingly, this result holds for any m < n, which shows that even if the algorithm is allowed to store n − 1 arms in the arm-memory at any time, we cannot hope to obtain a better regret guarantee in terms of T . This lower bound almost matches with the ˜O(n1/3T 2/3) bound on the expected cumulative regret, obtained by the standard uniform-exploration algorithm where we store m = 2 arms. We note here that the uniform-exploration algorithm with m = 2 can be trivially extended to an algorithm that stores m > 2 arms in the memory and achieves a regret guarantee that matches the lower bound by storing the 2 arms required by the uniform-exploration algorithm and ﬁlling the remaining m − 2 sized arm-memory with arbitrary arms that would have otherwise been discarded. Since this is already optimal for the single-pass setting, we cannot hope to get a better regret guarantee in terms of T , even if we store n − 1 arms in the memory. This shows an interesting dichotomy between m = n − 1 and m = n, as one can obtain expected cumulative regret of ˜O(cid:0)√ nT ) by standard UCB1 algorithm [5], where we are allowed to store n arms. Our lower bound holds even when the arms arrive in a uniformly random sequence.
Best-arm identiﬁcation: In Section 4, we propose an (ε, δ)-PAC streaming algorithm that takes r as input and stores r + 1 arms in the arm-memory at any time and has sample complexity O( n
ε2 · 2
ε2 log( 1
δ ))) 2. In particular, when r = log∗ n, our algorithm achieves the optimal worst-δ )(cid:1) for any best-arm identiﬁcation algorithm [13]. (ilog(r)(n) + ln( 1 case sample complexity O(cid:0) n
This problem was also studied in recent STOC’20 paper by Assadi and Wang [4] where they proposed an (ε, δ)-PAC algorithm with optimal sample complexity and O(log∗ n) arm-memory size. However, we show that due to an error in their analysis, their claim is incorrect and their algorithm is not (ε, δ)-PAC. In Appendix E, we construct a family of input instances for which their algorithm will output a non-ε-best arm with probability signiﬁcantly larger than δ. In Section 4, we show that the special case of our algorithm (when r = log∗ n) indeed provides the guarantee claimed in [4]. 1.2