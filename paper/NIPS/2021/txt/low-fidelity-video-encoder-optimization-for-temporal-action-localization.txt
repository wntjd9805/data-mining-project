Abstract
Most existing temporal action localization (TAL) methods rely on a transfer learn-ing pipeline, ﬁrst optimizing a video encoder on a large action classiﬁcation dataset (i.e., source domain), followed by freezing the encoder and training a TAL head on the action localization dataset (i.e., target domain). This results in a task dis-crepancy problem for the video encoder – trained for action classiﬁcation, but used for TAL. Intuitively, joint optimization with both the video encoder and TAL head is an obvious solution to this discrepancy. However, this is not operable for TAL subject to the GPU memory constraints, due to the prohibitive computational cost in processing long untrimmed videos. In this paper, we resolve this challenge by introducing a novel low-ﬁdelity (LoFi) video encoder optimization method. Instead of always using the full training conﬁgurations in TAL learning, we propose to reduce the mini-batch composition in terms of temporal, spatial or spatio-temporal resolution so that jointly optimizing the video encoder and TAL head becomes oper-able under the same memory conditions of a mid-range hardware budget. Crucially, this enables the gradients to ﬂow backwards through the video encoder conditioned on a TAL supervision loss, favourably solving the task discrepancy problem and providing more effective feature representations. Extensive experiments show that the proposed LoFi optimization approach can signiﬁcantly enhance the perfor-mance of existing TAL methods. Encouragingly, even with a lightweight ResNet18 based video encoder in a single RGB stream, our method surpasses two-stream (RGB + optical ﬂow) ResNet50 based alternatives, often by a good margin. Our code is publicly available at https://github.com/saic-ﬁ/loﬁ_action_localization . 1

Introduction
Video analysis has become an increasingly important area of research, encompassing multiple relevant problems such as action recognition [8, 13], temporal action localization [68, 7, 12, 22, 59, 60, 55, 67, 15], video grounding[36, 65, 64, 66, 49, 16, 24], and video question answering [25, 31]. Among those, temporal action localization (TAL) [68, 22] is a fundamental task, as natural videos are not temporally trimmed. Given an untrimmed video, TAL aims to identify the start and end points of all action instances and recognize their category labels simultaneously. A typical TAL model is based on deep convolutional neural networks (CNNs) composed of two modules: a video encoder and a TAL head.
The video encoder is often shared across different TAL methods (e.g., G-TAD [60], BC-GNN [3])
∗Work done during an internship at Samsung AI Centre. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
by taking a speciﬁc off-the-shelf action classiﬁcation model (e.g., C3D [51], I3D [8], TSM [33]), with the differences residing only in the TAL head. However, instead of short (e.g., 10 seconds) trimmed video clips as in action recognition, the input videos to a TAL model are characterized by much longer temporal duration (e.g., 120 seconds). This causes unique computational challenges that remain unsolved, particularly in model optimization.
In standard optimization of a TAL model, a two-stage transfer learning pipeline is often involved: 1. First, the video encoder is optimized on a large source video classiﬁcation dataset (e.g.,
Kinetics [28]) and, optionally, ﬁnetunned on the trimmed version of the target dataset under action classiﬁcation supervision; 2. Second, the video encoder is frozen and the TAL head is optimized on the target action localization dataset (e.g., ActivityNet [22], HACS [68]) under TAL task supervision.
With this widely-used TAL training pipeline, the video encoder is only optimal for action classiﬁcation but not for the target TAL task. Speciﬁcally, the video encoder is trained so that different short segments within an action sequence are mapped to similar outputs, thus encouraging insensitivity to the temporal boundaries of actions. This is not desirable for a TAL model. We identify this as a task discrepancy problem. Consequently, the ﬁnal TAL model could suffer from suboptimal performance.
Indeed, jointly optimizing all components of a CNN architecture end-to-end with the target task’s supervision is a common practice, e.g., training models for object detection in static images [18, 46, 38]. Unfortunately, this turns out to be non-trivial for TAL. As mentioned above, model training is severely restricted by the large input size of untrimmed videos and subject to the memory constraint of GPUs. This is why the two-stage optimization pipeline as described above becomes the most common and feasible choice in practice for optimizing a TAL model. On the other hand, existing transfer learning methods mostly focus on tackling the data distribution shift problem across different datasets [70, 50], rather than the task shift problem we study here. Regardless, we believe that solving this limitation of the TAL training design bears a great potential for improving model performance.
In this work, we present a simple yet effective low-ﬁdelity (LoFi) video encoder optimization method particularly designed for better TAL model training. It is designed to adapt the video encoder from action classiﬁcation to TAL whilst subject to the same hardware budget. This is achieved by introducing a simple strategy characterized by a new intermediate training stage where both the video encoder and the TAL head are optimized end-to-end using a lower temporal and/or spatial resolution (i.e., low-ﬁdelity) in the mini-batch construction. Compared to the standard training method, our proposed strategy does not increase the GPU memory standard (often a hard constraint for many practitioners). Crucially, with our LoFi training the gradients back-propagate to the video encoder from a temporal action localization loss whilst conditioned on the target TAL head, enabling the learning of a video encoder sensitive to the temporal localization objective.
We make the following contributions in this work. (1) We investigate the limitations of the standard optimization method for TAL models, and consider that the task discrepancy problem hinders the performance of existing TAL models. Despite it being a signiﬁcant ingredient, video encoder optimization is largely ignored by existing TAL methods, left without systematic investigation. (2)
To improve the training of TAL models, we present a novel, simple, and effective low-ﬁdelity (LoFi) video encoder optimization method. It is designed speciﬁcally to address the task discrepancy problem with the TAL model’s video encoder. (3) Extensive experiments show that the proposed LoFi optimization method yields new state-of-the-art performance when combined with off-the-shelf TAL models (e.g., G-TAD [60]). Critically, our method achieves superior efﬁciency/accuracy trade-off with clear inference cost advantage and good generalizability to varying-capacity video encoders. 2