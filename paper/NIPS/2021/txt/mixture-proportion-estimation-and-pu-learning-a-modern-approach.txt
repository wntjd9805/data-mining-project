Abstract
Given only positive examples and unlabeled examples (from both positive and negative classes), we might hope nevertheless to estimate an accurate positive-versus-negative classiﬁer. Formally, this task is broken down into two subtasks: (i) Mixture Proportion Estimation (MPE)—determining the fraction of positive examples in the unlabeled data; and (ii) PU-learning—given such an estimate, learning the desired positive-versus-negative classiﬁer. Unfortunately, classical methods for both problems break down in high-dimensional settings. Meanwhile, recently proposed heuristics lack theoretical coherence and depend precariously on hyperparameter tuning. In this paper, we propose two simple techniques: Best
Bin Estimation (BBE) (for MPE); and Conditional Value Ignoring Risk (CVIR), a simple objective for PU-learning. Both methods dominate previous approaches empirically, and for BBE, we establish formal guarantees that hold whenever we can train a model to cleanly separate out a small subset of positive examples.
Our ﬁnal algorithm (TED)n, alternates between the two procedures, signiﬁcantly improving both our mixture proportion estimator and classiﬁer1. 1

Introduction
When deploying k-way classiﬁers in the wild, what can we do when confronted with data from a previously unseen class (k ` 1)? Theory dictates that learning under distribution shift is impossible absent assumptions. And yet people appear to exhibit this capability routinely. Faced with new surprising symptoms, doctors can recognize the presence of a previously unseen ailment and attempt to estimate its prevalence. Similarly, naturalists can discover new species, estimate their range and population, and recognize them reliably going forward.
To begin making this problem tractable, we might make the label shift assumption [37, 41, 29], i.e., that while the class balance ppyq can change, the class conditional distributions ppx|yq do not. Moreover, we might begin by focusing on the base case, where only one class has been seen previously, i.e., k “ 1. Here, we possess (labeled) positive data from the source distribution, and (unlabeled) data from the target distribution, consisting of both positive and negative instances. This problem has been studied in the literature as learning from positive and unlabeled data [8, 27] and has typically been broken down into two subtasks: (i) Mixture Proportion Estimation (MPE) where we estimate α, the fraction of positives among the unlabeled examples; and (ii) PU-learning where this estimate is incorporated into a scheme for learning a Positive-versus-Negative (PvN) binary classiﬁer.
Traditionally, MPE and PU-learning have been motivated by settings involving large databases where unlabeled examples are abundant and a small fraction of the total positives have been extracted. For example, medical records might be annotated indicating the presence of certain diagnoses, but the unmarked passages are not necessarily negative. This setup has also been motivated by protein and 1Code is available at https://github.com/acmi-lab/PU_learning 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of proposed methods. (left) Estimate of α with varying fraction of unlabeled examples in the top bin. The shaded region highlights the upper and lower conﬁdence bounds. BBE selects the top bin that minimizes the upper conﬁdence bound. (right) Accuracy and MPE estimate as training proceeds. Till 100-th epoch (vertical line), we perform PvU training, i.e., warm start for (TED)n. Post 100-th epoch, we continue with both (TED)n and PvU training. Note that (TED)n improves both classiﬁcation accuracy and MPE compared to PvU training. Results with Resnet-18 on binary-CIFAR. For details and comparisons with other methods, see Sec. 6. gene identiﬁcation [16]. Databases in molecular biology often contain lists of molecules known to exhibit some characteristic of interest. However, many other molecules may exhibit the desired characteristic, even if this remains unknown to science.
Many methods have been proposed for both MPE [16, 12, 39, 35, 21, 4, 36, 20] and PU-learning [14, 11, 23]. However, classical MPE methods break down in high-dimensional settings [35] or yield estimators whose accuracy depends on restrictive conditions [12, 39]. On the other hand, most recent proposals either lack theoretical coherence, rely on heroic assumptions, or depend precariously on tuning hyperparameters that are, by the very problem setting, untunable. For PU learning, Elkan and Noto [16] suggest training a classiﬁer to distinguish positive from unlabeled data followed by a rescaling procedure. Du Plessis et al. [11] suggest an unbiased risk estimation framework for PU learning. However, these methods fail badly when applied with model classes capable of overﬁtting and thus implementations on high-dimensional datasets rely on extensive hyperparameter tuning and additional ad-hoc heuristics that do not transport effectively across datasets.
?
In this paper, we propose (i) Best Bin Estimation (BBE), an effective technique for MPE that produces consistent estimates pα under mild assumptions and admits ﬁnite-sample statistical guarantees achiev-ing the desired Op1{ nq rates; and (ii) learning with the Conditional Value Ignoring Risk (CVIR) objective, which discards the highest loss pα fraction of examples on each training epoch, removing the incentive to overﬁt to the unlabeled positive examples. Both methods are simple to implement, com-patible with arbitrary hypothesis classes (including deep networks), and dominate existing methods in our experimental evaluation. Finally, we combine the two in an iterated Transform-Estimate-Discard (TED)n framework that signiﬁcantly improves both MPE estimation error and classiﬁer error.
We build on label shift methods [29, 3, 2, 34, 17], that leverage black-box classiﬁers to reduce dimensionality, estimating the target label distribution as a functional of source and target push-forward distributions. While label shift methods rely on classiﬁers trained to separate previously seen classes, BBE is able to exploit a Positive-versus-Unlabeled (PvU) target classiﬁer, which gives each input a score indicating how likely it is to be a positive sample. In particular, BBE identiﬁes a threshold such that by estimating the ratio between the fractions of positive and unlabeled points receiving scores above the threshold, we obtain the mixture proportion α.
BBE works because in practice, for many datasets, PvU classiﬁers, even when uncalibrated, produce outputs with near monotonic calibration diagrams. Higher scores correspond to a higher proportion of positives, and when the positive data contains a separable sub-domain, i.e., a region of the input space where only the positive distribution has support, classiﬁers often exhibit a threshold above which the top bin contains mostly positive examples. We show that the existence of a (nearly) pure top bin is sufﬁcient for BBE to produce a (nearly) consistent estimate pα, whose ﬁnite sample convergence 2
rates depend on the fraction of examples in the bin and whose bias depends on the purity of the bin.
Crucially, we can estimate the optimal threshold from data.
We conduct a battery of experiments both to empirically validate our claim that BBE’s assumptions are mild and frequently hold in practice, and to establish the outperformance of BBE, CVIR, and (TED)n over the previous state of the art. We ﬁrst motivate BBE by demonstrating that in practice
PvU classiﬁers tend to isolate a reasonably large, reasonably pure top bin. We then conduct extensive experiments on semi-synthetic data, adapting a variety of binary classiﬁcation datasets to the PU learning setup and demonstrating the superior performance of BBE and PU-learning with the CVIR objective. Moreover, we show that (TED)n, which combines the two in an iterative fashion, improves signiﬁcantly over previous methods across several architectures on a range of image and text datasets. 2