Abstract
Graph Convolutional Networks (GCNs), aiming to obtain the representation of a node by aggregating its neighbors, have demonstrated great power in tackling vari-ous analytics tasks on graph (network) data. The remarkable performance of GCNs typically relies on the homophily assumption of networks, while such assumption cannot always be satisﬁed, since the heterophily or randomness are also widespread in real-world. This gives rise to one fundamental question: whether networks with different structural properties should adopt different propagation mechanisms?
In this paper, we ﬁrst conduct an experimental investigation. Surprisingly, we discover that there are actually segmentation rules for the propagation mechanism, i.e., 1-hop, 2-hop and k-nearest neighbor (kNN) neighbors are more suitable as neighborhoods of network with complete homophily, complete heterophily and randomness, respectively. However, the real-world networks are complex, and may present diverse structural properties, e.g., the network dominated by homophily may contain a small amount of randomness. So can we reasonably utilize these segmentation rules to design a universal propagation mechanism independent of the network structural assumption? To tackle this challenge, we develop a new univer-sal GCN framework, namely U-GCN. It ﬁrst introduces a multi-type convolution to extract information from 1-hop, 2-hop and kNN networks simultaneously, and then designs a discriminative aggregation to sufﬁciently fuse them aiming to given learn-ing objectives. Extensive experiments demonstrate the superiority of U-GCN over state-of-the-arts. The code and data are available at https://github.com/jindi-tju. 1

Introduction
Real-world complex systems can often be viewed as networks, such as social networks, biological networks and citation networks. Recently, research of analyzing networks with deep learning has received widespread attention both in academia and industry. In particular, Graph Convolutional
Networks (GCNs) [14], which obtain the meaningful representation of nodes in the network by integrating the neighborhood information, have achieved great success and been widely applied in tackling network analytics tasks, such as node classiﬁcation [23, 28], link prediction [33] and recommendation [30, 17].
While the success of GCNs and their variants [1, 6], a key weakness is the homophily assumption of networks, which restricts their performance on general network data. To be speciﬁc, most GCNs
†Equal contributions.
*Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
seem to be tailor-made to work on homophily networks [18], where nodes within the same class tend to connect with each other. In fact, heterophily [20] networks, where nodes of different classes tend to link together, are also widespread in real-world. For example, different types of amino acids are more likely to be connected in protein structure [35], and fraudsters tend to connect to accomplices than to other fraudsters in transaction networks [21]. Furthermore, the random networks also often exist in real-world, such as railway networks, where the edges between nodes are more likely to be randomly generated.
Most popular GCNs typically obtain node embeddings of these networks using 1-hop network neighbors as neighborhoods for information propagation [12, 29]. However, considering the different structural properties of networks (e.g., homophily or heterophily), whether different networks should adopt a uniﬁed or different propagation mechanisms? This is a very important question for GCNs since they mainly gain better performance through the propagation of information. A well informed answer can help us better understand the essence of GCNs, such as how different types of nodes affect the propagation, and what type of nodes are really required to achieve a certain level of predictive accuracy aiming to different networks.
Several recent works have studied the networks with different structural properties. For example,
Pei et al. [22] consider the heterophlily property of networks, and propose a geometric aggregation scheme to overcome neighborhood structural information losing and long-range dependencies lacking.
Zhu et al. [35] design an effective model which improves the representation power of GCNs under heterophily through theoretical and empirical analysis. Chien et al. [4] introduce a new generalized pageRank (GPR) architecture to jointly optimize node feature and topological information extraction.
Bo et al. [2] assess the roles of low-frequency and high-frequency signals, and propose an efﬁcient method that can adaptively integrate different signals in the process of message passing. However, there is still a lack of insightful understanding from the perspective of propagation mechanism.
As the ﬁrst contribution of this study, we conduct experiments analysing the propagation mechanism of GCNs in networks with different structural properties. Surprisingly, our experiments clearly illustrate that for networks with complete homophily, complete heterophily and randomness, 1-hop, 2-hop and k-nearest neighbor (kNN) neighbors are more suitable as neighborhoods for information propagation, respectively. This means that the depicting ability of the current propagation mechanism of GCNs is limited, and networks with different structural properties may need to adopt different propagation mechanisms.
In fact, while these segmentation rules seem to be able to select appropriate nodes as neighborhoods in an ideal way, the real-world networks are complex, and may present diverse properties, e.g., the network dominated by homophily may contain a small amount of randomness or heterophily.
A natural question is, “Can we reasonably utilize these segmentation rules to design a universal propagation mechanism independent of the network structural assumption?"
To tackle this challenge, we propose a novel and universal GCN model, i.e., U-GCN, for general network data. The central idea is that we learn node embeddings by making full use of the information from 1-hop, 2-hop and kNN neighbors, and fuse them adaptively to derive deeper correlation infor-mation for the given learning objectives. To be speciﬁc, we ﬁrst introduce a multi-type convolution mechanism. It uses 1-hop network (i.e., original input network), 2-hop network and kNN network that constructed by 1-hop, 2-hop and kNN neighbors for direct information propagation separately, and utilizes a node-level attention mechanism for each network, to extract three speciﬁc embeddings. We then make a discriminative aggregation to learn out the importance of these three embeddings, thereby extracting the most correlated information aiming to the ground truth such as node classiﬁcation.
Extensive experiments on a series of benchmark datasets demonstrate the superiority of U-GCN over some state-of-the-arts. 2 Notations and Preliminaries
Let G = (A, X) be an undirected attributed network, where A ∈ Rn×n represents the symmetric adjacency matrix with n nodes, and X ∈ Rn×p is the attribute (content) matrix of p attributes per node. Concretely, aij = 1 denotes there is an edge between nodes vi and vj, or 0 otherwise; and xi represents the attribute vectors of node vi. 2
Given an attribute network G, and a labeled node set VL containing u (cid:28) |V | nodes, where each node vi ∈ VL contains a unique class label yi ∈ Y . The goal of semi-supervised node classiﬁcation is to infer the labels of nodes in V \VL by learning a classiﬁcation function F.
Homophily. In this work, the level of homophily ratio of edges [35] is used to deﬁne networks with strong homophily/heterophily. Speciﬁcally, the level of homophily ratio of edges is the fraction of edges in a network which connect nodes that have the same class label (i.e., intra-class edges), described by:
α =
|(vi, vj) : aij = 1 ∧ yi = yj|
|m|
, (1) where m is the number of edges. Networks with α closer to 1 tend to have more edges connecting nodes within the same class, or stronger homophily; whereas networks with α closer to 0 have more edges connecting nodes in different classes, or stronger heterophily.
Graph Convolutional Network. Graph Convolutional Network (GCN) [14] is a variant of multi-layer convolutional neural networks that operates directly on networks. It learns embedding of each node by iteratively aggregating the information from its neighbors. Mathematically, let H (l) be the feature representation of the l-th layer, and H (0) be the node attribute matrix, the forward propagation can be deﬁned as: 2 H (l−1)W (l)),
H (l) = σ( ˜D− 1 2 ˜A ˜D− 1 (2) where ˜A = A + I stands for the adjacency matrix with self-loops, ˜D the node degree matrix of
˜A, i.e., ˜Dii = (cid:80)
˜Aij, W (l) a trainable weight matrix and σ the non-linear activation function.
While GCN works well on several network analysis tasks such as node classiﬁcation [10, 15], it still has a fundamental problem, that is, homophily assumption of networks, which leads to the main contribution in this work, i.e., analyse what type of nodes are more suitable as neighborhoods for direct information propagation independent of the network structural assumption. j 3 Motivating Observations
Here, we present a simple yet intuitive case study to illustrate and analyze the performance of GCN changes with different propagation mechanisms. The main idea is that we will ap-ply GCN to networks with different structural properties utilizing three types of nodes: 1-hop, 2-hop and k-nearest neighbor (kNN) neighbors, which are often believed to be the effective neighborhoods for node classiﬁcation in networks [28, 35], to realize the information propaga-tion, respectively. Then, we will check the performance of GCN on these cases. A univer-sal propagation mechanism should provide a good result in general network data. However, if the performance drops sharply in comparison with the other two situations, this will demonstrate that networks with different structural properties may need to use different propagation mechanisms.
Setup. We conduct experiments on the Newman ar-tiﬁcial networks [7] with different properties. The network consists 128 nodes divided into 4 classes, where each node has on average zin edges (i.e., intra-class edges) connecting to nodes of the same class and zout edges (i.e., inter-class edges) to nodes of other classes, and zin + zout = 16. Note that here we utilize two indicators: ρin = zin/32 and ρout = zout/96, to determine the network property, i.e., ρin > ρout, ρin = ρout and ρin <
ρout means the network with homophily, random-ness and heterophily, respectively.
For node attributes, we generate a 4h-dimensional binary attributes (i.e., xi) for each node to form 4 attribute clusters, corresponding to the 4 classes [9].
To be speciﬁc, for every node in the m-th class, we use a binomial distribution with mean pin = hin/h 3
Figure 1: The performance of GCN of using dif-ferent propagation mechanisms: 1-hop, 2-hop and kNN neighbors as neighborhoods respec-tively on Newman networks.
to generate a h-dimensional binary vector as its ((m − 1) × h + 1)-th to (m × h)-th attributes, and generated the rest attributes using a binomial distribution with mean pout = hout/(3h). In our experiments, we set 4h = 200 and hout = 4 (hin + hout = 16), so that pin > pout, the h-dimensional attributes are associated with the m-th class with a higher probability, whereas the rest 3h attributes are irrelevant.
As shown in Figure 1, for networks with strong homophily (e.g., ρout = 0.075), it is easy to obtain high accuracy using 1-hop network neighbors. However, as the inter-class edges increase, the accuracy is rapidly reduced. This mainly due to the homophily assumption, preventing GCN from effectively fusing information. On the other hand, for networks with strong heterophily (e.g., ρout = 0.165), it is surprising that, the accuracy of GCN of using 2-hop neighbors as neighborhoods (i.e., 83.15%) is much higher than that of using 1-hop network neighbors (i.e., 32.85%). Since the homophily ratio of 2-hop neighbors may rise with the increase of inter-class edges, GCN of using 2-hop neighbors is more effective to some extent. Interestingly, we can ﬁnd that GCN of utilizing kNN is easy to get the staple accuracy, i.e., 71.46%. In particular, it is much higher than those of using 1-hop and 2-hop neighbors on complete random network (i.e., ρout = 0.125).
Summary. This case study shows that the current propagation mechanism of GCN is not universal for general network data, but we can ﬁnd that there are rules in several special situations (i.e., complete random network). This motivates us that networks with different structural properties may need adopt different propagation mechanisms.
We conduct extra experiments on Newman networks [7] with complete homophily, randomness and complete heterophily utilizing GCN, so as to discover more appropriate propagation mechanism to select valuable nodes as neighborhoods, and thus improving the performance of GCNs for different networks. One straightforward strategy is to learn network embeddings with different GCNs using different types of nodes (i.e., 1-hop, 2-hop and kNN neighbors), and concatenate the embeddings into a single vector, so as to use the discriminative aggregation mechanism (which will be introduced in Section 4.2 below) to learn their importance for node classiﬁcation. We show the attention values as a function of the number of training iterations in Figure 2.
A
B
C
Figure 2: An example illustrating that the importance of three different types of neighbors (i.e., 1-hop, 2-hop and kNN neighbors) changes with network properties. The upper part of A-C represents networks with complete homophily, randomness and complete heterophily, respectively; while the lower part denotes the attention values as a function of the number of training iterations in corresponding networks.
Observation 1: Network with complete homophily tends to obtain better performance utilizing 1-hop network neighbors for direct information propagation. For the network in Figure 2A, the edges exist only in nodes within the same class, or complete homophily. As shown, 1-hop network neighbors 4
show great importance as the increase of training iterations. This partly validates that under the setting of complete homophily, 1-hop neighbors are more effective for direct information propagation compared to 2-hop and kNN neighbors.
Observation 2: Network with randomness tends to get better performance utilizing kNN for direct information propagation. The network in Figure 2B exhibits more randomness than Figure 2A, that is, complete random network. Obviously, with the increase of training iterations, the attention value of kNN is much higher than those of 1-hop and 2-hop neighbors. Since kNN typically constructed according to the similarity of node attributes, it can still realize the information fusion effectively, compared with the other two types of neighbors, in case that the network topology contains noise.
Observation 3: Network with complete heterophily tends to obtain better performance utilizing 2-hop neighbors for direct information propagation. For the network in Figure 2C, the edges exist only in nodes within different classes, or complete heterophily. While the learned attention values of these three types of neighbors differ slightly, the importance of 2-hop neighbors is relatively higher. This is mainly due to the fact that the homophily ratio of 2-hop neighbors becomes higher with the increase of inter-class edges (which is often the real life in many network analysis tasks).
While networks with different structural properties provide better performance utilize different propagation mechanisms, the real-world networks are complex, and may show diverse properties, e.g., the network dominated by homophily may contain a small amount of heterophily. Therefore, it is imperative to explore a universal propagation mechanism for GCNs independent of the network structural assumption.
Theorem 1. The real-world networks can be approximately decomposed into a mixture of three kinds of simple networks, namely complete homophily, complete random and complete heterophily, in different proportions.
Proof. Given a network G, where n and m denote the number of nodes and edges, respectively.
Assuming that r edges (0 ≤ r ≤ m) are generated randomly, i.e., the probability of nodes connecting nodes within the same class or different classes is the same, which form a complete random network (with n nodes and r edges). The remaining edges connecting two nodes within the same class can then be regarded as composing a complete homophily network, or a complete heterophily network.
Summary. Now, we can conclude that a universal GCN model may not only consider the 1-hop (Observation 1), but also the 2-hop (Observation 2) and kNN neighbors (Observation 3) for direct information propagation. More importantly, considering different network properties can be more correlated with one of them or even their combinations, the model itself should adaptively learn their corresponding importance, so as to achieve feature fusion more effectively. This case study, although leveraging speciﬁc artiﬁcial networks, is representative because real-world networks can often be considered as the combination of these three simple network cases. 4 Our Proposed Approach
To address the homophily assumption of GCNs, our basic idea is to design a universal GCN framework which is suitable for general networks with any structural properties. It can not only make full use of the information from 1-hop, 2-hop and kNN neighbors, but also fuse them sufﬁciently aiming to given learning objectives. In this section, we start by proposing a new simple multi-type convolution mechanism over three kinds of neighbors, and then introduce a discriminative aggregation to learn the importance of each part: 1-hop, 2-hop and kNN neighbors, automatically. 4.1 Multi-type Convolution Mechanism
To capture the information from 2-hop and kNN neighbors, we construct a 2-hop network GR = (AR, X) based on original input network GD = (AD, X), and a kNN network GF = (AF , X) based on node feature matrix X. 2-hop Network. For adjacency matrix AR, considering that the number of neighbors at exactly 2 hops away may raise exponentially with the increase of network scale, we introduce a constraint, i.e., select node pairs connected by at least two different paths for each node to set edges. Simultaneously, we adopt the classic two-layers GCN to perform message passing on this 2-hop network (AR, X), 5
and the l-th layer embedding matrix H (l)
R can be denoted as:
˜AR ˜D− 1
R H (l−1) 2
R = σ( ˜D− 1
H (l) 2
R W (l)
R ), (3)
R where ˜AR = AR + I, I is the identity matrix, ˜DR the diagonal degree matrix of ˜AR, W (l)
R the weight matrix and σ the non-linear activation function such as ReLU or Sigmoid. In this way, we can learn the node embeddings that capture the speciﬁc information from 2-hop neighbors. kNN Network. There are many ways to obtain kNN for each node, such as Jaccard similarity, Cosine similarity and Gauss kernel. In what follows, we calculate the similarity matrix S ∈ Rn×n among n nodes utilizing Cosine similarity, which adopts the cosine value of the angle between two vectors to measure the similarity. Mathematically, let xi be the feature vectors of node vi, the similarity sij between nodes vi and vj is deﬁned as: sij = xi · xj
|xi||xj|
. (4)
Then, the adjacency matrix AF can be obtained by choosing top k similar node pairs for each node to set edges. Accordingly, the l-th layer embedding matrix H (l)
F that gains the information from kNN can be calculated in the same way as in 2-hop network. Also of note, we use a linear algorithm
Ball-tree [16] for the calculation of kNN network which will not increase the complexity of GCN.
As for 1-hop network neighbors, we acquire the l-th layer embedding matrix H (l)
D performing direct information propagation on original input network GD = (AD, X). Therefore, the speciﬁc information encoded in 1-hop network neighbors can be extracted.
Node-level Attention. Before aggregating the information from original input network, 2-hop network and kNN network, we should note that the network-based neighbors of each node contribute to the embedding of the target node in different degrees. Here we adopt node-level attention [26] to learn the importance of network-based neighbors for each node. To be speciﬁc, given a node pair (vi, vj) and a speciﬁed network type t (where t ∈ {GD, GR, GF }), the importance coefﬁcient between nodes vi and vj can be formulated as: ij = LeakyReLU(µT et t [W hi||W hj]), αt ij = softmaxj(et ij) = exp(et ij) exp(et ir) (cid:80) r∈N t i
, (5) where µt is the parameterized attention vector for network type t, and W the mapping matrix applied to each node. Then, the embedding of node vi for network type t can be aggregated by the neighbor’s embeddings with its corresponding weight coefﬁcients as: ht i = σ( (cid:88)
αt ijW hj). j∈N t i (6) 4.2 Discriminative Aggregation
After the multi-type convolution above, we then perform a discriminative aggregation utilizing the attention mechanism, so as to learn the contributions of 1-hop, 2-hop and kNN neighbors automatically based on the given learning objectives. To be speciﬁc, for each node vi, let ht i denote its embedding in Ht, the attention value βt i can then be represented as: i = qT · tanh(Wt · (ht
βt i)T + bt), (7) where q denotes the parameterized attention vector, Wt the weight matrix and bt the bias vector.
After obtaining the attention value of each network, i.e., βD function to get the ﬁnal weight: i , we normalize them via softmax i , βR i , βF exp(βt i ) t exp(βt i )
Obviously, a larger γt i value means that the corresponding embedding is more important. The ouput embedding H can then be aggregated by these network-speciﬁc embeddings with its corresponding weight coefﬁcients as: i = softmax(βt
γt i ) = (8) (cid:80)
. i · H t.
γt (9)
H = (cid:88) t 6
Following GCN, we deﬁne the loss function by using cross entropy as:
L = − (cid:88)
F (cid:88) i∈YL f =1
Yif ln Hif , (10) where YL is the set of node indices that have labels, Y the label indicator matrix, and F the dimension of the output embedding, which is equal to the number of classes. 5 Experiments
We ﬁrst give the experimental setup, and then compare our U-GCN with some state-of-the-arts on node classiﬁcation. We ﬁnally give an in-depth analysis of different components of our new approach. 5.1 Experimental Settings
Datasets. We adopt eight public network datasets with edge homophily ratio α ranging from strong homophily to strong heterophily, as shown in Table 1, to evaluate the performance of different methods. We use three citation networks Cora, CiteSeer and PubMed [19, 25], two Wikipedia networks Chameleon and Squirrel [24], and three webpage networks1 Cornell, Wisconsin and Texas.
Baselines. We compare our U-GCN with eight baselines: (1) the methods utilizing both topological and attribute information: GCN [14], GAT [26], GraphSAGE [8], JK-Net [29], SSP [11], Geom-GCN
[22] and GCN-LPA [27], and (2) the method using node attribute: MLP. Especially, GCN is the base of our U-GCN.
Parameter Settings. For all methods, we set the dropout rate to 0.6 and use the same splits for training, validation and testing sets. We run 5 times with the same partition and report the average results. We employ the Adam optimizer with the learning rate setting to 0.005 and apply early stopping with a patience of 20. In addition, we set the number of attention heads to 8, weight decay
∈ {5e − 3, 5e − 4}, and k ∈ {3...7} for k-nearest neighbor network.
Table 1: Dataset Statistics.
Datasets
Cora
Pubm. Cite. Corn. Cham.
Squi. Wisc. Texa.
#Nodes
#Edges
#Features
#Classes
α 2708 5429 1433 7 0.83 19717 44338 500 3 0.79 3327 4732 3703 6 0.71 183 298 1703 5 0.30 2277 36101 2325 5 0.25 5201 217073 2089 5 0.22 251 515 1703 5 0.16 183 325 1703 5 0.06 5.2 Node Classﬁcation
On the node classiﬁcation task, we use accuracy as the evaluation metric, and the relevant results are summarized in Table 2. As shown, we observe that the U-GCN has consistently strong performance across the full spectrum of high-middle-low homophily. To be speciﬁc, on the dataset with strong homophily, e.g., Cora and Citeseer, U-GCN is comparable with the best baseline GAT that based on homophily assumption. On the dataset with middle homophily, e.g., Cornell and Chameleon,
U-GCN is 3.88% and 1.38% more accurate than the best baselines MLP and GCN-LPA, respectively.
Above all, on the dataset with strong heterophily, e.g., Wisconsin and Texas, our model U-GCN outperforms the best baseline MLP by a very large margin, i.e., 5.69% and 5.83%, which has been proved to be superior to a number of existing GNNs at the low level of homophily [35]. These results not only demonstrate the superiority of the new multi-type convolution mechanism that makes full use of the information from 1-hop, 2-hop and kNN neighbors, but also validate the effectiveness for distinguishing importance of information from different propagation mechanisms. In addition, the performance of U-GCN is much better than that of GCN, which further demonstrates the effectiveness of designing a universal propagation mechanism independent of network structural assumption. 1http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb 7
Table 2: Comparisons on node classiﬁcation (Percent).
Methods
Cora
Pubm.
Cite.
Corn. Cham.
Squi. Wisc.
Texa. 82.93
GCN 83.13
GAT 81.08
SSP 81.27
JK-Net
GraphSage 82.20
Geom-GCN 74.27 82.33
GCN-LPA 83.29 84.42 79.50 86.15 83.03 83.49 85.83 73.12 72.04 71.13 71.74 71.41 73.79 72.29 46.51 48.06 55.04 52.71 53.49 54.26 49.61
MLP 63.33 83.08 67.74 65.89
U-GCN 84.00 85.22 74.08 69.77 52.32 51.38 21.87 53.95 42.29 38.66 52.69 41.35 54.07 33.10 32.27 19.72 33.51 26.89 32.22 33.48 47.73 46.59 49.37 48.30 56.82 53.41 50.57 52.71 49.61 55.04 51.94 53.49 64.34 48.84 29.44 64.20 65.89 34.39 69.89 71.72 5.3 Ablation Study
Similar to most deep learning models, U-GCN also contains some important components that may have signiﬁcant impact on the performance. To gain deeper insight into the contributions of different components involved in our approach, we conduct experiments on comparing U-GCN with four variations. The variants are as follows: 1) GCN which serves as the base framework of U-GCN of using 1-hop network neighbors for propagation, 2) GCN of employing 2-hop neighbors for direct propagation, named as U-GCN-1, 3) GCN of utilizing kNN for direct propagation, named as U-GCN-2, and 4) U-GCN of removing 2-hop neighbors, named as U-GCN-3. We take their comparison on node classiﬁcation as an example.
As shown in Table 3, compared to GCN, U-GCN-1 (and U-GCN-2) of utilizing 2-hop neighbors (and kNN) is on average 3.37% (and 3.12%) more accurate on eight datasets. This validates that 2-hop neighbors (and kNN) play an important role during information propagation, especially on the networks dominated by heterophily. Furthermore, by introducing kNN, the derived U-GCN-3 improves performance of GCN (and U-GCN-2), i.e., on average 6.64% (and 3.32%) more accurate on eight datasets. This demonstrates that the performance of 1-hop and kNN neighbors can be mutually enhanced to a certain extent. In addition, U-GCN is on average 2.49% more accurate than U-GCN-3, which further validates the soundness of our new universal GCN framework that makes full use of different propagation mechanisms aiming to the network with diverse properties.
Table 3: Comparisons of our U-GCN with four variants on node classiﬁcation (Percent).
Methods
Cora
Pubm.
Cite.
Corn. Cham.
Squi. Wisc.
Texa. AVG
GCN
U-GCN-1
U-GCN-2
U-GCN-3
U-GCN 82.93 74.40 70.27 83.67 84.00 83.29 83.92 80.86 81.33 85.22 73.12 68.66 68.74 72.79 74.08 46.51 56.59 70.54 65.12 69.77 52.32 48.81 38.60 53.51 54.07 33.10 33.84 29.11 34.06 34.39 47.73 64.20 68.75 69.89 69.89 52.71 68.22 69.77 62.79 71.72 58.96 62.33 62.08 65.40 67.89 6