Abstract
This paper proposes a learning and scheduling algorithm to minimize the expected cumulative holding cost incurred by jobs, where statistical parameters deﬁning their individual holding costs are unknown a priori. In each time slot, the server can process a job while receiving the realized random holding costs of the jobs remaining in the system. Our algorithm is a learning-based variant of the cµ rule for scheduling: it starts with a preemption period of ﬁxed length which serves as a learning phase, and after accumulating enough data about individual jobs, it switches to nonpreemptive scheduling mode. The algorithm is designed to handle instances with large or small gaps in jobs’ parameters and achieves near-optimal performance guarantees. The performance of our algorithm is captured by its regret, where the benchmark is the minimum possible cost attained when the statistical parameters of jobs are fully known. We prove upper bounds on the regret of our algorithm, and we derive a regret lower bound that is almost matching the proposed upper bounds. Our numerical results demonstrate the effectiveness of our algorithm and show that our theoretical regret analysis is nearly tight. 1

Introduction
We consider the following algorithmic question: given a list of jobs, each of which requires a certain number of time steps to be completed while incurring a random cost in every time step until ﬁnished, learn the relative priorities of jobs and make scheduling decisions of which job to process in each time step, with the objective of minimizing the expected total cumulative cost. Here, we need an algorithm that seamlessly integrates learning and scheduling.
This question is motivated by several applications. Online social media platforms moderate content items, and here, jobs correspond to reviewing content items while the jobs have random costs driven by the number of accumulated views as views of harmful content items represent a community-integrity cost. Then the content review jobs are prioritized based on popularity of content items [18]. More generally, modern data processing platforms handle complex jobs whose characteristics are often unknown in advance [10], but as a system learns more about the jobs’ features, it may ﬂexibly adjust scheduling decisions to serve jobs with high priority ﬁrst. Another application is in optimizing energy consumption of servers in data centers, where a job waiting to be served uses energy-consuming resources [3, 7]. In emergency medical departments, patients undergo triage while being treated, and schedules for serving patients are ﬂexibly adjusted depending on their conditions [14, 15]. Note that patients’ conditions may get worse while waiting, which corresponds to holding costs in our problem.
For aircraft maintenance, diagnosing the conditions of parts and applying the required measures to repair them are conducted in a combined way [1].
We model and study the problem as a single-server scheduling system where job holding costs are according to stochastic processes with independent and identically distributed increments with unknown mean values, under the assumption that their mean service times are known. Recent works, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
e.g. [5], started investigating queuing system control policies under uncertainty about jobs’ mean service time parameters, but restricting to the setting where job holding costs are assumed to be deterministic (linear) functions of stochastic job waiting times. In our problem setting, job holding costs are stochastic in a different way in that job holding costs themselves are according to some stochastic processes. Under the aforementioned application scenarios, it is more natural to model a job’s holding costs by a stochastic process. As the ﬁrst step towards understanding the setting of stochastic job holding costs, we consider a single-server scheduling with a ﬁxed list of jobs.
Since the statistical parameters quantifying the random holding costs of jobs are unknown to the decision-maker, the main challenge is to learn the parameters of jobs, thereby obtaining a near-accurate priority ranking of jobs to ﬁnd a scheduling policy minimizing the total accumulated cost.
Problem formulation
We consider a discrete-time scheduling with a single server and N jobs. The jobs, labelled by i = 1, . . . , N , are present in the system from the beginning, and we assume no further job arrivals.
Every job remaining in a time slot incurs a random holding cost at the beginning of the time step, and holding costs incurred by a job are according to a sub-Gaussian distribution and i.i.d. across time slots. Sub-Gaussian distributions allow for different parametric distributions, e.g. Bernoulli and Poisson, which are suitable for modeling jobs incurring random costs incrementally. The mean holding costs of jobs are given by c1, . . . , cN whose values are a-priori unknown. The number of required time steps to complete job i is given deterministically by T /µi for each i, where T is a scaling parameter. Note that the larger the value of T the larger the number of per-job stochastic cost observations. The larger value of T does not necessarily mean that mean job service times are large in real time. In addition to the case of deterministic job service times, we also consider the case of stochastic job service times, assumed to be according to geometric distributions, which is a standard case studied in the context of queuing systems.
We analyze the performance of our scheduling policy against the minimum (expected) cumulative holding cost that can be achieved when the jobs’ mean holding cost values are fully known. The famous cµ rule, which sequentially processes jobs in the decreasing order of their ciµi values, is known to guarantee the minimum cumulative holding cost, so we use this as our benchmark.
Assuming c1µ1 ≥ c2µ2 ≥ · · · ≥ cN µN , we can deﬁne the regret of a scheduling algorithm ϕ as
R(T ) := C(T ) − (cid:88) (cid:88) ci
T /µj i∈[N ] j∈[i] where C(T ) is the expected cumulative holding cost of ϕ, i.e., C(T ) = (cid:80) i with T ϕ i denoting the completion time of job i and the second term on the right-hand side is the minimum expected cumulative holding cost, achievable by the cµ rule. Note that R(T ) is a random variable, as it may depend on randomness in the algorithm ϕ. We will consider E [R(T )], the expected regret of the algorithm. When ci’s and µi’s are constants, the minimum cumulative holding cost is O(N 2T ).
Here, our goal is to construct a policy whose expected regret is sublinear in T and subquadratic in N . i∈[N ] ciT ϕ
Previous work [5] has focused on the case when the values of µ1, . . . , µN are unknown while the values of c1, . . . , cN are known. As the service time of a job is instantiated only after completing it, this work relies on assumptions that there is a ﬁxed list of classes and multiple jobs may belong to the same class, which means that each µi can take one of a few values. In contrast, our paper is focused on addressing the challenge of uncertain costs. Our framework allows jobs to differ from each other (equivalently, each job corresponds to a unique class), but to make learning distinct jobs possible, we assume that the factor T in the service times is large. We can think of the large T regime as the case where the jobs’ features are observed frequently, as well as the setting of long job service times.
Our contributions
We develop our algorithm based on the empirical cµ rule, that is, the cµ rule applied with the current estimates of the mean holding costs. Since the ranking of jobs based on the ˆci,tµi values, where
ˆci,t denotes the empirical mean of job i’s holding cost in time slot t, may change over time, it is natural to come up with two types of the empirical cµ rule, preemptive and nonpreemptive. Under the preemptive empirical cµ rule, the server selects a job for every single time slot. In contrast, under the nonpreemptive version, once a job is selected in a certain time slot, the server has to commit to 2
serving this job until its completion, and then, it may select the next job based on the empirical cµ rule. The preemptive empirical cµ rule works well for instances with large gaps between the jobs’ mean holding costs, whereas the nonpreemptive one is better for cases where the jobs’ mean holding costs are close. However, it turns out that, under both the preemptive and nonpreemptive cases, the expected regret can grow linearly in T in the worst case. The preemptive case may result in undesired delays especially for jobs with similar values, while the nonpreemptive case may suffer from early commitment to a job with low priority.
Our policy, the preemptive-then-nonpreemptive empirical cµ rule, is basically a combination of the preemptive and nonpreemptive empirical cµ rules. This variant of empirical cµ rule has a ﬁxed length of preemption phase followed by nonpreemptive scheduling of jobs. The preemption period is long enough to separate jobs with large gaps in their mean holding costs, while it is not too long so that we can control delay costs from the preemption phase to be small, thereby avoiding undesired delays from continuous preemption and the risk of early commitment.
In Section 3, we give a theoretical analysis of our algorithm for the case of deterministic service times. We prove that the expected regret of our empirical cµ rule is sublinear in T and subquadratic in
N . We also show that this is near-optimal by providing a lower bound on the expected regret of any algorithm that has the same scaling in T while there is a small gap in terms of the dependence on N .
Furthermore, we give instance-dependent upper bounds that delineate how our algorithm performs depending on the gaps between ciµi values. In Section 4, we consider the setting where the service time of each job is stochastic and geometrically distributed. Our analysis reveals that the expected regret of our algorithm for the case of geometrically distributed service times is also sublinear in
T and subquadratic in N . Lastly, in Section 5, we discuss results from numerical experiments that demonstrate our algorithm’s performance and support our theoretical claims. 2