Abstract
Recently, game-playing agents based on AI techniques have demonstrated super-human performance in several sequential games, such as chess, Go, and poker. Sur-prisingly, the multi-agent learning techniques that allowed to reach these achieve-ments do not take into account the actual behavior of the human player, potentially leading to an impressive gap in performances. In this paper, we address the problem of designing artiﬁcial agents that learn how to effectively exploit unknown human opponents while playing repeatedly against them in an online fashion. We study the case in which the agent’s strategy during each repetition of the game is subject to constraints ensuring that the human’s expected utility is within some lower and upper thresholds. Our framework encompasses several real-world problems, such as human engagement in repeated game playing and human education by means of serious games. As a ﬁrst result, we formalize a set of linear inequalities encoding the conditions that the agent’s strategy must satisfy at each iteration in order to do not violate the given bounds for the human’s expected utility. Then, we use such formulation in an upper conﬁdence bound algorithm, and we prove that the resulting procedure suffers from sublinear regret and guarantees that the constraints are satisﬁed with high probability at each iteration. Finally, we empirically evaluate the convergence of our algorithm on standard testbeds of sequential games. 1

Introduction
Algorithmic game theory and machine learning have recently contributed to groundbreaking achieve-ments in artiﬁcial intelligence, leading to the deployment of artiﬁcial agents that defeated top human professionals in several recreational games. See, for example, the well-known milestones achieved in chess [Campbell et al., 2002], Go [Silver et al., 2016], and poker [Brown and Sandholm, 2018, 2019].
Surprisingly, multi-agent learning techniques that have been recently employed in such settings learn how to defeat humans without taking into account their actual behavior. Indeed, they learn strategies by simulating millions of plays in a self-play approach, without including any human player in the learning process. A direct effect of this methodology is that the resulting artiﬁcial agents do not
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
adapt to the actual capabilities of humans, potentially leading to an impressive gap in performances compared to the case in which the agent learns strategies while taking them into account.
In this paper, we address the problem of designing artiﬁcial agents that learn how to effectively exploit unknown human opponents while playing repeatedly against them in an online fashion. In particular, we study the case in which the strategy that the agent plays during each repetition of the game is subject to constraints ensuring that the human’s expected utility is within some lower and upper thresholds. Our framework models several real-world human-agent interactions, and it begets additional technical challenges compared to simple pure-exploitation scenarios.
One prominent application of our framework is when the artiﬁcial agent’s goal is not only exploiting the human opponent, but also ensuring that he/she remains engaged in the game. Guaranteeing humans’ engagement is crucial when designing artiﬁcial agents that play repeatedly against humans.
Indeed, when playing against a super-human agent, most human players drop out from game playing, since they realize they are losing too often against it. As Egri-Nagy and Törmänen [2020] sharply observe, it is “hopeless and frustrating to play against an AI, since it is practically impossible to win”. Different forms of engagement can be imagined (see, e.g., [Abbasi et al., 2017] for examples in computer games). In our framework, the human’s engagement is modeled with the threshold constraints over his/her expected utility, with the following rationale. If the utility value falls under a given satisfaction threshold, then the human will get bored playing, as he/she loses too much and believes he/she has no hope to win. On the other hand, if such value raises above another given threshold, then the human will get bored since he/she is winning too often.
Another application scenario for our framework is that of serious games [Dörner et al., 2016], whose purpose is to educate humans by asking them to perform tasks engagingly. If some tasks are excessively hard for the actual human’s capabilities, then the human will give up the training of the entire set of tasks, since he/she is sure that he/she will never be able to address them. On the other hand, if some tasks are excessively easy, the human will give up the training since he/she is sure that he/she can solve all remaining tasks. Serious games are used in many different ﬁelds, such as, e.g., military [DeFalco et al., 2018], transportation [Rossetti et al., 2013], urban planning [Poplin, 2011], and healthcare [Wang et al., 2016], and can be modeled as general-sum games between a human learner and a computer teacher [Mayer, 2012].
Original contributions. We study two-player sequential (i.e., extensive-form) games in which an artiﬁcial agent repeatedly plays against an unknown human opponent. We assume that the human has a ﬁxed stochastic behavior and he/she does not learn over time. We do not make any structural assumption on the human’s strategy, which requires the agent learning a probability distribution for each decision point of the human. While this is a crucial ﬁrst step towards a more complex setting in which the human learns over time, the resulting model still presents several technical challenges that are worth to be investigated. First, we show how to derive, after each game repetition, a conﬁdence region for the human’s strategy such that his/her actual strategy stays within it with high probability.
We show that such region is characterized by a set of linear constraints deﬁned over the sequence-form strategy space. Notice that, during each game repetition, the agent only observes a partial sample of the human’s strategy, made by the human’s actions on the path in the game tree followed during play.
By exploiting strong duality and the speciﬁc structure of the conﬁdence region, we show that the thresholds constraints on the human’s expected utility can be formulated as a set of linear inequalities, whose cardinality is linear in the size of the game tree. In particular, these constraints describe a subspace of the agent’s sequence-form strategy space such that, for every possible human’s strategy in the conﬁdence region, the human’s expected utility is within the given thresholds. We also derive a linear program with a linear number of constraints and variables to ﬁnd the best agent’s strategy satisfying the constraints. Then, we design an upper conﬁdence bound algorithm, called COX-UCB, and we prove that it suffers from a sublinear regret and guarantees that the aforementioned constraints are satisﬁed with high probability at every iteration. Finally, we empirically evaluate the convergence of our algorithm on standard testbeds and show that our bounds are asymptotically tight. All the omitted proofs are provided in Appendices A, B, and C.