Abstract
We introduce a simple yet effective framework for improving the robustness of learning algorithms against image corruptions for autonomous driving. These cor-ruptions can occur due to both internal (e.g., sensor noises and hardware abnormali-ties) and external factors (e.g., lighting, weather, visibility, and other environmental effects). Using sensitivity analysis with FID-based parameterization, we propose a novel algorithm exploiting basis perturbations to improve the overall performance of autonomous steering and other image processing tasks, such as classiﬁcation and detection, for self-driving cars. Our model not only improves the performance on the original dataset, but also achieves signiﬁcant performance improvement on datasets with multiple and unseen perturbations, up to 87% and 77%, respectively.
A comparison between our approach and other SOTA techniques conﬁrms the ef-fectiveness of our technique in improving the robustness of neural network training for learning-based steering and other image processing tasks. 1

Introduction
Autonomous driving is a complex task that requires many software and hardware components to operate reliably under highly disparate and often unpredictable conditions. In this work, we study
“learning-based steering” as it contains both perception and control, both critical components for autonomous driving. While on the road, vehicles are going to experience day and night, clear and foggy conditions, sunny and rainy days, as well as bright cityscapes and dark tunnels. All these external factors in conjunction with internal factors of the camera (e.g., those associated with hardware) can lead to quality variations in input data for image-based learning algorithms. One can harden machine learning systems to these degradations by simulating them at training time [6].
However, an algorithmic tool for analyzing the sensitivity of real-world neural network performance on the properties of (and corruptions to) training images is lacking. More importantly, a mechanism to leverage such a sensitivity analysis for improving learning outcomes needs to be developed. In this work, we quantify the inﬂuence of image quality on the task of “learning to steer,” study how training on degraded and low-quality images can boost robustness to image corruptions, and provide a systematic approach to improve the performance of learning algorithms using quantitative analysis.
Image degradations can be simulated by varying attributes such as blur, noise, distortion, color representations (such as RGB or CMY) hues, saturation, and intensity values (HSV). However, identifying the correct combination of the simulated corruptions to obtain optimal performance on 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1:
Pipeline of our method. Data generation. We generate perturbed datasets of each factor at multiple levels based on the FID-parameterized sensitivity analysis results. Training process. First stage: in each iteration, we ﬁrst augment the training dataset with “adversarial images” generated by applying an image corruption; we then combine the base and perturbed datasets to train our model to maximize the overall performance. A frequency-space branch is added to the backbone when frequency-related perturbations (e.g., blur, noise) need to be handled. Second stage: in post-learning, the model is ﬁne-tuned solely on clean data to boost accuracy, while performing validation on both clean and perturbed data with an early break when overall performance decreases to maintain the performance on the perturbed data. real data is a difﬁcult—if not impossible—task, as it requires domain transfer and exploring a high dimensional parameterized space.
We design a systematic method for measuring the severity of image degradation and predicting the impact of such degradation on model performance. Inspired by the use of image feature variance in sensitivity analysis [33], we measure the difference between real-world image distributions and simulated/degraded image distributions using Fréchet Inception Distance (FID). Our results conﬁrm that FID can help predict the performance of a model trained using simulated data and deployed in the real world. Next, we use FID between different simulated datasets as a uniﬁed metric to parameterize the severity of various image degradations due to different factors.
Borrowing concepts from the adversarial attack literature [28, 35, 43], we build a scalable training scheme for enhancing the robustness of autonomous driving against various combinations of image degradations, while increasing the overall accuracy of the steering task on clean data. Our proposed method constructs a dataset of adversarially degraded images by applying optimization within the space of possible degradations during training. As shown in Fig. 1, the method begins by training on a set of real and simulated/degraded images using arbitrary degradation parameters. During each training iteration, the parameters are updated to generate a new degradation set so that the model performance is (approximately) minimized. The network is then trained on these adversarially degraded images to promote robustness. A post-training step is applied to further improve the performance on clean data without weakening robustness. Our proposed algorithm uses our FID-based parameterization to discretize the search space of degradation parameters and accelerates the process of ﬁnding optimal parameters.
Experiments show that our algorithm improves the performance of “learning to steer” up to 97% in mean accuracy over baselines, and especially improves the performance on datasets contaminated with complex combinations of perturbations (up to 87%). It additionally boosts the test performance on degradations that are not seen during training, including simulated snow, fog, and frost (up to 77%). We also compare our approach with other SOTA techniques (e.g., data augmentation and adversarial training) on visual processing tasks such as detection and classiﬁcation. Our method consistently achieves higher performance. In addition, our method is easy to implement and can be readily integrated with other frameworks such as object detection, classiﬁcation, regression, etc.
Finally, we propose a comprehensive robustness evaluation standard under four different scenarios: clean data, single-perturbation data, multi-perturbation data, and previously unseen data. While 2
state-of-the-art studies usually conduct testing under one or two scenarios (e.g., ImageNet-C [20]), our work tests and veriﬁes results under four meaningful scenarios. We plan to release code and datasets for benchmarking “autonomous driving under perturbations” using unseen factors such as image corruptions in ImageNet-C [20], totaling 480 datasets and 26M images. 2