Abstract
Applications of Reinforcement Learning (RL) in robotics are often limited by high data demand. On the other hand, approximate models are readily available in many robotics scenarios, making model-based approaches like planning a data-efﬁcient alternative. Still, the performance of these methods suffers if the model is imprecise or wrong. In this sense, the respective strengths and weaknesses of RL and model-based planners are complementary. In the present work, we investigate how both approaches can be integrated into one framework that combines their strengths.
We introduce Learning to Execute (L2E), which leverages information contained in approximate plans to learn universal policies that are conditioned on plans. In our robotic manipulation experiments, L2E exhibits increased performance when compared to pure RL, pure planning, or baseline methods combining learning and planning. 1

Introduction
A central goal of robotics research is to design intelligent machines that can solve arbitrary and formerly unseen tasks while interacting with the physical world. Reinforcement Learning (RL) (Sutton & Barto, 2018) is a generic framework to automatically learn such intelligent behavior with little human engineering. Still, teaching an RL agent to actually exhibit general-purpose problem-solving behavior is, while possible in principle, prohibitive in practice. This is due to practical restrictions including limited computational resources and limited data availability. The latter limitation is particularly dramatic in robotics, where interaction with the physical world is costly.
On the other hand, for many robotics scenarios, there is a rough model of the environment avail-able. This can be exploited, e.g., using model-based planning approaches (Mordatch et al., 2012;
Kuindersma et al., 2016; Toussaint et al., 2018). Model-based planners potentially offer a more data-efﬁcient way to reason about an agent’s interaction with the world. Model-based planners have been used in many areas of robotics, such as for indoor and aerial robots (Faust et al., 2018), visual manipulation (Jeong et al., 2020), or humanoid walking (Mordatch et al., 2015). Still, if the model does not account for stochasticity or contains systematic errors, directly following the resulting plan will not be successful.
The present work starts from the observation that both pure RL approaches and pure planning approaches have strengths and limitations that are fairly complementary. RL makes no assumptions about the environment but is data-hungry, and model-based planning generally implies model simpliﬁcations but is data-efﬁcient. For robotic manipulation tasks, it seems natural to try and integrate both approaches into one framework that combines the strengths of both. In the present work we seek to add an additional perspective to the open question of how this can be achieved best. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We introduce a novel approach that we call Learning to Execute (L2E). Our approach translates sparse-reward goal-conditioned Markov Decision Processes (MDPs) (Bellman, 1957) into plan-conditioned
MDPs. L2E exploits a simple planning module to create crude plans, which are then used to teach any off-the-shelf off-policy RL agent to execute them. L2E makes use of ﬁnal-volume-preserving reward shaping (FV-RS) (Schubert et al., 2021), allowing it to train a universal plan-conditioned policy with high data efﬁciency. The contributions of this work are:
• We introduce L2E, which uses RL to efﬁciently learn to execute approximate plans from a model-based planner in a plan-conditioned MDP. We describe formally how FV-RS can be used as a tool to construct such plan-conditioned MDPs from goal-conditioned MDPs.
• We introduce plan replay strategies to efﬁciently learn universal plan-conditioned policies.
• We demonstrate, using robotic pushing problems, that L2E exhibits increased performance when compared to pure RL methods, pure planning methods, or other methods combining learning and planning.
We discuss work related to ours in section 2, explain background and notation in section 3, and introduce our method in section 4. We present our experimental results in section 5, discuss limitations in section 6, and conclude with section 7. 2