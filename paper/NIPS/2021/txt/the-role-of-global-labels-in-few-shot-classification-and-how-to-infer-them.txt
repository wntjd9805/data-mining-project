Abstract
Few-shot learning is a central problem in meta-learning, where learners must quickly adapt to new tasks given limited training data. Recently, feature pre-training has become a ubiquitous component in state-of-the-art meta-learning methods and is shown to provide signiﬁcant performance improvement. However, there is limited theoretical understanding of the connection between pre-training and meta-learning. Further, pre-training requires global labels shared across tasks, which may be unavailable in practice. In this paper, we show why exploiting pre-training is theoretically advantageous for meta-learning, and in particular the critical role of global labels. This motivates us to propose Meta Label Learning (MeLa), a novel meta-learning framework that automatically infers global labels to obtains robust few-shot models. Empirically, we demonstrate that MeLa is competitive with existing methods and provide extensive ablation experiments to highlight its key properties. 1

Introduction
A central problem in meta-learning is few-shot learning (FSL), where new tasks must be learned quickly given limited amount of training data. FSL has drawn increasing attention recently due to the high cost of collecting and annotating large datasets. To tackle the challenge of model generalization in FSL, meta-learning leverages past experiences of solving related tasks by directly learning transferable knowledge over a collection of FSL tasks. A diverse range of meta-learning methods tailored for FSL have been proposed, including optimization-based [e.g. 1, 5, 27], metric learning [e.g. 20, 21, 25], and model-based methods [e.g. 7, 14, 19]. The diversity of the existing strategies raises a natural question: do these methods share any common lessons for improving model generalization and for designing future methods?
Several papers addressed the above question. Chen et al. [2] identiﬁed that data augmentation and deeper network architecture signiﬁcantly improve generalization performance across several meta-learning methods. On the other hand, Tian et al. [23] investigated a simple yet competitive approach: a linear model on top of input embeddings learned via feature pre-training. This approach ignores task structures from meta-learning and merges all tasks into a “ﬂat” dataset of labeled samples.
The desired input embedding is then learned by classifying all classes of the ﬂat dataset.
Extensive empirical evidences supports the efﬁcacy of feature pre-training in FSL. Pre-training alone already outperforms various meta-learning algorithms [23]. Recently, it is used as a ubiquitous 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Global vs. Local labels (b) Global to Local Classiﬁer
Figure 1: (a) Colored squares represent samples. Tasks A and B can be “merged” meaningfully using global labels, but not local ones. (b) A global classiﬁer can be used as local classiﬁers given the indices Y of the intended classes to predict. pre-processing step in state-of-the-art meta-learning methods [e.g. 18, 26, 27]. In particular, [27] reported that pre-training also signiﬁcantly boosted earlier methods (see Fig. 2).
Despite the signiﬁcant empirical improvement, there is limited theoretical understanding of why feature pre-training provides signiﬁcant performance gain for meta-learning. On the other hand, pre-training requires task merging to construct the ﬂat dataset, which implicitly assumes access to some notion of global labels consistently adopted across tasks. However, global labels may not exist or are inaccessible, such as when each task is annotated independently with only local labels. This renders direct task merging and consequently pre-training impossible (see Fig. 1a). Independent annotation captures realistic scenarios when tasks are collected organically (e.g from different users), rather than generated synthetically from benchmark datasets (e.g. miniIMAGENET). Possible scenarios include non-descript task labels (e.g. tasks with numerical labels) or even concept overlaps among labels across tasks (e.g. sea animals vs mammals).
In this paper, we address the issues raised above with the following contributions:
• In Sec. 3, we show that feature pre-training directly relates to meta-learning as a loss upper bound. In particular, pre-training induces a conditional meta-learning formulation [3, 26], which provides a principled explanation for the empirical performance gains.
• In Sec. 4, we propose Meta Label Learning (MeLa), a novel framework that automatically infers some notion of latent global labels consistent with local task constraints. The inferred labels enable us to leverage pre-training to improve meta-learners’ generalization performance, and bridges the gap between experiment settings with or without acesss to global labels.
• In Sec. 5, we demonstrate empirically the competitive performance of MeLa over a suite of benchmark datasets. We also present ablation studies to highlight its key properties.
In Sec. 2, we ﬁrst review the key notions of meta-learning and FSL. The supplementary material contains proofs for the theoretical analysis, additional empirical results, and experiment setup. 2