Abstract
Efﬁcient exploration under sparse rewards remains a key challenge in deep re-inforcement learning. Previous exploration methods (e.g., RND) have achieved strong results in multiple hard tasks. However, if there are multiple novel areas to explore, these methods often focus quickly on one without sufﬁciently trying others (like a depth-wise ﬁrst search manner). In some scenarios (e.g., four corridor envi-ronment in Sec. 4.2), we observe they explore in one corridor for long and fail to cover all the states. On the other hand, in theoretical RL, with optimistic initializa-tion and the inverse square root of visitation count as a bonus, it won’t suffer from this and explores different novel regions alternatively (like a breadth-ﬁrst search manner). In this paper, inspired by this, we propose a simple but effective criterion called NovelD by weighting every novel area approximately equally. Our algorithm is very simple but yet shows comparable performance or even outperforms multiple
SOTA exploration methods in many hard exploration tasks. Speciﬁcally, NovelD solves all the static procedurally-generated tasks in Mini-Grid with just 120M environment steps, without any curriculum learning. In comparison, the previous
SOTA only solves 50% of them. NovelD also achieves SOTA on multiple tasks in
NetHack, a rogue-like game that contains more challenging procedurally-generated environments. In multiple Atari games (e.g., MonteZuma’s Revenge, Venture,
Gravitar), NovelD outperforms RND. We analyze NovelD thoroughly in Mini-Grid and found that empirically it helps the agent explore the environment more uniformly with a focus on exploring beyond the boundary. 1 1

Introduction
Deep reinforcement learning (RL) has experienced signiﬁcant progress over the last several years, with an impressive performance in games like Atari [41, 4], StarCraft [61], Go and Chess [57–59].
However, its success often requires massive computational resources or manually designed dense rewards. The dense rewards are often impractical for real-world settings as they require a signiﬁcant amount of task-speciﬁc domain knowledge.
An alternative approach is to allow the agent to explore the environment freely until it reaches the goal.
While basic RL exploration criteria (e.g., ✏-greedy) are quite simple, it fails to explore sufﬁciently in hard tasks. Modern works adopt various Intrinsic Reward (IR) designs to guide exploration in hard-exploration settings. However, we observe (Sec. 4.2) in our experiments if there are multiple 1Correspondence to: Tianjun Zhang <tianjunz@berkeley.edu>. Our code is available at https://github. com/tianjunz/NovelD. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
RND
End
Start 1. RND assigns high IR  throughout the environment 2. RND temporarily focuses  on the upper right corner 3. RND by chance explores  the bottom right corner  and forgets it already  explored upper right 4. RND re-explores the  upper right and forgets the  bottom right, gets trapped
BeBold
NovelD
High IR
Relative
High IR
Low IR
High IR
Relative
High IR
NovelD 1. BeBold assigns high IR  near the start and low IR  for the rest
NovelD 2. BeBold pushes every  direction to the frontier of  exploration uniformly
NovelD 3. BeBold continuously  pushes the exploration  frontier 4. BeBold reaches the end 
NovelD of exploration
Low IR
Figure 1: A hypothetical demonstration of how exploration is done in NovelD versus Random Network
Distillation (RND) [11], in terms of distribution of intrinsic reward (IR). NovelD reaches the goal by continuously pushing the frontier of exploration while RND gets trapped in the explored regions. Note that IR is deﬁned differently in RND versus NovelD (See Eqn. 2) and different colors are used to represent IR of RND and NovelD. regions of interest, these methods sometimes quickly are trapped in one area without sufﬁciently exploring others. This results in poor overall exploration of large state space [2]. This is also known as detachment problem in Go-Explore [17].
On the other hand, provably optimal theoretical RL equipped with optimistic initialization and visitation count bonus explores the environment much more uniformly [31]. Inspired by that, we introduce a very simple but effective exploration criterion by weighting each novel area approximate equally. Our algorithm uses regulated Novelty Difference (NovelD) of consecutive states in a trajectory. The novelty of a state is calculated by Random Network Distillation (RND [11]). The underlying intuition is that this criterion provides a large intrinsic reward at the boundary between the explored and the unexplored regions (Fig. 1). As a result, it induces a very different exploration pattern comparing to count-based approaches [6, 10, 11, 48, 5] and yields a much broader coverage over the state space.
We evaluate NovelD on three very challenging exploration environments: MiniGrid [13],
NetHack [34] and Atari games [9]. MiniGrid is a popular benchmark for evaluating exploration algorithms [52, 12, 24]; NetHack, built from a real game, is a much more realistic, PG-generated environment with complex goals and skills. Atari games is a widely used benchmark for RL algo-rithms [40, 11, 18]. NovelD manages to solve all the static environments in MiniGrid within 120M environment steps, without curriculum learning. In contrast, previous SOTA AMIGo [12] solves 50% of the tasks, categorized as “easy” and “medium”, by training a separate goal-generating teacher network in 500M steps. In NetHack, NovelD also outperforms all baselines with a signiﬁcant margin on various tasks. NovelD is also tested in various Atari games (e.g., MonteZuma’s Revenge, Venture), using image-based input and outperforms RND for both CNN and RNN-based networks.
Compared to previous works (e.g., RIDE [52], AMIGo [12] and Go-Explore [17]), NovelD has a few design advantages: (1) in NovelD, there is almost no hyperparameters; (2) NovelD is one-stage approach and can be readily combined with any policy learning methods (e.g., PPO), while many approaches (e.g., RIDE, Go-Explore) are two-stage approaches. (3) NovelD is asymptotic consistent: its IR vanishes after sufﬁcient exploration, while approaches like RIDE and AMIGo do not. During our experiments, we observe that compared to the count-based approach and RND, NovelD prioritizes the unexplored boundary states, yielding much more efﬁcient and broader exploration patterns. 2