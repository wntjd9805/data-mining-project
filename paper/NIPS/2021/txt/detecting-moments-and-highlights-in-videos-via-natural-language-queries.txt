Abstract
Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHIGHLIGHTS) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3)
ﬁve-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, ﬂexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr. 1

Introduction
Internet videos are growing at an unprecedented rate. Enabling users to efﬁciently search and browse these massive collections of videos is essential for improving user experience of online video platforms. While a good amount of work has been done in the area of natural language query based video search for complete videos (i.e., text-to-video retrieval [41, 42, 17]), returning the whole video is not always desirable, since they can be quite long (e.g., from few minutes to hours). Instead, users may want to locate precise moments within a video that are most relevant to their query or see highlights at a glance so that they can skip to relevant portions of the video easily.
Many datasets [13, 7, 18, 16, 31] have been proposed for the ﬁrst task of ‘moment retrieval’ – localizing moments in a video given a user query. However, most of the datasets are reported [4, 18] to have a strong temporal bias, where more moments appear at the beginning of the videos than at the end. Meanwhile, for each video-query pair, all of the datasets provide annotations with only a single moment. In reality, there are often multiple moments, i.e., several disjoint moments in a video, that are related to a given query. For the second task of ‘highlight detection’, many datasets [38, 12, 36, 8] are query-agnostic, where the detected highlights do not change for different input user queries.
[22, 43] are the two existing datasets that collect highlights based on user queries. However, only 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: QVHIGHLIGHTS examples. We show localized moments in dashed green boxes. The highlightness (or saliency) scores from 3 different annotators are shown under the frames as colored bars, with height and color intensity proportional to the scores. a small set of frames or clips are annotated ( 20 frames out of 331 seconds long videos in [22] or around 10 seconds clips out of 60 seconds video in [43]), limiting their ability to accurately learn and evaluate highlight detection methods. Lastly, although these two tasks of moment retrieval and highlight detection share many common characteristics (e.g., both require learning the similarity between user text query and video clips), they are typically studied separately, mostly due to the lack of annotations supporting both tasks in a single dataset.
To address these issues, we collect QVHIGHLIGHTS , a uniﬁed benchmark dataset that supports query-based video moment retrieval and highlight detection. Based on over 10,000 YouTube videos covering a diverse range of topics (from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos), we collect high-quality annotations for both tasks. Figure 1 shows two examples from QVHIGHLIGHTS. For moment retrieval, we provide one or multiple disjoint moments for a query in a video, enabling a more realistic, accurate, and less-biased (see
Section 3.2) evaluation of moment retrieval methods. Within the annotated moments, we also provide a ﬁve-point Likert-scale (from ‘Very Good’ to ‘Very Bad’) saliency/highlightness score annotation for each 2-second clip. This comprehensive saliency annotation gives more space for designing and evaluating query-based video highlight detection methods.
Next, to present strong initial models for this task, we take inspiration from recent work such as
DETR [3] for object detection, and propose Moment-DETR, an end-to-end transformer encoder-decoder architecture that views moment retrieval as a direct set prediction problem. With this method, we effectively eliminate the need for any manually-designed pre-processing (e.g., proposal generation) or post-processing (e.g., non-maximum suppression) steps commonly seen in moment retrieval methods. We further add a saliency ranking objective on top of the encoder outputs for highlight detection. While Moment-DETR does not encode any human prior in its design, our experiments show that it is still competitive when compared to highly-engineered architectures.
Furthermore, with additional weakly-supervised pretraining from ASR captions, Moment-DETR substantially outperforms these strong methods. Lastly, we also provide detailed ablations and visualizations to help understand the inner workings of Moment-DETR.
Overall, our contributions are 3-fold: (i) We collect the QVHIGHLIGHTS dataset with over 10,000 videos, annotated with human-written natural language queries, relevant moments, and saliency scores. (ii) We propose Moment-DETR to serve as a strong baseline for our dataset. With weakly supervised pretraining, Moment-DETR substantially outperforms several baselines, on both our proposed QVHIGHLIGHTS dataset and the moment retrieval dataset CharadesSTA [7]. (iii) We present detailed dataset analyses, model ablations and visualizations. For ablations, we examined various design choices of Moment-DETR as well as its pre-training strategy. We hope our work would inspire and encourage future work towards this important direction. 2