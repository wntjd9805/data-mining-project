Abstract
The effectiveness of unsupervised domain adaptation degrades when there is a large discrepancy between the source and target domains. Gradual domain adaption (GDA) is one promising way to mitigate such an issue, by leveraging additional un-labeled data that gradually shift from the source to the target. Through sequentially adapting the model along the “indexed” intermediate domains, GDA substantially improves the overall adaptation performance. In practice, however, the extra unla-beled data may not be separated into intermediate domains and indexed properly, limiting the applicability of GDA. In this paper, we investigate how to discover the sequence of intermediate domains when it is not already available. Concretely, we propose a coarse-to-ﬁne framework, which starts with a coarse domain dis-covery step via progressive domain discriminator training. This coarse domain sequence then undergoes a ﬁne indexing step via a novel cycle-consistency loss, which encourages the next intermediate domain to preserve sufﬁcient discriminative knowledge of the current intermediate domain. The resulting domain sequence can then be used by a GDA algorithm. On benchmark data sets of GDA, we show that our approach, which we name Intermediate DOmain Labeler (IDOL), can lead to comparable or even better adaptation performance compared to the pre-deﬁned do-main sequence, making GDA more applicable and robust to the quality of domain sequences. Codes are available at https://github.com/hongyouc/IDOL. 1

Introduction
The distributions of real-world data change dynamically due to many factors like time, locations, environments, etc. Such a fact poses a great challenge to machine-learned models, which implicitly assume that the test data distribution is covered by the training data distribution. To resolve this generalization problem, unsupervised domain adaption (UDA), which aims to adapt a learned model to the test domain given its unlabeled data [13, 15], has been an active sub-ﬁeld in machine learning.
Typically, UDA assumes that the “source” domain, in which the model is trained, and the “target” domain, in which the model is deployed, are discrepant but sufﬁciently related. Concretely, Ben-David et al. [1], Zhao et al. [75] show that the generalization error of UDA is bounded by the discrepancy of the marginal or conditional distributions between domains. Namely, the effectiveness of UDA may degrade along with the increase in domain discrepancy. Take one popular algorithm, self-training [38, 48, 49], for example. Self-training adapts the source model by progressively labeling the unlabeled target data (i.e., pseudo-labels) and using them to ﬁne-tune the model [2, 28, 30, 31, 41, 44, 77, 78].
Self-training works if the pseudo-labels are accurate (as it essentially becomes supervised learning), but is vulnerable if they are not, which occurs when there exists a large domain gap [35].
To address this issue, several recent works investigate gradual domain adaption (GDA) [12, 25, 35, 71], in which beyond the source and target domains, the model can access additional unlabeled data from the intermediate domains that shift gradually from the source to the target. By adapting 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Gradual domain adaption (GDA) without indexed intermediate domains. In this setting, one is provided with labeled source, unlabeled target, and additional unlabeled intermediate data that have not been grouped and indexed into a domain sequence. Our approach intermediate domain labeler (IDOL) can successfully discover the domain sequence, which can then be leveraged by a GDA algorithm to achieve a higher target accuracy than direct unsupervised domain adaptation (UDA). Images are from the Portraits dataset [14]. the model along the sequence of intermediate domains — i.e., from the ones close to the source to the ones close to the target — the large domain gap between source and target is chipped away by multiple sub-adaptation problems (between consecutive intermediate domains) whose domain gaps are smaller. Namely, every time the model moves a step closer to the target, instead of taking a huge jump that can signiﬁcantly decrease the performance (see Figure 1 for an illustration). GDA makes sense, as real-world data change gradually more often than abruptly [11, 60, 66]. The recent work by Kumar et al. [35] further demonstrates the strength of GDA both empirically and theoretically.
One potential drawback of GDA is the need for a well-deﬁned sequence of intermediate domains.
That is, prior to adaptation, the additional unlabeled data must be grouped into multiple domains and indexed with intermediate domain labels that reﬂect the underlying data distribution shift from the source to the target. Such information, however, may not be available directly. Existing methods usually leverage side information like time tags [25, 35, 71] to deﬁne the sequence, which may be sub-optimal. In some applications, even the side information may not be accessible (e.g., due to privacy concerns), greatly limiting the applicability of GDA.
In this paper, we therefore study GDA in the extreme case — the additional unlabeled data are neither grouped nor indexed. Speciﬁcally, we investigate how to discover the “domain sequence” from data, such that it can be used to drive GDA. We propose a two-stage coarse-to-ﬁne framework named Intermediate DOmain Labeler (IDOL). In the ﬁrst stage, IDOL labels each intermediate data instance with a coarse score that reﬂects how close it is to the source or target. We study several methods that estimate the distance from a data instance to a domain. We ﬁnd that a progressively trained domain discriminator — which starts with source vs. target data but gradually adds data close to either of them into training — performs the best, as it better captures the underlying data manifold.
In the second stage, IDOL then builds upon the coarse scores to group data into domains by further considering the discriminative (e.g., classiﬁcation) knowledge the model aims to preserve from the source to the target. Since the additional unlabeled data are fully unlabeled, we take a greedy approach to identify the next intermediate domain (i.e., a group of data) that can best preserve the discriminative knowledge in the current intermediate domain. Concretely, we employ self-training [38] along the domain sequence discovered so far to provide pseudo-labels for the current domain, and propose a novel cycle-consistency loss to discover the next domain, such that the model adapted to the next domain can be “adapted back” to the current domain and predict the same pseudo-labels. The output of IDOL is a sequence of intermediate domains that can be used by any GDA algorithms [25, 35, 71].
We validated IDOL on two data sets studied in [35], including Rotated MNIST [36] and Portraits over years [14]. IDOL can successfully discover the domain sequence that leads to comparable GDA performance to using the pre-deﬁned sequence (i.e., by side information). More importantly, IDOL is compatible with pre-deﬁned sequences — by treating them as the coarse sequences — to further improve upon them. We also investigate IDOL in scenarios where the additional unlabeled data may contain outliers and demonstrate IDOL’s effectiveness even in such challenging cases. To our knowledge, our work is the ﬁrst to tackle GDA without grouped and indexed intermediate domains.
The success of IDOL opens up broader application scenarios that GDA can contribute to. 2
2