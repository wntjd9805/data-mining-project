Abstract
The query-based black-box attacks have raised serious threats to machine learning models in many real applications. In this work, we study a lightweight defense method, dubbed Random Noise Defense (RND), which adds proper Gaussian noise to each query. We conduct the theoretical analysis about the effectiveness of RND against query-based black-box attacks and the corresponding adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or local search. The large magnitude ratio leads to the stronger defense performance of RND, and it’s also critical for mitigating adaptive attacks. Based on our analysis, we further propose to combine RND with a plausible Gaussian augmentation Fine-tuning (RND-GF). It enables RND to add larger noise to each query while maintaining the clean accuracy to obtain a better trade-off between clean accuracy and defense performance. Additionally, RND can be ﬂexibly combined with the existing defense methods to further boost the adversarial robustness, such as adversarial training (AT). Extensive experiments on
CIFAR-10 and ImageNet verify our theoretical ﬁndings and the effectiveness of
RND and RND-GF. 1

Introduction
Deep neural networks (DNNs) have been successfully applied in many safety-critical tasks, such as autonomous driving, face recognition and veriﬁcation, etc. However, it has been shown that DNN models are vulnerable to adversarial examples [18, 21, 26, 29, 48], which are indistinguishable from natural examples but make a model produce erroneous predictions. For real-world applications, the
DNN model as well as the training dataset, are often hidden from users. Instead, only the model feedback for each query (e.g., labels or conﬁdence scores) is accessible. In this case, the product providers mainly face severe threats from query-based black-box attacks, which don’t require any knowledge about the attacked models.
In this work, we focus on efﬁcient defense techniques against query-based black-box attacks, of which the main challenges are 1) the defender should not signiﬁcantly inﬂuence the model’s feedback to normal queries, but it is difﬁcult to know whether a query is normal or malicious; 2) the defender has no information about what kinds of black-box attack strategies adopted by the attacker. Considerable efforts have been devoted to improving the adversarial robustness of DNNs [13, 34, 49, 50]. Among them, adversarial training (AT) is considered as the most effective defense techniques [3, 49].
However, the improved robustness from AT is often accompanied by signiﬁcant degradation of the
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
clean accuracy. Besides, the training cost of AT is much higher than that of standard training, and then it also often suffers from poor generalization to new samples or adversarial attacks [20, 39, 45, 46].
Thus, we argue that AT-based defense is not a very suitable choice for black-box defense. In contrast, we expect that a good defense technique should satisfy the following requirements: well keeping clean accuracy, lightweight, and plug-and-play.
To this end, we study a lightweight defense strategy, dubbed Random Noise Defense (RND) against query-based black-box attacks. For query-based attacks [1, 2, 7, 9, 11, 19, 23, 26, 27, 32, 35], the core is to ﬁnd an attack direction by gradient estimation or random search based on the exact feedback of consecutive queries, which leads to a decrease of the designed objective. RND is realized by adding random noise to each query at the inference time. Therefore, the returned feedback with randomness results in poor gradient estimation or random search and slows down the attack process.
To better understand the effectiveness of RND, we provide a theoretical analysis of the defense performance of RND against query-based attacks and adaptive attacks. Our theoretical results reveal that the defense performance of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation or random search. The attack efﬁciency is signiﬁcantly affected by a large magnitude ratio. The attackers need more queries to
ﬁnd the adversarial examples or fail to ﬁnd a successful attack under limited query settings. That is, the larger ratio leads to the better defense performance of RND. Apart from standard attacks, the adaptive attack (EOT) [3] has been considered as an effective strategy to mitigate the random effect induced by the defenders. We also conduct theoretical analysis about the defense effect of RND against EOT attacks, and demonstrate that the magnitude ratio is also crucial to mitigate adaptive attacks and the adaptive attacks have the limited impact of evading the RND. On the other hand, large random noises to each query may lead to degradation of clean accuracy. To achieve a better trade-off between the defense effect and the clean accuracy while maintaining training time efﬁciency, we further propose combining RND with a lightweight Gaussian augmentation Fine-tuning (RND-GF).
RND-GF enables us to adopt larger noise in inference time to disturb the query process better.
We conduct extensive experiments on CIFAR-10 and ImageNet. The experimental results verify our theoretical results and demonstrate the effectiveness of RND-GF. It is worth noting that RND is plug-and-play and easy to combine with existing defense methods such as AT to boost the defense performance further. To verify these, we evaluate the performance of RND combined with AT [22] and ﬁnd that the RND can improve the robust accuracy of AT by up to 23.1% against the SOTA black-box attack Square attack [2] with maintaining clean accuracy.
The main contributions of this work are four-fold.
• We study a lightweight random noise defense (RND) against black-box attacks theoretically and empirically. Our theoretical results reveal that the effectiveness of RND is determined by the magnitude ratio between the noise induced by RND and the noise added by the attackers for gradient estimation and random search.
• We theoretically analyze the performance of RND against the adaptive attack (EOT) and demonstrate that EOT has the limited effect of evading the RND.
• Leveraging our theoretical analysis, we further propose an efﬁcient and stronger defense strategy RND-GF by combining the Gaussian augmentation Fine-tuning and RND towards a better trade-off between clean and adversarial performance.
• Extensive experiments verify our theoretical analysis and show the effectiveness of our defense methods against several state-of-the-art query-based attacks. 2