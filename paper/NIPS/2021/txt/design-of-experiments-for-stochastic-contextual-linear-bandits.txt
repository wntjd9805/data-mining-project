Abstract
In the stochastic linear contextual bandit setting there exist several minimax pro-cedures for exploration with policies that are reactive to the data being acquired.
In practice, there can be a signiﬁcant engineering overhead to deploy these algo-rithms, especially when the dataset is collected in a distributed fashion or when a human in the loop is needed to implement a different policy. Exploring with a sin-gle non-reactive policy is beneﬁcial in such cases. Assuming some batch contexts are available, we design a single stochastic policy to collect a good dataset from which a near-optimal policy can be extracted. We present a theoretical analysis as well as numerical experiments on both synthetic and real-world datasets. 1

Introduction
Many settings may substantially beneﬁt from data-driven contextualized decision policies that opti-mize the desired expected outcome. Online machine learning methods like multi-armed bandits and reinforcement learning, that adaptively change interventions in response to outcomes in a closed loop process (see Figure 1a), may not yet be practical for all domains due to the expertise and in-frastructure needed. However running an experiment with a ﬁxed decision policy to identify a good personalized policy is likely to be both simple logistically (since currently such decision policies are often speciﬁed by hand) and more easily accepted, since many areas (education, healthcare, social sciences) commonly deploy experiments across a few conditions to ﬁnd the best approach.
For example, education startups, political campaigns, and governmental agencies can use email and
∗The ﬁrst three authors contributed equally. The work was fully completed while Andrea Zanette was a
PhD Candidate in the Institute for Computational and Mathematical Engineering at Stanford University. Future revision of this article will be made available at https://arxiv.org/abs/2107.09912. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
text messages to provide targeted information and opportunities, such as information to encourage vaccination, or tips to parents to support their child’s developmental stage. Such organizations are familiar with standard experimental design, and but generally lack the infrastructure for continuous online contextual MAB learning. Experiments that involve deploying a nonadaptive policy that ﬁt in with standard workﬂows but enable data-efﬁcient learning of contextualized policies might offer substantial beneﬁts over AB testing or relying on other segmentation methods that may not directly optimize desired outcomes.
For these reasons, a key opportunity is to design static or nonadaptive policies that can be used to gather data to identify optimal contextualized decision policies. Indeed a ﬁxed data collection strategy is practically desirable (1) whenever multiple agents collect data asynchronously and com-munication to update their policy is difﬁcult or impossible, and (2) whenever changing the policy requires a signiﬁcant overhead either in the engineering infrastructure or in the training of human personnel. Several prior papers limit the number of policy switches with minimal sample complex-ity impact (Han et al., 2020; Ruan et al., 2020; Ren et al., 2020; Bai et al., 2019; Wang et al., 2021).
Motivated by the above settings, we look for a single, nonadaptive policy for data collection.
Setting and goal We consider the linear stochastic contextual bandit setting where each context s ∈ S is sampled from a distribution µ and a context-dependent action set As is made available to the learner. The bandit instance is deﬁned by a feature extractor φ(s, a) ∈ Rd and some unknown param-eter θ(cid:63) ∈ Rd. Upon choosing an action a ∈ As, the linear reward function r(s, a) = φ(s, a)(cid:62)θ(cid:63) + η is revealed to the learner corrupted by mean zero 1-subGaussian noise η. Our goal is to construct an exploration policy πe to gather a dataset, such that after that dataset is gathered, we can extract a near optimal policy (cid:98)π from it. In particular, we want to minimize the number of exploration samples.
Perhaps surprisingly, there has been relatively little work on this setting. Prior work on exploration to quickly identify a near-optimal policy focuses on best-arm identiﬁcation using adaptive policies that react to the observed rewards (Soare et al., 2014; Tao et al., 2018; Jedra and Proutiere, 2020) or design of experiments that produces a nonadaptive policy for data collection (Kiefer and Wol-fowitz, 1960; Esfandiari et al., 2019; Lattimore and Szepesvari, 2020); both lines of work assume that a single, repeated context with unchanging action set is presented to the learner. In contrast we are interested in identifying near-optimal context-speciﬁc decision policies. The closest related work (Ruan et al., 2020) investigates our task as a subtask for online regret learning, but requires an amount of data that scales as Ω(d16), which is impractical in applications with even moderate d.2
Without any apriori information, no algorithm can do much better than deploying a random policy, which can require an amount of data that scales exponentially in d, see appendix G.1. However, in many common applications, prior data about the context distribution µ(s) and the state–action feature representation φ is available. For example, an organization will often know information about its customers and specify the feature representation used for state–action spaces in advance.3
The initially available state contexts are referred to as ofﬂine (state) contexts data.
Our algorithm leverages historical context data to enable data efﬁcient design of experiments for
It uses ofﬂine context-only data C to design a nonadaptive stochastic linear contextual bandits. policy πe to collect new, online data where reward feedback is observed (see Figure 1b), and uses the resulting dataset D(cid:48) to learn a near-optimal on average decision policy (cid:98)π for future use. We highlight that the algorithm does not get to adjust the exploratory policy πe while the online data is being collected.
Contributions We make the following contributions.
• Using past state contexts only, we design a single, nonadaptive policy to acquire online data that can be used to compute a context-dependent decision policy that is near-optimal in expectation across the contexts with high probability, for future use.
• To identify an (cid:15)-optimal policy, our algorithm achieves the minimax lower bound
Ω(min{d log (cid:80) s As, d2)}/(cid:15)2) on the number of online samples (ignoring log and constants), while keeping the number of ofﬂine state contexts required polynomially small (O(d2/(cid:15)2) or
O(d3/(cid:15)2)). 2Note that the number of ofﬂine data in (Ruan et al., 2020) is independent of 1/(cid:15). But for most of the practical settings, our bound d3/(cid:15)2 can be much smaller than d16. 3In other words, given a set of previously observed states s1, . . . , sM , and a known state–action representa-tion φ, for any potential action a, we can compute the resulting representation φ(s, a). 2
(a) Traditional online RL framework (b) Design of experiment with an online and ofﬂine component
Figure 1: Comparison between the traditional RL setting and design of experiments
• Our experiments on a simulation and on a learning to rank Yahoo dataset show our strategic design of experiments approach can learn better decision policies with less exploration data compared to standard static exploration strategies that fail to exploit structure. 2