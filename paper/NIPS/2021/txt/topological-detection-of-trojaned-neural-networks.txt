Abstract
Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model’s behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientiﬁc principles, we discover subtle – yet critical – structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from shallow to deep layers.
Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks. 1

Introduction
Recent years have witnessed rapid development of deep neural networks (DNNs) [33, 25, 58, 16].
However, due to their high complexity and lack of transparency, DNNs are vulnerable to various malicious attacks [2, 56]. This paper focuses on one type of data poisoning attack called the Trojan attack [23]. In this scenario the attacker injects Trojaned samples into the training dataset – for example by using incorrectly labeled images overlaid with a special trigger. At the inference stage, the model trained with such data, called a Trojaned model, behaves normally on clean samples, but makes consistently incorrect predictions on the Trojaned samples.
The challenges in identifying such attacks stem from the conﬁned setting: the user has access only to the DNN model and few clean samples. In such data-limited setting, methods requiring dense sampling [9] are not very practical. Instead, state-of-the-art methods often follow a reverse engineering strategy [59, 43, 24, 62]. Starting with a clean sample, they try to reconstruct a Trojaned sample that can change the prediction. Network’s response to such a reverse engineered sample can help determine if the network was indeed Trojaned. However, in practice the search space for triggers is huge, and efﬁcient, reliable detection has proven challenging so far.
Previous approaches treat a neural network as a black-box, only inspecting the dependency between its input and output. In this paper, we open the box and look into the internal mechanisms of the model. We investigate our hypothesis that there exists signiﬁcant structural difference between clean and Trojaned networks. To this end, we follow a classic adage of neuroscience, “Neurons that ﬁre together, wire together” [26]. We consider neurons with highly correlated activation as wired together – even if they are not directly connected in the network. Unfortunately, direct inspection of such connectivity is not sufﬁcient, presumably due to the high heterogeneity of models and data.
To overcome this issue, we propose to use more advanced tools which allow us to model more subtle, higher-order structural information of neural networks. Our method uses tools from topological data analysis, particularly persistent homology [18, 4]. With principled algebraic-topological foundations 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[47], these tools are perfectly suited for modelling higher-order structural information. We use them to capture salient topological structures – particularly the connected components and holes present in the aforementioned neuron connectivity graph.
Equipped with topological tools, we compare clean and Trojaned neural networks. We observe a signiﬁcant discrepancy between their topology – and quantify this difference by comparing topological descriptors called persistence diagrams. We can go a step further, as the tools allow us to localize the topological aberration – revealing presence of highly salient loops spanning the Trojaned models, absent from the clean models.1
Trying to understand the implications of our observations, we ask: What does the topological abnormality reveal about a Trojaned network? We claim that these loops reveal strong short cuts that connect neurons from shallow and deep layers – bearing resemblance to the neuroscientiﬁc concept of a reﬂex arc. This is sensible as in Trojaned models, the classiﬁer has to switch prediction once it sees a trigger. The deep layer neurons (closer to prediction) have to be highly dependent on some shallow layer neurons (closer to input).
Our empirical observations are substantiated a theoretical result. Theorem 1 states that given sufﬁcient samples, the topological descriptor is provably consistent. This result serves as a sanity check, showing that what we observed was not a ﬂuke. We conclude by proposing a topology-based Trojan detection algorithm. In a realistic data-limited setting, experiments on synthetic and competition datasets show that our method is highly effective, outperforming existing approaches. The topological detector can help mitigate the security threat posed by Trojan attacks.
The code of this paper can be found at https://github.com/TopoXLab/TopoTrojDetection. 1.1