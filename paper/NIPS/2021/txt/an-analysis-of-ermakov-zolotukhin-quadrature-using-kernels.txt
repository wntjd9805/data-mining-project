Abstract
We study a quadrature, proposed by Ermakov and Zolotukhin in the sixties, through the lens of kernel methods. The nodes of this quadrature rule follow the distribution of a determinantal point process, while the weights are deﬁned through a linear system, similarly to the optimal kernel quadrature. In this work, we show how these two classes of quadrature are related, and we prove a tractable formula of the expected value of the squared worst-case integration error on the unit ball of an
RKHS of the former quadrature. In particular, this formula involves the eigenvalues of the corresponding kernel and leads to improving on the existing theoretical guarantees of the optimal kernel quadrature with determinantal point processes. 1

Introduction
Integrals appear in many scientiﬁc ﬁelds as quantities of interest per se. For example, in statistics, they represent expectations [27], while in mathematical ﬁnance, they represent the prices of ﬁnancial products [17]. Unfortunately, integrals that can be written in closed form are exceptional. In general, their values are only known through approximations. For this reason, numerical integration is at the heart of many tasks in applied mathematics and statistics. Among all the possible approximation schemes, quadratures are the most practical since they approximate the integral of a function by a
ﬁnite mixture of its evaluations. In this work, we focus on quadrature rules that take the form (cid:90)
X f (x)g(x)dω(x) ≈ (cid:88) i∈[N ] wif (xi), (1) where the nodes xi are independent of f and g, while the weights wi depend only on g. The nodes and the weights of a quadrature may be seen as degrees of freedom that the practitioner may tune in order to achieve a given level of approximation error. The design of quadratures gave birth to a rich literature from Gaussian quadrature [15] to Monte Carlo methods [25] to quadratures based on determinantal point processes (DPPs) [1]. These latter form a large class of probabilistic models of repulsive random subsets that make numerical integration possible in a variety of domains with strong theoretical guarantees. In particular, central limit theorems with asymptotic convergence rates that scale better than the typical Monte Carlo rate O(N −1/2) were proven for several DPP based quadratures: when the integrand is a C1 function [1] or even when the integrand is non-differentiable [10]. Moreover, it is possible to design quadrature rules based on DPPs with non-asymptotic guarantees and with rates of convergence that adapt to the smoothness of the integrand. This is the case of the quadrature proposed by Ermakov and Zolotukhin in [14] and recently revisited in [16], and the optimal kernel quadrature [3].
In this work, we study the quadrature rule proposed by Ermakov and Zolotukhin (EZQ) through the lens of kernel methods. We start by comparing the weights of EZQ to the weights of the optimal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
kernel quadrature (OKQ), and we prove that they both belong to a broader class of quadrature rules that we call kernel based interpolation quadrature. Then, we study the approximation quality of EZQ in reproducing kernel Hilbert spaces (RKHSs). This is done by proving a general tractable formula of the expected value of the squared worst-case integration error for functions that belong to the unit ball of an RKHS when the nodes follow the distribution of a determinantal point process. This formula involves principally the eigenvalues of the integral operator, and converges to 0 at a slightly slower rate than the optimal rate. Interestingly, this analysis yields a better upper bound for the optimal kernel quadrature with DPPs proposed initially in [3]. Comparably to the theoretical guarantees given in [14, 16], our theoretical guarantees are independent of the choice of the test function. This facilitates the comparison of EZQ with other quadratures such as OKQ.
The rest of the article is organized as follows. Section 2 reviews the work of [14] and recall key concepts on kernel based quadrature. In Section 3, we present the main results of this work and their consequences. A sketch of the proof of the main theorem is given in Section 4. We illustrate the theoretical results by numerical experiments in Section 5. Finally, we give a conclusion in Section 6.
Notation and assumptions. We use the notation N∗ = N (cid:114) {0}. We denote by ω a Borel measure supported on X , and we denote by L2(ω) the Hilbert space of square integrable real-valued functions on X with respect to ω, equipped with the inner product (cid:104)·, ·(cid:105)ω, and the associated norm (cid:107).(cid:107)ω. For
N ∈ N∗, we denote by ω⊗N the tensor product of ω deﬁned on X N . Moreover, we denote by F the
RKHS associated to the kernel k : X × X → R that we assume to be continuous and satisfying the condition (cid:82)
X k(x, x)dω(x) < +∞. In particular, we assume the Mercer decomposition k(x, y) = (cid:88) m∈N∗
σmφm(x)φm(y), (2) to hold, where the convergence is pointwise, and σm and φm are the corresponding eigenvalues and eigenfunctions of the integral operator Σ deﬁned for f ∈ L2(ω) by
Σf (·) = (cid:90)
X k(·, y)f (y)dω(y). (3)
We assume that the sequence σ = (σm)m∈N∗ is non-increasing and its elements are non-vanishing, and we assume that the corresponding eigenfunctions φm are continuous. Note that the φm are normalized: (cid:107)φm(cid:107)ω = 1 for m ∈ N∗. In particular, (φm)m∈N∗ is an o.n.b. of L2(ω), and every element f ∈ F satisﬁes (cid:88) m∈N∗ (cid:104)f, φm(cid:105)2
ω
σm
< +∞. (4)
Moreover, for every N ∈ N∗, we denote by EN the eigen-subspace of L2(ω) spanned by
φ1, . . . , φN . For any kernel κ : X × X → R, and for x ∈ X N , we deﬁne the kernel matrix
κ(x) := (κ(xi, xj))i,j∈[N ] ∈ RN ×N . Finally, we denote in bold fonts the corresponding kernel matrices: K(x) for the kernel k, KN (x) for the kernel kN , K⊥
N , κ(x) for the kernel κ... Similarly, for any function µ : X → R and for x ∈ X N , we deﬁne the vector of evaluations µ(x) := (µ(xi))i∈[N ] ∈ RN .
N (x) for the kernel k⊥ 2