Abstract
Real world applications such as economics and policy making often involve solv-ing multi-agent games with two unique features: (1) The agents are inherently asymmetric and partitioned into leaders and followers; (2) The agents have dif-ferent reward functions, thus the game is general-sum. The majority of existing results in this ﬁeld focuses on either symmetric solution concepts (e.g. Nash equilibrium) or zero-sum games. It remains open how to learn the Stackelberg equilibrium—an asymmetric analog of the Nash equilibrium—in general-sum games efﬁciently from noisy samples. This paper initiates the theoretical study of sample-efﬁcient learning of the Stackelberg equilibrium, in the bandit feedback setting where we only observe noisy samples of the reward. We consider three representative two-player general-sum games: bandit games, bandit-reinforcement learning (bandit-RL) games, and linear bandit games. In all these games, we identify a fundamental gap between the exact value of the Stackelberg equilibrium and its estimated version using ﬁnitely many noisy samples, which can not be closed information-theoretically regardless of the algorithm. We then establish sharp positive results on sample-efﬁcient learning of Stackelberg equilibrium with value optimal up to the gap identiﬁed above, with matching lower bounds in the dependency on the gap, error tolerance, and the size of the action spaces. Overall, our results unveil unique challenges in learning Stackelberg equilibria under noisy bandit feedback, which we hope could shed light on future research on this topic. 1

Introduction
Real-world problems such as economic design and policy making can often be modeled as multi-agent games that involves two levels of thinking: The policy maker—as a player in this game— needs to reason about the other player’s optimal behaviors given her decision, in order to inform her own optimal decision making. Consider for example the optimal taxation problem in the AI
Economist [53], a game modeling a real-world social-economic system involving a leader (e.g. the government) and a group of interacting followers (e.g. citizens). The leader sets a tax rate which determines an economics-like game for the followers; the followers then play in this game with the objective to maximize their own reward (such as individual productivity). However, the goal of the leader is to maximize her own reward (such as overall equality) which is in general different from the followers’ rewards, making these games general-sum [38]. Such two-level thinking appears broadly in other applications as well such as in automated mechanism design [11, 12], optimal auctions [10, 14], security games [43], reward shaping [25], and so on. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Another key feature in such games is that the players are asymmetric, and they act in turns: the leader
ﬁrst plays, then the follower sees the leader’s action and then adapts to it. This makes symmetric solution concepts such as Nash equilibrium [30] not always appropriate. A more natural solution concept for these games is the Stackelberg equilibrium: the leader’s optimal strategy, assuming the followers play their best response to the leader [42, 13]. The Stackelberg equlibrium is often the desired solution concept in many of the aforementioned applications. Furthermore, it is of compelling interest to understand the learning of Stackelberg equilibria from samples, as it is often the case that we can only learn about the game through interactively deploying policies and observing the (noisy) feedback from the game [53]. This may be the case even when the game rules are perfectly known but not yet represented in a desired form, as argued in the line of work on empirical game theory [50, 48, 20].
Despite the motivations, theoretical studies of learning Stackelberg equilibria in general-sum games remain open, in particular when we can only learn from noisy samples of the rewards. A line of work provides guarantees for ﬁnding Stackelberg equilibria in general-sum games, but restricts attention to either the full observation setting (so that the exact game is observable) or with an exact best-response oracle [13, 27, 47, 34]. These results lay out a foundation for analyzing the Stackelberg equilibrium, but do not generalize to the bandit feedback setting in which the game can only be learned from random samples of the rewards. Another line of work considers the sample complexity of learning the Nash equilibrium in Markov games [35, 4, 5, 28, 51, 52], which also do not imply algorithms for ﬁnding the Stackelberg equilibrium in these games as the Nash is in general different from the
Stackelberg equilibrium in general-sum games.
In this work, we study the sample complexity of learning Stackelberg equilibrium in general-sum games. We focus on general-sum games with two players (one leader and one follower), in which we wish to learn an approximate Stackelberg equilbrium for the leader from random samples. Our contributions can be summarized as follows.
•
•
•
As a warm-up, we consider bandit games in which the two players play an action in turns and observe their own rewards. We identify a fundamental gap between the exact Stackelberg value and its estimated version from ﬁnite samples, which cannot be closed information-theoretically regardless of the algorithm (Section 3.1). We then propose a rigorous deﬁnition gapε for this gap, and show that it is possible to sample-efﬁciently learn the (gapε + ε)-approximate Stackelberg equilibrium with (cid:101)O(AB/ε2) samples, where A, B are the number of actions for the two players.
We further show a matching lower bound Ω(AB/ε2) (Section 3.2). We also establish similar results for learning Stackelberg in simultaneous matrix games (Appendix B).
We consider bandit-RL games in which the leader’s action determines an episodic Markov
Decision Process (MDP) for the follower. We show that a (gapε + ε) approximate Stackelberg equilibrium for the leader can be found in (cid:101)O(H 5S2AB/ε2) episodes of play, where H, S are the horizon length and number of states for the follower’s MDP, and A, B are the number of actions for the two players (Section 4). Our algorithm utilizes recently developed reward-free reinforcement learning techniques to enable fast exploration for the follower within the MDPs.
Finally, we consider linear bandit games in which the action spaces for the two players can be arbitrarily large, but the reward is a linear function of a d-dimensional feature representation of the actions. We design an algorithm that achieves (cid:101)O(d2/ε2) sample complexity upper bound for linear bandit games (Section 5). This only depends polynomially on the feature dimension instead of the size of the action spaces, and has at most an (cid:101)O(d) gap from the lower bound. 1.1