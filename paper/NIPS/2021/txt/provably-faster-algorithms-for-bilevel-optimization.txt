Abstract
Bilevel optimization has been widely applied in many important machine learning applications such as hyperparameter optimization and meta-learning. Recently, several momentum-based algorithms have been proposed to solve bilevel optimiza-tion problems faster. However, those momentum-based algorithms do not achieve provably better computational complexity than (cid:101)O((cid:15)−2) of the SGD-based algo-rithm. In this paper, we propose two new algorithms for bilevel optimization, where the ﬁrst algorithm adopts momentum-based recursive iterations, and the second algorithm adopts recursive gradient estimations in nested loops to decrease the variance. We show that both algorithms achieve the complexity of (cid:101)O((cid:15)−1.5), which outperforms all existing algorithms by the order of magnitude. Our experiments validate our theoretical results and demonstrate the superior empirical performance of our algorithms in hyperparameter applications. 1

Introduction
Bilevel optimization has become a timely and important topic recently due to its great effectiveness in a wide range of applications including hyperparameter optimization [7, 5], meta-learning [33, 16, 1], reinforcement learning [14, 24]. Bilevel optimization can be generally formulated as the following minimization problem:
Φ(x) := f (x, y∗(x)) min x∈Rp s.t. y∗(x) = arg min y∈Rq g(x, y). (1)
Since the outer function Φ(x) := f (x, y∗(x)) depends on the variable x also via the optimizer y∗(x) of the inner-loop function g(x, y), the algorithm design for bilevel optimization is much more complicated and challenging than minimization and minimax optimization. For example, if the gradient-based approach is applied, then the gradient of the outer-loop function (also called hypergradient) will necessarily involve Jacobian and Hessian matrices of the inner-loop function g(x, y), which require more careful design to avoid high computational complexity.
This paper focuses on the nonconvex-strongly-convex setting, where the outer function f (x, y∗(x)) is nonconvex with respect to (w.r.t.) x and the inner function g(x, y) is strongly convex w.r.t. y for any x.
Such a case often occurs in practical applications. For example, in hyperparameter optimization [7], f (x, y∗(x)) is often nonconvex with x representing neural network hyperparameters, but the inner function g(x, ·) can be strongly convex w.r.t. y by including a strongly-convex regularizer on y. In few-shot meta-learning [1], the inner function g(x, ·) often takes a quadratic form together with a strongly-convex regularizer. To efﬁciently solve the deterministic problem in eq. (1), various bilevel optimization algorithms have been proposed, which include two popular classes of deterministic gradient-based methods respectively based on approximate implicit differentiation (AID) [31, 9, 8] and iterative differentiation (ITD) [28, 6, 7]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recently, stochastic bilevel opitimizers [8, 20] have been proposed, in order to achieve better efﬁciency than deterministic methods for large-scale scenarios where the data size is large or vast fresh data needs to be sampled as the algorithm runs.
In particular, such a class of problems adopt functions by:
Φ(x) := f (x, y∗(x)) := Eξ[F (x, y∗(x); ξ)], g(x, y) := Eζ[G(x, y; ζ)] where the outer and inner functions take the expected values w.r.t. samples ξ and ζ, respectively.
Along this direction, [20] proposed a stochastic gradient descent (SGD) type optimizer (stocBiO), and showed that stocBiO attains a computational complexity of (cid:101)O((cid:15)−2) in order to reach an (cid:15)-accurate stationary point. More recently, several studies [2, 11, 22] have tried to accelerate SGD-type bilevel optimizers via momentum-based techniques, e.g., by introducing a momentum (historical information) term into the gradient estimation. All of these optimizers follow a single-loop design, i.e., updating x and y simultaneously. Speciﬁcally, [22] proposed an algorithm MSTSA by updating x via a momentum-based recursive technique introduced by [3, 36]. [11] proposed an optimizer
SEMA similarly to MSTSA but using the momentum recursive technique for updating both x and y.
[2] proposed an algorithm STABLE, which applies the momentum strategy for updating the Hessian matrix, but the algorithm involves expensive Hessian inverse computation rather than hypergradient approximation loop. However, as shown in Table 1, SEMA, MSTSA and STABLE achieve the same complexity order of (cid:101)O((cid:15)−2) as the SGD-type stocBiO algorithm, where the momentum technique in these algorithms does not exhibit the theoretical advantage. Such a comparison is not consistent with those in minimization [3] and minimax optimization [15], where the single-loop momentum-based recursive technique achieves provable performance improvements over SGD-type methods. This motivates the following natural but important question:
• Can we design a faster single-loop momentum-based recursive bilevel optimizer, which achieves order-wisely lower computational complexity than SGD-type stocBiO (and all other momentum-based algorithms), and is also easy to implement with efﬁcient matrix-vector products?
Although the existing theoretical efforts on accelerating bilevel optimization algorithms have been ex-clusively focused on single-loop design1, empirical studies in [20] suggested that double-loop bilevel algorithms such as BSA [8] and stocBiO [20] achieve much better performances than single-loop algorithms such as TTSA [14]. A good candidate suitable for accelerating double-loop algorithms can be the popular variance reduction method, such as SVRG [21], SARAH [30] and SPIDER [4], which typically yield provably lower complexity. The basic idea is to construct low-variance gradient estimators using periodic high-accurate large-batch gradient evaluations. So far, there has not been any study on using variance reduction to accelerate double-loop bilevel optimization algorithms. This motivates the second question that we address in this paper:
• Can we develop a double-loop variance-reduced bilevel optimizer with improved computational complexity over SGD-type stocBiO (and all other existing algorithms)? If so, when such a double-loop algorithm holds advantage over the single-loop algorithms in bilevel optimization? 1.1 Main Contributions
This paper proposes two algorithms for bilevel optimization, both outperforming all existing algo-rithms in terms of complexity order.
We ﬁrst propose a single-loop momentum-based recursive bilevel optimizer (MRBO). MRBO updates variables x and y simultaneously, and uses the momentum recursive technique for constructing low-variance mini-batch estimators for both the gradient ∇g(x, ·) and the hypergradient ∇Φ(·); in contrast to previous momentum-based algorithms that accelerate only one gradient or neither. Further,
MRBO is easy to implement, and allows efﬁcient computations of Jacobian- and Hessian-vector products via automatic differentiation. Theoretically, we show that MRBO achieves a computational complexity (w.r.t. computations of gradient, Jacobian- and Hessian-vector product) of (cid:101)O((cid:15)−1.5), 1In the literature of bilevel optimization, although many hypergradient-based algorithms include an iteration loop of Hessian inverse estimation, such a loop is typically not counted when these algorithms are classiﬁed by the number of loops. This paper follows such a convention to be consistent with the existing literature.
Namely, the single- and double-loop algorithms mentioned here can include an additional loop of Hessian inverse estimation in the hypergradient approximation. 2
Table 1: Comparison of stochastic algorithms for bilevel optimization.
JV(G, (cid:15))
Gc(G, (cid:15))
Gc(F, (cid:15))
Algorithm
O((cid:15)−2)
O((cid:15)−2)
O((cid:15)−2)
MSTSA [22] (cid:101)O((cid:15)−2) (cid:101)O((cid:15)−2) (cid:101)O((cid:15)−2)
SEMA [11]
O((cid:15)−2)
O((cid:15)−2)
/
STABLE [2]
O (cid:0)(cid:15)−2(cid:1)
O((cid:15)−2)
O((cid:15)−2) stocBiO [20]
O((cid:15)−1.5) O((cid:15)−1.5) O (cid:0)(cid:15)−1.5(cid:1)
RSVRB [12] (Concurrent)
SUSTAIN [23] (Concurrent) O((cid:15)−1.5) O((cid:15)−1.5) O (cid:0)(cid:15)−1.5(cid:1)
O((cid:15)−1.5) O((cid:15)−1.5) O (cid:0)(cid:15)−1.5(cid:1) (cid:101)O (cid:0)(cid:15)−1.5(cid:1) (cid:101)O((cid:15)−1.5) (cid:101)O((cid:15)−1.5)
Gc(F, (cid:15)) and Gc(G, (cid:15)): number of gradient evaluations w.r.t. F and G.
Jv(G, (cid:15)): number of Jacobian-vector products ∇x∇yG(·)v. (cid:101)O(·): omit log 1
Hv(G, (cid:15)): number of Hessian-vector products ∇2
Hyyinv(G, (cid:15)): number of evaluations of Hessian inverse [∇2
HV(G, (cid:15)) (cid:101)O((cid:15)−2) (cid:101)O((cid:15)−2)
/ (cid:101)O (cid:0)(cid:15)−2(cid:1)
/ (cid:101)O (cid:0)(cid:15)−1.5(cid:1) (cid:101)O (cid:0)(cid:15)−1.5(cid:1) (cid:101)O (cid:0)(cid:15)−1.5(cid:1)
MRBO (ours)
VRBO (ours) (cid:15) terms. yG(·)v. yG]−1.
Hyyinv(G, (cid:15))
/
/
O((cid:15)−2)
/
O (cid:0)(cid:15)−1.5(cid:1)
/
/
/ which outperforms all existing algorithms by an order of (cid:15)−0.5. Technically, our analysis needs to ﬁrst characterize the estimation property for the momentum-based recursive estimator for the
Hessian-vector type hypergradient and then uses such a property to further bound the per-iteration error due to momentum updates for both inner and outer loops.
We then propose a double-loop variance-reduced bilevel optimizer (VRBO), which is the ﬁrst algorithm that adopts the recursive variance reduction for bilevel optimization. In VRBO, each inner loop constructs a variance-reduced gradient (w.r.t. y) and hypergradient (w.r.t. x) estimators through the use of large-batch gradient estimations computed periodically at each outer loop. Similarly to
MRBO, VRBO involves the computations of Jacobian- and Hessian-vector products rather than
Hessians or Hessian inverse. Theoretically, we show that VRBO achieves the same near-optimal complexity of (cid:101)O((cid:15)−1.5) as MRBO and outperforms all existing algorithms. Technically, differently from the use of variance reduction in minimization and minimax optimization, our analysis for VRBO needs to characterize the variance reduction property for the Hessian-vector type of hypergradient estimators, which only involves Hessian vector computation rather than Hessian. Such estimator introduces additional errors to handle in the telescoping and convergence analysis.
Our experiments2 show that VRBO achieves the highest accuracy among all comparison algorithms, and MRBO converges fastest among its same type of single-loop momentum-based algorithms. In particular, we ﬁnd that our double-loop VRBO algorithm converges much faster than other singlr-loop algorithms including our MRBO, which is in contrast to the existing efforts exclusively on accelerating the single-loop algorithms [2, 11, 22]. Such a result also differs from those phenomenons observed in minimization and minimax optimization, where single-loop algorithms often outperform double-loop algorithms. 1.2