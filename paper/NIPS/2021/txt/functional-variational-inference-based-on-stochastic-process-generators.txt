Abstract
Bayesian inference in the space of functions has been an important topic for
Bayesian modeling in the past. In this paper, we propose a new solution to this problem called Functional Variational Inference (FVI). In FVI, we minimize a divergence in function space between the variational distribution and the poste-rior process. This is done by using as functional variational family a new class of flexible distributions called Stochastic Process Generators (SPGs), which are cleverly designed so that the functional ELBO can be estimated efficiently using an-alytic solutions and mini-batch sampling. FVI can be applied to stochastic process priors when random function samples from those priors are available. Our experi-ments show that FVI consistently outperforms weight-space and function space VI methods on several tasks, which validates the effectiveness of our approach. 1

Introduction
As an important approach to Bayesian deep learning, Bayesian neural networks (BNNs) have been proposed and studied for decades [29, 34, 19]. Despite some successful applications in specific cases
[55, 44, 23], BNNs still suffer from several shortcomings. As over-parameterized models, BNNs often have multiple posterior modes in its weight space, that generate identical predictions [30].
Therefore, Bayesian inference in weight space can be very difficult [50]. Furthermore, due to the complexity of interactions between weights in the network forward pass, it is unclear the effect that a given prior distribution over the weights will have in the resulting distribution over functions.
To address these issues, there has been a recent resurgence of interest in applying the perspective of
Bayesian non-parametrics to neural nets. That is, BNNs are treated as probability measures over func-tions, i.e., as stochastic processes, with Bayesian inference being performed now in function space.e
Examples include functional BNNs (f-BNNs) [47], and variational implicit processes (VIPs) [28].
Despite their empirical advantage over weight space VI, they still suffer from a number of issues. i), f-BNNs optimize a functional KL divergence between BNNs and GPs, which is not always well-defined[5]. Also, f-BNNs rely on (spectral) stein gradient estimators, which are less efficient for high dimensional distributions [56, 14]. While VIPs do not have this problem, they use a wake-sleep procedure that does not correspond to a single unified objective. ii), F-BNNs rely on a mean-field approximation, which often lacks predictive in-between uncertainty on test data. This issue was ob-served both with single layer BNNs [10], and deeper BNNs (able to represent in-between uncertainty, but more over-confident than HMC empirically for certain settings [11, 9]). On the other hand, VIPs use as variational family a Gaussian process, which is not able to capture non-Gaussian processes such as structured implicit priors [47].
Therefore, it is an open challenge how to improve and justify (variational) inference in the space of functions using priors given by stochastic processes. In this paper, we investigate this old but important problem and propose a new solution. Our contributions are as follows: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) functional BNN (b) Mean field VI BNN (c) HMC BNN (d) Ours (FVI on BNN)
Figure 1: A regression task on a synthetic dataset (red crosses) from [10]. We plot predictive mean and uncertainties for each algorithm. This tasks is used to demonstrate the theoretical pathologies of weight-space VI for single-layer BNNs: there is no setting of the variational parameters that can model the in-between uncertainty between two data clusters. The functional BNNs [47] also have this problem, since mean-field BNNs are used as part of the model. On the contrary, our FVI method can produce sensible uncertainty estimates. See Appendix C.2 for more details.
• We propose a new objective function for variational inference in function space, as an alternative to functional KL divergence between stochastic prcoesses [47]. We show that this new objective is a valid divergence, and can avoid some of the problems that the functional KL divergence has.
• We propose a new class of flexible variational distributions in function space, called stochastic process generators (SPGs). SPGs are non-Gaussian generalizations of the VIP family [28], and can help avoid the fundamental limitation of mean field Gaussians [10] used in BNNs and functional
BNNs. A theorem regarding the expressiveness of SPGs is proved (Proposition 4). Based on SPGs, our proposed functional divergence between stochastic processes can be estimated efficiently using mini-batch sampling (Proposition 5, 6 and 7), which achieves a significant speed-up against the gradient estimator approach in [47].
• We compare our methods against existing weight-space and function-space inference methods in several tasks. Our method consistently outperforms the baselines, and is much faster than f-BNN, which validates the effectiveness of our approach. 2