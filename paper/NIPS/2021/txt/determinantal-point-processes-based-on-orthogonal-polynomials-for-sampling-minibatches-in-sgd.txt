Abstract
Stochastic gradient descent (SGD) is a cornerstone of machine learning. When the number N of data items is large, SGD relies on constructing an unbiased estimator of the gradient of the empirical risk using a small subset of the original dataset, called a minibatch. Default minibatch construction involves uniformly sampling a subset of the desired size, but alternatives have been explored for variance reduction.
In particular, experimental evidence suggests drawing minibatches from determi-nantal point processes (DPPs), tractable distributions over minibatches that favour diversity among selected items. However, like in recent work on DPPs for coresets, providing a systematic and principled understanding of how and why DPPs help has been difﬁcult. In this work, we contribute an orthogonal polynomial-based determinantal point process paradigm for performing minibatch sampling in SGD.
Our approach leverages the speciﬁc data distribution at hand, which endows it with greater sensitivity and power over existing data-agnostic methods. We substantiate our method via a detailed theoretical analysis of its convergence properties, inter-weaving between the discrete data set and the underlying continuous domain. In particular, we show how speciﬁc DPPs and a string of controlled approximations can lead to gradient estimators with a variance that decays faster with the batchsize than under uniform sampling. Coupled with existing ﬁnite-time guarantees for
SGD on convex objectives, this entails that, for a large enough batchsize and a ﬁxed budget of item-level gradients to evaluate, DPP minibatches lead to a smaller bound on the mean square approximation error than uniform minibatches. Moreover, our estimators are amenable to a recent algorithm that directly samples linear statistics of DPPs (i.e., the gradient estimator) without sampling the underlying DPP (i.e., the minibatch), thereby reducing computational overhead. We provide detailed synthetic as well as real data experiments to substantiate our theoretical claims.
∗Alphabetical order
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
Consider minimizing an empirical loss min
θ∈Θ 1
N
·
N (cid:88) i=1
L(zi, θ) + λ(θ), (1) with some penalty λ : Θ (cid:55)→ R+. Many learning tasks, such as regression and classiﬁcation, are usually framed that way [1]. When N (cid:29) 1, computing the gradient of the objective in (1) becomes a bottleneck, even if individual gradients ∇θL(zi, θ) are cheap to evaluate. For a ﬁxed computational budget, it is thus tempting to replace vanilla gradient descent by more iterations but using an approximate gradient, obtained using only a few data points. Stochastic gradient descent (SGD; [2]) follows this template. In its simplest form, SGD builds an unbiased estimator at each iteration of gradient descent, independently from past iterations, using a minibatch of random samples from the data set. Theory [3] and practice suggest that the variance of the gradient estimators in SGD should be kept as small as possible. It is thus natural that variance reduction for SGD has been a rich area of research; see for instance the detailed references in [4, Section 2].
In a related vein, determinantal point processes (DPPs) are probability distributions over subsets of a (typically large or inﬁnite) ground set that are known to yield samples made of collectively diverse items, while being tractable both in terms of sampling and inference. Originally introduced in electronic optics [5], they have been turned into generic statistical models for repulsion in spatial statistics [6] and machine learning [7, 8]. In ML, DPPs have also been shown to be efﬁcient sampling tools; see Section 2. Importantly for us, there is experimental evidence that minibatches in (1) drawn from DPPs and other repulsive point processes can yield gradient estimators with low variance for advanced learning tasks [4, 9], though a conclusive theoretical result has remained elusive. In particular, it is hard to see the right candidate DPP when, unlike linear regression, the objective function does not necessarily have a geometric interpretation.
Our contributions are as follows. We combine continuous DPPs based on orthogonal polynomials [10] and kernel density estimators built on the data to obtain two gradient estimators; see Section 3. We prove that their variance is OP (p−(1+1/d)), where p is the size of the minibatch, d is the dimension of data; see Section 4. This provides theoretical backing to the claim that DPPs yield variance reduction
[4] over, say, uniformly sampling without replacement. In passing, the combination of analytic tools –orthogonal polynomials–, and an essentially discrete subsampling task –minibatch sampling– sheds light on new ways to build discrete DPPs for subsampling. Finally, we demonstrate our theoretical results on simulated data in Section 5.
A cornerstone of our approach is to utilise orthogonal polynomials to construct our sampling paradigm, interweaving between the discrete set of data points and the continuum in which the orthogonal polynomials reside. A few words are in order regarding the motivation for our choice of techniques.
Roughly speaking, we would like to use a DPP that is tailored to the data distribution at hand.
Orthogonal Polynomial Ensembles (OPEs) provide a natural way of associating a DPP to a given measure (in this case, the probability distribution of the data points), along with a substantive body of mathematical literature and tools that can be summoned as per necessity. This makes it a natural choice for our purposes.
Notation. Let data be denoted by D := {z1, . . . , zN }, and assume that the zi’s are drawn i.i.d. from a distribution γ on Rd. Assume γ is compactly supported, with support D ⊂ [−1, 1]d bounded away from the border of [−1, 1]d. Assume also that γ is continuous with respect to the Lebesgue measure, and that its density is bounded away from zero on its support. While our assumptions exclude learning problems with discrete labels, such as classiﬁcation, we later give experimental support that our estimators yield variance reduction in that case too. We deﬁne the empirical measure
ˆγN := N −1 · (cid:80)N i=1 δzi , where δzi is the delta measure at the point zi ∈ Rd. Clearly, ˆγN → γ in
P(Rd); under our operating assumption of compact support, this amounts to convergence in P(D).
For simplicity, we assume that no penalty is used in (1), but our results will extend straightforwardly.
We denote the gradient of the empirical loss by ΞN = ΞN (θ) := N −1 · (cid:80)N i=1 ∇θL(zi, θ). A minibatch is a (random) subset A ⊂ [N ] of size |A| = p (cid:28) N such that the random variable wi∇θL(zi, θ), (2)
ΞA = ΞA(θ) := (cid:88) i∈A 2
for suitable weights (wi), provides a good approximation for ΞN . 2