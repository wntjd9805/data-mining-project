Abstract
UMAP has supplanted t-SNE as state-of-the-art for visualizing high-dimensional datasets in many disciplines, but the reason for its success is not well understood.
In this work, we investigate UMAP’s sampling based optimization scheme in detail.
We derive UMAP’s true loss function in closed form and ﬁnd that it differs from the published one in a dataset size dependent way. As a consequence, we show that UMAP does not aim to reproduce its theoretically motivated high-dimensional
UMAP similarities. Instead, it tries to reproduce similarities that only encode the k nearest neighbor graph, thereby challenging the previous understanding of
UMAP’s effectiveness. Alternatively, we consider the implicit balancing of attrac-tion and repulsion due to the negative sampling to be key to UMAP’s success. We corroborate our theoretical ﬁndings on toy and single cell RNA sequencing data. 1

Introduction
Today’s most prominent methods for non-parametric, non-linear dimension reduction are t-Distributed Stochastic Neighbor Embedding (t-SNE) [20, 19] and Uniform Manifold Approx-imation and Projection for Dimension Reduction (UMAP) [13]. The heart of UMAP is claimed to be its sophisticated method for extracting the high-dimensional similarities. However, the reason for UMAP’s excellent visualizations is not immediately obvious from this approach. In particular,
UMAP’s eponymous uniformity assumption, see Section 3, is arguably difﬁcult to defend for the wide range of datasets on which UMAP performs well. Therefore, it is not well understood what aspect of UMAP is responsible for its great visualizations.
Both t-SNE and UMAP have to overcome the computational obstacle of considering the quadratic number of interactions between all pairs of points. The breakthrough for t-SNE came with a
Barnes-Hut approximation [19]. Instead, UMAP employs a sampling based approach to avoid a quadratic number of repulsive interactions. Other than [4] little attention has been paid to this sampling based optimization scheme. In this work, we ﬁll this gap and analyze UMAP’s optimization method in detail. In particular, we derive the effective, closed form loss function which is truly minimized by UMAP’s optimization scheme. While UMAP’s use of negative sampling was intended to avoid quadratic complexity, we ﬁnd, surprisingly, that the resulting effective loss function differs signiﬁcantly from UMAP’s purported loss function. The weight of the loss function’s repulsive term is drastically reduced. As a consequence, UMAP is not actually geared towards reproducing the clever high-dimensional similarities. In fact, we show that most information beyond the shared kNN graph connectivity is essentially ignored as UMAP actually approximates a binarized version of the high-dimensional similarities. These theoretical ﬁndings underpin some empirical observations in [4] and demonstrate that the gist of UMAP is not its high-dimensional similarities. This resolves the disconnect between UMAP’s uniformity assumption and its success on datasets of varying density.
From a user’s perspective it is important to gain an intuition for deciding which features of a visualization can be attributed to the data and which ones are more likely artifacts of the visualization method. With our analysis, we can explain UMAP’s tendency to produce crisp, over-contracted substructures, which increases with the dataset size, as a side effect of its optimization. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Without the motivation of reproducing sophisticated high-dimensional similarities in embedding space, it seems unclear why UMAP performs well. We propose an alternative explanation for
UMAP’s success: The sampling based optimization scheme balances the attractive and repulsive loss terms despite the sparse high-dimensional attraction. Consequently, UMAP can leverage the connectivity information of the shared kNN graph via gradient descent effectively. 2