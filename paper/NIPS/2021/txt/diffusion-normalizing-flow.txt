Abstract
We present a novel generative modeling method called diffusion normalizing ﬂow based on stochastic differential equations (SDEs). The algorithm consists of two neural SDEs: a forward SDE that gradually adds noise to the data to transform the data into Gaussian random noise, and a backward SDE that gradually removes the noise to sample from the data distribution. By jointly training the two neural SDEs to minimize a common cost function that quantiﬁes the difference between the two, the backward SDE converges to a diffusion process the starts with a Gaussian distribution and ends with the desired data distribution. Our method is closely related to normalizing ﬂow and diffusion probabilistic models and can be viewed as a combination of the two. Compared with normalizing ﬂow, diffusion normalizing
ﬂow is able to learn distributions with sharp boundaries. Compared with diffusion probabilistic models, diffusion normalizing ﬂow requires fewer discretization steps and thus has better sampling efﬁciency. Our algorithm demonstrates competitive performance in both high-dimension data density estimation and image generation tasks. 1

Introduction
Generative model is a class of machine learning models used to estimate data distributions and sometimes generate new samples from the distributions [8, 35, 16, 37, 7]. Many generative models learn the data distributions by transforming a latent variable z with a tractable prior distribution to the data space [8, 35, 32]. To generate new samples, one can sample from the latent space and then follow the transformation to the data space. There exist a large class of generative models where the latent space and the data space are of the same dimension. The latent variable and the data are coupled through trajectories in the same space. These trajectories serve two purposes: in the forward direction x z, the trajectories infer the posterior distribution in the latent space associated with a x, it generates new samples by simulating given data sample x, and in the backward direction z the trajectories starting from the latent space. This type of generative model can be roughly divided into two categories, depending on whether these trajectories connecting the latent space and the data space are deterministic or stochastic.
→
→
When deterministic trajectories are used, these generative models are known as ﬂow-based models.
The latent space and the data space are connected through an invertible map, which could either be realized by the composition of multiple invertible maps [35, 8, 20] or a differential equation
[4, 14]. In these models, the probability density at each data point can be evaluated explicitly using the change of variable theorem, and thus the training can be carried out by minimizing the negative log-likelihood (NLL) directly. One limitation of the ﬂow-based model is that the invertible map parameterized by neural networks used in it imposes topological constraints on the transformation from z to x. Such limitation affects the performance signiﬁcantly when the prior distribution on z is a simple unimodal distribution such as Gaussian while the target data distribution is a well-separated multi-modal distribution, i.e., its support has multiple isolated components. In [6], it is shown that there are some fundamental issues of using well-conditioned invertible functions to approximate such complicated multi-modal data distributions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
When stochastic trajectories are used, the generative models are often known as the diffusion model
[38]. In a diffusion model, a prespeciﬁed stochastic forward process gradually adds noise into the data to transform the data samples into simple random variables. A separate backward process is trained to revert this process to gradually remove the noise from the data to recover the original data distributions. When the forward process is modeled by a stochastic differential equation (SDE), the optimal backward SDE [1] can be retrieved by learning the score function [39, 40, 17, 2]. When the noise is added to the data sufﬁciently slow in the forward process, the backward diffusion can often revert the forward one reasonably well and is able to generate high ﬁdelity samples. However, this also means that the trajectories have to be sufﬁciently long with a large number of time-discretization steps, which leads to slow training and sampling. In addition, since the forward process is ﬁxed, the way noise is added is independent of the data distribution. As a consequence, the learned model may miss some complex but important details in the data distribution, as we will explain later.
In this work, we present a new generative modeling algorithm that resembles both the ﬂow-based models and the diffusion models. It extends the normalizing ﬂow method by gradually adding noise to the sampling trajectories to make them stochastic. It extends the diffusion model by making the forward process from x to z trainable. Our algorithm is thus termed Diffusion Normalizing
Flow (DiffFlow). The comparisons and relations among DiffFlow, normalizing ﬂow, and diffusion models are shown in Figure 1. When the noise in DiffFlow shrinks to zero, DiffFlow reduces to a standard normalizing ﬂow. When the forward process is ﬁxed to some speciﬁc type of diffusion,
DiffFlow reduces to a diffusion model. xi 1
− f xi f xi 1
− xi s s xi 1
− f s xi f s xi+1 xi+1 xi+1
Normalizing Flows
Diﬀusion Models
DiﬀFlow
Backward/Sampling
Forward/Diﬀusing
Network
Noise
Figure 1: The schematic diagram for normalizing ﬂows, diffusion models, and DiffFlow. In normal-izing ﬂow, both the forward and the backward processes are deterministic. They are the inverse of each other and thus collapse into a single process. The diffusion model has a ﬁxed forward process and trainable backward process, both are stochastic. In DiffFlow, both the forward and the backward processes are trainable and stochastic.
In DiffFlow, the forward and backward diffusion processes are trained simultaneously by minimizing the distance between the forward and the backward process in terms of the Kullback-Leibler (KL) divergence of the induced probability measures [42]. This cost turns out to be equivalent to (see
Section 3 for a derivation) the (amortized) negative evidence lower bound (ELBO) widely used in variational inference [21]. One advantage to use the KL divergence directly is that it can be estimated with no bias using sampled trajectories of the diffusion processes. The KL divergence in the trajectory space also bounds the KL divergence of the marginals, providing an alternative method to bound the likelihood (see Section 3 for details). To summarize, we have made the following contributions. 1. We propose a novel density estimation model termed diffusion normalizing ﬂow (DiffFlow) that extends both the ﬂow-based models and the diffusion models. The added stochasticity in DiffFlow boosts the expressive power of the normalizing ﬂow and results in better performance in terms of sampling quality and likelihood. Compared with diffusion models, DiffFlow is able to learn a forward diffusion process to add noise to the data adaptively and more efﬁciently. This avoids adding noise to regions where noise is not so desirable. The learnable forward process also shortens the trajectory length, making the sampling much faster than standard diffusion models (We observe a 20 times speedup over diffusion models without decreasing sampling quality much). 2. We develop a stochastic adjoint algorithm to train the DiffFlow model. This algorithm evaluates the objective function and its gradient sequentially along the trajectory. It avoids storing all the 2
intermediate values in the computational graph, making it possible to train DiffFlow for high-dimensional problems. 3. We apply the DiffFlow model to several generative modeling tasks with both synthetic and real datasets, and verify the performance of DiffFlow and its advantages over other methods. 2