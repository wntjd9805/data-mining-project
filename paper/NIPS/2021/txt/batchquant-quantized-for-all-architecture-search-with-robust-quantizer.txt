Abstract
As the applications of deep learning models on edge devices increase at an acceler-ating pace, fast adaptation to various scenarios with varying resource constraints has become a crucial aspect of model deployment. As a result, model optimization strategies with adaptive configuration are becoming increasingly popular. While single-shot quantized neural architecture search enjoys flexibility in both model architecture and quantization policy, the combined search space comes with many challenges, including instability when training the weight-sharing supernet and difficulty in navigating the exponentially growing search space. Existing methods tend to either limit the architecture search space to a small set of options or limit the quantization policy search space to fixed precision policies. To this end, we propose BatchQuant, a robust quantizer formulation that allows fast and stable training of a compact, single-shot, mixed-precision, weight-sharing supernet. We employ BatchQuant to train a compact supernet (offering over 1076 quantized sub-nets) within substantially fewer GPU hours than previous methods. Our approach,
Quantized-for-all (QFA), is the first to seamlessly extend one-shot weight-sharing
NAS supernet to support subnets with arbitrary ultra-low bitwidth mixed-precision quantization policies without retraining. QFA opens up new possibilities in joint hardware-aware neural architecture search and quantization. We demonstrate the effectiveness of our method on ImageNet and achieve SOTA Top-1 accuracy under a low complexity constraint (< 20 MFLOPs). The code and models will be made publicly available at https://github.com/bhpfelix/QFA. 1

Introduction
In order to deploy deep learning models on resource-constrained edge devices, careful model opti-mization, including pruning and quantization is required. While existing works have demonstrated the effectiveness of model optimization techniques in speeding up model inference [1, 2, 3, 4], model optimization increases human labor by introducing extra hyperparameters. Consequently, automated methods such as neural architecture search (NAS) [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16] and automated quantization policy search [17, 18, 19] have emerged to alleviate the human bandwidth required for obtaining compact models with good performance.
In this paper, we focus on finding the best of both worlds—the best combination of architecture and mixed-precision quantization policy. However, combining two complex search spaces is inherently challenging, not to mention that quantization usually requires a lengthy quantization-aware training (QAT) procedure to recover performance. Thus, previous methods tend to employ proxies to estimate the performance of an architecture and quantization policy combination. For example, APQ [20] performs QAT on each of 5000 architecture and quantization policy combinations for 0.2 GPU hours
∗Corresponding author: Haoping Bai haoping_bai@apple.com. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparisons of quantized architecture search approaches: DNAS [19] SPOS [12], HAQ [17],
APQ [20], OQA [23] and BQ (Ours). “Single-shot mixed-precision QAT” means the supernet is directly trained with QAT on arbitrary mixed-precision quantization policies. “No training during search” means there is no need for re-training the sampled network candidate during the search phase, and this is accomplished by single-shot supernet training similar to [22]. “No evaluation during search” means that we do not have to evaluate sampled network candidates on the validation dataset during the search phase, and this is achieved by training an accuracy predictor similar to
[22]. “No retraining/finetuning” means that we do not have to finetune any searched architectures as weights inherited from supernet already allow inference for the given architecture under the specified quantization policy. “Weight-sharing” means that quantization policies share the same underlying full-precision weights so that we can obtain mixed-precision weights by fusing the corresponding quantizers with the same set of full precision weights. “Compact MobileNet search space” means that supernet is based on the Mobilenet search space, which offers better accuracy v.s. complexity trade-off but is more sensitive to quantization than heavy search space such as ResNet.
Single-shot mixed-precision QAT
No training during search
No evaluation during search
No retraining / finetuning
Mixed-precision quantization
Weight-sharing
Compact MobileNet search space
DNAS
SPOS HAQ APQ OQA QFA
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓ and uses the sampled combinations to train an accuracy predictor. BATS [21] searches for cell structures that are repeatedly stacked to form the target architecture. Proxy-based methods necessitate both careful treatments to ensure reliable ranking of quantized architectures and a time-consuming retraining procedure when the target quantized architecture is identified, rendering proxy-based approaches impractical for frequently changing deployment scenarios.
To avoid the lengthy retraining process, NAS methods that train a single-shot weight-sharing su-pernet [12, 22] are ideal. However, many previous works have shown evidence that QAT of mixed-precision supernets can easily become highly unstable [23, 21]. As a result, existing single-shot quantized architecture search methods usually limit the size of the combined search space. For example, SPOS [12] sacrifices architecture search space size to only allow channel search and re-quires retraining to recover performance. OQA [23] limits its quantization policy search space to fixed-precision quantization policies and trains a separate set of weights for each bitwidth.
To successfully train the mixed-precision supernet, we propose BatchQuant (BQ). Analogous to batch normalization, BQ leverages batch statistics to adapt to the shifting activation distribution as a result of quantized subnet selection, offering better robustness to outliers than quantizer with vanilla running min/max based scale estimation, and better flexibility than a learnable quantizer that only learns a fixed set of parameters. Without limiting the architecture search space, our joint architecture and quantization policy search space contains over 1076 possible quantized subnets, providing much more flexibility than previous search spaces (e.g. The OQA search space has 1020 possible quantized subnets). While our approach and OQA both follow the supernet training strategy introduced in
[22], our weight-sharing supernet takes only 190 epochs to train despite the complex search space, significantly less than the 495 epochs required by OQA. We further leverage the NSGA-II algorithm to produce a Pareto set of quantized architectures that densely covers varying complexity constraints, eliminating the marginal cost of adapting architecture to new deployment scenarios.
The contributions of the paper are
• To the best of our knowledge, we present the first result to train one-shot weight-sharing supernet to support subnets with arbitrary mixed-precision quantization policy without retraining.
• We propose BatchQuant, an activation quantizer formulation for stable mixed-precision supernet training. The general formulation of BatchQuant allows easy adaptation of new scale estimators.
• Compared with existing methods, our method, QFA, takes a shorter time to train a significantly more complex supernet with over 1076 possible quantized subnets and discovers quantized subnets at SOTA efficiency with no marginal search cost for new deployment scenarios. 2
Table 2: Design runtime comparison with state-of-the-art quantized architecture search methods. Here we use T (·) to denote the runtime of an algorithm. Note we separate the accuracy predictor training from the search procedure because it is a one-time cost amortized across deployment scenarios. We define marginal cost as the cost for searching in a new deployment scenario, and we use N to denote the number of deployment scenarios. Our method eliminates the marginal search cost completely.
Method
SPOS
APQ
OQA
Ours
Design Runtime
T(Supernet Training) + T(Search + Finetune) × N
T(Supernet Training + Accuracy Predictor Training) + T(Search + Finetune) × N
T(Supernet Training + Accuracy Predictor Training) + T(Search + Finetune) × N
T(Supernet Training + Accuracy Predictor Training + Search) 2