Abstract
Recently, there has been great success in applying deep neural networks on graph structured data. Most work, however, focuses on either node- or graph-level supervised learning, such as node, link or graph classiﬁcation or node-level un-supervised learning (e.g., node clustering). Despite its wide range of possible applications, graph-level unsupervised representation learning has not received much attention yet. This might be mainly attributed to the high representation complexity of graphs, which can be represented by n! equivalent adjacency ma-trices, where n is the number of nodes. In this work we address this issue by proposing a permutation-invariant variational autoencoder for graph structured data. Our proposed model indirectly learns to match the node order of input and output graph, without imposing a particular node order or performing expensive graph matching. We demonstrate the effectiveness of our proposed model for graph reconstruction, generation and interpolation and evaluate the expressive power of extracted representations for downstream graph-level classiﬁcation and regression. 1

Introduction
Graphs are an universal data structure that can be used to describe a vast variety of systems from social networks to quantum mechanics [1]. Driven by the success of Deep Learning in ﬁelds such as Computer Vision and Natural Language Processing, there has been an increasing interest in applying deep neural networks on non-Euclidean, graph structured data as well [2, 3]. Most notably, generalizing Convolutional Neural Networks and Recurrent Neural Networks to arbitrarily structured graphs for supervised learning has lead to signiﬁcant advances on task such as molecular property prediction [4] or question-answering [5]. Research on unsupervised learning on graphs mainly focused on node-level representation learning, which aims at embedding the local graph structure into latent node representations [6, 7, 8, 9, 10]. Usually, this is achieved by adopting an autoencoder framework where the encoder utilizes e.g., graph convolutional layers to aggregate local information at a node level and the decoder is used to reconstruct the graph structure from the node embeddings.
Graph-level representations are usually extracted by aggregating node-level features into a single vector, which is common practice in supervised learning on graph-level labels [4].
Unsupervised learning of graph-level representations, however, has not yet received much attention, despite its wide range of possible applications, such as feature extraction, pre-training for graph-level classiﬁcation/regression tasks, graph matching or similarity ranking. This might be mainly attributed to the high representation complexity of graphs arising from their inherent invariance with respect 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Network architecture of the proposed model. Input graph is depicted as fully connected graph (dashed lines for not direct neighbours) with the additional embedding node v0 and edges to it (black color). Different node and edge types are represented by different colors and real edges by solid lines between nodes. Transformations parameterized by a neural network are represented by block arrows. to the order of nodes within the graph. In general, a graph with n nodes, can be represented by n! equivalent adjacency matrices, each corresponding to a different node order. Since the general structure of a graph is invariant to the order of their individual nodes, a graph-level representation should not depend on the order of the nodes in the input representation of a graph, i.e. two isomorphic graphs should always be mapped to the same representation. This poses a problem for most neural network architectures which are by design not invariant to the order of their inputs. Even if carefully designed in a permutation invariant way (e.g., Graph Neural Networks with a ﬁnal node aggregation step), there is no straight-forward way to train an autoencoder network, due to the ambiguous reconstruction objective, requiring the same discrete order of input and output graphs to compute the reconstruction loss.
How can we learn a permutation-invariant graph-level representation utilizing a permutation-variant reconstruction objective? In this work we tackle this question proposing a graph autoencoder architecture that is by design invariant to the order of nodes in a graph. We address the order ambiguity issue by training alongside the encoder and decoder model an additional permuter model that assigns to each input graph a permutation matrix to align the input graph node order with the node order of the reconstructed graph. 2 Method 2.1 Notations and Problem Deﬁnition
An undirected Graph G = (V, E) is deﬁned by the set of n nodes V = {v1, . . . , vn} and edges
E = {(vi, vj)|vi, vj ∈ V}. We can represent a graph in matrix form by its node features Xπ ∈ Rn×dv and adjacency matrix Aπ ∈ {0, 1}n×n in the node order π ∈ Π, where Π is the set of all n! permutations over V. We deﬁne the permutation matrix P that reorders nodes from order π to order
π(cid:48) as Pπ→π(cid:48) = (pij) ∈ {0, 1}n×n, with pij = 1 if π(i) = π(cid:48)(j) and pij = 0 everywhere else. Since
Graphs are invariant to the order of their nodes, note that
Gπ = G(Xπ, Aπ) = G(Pπ→π(cid:48)Xπ, Pπ→π(cid:48)AπP(cid:62)
π→π(cid:48)) = G(Xπ(cid:48), Aπ(cid:48)) = Gπ(cid:48), (1) where (cid:62) is the transpose operator. Let us now consider a dataset of graphs G = {G(i)}N i=0 we would like to represented in a low-dimensional continuous space. We can adopt a latent variable approach and assume that the data is generate by a process pθ(G|z), involving an unobserved continuous random variable z. Following the work of Kingma and Welling [11], we approximate the intractable posterior by qφ(G|z) ≈ pθ(G|z) and minimize the lower bound on the marginal likelihood of graph
G(i): log pθ(G(i)) ≥ L(φ, θ; G(i)) = −KL (cid:104) qφ(z|G(i))||pθ(z) (cid:105)
+ Eqφ(z|G(i)) (cid:104) (cid:105) log pθ(G(i)|z)
, (2) 2
where the Kullback–Leibler (KL) divergence term regularizes the encoded latent codes of graphs G(i) and the second term enforces high similarity of decoded graphs to their encoded counterparts. As graphs can be completely described in matrix form by their node features and adjacency matrix, we can parameterize qφ and pθ in Eq. (2) by neural networks that encode and decode node features X(i)
π
π of graphs G(i) and adjacency matrices A(i)
π . However, as graphs are invariant under arbitrary node re-ordering, the latent code z should be invariant to the node order π: qφ(z|Gπ) = qφ(z|Gπ(cid:48)), for all π, π(cid:48) ∈ Π. (3)
This can be achieved by parameterizing the encoder model qφ by a permutation invariant function.
However, if the latent code z does not encode the input node order π, input graph Gπ and decoded graph ˆGπ(cid:48) are no longer necessarily in the same order, as the decoder model has no information about the node order of the input graph. Hence, the second term in Eq. (2) cannot be optimized anymore by minimizing the reconstruction loss between encoded graph Gπ and decoded graph ˆGπ(cid:48) in a straight-forward way. They need to be brought in the same node order ﬁrst. We can rewrite the expectation in Eq. (2) using Eq. (1): (cid:104) (cid:105)
π(cid:48) |z) log pθ(G(i)
Eqφ(z|G(i)
π )
= Eqφ(z|G(i)
π )
Since the ordering of the decoded graph π(cid:48) is subject to the learning process of the decoder and thus unknown in advance, ﬁnding Pπ→π(cid:48) is not trivial. In [12], the authors propose to use ap-proximate graph matching to ﬁnd the permutation matrix Pπ→π(cid:48) that maximizes the similarity s(Xπ(cid:48), Pπ→π(cid:48) ˆXπ; Aπ(cid:48), Pπ→π(cid:48) ˆAπP(cid:62)
π→π(cid:48)), which involves up to O(n4) re-ordering operations at each training step in the worst case [13].
π |z) (4)
. log pθ( ˆPπ→π(cid:48)G(i) (cid:105) (cid:104) 2.2 Permutation-Invariant Variational Graph Autoencoder
In this work we propose to solve the reordering problem in Eq. (4) implicitly by inferring the permutation matrix Pπ(cid:48)→π from the input graph Gπ by a model gψ(Gπ) that is trained to bring input and output graph in the same node order and is used by the decoder model to permute the output graph. We train this permuter model jointly with the encoder model qφ(z|Gπ) and decoder model pθ(Gπ|z, Pπ(cid:48)→π), optimizing:
L(φ, θ, ψ; G(i)
π ) = −KL (cid:104) qφ(z|G(i)
π )||pθ(z) (cid:105)
+ Eqφ(z|G(i)) (cid:104) log pθ(G(i)
π |z, gψ(G(i)
π )) (cid:105)
. (5)
Intuitively, the permuter model has to learn how the ordering of nodes in the graph generated by the decoder model will differ from a speciﬁc node order present in the input graph. During the learning process, the decoder will learn its own canonical ordering that, given a latent code z, it will always reconstruct a graph in. The permuter learns to transform/permute this canonical order to a given input node order. For this, the permuter predicts for each node i of the input graph a score si corresponding to its probability to have a low node index in the decoded graph. By sorting the input nodes indices by their assigned scores we can infer the output node order and construct the respective permutation matrix Pπ→π(cid:48) = (pij) ∈ {0, 1}n×n, with pij = (cid:26)1, 0, if j = argsort(s)i else (6) to align input and output node order. Since the argsort operation is not differentiable, we utilizes the continuous relaxation of the argsort operator proposed in [14, 15]:
P ≈ ˆP = softmax(
−d(sort(s)1(cid:62), 1s(cid:62))
τ
), (7) where the softmax operator is applied row-wise, d(x, y) is the L1-norm and τ ∈ R+ a temperature-parameter. By utilizing this continuous relaxation of the argsort operator, we can train the permuter model gψ in Eq. (5) alongside the encoder and decoder model with stochastic gradient descent. In order to push the relaxed permutation matrix towards a real permutation matrix (only one 1 in every row and column), we add to Eq. (5) a row- and column-wise entropy term as additional penalty term:
C(P) = (cid:88) i
H(pi,·) + (cid:88) j
H(p·,j), (8) 3
i xi log(xi) and normalized probabilities pi,· = pi,· j pi,j with Shannon entropy H(x) = − (cid:80)
Propositions 1. A square matrix P is a real permutation matrix if and only if C(P) = 0 and the doubly stochastic constraint pij ≥ 0 ∀(i, j), (cid:80)
Proof. See Appendix A.
By enforcing ˆP → P, we ensure that no information about the graph structure is encoded in ˆP and decoder model pθ(Gπ|z, P) can generate valid graphs during inference, without providing a speciﬁc permutation matrix P (e.g., one can set P = I and decode the learned canonical node order). At this point it should also be noted, that our proposed framework can easily be generalized to arbitrary sets of elements, although we focus this work primarily on sets of nodes and edges deﬁning a graph. i pij = 1 ∀j, (cid:80) j pij = 1 ∀i holds. (cid:80)
.
Graph Isomorphism Problem. Equation (5) gives us means to train an autoencoder framework with a permutation invariant encoder that maps a graph f : G → Z in an efﬁcient manner. Such an encoder will always map two topologically identical graphs (even with different node order) to the same representation z. Consequently, the question arises, if we can decide for a pair of graphs whether they are topologically identical. This is the well-studied graph isomorphism problem for which no polynomial-time algorithm is known yet [16, 17]. As mentioned above, in our framework, two isomorphic graphs will always be encoded to the same representation. Still, it might be that two non-isomorphic graphs will be mapped to the same point (non-injective). However, if the decoder is able to perfectly reconstruct both graphs (which is easy to check since the permuter can be used to bring the decoded graph in the input node order), two non-isomorphic graphs must have a different representation z. If two graphs have the same representation and the reconstruction fails, the graphs might still be isomorphic but with no guarantees. Hence, our proposed model can solve the graph isomorphism problem at least for all graphs it can reconstruct. 2.3 Details of the Model Architecture
In this work we parameterize the encoder, decoder and permuter model in Eq. (5) by neural networks utilizing the self-attention framework proposed by Vaswani et al. [18] on directed messages represent-ing a graph. Figure 1, visualizes the architecture of the proposed permutation-invariant variational autoencoder. In the following, we describe the different parts of the model in detail1.
Graph Representation by Directional Messages.
In general, most graph neural networks can be thought of as so called Message Passing Neural Networks (MPNN) [19]. The key idea of MPNNs is the aggregation of neighbourhood information by passing and receiving messages of each node to and from neighbouring nodes in a graph. We adopt this view and represent graphs by its messages between nodes. We represent a graph G(X, E), with node features X ∈ Rn×dv and edge features
E ∈ Rn×n×de , by its message matrix M = (mij) ∈ Rn×n×dm: mij = σ ([xi||xj||eij] W + b) , (9) with non-linearity σ, concatenation operator || and trainable parameters W and b. Note, that nodes in this view are represented by self-messages diag(M), messages between non-connected nodes exists, although the presence or absence of a connection might be encoded in eij, and if M is not symmetric, edges have an inherent direction.
Self-Attention on Directed Messages. We follow the idea of aggregating messages from neigh-bours in MPNNs, but utilize the self-attention framework proposed by Vaswani et al. [18] for sequential data. Our proposed model comprises multiple layers of multi-headed scaled-dot product attention. One attention head is deﬁned by:
Attention (Q, K, V) = softmax (cid:19) (cid:18) QK(cid:62)
√ dk
V (10) with queries Q = MWQ, keys K = MWK, and values V = MWV and trainable weights WQ ∈
Rdm×dq , WK ∈ Rdm×dk and WV ∈ Rdm×dv . For multi-headed self-attention we concatenate multiple attention heads together and feed them to a linear layer with dm output features. Since the message matrix M of a graph with n nodes comprises n2 messages, attention of all messages to all messages would lead to a O(n4) complexity. We address this problem by letting messages 1Code available at https://github.com/jrwnter/pigvae 4
mij only attend on incoming messages mki, reducing the complexity to O(n3). We achieve this by representing Q as a (m × n × d) tensor and K and V by a transposed (n × m × d) tensor, resulting into a (m × n × m) attention tensor, with number of nodes m = n and number of features d. That way, we can efﬁciently utilize batched matrix multiplications in Eq. (10), in contrast to computing the whole (n2 × n2) attention matrix and masking attention on not incoming messages out.
Encoder To encode a graph into a ﬁxed-sized, permutation-invariant, continuous latent represen-tation, we add to input graphs a dummy node v0, acting as an embedding node. To distinguish the embedding node from other nodes, we add an additional node and edge type to represent this node and edges to and from this node. After encoding this graph into a message matrix M(enc, 0) as deﬁned in Eq. (9), we apply L iterations of self-attention to update M(enc, L), accumulating the graph structure in the embedding node, represented by the self-message m(enc, L)
. Following [11], we utilize the reparameterization trick and sample the latent representation z of a graph by sampling from a multivariate normal distribution: 0,0 z ∼ N (fµ(m(enc, L) 0,0
), fσ(m(enc, L) 0,0
)I), (11) with fµ : m0,0 → µ ∈ Rdz and fσ : m0,0 → σ ∈ Rdz , parameterized by a linear layer.
Permuter To predict how to re-order the nodes in the output graph to match the order of nodes in the input graph, we ﬁrst extract node embeddings represented by self-messages on the main diagonal of the encoded message matrix m(enc, L)
= diag(M(enc, L)) for i > 0. We score these messages by a function fs : mi,i → s ∈ R, parameterized by a linear layer and apply the soft-sort operator (see
Eq. (7)) to retrieve the permutation matrix ˆP. i,i
Decoder We initialize the message matrix for the decoder models input with the latent representa-tion z at each entry. To break symmetry and inject information about the relative position/order of nodes to each other, we follow [18] and deﬁne position embeddings in dimension k
PE(i)k = (cid:26) sin(i/100002k/dz ), cos(i/100002k/dz ), for even k for odd k (12)
It follows for the initial decoder message matrix M(dec, 0): m(dec, 0) ij (13)
Since the self-attention based decoder model is permutation equivariant, we can move the permutation operation in Eq. (5) in front of the decoder model and directly apply it to the position embedding sequence (see Figure 1). After L iterations of self-attention on the message matrix M, we extract node features xi ∈ X and edge features ei,j ∈ E by a ﬁnal linear layer:
= σ ([z + [PE(i)||PE(j)]] W + b) , xi = mi,iWv + bv ei,j = 0.5 · (mi,j + mj,i)We + be, (14) with learnable parameters Wv ∈ Rdm×dv , We ∈ Rdm×de , bv ∈ Rdv and be ∈ Rde . 0,0
Overall Architecture We now describe the full structure of our proposed method using the in-gredients above (see Figure 1). Initially, the input graph is represented by the directed message matrix M(enc, 0), including an additional graph embedding node v0. The encoder model performs L iterations of self-attention on incoming messages. Next, diagonal entries of the resulting message matrix M(enc, L) are extracted. Message m(enc, L)
, representing embedding node v0, is used to condition the normal distribution, graph representation z is sampled from. The other diagonal entries m(enc, L) are transformed into scores and sorted by the Soft-Sort operator to retrieve the permutation i,i matrix ˆP. Next, position embeddings (in Figure 1 represented by single digits) are re-ordered by applying ˆP and added by the sampled graph embedding z. The resulting node embeddings are used to initialize message matrix M(dec, 0) and fed into the decoding model. After L iterations of self-attention, diagonal entries are transformed to node features X and off-diagonal entries to edge features E to generate the output graph. In order to train and infer on graphs of different size, we pad all graphs in a batch with empty nodes to match the number of nodes of the largest graph. Attention on empty nodes is masked out at all time. To generate graphs of variable size, we train alongside the variational autoencoder an additional multi-layer perceptron to predict the number of atoms of graph from its latent representation z. During inference, this model informs the decoder on how many nodes to attend to. 5
Table 1: Negative log likelihood (NLL) and area under the receiver operating characteristics curve (ROC-AUC) for reconstruction of the adjacency matrix of graphs from different families. We compare our proposed method (PIGAE) with Graph Autoencoder (GAE) [8] and results of Graphite and Graph
Autoencoder (GAE*) reported in [20]. PIGAE∗ utilize topological distances of nodes in a graph as edge feature.
MODELS
ERDOS-RENYI
BARABASI-ALBERT
EGO
PIGAE
PIGAE∗
GAE
GAE∗
GRAPHITE
NLL 20.5 ± 0.9 19.5 ± 0.8 186 ± 3 222 ± 8 196 ± 1
ROC-AUC 98.3 ± 0.1 99.4 ± 0.1 57.9 ± 0.1
--NLL 27.2 ± 0.9 15.2 ± 0.8 199 ± 3 236 ± 15 192 ± 2
ROC-AUC 96.7 ± 0.2 99.5 ± 0.1 57.4 ± 0.1
--NLL 23.4 ± 0.5 22.4 ± 0.5 191 ± 4 197 ± 2 183 ± 1
ROC-AUC 97.8 ± 0.3 98.8 ± 0.3 59.1 ± 0.1
--Key Architectural Properties Since no position embeddings are added to the input of the encoders self-attention layers, accumulated information in the single embedding node v0 (m0,0) is invariant to permutations of the input node order. Hence, the resulting graph embedding z is permutation invariant as well. This is in stark contrast to classical graph autoencoder frameworks [8, 20, 21], that encode whole graphs effectively by concatenating all node embeddings, resulting in a graph-level representation that is different for isomorphic graphs, as the sequence of node embeddings permutes equivalently with the input node order. As no information about the node order is encoded in the graph embedding z, the decoder learns its own (canonical) node order, distinct graphs are deterministically decoded in. The input node order does not inﬂuence this decoded node order. As the decoder is based on permutation equivariant self-attention layers, this canonical order is solely deﬁned with respect to the sequence of position embeddings used to initialize the decoders input. If the sequence of position embeddings is permuted, the decoded node order permutes equivalently. Thus, by predicting the right permutation matrix, input and output order can be aligned to correctly calculate the reconstruction loss. Input to the permuter model [m1,1, . . . , mn+1,n+1] is equivariant to permutations in the input node order (due to the equivariant self-attention layers in the encoder). Since the permuter model itself (i.e., the scoring function) is also permutation equivariant (node-wise linear layer), resulting permutation matrices P are equivariant to permutations in the input node order. Consequently, if the model can correctly reconstruct a graph in a certain node order, it can do it for all n! input node orders, and the learning process of the whole model is independent to the node order of graphs in the training set. 3