Abstract
The quadratic computational and memory complexities of the Transformer’s at-tention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear uniﬁed nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a ﬁxed length as input and an additional corresponding output, which allows
Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efﬁciency of Luna compared to a variety of strong baseline methods including the full-rank attention and other efﬁcient sparse and dense attention methods. The implementation of our model is available at https://github.com/XuezheMax/fairseq-apollo. 1

Introduction
Transformers (Vaswani et al., 2017) are surprisingly versatile models that preform well on a wide range of language and vision tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language understanding (Devlin et al., 2019), im-age recognition (Dosovitskiy et al., 2020) and bioin-formatics (Madani et al., 2020). Attention (Bah-danau et al., 2015) provides the key mechanism that captures contextual information from the entire se-quence by modeling pairwise interactions between the inputs at every timestep. However, a common weakness of Transformers is their quadratic time and memory complexity within the attention mech-anism w.r.t the length of the input sequence, which prohibitively restricts their potential application to tasks requiring longer input sequences.
Figure 1: Trade-off between accuracy (y-axis), speed (x-axis) and memory (cir-radius) on LRA.
A number of techniques have been recently introduced to improve the time and memory efﬁciency of Transformer models (‘xformers’) (Tay et al., 2020b, 2021). One popular technique is using sparsity to restrict the attention ﬁeld range, such as local attention (Parmar et al., 2018), blockwise attention (Qiu et al., 2019), strided attention patterns (Child et al., 2019; Beltagy et al., 2020),
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
compressed attention (Liu et al., 2018), and attention with learnable patterns (Kitaev et al., 2020; Tay et al., 2020a; Roy et al., 2021). Another emerging approach is to improve efﬁciency by leveraging low-rank approximations of the attention matrix. Linformer (Wang et al., 2020), for example, projects the length dimension of key and value matrices to a ﬁxed-dimensional representation by assuming low-rank structure in the full-rank attention matrix. Recently, some kernel-based methods, such as
Linear Transformer (Katharopoulos et al., 2020), Performer (Choromanski et al., 2020) and Random
Feature Attention (Peng et al., 2021), attempt to efﬁciently approximate regular (softmax) full-rank attention through kernelization. Although these models demonstrate better asymptotic complexity for long sequences, their efﬁciency gains are less prominent for moderate length sequences and their performance remains behind Transformers with regular attention.
In this work, we propose a linear uniﬁed nested attention mechanism (Luna), which uses two nested attention functions to approximate the regular softmax attention in Transformer (§2). Speciﬁcally, with the ﬁrst attention function, Luna packs the input sequence into a sequence of ﬁxed length. Then, the packed sequence is unpacked using the second attention function (§3.1). As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a ﬁxed length as input and an additional corresponding output. Importantly, the extra input allows Luna to perform attention operation linearly as efﬁciently as Linformer (Wang et al., 2020), while also storing adequate contextual information. Unlike Linformer, Luna is capable of modeling variable-length sequences and autoregressive (causal) attention (§3.4). We perform extensive experiments on three sequence modeling tasks, including long-context sequence modeling, neural machine translation, and masked language modeling for large-scale pretraining and downstream task ﬁnetuning. Compared to a variety of strong baseline models, Luna achieves competitive or even better performance, while acquiring prominent gains of efﬁciency in both speed and memory (see Figure 1). More importantly, Luna manages to obtain superior performance with small projection lengths such as 16 (§4). 2