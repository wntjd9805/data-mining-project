Abstract
Automated data augmentation (ADA) techniques have played an important role in boosting the performance of deep models. Such techniques mostly aim to opti-mize a parameterized distribution over a discrete augmentation space. Thus, are restricted by the discretization of the search space which normally is handcrafted.
To overcome the limitations, we take the ﬁrst step to constructing a continuous mapping from Rd to image transformations (an augmentation space). Using this mapping, we take a novel approach where 1) we pose the ADA as a continuous optimization problem over the parameters of the augmentation distribution; and 2) use Stochastic Gradient Langevin Dynamics to learn and sample augmentations.
This allows us to potentially explore the space of inﬁnitely many possible augmen-tations, which otherwise was not possible due to the discretization of the space.
This view of ADA is radically different from the standard discretization based view of ADA , and it opens avenues for utilizing the vast efﬁcient gradient-based algorithms available for continuous optimization problems. Results over multiple benchmarks demonstrate the efﬁciency improvement of this work compared with previous methods. 1

Introduction
Data augmentation [22] is one of the advanced techniques or recipes that proliferate the success of deep learning. The crux behind data augmentation is to make a model invariant towards various input transformations that we expect to observe during inference, thus, improves the generalization of the model. Since it is not clear which transformations would beneﬁt most, manual design is difﬁcult. Thus, there has been a growing interest in automatically learning such augmentations, a new thriving subarea of data augmentation known as the Automated Data Augmentation (ADA) [5, 16, 21, 20, 19, 28].
ADA is generally formulated as a bilevel optimization problem over a discrete search space, and aims to ﬁnd an optimal augmentation policy. For example, the search space used in AutoAugment [5], the very ﬁrst promising work in this topic, contains several augmentation candidates, e.g., rota-tion/blurring/brightening with different discrete magnitudes, and the augmentation policy is deﬁned as a parameterized discrete distribution over this space. Such approach has been shown to provide
∗Authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
far more superior performances over several datasets and tasks compared against the traditional handcrafted augmentation counterparts. This, however, comes at the expense of a high computational budget, taking far more time than model training, resulting in an untenable training procedure in the presence of real-world large-scale datasets. While several approaches have been proposed to address the efﬁciency issue [20, 16, 21, 19], the compromise in performance due to such efﬁciency measures was nontrivial; some of the current state-of-art remains computationally expensive [28].
The contradiction between performance and efﬁciency has raised a key challenge in this ﬁeld.
Another major limitation of such approaches is due to the discretized search space which limits the diversity of the learnable policy. In fact, the discretization results in augmentations that do not always represent the variations in the real world sufﬁciently. In the real world, the data is collected with a great range of freedom such as varying camera viewpoint and lighting conditions. However, the discrete augmentation would only be able to produce transformed data with speciﬁc types and magnitudes. Furthermore, quantization of augmentation search space introduces extra manual design, which is contrary to the aim of ADA.
In this paper, we resolve the aforementioned issues of discretization by constructing a continuous and differentiable mapping φ which maps an augmentation vector α ∈ Rd to an image transformation function tα ∈ T that transforms an image I to tα(I). Speciﬁcally, we group basic augmentations into three categories: Color Adjustment that adjusts the color values, Image Filtering like blurring or sharpening, and Image Warping which represents the geometric transformation. We then split the augmentation vector α to three sub-vectors (αCA, αIF and αIW), each of which would map to a speciﬁc transformation (tαCA, tαIF or tαIW ). We ﬁnish the construction by regard tα as the composite transformation, i.e., tα = tαIW ◦ tαIF ◦ tαCA, as visualized in Fig. 1. Based on this construction, we pose ADA as learning an augmentation distribution in the continuous augmentation space. This gives us the freedom to search over the entire continuous space in order to obtain the optimal augmentations. For optimization and inference, we use the well-known Stochastic Gradient Langevin
Dynamics (SGLD) and propose a pseudo one-step approximation that allows us to efﬁciently learn the underlying augmentation distribution and sample augmentations from it. On a wide range of experiments involving state-of-the-art architectures on multiple image classiﬁcation benchmarks, we show that our approach not only provides results at par with existing ADA methods, it also is orders of magnitude faster than them.
Our main contributions can be summarized as: 1. We propose a continuous augmentation mapping and ﬁrst pose ADA as a continuous optimization problem, thus, avoiding the pitfalls of the discrete augmentation space. 2. We develop a gradient-based approach to learn the augmentation distribution in the continu-ous space that allows for highly efﬁcient sampling of augmentations, offering a favorable speed-performance trade-off. 2