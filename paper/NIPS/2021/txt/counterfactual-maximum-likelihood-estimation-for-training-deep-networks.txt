Abstract
Although deep learning models have driven state-of-the-art performance on a wide array of tasks, they are prone to spurious correlations that should not be learned as predictive clues. To mitigate this problem, we propose a causality-based training framework to reduce the spurious correlations caused by observed confounders. We give theoretical analysis on the underlying general Structural Causal Model (SCM) and propose to perform Maximum Likelihood Estimation (MLE) on the interven-tional distribution instead of the observational distribution, namely Counterfactual
Maximum Likelihood Estimation (CMLE). As the interventional distribution, in general, is hidden from the observational data, we then derive two different upper bounds of the expected negative log-likelihood and propose two general algorithms,
Implicit CMLE and Explicit CMLE, for causal predictions of deep learning models using observational data. We conduct experiments on both simulated data and two real-world tasks: Natural Language Inference (NLI) and Image Captioning. The results show that CMLE methods outperform the regular MLE method in terms of out-of-domain generalization performance and reducing spurious correlations, while maintaining comparable performance on the regular evaluations.1 1

Introduction
Deep neural networks have been tremendously successful across a variety of tasks and domains in recent years. However, studies have shown that deep learning models trained with traditional supervised learning framework tend to learn spurious correlations as predictive clues [1–4]. For example, in computer vision, deep learning models can rely on surface-level textures [1, 5] or background environment [2, 6] instead of the object of interest in images. In natural language processing (NLP), question-answering models are insensitive to the choice of question [7] and natural language inference models are surprisingly accurate at predicting the logical relationship between a pair of sentences from just one of them [3]. In image captioning, a phenomenon called object hallucination is observed where models tend to include nonexistent objects in a caption based on their common association with other objects that are actually present in the image [4].
Another related problem in the current associative learning framework, like maximum likelihood estimation (MLE), is that neural networks can achieve almost perfect performance on one dataset but dramatically fail to generalize on another because of a naturally occurring or adversarially enforced distribution shift [8–11]. One explanation is that the model learns ‘fake’ features induced by spurious correlations instead of invariant features that would hold in different domains for the same task [12]; this kind of invariance can be explained by the underlying causal mechanism of the task [13]. 1Our code is released at https://github.com/WANGXinyiLinda/CMLE. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
It is usually impractical to identify and disentangle all of the potential sources of spurious correlations.
Various training algorithms have been proposed to reduce the inﬂuence of spurious correlations by learning the underlying causal mechanism. One approach for discovering the underlying causal mechanism is by invariant prediction based on the invariance of causal mechanisms across different environments [13, 12, 14]. However, constructing such diverse environments is usually unrealistic.
Another approach is counterfactual data augmentation [15–19], which directly modiﬁes the part of the input that causes the target variable, but usually involves expensive human efforts in generating the counterfactual examples. We propose Counterfactual Maximum Likelihood Estimation (CMLE), which tries to perform MLE on an interventional distribution instead of the observational distribu-tion. Here, obervational distribution means the distribution that the train data is sampled from.
Interventional distribution means the distribution with one or some of the variables intentionally intervened (set to some ﬁxed value). More speciﬁcally, we aim to remove observable confounders and reduce spurious correlations by directly intervening on the label variable, without requiring human annotators or the curation of diverse environments.
In this paper, we focus on the setting of predicting the outcome Y of a certain action T on X, with the underlying causal model as X → Y ← T and an observed train dataset in the form of (X, Y, T ).
Such a setting enables us to assume that there exist some observed confounders in X that inﬂuence both Y and T . This means there is potentially a false causal relation from X to T , indicated by a red arrow in Figure 1a. In our framework, instead of trying to identify all the confounders, we try to rule out the effect of the confounders by considering the interventional distribution that directly deleting the false causal edge from X to T with a resulting causal graph as shown in Figure 1d. This causal graph can be viewed as a Randomized Controlled Trial (RCT), which is the gold standard approach for estimating the treatment effect [20].
Our method is inspired by individual treatment effect (ITE) prediction [21, 22], which tries to predict the expected effect E[Y1 − Y0|X = x] (e.g. differnce in blood pressure) of a treatment T ∈ {0, 1} (e.g. drug/surgery) on a individual unit X = x (e.g. a patient). In this case, the observed confounder in X can be explained by selection bias [22], which means the treatment T applied to an individual
X is dependent on X. For example, young patients are more likely to be treated by surgery, while elder patients are more likely to be treated by drugs.
There are two broad kinds of tasks to which our proposed CMLE framework can apply: relationship prediction tasks that predicts the relation T given two inputs X and Y (e.g. natural language inference
[23, 24], paraphrase identiﬁcation [25, 26], natural language for visual reasoning [27], etc.), and conditional generation tasks that generate Y given precondition X and constraint T (e.g. style transfer
[28, 29], controllable image/text generation [30–32], controllable image captioning [33, 34] etc.). For conditional generation tasks, the causal relation of X, Y, T ﬁts in our setting. For relation prediction tasks, we ﬁrst assume there is an underlying conditional generation process of Y for a given pair of X and T . This is a natural assumption as this is usually how the datasets for this type of task is generated. Then we augment the original dataset using the counterfactual examples generated by our method. In this paper, we try to reduce the spurious correlations contained in both kinds of tasks.
Since we generally cannot observe the interventional distribution, we derive two different upper bounds of the expected interventional negative log-likelihood using only the observational distribution: one is Implicit CMLE, as it does not involve any explicit generation of counterfactual examples; while the other is Explicit CMLE, as it explicitly generates counterfactual examples during the training process. We test our framework with deep neural networks on a simulated dataset and two real-world tasks with well-known spurious correlations: natural language inference [23] and image captioning [35]. Compared to regular MLE, we improve the out-of-domain accuracy of NLI on a hard adversarial dataset by 2.9% relatively and beat baselines on human preference evaluations by more than 10%, while maintaining comparable performance on automated evaluations. Our results show that our learning framework can better capture the underlying causal mechanism and reduce spurious correlations without degrading performance. 2