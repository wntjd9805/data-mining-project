Abstract
Direct policy search serves as one of the workhorses in modern reinforcement learning (RL), and its applications in continuous control tasks have recently at-tracted increasing attention. In this work, we investigate the convergence theory of policy gradient (PG) methods for learning the linear risk-sensitive and robust controller. In particular, we develop PG methods that can be implemented in a derivative-free fashion by sampling system trajectories, and establish both global convergence and sample complexity results in the solutions of two fundamental settings in risk-sensitive and robust control: the ﬁnite-horizon linear exponential quadratic Gaussian, and the ﬁnite-horizon linear-quadratic disturbance attenuation problems. As a by-product, our results also provide the ﬁrst sample complexity for the global convergence of PG methods on solving zero-sum linear-quadratic dynamic games, a nonconvex-nonconcave minimax optimization problem that serves as a baseline setting in multi-agent reinforcement learning (MARL) with continuous spaces. One feature of our algorithms is that during the learning phase, a certain level of robustness/risk-sensitivity of the controller is preserved, which we termed as the implicit regularization property, and is an essential requirement in safety-critical control systems. 1

Introduction
Recent years have witnessed the rapid development of reinforcement learning (RL) methods in handling continuous control tasks [1, 2, 3]. Central to the success of RL are policy optimization (PO) methods, including policy gradient (PG) [4, 5, 6], actor-critic [7, 8], and other variants [9, 10].
Progress reported in the literature has clearly shown an increasing interest in understanding theoretical properties of PO methods for relatively simple baselines such as linear control problems [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]. However, the theory of model-free PO methods on risk-sensitive/robust control remains underdeveloped in the literature. Since risk-sensitivity and robustness are important issues for designing safety-critical systems, it is natural to bring up the questions of whether and how model-free PO methods would converge for these continuous control tasks.
Our work in this paper is motivated by the above concern, and studies the sample complexity of model-free PG methods on two important baseline problems in risk-sensitive/robust control, namely the linear exponential quadratic Gaussian (LEQG), and the linear quadratic (LQ) disturbance attenuation problems. The former covers a fundamental setting in risk-sensitive control, and the latter is an important baseline for robust control. Based on the well-known equivalence between these problems and LQ dynamic games [22, 23, 24, 25, 26], we develop a uniﬁed PO perspective
\Equal contribution. K. Zhang is with the LIDS and the CSAIL at the Massachusetts Institute of Technology.
Email: kaiqing@mit.edu. X. Zhang, B. Hu, and T. Ba¸sar are with the Department of ECE and the CSL at the
University of Illinois at Urbana-Champaign. Emails: {xz7, binhu7, basar1}@illinois.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
for both. A common feature for the above two problems is that their optimization landscapes are by nature more challenging than that of the linear quadratic regulator (LQR) problem, and existing proof techniques for model-free PG methods [11, 17, 18, 19] are no longer effective due to lack of coercivity of the objective functions. Speciﬁcally, when applying PG methods to LQR, the feasible set for the resultant constrained optimization problem is the set of all linear state-feedback controllers that stabilize the closed-loop dynamics. The objective function of LQR is coercive on this feasible set and serves as a barrier function itself [14], guaranteeing for the PG iterates to stay in the feasible set and converge to the globally optimal controller. For the LEQG and LQ disturbance attenuation problems, the risk-sensitivity/robustness conditions have reshaped the feasible set in a way that the objective function becomes non-coercive, i.e. the objective value can remain ﬁnite while approaching the boundary. The objective is no longer a barrier function, and new proof techniques are needed to show that model-free PG iterates will stay in the feasible set. This is signiﬁcant for safety-critical control systems with model uncertainty, as the iterates’ feasibility here is equivalent to risk-sensitivity/robustness of the controller (cf. Remark A.8), and the failure to preserve robustness during learning can cause catastrophic effects, e.g., destabilizing the system in face of disturbances.
H2/
The most relevant result was developed in [16], which proposes implicit regularization (IR) arguments to show the convergence of several PG methods on the mixed control design problem (which can be viewed as the inﬁnite-horizon variant of the LQ disturbance attenuation problem studied in this paper). The main ﬁnding there is that two speciﬁc PG search directions (with perfect model information) are automatically biased towards the interior of the robustness-related feasible set. In
[16], it is emphasized that IR is a feature of both the problem and the algorithm, contrasting to that the stability-preserving nature of PG methods for LQR problems is based on the barrier function property of the objective and hence is algorithm-agnostic. Although the idea of IR is relevant to PO problems with non-coercive objective functions, the arguments in [16] only apply to the setting with a known model, since they rely on a specialized perturbation technique which may potentially generate arbitrarily small “margins". However, in the model-free setting, a uniform margin is required for provable tolerance of statistical errors. See a detailed comparison of the literature in §A.1.
H1
In this paper, for the LEQG and LQ disturbance attenuation problems, we overcome the above margin issue and obtain the ﬁrst IR result in the model-free setting. This enables the ﬁrst model-free PG method that provably solves these control problems with a ﬁnite number of samples. We highlight our contributions as follows.
Contributions. We provide the ﬁrst sample complexity results for model-free PG methods for solving linear control problems with risk-sensitivity/robustness concerns (the LEQG and LQ distur-bance attenuation problems), which were viewed as important open problems in the seminal work
[11]. From the robust control perspective, one feature of our algorithms is that, during the learning process, a certain level of robustness/risk-sensitivity of the controller is proved to be preserved. This has generalized the results in [16] with a known model, and has thus enabled the ﬁnite-sample convergence guarantees of PG methods for risk-sensitive/robust control design. Our algorithms and sample complexity results also address two-player zero-sum LQ dynamic games in the ﬁnite-horizon time-varying setting, which are among the ﬁrst sample complexity results for the global convergence of policy-based methods for competitive multi-agent RL. Second, in the context of minimax opti-mization, our results address a class of nonconvex-nonconcave minimax constrained optimization problems, using zeroth-order multi-step gradient descent-ascent methods. Finally, part of our results provide the sample complexity analysis for PG methods that solve the ﬁnite-horizon time-varying
LQR problem with system noises and a possibly indeﬁnite state-weighting matrix. 2