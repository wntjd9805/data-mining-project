Abstract
Humans are good at compositional zero-shot reasoning; someone who has never seen a zebra before could nevertheless recognize one when we tell them it looks like a horse with black and white stripes. Machine learning systems, on the other hand, usually leverage spurious correlations in the training data, and while such correlations can help recognize objects in context, they hurt generalization. To be able to deal with underspeciﬁed datasets while still leveraging contextual clues during classiﬁcation, we propose ProtoProp, a novel prototype propagation graph method. First we learn prototypical representations of objects (e.g., zebra) that are independent w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate the independent prototypes through a compositional graph, to learn compositional prototypes of novel attribute-object combinations that reﬂect the dependencies of the target distribution. The method does not rely on any external data, such as class hierarchy graphs or pretrained word embeddings. We evaluate our approach on AO-Clevr, a synthetic and strongly visual dataset with clean labels,
UT-Zappos, a noisy real-world dataset of ﬁne-grained shoe types, and C-GQA, a large-scale object detection dataset modiﬁed for compositional zero-shot learning.
We show that in the generalized compositional zero-shot setting we outperform state-of-the-art results, and through ablations we show the importance of each part of the method and their contribution to the ﬁnal results. The code is available on github1. 1

Introduction
As humans, hearing the phrase ‘a tiny pink penguin reading a book’ can conjure up a vivid image, even though we have likely never seen such a creature before. This is because humans can compose their knowledge of a small number of visual primitives to recognize novel concepts [1], a property which Lake et al. [2] argue is one of the key building blocks for human intelligence missing in current artiﬁcial intelligence systems. Machines, on the other hand, are largely data-driven, and usually require many labeled examples from various viewpoints and lighting conditions in order to recognize novel concepts. Since visual concepts follow a long-tailed distribution [3, 4], such an approach makes it near impossible to gather sufﬁcient examples for all possible concepts. Compounded by that, in the absence of sufﬁcient data, vanilla convolutional neural networks will use any correlation they can
ﬁnd to classify training samples, even when they are spurious [5]. In this work, we aim to tackle both of these issues.
Compositional Zero-Shot Learning (CZSL) [6] is the problem of learning to model novel objects and their attributes as a composition of visual primitives. Previous works in CZSL [6, 7, 8] largely ignore the dependencies between classes with shared visual primitives, and the spurious correlations between 1https://github.com/FrankRuis/protoprop 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: ProtoProp A sketch of our proposed method; we learn independent prototypical repre-sentations of visual primitives in the form of objects (e.g., horse) and attributes (e.g., stripes). The prototypes are then propagated through a compositional graph, where they are combined into novel compositional prototypes to recognize both seen and unseen classes (e.g., zebra). attributes and objects. More recently, Atzmon et al. [9] tackle the latter by ensuring conditional independence between attribute and object representations, while Naeem et al. [10] explicitly promote dependencies between the primitives and their compositions. While the independence approach improves generalization, it hurts accuracy on seen classes by removing useful correlations. The explicit dependencies, on the other hand, can share these useful correlations with unseen classes, but there will always be some that are spurious, hurting generalization.
In this work, we propose to take advantage of the strengths of both approaches, respectively, by learning independent visual representations of objects and attributes, and by learning their compo-sitions for the target classes. First, we represent visual primitives by learning local independent prototypical representations. Prototype networks [11] learn an embedding function, where inputs of the same class cluster around one prototypical representation of that class. Here we adopt such a function for learning prototypes of objects and attributes. Next, we leverage a compositional graph to learn the dependencies between the independent prototypes on the one hand, and the desired seen and unseen classes on the other, by propagating the prototypes to compositional nodes. Here, the compositional graph allows some information to be shared between objects that share attributes, e.g., between tigers and zebras. Sylvain et al. [12] show the importance of locality and compositionality for model generalization in zero-shot learning. They also propose a measure for the compositionality of a representation, which is equivalent to our compositional loss function. The proposed method,
ProtoProp, is outlined in Figure 1.
Our main contributions are: 1) We propose a novel graph propagation method that learns to combine local, independent, attribute and object prototypes into one compositional prototype that can accurately detect unseen compositional classes. 2) A spatial attention-based pooling method that allows us to obtain differentiable attribute and object patches for use in an independence loss function. 3) Our method effectively deals with bias from an underspeciﬁed dataset by learning independent representations that then take on the dependencies of the desired target distribution. 4)
We validate through ablations the importance of each part of the method (local prototypes vs semantic embeddings, independence loss, backbone ﬁnetuning) and their contribution to the ﬁnal results. 5)
We show that we improve on state-of-the-art results on three challenging compositional zero-shot learning benchmarks: 2.5 to 20.2% harmonic mean improvement on AO-Clevr [9], 3.1% harmonic mean improvement on UT-Zappos [13], and a slight improvement on C-GQA compared to the best existing method. 2
2