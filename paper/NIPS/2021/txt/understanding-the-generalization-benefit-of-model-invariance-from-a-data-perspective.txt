Abstract
Machine learning models that are developed to be invariant under certain types of data transformations have shown improved generalization in practice. However, a principled understanding of why invariance beneﬁts generalization is limited.
Given a dataset, there is often no principled way to select “suitable” data transfor-mations under which model invariance guarantees better generalization. This paper studies the generalization beneﬁt of model invariance by introducing the sample cover induced by transformations, i.e., a representative subset of a dataset that can approximately recover the whole dataset using transformations. For any data transformations, we provide reﬁned generalization bounds for invariant models based on the sample cover. We also characterize the "suitability” of a set of data transformations by the sample covering number induced by transformations, i.e., the smallest size of its induced sample covers. We show that we may tighten the generalization bounds for “suitable” transformations that have a small sample cover-ing number. In addition, our proposed sample covering number can be empirically evaluated and thus provides a guidance for selecting transformations to develop model invariance for better generalization. In experiments on multiple datasets, we evaluate sample covering numbers for some commonly used transformations and show that the smaller sample covering number for a set of transformations (e.g., the 3D-view transformation) indicates a smaller gap between the test and training error for invariant models, which veriﬁes our propositions. 1

Introduction
Invariance is ubiquitous in many real-world problems. For instance, categorical classiﬁcation of visual objects is invariant to slight viewpoint changes [18, 2, 23], text understanding is invariant to synonymous substitution and minor typos [53, 36, 27]. Intuitively, models capturing the underlying invariance exhibit improved generalization in practice [21, 13, 50, 14, 45, 14]. Such generalization beneﬁt is especially crucial when the data are scarce as in some medical tasks [46], or when the task requires more data than usual as in cases of distribution shift [38] and adversarial attack [40, 49, 5].
A commonly accepted intuition attributes the generalization beneﬁt of model invariance to the reduced model complexity, especially the reduced sensitivity to spurious features. However, a principled understanding of why model invariance helps generalization remains elusive, thus leaving many open questions. Since model invariance may come at a cost (e.g., compromised accuracy, increase computational overhead), given a task, how should we choose among various data transformations under which model invariance guarantees better generalization? If existing data transformations are not good enough for a given task, what is the guiding principle to ﬁnd new ones? The lack of a principled understanding limits better exploitation of model invariance to further improve
* Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) G = ∅ (b) G = {"rotation"}
Figure 1: Illustration of the pseudometric and sample cover induced by data transformations. generalization. In addition, since identifying instructive generalization bound is a central topic in machine learning, we may expect to tighten existing generalization bounds by additionally considering the data-dependent model invariance property.
The many faces of data transformations and model classes pose signiﬁcant challenges to a principled understanding of model invariance’s generalization beneﬁt. To address this, [2, 4, 34, 42, 39] characterize the input space and show that certain data transformations equivalently shrink the input space for invariant models, which then simplify the input and improves generalization. From another perspective, [19, 30] directly characterize the function space and show that the volume of the invariant model class is reduced, which then simpliﬁes the learning problem and improves generalization. These understandings provide valuable insights, yet they may become less informative on high-dimensional input data or require the model invariance to be obtained exclusively via feature averaging. Some certain assumptions on data transformations (e.g., ﬁniteness, group structure with certain measure) also make these understandings less applicable to more general data transformations.
In this paper, we derive generalization bounds for invariant models based on the sample cover induced by data transformations and empirically show that the introduced notion can guide the data transformations selection. Different from previous understandings, we ﬁrst identify a data-dependent property of data transformations in a model-agnostic way, and then establish its connections with the reﬁned generalization bounds of invariant models. The analysis applies to more general data transformations regardless of how the model invariance is obtained and naturally provides a model-agnostic guidance for data transformations selection. We summarize our contributions as follows.
Contributions. At the core of our understanding is the notion of sample cover induced by data transformations, deﬁned informally as a representative subset of a dataset that can approximately recover the whole dataset using data transformations (illustrated in Figure 1). We show that this notion identiﬁes a data-dependent property of data transformations which is related to the generalization beneﬁt of the corresponding invariant models. Under a special setting of the sample cover, we ﬁrst bound the model complexity of any invariant and output-bounded model class in terms of the sample covering numbers. Since this general bound requires a restrictive condition on data transformations in order to be informative, we then assume the model Lipschitzness to relax the requirement and reﬁne the model complexity bound for invariant models. Finally, we outline a framework for model-invariance-sensitive generalization bounds based on the invariant models’ complexities, and use it to discuss the generalization beneﬁt of model invariance.
Given the usefulness of sample cover in the analysis, we propose an algorithm to empirically estimate the sample cover. This algorithm exactly veriﬁes whether a given subset of a sample forms a valid sample cover, and always estimates a sample covering number that upper-bounds the ground-truth. Inspired by our analysis, we also propose to use the sample covering number as a suitability measurement for practical data transformation selections. This measurement is data-driven, widely applicable, and empirically correlates with invariant models’ actual generalization performance. We discuss its limitations and the empirical mitigation. 2
To empirically verify our propositions, we ﬁrst estimate the sample covering number for some commonly used data transformations on four image datasets, including CIFAR-10 and ShapeNet (a 3D dataset). Under typical settings, the 3D-view transformation induces a much smaller sample covering number than others on ShapeNet, while cropping induces the smallest sample covering number on others datasets. For those data transformations, we then train invariant models via data augmentation and invariance loss regularization to evaluate the actual generalization beneﬁt. Results show a clear correlation between smaller sample covering numbers induced by data transformations and the better generalization beneﬁt enjoyed by invariant models. 2 Preliminaries
Data transformations. We refer to the data transformation as a function from the input space
X → X , and data transformations as a set of such functions. Unless otherwise speciﬁed, we do not assume data transformations to have group structures since many non-invertible transformations (e.g., cropping) do not ﬁt into a group structure directly. For a set of data transformations G = {g : X → X } and a data point (also referred to as an example) x ∈ X , we overload the notion of orbit in group theory and denote by G(x) the orbit of x deﬁned as follows. The orbit of x generated by data transformations G is the collection of outputs after applying any transformation g ∈ G on x:
G(x) = {g(x) ∈ X : g ∈ G}.
Model invariance. Let D be the underlying data distribution and supp(D) be its support. A model h : X → Y is said to be invariant under data transformations G on D if h(g(x)) = h(x) for any x ∈ supp(D) and any g ∈ G. We refer to a class of invariant models as the G-invariant model class.
Complexity measurements. Covering number and Rademacher complexity [33] are two commonly used complexity measurements for model classes (including neural networks [6]) that can provide uniform generalization bounds. The covering number can also be directly used to upper bound the
Rademacher complexity via Dudley’s entropy integral theorem [17, 32].
Covering number. Let (F, d) be a (pseudo)metric space with some (pseudo)metric1 d. An (cid:15)-cover of a set H ⊆ F is deﬁned as a subset (cid:98)H ⊆ H such that for any h ∈ H, there exists (cid:98)h ∈ (cid:98)H such that d(h, (cid:98)h) ≤ (cid:15). The covering number N ((cid:15), H, d) is deﬁned as the minimum cardinality of an (cid:15)-cover (among all (cid:15)-covers) of H. In this paper, we use the concept of covering number both for measuring model class complexities and for deﬁning the sample covering number on datasets.
Empirical Rademacher complexity. Let H be a class of functions h : X → R. Given a sample
S = {xi}n i=1, the empirical Rademacher complexity of model class H is deﬁned as: RS (H) = (cid:2)suph∈H i=1 σih(xi)(cid:3) where σ = [σ1, ..., σn](cid:62) is the vector of i.i.d. Rademacher random
Eσ variables, each uniformly chosen from {−1, 1}. (cid:80)n 1 n
Generalization error and gap. Let S = {xi}n from some data distribution D, and H be a model class. Given a loss function (cid:96) : R → [0, 1], for a h ∈ H, we deﬁne the empirical error as RS (h) = 1 i=1 (cid:96)(h(xi), yi), the generalization error as n
R(h) = E(x,y)∼D[(cid:96)(h(x), y)], and the generalization gap as R(h) − RS (h). i=1 be a sample drawn i.i.d. (cid:80)n 3 Generalization Beneﬁt of Model Invariance
In this section, we derive the generalization bounds for invariant models by identifying the model invariance properties. We start by introducing the notion of sample cover induced by data transfor-mations and based on it bound the Rademacher complexity of any invariant models with bounded output (Section 3.1). Then, we assume model Lipschitzness to provide a more informative model complexity bound for any data transformations (Section 3.2). Finally, we provide a framework for model-invariance-sensitive generalization bounds and discuss the generalization beneﬁt of model invariance (Section 3.3). 1A pseudometric is a metric if and only if it separates distinct points, namely d(x, y) > 0 for any x (cid:54)= y. 3
3.1 Sample Cover Induced by Data Transformations
Existing empirical results suggest that, compared with standard models, invariant models may have certain properties reducing their effective model complexities. To identify such properties, we alternatively identify the related properties of the corresponding data transformations via the notion of sample cover induced by data transformations. We now formalize the introduced notion.
The deﬁnition of sample cover relies on the pseudometric induced by the data transformations G.
Note that G generates an orbit G(x) ⊆ X for each example x ∈ S. Let (cid:107) · (cid:107) be any norm on the input space X . Given a set of transformations G, we deﬁne the G-induced pseudometric2 as
ρG(x1, x2) = inf
γ∈Γ(x1,x2) (cid:90)
γ c(r)ds, where c(r) = (cid:26) 0, 1, if r ∈ ∪x∈S G(x) otherwise (3.1) where ds = (cid:107)dr(cid:107), and Γ denotes the set of all paths (curves) in X from x1 to x2. The ρG is essentially calculating the line integral along the shortest (if achievable) path γ in the scalar ﬁeld c, where c can also be viewed as the "moving cost" function depending on G. The norm (cid:107) · (cid:107) here can be selected as any meaningful norm on the input space (e.g., Euclidean norm as in our experiments) and will later be used in deﬁning the model’s Lipschitz constant. It can be checked that ρG satisﬁes pseudometric axioms.
Deﬁnition 3.1 (Sample cover induced by data transformations). Let (X , ρG) be a pseudometric space and S = {xi}n i=1 be a sample of size n. An (cid:15)-sample cover (cid:98)SG,(cid:15) of the sample S induced by data transformations G at resolution (cid:15) is deﬁned as a subset of the sample S such that for any x ∈ S, there exists (cid:98)x ∈ (cid:98)SG,(cid:15) satisfying ρG(x, (cid:98)x) ≤ (cid:15).
Deﬁnition 3.2 (Sample covering number induced by data transformations). The sample covering number N ((cid:15), S, ρG) induced by data transformations G is deﬁned as the minimum cardinality of an (cid:15)-sample cover:
N ((cid:15), S, ρG) = min{| (cid:98)SG,(cid:15)| : (cid:98)SG,(cid:15) is an (cid:15)-sample cover of S}. (3.2)
Informally, the G-induced sample cover speciﬁes a representative subset of examples which can approximately recover all the original examples using the given data transformations G. This notion is closely related to the sample compression [20] which represents a scheme to prove the learnability of concepts through a compressed set of sample. While identifying the generalization-related properties of data transformations, this notion is insensitive to other unrelated properties (e.g., ﬁniteness, group structures) and thus applies to any data transformations.
The intuition behind sample cover is that G-invariant models may have consistent behaviors on an example and its associated approximation in the G-induced sample cover. As such, we can analyze the model complexities of invariant models by considering the models’ behaviour only on the potentially small-sized sample covers. Indeed, we directly have the following model complexity result. The proof is in Appendix B.
Proposition 3.3. Let S = {xi}n i=1 be a sample of size n. Let H be a model class mapping from X to
[−B, B] for some B > 0 and is invariant to data transformations G. Then the empirical Rademacher complexity of H satisfy
RS (H) ≤ 24B (cid:114)
N (0, S, ρG) n
. (3.3)
Proposition 3.3 generally bounds the model complexity of any output-bounded and G-invariant model class in terms of the sample covering number N (0, S, ρG) induced by G. A small G-induced sample covering number at resolution (cid:15) = 0 thus tightens the model complexity bound for a general class of
G-invariant models.
Note, however, that Proposition 3.3 is informative only when the data transformations G yields
N (0, S, ρG) (cid:28) n on the sample S — a condition requiring G to be able to exactly recover S from a small-sized subset of S. This condition is unfortunately too strict to hold for many commonly used 2Note that ρG is not a metric since it allows ρG(x, y) = 0 for x (cid:54)= y. 4
data transformations which only generate orbits with measure zero (with respect to the data measure) at most data points. For example, the rotation transformations on CIFAR-10 do not satisfy this condition, since no two images in CIFAR-10 are rotated versions of each other. To better understand the generalization beneﬁt brought by any data transformations (e.g., rotation), we further assume speciﬁc model properties which equivalently expand the orbits in order to get more general results.
We study Lipschitz models in Section 3.2, and relegate a sharper (and relatively independent) analysis for linear models under linear data transformations to Appendix C. 3.2 Reﬁned Complexity Analysis of Lipschitz Models
This subsection reﬁnes the model complexity analysis for Lipschitz models that are invariant. Char-acterizing the Lipschitz constant of models has been the focus of a line of work. For example, the
Lipschitz constant of ReLU networks can be upper-bounded by the product of the spectral norms of the weight matrices, considering the worst-case inputs [6, 22]. Assuming Lipschitzness, the following theorem reﬁnes the covering number analysis for invariant models. The proof is in Appendix B.
Theorem 3.4. Let S = {xi}n i=1 be a sample of size n. Let H be a model class such that every h ∈ H is κ-Lipschitz with respect to (cid:107) · (cid:107) (used in deﬁning the sample cover) and is invariant to G.
Then the covering number of H satisﬁes
N (cid:0)τ, H, L2(PS )(cid:1) ≤ inf (cid:15)≥0, (cid:98)SG,(cid:15) (cid:115)
N (cid:0)τ − κ(cid:15) 1 −
| (cid:98)SG,(cid:15)| n
, H, L2(P (cid:98)SG,(cid:15)
)(cid:1), (3.4) where ∀h, g ∈ H, the L2(PS ) metric is deﬁned as (cid:107)h − g(cid:107)L2(PS ) = (cid:16)(cid:80) x∈S (cid:0)h(x) − g(x)(cid:1)2(cid:17) 1 2
, 1 n and the L2(P (cid:98)SG,(cid:15)
) metric is deﬁned as3 (cid:107)h − g(cid:107)L2(P
) = (cid:98)SG,(cid:15) (cid:16)(cid:80) x∈ (cid:98)SG,(cid:15) (cid:0)h(x) − g(x)(cid:1)2(cid:17) 1 2
. p(x) n
Theorem 3.4 upper-bounds the covering number of H evaluated at the sample S by the new covering number evaluated at any sample cover (cid:98)SG,(cid:15), under a modiﬁed metric and at the cost of an additional error term depending on (cid:15) and κ. The equality trivially holds by taking (cid:98)SG,(cid:15) = S, while by searching over all sample covers with different resolution (cid:15) it is possible to tighten the covering number bound for invariant models. Additionally, Theorem 3.4 leads to a reﬁned version of Dudley’s entropy integral theorem (see Lemma B.1) that bounds the Rademacher complexity of invariant models.
Theorem 3.4 suggests that we may improve existing covering-number-based model complexity analysis by weakening the dependence on input dimensions. Note that covering numbers that do not yield N (cid:0)τ, H, L2(PS )(cid:1)/n → 0 as n → ∞ are vacuous. Therefore, existing covering number results typically avoid linear dependence on n at the cost of (explicitly or implicitly) increased dependence on the input dimension [52]. With the reﬁned result in Theorem 3.4, however, a covering number linear in n can now be replaced by one that is linear in a potentially much smaller sample covering number
N ((cid:15), S, ρG) and consequently become informative, thus circumvent the increased dependence on input dimensions. An interesting direction for future work is to instantiate the result in Equation 3.4 for speciﬁc model classes to get more interpretable results. 3.3 Framework for Model-invariance-sensitive Generalization Bounds
This subsection presents the framework for generalization bounds sensitive to the model invariance.
While the results are straightforward applications of the derived complexities of invariant models, our goal is to justify the selection of suitable data transformations to maximize the generalization beneﬁt.
We start with the generalization analysis of invariant models and then present the framework.
Generalization beneﬁt for invariant models. The generalization bounds of invariant models follow immediately by applying the Rademacher model complexities (Proposition 3.3, Proposition B.1, and
Theorem C.1) to the standard generalization bound (Theorem A.2). Compared with standard models, invariant models’ tightened model complexity bounds already imply their reduced generalization gaps, 3The term p(x)/n can be viewed as the probability mass at x where the numerator indicates the number of examples that x covers. See Appendix B.1 for the formal deﬁnition of p(x). 5
whereas for reduced generalization error they further need to have low empirical error. Since enforcing the model invariance may simultaneously increase the empirical error, we can use standard model selection techniques (e.g., structural risk minimization [33]) to select suitable data transformations and control the trade-off.
Model-invariance-sensitive generalization bound. We outline the generalization bound that identi-ﬁes model invariance properties based on the derived invariant models’ complexities. It follows by the post-hoc analysis which speciﬁes a proper set of invariant models using the "invariant loss" — the loss when composed with any model, makes the composition invariant. For data transformations with group structures, we can construct such loss by averaging (assuming Haar measure) or adversarially perturbing any given loss over the orbits of input examples [30, 19]. Speciﬁcally, the adversarial loss with respect to data transformations G is deﬁned as (cid:101)(cid:96)G (h(x), y) = maxx(cid:48)∈G(x) (cid:96) (h(x(cid:48)), y), where (cid:96) is any given loss. Using the adversarial loss, the following proposition provides the model-invariance-dependent generalization bound by applying the model selection framework [33]. Appendix B.3 further describes a binary coding construction of combinations of data transformation classes.
Proposition 3.5. Let S = {xi}n i=1 be a sample of size n. Let H be any given model class and (cid:96) be any given loss. Suppose we have K sets of group-structured data transformations {G1, G2, ..., GK}.
Then with probability at least 1 − δ, the following generalization bound holds for any h ∈ H and any k ∈ [K]:
R(h) ≤ 1 n n (cid:88) i=1 (cid:101)(cid:96)Gk (h(xi), yi) + 4RS ((cid:101)(cid:96)Gk ◦ H) + (cid:115) (cid:114) log k n
+ 3 log 4
δ 2n
, (3.5) where RS ((cid:101)(cid:96)Gk ◦H) is upper-bounded by the complexity of Gk-invariant models. For any model trained on S, Proposition 3.5 shows that we can optimize over all selections of data transformations to improve its generalization bound. Note that the selection of Gk is subject to a potential trade-off between the reduced model complexity RS ((cid:101)(cid:96)Gk ◦ H) and the increased empirical error (cid:80)n i=1 (cid:101)(cid:96)Gk (h(xi), yi).
Thus, if a suitable Gk reduces the model complexity while keeping the empirical error low, then the trained model will beneﬁt from a tightened generalization bound. This generalization bound does not require the models to be (strictly) invariant and potentially explains the improved generalization of models with trained invariance (e.g., via data augmentation [43, 41] or consistency regularization
[31, 47]). The difﬁculty in instantiating Proposition 3.5 is that the model complexity with adversarial loss may be hard to compute for general data transformations. Therefore, we discuss more practical data transformations selections based on the sample covering numbers in Section 5. 4 Sample Cover Estimation Algorithm
The sample cover induced by data transformations plays a central role in our understanding of model invariance. Despite the usefulness in the analysis, exactly computing the sample cover turns out to be non-trivial in general. Indeed, computing the transformation-induced metrics can be difﬁcult for continuous data transformations, and ﬁnding the smallest sample cover is NP-hard. To address this problem, we propose an algorithm to estimate the sample covering number and ﬁnd the associated sample cover. We outline the algorithm and discuss the algorithmic challenges in this section. The algorithmic details appear in Appendix D.
Setup. The estimation algorithm takes as input a sample S, a set of data transformations G, and the resolution parameter (cid:15). It then returns the estimated sample covering number N ((cid:15), S, ρG) and the associated sample cover (cid:98)SG,(cid:15). The estimation algorithm has the following steps.
Step 1. Compute (or approximate) the direct orbit distance between any two examples in S. The direct orbit distance between any two examples xi, xj ∈ S is dG(xi, xj) = (cid:107)G(xi) − G(xj)(cid:107) = min g1,g2∈G (cid:107)g1(xi) − g2(xj)(cid:107), which can be exactly computed for ﬁnite transformations (e.g., ﬂipping) with complexity O(|G|2)), or can be approximated for continuous transformations (e.g., rotation) via optimization or sampling. 6
Step 2. Compute the ρG distance between any two examples in S. Given results in step 1, computing the ρG distance between any two examples can be formulated as a shortest path problem on a complete graph, where each node represents an example and the cost of each edge is the direct orbit distance computed in step 1 (see formulations in Appendix D). Note that the shortest path is always included in our ﬁnite candidates even though the ρG distance considers inﬁnitely many paths. This is because any other path outside our ﬁnite candidates will be longer than its counterparts (depending on what orbits it intersects) in our ﬁnite candidates. Standard shortest path algorithms solve for all pairs of examples in polynomial time (e.g., via Dijkstra’s algorithm [16] in O(n3)).
Step 3. Construct the pairwise distance matrix [ρG(xi, xj)]i,j and approximate the sample covering number. This step can be formulated as a set cover problem where each example x covers a subset of
S in which each element’s ρG distance to x is less than or equal to (cid:15). Our goal is to ﬁnd a minimum number of those subset such that their union contains S. This problem is known to be NP-hard in general but admits polynomial time approximations [24]. In experiments, we use modiﬁed k-medoids
[35] clustering method to ﬁnd the approximation of N ((cid:15), S, ρG) (see Algorithm 1).
Note that the estimated sample covering number returned by the algorithm is always an upper bound of the ground-truth, regardless of the approximation error in step 1 and 3. When step 1 is exact, the algorithm also exactly veriﬁes whether a given subset of S forms a valid sample cover. In our experiment, the step 2 becomes the computation bottleneck for large-sized sample. We leave improving the scalability as well as evaluating the approximation quality for future work. 5 Data-driven Selection of Data Transformations
The pool of candidate data transformations on a given dataset may be inﬁnitely large. To maximize the generalization beneﬁt of model invariance, we usually make selections based on expensive cross-validations due to the absence of a model-training-free guidance. Section 3 suggests that invariant models may beneﬁt from improved generalization guarantees if the corresponding data transformations induce small sample covering numbers. Therefore, we propose to use the sample covering number as an empirical suitability measurement to guide the data transformations selection.
We discuss its advantages, limitations, and empirical mitigation in this section.
Suitability measurement. To maximize the generalization beneﬁt of model invariance on a dataset
S, we measure the suitability of data transformations G by the sample covering number induced by G and favor the small ones.
Advantages. One advantage of this suitability measurement is that it is model-training-free. It provides a-priori guidance depending only on the dataset and the data transformations, thus avoids expensive cross-validations and fuels the exploration of new types of data transformations. Another advantage is that it applies to any types of data transformations (including the continuous and non-invertible ones) and provides a uniform benchmark.
Limitations and empirical mitigation. Being model-agnostic also poses two limitations to the suitability measurement. One limitation is that this suitability measurement, while capturing invariant models’ reduced generalization gap, ignores their potentially increased empirical error. Note that certain data transformations on a dataset may drastically increase invariant models’ empirical error and overturn the beneﬁt of reduced generalization gap. To mitigate this limitation, we consider two necessary conditions for maintaining low empirical error. First, the data transformations should preserve the underlying ground-truth labeling. We may use domain knowledge to meet this condition.
Second, the model class should be rich enough to contain a low-error invariant hypothesis. In our experiment, neural networks which are invariant and achieve low training error sufﬁce this condition.
Another limitation is that this suitability measurement ignores models’ potential Lipschitz constant change after enforcing the invariance. Theorem 3.4 suggests that the generalization beneﬁt enjoyed by invariant models depends on models’ Lipschitz constant and can be overturned if enforcing invariance leads to signiﬁcantly larger Lipschitz constant. To mitigate this limitation, we use the fact that we are doing classiﬁcation tasks and use the label information to heuristically offset the Lipschitz constant increase. We use the minimum inter-class distance change after applying data transformations to 7
(a) CIFAR-10 (b) ShapeNet
Figure 2: Estimated sample covering numbers induced by different data transformations at different resolutions (cid:15). “base” indicates no transformation. Note that as (cid:15) increases, it starts to exceed the
L2 distance between some images and thus some images get covered by others without doing any transformation. Three vertical dashed lines indicate the maximum resolution (cid:15) at which the “base” yields a certain sample covering number, and from left to right they are 100%n, 99%n, 95%n. capture the Lipschitz constant change and use it to normalize the sample covering number for better data transformation selections (see Appendix E.5.2). 6 Experiments
In this section, we implement the sample cover estimation algorithm and verify the effectiveness of using sample covering numbers to guide the data transformations selection. We ﬁrst estimate the sample covering number induced by different types of data transformations on common image datasets. Then, we investigate the actual generalization beneﬁt for models invariant to those data transformations and analyze the correlation4.
Datasets. We report experimental results on CIFAR-10 [29] and ShapeNet [10] in this section, and relegate results on CIFAR-100 and Restricted ImageNet to Appendix E.5.1. ShapeNet is a large-scale 3D data repository which enables us to do more complex data transformations (e.g., change of 3D-view) beyond the common 2D geometric transformations. The work [12] provides 24 multi-view pre-rendered images for each 3D object in 10 chosen categories. For convenience, we use those images to approximate the random perturbation of the 3D-view.
Data transformations. We evaluate some commonly used data transformations with typical param-eter settings which we assume to be label-preserving. We choose ﬂipping, cropping, and rotation on CIFAR-10, and additionally consider the 3D-view change on ShapeNet. We use the same data transformations with the same parameter settings during estimating the sample covering number and evaluating the generalization beneﬁt. Appendix E provides more details of our experimental settings. 6.1 Estimation of Sample Covering Numbers
We implement the algorithm in Section 4 to estimate the sample covering number induced by different data transformations. For efﬁciency, we randomly sample 1000 training images from CIFAR-10 and randomly sample 800 training images from ShapeNet. Appendix E compares results with different sample sizes. We use the Euclidean norm for deﬁning the sample cover. For continuous data transformations, we do uniform random sampling to approximate the orbit of a data point.
Figure 2 illustrates the estimated sample covering numbers induced by different transformations at different resolution (cid:15). As the resolution (cid:15) increases, the sample covering number N ((cid:15), S, ρG) induced 4Code is available at https://github.com/bangann/understanding-invariance. 8
n = 100 n = 1000 n = all
Model
Base
Flip
Rotate
Crop acc (%) gap acc (%) gap acc (%) gap 41.05 ± 0.52 44.19 ± 0.74 47.02 ± 0.46 50.47 ± 0.48 58.95 ± 0.52 55.81 ± 0.74 52.93 ± 0.51 49.53 ± 0.48 68.62 ± 0.90 75.12 ± 0.20 76.07 ± 0.28 81.84 ± 0.12 31.38 ± 0.90 24.88 ± 0.20 23.92 ± 0.27 18.15 ± 0.11 85.43 ± 0.35 89.67 ± 0.24 89.91 ± 0.13 92.52 ± 0.08 14.57 ± 0.35 10.33 ± 0.24 10.05 ± 0.16 7.48 ± 0.08
Table 1: Classiﬁcation accuracy and generalization gap (the difference between training and test accuracy) for ResNet18 on CIFAR-10. The number n denotes the sample size per class. n = 100 n = 1000 n = all
Model acc (%) gap acc (%) gap acc (%) gap 67.75 ± 2.02
Base 69.75 ± 1.55
Flip 70.25 ± 1.19
Rotate 74.88 ± 1.03
Crop 3D-View 78.13 ± 1.31 32.25 ± 2.02 30.25 ± 1.55 29.50 ± 1.15 23.53 ± 1.30 14.94 ± 1.76 83.33 ± 0.38 84.24 ± 0.30 83.93 ± 0.38 86.13 ± 0.39 88.79 ± 0.34 16.67 ± 0.38 15.76 ± 0.30 15.94 ± 0.35 13.75 ± 0.32 8.38 ± 0.79 91.81 ± 0.22 92.07 ± 0.20 91.85 ± 0.20 92.64 ± 0.12 94.38 ± 0.08 8.18 ± 0.22 7.92 ± 0.20 8.03 ± 0.26 7.17 ± 0.19 3.09 ± 0.10
Table 2: Classiﬁcation accuracy and generalization gap (the difference between training and test accuracy) for ResNet18 on ShapeNet. The number n denotes the sample size per class. by any data transformation starts to decrease, indicating a smaller-sized sample cover needed to cover the entire dataset. Meanwhile, different transformations behave differently. On CIFAR-10, cropping induces the smallest sample covering number. On ShapeNet, 3D-view transformation induces the smallest sample covering number and the gap is signiﬁcant. Our propositions suggest that data transformations which induce smaller sample covering numbers tends to bring more generalization beneﬁt for the corresponding invariant models. Therefore, Figure 2 indicates that models should generalize well if it is invariant to 3D-view transformation on ShapeNet or to cropping on CIFAR-10. 6.2 Evaluation of Generalization Beneﬁt
We now evaluate the actual generalization performance of invariant models to verify if the sample covering number is a good suitability measurement. We use ResNet18 [25] on both datasets and discuss the inﬂuence of model class’s implicit bias in Appendix E. A simple method to learn invariant models is to do data augmentation. The augmented loss function is Laug(x) = L(f (g(x))), where f (·) denotes the model and g(x) denotes a randomly sampled example in x’s orbit induced by transformation G. We use this method on CIFAR-10 and ShapeNet and show results in Table 1 and 2.
Sample covering number correlates well with generalization beneﬁt. We use the generalization gap (the gap between training accuracy and test accuracy) to measure actual generalization beneﬁt.
Compared with the baseline, invariant models show an improved reduced generalization gap and also improved test accuracy. On CIFAR-10, cropping-invariant model shows the smallest generalization gap and the highest accuracy. On ShapeNet, the model that is invariant to 3D-view changes shows the smallest generalization gap and the highest accuracy, especially when the training data size is small. By comparing results in Figure 2 and Table 1-2, we observe a clear correlation between the smaller sample covering number and better generalization beneﬁt. This veriﬁes our proposition — invariance to more suitable data transformations gives the model more generalization beneﬁt.
Model invariance indeed improves after learning. To verify that the improved generalization is indeed brought by the model invariance, we further enforce the invariance using the invariance regularization loss similar to [48, 51]: L = Lcls(f (x)) + λKL(f (x), f (g(x))). Speciﬁcally, in addition to minimizing the classiﬁcation loss on original images, we penalize the model by minimizing the KL divergence between model outputs on original images and on transformed ones. At test time, we use Linv(x) = Eg1,g2∈G[KL(f (g1(x)), f (g2(x)))] to evaluate the model invariance under transformation G. Table 3 shows that, as we increase the invariance penalty by increasing λ, invariant models enjoy smaller generalization gap. Moreover, the decreased invariance loss and increased 9
λ 0 0.01 0.1 0.3 1 3 10 100 train acc (%) test acc (%) gap
Linv
Ainv(%) 99.99 ± 0.01 99.98 ± 0.00 99.99 ± 0.00 99.98 ± 0.00 99.58 ± 0.04 97.74 ± 0.19 95.67 ± 0.26 92.89 ± 0.25 91.81 ± 0.22 92.77 ± 0.16 93.87 ± 0.19 94.23 ± 0.11 94.68 ± 0.09 94.48 ± 0.19 93.56 ± 0.29 91.85 ± 0.26 8.19 ± 0.22 7.21 ± 0.16 6.11 ± 0.19 5.76 ± 0.11 4.90 ± 0.09 3.26 ± 0.09 2.11 ± 0.04 1.03 ± 0.03 0.0548 ± 0.0028 0.0290 ± 0.0029 0.0152 ± 0.0003 0.0121 ± 0.0003 0.0095 ± 0.0001 0.0060 ± 0.0003 0.0037 ± 0.0002 0.0018 ± 0.0001 62.0 ± 0.6 74.78 ± 1.61 83.12 ± 0.50 85.10 ± 0.20 86.94 ± 0.08 88.15 ± 0.18 89.20 ± 0.16 89.82 ± 0.10
Table 3: Evaluation of ResNet18 on ShapeNet under 3D-view transformations. Linv denotes the test invariance loss. Ainv denotes the test consistency accuracy (indicating whether model’s prediction is unchanged after data transformation) under the worst-case data transformations. consistency accuracy verify that the model invariance indeed improves after training, supporting that the generalization beneﬁt is brought by the model invariance. 7