Abstract
This paper develops a conformal method to compute prediction intervals for non-parametric regression that can automatically adapt to skewed data. Leveraging black-box machine learning algorithms to estimate the conditional distribution of the outcome using histograms, it translates their output into the shortest prediction intervals with approximate conditional coverage. The resulting prediction intervals provably have marginal coverage in finite samples, while asymptotically achiev-ing conditional coverage and optimal length if the black-box model is consistent.
Numerical experiments with simulated and real data demonstrate improved perfor-mance compared to state-of-the-art alternatives, including conformalized quantile regression and other distributional conformal prediction approaches. 1

Introduction 1.1 Problem statement and motivation
We consider the problem of predicting with confidence a response variable Y ∈ R given p features
X ∈ Rp for a test point n+1, utilizing n pairs of observations {(X (i), Y (i))}n i=1 drawn exchangeably from some unknown distribution, and leveraging any machine-learning algorithm. (e.g., i.i.d.)
Precisely, ∀α ∈ (0, 1), we seek a prediction interval ˆCn,α(Xn+1) ⊂ R for Yn+1 satisfying the following three criteria. First, ˆCn,α should have finite-sample marginal coverage at level 1 − α,
P (cid:104) (cid:105)
Yn+1 ∈ ˆCn,α(Xn+1)
≥ 1 − α.
Second, ˆCn,α should approximately have conditional coverage at level 1 − α, (cid:104)
P
Yn+1 ∈ ˆCn,α(x) | Xn+1 = x (cid:105)
≥ 1 − α,
∀x ∈ Rp, (1) (2) meaning it should approximate this objective in practice, and ideally achieve it asymptotically under suitable conditions in the limit of large sample sizes. Third, ˆCn,α should be as narrow as possible.
We tackle this challenge with conformal inference [24, 35], which allows one to convert the output of any black-box machine learning algorithm into prediction intervals with provable marginal cov-erage (1). The key idea of this framework is to compute a conformity score for each observation, measuring the discrepancy, according to some metric, between the true value of Y and that predicted 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
by the black-box model. The model fitted on the training data is then applied to hold-out calibration samples, producing a collection of conformity scores. As all data points are exchangeable, the empirical distribution of the calibration scores can be leveraged to make predictive inferences about the conformity score of a new test point. Finally, inverting the function defining the conformity scores yields a prediction set for the test Y . This framework can accommodate almost any choice of conformity scores, and in fact many different implementations have already been proposed to address our problem. However, it remains unclear how to implement a concrete method from this broad family that can lead to the most informative possible prediction intervals. Our contribution here is to develop a practical solution, following the three criteria defined above, that performs better compared to existing alternatives and is asymptotically optimal under certain assumptions.
It is worth emphasizing that constructing a short prediction interval with guaranteed coverage is a reasonable approach to quantify and communicate predictive uncertainty in regression problems, although it is of course not the only one. To name an alternative, one could compute a non-convex prediction set with analogous coverage [20], which might be more appropriate in some situations, but is also more easily confusing. For example, it could be informative for a physician to know that the future blood pressure of a patient with certain characteristics is predicted to be within the range [120,129] mmHg. However, it would not be more helpful to report instead the following non-convex region: [120, 120.012] ∪ [120.015, 120.05] ∪ [121, 122.7] ∪ [123.1, 127.2] ∪ [127.8, 129] mmHg. Indeed, in the second case it would not be clear (a) whether the multi-modal nature of that prediction is significant or a spurious consequence of overfitting, and (b) how the physician would act upon that prediction any differently than if it had been [120,129] mmHg. Therefore, we focus on prediction intervals in this paper because they are generally easier to interpret than arbitrary regions, and they are also less likely to convey a false sense of confidence. 1.2 Preview of conformal histogram regression
Imagine an oracle with access to PY |X , the distribution of Y conditional on X, which leverages such information to construct optimal prediction intervals as follows. For simplicity, suppose PY |X has a continuous density f (y | x) with respect to the Lebesgue measure, although this could be relaxed with more involved notation. Then, the oracle interval for Yn+1 | Xn+1 = x would be:
C oracle
α (x) = (cid:2)loracle 1−α (x), uoracle 1−α (x)(cid:3) , where, for any τ ∈ (0, 1], loracle
τ (x) and uoracle
τ (x) are defined as:
[loracle
τ (x), uoracle
τ (x)] := arg min (l,u)∈R2 : l≤u (cid:26) u − l : (cid:90) u l f (y | x)dy ≥ τ
. (cid:27) (3) (4)
This is the shortest interval with conditional coverage (2). If the solution to (4) is not unique (e.g., if f (· | x) is piece-wise constant), the oracle picks any solution at random. Of course, this is not a practical method because f is unknown. Therefore, we will approximate (4) by fitting a black-box model on the training data, and then use conformal prediction to construct an interval accounting for any possible estimation errors. Specifically, we replace f in (4) with a histogram approximation, hence why we call our method conformal histogram regression, or CHR. The output interval is then
ˆCn,α(x) = (cid:105) (cid:104)ˆlˆτ (x), ˆuˆτ (x)
, (5) where ˆlˆτ (x) and ˆuˆτ (x) approximate the analogous oracle quantities in (4). The value of ˆτ in (5) will be determined by suitable conformity scores evaluated on the hold-out data, and it may be larger than 1 − α if the model for f is not very accurate. However, if the fitted histogram is close to the true
PY |X , the interval in (5) will resemble that of the oracle (3).
Figure 1 previews an application to toy data, comparing CHR to conformalized quantile regression (CQR) [30]; see Section 4.2 for more details. CHR finds the shortest interval such the corresponding area under the histogram is above τ , for any τ ∈ (0, 1], and then calibrates τ to guarantee marginal coverage above 1 − α; this extracts more information from the model compared to CQR. For example,
CHR adapts automatically to the skewness of Y | X, returning intervals delimited by the 0%–90% quantiles in this example, which are shorter than the symmetric ones (5%–95%) sought by CQR. 2
(a) (b)
Figure 1: CHR prediction intervals in an example with one variable, compared to those obtained with
CQR [30]. Both methods guarantee 90% marginal coverage and are based on the same deep quantile model. (a) Histogram estimate of PY |X for a point with X ≈ 0.2. The CHR interval corresponds to the shaded part of the histogram, whose area is approximately 0.9, as marked by the solid vertical lines. The dashed lines denote the CQR interval. (b) Prediction bands for the two methods, as a function of X. CHR: empirical marginal coverage 0.9, estimated conditional coverage 0.9, and average length 3.2. The corresponding quantities for CQR are: 0.9, 0.9, and 5.2, respectively. 1.3