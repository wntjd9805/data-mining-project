Abstract
We study a theory of reinforcement learning (RL) in which the learner receives binary feedback only once at the end of an episode. While this is an extreme test case for theory, it is also arguably more representative of real-world applications than the traditional requirement in RL practice that the learner receive feedback at every time step. Indeed, in many real-world applications of reinforcement learning, such as self-driving cars and robotics, it is easier to evaluate whether a learner’s complete trajectory was either “good” or “bad,” but harder to provide a reward signal at each step. To show that learning is possible in this more challenging setting, we study the case where trajectory labels are generated by an unknown parametric model, and provide a statistically and computationally efﬁcient algorithm that achieves sublinear regret. 1

Introduction
The Reinforcement Learning (RL) paradigm involves a learning agent interacting with an unknown dynamical environment over multiple time steps. The learner receives a reward signal after each step which it uses to improve its performance over time. This formulation of RL has had signiﬁcant empirical success in the recent past [24, 23, 33, 32].
While this empirical success is encouraging, as RL starts to tackle a more wide-ranging class of consequential real-world problems, such as self-driving cars, supply chains, and medical care, a new set of challenges arise. Foremost among them is the lack of a well-speciﬁed reward signal associated with every state-action pair in many real-world settings. For example, consider a robot manipulation task where the robot must fold a pile of clothes. It is not clear how to design a useful reward signal that aids the robot to learn to complete this task. However, it is fairly easy to check whether the task was successfully completed (that is, whether the clothes were properly folded) and provide feedback at the end of the episode.
This is a classical challenge but it is one that is often neglected in theoretical treatments of RL.
To address this challenge we introduce a framework for RL that eschews the need for a Markovian reward signal at every step and provides the learner only with binary feedback based on its complete trajectory in an episode. In our framework, the learner interacts with the environment for a ﬁxed number of time steps (H) in each episode to produce a trajectory (τ ) which is the collection of all 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
∗Equal contribution.
states visited and actions taken in these rounds. At the end of the episode a binary reward yτ ∈ {0, 1} is drawn from an unknown distribution Q(·|τ ) and handed to the learner. This protocol continues for
N episodes and the learner’s goal is to maximize the number of expected binary “successes.”
One approach to deal with the lack of a reward function in the literature is Inverse Reinforcement
Learning [25], which uses demonstrations of good trajectories to learn a reward function. However, this approach is difﬁcult to use when good demonstrations are either prohibitively expensive or difﬁcult to obtain. Another closely related line of work studies reinforcement learning with preference feedback [2, 15, 3, 5, 37, 26, 38]. Our framework provides the learner with an even weaker form of feedback than that studied in this line of work. Instead of providing preferences between trajectories, we only inform the learner whether the task was completed successfully or not at the end.
To study whether it is possible to learn under such drastically limited feedback we study the case where the conditional rewards (yτ ) are drawn from an unknown logistic model (see Assumption 2.1).
Under this assumption we show that learning is possible—we provide an optimism-based algorithm that achieves sublinear regret (see Theorem 3.2). Technically our theory leverages recent results of Russac et al. [31] for the online estimation of the parameters of the underlying logistic model, and combining them with the UCBVI algorithm [4] to obtain regret bounds. Under an explorability assumption we also show that our algorithm is computationally efﬁcient and we provide a dynamic programming algorithm to solve for the optimistic policy at every episode.
We note that Efroni et al. [11] study a similar problem to ours, such that a reward is revealed only at the end of the episode, but they assume that there exists an underlying linear model that determines the reward associated with each state-action pair, and reward revealed to the learner is the sum of rewards over the state-action pairs with added stochastic noise. This assumption ensures that the reward function is Markovian, and allows them to use an online linear bandit algorithm [1] to directly estimate the underlying reward function. This is not possible in our setting since we do not assume the existence of an underlying Markovian reward function. Cohen et al. [6] provided an algorithm that learns in this setting even when the noise is adversarially chosen. An open problem posed by
Efroni et al. [11] was to ﬁnd an algorithm that learns in this setting of reinforcement learning, with once per episode feedback, when the rewards are drawn from an unknown generalized linear model (GLM). In this paper we consider a speciﬁc GLM—the logistic model.
The remainder of the paper is organized as follows. In Section 2 we introduce notation and describe our setting. In Section 3 we present our algorithm and main results. Under an explorability assumption we prove that our algorithm is computationally efﬁcient (in Appendix E). Section 4 points to other related work and we conclude with a discussion in Section 5. Other technical details, proofs and experiments are deferred to the appendix. 2 Preliminaries
This section presents notational conventions and a description of the setting. 2.1 Notation
For any k ∈ N we denote the set {1, . . . , k} by [k]. Given any set T , let ∆T denote the simplex over this set. Given a vector v, for any p ∈ N, let (cid:107)v(cid:107)p denote the (cid:96)p norm of the vector. Given a v(cid:62)Mv. Given a matrix M let vector v and positive semi-deﬁnite matrix M, deﬁne (cid:107)v(cid:107)M := (cid:107)M(cid:107)op denote its operator norm. For any positive semi-deﬁnite matrix M we use λmax(M) and
λmin(M) to denote its maximum and minimum eigenvalues respectively. We will use C1, C2, . . . to denote absolute constants whose values are ﬁxed throughout the paper, and c, c(cid:48), . . . to denote
“local” constants, which may take different values in different contexts. We use the standard “big Oh notation” [see, e.g., 7].
√ 2.2 The Setting
We study a Markov decision process (MDP) M = (S, A, P, H), where S is the set of states, A is the set of actions, P(·|s, a) is the law that governs the transition dynamics given a state and action pair 2
(s, a), and H ∈ N is the length of an episode. Both the state space S and action space A are ﬁnite in our paper. The learner’s trajectory τ is the concatenation of all states and actions visited during an episode; that is, τ := (s1, a1, · · · , sH , aH ). Given any h ∈ [H] and trajectory τ , a sub-trajectory
τh := (s1, a1, . . . , sh, ah) is all the states and actions taken up to step h. Also set τ0 := ∅. Let
τh:H := (sh, ah, . . . , sH , aH ) denote the states and action from step h until the end of the episode.
Let Γ be the set of all possible trajectories τ . Analogously, for any h ∈ [H] let Γh be the set of all sub-trajectories up to step h. At the start of each episode the initial state s1 is drawn from a ﬁxed distribution ρ that is known to the learner.
At the end of an episode the trajectory τ gets mapped to a feature map φ(τ ) ∈ Rd. We also assume that the learner has access to this feature map φ. Here are two examples of feature maps: 1. Direct parametrization: Without loss of generality assume that S = {1, . . . , |S|} and h=1 φh(sh, ah), where the per-step maps
A = {1, . . . , |A|}. The feature map φ(τ ) = (cid:80)H
φh(s, a) ∈ R|S||A|H are deﬁned as follows: (φh(s, a))j = (cid:40) 1 0 if j = (h − 1)|S||A| + (s − 1)|A| + a, otherwise.
The complete feature map φ(τ ) ∈ R|S||A|H is therefore an encoding of the trajectory τ . 2. Reduced parametrization: Any trajectory τ is associated with a feature φ(τ ) ∈ Rd, where d < |S||A|H.
After the completion of an episode the learner is given a random binary reward yτ ∈ {0, 1}. Let w(cid:63) ∈ Rd be a vector that is unknown to the learner. We study the case where the rewards are drawn from a binary logistic model as described below.
Assumption 2.1 (Logistic model). Given any trajectory τ ∈ Γ, the rewards are said to be drawn from a logistic model if the law of yτ |τ is (cid:40) yτ |τ =
µ (cid:0)w(cid:62) (cid:63) φ(τ )(cid:1) 1 w.p. 0 w.p. 1 − µ (cid:0)w(cid:62) (cid:63) φ(τ )(cid:1) , (1) where for any z ∈ R, µ(z) = parameters.” 1 1+exp(−z) is the logistic function. We shall refer to w(cid:63) as the “reward
We make the following boundedness assumptions on the features and reward parameters.
Assumption 2.2 (Bounded features and parameters). We assume that
• (cid:107)w(cid:63)(cid:107)2 ≤ B for some known value B > 0 and
• for all τ ∈ Γ, (cid:107)φ(τ )(cid:107)2 ≤ 1.
We note that such boundedness assumptions are standard in the logistics bandits literature [13, 31, 14].
A policy π is a collection of per-step policies (π1, . . . , πH ) such that
πh : Γh−1 × S → ∆A.
If the agent is using the policy π then at round h of the episode the learner plays according to the policy πh. We let Πh denote the set of all valid policies at step h and let Π denote the set of valid policies over the trajectory. Let Pπ(·|s1) denote the joint probability distribution over the learner’s trajectory τ and the reward yτ when the learner plays according to the policy π and the initial state is s1. Often when the initial state is clear from the context we will refer to Pπ(·|s1) by simply writing Pπ.
Also with some abuse of notation we will sometimes let Pπ denote the distribution of the trajectory and the reward where the initial state is drawn from the distribution ρ.
Given an initial state s ∈ S the value function corresponding to a policy π is
V π(s) := Eyτ ,τ ∼Pπ [yτ | s1 = s] = Eτ ∼Pπ 3 (cid:2)µ (cid:0)w(cid:62) (cid:63) φ(τ )(cid:1) (cid:12) (cid:12) s1 = s(cid:3) ,
where the second equality follows as the mean of yτ conditioned on τ is µ(w(cid:62) abuse of notation we denote the average value function as V π := Es1∼ρ [V π(s1)] . (cid:63) φ(τ )). With some
Deﬁne the optimal policy as π(cid:63) ∈ arg maxπ∈Π V π. It is worth noting that in our setting the optimal policy may be non-Markovian. The learner plays for a total of N episodes. The policy played in episode t ∈ [N ] is π(t) and its value function is V (t) := V π(t)
. Also deﬁne the value function for the optimal policy to be V(cid:63) := V π(cid:63) . Our goal shall be to control the regret of the learner, which is deﬁned as
R(N ) :=
N (cid:88) t=1
V(cid:63) − V (t). (2)
The trajectories in these N episodes are denoted by {τ (t)}N
{y(t)}N t=1. t=1 and rewards received are denoted by 3 Optimistic Algorithms that Use Trajectory Labels
We now present an algorithm to learn from labeled trajectories. Throughout this section we assume that both Assumptions 2.1 and 2.2 are in force.
The derivative of the logistic function is µ(cid:48)(z) = exp(−z)
The following quantity will play an important role in our bounds (1+exp(−z))2 , and therefore, µ is 1/4-Lipschitz.
κ := max
τ ∈Γ sup w:(cid:107)w(cid:107)≤B 1
µ(cid:48)(w(cid:62)φ(τ ))
.
A consequence of Assumption 2.2 is that κ ≤ exp(B). We brieﬂy note that κ is a measure of curvature of the logistic model. It also plays an important role in the analysis of logistic bandit algorithms [13, 31].
Since the true reward parameter w(cid:63) is unknown we will estimate it using samples. At any episode t ∈ [N ], a natural way of computing an estimator of w(cid:63), given past trajectories {τ (q)}q∈[t−1] and labels {y(q)}q∈[t−1], is by minimizing the (cid:96)2-regularized cross-entropy loss:
Lt(w) := − t−1 (cid:88) q=1 y(q) log (cid:16) (cid:16)
µ w(cid:62)φ(τ (q)) (cid:17)(cid:17)
− (1 − y(q)) log (cid:16) 1 − µ (cid:16) w(cid:62)φ(τ (q)) (cid:17)(cid:17)
+ (cid:107)w(cid:107)2 2 2
.
This function is strictly convex and its minimizer is deﬁned to be
Deﬁne a design matrix at every episode (cid:98)wt := arg min w∈Rd
Lt(w).
Σ1 := κI, and Σt := κI + t−1 (cid:88) q=1
φ(τ (q))φ(τ (q))(cid:62), for all t ≥ 1.
Further, deﬁne the conﬁdence radius βt(δ) as follows (cid:16)
βt(δ) := (cid:16)√ (cid:17)(cid:17)3/2 1 + B + ρt(δ) (cid:19) (cid:18) 4t d 1 + B + ρt(δ) (cid:18) N
δ
+ (cid:19) 1 2
. where, ρt(δ) := d log 4 +
+ 2 log (3) (4)
We adapt a result due to Russac et al. [31, Proposition 7] who studied the online logistic bandits problem to establish that at every episode and every trajectory the difference between µ(w(cid:62) (cid:63) φ(τ )) and µ( (cid:98)w(cid:62)
Lemma 3.1. For any δ ∈ (0, 1], deﬁne the event t φ(τ )) is small.
Eδ := (cid:110) for all t ∈ [N ], τ ∈ Γ : (cid:12) (cid:12)µ(w(cid:62) (cid:63) φ(τ )) − µ( (cid:98)w(cid:62) t φ(τ ))(cid:12) (cid:12) ≤
√
κβt(δ)(cid:107)φ(τ )(cid:107)Σ−1 t (cid:111)
. (5)
Then P(Eδ) ≥ 1 − δ. 4
We provide a proof in Appendix B.2. The proof follows by simply translating [31, Proposition 7] into our setting. We note that we speciﬁcally adapt these recent results by Russac et al. [31] since they directly apply to (cid:98)wt, the minimizer of the (cid:96)2-regularized cross-entropy loss. In contrast, previous work on the logistic bandits problem [see, e.g., 14, 13] established conﬁdence sets for an estimator that was obtained by performing a non-convex (and potentially computationally intractable) projection of (cid:98)wt onto the ball of Euclidean radius B.
Our algorithm shall construct an estimate of the transition dynamics (cid:98)Pt. Let Nt(s, a) be the number of times that the state-action pair (s, a) is encountered before the start of episode t, and let Nt(s(cid:48); s, a) be the number of times the learner encountered the state s(cid:48) after taking action a at state s before the start of episode t. Deﬁne the estimator of the transition dynamics as follows:
Also deﬁne the state-action bonus at episode t (cid:98)Pt(s(cid:48)|a, s) :=
Nt(s(cid:48); s, a)
Nt(s, a)
. (cid:40) (cid:118) (cid:117) (cid:117) (cid:116)
ξ(t) s,a := min 2, 4 log (cid:16) 6(|S||A|H)H (8N H 2)|S| log(Nt(s,a))
δ (cid:17) (cid:41)
Nt(s, a)
. (6) (7)
In this deﬁnition whenever Nt(s, a) = 0, that is, when a state-action pair hasn’t been visited yet, we deﬁne ξ(t) s,a to be equal to 2. Finally, we deﬁne the optimistic reward functions (cid:110)
µ (cid:0)w(cid:62)φ(τ )(cid:1) +
¯µt(w, τ ) := min
κβt(δ)(cid:107)φ(τ )(cid:107)Σ−1
, 1
√ (cid:111) t and (8a) (8b) (cid:101)µt(w, τ ) := ¯µt(w, τ ) +
H−1 (cid:88) h=1
ξ(t) sh,ah
.
The ﬁrst reward function ¯µt is deﬁned as above to account for the uncertainty in the predicted value of w(cid:63) in light of Lemma 3.1, and the second reward function (cid:101)µt is designed to account for the error in the estimation of the transition dynamics P. With these additional deﬁnitions in place we are ready to present our algorithms and main results. 3.1 UCBVI with Trajectory Labels
Our ﬁrst algorithm is an adaptation of the UCBVI algorithm [4] to our setting with labeled trajectories.
Algorithm 1: UCBVI with trajectory labels. 1 Input: State and action spaces S, A. 2 Initialize (cid:98)P1 = 0, visitation set K = ∅. 3 for t = 1, · · · do 4 1. Calculate the (cid:98)wt by solving equation (3). 2. If t > 1, compute π(t) 5 6 7 8 9
π(t) ∈ arg max
π∈Π
E s1∼ρ, τ ∼(cid:98)Pπ t (·|s1) [(cid:101)µt( (cid:98)wt, τ )] . (9)
Else for all h, s, τh−1 ∈ [H] × S × Γh−1, set π(1) over the action set. 3. Observe the trajectory τ (t) ∼ Pπ(t) and update the design matrix h (·|s, τh−1) to be the uniform distribution
Σt+1 = κI + t (cid:88) q=1
φ(τ (q))φ(τ (q))(cid:62). (10) 4. Update the visitation set K = {(s, a) ∈ S × A : Nt(s, a) > 0}. 5. For all (s, a) ∈ K, update (cid:98)Pt+1(·|s, a) according to equation (6). 6. For all (s, a) /∈ K, set (cid:98)Pt+1(·|s, a) to be the uniform distribution over states. 5
Theorem 3.2. For any ¯δ ∈ (0, 1], set δ = ¯δ/(6N ) then under Assumptions 2.1 and 2.2 the regret of
Algorithm 1 is upper bounded as follows:
R(N ) ≤ (cid:101)O (cid:16)(cid:104)
H(cid:112)(H + |S|)|S||A| + H 2 +
√
κd(d3 + B3/2) (cid:105) √
N + (H + |S|)H|S||A| (cid:17)
, with probability at least 1 − ¯δ.
√
The regret of our algorithm scales with
N and polynomially with the horizon, number of states, number of actions, κ, dimension of the feature maps and length of the reward parameters (B). The minimax regret in the standard episodic reinforcement learning is O((cid:112)H|S||A|N ) [27, 4]. Here we pay for additional factors in H, |S| and κ since our rewards are non-Markovian and are revealed to the learner only at the end of the episode. We provide a proof of this theorem in Appendix B. For a more detailed bound on the regret with the logarithmic factors and constants speciﬁed we point the interested reader to inequality (41) in the appendix. t (·|s1) s1∼ρ, τ ∼(cid:98)Pπ(t)
Proof sketch. First we show that with high probability at each episode the value function of the optimal policy V(cid:63) is upper bounded by (cid:101)V (t) := E
[(cid:101)µt( (cid:98)wt, τ )] (the value function of the policy π(t) when the rewards are dictated by (cid:101)µt and the transition dynamics are given by (cid:98)Pt).
Then we provide a high probability bound on the difference between the optimistic value function (cid:101)V (t) and the true value function V (t) to obtain our upper bound on the regret. In both of these steps we need to relate expectations with respect to the true transition dynamics P to expectations with respect to the empirical estimate of the transition dynamics (cid:98)Pt. We do this by using our concentration results: Lemmas B.1 and B.2 proved in the appendix. While analogs of these concentration lemmas do exist in previous theoretical studies of episodic reinforcement learning, here we had to prove these lemmas in our setting with non-Markovian trajectory-level feedback (which explains why we pay extra factors in H and |S|). 3.2 UCBVI with Added Exploration
Although the regret of Algorithm 1 is sublinear it is not guaranteed to be computationally efﬁcient since ﬁnding the optimistic policy π(t) (in equation (9)) at every episode might prove to be difﬁcult.
In this section, we will show that when the features are sum-decomposable and the MDP satisﬁes an explorability assumption then it will be possible to ﬁnd a computationally efﬁcient algorithm with sublinear regret (albeit with a slightly worse scaling with the number of episodes N ).
Assumption 3.3 (Sum-decomposable features). We assume that the feature maps φ ∈ Rd are sum-decomposable over the different steps of the trajectory, that is, φ(τ ) = (cid:80)H h=1 φh(sh, ah). (cid:80)H
Under this assumption, given any w ∈ Rd and any trajectory τ ∈ Γ, w(cid:62)φ(τ ) = h=1 w(cid:62)φh(sh, ah). We stress that even under this sum-decomposablity assumption, the opti-mal policy is potentially non-Markovian due to the presence of the logistic map that governs the reward.
We also make the following explorability assumption.
Assumption 3.4 (Explorability). For any s, s(cid:48) ∈ S, a, a(cid:48) ∈ A, and h (cid:54)= h(cid:48) ∈ [H], suppose that
Further assume that there exists ω ∈ (0, 1) such that for any unit vector v ∈ Rd we have that
φh(s, a)(cid:62)φh(cid:48)(s(cid:48), a(cid:48)) = 0.
Es1∼ρ,τ ∼Pπ sup
π∈Π

 (cid:88) h∈[H]
 v(cid:62)φh(sh, ah)
 ≥ ω.
In a setting with Markovian rewards a similar assumption has been made previously by
Zanette et al. [40]. This assumption allows us to efﬁciently “explore” the feature space, and con-that we will use instead of struct a sum-decomposable bonus
κβt(δ) (cid:80)H h=1(cid:107)φh(sh, ah)(cid:107)Σ−1
√ t 6
√
κβt(δ)(cid:107)φ(τ )(cid:107)Σ−1 t in the deﬁnition of ¯µt (see equation (8a)). Deﬁne the reward functions
¯µsd t (w, τ ) := min (cid:40)
µ (cid:0)w(cid:62)φ(τ )(cid:1) +
√
κβt(δ)
H (cid:88) (cid:41) (cid:107)φh(sh, ah)(cid:107)Σ−1 t
, 1 and (11a) h=1 t (w, τ ) := ¯µsd (cid:101)µsd t (w, τ ) +
H−1 (cid:88) h=1
ξ(t) sh,ah
. (11b)
To prove a regret bound for an algorithm that uses these rewards our ﬁrst step shall be to prove that the sum-decomposable bonus also leads to an optimistic reward function (that is, the value function deﬁned by these rewards sufﬁciently over-estimates the true value function). To this end, we will ﬁrst use Algorithm 2 to ﬁnd an exploration mixture policy ¯U and play according to it at episode t with probability 1/t1/3. This policy ¯U will be such that the minimum eigenvalue of
E s1∼ρ, τ ∼P ¯U (·|s1) (cid:2)φ(τ )φ(τ )(cid:62)(cid:3) (12) is lower bounded by a function of d, ω and N (see Lemma 3.5). This property shall allow us to upper bound the condition number of the design matrix Σt and subsequently ensure that the rewards ¯µsd t and (cid:101)µsd t are optimistic. Given a unit vector v deﬁne a reward function at step h as follows: (13)
Let rv := (rv
H ) be a reward function over the entire episode. As a subroutine Algorithm 2 uses the EULER algorithm [39]. (We brieﬂy note that other reinforcement learning algorithms with
PAC or regret guarantees [e.g., 4, 19] could also be used here in place of EULER.) h(s, a) := v(cid:62)φh(s, a). rv 1 , . . . , rv
Algorithm 2: Find exploration mixture. 1 Input: Initial unit vector v1, Exploration lower bound ω, number of EULER episodes NEUL, number of evaluation episodes NEVAL. 16 I, n = 0 and λmin = inf z∈Rd z(cid:62)A0z. 2 Initialize: A0 = ω2 3 while λmin < ω2 8 do
Update the counter n ← n + 1. 4
Set Un ← EULER({rvn , NEUL) //run EULER for NEUL episodes. for t=1,. . . ,NEVAL episodes do
Sample a trajectory τ (t) n ∼ ρ × PUn . 5 7 6 8 9 10 11
Calculate the average feature (cid:98)an = (cid:80)NEVAL t=1 φ(τ (t)
Update the matrix An ← An−1 + (cid:98)an(cid:98)a(cid:62) n .
Update the minimum eigenvalue: λmin ← inf z∈Rd z(cid:62)Anz.
Set vn to be the minimum eigenvector of An. n )/NEVAL. 12 Set nloop = n. 13 Return: (i) ¯U = Unif(U1, · · · , Unloop ) //the uniform mixture over the policies; 14 (ii) Nexp = nloop × (NEUL + NEVAL)
//total number of episodes.
Lemma 3.5. There exist positive absolute constants C1 and C2 such that, under Assumptions 2.2, 3.3 (cid:17)
C1|S|2|A|H 2 log and 3.4, if Algorithm 2 is run with NEUL = dω2 ) d log(1+ 16N log(3/2)
, (NEUL + NEVAL) =: ¯Nexp then, with probability at least 1 − 2δ, we have and NEVAL =
C2d3 log3(cid:16) N d2
δω2
ω4
|S||A|N 2 d
δω2
ω2 (cid:18) (cid:19) and N >
Nexp ≤ ¯Nexp and furthermore:
E s1∼ρ, τ ∼P ¯U (·|s1) (cid:2)φ(τ )φ(τ )(cid:62)(cid:3) (cid:23)
ω2 log(3/2) 32d log (cid:0)d log (cid:0)1 + 16N dω2 (cid:1)(cid:1) I.
This lemma is proved in Appendix C. With this lemma in place we now present our modiﬁed algorithm under the explorability assumption. In the ﬁrst few episodes this algorithm ﬁnds the exploration mixture policy ¯U . In a subsequent episode t this algorithm acts according to the policy
π(t) which maximizes the value function associated with the rewards (cid:101)µsd t ( (cid:98)wt, τ ) with probability 1 − 1 t1/3 . Otherwise it uses the exploration mixture policy ¯U . 7
6 7 8 9 10 11 12
Algorithm 3: UCBVI with trajectory labels and added exploration. 1 Input: State and action spaces S, A, Initial unit vector v1, Exploration lower bound ω, number of EULER episodes NEUL, number of evaluation episodes NEVAL. 2 Initialize (cid:98)P1 = 0, visitation set K = ∅. 3 Find exploration mixture policy ¯U in Nexp episodes by running Algorithm 2. 4 for t = Nexp + 1, · · · , N do 5 1. Calculate (cid:98)wt by solving equation (3). 2. If t > Nexp + 1, compute π(t)
π(t) ∈ arg max
π
E s1∼ρ, τ ∼(cid:98)Pπ t (·|s1) t ( (cid:98)wt, τ )(cid:3) . (cid:2) (cid:101)µsd (14)
Else for all h, s, τh−1 ∈ [H] × S × Γh−1, set π(1) over the action set. h (·|s, τh−1) to be the uniform distribution (cid:40) 0 w.p. 1 − 1 1 w.p. t1/3 , 3. Sample bt = 1 t1/3 . 4. If bt = 1 then set π(t) ← ¯U . 5. Observe the trajectory τ (t) ∼ Pπ(t) and update the design matrix
Σt+1 = κI + t (cid:88) q=Nexp+1
φ(τ (q))φ(τ (q))(cid:62). (15) 6. Update the visitation set K = {(s, a) ∈ S × A : Nt(s, a) > 0}. 7. For all (s, a) ∈ K, update (cid:98)Pt+1(·|s, a) according to equation (6). 8. For all (s, a) /∈ K, set (cid:98)Pt+1(·|s, a) to be the uniform distribution over states.
The following is our regret bound for Algorithm 3.
Theorem 3.6. For any ¯δ ∈ (0, 1], set δ = ¯δ/(12N ). Under Assumptions 2.1, 2.2, 3.3 and 3.4, and for all N > ¯Nexp (see its deﬁnition in Lemma 3.5) if Algorithm 3 is run with the parameters NEUL and NEVAL set as speciﬁed in Lemma 3.5 then its regret is upper bounded as follows: (cid:32) √
R(N ) ≤ (cid:101)O (d3 + B3/2)N 2/3 + (cid:104)
H(cid:112)(H + |S|)|S||A| + H 2(cid:105) √
N
κHd
ω
+(H + |S|)H|S||A| + d2
ω2 (cid:18) d2
ω2 + |S|2|A|H 2 (cid:19)(cid:19)
, with probability at least 1 − ¯δ.
√
The proof of Theorem 3.6 is in Appendix D. For a more detailed bound on the regret with the logarithmic factors and constants speciﬁed we point the interested reader to inequality (58) in the appendix. The bound on the regret of this algorithm scales with N 2/3 up to poly-logarithmic factors.
N regret bound (again up to poly-logarithmic factors) that we proved above
This is larger than the for Algorithm 1 since here the learner plays according to the exploration policy ¯U with probability 1/t1/3 throughout the run of the algorithm. However, the next proposition shows that by using the the policy π(t) deﬁned in equation (14) can be efﬁciently sum-decomposable reward function (cid:101)µsd approximated.
Proposition 3.7. For any t ∈ [N ] deﬁne (cid:101)V sd under Assumptions 2.2, 3.3 and 3.4 it is possible to ﬁnd a policy (cid:98)π(t) that satisﬁes t ( (cid:98)wt, τ )(cid:3). Given any ε > 0, (cid:2) (cid:101)µsd t (π) := E s1∼ρ, τ ∼(cid:98)Pπ t (·|s1) t t (π(t)) − (cid:101)V sd (cid:101)V sd t ((cid:98)π(t)) ≤ ε, using at most poly (cid:0)|S|, |A|, H, d, B, (cid:107) (cid:98)wt(cid:107)2, 1
ε , log (cid:0) N
We describe the approximate dynamic programming algorithm that can be used to ﬁnd this policy (cid:98)π(t) and present a proof of this proposition in Appendix E. We also note that if we use an (cid:1)(cid:1) time and memory.
δ 8
ε-approximate policy (cid:98)π(t) instead of π(t) in Algorithm 3 then its regret increases by an additive factor of at most εN . (It is possible to easily check this by inspecting the proof of Theorem 3.6.) Thus, for example a choice of ε = 1/N 1/3 ensures that the regret of Algorithm 3 is bounded by O(N 2/3) with high probability if the approximate policy (cid:98)π(t) (which can be found efﬁciently) is used instead. 4 Additional