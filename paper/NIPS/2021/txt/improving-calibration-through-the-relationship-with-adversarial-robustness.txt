Abstract
Neural networks lack adversarial robustness, i.e., they are vulnerable to adversarial examples that through small perturbations to inputs cause incorrect predictions.
Further, trust is undermined when models give miscalibrated predictions, i.e., the predicted probability is not a good indicator of how much we should trust our model. In this paper, we study the connection between adversarial robustness and calibration and ﬁnd that the inputs for which the model is sensitive to small pertur-bations (are easily attacked) are more likely to have poorly calibrated predictions.
Based on this insight, we examine if calibration can be improved by addressing those adversarially unrobust inputs. To this end, we propose Adversarial Robust-ness based Adaptive Label Smoothing (AR-AdaLS) that integrates the correlations of adversarial robustness and calibration into training by adaptively softening labels for an example based on how easily it can be attacked by an adversary. We ﬁnd that our method, taking the adversarial robustness of the in-distribution data into consideration, leads to better calibration over the model even under distributional shifts. In addition, AR-AdaLS can also be applied to an ensemble model to further improve model calibration. 1

Introduction
The robustness of machine learning algorithms is becoming increasingly important as ML systems are being used in higher-stakes applications. In one line of research, neural networks are shown to lack adversarial robustness – small perturbations to the input can successfully fool classiﬁers into making incorrect predictions (Szegedy et al., 2014; Goodfellow et al., 2014; Carlini & Wagner, 2017b; Madry et al., 2017; Qin et al., 2020b). In largely separate lines of work, researchers have studied uncertainty in model’s predictions. For example, models are often miscalibrated where the predicted conﬁdence is not indicative of the true likelihood of the model being correct (Guo et al., 2017; Thulasidasan et al., 2019; Lakshminarayanan et al., 2017; Wen et al., 2020; Kull et al., 2019).
The calibration issue is exacerbated when models are asked to make predictions on data different from the training distribution (Snoek et al., 2019), which becomes an issue in practical settings where it is important that we can trust model predictions under distributional shift.
Despite robustness, in all its forms, being a popular area of research, the relationship between these perspectives has not been extensively explored previously. In this paper, we study the correlation between adversarial robustness and calibration. We discover that input data points that are sensitive to small adversarial perturbations (are easily attacked) are more likely to have poorly calibrated predictions. This holds true on a number of network architectures for classiﬁcation and on all the datasets that we consider: CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009) and
ImageNet (Russakovsky et al., 2015). This suggests that the miscalibrated predictions on those adversarially unrobust data points greatly degrades the performance of model calibration. Based on this insight, we hypothesize and study if calibration can be improved by giving different supervision to the model depending on adversarial robustness of each training data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Inputs that are adversarially unrobust are more likely to have poorly calibrated and unstable predictions on CIFAR-10, CIFAR-100 and ImageNet. Top: Accuracy and conﬁdence of the predicted class. Bottom: ECE (lower is better) and variance (lower is better) in each adversarial robustness subset. Higher adversarial robustness level means the input are more adversarially robust (require larger adversarial perturbations to fool the classiﬁer into wrong predictions).
To this end, we propose Adversarial Robustness based Adaptive Label Smoothing (AR-AdaLS) to integrate the correlations between adversarial robustness and calibration into training. Speciﬁcally,
AR-AdaLS adaptively smooths the training labels conditioned on how vulnerable an input is to adversarial attacks. Our method improves label smoothing (Szegedy et al., 2014) by explicitly teaching the model to differentiate the training data according to their adversarial robustness and then adaptively smooth their labels. By giving different supervision to the training data, our method leads to better calibration over the model without an increase of latency during inference.
In addition, since adversarially unrobust data points can be considered as outliers of the underlying data distribution (Carlini et al., 2019), our method can even greatly improve model calibration on held-out shifted data. Further, we propose “AR-AdaLS of Ensemble” to combine our AR-AdaLS and deep ensembles (Lakshminarayanan et al., 2017; Snoek et al., 2019), to further improve the calibration performance under distributional shift. Last, we ﬁnd an additional beneﬁt of AR-AdaLS is improving model stability (i.e., decreasing variance over multiple independent runs), which is valuable in practical applications where changes in predictions across runs (churn) is problematic.
In summary, our main contributions are as follows:
• Relationship among Robustness Metrics: We ﬁnd a signiﬁcant correlation between adversar-ial robustness and calibration: inputs that are unrobust to adversarial attacks are more likely to have poorly calibrated predictions.
• Algorithm: We hypothesize that training a model with different supervision based on adversarial robustness of each input will make the model better calibrated. To this end, we propose
AR-AdaLS to automatically learn how much to soften the labels of training data based on their adversarial robustness. Further, we introduce “AR-AdaLS of Ensemble” to show how to apply
AR-AdaLS to an ensemble model.
• Experimental Analysis: On CIFAR-10, CIFAR-100 and ImageNet, we ﬁnd that AR-AdaLS is more effective than previous label smoothing methods in improving calibration, particularly for shifted data. Further, we ﬁnd that while ensembling can be beneﬁcial, applying AR-AdaLS to adaptively calibrate ensembles offers further improvements over calibration. 2