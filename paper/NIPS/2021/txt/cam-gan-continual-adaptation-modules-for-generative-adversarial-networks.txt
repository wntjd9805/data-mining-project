Abstract
We present a continual learning approach for generative adversarial networks (GANs), by designing and leveraging parameter-efﬁcient feature map transforma-tions. Our approach is based on learning a set of global and task-speciﬁc parameters.
The global parameters are ﬁxed across tasks whereas the task-speciﬁc parameters act as local adapters for each task, and help in efﬁciently obtaining task-speciﬁc feature maps. Moreover, we propose an element-wise addition of residual bias in the transformed feature space, which further helps stabilize GAN training in such settings. Our approach also leverages task similarities based on the Fisher informa-tion matrix. Leveraging this knowledge from previous tasks signiﬁcantly improves the model performance. In addition, the similarity measure also helps reduce the parameter growth in continual adaptation and helps to learn a compact model.
In contrast to the recent approaches for continually-learned GANs, the proposed approach provides a memory-efﬁcient way to perform effective continual data generation. Through extensive experiments on challenging and diverse datasets, we show that the feature-map-transformation approach outperforms state-of-the-art methods for continually-learned GANs, with substantially fewer parameters.
The proposed method generates high-quality samples that can also improve the generative-replay-based continual learning for discriminative tasks. 1

Introduction
Lifelong learning is an innate human characteristic; we continuously acquire knowledge and upgrade our skills. We also accumulate our previous experiences to learn a new task efﬁciently. However, learning new tasks rarely affects our performance on already-learned tasks. For example, after learning one programming language, if we learn a new such language it is rare that our skill on the ﬁrst deteriorates. In fact, the knowledge of the previous task(s) often speeds up learning of the subsequent task(s). On the other hand, attaining lifelong learning in deep neural networks is still a challenge. Naive implementation of continual learning with deep learning models suffers from catastrophic forgetting [1, 2, 3], which makes lifelong or continual learning (CL) difﬁcult.
Catastrophic forgetting refers to the situation when a model exhibits a decline in its performance on previously learned tasks, after it learns a new task.
Several prior works [1, 3, 2, 4, 5, 6, 7, 8] have been proposed to address catastrophic forgetting in deep neural networks, with most of them focusing on classiﬁcation problems. In contrast, continual learning for unsupervised deep generative models, such as generative adversarial networks (GAN) [9] and variational auto-encoders (VAE) [10] is relatively less studied so far. Some recent work [6, 11, 12, 13] has tried to address catastrophic forgetting in GANs. Among these, the generative-replay-based approach [6, 11] usually works well only when the number of tasks is small. As the number of tasks
∗Equal contribution. † Currently with Google Research 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
become large, the model starts generating unrealistic samples that are not suitable for generative replay. A regularization-based approach for continual learning in GANs [12] often gives sub-optimal solutions. In particular, instead of learning the optimal weights for each task, it learns a mean weight for all tasks, since a single set of weights tries to generalize across all tasks. Therefore, these approaches mix the sample generation of various tasks and generate blurry and unrealistic samples.
Also, continually learning new tasks on a ﬁxed size network is not feasible since, after learning a few tasks, the model capacity exhausts and the model becomes unable to learn future tasks.
Recently, expansion-based CL models [14, 15, 16, 17, 7, 18, 19, 20, 21, 22] have shown promising results for discriminative (supervised) continual learning settings. These approaches are dynamic in nature and allow the number of network parameters to grow to accommodate new tasks. Moreover, these approaches can also be regularized appropriately by partitioning the previous and new task pa-rameters. Therefore, unlike regularization-based approaches, the model is free to adapt to novel tasks.
However, despite their promising performance, the excessive growth in the number of parameters
[15, 19, 20, 21, 22] and ﬂoating-point (FLOP) requirements are critical concerns.
In this work, we propose a simple expansion-based approach for GANs, which continually adapts to novel tasks without forgetting the previously-learned knowledge. The proposed approach considers a base model with global parameters and corresponding global feature maps. While learning each novel task, the base model is expanded to consider a task-speciﬁc feature transformation which efﬁciently adapts a global feature map to a task-speciﬁc feature map in each layer. Even though the total number of parameters increases due to the additional local/task-speciﬁc parameters, this feature-map transformation approach allows the leveraging of efﬁcient architecture design choices (e.g., groupwise and pointwise convolution) to obtain compact-sized task-speciﬁc parameters, which controls the growth and keeps the proposed model compact.
We empirically observe that our feature map transformation is highly efﬁcient, simple and effective, and shows more promising results compared to the weight space transformation based approaches [13].
In addition, our approach enables leveraging the task similarities. In particular, learning a new task that is similar to previous tasks should require less effort compared to learning a very different task; if we already know statistics and linear algebra, learning the subject machine learning is more easy compared to learning a completely different subject, e.g., computer architecture. Most existing CL approaches ignore the task-similarity information during the continual adaptation. We ﬁnd that learning a novel task by initializing its task-speciﬁc parameters with the task-speciﬁc parameters of the most similar previous task signiﬁcantly boosts performance. To this end, we learn a compact embedding for each task using the mean of the Fisher information matrix (FIM) per ﬁlter, and use it to measure task similarity. We observe that considering the task similarity information for parameter initialization not only boosts the model performance but can also be useful to reduce the number of task-speciﬁc parameters. While FIM has been used in regularization-based approaches for continual learning [2], we show how it can be used in expansion based methods like ours.
To show the efﬁcacy of the proposed model, we conduct extensive experiments in various settings on real-world datasets. We show that the proposed approach can sequentially learn a large number of tasks without catastrophic forgetting, while incurring much smaller parameter and FLOP growth compared to the existing continual-learning GAN models. Further, we show that our approach is also applicable to the generative-replay-based discriminative continual learning (e.g., for classiﬁcation problems). We empirically show that the pseudo-rehearsal provided by the proposed approach shows promising results for generative-replay-based discriminative models. Also, we conduct experiments to demonstrate the effectiveness of considering the task similarity in continual image generation, which we believe can lead to a promising direction for continual learning. Our contribution can be summarized as follows:
• We propose an efﬁcient feature-map-based transformation approach to continually adapt to new tasks in a task sequence, without forgetting the knowledge from previous tasks.
• We propose a parallel combination of groupwise and pointwise 3 × 3 and 1 × 1 ﬁlters for efﬁcient adaptation to novel tasks, and require signiﬁcantly fewer parameters for adaptation.
• We propose an approach that leverages the Fisher information matrix to measure task similarities for expansion-based continual learning. We empirically demonstrate its superior performance on several datasets. 2
Figure 1: The proposed efﬁcient transformation module over a convolutional layer. Each layer consists of three task-speciﬁc parameter φt lg and φt lr is the residual bias in the transformed space. lr and one global parameter θl. Here φt lp, φt
• We demonstrate the efﬁcacy of the proposed model for continual learning of GANs and generative-replay-based discriminative tasks. We observe that, with much less parameter and FLOP growth, our approach outperforms existing state-of-the-art methods. 2 Lightweight Continual Adapter
Notation and Problem Setting: We assume a set of tasks T1, T2, . . . , TK to be learned, and they arrive in a sequence. At a particular time t, data corresponding to the only current task Tt are available.
The data for the tth task are given as Tt = {xi, yi}Nt i=1 where (xi,yi) represents data samples and class labels, respectively. For unconditional data generation, we assume yi = 1, and for the conditional generation yi ∈ Y, where Y is the label set for the task Tt.
The proposed approach consists of a base model M with generator parameters θ, which serve as the global parameters. The global parameters at layer l are denoted by θl. Each layer of M also contains the task-speciﬁc parameters which perform task-speciﬁc feature-map transformation. Let φt l denote the task-speciﬁc parameter for the lth layer during the training of task t. We use the GAN framework as a base generative model, based on the architecture proposed by [23], although it is applicable to other GAN architectures as well. The base model M comprises the discriminator (D) and generator (G). Since our main aim is to learn a good generator for each task, our continual learning approach considers only the generator to have task-speciﬁc adapters. The discriminator parameters (θd) can be modiﬁed using each new task’s data. We learn the global parameters from a base task using the GAN objective [23] and learn the task speciﬁc parameters φt as we adapt to a new task t: min
φt max
θd
Ex∼pt data
[log(D(x; θd))] + Ez∼pz(z)[log(1 − D(G(z; θ, φt); θd))] + R(θd, φt) (1) where E denotes expectation and R() is the regularization term deﬁned in [23].
We propose an efﬁcient expansion-based approach to mitigate catastrophic forgetting in GANs while learning multiple tasks in a sequential manner. The proposed adapter module is fairly lightweight and easy to use in any standard model for continual adaptation. The added adapter in the standard architecture may, however, destabilize GAN training. To address this issue, we add a residual adapter in the transformed feature space that results in smooth training for the modiﬁed architecture. The parameters in efﬁcient adapters and residual bias serve as local/task-speciﬁc parameters and are speciﬁc to a particular task. The details of the proposed adapter and residual bias are provided in
Sections 2.1 and 2.2, respectively. 2.1 Efﬁcient Adapter
We propose task-speciﬁc adapter modules to continually adapt the model to accommodate novel tasks, without suffering from catastrophic forgetting for the previous tasks. The proposed approach 3
leverages a layerwise transformation of the feature maps of the base model, using the adapter modules to learn each novel task. While training the task sequence, the global parameters corresponding to l ∈ Rm×n×c be the feature map obtained at the lth layer upon base model M remain ﬁxed. Let F t applying the global parameters θl of the lth layer during training of task t, i.e., l = fθl (F t
F t l−1) (2)
Here fθl represents convolution and F t l−1 is the previous layer’s feature map. Our objective is to transform the feature map F t l that is obtained at the lth layer, but not yet task-speciﬁc, to a task-speciﬁc feature map using the task-speciﬁc adapters. Our task-speciﬁc adapters comprise two types of efﬁcient convolutional operators. We combine both in a single operation to reduce layer latency. 3 × 3 Groupwise Convolutional Operation Let f g be the 3×3 groupwise convolutional operation (3 × 3−GCO) [24] with a group size k at a particular layer l, and deﬁned by local parameters φt lg.
Therefore, we have each convolutional ﬁlter ci ∈ R3×3×k in contrast to the standard convolutional
ﬁlter cs ∈ R3×3×c. Here k (cid:28) c, and therefore each ﬁlter ci is highly efﬁcient and requires c k times fewer parameters and FLOPs. We apply the function fg on the feature map obtained by (2). For the tth task, let the new feature map obtained after applying f g be M t g, which can be written as
M t g = f g
φt lg (F t l ) (3) g ∈ Rm×n×c is the feature map of the same dimension as the feature map obtained by (2).
Here M t
Therefore, we can easily feed the obtained feature map to the next layer without any architectural modiﬁcations. Note that the standard convolution cs accumulates the information across all c channels of the feature map, and in each channel its receptive ﬁeld is 3 × 3. If we reduce the group size to k, then the new convolution ci covers the same receptive ﬁeld but it accumulates the information across only k (cid:28) c channels. The above process is highly efﬁcient but reduces the model’s performance, since it suffers from a non-negligible information loss by capturing fewer channels. To mitigate this potential information loss, we further leverage a specialized FLOP- and parameter-efﬁcient convolutional ﬁlter, as described below. 1 × 1 Groupwise Convolutional Operation Pointwise convolution (PWC) is widely used for efﬁcient architecture design [25, 26]. The standard PWC contains ﬁlters of size 1 × 1 × c. It is 9 times more efﬁcient as compared to the 3 × 3 convolution but still contains signiﬁcant number of parameters. To increase the efﬁciency of the PWC, we further divide it into z groups. Therefore, each convolutional ﬁlter ai contains the ﬁlter of size 1 × 1 × z and, since z (cid:28) c, it reduces the FLOP requirements and number of parameters c z × as compared to the standard PWC. Also, since z > k, it captures more feature maps and overcomes the drawback of the 3 × 3−GCO. Let f p be the 1 × 1 groupwise convolutional operational (1 × 1-GCO) and φt lp be the lth layer local parameter given by the 1 × 1 groupwise convolutional operation. Assume that, after applying f p, we obtain the feature map M t p, which can be written as
M t p = f p
φt lp (F t l ) (4)
Here M t easily apply this input to the next layer without any signiﬁcant changes to the base architecture. p ∈ Rm×n×c also has the same dimension as the incoming feature map, and therefore we can
Parallel Combination Note that local feature map adapters, i.e. (3) and (4), can be applied in a parallel manner and the joint feature map for task t can be written as
M t l = M t g ⊕ βM t p (5)
Here β = 1 when we use 1 × 1 groupwise convolution operation, otherwise β = 0, and ⊕ is the element-wise addition. Therefore, using β, we can trade-off the model’s parameter and FLOP growth with its performance. In this combination, M t g is obtained by ﬁlters that capture a bigger receptive ﬁeld and M t p is obtained by ﬁlters that capture the information across longer feature-map sequences. Therefore, the combination of the two helps increase a model’s performance without a signiﬁcant increase in the model’s parameters and FLOPs. Like most of the efﬁcient architecture models [26, 27], we can apply the 3 × 3 and 1 × 1 convolutional operation in a sequential manner, denoted as: M t l requires two sequential layers in the model architecture l )). Here M t (F t l = f p
φt lp (f g
φt lg 4
and execution of the second layer is followed by the ﬁrst layer, which limits the parallelization of the model. Therefore, throughout the paper, we follow (5), i.e., the parallel combination. Again, note that the proposed adapter is added only to the generator network since the discriminator model’s parameter can be discarded once the model for each task is trained. 2.2 Residual Bias
We append the proposed adapters to the base model; therefore the new architecture is deﬁned by parameters [θ, φ]. The parameters θ are considered as global parameters and each task t has its own local/task-speciﬁc parameter φ. After adding the adapter layer (discussed in Sec. 2.1) to the base architecture, the model becomes unstable, and after training for a few epochs, the generator loss diverges and discriminator loss goes to zero. The most probable reason for this behaviour is that, after adding the efﬁcient adapter layer, each residual block doubles the layer depth, and the effect of the residual connection for such a long sequence is not prominent. Therefore the model starts diverging.
Another reason for the divergence could be that the lth layer expects the feature map obtained by the base task, but because of the local transformation, the obtained feature maps are very different, which cause the instability in the training. To overcome this problem, we learn another function f r using efﬁcient convolutional operations with parameters φt lr. Here, f r is also a 3 × 3 groupwise convolution and, on layer (l + 1)th, it takes the feature map of layer l − 1 as input. Therefore, the r ∈ Rm×n×c has the same dimension as residual bias can be deﬁned as: M t (Fl−1), where M t
M t i . Therefore, we can perform element-wise addition of this feature map to Eq. (5). We consider this as an element-wise bias, added to the output feature map. Moreover, note that it is like a residual connection from the previous layer, but instead of directly taking the output of the (l − 1)th layer, it transforms the (l − 1)th layer’s feature map. We call this term as residual bias and the ﬁnal feature l ⊕ M t map after the continual adapter and residual bias is deﬁned as: F t l+1 = M t r. Therefore, at lg, φt l = [φt the lth layer for the tth task, the model has θl as the global parameter and φt lr] as the local/task-speciﬁc parameters. Here, |φt l| (cid:28) |θl|, i.e., the number of parameters in φt l is much smaller than in θl, which helps control the growth in model size as it adapts to a novel task. r = f r
φt lr lp, φt 2.3 Task-similarity based Transfer Learning (TSL)
Transfer learning plays a crucial role in achieving state-of-the-art performance in deep learning models. In continual learning approaches, each novel task’s parameters are initialized with the previously learned model parameters. However, this can exhibit high variance in performance [28] depending on the order in which tasks arrive. Also, these approaches may have limited transfer learning ability because we may initialize the novel task’s parameters with those of the previous tasks which may be very different. We observe that initializing the novel task’s parameters with the most similar task’s parameters not only boosts the model performance but also reduces variance in performance. It is easy to learn the task similarity for supervised learning settings and [29] explores the same for the meta-learning. In this work, we explore the task-similarity for more challenging settings, i.e., unsupervised learning connected to GANs.
We empirically observe that similar tasks share a similar gradient. Therefore, to measure the task correlation, our proposed approach leverages gradient information. In particular, we calculate the
Fisher Information Matrix (FIM) of the generator parameters without using explicit label information.
Let Lg be the generator loss w.r.t. parameter θg. The diagonal approximation of the FIM for the task
Tt can be deﬁned as: (cid:33)
Ft = Diag
∇Lg(xi|θg)∇Lg(xi|θg)T (6) (cid:32) 1
Nt
Nt(cid:88) i=1
Here we consider θg as ﬂattened, and consequently it is a vector. We use Ft as an embedding to represent the task; note that it has the same dimension as the generator’s parameters. However, directly learning and representing task embeddings in such a manner is expensive, both computationally as well as in terms of storage. We consider the convolutional ﬁlters used in the adapter modules for calculating FIM to reduce the number of parameters and we further replace each convolutional ﬁlter by its mean value, and ignore the fully connected layers to learn a compact task embedding. We use these learnt task embeddings to calculate task similarities among various tasks. Based on the similarity measure, we initialize adapter modules parameters of the current task with the parameters of the most similar previously seen task. 5
3