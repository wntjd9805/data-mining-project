Abstract
When a graph neural network (GNN) made a prediction, one raises question about explainability: “Which fraction of the input graph is most inﬂuential to the model’s decision?” Producing an answer requires understanding the model’s inner workings in general and emphasizing the insights on the decision for the instance at hand. Nonetheless, most of current approaches focus only on one aspect: (1) local explainability, which explains each instance independently, thus hardly exhibits the class-wise patterns; and (2) global explainability, which systematizes the globally important patterns, but might be trivial in the local context. This dichotomy limits the ﬂexibility and effectiveness of explainers greatly. A performant paradigm towards multi-grained explainability is until-now lacking and thus a focus of our work. In this work, we exploit the pre-training and ﬁne-tuning idea to develop our explainer and generate multi-grained explanations. Speciﬁcally, the pre-training phase accounts for the contrastivity among different classes, so as to highlight the class-wise characteristics from a global view; afterwards, the ﬁne-tuning phase adapts the explanations in the local context. Experiments on both synthetic and real-world datasets show the superiority of our explainer, in terms of AUC on explaining graph classiﬁcation over the leading baselines. Our codes and datasets are available at https://github.com/Wuyxin/ReFine. 1

Introduction
While graph neural networks (GNNs) [1, 2] have achieved great success in a variety of applications, they usually come as black-box models. The general problem about GNN explainability [3] is to answer “What knowledge does the model use to arrive at the conclusions in general and the speciﬁc decision at hand?”. Thoroughly answering this question requires the global understanding of the model’s inner workings and the local insights on a speciﬁc instance. Take a GNN model for molecular property prediction as an example. The global understanding exhibits the knowledge encoded in the model, such as the distribution of the chemical groups; meanwhile, the local insight identiﬁes certain chemical groups responsible for a given molecule’s property. Such multi-grained explainability
ﬂexibly and reliably inspects the decision-making process of the GNN [4, 5], which is critical to the applications on safety, fairness, and privacy [6, 7].
In the ﬁeld of GNN explainability [8], explainer models broadly attribute model prediction to the input graph, then sample a salient subgraph as the explanation for the model prediction. However, most of current explainers focus on either on local [9, 10, 6, 11, 12] or global explainability [13, 7], thereby suffer from inherent limitations correspondingly:
∗Xiangnan He is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Explanations on Visual Genome dataset generated from ReFine, including the pre-training and ﬁne-tuning phases. Right indicates the changes before and after the ﬁne-tuning.
• Local explainability aims to customize the explanatory subgraph for each instance individually.
However, such local explanations fall short in systematizing the prototypical patterns shared within a class or group of instances. Thus, they lack the global understanding of the model’s workings
[7, 13], which is vital to generalize to other instances being explained.
• Global explainability targets at the globally important patterns across multiple instances, which could violate the local ﬁdelity [14] — the globally important substructure may not be important or even appear in the local context, thus might fail to explain a speciﬁc instance reliably.
Brieﬂy put, these approaches overlook the multi-granularity nature of explainability, while we argue that the local and global explainability should be exhibited simultaneously to obtain faithful expla-nations. Taking Figure 1 as an example, the global explainability differentiates the explanations for various classes, such as livestock-background subgraphs for the farm class, human-sports subgraphs for the stadium class. When zooming in a speciﬁc scene graph, the local explainability reﬁnes on the farm-wise patterns and speciﬁes (sheep, on, meadow) as the ﬁnal explanation. A paradigm towards such multi-grained explainability is until-now lacking, to the best of our knowledge.
Towards multi-grained explainability, we propose a novel explainer, ReFine, with pre-training and
ﬁne-tuning [15, 16] techniques for explaining GNN models. Speciﬁcally, pre-training aims to answer “What class-wise knowledge does the GNN leverage to make predictions in general?”. We combine the contrastive learning [17, 18] into class-wise generative probabilistic models [7], thereby approach coarser-grained explanations (i.e. saliency maps of all edges). Going beyond the global view, ﬁne-tuning is to answer “Why the GNN model made the certain prediction for the instance at hand?”, where we upgrade the coarser-grained explanations to the ﬁner-grained explanations (i.e. explanatory subgraphs of salient edges). Through this way, ReFine can faithfully generate multi-grained explanations, and we empirically show its effectiveness as compared to some state-of-the-art explainers [9, 6, 7, 19]. It is also worth mentioning that, although the general understanding of GNN predictions has been considered in a recent work PGExplainer [7], it is only exploited to train a generative probabilistic model shared across all the explained instances, rather than dissecting and modeling the class-wise knowledge explicitly. Overall, our contributions are summarized as:
• We investigate the local explainability and global explainability for explaining GNNs and put forward the concept of multi-grained explainability.
• We propose a pre-training and ﬁne-tuning framework to generate multi-grained explanations, which has both global understanding of model workings and local insights on speciﬁc instances.
• We achieve state-of-the-art performance on various datasets w.r.t. predictive accuracy on explaining
GNNs. Quantitative and qualitative results verify multi-granularity explainability of ReFine. 2