Abstract
We propose a multi-agent variant of the classical multi-armed bandit problem, in which there are N agents and K arms, and pulling an arm generates a (possibly different) stochastic reward for each agent. Unlike the classical multi-armed bandit problem, the goal is not to learn the “best arm”; indeed, each agent may perceive a different arm to be the best for her personally. Instead, we seek to learn a fair distribution over the arms. Drawing on a long line of research in economics and computer science, we use the Nash social welfare as our notion of fairness. We design multi-agent variants of three classic multi-armed bandit algorithms and show that they achieve sublinear regret, which is now measured in terms of the lost
Nash social welfare. 1

Introduction
In the classical (stochastic) multi-armed bandit (MAB) problem, a principal has access to K arms and pulling arm j generates a stochastic reward for the principal from an unknown distribution Dj with an unknown mean µ∗ j . If the mean rewards were known a priori, the principal could just repeatedly pull the best arm given by arg maxj µ∗ j . However, the principal has no apriori knowledge of the quality of the arms. Hence, she uses a learning algorithm which operates in rounds, pulls arm jt in round t, observes the stochastic reward generated, and uses that information to learn the best arm over time. The performance of such an algorithm is measured in terms of its cumulative regret up to a horizon T , deﬁned as (cid:80)T jt). Note that this is the difference between the total mean reward that would have been achieved if the best arm was pulled repeatedly and the total mean reward of the arms pulled by the learning algorithm up to round T . t=1(maxj µ∗ j − µ∗
This problem can model situations where the principal is deliberating a policy decision and the arms correspond to the different alternatives she can implement. However, in many real-life scenarios, making a policy decision affects not one, but several agents. For example, imagine a company making a decision that affects all its employees, or a conference deciding the structure of its review process, which affects various research communities. This can be modeled by a multi-agent variant of the multi-armed bandit (MA-MAB) problem, in which there are N agents and pulling arm j generates a (possibly different) stochastic reward for each agent i from an unknown distribution Di,j with an unknown mean µ∗ i,j.
Before pondering about learning the “best arm” over time, we must ask what the best arm even means in this context. Indeed, the “best arm” for one agent may not be the best for another. A ﬁrst attempt may be to associate some “aggregate quality” to each arm; for example, the quality of arm 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
i µ∗ j may be deﬁned as the total mean reward it gives to all agents, i.e., (cid:80) i,j. This would nicely reduce our problem to the classic multi-armed bandit problem, for which we have an armory of available solutions [1]. However, this approach suffers from the tyranny of the majority [2]. For example, imagine a scenario with ten agents, two arms, and deterministic rewards. Suppose four agents derive a reward of 1 from the ﬁrst arm but 0 from the second, while the remaining six derive a reward of 1 from the second arm but 0 from the ﬁrst. The aforementioned approach will deem the second arm as the best and a classical MAB algorithm will converge to repeatedly pulling the second arm, thus unfairly treating the ﬁrst four agents (a minority). A solution which treats each group in a
“proportionally fair” [2] manner should ideally converge to pulling the ﬁrst arm 40% of the time and the second 60% of the time. Alternatively, we can allow the learning algorithm to “pull” a probability distribution over the arms and seek an algorithm that converges to placing probability 0.4 on the ﬁrst arm and 0.6 on the second.
This problem of making a fair collective decision when the available alternatives — in this case, probability distributions over the arms — affect multiple agents is well-studied in computational social choice [3]. The literature offers a compelling fairness notion called the Nash social welfare, named after John Nash. According to this criterion, the fairest distribution maximizes the product of the expected utilities (rewards) to the agents. A distribution p that places probability pj on each arm j gives expected utility (cid:80) i,j to agent i. Hence, the goal is to maximize NSW(p, µ∗) = (cid:81)N j=1 pj · µ∗ i,j) over p. One can verify that this approach on the aforementioned example indeed yields probability 0.4 on the ﬁrst arm and 0.6 on the second, as desired. It is also interesting to point out that with a single agent (N = 1), the distribution maximizing the Nash social welfare puts probability 1 on the best arm, thus effectively reducing the problem to the classical multi-armed bandit problem (albeit with subtle differences which we highlight in Appendix A). i=1((cid:80)K j pj · µ∗
Maximizing the Nash social welfare is often seen as a middle ground between maximizing the utilitarian social welfare (sum of utilities to the agents), which is unfair to minorities (as we observed), and maximizing the egalitarian social welfare (minimum utility to any agent), which is considered too extreme [2]. The solution maximizing the Nash social welfare is also known to satisfy other qualitative fairness desiderata across a wide variety of settings [4–11]. For example, a folklore result shows that in our setting such a solution will always lie in the core; we refer the reader to the work of
Fain et al. [4] for a formal deﬁnition of the core as well as a short derivation of this fact using the
ﬁrst-order optimality condition. For further discussion on this, see Sections 1.2 and 6.
When exactly maximizing the Nash social welfare is not possible (either due to a lack of com-plete information, as in our case, or due to computational difﬁculty), researchers have sought to achieve approximate fairness by approximately maximizing this objective [12–17]. Following this approach in our problem, we deﬁne the (cumulative) regret of an algorithm at horizon T as (cid:80)T t=1(maxp NSW(p, µ∗) − NSW(pt, µ∗)), where pt is the distribution selected in round t. Our goal in this paper is to design algorithms whose regret is sublinear in T . 1.1 Our Results
We consider three classic algorithms for the multi-armed bandit problem: Explore-First, Epsilon-Greedy, and UCB [1]. All three algorithms attempt to balance exploration (pulling arms only to learn their rewards) and exploitation (using the information learned so far to pull “good” arms).
Explore-First performs exploration for a number of rounds optimized as a function of T followed by exploitation in the remaining rounds to achieve regret (cid:101)O (cid:0)K 1/3T 2/3(cid:1). Epsilon-Greedy ﬂips a coin in each round to decide whether to perform exploration or exploitation and achieves the same regret bound. Its key advantage over Explore-First is that it does not need to know the horizon T upfront. UCB merges exploration and exploitation to achieve a regret bound of (cid:101)O (cid:0)K 1/2T 1/2(cid:1). Here, (cid:101)O hides log factors. Traditionally, the focus is on optimizing the exponent of T rather than that of K as the horizon T is often much larger than the number of arms K. It is known that the dependence of
UCB’s regret on T is optimal: no algorithm can achieve instance-independent o(T 1/2) regret [18].1 1In instance-independent bounds, the constant inside the big-Oh notation is not allowed to depend on the (unknown) distributions in the given instance. UCB also achieves an O(log T ) instance-dependent regret bound, which is also known to be asymptotically optimal [19]. For further discussion, see Section 6. 2
We propose natural multi-agent variants of these three algorithms. Our variants take the Nash social welfare objective into account and select a distribution over the arms in each round instead of a single arm. For Explore-First, we derive (cid:101)O (cid:0)N 2/3K 1/3T 2/3(cid:1) regret bound, which recovers the aforementioned single-agent bound with an additional factor of N 2/3. We also show that changing a parameter of the algorithm yields a regret bound of (cid:101)O (cid:0)N 1/3K 2/3T 2/3(cid:1), which offers a different tradeoff between the dependence on N and K. For Epsilon-Greedy, we recover the same two regret bounds, although the analysis becomes much more intricate. This is because, as mentioned above,
Epsilon-Greedy is a horizon-independent algorithm (i.e. it does not require apriori knowledge of T ), unlike Explore-First. For UCB, we derive (cid:101)O (cid:0)N KT 1/2(cid:1) and (cid:101)O regret bounds; our dependence on K worsens compared to the classical single-agent case, but importantly, we recover the same
T dependence, which is provably optimal (see Appendix A). (cid:16)
N 1/2K 3 2 T 1/2(cid:17)
√
Deriving these regret bounds for the multi-agent case requires overcoming two key difﬁculties that do not appear in the single-agent case. First, our goal is to optimize a complicated function, the Nash social welfare, rather than simply selecting the best arm. This requires a Lipschitz-continuity analysis of the Nash social welfare function and the use of new tools such as the McDiarmid’s inequality which are not needed in the standard analysis. Second, the optimization is over an inﬁnite space (the set of distributions over arms) rather than over a ﬁnite space (the set of arms). Thus, certain tricks such as a simple union bound no longer work; we use the concept of δ-covering, used heavily in the
Lipschitz bandit framework [20], in order to address this.
Our contributions are twofold. Conceptually, we promote a multi-agent viewpoint that requires striking a tradeoff between multiple reward functions; this can be applied to other classical single-agent problems. More technically, as explained below, the existing MAB literature does not provide a
√
T bound for the Nash social welfare objective. Our regret bounds may be generalizable to a broader
T bound was previously unknown. family of objectives for which a
√ 1.2