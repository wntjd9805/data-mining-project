Abstract
We study a class of classiﬁcation problems best exempliﬁed by the bank loan problem, where a lender decides whether or not to issue a loan. The lender only observes whether a customer will repay a loan if the loan is issued to begin with, and thus modeled decisions affect what data is available to the lender for future decisions. As a result, it is possible for the lender’s algorithm to “get stuck” with a self-fulﬁlling model. This model never corrects its false negatives, since it never sees the true label for rejected data, thus accumulating inﬁnite regret.
In the case of linear models, this issue can be addressed by adding optimism directly into the model predictions. However, there are few methods that extend to the function approximation case using Deep Neural Networks. We present Pseudo-Label Optimism (PLOT), a conceptually and computationally simple method for this setting applicable to DNNs. PLOT adds an optimistic label to the subset of decision points the current model is deciding on, trains the model on all data so far (including these points along with their optimistic labels), and ﬁnally uses the resulting optimistic model for decision making. PLOT achieves competitive performance on a set of three challenging benchmark problems, requiring minimal hyperparameter tuning. We also show that PLOT satisﬁes a logarithmic regret guarantee, under a Lipschitz and logistic mean label model, and under a separability condition on the data. 1

Introduction
Binary classiﬁcation models are used for online decision-making in a wide variety of practical settings.
These settings include bank lending [44, 22, 42], criminal recidivism prediction [43, 45, 5], credit card fraud [7, 41], spam detection [20, 39], self-driving motion planning [35, 28], and recommendation systems [36, 10, 18]. In many of these tasks, the model only receives the true labels for examples accepted by the learner. We refer to this class of online learning problem as the bank loan problem (BLP), motivated by the following prototypical task. A learner interacts with an online sequence of loan applicants. At the beginning of each time-step the learner receives features describing a loan applicant, from which the learner decides whether or not to issue the requested loan. If the learner decides to issue the loan, the learner, at some point in the future, observes whether or not the applicant repays the loan. If the loan is not issued, no further information is observed by the learner.
The process is then repeated in the subsequent time-steps. The learner’s objective is to maximize its reward by handing out loans to as many applicants that can repay them and deny loans to applicants unable to pay them back.
The BLP can be viewed as a contextual bandit problem with two actions: accepting/rejecting a loan application. Rejection carries a ﬁxed, known, reward of 0: with no loan issued, there is no dependency on the applicant. If in contrast the learner accepts a loan application, it receives a reward of 1 in case the loan is repaid and suffers a loss of −1 if the loan is not repaid. The probabilities of repayment
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
associated to any individual are not known in advance, thus the learner is required to build an accurate predictive model of these while ensuring not too many loans are handed out to individuals who can’t repay them and not too many loans are denied to individuals that can repay. This task can become tricky since the samples used to train the model, which govern the model’s future decisions, can suffer from bias as they are the result of past predictions from a potentially incompletely trained model. In the BLP setting, a model can get stuck in a self-fulﬁlling false rejection loop, in which the very samples that could correct the erroneous model never enter the training data in the ﬁrst place because they are being rejected by the model.
Existing contextual bandit approaches typically assume a known parametric form on the reward function. With restrictions on the reward function, a variety of methods ([13, 8]) introduce strong theoretical guarantees and empirical results, both in the linear and generalized linear model settings2.
These methods often make use of end-to-end optimism, incorporating uncertainty about the reward function in both the reward model and decision criteria.
In practice, however, deep neural networks (DNNs) are often used to learn the binary classiﬁcation model [38, 47], presenting to us a scenario that is vastly richer than the linear and generalized linear model assumptions of many contextual bandit works. In the static setting these methods have achieved effective practical performance, and in the case of [47], theoretical guarantees. A large class of these methods use two components: a feature extractor, and a post-hoc exploration strategy fed by the feature extractor. These methods leave the feature extractor itself open to bias, with the limited post-hoc exploration strategy. Another class of methods incorporate uncertainty into the neural network feature extractor ([32]), building on the vast literature on uncertainty estimation in neural networks.
We introduce an algorithm, PLOT (see Algorithm 1), which explicitly trains DNNs to make optimistic online decisions for the BLP, by incorporating optimism in both representation learning and decision making. The intuition behind PLOT’s optimistic optimization procedure is as follows:
“If I trained a model with the query point having a positive label, would it predict it to be positive?"
If the answer to this question is yes, PLOT would accept this point. To achieve this, at each time step, the PLOT algorithm re-trains its base model, treating the new candidate points as if they had already been accepted and temporarily adds them to the existing dataset with positive pseudo-labels.
PLOT’s accept and reject decisions are based on the predictions from this optimistically retrained base model. The addition of the fake pseudo-labeled points prevents the emergence of self-fulﬁlling false negatives. In contrast to false rejections, any false accepts introduced by the pseudo-labels are self-correcting. Once the model has (optimistically) accepted enough similar data points and obtained their true, negative label, these will overrule the impact of the optimistic label and result in a reject for novel queries.
While conceptually and computationally simple, we empirically show that PLOT obtains competitive performance across a set of 3 different benchmark problems in the BLP domain. With minimal hyperparameter tuning, it matches or outperforms greedy, (cid:15)-greedy (with a decaying schedule) and state-of-the-art methods from the literature such as NeuralUCB[47]. Furthermore, our analysis shows that PLOT is 3-5 times more likely to accept a data point that the current model rejects if the data point is indeed a true accept, compared to a true reject. 1.1