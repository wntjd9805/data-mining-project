Abstract
Despite the potential of reinforcement learning (RL) for building general-purpose robotic systems, training RL agents to solve robotics tasks still remains challenging due to the difﬁculty of exploration in purely continuous action spaces. Addressing this problem is an active area of research with the majority of focus on improving
RL methods via better optimization or more efﬁcient exploration. An alternate but important component to consider improving is the interface of the RL algorithm with the robot. In this work, we manually specify a library of robot action primitives (RAPS), parameterized with arguments that are learned by an RL policy. These parameterized primitives are expressive, simple to implement, enable efﬁcient exploration and can be transferred across robots, tasks and environments. We perform a thorough empirical study across challenging tasks in three distinct domains with image input and a sparse terminal reward. We ﬁnd that our simple change to the action interface substantially improves both the learning efﬁciency and task performance irrespective of the underlying RL algorithm, signiﬁcantly outperforming prior methods which learn skills from ofﬂine expert data. Code and videos at https://mihdalal.github.io/raps/ 1

Introduction
Meaningful exploration remains a challenge for robotic reinforcement learning systems. For example, in the manipulation tasks shown in Figure 1, useful exploration might correspond to picking up and placing objects in different conﬁgurations. However, random motions in the robot’s joint space will rarely, if ever, result in the robot touching the objects, let alone pick them up. Recent work, on the other hand, has demonstrated remarkable success in training RL agents to solve manipulation tasks
[4, 24, 26] by sidestepping the exploration problem with careful engineering. Levine et al. [26] use densely shaped rewards, while Kalashnikov et al. [24] leverage a large scale robot infrastructure and
Andrychowicz et al. [4] require training in simulation with engineered reward functions in order to transfer to the real world. In general, RL methods can be prohibitively data inefﬁcient, require careful reward development to learn, and struggle to scale to more complex tasks without the aid of human demonstrations or carefully designed simulation setups.
An alternative view on why RL is difﬁcult for robotics is that it requires the agent to learn both what to do in order to achieve the task and how to control the robot to execute the desired motions.
For example, in the kitchen environment featured at the bottom of Figure 1, the agent would have to learn how to accurately manipulate the arm to reach different locations as well as how to grasp different objects, while also ascertaining what object it has to grasp and where to move it. Considered independently, the problems of controlling a robot arm to execute particular motions and ﬁguring out the desired task from scalar reward feedback, then achieving it, are non-trivial. Jointly learning to solve both problems makes the task signiﬁcantly more difﬁcult.
†Equal advising 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Visual depiction of RAPS, outlining the process of how a primitive is executed on a robot. Given an input image, the policy outputs a distribution over primitives and a distribution over all the arguments of all primitives, samples a primitive and selects its corresponding argument distribution parameters, indexed by which primitive was chosen, samples an argument from that distribution and executes a controller in a feedback loop on the robot for a ﬁxed number of timesteps (Hk) to reach a new state. We show an example sequence of executing the lift primitive after having grasped the kettle in the Kitchen environment. The agent observes the initial (0) and ﬁnal states (Hk) and receives a reward equal to the reward accumulated when executing the primitive. Below we visualize representative tasks from the three environment suites that we evaluate on.
In contrast to training RL agents on raw actions such as torques or delta positions, a common strategy is to decompose the agent action space into higher (i.e., what) and lower (i.e., how) level structures.
A number of existing methods have focused on designing or learning this structure, from manually architecting and ﬁne-tuning action hierarchies [14, 27, 32, 47], to organizing agent trajectories into distinct skills [3, 20, 41, 50] to more recent work on leveraging large ofﬂine datasets in order to learn skill libraries [29, 40]. While these methods have shown success in certain settings, many of them are either too sample inefﬁcient, do not scale well to more complex domains, or lack generality due to dependence on task relevant data.
In this work, we investigate the following question: instead of learning low-level primitives, what if we were to design primitives with minimal human effort, enable their expressiveness by parameterizing them with arguments and learn to control them with a high-level policy? Such primitives have been studied extensively in task and motion planning (TAMP) literature [22] and implemented as parameterized actions [19] in RL. We apply primitive robot motions to redeﬁne the policy-robot interface in the context of robotic reinforcement learning. These primitives include manually deﬁned behaviors such as lift, push, top-grasp, and many others. The behavior of these primitives is parameterized by arguments that are the learned outputs of a policy network. For instance, top-grasp is parameterized by four scalar values: grasp position (x,y), how much to move down (z) and the degree to which the gripper should close. We call this application of parameterized behaviors, Robot
Action Primitives for RL (RAPS). A crucial point to note is that these parameterized actions are easy to design, need only be deﬁned once and can be re-used without modiﬁcation across tasks.
The main contribution of this work is to support the effectiveness of RAPS via a thorough empirical evaluation across several dimensions:
• How do parameterized primitives compare to other forms of action parameterization?
• How does RAPS compare to prior methods that learn skills from ofﬂine expert data?
• Is RAPS agnostic to the underlying RL algorithm?
• Can we stitch the primitives to perform multiple complex manipulation tasks in sequence?
• Does RAPS accelerate exploration even in the absence of extrinsic rewards?
We investigate these questions across complex manipulation environments including Kitchen Suite,
Metaworld and Robosuite domains. We ﬁnd that a simple parameterized action based approach outperforms prior state-of-the-art by a signiﬁcant margin across most of these settings2. 2Please view our website for performance videos and links to our code: https://mihdalal.github.io/raps/ 2
2