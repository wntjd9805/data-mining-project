Abstract
Achieving transferability of targeted attacks is reputed to be remarkably difﬁcult.
The current state of the art has resorted to resource-intensive solutions that necessi-tate training model(s) for each target class with additional data. In our investigation, we ﬁnd, however, that simple transferable attacks which require neither model training nor additional data can achieve surprisingly strong targeted transferability.
This insight has been overlooked until now, mainly because the widespread practice of attacking with only few iterations has largely limited the attack convergence to optimal targeted transferability. In particular, we, for the ﬁrst time, identify that a very simple logit loss can largely surpass the commonly adopted cross-entropy loss, and yield even better results than the resource-intensive state of the art.
Our analysis spans a variety of transfer scenarios, especially including three new, realistic scenarios: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API. Results in these new transfer scenarios demonstrate that the commonly adopted, easy scenarios cannot fully reveal the actual strength of different attacks and may cause misleading comparative results. We also show the usefulness of the simple logit loss for generating targeted universal adver-sarial perturbations in a data-free manner. Overall, the aim of our analysis is to inspire a more meaningful evaluation on targeted transferability. Code is available at https://github.com/ZhengyuZhao/Targeted-Tansfer. 1

Introduction
Deep neural networks have achieved remarkable performance in various machine learning tasks, but are known to be vulnerable to adversarial attacks [1]. A key property of adversarial attacks that makes them critical in realistic, black-box scenarios is their transferability [2, 3]. Current work on adversarial transferability has achieved great success for non-targeted attacks [4–12], while several initial attempts [3, 4, 13] at targeted transferability have shown its extreme difﬁculty. Targeted transferability is known to be much more challenging and worth exploring since it can raise more critical concerns by fooling models into predicting a chosen, highly dangerous target class.
However, so far state-of-the-art results can only be secured by resource-intensive transferable attacks [14–16]. Speciﬁcally, the FDA approach [14, 15] is based on modeling layer-wise feature distributions by training target-class-speciﬁc auxiliary classiﬁers on large-scale labeled data, and then optimizing adversarial perturbations using these auxiliary classiﬁers from across the deep feature space. The TTP approach [16] is based on training target-class-speciﬁc Generative Adversarial
Networks (GANs) through global and local distribution matching, and then using the trained generator to directly generate perturbations on any given input image.
In this paper, we take a second, thorough look at current research on targeted transferability. Our main contribution is the ﬁnding that simple transferable attacks [4, 6, 8] that require neither model 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
training nor additional data can actually achieve surprisingly strong targeted transferability. We argue that this insight has been overlooked mainly because current research has unreasonably restricted the attack convergence by only using a small number of iterations (see detailed discussion in Section 3).
Another key contribution of our work is, for the ﬁrst time, demonstrating the general superiority of a very simple logit loss, which even outperforms the resource-intensive state of the art.
In order to validate the general effectiveness of simple transferable attacks, in Section 4.1, we con-duct extensive experiments in a wide range of transfer scenarios. We test the commonly adopted single-model and ensemble transfer scenarios, but also introduce three new scenarios that are more challenging and realistic: an ensemble transfer scenario with little model similarity, a worse-case scenario with low-ranked target classes, and also a real-world attack on the Google Cloud Vision API.
Experimental results in these new scenarios suggest that evaluation in only the commonly adopted, easy scenarios cannot reveal the actual strength of different attacks, and may cause misleading comparative results. Additional experiments in Section 4.2 have shown the better performance of the simple transferable attacks than the state-of-the-art resource-intensive approaches. Finally, in Sec-tion 4.3, inspired by the observation that the generated perturbations themselves reﬂect speciﬁc target semantics, we use the simple Logit attack to generate targeted Universal Adversarial Perturbations (UAPs) in a data-free manner. In contrast, recent advances in targeted UAPs [16–19] have inevitably relied on large-scale optimization over additional data.
Overall, we hope our analysis of the weakness of commonly adopted attack settings and transfer scenarios will inspire a more meaningful evaluation on targeted transferability. 2