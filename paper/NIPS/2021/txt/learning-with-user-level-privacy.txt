Abstract
We propose and analyze algorithms to solve a range of learning tasks under user-level differential privacy constraints. Rather than guaranteeing only the privacy of individual samples, user-level DP protects a user’s entire contribution (m ≥ 1 samples), providing more stringent but more realistic protection against informa-tion leaks. We show that for high-dimensional mean estimation, empirical risk minimization with smooth losses, stochastic convex optimization, and learning hy-pothesis classes with ﬁnite metric entropy, the privacy cost decreases as O(1/ m) as users provide more samples. In contrast, when increasing the number of users n, the privacy cost decreases at a faster O(1/n) rate. We complement these results with lower bounds showing the minimax optimality of our algorithms for mean estimation and stochastic convex optimization. Our algorithms rely on novel tech-niques for private mean estimation in arbitrary dimension with error scaling as the concentration radius τ of the distribution rather than the entire range.
√ 1

Introduction
Releasing seemingly innocuous functions of a data set can easily compromise the privacy of in-dividuals, whether the functions are simple counts [35] or complex machine learning models like deep neural networks [52, 30]. To protect against such leaks, Dwork et al. proposed the notion of differential privacy (DP). Given some data from n participants in a study, we say that a statistic of the data is differentially private if an attacker who already knows the data of n − 1 participants cannot reliably determine from the statistic whether the n-th remaining participant is Alice or Bob. With the recent explosion of publicly available data, progress in machine learning, and widespread public release of machine learning models and other statistical inferences, differential privacy has become an important standard and is widely adopted by both industry and government [32, 5, 21, 55].
The standard setting of DP described in [22] assumes that each participant contributes a single data point to the dataset, and preserves privacy by “noising” the output in a way that is commensurate with the maximum contribution of a single example. This is not the situation faced in many applications of machine learning models, where users often contribute multiple samples to the model—for example, when language and image recognition models are trained on the users’ own data, or in federated learning settings [37]. As a result, current techniques either provide privacy guarantees that degrade with a user’s increased participation or naively add a substantial amount of noise, relying on the group property of differential privacy, which signiﬁcantly harms the performance of the deployed model.
To remedy this issue, we consider user-level DP, which instead of guaranteeing privacy for individual samples, protects a user’s entire contribution (m ≥ 1 samples). This is a more stringent but more realistic privacy desideratum. To hold, it requires that the output of our algorithm does not signiﬁcantly
∗Equal contribution. Work was done during an internship at Google Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
change when changing user’s entire contribution—i.e. possibly swapping up to m samples in total.
We make this formal in Deﬁnition 1. Very recently, for the reasons outlined above, there has been increasing interest in user-level DP for applications such as estimating discrete distributions under user-level privacy constraints [46], PAC learning with user-level privacy [31], and bounding user contributions in ML models [4, 26]. Differentially private SQL with bounded user contributions was proposed in [59]. User-level privacy has been also studied in the context of learning models via federated learning [49, 48, 58, 6].
In this paper, we tackle the problem of learning with user-level privacy in the central model of DP.
In particular, we provide algorithms and analyses for the tasks of mean estimation, empirical risk minimization (ERM), stochastic convex optimization (SCO), and learning hypothesis classes with
ﬁnite metric entropy. Our utility analyses assume that all users draw their samples i.i.d. from related distributions, a setting we refer to as limited heterogeneity. On these tasks, naively applying standard mechanisms, such as Laplace or Gaussian, or using the group property with item-level DP estimators, both yield a privacy error independent of m. We ﬁrst develop novel private mean estimators in high dimension with statistical and privacy error scaling with the (arbitrary) concentration radius rather than the range, and apply these to the statistical query setting [SQ; 41]. Our algorithms then rely on (privately) answering a sequence of adaptively chosen queries using users’ samples, e.g., gradient queries in stochastic gradient descent algorithms. We show that for these tasks, the additional error m), contrasting with the naive rate—independent of due to privacy constraints decreases as O(1/ m. Interestingly, increasing n, the number of users, decreases the privacy cost at a faster O(1/n) rate.
√
Importantly, our results imply concrete practical recommendations on sample collection, regardless of the level of heterogeneity. Indeed, increasing m will yield the most value in the i.i.d. setting and will yield no improvement when the users’ distributions are arbitrary. As the real-world will lie somewhere in between, our results exhibit a regime where, for any heterogeneity, it is strictly better to collect more users (increasing n) than more samples per user (increasing m). 1.1 Our Contributions and