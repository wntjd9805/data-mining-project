Abstract
Structured non-convex learning problems, for which critical points have favorable statistical properties, arise frequently in statistical machine learning. Algorithmic convergence and statistical estimation rates are well-understood for such problems.
However, quantifying the uncertainty associated with the underlying training al-gorithm is not well-studied in the non-convex setting. In order to address this short-coming, in this work, we establish an asymptotic normality result for the constant step size stochastic gradient descent (SGD) algorithm—a widely used algorithm in practice. Speciﬁcally, based on the relationship between SGD and
Markov Chains [1], we show that the average of SGD iterates is asymptotically nor-mally distributed around the expected value of their unique invariant distribution, as long as the non-convex and non-smooth objective function satisﬁes a dissipa-tivity property. We also characterize the bias between this expected value and the critical points of the objective function under various local regularity conditions.
Together, the above two results could be leveraged to construct conﬁdence intervals for non-convex problems that are trained using the SGD algorithm. 1

Introduction
Non-convex learning problems are prevalent in modern statistical machine learning applications such as matrix and tensor decomposition [2, 3, 4, 5, 6, 7, 8], deep neural networks [9, 10, 11], and robust empirical risk minimization [12, 13, 14]. Developing theoretically principled approaches for tackling such non-convex problems depends critically on the interplay between two aspects.
From a computational perspective, variants of stochastic gradient descent (SGD) converge to ﬁrst-order critical points [15, 16] or local minimizers [17, 2, 18, 19] of the objective function. From a statistical perspective, oftentimes these critical points or local minimizers have nice statistical properties [20, 3, 12, 21, 22, 5]; see also [23] for a counterexample. For the purpose of uncertainty quantiﬁcation in such non-convex settings, studying the ﬂuctuations of iterative algorithms used for training becomes extremely important. In this work, we focus on the widely used constant step size
SGD, and develop results for quantifying the uncertainty associated with this algorithm for a class of non-convex problems.
We consider minimizing a non-smooth and non-convex objective function f : Rd → R, min
θ∈Rd f (θ) . (1) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The iterations of SGD with a constant step size η > 0, initialized at θ(η) 0 ≡ θ0 ∈ Rd, are given by k+1 = θ(η)
θ(η) k − η(cid:0)∇f (θ(η) k ) + ξk+1(θ(η) k )(cid:1), k ≥ 0 , (2) where {ξk}k≥1 is a sequence of random functions from Rd to Rd corresponding to the stochasticity in the gradient estimate. Several problems in machine learning and statistics are naturally formulated as the optimization problem in (1), where the function f (θ) is given by f (θ) := (cid:82) F (θ, Z) dP (Z) , (3) for an unknown distribution over the random variable Z ∈ Rp. The function F (θ, Z) is typically the loss function composed with functions from the hypothesis class parametrized by θ ∈ Rd. In online SGD with batch size b, at each iteration k, b independent samples Zj ∼ P (Z) are used to estimate the true gradient with 1 k , Zj). The above iterates are indeed a special case b of the iterates in (2), with the noise sequence {ξk+1(θ(η) j=1 ∇F (θ(η) (cid:80)b
ξk+1(θ(η) k ) := 1 b (cid:80)b j=1 k )}k≥0 given by (cid:105) k , Zj) − ∇f (θ(η) k ) (cid:104)
∇F (θ(η)
. (4)
Although proposed in the 1950s by [24], SGD has been the algorithm of choice for training statistical models due to its simplicity, and superior performance in large-scale settings [25, 1, 26, 27]. However, the ﬂuctuations of this algorithm is well-understood only when the objective function f is strongly convex and smooth, and the step size η satisﬁes a speciﬁc decreasing schedule so that the iterates asymptotically converge to the unique minimizer [28, 29, 30]. On the other hand, it is well-known that the SGD iterates in (2) can be viewed as a Markov chain which allows them to converge to a random vector rather than a single critical point [1]. Building on this analogy between SGD and Markov chains, the aforementioned shortcomings can be alleviated by simply relaxing the global smoothness as well as the strong convexity assumptions to the tails of the objective function f , which allows for a
ﬂexible non-convex structure around the region of interest. Similar kinds of tail relaxations have been successfully employed in the diffusion theory when the target potential is non-convex [31, 32, 33], but they are not studied in the context of non-convex optimization when the algorithm is SGD. In this work, we study the ﬂuctuations and the bias of the averaged SGD iterates in (2), around the ﬁrst-order critical points of the minimization problem (1). Our contributions can be summarized as follows.
• For a non-convex and non-smooth objective function f with tails growing at least quadratically, we establish the uniqueness of the stationary distribution of the constant step size SGD iterates in Proposition 2.1, and the asymptotic normality of Polyak-Ruppert averaging in Theorem 2.1.
To the best of our knowledge, these are the ﬁrst uniqueness and normality results for the SGD algorithm when the objective function is non-convex (even not strongly convex) and non-smooth.
• We further show in Theorems 3.1 and 3.2 that, with additional local smoothness assumptions on the non-convex objective function f , we can establish a control over the bias in terms of the step size. We further characterize the bias when the objective is (not strongly) convex in Theorem 3.3, providing a thorough bias analysis for the constant step size SGD under various settings that are frequently encountered in statistical learning.
Our results provide algorithm-dependent guarantees for uncertainty quantiﬁcation, and they could be leveraged to obtain conﬁdence intervals (CIs) for non-convex and non-smooth learning problems.
This is contrary to the majority of the existing results in statistics, which only establish normality results for the true stationary point of the non-convex objective function; see for example [12, 34].
While being useful, such results completely ignore the computational hardships associated with non-convex optimization; hence, their practical implications are limited. On the other hand, in the optimization and learning theory literature, a majority of the existing results establish the rate of convergence of an algorithm to a critical point, and do not quantify the ﬂuctuations associated with that algorithm. Our work bridges these separate lines of thought by providing asymptotic normality results directly for the SGD algorithm used for minimizing non-convex and non-smooth functions.
More