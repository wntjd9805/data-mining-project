Abstract
Certiﬁed robustness is a desirable property for deep neural networks in safety-critical applications, and popular training algorithms can certify robustness of a neural network by computing a global bound on its Lipschitz constant. However, such a bound is often loose: it tends to over-regularize the neural network and de-grade its natural accuracy. A tighter Lipschitz bound may provide a better tradeoff between natural and certiﬁed accuracy, but is generally hard to compute exactly due to non-convexity of the network. In this work, we propose an efﬁcient and trainable local Lipschitz upper bound by considering the interactions between activation functions (e.g. ReLU) and weight matrices. Speciﬁcally, when comput-ing the induced norm of a weight matrix, we eliminate the corresponding rows and columns where the activation function is guaranteed to be a constant in the neighborhood of each given data point, which provides a provably tighter bound than the global Lipschitz constant of the neural network. Our method can be used as a plug-in module to tighten the Lipschitz bound in many certiﬁable training algorithms. Furthermore, we propose to clip activation functions (e.g., ReLU and
MaxMin) with a learnable upper threshold and a sparsity loss to assist the network to achieve an even tighter local Lipschitz bound. Experimentally, we show that our method consistently outperforms state-of-the-art methods in both clean and certiﬁed accuracy on MNIST, CIFAR-10 and TinyImageNet datasets with various network architectures. 1

Introduction
With the ever-growing deployment of deep neural networks, formal robustness guarantees are needed in many safety-critical applications. Strategies to improve robustness such as adversarial training only provide empirical robustness, without formal guarantees, and many existing adversarial defenses have been successfully broken using stronger attacks [1]. In contrast, certiﬁed defenses give formal robustness guarantees that any norm-bounded adversary cannot alter the prediction of a given network.
Bounding the global Lipschitz constant of a neural network is a computationally efﬁcient and scalable approach to provide certiﬁable robustness guarantees [2–4]. The global Lipschitz bound is typically computed as the product of the spectral norm of each layer. However, this bound can be quite loose because it needs to hold for all points from the input domain, including those inputs that are far away from each other. Training a network while constraining this loose bound often imposes to high a degree of regularization and reduces network capacity. It leads to considerably lower clean accuracy in certiﬁed training compared to standard and adversarial training [5, 6].
A local Lipschitz constant, on the other hand, bounds the norm of output perturbation only for inputs from a small region, usually selected as a neighborhood around each data point. It produces a 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
ReLU outputs under perturbation
"
#
Varying
]
Constant
!#
!!
Constant
[           ]
)!
LB1
!"
[                 ]
)"
[
UB1 LB2
UB2
+ ≤
#
[           ]
)#
LB3
UB3
!
)!
)"
)#
)$ − ) ≤ 0 perturb
≤ 0
LB! ≤ )!
LB" ≤ )"
LB# ≤ )#
$ ≤ UB!
$ ≤ UB"
$ ≤ UB#
!!
!"
!# 0
$
!"
+
= ReLU
'!! '!" '!#	
'"! '"" '"#	 	'#!	 '#" '##
)!
)"
)#
= ReLU
'!! '!" '!#	
'"! '"" '"#	 	'#!	 '#" '##
$
)!
$
)"
$
)#
+ 0 0
+
Local Lipschitz bound at )
∆! ≤ '"! '"" '"# ∆)
Figure 1: Illustration of tighter (local) Lipschitz constant with bounded ReLU. tighter bound by considering the geometry in a local region and often yields much better robustness certiﬁcation [7, 8]. Unfortunately, computing the exact local Lipschitz constant is NP-complete
[9]. Obtaining reasonably tight local Lipschitz bounds via semideﬁnite programming [10] or mixed integer programming [11] is typically only applicable to small, previously trained networks since it is difﬁcult to parallelize the optimization solver and make it differentiable for training. On the other hand, many existing certiﬁed defense methods have achieved success by using a training-based approach with a relatively weak but efﬁcient bound [12–14]. Therefore, to incorporate local Lipschitz bound in training, a computationally efﬁcient and training-friendly method must be developed.
Our contributions: We propose an efﬁcient method to incorporate a local Lipschitz bound in training deep networks, by considering the interactions between an activation layer such as a Rectiﬁed
Linear Unit (ReLU) layer and a linear (or convolution) layer. Our bound is computed for each data point and this translates to different types of outputs from the activation function: constant or varying under input perturbations. If the outputs of some activation neurons are constant under local perturbation, we eliminate the corresponding rows in the previous weight layer and the corresponding columns in the next weight layer, and then compute the spectral norm of the reduced matrix.
Our main insight is to use training to make the proposed local Lipschitz bound tight. This is different from existing works that ﬁnd local Lipschitz bound for a ﬁxed network [8, 10, 11]. Instead, we aim to enable a network to learn to tighten our proposed local Lipschitz bound during training. To achieve this, we propose to clip the activation function with an individually learnable threshold ✓. Take ReLU for example, the output of a “clipped” ReLU becomes a constant when input is greater than this threshold (see Figure 1). Once the input of the ReLU is greater than the threshold or less than 0, then this ReLU neuron does not contribute to the local Lipschitz constant, and thus the corresponding row or column of weight matrices can be removed. We also apply this method to non-ReLU activation functions such as MaxMin [15] to create constant output regions. Additionally, we also use a hinge loss function to encourage more neurons to have constant outputs. Our method can be used as a plug-in module in existing certiﬁable training algorithms that involve computing Lipschitz bound.
Our contributions can be summarized as:
• To the best of our knowledge, we are the ﬁrst to incorporate a local Lipschitz bound during training for certiﬁed robustness. Our bound is provably tighter than the global Lipschitz bound and is also computationally efﬁcient for training.
• We propose to use activation functions with learnable threshold to encourage more ﬁxed neurons during training, which assists the network to learn to tighten our bound. We show that more than 45% rows and columns can be removed from weight matrices.
• We consistently outperform state-of-the-art Lipschitz based certiﬁed defense methods for `2 norm robustness. On CIFAR-10 with perturbation ✏ = 36 255 , we obtain 54.3% veriﬁed accuracy with
ReLU activation function and 60.7% accuracy with MaxMin [15], outperforming the SOTA baselines, and also achieve better clean accuracy. Our code is available at https://github. com/yjhuangcd/local-lipschitz. 2