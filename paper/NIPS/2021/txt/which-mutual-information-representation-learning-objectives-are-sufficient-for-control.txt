Abstract
Mutual information (MI) maximization provides an appealing formalism for learn-ing representations of data. In the context of reinforcement learning (RL), such representations can accelerate learning by discarding irrelevant and redundant in-formation, while retaining the information necessary for control. Much prior work on these methods has addressed the practical difﬁculties of estimating MI from samples of high-dimensional observations, while comparatively less is understood about which MI objectives yield representations that are sufﬁcient for RL from a theoretical perspective. In this paper, we formalize the sufﬁciency of a state representation for learning and representing the optimal policy, and study several popular MI based objectives through this lens. Surprisingly, we ﬁnd that two of these objectives can yield insufﬁcient representations given mild and common as-sumptions on the structure of the MDP. We corroborate our theoretical results with empirical experiments on a simulated game environment with visual observations. 1

Introduction
Deep reinforcement learning (RL) algorithms are in principle capable of learning policies from high-dimensional observations, such as camera images [49, 39, 32]. However, policy learning in practice faces a bottleneck in acquiring useful representations of the observation space [58]. State representation learning approaches aim to remedy this issue by learning structured and compact representations on which to perform RL. A useful state representation should be sufﬁcient to learn and represent the optimal policy or the optimal value function, while discarding irrelevant and redundant information. Understanding whether or not an objective is guaranteed to yield sufﬁcient representations is important, because insufﬁcient representations make it impossible to solve certain problems. For example, an autonomous vehicle would not be able to navigate safely if its state representation did not contain information about the color of the stoplight in front of it. With the increasing interest in leveraging ofﬂine datasets to learn representations for RL [19, 35, 64], the question of sufﬁciency becomes even more important to understand if the representation is capable of representing policies and value functions for downstream tasks.
While a wide range of representation learning objectives have been proposed in the literature [41], in this paper we focus on analyzing representations learned by maximizing the mutual information (MI) between random variables. Prior work has proposed many different MI objectives involving the variables of states, actions, and rewards at different time-steps [4, 52, 53, 58]. While much prior work has focused on how to optimize these various MI objectives in high dimensions [61, 8, 52, 27], we focus instead on their ability to yield theoretically sufﬁcient representations. We ﬁnd that two commonly used objectives are insufﬁcient for the general class of MDPs, in the most general case, and prove that another typical objective is sufﬁcient. We illustrate the analysis with both didactic examples in which MI can be computed exactly and deep RL experiments in which we approximately maximize
⇤Correspondence to rakelly@eecs.berkeley.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the MI objective to learn representations of visual inputs. The experimental results corroborate our theoretical ﬁndings, and demonstrate that the sufﬁciency of a representation can have a substantial impact on the performance of an RL agent that uses that representation. This paper provides guidance to the deep RL practitioner on when and why objectives may work well or fail, and also provides a formal framework to analyze newly proposed representation learning objectives based on MI. 2