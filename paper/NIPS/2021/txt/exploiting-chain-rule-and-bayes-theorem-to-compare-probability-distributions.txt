Abstract
To measure the difference between two probability distributions, referred to as the source and target, respectively, we exploit both the chain rule and Bayes’ theorem to construct conditional transport (CT), which is constituted by both a forward component and a backward one. The forward CT is the expected cost of moving a source data point to a target one, with their joint distribution deﬁned by the product of the source probability density function (PDF) and a source-dependent conditional distribution, which is related to the target PDF via Bayes’ theorem. The backward CT is deﬁned by reversing the direction. The CT cost can be approximated by replacing the source and target PDFs with their discrete empirical distributions supported on mini-batches, making it amenable to implicit distributions and stochastic gradient descent-based optimization. When applied to train a generative model, CT is shown to strike a good balance between mode-covering and mode-seeking behaviors and strongly resist mode collapse. On a wide variety of benchmark datasets for generative modeling, substituting the default statistical distance of an existing generative adversarial network with CT is shown to consistently improve the performance. PyTorch code is provided. 1

Introduction
Measuring the difference between two probability distributions is a fundamental problem in statistics and machine learning [1–3]. A variety of statistical distances, such as the Kullback–Leibler (KL) divergence [4], Jensen–Shannon (JS) divergence [5], maximum mean discrepancy (MMD) [6], and
Wasserstein distance [7], have been proposed to quantify the difference. They have been widely used for generative modeling with different mode covering/seeking behaviors [8–13]. The KL divergence, directly related to both maximum likelihood estimation and variational inference [14–16], requires the two probability distributions to share the same support and is often inapplicable if either is an implicit distribution whose probability density function (PDF) is unknown [17–20]. Variational auto-encoders (VAEs) [8], the KL divergence based deep generative models, are stable to train, but often exhibit mode-covering behaviors in its generated data, producing blurred images. The JS divergence is directly related to the min-max loss of a generative adversarial net (GAN) when the discriminator is optimal [9], while the Wasserstein-1 distance is directly related to the min-max loss of a Wasserstein GAN [11], whose critic is optimized under the 1-Lipschitz constraint. However, it is difﬁcult to maintain a good balance between the updates of the generator and discriminator/critic, making (Wasserstein) GANs notoriously brittle to train. MMD [6] is an RKHS-based statistical distance behind MMD-GANs [10, 21, 22], which have also shown promising results in generative modeling when trained with a min-max loss. Different from VAEs, these GAN-based models often exhibit mode dropping and face the danger of mode collapse if not well tuned during the training. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we introduce conditional transport (CT) as a new method to quantify the difference between two probability distributions, which will be referred to as the source distribution pX (x) and target distribution pY (y), respectively. The construction of CT is motivated by the following observation: the difference between pX (x) and pY (y) can be reﬂected by the expected difference of two dependent random variables x and y, whose joint distribution π(x, y) is constrained by both pX (x) and pY (y) in a certain way. Denoting c(x, y) ≥ 0 as a cost function to measure the difference between points x and y, such as c(x, y) = (cid:107)x − y(cid:107)2 2, the expected difference is expressed as Eπ(x,y)[c(x, y)]. A basic way to constrain π(x, y) with both pX (x) and pY (y) is to let π(x, y) = pX (x)pY (y), which means drawing x and y independently from pX (x) and pY (y), respectively; this expected difference EpX (x)pY (y)[c(x, y)] is closely related to the energy distance [23]. Another constraining method is to require both (cid:82) π(x, y)dy = pX (x) and (cid:82) π(x, y)dx = pY (y), under which minπ{Eπ(x,y)[c(x, y)]} becomes the Wasserstein distance [7, 24–26].
A key insight of this paper is that by exploiting the chain rule and Bayes’ theorem, there exist two additional ways to constrain π(x, y) with both pX (x) and pY (y): 1) A forward CT that can be viewed as moving the source to target distribution; 2) A backward CT that reverses the direction.
Our intuition is that given a source (target) point, it is more likely to be moved to a target (source) point closer to it. More speciﬁcally, if the target distribution does not provide good coverage of the source density, then there will exist source data points that lie in low-density regions of the target, making the expected cost of the forward CT high. Therefore, we expect that minimizing the forward
CT will encourage the target distribution to exhibit a mode-covering behavior with respect to (w.r.t.) the source PDF. Reversing the direction, we expect that minimizing the backward CT will encourage the target distribution to exhibit a mode-seeking behavior w.r.t. the source PDF. Minimizing the combination of both is expected to strike a good balance between these two distinct behaviors.
To demonstrate the use of CT, we apply it to train implicit (or explicit) distributions to model both 1D and 2D toy data, MNIST digits, and natural images. The implicit distribution is deﬁned by a deep generative model (DGM) that is simple to sample from. We provide empirical evidence to show how to control the mode-covering versus mode-seeking behaviors by adjusting the ratio of the forward
CT versus backward CT. To train a DGM for natural images, we focus on adapting existing GANs, with minimal changes to their settings except for substituting the statistical distances in their loss functions with CT. We leave tailoring the network architectures and settings to CT for future study.
Modifying the loss functions of various existing DGMs with CT, our experiments show consistent improvements in not only quantitative performance and generation quality, but also learning stability.
Our code is available at https://github.com/JegZheng/CT-pytorch. 2 Chain rule and Bayes’ theorem based conditional transport
Exploiting the chain rule and Bayes’ theorem, we can constrain π(x, y) with both pX (x) and pY (y) in two different ways, leading to the forward CT and backward CT, respectively. To deﬁne the forward CT, we use the chain rule to factorize the joint distribution as
π(x, y) = pX (x)πY (y | x), where πY (y | x) is a conditional distribution of y given x. This construction ensures (cid:82) π(x, y)dy = pX (x) but not (cid:82) π(x, y)dx = pY (y). Denote dφ(h1, h2) ∈ R as a function parameterized by φ, which measures the difference between two vectors h1, h2 ∈ RH of dimension H. While allowing (cid:82) π(x, y)dx (cid:54)= pY (y), to appropriately constraint π(x, y) by pY (y), we treat pY (y) as the prior distribution, view e−dφ(x,y) as an unnormalized likelihood term, and follow Bayes’ theorem to deﬁne
πY (y | x) = e−dφ(x,y)pY (y)/Q(x), Q(x) := (cid:82) e−dφ(x,y)pY (y)dy, (1) where Q(x) is a normalization term that ensures (cid:82) πY (y | x)dy = 1. We refer to πY (y | x) as the forward “navigator,” which speciﬁes how likely a given x will be mapped to a target point y ∼ pY (y).
We now deﬁne the cost of the forward CT as
C(X → Y ) = Ex∼pX (x)Ey∼πY (· | x)[c(x, y)]. (2)
In the forward CT, we expect large c(x, y) to typically co-occur with small πY (y | x) as long as pY (y) provides a good coverage of the density of x. Thus minimizing the forward CT cost is expected to encourage pY (y) to exhibit a mode-covering behavior w.r.t. pX (x). Such kind of behavior is also 2
expected when minimizing the forward KL divergence as KL(pX ||pY ) = Ex∼pX calls for pY (x) > 0 whenever pX (x) > 0. (cid:2) ln pX (x) pY (x) (cid:3), which
Reversing the direction, we construct the backward CT, where the joint is factorized as π(x, y) = pY (y)πX (x | y) and the backward navigator is deﬁned as
πX (x | y) = e−dφ(x,y)pX (x)/Q(y), Q(y) := (cid:82) e−dφ(x,y)pX (x)dx. (3)
This ensures (cid:82) π(x, y)dx = pY (y); while allowing (cid:82) π(x, y)dy (cid:54)= pX (x), it constrains π(x, y) by treating pX (x) as the prior to construct πX (x | y). The backward CT cost is now deﬁned as
C(X ← Y ) = Ey∼pY (y)Ex∼πX (· | y)[c(x, y)].
In the backward CT, we expect large c(x, y) to typically co-occur with small πX (x | y) as long as pX (x) has good coverage of the density of y. Thus minimizing the backward CT cost is expected to encourage pY (y) to exhibit a mode-seeking behavior w.r.t. pX (x). Such kind of behavior is also (cid:3), which expected when minimizing the reverse KL divergence as KL(pY ||pX ) = Ex∼pY allows pY (x) = 0 when pX (x) > 0 and it is ﬁne for pY to just ﬁt some portion of pX . (cid:2) ln pY (x) pX (x) (4)
In comparison to the forward and revers KLs, the proposed forward and backward CT are more broadly applicable as they don’t require pX and pY to share the same distribution support and have analytic PDFs. For the cases where the KLs can be evaluated, we introduce
D(X, Y ) = KL(pX ||pY ) − KL(pY ||pX ) as a formal way to quantify the mode-seeking and mode-covering behavior of pY w.r.t. pX , with
D(X, Y ) > 0 implying mode seeking and with D(X, Y ) < 0 implying mode covering.
Combining both the forward and backward CTs, we now deﬁne the CT cost as
Cρ(X, Y ) := ρC(X → Y ) + (1 − ρ)C(X ← Y ), (5) where ρ ∈ [0, 1] is a parameter that can be adjusted to encourage pY (y) to exhibit w.r.t. pX (x) mode-seeking (ρ = 0), mode-covering (ρ = 1), or a balance of two distinct behaviors (ρ ∈ (0, 1)).
By deﬁnition we have Cρ(X, Y ) ≥ 0, where the equality can be achieved when pX = pY and the navigator parameter φ is optimized such that e−dφ(x,y) is equal to one if and only if x = y and zero otherwise. We also have Cρ=0.5(X, Y ) = Cρ=0.5(Y, X). We ﬁx ρ = 0.5 unless speciﬁed otherwise. 2.1 Conjugacy based analytic conditional distributions
Estimating the forward and backward CTs involves πY (y | x) and πX (x | y), respectively. Both conditional distributions, however, are generally intractable to evaluate and sample from, unless pX (x) and pY (y) are conjugate priors for likelihoods proportional to e−d(x,y), i.e., πX (x | y) and
πY (y | x) are in the same probability distribution family as pX (x) and pY (y), respectively. For example, if d(x, y) = (cid:107)x − y(cid:107)2 2 and both pX (x) and pY (y) are multivariate normal distributions, then both πX (x | y) and πY (y | x) will follow multivariate normal distributions.
To be more speciﬁc, we provide a univariate normal based example, with x, y, φ, θ ∈ R and pX (x) = N (0, 1), pY (y) = N (0, eθ), dφ(x, y) = (x − y)2/(2eφ), c(x, y) = (x − y)2. (6)
Here we have D(X, Y ) = KL[N (0, 1)||N (0, eθ)] − KL[N (0, eθ)||N (0, 1)] = θ − sinh(θ), which is positive when θ < 0, implying mode-seeking, and negative when θ > 0, implying mode-covering.
As shown in Appendix C, we have analytic forms of the forward and backward navigators as
πY (y | x) = N (σ(θ − φ)x, σ(θ − φ)eφ), πX (x | y) = N (σ(−φ)y, σ(φ)), where σ(a) = 1/(1 + e−a) denotes the sigmoid function, and forward and backward CT costs as
C(X → Y ) = σ(φ − θ)(eθ + σ(φ − θ)), C(X ← Y ) = σ(φ)(1 + σ(φ)eθ).
As a proof of concept, we illustrate the optimization under CT using the above example, for which
θ = 0 is the optimal solution that makes pX = pY . Thus when applying gradient descent to minimize the CT cost Cρ=0.5(X, Y ), we expect the generator parameter θ → 0 with proper learning dynamic, as long as the learning of the navigator parameter φ is appropriately controlled. This is conﬁrmed by 3
Figure 1: Illustration of minimizing the CT cost Cφ,θ(X, Y ) between N (0, 1) and N (0, eθ). Left: Evolution of CT cost, its parameters, and forward and backward costs; Right: 4 CT cost curves against θ as eφ is being optimized to a small value to jointly show the optimized φ provides better learning dynamic for the learning of θ.
Fig. 1, which shows that as the navigator φ gets optimized by minimizing CT cost, it is more obvious that θ will minimize the CT cost at zero. This suggests that the navigator parameter φ mainly plays the role in assisting the learning of θ. The right four subplots describe the log-scale curves of forward cost, backward cost and bi-directional CT costs w.r.t. θ as φ gets optimized to four different values.
It is worth noting that the forward cost is minimized at eθ > 1, which implies a mode-covering behavior, and the backward cost is minimized at eθ → 0, which implies a mode-seeking behavior, while the bi-directional cost is minimized at around the optimal solution eθ = 1; the forward CT cost exhibits a ﬂattened curve on the right hand side of its minimum, adding to which the backward CT cost not only moves that minimum left, making it closer to θ = 0, but also raises the whole curve on the right hand side, making the optimum of θ become easier to reach via gradient descent.
To apply CT in a general setting where the analytical forms of the distributions are unknown, there is no conjugacy, or we only have access to random samples from the distributions, below we show we can approximate the CT cost by replacing both pX (x) and pY (y) with their corresponding discrete empirical distributions supported on mini-batches. Minimizing this approximate CT cost, amenable to mini-batch SGD based optimization, is found to be effective in driving the target (model) distribution pY towards the source (data) distribution pX , with the ability to control the mode-seeking and mode-covering behaviors of pY w.r.t. pX . 2.2 Approximate CT given empirical samples
Below we use generative modeling as an example to show how to apply the CT cost in a general setting that only requires access to random samples of both x and y. Denote x as a data taking its value in RV . In practice, we observe a ﬁnite set X = {xi}|X | i=1, consisting of |X | data samples assumed to be iid drawn from pX (x). Given X , the usual task is to learn a distribution to approximate pX (x), for which we consider a deep generative model (DGM) deﬁned as y = Gθ((cid:15)), (cid:15) ∼ p((cid:15)), where Gθ is a generator that transforms noise (cid:15) ∼ p((cid:15)) via a deep neural network parameterized by
θ to generate random sample y ∈ RV . While the PDF of the generator, denoted as pY (y; θ), is often intractable to evaluate, it is straightforward to draw y ∼ pY (y; θ) with Gθ.
While knowing neither pX (x) nor pY (y; θ), we can obtain discrete empirical distributions p ˆXN and p ˆYM supported on mini-batches x1:N and y1:M , as deﬁned below, to guide the optimization of Gθ in an iterative manner. With N random observations sampled without replacement from X , we deﬁne p ˆXN (x) = 1
N (cid:80)N i=1 δ(x − xi), {x1, . . . , xN } ⊆ X (7) as an empirical distribution for x. Similarly, with M random samples of the generator, we deﬁne p ˆYM (y) = 1
M (cid:80)M j=1 δ(y − yj), yj = Gθ((cid:15)j), (cid:15)j iid∼ p((cid:15)) . (8)
Substituting pY (y; θ) in (2) with p ˆYM as (y), the continuous forward navigator becomes a discrete one
ˆπY (y | x) = (cid:80)M j=1 ˆπM (yj | x, φ)δyj , ˆπM (yj | x, φ) := e−dφ(x,yj )
−dφ(x,y (cid:80)M j(cid:48)=1 e j(cid:48) ) . (9)
Thus given p ˆYM
, the cost of a forward CT can be approximated as
Cφ,θ(X → ˆYM ) = E y1:M iid∼ pY (y;θ)
Ex∼pX (x) (cid:104)(cid:80)M (cid:105) j=1 c(x, yj)ˆπM (yj | x, φ)
, (10) 4
which can be interpreted as the expected cost of following the forward navigator to stochastically transport a random source point x to one of the M randomly instantiated “anchors” of the target distribution. Similar to previous analysis, we expect this approximate forward CT to stay small as long as pY (y; θ) exhibits a mode covering behavior w.r.t. pX (x).
Similarly, we can approximate the backward navigator and CT cost as
ˆπX (x | y) = (cid:80)N i=1 ˆπN (xi | y, φ)δxi , ˆπN (xi | y, φ) := e−dφ(xi,y) i(cid:48)=1 e−dφ(x (cid:80)N i(cid:48) ,y) ,
Cφ,θ( ˆXN ← Y ) = E
Ey∼pY (y;θ)
Similar to previous analysis, we expect this approximate backward CT to stay small as long as pY (y; θ) exhibits a mode-seeking behavior w.r.t. pX (x). iid∼ pX (x) (cid:105) i=1 c(xi, y)ˆπN (xi | y, φ) (cid:104)(cid:80)N (11) x1:M
.
Combining (10) and (11), we deﬁne the approximate CT cost as
Cφ,θ,ρ( ˆXN , ˆYM ) = ρCφ,θ(X → ˆYM ) + (1 − ρ)Cφ,θ( ˆXN ← Y ), (12) an unbiased sample estimate of which, given mini-batches x1:N and y1:M , can be expressed as
Lφ,θ,ρ(x1:N , y1:M ) = (cid:80)N (cid:80)M i=1
= (cid:80)N i=1 (cid:80)M j=1 c(xi, yj) (cid:0) ρ (cid:18) j=1 c(xi, yj)
ρ
N
N ˆπM (yj | xi, φ) + 1−ρ e−dφ(xi,yj )
−dφ(xi,y j(cid:48) ) + 1−ρ
M
M ˆπN (xi | yj, φ)(cid:1) (cid:19) e−dφ(xi,yj ) i(cid:48)=1 e−dφ(x i(cid:48) ,yj ) (cid:80)N (cid:80)M j(cid:48)=1 e
. (13)
Lemma 1. Approximate CT in (12) is asymptotic as limN,M→∞ Cφ,θ,ρ( ˆXN , ˆYM ) = Cφ,θ,ρ(X, Y ). 2.3 Cooperatively-trained or adversarially-trained feature encoder
To apply CT for generative modeling of high-dimensional data, such as natural images, we need to deﬁne an appropriate cost function c(x, y) to measure the difference between two random points. A naive choice is some distance between their raw feature vectors, such as c(x, y) = (cid:107)x − y(cid:107)2 2, which, however, is known to often poorly reﬂect the difference between high-dimensional data residing on low-dimensional manifolds. For this reason, with cosine similarity
, we further introduce a feature encoder Tη(·), parameterized
[27] as cos(h1, h2) := hT by η, to help redeﬁne the point-to-point cost and both navigators as (cid:16) Tη(x) (cid:107)Tη(x)(cid:107) , Tη(y) cη(x, y) = 1 − cos(Tη(x), Tη(y)), dφ hT 1 h1 (cid:107)Tη(y)(cid:107) (14) 1 h2
√ 2 h2
√ hT (cid:17)
.
To apply the CT cost to train a DGM, we ﬁnd that the feature encoder Tη(·) can be learned in two different ways: 1) Cooperatively-trained: Training them cooperatively by alternating between two different losses: training the generator under a ﬁxed Tη(·) with the CT loss, and training Tη(·) under a ﬁxed generator with a different loss, such as the GAN discriminator loss, WGAN critic loss, and
MMD-GAN [10] critic loss. 2) Adversarially-trained: Viewing the feature encoder as a critic and training it to maximize the CT cost, by not only inﬂating the point-to-point cost, but also distorting the feature space used to construct the forward and backward navigators’ conditional distributions.
To be more speciﬁc, below we present the details for the adversarial way to train Tη. Given training data X , to train the generator Gθ, forward navigator πφ(y | x), backward navigator πφ(x | y), and encoder Tη, we view the encoder as a critic and propose to solve a min-max problem as min
φ,θ max
η
E x1:N ⊆X , (cid:15)1:M
[Lφ,θ,ρ,η(x1:N , {Gθ((cid:15)j)}M j=1)], iid∼ p((cid:15)) (15) where Lφ,θ,ρ,η is deﬁned the same as in (13), except that we replace c(xi, yj) and dφ(·, ·) with their corresponding ones shown in (14) and use reparameterization in (8) to draw y1:M := {Gθ((cid:15)j)}M j=1.
With SGD, we update φ and θ using ∇φ,θLφ,θ,ρ,η(x1:N , {Gθ((cid:15)j)}M j=1)) and, if the feature encoder is adversarially-trained, update η using −∇ηLφ,θ,ρ,η(x1:N , {Gθ((cid:15)j)}M
We ﬁnd by experiments that both ways to learn the encoder work well, with the adversarial one generally providing better performance.
It is worth noting that in (Wasserstein) GANs, while the adversarially-trained discriminator/critic plays a similar role as a feature encoder, the learning dynamics between the discriminator/critic and generator need to be carefully tuned to maintain training stability and prevent trivial solutions (e.g., mode collapse). By contrast, the feature encoder of the CT cost based DGM can be stably trained in two different ways. Its update does not need to be well synchronized with the generator and can be stopped at any time of the training. j=1)). 5
3