Abstract
Neural network compression techniques have become increasingly popular as they can drastically reduce the storage and computation requirements for very large networks. Recent empirical studies have illustrated that even simple pruning strategies can be surprisingly effective, and several theoretical studies have shown that compressible networks (in speciﬁc senses) should achieve a low generalization error. Yet, a theoretical characterization of the underlying causes that make the networks amenable to such simple compression schemes is still missing. In this study, focusing our attention on stochastic gradient descent (SGD), our main contribution is to link compressibility to two recently established properties of SGD: (i) as the network size goes to inﬁnity, the system can converge to a mean-ﬁeld limit, where the network weights behave independently [DBDF ¸S20], (ii) for a large step-size/batch-size ratio, the SGD iterates can converge to a heavy-tailed stationary distribution [HM20, G ¸SZ21]. Assuming that both of these phenomena occur simultaneously, we prove that the networks are guaranteed to be ‘(cid:96)p-compressible’, and the compression errors of different pruning techniques (magnitude, singular value, or node pruning) become arbitrarily small as the network size increases. We further prove generalization bounds adapted to our theoretical framework, which are consistent with the observation that the generalization error will be lower for more compressible networks. Our theory and numerical study on various neural networks show that large step-size/batch-size ratios introduce heavy tails, which, in combination with overparametrization, result in compressibility. 1

Introduction
With the increasing model sizes in deep learning and with its increasing use in low-resource environ-ments, network compression is becoming ever more important. Among many network compression techniques, network pruning has been arguably the most commonly used method [O’N20], and it is rising in popularity and success [BOFG20]. Though various pruning methods are successfully used in practice and their theoretical implications in terms of generalization are increasingly apparent
[AGNZ18], a thorough understanding of why and when neural networks are compressible is lacking.
*Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A common conclusion in pruning research is that overparametrized networks can be greatly com-pressed by pruning with little to no cost at generalization, including with simple schemes such as magnitude pruning [BOFG20, O’N20]. For example, research on iterative magnitude pruning [FC19] demonstrated the possibility of compressing trained deep learning models by iteratively eliciting a much sparser substructure. While it is known that the choice of training hyperparameters such as learning rate affects the performance of such pruning strategies [FDRC20, HJRY20, RFC20], usually such observations are low in granularity and almost never theoretically motivated. Overall, the ﬁeld lacks a framework to understand why or when a pruning method should be useful [O’N20].
Another strand of research that highlights the importance of understanding network compressibility includes various studies [AGNZ18, SAM+20, SAN20, HJTW21, KLG+21] that presented general-ization bounds and/or empirical evidence that imply that the more compressible a network is, the more likely it is to generalize well. The aforementioned bounds are particularly interesting since classical generalization bounds increase with the dimension and hence become irrelevant in high dimensional deep learning settings, and fall short of explaining the generalization behavior of overparametrized neural networks. These results again illustrate the importance of understanding the conditions that give rise to compressibility given their implications regarding generalization.
In this paper, we develop a theoretical framework to address (i) why and when modern neural networks can be amenable to very simple pruning strategies and (ii) how this relates to generalization.
Our theoretical results are based on two recent disparate discoveries regarding deep neural networks trained with the stochastic gradient descent (SGD) algorithm. The ﬁrst one is the emergence of heavy-tailed stationary distributions, which appear when the networks are trained with large learning rates and/or small batch-sizes [HM20, G ¸SZ21]. The second one is the propagation of chaos phenomenon, which indicates that, as the network size goes to inﬁnity, the network weights behave independently
[MMN18, SS20, DBDF ¸S20].
We show that, assuming both of the aforementioned phenomena occur simultaneously, fully connected neural networks will be provably compressible in a precise sense, and the compression errors of (i) unstructured global or layer-wise magnitude pruning, (ii) pruning based on singular values of the weight matrices, (iii) and node pruning can be made arbitrarily small for any compression ratio as the dimension increases. Our formulation of network compressibility in terms of ‘(cid:96)p-compressibility’ enables us to access results from compressed sensing literature [GCD12, AUM11] to be used in neural network analysis. Moreover, we prove generalization bounds adapted to our framework that agree with existing compression-based generalization bounds [AGNZ18, SAM+20, SAN20] and conﬁrm that compressibility implies better generalization. We conduct experiments on fully connected and convolutional networks and show that the results are in strong accordance with our theory.
Our study reveals an interesting phenomenon: depending on the algorithm hyperparameters, such as learning rate and batch-size, the resulting neural networks might possess different compressibility properties. Under the decoupling effect of propagation of chaos that emerges with overparametriza-tion [DBDF ¸S20], the networks become compressible in a way that they are amenable to simple pruning strategies as the tails get heavier, which is shown to depend on the step-size/batch-size ratio [G ¸SZ21]. Finally, when compressible, the networks become likely to generalize better. In this sense, our results also provide an alternative perspective to the recent theoretical studies that suggest that heavy tails can be beneﬁcial in terms of generalization [ ¸SSDE20, ZFM+20]. 2 Preliminaries and Technical