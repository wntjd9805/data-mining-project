Abstract
In this paper, we propose a practical online method for solving a class of dis-tributionally robust optimization (DRO) with non-convex objectives, which has important applications in machine learning for improving the robustness of neural networks. In the literature, most methods for solving DRO are based on stochastic primal-dual methods. However, primal-dual methods for DRO suffer from several drawbacks: (1) manipulating a high-dimensional dual variable corresponding to the size of data is time expensive; (2) they are not friendly to online learning where data is coming sequentially. To address these issues, we consider a class of DRO with an KL divergence regularization on the dual variables, transform the min-max problem into a compositional minimization problem, and propose practical duality-free online stochastic methods without requiring a large mini-batch size.
We establish the state-of-the-art complexities of the proposed methods with and without a Polyak-Łojasiewicz (PL) condition of the objective. Empirical studies on large-scale deep learning tasks (i) demonstrate that our method can speed up the training by more than 2 times than baseline methods and save days of training time on a large-scale dataset with ∼ 265K images, and (ii) verify the supreme performance of DRO over Empirical Risk Minimization (ERM) on imbalanced datasets. Of independent interest, the proposed method can be also used for solving a family of stochastic compositional problems with state-of-the-art complexities. 1

Introduction
Distributionally robust optimization (DRO) has received tremendous attention in machine learning due to its capability to handle noisy data, adversarial data and imbalanced classiﬁcation data [42, 33, 4].
Given a set of observed data {z1, . . . , zn}, where zi = (xi, yi), a DRO formulation can be written as: min w∈Rd max p∈∆n
Fp(w) = n (cid:88) i=1 pi(cid:96)(w; zi) − h(p, 1/n) + r(w), (1) where w denotes the model parameter, ∆n = {p ∈ Rn : (cid:80) i pi = 1, pi ≥ 0} denotes a n-dimensional simplex, (cid:96)(w; z) denotes a loss function on data z, h(p, 1/n) is a divergence measure between p and uniform probabilities 1/n, and r(w) is convex regularizer of w. When (cid:96)(w; z) is a convex function (e.g., for learning a linear model), many stochastic primal-dual methods can be employed for solving the above min-max problem [35, 20, 49, 48, 32]. When (cid:96)(w; z) is a non-convex function (e.g., for learning a deep neural network), some recent studies also proposed stochastic primal-dual methods [41, 48].
∗The ﬁrst two authors make equal contributions. Correspondence to tianbao-yang@uiowa.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
However, stochastic primal-dual methods for solving DRO problems with a non-convex (cid:96)(w; z) loss function (e.g., the predictive model is a deep neural network) suffer from several drawbacks. First, primal-dual methods need to maintain and update a high-dimensional dual variable p ∈ Rn for large-scale data, whose memory cost is as high as O(n) per-iteration. Second, existing primal-dual methods usually need to sample data according to probabilities p in order to update w, which brings additional costs than random sampling. Although random sampling can be used for computing the stochastic gradient in terms of w, the resulting stochastic gradient could have n-times larger variance than using non-uniform sampling according to p (please refer to the supplement for a simple illustration). Third, due to the constraint on p ∈ ∆n, the min-max formulation (1) is not friendly to online learning in which the data is received sequentially and n is rarely known in prior. (cid:11)
Can we design an efﬁcient online algorithm to address the DRO formulation (1) without dealing with p ∈ Rn for a non-convex objective that is applicable to deep learning? (cid:10) (cid:8) (cid:9)
To address this question, we restrict our attention to a family of DRO problems, in which the KL divergence h(p, 1/n) = λ (cid:80) i pi log(npi) is used for regularizing the dual variables p, where λ > 0 is a regularization parameter. We note that this consideration does not impose strong restriction to the modeling capability. It has been shown that for a family of divergence functions h(p, 1/n), different DRO formulations are statistically equivalent to a certain degree [12]. The proposed method is based on an equivalent minimization formulation for h(p, 1/n) = λ (cid:80) i pi log(npi). In particular, by maximizing over p exactly, (1) is equivalent to n (cid:88) (cid:33) (cid:32) (cid:110)
Fdro(w) = λ log 1/n exp((cid:96)(w; zi)/λ))
+ r(w) (cid:111)
. (2) min w∈Rd i=1
In an online learning setting, we can consider a more general formulation: (cid:110) min w∈Rd
Fdro(w) = λ log (Ez exp ((cid:96)(w; z)/λ)) + r(w) (cid:111)
.
The above problem is an instance of stochastic compositional problems of the following form: min w∈Rd
F (w) := f (Ez[gz(w)]) + r(w), (3) (4) by setting f (s) = λ log(s), s ≥ 1 and gz(w) = exp((cid:96)(w; z)/λ). Stochastic algorithms have been developed for solving the above compositional problems. [44] proposed the ﬁrst stochastic algorithms for solving (4), which are easy to implement. However, their sample complexities are sub-optimal for solving (4). Recently, a series of works have tried to improve the convergence rate by using advanced variance reduction techniques (e.g., SVRG [19], SPIDER [13], SARAH [36]). However, most of them require using a mega mini-batch size in the order of O(n) or O(1/(cid:15))2 at every iteration or many iterations for updating w, which hinders their applications on large-scale problems. In addition, these algorithms usually use a constant step size, which may harm the generalization performance.
This paper aims to develop more practical stochastic algorithms for solving (3) without suffering from the above issues in order to enable practitioners to explore the capability of DRO for deep learning with irregular data (e.g., imbalanced data, noisy data). To this end, we proposed an online stochastic method (COVER) and its restarted variant (RECOVER). We establish a state-of-the-art complexity of COVER for ﬁnding an (cid:15)-stationary solution and a state-of-the-art complexity of RECOVER under a Polyak-Łojasiewicz (PL) condition of the problem. PL condition has been widely explored for developing practical optimization algorithms for deep learning [52]. Compared with other stochastic algorithms, the practical advantages of RECOVER are: 1. RECOVER is an online duality-free algorithm for addressing large-scale KL regularized
DRO problem that is independent of the high dimensional dual variable p ∈ Rn, which makes it suitable for deep learning applications. 2. RECOVER also enjoys the beneﬁts of stagewise training similar to existing stochastic methods for deep learning [52], i.e., the step size is decreased geometrically in a stagewise manner. 2(cid:15) is either the objective gap accuracy F (w) − min F (w) ≤ (cid:15) or the gradient norm square bound (cid:107)∇F (w)(cid:107)2 ≤ (cid:15) 2
Table 1: Summary of properties of state-of-the-art algorithms for solving our DRO problem. The sample complexity is measured in terms of ﬁnding an (cid:15)-stationary point w/o PL condition, i.e., (cid:107)∇F (w)(cid:107)2 ≤ (cid:15), or achieving (cid:15)-objective gap, i.e, F (w) − minw F (w) ≤ (cid:15) with PL condition. (cid:101)O omits a logarithmic dependence over (cid:15). n represents the size of datasets for a ﬁnite sum problem, d denotes the dimension of w. GDS represents whether the step size is geometrically decreased.
Settings
Algorithms w/o PL w/ PL
PG-SMD2 [41]
ASC-PG [45]
CIVR [53]
COVER (This paper)
Stoc-AGDA [50]
PES-SGDA [15]
RCIVR [53]
RECOVER (This paper)
Sample Complexity
O(n/(cid:15) + 1/(cid:15)2)
O(1/(cid:15)2)
O(1/(cid:15)3/2) (cid:101)O(1/(cid:15)3/2)
O(1/µ2(cid:15))
O(1/µ2(cid:15)) (cid:101)O(1/µ(cid:15))
O(1/µ(cid:15)) batch size GDS η
O(1)
O(1)
O(1/(cid:15))
O(1)
O(1)
O(1)
O(1/(cid:15))
O(1) x x x x x (cid:88) x (cid:88)
Memory
Cost
O(n + d)
O(d)
O(d)
O(d)
O(n + d)
O(n + d)
O(d)
O(d)
Style
Primal-Dual
Compositional
Compositional
Compositional
Primal-Dual
Primal-Dual
Compositional
Compositional
In addition, this paper also makes several important theoretical contributions for stochastic non-convex optimization, including 1. We establish a nearly optimal complexity for ﬁnding (cid:15)-stationary point, i.e., (cid:107)∇F (w)(cid:107)2 ≤ (cid:15), for a class of two-level compositional problems in the order of (cid:101)O(1/(cid:15)3/2) without a large mini-batch size, which is better than existing results [44, 45, 14, 5]. 2. We etablish an optimal complexity for ﬁnding (cid:15)-optimal solution under an µ−PL condition for a class of two-level compositional problems in the order of O(1/(µ(cid:15))) without a large mini-batch size, which is better than existing results [53].
A theoretical comparison between our results and existing results is shown in Table 1. Empirical studies vividly demonstrate the effectiveness of RECOVER for deep learning on imbalanced data. 2