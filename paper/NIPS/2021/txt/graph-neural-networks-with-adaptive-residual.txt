Abstract
Graph neural networks (GNNs) have shown the power in graph representation learning for numerous tasks. In this work, we discover an interesting phenomenon that although residual connections in the message passing of GNNs help improve the performance, they immensely amplify GNNs’ vulnerability against abnormal node features. This is undesirable because in real-world applications, node features in graphs could often be abnormal such as being naturally noisy or adversarially manipulated. We analyze possible reasons to understand this phenomenon and aim to design GNNs with stronger resilience to abnormal features. Our understandings motivate us to propose and derive a simple, efﬁcient, interpretable, and adaptive message passing scheme, leading to a novel GNN with Adaptive residual, AirGNN1.
Extensive experiments under various abnormal feature scenarios demonstrate the effectiveness of the proposed algorithm. 1

Introduction
Recent years have witnessed the great success of graph neural networks (GNNs) in representation learning for graph structure data [1]. Essentially, GNNs generalize deep neural networks (DNNs) from regular grids, such as image, video and text, to irregular data such as social, energy, transportation, citation, and biological networks. Such data can be naturally represented as graphs with nodes and edges. The key building block for such generalization is the neural message passing framework [2]:
= UPDATE(k)(cid:0)x(k) (1) (cid:1) x(k+1) u u , m(k)
N (u) where x(k) u ∈ Rd denotes the feature vector of node u in the k-th iteration of message passing, and m(k)
N (u) is the message aggregated from u’s neighborhood N (u). The speciﬁc design of message passing scheme can be motivated from spectral domain [3, 4] or spatial domain [5, 6, 7, 2]. It usually linearly smooths the features in a local neighborhood on the graph.
GNNs have achieved superior performance in a large number of benchmark datasets [8] where the node features are assumed to be complete and informative. However, in real-world applications, some node features could be abnormal from various aspects. For instance, in social networks, new users might not have complete proﬁle before they make connections with others, leading to missing user features. In transportation networks, node features can be noisy since there exist certain 1The implementation is available at https://github.com/lxiaorui/AirGNN. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
uncertainty and dynamics in the observation of the trafﬁc information. What is worse, node features can be adversarially chosen by the attacker to maliciously manipulate the prediction made by GNNs.
Therefore, it is greatly desired to design GNN models with stronger resilience to abnormal node features.
In this work, we ﬁrst perform empirical investigations on how representative GNN models behave on graphs with abnormal features. Speciﬁcally, based upon standard benchmark datasets, we simulate the abnormal features by replacing the features of randomly selected nodes with random Gaussian noise. Then the performance of node classiﬁcation on abnormal features and normal features are examined separately. From our preliminary study in Section 2, we reveal two interesting observations: (1) Feature aggregation can boost the resilience to abnormal features, but too many aggregations could hurt the performance on both normal and abnormal features; and (2) Residual connection helps
GNNs beneﬁt from more layers for normal features, while making GNNs more fragile to abnormal features. We then provide possible explanations to understand these observed phenomena from the perspective of graph Laplacian smoothing. Our analyses imply that there might exist an intrinsic tension between feature aggregation and residual connection, which results in a performance tradeoff between normal features and abnormal features.
Motivated by these ﬁndings and understandings, we aim to design new GNNs with stronger resilience to abnormal features while largely maintaining the performance on normal features. Our contributions can be summarized as follows:
• We discover an intrinsic tension between feature aggregation and residual connection in GNNs, and the corresponding performance tradeoff between abnormal and normal features. We also analyze possible reasons to explain and understand these ﬁndings.
• We propose a simple, efﬁcient, principled and adaptive message passing scheme, which leads to a novel GNN model with adaptive residual, named as AirGNN.
• Extensive experiments under various abnormal feature scenarios demonstrate the superiority of the proposed algorithm. The ablation study demonstrates how the adaptive residuals mitigate the impact of abnormal features. 2 Preliminary
Before introducing the preliminary study, we ﬁrst deﬁne the notations used throughout the paper.
Notations. We use bold upper-case letters such as X to denote matrices. Given a matrix X ∈ Rn×d, we use Xi to denote its i-th row and Xij to denote its element in i-th row and j-th column. The ij and (cid:107)X(cid:107)21 =
Frobenius norm and (cid:96)21 norm of a matrix X are deﬁned as (cid:107)X(cid:107)F = (cid:80) ij, respectively. We deﬁne (cid:107)X(cid:107)2 = σmax(X) where σmax(X) is the i (cid:107)Xi(cid:107)2 = (cid:80) ij X2 (cid:113)(cid:80) (cid:113)(cid:80) j X2 largest singular value of X. i
Let G = {V, E} be a graph with the node set V = {v1, . . . , vn} and the undirected edge set
E = {e1, . . . , em}. We use N (vi) to denote the neighboring nodes of node vi, including vi itself.
Suppose that each node is associated with a d-dimensional feature vector, and the features for all nodes are denoted as Xfea ∈ Rn×d. The graph structure G can be represented as an adjacent matrix A ∈ Rn×n, where Aij = 1 when there exists an edge between nodes vi and vj, and
Aij = 0 otherwise. The graph Laplacian matrix is deﬁned as L = D − A, where D is the diagonal degree matrix. Let us denote the commonly used feature aggregation matrix in GNNs [3] as
˜A = ˆD− 1 2 where ˆA = A + I is the adjacent matrix with self-loop and its degree matrix is
ˆD. The corresponding Laplacian matrix is deﬁned as ˜L = I − ˜A. 2 ˆA ˆD− 1
In this work, we focus on the setting where a subset of nodes in the graph contain abnormal features, while the remaining nodes have normal features. In the remaining of this paper, we use abnormal/normal features to denote nodes with abnormal/normal features, for simplicity. 2.1 Preliminary Study
Experimental setup. To investigate how GNNs behave on abnormal and normal node features, we design semi-supervised node classiﬁcation experiments on three common datasets (i.e., Cora, 2
CiteSeer and PubMed), following the data splits in the work [3]. Moreover, we simulate the abnormal features by assigning 10% of the nodes with random features sampled from a standard Gaussian distribution. The experiments are performed on representative GNN models covering coupled and decoupled architectures, including GCN [3], GCNII [9], APPNP [10], and their variants with or without residual connections in feature aggregations, denoted as w/Res and wo/Res. All methods follow the hyperparameter settings in their original papers. We examine how these models perform when the number of layers increases. Note that for the decoupled architectures such as APPNP, we ﬁx the 2-layer MLP and increase the number of propagation layers. While for the coupled architectures such as GCN and GCNII, we increase the number of feature transformation and propagation layers simultaneously. We report the average performance over 10 times of random selection of the noise node sets. The node classiﬁcation accuracy (mean and standard variance) on nodes with abnormal and normal features is illustrated in Figure 1 and Figure. 2, separately. (a) APPNP (b) GCNII (c) GCN
Figure 1: Node classiﬁcation accuracy on abnormal nodes (Cora) (a) APPNP (b) GCNII (c) GCN
Figure 2: Node classiﬁcation accuracy on normal nodes (Cora)
Observations. From Figure 1 and Figure 2, we can make the following observations: (1) Without residual connection, more layers (e.g., > 2 for GCN and GCNII, > 10 for APPNP) hurt the accuracy on nodes with normal features. However, more layers boost the accuracy on nodes with abnormal features signiﬁcantly, before ﬁnally starting to decrease; (2) With residual connection, the accuracy on nodes with normal features keeps increasing with more layers2. However, the accuracy on nodes with abnormal features only increases marginally when stacking more layers, and then starts to decrease.
While we only present the experiments on Cora, we defer the results on other datasets to Appendix C, which provide similar observations. To conclude, we can summarize these observations into two major ﬁndings:
• Finding I: Feature aggregation can boost the resilience to abnormal features, but too many aggregations could hurt the performance on both normal and abnormal nodes;
• Finding II: Residual connection helps GNNs beneﬁt from more layers for nodes with normal features, while making GNNs more fragile to abnormal features. 2GCN w/Res is an exception because its residual is not appropriate, which is consistent with the experiments in the work [3]. 3
2.2 Understandings
In this subsection, we provide the understanding and explanation for aforementioned ﬁndings, from the perspective of graph Laplacian smoothing.
Understanding Finding I: Feature aggregation as Laplacian smoothing
The message passing in GCN [3], GCNII wo/ residual and APPNP wo/ residual (as well as many popular GNN models), follows the feature aggregation
Xout = ˜AXin, where Xin and Xout represent the features before and after message passing layer, respectively. It can be interpreted as one gradient descent step for the Laplacian smoothing problem [11] (2) arg min
X∈Rn×d
L1(X) := 1 2 (cid:16)
X(cid:62)(I − ˜A)X (cid:17) tr
= 1 2 (cid:88) (vi,vj )∈E (cid:107)
Xi√ di + 1
−
Xj (cid:112)dj + 1 (cid:107)2 2, (3) where di is the node degree of node vi. Eq. (2) can be derived from Xout = Xin−(I− ˜A)Xin = ˜AXin, with the initialization X = Xin and stepsize γ = 1. The Laplacian smoothing problem penalizes the feature difference between neighboring nodes. To reduce this penalty, the feature aggregation in
Eq. (2) smooths the node features by taking the average of local neighbors, and thus can be considered as low-pass ﬁlter which gradually ﬁlters out high-frequency signals [12, 13]. Therefore, it increases the resilience to abnormal features which are likely to be high-frequency signals. In other words, the local neighboring nodes help to correct the abnormal features. Unfortunately, if applied too many times, these low-pass ﬁlters could overly smooth the features (well-known as oversmoothing [14, 15]) such that nodes are not distinguishable enough, providing an explanation to the degraded performance on both abnormal and normal features when stacking too many layers.
Understanding Finding II: Residual connection maintains feature proximity
To adjust the feature smoothness for better performance, APPNP [10] utilizes residual connections in message passing as follows
Xk+1 = (1 − α) ˜AXk + αXin, where X0 = Xin. It can be considered as an iterative solution for the regularized Laplacian smoothing problem [11] (4) arg min
X∈Rn×d
L2(X) :=
α 2(1 − α) (cid:107)X − Xin(cid:107)2
F + (cid:16) tr (cid:17)
X(cid:62)(I − ˜A)X
, 1 2 (5) with initialization X = Xin and stepsize γ = 1 − α due to
Xk+1 = Xk − (1 − α) (cid:16) α 1 − α (Xk − Xin) + (I − ˜A)Xk(cid:17)
= (1 − α) ˜AXk + αXin.
GCNII [9] adopts a similar message passing but further combines a feature transformation layer in each message passing step, which leads to a coupled architecture, as contrast to the decoupled architecture of APPNP. The residual connection naturally arises when regularizing the proximity between input and output features, as showed in the ﬁrst term of L2(X). Such proximity can help avoid the trivial solution for the problem in Eq. (3), i.e., totally oversmoothed features only depending on node degrees, and consequently mitigates the oversmoothing issue. More intuitively, residual connections in GNNs provide direct information ﬂows between layers that can preserve some necessary high-frequency signals for better discrimination between classes. More layers with residual provide a more accurate solution to Eq. (5), which explains the performance gain from deeper GNNs.
Unfortunately, these residual connections also undesirably carry on abnormal features which are detrimental, leading to the inferior performance on abnormal features. 3 The Proposed Framework
In this section, we ﬁrst motivate the proposed adaptive message passing scheme (AMP) with further discussions on our preliminary study. We then introduce more details about AMP, its interpretations, convergence guarantee and computation complexity, as well as the model architecture of AirGNN. 4
3.1 Design Motivation
Our preliminary study in Section 2 reveals an intrinsic tension between feature aggregation and residual connection: (1) feature aggregation helps smooth out abnormal features, while it could cause inappropriate smoothing for normal features; (2) residual connection is essential for adjusting the feature smoothness, but it could be detrimental for abnormal features. Although this conﬂict can be partially mitigated by adjusting the residual connection such as the residual weight α in GCNII [9] and APPNP [10], such global adjustment cannot be adaptive to a subset of the nodes, e.g., the nodes with abnormal features. This is crucial because in practice we often encounter the scenario where only a subset of nodes contain abnormal features. Therefore, how to reconcile this dilemma still desires dedicated efforts. We then naturally ask a question: Can we design a better message passing scheme with node-wise adaptive feature aggregation and residual connection?
The motivation of the proposed idea builds upon the following intuition: while it is important to maintain the proximity between input and output features as in Eq. (5), it could be over aggressive to i=1 (cid:107)Xi − (Xin)i(cid:107)2 penalize their deviations by the square of Frobenius norm, i.e., (cid:107)X − Xin(cid:107)2 2.
The fact that this penalty does not tolerate large deviations weakens the capability to remove abnormal features through Laplacian smoothing. This motivates us to consider an alternative proximity penalty n (cid:88)
F = (cid:80)n (cid:107)X − Xin(cid:107)21 := (cid:107)Xi − (Xin)i(cid:107)2, (6) i=1 which instead penalizes the deviations by the (cid:96)1 norm of row-wise (cid:96)2 norms, namely (cid:96)21 norm. The (cid:96)21 norm promotes row sparsity in X − Xin, and it also allows large deviations because the penalty on large values is less aggressive, leading to the potential removal of abnormal features. Therefore, we propose the following Laplacian smoothing problem regularized by (cid:96)21 norm proximity control: arg min
X∈Rn×d
λ(cid:107)X − Xin(cid:107)21 + 1 2 tr(X(cid:62)(I − ˜A)X), (7) where λ ∈ [0, ∞) is a parameter to adjust the balance between proximity and Laplacian smoothing.
In order to easy the tuning of λ, we made a modiﬁcation of Eq. (7): arg min
X∈Rn×d
L(X) := λ(cid:107)X − Xin(cid:107)21 + (1 − λ)tr(X(cid:62)(I − ˜A)X), (8) where λ ∈ [0, 1] controls the balance. 3.2 Adaptive Message Passing
Figure 3: Diagram of Adaptive Message Passing
L(X) is a composite objective with non-smooth and smooth components. We optimize it by proximal gradient descent [16] and obtain the following iterations as the adaptive message passing (AMP):
Yk = Xk − 2γ(1 − λ)(I − ˜A)Xk = (cid:0)1 − 2γ(1 − λ)(cid:1)Xk + 2γ(1 − λ) ˜AXk (9) (cid:110)
Xk+1 = arg min 1 2γ where X0 = Xin and γ is the stepsize to be speciﬁed later. Let Z = X − Xin, and Eq. (10) can be rewritten as:
λ(cid:107)X − Xin(cid:107)21 + (cid:107)X − Yk(cid:107)2
F (10)
X (cid:111)
Zk+1 = arg min (cid:110)
λ(cid:107)Z(cid:107)21 + 1 2γ
= proxγλ(cid:107)·(cid:107)21 (Yk − Xin)
Z (cid:107)Z − (Yk − Xin)(cid:107)2
F
Xk+1 = Xin + Zk+1. 5 (cid:111) (11) (12)
The i-th row of the proximal operator in Eq. (11) can be computed analytically (cid:0)proxγλ(cid:107)·(cid:107)21 (X)(cid:1) i =
Xi (cid:107)Xi(cid:107)2 max((cid:107)Xi(cid:107)2 − γλ, 0) = max(1 −
γλ (cid:107)Xi(cid:107)2
, 0) · Xi. (13)
Note that the proximal operator returns 0 if the input vector is 0. Substituting X in Eq. (13) with
Yk − Xin and combining Eq. (11) and Eq. (12), then Eq. (12) becomes
Xk+1 i = (Xin)i + βi(Yk i − (Xin)i) = (1 − βi)(Xin)i + βiYk i ,
∀i ∈ [n], (14) where βi := max(1 −
, 0). To summarize, the proposed adaptive message passing (AMP) scheme is showed in Figure 4, and a diagram is showed in Figure 3. In detail, AMP works as follows: i −(Xin)i(cid:107)2 (cid:107)Yk
γλ
• The ﬁrst step takes a feature aggregation within the local neighbors with a self-loop weighted by 1 − 2γ(1 − λ);
• The second step computes a weight βi ∈ [0, 1] for each node vi depending on the local deviation (cid:107)Yk i − (Xin)i(cid:107)2.
• The ﬁnal step takes a linear combination of input features Xin and the aggregated features Yk, where the node-wise residual is adaptively weighted by 1 − βi for each node vi.



Yk = (cid:0)1 − 2γ(1 − λ)(cid:1)Xk + 2γ(1 − λ) ˜AXk
βi = max(1 −
γλ (cid:107)Yk i − (Xin)i(cid:107)2
, 0) ∀i ∈ [n]
Xk+1 i = (1 − βi)(Xin)i + βiYk i
∀i ∈ [n]
Figure 4: Adaptive Message Passing (AMP)
The convergence guarantee of AMP and parameter setting for the stepsize γ are illustrated in
Theorem 1 and proved in Appendix A. According to Theorem 1, if we set γ = 1 2(1−λ) ,
˜AXk and Yk = ˜AXk, respectively. then the ﬁrst step of AMP can be simpliﬁed as Yk = 1
The choice of stepsize will only impact the convergence speed but not the ultimate effect of AMP when it convergences to the ﬁxed point solution. We also discuss the computation complexity per iteration of AMP in Remark 1.
Theorem 1 (Convergence of AMP). Under the stepsize setting γ <
, the proposed adaptive message passing scheme (AMP) in Eq. (9) and Eq. (10) converges to the optimal solution of the 2(1−λ) since (cid:107)˜L(cid:107)2 ≤ 2. problem deﬁned in Eq. (8). In practice, it is sufﬁcient to choose any γ < 1
Moreover, if the connected components of the graph G are not bipartite graphs, it is sufﬁcient to choose γ = 1 4(1−λ) or γ = 1 1 (1−λ)(cid:107) ˜L(cid:107)2 2 Xk + 1 2(1−λ) since (cid:107)˜L(cid:107)2 < 2. 2
Remark 1 (Computation complexity). AMP is as efﬁcient as simple feature aggregation Xout =
˜AXin because the additional computation cost from the second and third steps in Figure 4 is in the order O(nd), where n is the number of nodes and d is the feature dimension. This is negligible compared with the computation cost O(md) in feature aggregation, where m is the number of edges, due to the fact that usually there are many more edges than nodes in real-world graphs, i.e., m (cid:29) n. 3.3
Interpretation of AMP
Interestingly, the proposed AMP has a simple and intuitive interpretation as adaptive residual connec-tion, which aligns well with our design motivation:
• If the feature of node vi, i.e., (Xin)i, is signiﬁcantly inconsistent with its local neighbors, i.e., the aggregated feature Yk i − (Xin)i(cid:107)2 will be large, which leads to a βi close to 1. Therefore, the ﬁnal step will assign a small weight to the residual, i.e., (1 − βi)(Xin)i, and the aggregated feature Yk i , then the local deviation (cid:107)Yk i will dominate. 6
• On the contrary, if (Xin)i is already consistent with its local neighbors, (cid:107)Yk i − (Xin)i(cid:107)2 will be small, which leads to a βi close to 0. Thus, the residual will dominate, which is reasonable since there is less need to aggregate features in this case.
• To summarize, the local deviation (cid:107)Yk i − (Xin)i(cid:107)2 provides a natural transition from βi → 1 to βi → 0, and the transition can be modulated by λ which can be either learned or tuned as a hyperparameter through cross-validation. This transition provides an node-wise adaptive residual connection for the message passing scheme.
Adaptivity for abnormal & normal features. According to the homophily assumption on graph structure data [17, 18, 19, 3], the feature representations of normal features should be more consistent with local neighbors than abnormal features. As a result, AMP will assign more residual (i.e., smaller
β) to normal features but less residual (i.e., larger β) to abnormal features, providing a customized tradeoff between feature aggregation and residual connection. Consequently, it can promote both the resilience to abnormal features and the performance on normal features. Above discussion also implies a clear physical meaning for β in AMP, and we formally deﬁne it as the adaptive score.
Deﬁnition 1 (Adaptive score). The variables {β1, · · · , βn} in the adaptive message passing scheme (AMP) are deﬁned as the adaptive scores for nodes {v1, · · · , vn} respectively in graph G.
In particular, the larger βi is, the more likely the feature of node vi is abnormal.
Remark 2 (Nonlinear smoother). Different from most existing message passing scheme which are linear smoothers, AMP is a nonlinear smoother because the weights {βi} are computed from Yk and Xin. This nonlinearity is the key to achieve adaptive residual connection for different nodes. 3.4 The Model Architecture
The proposed adaptive message passing (AMP) can be used as a building block in many GNN models to improve the resilience to abnormal node features. In this work, we choose the the decoupled architectures as APPNP [10] and DAGNN [20], and propose the Adaptive residual GNN (AirGNN):
Xin = hθ(Xfea), (15)
Ypre = AMP (Xin, K, λ). (16) hθ(·) is any machine learning model parameterized by learnable parameters θ, such as multilayer perceptrons (MLPs). Xfea ∈ Rn×d denotes the initial node features. The model hθ(·) will ﬁrst transform the initial node features as Xin = hθ(Xfea). AMP takes hθ(Xfea) as input, and performs
K steps of AMP with the hyperparameter λ. Similar to the majority of existing GNN models, the training objective is the cross-entropy classiﬁcation loss on the labeled nodes, and the whole model is trained in an end-to-end way. Note that AirGNN is very efﬁcient as explained in Remark 1, and it only requires two hyperparameters K and λ without introducing additional parameters to learn, which could reduce the risk of overﬁtting. 4 Experiment
In this section, we aim to verify the effective of the proposed adaptive message passing scheme (AMP) and the AirGNN model through the semi-supervised node classiﬁcation tasks. Speciﬁcally, we try to answer the following questions: (1) How does AirGNN perform on abnormal and normal features? (Section 4.2 and 4.3) and (2) How does AirGNN work by adjusting the adaptive residual? (Section 4.4) 4.1 Experimental Settings
Datasets and baselines. We conduct experiments on 8 real-world datasets including three citation graphs, i.e., Cora, Citeseer, Pubmed [21], two co-authorship graphs, i.e., Coauthor CS and Coauthor
Physics [22], two co-purchase graphs, i.e., Amazon Computers and Amazon Photo [22], and one
OGB dataset, i.e., ogbn-arxiv [23]. Due to the space limit, we only present the results on Cora,
Citeseer, and Pubmed in this section, but defer the results on other datasets to Appendix D.1. More details about the data statistics and data splits are summarized in Appendix B. The proposed AirGNN is compared with representative GNNs, including GCN [3], GAT [6], APPNP [10] and GCNII [9].
We defer the comparison with the variants of APPNP and Robust GCN [24] to Appendix D.3 and D.4 respectively. 7
Parameter settings. For all baselines, we follow the best hyperparameter settings in their original papers. Additionally, we tune a best residual weight α for APPNP and GCNII in the range [0, 1]. For
AirGNN, we use a two-layer MLP as the base model hθ(·), following APPNP. We ﬁx the learning rate 0.01, dropout 0.8, and weight decay 0.0005. Moreover, we set γ = 2(1−λ) as suggested by
Theorem 1. We choose K = 10 and tune λ in the range [0, 1]. Adam optimizer [25] is used in all experiments. We run all experiments by 10 times, and report the mean and variance. 1
Evaluation setting. We assess the performance of all models under two types of abnormal feature scenarios, including noisy features and adversarial features. The abnormal features are injected to randomly selected test nodes after model training. By default, all hyperparameters are tuned according to the performance on validation sets when the dataset is clean. If tuning the hyperparameter λ of AirGNN according to the validation sets after injecting abnormal features, the performance will be even better, as discussed in Appendix D.2. The performance on clean data are showed in
Appendix D.5 to demonstrate that AirGNN doesn’t need to sacriﬁce accuracy for better robustness against abnormal features. 4.2 Performance Comparison with Noisy Features
In this subsection, we consider the abnormal features in the noisy feature scenario. Speciﬁcally, we simulate the noisy features by assigning a subset of the nodes with random features sampled from a multivariate standard Gaussian distribution. Note that the selection of noise subsets has a apparent impact on the performance since some nodes are less vulnerable to abnormal features while others are more vulnerable. To reduce such variance, we report the average performance over 10 times of random selection of the noise node sets, similar to the settings in the preliminary study in Section 2.
We report the node classiﬁcation test accuracy on abnormal (noisy) features and normal features in
Figure 5 and Figure 6, separately, under varying noisy ratio. From these ﬁgures, we can observe:
• Figure 5 shows that AirGNN signiﬁcantly outperforms all baselines on all datasets in terms of the performance on noisy nodes. This veriﬁes that AMP is able to improve the resilience to noisy features, aligning well with the design motivation.
• Figure 6 shows that AirGNN promotes the performance on normal nodes when abnormal nodes exist. This is because AMP can remove some abnormal features which are detrimental to normal nodes. (a) Cora (b) CiteSeer (c) PubMed
Figure 5: Node classiﬁcation accuracy on abnormal (noisy) nodes 4.3 Performance Comparison with Adversarial Features
In this subsection, we consider the abnormal feature scenario when the node features are maliciously attacked by the attacker to manipulate the prediction of GNNs. We use the Nettack [26] implemented in DeepRobust3 [27], a PyTorch library for adversarial attacks and defenses, to generate the adversarial features. We randomly choose 40 test nodes as the targeted nodes, and assess the performance under increasing perturbation budgets {0, 5, 10, 20, 50, 80}, where the perturbation numbers denote the number of feature dimensions that can be manipulated. The node classiﬁcation accuracy on these attacked nodes are showed in Figure 7. From these ﬁgures, we can make the following observations: 3https://github.com/DSE-MSU/DeepRobust 8
(a) Cora (b) CiteSeer (c) PubMed
Figure 6: Node classiﬁcation accuracy on normal nodes
• AirGNN is signiﬁcantly more robust against adversarially attacked features than all baselines.
MLP is the most vulnerable model, which demonstrates the usefulness of graph structure information in combating against abnormal node features.
• The advantages of AirGNN over the baselines become much stronger with larger perturbation budgets. This suggests that AMP can signiﬁcantly improve the resilience to abnormal features. (a) Cora (b) CiteSeer (c) PubMed
Figure 7: Node classiﬁcation accuracy on adversarial nodes 4.4 Adaptive Residual for Abnormal & Normal Nodes
To further understand and verify how AMP and AirGNN work, we investigate the adaptive score βi for each node vi. Speciﬁcally, the average adaptive scores for abnormal nodes and normal nodes in the last layer of AMP are computed separately. In the noisy feature scenario, we ﬁx ratio of noisy nodes as 10%. In the adversarial feature scenario, we choose 40 target nodes and ﬁx the perturbation number as 80. The results in noisy and adversarial feature scenarios are showed in Table 1 and
Table 2, respectively. From these tables, we can observe:
• On the one hand, it can be clearly observed that in both scenarios, the average adaptive scores for abnormal nodes are signiﬁcantly higher than those for normal nodes. Therefore, it veriﬁes our intuition that large adaptive scores are strongly related to abnormal features.
• On the other hand, it also implies that the residual weights (i.e., 1 − βi) for abnormal nodes are much lower than those of normal nodes. This perfectly aligns with our motivation to remove abnormal features by reducing their residual connections.
The study on adaptive scores veriﬁes how the adaptive residuals in AMP and AirGNN work as designed. It corroborates that AirGNN not only tremendously boosts the resilience to abnormal features but also provides interpretable information for anomaly detection that will be useful in many security-critical scenarios since the adaptive score serves as a good indicator of abnormal nodes.
Morever, it is expected that APPNP without residual will perform well on abnormal nodes but it will sacriﬁce the performance on normal nodes. We provide detailed comparison with APPNP w/Res and
APPNP wo/Res in Appendix D.3 to show the advantages of adaptive residual of AirGNN. 9
Table 1: Average adaptive score (β) and residual weight (1 − β) in the noisy feature scenario.
Measure
Cora
CiteSeer
PubMed
Average adaptive score for abnormal nodes
Average adaptive score for normal nodes 0.998 ± 0.000 0.924 ± 0.002 0.988 ± 0.000 0.807 ± 0.005 0.996 ± 0.000 0.869 ± 0.006
Average residual weight for abnormal nodes
Average residual weight for normal nodes 0.002 ± 0.000 0.076 ± 0.002 0.012 ± 0.000 0.193 ± 0.005 0.004 ± 0.000 0.131 ± 0.006
Table 2: Average adaptive score (β) and residual weight (1 − β) in the adversarial feature scenario.
Measure
Cora
CiteSeer
PubMed
Average adaptive score for abnormal nodes
Average adaptive score for normal nodes 0.987 ± 0.000 0.922 ± 0.004 0.930 ± 0.007 0.689 ± 0.024 0.959 ± 0.005 0.826 ± 0.016
Average residual weight for abnormal nodes
Average residual weight for normal nodes 0.013 ± 0.000 0.078 ± 0.004 0.070 ± 0.007 0.311 ± 0.024 0.041 ± 0.005 0.174 ± 0.016 5