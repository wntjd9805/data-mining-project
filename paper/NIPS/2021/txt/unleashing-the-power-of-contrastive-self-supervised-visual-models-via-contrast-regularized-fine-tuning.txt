Abstract
Contrastive self-supervised learning (CSL) has attracted increasing attention for model pre-training via unlabeled data. The resulted CSL models provide instance-discriminative visual features that are uniformly scattered in the feature space.
During deployment, the common practice is to directly ﬁne-tune CSL models with cross-entropy, which however may not be the best strategy in practice. Al-though cross-entropy tends to separate inter-class features, the resulting models still have limited capability for reducing intra-class feature scattering that exists in
CSL models. In this paper, we investigate whether applying contrastive learning to ﬁne-tuning would bring further beneﬁts, and analytically ﬁnd that optimiz-ing the contrastive loss beneﬁts both discriminative representation learning and model optimization during ﬁne-tuning. Inspired by these ﬁndings, we propose
Contrast-regularized tuning (Core-tuning), a new approach for ﬁne-tuning CSL models. Instead of simply adding the contrastive loss to the objective of ﬁne-tuning,
Core-tuning further applies a novel hard pair mining strategy for more effective contrastive ﬁne-tuning, as well as smoothing the decision boundary to better ex-ploit the learned discriminative feature space. Extensive experiments on image classiﬁcation and semantic segmentation verify the effectiveness of Core-tuning. 1

Introduction
Pre-training a deep neural network on a large database and then ﬁne-tuning it on downstream tasks has been a popular training scheme. Recently, contrastive self-supervised learning (CSL) has attracted increasing attention on model pre-training, since it does not rely on any hand-crafted annotations but even achieves more promising performance than supervised pre-training on downstream tasks [6, 7, 20, 22, 47]. Speciﬁcally, CSL leverages unlabeled data to train visual models via contrastive learning, which maximizes the feature similarity for two augmentations of the same instance and minimizes the feature similarity of two instances [58]. The learned models provide instance-discriminative visual representations that are uniformly scattered in the feature space [53].
Although there have been substantial CSL studies on model pre-training [23, 47], few have explored the ﬁne-tuning process. The common practice is to directly ﬁne-tune CSL models with the cross-entropy loss [6, 13, 20]. However, we empirically (cf. Table 1) ﬁnd that different ﬁne-tuning methods signiﬁcantly inﬂuence the model performance on downstream tasks, and ﬁne-tuning with only cross-entropy is not the optimal strategy. Intuitively, although cross-entropy tends to learn separable features among classes, the resulting model is still limited in its capability for reducing intra-class feature scattering [37, 55] that exists in CSL models. Meanwhile, most existing ﬁne-tuning methods [33, 35] are devised for supervised pre-trained models and tend to enforce regularizers to prevent the ﬁne-tuned models changing too much from the pre-trained ones. However, they suffer from the issue of negative transfer [9], since downstream tasks are often different from the pre-training contrastive task.
In this sense, how to ﬁne-tune CSL models remains an important yet under-explored question.
∗Corresponding to: Yifan Zhang <yifan.zhang@u.nus.edu> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of two challenges in contrastive ﬁne-tuning. (1) How to mine hard sample pairs for more effective contrastive ﬁne-tuning. As shown in (a), the majority of sample pairs are easy-to-contrast, which may induce negligible contrastive loss gradients that contribute little to learning discriminative representations. (2) How to improve the generalizability of the model. As shown in (d), the classiﬁer simply trained with cross-entropy is often sharp and near training data, leading to limited generalization performance.
Considering that optimizing the unsupervised contrastive loss during pre-training yields models with instance-level discriminative power, we investigate whether applying contrastive learning to
ﬁne-tuning would bring further beneﬁts. To answer this, we analyze the contrastive loss during
ﬁne-tuning (cf. Section 3) and ﬁnd that it offers two beneﬁts. First, integrating the contrastive loss into cross-entropy can provide an additional regularization effect, as compared to cross-entropy based
ﬁne-tuning, for discriminative representation learning. Such an effect encourages the model to learn a low-entropy feature cluster for each class (i.e., high intra-class compactness) and a high-entropy feature space (i.e., large inter-class separation degree). Second, optimizing the contrastive loss will minimize the inﬁmum of the cross-entropy loss over training data, which can provide an additional optimization effect for model ﬁne-tuning. Based on the optimization effectiveness as well as the regularization effectiveness on representations, we argue that optimizing the contrastive loss during
ﬁne-tuning can further improve the performance of CSL models on downstream tasks.
Considering the above beneﬁts, a natural idea is to directly add the contrastive loss to the objective for
ﬁne-tuning, e.g., one recent study [18] simply uses contrastive learning to ﬁne-tune language models.
However, such a method cannot take full advantage of contrastive learning, since it ignores an impor-tant challenge in contrastive ﬁne-tuning. That is, contrastive learning highly relies on positive/negative sample pairs, but the majority of sample features are easy-to-contrast (cf. Figure 1 (a)) [19, 56] and may produce negligible contrastive loss gradients. Ignoring this makes the method [18] fail to learn more discriminative features via contrastive learning and thus cannot ﬁne-tune CSL models well.
In this paper, to better ﬁne-tune CSL models and enhance their performance on downstream tasks, we propose a contrast-regularized tuning approach (termed Core-tuning), based on a novel hard pair mining strategy. Speciﬁcally, Core-tuning generates both hard positive and hard negative pairs for each anchor data via a new hardness-directed mixup strategy (cf. Figure 1 (b-c)). Here, hard positives indicate the positive pairs far away from the anchor, while hard negatives are the negative pairs close to the anchor. Meanwhile, since hard pairs are more informative for contrastive learning [19],
Core-tuning further assigns higher importance weights to hard positive pairs based on a new focal contrastive loss. In this way, the resulting model is able to learn a more discriminative feature space by contrastive ﬁne-tuning. Following that, we also explore how to better exploit the learned discriminative feature space in Core-tuning. Previous work has found that the decision boundary simply trained with cross-entropy is often sharp and close to training data [52], which may make the classiﬁer fail to exploit the high inter-class separation degree in the discriminative feature space (cf. Figure 1 (d)), and also suffer from limited generalization performance. To address this, Core-tuning further uses the mixed features to train the classiﬁer, so that the learned decision boundaries can be more smooth and far away from the original training data (cf. Figure 1 (e)).
The key contributions are threefold. 1) To our knowledge, we are among the ﬁrst to look into the
ﬁne-tuning stage of CSL models, which is an important yet under-explored question. To address this, we propose a novel Core-tuning method. 2) We theoretically analyze the beneﬁts of the supervised contrastive loss on representation learning and model optimization, revealing that it is beneﬁcial to model ﬁne-tuning. 3) Promising results on image classiﬁcation and semantic segmentation verify the effectiveness of Core-tuning for improving the ﬁne-tuning performance of CSL models. We also empirically ﬁnd that Core-tuning beneﬁts CSL models in terms of domain generalization and adversarial robustness on downstream tasks. Considering the theoretical guarantee and empirical effectiveness of Core-tuning, we recommend using it as a standard baseline to ﬁne-tune CSL models. 2
2