Abstract
We theoretically analyze the typical learning performance of (cid:96)1-regularized linear regression ((cid:96)1-LinR) for Ising model selection using the replica method from statis-tical mechanics. For typical random regular graphs in the paramagnetic phase, an accurate estimate of the typical sample complexity of (cid:96)1-LinR is obtained. Remark-ably, despite the model misspeciﬁcation, (cid:96)1-LinR is model selection consistent with the same order of sample complexity as (cid:96)1-regularized logistic regression ((cid:96)1-LogR), i.e., M = O (log N ), where N is the number of variables of the Ising model. Moreover, we provide an efﬁcient method to accurately predict the non-asymptotic behavior of (cid:96)1-LinR for moderate M, N , such as precision and recall.
Simulations show a fairly good agreement between theoretical predictions and experimental results, even for graphs with many loops, which supports our ﬁndings.
Although this paper mainly focuses on (cid:96)1-LinR, our method is readily applicable for precisely characterizing the typical learning performances of a wide class of (cid:96)1-regularized M -estimators including (cid:96)1-LogR and interaction screening. 1

Introduction
The advent of massive data across various scientiﬁc disciplines has led to the widespread use of undirected graphical models, also known as Markov random ﬁelds (MRFs), as a tool for discovering and visualizing dependencies among covariates in multivariate data [1]. The Ising model, originally proposed in statistical physics, is one special class of binary MRFs with pairwise potentials and has been widely used in different domains such as image analysis, social networking, gene network analysis [2, 3, 4, 5, 6, 7]. Among various applications, one fundamental problem of interest is called
Ising model selection, which refers to recovering the underlying graph structure of the original Ising model from independent, identically distributed (i.i.d.) samples. A variety of methods have been proposed [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18], demonstrating the possibility of successful Ising model selection even when the number of samples is smaller than that of the variables. Notably, it has been demonstrated that for the (cid:96)1-regularized logistic regression ((cid:96)1-LogR) [10, 16] and interaction screening (IS) [14, 15] estimators, M = O (log N ) samples sufﬁce for an Ising model with N spins under certain assumptions, which is consistent with respect to (w.r.t.) previously established
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
information-theoretic lower-bound [11]. Both (cid:96)1-LogR and IS are (cid:96)1-regularized M -estimators [19] with logistic and IS objective (ISO) loss functions, respectively.
In this paper, we focus on one simpler linear estimator called (cid:96)1-regularized linear regression ((cid:96)1-LinR) and theoretically investigate its typical learning performance using the powerful replica method
[20, 21, 22, 23] from statistical mechanics. The (cid:96)1-LinR estimator, widely known as least absolute shrinkage and selection operator (LASSO) [24] in statistics and machine learning, is considered here mainly for two reasons. On the one hand, it is one representative example of model misspeciﬁcation since the quadratic loss of (cid:96)1-LinR does not match the true log-conditional-likelihood as (cid:96)1-LogR, nor does it have the interaction screening property as IS. On the other hand, as one of the most popular linear estimator, (cid:96)1-LinR is more computationally efﬁcient than (cid:96)1-LogR and IS, and thus it is of practical importance to investigate its learning performance for Ising model selection. Since it is difﬁcult to obtain results for general graphs, as a ﬁrst step we consider the random regular (RR) graphs GN,d,K0 in the paramagnetic phase [23], where GN,d,K0 denotes the ensemble of RR graphs with constant node degree d and uniform coupling strength K0 on the edges. 1.1 Contributions
The main contributions are summarized as follows. First, we obtain an accurate estimate of the typical sample complexity of (cid:96)1-LinR for Ising model selection for typical RR graphs in the paramagnetic phase, which, remarkably, has the same order as (cid:96)1-LogR. Speciﬁcally, for a typical RR graph G ∈
GN,d,K0, using (cid:96)1-LinR with a regularization parameter 0 < λ < tanh (K0), one can consistently 2(1−tanh2(K0)+dλ2) reconstruct the structure with M > c(λ,K0) log N 1+(d−1) tanh2(K0) .
The accuracy of our typical sample complexity prediction is veriﬁed by its excellent agreement with experimental results. To the best of our knowledge, this is the ﬁrst result that provides an accurate typical sample complexity for Ising model selection. Interestingly, as λ → tanh (K0), a lower bound
M > 2 log N tanh2(K0) of the typical sample complexity is obtained, which has the same scaling as the information-theoretic lower bound M > c(cid:48) log N
[11] for some constant c(cid:48) at high temperatures (i.e., small K0) since tanh (K0) = O (K0) as K0 → 0. samples, where c (λ, K0) =
K2 0
λ2
Second, we provide a computationally efﬁcient method to precisely predict the typical learning performance of (cid:96)1-LinR in the non-asymptotic case with moderate M, N , such as precision, recall, and residual sum of square (RSS). Such precise non-asymptotic predictions of (cid:96)1-LinR for Ising model selection have been unavailable even for (cid:96)1-LogR [10, 16] and IS [14, 15], nor are they the same as previous asymptotic results of (cid:96)1-LinR assuming ﬁxed α ≡ M/N [25, 26, 27, 28]. Moreover, although our theoretical analysis is based on a tree-like structure assumption, experimental results on two dimensional (2D) grid graphs also show a fairly good agreement, indicating that our theoretical result can be a good approximation even for graphs with many loops.
Third, while this paper mainly focuses on (cid:96)1-LinR, our method is readily applicable to a wide class of (cid:96)1-regularized M -estimators [19], including (cid:96)1-LogR [10] and IS [14, 15]. Thus, an additional technical contribution is providing a generic approach for precisely characterizing the typical learning performances of various (cid:96)1-regularized M -estimators for Ising model selection. Although the replica method from statistical mechanics is non-rigorous, our results are conjectured to be correct, which is supported by their excellent agreement with the experimental results. Additionally, several technical advances we propose in this paper, e.g., the entropy term computation by averaging over the Haar measure and the modiﬁcation of EOS to address the ﬁnite-size effect, might be of general interest to those who use the replica method as a tool for performance analysis. 1.2