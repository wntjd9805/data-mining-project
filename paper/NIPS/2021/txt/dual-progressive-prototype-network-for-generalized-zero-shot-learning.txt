Abstract 
Generalized Zero-Shot Learning (GZSL) aims to recognize new categories with  auxiliary semantic information, e.g., category attributes.  In this paper, we han-dle the critical issue of domain shift problem, i.e., confusion between seen and  unseen categories, by progressively improving cross-domain transferability and  category discriminability of visual representations.  Our approach, named Dual 
Progressive Prototype Network (DPPN), constructs two types of prototypes that  record prototypical visual patterns for attributes and categories, respectively. With  attribute  prototypes,  DPPN  alternately  searches  attribute-related  local  regions  and updates corresponding attribute prototypes to progressively explore accurate  attribute-region correspondence. This enables DPPN to produce visual representa-tions with accurate attribute localization ability, which benefts the semantic-visual  alignment and representation transferability.  Besides, along with progressive at-tribute localization, DPPN further projects category prototypes into multiple spaces  to progressively repel visual representations from different categories, which boosts  category discriminability.  Both attribute and category prototypes are collabora-tively learned in a unifed framework, which makes visual representations of DPPN  transferable and distinctive.  Experiments on four benchmarks prove that DPPN  effectively alleviates the domain shift problem in GZSL.  1

Introduction 
Deep learning methods depend heavily on enormous manually-labelled data,  which limits their  further applications [7, 8, 17, 52, 36, 19, 11]. Therefore, Generalized Zero-Shot Learning (GZSL)  recently attracts increasing attention, which aims to recognize images from novel categories with  only seen domain training data.  Due to unavailable unseen domain data during training,  GZSL  methods introduce category descriptions, such as category attributes [15, 14] or word embedding 
[5, 37, 26, 41], to associate two domain categories. 
A basic framework of embedding-based GZSL is to align global image representations with corre-sponding category descriptions in a joint embedding space [16, 3, 55, 38, 2, 6], as shown in Fig. 1 (a). 
Due to the domain shift problem across two domain categories, unseen domain images tend to be  misclassifed as seen categories. To address this issue, recent methods focus on discovering discrimi-native local regions to capture subtle differences between two domain categories. For example, AREN 
[45] and VSE [55] leverage attention mechanism to discover important part regions, which improves  feature discrimination.  DAZLE [20] and RGEN [46] introduce semantic guidance, e.g., category 
âˆ—Corresponding Author  35th Conference on Neural Information Processing Systems (NeurIPS 2021). 
Figure 1: The motivation of DPPN. (a) General GZSL methods directly align global image features  with category attributes. (b) A typical part-based method, i.e., APN [47], learns prototypes shared by  all images for attribute localization. (c) DPPN progressively adjusts prototypes according to different  images and introduces category prototypes to enhance category discriminability.  attributes, into region localization to narrow the semantic-visual gap. Among existing methods, APN 
[47] is most related to our approach.  As shown in Fig. 1 (b), APN constructs visual prototypes to  indicate the typical visual patterns of each attribute, for example describing what attribute "Furry"  visually refers to, and these prototypes are shared across all images to search attribute-matched local  regions. However, due to image variances, the textures corresponding to the same attribute may vary  seriously across images. Thus, sharing prototypes in APN can not well depict the target image. 
In this paper, we propose a novel Dual Progressive Prototype Network (DPPN), which constructs  two types of progressive prototypes for respective attributes and categories to gradually improve  cross-domain  transferability  and  category  discriminability  of  visual  representations.  Instead  of  sharing prototypes, DPPN dynamically adjusts attribute prototypes for each image to capture the vital  visual differences of the same attribute in different images. This is achieved by alternately localizing  attribute regions and updating attributes prototypes in turn, as shown in Fig. 1 (c).  With image-specifc prototypes, attribute localization, i.e., attribute-region correspondence, gets more accurate. 
To explicitly preserve such correspondence in the fnal representations, DPPN aggregates the attribute-related local features by concatenation, instead of widely-used Global Average Pooling (GAP) that  will damage the attribute localization ability. Furthermore, along with progressively-updated attribute  prototypes, DPPN also builds category prototypes to record prototypical visual patterns for different  categories.  The category prototypes are projected into multiple spaces to progressively enlarge  category margins, strengthening category discriminability of visual representations. Consequently,  with cross-domain transferability and category discriminability, DPPN can effectively bridge the gap  between seen and unseen domains. 
Experiments on four benchmarks demonstrate that our DPPN alleviates the domain shift problem in 
GZSL and obtains new state-of-the-art performance. Our contributions can be summarized as three-fold. a) We propose a novel Dual Progressive Prototype Network (DPPN) that constructs progressive  prototypes for both attributes and categories to gradually improve cross-domain transferability and  category discriminability of visual representations. b) An alternation updating strategy is designed  to dynamically adjust attribute prototypes according to target images.  Besides, DPPN aggregates  attribute-related local features by concatenation to produce image representations, which explicitly  preserves the attribute-region correspondence. c) DPPN projects category prototypes into multiple  spaces to progressively enhance category discriminability.  2