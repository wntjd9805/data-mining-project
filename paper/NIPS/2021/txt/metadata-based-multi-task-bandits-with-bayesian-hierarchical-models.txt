Abstract
How to explore efﬁciently is a central problem in multi-armed bandits. In this pa-per, we introduce the metadata-based multi-task bandit problem, where the agent needs to solve a large number of related multi-armed bandit tasks and can lever-age some task-speciﬁc features (i.e., metadata) to share knowledge across tasks.
As a general framework, we propose to capture task relations through the lens of Bayesian hierarchical models, upon which a Thompson sampling algorithm is designed to efﬁciently learn task relations, share information, and minimize the cumulative regrets. Two concrete examples for Gaussian bandits and Bernoulli bandits are carefully analyzed. The Bayes regret for Gaussian bandits clearly demonstrates the beneﬁts of information sharing with our algorithm. The pro-posed method is further supported by extensive experiments. 1

Introduction
The multi-armed bandit (MAB) is a popular framework for sequential decision making problems, where the agent will sequentially choose among a few arms and receive a random reward for the arm [37]. Since the mean rewards for the arms are not known a priori and must be learned through partial feedbacks, the agent is faced with the well-known exploration-exploitation trade-off. MAB is receiving increasing attention and has been widely applied to areas such as clinical trials [19],
ﬁnance [54], recommendation systems [70], among others.
How to explore efﬁciently is a central problem in MAB. In many modern applications, we usually have a large number of separate but related MAB tasks. For example, in e-commerce, the company needs to ﬁnd the optimal display mode for each of many products, and in medical applications, each patient needs to individually undergo a series of treatment periods to ﬁnd the personalized optimal treatment. These tasks typically share similar problem structures, but may have different reward distributions. Intuitively, appropriate information sharing can largely speed up our learning process and reduce the regret, while a naive pooling may cause a linear regret due to the bias.
This paper is concerned with the following question: given a large number of MAB tasks, how do we efﬁciently share information among them? The central task is to capture task relations in a principled way [68]. While a few approaches have been proposed (see Section 2), most of them only utilize the action-reward pairs observed for each task. To our knowledge, none of the existing works can leverage another valuable information source, i.e., the metadata of each task, which contains some static task-speciﬁc features. Such metadata is commonly available in real applications
[66, 68, 57], such as the demographic features of each customer or patient, or basic information of each web page or product. Although the metadata can hardly be directly utilized in a single MAB task, it usually contains intrinsic information about each task, and hence can guide us to learn task relations and efﬁciently share knowledge in the multi-task setup. Speciﬁcally, suppose task i has a feature vector xi (i.e., metadata) and its expected reward vector for all arms is ri. We consider a general formulation that ri is sampled from the conditional distribution P(ri|xi). When P(ri|xi) is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) The meta MAB setting [34, 28, 8], where xi is a constant. (b) There exists several task clus-ters, where xi is categorical. (c) x1 is a continuous variable.
Figure 1: Illustrations of the task distribution P(ri|xi). In the ﬁrst two subplots, each dot represents a task and the two axes denote the expected rewards for two arms. In the last subplot, we have three arms, each dot represents one arm of a task, the x-axis denotes one continuous feature x1, and the y-axis denotes the expected reward. In the last two settings, the metadata is informative and can guide the exploration to reduce regrets, while there is still inter-task heterogeneity that can not be captured. informative, the metadata, being appropriately utilized, can guide our exploration and hence reduce the regret. Several speciﬁc cases of P(ri|xi) are illustrated in Figure 1.
As one of our motivating examples, consider that a company needs to learn about the personal pref-erence of many users over several options (e.g., message templates), via sequential interactions with each user. Suppose some user features (e.g., gender, age, geography) are informative in predicting the preference. Then, even before any interactions with a speciﬁc user, we can predict her preference with high probability, based on her features and our experience with other users. Such knowledge can be concisely encoded as P(ri|xi). However, ri typically can not be fully determined by xi.
Given the possible heterogeneity of ri conditional on xi, the true reward distribution (and optimal arm) of this speciﬁc user still needs to be learned and conﬁrmed via interactions with herself. There-fore, we must carefully utilize these features in a multi-task algorithm. Indeed, we conjecture this is one way how humans adapt to tasks effectively, by constructing a rough ﬁrst impression according to features, and then adjusting the impression gradually in the following interactions.
To judiciously share information and minimize cumulative regrets, we design a multi-task bandit algorithm. Recall that, when P(ri|xi) is known a priori, a Thompson sampling (TS) algorithm [53] with P(ri|xi) as the prior of ri for each task i is known to be (nearly) optimal in Bayes regret, for a wide range of problems [37]. However, such knowledge typically is absent, and many concerns have been raised regarding that TS is sensitive to the prior [42, 25, 7, 23]. In this work, we aim to aggregate data from all tasks to construct an informative prior for each task. Speciﬁcally, we propose to characterize the task relations via a Bayesian hierarchical model, upon which the Multi-Task TS (MTTS) algorithm is designed to continuously learn P(ri|xi) by aggregating data. With such a design, MTTS yields a similar low regret to the oracle TS algorithm which knows P(ri|xi) a priori.
In particular, the metadata xi allows us to efﬁciently capture task relations and share information.
The beneﬁts can be seen clearly from our experiments and analysis.
Contribution. Our contributions can be summarized as follows. First of all, motivated by the usefulness of the commonly available metadata, we introduce and formalize the metadata-based multi-task MAB problem. The related notion of regret (multi-task regret) is deﬁned, which serves as an appropriate metric for this problem. Second, we propose to address this problem with a uni-ﬁed hierarchical Bayesian framework, which characterizes the task relations in a principled way.
We design a TS-type algorithm, named MTTS, to continuously learn the task distribution while minimizing the cumulative regrets. The framework is general to accommodate various reward dis-tributions and sequences of interactions, and interpretable as one can study how the information from other tasks affects one speciﬁc task and how its metadata determines its prior. Two concrete examples for Gaussian bandits and Bernoulli bandits are carefully analyzed, and efﬁcient imple-mentations are discussed. Our formulation and framework open a door to connect the multi-task bandit problem with the rich literature on Bayesian hierarchical models. Third, we theoretically demonstrate the beneﬁts of metadata-based information sharing, by deriving the regret bounds for
MTTS and several baselines under a Gaussian bandit setting. Speciﬁcally, the average multi-task regret of MTTS decreases as the number of tasks grows, while the other baselines suffer a linear regret. Lastly, systematic simulation experiments are conducted to investigate the performance of
MTTS under different conditions, which provides meaningful insights. 2
2