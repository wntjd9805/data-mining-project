Abstract
With the preponderance of pretrained deep learning models available off-the-shelf from model banks today, ﬁnding the best weights to ﬁne-tune to your use-case can be a daunting task. Several methods have recently been proposed to ﬁnd good models for transfer learning, but they either don’t scale well to large model banks or don’t perform well on the diversity of off-the-shelf models. Ideally the question we want to answer is, “given some data and a source model, can you quickly predict the model’s accuracy after ﬁne-tuning?” In this paper, we formalize this setting as
“Scalable Diverse Model Selection” and propose several benchmarks for evaluating on this task. We ﬁnd that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. We then introduce simple techniques to improve the performance and speed of these algorithms.
Finally, we iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection. We have released the benchmarks and method code† in hope to inspire future work in model selection for accessible transfer learning. 1

Introduction
Deep Neural Networks (DNNs) have shown to be very capable of solving a wide variety of visual tasks. However, these networks often require large amounts of data and training time to perform well, limiting the accessibility of deep learning for computer vision. One approach to alleviate this problem is to employ transfer learning, commonly by ﬁne-tuning an off-the-shelf model on the desired task.
With the increasing number of off-the-shelf models available spanning different tasks, datasets, training methods, and architectures, choosing the best model from which to transfer is a challenging endeavor. A common heuristic in computer vision has been to use models pretrained on ImageNet [7] (speciﬁcally the ILSVRC challenge data), but more recent work is starting to expose weaknesses in the generalization performance of ImageNet features [39, 28, 40], and many of these works ﬁnd that other pretraining datasets may perform better for some target tasks. Aside from source dataset, which architecture to select isn’t clear (when comparing similarly fast models). One could theoretically train several transfers from a diverse set of pretrained weights and keep the best performing model, but this isn’t feasible in most practical applications where compute is limited.
This motivates the need for a model selection method that could query a large bank of existing pretrained models (of which several already exist, e.g., [36, 56, 18]) with a small subset of the target data and return a set of weights which performs well when ﬁne-tuned on all target data. Because of the sheer quantity of pretrained weights available to download off the internet today, such a model bank has the potential to be massive and cover a wide variety of tasks, datasets, and architectures. Any
∗Equal Contribution
†https://dbolya.github.io/parc/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
selection method that intends to operate on such a massive library of weights would thus need two important properties: it would need to be scalable in order to accommodate 100s to 1000s of source models, and it would need to perform well on a diverse set of input weights due to the wide variety of models available. In this paper, we formalize this setting as Scalable Diverse Model Selection.
Currently, there are two main lines of work that could address this scenario. The ﬁrst is model selection [11, 46, 10], which attempts to select a viable transfer from a suite of arbitrary pretrained models. However, existing approaches require an initial model trained on the target data to suggest the transfer (which limits accessibility) and are not very successful when comparing across architectures (see Tab. 1). The second line of work is in transferability estimation [4, 52, 34], which attempts to predict how well a source model will transfer to a target task. While this might seem similar to model selection, the two tasks are subtly different in evaluation. Transferability estimation methods usually
ﬁx one source model and vary the target task (i.e., “For my source model, which task would it transfer to the best?”), while model selection methods do the opposite: ﬁx a target task and vary the source models (i.e., “For my target task, which source model will transfer the best?”). While it might seem the same methods could work for either case, this turns out to not be the case (see Tab. 2).
Contributions. We formalize the task of Scalable Diverse Model Selection, which intends to make deep learning for computer vision more accessible. While other papers might have explored aspects of this space already, we standardize it by introducing several tools and benchmarks for evaluating model selection methods in this setting. First, we provide a controlled environment that includes exhaustively trained transfers from 8 source datasets to 6 target datasets across 4 different commonly used architectures for a total of 168 ground truth transfers for analysis (Sec. 3). We show that current state-of-the-art transferability and model selection methods fail to beat simple baselines in this new setting (Tab. 1). We then analyze why this is the case and provide techniques to improve performance (Sec. 4). Using insights from this analysis, we develop PARC, a method that outperforms other methods on this benchmark (Sec. 5). Finally, we show that these results generalize to a larger experiment with an extra dataset and 33 additional off-the-shelf pretrained models downloaded from the internet (for a total of 65 source models and 423 transfers) and by extending PARC to object detection (Sec. 6). We have released all benchmarks and evaluation code at https://dbolya.github.io/parc/ in hopes to further the development of this promising area of research. 2