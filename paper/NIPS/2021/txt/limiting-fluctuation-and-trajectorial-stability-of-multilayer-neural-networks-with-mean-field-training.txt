Abstract
The mean ﬁeld theory of multilayer neural networks centers around a particu-lar inﬁnite-width scaling, in which the learning dynamics is shown to be closely tracked by the mean ﬁeld limit. A random ﬂuctuation around this inﬁnite-width limit is expected from a large-width expansion to the next order. This ﬂuctuation has been studied only in the case of shallow networks, where previous works em-ploy heavily technical notions or additional formulation ideas amenable only to that case. Treatment of the multilayer case has been missing, with the chief difﬁ-culty in ﬁnding a formulation that must capture the stochastic dependency across not only time but also depth.
In this work, we initiate the study of the ﬂuctuation in the case of multilayer net-works, at any network depth. Leveraging on the neuronal embedding framework recently introduced by Nguyen and Pham [17], we systematically derive a system of dynamical equations, called the second-order mean ﬁeld limit, that captures the limiting ﬂuctuation distribution. We demonstrate through the framework the com-plex interaction among neurons in this second-order mean ﬁeld limit, the stochas-ticity with cross-layer dependency and the nonlinear time evolution inherent in the limiting ﬂuctuation. A limit theorem is proven to relate quantitatively this limit to the ﬂuctuation realized by large-width networks.
We apply the result to show a stability property of gradient descent mean ﬁeld training: in the large-width regime, along the training trajectory, it progressively biases towards a solution with “minimal ﬂuctuation” (in fact, vanishing ﬂuctua-tion) in the learned output function, even after the network has been initialized at or has converged (sufﬁciently fast) to a global optimum. This extends a similar phenomenon previously shown only for shallow networks with a squared loss in the empirical risk minimization setting, to multilayer networks with a loss func-tion that is not necessarily convex in a more general setting. 1

Introduction
Recent literature has witnessed much interest and progresses in the mean ﬁeld theory of neural networks. In particular, it is shown that under a suitable scaling, as the widths tend to inﬁnity, the neural network’s learning dynamics converges to a nonlinear deterministic limit, known as the mean
ﬁeld (MF) limit [14, 17]. This line of works starts with analyses of the shallow case under various settings and has led to a number of nontrivial exciting results [18, 14, 5, 23, 25, 9, 22, 19, 29, 24, 30, 12, 1, 16]. The generalization to multilayer neural networks, already much more conceptually and technically challenging, has also been met with serious efforts from different groups of authors, with various novel ideas and insights [15, 17, 20, 2, 26, 6].
Since the MF limit is basically a ﬁrst-order inﬁnite-width approximation of the neural network, it is natural to consider the next order term in the expansion for a more faithful ﬁnite-width description.
⇤The author ordering is randomized. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This leads to the consideration of the ﬂuctuation, up-scaled appropriately with the widths, around the MF limit. On one hand, this ﬂuctuation should display MF interactions among neurons. On the other hand, it is random due to the inherent stochasticity in the ﬁnite-width network which, for instance, is randomly initialized and hence induces randomness in the interactions among neurons.
For shallow networks, [27, 23, 3] has identiﬁed the limiting ﬂuctuation in the form of a time-evolving signed measure over weights, leading to a central limit theorem (CLT) in the space of measures. In particular, [27] pinpoints the ﬂuctuation via a compactness argument in an appropriate measure space. Avoiding the heavy technicality, [3] realizes a neat trick, speciﬁc to shallow networks, where quantities of interest are described via push-forward of the initialized values of the weights.
There has been no similar attempt for the multilayer case, which faces major obstacles in formu-lating the limiting ﬂuctuation. Firstly, existing formulations of the MF limit already move away from working with measures over weights [15, 17, 26, 6], unless restricted conditions are assumed
[2]. This is due to the complexity of MF interactions: the presence of middle layers brings about simultaneous actions of multiple symmetry groups. We face the same complexity when formulating the ﬂuctuation. Secondly, unlike shallow networks, the ﬂuctuation in multilayer networks displays stochastic dependency not only through time but also across layers. Speciﬁcally the cross-layer dependency is propagated by both forward and backward propagation signals, at any time instance.
Contributions. We tackle the challenge by leveraging on the neuronal embedding framework, recently proposed by [17]. An important concept that is supported by the neuronal embedding is the sampling of neurons. We use this concept to formulate the limiting ﬂuctuation via a decomposition into two components with different roles in Section 3. One component is a random process, which encodes CLT-like stochasticity in the ﬂuctuation of the sampled neurons around the MF ensemble.
The other component, named the second-order mean ﬁeld limit, is a system of ordinary differential equations (ODEs), which displays MF interactions in the deviation of the neural network under gradient descent (GD) training from the sampled neurons.
This decomposition is an innovation of the work. Our formulation enjoys the generality brought about by the neuronal embedding framework without restrictive assumptions (e.g. i.i.d. initialization assumption made in [2, 26]) and faithfully describes the expected stochastic dependency across time and depth. We prove a limit theorem that establishes the connection with the ﬂuctuation in ﬁnite-sized networks (Theorems 5 and 6). Unlike [27, 3], this result is quantitative.
Using this formulation, in Section 4, we show a trajectorial stability property in a large-width mul-tilayer network, particularly a variance reduction effect: GD training traverses a solution trajectory that reduces and eventually eliminates the (width-corrected) variance of the learned output function.
That is, a bias towards “minimal ﬂuctuation”. This holds even if the network is initialized at, or if it is trained to converge sufﬁciently fast to, a global optimum (Theorems 9 and 10). The same phenomenon has been shown in the shallow case [3], which requires a squared loss in the empirical risk minimization setup. As demonstrated by our result, it is not an isolated phenomenon and can indeed hold for a neural architecture where MF interactions are more complex, a loss function that is not necessarily convex and a training setup unrestricted to ﬁnitely many training data points.
Limitations and potential extensions. Let us ﬁnally mention a few limitations. Our work consid-ers fully-connected multilayer networks trained with GD, a setup much less general than [17]. We expect certain extensions in this direction are doable. We also do not treat stochastic GD and hence disregard the stochasticity of data sampling, unlike [27]. Given the broader literature on this subject (e.g. [11]), this extension is foreseeable. Here we focus instead on the stochasticity that is inherent in the interactions among neurons, which is the more interesting aspect of neural networks.
Our result on the output variance is speciﬁc to unregularized training, unlike [3], and also requires a sufﬁcient global convergence rate. While there have been several proven global convergence guar-antees for multilayer networks [17, 21, 20], understanding of the convergence rate is still lacking.
Even in the shallow case, global convergence has been studied only for a type of sparsity-inducing regularization [5, 4]. Unless the convergence rate for multilayer networks is generally perilous (an unlikely scenario in light of the experiments in [15]), our result is expected to be relevant.
Our development is speciﬁc to the MF scaling. This scaling allows for nonlinear feature learning, unlike the NTK scaling [8]. While there are other scalings that also admit a certain sense of feature learning [7, 31], the standard parameterization in practice – in the inﬁnite-width limit – is known to degenerate into NTK-like behaviors, which are not expected of practical ﬁnite-but-large-width 2
neural networks [13, 31]. In other words, all inﬁnite-width scalings that display feature learning are only proxies of practical networks. This limitation motivates ﬁnite-width studies as we pursue here.
Notations. We use K to denote a generic constant that may change from line to line, and similarly
Ku for a ﬁnite constant that may depend on some constant u. For simplicity, for L the network depth, we write K in place of KL. For E (resp. E) being the expectation, we use V and C (resp. V and C) for the variance and covariance. We write EZ for the expectation w.r.t. data Z = (X, Y )
. We write @if to denote the partial derivative w.r.t. the i-th variable of f .
⇠P 2