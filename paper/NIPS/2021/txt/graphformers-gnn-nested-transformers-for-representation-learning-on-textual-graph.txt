Abstract
The representation learning on textual graph is to generate low-dimensional embed-dings for the nodes based on the individual textual features and the neighbourhood information. Recent breakthroughs on pretrained language models and graph neu-ral networks push forward the development of corresponding techniques. The existing works mainly rely on the cascaded model architecture: the textual fea-tures of nodes are independently encoded by language models at ﬁrst; the textual embeddings are aggregated by graph neural networks afterwards. However, the above architecture is limited due to the independent modeling of textual features.
In this work, we propose GraphFormers, where layerwise GNN components are nested alongside the transformer blocks of language models. With the proposed architecture, the text encoding and the graph aggregation are fused into an iterative workﬂow, making each node’s semantic accurately comprehended from the global perspective. In addition, a progressive learning strategy is introduced, where the model is successively trained on manipulated data and original data to reinforce its capability of integrating information on graph. Extensive evaluations are conducted on three large-scale benchmark datasets, where GraphFormers outperform the
SOTA baselines with comparable running efﬁciency. The source code is released at https://github.com/microsoft/GraphFormers . 1

Introduction
The textual graph is a widely existed data format, where each node is annotated with its textual feature.
The representation learning on textual graph is to generate low-dimensional node embeddings based on the individual textual features and the information from the neighbourhood. In recent years, the breakthroughs in pretrained language models and graph neural networks contribute to the development of corresponding techniques. Particularly, with pretrained language models, such as BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019a), the underlying semantics of texts can be captured more precisely; at the same time, with graph neural networks, like GraphSage (Hamilton et al., 2017a) and
GAT (Veliˇckovi´c et al., 2018), neighbours can be effectively aggregated for more informative node embeddings. It is necessary to combine both techniques for better textual graph representation. As
∗Work was done by 2021.01 during Junhan Yang and Shitao Xiao’s internship in MSRA 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Model architecture comparison (a center node C is connected with two neighbours N1, N2). (A) Cascaded Transformers-GNN: text embeddings are independently generated by language models and aggregated by rear-mounted GNNs. (B) GNN-nested Transformers: the text encoding and graph aggregation are iteratively performed with the layerwise GNNs and Transformers (TRM). suggested by GraphSage (Hamilton et al., 2017a) and PinSage (Ying et al., 2018), the textual feature can be independently modeled by text encoders and further aggregated by rear-mounted GNNs for the ﬁnal node embeddings. Such a representation paradigm has been widely adopted by subsequent works on various scenarios (Zhu et al., 2021; Li et al., 2021; Hu et al., 2020; Liu et al., 2019b; Zhou et al., 2019), where GNNs are combined with powerful PLM-based text encoders.
The above way of combination is called the “Cascaded Transformers-GNN” architecture (Figure 1 A), as the language models (built upon Transformers) are deployed ahead of the GNN component. With the above architecture, the text encoding and the graph aggregation are performed in two consecutive steps, where there is no information exchange between the nodes when text embeddings are generated.
However, the above workﬂow is defective considering that the linked nodes are correlated, whose underlying semantics can be mutually enhanced. For example, given a node “notes on transformers” and its neighbour “tutorials on machine translation”; by making reference to the whole context, the
“transformers” here can be interpreted as a machine learning model, rather than an electric device.
Our Work. We propose “GNN-nested Transformers” (GraphFormers), which are highlighted for the fusion of GNNs and language models (Figure 1 B). In GraphFormers, the GNN components are nested alongside the transformer layers (TRM) of language models, where the text encoding and graph aggregation are fused as an iteratively workﬂow. In each iteration, the linked nodes will exchange information with each other in the layerwise GNN component; thus, each node will be augmented by its neighbourhood information. The transformer component will work on the augmented node features, where increasingly informative node representations can be generated for the next iteration.
Compared with the cascaded architecture, GraphFormers achieve more sufﬁcient utilization of the cross-node information on graph, which signiﬁcantly beneﬁt the representation quality. Given that the layerwise GNN components merely involve simple and effective multi-head attention, GraphFormers preserve comparable running costs as the existing cascaded Transformers-GNN models.
On top of the proposed model architecture, we further improve GraphFormers’ representation quality and practicability as follows. Firstly, the training of GraphFormers is likely to be shortcut: in many cases, the center node itself can be “sufﬁciently informative”, where the training tasks can be accomplished without leveraging the neighbourhood information. As such, GraphFormers may end up with insufﬁciently trained GNNs. Inspired by recent success of curriculum learning (Bengio et al., 2009), we propose to train the model progressively: the ﬁrst round of training is performed with manipulated data, where the nodes are randomly polluted; thus, it becomes harder to make prediction merely rely on the center nodes, and the model will be forced to leverage the whole input nodes.
The second round of training gets back to the unpolluted data, where the model will be ﬁt into the targeted distribution. Another concern about GraphFormers is that all the linked nodes are mutually dependent in the representation process: once a new node is presented, all the neighbours, regardless of whether they have been processed before, need to be encoded from scratch. As a result, a great deal of unnecessary computations will be incurred. We introduce unidirectional graph attention to alleviate this problem: only the center node is required to make reference to the neighbours, while the neighbour nodes remain independently encoded. By this means, the existing neighbours’ encoding results can be cached and reused, which signiﬁcantly saves the computation cost. 2
Extensive evaluations are conducted with three million-scale textual graph datasets: DBLP, Wiki and
Product, where the representation quality is measured by the link prediction accuracy. According to our experiment results, GraphFormers signiﬁcantly outperform the SOTA cascaded Transformers-GNN baselines with comparable running efﬁciency. 2