Abstract
Invariant Risk Minimization (IRM) is a recently proposed framework for out-of-distribution (o.o.d) generalization. Most of the studies on IRM so far have focused on theoretical results, toy problems, and simple models. In this work, we investigate the applicability of IRM to bias mitigation—a special case of o.o.d generalization—in increasingly naturalistic settings and deep models. Using natural language inference (NLI) as a test case, we start with a setting where both the dataset and the bias are synthetic, continue with a natural dataset and synthetic bias, and end with a fully realistic setting with natural datasets and bias. Our results show that in naturalistic settings, learning complex features in place of the bias proves to be difﬁcult, leading to a rather small improvement over empirical risk minimization. Moreover, we ﬁnd that in addition to being sensitive to random seeds, the performance of IRM also depends on several critical factors, notably dataset size, bias prevalence, and bias strength, thus limiting IRM’s advantage in practical scenarios. Our results highlight key challenges in applying IRM to real-world scenarios, calling for a more naturalistic characterization of the problem setup for o.o.d generalization. 1

Introduction
Deep learning models show strong performance when tested on data from the same distribution they were trained on, matching or even surpassing humans (Zhang et al., 2017; Brinker et al., 2019).
However, this performance often stems from relying on spurious correlations rather than human-like reasoning, causing these models to “break” in real world scenarios (Geirhos et al., 2018; McCoy et al., 2019). A recent method called Invariant Risk Minimization (IRM; Arjovsky et al. 2020) aims to learn causal features whose correlation with the label is invariant across different distributions, thus leading to better out-of-distribution (o.o.d) generalization. IRM splits the training data into different subsets, or environments, across which varying spurious correlations cause distribution shifts. The goal is to learn a representation that yields the same optimal classiﬁer for all environments.
Although several studies (Choe et al., 2020; Ahuja et al., 2020a; Kamath et al., 2021) have investigated
IRM and its variants, the empirical results so far have mainly focused on synthetic settings and simpler models (e.g., shallow multilayer-perceptrons, bag-of-words models). This motivates us to investigate
IRM in natural settings with complex models. We take natural language inference (NLI) as a test case, where the model needs to predict if a hypothesis sentence is entailed by the premise sentence. While theoretically requiring deep language understanding, many NLI datasets contain biases, or spurious correlations between superﬁcial features and labels (Gururangan et al., 2018; Poliak et al., 2018;
Tsuchiya, 2018), such as word overlap correlating with the entailment label. Such heuristics allow models to perform superﬁcially well on the benchmark, but fail catastrophically when these heuristics
∗Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
no longer hold (Naik et al., 2018; Glockner et al., 2018; McCoy et al., 2019). Common debiasing methods typically rely on explicit modeling of the biases (Belinkov et al., 2019a,b; Karimi Mahabadi et al., 2020; He et al., 2019). In contrast, IRM offers interesting possibilities. It is model agnostic, not built for a speciﬁc bias and, given appropriate environments, generalizes to distributions where the spurious correlation varies.
In this work, we design a series of debiasing experiments, intended to bridge the gap between fully synthetic and naturalistic scenarios. Table 1 shows examples of each setting. We begin with a toy experiment where both the dataset and the bias are synthetic. Next, we add a level of complexity by injecting synthetic bias to a real NLI dataset and use state-of-the-art pre-trained models. Last, we investigate natural NLI datasets with known dataset biases. For the last setting, splitting environments using categorical biased features as done in previous work is not applicable. Therefore, for real-world scenarios where the bias is known but might be a high dimensional feature, we develop a simple way to split data into different environments, described in “Environment generation” in Section 3.2.
In each of these settings, we train models with empirical risk minimization (ERM) and IRM and evaluate them on o.o.d test sets. Our experiments yield the following results:
• In the toy setting, performance follows the theory: ERM performs well on the training set but fails completely on the o.o.d test set, while IRM ignores the bias and thus performs slightly worse on the training set, but perfectly on the o.o.d test set.
• On natural NLI datasets and deep models—with either synthetic bias or natural bias—IRM outperforms ERM when evaluated on o.o.d test sets.
• However, in these more naturalistic settings, IRM is not able to completely discard the bias, while
ERM does not rely solely on the bias. Thus, in practice the advantage of IRM is small.
• When used with state-of-the-art models, IRM shows a large variance in performance across random seeds.
To better characterize when IRM works better or worse than ERM, we highlight three criteria that govern its success: the prevalence of the bias in the training set (how many examples are biased), the strength of the bias (how strong is the correlation between a label and a biased feature), and the training data size. While bias strength and data size were not empirically investigated in real-world scenarios, bias prevalence was completely overlooked in previous work. We ﬁnd that when all three criteria are met (prevalent and strong bias, large training set), IRM tends to perform better than
ERM. When bias prevalence, strength, or training data size are limited, IRM is less stable and ERM performs better.
To conclude, our investigation shows important challenges in assessing the performance of IRM in plausible real-world scenarios, pointing to the need to employ a more naturalistic approach to characterizing the settings in which IRM can perform well. 2