Abstract
We introduce a new general identiﬁable framework for principled disentanglement referred to as Structured Nonlinear Independent Component Analysis (SNICA).
Our contribution is to extend the identiﬁability theory of deep generative models for a very broad class of structured models. While previous works have shown identiﬁability for speciﬁc classes of time-series models, our theorems extend this to more general temporal structures as well as to models with more complex structures such as spatial dependencies. In particular, we establish the major result that identiﬁability for this framework holds even in the presence of noise of unknown distribution. Finally, as an example of our framework’s ﬂexibility, we introduce the ﬁrst nonlinear ICA model for time-series that combines the following very useful properties: it accounts for both nonstationarity and autocorrelation in a fully unsupervised setting; performs dimensionality reduction; models hidden states; and enables principled estimation and inference by variational maximum-likelihood. 1

Introduction
A central tenet of unsupervised deep learning is that noisy and high dimensional real world data is generated by a nonlinear transformation of lower dimensional latent factors. Learning such lower dimensional features is valuable as they may allow us to understand complex scientiﬁc observations in terms of much simpler, semantically meaningful, representations (Morioka et al., 2020; Zhou and
Wei, 2020). Access to a ground truth generative model and its latent features would also greatly enhance several other downstream tasks such as classiﬁcation (Klindt et al., 2021; Banville et al., 2021), transfer learning (Khemakhem et al., 2020b), as well as causal inference (Monti et al., 2019;
Wu and Fukumizu, 2020).
A recently popular approach to deep representation learning has been to learn disentangled features.
Whilst not rigorously deﬁned, the general methodology has been to use deep generative models such as VAEs (Kingma and Welling, 2014; Higgins et al., 2017) to estimate semantically distinct factors of variation that generate and encode the data. A substantial problem with the vast majority of work on disentanglement learning is that the models used are not identiﬁable – that is, they do not learn the true generative features, even in the limit of inﬁnite data – in fact, this task has been proven
∗hermanni.halva@helsinki.ﬁ
†Equal senior authorship 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
impossible without inductive biases on the generative model (Hyvärinen and Pajunen, 1999; Locatello et al., 2019). Lack of identiﬁability plagues deep learning models broadly and has been implicated as one of the reasons for unexpectedly poor behaviour when these models are deployed in real world applications (D’Amour et al., 2020). Fortunately, in many applications the data have dependency structures, such as temporal dependencies which introduce inductive biases. Recent advances in both identiﬁability theory and practical algorithms for nonlinear ICA (Hyvärinen and Morioka, 2016, 2017;
Hälvä and Hyvärinen, 2020; Morioka et al., 2021; Klindt et al., 2021; Oberhauser and Schell, 2021) exploit this and offer a principled approach to disentanglement for such data. Learning statistically independent nonlinear features in such models is well-deﬁned, i.e. those models are identiﬁable.
However, the existing nonlinear ICA models suffer from numerous limitations. First, they only exploit speciﬁc types of temporal structures, such as either temporal dependencies or nonstationarity.
Second, they often work under the assumption that some ’auxiliary’ data about a latent process is observed, such as knowledge of the switching points of a nonstationary process as in Hyvärinen and Morioka (2016); Khemakhem et al. (2020a) . Furthermore, all the nonlinear ICA models cited above, with the exception of Khemakhem et al. (2020a), assume that the data are fully observed and noise-free, even though observation noise is very common in practice, and even Khemakhem et al. (2020a) assumes the noise distribution to be exactly known. This approach of modelling observation noise explicitly is in stark contrast to the approach taken in papers, such as Locatello et al. (2020), who instead consider general stochasticity of their model to be captured by latent variables – this approach would be ill-suited to the type of denoising one would often need in practice. Lastly, the identiﬁability theorems in previous nonlinear ICA works usually restrict the latent components to a speciﬁc class of models such as exponential families (but see Hyvärinen and Morioka (2017)).
In this paper we introduce a new framework for identiﬁable disentanglement, Structured Nonlinear
ICA (SNICA), which removes each of the aforementioned limitations in a single unifying framework.
Furthermore, the framework guarantees identiﬁability of a rich class of nonlinear ICA models that is able to exploit dependency structures of any arbitrary order and thus, for instance, extends to spatially structured data. This is the ﬁrst major theoretical contribution of our paper.
The second important theoretical contribution of our paper proves that models within the SNICA framework are identiﬁable even in the presence of additive output noise of arbitrary, unknown distribution. We achieve this by extending the theorems by Gassiat et al. (2020b,a). The subsequent practical implication is that SNICA models can perform dimensionality reduction to identiﬁable latent components and de-noise observed data. We note that noisy-observation part of the identiﬁability theory is not even limited to nonlinear ICA but applies to any system observed under noise.
Third, we give mild sufﬁcient conditions, relating to the strength and the non-Gaussian nature of the temporal or spatial dependencies, enabling identiﬁability of nonlinear independent components in this general framework. An important implication is that our theorems can be used, for example, to develop models for disentangling identiﬁable features from spatial or spatio-temporal data.
As an example of the ﬂexibility of the SNICA framework, we present a new nonlinear ICA model called ∆-SNICA . It achieves the following very practical properties which have previously been unattainable in the context of nonlinear ICA: the ability to account for both nonstationarity and autocorrelation in a fully unsupervised setting; ability perform dimensionality reduction; model latent states; and to enable principled estimation and inference by variational maximum-likelihood methods.
We demonstrate the practical utility of the model in an application to noisy neuroimaging data that is hypothesized to contain meaningful lower dimensional latent components and complex temporal dynamics. 2