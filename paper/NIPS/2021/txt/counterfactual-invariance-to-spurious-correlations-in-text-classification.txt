Abstract
Informally, a ‘spurious correlation’ is the dependence of a model on some aspect of the input data that an analyst thinks shouldn’t matter.
In machine learning, these have a know-it-when-you-see-it character; e.g., changing the gender of a sentence’s subject changes a sentiment predictor’s output. To check for spurious correlations, we can ‘stress test’ models by perturbing irrelevant parts of input data and seeing if model predictions change. In this paper, we study stress test-ing using the tools of causal inference. We introduce counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn’t change model predictions. We connect counterfactual invariance to out-of-domain model performance, and provide practical schemes for learning (ap-proximately) counterfactual invariant predictors (without access to counterfactual examples). It turns out that both the means and implications of counterfactual invariance depend fundamentally on the true underlying causal structure of the data—in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactual invariance. Similarly, counterfactual invariance implies different domain shift guarantees depending on the underlying causal structure. This the-ory is supported by empirical results on text classiﬁcation. 1

Introduction
Our focus in this paper is the sort of spurious correlations revealed by “poke it and see what happens” testing procedures for machine-learning models. For example, we might test a sentiment analysis tool by changing one proper noun for another (“tasty Mexican food” to “tasty Indian food”), with the expectation that the predicted sentiment should not change. This kind of perturbative stress testing is increasingly popular: it is straightforward to understand and offers a natural way to test the behavior of models against the expectations of practitioners [Rib+20; Wu+19; Nai+18].
Intuitively, models that pass such stress tests are preferable to those that do not. However, fundamen-tal questions about the use and meaning of perturbative stress tests remain open. For instance, what is the connection between passing stress tests and model performance on prediction? Eliminating predictor dependence on a spurious correlation should help with domain shifts that affect the spuri-ous correlation—but how do we make this precise? And, how should we develop models that pass stress tests when our ability to generate perturbed examples is limited? For example, automatically perturbing the sentiment of a document in a general fashion is difﬁcult.
The ad hoc nature of stress testing makes it difﬁcult to give general answers to such questions. In this paper, we will use the tools of causal inference to formalize what it means for models to pass stress tests, and use this formalization to answer the questions above. We will formalize passing stress tests as counterfactual invariance, a condition on how a predictor should behave when given 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
certain (unobserved) counterfactual input data. We will then derive implications of counterfactual invariance that can be measured in the observed data. Regularizing predictors to satisfy these ob-servable implications provides a means for achieving (partial) counterfactual invariance. Then, we will connect counterfactual invariance to robust prediction under certain domain shifts, with the aim of clarifying what counterfactual invariance buys and when it is desirable.
An important insight that emerges from the formalization is that the true underlying causal structure of the data has fundamental implications for both model training and guarantees. Methods for handing ‘spurious correlations’ in data with a given causal structure need not perform well when blindly translated to another causal structure.
Counterfactual Invariance Consider the problem of learning a predictor f that predicts a label Y from covariates X. In this paper, we’re interested in constructing predictors whose predictions are invariant to certain perturbations on X. Our ﬁrst task is to formalize the invariance requirement.
To that end, assume that there is an additional variable Z that captures information that should not inﬂuence predictions. However, Z may causally inﬂuence the covariates X. Using the potential outcomes notation, let X(z) to denote the counterfactual X we would have seen had Z been set to z, leaving all else ﬁxed. Informally, we can understand perturbative stress tests as a way of producing particular realizations of counterfactual pairs X(z), X(z(cid:48)) that differ by an intervention on z. Then, we formalize the requirement that an arbitrary change to z does not change predictions:
Deﬁnition 1.1. A predictor f is counterfactually invariant to Z if f (X(z)) = f (X(z(cid:48))) almost everywhere, for all z, z(cid:48) in the sample space of Z. When Z is clear from context, we’ll just say the predictor is counterfactually invariant. 2 Causal Structure
Counterfactual invariance is a condition on how the predicted label behaves under interventions on parts of the input data. However, intuitions about stress testing are based on how the true label behaves under interventions on parts of the input data. We will see that the true causal structure fundamentally affects both the implications of counterfactual invariance, and the techniques we use to achieve it. To study this phenomenon, we’ll use two causal structures that are commonly encountered in applications; see Figure 1. 2.1 Prediction in the Causal Direction
We begin with the case where X is a cause of Y .
Example 2.1. We want to automatically classify the quality of product reviews. Each review has a number of “helpful” votes Y (from site users). We predict Y using the text of the product review
X. However, we ﬁnd interventions on the sentiment Z of the text change our prediction; changing
“Great shoes!” to “Bad shoes!” changes the prediction.
Y
Y
Z
X ⊥
Z
X ⊥
Z
XY ∧Z
In the examples in this paper, the covariate X is text data. Usually, the causal relationship between the text and Y and Z will be complex— the relationships may depend e.g., on abstract, unlabeled, parts of the text such as topic, writing quality, or tone.
In principle, we could enu-merate all such latent variables, con-struct a causal graph capturing the relationships between these variables and Y, Z, and use this causal struc-ture to study counterfactual invari-ance. For instance, if we think that topic causally inﬂuences the helpful-ness Y , but is not inﬂuenced by sen-timent Z, then we could build a counterfactually invariant predictor by extracting the topic and
Figure 1: Causal models for the data generating process. We de-compose the observed covariate X into latent parts deﬁned by their causal relationships with Z and Y . Solid arrows denote causal rela-tionships, while dashed lines denote non-causal associations. The differences between these causal structures will turn out to be key for understanding counterfactual invariance. (b) Anticausal direction (a) Causal direction
XY ∧Z
X ⊥
Y
X ⊥
Y
Z 2
predicting Y using topic alone. However, exhaustively articulating all possible such variables is a herculean task.
Instead, notice that the only thing that’s relevant about these latent variables is their causal relation-ship with Y and Z. Accordingly, we’ll decompose the observed variable X into parts deﬁned by their causal relationships with Y and Z. We remain agnostic to the semantic interpretation of these parts. Namely, we deﬁne X ⊥
Z as the part of X that is not causally inﬂuenced by Z (but may inﬂuence
Y ), X ⊥
Y as the part that does not causally inﬂuence Y (but may be inﬂuenced by Z), and XY ∧Z is the remaining part that is both inﬂuenced by Z and that inﬂuences Y . The causal structure is shown in Figure 1a.
We see there are two paths that lead to Y and Z being associated. The ﬁrst is when Z affects
XY ∧Z which, in turn, affects Y . For example, a very enthusiastic reviewer might write a longer, more detailed review, which will in turn be more helpful. The second is when a common cause or selection effect in the data generating process induces an association between Z and Y , which we denote with a dashed arrow. For example, if books tend to get more positive reviews, and also people who buy books are more likely to ﬂag reviews as helpful, then the product type would be a common cause of sentiment and helpfulness. 2.2 Prediction in the Anti-Causal Direction
We also consider the case where Y causes X.
Example 2.2. We want to predict the star rating Y of movie reviews from the text X. However, we
ﬁnd that predictions are inﬂuenced by the movie genre Z; e.g., changing “Adam Sandler” (a comedy actor) to “Hugh Grant” (a romance actor) changes the predictions.
Figure 1b shows the causal structure. Here, the observed X is inﬂuenced by both Y and Z. Again, we decompose X into parts deﬁned by their causal relationship with Z and Y . Here, Z (and thus
X ⊥
Y ) can be associated with Y through two paths. First, if XY ∧Z is non-trivial, then conditioning on it causes a dependence between Z and Y (because XY ∧Z is a collider). For example, if Adam
Sandler tends to appear in good comedy movies but bad movies of other genres then seeing “San-dler” in the text induces a dependency between sentiment and genre. Second, Z and Y may be associated due to a common cause, or due to selection effects in the data collection protocol—this is represented by the dashed line between Z and Y . For example, fans of romantic comedies may tend to give higher reviews (to all ﬁlms) than fans of horror movies. 2.3 Non-Causal Associations
Frequently, a predictor trained to predict Y from X will rely on X ⊥ connection between Y and X ⊥
X ⊥
Y , even though there is no causal
Y , and therefore will fail counterfactual invariance. The reason is that
Y serves as a proxy for Z, and Z is predictive of Y due to the non-causal (dashed line) association.
There are two mechanisms that can induce such associations. First, Y and Z may be confounded: they are both inﬂuenced by an unobserved common cause U . For example, people who review books may be more upbeat than people who review clothing. This leads to positive sentiments and high helpfulness votes for books, creating an association between sentiment and helpfulness. Second,
Y and Z may be subject to selection: there is some condition (event) S that depends on Y and
Z, such that a data point from the population is included in the sample only if S = 1 holds. For example, our training data might only include movies with at least 100 reviews. If only excellent horror movies have so many reviews (but most rom-coms get that many), then this selection would induce an association between genre and score. Formally, the dashed-line causal graphs mean our sample is distributed according to P (X, Y, Z) = (cid:82) P(X, Y, Z, u | S = 1)dP(u) where Y, Z are caused by U and are causes of S, and (X, Y, Z) are causally related according to the graph.
In addition to the non-causal dashed-line relationship, there is also dependency induced by between
Y and Z by XY ∧Z. Whether or not each of these dependencies is “spurious” is a problem-speciﬁc judgement that must be made by each analyst based on their particular use case. E.g., using genre to predict sentiment may or may not be reasonable, depending on the actual application in mind.
However, there is a special case that captures a common intuition for purely spurious association.
Deﬁnition 2.3. We say that the association between Y and Z is purely spurious if Y ⊥⊥ X | X ⊥
Z , Z. 3
That is, if the dashed-line association did not exist (removed by conditioning on Z) then the part of
X that is not inﬂuenced by Z would sufﬁce to estimate Y . 3 Observable Signatures of Counterfactually Invariant Predictors
We now consider the question of how to achieve counterfactual invariance in practice. The challenge is that counterfactual invariance is deﬁned by the behavior of the predictor on counterfactual data that is never actually observed. This makes checking counterfactual invariance impossible. Instead, we’ll derive a signature of counterfactual invariance that actually can be measured—and enforced— using ordinary datasets where Z (or a proxy) is measured. For example, the star rating of a review as a proxy for sentiment, or genre labels in the movie review case.
Intuitively, a predictor f is counterfactually invariant if it depends only on X ⊥ not affected by Z. To formalize this, we need to show that such a X ⊥
Lemma 3.1. Let X ⊥ we have that f is counterfactually invariant if and only if f (X) is X ⊥ then such a X ⊥
Z be a X-measurable random variable such that, for all measurable functions f ,
Z -measurable. If Z is discrete1
Z , the part of X that is
Z is well deﬁned:
Z exists.
Accordingly, we’d like to construct a predictor that is a function of X ⊥
Z measurable).
The key insight is that we can use the causal graphs to read off a set of conditional independence relationships that are satisﬁed by Z, X ⊥
Z , Y . Critically, these conditional independence relationships are testable from the observed data. Thus, they provide a signature of counterfactual invariance:
Theorem 3.2. If f is a counterfactually invariant predictor:
Z only (i.e., is X ⊥ 1. Under the anti-causal graph, f (X) ⊥⊥ Z | Y . 2. Under the causal-direction graph, if Y and Z are not subject to selection (but possibly confounded), f (X) ⊥⊥ Z. 3. Under the causal-direction graph, if the association is purely spurious, Y ⊥⊥ X | X ⊥
Z , Z, and Y and Z are not confounded (but possibly selected), f (X) ⊥⊥ Z | Y .
Remark 3.3 (Connection to Fairness). This result has an interesting interpretation when Z is a pro-tected attribute (e.g., sex or race) that we’d like to be fair with respect to. There are many ways of formalizing fairness, which are usually mutually incompatible. In the fairness setting, counterfactual invariance is equivalent to counterfactual fairness [Kus+17; Gar+19], the condition f (X) ⊥⊥ Z is equivalent to demographic parity, and the condition f (X) ⊥⊥ Z | Y is equivalent to equalized odds
[Meh+19]. Theorem 3.2 then says that counterfactual fairness implies either demographic parity or equalized odds, depending on the true underlying causal structure of the problem. Hence, the relationship between disparate fairness notions is clariﬁed by taking the underlying causal structure into account. This also suggests we can take counterfactual fairness as the fundamental notion, then use demographic parity in the causal-confounding case and equalized odds otherwise. However, we leave the (ethical) question of whether this is a sound strategy to future work.
Causal Regularization Without access to counterfactual examples, we cannot directly enforce counterfactual invariance. However, we can require a trained model to satisfy the counterfactual in-variance signature of Theorem 3.2. The hope is that enforcing the signature will lead the model to be counterfactually invariant. To do this, we regularize the model to satisfy the appropriate conditional independence condition. For simplicity of exposition, we restrict to binary Y and Z. The (inﬁnite data) regularization terms are marginal regularization = MMD(P(f (X) | Z = 0), P(f (X) | Z = 1)) conditional regularization = MMD(P(f (X) | Z = 0, Y = 0), P(f (X) | Z = 1, Y = 0)) (3.1) (3.2)
+ MMD(P(f (X) | Z = 0, Y = 1), P(f (X) | Z = 1, Y = 1)).
Maximum mean discrepancy (MMD) is a metric on probability measures.2 The marginal indepen-dence condition is equivalent to (3.1) equal 0, and the conditional independence is equivalent to (3.2) 1In fact, it sufﬁces that all potential outcomes {Y (z)}z are jointly measurable with respect to a single well-behaved sigma algebra; discrete Z is sufﬁcient but not necessary. 2The choice of regularization by MMD is for concreteness. Any technique for enforcing the independence signatures would do in principle—e.g., adversarial methods borrowed from the fairness literature. The key point here is the observation that different causal structures imply different independence signatures. 4
equal 0. In practice, we can estimate the MMD with ﬁnite data samples [Gre+12]. When training with stochastic gradient descent, we compute the penalty on each minibatch. if the data has causal-direction structure and the Y ↔ Z association is
The procedure is then: due to confounding, add the marginal regularization term to the the training objective. If the data has anti-causal structure, or the association is due to selection, add the conditional regularization term instead. In this way, we regularize towards models that satisfy the counterfactual invariance signature.
A key point is that the regularizer we must use depends on the true causal structure. The conditional and marginal independence conditions are generally incompatible. Enforcing the condition that is mismatched to the true underlying causal structure may fail to encourage counterfactual invariance, or may throw away more information than is required.
Gap to Counterfactual Invariance The conditional independence signature of Theorem 3.2 is necessary but not sufﬁcient for counterfactual invariance. This is for two reasons. First, counter-factual invariance applies to individual datapoint realizations, but the signature is distributional. In particular, the invariance P(f (X) | do(Z = z)) = P(f (X) | do(Z = z(cid:48))) for all z, z(cid:48) would also imply the conditional independence signature. But, this invariance is weaker than counterfactual invariance, since it doesn’t require access to counterfactual realizations. Second, f (X) ⊥⊥ Z does not imply, in general, that Z is not a cause of f (X). This (unusual) behavior can happen if, e.g., there are levels of Z that we do not observe in the training data, or there are variables omitted from the causal graph that are a common cause of Z and X.
Unfortunately, the gap between the signature and counterfactual invariance is a fundamental re-striction of using observational data. The conditional independence signature is in some sense the closest proxy for counterfactual invariance we can hope for. In section 5, we’ll see that enforcing the signature does a good job of enforcing counterfactual invariance in practice. 4 Performance Out of Domain
Counterfactual invariance is an intuitively desirable property for a predictor to have. However, it’s not immediately clear how it relates to model performance as measured by, e.g., accuracy. Intu-itively, eliminating predictor dependence on a spurious Z may help with domain shift, where the data distribution in the target domain differs from the distribution of the training data. We now turn to formalizing this idea.
First, we must articulate the set of domain shifts to be considered. In our setting, the natural thing is to hold the causal relationships ﬁxed across domains, but to allow the non-causal (“spurious”) de-pendence between Y and Z to vary. Demanding that the causal relationships stay ﬁxed reﬂects the requirement that the causal structure describes the dynamics of an underlying real-world process— e.g., the author’s sentiment is always a cause (not an effect) of the text in all domains. On the other hand, the dependency between Y and Z induced by either confounding or selection can vary without changing the underlying causal structure. For confounding, the distribution of the confounder may differ between domains—e.g., books are rare in training, but common in deployment. For selec-tion, the selection criterion may differ between domains—e.g., we include only frequently reviewed movies in training, but make predictions for all movies in deployment.
We want to capture spurious domain shifts by considering domain shifts induced by selection or confounding. However, there is an additional nuance. Changes to the marginal distribution of Y will affect the risk of a predictor, even in the absence of any spurious association between Y and Z.
Therefore, we restrict to shifts that preserve the marginal distribution of Y .
Deﬁnition 4.1. We say that distributions P, Q are causally compatible if both obey the same causal graph, P (Y ) = Q(Y ), and there is a confounder U and/or selection conditions S, ˜S such that
P = (cid:82) P(X, Y, Z | U, S = 1)d ˜P (U ) and Q = (cid:82) P(X, Y, Z | U, ˜S = 1)d ˜Q(U ) for some ˜P (U ), ˜Q(U ).
We can now connect counterfactual invariance and robustness to domain shift.
Theorem 4.2. Let F invar be the set of all counterfactually invariant predictors. Let L be either square error or cross entropy loss. And, let f ∗ := argminf ∈F invar EP [L(Y, f (X))] be the counter-factually invariant risk minimizer. Suppose that the target distribution Q is causally compatible with the training distribution P . Suppose that any of the following conditions hold: 5
1. the data obeys the anti-causal graph 2. the data obeys the causal-direction graph, there is no confounding (but possibly selection), and the association is purely spurious, Y ⊥⊥ X | X ⊥
Z , Z, or 3. the data obeys the causal-direction graph, there is no selection (but possibly confounding),
Z on Y is additive, i.e., the the association is purely spurious and the causal effect of X ⊥ true data generating process is
Y ← g(X ⊥
Z ) + ˜g(U ) + ξ where E[ξ | X ⊥
Z ] = 0, (4.1) for some functions g, ˜g.
Then, the training domain counterfactually invariant risk minimizer is also the target domain coun-terfactually invariant risk minimizer, f ∗ = argminf ∈F invar EQ[L(Y, f (X))].
Remark 4.3. The causal case with confounding requires an additional assumption (additive struc-ture) because, e.g., an interaction between confounder and X ⊥
Z can yield a case where X ⊥
Z and Y have a different relationship in each domain (whence, out-of-domain learning is impossible).
This result gives a recipe for ﬁnding a good predictor in the target domain even without access to any target domain examples at training time. Namely, ﬁnd the counterfactually invariant risk minimizer in the training domain. In practice, we can use the regularization scheme of section 3 to (approximately) achieve this. We’ll see in section 5 that this works well in practice.
Optimality Theorem 4.2 begs the question: if the only thing we know about the target set-ting is that it’s causally compatible with the training data, is the best predictor the counterfac-tually invariant predictor with lowest training risk? A natural way to formalize this question is to study the predictor with the best performance in the worst case target distribution. We deﬁne Q = {Q : Q causally compatible with P } and the Q-minimax predictor f ∗ minimax = argminf ∈F maxQ∈Q EQ[L(Y, f (X)]. The question is then: what’s the relationship between the counterfactually invariant risk minimizer and the minimax predictor?
Theorem 4.4. The counterfactually invariant risk minimizer is not Q-minimax in general. However, under the conditions of Theorem 4.2, if the association is purely spurious, XY ∧Z ⊥⊥ Y | X ⊥
Z , Z, and
P(Z, Y ) satisﬁes overlap, then the two predictors are the same. By overlap we mean that P(Z, Y ) is a discrete distribution such that for all (z, y), if P(z, y) > 0 then there is some y(cid:48) (cid:54)= y such that also P(z, y(cid:48)) > 0.
Conceptually, Theorem 4.4 just says that the counterfactually invariant predictor excludes XY ∧Z, even when this information is useful in every domain. In the purely spurious case, XY ∧Z carries no useful information, so counterfactual invariance is optimal. 5 Experiments
The main claims of the paper are: 1. Stress test violations can be reduced by suitable conditional independence regularization. 2. This reduction will improve out-of-domain prediction performance. 3. To get the full effect, the imposed penalty must match the causal structure of the data.
Setup To assess these claims, we’ll examine the behavior of predictors trained with the marginal or conditinal regularization on multiple text datasets that have either causal or anti-causal structure.
We expect to see that marginal regularization improves stress test and out-of-domain performance on data with causal-confounded structure, and conditional regularization improves these on data with anti-causal structure.
For each experiment, we use BERT [Dev+19] ﬁnetuned to predict a label Y from the text as our base model. We train multiple causally-regularized models on the each dataset. The training varies by whether we use the conditional or marginal penalty, and by the strength of the regularization term. That is, we train identical architectures using CrossEntropy + λ · Regularizer as the objective function, where we vary λ and take Regularizer as either the marginal penalty, (3.1), or conditional penalty, (3.2). We compare these models’ predictions on data with causal and anti-causal structure.
See supplement for experimental details. 6
Figure 2: Regularizing conditional MMD improves counterfactual invariance on synthetic anti-causal data. Sufﬁciently high regularization of marginal MMD also improves invariance, but impairs accuracy.
Dashed lines show baseline performance of an unregularized predictor. Left: lower conditional MMD implies that predictive probabilities are invariant to perturbation. Although marginal MMD penalization can result in low conditional MMD and good stress test performance, this comes at the cost of very low in-domain accuracy.
Right: MMD regularization reduces the rate of predicted label ﬂips on perturbed data, with little affect on in-domain accuracy. Conditional MMD regularization reduces predicted label ﬂips to 1.4%, while the best result for marginal MMD is 2.8%. 5.1 Robustness to Stress Tests
First, we examine whether enforcing the causal regularization actually helps to enforce counterfac-tual invariance. We create counterfactual (stress test) examples by perturbing the input data and compare the prediction on these. We build the experimental datasets using Amazon reviews from the product category “Clothing, Shoes, and Jewelry” [NLM19].
Synthetic To study the relationship between counterfactual invariance and the distributional sig-nature of Theorem 3.2, we construct a synthetic confound. For each review, we draw a Bernoulli random Z, and then perturb the text X so that the common words “the” and “a” carry information about Z: for example, we replace “the” with the token “thexxxxx” when Z = 1. We take Y to be the review score, and subsample so Y is balanced. This data has anti-causal structure: the text X is written to explain the score Y . Further, we expect that the Y, Z association is purely spurious, because “the” and “a” carry little information about the label.
We train the models on data where P(Y = Z) = 0.3. We then create perturbed stress-test datasets by changing each example Xi(z) to the counterfactual Xi(1 − z) (using the synthetic model). By measuring the performance of each model on the perturbed data, we can test whether the distribu-tional properties enforced by the regularizers result in counterfactual invariance at the instance level.
Figure 2 shows that conditional regularization (matching the anti-causal structure) reduces checklist failures, as measured by the frequency that the predicted label changes due to perturbation as well as the mean absolute difference in predictive probabilities that is induced by perturbation.
Natural To study the relationship in real data, we use the review data in a different way. We now take Z to be the score, binarized as Z ∈
{1 or 2 stars, 4 or 5 stars}. We use this Z as a proxy for sentiment, and consider problems where sentiment should (plausibly) not have a causal effect on Y . For the causal prediction problem, we take Y to be the helpfulness score of the review (binarized as described below).
This is causal because readers decide whether the review is helpful based on the text. For the anti-causal prediction problem, we take Y to be whether “Clothing” is included as a cat-egory tag for the product under review (e.g., boots typically do not have this tag). This is anti-causal because the product category affects the text.
Figure 3: Penalizing the MMD matching the causal structure improves stress test performance on nat-ural product review data. Note that penalizing the wrong MMD may not help: the marginal MMD hurts on the anticausal dataset. Perturbations are generated by swapping positive and negative sentiment adjectives in examples.
We control the strength of the spurious association between Y and Z.
In the anti-causal case, this is done by selection: we randomly subset the data to enforce a target level of dependence between Y and Z. The causal-direction case with confounding is more complicated. To manipulate confounding strength, we binarize the number of helpfulness votes V in a manner determined by 7
the target level of association. We take Y = 1[V > TZ] where TZ is a Z-dependent threshold, chosen to induce a target association. We choose P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = 0.3.
We balance Z by subsampling, which also balances Y .
Now, we create stress test perturbations of these datasets by randomly changing adjectives in the examples. Using predeﬁned lists of postive sentiment adjectives and negative sentiment adjectives, we swap any adjective that shows up on a list with a randomly sampled adjective from the other list.
This preserves basic sentence structure, and thus creates a limited set of counterfactual pairs that differ on sentiment.
Results for differences in predicted probabilities between original and perturbed data are shown in Figure 3. Each point is a trained model, which vary in measured MMD on the test data and on sensitivity to perturbations. Recall that the conditional independence signature of Theorem 3.2 are necessary but not sufﬁcient for counterfactual invariance, so it’s not certain that regularizing to reduce the MMD will reduce perturbation sensitivity. Happily, we see that regularizing to reduce the MMD that matches the causal structure does indeed reduce sensitivity to perturbations.
Notice that regularizing the causally mismatched MMD can have strange effects. Regularizing marginal MMD in the anti-causal case actually makes the model more sensitive to perturbations! 5.2 Domain Shift
Next, we study the effect of causal regularization on model performance under domain shift.
Natural Product Review We again use the natural Amazon review data described above.
For both the causal and anti-causal data, we create multi-ple test sets with variable spu-rious correlation strength. This is done in the manner described above, varying P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) =
γ. Here, γ is the strength of spurious association. The test sets are out-of-domain samples.
By design, Y is balanced in each dataset, so these samples are causally compatible with the training data.
For both the causal and anti-causal datasets, the training data has P(Y = 0 | Z = 0) = P(Y = 1 | Z = 1) = 0.3. We train a classi-ﬁer for each regularization type and regularization strength, and measure the accuracy on each test domain.
The results are shown in Figure 4.
Anti-Causal Data: conditional regularization improves domain-shift robustness.
Causal-Direction Data: marginal regularization improves domain-shift robustness.
Figure 4: The best domain-shift robustness is obtained by using the regularizer that matches the underlying causal structure of the data.
The plots show out-of-domain accuracy for models trained on the (natu-ral) review data. In each row, the left ﬁgure shows out-of-domain accu-racies (lines are models), with the X-axis showing the level of spurious correlation in the test data (0.3 is the training condition); the right ﬁg-ure shows worst out-of-domain accuracy versus in-domain test accuracy (dots are models).
First, the unregularized predictors do indeed learn to rely on the spurious association between senti-ment and the label. The accuracy of these predictors decays dramatically as the spurious assocation moves from negative (0.3) to positive—in the causal case, the unregularized predictor is worse than chance in the 0.8 domain.
Following section 3, the regularization that matches the underlying causal structure should yield a predictor that is (approximately) counterfactually invariant. Following Theorem 4.2, we expect that good performance of a counterfactually-invariant predictor in the training domain should imply good performance in each of the other domains. Indeed, we see that this is so. Models that are regularized to have small values of the appropriate MMD do indeed have better out-of-domain performance. 8
Figure 5: Conditional MMD penalization improves robustness in anti-causal MNLI data. Marginal regu-larization does not improve over the baseline unregularized model, shown with dashed lines. Left: Conditional regularization improves minimum accuracy across (Y, Z) groups. When overregularized, the predictor returns the same ˆY for all inputs, yielding a worst-group accuracy of 0. Right: Conditional MMD regularization sig-niﬁcantly improves worst (Y, Z) group accuracy (y-axis) while only mildly reducing overall accuracy (x-axis).
Such models have somewhat worse in-domain performance, because they no longer exploit the spurious correlation. language inference (MNLI) dataset [WNB18].
MNLI Data For an additional test on naturally-occurring confounds, we use the multi-genre natural
Instances are concatenations of two sentences, and the label describes the semantic relationship between them, Y ∈
{contradiction, entailment, neutral}. There is a well-known confound in this dataset: examples where the second sentence contain a negation word (e.g., “not”) are much more likely to be la-beled as contradictions [Gur+18]. Following Sagawa et al. [Sag+20], we set Z to indicate whether one of a small set of negation words is present. Although Z is derived from the text X, it can be viewed as a proxy for a latent variable indicating whether the author intended to use negation in the text. This is an anti-causal prediction problem: the annotators were instructed to write text to reﬂect the desired label [WNB18].
Following Sagawa et al. [Sag+20], we divide the MNLI data into groups by (Y, Z) and compute the
“worst group accuracy” across all such groups. Because this is an anti-causal problem, we predict that the conditional MMD is a more appropriate penalty than the marginal MMD. As shown in
Figure 5, this prediction holds: conditional MMD regularization dramatically improves performance on the worst group, while only lightly impacting the overall accuracy across groups. 6