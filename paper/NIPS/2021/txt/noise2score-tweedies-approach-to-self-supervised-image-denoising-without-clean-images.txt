Abstract
Recently, there has been extensive research interest in training deep networks to denoise images without clean reference. However, the representative approaches such as Noise2Noise, Noise2Void, Stein’s unbiased risk estimator (SURE), etc. seem to differ from one another and it is difﬁcult to ﬁnd the coherent mathematical structure. To address this, here we present a novel approach, called Noise2Score, which reveals a missing link in order to unite these seemingly different approaches.
Speciﬁcally, we show that image denoising problems without clean images can be addressed by ﬁnding the mode of the posterior distribution and that the Tweedie’s formula offers an explicit solution through the score function (i.e. the gradient of loglikelihood). Our method then uses the recent ﬁnding that the score function can be stably estimated from the noisy images using the amortized residual denoising autoencoder, the method of which is closely related to Noise2Noise or Nose2Void.
Our Noise2Score approach is so universal that the same network training can be used to remove noises from images that are corrupted by any exponential family distributions and noise parameters. Using extensive experiments with Gaussian,
Poisson, and Gamma noises, we show that Noise2Score signiﬁcantly outperforms the state-of-the-art self-supervised denoising methods in the benchmark data set such as (C)BSD68, Set12, and Kodak, etc. 1

Introduction
Bayesian inference, which derives the posterior probability using a prior probability and a likelihood function for the observed data, has been an important tool in statistics. This approach has been used extensively for image denoising from early ages to the modern era of deep learning. For example, in the recent unsupervised deep learning approach for image denoising using Stein’s risk estimate (SURE) [1], the unknown Bayesian risk is replaced by the SURE that can be calculated from the noisy measurement so that deep neural network training is performed by minimizing it. Unfortunately, this method is sensitive to hyper-parameters, and the neural network must be retrained if the underlying noise model varies [2]. On the other hand, there has been increased research interest in image denoisers that can be trained by minimizing variants of empirical risks that are not associated with clean data. Noise2Noise [3] was the ﬁrst representative approach that does not require clean data.
Unfortunately, multiple noisy versions of the same images are necessary for training. To address this, self-supervised learning approaches such as Noise2Void [4], Noise2Self [5], etc. have been developed in order to use only a single noisy image. This class of approaches, which we will call
Noise2X throughout the paper, are especially important for practical applications, where noiseless clean images or multiple noisy realization of the same image are difﬁcult or impossible to collect. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overall reconstruction ﬂow of (a) supervised learning, SURE and Noise2X, where the target z and the distance measure d(·, ·) are uniquely determined by each algorithm, and (b) Noise2Score, where the ﬁrst step is the estimation of the score function ˆl(cid:48) using neural network training, which is followed by Tweedie’s formula to obtain the ﬁnal denoising result.
Though SURE and Noise2X hold promise for practical applications, one of the fundamental questions is how these seemingly different approaches are related and whether there is a coherent mathematical theory that can be leveraged to further improve the performance. Although we are aware of a preliminary prior work [6] attempting to unite a subset of them, there is largely a lack of a principled method of designing image denoiser without clean data.
In this article, we try to approach this open problem from Bayesian statistics in a completely different route. Instead of minimizing different forms of empirical risks or SURE, one of the most important contributions of this paper is the discovery of the importance of classical result from Bayesian statistics - the Tweedie’s formula [7] which provide an explicit way of computing the posterior mean of canonical parameters from the noisy measurements corrupted with exponential family noises.
Speciﬁcally, we show that the Tweedie’s formula provides a uniﬁed approach for image denoising from any exponential family noises through a score function (i.e. the gradient of the loglikelihood).
Therefore, the self-supervised image denoising problem without clean image can be reduced to the problem of estimating the score function.
This change of perspective has many important theoretical implications as well as the ﬂexibility with regard to the implementation of the algorithm. In fact, the score function estimation problem has been an important research topic in Bayesian statistics and machine learning [8–10]. In particular, Alain and Bengio [10] showed that the minimization of the denoising autoencoder (DAE) objective function provides an explicit way of approximating the score function. This result was further extended using the amortized residual denoising autoencoder (AR-DAE) for numerical stability and accuracy [11].
Interestingly, the training method for DAE or its variants is strikingly similar to that of Noise2X.
Therefore, by combining this with the Tweedie’s formula, we can obtain a novel uniﬁed framework of
Bayesian approach for self-supervised image denoising. The conceptual difference and similarity of our method to the existing approaches are illustrated in Fig. 1(a)(b). As the ﬁrst step of our algorithm is similar to Noise2X, we call our method Noise2Score.
Noise2Score is so powerful that it can be used to deal with any exponential family noises. Moreover, in contrast to the SURE approach, in which the network must be retrained using a different loss function depending on the noise model and parameters [1, 12], Noise2Score is universal in the sense that the identical neural network training step is used regardless of noise models. This property can be exploited to extend Noise2Score to blind setups in which noise parameters are unknown and should be estimated with minimal complexity. In addition to the novel theoretical ﬁndings mentioned above, our empirical results using additive Gaussian, Poisson and Gamma noise models have shown that
Noise2Score signiﬁcantly outperforms SURE and Noise2X under similar experimental conditions. 2