Abstract
Deep Reinforcement Learning (RL) powered by neural net approximation of the
Q function has had enormous empirical success. While the theory of RL has traditionally focused on linear function approximation (or eluder dimension) ap-proaches, little is known about nonlinear RL with neural net approximations of the
Q functions. This is the focus of this work, where we study function approximation with two-layer neural networks (considering both ReLU and polynomial activation functions). Our first result is a computationally and statistically efficient algorithm in the generative model setting under completeness for two-layer neural networks.
Our second result considers this setting but under only realizability of the neural net function class. Here, assuming deterministic dynamics, the sample complexity scales linearly in the algebraic dimension. In all cases, our results significantly improve upon what can be attained with linear (or eluder dimension) methods. 1

Introduction
In reinforcement learning (RL), an agent aims to learn the optimal decision-making rule by interacting with an unknown environment [71]. Deep Reinforcement Learning, empowered by deep neural networks [48, 32], has achieved tremendous success in various real-world applications, such as Go
[68], Atari [54], Dota2 [7], Texas Holdém poker [56], and autonomous driving [66]. Those modern
RL applications are characterized by large state-action spaces, and the empirical success of deep
RL corroborates the observation that deep neural networks can extrapolate well across state-action spaces [35, 55, 50].
Although in practice non-linear function approximation scheme is prevalent, theoretical understand-ings of the sample complexity of RL mainly focus on tabular or linear function approximation settings [69, 38, 5, 41, 63, 88, 1, 43, 44, 77]. These results rely on finite state space or exact linear ap-proximations. Recently, sample efficient algorithms under non-linear function approximation settings are proposed [82, 11, 20, 12, 51, 72, 13, 91, 86]. Those algorithms are based on Bellman rank [40], eluder dimension [64], neural tangent kernel [37, 4, 14, 92], or sequential Rademacher complexity
[60, 61]. Yet, the understanding of how deep RL learns and generalizes in large state spaces is far from complete. While the aforementioned works study function approximation structures that possess the nice properties of linear models, such as low information gain and low eluder dimensions, the highly non-linear nature of neural networks renders challenges on their applicability to deep RL.
For one thing, recent wisdoms in deep learning theory cast doubt on the ability of neural tangent kernel and random features to model the actual neural networks. Indeed, the neural tangent kernel
∗Alphabetical order. Correspondence to: Baihe Huang, baihehuang@pku.edu.cn, Jason D. Lee, Jasondl@ princeton.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
approximately reduces neural networks to linear models, but the RKHS norm of neural networks is exponential [87]. Moreover, it remains unclear what neural networks models have low eluder dimensions. For example, recent work [13] shows that two layer neural networks have exponential eluder dimension in the dimension of features. Thus, the mismatch between the empirical success of deep RL and its theoretical understanding remains significant, which yields the following important question:
What are the structural properties that allow sample-efficient algorithms for RL with neural network function approximation?
Recent work in RL suggests that learning RL with neural networks function approximation is exponentially hard [13, 49, 52]. In this paper, however, we advance the understanding of the above question by displaying several structural properties that allow efficient RL algorithms with neural function approximations. We consider several value function approximation models that possess high information gain and high eluder dimension. Specifically, we study two structures, namely two-layer neural networks and structured polynomials (i.e. two-layer neural networks with polynomial activation functions), under two RL settings, namely RL with simulator model and online RL. In the simulator (generative model) setting [45, 67], the agent can simulate the MDP at any state-action pair.
In online RL, the agent can only start at an initial state and interact with the MDP step by step. The goal in both settings is to find a near-optimal policy while minimizing the number of samples used.
We obtain the following results. For the simulator setting, we propose sample-efficient algorithms for RL with two-layer neural network function approximation. Under either policy completeness,
Bellman completeness, or gap conditions, our method provably learns near-optimal policy with polynomial sample complexities. For online RL, we provide sample-efficient algorithms for RL with structured polynomial function approximation. When the transition is deterministic, we also present sample-efficient algorithms under only the realizability assumption [18, 78]. Our main techniques are based on neural network recovery [90, 39, 28], and algebraic geometry [53, 65, 8, 76]. 1.1 Summary of our results
Our main results in different settings are summarized in Table 1. We consider two-layer neural networks f (x) = ⟨v, σ(W x)⟩ (where σ is ReLU activation) and rank k polynomials (see Example 4.3).
Table 1: Baselines and our main results for the sample complexity to find an ϵ-optimal policy. rank k polynomial
Sim. + Det. (R) Onl. + Det. (R)
Baseline
Our results
O(dp)
O(dk)
O(dp)
O(dk)
Neural Net of Width k
Sim. + Det. (R)
O(dpoly(1/ϵ)) (*) (cid:101)O(poly(d) · exp(k))
Sim. + Gap. (R)
O(dpoly(1/ϵ)) (cid:101)O(poly(d, k))
Sim. + Stoch. (C)
O(dpoly(1/ϵ)) (cid:101)O(poly(d, k)/ϵ2)
We make the following elaborations on Table 1.
• For simplicity, we display only the dependence on the feature dimension d, network width or polynomial rank k, precision ϵ, and degree p (of polynomials).
• In the table Sim. denotes simulator model, Onl. denotes online RL, Det. denotes deter-ministic transitions, Stoch. denotes stochastic transitions, Gap. denotes gap condition, (R) denotes realizability assumption only, and (C) denotes completeness assumption (either policy complete or Bellman complete) together with realizability assumption.
• We apply [21] for the deterministic transition baseline, and apply [19] for the stochastic transition baseline. We are unaware of any methods that can directly learn MDP with neural network value function approximation2.
• In polynomial case, the baseline first vectorizes the tensor (cid:19)⊗p (cid:18)1 x into a vector in R(d+1)p and then performs on this vector. In the neural network case, the baseline uses a polynomial of degree 1/ϵ to approximate the neural network with precision ϵ and then vectorizes the polynomial into a vector in Rdpoly(1/ϵ)
. The baseline method for realizable model (denoted 2
by (*)) needs a further gap assumption of gap ≥ dpoly(1/ϵ)ϵ to avoid the approximation error from escalating [21]; note for small ϵ this condition never holds but we include it in the table for the sake of comparison.
• In rank k polynomial case, our result O(dk) in simulator model can be found in Theo-rem 4.7 and our result O(dk) in online RL model can be found in Theorem 4.8. These results only require a realizability assumption. Efficient explorations are guaranteed by algebraic-geometric arguments. In neural network model, our result (cid:101)O(poly(d) · exp(k)) in simulator model can be found in Theorem 3.4. This result also only relies on the realizability assumption. For stochastic transitions, our result (cid:101)O(poly(d, k)/ϵ2) works for either policy complete or Bellman complete settings, as in Theorem 3.5 and Theorem 3.6 respectively.
The (cid:101)O(poly(d, k)) result for gap condition can be found in Theorem 3.8. 1.2