Abstractions
Samuel Sokota
Carnegie Mellon University ssokota@andrew.cmu.edu
Caleb Ho
Independent Researcher caleb.yh.ho@gmail.com
Zaheen Ahmad
University of Alberta zfahmad@ualberta.ca
J. Zico Kolter
Carnegie Mellon University zkolter@cs.cmu.edu
Abstract
Decision-time planning is the process of constructing a transient, local policy with the intent of using it to make the immediate decision. Monte Carlo tree search (MCTS), which has been leveraged to great success in Go, chess, shogi, Hex, Atari, and other settings, is perhaps the most celebrated decision-time planning algorithm.
Unfortunately, in its original form, MCTS can degenerate to one-step search in domains with stochasticity. Progressive widening is one way to ameliorate this issue, but we argue that it possesses undesirable properties for some settings. In this work, we present a method, called abstraction reﬁning, for extending MCTS to stochastic environments which, unlike progressive widening, leverages the geometry of the state space. We argue that leveraging the geometry of the space can offer advantages. To support this claim, we present a series of experimental examples in which abstraction reﬁning outperforms progressive widening, given equal simulation budgets. 1

Introduction
In control problems, an agent makes sequential decisions toward the end of accumulating a large amount of reward [21]. Many reinforcement learning algorithms approach this task by computing a policy, which, given a state, dictates the agent’s behavior; others operate under the paradigm of decision-time planning, in which, after the agent has arrived at a state, it spends additional computation constructing or revising its policy for the immediate decision(s). The decision-time planning paradigm can be advantageous because it allows for an additional step of policy improvement that is not available to agents acting according to a predetermined policy.
Of the numerous ways to perform decision-time planning, Monte Carlo tree search (MCTS) is among the most inﬂuential [6, 10, 15]. Algorithms powered by MCTS, like AlphaZero and MuZero, yield strong performance in games with complex value function landscapes, like Go, chess, Hex, and shogi
[2, 20, 19]. MuZero even shows strong performance on Atari games, where model-free algorithms had previously been the most performant.
Unfortunately, in its original form, MCTS can degenerate to a one-step search when the transition function has an inﬁnite support. Even if the support is ﬁnite but non-trivial, MCTS may be relegated to building shallow search trees. However, despite the importance of stochasticity in real-world problems, relatively little attention has been paid to this issue, perhaps as a result of the fact that many popular benchmarks are deterministic or close-to-deterministic. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
One way to address this issue is by using progressive widening [9]. Progressive widening works by alternating between adding new children and selecting among existing children such that, asympototi-cally, the search tree becomes both inﬁnitely deep and inﬁnitely wide. Progressive widening is also advantageous because it includes hyperparameters that control its propensity to add new children (as opposed to selecting among existing children). These hyperparameters can interpolate between width-wise expansion only (vanilla MCTS) and depth-wise expansion only (transition determiniza-tion). As a result, with proper tuning, progressive widening is capable of performing well in both settings with important stochasticity and in settings with unimportant (or non-existent) stochasticity.
However, progressive widening’s decision rule for expansion is the same for every transition. In settings with non-uniform stochasticity, we argue that this property is undesirable, as it requires progressive widening to compromise between focusing on unimportant stochasticity and ignoring important stochasticity.
In this work, we propose a new method, called abstraction reﬁning, for extending MCTS to stochastic settings. Abstraction reﬁning uses random and iteratively reﬁning state abstractions deﬁned by the geometry of the state space. If a newly sampled state is similar to a state that is already in the tree, it is discarded in favor of the pre-established state; otherwise, the newly sampled state is added to the tree. Because the criteria for similarity becomes more strict the more often an abstraction is used, abstraction reﬁning guarantees that, in the limit, the search tree will grow both inﬁnitely wide and inﬁnitely deep.
In addition to proposing the abstraction reﬁning algorithm, our contributions are twofold. First, we present both a proof that, when used for policy evaluation, abstraction reﬁning converges almost surely in ﬁnite MDPs. Second, we make an empirical case that, provided a good notion of state similarity, abstraction reﬁning can outperform progressive widening in settings with stochasticity of varying importance, given an equal simulation budget. To make this case, we present a series of domains in which i) stochasticity is sometimes, but not always, relevant to performing well on the task and ii) the naive notion of distance between states is reﬂective of behavioral similarity (though we also include some settings in which we use a learned notion of distance). In these domains, we
ﬁnd that abstraction reﬁning can outperform progressive widening. 2