Abstract
In this paper, we present token labeling—a new training objective for training high-performance vision transformers (ViTs). Different from the standard training objective of ViTs that computes the classiﬁcation loss on an additional trainable class token, our proposed one takes advantage of all the image patch tokens to com-pute the training loss in a dense manner. Speciﬁcally, token labeling reformulates the image classiﬁcation problem into multiple token-level recognition problems and assigns each patch token with an individual location-speciﬁc supervision gen-erated by a machine annotator. Experiments show that token labeling can clearly and consistently improve the performance of various ViT models across a wide spectrum. For a vision transformer with 26M learnable parameters serving as an example, with token labeling, the model can achieve 84.4% Top-1 accuracy on
ImageNet. The result can be further increased to 86.4% by slightly scaling the model size up to 150M, delivering the minimal-sized model among previous mod-els (250M+) reaching 86%. We also show that token labeling can clearly improve the generalization capability of the pretrained models on downstream tasks with dense prediction, such as semantic segmentation. Our code and model are publicly available at https://github.com/zihangJiang/TokenLabeling. 1

Introduction
Transformers [39] have achieved great performance for almost all the natural language processing (NLP) tasks over the past years [4, 14, 24]. Motivated by such success, recently, many researchers attempt to build transformer models for vision tasks, and their encouraging results have shown the great potential of transformer based models for image classiﬁcation [6, 15, 25, 36, 40, 46], especially the strong beneﬁts of the self-attention mechanism in building long-range dependencies between pairs of input tokens.
Despite the importance of gathering long-range dependencies, recent work on local data augmenta-tion [57] has demonstrated that well modeling and leveraging local information for image classiﬁca-tion would avoid biasing the model towards skewed and non-generalizable patterns and substantially
∗Work done as an intern at ByteDance AI Lab.
†Corresponding author. Part of this work was done as a research fellow at NUS. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Comparison between the proposed LV-ViT and other recent works based on vision trans-formers, including T2T-ViT [46], ConViT [12], BoTNet [31], DeepViT [59], DeiT [36], ViT [15],
Swin Transformer [25], LambdaNet [1], CvT [43], CrossViT [6], PVT [40], CaiT [37]. Note that we only show models whose model sizes are under 100M. As can be seen, our LV-ViT achieves the best results using the least amount of learnable parameters. The default test resolution is 224 × 224 unless speciﬁed after @. improve the model performance. However, recent vision transformers normally utilize class tokens that aggregate global information to predict the output class while neglecting the role of other patch tokens that encode rich information on their respective local image patches.
In this paper, we present a new training objective for vision transformers, termed token labeling, that takes advantage of both the patch tokens and the class tokens. Our method takes a K-dimensional score map generated by a machine annotator as supervision to supervise all the tokens in a dense manner, where K is the number of categories for the target dataset. In this way, each patch token is explicitly associated with an individual location-speciﬁc supervision indicating the existence of the target objects inside the corresponding image patch, so as to improve the object grounding and recognition capabilities of vision transformers with negligible computation overhead. To the best of our knowledge, this is the ﬁrst work demonstrating that dense supervision is beneﬁcial to vision transformers in image classiﬁcation.
According to our experiments, utilizing the proposed token labeling objective can clearly boost the performance of vision transformers. As shown in Figure 1, our model, named LV-ViT, with 56M parameters, yields 85.4% top-1 accuracy on ImageNet [13], behaving better than all the other transformer-based models having no more than 100M parameters. When the model size is scaled up to 150M, the result can be further improved to 86.4%. In addition, we have empirically found that the pretrained models with token labeling are also beneﬁcial to downstream tasks with dense prediction, such as semantic segmentation. 2