Abstract
Gathering training data is a key step of any supervised learning task, and it is both critical and expensive. Critical, because the quantity and quality of the training data has a high impact on the performance of the learned function. Expensive, because most practical cases rely on humans-in-the-loop to label the data. The process of determining the correct labels is much more expensive than comparing two items to see whether they belong to the same class. Thus motivated, we propose a setting for training data gathering where the human experts perform the comparatively cheap task of answering pairwise queries, and the computer groups the items into classes (which can be labeled cheaply at the very end of the process). Given the items, we consider two random models for the classes: one where the set partition they form is drawn uniformly, the other one where each item chooses its class independently following a ﬁxed distribution. In the ﬁrst model, we characterize the algorithms that minimize the average number of queries required to cluster the items and analyze their complexity. In the second model, we analyze a speciﬁc algorithm family, propose as a conjecture that they reach the minimum average number of queries and compare their performance to a random approach. We also propose solutions to handle errors or inconsistencies in the experts’ answers. 1

Introduction
There is an increasing demand for software implementing supervised learning for classiﬁcation.
Training data input for such software consists of items belonging to distinct classes. The output is a classiﬁer: a function that predicts, for any new item, the class it most likely belongs to. Its quality depends critically on the available learning data, in terms of both quantity and quality [21]. But labeling large quantities of data is costly. This task cannot be fully automated, as doing so would assume access to an already trained classiﬁer. Thus, human intervention, although expensive, is required. In this article, we focus on helping the human experts build the learning data efﬁciently.
One natural way for the human experts to proceed is to learn (or discover) the classes and write down their characteristics. Then, items are considered one by one, assigning an existing class to each of them, or creating a new one if necessary. This approach requires the experts to learn the various classes, which, depending on the use-case, can be difﬁcult. A different approach, proposed to us by
Nokia engineer Maria Laura Maag, is to discover the partition by querying the experts on two items at a time asking whether these belong to the same class or not. This approach avoids the part of the process where classes are learned, and can therefore be cheaper. It is the setting we consider here, and we call the corresponding algorithm an active clustering algorithm, or for short, AC algorithm.
∗Authors presented in alphabetical order. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
More precisely, we assume there is a set of size n, with a partition unknown to us. An AC algorithm will, in each step, choose a pair of elements and asks the oracle whether they belong to the same partition class or not. The choices of the queries of the algorithm are allowed to depend on earlier answers. The algorithm will use transitivity inside the partition classes: if each of the pairs x, y and y, z is known to lie in the same class (for instance because of positive answers from the oracle), then the algorithm will ‘know’ that x and z are also in the same class, and it will not submit a query for this pair. The algorithm terminates once the partition is recovered, i.e. when all elements from the same partition class have been shown to belong to the same class, and when for each pair of partition classes, there has been at least one query between their elements.
We investigate AC algorithms under two different random models for the unknown set partition.
In Section 2.1, the set partition is sampled uniformly at random, while in Section 2.2, the number of blocks is ﬁxed and each item chooses its class independently following the same distribution.
Section 2.3 analyzes the cases where the experts’ answers contain errors or inconsistencies. Our proofs, sketched in Section 3, rely on a broad variety of mathematical tools: probability theory, graph theory and analytic combinatorics. We conclude in Section 4, with some interesting open problems and research topics.