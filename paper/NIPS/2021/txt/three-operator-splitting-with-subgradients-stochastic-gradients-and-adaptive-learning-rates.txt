Abstract
Three Operator Splitting (TOS) (Davis & Yin, 2017) can minimize the sum of multiple convex functions effectively when an efﬁcient gradient oracle or proximal operator is available for each term. This requirement often fails in machine learning applications: (i) instead of full gradients only stochastic gradients may be available; and (ii) instead of proximal operators, using subgradients to handle complex penalty functions may be more efﬁcient and realistic. Motivated by these concerns, we analyze three potentially valuable extensions of TOS. The ﬁrst two permit using subgradients and stochastic gradients, and are shown to ensure a O(1/ t) convergence rate. The third extension ADAPTOS endows TOS with adaptive step-sizes. For the important setting of optimizing a convex loss over the intersection of convex sets ADAPTOS attains universal convergence rates, i.e., the rate adapts to the unknown smoothness degree of the objective function. We compare our proposed methods with competing methods on various applications.
√ 1

Introduction
We study convex optimization problems of the form min x∈Rn
φ(x) := f (x) + g(x) + h(x), (1) where f : Rn → R and g, h : Rn → R ∪ {+∞} are proper, lower semicontinuous and convex func-tions. Importantly, this template captures constrained problems via indicator functions. To avoid pathological examples, we assume that the relative interiors of dom(f ), dom(g) and dom(h) have a nonempty intersection.
Problem (1) is motivated by a number of applications in machine learning, statistics, and signal processing, where the three functions comprising the objective φ model data ﬁtting, structural priors, or decision constraints. Examples include overlapping group lasso (Yuan et al., 2011), isotonic regression (Tibshirani et al., 2011), dispersive sparsity (El Halabi & Cevher, 2015), graph transduction (Shivanna et al., 2015), learning with correlation matrices (Higham & Strabi´c, 2016), and multidimensional total variation denoising (Barbero & Sra, 2018).
An important technique for addressing composite problems is operator splitting (Bauschke et al., 2011). However, the basic proximal-(sub)gradient method may be unsuitable for Problem (1) since it
Alp Yurtsever and Alex Gu contributed equally to this paper. The paper is based primarily on the work done while Alp Yurtsever was at Massachusetts Institute of Technology. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
requires the prox-operator of g + h, computing which may be vastly more expensive than individual prox-operators of g and h. An elegant, recent method, Three Operator Splitting (TOS, Davis &
Yin (2017), see Algorithm 1) offers a practical choice for solving Problem (1) when f is smooth.
Importantly, at each iteration, TOS evaluates the gradient of f and the proximal operators of g and h only once. Moreover, composite problems with more than three functions can be reformulated as an instance of Problem (1) in a product-space and solved by using TOS. This is an effective method as long as each function has an efﬁcient gradient oracle or proximal operator (see Section 2).
Unfortunately, TOS is not readily applicable to many optimization problems that arise in machine learning. Most important among those are problems where only access to stochastic gradients is feasible, e.g., when performing large-scale empirical risk minimization and online learning. Moreover, prox-operators for some complex penalty functions are computationally expensive and it may be more efﬁcient to instead use subgradients. For example, proximal operator for the maximum eigenvalue function that appears in dual-form semideﬁnite programs (e.g., see Section 6.1 in (Ding et al., 2019)) may require computing a full eigendecomposition. In contrast, we can form a subgradient by computing only the top eigenvector via power method or Lanczos algorithm.
√
Contributions. With the above motivation, this paper contributes three key extensions of TOS. We tackle nonsmoothness in Section 3 and stochasticity in Section 4. These two extensions enable us to use subgradients and stochastic gradients of f (see Section 2 for a comparison with related work),
T ) error bound in function value after T iterations. The third main contribution and satisfy a O(1/ is ADAPTOS in Section 5. This extension provides an adaptive step-size rule in the spirit of AdaGrad (Duchi et al., 2011; Levy, 2017) for an important subclass of Problem (1). Notably, for optimizing a convex loss over the intersection of two convex sets, ADAPTOS ensures universal convergence rates.
That is, ADAPTOS implicitly adapts to the unknown smoothness degree of the problem, and ensures a
˜O(1/ t) convergence rate when the problem is nonsmooth but the rate improves to ˜O(1/t) if the problem is smooth and a solution lies in the relative interior of the feasible set.
√
In Section 6, we discuss empirical performance of our methods by comparing them against present established methods on various benchmark problems from COPT Library (Pedregosa et al., 2020) including the overlapping group lasso, total variation deblurring, and sparse and low-rank matrix recovery. We also test our methods on nonconvex optimization by training a neural network model.
We present more experiments on isotonic regression and portfolio optimization in the supplements.
Notation. We denote a solution of Problem (1) by x(cid:63) and φ(cid:63) := φ(x(cid:63)). The distance between a point x ∈ Rn and a closed and convex set G ⊆ Rn is dist(x, G) := miny∈G (cid:107)x − y(cid:107); the projection of x onto G is given by projG(x) := arg miny∈G (cid:107)x − y(cid:107). The prox-operator of a function g : Rn → R ∪ {+∞} is deﬁned by proxg(x) := arg miny∈Rn {g(y) + 1 2 (cid:107)x − y(cid:107)2}. The indicator function of G gives 0 for all x ∈ G and +∞ otherwise. Clearly, the prox-operator of an indicator function is the projection onto the corresponding set. 2