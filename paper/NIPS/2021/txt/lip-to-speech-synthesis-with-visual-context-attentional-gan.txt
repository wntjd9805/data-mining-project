Abstract
In this paper, we propose a novel lip-to-speech generative adversarial network,
Visual Context Attentional GAN (VCA-GAN), which can jointly model local and global lip movements during speech synthesis. Speciﬁcally, the proposed VCA-GAN synthesizes the speech from local lip visual features by ﬁnding a mapping function of viseme-to-phoneme, while global visual context is embedded into the intermediate layers of the generator to clarify the ambiguity in the mapping induced by homophene. To achieve this, a visual context attention module is proposed where it encodes global representations from the local visual features, and provides the desired global visual context corresponding to the given coarse speech representation to the generator through audio-visual attention. In addition to the explicit modelling of local and global visual representations, synchronization learning is introduced as a form of contrastive learning that guides the generator to synthesize a speech in sync with the given input lip movements. Extensive experiments demonstrate that the proposed VCA-GAN outperforms existing state-of-the-art and is able to effectively synthesize the speech from multi-speaker that has been barely handled in the previous works. 1

Introduction
Lip to speech synthesis (Lip2Speech) is to predict an audio speech by watching a silent talking face video. While conventional visual speech recognition tasks require human annotations (i.e., text),
Lip2Speech does not require additional annotations. Thus, it has drawn big attention as another form of lip reading. However, due to the ambiguity of homophenes that have similar lip movements and the voice characteristics varying from different identities, it is still considered as a challenging problem.
Basically, synthesizing a speech from a silent lip movement video can be viewed as ﬁnding a mapping function of visemes into corresponding phonemes. However, only watching short clip-level (i.e., local) lip movements could be challenging to distinguish the homophenes. Thus, global-level lip movements containing the visual context, hints for ambiguity of viseme-to-phoneme mapping, should also be considered along with local-level lip movements. Early deep learning-based works [1, 2, 3] predict each auditory feature (e.g., LPC, mel-spectrogram, spectrogram) within a short video clip and extend the prediction to the entire speech by sliding a window over the whole video sequences. As they operate with clip-level videos of ﬁxed length, they could fail on capturing the global context of the spoken speech. A recent work [4] brings Sequence-to-Sequence (Seq2Seq) architecture [5, 6] that predicts the auditory feature conditioned on both the encoded visual context and the previous prediction and shows a promising performance. However, since they do not explicitly consider local visual features, they may produce out-of-sync speech to the input video. Moreover, due to the sequential nature of the Seq2Seq architecture, the method demands heavy inference time. Since the
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
output audio sequence length is determined when the input video is given for the Lip2Speech task, the time costs can be reduced by adopting different architectures that could predict the speech with one forward step, instead of using a Seq2Seq model which is originally designed to handle input and output with different varying sequence lengths. Lastly, all the above methods focus on handling speech synthesis of constrained speakers (i.e., 1 to 4 speakers), so they could fail to properly handle diverse speakers with one trained model.
In this paper, we design a novel deep architecture, namely Visual Context Attentional GAN (VCA-GAN), that jointly models the local and global visual representations to synthesize accurate speech from silent talking face video. Concretely, the proposed VCA-GAN synthesizes the speech (i.e., mel-spectrogram) based on the local visual features by ﬁnding a mapping function of viseme-to-phoneme, while the global visual context assists the generator for clarifying the ambiguity of the mapping. To this end, a visual context attention module is proposed where it extracts the global visual features from the local visual features and provides the global visual context to the generator. It is applied to the generator in multi-scale scheme so that the generator can reﬁne the speech representation from coarse- to ﬁne-level by jointly modelling both the local and the global visual context. Moreover, to guarantee the generated speech to be synced with the input lip movements, synchronization learning is performed that gives feedback to the generator whether the synthesized speech is synchronized or not with the input lip movement. The effectiveness of the proposed framework is evaluated on three public benchmark databases, GRID [7], TCD-TIMIT[8], and LRW[9] in both constrained-speaker setting and multi-speaker setting.
The major contributions of this paper are as follows, 1) To the best of our knowledge, this is the
ﬁrst work to explicitly model the local and global lip movements for synthesizing detailed and accurate speech from silent talking face video. 2) We consider a mel-spectrogram as an image and solve the Lip2Speech problem efﬁciently using video-to-image translation. 3) This paper introduces synchronization learning which guides the generated mel-spectrogram to be in sync with the input lip video. 4) We show the proposed VCA-GAN can synthesize speech from diverse speakers without the prior knowledge of speaker information such as speaker embeddings. 2