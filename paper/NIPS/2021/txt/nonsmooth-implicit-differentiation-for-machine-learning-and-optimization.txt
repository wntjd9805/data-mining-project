Abstract
In view of training increasingly complex learning architectures, we establish a non-smooth implicit function theorem with an operational calculus. Our result applies to most practical problems (i.e., deﬁnable problems) provided that a nonsmooth form of the classical invertibility condition is fulﬁlled. This approach allows for formal subdifferentiation: for instance, replacing derivatives by Clarke Jacobians in the usual differentiation formulas is fully justiﬁed for a wide class of nons-mooth problems. Moreover this calculus is entirely compatible with algorithmic differentiation (e.g., backpropagation). We provide several applications such as training deep equilibrium networks, training neural nets with conic optimization layers, or hyperparameter-tuning for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we present numerical experiments showcasing the extremely pathological gradient dynamics one can encounter when applying implicit algorithmic differentiation without any hypothesis. 1

Introduction
Differentiable programming. The recent introduction of deep equilibrium networks [7], the in-creasing importance of bilevel programming (e.g., hyperparameter optimization) [48] and the ubiquity of differentiable programming (e.g., TensorFlow [1], PyTorch [47], JAX [16]) in modern optimization call for the development of a versatile theory of nonsmooth differentiation. Our focus is on nonsmooth implicit differentiation. There are currently two practices lying at the crossroads of mathematics and computer science: on the one hand the use of the standard smooth implicit function theorem
“almost everywhere” [31, 30] and on the other hand the development of algorithmic differentiation tools [2, 3, 59]. The empirical use of the latter in the nonsmooth world has shown surprisingly efﬁcient results [59], but the current theories cannot explain this success. We bridge this gap by providing nonsmooth implicit differentiation results and illustrating their impact on the training of neural networks and hyperparameter optimization.
Backpropagation: a formal differentiation approach. Let us consider z implicitly deﬁned through F (z(x)) = h(x) where F and h have full domain and adequate dimensions. How does autograd apply to evaluating the “derivative” of the implicitly deﬁned function z? Regardless of differentiability or nonsmoothness, and provided that inversion is possible, one commonly uses (or dynamically approximates) this derivative by (backpropF (z(x)))−1 backprophx, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where backprop outputs the result of formal backpropagation, see e.g., [50]. This identity1 is used to provide efﬁcient training despite the fact that the rules of classical nonsmooth calculus are transgressed [7, 59]. Note that spurious outputs may be created by this approach, but on a negligible set. Consider for example the simple implicit problem x = f (z(x)) where f (z) := tanh(z) + relu(−z) + z − relu(z), whose solution is z(x) = tanh x. Yet applying the implicit differentiation framework of [7] using JAX library, as presented in [59], provides inconsistency of the derivative at the origin, see Figure 1. As mentioned above, despite these unpredictable outputs, propagating derivatives leads to an undeniable efﬁciency. But can we parallel these propagation ideas with a simple mathematical counterpart? Is there a rigorous theory backing up formal (sub)differentiation or formal propagation? The answer is positive and was initiated in [14, 15] through conservative
Jacobians (see also [41, 24]).
A mathematical model for propagating derivatives. Conservative calculus models nonsmooth algorithmic differentiation faithfully and allows for a sharp study of training methods in Deep
Learning [14, 15]. It involves a new class of derivatives, generalizing Clarke Jacobians [20]. A distinctive feature of conservative calculus is that it is preserved by Jacobian multiplication. Consider for example a feed forward network combining analytic or relu activations and max pooling. A conservative Jacobian for this network can be obtained by using Clarke Jacobians formally as classical
Jacobians, regardless of qualiﬁcation conditions, compared to other approaches, e.g. [37], for which stricter qualiﬁcation conditions are imposed to ensure that an element of the Clarke Jacobian itself can be computed using algorithmic differentiation. For instance, Figure 1 depicts a selection in a conservative Jacobian. This approach is general enough to handle spurious points such as in Figure 1 while keeping the essence of the properties one expects from a derivative. It was proved in [14] that backprop, applied to any reasonable program of a function, is a conservative Jacobian for this function; in contrast, backprop cannot be modelled by some subdifferential operator. For instance for the ﬁxed point problem above, given conservative Jacobians JF and Jh (e.g., Clarke Jacobians) for F and h one obtains a new conservative Jacobian Jz implicitly deﬁned through
JF (z(x))Jz(x) = Jh(x).
This property exactly parallels the idea of “propagating derivatives” in practice. It gives a strong meaning to the formal use of Jacobians proposed in [7], and many empirical approaches [32, 2, 31, 30].
Main contributions:
— We establish a nonsmooth conservative implicit function theorem that comes with an implicit calculus which is the central focus of this paper. Our calculus amounts somehow to formal subdiffer-entiation with Clarke Jacobians. This approach cannot rely on classical tools like the inverse of a
Clarke Jacobian or a composition of Clarke Jacobians, which are not in general Clarke Jacobians.
Indeed, a surprising example (Example 1) shows that an “inverse function theorem with Clarke calculus” is not possible.
— We study a wide range of applications of our implicit differentiation theorem, covering deep equilibrium problems [7], conic optimization layers [2], and hyperparameter optimization for the
Lasso [9]. Each case is detailed and its speciﬁcities are discussed.
— As a consequence, we obtain convergence guarantees for mini-batched stochastic algorithms with vanishing step size for training wide classes of Neural Nets, or for Lasso hyperparameter selection.
The assumptions needed for our results are mild and fulﬁlled by most losses occurring in ML in the spirit of [15, 40]: elementary log-exp functions [15], semialgebraic functions [12], all being subclasses of deﬁnable functions [23, 55]. The use of such structural classes has become standard in nonsmooth optimization and is more and more common in ML (see, e.g., [18, 15, 40, 35]).
— As in the smooth implicit function theorem, the invertibility condition is not avoidable in general.
We provide various examples for which the assumption is not satisﬁed; this results in severe failures for the corresponding gradient methods. In Figure 1, one sees how lack of invertibility on an otherwise ordinary problem may provide totally unpredictable behavior for smooth quadratic optimization.