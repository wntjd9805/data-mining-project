Abstract
This paper studies Dropout Graph Neural Networks (DropGNNs), a new approach that aims to overcome the limitations of standard GNN frameworks. In DropGNNs, we execute multiple runs of a GNN on the input graph, with some of the nodes randomly and independently dropped in each of these runs. Then, we combine the results of these runs to obtain the ﬁnal result. We prove that DropGNNs can distinguish various graph neighborhoods that cannot be separated by message passing GNNs. We derive theoretical bounds for the number of runs required to ensure a reliable distribution of dropouts, and we prove several properties regarding the expressive capabilities and limits of DropGNNs. We experimentally validate our theoretical ﬁndings on expressiveness. Furthermore, we show that DropGNNs perform competitively on established GNN benchmarks. 1

Introduction
Neural networks have been successful in handling various forms of data. Since some of the world’s most interesting data is represented by graphs, Graph Neural Networks (GNNs) have achieved state-of-the-art performance in various ﬁelds such as quantum chemistry, physics, or social networks
[12; 27; 18]. On the other hand, GNNs are also known to have severe limitations and are sometimes unable to recognize even simple graph structures.
In this paper, we present a new approach to increase the expressiveness of GNNs, called Dropout
Graph Neural Networks (DropGNNs). Our main idea is to execute not one but multiple different runs of the GNN. We then aggregate the results from these different runs into a ﬁnal result.
In each of these runs, we remove (“drop out”) each node in the graph with a small probability p. As such, the different runs of an episode will allow us to not only observe the actual extended neighborhood of a node for some number of layers d, but rather to observe various slightly perturbed versions of this d-hop neighborhood. We emphasize that this notion of dropouts is very different from the popular dropout regularization method; in particular, DropGNNs remove nodes during both training and testing, since their goal is to observe a similar distribution of dropout patterns during training and testing.
This dropout technique increases the expressive power of our GNNs dramatically: even when two distinct d-hop neighborhoods cannot be distinguished by a standard GNN, their dropout variants (with a few nodes removed) are already separable by GNNs in most cases. Thus by learning to identify the dropout patterns where the two d-hop neighborhoods differ, DropGNNs can also distinguish a wide variety of cases that are beyond the theoretical limits of standard GNNs.
Our contributions. We begin by showing several example graphs that are not distinguishable in the regular GNN setting but can be easily separated by DropGNNs. We then analyze the theoretical properties of DropGNNs in detail. We ﬁrst show that executing (cid:101)O(γ) different runs is often already 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
sufﬁcient to ensure that we observe a reasonable distribution of dropouts in a neighborhood of size γ.
We then discuss the theoretical capabilities and limitations of DropGNNs in general, as well as the limits of the dropout approach when combined with speciﬁc aggregation methods.
We validate our theoretical ﬁndings on established problems that are impossible to solve for standard
GNNs. We ﬁnd that DropGNNs clearly outperform the competition on these datasets. We further show that DropGNNs have a competitive performance on several established graph benchmarks, and they provide particularly impressive results in applications where the graph structure is really a crucial factor. 2