Abstract
Advances in unsupervised learning of object-representations have culminated in the development of a broad range of methods for unsupervised object segmentation and interpretable object-centric scene generation. These methods, however, are limited to simulated and real-world datasets with limited visual complexity. Moreover, object representations are often inferred using RNNs which do not scale well to large images or iterative reﬁnement which avoids imposing an unnatural ordering on objects in an image but requires the a priori initialisation of a ﬁxed number of object representations. In contrast to established paradigms, this work proposes an embedding-based approach in which embeddings of pixels are clustered in a differentiable fashion using a stochastic stick-breaking process. Similar to iterative reﬁnement, this clustering procedure also leads to randomly ordered object repre-sentations, but without the need of initialising a ﬁxed number of clusters a priori.
This is used to develop a new model, GENESIS-V2, which can infer a variable number of object representations without using RNNs or iterative reﬁnement. We show that GENESIS-V2 performs strongly in comparison to recent baselines in terms of unsupervised image segmentation and object-centric scene generation on established synthetic datasets as well as more complex real-world datasets. 1

Introduction
Reasoning about discrete objects in an environment is foundational to how agents perceive their surroundings and act in it. For example, autonomous vehicles need to identify and respond to other road users (e.g. [1, 2]) and robotic manipulation tasks involve grasping and pushing individual objects (e.g. [3]). While supervised methods can identify selected objects (e.g. [4, 5]), it is intractable to manually collect labels for every possible object category. Furthermore, we often desire the ability to predict, or imagine, how a collection of objects might behave (e.g. [6]). A range of works have thus explored unsupervised segmentation and object-centric generation in recent years (e.g. [7–36]).
These models are often formulated as variational autoencoders (VAEs) [37, 38] which allow the joint learning of inference and generation networks to identify objects in images and to generate scenes in an object-centric fashion (e.g. [15, 17, 28]).
Moreover, such models require a differentiable mechanism for separating objects in an image. While some works use spatial transformer networks (STNs) [39] to process crops that contain objects (e.g. [7– 15]), others directly predict pixel-wise instance segmentation masks (e.g. [16–27]). The latter avoids the use of ﬁxed-size sampling grids which are ill-suited for objects of varying size. Instead, object representations are inferred either by iteratively reﬁning a set of randomly initialised representations (e.g. [19–24]) or by using a recurrent neural networks (RNN) (e.g. [16–18]). One particularly interesting model of the latter category is GENESIS [17] can perform both scene segmentation and generation by capturing relationships between objects with an autoregressive prior.
∗Now afﬁliated with Google DeepMind. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
As noted in Novotny et al. [40], however, using RNNs for instance segmentation requires processing high-dimensional inputs in a sequential fashion which is computationally expensive and does not scale well to large images with potentially many objects. We also posit that recurrent inference is not only problematic from a computational point of view, but that it can also inhibit the learning of object representations by imposing an unnatural ordering on objects. In particular, we argue that this leads to different object slots receiving gradients of varying magnitude which provides a possible explanation for models collapsing to a single object slot during training, unless the ﬂexibility of the model is restricted (see [18]). While iterative reﬁnement instead infers unordered object representations, it requires the a priori initialisation of a ﬁxed number of object slots even though the number of objects in an image is unknown.
In contrast, our work takes inspiration from the literature on supervised instance segmentation and adopts an instance colouring approach (e.g. [40–43]) in which pixel-wise embeddings—or colours—are clustered into attention masks. Typically, either a supervised learning signal is used to obtain cluster seeds (e.g. [40, 41]) or clustering is performed as a non-differentiable post-processing operation (e.g. [42, 43]). Neither of these approaches is suitable for unsupervised, end-to-end learning of segmentation masks. We hence develop an instance colouring stick-breaking process (IC-SBP) to cluster embeddings in a differentiable fashion. This is achieved by stochastically sampling cluster seeds from the pixel embeddings to perform a soft grouping of the embeddings into a set of randomly ordered attention masks. It is therefore possible to infer object representations both without imposing a ﬁxed ordering or performing iterative reﬁnement.
Inspired by GENESIS [17], we leverage the IC-SBP to develop GENESIS-V2, a novel model that learns to segment objects in images without supervision and that uses an autoregressive prior to generate scenes in an interpretable, object-centric fashion. GENESIS-V2 is comprehensively benchmarked against recent prior art [16, 17, 24] on established synthetic datasets—ObjectsRoom
[44] and ShapeStacks [45]—where it performs strongly in comparison to several recent base-lines. We also evaluate GENESIS-V2 on more challenging real-world images from the Sketchy
[46] and the MIT-Princeton Amazon Picking Challenge (APC) 2016 Object Segmentation datasets
[47], where it also achieves promising results. Code and pre-trained models are available at https://github.com/applied-ai-lab/genesis. 2