Abstract
We consider how to privately share the personalized privacy losses incurred by objective perturbation, using per-instance differential privacy (pDP). Standard differential privacy (DP) gives us a worst-case bound that might be orders of magnitude larger than the privacy loss to a particular individual relative to a ﬁxed dataset. The pDP framework provides a more ﬁne-grained analysis of the privacy guarantee to a target individual, but the per-instance privacy loss itself might be a function of sensitive data. In this paper, we analyze the per-instance privacy loss of releasing a private empirical risk minimizer learned via objective perturbation, and propose a group of methods to privately and accurately publish the pDP losses at little to no additional privacy cost. 1

Introduction
An explosion of data has fueled innovation in machine learning applications and demanded, in equal turn, privacy protection for the sensitive data with which machine learning practitioners train and evaluate models.
Differential privacy (DP) (Dwork et al., 2006, 2014a) has become a mainstay of privacy-preserving data analysis, replacing less robust privacy deﬁnitions such as k-anonymity which fail to protect against sufﬁciently powerful de-anonymization attacks (Narayanan & Shmatikov, 2008). In contrast,
DP offers provable privacy guarantees that are robust against an arbitrarily strong adversary.
The data curator could trivially protect against privacy loss by reporting a constant function, or by releasing only data-independent noise. The key challenge of DP is to release privatized output that retains utility to the data analyst.
A desired level of utility in a machine learning application might necessitate a high value of (cid:15), but the privacy guarantees degrade quickly past (cid:15) = 1. (Triastcyn & Faltings, 2020) construct an example whereby a differentially private algorithm with (cid:15) = 2 allows an attacker to use a maximum-likelihood estimate to conclude with up to 88% accuracy that an individual is in a dataset. For (cid:15) = 5, the theoretical upper bound on the accuracy of an optimal attack is 99.3%.
Moreover, practical applications of differential privacy commonly use large values of (cid:15). A study of
Apple’s deployment of differential privacy revealed that the overall daily privacy loss permitted by the system was as high as (cid:15) = 6 for Mac OS 10.12.3 and (cid:15) = 14 for iOS 10.1.1 (Tang et al., 2017) – offering only scant privacy protection!
Recent work (Yu et al., 2021) has empirically justiﬁed large privacy parameters by conducting membership inference attacks to demonstrate that these seemingly tenuous privacy guarantees are actually much stronger in practice. These results are unsurprising from the perspective that DP gives a data-independent bound on the worst-case privacy loss which is likely to be a conservative estimate of the risk to a particular individual when a DP algorithm is applied to a particular input dataset. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Per-instance differential privacy provides a theoretically sound alternative to the empirical approach for revealing the gap between the worst-case DP bound and the actual privacy loss in practice. The privacy loss to a particular individual relative to a ﬁxed dataset might be orders of magnitude smaller than the worst-case bound guaranteed by standard DP. In this case, an algorithm meeting a desired level of utility but providing weak DP guarantees may, for the same level of utility, achieve drastically more favorable per-instance DP guarantees.
The remaining challenge is that the per-instance privacy loss is a function of the entire dataset; publishing it directly would negate the purpose of privately training a model in the ﬁrst place! In this paper, we propose a methodology to privately release the per-instance privacy losses associated with private empirical risk minimization. Our contributions are as follows:
• We introduce ex-post per-instance differential privacy to provide a sharp characterization of the privacy loss to a particular individual that adapts to both the input dataset and the algorithm’s output.
• We present a novel analysis of the ex-post per-instance privacy losses incurred by the objective perturbation mechanism, demonstrating that these ex-post pDP losses are orders of magnitude smaller than the worst-case guarantee of differential privacy.
• We propose a group of methods to privately and accurately release the ex-post pDP losses. In the particular case of generalized linear models, we show that we can accurately publish the private ex-post pDP losses using a dimension- and dataset-independent bound.
• One technical result of independent interest is a new DP mechanism that releases the Hessian matrix by adding a Gaussian Orthogonal Ensemble matrix, which improves the classical
“AnalyzeGauss” (Dwork et al., 2014b) by roughly a constant factor of 2. 1.1