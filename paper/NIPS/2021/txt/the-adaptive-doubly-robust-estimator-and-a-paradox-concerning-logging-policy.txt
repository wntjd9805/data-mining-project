Abstract
The doubly robust (DR) estimator, which consists of two nuisance parameters, the conditional mean outcome and the logging policy (the probability of choosing an action), is crucial in causal inference. This paper proposes a DR estimator for dependent samples obtained from adaptive experiments. To obtain an asymptoti-cally normal semiparametric estimator from dependent samples with non-Donsker nuisance estimators, we propose adaptive-ﬁtting as a variant of sample-splitting.
We also report an empirical paradox that our proposed DR estimator tends to show better performances compared to other estimators utilizing the true logging policy. While a similar phenomenon is known for estimators with i.i.d. samples, traditional explanations based on asymptotic efﬁciency cannot elucidate our case with dependent samples. We conﬁrm this hypothesis through simulation studies. 1

Introduction
Adaptive experiments, including efﬁcient treatment effect estimation (van der Laan, 2008; Hahn et al., 2011), treatment regimes (Zhang et al., 2012a; Zhao et al., 2012), and multi-armed bandit (MAB) problems (Gittins, 1989; Lattimore & Szepesvári, 2020), are widely accepted and used in real-world applications, for example, in social experiments (Hahn et al., 2011), online advertisement (Zhang et al., 2012b), clinical trials (Chow & Chang, 2011; Villar, 2018), website optimization (White, 2012), and recommendation systems (Li et al., 2010). In those various applications, there is a signiﬁcant interest in off-line counterfactual inference using samples obtained from past trials generated by a logging policy (the probability of choosing an action: Li et al., 2010, 2011). Among several off-line evaluation criteria, this paper focuses on off-policy value estimation (OPVE: Li et al., 2015). The goal of OPVE is to estimate the expected value of the weighted outcome, which includes average treatment effect (ATE) estimation as a special case (Hirano et al., 2003; Bang & Robins, 2005).
In OPVE, when the true logging policy is known, there are mainly three types of estimators: inverse probability weighting (IPW: Horvitz & Thompson, 1952), direct method (DM), and augmented IPW (AIPW Bang & Robins, 2005) estimators. IPW-type estimators refer to a sample average of the outcomes weighted by the true logging policy. DM-type estimators refer to a sample average of an estimated conditional outcome. AIPW-type estimators consist of two components: the IPW part and the DM part. More details are given in Section 3. In addition, when the true logging policy is unknown, we call IPW-type and AIPW-type estimators that use the estimated logging policy
EIPW-type and DR-type estimators, respectively. In particular, and the focus of this paper, DR-type estimators are crucial in OPVE, since in most applications of interest the true logging policy is either unknown, difﬁcult, or costly to obtain, making IPW and AIPW-type estimators often infeasible.
While existing OPVE methods typically assume that the samples are independent and identically distributed (i.i.d.: Hirano et al., 2003; Dudík et al., 2011), adaptive experiments usually allow logging policies to be updated based on past observations, under which the generated samples are non-i.i.d.
∗masahiro_kato@cyberagent.co.jp 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this case, theoretical results shown under i.i.d. assumptions, such as consistency and asymptotic normality, are not guaranteed. In particular, the asymptotic normality is critical, as we can obtain
√
T -rate conﬁdence intervals, where T is the sample size. For this problem, van der Laan (2008) proposed an asymptotically normal AIPW estimator under dependent samples, with several follow-up studies, including Luedtke & van der Laan (2016, 2018), Hadad et al. (2021), Kato et al. (2020), and Zhan et al. (2021). However, a DR-type estimator for dependent samples has not been formally proposed, despite its importance.
Readers might think that, in many experimental designs, the experiment can be manipulated by the experimenter, thus the information of the logging policy is always accessible. However, the logging policy is often not recorded. This is because it is costly to do so in adaptive experiments, since we need to preserve a policy for each sample. Consider the case of Thompson sampling, where we select the arm with the highest expected reward sampled from the posterior distribution (Thompson, 1933).
In this case, if we want to know the logging policy, we need to calculate the probability that each arm is chosen from the posterior distribution. The calculation of this probability is not necessary for the bandit algorithm itself, but is an additional task for policy evaluation. In addition, to compute the logging policy, we not only need to record the information of the posterior distribution, but also need to calculate the probability by Monte Carlo simulation from the posterior distribution, if no analytical solution is available. This is a difﬁcult task in terms of both computational volume and computational resource. Moreover, in adaptive experiments in industry (e.g. AB tests), the number of logs is often in the order of several million. In such cases, storing the logging policy is also a difﬁcult task. By using DR-type estimators, we can alleviate this cost. Further, as we report below as a paradox, it is even possible that DR-type estimators, that do not use the true logging policy, outperform estimators using the true logging policy,
This paper proposes an asymptotically normal DR estimator for dependent samples. We call this
DR estimator the adaptive DR (ADR) estimator. There are mainly two difﬁculties concerning this estimator. First, we cannot use the central limit theorem (CLT) for i.i.d. or martingale difference sequences directly, as done in van der Laan (2008), Hadad et al. (2021), and Kato et al. (2020).
Second, when nuisance parameters are estimated with complicated models, the Donsker condition does not hold in general, which is required to show the asymptotic normality of semiparametric estimators, such as our proposed ADR estimator. In this paper, to solve these problems, we propose adaptive-ﬁtting, inspired by the double/debiased machine learning for i.i.d. samples (Chernozhukov et al., 2018). We also ﬁnd that by using the ADR estimator, not only can OPVE be done without knowing the true logging policy, the ADR estimator paradoxically often outperforms the performance of AIPW estimators that use the true logging policy. We list our contributions as follows. (I) Asymptotic normality of the ADR estimator. We show the asymptotic normality of our pro-posed ADR estimator. To the best of our knowledge, a DR-type estimator for dependent samples obtained in adaptive experiments has not been formally proposed. (II) Adaptive ﬁtting. While the asymptotic normalities of IPW and AIPW estimators are shown by the martingale CLT, we cannot directly apply this technique to our proposed ADR estimator. This is because the martingale condition does not hold when the true logging policy is unknown. To solve this problem, we utilize a novel sample-splitting method called adaptive-ﬁtting to show the asymptotic normality of the ADR estimator. We generalize this technique as a variant of sample-splitting (Chernozhukov et al., 2018) for semiparametric inference with dependent samples without the Donsker condition. Unlike existing sample-splitting methods, which assume that samples are i.i.d., the proposed adaptive-ﬁtting is applicable to non-i.i.d. samples. (III) Empirical paradox on using estimated logging policy. Through experimental studies, we investigate the empirical performance of our proposed ADR estimator. We ﬁnd that the ADR estimator often exhibits improved performances over other estimators using the true logging policy, such as the
AIPW estimator. Estimators requiring the true logging policy tend to empirically be unstable due to the instability of the nuisance parameter; the instability being caused by the logging policy being near zero before convergence. Our ﬁnding implies that the ADR estimator mitigates this instability. We call this phenomenon a paradox concerning logging policy because using more information (the true logging policy) does not improve empirical performance. A similar paradox is known for IPW-type estimators for i.i.d. samples (Hahn, 1998; Henmi & Eguchi, 2004). However, we cannot explain our paradox from a conventional semiparametric efﬁciency perspective (Hahn, 1998; Hirano et al., 2003), 2
as the asymptotic variances are the same between the ADR estimator (with an estimated logging policy) and the AIPW estimator (with the true logging policy), unlike IPW-type estimators.
Organization of this paper. In Section 2, we formulate our problem. In Section 3, we introduce existing OPVE estimators for dependent samples. In Section 4, we propose the ADR estimator, which is our main contribution. We call the method used for the ADR estimator adaptive-ﬁtting, and generalize it in Section 5. In Section 6, we report a paradox through simulation studies and explain the cause of this phenomenon. In Section 7, we conduct experiments using benchmark datasets. 2 Problem setting
Suppose that there is a time series 1, 2, . . . , T , and denote the set as [T ] = {1, . . . , T }. For t ∈ [T ], let At be an action in A = {1, 2, . . . , K}, Xt be a covariate observed by a decision maker when choosing an action, and X be its space. Following the Neyman–Rubin causal model (Rubin, 1974), let a reward at period t be Yt = (cid:80)K 1[At = a]Yt(a), where Yt(a) : A → R is a potential (random) outcome. In summary, we have a dataset (cid:8)(Xt, At, Yt)(cid:9)T t=1 with the following data-generating process (DGP): a=1
Step 1: we observe a covariate Xt ∼ p(x);
Step 2: we choose At = a with probability pt(a|x);
Step 3: we observe an outcome Yt = (cid:80)K a=1 1[At = a]Yt(a), where Yt(a) ∼ pa(y|x), where p(x) denotes the density of the covariate Xt, pt(a|x) denotes the probability of choosing an action a conditional on a covariate x at period t, and pa(y|x) denotes the density of a reward Yt(a) conditional on a covariate x. We assume that p(x) and pa(y|x) are invariant across periods, and Xt and Yt(a) are drawn from these densities independently of the other period random variables; that is,
{(Xt, Yt(1), . . . , Yt(K))}T t=1 is i.i.d., but pt(a|x) can take different values across periods based on past observations. In this case, the samples (cid:8)(Xt, At, Yt)(cid:9)T t=1 are correlated over time, that is, the samples are not i.i.d. Let Ωt−1 = {Xt−1, At−1, Yt−1, . . . , X1, A1, Y1} be the history and Mt−1 be a set of possible histories until the t-th period. The probability pt(a|x) is determined by a logging policy πt : A × X × Mt−1 → (0, 1), such that (cid:80)K a=1 πt(a|x, Ωt−1) = 1, which is a function of a covariate Xt, an action At, and history Ωt−1. We also assume that πt is conditionally independent of
Yt(a) to satisfy unconfoundedness (Remark 2).
Remark 1 (Stable unit treatment value assumption (SUTVA)). The DGP implies SUTVA; that is, pa(y|x) is invariant to random assignments in other periods (Angrist et al., 1996).
Remark 2 (Unconfoundedness). In this paper, unconfoundedness refers to independence between (Yt(1), . . . , Yt(K)) and At, conditional on Xt and Ωt−1, which is required for identiﬁcation. 2.1 OPVE
The goal of OPVE is to estimate the expected value of the sum of the outcomes Yt(a) weighted by an evaluation function πe : A × X → R; that is,
R(πe) := E(Xt,Yt(1),...,Yt(K)) (cid:35)
πe(a|Xt)Yt(a)
, (cid:34) K (cid:88) a=1 where the expectation E(Xt,Yt(1),...,Yt(K)) is taken over (Xt, Yt(1), . . . , Yt(K)). We denote it as E, when there is no ambiguity. As with Dudík et al. (2011), the evaluation function is usually referred to as an evaluation policy, where πe(a|x) ∈ [0, 1] and the sum is 1. However, to not restrict it so we can include other forms, such as the ATE, we refer to it differently. The ATE is also a special case of
OPVE for A = {1, 2}, where πe(1|x) = 1 and πe(2|x) = −1. To identify R(πe), we assume the overlap in the distributions of policies, convergence of πt−1, and the boundedness of reward.
Assumption 1. For all t ∈ [T ], a ∈ A, x ∈ X , and Ωt−1 ∈ Mt−1, there exists a constant Cπ > 0, such that (cid:12) (cid:12) (cid:12)
πe(a|x)
πt(a|x,Ωt−1) (cid:12) (cid:12) (cid:12) ≤ Cπ.
Assumption 1 equivalently means that πt(a|x) > 0 for all a ∈ A and x ∈ X . 3
p
Assumption 2. For all x ∈ X and a ∈ A, πt(a|x, Ωt−1) − ˜π(a|x)
−→ 0, where ˜π : A × X → (0, 1).
Assumption 3. For all t ∈ [T ] and a ∈ A, there exists a constant CY > 0, such that |Yt(a)| ≤ CY .
Although the reader may feel that Assumption 1–2 and the SUTVA 1 are strong, we adopt it as a simple and basic case for the application, in order to introduce adaptive-ﬁtting and the ADR estimator.
We can extend the proposed method for different cases, such as when the data has the structure of batches or when the average of logging policy converges (Kato, 2021). 2.2 Notations
We denote E[Yt(a)|x] and Var(Yt(a)|x) as f ∗(a, x) and v∗(a, x), respectively. Let ˆft(a, x) be an estimator of f ∗(a, x) constructed from Ωt. Let N (µ, var) be the normal distribution with the mean µ and the variance var. For a random variable Z with density p(z) and function µ, let (cid:107)µ(Z)(cid:107)2 = (cid:16) (cid:82) |µ(z)|2p(z)dz (cid:17)1/2 be the L2-norm. 3 Preliminaries of OPVE 3.1 Existing estimators
For estimating R(πe) from dependent samples, existing studies propose various estimators. An adap-πe(At|Xt)1[At=a]Yt tive version of the IPW estimator is deﬁned as RIPW
πt−1(At|Xt,Ωt−1) .
T (πe) =
If a=1 πe(a|x) ˆfT (a|Xt) is known to be consistent to R(πe). As an adaptive version of 1
T the AIPW estimator, van der Laan (2008) proposed an estimator (cid:98)RAIPW the model speciﬁcation is correct, (cid:80)T the direct method (DM) estimator RDM (πe) deﬁned as (πe) = 1
T (cid:80)K (cid:80)K (cid:80)T a=1 t=1 t=1
T
T 1
T
T (cid:88)
K (cid:88) t=1 a=1 (cid:40) πe(a|Xt)1[At = a] (cid:16) (cid:17)
Yt − ˆft−1(a, Xt)
πt−1(a|Xt, Ωt−1)
+ πe(a|Xt) ˆft−1(a, Xt)
. (cid:41)
Using the martingale property, van der Laan (2008) showed asymptotic normality under Assumption 2. 3.2 Asymptotic efﬁciency
We are often interested in the asymptotic efﬁciency of estimators. The lower bound of the asymptotic variance is deﬁned for an estimator under some posited models of the DGP described in Section 2.
As with the Cramér-Rao lower bound for the parametric model, we can also deﬁne the lower bound for the non- or semiparametric model (Bickel et al., 1998). The semiparametric lower bound of the
DGP under p1(a|x) = · · · = pT (a|x) = p(a|x) is given as follows (Hahn, 1998; Narita et al., 2019):
Ψ(πe, p) = E (cid:34) K (cid:88) (cid:0)πe(a|Xt)(cid:1)2 v∗(a, Xt) a=1 p(a|Xt)
+ (cid:32) K (cid:88) a=1
πe(a|Xt)f ∗(a, Xt) − R(πe) (cid:33)2 (cid:35)
.
The asymptotic variance of the asymptotic distribution is also known as the asymptotic mean squared error (MSE); that is, an OPVE estimator ˆRT achieving the semiparametric lower bound also minimizes the MSE to the true value R(πe), E
, not just obtaining a tight (cid:20)(cid:16) ˆRT − R(πe) (cid:17)2(cid:21) conﬁdence interval. 3.3