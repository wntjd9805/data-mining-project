Abstract
Comparing metric measure spaces (i.e. a metric space endowed with a probability distribution) is at the heart of many machine learning problems. The most popular distance between such metric measure spaces is the Gromov-Wasserstein (GW) distance, which is the solution of a quadratic assignment problem. The GW dis-tance is however limited to the comparison of metric measure spaces endowed with a probability distribution. To alleviate this issue, we introduce two Unbalanced
Gromov-Wasserstein formulations: a distance and a more tractable upper-bounding relaxation. They both allow the comparison of metric spaces equipped with ar-bitrary positive measures up to isometries. The ﬁrst formulation is a positive and deﬁnite divergence based on a relaxation of the mass conservation constraint using a novel type of quadratically-homogeneous divergence. This divergence works hand in hand with the entropic regularization approach which is popular to solve large scale optimal transport problems. We show that the underlying non-convex optimization problem can be efﬁciently tackled using a highly paral-lelizable and GPU-friendly iterative scheme. The second formulation is a distance between mm-spaces up to isometries based on a conic lifting. Lastly, we provide numerical experiments on synthetic examples and domain adaptation data with a
Positive-Unlabeled learning task to highlight the salient features of the unbalanced divergence and its potential applications in ML. 1

Introduction
Comparing data distributions on different metric spaces is a basic problem in machine learning. This class of problems is for instance at the heart of surfaces [Bronstein et al., 2006] or graph matching [Xu et al., 2019] (equipping the surface or graph with its associated geodesic distance), regression problems in quantum chemistry [Gilmer et al., 2017] (viewing the molecules as distributions of points in R3) and natural language processing [Grave et al., 2019, Alvarez-Melis and Jaakkola, 2018] (where texts in different languages are embedded as points distributions in different vector spaces).
Metric measure spaces. The mathematical way to formalize these problems is to model the data as metric measure spaces (mm-spaces). A mm-space is denoted as X = (X, d, µ) where X is a complete separable set endowed with a distance d and a positive Borel measure µ ∈ M+(X). For instance, if X = (xi)i is a ﬁnite set of points, then µ = (cid:80) i miδxi (here δxi is the Dirac mass at xi) is simply a set of positive weights mi = µ({xi}) ≥ 0 associated to each point xi, which accounts for its mass or importance. For instance, setting some mi to 0 is equivalent to removing the point xi. We 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
refer to Sturm [2012] for a mathematical account on the theory of mm-spaces. In all the applications highlighted above, it makes sense to perform the comparisons up to isometric transformations of the data. Two mm-spaces X = (X, dX , µ) and Y = (Y, dY , ν) are considered to be equal (denoted
X ∼ Y) if they are isometric, meaning that there is a bijection ψ : spt(µ) → spt(ν) (where spt(µ) is the support of µ) such that dX (x, y) = dY (ψ(x), ψ(y)) and ψ(cid:93)µ = ν. Here ψ(cid:93) is the push-forward operator, so that ψ(cid:93)µ = ν is equivalent to imposing ν(A) = µ(ψ−1(A)) for any set A ⊂ Y . For discrete spaces where µ = (cid:80) i miδxi , then one should have ν = ψ(cid:93)µ = (cid:80) i miδψ(xi). As highlighted by Mémoli [2011], considering mm-spaces up to isometry is a powerful way to formalize and analyze a wide variety of problems such as matching, regression and classiﬁcation of distributions of points belonging to different spaces. Most often, the objects of interest come with a natural distance such as an intrinsic or extrinsic distance and the uniform measure is the usual choice to make mm-spaces widely applicable. The key to unlock all these problems is the computation of a distance between mm-spaces up to isometry. So far, existing distances (reviewed below) assume that µ is a probability distribution, i.e. µ(X) = 1. This constraint is not natural and sometimes problematic for most of the practical applications to machine learning. The goal of this paper is to alleviate this restriction.
We deﬁne for the ﬁrst time a class of distances between unbalanced metric measure spaces, these distances being upper-bounded by divergences which can be approximated by an efﬁcient numerical scheme.
Csiszár divergences The simplest case is when X = Y and one simply ignores the underlying metric. One can then use Csiszár divergences (or ϕ-divergences), which perform a pointwise comparison (in contrast with optimal transport distances, which perform a displacement comparison).
It is deﬁned using an entropy function ϕ : R+ → [0, +∞], which is a convex, lower semi-continu-(cid:1)dν + ous, positive function with ϕ(1) = 0. The Csiszár ϕ-divergence reads Dϕ(µ|ν) (cid:44) (cid:82)
X dµ⊥, where µ = dµ dν ν + µ⊥ is called the Radon-Nikodym or the Lebesgue decomposition
ϕ(cid:48)
∞ = limr→∞ ϕ(r)/r ∈ R ∪ {+∞} is called the recession constant. of µ with respect to ν and ϕ(cid:48)
This divergence Dϕ is convex, positive, 1-homogeneous and weak* lower-semicontinuous, see Liero et al. [2015] for details. Particular instances of ϕ-divergences are Kullback-Leibler (KL) for ϕ(r) = r log(r) − r + 1 (note that ϕ(cid:48)
∞ = ∞) and Total Variation (TV) for ϕ(r) = |r − 1|.
X ϕ(cid:0) dµ dν
∞ (cid:82)
If the common embedding space X is equipped
Balanced and unbalanced optimal transport. with a distance d(x, y), one can use more elaborated methods such as optimal transport (OT) distances, which are computed by solving convex optimization problems. This type of methods has proven useful for ML problems as diverse as domain adaptation [Courty et al., 2014], supervised learning over histograms [Frogner et al., 2015] and unsupervised learning of generative models [Arjovsky et al., 2017]. In this case, the extension from probability distributions to arbitrary positive measures (µ, ν) ∈
M+(X)2 is now well understood and corresponds to the theory of unbalanced OT. Following Liero et al. [2015], Chizat et al. [2018a], a family of unbalanced Wasserstein distances is deﬁned by solving
UW(µ, ν)q (cid:44) inf
π∈M(X×X) (cid:90)
λ(d(x, y))dπ(x, y) + Dϕ(π1|µ) + Dϕ(π2|µ). (1)
Here (π1, π2) are the two marginals of the joint distribution π, deﬁned by π1(A) = π(A × Y ) for
A ⊂ X. The mapping λ : R+ → R and exponent q ≥ 1 should be chosen wisely to ensure for instance that UW deﬁnes a distance (see Section 2.2.1). It is frequent to take ρDϕ instead of Dϕ (i.e. take ψ = ρϕ) to adjust the strength of the marginals’ penalization. Balanced OT is retrieved with the convex indicator ϕ = ι{1} (i.e. ϕ(1) = 0 and ϕ(x) = +∞ otherwise) or by taking the limit
ρ → +∞, which enforces π1 = µ and π2 = ν. When 0 < ρ < +∞, unbalanced OT operates a trade-off between transportation and creation of mass, which is crucial to be robust to outliers in the data and to cope with mass variations in the modes of the distributions. For supervised tasks, the value of ρ should be cross-validated to obtain the best performances. Its use is gaining popularity in applications, such as medical imaging registration [Feydy et al., 2019a], videos [Lee et al., 2019], generative learning [Balaji et al., 2020] and gradient ﬂow to train neural networks [Chizat and Bach, 2018, Rotskoff et al., 2019]. Furthermore, existing efﬁcient algorithms for balanced OT extend to this unbalanced problem. In particular Sinkhorn’s iterations, introduced in ML for balanced OT by Cuturi
[2013], extend to unbalanced OT [Chizat et al., 2018b, Séjourné et al., 2019], as detailed in Section 3.
The Gromov-Wasserstein distance and its applications. The Gromov-Wasserstein (GW) dis-tance [Mémoli, 2011, Sturm, 2012] generalizes the notion of OT to the setting of mm-spaces up to 2
isometries. It replaces the linear cost (cid:82) λ(d)dπ of OT by a quadratic function. It reads
GW(X , Y)q (cid:44) min
π∈M+(X×Y ) (cid:26)(cid:90)
λ(|dX (x, x(cid:48)) − dY (y, y(cid:48))|)dπ(x, y)dπ(x(cid:48), y(cid:48)) : (cid:27)
π1 = µ
π2 = ν
. (2)
It is proved in Mémoli [2011], Sturm [2012] that GW deﬁnes with λ(t) = tq a distance up to isometries on balanced mm-spaces (i.e. the measures are probability distributions). The GW distance is applied successfully in natural language processing for unsupervised translation learning [Grave et al., 2019, Alvarez-Melis and Jaakkola, 2018], in generative learning for objects lying in spaces of different dimensions [Bunne et al., 2019] and to build VAE for graphs [Xu et al., 2020]. It has been adapted for domain adaptation over different spaces [Redko et al., 2020]. It is also a relevant distance to compute barycenters between graphs or shapes [Vayer et al., 2018, Chowdhury and
Needham, 2020]. When (X , Y) are Euclidean spaces, this distance compares distributions up to rigid isometry, and is closely related (but not equal) to metrics deﬁned by procrustes analysis [Grave et al., 2019, Alvarez-Melis et al., 2019]. The problem (2) is non convex because the quadratic form (cid:82) λ(|dX − dY |)dπ ⊗ π is not positive in general. It is in fact closely related to quadratic assignment problems [Burkard et al., 1998], which are used for graph matching problems, and are known to be NP-hard in general. Nevertheless, non-convex optimization methods have been shown to be successful in practice to use GW distances for ML problems. This includes for instance alternating minimization [Mémoli, 2011, Redko et al., 2020] and entropic regularization [Peyré et al., 2016, Gold and Rangarajan, 1996].