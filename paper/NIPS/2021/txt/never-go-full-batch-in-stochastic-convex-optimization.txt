Abstract
We study the generalization performance of full-batch optimization algorithms for stochastic convex optimization: these are ï¬rst-order methods that only access the exact gradient of the empirical risk (rather than gradients with respect to individual data points), that include a wide range of algorithms such as gradient descent, mirror descent, and their regularized and/or accelerated variants. We provide a new separation result showing that, while algorithms such as stochastic gradient descent can generalize and optimize the population risk to within Îµ after
ğ‘‚ (1/Îµ2) iterations, full-batch methods either need at least â„¦(1/Îµ4) iterations or exhibit a dimension-dependent sample complexity. 1

Introduction
Stochastic Convex Optimization (SCO) is a fundamental problem that received considerable attention from the machine learning community in recent years [28, 15, 4, 11, 2]. In this problem, we assume a learner that is provided with a ï¬nite sample of convex functions drawn i.i.d. from an unknown distribution. The learnerâ€™s goal is to minimize the expected function. Owing to its simplicity, it serves as an almost ideal theoretical model for studying generalization properties of optimization algorithms ubiquitous in practice, particularly ï¬rst-order methods which utilize only ï¬rst derivatives of the loss rather than higher-order ones.
One prominent approach for SCOâ€”and learning more broadlyâ€”is to consider the empirical risk (the average objective over the sample) and apply a ï¬rst-order optimization algorithm to minimize it.
The problem of learning is then decoupled into controlling the optimization error over the empirical risk (training error) and bounding the diï¬€erence between the empirical error and the expected error (generalization error).
In convex optimization, the convergence of diï¬€erent ï¬rst-order methods has been researched exten-sively for many years (e.g., [26, 25, 5]), and we currently have a very good understanding of this setting in terms of upper as well lower bounds on worst-case complexity. However, in SCO where the generalization error must also be taken into account, our understanding is still lacking. In fact, this is one of the few theoretical learning models where the optimization method aï¬€ects not only the optimization error but also the generalization error (distinctively from models such as PAC learning and generalized linear models). In particular, it has been shown [28, 15] that some minima of the empirical risk may obtain large generalization error, while other minima have a vanishingly small 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
generalization error. To put diï¬€erently, learning in SCO is not only a question of minimizing the empirical risk, but also a question of how one minimizes it. However, the results of [28, 15] leave open the question of whether concrete optimization also have diï¬€erent generalization properties.
Towards better understanding, Amir et al. [2] recently studied the generalization properties of full-batch gradient descent (GD), where each step is taken with respect to the gradient of the empirical risk. For GD (and a regularized variant thereof), they gave a lower bound on the generalization error as a function of iteration number, which is strictly larger than the well-known optimal rate obtained by stochastic gradient descent (SGD), where each step is taken with respect to the gradient at a sampled example. Notably, the lower bound of [2] precisely matches the dimension-independent stability-based upper bound recently shown for full-batch GD by Bassily et al. [4]. The separation between full-batch GD and SGD is the ï¬rst evidence that not only abstract Empirical Risk Minimizers may fail to generalize in SCO, but in fact also basic methods such as GD could be prone to such overï¬tting. A natural question is, then, whether overï¬tting is inherent to full-batch algorithms, that minimize the objective only through access to the exact empirical risk, or whether this suboptimality can be remedied by adding regularization, noise, smoothing, or any other mechanism for improving the generalization of GD.
In this work we present and analyze a model of full-batch optimization algorithms for SCO. Namely, we focus on algorithms that access the empirical risk only via a ï¬rst-order oracle that computes the exact (full-batch) gradient of the empirical loss, rather than directly accessing gradients with respect to individual samples. Our main result provides a negative answer to the question above by signiï¬cantly generalizing and extending the result of Amir et al. [2]: we show that any optimization method that uses full-batch gradients needs at least â„¦(1/Îµ4) iterations to minimize the expected loss to within Îµ error. This is in contrast with the empirical loss, which can be minimized with only
ğ‘‚ (1/Îµ2) steps.
Comparing SGD and GD in terms of the sample size ğ‘›, we see that SGD converges to an optimal generalization error of ğ‘‚ (1/
ğ‘›) after ğ‘‚ (ğ‘›) iterations, whereas a full-batch method must perform
ğ‘›) test error. We emphasize that we account here for the
â„¦(ğ‘›2) iterations to achieve the same ğ‘‚ (1/ oracle complexity, which coincides with the iteration complexity in the case of gradient methods. In terms of individual gradients calculations, while SGD uses at most ğ‘‚ (ğ‘›) gradient calculations (one sample per iteration), a full-batch method will perform â„¦(ğ‘›3) calculations (ğ‘› samples per iteration).
âˆš
âˆš
The above result is applicable to a wide family of full-batch learning algorithms: regularized GD (with any data-independent regularization function), noisy GD, GD with line-search or adaptive step sizes, GD with momentum, proximal methods, coordinate methods, and many more. Taken together with upper bound of Bassily et al. [4], we obtain a sharp rate of Î˜(1/Îµ4) for the generalization-complexity of full-batch methods. Surprisingly, this rate is achieved by standard GD (with an unusual step-size choice of Î· = Î˜(Îµ3)), and it cannot be improved by adding regularization of any sort, nor by adding noise or any other form of implicit/explicit bias. 1.1