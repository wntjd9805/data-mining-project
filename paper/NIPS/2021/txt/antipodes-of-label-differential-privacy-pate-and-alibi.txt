Abstract
We consider the privacy-preserving machine learning (ML) setting where the trained model must satisfy differential privacy (DP) with respect to the labels of the training examples. We propose two novel approaches based on, respectively, the
Laplace mechanism and the PATE framework, and demonstrate their effectiveness on standard benchmarks.
While recent work by Ghazi et al. proposed Label DP schemes based on a ran-domized response mechanism, we argue that additive Laplace noise coupled with
Bayesian inference (ALIBI) is a better ﬁt for typical ML tasks. Moreover, we show how to achieve very strong privacy levels in some regimes, with our adaptation of the PATE framework that builds on recent advances in semi-supervised learning.
We complement theoretical analysis of our algorithms’ privacy guarantees with empirical evaluation of their memorization properties. Our evaluation suggests that comparing different algorithms according to their provable DP guarantees can be misleading and favor a less private algorithm with a tighter analysis.
Code for implementation of algorithms and memorization attacks is available from https://github.com/facebookresearch/label_dp_antipodes. 1

Introduction
Sophisticated machine learning models perform well on their target tasks despite of—or thanks to—their predilection for data memorization [39, 2, 17]. When such models are trained on non-public inputs, privacy concerns become paramount [31, 6, 7], which motivates the actively developing area of privacy-preserving machine learning.
With some notable exceptions, which we discuss in the related works section, existing research has focused on an “all or nothing” privacy deﬁnition, where all of the training data, i.e., both features and labels, is considered private information. While this goal is appropriate for many applications, there are several important scenarios where label-only privacy is the right solution concept.
A prominent example of label-only privacy is in online advertising, where the goal is to predict conversion of an ad impression (the label) given a user’s proﬁle and the spot’s context (the features).
The features are available to the advertising network, which trains the model, while the labels (data on past conversion events) are visible only to the advertiser. More generally, any two-party setting where data is vertically split between public inputs and (sensitive) outcomes is a candidate for training with label-only privacy.
In this work we adopt the deﬁnition of differential privacy as our primary notion of privacy. The deﬁ-nition, introduced in the seminal work by Dwork et al. [13], satisﬁes several important properties that make it well-suited for applications: composability, robustness to auxiliary information, preservation under post-processing, and graceful degradation in the presence of correlated inputs (group privacy).
Following Ghazi et al. [18], we refer to the setting of label-only differential privacy as Label DP.
∗{manimalek,imironov,krp,shilov}@fb.com, Facebook AI.
†tramer@cs.stanford.edu, Stanford University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our contributions. We explore two approaches—with very different characteristics—toward
Label DP. These approaches and our methodology for estimating empirical privacy loss via label memorization are brieﬂy reviewed below:
• PATE: We adapt the PATE framework (Private Aggregation of Teacher Ensembles) of
Papernot et al. [28] to the Label DP setting by observing that PATE’s central assumption— availability of a public unlabeled dataset—can be satisﬁed for free.
Indeed, a public unlabeled dataset is simply the private dataset with the labels removed! By instantiating
PATE with a state-of-the-art technique for semi-supervised learning, we demonstrate an excellent tradeoff between empirical privacy loss and accuracy.
• ALIBI (Additive Laplace with Iterative Bayesian Inference): We propose applying additive
Laplace noise to a one-hot encoding of the label. Since differential privacy is preserved under post-processing, we can de-noise the mechanism’s output by performing Bayesian inference using the prior provided by the model itself, doing so continuously during the training process. ALIBI improves, particularly in a high-privacy regime, Ghazi et al.’s approach based on randomized response [18].
• Empirical privacy loss. We describe a memorization attack targeted at the Label DP scenario that efﬁciently extracts lower bounds for the privacy parameter that, in some cases, come within a factor of 2.5 from the theoretical, worst-case upper bounds against practically relevant, large-scale models. For comparison, recent, most advanced black-box attacks on
DP-SGD have approximately a 10 gap between the upper and lower bounds [20, 26].
Both algorithms, PATE and ALIBI, come with rigorous privacy analyses that, for any ﬁxed setting of parameters, result in an (ε, δ)-DP bound. We emphasize that these bounds are just that—they are upper limits on an attacker’s ability to breach privacy (in a precise sense) on worst-case inputs. Even though these bounds can be pessimistic, intuitively, a smaller ε for the same δ corresponds to a more private instantiation of a mechanism, and indeed this is likely the case within an algorithmic family.
×
Can the privacy upper bounds be used to compare diverse mechanisms with widely dissimilar analyses, such as ours? We argue that focusing on the upper bounds alone may be misleading to the point of favoring a less private algorithm with tighter analysis over a more private one whose privacy analysis is less developed. For a uniform perspective, we estimate the empirical privacy loss of all trained models by evaluating the performance of a black-box membership inference attack [31, 20, 26].
Our attack directly measures memorization of deliberately mislabeled examples, or canaries [6].
Following prior work [26, 20], we use the success rate of our attack to compute a lower bound on the level ε of label-DP achieved by the training algorithm. To avoid the prohibitive cost of retraining a model for each canary, we propose and analyze a heuristic that consists in inserting multiple canaries into one model, and measuring an attacker’s success rate in guessing each canary’s label. In some settings, we ﬁnd that the empirical privacy of PATE (as measured by our attack) is signiﬁcantly stronger than that of ALIBI, even though the provable DP bound for PATE is much weaker.
Finally, to characterize setups where label-only privacy may be applicable, it is helpful to distinguish between inference tasks, where the label is fully determined by the features, and prediction, where there is some uncertainty as to the eventual outcome, despite (hopefully) some predictive value of the feature set. We are mostly concerned with the latter scenario, where the outcome is not evident from the features, or protecting choices made by individuals may be mandated or desirable (for connection between privacy and self-determination see Schwartz [30]). While we do use public data sets (CIFAR-10 and CIFAR-100) in which it is possible to “infer” the label, we do so only to evaluate our approaches on standard benchmarks. We completely remove the triviality of inference by adding mislabeled canaries in our attack, thereby introducing a measure of non-determinism. 2 Notation and Deﬁnitions
X are the features and y
Throughout the paper we consider standard supervised learning problems. The inputs are (x, y) pairs
Y are the labels. (If some labels are absent, the problem where x becomes semi-supervised learning.) The cardinality of the set Y is the number of classes C. The task is to learn a model M parameterized with weights W that predicts y given x. To this end the training algorithm has access to a training set of n samples
∈
∈ (xi, yi)
{ n.
} 2
Differential privacy due to Dwork et al. [13] is a rigorous, quantiﬁable notion of individual privacy for statistical algorithms. (See Dwork and Roth [14] and Vadhan [33] for book-length introductions.)
Deﬁnition 2.1 (Differential Privacy). We say that a randomized mechanism
R satisﬁes
: U (ε, δ)-differential privacy (DP) if for all adjacent inputs D, D(cid:48)
U that differ in contributions of a single sample, the following holds:
M
→
∈
If δ = 0, we refer to it as ε-DP.
S
∀
⊂
R Pr[ (D)
M
S] eε
·
≤
∈
Pr[ (D(cid:48))
M
S] + δ.
∈
The notion of adjacency between databases D and D(cid:48) is domain- and application-dependent. To instantiate the Label DP setting, we say that D and D(cid:48) are adjacent if they consist of the same n and are identical except for a difference in a single label at index i∗: number of examples
}
D(cid:48). (xi∗ , yi∗ )
∈ (xi, yi)
{ i∗ )
D and (xi∗ , y(cid:48)
∈ 3