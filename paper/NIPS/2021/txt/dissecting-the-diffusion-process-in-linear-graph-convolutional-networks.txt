Abstract
Graph Convolutional Networks (GCNs) have attracted more and more attentions in recent years. A typical GCN layer consists of a linear feature propagation step and a nonlinear transformation step. Recent works show that a linear GCN can achieve comparable performance to the original non-linear GCN while being much more computationally efﬁcient. In this paper, we dissect the feature prop-agation steps of linear GCNs from a perspective of continuous graph diffusion, and analyze why linear GCNs fail to beneﬁt from more propagation steps. Fol-lowing that, we propose Decoupled Graph Convolution (DGC) that decouples the terminal time and the feature propagation steps, making it more ﬂexible and capa-ble of exploiting a very large number of feature propagation steps. Experiments demonstrate that our proposed DGC improves linear GCNs by a large margin and makes them competitive with many modern variants of non-linear GCNs. Code is available at https://github.com/yifeiwang77/DGC. 1

Introduction
Recently, Graph Convolutional Networks (GCNs) have successfully extended the powerful repre-sentation learning ability of modern Convolutional Neural Networks (CNNs) to the graph data [7].
A graph convolutional layer typically consists of two stages: linear feature propagation and non-linear feature transformation. Simple Graph Convolution (SGC) [21] simpliﬁes GCNs by removing the nonlinearities between GCN layers and collapsing the resulting function into a single linear transformation, which is followed by a single linear classiﬁcation layer and then becomes a linear
GCN. SGC can achieve comparable performance to canonical GCNs while being much more com-putationally efﬁcient and using signiﬁcantly fewer parameters. Thus, we mainly focus on linear
GCNs in this paper.
Although being comparable to canonical GCNs, SGC still suffers from a similar issue as non-linear
GCNs, that is, more (linear) feature propagation steps K will degrade the performance catastroph-ically. This issue is widely characterized as the “over-smoothing” phenomenon. Namely, node features become smoothed out and indistinguishable after too many feature propagation steps [10].
In this work, through a dissection of the diffusion process of linear GCNs, we characterize a funda-mental limitation of SGC. Speciﬁcally, we point out that its feature propagation step amounts to a very coarse ﬁnite difference with a ﬁxed step size ∆t = 1, which results in a large numerical error.
∗Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
And because the step size is ﬁxed, more feature propagation steps will inevitably lead to a large terminal time T = K · ∆t → ∞ that over-smooths the node features.
To address these issues, we propose Decoupled Graph Convolution (DGC) by decoupling the termi-nal time T and propagation steps K. In particular, we can ﬂexibly choose a continuous terminal time
T for the optimal tradeoff between under-smoothing and over-smoothing, and then ﬁx the terminal time while adopting more propagation steps K. In this way, different from SGC that over-smooths with more propagation steps, our proposed DGC can obtain a more ﬁne-grained ﬁnite difference approximation with more propagation steps, which contributes to the ﬁnal performance both theo-retically and empirically. Extensive experiments show that DGC (as a linear GCN) improves over
SGC signiﬁcantly and obtains state-of-the-art results that are comparable to many modern non-linear
GCNs. Our main contributions are summarized as follows:
• We investigate SGC by dissecting its diffusion process from a continuous perspective, and characterize why it cannot beneﬁt from more propagation steps.
• We propose Decoupled Graph Convolution (DGC) that decouples the terminal time T and the propagation steps K, which enables us to choose a continuous terminal time ﬂexibly while beneﬁting from more propagation steps from both theoretical and empirical aspects.
• Experiments show that DGC outperforms canonical GCNs signiﬁcantly and obtains state-of-the-art (SOTA) results among linear GCNs, which is even comparable to many competi-tive non-linear GCNs. We think DGC can serve as a strong baseline for the future research. 2