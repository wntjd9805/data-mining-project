Abstract
Deep convolutional neural networks (CNNs) are often of sophisticated design with numerous learnable parameters for the accuracy reason. To alleviate the expensive costs of deploying them on mobile devices, recent works have made huge efforts for excavating redundancy in pre-deﬁned architectures. Nevertheless, the redundancy on the input resolution of modern CNNs has not been fully investigated, i.e., the resolution of input image is ﬁxed. In this paper, we observe that the smallest reso-lution for accurately predicting the given image is different using the same neural network. To this end, we propose a novel dynamic-resolution network (DRNet) in which the input resolution is determined dynamically based on each input sample.
Wherein, a resolution predictor with negligible computational costs is explored and optimized jointly with the desired network. Speciﬁcally, the predictor learns the smallest resolution that can retain and even exceed the original recognition accuracy for each image. During the inference, each input image will be resized to its predicted resolution for minimizing the overall computation burden. We then conduct extensive experiments on several benchmark networks and datasets.
The results show that our DRNet can be embedded in any off-the-shelf network architecture to obtain a considerable reduction in computational complexity. For instance, DR-ResNet-50 achieves similar performance with an about 34% com-putation reduction, while gaining 1.4% accuracy increase with 10% computation reduction compared to the original ResNet-50 on ImageNet. Code will be available at https://gitee.com/mindspore/models/tree/master/research/cv/DRNet. 1

Introduction
Deep convolutional neural networks (CNNs) have achieved remarkable success in various computer vision tasks, under the development of algorithms [6, 22, 37], computation power, and large-scale datasets [2, 17]. However, the outstanding performance is accompanied by large computational costs, which makes CNNs difﬁcult to deploy on mobile devices. With the increasing demand for CNNs on real-world applications, it is imperative to reduce the computational cost and meanwhile maintain the performance of neural networks.
Recently, researchers have devoted much effort to model compression and acceleration methods, including network pruning, low-bit quantization, knowledge distillation, and efﬁcient model design.
∗Equal contribution. † Corresponding author. This research has been supported by the Key R&D program of Zhejiang Province (Grant No. 2021C03139). This work was supported by NSFC (62072449, 61632003),
Guangdong-Hongkong-Macao Joint Research Grant (2020B1515130004), and Macao FDCT (0018/2019/ AKP). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Network pruning aims to prune the unimportant ﬁlters or blocks that are insensitive to model performance through a certain criterion [31, 15, 18, 25]. Low-bit quantization methods represent weights and activation in neural networks with low-bit values [11, 13]. Knowledge distillation transfers the knowledge of the teacher models to the student models to improve the performance [7, 38, 40]. The efﬁcient model design utilizes lightweight operations like depth-wise convolution to construct some novel architectures successfully[8, 42, 5]. Orthogonal to those methods that usually focus on the network weights or architectures, Guo et.al. [4] and Wang et.al. [33] study the redundacy that exists in the input images. However, the resolutions of the input images in most of existing compressed networks are still ﬁxed. Although deep networks are often trained using an uniform resolution (e.g., 224× 224 on the ImageNet), sizes and locations of objects in images are radically different. Figure 1 shows some samples that the required resolution for achieving the highest performance are different. For the given network architecture, the FLOPs (ﬂoating-number operations) of the network for processing image will be signiﬁcantly reduced for images with lower resolution.
Admittedly, the input resolution is a very impor-tant factor that affects the computational costs and the performance of CNNs. For the same net-work, a higher resolution usually results in larger
FLOPs and higher accuracy [26]. In contrast, the model with a smaller input resolution has lower performance while the required FLOPs are also smaller. However, the shrink of input resolutions of deep networks provides us an-other potential to alleviate the computation bur-den of CNNs. To have an explict illustration, we ﬁrst test some images under different reso-lutions with a pre-trained ResNet50 as shown in Figure 1 and count the minimum resolution required to give the correct prediction for each sample. In practice, “easy” samples, such as the panda with obvious foreground, can be classiﬁed correctly in both low and high resolution, and
“hard” samples, such as the damselﬂy whose foreground and background are tangled can only be predicted accurately in high resolution. This observation indicates that a larger proportion of images in our datasets can be efﬁciently pro-cessed by reducing their resolutions. On the other hand, it is also compatible with the human perception system [1], i.e., some samples can be understood easily just in blurry mode while the others need to be seen in clear mode.
Figure 1: The prediction results of a well-trained
ResNet-50 model for samples under different res-olutions (112×112, 168×168, 224×224). Some
"easy" samples like the left column (panda), can be classiﬁed correctly using both low and high resolu-tions. However, some "hard" samples like the right column (damselﬂy), where the foreground objects are hidden or blend with the background, can only be classiﬁed correctly using the high resolution.
In this paper, we propose a novel dynamic-resolution network (DRNet) which dynamically adjusts the input resolution of each sample for efﬁcient inference. To accurately ﬁnd the required minimum resolution of each image, we introduce a resolution predictor which is embedded in front of the entire network. In practice, we set several different resolutions as candidates and feed the image into the resolution predictor to produce a probability distribution over candidate resolutions as the output. The network architecture of the resolution predictor is carefully designed with negligible computational complexity and trained jointly with classiﬁer for recognition in an end-to-end fashion.
By exploiting the proposed dynamic resolution network inference approach, we can excavate the reduncancy of each image from its input resolution. Thus, computational costs of easy samples with lower resolutions can be saved, and the accuracy for hard samples can also be preserved by maintaining higher resolutions. Extensive experiments on the large-scale visual benchmarks and the conventional ResNet architectures demonstrate the effectiveness of our proposed method for reducing the overall computational costs with comparable network performance. 2
Figure 2: Overall framework, the resolution predictor guides the resolution selection for the large classiﬁer. ’BN’: batch normalization layer; ’Pri’: probability distribution over categories under resolution ri; In the inference stage, a one-hot vector is predicted by the resolution predictor, in which the ’1’ denotes a corresponding selected resolution. The original image is then resized to the selected resolution and input to the large classiﬁer with chosen BN. 2