Abstract
Communication cost is one major bottleneck for the scalability for distributed learning. One approach to reduce the communication cost is to compress the gradient during communication. However, directly compressing the gradient decel-erates the convergence speed, and the resulting algorithm may diverge for biased compression. Recent work addressed this problem for stochastic gradient descent by adding back the compression error from the previous step. This idea was further extended to one class of variance reduced algorithms, where the variance of the stochastic gradient is reduced by taking a moving average over all history gradients.
However, our analysis shows that just adding the previous step’s compression error, as done in existing work, does not fully compensate the compression error. So, we propose ErrorCompensatedX, which uses the compression error from the previous two steps. We show that ErrorCompensatedX can achieve the same asymptotic convergence rate with the training without compression. Moreover, we provide a uniﬁed theoretical analysis framework for this class of variance reduced algorithms, with or without error compensation. 1

Introduction
Data compression reduces the communication volume and alleviates the communication overhead in distributed learning. E.g., Alistarh et al. (2017) compress the gradient being communicated using quantization and ﬁnd that reducing half of the communication size does not degrade the convergence speed. However, the convergence speed would be slower if we further reduce the communication size, and it requires the compression to be unbiased (Alistarh et al., 2017; Tang et al., 2019). Alternatively, recent work (Stich et al., 2018a) shows that compression with error compensation, which adds back the compression error to the next round of compression, using only 3% of the original communication volume does not degrade the convergence speed, and it works for both biased and unbiased compression operators.
Despite the promising performance of error compensation on stochastic gradient descent (SGD),
SGD admits a slow convergence speed if the stochastic gradient has a large variance. Variance reduction techniques, such as Momentum SGD (Zhang et al., 2015), ROOT-SGD (Li et al., 2020),
STORM (Cutkosky & Mehta, 2020), and IGT (Arnold et al., 2019), are developed, and they admit increased convergence speeds either theoretically or empirically. We found that directly applying 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
error compensation to those variance reduced algorithms is not optimal, and their convergence speeds degrade. So a natural question arises: what is the best compression method for variance reduced algorithms? In this paper, we answer this question and propose ErrorCompensatedX, a general method for error compensation, and show that it admits faster convergence speeds than previous error compensation methods. The contributions of this paper can be summarized as follows:
• We propose a novel error compensation algorithm for variance reduced algorithms and SGD.
Our algorithm admits a faster convergence rate compared to previous methods (Zheng et al., 2019a; Stich et al., 2018b) by fully compensating all error history.
• We provide a general theoretical analysis framework to analyze error compensated algo-rithms. More speciﬁcally, we decompose the convergence rate into the sum of two terms 1
T
T (cid:88) t=1 (cid:107)∇f (xt)(cid:107)2 ≤Runcompressed + R(cid:15), where Runcompressed depends only on the convergence rate of the original algorithm without compression and R(cid:15) is depends only on the magnitude of the compression error (cid:15). It means that we can easily attain the convergence rate for any compressed algorithm in the form of (2). To the best of our knowledge, this is the ﬁrst general result for error compensation. 2