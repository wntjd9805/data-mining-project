Abstract
We consider the problem of maximizing submodular functions in single-pass streaming and secretaries-with-shortlists models, both with random arrival or-der. For cardinality constrained monotone functions, Agrawal, Shadravan, and
Stein [ASS19] gave a single-pass (1 − 1/e − ε)-approximation algorithm using only linear memory, but their exponential dependence on ε makes it impractical even for ε = 0.1. We simplify both the algorithm and the analysis, obtaining an exponential improvement in the ε-dependence (in particular, O(k/ε) memory).
Extending these techniques, we also give a simple (1/e − ε)-approximation for non-monotone functions in O(k/ε) memory. For the monotone case, we also give a corresponding unconditional hardness barrier of 1 − 1/e + ε for single-pass algorithms in randomly ordered streams, even assuming unlimited computation.
Finally, we show that the algorithms are simple to implement and work well on real world datasets. 1

Introduction
Over the past few decades, submodularity has become recognized as a useful property occurring in a wide variety of discrete optimization problems. Submodular functions model the property of diminishing returns, whereby the gain in a utility function decreases as the set of items considered increases. This property occurs naturally in machine learning, information retrieval, and inﬂuence maximization, to name a few (see [IKBA20] and the references within).
In many settings, the data is not available in a random-access model; either for external reasons (customers arriving online) or because of the massive amount of data. When the data becomes too big to store in memory, we look for streaming algorithms that pass (once) through the data, and efﬁciently decide for each element whether to discard it or keep it in a small buffer in memory.
In this work, we consider algorithms that process elements arriving in a random order. Note that the classical greedy algorithm which iteratively adds the best element [NWF78] cannot be used here, and hence we must look for new algorithmic techniques. The motivation for considering random element arrival comes from the prevalence of submodularity in big-data applications, in which data is often logged in batches that can be modelled as random samples from an underlying 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
distribution.1 The problem of streaming submodular maximization has recently received signiﬁcant attention both for random arrival order and the more pessimistic worst-case order arrival [ASS19,
AEF+20, AF19, BMKK14, BV14, CGQ15, FKK18, FNSZ20, HKMY20, IV19, KMZ+19, MV19,
NTM+18, HTW20, Sha20]. 1.1 Submodular functions and the streaming model
Let f : 2E → R+ be a non-negative set function satisfying f (S ∪ {e}) − f (S) ≥ f (T ∪ {e}) − f (T ) for all S ⊆ T ⊆ E \{e}. Such a function is called submodular. For simplicity, we assume f (∅) = 0.2
We use the shorthand f (e|S) := f (S ∪ {e}) − f (S) to denote the marginal of e on top of S. When f (S) ≤ f (T ) for all S ⊆ T , f is called monotone. We consider the following optimization problem:
OP T := max
|S|≤k f (S) where k is a cardinality constraint for the solution size.
Our focus on submodular maximization in the streaming setting. In this setting, an algorithm is given a single pass over a dataset in a streaming fashion, where the stream is a some permutation of the input dataset and each element is seen once. The stream is in random order when the permutation is uniformly random. When there are no constraints on the stream order, we call the stream adversarial.
At each step of the stream, our algorithm is allowed to maintain a buffer of input elements. When a new element is streamed in, the algorithm can choose to add the element to its buffer. To be as general as possible, we assume an oracle model — that is, we assume there is an oracle that returns the value of f (S) for any set S. The decision of the algorithm to add an element is based only on queries to the oracle on subsets of the buffer elements. The algorithm may also choose to throw away buffered elements at any given time. The goal in the streaming model is to minimize memory use, and the complexity of the algorithm is the maximum number of input elements the algorithm stores in the buffer at any given time.
For the oracle model, an important distinction is between weak oracle access or and strong oracle access. In the weak oracle setting, the algorithm is only allowed to query sets of feasible elements (sets that have cardinality less than k). In the strong oracle setting however, the algorithm is allowed to query any set of elements. All our results apply to both the weak and strong oracle models.
Our aim will be to develop algorithms that only make one pass over the data stream, using ˜Oε(k) memory, where k is the maximum size of a solution set in I. We assume that k is small relative to n := |E|, the size of the ground set. 1.2 Our contributions
On the algorithmic side, we give (1−1/e−ε) and (1/e−ε) approximation for cardinality constrained monotone and non-monotone submodular maximization respectively, both using O(k/ε) memory.
The monotone result has an exponential improvement in memory requirements compared to Agrawal et al. [ASS19] (in terms of the dependence on ε), while the non-monotone result is the ﬁrst to appear in the random-order streaming model, and improves upon the best known polynomial-time approximations under adversarial orders [AEF+20]. The algorithms are extremely simple to implement, and perform well on real world data (see Section 5), even compared to ofﬂine greedy algorithms.
On the hardness side, we prove that a (1 − 1/e + ε)-approximation for monotone submodular maximization would require Ω(n) memory (even with unlimited queries and computational power).
This improves the hardness bound of 7/8 from [ASS19]. 1.3