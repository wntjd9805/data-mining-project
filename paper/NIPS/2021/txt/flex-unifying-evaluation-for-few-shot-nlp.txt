Abstract 
Few-shot NLP research is highly active, yet conducted in disjoint research threads  with evaluation suites that lack challenging-yet-realistic testing setups and fail  to employ careful experimental design.  Consequently, the community does not  know which techniques perform best or even if they outperform simple baselines. 
In response, we formulate the FLEX Principles, a set of requirements and best  practices for uniﬁed, rigorous, valid, and cost-sensitive few-shot NLP evaluation. 
These principles include Sample Size Design,  a novel approach to benchmark  design that optimizes statistical accuracy and precision while keeping evaluation  costs manageable.  Following the principles, we release the FLEX benchmark,2  which includes four few-shot transfer settings, zero-shot evaluation, and a public  leaderboard that covers diverse NLP tasks.  In addition, we present UniFew,3  a  prompt-based model for few-shot learning that uniﬁes pretraining and ﬁnetuning  prompt formats, eschewing complex machinery of recent prompt-based approaches  in adapting downstream task formats to language model pretraining objectives. We  demonstrate that despite simplicity, UniFew achieves results competitive with both  popular meta-learning and prompt-based approaches.  1

Introduction 
Few-shot  learning,  the  challenge  of  learning  from  a  small  number  of  examples,  is  critical  for  developing efﬁcient, robust NLP techniques [71, 76]. In recent years, separate threads of few-shot 
NLP research have pursued goals like generalization to new classes [e.g., 5, 25], adaptation to new  domains and tasks [e.g., 3, 4, 21], and direct application of pretrained language models (LMs) [e.g.,  10, 24, 55, 56]. Unfortunately, despite the shared goal of advancing few-shot NLP techniques, the  community does not know which techniques work best or even if they perform better than simple  baselines. Evaluation suites across these research threads are disjoint, lack challenging-yet-realistic  testing setups (e.g., class imbalance, variable training set sizes, etc.), and do not employ careful  experimental design to ensure accurate and precise evaluation estimates and minimal computational  burden. Prior work in few-shot learning outside of NLP serves as a stark warning of the consequences  of improper measurement:  Dhillon et al. [19] showed that techniques from several years of prior  work did not make clear progress due to large overlapping accuracy distributions and, moreover, do  not outperform a simple, carefully-tuned baseline. 
Need for systematic benchmark design As such, a high-quality benchmark is urgently needed to  enable rigorous comparison of techniques across disjoint, highly-active threads of few-shot NLP  research. But what should such an evaluation suite look like? Some best practices for evaluation of  few-shot methods have been introduced in the computer vision (CV) literature [19, 67] and should 
∗Equal contribution  2Benchmark,  leaderboard,  and  benchmark  creation  toolkit:  https://github.com/allenai/flex. 
Apache License 2.0  3Few-shot model: https://github.com/allenai/unifew. Apache License 2.0  35th Conference on Neural Information Processing Systems (NeurIPS 2021).        
Table 1: Comparison of the FLEX benchmark with closest prior work. Our benchmark consists of  episodes with variable number of shots in the range [1-5] and with class imbalance. “No extra test  data” refers to excluding validation data from testing tasks, to avoid unfairly advantaging models  that use such data [50]. Our benchmark’s number of test episodes is selected to balance statistical  accuracy and precision, which suffers in few-episode setups, and compute requirements, which is too  costly in many-episode setups (§5). 
CrossFit[75]  LM-BFF[24]  GPT-3[10]  DS[5]  SMLMT[4]  FewGlue[56]  FLEX (ours) 
Class Transfer 
Domain Transfer 
Task Transfer 
Pretraining Transfer 
Shots per class 
Variable shots 
Unbalanced 
Textual labels 
Zero-shot 
No extra test data 
# test episodes 
Reporting 
# datasets 
--X
-{16, 32} 
--X
--5  avg  160 
---X 16 
--X
X
-5  avg, SD  16 
---X variable 
X 
-X
X
-1  avg  37 
X
----X
X
-{1,5}  {4,8,16,32} 
----X 1000  avg, SD  7 
----X 10  avg, SD  18 
---X
{total 32}4
--X
-mixed5 1  avg, SD  8 
X 
X 
X 
X 
[1–5]
X 
X 
X 
X 
X  90  all6  20  be applied to NLP. However, unifying few-shot NLP work introduces new challenges. For example,  the benchmark needs to test all types of transfer studied in separate research threads to measure  progress on new techniques that make gains in each of these important generalization settings (§2). 
Also, given the importance of zero-shot learning and learning from task descriptions [29, 73], the  benchmark needs to include zero-shot episodes and textual labels to enable measuring progress  for models that do not use conventional supervised training, including methods that leverage the  latent knowledge in pretrained LMs [10, 24, 78]. Further, the benchmark must accommodate new,  computationally-expensive approaches, without overly reducing the number of evaluation episodes at  the expense of statistical accuracy [3, 24, 75]. 
Need for a robust few-shot model Recent prompt-based models [10] have shown strong results in  few-shot learning.  These models leverage the power of (often large) pretrained language models  and adapt the format of downstream tasks to the underlying pretraining objective (e.g., Masked 
Language Modeling). This way, given the right natural language prompt (and sometimes verbalizers 
[55] and additional demonstrative examples), the model can quickly ﬁne-tune on the downstream task 
[24, 43, 44, 55, 56]. However, adapting task formats to the underlying (masked) language modeling  objectives is not straightforward; such models have been shown to be sensitive to varying choices of  the prompt/demonstrations, training settings, hyperparameters, and learning algorithms [33, 50, 78],  often requiring large held out sets and/or complex methods to overcomes such challenges.  Can  models eschew complex prompt engineering by unifying pretraining and downstream task formats? 
In this paper, we tackle these key issues by introducing FLEX—Few-shot Language Evaluation  across (X) many transfer types—and contributing the following: 
•  FLEX Principles (§3), a set of requirements and best practices for few-shot NLP evaluation  that enables uniﬁed, rigorous, valid, and cost-sensitive measurements.  –  Sample Size Design: In support of valid, cost-sensitive measurement, we introduce a  novel approach to few-shot sample size design (§5) that optimizes for a benchmark’s  statistical accuracy and precision while keeping computational costs accessible to a  broad range of researchers. 
•  FLEX benchmark (§4), an implementation of the FLEX Principles.  It tests across four  few-shot transfer settings,7  and includes a public leaderboard for few-shot NLP that covers  20 datasets across diverse NLP tasks (e.g., NLI, relation classiﬁcation, entity typing). Table 1  summarizes key differences between FLEX and other few-shot NLP evaluation suites.  4The total number of training shots in each episode, not number of shots per class per episode.  5Most users use unlabeled examples, though recently, Tam et al. [65] do not.  6Average (avg), conﬁdence interval (CI), standard deviation (SD), individual episode metrics  7Prior work evaluated at most two settings.  2                                                                               
•  UniFew (§6), a prompt-based model for few-shot learning in NLP. While most existing  methods leverage pre-trained LMs for few-shot learning,  LM pre-training tasks do not  closely match natural downstream task formats, requiring complex methods (e.g., extensive  prompt-engineering, use of verbalizers, episodic hyperparameter tuning, custom learning  algorithms) to make these models work in few-shot setting.  Instead, the key idea of our  model, UniFew, is to close the gap between pre-training and ﬁne-tuning formats by posing  tasks as multiple-choice QA and using an underlying model that is pre-trained on a similar  natural QA task format. This eliminates the need for complexities of adapting downstream  tasks to the LM objectives, while resulting in competitive performance with both recent  few-shot and meta-learning methods. 
To aid similar efforts, our release of FLEX includes a toolkit for benchmark creation and few-shot 
NLP model development, which we used to create the FLEX benchmark and train UniFew.  2