Abstract
Multilayer-perceptrons (MLP) are known to struggle with learning functions of high-frequencies, and in particular cases with wide frequency bands. We present a spatially adaptive progressive encoding (SAPE) scheme for input signals of MLP networks, which enables them to better ﬁt a wide range of frequencies without sacriﬁcing training stability or requiring any domain speciﬁc preprocessing. SAPE gradually unmasks signal components with increasing frequencies as a function of time and space. The progressive exposure of frequencies is monitored by a feedback loop throughout the neural optimization process, allowing changes to propagate at different rates among local spatial portions of the signal space. We demonstrate the advantage of SAPE on a variety of domains and applications, including regression of low dimensional signals and images, representation learning of occupancy networks, and a geometric task of mesh transfer between 3D shapes. 1

Introduction
Neural implicit functions have recently emerged as a powerful representation paradigm for modeling complex signals. Their continuous nature sets them apart from typical discrete representations (i.e. pixels, voxels, meshes), allowing to capture high resolution details in various domains such as images [41, 43], 3d shapes [31, 6] and radiance ﬁelds [30, 32], while retaining a reasonably compact representation. In this formulation, a deep neural network is trained with the goal of faithfully mapping input coordinates to a corresponding target domain, effectively learning the representation of signal properties such as magnitude, color, or shape occupancy.
Implementing implicit neural representations with common neural structures, e.g., multilayer perecep-trons with ReLU activations (ReLU MLPs), proves to be challenging in the presence of signals with high frequencies. Consequently, recent works have demonstrated that deep implicit networks beneﬁt from mapping the input coordinates [30, 43], or the intermediate features [41] to positional encodings.
That is, before feeding them into a neural layer, they are ﬁrst transformed to an overparameterized, high dimensional space, typically by multiple periodic functions.
Positional encodings1 have been shown to enable highly detailed mappings of signals by MLP networks. For example, Fourier Feature Networks [43] suggested to map input coordinates of signals to a high dimensional space using sinusoidal functions. In their work, they show that the frequency 1In this paper, we use the term “positional encodings” in lower case letters to denote the family of encoding methods that map coordinates to a higher dimensional space. Not to be confused with the term “Positional
Encoding” coined by [30, 43], which refers to a particular mapping scheme in this family. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
FFN σ = 5
FFN σ = 25
Figure 1: Left: Fourier Features Network (FFN) encoding, tuned to a bandwidth of low frequencies in order to ﬁt the smooth snow areas. The same frequency bandwidth yields blurry buildings in the top image. Middle: FFN tuned to a higher bandwidth to ﬁt to the sharp details of the city. The same bandwidth results in the appearance of noisy artifacts in the mountain image. Right: SAPE is able to
ﬁt both examples without extra tuning, using the same choice of frequency bandwidth in both cases.
SAPE target function no encoding
FFN progressive encoding spatially-adaptive progressive encoding
Figure 2: 1D signal regression. Red: Samples of positional coordinates as network input, and signal magnitude as labels. Black: Predicted implicit signal at inference time. MLPs with “no encoding” struggle to ﬁt high frequency segments (see appendix for train details). Efforts of static positional encoding models (FFN) to ﬁt high frequency areas of the signal introduce noise at the low frequency region. Fitting varying signals with spatial adaptivity allows MLPs to recover all signal frequencies. of these sinusoidal encodings is the dominant factor in obtaining high quality signal reconstructions.
In particular, they present compelling arguments for randomly sampling frequency values from an isotropic Gaussian distribution with a carefully selected scale, providing a striking improvement over mapping coordinates directly via standard MLPs.
Despite the success of positional encodings, there are still some concerns left unaddressed: (i) Choos-ing the right frequency scale requires manual tuning, oftentimes involving a tedious parameter sweep; (ii) The frequency distribution scale may change between different inputs, and accordingly it becomes harder to tune a “one-ﬁts-all” model for signals that are composed of a large range of frequencies (Fig. 1); (iii) Frequencies are selected for the entire input in a global, spatially invariant manner, thus missing an opportunity to better adapt to local high frequencies (Fig. 2).
Our work investigates mitigations to the aforementioned challenges. We study the setting of positional encodings as input to implicit neural networks and present Spatially-Adaptive Progressive Encoding (SAPE). SAPE is a policy for learning implicit functions, relying on two core ideas: (i) guiding the neural optimization process by gradually unmasking signal components with increasing frequencies over time, and (ii) allowing the mask progression to propagate at different rates among local spatial portions of the signal space. To govern this process, we introduce a feedback loop to control the progression of revealed encoding frequencies as a bi-variate function of time and space.
Our work enables MLP networks to adaptively ﬁt a varying spectrum of ﬁne details that previous methods struggle to capture in a single shot, without involved tuning of parameters or domain speciﬁc preprocessing. SAPE excels in learning implicit functions with a large Lipschitz constant, without sacriﬁcing quality of details or optimization stability, in problems that require meticulous conﬁguration to achieve convergence. To highlight the latter, in Section 5.1 we present the tasks of 2D silhouettes deformation and 3D mesh transfer – both require stable optimization from the get-go in order to avoid convergence to sub-optimal local minima.
SAPE is encoding-agnostic: it is not limited to a speciﬁc positional encoding type. It can be easily applied to the learning process of coordinate-based neural implicit functions of various domains including images, 2D shapes, 3D occupancy maps and surfaces, showing improvement in all of them. 2
p
E
ˆy y
Figure 3: Method overview. Coordinates p are fed into an implicit function network, regressing the signal value ˆy at position p as output. An encoder E maps the input coordinates to a high dimensional embedding space. Our encoding layer then masks the encoded features. In this example, samples p are encoded and progressively exposed during the network training, going from low frequency (top node) to high (bottom node). Node colors indicate the various neuron states: on, off, and partially masked. Finally, the loss between output ˆy and target y is fed back to encoder E, which spatially adapts the encoding mask according to the spatial error, indicated by the heatmap on the pufferﬁsh. 2