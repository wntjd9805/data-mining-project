Abstract
Training multiple deep neural networks (DNNs) and averaging their outputs is a simple way to improve the predictive performance. Nevertheless, the multiplied training cost prevents this ensemble method to be practical and efﬁcient. Several recent works attempt to save and ensemble the checkpoints of DNNs, which only requires the same computational cost as training a single network. However, these methods suffer from either marginal accuracy improvements due to the low diversity of checkpoints or high risk of divergence due to the cyclical learning rates they adopted. In this paper, we propose a novel method to ensemble the checkpoints, where a boosting scheme is utilized to accelerate model convergence and maximize the checkpoint diversity. We theoretically prove that it converges by reducing exponential loss. The empirical evaluation also indicates our proposed ensemble outperforms single model and existing ensembles in terms of accuracy and efﬁciency. With the same training budget, our method achieves 4.16% lower error on Cifar-100 and 6.96% on Tiny-ImageNet with ResNet-110 architecture.
Moreover, the adaptive sample weights in our method make it an effective solution to address the imbalanced class distribution. In the experiments, it yields up to 5.02% higher accuracy over single EfﬁcientNet-B0 on the imbalanced datasets. 1

Introduction
DNN ensembles often outperform individual networks in either accuracy or robustness [Hansen and
Salamon, 1990, Zhou et al., 2002]. Particularly, in the cases when memory is restricted, or complex models are difﬁcult to deploy, ensembling light-weight networks is a good alternative to achieve the performance comparable with deep models. However, since training DNNs is computationally expensive, straightforwardly ensembling multiple networks is not acceptable in many real-world scenarios.
A trick to avoid the exorbitant computational cost is to fully utilize the middle stages — or so-called checkpoints — in one training process, instead of training additional networks. Here we refer to this technique as Checkpoint Ensemble (CPE). Despite its merit of no additional training computation, an obvious problem of CPE is that the checkpoints sampled from one training process are often very similar, which violates the consensus that we desire the base models in an ensemble are accurate but sufﬁciently diverse. To enhance the diversity, conventional ensembles often differ the base models from initialization, objective function, or hyperparameters [Breiman, 1996, Freund and Schapire, 1997, Zhou et al., 2002], whilst the recent CPE methods try to achieve this goal by cyclical learning
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
rate scheduling, with the assumption that the high learning rates are able to force the model jumping out of the current local optima and visiting others [Huang et al., 2017a, Garipov et al., 2018, Zhang et al., 2020]. However, this effect is not theoretically promised and the cyclically high learning rates may incur oscillation or even divergence when training budget is limited.
In this paper, we are motivated to efﬁciently exploit the middle stages of neural network training process, obtaining signiﬁcant performance improvement only at the cost of additional storage, meanwhile the convergence is promised. We design a novel boosting strategy to ensemble the checkpoints, named CBNN (Checkpoint-Boosted Neural Network). In contrast to cyclical learning rate scheduling, CBNN encourages checkpoints diversity by adaptively reweighting training samples.
We analyze its advantages in Section 4 from two aspects. Firstly, CBNN is theoretically proved to be able to reduce the exponential loss, which aims to accelerate model convergence even if the network does not have sufﬁciently low error rate. Secondly, reweighting training samples is equivalent to tuning the loss function. Therefore, each reweighting process affects the distribution of local minimum on the loss surface, and thereby changes the model’s optimization direction, which explains the high checkpoint diversity of CBNN.
Moreover, the adaptability of CBNN makes it effective in tackling with imbalanced datasets (datasets with imbalanced class distribution). This is due to CBNN’s ability to automatically allocate high weights to the “minority-class” samples, while most commonly used imbalanced learning methods require manually assigning sample weights [Buda et al., 2018, Johnson and Khoshgoftaar, 2019]. 2