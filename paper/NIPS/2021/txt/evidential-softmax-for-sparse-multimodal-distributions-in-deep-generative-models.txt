Abstract
Many applications of generative models rely on the marginalization of their high-dimensional output probability distributions. Normalization functions that yield sparse probability distributions can make exact marginalization more computation-ally tractable. However, sparse normalization functions usually require alternative loss functions for training since the log-likelihood is undeﬁned for sparse probabil-ity distributions. Furthermore, many sparse normalization functions often collapse the multimodality of distributions. In this work, we present ev-softmax, a sparse normalization function that preserves the multimodality of probability distributions.
We derive its properties, including its gradient in closed-form, and introduce a continuous family of approximations to ev-softmax that have full support and can be trained with probabilistic loss functions such as negative log-likelihood and
Kullback-Leibler divergence. We evaluate our method on a variety of generative models, including variational autoencoders and auto-regressive architectures. Our method outperforms existing dense and sparse normalization techniques in dis-tributional accuracy. We demonstrate that ev-softmax successfully reduces the dimensionality of probability distributions while maintaining multimodality. 1

Introduction
Learning deep generative models over discrete probability spaces has enabled state-of-the-art perfor-mance across tasks in computer vision [1], natural language processing [2]–[4], and robotics [5]–[7].
The probability distributions of deep generative models are often obtained from a neural network using the softmax transformation. Since the softmax function has full support, the logarithm of the output probabilities is well-deﬁned, which is important for common probabilistic loss functions such as the negative log likelihood, cross entropy, and Kullback-Leibler (KL) divergence.
Several applications highlight the importance of achieving sparsity in high dimensional dense distributions to make downstream tasks computationally feasible and perhaps even interpretable [8]–
[11]. In discrete variational autoencoders (VAEs), for instance, calculating the posterior probabilities exactly requires marginalization over all possible latent variable assignments, which is intractable for large latent spaces [9], [10]. Stochastic approaches to circumvent exact marginalization include the score function estimator [12], [13], which suffers from high variance, and continuous relaxations of the discrete latent variable, such as Gumbel-Softmax [14], [15], which introduce bias.
Alternatively, high-dimensional latent spaces can be tractably marginalized using normalization functions that produce sparse probability distributions. Several sparse alternatives to softmax have been proposed, including sparsemax [8], α-entmax [16], and sparsehourglass [17]. However, such approaches often collapse the multimodality of distributions, resulting in unimodal probability distributions [10] (see Fig. 1). When marginalizing over discrete latent spaces, maintaining this multi-modality is crucial for applications such as image generation, where the latent space is multimodal in nature [1], and machine translation, where multiple words can be valid translations and require 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(1, 0, 0)
Input
Softmax
Sparsemax
Ev-Softmax (Ours) (0, 1, 0) (0, 0, 1)
Figure 1: We evaluate the softmax, sparsemax, and ev-softmax (ours) functions on an example input point v ∈ R3 and plot the resulting values on the plane v1 + v2 + v3 = 1. We plot the points v = (0.4, 1.4, −0.8), softmax(v) ≈ (0.25, 0.67, 0.07), sparsemax(v) = (0, 1, 0), and ev-softmax(v) ≈ (0.27, 0.73, 0). Sparsemax yields a unimodal distribution, while ev-softmax yields a bimodal distribution which is close to that of softmax. context to distinguish between the optimal word choice [9]. Furthermore, these approaches either require the construction of alternative loss functions to the negative log-likelihood, which is undeﬁned for zero-valued output probabilities [8], [16]–[18], or they are applied as a post-hoc transformation at test time [10]. For the latter case, the latent distributions obtained through these existing sparse normalization functions and their resulting posteriors are not trained directly.
The post-hoc sparsiﬁcation procedure introduced by Itkina et al. [10] provides a method for obtaining sparse yet multimodal distributions at test time. We generalize this method to a sparse normalization function termed evidential softmax (ev-softmax) which can be applied during both training and test time. We deﬁne continuous approximations of this function that have full support and are thus compatible with typical loss terms, such as negative log-likelihood and KL divergence. We use this full-support approximation of ev-softmax to directly optimize discrete generative models for objective functions with probabilistic interpretations. This approach is in contrast with post-hoc sparsiﬁcation methods that optimize a proxy model [10] and methods that use non-probabilistic objective functions, such as sparsemax [8]. We evaluate these discrete generative models at test time with the ev-softmax function to obtain sparse yet multimodal latent distributions.
Contributions: This paper proposes a strategy for training neural networks with sparse probability distributions that is compatible with negative log-likelihood and KL-divergence computations. We generalize the evidential sparsiﬁcation procedure developed by Itkina et al. [10] to the ev-softmax function. We prove properties of ev-softmax and its continuous approximation, which has full support.
Our approach outperforms existing dense and sparse normalization functions in distributional accuracy across image generation and machine translation tasks. 2