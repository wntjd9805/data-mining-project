Abstract
Stochastic gradient descent (SGD) exhibits strong algorithmic regularization ef-fects in practice, which has been hypothesized to play an important role in the generalization of modern machine learning approaches. In this work, we seek to understand these issues in the simpler setting of linear regression (including both underparameterized and overparameterized regimes), where our goal is to make sharp instance-based comparisons of the implicit regularization afforded by (unregularized) average SGD with the explicit regularization of ridge regres-sion. For a broad class of least squares problem instances (that are natural in high-dimensional settings), we show: (1) for every problem instance and for ev-ery ridge parameter, (unregularized) SGD, when provided with logarithmically more samples than that provided to the ridge algorithm, generalizes no worse than the ridge solution (provided SGD uses a tuned constant stepsize); (2) conversely, there exist instances (in this wide problem class) where optimally-tuned ridge regression requires quadratically more samples than SGD in order to have the same generalization performance. Taken together, our results show that, up to the logarithmic factors, the generalization performance of SGD is always no worse than that of ridge regression in a wide range of overparameterized problems, and, in fact, could be much better for some problem instances. More generally, our results show how algorithmic regularization has important consequences even in simpler (overparameterized) convex settings. 1

Introduction
Deep neural networks often exhibit powerful generalization in numerous machine learning applica-tions, despite being overparameterized. It has been conjectured that the optimization algorithm itself, e.g., stochastic gradient descent (SGD), implicitly regularizes such overparameterized models [29]; here, (unregularized) overparameterized models could admit numerous global and local minima (many of which generalize poorly [29, 21]), yet SGD tends to ﬁnd solutions that generalize well, even in the absence of explicit regularizers [22, 29, 19]. This regularizing effect due to the choice of the optimization algorithm is often referred to as implicit regularization [22].
∗Equal Contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Before moving to the non-convex regime, we may hope to start by understanding this effect in the (overparameterized) convex regime. At least for linear models, there is a growing body of evidence suggesting that the implicit regularization of SGD is closely related to an explicit, (cid:96)2-type of (ridge) regularization [25]. For example, (multi-pass) SGD for linear regression converges to the minimum-norm interpolator, which corresponds to the limit of the ridge solution with a vanishing penalty [29, 14]. Tangential evidence for this also comes from examining gradient descent, where a continuous time (gradient ﬂow) analysis shows how the optimization path of gradient descent is (pointwise) closely connected to an explicit, (cid:96)2-regularization [24, 1]. Similar results [2] have been further extended to SGD, where a (early-stopped) continuous-time SGD is demonstrated to perform similarly to ridge regression with certain regularization parameters.
However, as of yet, a precise comparison between the implicit regularization afforded by SGD and the explicit regularization of ridge regression (in terms of the generalization performance) is still lacking, especially when the hyperparameters (e.g., stepsize for SGD and regularization parameter for ridge regression) are allowed to be tuned. This motivates the central question in this work:
How does the generalization performance of SGD compare with that of ridge regression in least square problems?
In particular, even in the arguably simplest setting of linear regression, we seek to understand if/how SGD behaves differently from using an explicit (cid:96)2-regularizer, with a particular focus on the overparameterized regime.
Our Contributions. Due to recent advances on sharp, instance-dependent excess risks bounds of both (single-pass) SGD and ridge regression for overparameterized least square problems [26, 30], a nearly complete answer to the above question is now possible using these tools. In this work, we deliver an instance-based risk comparison between SGD and ridge regression in several interesting settings, including one-hot distributed data and Gaussian data. In particular, for a broad class of least squares problem instances that are natural in high-dimensional settings, we show that
• For every problem instance and for every ridge parameter, (unregularized) SGD, when provided with logarithmically more samples than that provided to ridge regularization, generalizes no worse than the ridge solution, provided SGD uses a tuned constant stepsize.
• Conversely, there exist instances in our problem class where optimally-tuned ridge regression requires quadratically more samples than SGD to achieve the same generalization performance.
Quite strikingly, the above results show that, up to some logarithmic factors, the generalization performance of SGD is always no worse than that of ridge regression in a wide range of overpa-rameterized least square problems, and, in fact, could be much better for some problem instances.
As a special case (for the above two claims), our problem class includes a setting in which: (i) the signal-to-noise is bounded and (ii) the eigenspectrum decays at a polynomial rate 1/iα, for 0 ≤ α ≤ 1 (which permits a relatively fast decay). This one-sided near-domination phenomenon (in these natural overparameterized problem classes) could further support the preference for the implicit regularization brought by SGD over explicit ridge regularization.
Several novel technical contributions are made to make the above risk comparisons possible. For the one-hot data, we derive similar risk upper bound of SGD and risk lower bound of ridge regression.
For the Gaussian data, while a sharp risk bound of SGD is borrowed from [30], we prove a sharp lower bound of ridge regression by adapting the proof techniques developed in [26, 7]. By carefully comparing these upper and lower bound results (and exhibiting particular instances to show that our sample size inﬂation bounds are sharp), we are able to provide nearly complete conditions that characterize when SGD generalizes better than ridge regression.
Notation. For two functions f (x) ≥ 0 and g(x) ≥ 0 deﬁned on x > 0, we write f (x) (cid:46) g(x) if f (x) ≤ c · g(x) for some absolute constant c > 0; we write f (x) (cid:38) g(x) if g(x) (cid:46) f (x); we write f (x) (cid:104) g(x) if f (x) (cid:46) g(x) (cid:46) f (x). For a vector w ∈ Rd and a positive semideﬁnite matrix
H ∈ Rd×d, we denote (cid:107)w(cid:107)H := w(cid:62)Hw.
√ 2