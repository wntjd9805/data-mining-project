Abstract
Most existing policy learning solutions require the learning agents to receive high-quality supervision signals such as well-designed rewards in reinforcement learning (RL) or high-quality expert demonstrations in behavioral cloning (BC). These quality supervisions are usually infeasible or prohibitively expensive to obtain in practice. We aim for a uniﬁed framework that leverages the available cheap weak supervisions to perform policy learning efﬁciently. To handle this problem, we treat the “weak supervision” as imperfect information coming from a peer agent, and evaluate the learning agent’s policy based on a “correlated agreement” with the peer agent’s policy (instead of simple agreements). Our approach explicitly punishes a policy for overﬁtting to the weak supervision. In addition to theoretical guarantees, extensive evaluations on tasks including RL with noisy rewards, BC with weak demonstrations, and standard policy co-training show that our method leads to substantial performance improvements, especially when the complexity or the noise of the learning environments is high. 1

Introduction
Recent breakthroughs in policy learning (PL) open up the possibility to apply reinforcement learning (RL) or behavioral cloning (BC) in real-world applications such as robotics [1, 2] and self-driving [3, 4]. Most existing works require agents to receive high-quality supervision signals, e.g., reward or expert demonstrations, which are either infeasible or expensive to obtain in practice [5, 6].
The outputs of reward functions in RL are subject to multiple kinds of randomness. For example, the reward collected from sensors on a robot may be biased and have inherent noise due to physical conditions such as temperature and lighting [7, 8, 9]. For the human-deﬁned reward, different human instructors might provide drastically different feedback that leads to biased rewards [10]. Besides, the demonstrations by an expert in behavioral cloning (BC) are often imperfect due to limited resources and environment noise [11, 12, 13]. Therefore, learning from weak supervision signals such as noisy rewards [7] or low-quality demonstrations produced by untrustworthy expert [12, 14] is one of the outstanding challenges that prevents a wider application of PL.
Although some works have explored these topics separately in their speciﬁc domains [7, 15, 14, 16], there lacks a uniﬁed solution for robust policy learning in imperfect situations. Moreover, the noise model as well as the corruption level in supervision signals is often required. To handle these challenges, we ﬁrst formulate a meta-framework to study RL/BC with weak supervision and call it weakly supervised policy learning. Then we propose a theoretically principled solution, PeerPL, to perform efﬁcient policy learning using the available weak supervision without requiring noise rates.
Our solution is inspired by peer loss [17], a recently proposed loss function for learning with noisy labels but does not require the speciﬁcation of noise rates. In peer loss, the noisy labels are treated as a peer agent’s supervision. This loss function explicitly punishes the classiﬁer from simply agreeing with the noisy labels, but would instead reward it for a “correlated agreement" (CA). We adopt a 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
similar idea and treat the “weak supervision” as the noisy information coming from an imperfect peer agent, and evaluate the learning agent’s policy based on a “correlated agreement” (CA) with the weak supervision signals. Compared to standard reward and evaluation functions that encourage simple agreements with the supervision, our approach punishes “over-agreement" to avoid overﬁtting to the weak supervision, which offers us a family of solutions that do not require prior knowledge of the corruption level in supervision signals.
To summarize, the contributions in the paper are: (1) We provide a uniﬁed formulation of the weakly supervised policy learning problems; (2) We propose PeerPL, a new way to perform policy evaluation for RL/BC tasks, and demonstrate how it adapts in challenging tasks including RL with noisy rewards and BC from weak demonstrations; (3) PeerPL is theoretically guaranteed to recover the optimal policy, as if the supervision are of high-quality and clean. (4) Experiment results show strong evidence that PeerPL brings signiﬁcant improvements over state-of-the-art solutions. Code is online available at: https://github.com/wangjksjtu/PeerPL. 1.1