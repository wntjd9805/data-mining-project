Abstract
Few-shot segmentation aims to train a segmentation model that can fast adapt to novel classes with few exemplars. The conventional training paradigm is to learn to make predictions on query images conditioned on the features from support images. Previous methods only utilized the semantic-level prototypes of support images as the conditional information. These methods cannot utilize all pixel-wise support information for the query predictions, which is however critical for the segmentation task. In this paper, we focus on utilizing pixel-wise relationships between support and query images to facilitate the few-shot segmentation task.
We design a novel Cycle-Consistent TRansformer (CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR performs cross-attention between features from different images, i.e. support and query images. We observe that there may exist unexpected irrelevant pixel-level support features. Directly performing cross-attention may aggregate these features from support to query and bias the query features. Thus, we propose using a novel cycle-consistent attention mechanism to filter out possible harmful support features and encourage query features to attend to the most informative pixels from support images. Experiments on all few-shot segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable improvement compared to previous state-of-the-art methods.
Specifically, on Pascal-5i and COCO-20i datasets, we achieve 67.5% and 45.6% mIoU for 5-shot segmentation, outperforming previous state-of-the-art method by 5.6% and 7.1% respectively. 1

Introduction
Recent years have witnessed great progress in semantic segmentation [19, 4, 47]. The success can be largely attributed to large amounts of annotated data [48, 17]. However, labeling dense segmentation masks are very time-consuming [45]. Semi-supervised segmentation [15, 39, 38] has been broadly explored to alleviate this problem, which assumes a large amount of unlabeled data is accessible.
However, semi-supervised approaches may fail to generalize to novel classes with very few exemplars.
In the extreme low data regime, few-shot segmentation [26, 35] is introduced to train a segmentation model that can quickly adapt to novel categories.
∗Part of this work was done when Gengwei Zhang was an intern at Baidu Research.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Different learning frameworks for few-shot segmentation, from the perspective of ways to utilize support information. (a) Class-wise mean pooling based method. (b) Clustering based method. (c) Foreground pixel attention method. (d) Our Cycle-Consistent TRansformer (CyCTR) framework that enables all beneficial support pixel-level features (foreground and background) to be considered.
Most few-shot segmentation methods follow a learning-to-learn paradigm where predictions of query images are made conditioned on the features and annotations of support images. The key to the success of this training paradigm lies in how to effectively utilize the information provided by support images. Previous approaches extract semantic-level prototypes from support features and follow a metric learning [29, 7, 35] pipeline extending from PrototypicalNet [28]. According to the granularity of utilizing support features, these methods can be categorized into two groups, as illustrated in
Figure 1: 1) Class-wise mean pooling [35, 46, 44] (Figure 1(a)). Support features within regions of different categories are averaged to serve as prototypes to facilitate the classification of query pixels. 2) Clustering [18, 41] (Figure 1(b)). Recent works attempt to generate multiple prototypes via EM algorithm or K-means clustering [41, 18], in order to extract more abundant information from support images. These prototype-based methods need to “compress" support information into different prototypes (i.e. class-wise or cluster-wise), which may lead to various degrees of loss of beneficial support information and thus harm segmentation on query image. Rather than using prototypes to abstract the support information, [43, 34] (Figure 1(c)) propose to employ the attention mechanism to extract information from support foreground pixels for segmenting query. However, such methods ignore all the background support pixels that can be beneficial for segmenting query image, and incorrectly consider partial foreground support pixels that are quite different from the query ones, leading to sub-optimal results.
In this paper, we focus on equipping each query pixel with relevant information from support im-ages to facilitate the query pixel classification.
Inspired by the transformer architecture [32] which performs feature aggregation through at-tention, we design a novel Cycle-Consistent
Transformer (CyCTR) module (Figure 1(d)) to aggregate pixel-wise support features into query ones. Specifically, our CyCTR consists of two types of transformer blocks: the self-alignment block and the cross-alignment block. The self-alignment block is employed to encode the query image features by aggregating its relevant con-text information, while the cross-alignment aims to aggregate the pixel-wise features of support images into the pixel-wise features of query image. Different from self-alignment where
Query3, Key and Value come from the same embedding, cross-alignment takes features from query images as Query, and those from support images as Key and Value. In this way, CyCTR provides abundant pixel-wise support information for pixel-wise features of query images to make predictions.
Figure 2: The motivation of our proposed method.
Many pixel-level support features are quite differ-ent from the query ones, and thus may confuse the attention. We incorporate cycle-consistency into attention to filter such confusing support features.
Note that the confusing support features may come from foreground and background.
Moreover, we observe that due to the differences between support and query images, e.g., scale, color and scene, only a small proportion of support pixels can be beneficial for the segmentation of query image. In other words, in the support image, some pixel-level information may confuse the attention in the transformer. Figure 2 provides a visual example of a support-query pair together with the label 3To distinguish from the phrase "query" in few-shot segmentation, we use "Query" with capitalization to note the query sequence in the transformer. 2
masks. The confusing support pixels may come from both foreground pixels and background pixels.
For instance, point p1 in the support image located in the plane afar, which is indicated as foreground by the support mask. However, the nearest point p2 in the query image (i.e. p2 has the largest feature similarity with p1) belongs to a different category, i.e. background. That means, there exists no query pixel which has both high similarity and the same semantic label with p1. Thus, p1 is likely to be harmful for segmenting "plane" and should be ignored when performing the attention. To overcome this issue, in CyCTR, we propose to equip the cross-alignment block with a novel cycle-consistent attention operation. Specifically, as shown in Figure 2, starting from the feature of one support pixel, we find its nearest neighbor in the query features. In turn, this nearest neighbor finds the most similar support feature. If the starting and the end support features come from the same category, a cycle-consistency relationship is established. We incorporate such an operation into attention to force query features only attend to cycle-consistent support features to extract information. In this way, the support pixels that are far away from query ones are not considered. Meanwhile, cycle-consistent attention enables us to more safely utilize the information from background support pixels, without introducing much bias into the query features.
In a nutshell, our contributions are summarized as follows: (1) We tackle few-shot segmentation from the perspective of providing each query pixel with relevant information from support images through pixel-wise alignment. (2) We propose a novel Cycle-Consistent TRansformer (CyCTR) to aggregate the pixel-wise support features into the query ones. In CyCTR, we observe that many support features may confuse the attention and bias pixel-level feature aggregation, and propose incorporating cycle-consistent operation into the attention to deal with this issue. (3) Our CyCTR achieves state-of-the-art results on two few-shot segmentation benchmarks, i.e., Pascal-5i and COCO-20i. Extensive experiments validate the effectiveness of each component in our CyCTR. 2