Abstract
Our work reveals a structured shortcoming of the existing mainstream self-supervised learning methods. Whereas self-supervised learning frameworks usually take the prevailing perfect instance level invariance hypothesis for granted, we carefully investigate the pitfalls behind. Particularly, we argue that the existing augmentation pipeline for generating multiple positive views naturally introduces out-of-distribution (OOD) samples that undermine the learning of the downstream tasks. Generating diverse positive augmentations on the input does not always pay off in beneﬁting downstream tasks. To overcome this inherent deﬁciency, we intro-duce a lightweight latent variable model UOTA, targeting the view sampling issue for self-supervised learning. UOTA adaptively searches for the most important sampling region to produce views, and provides viable choice for outlier-robust self-supervised learning approaches. Our method directly generalizes to many mainstream self-supervised learning approaches, regardless of the loss’s nature contrastive or not. We empirically show UOTA’s advantage over the state-of-the-art self-supervised paradigms with evident margin, which well justiﬁes the existence of the OOD sample issue embedded in the existing approaches. Especially, we theoretically prove that the merits of the proposal boil down to guaranteed estimator variance and bias reduction. Code is available: https://github.com/ssl-codelab/uota.

Introduction 1
Self-supervised learning is an increasingly appealing direction in learning effective deep repre-sentations. In the regime of computer vision, natural language processing and machine learning tasks, multiple milestones under such self-supervised learning frameworks have been established
[4, 5, 14, 12, 18, 32, 31, 33, 39, 43, 47, 48, 49]. The premise here is that self-supervised learning methods usually generate multiple views and assume one view be predictive of another.
So far, the most prevailing assumption is to force views from the same instance invariant in the feature space [7, 18, 33, 47]. While it is certainly natural to consider generating diverse augmentations to spread the feature distribution and force consistency in between views, we reveal a structured shortcoming of such popular augmentation pipeline: excessive distortions applied on original image would produce samples that deviate drastically in the semantics. We ﬁnd these produced views have semantically deviated from the original instance and thus behave like out of distribution (OOD) samples. The model therefore fail to generalize in some certain parameter region due to the interference of the OOD samples.
We support our claim by empirically verifying the non-negligible detrimental impact of the OOD noise on downstream task performances. Particularly, we conduct extensive experiments and show that importance sampling technique and the associated analysis can help effectively differentiate the noises. The proposed new model suppresses the effect of noisy samples and efﬁciently improve the downstream task performance over a broad spectrum of state-of-the-art self-supervised learning 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
approaches. This indicates the evident failure of baseline model in some parameter space owing to the OOD noise. Our contribution in this paper are summarized as follows: 1. We present the ﬁrst formal analysis of OOD issue under the currently prevailing self-supervised learning framework. 2.
We propose a lightweight latent variable model UOTA that is able to differentiate noise from original instance’s semantics. The new model does not introduce extra computational complexity whereas it can efﬁciently suppress the inﬂuence of OOD samples. In contrast to existing SSL paradigms, the proposal method is able to automatically balance the bias-variance trade-off in the mean squared error (MSE) of the deep estimator: We do desire large data variance through strong augmentations, on the other hand, we should be careful not to introduce extra estimator bias through extremely large data distortions. We argue that in the context of self-supervised learning, the hidden OOD samples and such bias-variance trade-off have a clear and non-negligible impact that should be attended to. 2