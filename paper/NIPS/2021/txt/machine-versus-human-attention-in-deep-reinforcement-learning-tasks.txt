Abstract
Deep reinforcement learning (RL) algorithms are powerful tools for solving visuo-motor decision tasks. However, the trained models are often difﬁcult to interpret, because they are represented as end-to-end deep neural networks. In this paper, we shed light on the inner workings of such trained models by analyzing the pixels that they attend to during task execution, and comparing them with the pixels attended to by humans executing the same tasks. To this end, we investigate the following two questions that, to the best of our knowledge, have not been previously studied. 1) How similar are the visual representations learned by RL agents and humans when performing the same task? and, 2) How do similarities and differences in these learned representations explain RL agents’ performance on these tasks?
Speciﬁcally, we compare the saliency maps of RL agents against visual attention models of human experts when learning to play Atari games. Further, we analyze how hyperparameters of the deep RL algorithm affect the learned representations and saliency maps of the trained agents. The insights provided have the potential to inform novel algorithms for closing the performance gap between human experts and RL agents. 1

Introduction
Researchers have devoted much effort to understanding deep neural networks (DNNs). Since DNNs are partially inspired by the biological nervous systems, researchers have compared these neural networks with the human brain and sensory systems by asking two typical types of questions. The
ﬁrst one considers representation: How similar are the visual representations learned by DNNs and humans when performing the same tasks? The second one concerns explainability: How do similarities and differences in the learned representations explain DNNs’ performance on their tasks?
The ﬁrst question motivates a seminal line of research that compares representations learned by DNNs with those learned by humans. In computer vision, a linear model that uses the feature map activations generated by a trained CNN can accurately predict neural activities in the early visual cortex, indicating that the two systems have learned similar visual representations [19, 78, 79]. In language learning, a similar connection is found between deep language models and cortical areas [31, 34]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, this type of comparison has just emerged in decision-making research [44, 13], which motivates us to compare DNNs trained for deep reinforcement learning (RL) tasks with human decision-making. We ask: Do deep RL agents and humans agree on what visual features are important? In other words, do agents pay attention to the same visual features as humans do?
The explainability question is motivated by the Explainable AI (XAI) for deep RL agents [29, 2, 55].
Deep RL has achieved many successes, but few of us fully understand these agents. These agents often learn a mapping from raw images to actions end-to-end where it is not clear why a particular decision is made. Our work addresses the explainability of these black-box models: Do RL agents make mistakes because they fail to attend to important visual features that matter for making the correct decision? An expert human’s attention data could serve as a useful reference for identifying important visual features, which has been validated in object recognition tasks [50].
To answer these two questions, we situate our research in the domain of Atari games [6]. These games span a variety of dynamics, visual features, reward mechanisms, and difﬁculty levels for both humans and AIs. Two recent lines of research have made our work possible. On the one hand, a human attention dataset on Atari games has been collected using eye trackers [85] and convolution-deconvolution networks have been shown to accurately predict human attention distributions [83].
On the other hand, tools have been introduced to generate visual interpretations of RL agents [23] so one can visualize an RL agent’s saliency map given an image, which is a topographically arranged map that assigns an importance weight to each image pixel and is often treated as the “attention map" of the RL agent. However, these tools can only provide qualitative results – we still need to manually inspect the saliency maps to interpret the performance. A useful comparison standard like human attention had not been considered hence systematic quantiﬁcation was impossible.
In this work, we present a novel study that compares the RL agent’s attention with both real and estimated human attention. We analyze how learning and hyperparameters of the RL algorithm affect the learned representations and saliency maps. We analyze failure and unseen states for RL agents and identify potential challenges to achieve human-level performance. We conclude by discussing insights gained for RL researchers in both cognitive science and AI. 2