Abstract
Most prior approaches to ofﬂine reinforcement learning (RL) have taken an iterative actor-critic approach involving off-policy evaluation. In this paper we show that simply doing one step of constrained/regularized policy improvement using an on-policy Q estimate of the behavior policy performs surprisingly well. This one-step algorithm beats the previously reported results of iterative algorithms on a large portion of the D4RL benchmark. The one-step baseline achieves this strong performance while being notably simpler and more robust to hyperparameters than previously proposed iterative algorithms. We argue that the relatively poor performance of iterative approaches is a result of the high variance inherent in doing off-policy evaluation and magniﬁed by the repeated optimization of policies against those estimates. In addition, we hypothesize that the strong performance of the one-step algorithm is due to a combination of favorable structure in the environment and behavior policy. 1

Introduction
An important step towards effective real-world RL is to improve sample efﬁciency. One avenue towards this goal is ofﬂine RL (also known as batch RL) where we attempt to learn a new policy from data collected by some other behavior policy without interacting with the environment. Recent work in ofﬂine RL is well summarized by Levine et al. [2020].
In this paper, we challenge the dominant paradigm in the deep ofﬂine RL literature that primarily relies on actor-critic style algorithms that alternate between policy evaluation and policy improvement
[Fujimoto et al., 2018a, 2019, Peng et al., 2019, Kumar et al., 2019, 2020, Wang et al., 2020b, Wu et al., 2019, Kostrikov et al., 2021, Jaques et al., 2019, Siegel et al., 2020, Nachum et al., 2019].
All these algorithms rely heavily on off-policy evaluation to learn the critic. Instead, we ﬁnd that a simple baseline which only performs one step of policy improvement using the behavior Q function often outperforms the more complicated iterative algorithms. Explicitly, we ﬁnd that our one-step algorithm beats prior results of iterative algorithms on most of the gym-mujoco [Brockman et al., 2016] and Adroit [Rajeswaran et al., 2017] tasks in the the D4RL benchmark suite [Fu et al., 2020].
We then dive deeper to understand why such a simple baseline is effective. First, we examine what goes wrong for the iterative algorithms. When these algorithms struggle, it is often due to poor off-policy evaluation leading to inaccurate Q values. We attribute this to two causes: (1) distribution shift between the behavior policy and the policy to be evaluated, and (2) iterative error exploitation whereby policy optimization introduces bias and dynamic programming propagates this bias across the state space. We show that empirically both issues exist in the benchmark tasks and that one way to avoid these issues is to simply avoid off-policy evaluation entirely.
Finally, we recognize that while the the one-step algorithm is a strong baseline, it is not always the best choice. In the ﬁnal section we provide some guidance about when iterative algorithms can perform better than the simple one-step baseline. Namely, when the dataset is large and behavior policy has good coverage of the state-action space, then off-policy evaluation can succeed and iterative 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A cartoon illustration of the difference between one-step and multi-step methods. All algorithms constrain themselves to a neighborhood of “safe” policies around β. A one-step approach (left) only uses the on-policy (cid:98)Qβ, while a multi-step approach (right) repeatedly uses off-policy (cid:98)Qπi. algorithms can be effective. In contrast, if the behavior policy is already fairly good, but as a result does not have full coverage, then one-step algorithms are often preferable.
Our main contributions are:
• A demonstration that a simple baseline of one step of policy improvement outperforms more complicated iterative algorithms on a broad set of ofﬂine RL problems.
• An examination of failure modes of off-policy evaluation in iterative ofﬂine RL algorithms.
• A description of when one-step algorithms are likely to outperform iterative approaches. 2 Setting and notation
We will consider an ofﬂine RL setup as follows. Let M = {S, A, ρ, P, R, γ} be a discounted inﬁnite-horizon MDP. In this work we focus on applications in continuous control, so we will generally assume that both S and A are continuous and bounded. We consider the ofﬂine setting where rather than inter-acting with M, we only have access to a dataset DN of N tuples of (si, ai, ri) collected by some be-havior policy β with initial state distribution ρ. Let r(s, a) = Er|s,a[r] be the expected reward. Deﬁne the state-action value function for any policy π by Qπ(s, a) := EP,π|s0=s, a0=a[(cid:80)∞ t=0 γtr(st, at)].
The objective is to maximize the expected return J of the learned policy:
J(π) := E
ρ,P,π (cid:35)
γtr(st, at) (cid:34) ∞ (cid:88) t=0
[Qπ(s, a)].
= E s∼ρ a∼π|s (1)
Following Fu et al. [2020] and others in this line of work, we allow access to the environment to tune a small (< 10) set of hyperparameters. See Paine et al. [2020] for a discussion of the active area of research on hyperparameter tuning for ofﬂine RL. We also discuss this further in Appendix C. 3