Abstract
Dental forensic identiﬁcation targets to identify persons with dental traces. The task is vital for the investigation of criminal scenes and mass disasters because of the resistance of dental structures and the wide-existence of dental imaging.
However, no widely accepted automated solution is available for this labour-costly task. In this work, we pioneer to study deep learning for dental forensic identiﬁca-tion based on panoramic radiographs. We construct a comprehensive benchmark with various dental variations that can adequately reﬂect the difﬁculties of the task.
By considering the task’s unique challenges, we propose FoID, a deep learning method featured by: (i) clinical-inspired attention localization, (ii) domain-speciﬁc augmentations that enable instance discriminative learning, and (iii) transformer-based self-attention mechanism that dynamically reasons the relative importance of attentions. We show that FoID can outperform traditional approaches by at least 22.98% in terms of Rank-1 accuracy, and outperform strong CNN baselines by at least 10.50% in terms of mean Average Precision (mAP). Moreover, ex-tensive ablation studies verify the effectiveness of each building blocks of FoID.
Our work can be a ﬁrst step towards the automated system for forensic identi-ﬁcation among large-scale multi-site databases. Also, the proposed techniques, e.g., self-attention mechanism, can also be meaningful for other identiﬁcation tasks, e.g., pedestrian re-identiﬁcation. Related data and codes can be found at https://github.com/liangyuandg/FoID. 1

Introduction
Forensic identiﬁcation targets to identify living or deceased persons by analyzing their trace evidences.
The identiﬁcation with dental data is particularly vital for the investigation of criminal scenes, accidents and mass disasters for at least two reasons [30, 27]: (i) dental patterns can be highly identifying while their traces are widely archived than other methodologies e.g., DNA proﬁling
[4, 45, 59]. For instance, around 1.4 billion dental X-rays were performed in 2019 in the U.S.2; (ii) dental structures are more resistant to damages than other body tissues including bones [7, 46].
In the current practice, forensic dental examinations are mostly done by the visual comparison of radiographs [46, 45] between a target entity (person) and those from databases. Due to the lack of widely accepted automated solutions, such examination cannot scale up to large databases, and its results are vulnerable to oversights and/or mistakes [19, 39]. This work pioneers to study deep learning for forensic identiﬁcation with dental panoramic radiographs — one of the most common dental traces nowadays [59].
∗Corresponding authors 2https://idataresearch.com/product/dental-imaging-market/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A challenging case of dental identiﬁcation with two dental radiographs captured from a same person. The case exists large anatomical variations including tooth reduction (d), the addition of braces and implants (e1, e2), and the change of teeth alignment (f1, f2). In the forensic practice, teeth, condyle (a), angle of mandible (b), and maxillary sinus (c) are the key anatomies for dentists.
There have been a few attempts for forensic dental identiﬁcation, most of which take the approaches of matching handcrafted feature [1, 78, 9]. According to our best knowledge, only one work utilizes off-the-shelf convolutional neural networks (CNNs) for learning discriminative representations of a dental radiograph for identiﬁcation [39]. However, the work is preliminary since it is based on a small-scale dataset with both temporal and heterogeneous limitations — it is different from the applied situation where signiﬁcant intra-entity variations in dental structures can exist because of external factors, e.g., interventions, and internal factors, e.g., decaying, over long scanning intervals
[46, 1]. Moreover, no architecture exploration was made for this unique identiﬁcation task.
We performed a formative study with two board-certiﬁed dentists, and outlined the distinct challenges of forensic dental identiﬁcation: (C1) the lack of paired dental radiographs for metric learning; (C2) sparseness of identiﬁcation information in high-resolution radiographs; and (C3) dental variations can be heterogeneous but useful as inclusion or exclusion criteria for identifying. To study the task, we construct a comprehensive benchmark: it involves 583 persons, and comes with a challenging testing set spanning over 21.0±11.5 months of scanning intervals, covering a wide range of dental variations including orthodontics, tooth loss, and implanting/ﬁlling. Moreover, we propose the
ﬁrst optimized deep learning Forensic IDentiﬁcation solution, named FoID. In speciﬁc, to take the advantage of overwhelmingly unpaired radiographs (C1), we develop a set of domain-speciﬁc augmentations (DSA) for effective self-supervised metric learning. To focus on meaningful areas,
FoID incorporates hard attention extraction and alignment guided by semantic segmentation and anatomical registration (C2). Moreover, FoID introduces a novel transformer-based self-attention mechanism, as inspired by dentist’s practice, to dynamically reason the optimal aggregation of partial attentions for representation (C3). Overall, the main contributions of our work are three folds:
• We give the ﬁrst in-depth study of deep learning for dental forensic identiﬁcation. The formative study with dentists outlines the unique challenges and offers insights into solutions.
• We construct a comprehensive dental forensic benchmark, which covers a wide range of dental variations to reﬂect the realistic scenario of the task.
• We present the ﬁrst deep learning solution FoID, which achieves an mAP of 59.62%, and largely outperforms the existing dental identiﬁcation models and strong CNN baselines.
Ablation studies show that the proposed DSA enables effective representation learning from unpaired data, boosting models by up to 12.33% in terms of mAP; Meanwhile, the proposed transformer-based self-attention outperforms state-of-the-art attention models by at least 4.82% in terms of mAP. 2 Formative Study
Challenge 1: paired scan instances are scarce With the high radiation of dental imaging, over-whelming cases from dental clinics contain only one scan instance per person. As such, it poses as a challenge for metric learning without positive instance pairs. To tackle it, we develop a set of domain-speciﬁc augmentations (DSA) by working with dentists to simulate potential dental 2
anatomical variations. Different views of one entity are generated on the ﬂy, which enables effective intra-/inter-reasoning in a self-supervised manner.
Challenge 2: identiﬁcation information is sparse Forensic dentists mostly focus on several key anatomies on panoramic imaging for identiﬁcation, since others are either non-discriminative or not well approximated on radiographs. As such, models should focus on such regions for image embedding to avoid over-ﬁtting. Accordingly, as shown in Figure 1, our method such incorporate domain knowledge by utilizing a hard attention mechanism on teeth, condyle (Figure 1(a)), angle of mandible (Figure 1(b)), and maxillary sinus (Figure 1(c)). A combination of semantic segmentation and registration is applied for efﬁcient key-point localization.
Challenge 3: dental variations are heterogeneous but can be useful There can exist both local and global anatomical changes over two scanning. As demonstrated by a typical case in Figure 1, two scans of a same entity can see anatomy loss (Figure 1(d)), artifact implanting (Figure 1(e1, e2)), structural changes (Figure 1(f1, f2)). In the forensic practice, dentists commonly look through all key anatomies in a scan, determine most discriminative features among them according to experience, and then apply those as indexes for matching. Our method follows the philosophy by introducing a transformer-based self-attention module, which dynamically reasons the relative importance of attentions among a set of them, for the effective representation learning of a radiograph. 3