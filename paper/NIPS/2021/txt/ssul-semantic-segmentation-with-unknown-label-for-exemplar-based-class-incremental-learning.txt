Abstract
This paper introduces a solid state-of-the-art baseline for a class-incremental seman-tic segmentation (CISS) problem. While the recent CISS algorithms utilize variants of the knowledge distillation (KD) technique to tackle the problem, they failed to fully address the critical challenges in CISS causing the catastrophic forgetting; the semantic drift of the background class and the multi-label prediction issue. To better address these challenges, we propose a new method, dubbed SSUL-M (Se-mantic Segmentation with Unknown Label with Memory), by carefully combining techniques tailored for semantic segmentation. Speciﬁcally, we claim three main contributions. (1) deﬁning unknown classes within the background class to help to learn future classes (help plasticity), (2) freezing backbone network and past classi-ﬁers with binary cross-entropy loss and pseudo-labeling to overcome catastrophic forgetting (help stability), and (3) utilizing tiny exemplar memory for the ﬁrst time in CISS to improve both plasticity and stability. The extensively conducted experiments show the effectiveness of our method, achieving signiﬁcantly better performance than the recent state-of-the-art baselines on the standard benchmark datasets. Furthermore, we justify our contributions with thorough ablation analyses and discuss different natures of the CISS problem compared to the traditional class-incremental learning targeting classiﬁcation. The ofﬁcial code is available at https://github.com/clovaai/SSUL. 1

Introduction
Class incremental learning (CIL) problem, in which a learner should incrementally learn newly arriving class objects while not “catastrophically” forgetting the past learned classes, is one of the fundamental, yet still open, problems in machine learning. After the seminal work, [27], most of the recent neural network-based CIL research has focused on the classiﬁcation setting, and various approaches have been proposed to address the main challenge of the problem, the so-called plasticity-stability dilemma, e.g., [20, 30, 12, 1, 2, 8], to name a few.
The CIL framework has been extended to more complex semantic segmentation tasks [24, 3, 7], motivated by the practical need in various applications such as autonomous driving. One of the key additional difﬁculties of the class-incremental semantic segmentation (CISS) problem lies in the semantic drift of the background class present in the incrementally arriving training data. Namely, the label ‘background (BG)” is assigned to all the pixels not included in the current class object region.
The “BG” pixels belong to three categories: future object classes that the model does not yet observes, past object classes that are already learned, and the true background.
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A few recent works attempted to address the above semantic drift issue by leveraging and modifying the knowledge-distillation (KD) [20] technique, popular for CIL in standard image classiﬁcation.
Namely, the initiative study [24] modiﬁed the KD for CISS straightforwardly, and [3] proposed a strategy to incorporate the BG class probability in computing the cross-entropy and distillation losses.
Furthermore, [7], the current state-of-the-art, utilized the pseudo-labeling of the BG pixels of the current task data with the model of the previous task using the cross-entropy loss, and it applies the multi-scale feature distillation scheme adopted from [8]. However, we argue that above works only partially addressed the BG label issue. That is, [3] naively added the class probabilities to modify the cross-entropy and distillation losses making it hard to have ﬁne-grained learning of prediction probability for each class, each pixel. Moreover, [7] could only handle the BG class pixels to the past classes via pseudo-labeling and lacked any mechanism for handling the future class case, one possible option for the the BG class. Consequently, their CIL segmentation performance, measured by mean Intersection-over-Union (mIoU), has been signiﬁcantly lower than the upper-bound, the case of joint-training with all the labels.
This paper ﬁrst identiﬁes that the multi-label prediction of semantic segmentation is another critical challenge of CISS and proposes SSUL-M (Semantic Segmentation with Unknown Label with
Memory) to address the challenge. Speciﬁcally, Our contributions are summarized as follows. First, we introduce an additional “Unknown” class label assigned to the objects in the background, detected by the off-the-shelf saliency-map detector. We let the base feature extractor distinguish the representations of the potential future class objects and the actual background region by augmenting the BG label with this additional class. Second, we adopt the pseudo-labeling strategy as in [7] and further augment the BG & Unknown class labels with the past class labels, but with two essential differences in concrete learning strategies to stabilize the classiﬁcation scores and improve the precision of the prediction. One is using the separate sigmoid, instead of the softmax, output classiﬁer for each class so that the model can learn the logit score in an absolute sense per class. The other is freezing the base feature extractor and the classiﬁers for past classes after initial learning to strictly maintain the past classes’ knowledge. Third, we utilize an exemplar memory to store a tiny portion of training data, including past classes, as anchors and further improve the mIoU. Note that using the exemplar memory is a standard practice for CIL in classiﬁcation but has been overlooked in CISS. Moreover, we show that the memory helps improve the mIoU for the current classes, in contrast to a common belief that it is a tool to prevent forgetting of past classes.
By integrating the above contributions, SSUL-M achieved the state-of-the-art performance on popular benchmarks with a signiﬁcantly large margin over the recent baselines [3, 7], particularly when the number of incremental tasks gets larger. Furthermore, we conduct extensive ablation studies and both quantitative and qualitative analyses to convincingly highlight the strength of our method. 2