Abstract
Shapley values provide model agnostic feature attributions for model outcome at a particular instance by simulating feature absence under a global population distri-bution. The use of a global population can lead to potentially misleading results when local model behaviour is of interest. Hence we consider the formulation of neighbourhood reference distributions that improve the local interpretability of Shapley values. By doing so, we ﬁnd that the Nadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as a self-normalised importance sampling estimator. Empirically, we observe that Neighbourhood Shapley values identify meaningful sparse feature relevance attributions that provide insight into local model behaviour, complimenting conventional Shapley analysis. They also in-crease on-manifold explainability and robustness to the construction of adversarial classiﬁers. 1

Introduction
The ability to correctly interpret a prediction model is increasingly important as we move to widespread adoption of machine learning methods, in particular within safety critical domains such as health care [22, 17]. In this paper, we consider the task of attributing the features {1, . . . , m} of a complex machine learning model f : Rm → Rl, abstracted as a function that predicts a response given a test instance x ∈ Rm, given only black-box access to the model. We especially focus on two popular model-agnostic feature removal based local explanation models, namely LIME [32] and
SHAP [26]. However our ﬁndings are applicable to other local explanation models that we do not consider in this paper. As these methods are often described as ﬁtting a local surrogate model to the black box [34], a natural question is: how ‘local’ are local explanation methods? 2 − I(x1 ≤ 0)x2
As a simple motivating example as to why this question matters, consider a black box model given by f (x) = I(x1 > 0)2x2 2 where I(·) denotes the indicator function. When attributing the local feature importance at a test instance x = (x1, 2), with x2 ﬁxed at 2, we would expect Feature-1 to receive a higher absolute attribution when x is closer to the decision boundary at x1 = 0. In
Figure 1 we report the results on this example from LIME and SHAP as well as for our proposed
‘Neighbourhood SHAP’ approach. We observe that Neighbourhood SHAP assigns Feature-1 a smaller attribution, the higher the absolute value of x1 is. SHAP and LIME, however, assign Feature-1 an attribution which is constant either side of x1 = 0 which illustrates that these methods capture global
∗equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Attributions at x = (x1, 2) with x1 varying for a reference distribution of X ∼ Normal(0, 1) and black box f (x) = I(x1 > 0)2x2 2 − I(x1 ≤ 0)x2 2. model behaviour. The ﬁgure also shows that training a local linear approximation to the black box
[30, 8] is misleading since Feature-2 receives a signiﬁcantly positive attribution for x1 ∈ [−0.4, 0], even though Feature-2 contributes clearly negatively to the model outcome whenever x1 < 0.
This motivates the following contributions 1. We propose Neighbourhood SHAP (Section 3) which considers local reference populations for prediction points as a complimentary approach to SHAP. By doing so, we show that the
Nadaraya-Watson estimator at x can be interpreted as an importance sampling estimator where the expectation is taken over the proposed neighbourhood. Empirically, we ﬁnd that greater locality increases the number of model evaluations on the data manifold and with this the robustness of the attributions against adversarial attacks. 2. We consider how smoothing can also be used to stabilise SHAP values (Section 4). We quantify the loss in information incurred by our smoothing procedure and characterise its
Lipschitz continuity. 2