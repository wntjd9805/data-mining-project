Abstract
In Vision-and-Language Navigation (VLN) task, an agent is asked to navigate inside 3D indoor environments following given instructions. Cross-modal align-ment is one of the most critical challenges in VLN because the predicted trajec-tory needs to match the given instruction accurately. In this paper, we address the cross-modal alignment challenge from the perspective of ﬁne-grain. Firstly, to alleviate weak cross-modal alignment supervision from coarse-grained data, we introduce a human-annotated ﬁne-grained VLN dataset, namely Landmark-RxR.
Secondly, to further enhance local cross-modal alignment under ﬁne-grained su-pervision, we investigate the focal-oriented rewards with soft and hard forms, by focusing on the critical points sampled from ﬁne-grained Landmark-RxR. More-over, to fully evaluate the navigation process, we also propose a re-initialization mechanism that makes metrics insensitive to difﬁcult points, which can cause the agent to deviate from the correct trajectories. Experimental results show that our agent has superior navigation performance on Landmark-RxR, en-RxR and R2R.
Our dataset and code are available at https://github.com/hekj/Landmark-RxR. 1

Introduction
Vision-and-language navigation (VLN) is an important task about cross-modal intelligence and can be applied to service and rescue robots. Different from other cross-modal tasks like image/video captioning [1, 2] and visual question answering [3], where the agent only needs to understand ﬁxed images or videos, VLN agent has to learn and reason by dynamically interacting with the real envi-ronment guided by human instructions. Since Anderson et al. [4] ﬁrst introduced the VLN task with the coarse-grained dataset Room-to-Room (R2R), great progresses in this direction have been made, ranging from the sequence-to-sequence models [4] combined with cross-modal grounding modules
[5–8] to Transformer-based models [7, 9–12].
Cross-modal alignment is one of the most critical challenges in VLN as the predicted trajectory needs to match the given instruction accurately. However, it is difﬁcult for the agent to learn very
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
accurate visual and textual modality alignment from coarse-grained data, i.e., only coarse-level cor-respondences between global instructions and trajectories are annotated, without ﬁner ones between sub-instructions and sub-trajectories. Works mentioned above [4–8, 10–12] tried attending on spe-ciﬁc parts of the instructions during navigation to help the agent better align salient visual con-tents with the textual modality. But only coarse-grained data like R2R are not capable of providing enough supervising signals of accurate cross-modal alignment. Some recent works such as Fine-Grained R2R [13] and BabyWalk [14] also found this same issue and proposed ﬁne-grained datasets based on R2R. However, their datasets are generated by heuristic rules and are not precise enough, which inevitably limit the navigation performance. Although Room-across-Room (RxR) dataset
[15] includes word-level alignment between each word and point in the trajectory, it lacks segmenta-tion tags to split an instruction into sub-instructions with independent meanings and to ﬁgure out the landmarks in a trajectory, namely the marked end points of sub-trajectories. Since words in the same sub-instruction are highly correlated, these important correlations are lost because no sub-instruction exists in RxR. It is the same for the sub-trajectories. As a result, there is also an absence of the corre-lation between the words in a sub-instruction and the points in corresponding sub-trajectories, which should be an important cross-modal alignment supervision signal.
In addition to the dataset annotation, designing better rewards during reinforcement learning is an-other important issue on cross-modal alignment learning. Currently, reward shaping has been well investigated in VLN based on coarse-grained data to align instructions and trajectories globally, but pays little attention to enhance local cross-modal alignment. The goal-oriented reward simply [6] uses the arrival signal and the reduced distance to the goal point as the reward, guiding agent to ﬁnd the global goal points. The ﬁdelity-oriented rewards [16, 17] help agent improve the similarity be-tween the predicted trajectory and demonstration trajectory globally by taking intermediate points as external supervision. However, some of the points in the trajectory have more detailed descriptions and are more helpful to navigation process, i.e., they can assist agent in knowing the visual scenes around these speciﬁc points in the trajectories could be better aligned with certain parts of the in-structions than the trivial points, where the alignment is in a local to local manner. These points are called critical points in this paper. Thus, an emphasis on these critical points in the reward shaping for better local cross-modal alignment is essential.
In this paper, we address the challenges above from the perspective of ﬁne-grain. Firstly, based on the English Guide part of RxR (en-RxR), we introduce a ﬁne-grained dataset Landmark-RxR, which is human-annotated, landmark-based, ﬁne-grained and currently the largest scale. With the groundtruth ﬁne-grained data, experiments demonstrate that agent generalizes better to unseen en-vironments and instructions with domain gap. This indicates that the ﬁne-grained data help agent to align textual and visual modalities better. Secondly, we propose two kinds of focal-oriented re-wards that encourage local alignment between instructions and critical points. Since the landmarks in Landmark-RxR naturally meet the requirements of critical points, we just sampled critical points from the landmark set in Landmark-RxR. The focal-oriented rewards outperform the commonly used goal-oriented reward and ﬁdelity-oriented reward. We also propose the re-initialization mech-anism to fully evaluate the navigation process in a way that is insensitive to difﬁcult points, which can cause the deviation from the correct trajectory. With the ﬁne-grained data and focal-oriented rewards, our agent shows superior navigation performance on Landmark-RxR, en-RxR and R2R. 2