Abstract
Recent advances in deep generative models have led to impressive results in a variety of application domains. Motivated by the possibility that deep learning models might memorize part of the input data, there have been increased efforts to understand how memorization arises. In this work, we extend a recently pro-posed measure of memorization for supervised learning (Feldman, 2019) to the unsupervised density estimation problem and adapt it to be more computationally efﬁcient. Next, we present a study that demonstrates how memorization can occur in probabilistic deep generative models such as variational autoencoders. This reveals that the form of memorization to which these models are susceptible differs fundamentally from mode collapse and overﬁtting. Furthermore, we show that the proposed memorization score measures a phenomenon that is not captured by commonly-used nearest neighbor tests. Finally, we discuss several strategies that can be used to limit memorization in practice. Our work thus provides a framework for understanding problematic memorization in probabilistic generative models. 1

Introduction
In the last few years there have been incredible successes in generative modeling through the development of deep learning techniques such as variational autoencoders (VAEs) [1, 2], generative adversarial networks (GANs) [3], normalizing ﬂows [4, 5], and diffusion networks [6, 7], among others. The goal of generative modeling is to learn the data distribution of a given data set, which has numerous applications such as creating realistic synthetic data, correcting data corruption, and detecting anomalies. Novel architectures for generative modeling are typically evaluated on how well a complex, high dimensional data distribution can be learned by the model and how realistic the samples from the model are. An important question in the evaluation of generative models is to what extent training observations are memorized by the learning algorithm, as this has implications for data privacy, model stability, and generalization performance. For example, in a medical setting it is highly desirable to know if a synthetic data model could produce near duplicates of the training data.
A common technique to assess memorization in deep generative models is to take samples from the model and compare these to their nearest neighbors in the training set. There are several problems with this approach. First, it has been well established that when using the Euclidean metric this test can be easily fooled by taking an image from the training set and shifting it by a few pixels [8]. For this reason, nearest neighbors in the feature space of a secondary model are sometimes used, as well as cropping and/or downsampling before identifying nearest neighbors (e.g., [9–11]). Second, while there may not be any neighbors in the training set for a small selection of samples from the model, this does not demonstrate that there are no observations that are highly memorized. Indeed, in several
∗Work done while at The Alan Turing Institute. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Memorization (b) Overﬁtting
Figure 1: Memorization and overﬁtting. In (a) the solid blue curve reﬂects the probability density when all observations are included, whereas the dashed yellow curve is the density when only yellow observations are included. The local change in density that occurs when an observation is removed from the training data indicates the extent to which the model memorized the observation. The density typically associated with overﬁtting due to overtraining is shown by the solid red curve in (b). recent publications on deep generative models it is possible to identify observations highly similar to the training set in the illustrations of generated samples (see Supplement A).
Memorization in generative models is not always surprising. When the training data set contains a number of highly similar observations, such as duplicates, then it would be expected that these receive an increased weight in the model and are more likely to be generated. The fact that commonly-used data sets contain numerous (near) duplicates [12] therefore provides one reason for memorization of training observations. While important, memorization due to duplicates is not the focus of this work.
Instead, we are concerned with memorization that arises as an increased probability of generating a sample that closely resembles the training data in regions of the input space where the algorithm has not seen sufﬁcient observations to enable generalization. For example, we may expect that highly memorized observations are either in some way atypical or are essential for properly modeling a particular region of the data manifold.
Figure 1a illustrates this kind of local memorization in probabilistic generative models. We focus on explicit density models as these are more amenable to a direct analysis of the learned probability distribution (as opposed to implicit density models such as GANs). The ﬁgure shows that in certain regions of the input space the learned probability density can be entirely supported by a single, potentially outlying, observation. When sampling from the model in these parts of the space it is thus highly likely that a sample similar to an input observation will be generated. The ﬁgure also illustrates that in regions of the input space that are densely supported by closely-related observations, sampling will yield observations that resemble the input data. The change in the probability density of an observation that occurs when it is removed from the data forms the basis of the memorization score we propose in Section 3. This form of memorization should be contrasted with what is commonly associated with memorization due to overﬁtting (illustrated in Figure 1b). Overﬁtting is a global property of a model that typically occurs when it is trained for too long or with too high a learning rate (i.e., overtraining), so that a gap develops between the training and test performance. Thus, we emphasize that in generative models memorization and generalization can occur simultaneously at distinct regions of the input space, and that memorization is not necessarily caused by overtraining.
To understand memorization further, consider the simple case of ﬁtting a multivariate normal distribu-tion. In this scenario, the presence or absence of a particular observation in the data set will have a small effect on the learned model unless the observation is an outlier. By contrast, a kernel density estimate (KDE) [13, 14] of the probability density may be more sensitive to the presence or absence of a particular observation. To see why this is the case, consider that in sparsely-populated regions of the input space the KDE can be supported by a relatively small number of observations. Although deep generative models typically operate in much higher dimensional spaces than the aforementioned methods, the same problem can arise when generalizing to regions of the space that are weakly supported by the available data. Because these models are optimized globally, the model has to place some probability mass in these regions. As we will demonstrate below, it is not necessarily the case that the model places low probability on such observations, resulting in observations that are both highly memorized and not signiﬁcantly less likely under the model than other observations.
In this work, we extend a recently proposed measure of memorization for supervised learning [15, 16] to probabilistic generative models and introduce a practical estimator of this memorization score. We subsequently investigate memorization experimentally, where we focus on the variational autoencoder. 2
In our experiments we demonstrate that highly memorized observations are not necessarily outliers, that memorization can occur early during the training process, and show the connection between nearest neighbor tests for memorization and the proposed memorization score. Finally, we discuss approaches that can limit memorization in practice. 2