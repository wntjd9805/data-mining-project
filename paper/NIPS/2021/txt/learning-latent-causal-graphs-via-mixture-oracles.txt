Abstract
We study the problem of reconstructing a causal graphical model from data in the presence of latent variables. The main problem of interest is recovering the causal structure over the latent variables while allowing for general, potentially nonlinear dependencies. In many practical problems, the dependence between raw observa-tions (e.g. pixels in an image) is much less relevant than the dependence between certain high-level, latent features (e.g. concepts or objects), and this is the setting of interest. We provide conditions under which both the latent representations and the underlying latent causal model are identiﬁable by a reduction to a mixture oracle.
These results highlight an intriguing connection between the well-studied problem of learning the order of a mixture model and the problem of learning the bipartite structure between observables and unobservables. The proof is constructive, and leads to several algorithms for explicitly reconstructing the full graphical model.
We discuss efﬁcient algorithms and provide experiments illustrating the algorithms in practice. 1

Introduction
Understanding causal relationships between objects and/or concepts is a core component of human reasoning, and by extension, a core component of artiﬁcial intelligence [40, 53]. Causal relationships are robust to perturbations, encode invariances in a system, and enable agents to reason effectively about the effects of their actions in an environment. Broadly speaking, the problem of inferring causal relationships can be broken down into two main steps: 1) The extraction of high-level causal features from raw data, and 2) The inference of causal relationships between these high-level features.
From here, one may consider estimating the magnitude of causal effects, the effect of interventions, reasoning about counterfactuals, etc. Our focus in this paper will be the problem of learning causal relationships between latent variables, which is closely related to the problem of learning causal representations [63]. This problem should be contrasted with the equally important problem of causal inference in the presence of latent confounders [e.g. 3, 18, 35, 68, 72]; see also Remark 2.1.
Causal graphical models [53, 54] provide a natural framework for this problem, and have long been used to model causal systems with hidden variables [23–26, 58, 59]. It is well-known that in general, without additional assumptions, a causal graphical model given by a directed acyclic graph (DAG) is not identiﬁable in the presence of latent variables [e.g., 53, 71]. In fact, this is a generic property of nonparametric structural models: Without assumptions, identiﬁability is impossible, however, given enough structure, identiﬁability can be rescued. Examples of this phenomenon include linearity
[3, 6, 15, 28, 78], independence [1, 10, 78], rank [15, 28], sparsity [6], and graphical constraints
[3, 4]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we consider a general setting for this problem with discrete latent variables, while allowing otherwise arbitrary (possibly nonlinear) dependencies. The latent causal graph between the latent variables is also allowed to be arbitrary: No assumptions are placed on the structure of this
DAG. We do not assume that the number of hidden variables, their state spaces, or their relationships are known; in fact, we provide explicit conditions under which all of this can be recovered uniquely.
To accomplish this, we highlight a crucial reduction between the problem of learning a DAG model over these variables—given access only to the observed data—and learning the parameters of a ﬁnite mixture model. This observation leads to new identiﬁability conditions and algorithms for learning causal models with latent structure.
Overview Our starting point is a simple reduction of the graphical model recovery problem to three modular subproblems: 1. The bipartite graph Γ between hidden and observed nodes, 2. The latent distribution P(𝐻) over the hidden variables 𝐻, and 3. A directed acyclic graph (DAG) Λ over the latent distribution.
From here, the crucial observation is to reduce the recovery problems for Γ and P(𝐻) to the problem of learning a ﬁnite mixture over the observed data. The latter is a well-studied problem with many practical algorithms and theoretical guarantees. We do not require parametric assumptions on this mixture, which allows for very general dependencies between the observed and hidden variables.
From this mixture model, we extract what is needed to learn the full graph structure.
This perspective leads to a systematic, modular approach for learning the latent causal graph via mixture oracles (see Section 2 for deﬁnitions). Ultimately, the application of these ideas requires a practical implementation of this mixture oracle, which is discussed in Section 6.
Contributions More precisely, we make the following contributions: 1. (Section 3) We provide general conditions under which the latent causal model 𝐺 is iden-tiﬁable (Theorem 3.2). Surprisingly, these conditions mostly amount to nondegeneracy conditions on the joint distribution. As we show, without these assumptions identiﬁability breaks down and reconstruction becomes impossible. 2. (Section 4) We carefully analyze the problem of reconstructing Γ under progressively weaker assumptions: First, we derive a brute-force algorithm that identiﬁes Γ in a general setting (Theorem 4.2), and then under a linear independence condition we derive a polynomial-time algorithm based on tensor decomposition and Jennrich’s algorithm (Theorem 4.8). 3. (Section 5) Building on top of the previous step, where we learn the bipartite graph and sizes of the domains of latent variables, we develop an efﬁcient algorithm for learning the latent distribution P(𝐻) from observed data (Theorem 5.4). 4. (Section 6-7) We implement these algorithms as part of an end-to-end pipeline for learning the full causal graph and illustrate its performance on simulated data.
A prevailing theme throughout is the fact that the hidden variables leave a recognizable “signature” in the observed data through the marginal mixture models induced over subsets of observed variables.
By cleverly exploiting these signatures, the number of hidden variables, their states, and their relationships can be recovered exactly.
Previous work Latent variable graphical models have been extensively studied in the literature; as such we focus only on the most closely related work on causal graphical models here. Early work on this problem includes seminal work by Elidan et al. [22], Friedman et al. [27], Martin and VanLehn
[47]. More recent work has focused on linear models [3, 28, 68, 78] or known structure [21, 38, 65].
When the structure is not known a priori, we ﬁnd ourselves in the realm of structure learning, which is our focus. Less is known regarding structure learning between latent variables for nonlinear models, although there has been recent progress based on nonlinear ICA [36, 50]. For example, [80] proposed
CausalVAE, which assumes a linear structural equation model and knowledge of the concept labels for the latent variables, in order to leverage the iVAE model from [36]. By contrast, our results make no linearity assumptions and do not require these additional labels. While this paper was under 2
𝐻1
𝐻1
𝐻2
𝐻1
𝐻2
𝐻3
𝑋1
𝑋2
𝑋1
𝑋2
𝑋1
𝑋2
𝑋3 (a) A single hidden state. (b) Two independent hidden states. (c) Three dependent hidden states.
Figure 1: Illustration of the basic model. Note that there are no edges between observed variables or edges oriented from observed to hidden. (a) A latent variable model with a single hidden state; i.e. a mixture model. (b)-(c) Two examples of latent variable models with more complicated hidden structure. review, we were made aware of the recent work [46] that studies a similar problem to ours in a general, nonlinear setting under faithfulness assumptions. It is also worth noting recent progress on learning discrete Boltzmann machines [11, 12], which can be interpreted as an Ising model with a bipartite structure and a single hidden layer—in particular, there is no hidden causal structure.
Nevertheless, this line of work shows that learning Boltzmann machines is computationally hard in a precise sense. More broadly, the problem of learning latent structure has been studied in a variety of other applications including latent Dirichlet allocation [8, 9], phylogenetics [51, 64], and hidden
Markov models [2, 30].
A prevailing theme in the causal inference literature has been negative results asserting that in the presence of latent variables, causal inference is impossible [20, 32, 61, 62]. Our results do not contradict this important line of work, and instead adopts a more optimistic tone: We show that under reasonable assumptions—essentially that the latent variables are discrete and well-separated— identiﬁability and exact recovery of latent causal relationships is indeed possible. This optimistic approach is implicit in recent progress on visual relationship detection [52], causal feature learning
[14, 43], and interaction modeling [37, 41]. In this spirit, our work provides theoretical grounding for some of these ideas.
Mixture models and clustering While our theoretical results in Sections 3-5 assume access to a mixture oracle (see Deﬁnition 2.5), in Section 6 we discuss how this oracle can be implemented in practice. To provide context for these results, we brieﬂy mention related work on learning mixture models from data. Mixture models can be learned under a variety of parametric and nonparametric assumptions. Although much is known about parametric models [e.g. 42], of more interest to us are nonparametric models in which the mixture components are allowed to be ﬂexible, such as mixtures of product distributions [31, 33], grouped observations [60, 75] and general nonparametric mixtures
[7, 66]. In each of these cases, a mixture oracle can be implemented without parametric assumptions.
In practice, we use clustering algorithms such as 𝐾-means or hierarchical clustering to implement this oracle. We note also that the speciﬁc problem of consistently estimating the order of a mixture model, which will be of particular importance in the sequel, has been the subject of intense scrutiny in the statistics literature [e.g. 16, 19, 39, 45].
Broader impacts and societal impact Latent variable models have numerous practical applica-tions. Many of these applications positively address important social problems, however, these models can certainly be applied nefariously. For example, if the latent variables represent private, protected information, our results imply that this hidden private data can be leaked into publicly released data, which is obviously undesirable. Understanding how to infer unprotected data while safeguarding protected data is an important problem, and our results shed light on when this is and isn’t possible. 2