Abstract
Graph neural networks (GNNs) are widely used for modelling graph-structured data in numerous applications. However, with their inherently ﬁnite aggregation layers, existing GNN models may not be able to effectively capture long-range dependencies in the underlying graphs. Motivated by this limitation, we propose a GNN model with inﬁnite depth, which we call Efﬁcient Inﬁnite-Depth Graph
Neural Networks (EIGNN), to efﬁciently capture very long-range dependencies.
We theoretically derive a closed-form solution of EIGNN which makes training an inﬁnite-depth GNN model tractable. We then further show that we can achieve more efﬁcient computation for training EIGNN by using eigendecomposition.
The empirical results of comprehensive experiments on synthetic and real-world datasets show that EIGNN has a better ability to capture long-range dependen-cies than recent baselines, and consistently achieves state-of-the-art performance.
Furthermore, we show that our model is also more robust against both noise and adversarial perturbations on node features. 1

Introduction
Graph-structured data are ubiquitous in the real world. To model and learn from such data, graph rep-resentation learning aims to produce meaningful node representations by simultaneously considering the graph topology and node attributes. It has attracted growing interest in recent years, as well as numerous real-world applications [32].
In particular, graph neural networks (GNNs) are a widely used approach for node, edge, and graph prediction tasks. Recently, many GNN models have been proposed (e.g., graph convolutional network
[14], graph attention network [26], simple graph convolution [30]). Most modern GNN models follow a “message passing” scheme: they iteratively aggregate the hidden representations of every node with those of the adjacent nodes to generate new hidden representations, where each iteration is parameterized as a neural network layer with learnable weights.
Despite the success existing GNN models achieve on many different scenarios, they lack the ability to capture long-range dependencies. Speciﬁcally, for a predeﬁned number of layers T , these models cannot capture dependencies with a range longer than T -hops away from any given node. A straightforward strategy to capture long-range dependencies is to stack a large number of GNN layers for receiving “messages” from distant nodes. However, existing work has observed poor empirical performance when stacking more than a few layers [16], which has been referred to as oversmoothing.
This has been attributed to various reasons, including node representations becoming indistinguishable as depth increases. Besides oversmoothing, GNN models with numerous layers require excessive computational cost in practice since they need to repeatedly propagate representations across many layers. For these two reasons, simply stacking many layers for GNNs is not a suitable way to capture long-range dependencies. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
Recently, instead of stacking many layers, several works have been proposed to capture long-range dependencies in graphs. Pei et al. [21] propose a global graph method Geom-GCN which builds structural neighborhoods based on the graph and the embedding space, and uses a bi-level aggregation to capture long-range dependencies. However, as Geom-GCN still only has ﬁnite layers, it still fails to capture very long range dependencies.
To model longer range dependencies, Gu et al. [10] propose the implicit graph neural network (IGNN), which can be considered as a GNN model with inﬁnite layers. Thus, IGNN does not suffer from any a priori limitation on the range of information it can capture. To achieve this, IGNN generates predictions as the solution to a ﬁxed-point equilibrium equation. Speciﬁcally, the solution is obtained by an iterative solver. Moreover, they require additional conditions to ensure well-posedness of the model and use projected gradient descent method to train the model. However, the practical limitations of iterative solvers are widely recognized: they can lack robustness; the generated solution is approximated; and the number of iterations cannot be known in advance [23]. In our experiments, we found that IGNN sometimes experiences non-convergence in its iterative solver. Besides the non-convergence issue, the iterative solver of IGNN can be inefﬁcient as IGNN needs to run it once per forward or backward pass.
Motivated by these limitations, we propose our Efﬁcient Inﬁnite-Depth Graph Neural Network (EIGNN) approach, which can effectively capture very long range dependencies in graphs. Instead of relying on iterative solvers to generate the solution like in IGNN [10], we derive a closed-form solution for EIGNN without additional conditions, avoiding the need for projected gradient descent to train the model. Furthermore, we propose to use eigendecomposition to improve the efﬁciency of
EIGNN, without affecting its accuracy.
The contributions of this work are summarized as follows:
• To capture long-range dependencies, we propose our inﬁnite-depth EIGNN model. To do this, we
ﬁrst deﬁne our model as the limit of an inﬁnite sequence of graph convolutions, and theoretically prove its convergence. Then, we derive tractable forward and backward computations for EIGNN.
• We then further derive an eigendecomposition-based approach to improve the computa-tional/memory complexity of our approach, without affecting its accuracy.
• We empirically compare our model to recent baseline GNN models on synthetic and real-world graph datasets. The results show that EIGNN has a better ability to capture very long range dependencies and provides better performance compared with other baselines. Moreover, the empirical results of noise sensitivity experiments demonstrate that EIGNN is more robust against both noise and adversarial perturbations.
In Section 2, we provide an overview of GNNs, implicit models, and the oversmooth-Paper outline ing problem. Section 3 introduces the background and major mathematics symbols used in this paper.
In Section 4, we ﬁrst present the EIGNN model and discuss how to train this inﬁnitely deep model in practice. In addition, we show that with eigendecomposition we can reduce the complexity and achieve more efﬁcient computation for EIGNN. In Section 5, we empirically compare EIGNN with other representative GNN methods. 2