Abstract
Neuronal computations depend on synaptic connectivity and intrinsic electrophysio-logical properties. Synaptic connectivity determines which inputs from presynaptic neurons are integrated, while cellular properties determine how inputs are ﬁltered over time. Unlike their biological counterparts, most computational approaches to learning in simulated neural networks are limited to changes in synaptic connec-tivity. However, if intrinsic parameters change, neural computations are altered drastically. Here, we include the parameters that determine the intrinsic properties, e.g., time constants and reset potential, into the learning paradigm. Using sparse feedback signals that indicate target spike times, and gradient-based parameter updates, we show that the intrinsic parameters can be learned along with the synaptic weights to produce speciﬁc input-output functions. Speciﬁcally, we use a teacher-student paradigm in which a randomly initialised leaky integrate-and-ﬁre or resonate-and-ﬁre neuron must recover the parameters of a teacher neuron. We show that complex temporal functions can be learned online and without backprop-agation through time, relying on event-based updates only. Our results are a step towards online learning of neural computations from ungraded and unsigned sparse feedback signals with a biologically inspired learning mechanism. 1

Introduction
In biological and artiﬁcial spiking neurons, information processing depends on synaptic connectivity as well as intrinsic variables. How a neuron integrates synaptic inputs over time changes its input-output function, leading to a large variety of neural computations [1]. In biology, the input-output function of a neuron is modiﬁed not only by synaptic plasticity [2], but also by alterations of the neuron’s electrophysiology, e.g., through changes in the composition of trans-membrane ion channels, so-called intrinsic plasticity [3, 4]. Synaptic interactions inﬂuence the amount of neurotransmitter released from the presynaptic neuron or the amount of receptors in the postsynaptic terminal and therefore how strongly neurons are connected to each other. Intrinsic properties of the neuron, on the other hand, affect the neuron’s threshold and resting potential and how strongly postsynaptic potentials are ampliﬁed [4].
Correspondingly, the computational properties of an artiﬁcial neuron depend on the values of its synaptic weights and intrinsic parameters. For instance, the neural computation performed by a leaky integrate-and-ﬁre neuron [5] or a leaky resonate-and-ﬁre neuron [6] with identical synaptic inputs and synaptic weights but different intrinsic parameters varies strongly within and across neuron models (Figure 1A). Learning in artiﬁcial neurons is thus the process of modifying a set of synaptic weights and intrinsic parameters to reach a target input-output function. However, unlike their biological counterparts, most computational approaches to learning in spiking neurons are limited to effects of 1. Department of Experimental Psychology, University of Oxford, Oxford, United Kingdom 2. Bernstein Center for Computational Neuroscience Berlin, Berlin, Germany 4. Institute of Science and Technology Austria, Klosterneuburg, Austria 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Neural computations and the teacher-student paradigm. A: Despite identical synaptic weights and synaptic inputs, the input-output function varies strongly within and across neuron models due to different intrinsic parameters. B: The teacher-student paradigm is used to study the convergence properties of a learning algorithm. The teacher and the student receive identical inputs, but their synaptic weights and intrinsic parameters have different values and therefore generate different outputs. The learning problem for the student neuron is to recover the ﬁxed set of parameters of the teacher and thus to learn the neural computation performed by the teacher neuron. alterations in synaptic connectivity. Here, we derive a gradient-based online learning rule for leaky integrate-and-ﬁre neurons and leaky resonate-and-ﬁre neurons that can learn the synaptic connectivity along their intrinsic parameters from sparse temporal feedback in the teacher-student paradigm. 1.1