Abstract
Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difﬁ-culty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for gen-eralization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predeﬁned program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a ﬂexible and expressive manner, solely from reward signals.
To alleviate the difﬁculty of learning to compose programs to induce the desired agent behavior from scratch, we propose to ﬁrst learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing in-terpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding. Website at https://clvrai.com/leaps. 1

Introduction
Recently, deep reinforcement learning (DRL) methods have demonstrated encouraging performance on a variety of domains such as outperforming humans in complex games [1–4] or controlling robots [5–11]. Despite the recent progress in the ﬁeld, acquiring complex skills through trial and error still remains challenging and these neural network policies often have difﬁculty generalizing to novel scenarios. Moreover, such policies are not interpretable to humans and therefore are difﬁcult to debug when these challenges arise.
To address these issues, a growing body of work aims to learn programmatic policies that are structured in more interpretable and generalizable representations such as decision trees [12], state-machines [13], and programs described by domain-speciﬁc programming languages [14, 15]. Yet, the programmatic representations employed in these works are often limited in expressiveness due to constraints on the policy spaces. For example, decision tree policies are incapable of naïvely generating repetitive behaviors, state machine policies used in [13] are computationally complex to
∗Contributed equally.
†Work partially done as a visiting scholar at USC.
‡AI Advisor at NAVER AI Lab. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
scale to policies representing diverse behaviors, and the programs of [14, 15] are constrained to a set of predeﬁned program templates. On the other hand, program synthesis works that aim to represent desired behaviors using ﬂexible domain-speciﬁc programs often require extra supervision such as input/output pairs [16–20] or expert demonstrations [21, 22], which can be difﬁcult to obtain.
In this paper, we present a framework to instead synthesize human-readable programs in an expressive representation, solely from rewards, to solve tasks described by Markov Decision Processes (MDPs).
Speciﬁcally, we represent a policy using a program composed of control ﬂows (e.g. if/else and loops) and an agent’s perceptions and actions. Our programs can ﬂexibly compose behaviors through perception-conditioned loops and nested conditional statements. However, composing individual program tokens (e.g. if, while, move()) in a trial-and-error fashion to synthesize programs that can solve given MDPs can be extremely difﬁcult and inefﬁcient.
To address this problem, we propose to ﬁrst learn a latent program embedding space where nearby latent programs correspond to similar behaviors and allows for smooth interpolation, together with a program decoder that can decode a latent program to a program consisting of a sequence of program tokens. Then, when a task is given, this embedding space allows us to iteratively search over candidate latent programs to ﬁnd a program that induces desired behavior to maximize the reward. Speciﬁcally, this embedding space is learned through reconstruction of randomly generated programs and the behaviors they induce in the environment in an unsupervised manner. Once learned, the embedding space can be reused to solve different tasks without retraining.
To evaluate the proposed framework, we consider the Karel domain [23], featuring an agent navigating through a gridworld and interacting with objects to solve tasks such as stacking and navigation. The experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms program synthesis and deep RL baselines. In addition, we justify the necessity of the proposed two-stage learning scheme as well as conduct an extensive analysis comparing various approaches for learning the latent program embedding spaces. Finally, we perform experiments which highlight that the programs produced by our proposed framework can both generalize to larger state spaces and unseen state conﬁgurations as well as be interpreted and edited by humans to improve their task performance. 2