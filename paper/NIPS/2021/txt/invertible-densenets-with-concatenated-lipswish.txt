Abstract
We introduce Invertible Dense Networks (i-DenseNets), a more parameter efﬁcient extension of Residual Flows. The method relies on an analysis of the Lipschitz continuity of the concatenation in DenseNets, where we enforce invertibility of the network by satisfying the Lipschitz constant. Furthermore, we propose a learnable weighted concatenation, which not only improves the model performance but also indicates the importance of the concatenated weighted representation. Additionally, we introduce the Concatenated LipSwish as activation function, for which we show how to enforce the Lipschitz condition and which boosts performance. The new architecture, i-DenseNet, out-performs Residual Flow and other ﬂow-based models on density estimation evaluated in bits per dimension, where we utilize an equal parameter budget. Moreover, we show that the proposed model out-performs Residual Flows when trained as a hybrid model where the model is both a generative and a discriminative model. 1

Introduction
Neural networks are widely used to parameterize non-linear models in supervised learning tasks such as classiﬁcation. In addition, they are also utilized to build ﬂexible density estimators of the true distribution of the observed data [25, 33]. The resulting deep density estimators, also called deep generative models, can be further used to generate realistic-looking images that are hard to separate from real ones, detection of adversarial attacks [9, 17], and for hybrid modeling [27] which have the property to both predict a label (classify) and generate.
Many deep generative models are trained by maximizing the (log-)likelihood function and their architectures come in different designs. For instance, causal convolutional neural networks are used to parameterize autoregressive models [28, 29] or various neural networks can be utilized in Variational
Auto-Encoders [19, 32]. The other group of likelihood-based deep density estimators, ﬂow-based models (or ﬂows), consist of invertible neural networks since they are used to compute the likelihood through the change of variable formula [31, 37, 36]. The main difference that determines an exact computation or approximation of the likelihood function for a ﬂow-based model lies in the design of the transformation layer and tractability of the Jacobian-determinant. Many ﬂow-based models formulate the transformation that is invertible and its Jacobian is tractable [3, 6–8, 21, 30, 31, 38].
Recently, Behrmann et al. [2] proposed a different approach, namely, deep-residual blocks as a transformation layer. The deep-residual networks (ResNets) of [12] are known for their successes in supervised learning approaches. In a ResNet block, each input of the block is added to the output, which forms the input for the next block. Since ResNets are not necessarily invertible, Behrmann et al. [2] enforce the Lipschitz constant of the transformation to be smaller than 1 (i.e., it becomes a contraction) that allows applying an iterative procedure to invert the network. Furthermore, Chen et al. [4] proposed Residual Flows, an improvement of i-ResNets, that uses an unbiased estimator for the logarithm of the Jacobian-determinant. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
(a) Residual block (b) Dense block
Figure 1: A schematic representation for: (a) a residual block, (b) a dense block. The pink part in (b) expresses a 1 1 convolution to reduce the dimension of the last dense layer. Wi denotes the (convolutional) layer at step i that satisfy
⇥
Wi||2 < 1.
||
In supervised learning, an architecture that uses fewer parameters and is even more powerful than the deep-residual network is the Densely Connected Convolution Network (DenseNet), which was
ﬁrst presented in [15]. Contrary to a ResNet block, a DenseNet layer consists of a concatenation of the input with the output. The network showed to improve signiﬁcantly in recognition tasks on benchmark datasets such as CIFAR10, SVHN, and ImageNet, by using fewer computations and having fewer parameters than ResNets while performing at a similar level.
In this work, we extend Residual Flows [2, 4], and use densely connected blocks (DenseBlocks) as a residual layer. First, we introduce invertible Dense Networks (i-DenseNets), and we show that we can derive a bound on the Lipschitz constant to create an invertible ﬂow-based model.
Furthermore, we propose the Concatenated LipSwish (CLipSwish) as an activation function, and derive a stronger Lipschitz bound. The CLipSwish function preserves more signal than LipSwish activation functions. Finally, we demonstrate how i-DenseNets can be efﬁciently trained as a generative model, outperforming Residual Flows and other ﬂow-based models under an equal parameter budget. 2