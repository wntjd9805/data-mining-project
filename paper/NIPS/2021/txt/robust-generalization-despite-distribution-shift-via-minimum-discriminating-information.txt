Abstract
Training models that perform well under distribution shifts is a central challenge in machine learning. In this paper, we introduce a modeling framework where, in addition to training data, we have partial structural knowledge of the shifted test distribution. We employ the principle of minimum discriminating information to embed the available prior knowledge, and use distributionally robust optimization to account for uncertainty due to the limited samples. By leveraging large deviation results, we obtain explicit generalization bounds with respect to the unknown shifted distribution. Lastly, we demonstrate the versatility of our framework by demonstrating it on two rather distinct applications: (1) training classiﬁers on sys-tematically biased data and (2) off-policy evaluation in Markov Decision Processes. 1

Introduction
Developing machine learning-based systems for real world applications is challenging, particularly because the conditions under which the system was trained are rarely the same as when using the system. Unfortunately, a standard assumption in most machine learning methods is that test and training distribution are the same [78, 59, 12]. This assumption, however, rarely holds in practice, and the performance of many models suffers in light of this issue, often called dataset shift [52] or equivalently distribution shift. Consider building a model for diagnosing a speciﬁc heart disease, and suppose that most participants of the study are middle to high-aged men. Further suppose these participants have a higher risk for the speciﬁc disease, and as such do not reﬂect the general population with respect to age and gender. Consequently, the training data suffers from the so-called sample selection bias inducing a covariate shift [62, 52]. Many other reasons lead to distribution shifts, such as non-stationary environments [67], imbalanced data [52], domain shifts [3], label shifts
[83] or observed contextual information [8, 9]. A speciﬁc type of distribution shift takes center stage in off-policy evaluation (OPE) problems. Here, one is concerned with the task of estimating the resulting cost of an evaluation policy for a sequential decision making problem based on historical data obtained from a different policy known as behavioral policy [73]. This problem is of critical importance in various applications of reinforcement learning—particularly, when it is impossible or unethical to evaluate the resulting cost of an evaluation policy by running it on the underlying system.
Solving a learning problem facing an arbitrary and unknown distribution shift based on training data in general is hopeless. Oftentimes, fortunately, partial knowledge about the distribution shift is available. In the medical example above, we might have prior information how the demographic attributes in our sample differ from the general population. Given a training distribution and partial knowledge about the shifted test distribution, one might ask what is the “most natural" distribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
shift mapping the training distribution into a test distribution consistent with the available structural information. Here, we address this question, interpreting “most natural" as maximizing the underlying
Shannon entropy. This concept has attracted signiﬁcant interest in the past in its general form, called principle of minimum discriminating information dating back to Kullback [37], which can be seen as a generalization of Jaynes’ maximum entropy principle [31]. While these principles are widely used in tasks ranging from economics [27] to systems biology [63] and regularized Markov decision processes [48, 25, 2], they have not been investigated to model general distribution shifts as we consider in this paper.
Irrespective of the underlying distribution shift, the training distribution of any learning problem is rarely known, and one typically just has access to ﬁnitely many training samples. It is well-known that models can display a poor out-of-sample performance if training data is sparse. These overﬁtting effects are commonly avoided via regularization [12]. A regularization technique that has become popular in machine learning during the last decade and provably avoids overﬁtting is distributionally robust optimization (DRO) [36].
Contributions. We highlight the following main contributions of this paper:
• We introduce a new modelling framework for distribution shifts via the principle of minimum discriminating information, which encodes prior structural information on the resulting test distribution.
• Using our framework and the available training samples, we provide generalization bounds via a
DRO program and prove that the introduced DRO model is optimal in a precise statistical sense.
• We show that the optimization problems characterizing the distribution shift and the DRO program can be efﬁciently solved by exploiting convex duality and recent accelerated ﬁrst order methods.
• We demonstrate the versatility of the proposed Minimum Discriminating based DRO (MDI-DRO) method on two distinct problem classes: Training classiﬁers on systematically biased data and the
OPE for Markov decision processes. In both problems MDI-DRO outperforms existing approaches. 2