Abstract
Generative adversarial networks (GANs) have recently found applications in im-age editing. However, most GAN-based image editing methods often require large-scale datasets with semantic segmentation annotations for training, only provide high level control, or merely interpolate between different images. Here, we propose EditGAN, a novel method for high-quality, high-precision semantic image editing, allowing users to edit images by modifying their highly detailed part segmentation masks, e.g., drawing a new mask for the headlight of a car.
EditGAN builds on a GAN framework that jointly models images and their seman-tic segmentations [1, 2], requiring only a handful of labeled examples – making it a scalable tool for editing. Speciﬁcally, we embed an image into the GAN’s latent space and perform conditional latent code optimization according to the segmentation edit, which effectively also modiﬁes the image. To amortize op-timization, we ﬁnd “editing vectors” in latent space that realize the edits. The framework allows us to learn an arbitrary number of editing vectors, which can then be directly applied on other images at interactive rates. We experimentally show that EditGAN can manipulate images with an unprecedented level of detail and freedom, while preserving full image quality.We can also easily combine multiple edits and perform plausible edits beyond EditGAN’s training data. We demon-strate EditGAN on a wide variety of image types and quantitatively outperform several previous editing methods on standard editing benchmark tasks. Project page: https://nv-tlabs.github.io/editGAN. 1

Introduction
Change Shape
Enlarge Wheels
Shrink Front-Light
AI-driven photo and image edit-ing has the potential to streamline the workﬂow of photographers and content creators and to enable new levels of creativity and dig-ital artistry [3]. AI-based image editing tools have already found their way into consumer software in the form of neural photo edit-ing ﬁlters, and the deep learning research community is actively developing further techniques. A particularly promising line of research uses generative adversarial networks (GANs) [4, 5, 6, 7, 8] and either embeds images into
Figure 1: High-precision semantic image editing with EditGAN.
Look Left
Frown
Smile
∗These authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: (1) EditGAN builds on a GAN framework that jointly models images and their semantic segmentations. (2 & 3) Users can modify segmentation masks, based on which we perform optimization in the GAN’s latent space to realize the edit. (4) Users can perform editing simply by applying previously learnt editing vectors and manipulate images at interactive rates. the GAN’s latent space or works directly with GAN-generated images. Careful modiﬁcations of the latent embeddings then translate to desired changes in generated output, allowing, for example, to coherently change facial expressions in portraits [9, 10, 11, 12, 13, 14, 15, 16], change viewpoint or shapes and textures of cars [17], or to interpolate between different images in a semantically meaningful manner [18, 19, 20, 21].
Most GAN-based image editing methods fall into few categories. Some works rely on GANs conditioning on class labels or pixel-wise semantic segmentation annotations [19, 10, 22, 11], where different conditionings lead to modiﬁcations in the output, while others use auxiliary attribute classiﬁers [23, 15] to guide synthesis and edit images. However, training such conditional GANs or external classiﬁers requires large labeled datasets. Therefore, these methods are currently limited to image types for which large annotated datasets are available, like portraits [10]. Furthermore, even if annotations are available, most techniques offer only limited editing control, since these annotations usually consist only of high-level global attributes or relatively coarse pixel-wise segmentations.
Another line of work focuses on mixing and interpolating features from different images [18, 19, 20, 21], thereby requiring reference images as editing targets and usually also not offering ﬁne control. Other approaches carefully analyze and dissect GANs’ latent spaces, ﬁnding disentangled latent variables suitable for editing [24, 25, 12, 13, 14, 26, 27], or control the GANs’ network parameters [25, 28, 16]. Usually, these methods do not enable detailed editing and are often slow.
In this work, we are addressing these limitations and propose EditGAN, a novel GAN-based image editing framework that enables high-precision semantic image editing by allowing users to modify detailed object part segmentations. EditGAN builds on a recently proposed GAN that jointly models both images and their semantic segmentations based on the same underlying latent code [1, 2], and requires as few as 16 labeled examples – allowing it to scale to many object classes and choices of part labels. We achieve editing by modifying the segmentation mask according to a desired edit and optimizing the latent code to be consistent with the new segmentation, thus effectively changing the
RGB image. To achieve efﬁciency, we learn editing vectors in latent space that realize the edits, and that can be directly applied on other images, without any or only few additional optimization steps.
We can thus pre-train a library of interesting edits that a user can directly utilize in an interactive tool.
We apply EditGAN on a wide range of images, including images of cars, cats, birds, and human faces, demonstrating unprecedented high-precision editing. We perform quantitative comparisons to multiple baselines and outperform them in metrics such as identity preservation, quality preservation, and target attribute accuracy, while requiring orders of magnitude less annotated training data.
EditGAN is the ﬁrst GAN-driven image editing framework, which simultaneously (i) offers very high-precision editing, (ii) requires only very little annotated training data (and does not rely on external classiﬁers), (iii) can be run interactively in real time, (iv) allows for straightforward compositionality of multiple edits, (v) and works on real embedded, GAN-generated, and even out-of-domain images. 2