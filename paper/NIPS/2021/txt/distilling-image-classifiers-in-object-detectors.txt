Abstract
Knowledge distillation constitutes a simple yet effective way to improve the per-formance of a compact student network by exploiting the knowledge of a more powerful teacher. Nevertheless, the knowledge distillation literature remains lim-ited to the scenario where the student and the teacher tackle the same task. Here, we investigate the problem of transferring knowledge not only across architectures but also across tasks. To this end, we study the case of object detection and, instead of following the standard detector-to-detector distillation approach, introduce a classifier-to-detector knowledge transfer framework. In particular, we propose strategies to exploit the classification teacher to improve both the detector’s recog-nition accuracy and localization performance. Our experiments on several detectors with different backbones demonstrate the effectiveness of our approach, allowing us to outperform the state-of-the-art detector-to-detector distillation methods. 1

Introduction
Object detection plays a critical role in many real-world applications, such as autonomous driving and video surveillance. While deep learning has achieved tremendous success in this task [25, 26, 31, 32, 40], the speed-accuracy trade-off of the resulting models remains a challenge. This is particularly important for real-time prediction on embedded platforms, whose limited memory and computation power impose strict constraints on the deep network architecture.
To address this, much progress has recently been made to obtain compact deep networks. Existing methods include pruning [1, 2, 13, 22, 38] and quantization [7, 30, 44], both of which aim to reduce the size of an initial deep architecture, as well as knowledge distillation, whose goal is to exploit a deep teacher network to improve the training of a given compact student one. In this paper, we introduce a knowledge distillation approach for object detection.
While early knowledge distillation techniques [18, 33, 36] focused on the task of image classification, several attempts have nonetheless been made for object detection. To this end, existing techniques [5, 12, 39] typically leverage the fact that object detection frameworks consist of three main stages depicted by Figure 1(a): A backbone to extract features; a neck to fuse the extracted features; and heads to predict classes and bounding boxes. Knowledge distillation is then achieved using a teacher with the same architecture as the student but a deeper and wider backbone, such as a Faster RCNN [32] with ResNet152 [14] teacher for a Faster RCNN with ResNet50 student, thus facilitating knowledge transfer at all three stages of the frameworks. To the best of our knowledge, [43] constitutes the only exception to this strategy, demonstrating distillation across different detection frameworks, such as from a RetinaNet [25] teacher to a RepPoints [40] student. This method, however, requires the teacher and the student to rely on a similar detection strategy, i.e., both must be either one-stage detectors or two-stage ones, and, more importantly, still follows a detector-to-detector approach to distillation.
∗The work is done during an internship at NVIDIA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In other words, the study of knowledge distillation remains limited to transfer across architectures tackling the same task. Our classification teacher tackles a different task from the detection student and is trained in a different manner but on the same dataset. Therefore, the classification teacher is capable of providing a different knowledge to the student, for both classification and localization, than that extracted by a detection teacher.
In this paper, we investigate the problem of transferring knowledge not only across architectures but also across tasks. In particular, we observed that the classification head of state-of-the-art object detectors still typically yields inferior performance compared to what can be expected from an image classifier. Thus, as depicted by Figure 1(b), we focus on the scenario where the teacher is an image classifier while the student is an object detector. We then develop distillation strategies to improve both the recognition accuracy and the localization ability of the student.
Our contributions can thus be summarized as follows:
• We introduce the idea of classifier-to-detector knowledge distillation to improve the perfor-mance of a student detector using a classification teacher.
• We propose a distillation method to improve the student’s classification accuracy, applicable when the student uses either a categorical cross-entropy loss or a binary cross-entropy one.
• We develop a distillation strategy to improve the localization performance of the student be exploiting the feature maps from the classification teacher.
We demonstrate the effectiveness of our approach on the COCO2017 benchmark [23] using diverse detectors, including the relatively large two-stage Faster RCNN and single-stage RetinaNet used in previous knowledge distillation works, as well as more compact detectors, such as SSD300,
SSD512 [26] and Faster RCNNs[32] with lightweight backbones. Our classifier-to-detector distil-lation approach outperforms the detector-to-detector distillation ones in the presence of compact students, and helps to further boost the performance of detector-to-detector distillation techniques for larger ones, such as Faster RCNN and RetinaNet with a ResNet50 backbone. Our code is avlaible at: https://github.com/NVlabs/DICOD. 2