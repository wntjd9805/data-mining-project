Abstract
We study private synthetic data generation for query release, where the goal is to construct a sanitized version of a sensitive dataset, subject to differential privacy, that approximately preserves the answers to a large collection of statistical queries.
We ﬁrst present an algorithmic framework that uniﬁes a long line of iterative al-gorithms in the literature. Under this framework, we propose two new methods.
Our ﬁrst method, generative networks with the exponential mechanism (GEM), cir-cumvents computational bottlenecks in algorithms such as MWEM by optimizing over generative models parameterized by neural networks, which capture a rich family of distributions while enabling fast gradient-based optimization. The second method, private entropy projection (PEP), can be viewed as an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. We demonstrate that GEM and PEP empirically outperform existing algorithms. Fur-thermore, we show that GEM nicely incorporates prior information from public data while overcoming limitations of PMWPub, the existing state-of-the-art method that also leverages public data. 1

Introduction
As the collection and analyses of sensitive data become more prevalent, there is an increasing need to protect individuals’ private information. Differential privacy [15] is a rigorous and meaningful criterion for privacy preservation that enables quantiﬁable trade-offs between privacy and accu-racy. In recent years, there has been a wave of practical deployments of differential privacy across organizations such as Google, Apple, and most notably, the U.S. Census Bureau [3].
In this paper, we study the problem of differentially private query release: given a large collection of statistical queries, the goal is to release approximate answers subject to the constraint of differential privacy. Query release has been one of the most fundamental and practically relevant problems in differential privacy. For example, the release of summary data from the 2020 U.S. Decennial
Census can be framed as a query release problem. We focus on the approach of synthetic data generation—that is, generate a privacy-preserving "fake" dataset, or more generally a representation of a probability distribution, that approximates all statistical queries of interest. Compared to simple
Gaussian or Laplace mechanisms that perturb the answers directly, synthetic data methods can provably answer an exponentially larger collection of queries with non-trivial accuracy. However, their statistical advantage also comes with a computational cost. Prior work has shown that achieving better accuracy than simple Gaussian perturbation is intractable in the worst case even for the simple query class of 2-way marginals that release the marginal distributions for all pairs of attributes [38].
∗First two authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite its worst-case intractability, there has been a recent surge of work on practical algorithms for generating private synthetic data. Even though they differ substantially in details, these algorithms share the same iterative form that maintains and improves a probability distribution over the data domain: identifying a small collection of high-error queries each round and updating the distribution to reduce these errors. Inspired by this observation, we present a unifying algorithmic framework that captures these methods. Furthermore, we develop two new algorithms, GEM and PEP, and extend the former to the setting in which public data is available. We summarize our contributions below:
Unifying algorithmic framework. We provide a framework that captures existing iterative algo-rithms and their variations. At a high level, algorithms under this framework maintain a probability distribution over the data domain and improve it over rounds by optimizing a given loss function. We therefore argue that under this framework, the optimization procedures of each method can be reduced to what loss function is minimized and how its distributional family is parameterized. For example, we can recover existing methods by specifying choices of loss functions—we rederive MWEM [21] using an entropy-regularized linear loss, FEM [40] using a linear loss with a linear perturbation, and
DualQuery [19] with a simple linear loss. Lastly, our framework lends itself naturally to a softmax variant of RAP [5], which we show outperforms RAP itself.2
Generative networks with the exponential mechanism (GEM). GEM is inspired by MWEM, which attains worst-case theoretical guarantees that are nearly information-theoretically optimal [11].
However, MWEM maintains a joint distribution over the data domain, resulting in a runtime that is exponential in the dimension of the data. GEM avoids this fundamental issue by optimizing the absolute loss over a set of generative models parameterized by neural networks. We empirically demonstrate that in the high-dimensional regime, GEM outperforms all competing methods.
Private Entropy Projection (PEP). The second algorithm we propose is PEP, which can be viewed as a more advanced version of MWEM with an adaptive and optimized learning rate. We show that PEP minimizes a regularized exponential loss function that can be efﬁciently optimized using an iterative procedure. Moreover, we show that PEP monotonically decreases the error over rounds and empirically ﬁnd that it achieves higher accuracy and faster convergence than MWEM.
Incorporating public data. Finally, we consider extensions of our methods that incorporate prior information in publicly available datasets (e.g., previous data releases from the American Community
Survey (ACS) prior to their differential privacy deployment). While Liu et al. [26] has established
PMWPub as a state-of-the-art method for incorporating public data into private query release, we discuss how limitations of their algorithm prevent PMWPub from effectively using certain public datasets. We then demonstrate empirically that GEM circumvents such issues via simple pretraining, achieving max errors on 2018 ACS data for Pennsylvania (at ε = 1) 9.23x lower than PMWPub when using 2018 ACS data for California as the public dataset. 1.1