Abstract
In online reinforcement learning (RL), efﬁcient exploration remains particularly challenging in high-dimensional environments with sparse rewards.
In low-dimensional environments, where tabular parameterization is possible, count-based upper conﬁdence bound (UCB) exploration methods achieve minimax near-optimal rates. However, it remains unclear how to efﬁciently implement UCB in realistic RL tasks that involve nonlinear function approximation. To address this, we propose a new exploration approach via maximizing the deviation of the occupancy of the next policy from the explored regions. We add this term as an adaptive regularizer to the standard RL objective to trade off between exploration and exploitation.
We pair the new objective with a provably convergent algorithm, giving rise to a new intrinsic reward that adjusts existing bonuses. The proposed intrinsic reward is easy to implement and combine with other existing RL algorithms to conduct exploration. As a proof of concept, we evaluate the new intrinsic reward on tabular examples across a variety of model-based and model-free algorithms, showing improvements over count-only exploration strategies. When tested on navigation and locomotion tasks from MiniGrid and DeepMind Control Suite benchmarks, our approach signiﬁcantly improves sample efﬁciency over state-of-the-art methods.2 1

Introduction
Online RL is a useful tool for an agent to learn how to perform tasks, particularly when expert demon-strations are unavailable and reward information needs to be used instead [92]. To learn a satisfactory policy, an RL agent needs to effectively balance between exploration and exploitation, which remains a central question in RL [23, 15]. Exploration is particularly challenging in environments with sparse rewards. One popular approach to exploration is based on intrinsic motivation, often applied by adding an intrinsic reward (or bonus) to the extrinsic reward provided by the environment. In provable exploration methods, bonus often captures the value estimate uncertainty and the agent takes an action that maximizes the upper conﬁdence bound (UCB) [5, 8, 41, 48, 44]. In tabular setting, UCB bonuses are often constructed based on either Hoeffding’s inequality, which only uses visitation counts, or Bernstein’s inequality, which uses value function variance in addition to visitation counts.
⇤Equal Contribution. 2Our code is available at https://github.com/tianjunz/MADE. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Normalized samples use of different methods with respect to MADE (smaller values are better). MADE consistency achieves a better sample efﬁciency compared to all other baselines.
Inﬁnity means the method fails to achieve maximum reward in given steps.
The latter is proved to be minimax near-optimal in environments with bounded rewards [44, 65] as well as bounded total reward [112] and reward-free settings [64, 49, 45, 113]. It remains an open question how one can efﬁciently compute conﬁdence bounds to construct UCB bonus in non-linear function approximation. Furthermore, Bernstein-style bonuses are often hard to compute in practice beyond tabular setting, due to difﬁculties in computing value function variance.
In practice, various approaches are proposed to design intrinsic rewards: visitation pseudo-count bonuses estimate count-based UCB bonuses using function approximation [10, 15], curiosity-based bonuses seek states where model prediction error is high, uncertainty-based bonuses [77, 87] adopt ensembles of networks for estimating variance of the Q-function, empowerment-based approaches [52, 34, 84, 68] lead the agent to states over which the agent has control, and information gain bonuses [51] reward the agent based on the information gain between state-action pairs and next states.
Although the performance of practical intrinsic rewards is good in certain domains, empirically they are observed to suffer from issues such as detachment, derailment, and catastrophic forgetting [3, 23].
Moreover, these methods usually lack a clear objective and can get stuck in local optimum [3]. Indeed, the impressive performance currently achieved by some deep RL algorithms often revolves around manually designing dense rewards [13], complicated exploration strategies utilizing a signiﬁcant amount of domain knowledge [23], or operating in the known environment regime [88, 69].
Motivated by current practical challenges and the gap between theory and practice, we propose a new algorithm for exploration by maximizing deviation from explored regions. This yields a practical algorithm with strong empirical performance. To be speciﬁc, we make the following contributions: 1. Exploration via maximizing deviation Our approach is based on modifying the standard RL objective (i.e. the cumulative reward) by adding a regularizer that adaptively changes across iterations.
The regularizer can be a general function depending on the state-action visitation density and previous state-action coverage. We show that the regularized objective naturally admits several common existing exploration methods. We then choose a particular regularizer that MAximizes the DEviation (MADE) of the next policy visitation d⇡ from the regions covered by prior policies ⇢k cov:
Lk(d⇡) = J(d⇡) + ⌧k d⇡(s,a) cov(s,a) .
⇢k (1) s,a q
X
Here, k is the iteration number, J(d⇡) is the standard RL objective, and the regularizer encourages d⇡(s, a) to be large when ⇢k cov(s, a) is small. We give an algorithm for solving the regularized objective and prove that with access to an approximate planning oracle, it converges to the global optimum. We show that objective (1) results in an intrinsic reward that can be easily added to any RL algorithm to improve performance, as suggested by our empirical studies. Furthermore, the intrinsic reward applies a simple modiﬁcation to the UCB-style bonus that considers prior visitation counts.
This simple modiﬁcation can also be added to existing bonuses in practice.
In the special case of tabular parameterization, we show that MADE only applies 2. Tabular studies some simple adjustments to the Hoeffding-style count-based bonus. We compare the performance of MADE to Hoeffding and Bernstein bonuses in three different RL algorithms, for the exploration task in the stochastic diabolical bidirectional lock [3, 66], which has sparse rewards and local optima.
Our results show that MADE robustly improves over the Hoeffding bonus and is competitive to the
Bernstein bonus, across all three RL algorithms. Interestingly, MADE bonus and exploration strategy 2
appear to be very close to the Bernstein bonus, without computing or estimating variance, suggesting that MADE potentially captures some environmental structures. Additionally, we empirically show that MADE regularizer can improve the optimization rate in policy gradient methods. 3. Experiments on MiniGrid and DeepMind Control Suite We empirically show that MADE works well when combined with model-free (IMAPLA [25], RAD [55]) and model-based (Dreamer [35]) RL algorithms, greatly improving the sample efﬁciency over existing baselines.
When tested in the procedurally-generated MiniGrid environments, MADE manages to converge with two to ﬁve times fewer samples compared to state-of-the-art method BeBold [111]. In DeepMind
Control Suite [95], we build upon the model-free method RAD [55] and the model-based method
Dreamer [35], improving the return up to 150 in 500K steps compared to baselines. Figure 1 shows normalized sample size to achieve maximum reward with respect to our algorithm. 2