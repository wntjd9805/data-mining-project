Abstract
We study reinforcement learning (RL) with linear function approximation under the adaptivity constraint. We consider two popular limited adaptivity models: the batch learning model and the rare policy switch model, and propose two efﬁcient online RL algorithms for episodic linear Markov decision processes, where the transition probability and the reward function can be represented as a linear function of some known feature mapping. In speciﬁc, for the batch learning model, our d3H 3T + dHT /B) regret, proposed LSVI-UCB-Batch algorithm achieves an (cid:101)O( where d is the dimension of the feature mapping, H is the episode length, T is the number of interactions and B is the number of batches. Our result suggests that it sufﬁces to use only (cid:112)T /dH batches to obtain (cid:101)O( d3H 3T ) regret. For the rare policy switch model, our proposed LSVI-UCB-RareSwitch algorithm enjoys an (cid:101)O((cid:112)d3H 3T [1 + T /(dH)]dH/B) regret, which implies that dH log T policy d3H 3T ) regret. Our algorithms achieve the switches sufﬁce to obtain the (cid:101)O( same regret as the LSVI-UCB algorithm (Jin et al., 2020), yet with a substantially smaller amount of adaptivity. We also establish a lower bound for the batch learning model, which suggests that the dependency on B in our regret bound is tight.
√
√
√ 1

Introduction
Real-world reinforcement learning (RL) applications often come with possibly inﬁnite state and action space, and in such a situation classical RL algorithms developed in the tabular setting are not applicable anymore. A popular approach to overcoming this issue is by applying function approximation techniques to the underlying structures of the Markov decision processes (MDPs).
For example, one can assume that the transition probability and the reward are linear functions of a known feature mapping φ : S × A → Rd, where S and A are the state space and action space, and d is the dimension of the embedding. This gives rise to the so-called linear MDP model (Yang and
Wang, 2019; Jin et al., 2020). Assuming access to a generative model, efﬁcient algorithms under
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
√ this setting have been proposed by Yang and Wang (2019) and Lattimore et al. (2020). For online
ﬁnite-horizon episodic linear MDPs, Jin et al. (2020) proposed an LSVI-UCB algorithm that achieves d3H 3T ) regret, where H is the planning horizon (i.e., length of each episode) and T is the (cid:101)O( number of interactions.
However, all the aforementioned algorithms require the agent to update the policy in every episode. In practice, it is often unrealistic to frequently switch the policy in the face of big data, limited computing resources as well as inevitable switching costs. Thus one may want to batch the data stream and update the policy at the end of each period. For example, in clinical trials, each phase (batch) of the trial amounts to applying a medical treatment to a batch of patients in parallel. The outcomes of the treatment are not observed until the end of the phase and will be subsequently used to design experiments for the next phase. Choosing the appropriate number and sizes of the batches is crucial to achieving nearly optimal efﬁciency for the clinical trial. This gives rise to the limited adaptivity setting, which has been extensively studied in many online learning problems including prediction-from-experts (PFE) (Kalai and Vempala, 2005; Cesa-Bianchi et al., 2013), multi-armed bandits (MAB) (Arora et al., 2012; Cesa-Bianchi et al., 2013) and online convex optimization (Jaghargh et al., 2019; Chen et al., 2020), to mention a few. Nevertheless, in the RL setting, learning with limited adaptivity is relatively less studied. Bai et al. (2019) introduced two notions of adaptivity in RL, local switching cost and global switching cost, that are deﬁned as follows
Nlocal =
K−1 (cid:88)
H (cid:88) (cid:88) k=1 h=1 s∈S 1{πk h(s) (cid:54)= πk+1 h (s)} and Nglobal =
K−1 (cid:88) k=1 1{πk (cid:54)= πk+1}, (1.1) h : S → A}h∈[H] is the policy for the k-th episode of the MDP, πk (cid:54)= πk+1 means where πk = {πk h(s) (cid:54)= πk+1 that there exists some (h, s) ∈ [H] × S such that πk (s), and K is the number of episodes.
H 3SAT )
Then they proposed a Q-learning method with UCB2H exploration that achieves (cid:101)O( regret with O(H 3SA log(T /(AH)) local switching cost for tabular MDPs, but they did not provide tight bounds on the global switching cost.
√ h
In this paper, based on the above motivation, we aim to develop online RL algorithms with linear function approximation under adaptivity constraints. In detail, we consider time-inhomogeneous2 episodic linear MDPs (Jin et al., 2020) where both the transition probability and the reward function are unknown to the agent. In terms of the limited adaptivity imposed on the agent, we consider two scenarios that have been previously studied in the online learning literature (Perchet et al., 2016;
Abbasi-Yadkori et al., 2011): the batch learning model and the rare policy switch model. More speciﬁcally, in the batch learning model (Perchet et al., 2016), the agent is forced to pre-determine the number of batches (or equivalently batch size). Within each batch, the same policy is used to select actions, and the policy is updated only at the end of this batch. The amount of adaptivity in the batch learning model is measured by the number of batches, which is expected to be as small as possible. In contrast, in the rare policy switch model (Abbasi-Yadkori et al., 2011), the agent can adaptively choose when to switch the policy and therefore start a new batch in the learning process as long as the total number of policy updates does not exceed the given budget on the number of policy switches. The amount of adaptivity in the rare policy switch model can be measured by the number of policy switches, which turns out to be the same as the global switching cost introduced in Bai et al. (2019). It is worth noting that for the same amount of adaptivity3, the rare policy switch model can be seen as a relaxation of the batch learning model since the agent in the batch learning model can only change the policy at pre-deﬁned time steps. In our work, for each of these limited adaptivity models, we propose a variant of the LSVI-UCB algorithm (Jin et al., 2020), which can be viewed as an RL algorithm with full adaptivity in the sense that it switches the policy at a per-episode scale.
Our algorithms can attain the same regret as LSVI-UCB, yet with a substantially smaller number of batches/policy switches. This enables parallel learning and improves the large-scale deployment of
RL algorithms with linear function approximation.
The main contributions of this paper are summarized as follows: 2We say an episodic MDP is time-inhomogeneous if its reward and transition probability are different at different stages within each episode. See Deﬁnition 3.2 for details. 3The number of batches in the batch learning model is comparable to the number of policy switches in the rare policy switch model. 2
√
• For the batch learning model, we propose an LSVI-UCB-Batch algorithm for linear MDPs and d3H 3T + dHT /B) regret, where d is the dimension of the feature show that it enjoys an (cid:101)O( mapping, H is the episode length, T is the number of interactions and B is the number of batches.
Our result suggests that it sufﬁces to use only (cid:112)T /dH batches, rather than T batches, to obtain the d3H 3T ) achieved by LSVI-UCB (Jin et al., 2020) in the fully sequential decision same regret (cid:101)O( model. We also prove a lower bound of the regret for this model, which suggests that the required number of batches (cid:101)O(
T ) is sharp.
√
√
• For the rare policy switch model, we propose an LSVI-UCB-RareSwitch algorithm for linear
MDPs and show that it enjoys an (cid:101)O((cid:112)d3H 3T [1 + T /(dH)]dH/B) regret, where B is the number of policy switches. Our result implies that dH log T policy switches are sufﬁcient to obtain the d3H 3T ) achieved by LSVI-UCB. The number of policy switches is much smaller same regret (cid:101)O( than that4 of the batch learning model when T is large.
√ d3H 3T ) regret
Concurrent to our work, Gao et al. (2021) proposed an algorithm achieving (cid:101)O( with a O(dH log K) global switching cost in the rare policy switch model. They also proved a
Ω(dH/ log d) lower bound on the global switching cost. The focus of our paper is different from theirs: our goal is to design efﬁcient RL algorithms under a switching cost budget B, while their goal is to achieve the optimal rate in terms of T with as little switching cost as possible. On the other hand, for the rare policy switch model, our proposed algorithm (LSVI-UCB-RareSwitch) along its regret bound can imply their results by optimizing our regret bound concerning the switching cost budget
B.
√
The rest of the paper is organized as follows. In Section 2 we discuss previous works related to this paper, with a focus on RL with linear function approximation and online learning with limited adaptivity. In Section 3 we introduce necessary preliminaries for MDPs and adaptivity constraints.
Sections 4 and 5 present our proposed algorithms and the corresponding theoretical results for the batch learning model and the rare policy switch model respectively. In Section 6 we present the numerical experiment which supports our theory. Finally, we conclude our paper and point out a future direction in Section 7.
√
Notation We use lower case letters to denote scalars and use lower and upper case boldface letters to denote vectors and matrices respectively. For any real number a, we write [a]+ = max(a, 0).
For a vector x ∈ Rd and matrix Σ ∈ Rd×d, we denote by (cid:107)x(cid:107)2 the Euclidean norm and deﬁne x(cid:62)Σx. For any positive integer n, we denote by [n] the set {1, . . . , n}. For any ﬁnite set (cid:107)x(cid:107)Σ =
A, we denote by |A| the cardinality of A. For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn, and we write an = Ω(bn) if there exists an absolute constant C such that an ≥ Cbn. We use (cid:101)O(·) to further hide the logarithmic factors. 2