Abstract
With the rapid development of deep reinforcement learning (DRL) techniques, there is an increasing need to understand and interpret DRL policies. While recent research has developed explanation methods to interpret how an agent determines its moves, they cannot capture the importance of actions/states to a game’s final result. In this work, we propose a novel self-explainable model that augments a
Gaussian process with a customized kernel function and an interpretable predictor.
Together with the proposed model, we also develop a parameter learning procedure that leverages inducing points and variational inference to improve learning effi-ciency. Using our proposed model, we can predict an agent’s final rewards from its game episodes and extract time step importance within episodes as strategy-level explanations for that agent. Through experiments on Atari and MuJoCo games, we verify the explanation fidelity of our method and demonstrate how to employ inter-pretation to understand agent behavior, discover policy vulnerabilities, remediate policy errors, and even defend against adversarial attacks. 1

Introduction
Deep reinforcement learning has shown great success in automatic policy learning for various sequential decision-making problems, such as training AI agents to defeat professional players in sophisticated games [74, 65, 24, 37] and controlling robots to accomplish complicated tasks [33, 38]. However, existing DRL agents make decisions in an opaque fashion, taking actions without accompanying explanations. This lack of transparency creates key barriers to establishing trust in an agent’s policy and scrutinizing policy weakness. This issue significantly limits the applicability of
DRL techniques in critical application fields (e.g., finance [47] and self-driving cars [11]).
To tackle this limitation, prior research (e.g., [9, 13, 73]) proposes to derive an explanation for a target agent’s action at a specific time step. Technically, this explanation can be obtained by pinpointing the features within the agent’s observation of a particular state that contribute most to its corresponding action at that state. Despite demonstrating great potential to help users understand a target agent’s
∗Equal Contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
individual actions, they lack the capability to extract insights into the overall policy of that agent. In other words, existing methods cannot shed light on the general sensitivity of an agent’s final reward from a game in regards to the actions/states in that game episode. Consequently, these methods fall short in troubleshooting an agent’s policy’s weaknesses when it fails its task.
We propose a novel explanation method to derive strategy-level interpretations of a DRL agent. As we discuss later in Section 3, we define such interpretations as the identification of critical time steps contributing to a target agent’s final reward from each game episode. At a high level, our method identifies the important time steps by approximating the target agent’s decision-making process with a self-explainable model and extracting the explanations from this model. Specifically, given a well-trained DRL agent, our method first collects a set of episodes and the corresponding final rewards of this agent. Then, it fits a self-explainable model to predict final rewards from game episodes. To model the unique correlations in DRL episodes and enable high-fidelity explanations, rather than simply applying off-the-shelf self-explanation techniques, we develop a novel self-explainable model that integrates a series of new designs. First, we augment a Gaussian Process (GP) with a customized deep additive kernel to capture not only correlations between time steps but, more importantly, the joint effect across episodes. Second, we combine this deep GP model with our newly designed explainable prediction model to predict the final reward and extract the time step importance. Third, we develop an efficient inference and learning framework for our model by leveraging inducing points and variational inference. We refer to our method as “Strategy-level Explanation of Drl aGEnts” (for short EDGE). 2
With extensive experiments on three representative RL games, we demonstrate that EDGE outperforms alternative interpretation methods in terms of explanation fidelity. Additionally, we demonstrate how
DRL policy users and developers can benefit from EDGE. Specifically, we first show that EDGE could help understand the agent’s behavior and establish trust in its policy. Second, we demonstrate that guided by the insights revealed from our explanations, an attacker could launch efficient adversarial attacks to cause a target agent to fail. Third, we demonstrate how, with EDGE’s capability, a model developer could explain why a target agent makes mistakes. This allows the developer to explore a remediation policy following the explanations and using it to enhance the agent’s original policy.
Finally, we illustrate that EDGE could help develop a defense mechanism against a newly emerging adversarial attack on DRL agents. To the best of our knowledge, this is the first work that interprets a DRL agent’s policy by identifying the most critical time steps to the agent’s final reward in each episode. This is also the first work that demonstrates how to use an explanation to understand agent behavior, discover policy vulnerabilities, patch policy errors, and robustify victim policies. 2