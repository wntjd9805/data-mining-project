Abstract
When facing uncertainty, decision-makers want predictions they can trust. A ma-chine learning provider can convey conﬁdence to decision-makers by guaranteeing their predictions are distribution calibrated — amongst the inputs that receive a predicted class probabilities vector q, the actual distribution over classes is q. For multi-class prediction problems, however, achieving distribution calibration tends to be infeasible, requiring sample complexity exponential in the number of classes
C. In this work, we introduce a new notion—decision calibration—that requires the predicted distribution and true distribution to be “indistinguishable” to a set of downstream decision-makers. When all possible decision makers are under consideration, decision calibration is the same as distribution calibration. However, when we only consider decision makers choosing between a bounded number of actions (e.g. polynomial in C), our main result shows that decisions calibration becomes feasible — we design a recalibration algorithm that requires sample com-plexity polynomial in the number of actions and the number of classes. We validate our recalibration algorithm empirically: compared to existing methods, decision calibration improves decision-making on skin lesion and ImageNet classiﬁcation with modern neural network predictors. 1

Introduction
Machine learning predictions are increasingly employed by downstream decision makers who have little or no visibility on how the models were designed and trained. In high-stakes settings, such as healthcare applications, decision makers want predictions they can trust. For example in healthcare, suppose a machine learning service offers a supervised learning model to healthcare providers that claims to predict the probability of various skin diseases, given an image of a lesion. Each healthcare provider want assurance that the model’s predictions lead to beneﬁcial decisions, according to their own loss functions. As a result, the healthcare providers may reasonably worry that the model was trained using a loss function different than their own. This mismatch is often inevitable because the
ML service may provide the same prediction model to many healthcare providers, which may have different treatment options available and loss functions. Even the same healthcare provider could have different loss functions throughout time, due to changes in treatment availability.
If predicted probabilities perfectly equal the true probability of the event, this issue of trust would not arise because they would lead to optimal decision making regardless of the loss function or task considered by downstream decision makers. In practice, however, predicted probabilities are never perfect. To address this, the healthcare providers may insist that the prediction function be 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
distribution calibrated, requiring that amongst the inputs that receive predicted class probability vectors q, the actual distribution over classes is q. This solves the trust issue because among the patients who receive prediction q, the healthcare providers knows that the true label distribution is q, and hence knows the true expected loss of a treatment on these patients. Unfortunately, to achieve distribution calibration, we need to reason about the set of individuals x who receive prediction q, for every possible predicted q. As the number of distinct predictions may naturally grow exponentially in the number of classes C, the amount of data needed to accurately certify distribution calibration tends to be prohibitive. Due to this statistical barrier, most work on calibrated multi-class predictions focuses on obtaining relaxed variants of calibration. These include conﬁdence calibration (Guo et al., 2017), which calibrates predictions only over the most likely class, and classwise calibration (Kull et al., 2019), which calibrates predictions for each class marginally. While feasible, these notions are signiﬁcantly weaker than distribution calibration and do not address the trust issue highlighted above.
Is there a calibration notion that addresses the issue of trust, but can also be veriﬁed and achieved efﬁciently? Our paper answers this question afﬁrmatively.
Our Contributions. We introduce a new notion of calibration—decision calibration—where we take the perspective of potential decision-makers: the only differences in predictions that matter are those that could lead to different decisions. Inspired by Dwork et al. (2021), we formalize this intuition by requiring that predictions are “indistinguishable” from the true outcomes, according to a collection of decision-makers.
First, we show that prior notions of calibration can be characterized as special cases of decision calibration under different collections of decision-makers. This framing explains the strengths and weakness of existing notions of calibration, and clariﬁes the guarantees they offer to decision makers. For example, we show that a predictor is distribution calibrated if and only if it is decision calibrated with respect to all loss functions and decision rules. This characterization demonstrates why distribution calibration is so challenging: achieving distribution calibration requires simultaneously reasoning about all possible decision tasks.
The set of all decision rules include those that choose between exponentially (in number of classes
C) many actions. In practice, however, decision-makers typically choose from a bounded (or slowly-growing as a function of C) set of actions. Our main contribution is an algorithm that guarantees decision calibration for such more realistic decision-makers. In particular, we give a sample-efﬁcient algorithm that takes a pre-trained predictor and post-processes it to achieve decision calibration with respect to all decision-makers choosing from a bounded set of actions. Our recalibration procedure does not harm other common performance metrics, and actually improves accuracy and likelihood of the predictions. In fact, we argue formally that, in the setting of bounded actions, optimizing for decision calibration recovers many of the beneﬁts of distribution calibration, while drastically improving the sample complexity. Empirically, we use our algorithm to recalibrate deep network predictors on two large scale datasets: skin lesion classiﬁcation (HAM10000) and Imagenet.
Our recalibration algorithm improves decision making, and allow for more accurate decision loss estimation compared to existing recalibration methods even under distribution shift. 2