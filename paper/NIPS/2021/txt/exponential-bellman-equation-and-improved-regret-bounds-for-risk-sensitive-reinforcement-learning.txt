Abstract
We study risk-sensitive reinforcement learning (RL) based on the entropic risk measure. Although existing works have established non-asymptotic regret guar-antees for this problem, they leave open an exponential gap between the upper and lower bounds. We identify the deﬁciencies in existing algorithms and their analysis that result in such a gap. To remedy these deﬁciencies, we investigate a simple transformation of the risk-sensitive Bellman equations, which we call the exponential Bellman equation. The exponential Bellman equation inspires us to develop a novel analysis of Bellman backup procedures in risk-sensitive RL algorithms, and further motivates the design of a novel exploration mechanism.
We show that these analytic and algorithmic innovations together lead to improved regret upper bounds over existing ones. 1

Introduction
Risk-sensitive reinforcement learning (RL) is important for practical and high-stake applications, such as self-driving and robotic surgery. In contrast with standard and risk-neutral RL, it optimizes some risk measure of cumulative rewards instead of their expectation. One foundational framework for risk-sensitive RL maximizes the entropic risk measure of the reward, which takes the form of
V π = 1
β log{Eπ[eβR]}, with respect to the policy π, where β (cid:54)= 0 is a given risk parameter and R denotes the cumulative rewards.
Recently, the works of [20, 21] investigate the online setting of the above risk-sensitive RL problem.
Under K-episode MDPs with horizon length of H, they propose two model-free algorithms, namely
RSVI and RSQ, and prove that their algorithms achieve the regret upper bound (with its informal form given by)
Regret(K) (cid:46) e|β|H 2
· e|β|H − 1
|β|H (cid:112) poly(H) · K without assuming knowledge of the transition distribution or access to a simulator. They also provide a lower bound (informally presented as)
Regret(K) (cid:38) e|β|H (cid:48)
|β|H
− 1 (cid:112) poly(H) · K that any algorithm has to incur, where H (cid:48) is a linear function of H. Despite the non-asymptotic nature of their results, it is not hard to see that a wide gap exists between the two bounds. Speciﬁcally, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the upper bound has an additional e|β|H 2 factor is dominating in the upper bound since the quadratic exponent in e|β|H 2 larger than e|β|H −1 is intrinsic in the upper bound. factor compared to the lower bound, and even worse, this makes it exponentially
|β|H even for moderate values of |β| and H. It is unclear whether the factor of e|β|H 2
In this paper, we show that the additional factor in the upper bound is not intrinsic for the upper bound and can be eliminated by a reﬁned algorithmic design and analysis. We identify two deﬁciencies in the existing algorithms and their analysis: (1) the main element of the analysis follows existing analysis of risk-neutral RL algorithms, which fails to exploit the special structure of the Bellman equations of risk-sensitive RL; (2) the existing algorithms use an excessively large bonus that results in the exponential blow-up in the regret upper bound.
To address the above shortcomings, we consider a simple transformation of the Bellman equations analyzed so far in the literature, which we call the exponential Bellman equation. A distinctive feature of the exponential Bellman equation is that they associate the instantaneous reward and value function of the next step in a multiplicative way, rather than in an additive way as in the standard Bellman equations. From the exponential Bellman equation, we develop a novel analysis of the Bellman backup procedure for risk-sensitive RL algorithms that are based on the principle of optimism. The analysis further motivates a novel exploration mechanism called doubly decaying bonus, which helps the algorithms adapt to their estimation error over each horizon step while at the same time exploring efﬁciently. These discoveries enable us to propose two model-free algorithms for RL with the entropic risk measure based on the novel bonus. By combining the new analysis and bonus design, we prove that the preceding algorithms attain nearly optimal regret bounds under episodic and ﬁnite-horizon
MDPs. Compared to prior results, our regret bounds feature an exponential improvement with respect to the horizon length and risk parameter, removing the factor of e|β|H 2 from existing upper bounds.
This signiﬁcantly narrows the gap between upper bounds and the existing lower bound of regret.
In summary, we make the following theoretical contributions in this paper. 1. We investigate the gap between existing upper and lower regret bounds in the context of risk-sensitive RL, and identify deﬁciencies of the existing algorithms and analysis; 2. We consider the exponential Bellman equation, which inspires us to propose a novel analysis of the Bellman backup procedure for RL algorithms based on the entropic risk measure. It further motivates a novel bonus design called doubly decaying bonus. We then design two model-free risk-sensitive RL algorithms equipped with the novel bonus. 3. The novel analytic framework and bonus design together enable us to prove that the preced-ing algorithms achieve nearly optimal regret bounds, which improve upon existing ones by an exponential factor in terms of the horizon length and risk sensitivity. 2