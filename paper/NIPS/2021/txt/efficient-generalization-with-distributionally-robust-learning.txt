Abstract
Distributionally robust learning (DRL) is increasingly seen as a viable method to train machine learning models for improved model generalization. These minimax formulations, however, are more difﬁcult to solve. We provide a new stochastic gradient descent algorithm to efﬁciently solve this DRL formulation. Our approach applies gradient descent to the outer minimization formulation and estimates the gradient of the inner maximization based on a sample average approximation. The latter uses a subset of the data sampled without replacement in each iteration, pro-gressively increasing the subset size to ensure convergence. We rigorously establish convergence to a near-optimal solution under standard regularity assumptions and, for strongly convex losses, match the best known O((cid:15)−1) rate of convergence up to a known threshold. Empirical results demonstrate the signiﬁcant beneﬁts of our approach over previous work in improving learning for model generalization. 1

Introduction
Consider a general formulation of the distributionally robust learning (DRL) problem of active interest. Let X denote a sample space, P denote a probability distribution on X, and Θ ⊆ Rd denote a parameter space. Let us deﬁne LP (θ) := EP [l(θ, ξ)] to be the expectation with respect to (w.r.t.) P of a loss function l : Θ × X → R representing the estimation error for a learning model with parameters θ ∈ Θ over data ξ ∈ X. Further deﬁne the worst-case expected loss function R(θ) := EP ∗(θ)[l(θ, ξ)] = supP ∈P {LP (θ)}, which maximizes the loss LP over a well-deﬁned set of measures P. Letting Pb denote a base distribution, this set often takes the form
P = {P | D(P, Pb) ≤ ρ, (cid:82) dP (ξ) = 1, P (ξ) ≥ 0} where D(·, ·) is a distance function on the space of probability distributions on X and the constraints limit the feasible candidates to be within a distance of ρ from Pb. We seek to ﬁnd parameters θ ∈ Θ that solve the DRL problem formulated as (cid:111) (cid:110) (cid:110)
R(θ∗ rob) = min
θ∈Θ
R(θ)
= min
θ∈Θ
{LP (θ)} sup
P ∈P (cid:111)
, (1) for a given X and P. In practice, training with DRL amounts to dynamically reweighing the data using the inner optima P ∗(θ) at any value the parameters θ take over the space Θ. The inner maximization problem sets these weights to emphasize data that experience high loss at θ. This reweighing approach arises from solid theoretical foundations and can provide strong guarantees on model generalization.
DRL and Model Generalization: In machine learning, optimal values for the model parameters
θ are calculated from a ﬁnite training dataset (of size N ) and this model is then used for inference over other test datasets, all of which are typically assumed to be identically distributed [33, 34].
The equal-weight empirical distribution UN = {1/N } over the ﬁnite training dataset is the non-parametric maximum likelihood estimator [23] of the (unknown) distribution underlying the datasets. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Let θ∗ erm denote the minimizer of the empirical loss LUN (·) over Θ. In real-world applications, any two ﬁnite datasets sampled from the same underlying distribution can violate the identical distribution assumption, leading to poor generalization when using θ∗ erm over other datasets [24]. Popular model selection techniques, such as cross-validation [32], seek to improve the estimation error between training and testing datasets, but they can be computationally prohibitive and lack rigorous guarantees.
With roots in non-parametric statistics [23] and optimization [28, 7], several studies [20, 2, 7, 21, 19] have proposed as an alternative approach the DRL formulation (1) using the empirical distribution
UN over the ﬁnite training dataset as the base distribution Pb. This alternative approach explicitly treats the ambiguity in the identity of the true (stationary but unknown) distribution, denoted by
P0. In general, we know that UN is not equal to P0 and it is highly likely that, at θ∗ rob, the worst-case distribution P ∗(θ∗ rob) is not equal to P0. However, for Wasserstein distance metrics and an appropriately chosen value of ρ in the set of measures P, Blanchet et al. [2] show that P contains a distribution P whose relevant loss function characteristics are the same as those for the true (unknown) distribution P0 with high probability. A broad guideline is provided in [2, 21] such that these high probability guarantees are achieved by setting ρ = O((cid:112)d/N ) for binary classiﬁcation with logistic models. The DRL approach thus holds great promise and a general theory is actively being pursued.
Goal: Our main objective is to solve the DRL problem (1) in an efﬁcient manner to ensure it is a viable alternative approach for model generalization. The primary difﬁculty is the minimax formulation, particularly the inner maximization problem. While its solution may be explicitly available in some cases – e.g., constraining P by certain Wasserstein distance metrics admits an explicit characterization of the robust objective function EP ∗(θ)[l(θ, ξ)] [2, 30, 8, 4, 5] – these reductions do not hold in general for all interesting Wasserstein distance metrics and they require solving a convex non-linear program [5]. Namkoong and Duchi [21] show that the inner maximization can be efﬁciently solved (see Section 2) under χ2-divergence constraints. Hence, we focus on the entire general class of
φ-divergence distance functions: Dφ(P, Pb) = EPb [φ( dP
)], where φ(s) is a non-negative convex dPb function having the value 0 only at s = 1. Members of this class include the modiﬁed χ2 divergence, with φ(s) = (s−1)2, and the Kullback-Leibler (KL) divergence, with φ(s) = s log s−s+1. Deﬁning the vector P := (pn) of dimension N and setting the base Pb to the uniform empirical distribution
UN , we then have that the loss function and constraint set P are given by LP (θ) = (cid:80)N n=1 pnl(θ, ξn) and P = {P | (cid:80)N