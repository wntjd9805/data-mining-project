Abstract
Reinforcement learning (RL) promises to enable autonomous acquisition of com-plex behaviors for diverse agents. However, the success of current reinforcement learning algorithms is predicated on an often under-emphasised requirement – each trial needs to start from a ﬁxed initial state distribution. Unfortunately, resetting the environment to its initial state after each trial requires substantial amount of human supervision and extensive instrumentation of the environment which defeats the goal of autonomous acquisition of complex behaviors. In this work, we propose
Value-accelerated Persistent Reinforcement Learning (VaPRL), which generates a curriculum of initial states such that the agent can bootstrap on the success of easier tasks to efﬁciently learn harder tasks. The agent also learns to reach the initial states proposed by the curriculum, minimizing the reliance on human interventions into the learning. We observe that VaPRL reduces the interventions required by three orders of magnitude compared to episodic RL while outperforming prior state-of-the art methods for reset-free RL both in terms of sample efﬁciency and asymptotic performance on a variety of simulated robotics problems1. 1

Introduction
Reinforcement learning (RL) offers an appealing opportunity to enable autonomous acquisition of complex behaviors for interactive agents. Despite recent RL successes on robots [26, 34, 25, 28, 35, 22, 32, 23, 14], several challenges exist that inhibit wider adoption of reinforcement learning for robotics [48]. One of the major challenges to the autonomy of current reinforcement learning algorithms, particularly in robotics, is the assumption that each trial starts from an initial state drawn from a speciﬁc state distribution in the environment. Conventionally, reinforcement learning algorithms assume the ability to arbitrarily sample and reset to states drawn from this distribution, making such algorithms impractical for most real-world setups.
Many prior examples of reinforcement learning on real robots have relied on extensive instrumentation of the robotic setup and human supervision to enable environment resets to this initial state distribution.
This is accomplished through a human providing the environment reset themselves throughout the training [8, 12, 4], scripted behaviors for the robot to reset the environment [28, 39], an additional robot executing scripted behavior to reset the environment [32], or engineered mechanical contraptions
[46, 23]. The additional instrumentation of the environment and creating scripted behaviors are both time-intensive and often require additional resources such as sensors or even robots. The scripted reset behaviors are narrow in application, often designed for just a single task or environment, and their brittleness mandates human oversight of the learning process. Eliminating or minimizing the algorithmic reliance on the reset mechanisms can enable more autonomous learning, and in turn it will allow agents to scale to broader and harder set of tasks. 1Code and supplementary videos are available at https://sites.google.com/view/vaprl/home 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Comparison of the persistent RL setting with the episodic RL setting. Interventions (human or otherwise orchestrated) reset the environment to the initial state distribution after every episode in episodic RL, while the state of the environment persists through the training in persistent RL. The learned policy is tested starting from the initial state distribution for both the settings.
To address these challenges, some recent works have developed reinforcement learning algorithms that can effectively learn with minimal resets to the initial distribution [19, 6, 48, 43, 14]. We provide a formal problem deﬁnition that encapsulates and sheds light on the general setting addressed by these prior methods, which we refer to as the persistent reinforcement learning in this work. In this problem setting, we disentangle the training and the test time settings such that the test-time objective matches that of the conventional RL setting but the train-time setting restricts access to the initial state distribution by giving a low frequency periodic reset. In this setting, the agent must persistently learn and interact with the environment with minimal human interventions, as shown in Figure 1.
Conventional episodic RL algorithms often fail to solve the task entirely in this setting, as shown by Zhu et al. [48] and Figure 2. This is because these methods rely on the ability to sample the initial state distribution arbitrarily. One solution to this problem is to additionally learn a reset policy that recovers the initial state distribution [19, 6] allowing the agent to repeatedly alternate between practicing the task and practicing the reverse. Unfortunately, not only can solving the task directly from the initial state distribution be hard from an exploration standpoint, but (attempting to) return to the initial state repeatedly can be sample inefﬁcient. In this paper, we propose to instead have the agent reset itself to and attempt the task from different initial states along the path to the goal state. In particular, the agent can learn to solve the task from easier starting states that are closer to the goal and bootstrap on these to solve the task from harder states farther away from the goal.
The main contribution of this work is Value-accelerated Persistent
Reinforcement Learning (VaPRL), a goal-conditioned RL method that creates an adaptive curriculum of starting states for the agent to efﬁciently improve test-time performance while substantially re-ducing the reliance on extrinsic reset mechanisms. Additionally, we provide a formal description of the persistent RL problem setting to conceptualize our work and prior methods. We benchmark VaPRL on several robotic control tasks in the persistent RL setting against state-of-the-art methods, which either simulate the initial state dis-tribution by learning a reset controller, or incrementally grow the state-space from which the given task can be solved. Our experi-ments indicate that using a tailored curriculum generated by VaPRL can be up to 30% more sample-efﬁcient in acquiring task behaviors compared to these prior methods. For the most challenging dexterous manipulation problem, VaPRL provides a 2.5× gain in performance compared to the next best performing method.
Figure 2: The performance of episodic RL algorithms substan-tially deteriorates when environ-ment resets are not available. 2