Abstract
√
Kernel k-means is one of the most popular approaches to clustering and its the-oretical properties have been investigated for decades. However, the existing state-of-the-art risk bounds are of order O(k/ n), which do not match with the stated lower bound Ω((cid:112)k/n) in terms of k, where k is the number of clusters and n is the size of the training set. In this paper, we study the statistical properties of kernel k-means and Nyström-based kernel k-means, and obtain optimal clustering risk bounds, which improve the existing risk bounds. Particularly, based on a reﬁned upper bound of Rademacher complexity [21], we ﬁrst derive an optimal risk bound of rate O((cid:112)k/n) for empirical risk minimizer (ERM), and further extend it to general cases beyond ERM. Then, we analyze the statistical effect of computa-tional approximations of Nyström kernel k-means, and prove that it achieves the same statistical accuracy as the original kernel k-means considering only Ω( nk)
Nyström landmark points. We further relax the restriction of landmark points from n) under a mild condition. Finally, we validate the theoretical
Ω(
ﬁndings via numerical experiments. nk) to Ω(
√
√
√ 1

Introduction
Clustering, a fundamental data mining task, is used in numerous applications including web search, medical imaging, gene expression analysis, social network analysis and recommendation systems
[56, 55, 23, 42]. k-means is arguably one of the most popular approaches to clustering, producing clusters with piece-wise linear boundaries. Its kernel version, which employs a nonlinear distance function, has the ability to ﬁnd clusters of varying densities and distributions, greatly improving the
ﬂexibility of the approach [18, 53, 38, 37, 39, 60, 29, 36, 35, 61].
To understand (kernel) k-means and guide the development of new clustering algorithms, researchers have investigated its theoretical properties for decades. The consistency of the empirical minimizer was demonstrated by [45, 47, 1]. Rates of convergence and non-asymptotic performance bounds were considered by [46, 13, 33, 7, 32, 14, 20]. Most of the proposed risk bounds are dependent upon the dimension of the hypothesis space. For example, Bartlett et al. [7] provided, under certain mild assumptions, a clustering risk bound of order O((cid:112)kd/n), where d is the dimension of the hypothesis space and n is the size of the training set. However, the hypothesis space of kernel k-means is typically an inﬁnite-dimensional Hilbert space, such as the reproducing kernel Hilbert space (RKHS) associated with Gaussian kernels [50]. Thus, the existing theoretical analysis of k-means are not usually suitable for explaining its kernel version. Recently, [20, 16, 10, 41, 3, 27, 24, 9] extended the previous results, and provided dimension-independent bounds for kernel k-means. As shown in
[16], if the feature map associated with the kernel function satisﬁes (cid:107)Φ(cid:107) ≤ 1, then the clustering risk n). These clustering risk bounds for kernel k-means are usually linearly bounds are of order O(k/
√ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dependent on the number of clusters k. However, the number of clusters k may be very large in some domains, such as social networks and recommendation systems. Thus, from a theoretical perspective, these existing bounds of O(k/ n) do not match with the stated lower bound Ω((cid:112)k/n) in k [7].
√
Although kernel k-means is one of the most popular clustering methods, it requires the computation of a n × n kernel matrix. As for other kernel methods, this becomes unfeasible for large-scale problems, and thus deriving approximate computations, such as partial decompositions [6, 28], random projection [16, 15], Nyström approximations [19, 11, 9, 43, 53, 58, 57], and random feature approximations [48, 12, 5, 49, 34, 31], has become the subject of numerous recent works. However, few of these optimization-based methods focused on the underlying excess risk problem. To the best of our knowledge, the only two results providing excess risk guarantees for approximate kernel k-means are [16] and [9]. In [16], Devroye and Lugosi considered the excess clustering risk when the approximate Hilbert space is obtained using Gaussian projections. In [9], Calandriello and Rosasco showed that, when sampling Ω( n).
The excess risk bounds of [9] and [16] are both linearly dependent on k and thus do not match with the theoretical lower bound [7]. n) Nyström landmarks, the excess risk bound can reach O(k/
√
√
√
In the recent work [21], the authors showed that the Rademacher complexity of the k-valued func-tion class of Lipschitz continuity with respect to the L∞ norm can be bounded by the maximum
Rademacher complexity of the restriction of the function class along each coordinate, times a factor of
O( k). Although it may be not very difﬁcult to use the result of [21] for kernel k-means, the optimal bound of O((cid:112)k/n) for kernel k-means has never been given before. Moreover, we creatively extend the results of kernel k-means to the approximate one. Our major contributions include two parts: 1) A (nearly) optimal excess clustering risk bound of rate ˜O((cid:112)k/n)1 is proposed for empirical risk minimization (ERM) (see Theorem 1). To the best of our knowledge, this is the ﬁrst (nearly) optimal excess risk bound for kernel k-means in terms of both k and n. Beyond
ERM, we further extend the result of Theorem 1 to general cases (see Theorem 2 and
Theorem 3).
√ 2) A (nearly) optimal excess risk bound for Nyström kernel k-means is also obtained when nk) points (see Theorem 4). We further relax the restriction of landmark sampling Ω(
√ points from Ω( n) (see Theorem 5) and extend it to general cases (see Theorem 6 and Theorem 7). This result shows that we can use the Nyström method to improve the effectiveness of kernel k-means, while guaranteeing the optimal generalization performance. nk) to Ω(
√
The rest of the paper is organized as follows. In Section 2, we introduce some notations and provide an overview of kernel k-means. In Section 3, we provide nearly optimal excess risk bounds. In
Section 4, we quantify the statistical effect of computational approximations of the Nyström-based kernel k-means. In Section 5, we validate our theoretical ﬁndings by performing experiments on both simulated and real data. We end in Section 6 with conclusion. All the detailed proofs are deferred to the Appendix. 2