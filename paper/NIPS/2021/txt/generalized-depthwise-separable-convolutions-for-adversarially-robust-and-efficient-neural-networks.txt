Abstract
Despite their tremendous successes, convolutional neural networks (CNNs) incur high computational/storage costs and are vulnerable to adversarial perturbations.
Recent works on robust model compression address these challenges by combining model compression techniques with adversarial training. But these methods are unable to improve throughput (frames-per-second) on real-life hardware while simultaneously preserving robustness to adversarial perturbations. To overcome this problem, we propose the method of Generalized Depthwise-Separable (GDWS) convolution – an efﬁcient, universal, post-training approximation of a standard 2D convolution. GDWS dramatically improves the throughput of a standard pre-trained network on real-life hardware while preserving its robustness. Lastly, GDWS is scalable to large problem sizes since it operates on pre-trained models and doesn’t require any additional training. We establish the optimality of GDWS as a 2D convolution approximator and present exact algorithms for constructing optimal
GDWS convolutions under complexity and error constraints. We demonstrate the effectiveness of GDWS via extensive experiments on CIFAR-10, SVHN, and
ImageNet datasets. Our code can be found at https://github.com/hsndbk4/
GDWS. 1

Introduction
Nearly a decade of research after the release of AlexNet [18] in 2012, convolutional neural networks (CNNs) have unequivocally established themselves as the de facto classiﬁcation algorithm for various machine learning tasks [11, 38, 4]. The tremendous success of CNNs is often attributed to their unrivaled ability to extract correlations from large volumes of data, allowing them to surpass human level accuracy on some tasks such as image classiﬁcation [11].
Today, the deployment of CNNs in safety-critical Edge applications is hindered due to their high computational costs [11, 30, 31] and their vulnerability to adversarial samples [37, 5, 16]. Tradi-tionally, those two problems have been addressed in isolation. Recently, very few bodies of works
[19, 35, 42, 6, 34, 7] have addressed the daunting task of designing both efﬁcient and robust CNNs.
A majority of these methods focus on model compression, i.e. reducing the storage requirements of
CNNs. None have demonstrated their real-time beneﬁts in hardware. For instance, Fig. 1a shows recent robust pruning works HYDRA [34] and ADMM [42] achieve high compression ratios (up to 97×) but either fail to achieve high throughput measured in frames-per-second (FPS) or compromise signiﬁcantly on robustness. Furthermore, the overreliance of current robust complexity reduction techniques on adversarial training (AT) [45, 21] increases their training time signiﬁcantly (Fig. 1b).
This prohibits their application to complex ImageNet scale problems with stronger attack models, such as union of norm-bounded perturbations [22]. Thus, there is critical need for methods to 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b)
Figure 1: Performance of existing robust pruning works (HYDRA [34] and ADMM [42]) and the proposed GDWS with VGG-16 on CIFAR-10, captured by: (a) robust accuracy against (cid:96)∞-bounded perturbations vs frames-per-second measured on an NVIDIA Jetson Xavier, and (b) total time required to implement these methods measured on a single NVIDIA 1080 Ti GPU. To ensure a fair comparison, the same AT baseline (obtained from [34]) is used for all methods. The compression ratio of each method, highlighted in parenthesis, is with respect to the AT baseline. design deep nets that are both adversarially robust and achieve high throughput when mapped to real hardware.
To address this need, we propose Generalized Depthwise-Separable (GDWS) convolutions, a universal post-training approximation of a standard 2D convolution that dramatically improves the real hardware FPS of pre-trained networks (Fig. 1a) while preserving their robust accuracy.
Interestingly, we ﬁnd GDWS applied to un-pruned robust networks simultaneously achieves higher
FPS and higher robustness than robust pruned models obtained from current methods. This in spite of GDWS’s compression ratio being smaller than those obtained from robust pruning methods.
Furthermore, GDWS easily scales to large problem sizes since it operates on pre-trained models and doesn’t require any additional training.
Our contributions: 1. We propose GDWS, a novel convolutional structure that can be seamlessly mapped onto off-the-shelf hardware and accelerate pre-trained CNNs signiﬁcantly while maintaining robust accuracy. 2. We show that the error-optimal and complexity-optimal GDWS approximations of any pre-trained standard 2D convolution can be obtained via greedy polynomial time algorithms, thus eliminating the need for any expensive training. 3. We apply GDWS to a variety of networks on CIFAR-10, SVHN, and ImageNet to simultane-ously achieve higher robustness and higher FPS than existing robust complexity reduction techniques, while incurring no extra training cost. 4. We demonstrate the versatility of GDWS by using it to design efﬁcient CNNs that are robust to union of ((cid:96)∞, (cid:96)2, (cid:96)1) perturbation models. To the best of our knowledge, this is the ﬁrst work that proposes efﬁcient and robust networks to the union of norm-bounded perturbation models. 2