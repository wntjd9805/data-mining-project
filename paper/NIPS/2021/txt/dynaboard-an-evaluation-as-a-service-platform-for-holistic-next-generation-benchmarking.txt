Abstract
We introduce Dynaboard, an evaluation-as-a-service framework for hosting bench-marks and conducting holistic model comparison, integrated with the Dynabench platform. Our platform evaluates NLP models directly instead of relying on self-reported metrics or predictions on a single dataset. Under this paradigm, models are submitted to be evaluated in the cloud, circumventing the issues of reproducibil-ity, accessibility, and backwards compatibility that often hinder benchmarking in
NLP. This allows users to interact with uploaded models in real time to assess their quality, and permits the collection of additional metrics such as memory use, throughput, and robustness, which – despite their importance to practitioners – have traditionally been absent from leaderboards. On each task, models are ranked according to the Dynascore, a novel utility-based aggregation of these statistics, which users can customize to better reﬂect their preferences, placing more/less weight on a particular axis of evaluation or dataset. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for a more diverse and comprehensive evaluation of model quality. 1

Introduction
Benchmarks have been critical to driving progress in AI: they provide a standard by which models are measured, they support direct comparisons of different proposals, and they provide clear-cut goals for the research community. This has led to an outpouring of new benchmarks designed not only to evaluate models on new tasks, but also to address weaknesses in existing models [49, 55, 34], and expose artifacts in existing benchmarks [42, 19, 25, 33, 22, 37]. These efforts are helping to provide us with a more realistic picture of how much progress the ﬁeld has made.
To date, the metrics by which we assess system performance have received much less systematic attention. Even as the benchmarks have changed, the community has continued to rely heavily on accuracy as the sole primary metric. This gives rise to a “leaderboard culture” in which accuracy is the only thing that matters, even in contexts in which other pressures – e.g., compactness, fairness, efﬁciency – are clearly important [60, 6, 16, 63, 68, 38, 2, 51, 17]. Recently, Ethayarajh and
Jurafsky [17] provided a microeconomic framing of the problem: leaderboard viewers are consumers of models, and each viewer has their own set of preferences: some care only about accuracy, others value compactness in addition to accuracy, and so on. Static leaderboards that only rank by model performance thus have a scoring function that is misaligned with the preferences of most users.
Moreover, merely focusing on a single metric, such as accuracy, limits the scope of possible issues
⇤Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
even to consider. In other words, focusing on one standard metric, like accuracy, lets researchers off the hook too much on other pressing issues.
Reproducibility, accessibility, and backwards compatibility are important issues as well. There is a reliance on self-reported results with no trust guarantees [75], and many state-of-the-art claims are famously difﬁcult to reproduce [52]. In some cases, the state-of-the-art model cannot be accessed without sufﬁcient computational resources and/or technical expertise, and older models are often incomparable to newer ones because they have not been evaluated on the same data.
To begin to address these issues, we propose Dynaboard. Dynaboard’s evaluation-as-a-service backend allows models to be submitted directly to be evaluated in the cloud, circumventing issues of reproducibility, accessibility, and backwards compatibility. Older models can be compared to newer ones because any model can be evaluated on demand. Dynaboard supports reasoning about many different metrics – not just standard accuracy-style metrics, but also memory use and throughput (i.e., inference speed) on identical hardware, robustness, fairness, and so forth. Building on [17], we borrow from microeconomic theory to aggregate these metrics – along with performance – into a
Dynascore that is used to rank models. The frontend of Dynaboard is a dynamic leaderboard that allows users to customize the Dynascore by placing more/less weight on a particular metric or dataset.
As the user modiﬁes the weights that govern the Dynascore, models are re-ranked in real time. Users can also directly interact with a model via the platform, receiving real-time model predictions that help them understand a model’s capabilities and limitations, and allowing the community to ﬁnd challenging examples where the current state of the art still falls short. Together, this allows users to develop and directly express the complex and interrelated values they have for their models.
Although other platforms – CodaLab, DAWNBench [10], and ExplainaBoard [39], to name a few – address some subset of the problems we describe above, Dynaboard is the ﬁrst to address all of them in a single end-to-end system. It requires minimal overhead for model creators wishing to submit their model for evaluation, but offers maximal ﬂexibility for users wishing to make ﬁne-grained comparisons between models. As state-of-the-art NLP models push the limits of traditional benchmarks, Dynaboard offers a standardized solution for creating the next generation of benchmarks in a manner that allows for the diverse and comprehensive evaluation of model quality. 2 Objectives
Dynaboard aims to address the following issues in our current model evaluation paradigm:
Reproducibility The reliance on self-reported results with no trust guarantees [75] makes many state-of-the-art claims difﬁcult to reproduce [52]. Even the choice of random seed can lead to substantially different results [15], and improvements on the state-of-the-art are often not statistically signiﬁcant [8]. Implementational differences in evaluation metrics can also lead to different scores
[54]. Given that the test data for static benchmarks is often publicly available, reported results may also be the outcome of overﬁtting hyperparameters [4].
Accessibility Whichever model happens to hold the “state-of-the-art” title should be accessible by as many researchers as possible. Democratizing model evaluation is essential to understanding our current weaknesses, for making progress in the long tails of the data distribution, and for collecting new adversarial datasets where the state-of-the-art fails. Just open sourcing the model weights is insufﬁcient, because even when given trained models, inference-time compute resources may not be readily available in many parts of the world.
Backwards Compatibility Dynamic human-in-the-loop data collection is a great alternative to static benchmarks, but as new evaluation sets are introduced in later rounds, old models become incomparable to new ones, simply because they haven’t been evaluated on the same data. It should be possible to evaluate a model on demand, rather than the model’s predictions at a single point in time.
Forwards Compatibility Automated metrics such as BLEU and ROUGE have many known ﬂaws, and creating better automated metrics (e.g., BLEURT [64], BERTScore [77]) is an active area of research. However, the current paradigm does not allow for old models to be evaluated on new automated metrics that come later, since it relies on reporting from the model creator. If our ﬁeld comes up with new and better metrics, we should be able to immediately gauge overall model quality. 2
Prediction Costs Most leaderboards treat the cost of making predictions as zero, which does not hold in practice; memory use, latency, throughput, and lack of robustness – among many other factors – have real-world implications in deployment [60, 17]. A highly accurate model may be useless to an embedded systems engineer if it is untenably large, for example. For users to be able to make informed decisions about the risks and rewards of using a particular model, they need to be given as much information as possible.
Utility Estimation There is no single correct way to rank models: every leaderboard viewer has a different set of preferences, as manifested in their utility function [17]. Some users only care about model performance, but others care about memory use, throughput, fairness, and more. A leaderboard should not be overly prescriptive when ranking models – it should provide a default ranking, but ultimately give users the freedom to customize the scoring/ranking function to better approximate their utility function.
The evaluation-as-a-service backend of Dynaboard addresses the ﬁrst four of these. By having models submitted directly to the platform, they can be accessed and evaluated on demand on any dataset (see
Section 4). The frontend of Dynaboard is a dynamic leaderboard that addresses the last two issues: prediction costs and utility estimation. Although the leaderboard has a default ranking, the scoring function is constructed in a principled way, borrowing from economic theory to estimate the rates at which users are willing to trade-off prediction costs for performance. More importantly, users are given the freedom to manipulate the scoring function to better approximate their preferences. 3