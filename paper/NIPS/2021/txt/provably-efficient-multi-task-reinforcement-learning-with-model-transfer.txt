Abstract
We study multi-task reinforcement learning (RL) in tabular episodic Markov deci-sion processes (MDPs). We formulate a heterogeneous multi-player RL problem, in which a group of players concurrently face similar but not necessarily identical
MDPs, with a goal of improving their collective performance through inter-player information sharing. We design and analyze an algorithm based on the idea of model transfer, and provide gap-dependent and gap-independent upper and lower bounds that characterize the intrinsic complexity of the problem. 1

Introduction
In many real-world applications, reinforcement learning (RL) agents can be deployed as a group to complete similar tasks at the same time. For example, in healthcare robotics, robots are paired with people with dementia to perform personalized cognitive training activities by learning their preferences [42, 21]; in autonomous driving, a set of autonomous vehicles learn how to navigate and avoid obstacles in various environments [27]. In these settings, each learning agent alone may only be able to acquire a limited amount of data, while the agents as a group have the potential to collectively learn faster through sharing knowledge among themselves. Multi-task learning [7] is a practical framework that can be used to model such settings, where a set of learning agents share/transfer knowledge to improve their collective performance.
Despite many empirical successes of multi-task RL (see, e.g., [51, 28, 27]) and transfer learning for RL (see, e.g., [26, 39]), a theoretical understanding of when and how information sharing or knowledge transfer can provide beneﬁts remains limited. Exceptions include [16, 6, 11, 17, 32, 25], which study multi-task learning from parameter or representation transfer perspectives. However, these works still do not provide a completely satisfying answer: for example, in many application scenarios, the reward structures and the environment dynamics are only slightly different for each task—this is, however, not captured by representation transfer [11, 17] or existing works on clustering-based parameter transfer [16, 6]. In such settings, is it possible to design provably efﬁcient multi-task RL algorithms that have guarantees never worse than agents learning individually, while outperforming the individual agents in favorable situations?
In this work, we formulate an online multi-task RL problem that is applicable to the aforementioned settings. Speciﬁcally, inspired by a recent study on multi-task multi-armed bandits [43], we formulate the �-Multi-Player Episodic Reinforcement Learning (abbreviated as �-MPERL) problem, in which all tasks share the same state and action spaces, and the tasks are assumed to be similar—i.e., the dissimilarities between the environments of different tasks (speciﬁcally, the reward distributions and transition dynamics associated with the players/tasks) are bounded in terms of a dissimilarity 0. This problem not only models concurrent RL [34, 16] as a special case by taking parameter �
� = 0, but also captures richer multi-task RL settings when � is nonzero. We study regret minimization for the �-MPERL problem, speciﬁcally:
≥ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1. We identify a problem complexity notion named subpar state-action pairs, which captures the amenability to information sharing among tasks in �-MPERL problem instances. As shown in the multi-task bandits literature (e.g., [43]), inter-task information sharing is not always helpful to reduce the players’ collective regret. Subpar state-action pairs, intuitively speaking, are clearly suboptimal for all tasks, for which we can robustly take advantage of (possibly biased) data collected for other tasks to achieve a lower regret in a certain task. 2. In the setting where the dissimilarity parameter � is known, we design a model-based algorithm MULTI-TASK-EULER (Algorithm 1), which is built upon state-of-the-art algo-rithms for learning single-task Markov decision processes (MDPs) [3, 46, 36], as well as algorithmic ideas of model transfer in RL [39]. MULTI-TASK-EULER crucially utilizes the dissimilarity assumption to robustly take advantage of information sharing among tasks, and achieves regret upper bounds in terms of subpar state-action pairs, in both (value function suboptimality) gap-dependent and gap-independent fashions. Speciﬁcally, compared with a baseline algorithm that does not utilize information sharing, MULTI-TASK-EULER has a regret guarantee that: (1) is never worse, i.e., it avoids negative transfer [33]; (2) can be much superior when there are a large number of subpar state-action pairs. 3. We also present gap-dependent and gap-independent regret lower bounds for the �-MPERL problem in terms of subpar state-action pairs. These lower bounds nearly match the upper bounds when the episode length of the MDP is a constant. Together, the upper and lower bounds characterize the intrinsic complexity of the �-MPERL problem. 2 Preliminaries
\ 1, . . . , n
A to denote its complement. Denote by Δ(
Throughout this paper, we denote by [n] :=
AC = U
. For a set A in a universe U , we use
}
) the set of probability distributions over
X
. For functions f, g, we use f � g or f = O(g) (resp. f � g or f = Ω(g)) to denote that there cg), and use f � g to denote f � g and cg (resp. f b := min(a, b). We use E to denote the
∧
) and ˜Ω(
)
X exists some constant c > 0, such that f f � g simultaneously. Deﬁne a expectation operator, and use var to denote the variance operator. Throughout, we use ˜O( notation to hide polylogarithmic factors.
≥ b := max(a, b), and a
≤
∨
{
·
·
∈
Δ(
). Let
[M ]. Each MDP
Mp = (H,
N+, ﬁnite state space
Multi-task RL in episodic MDPs. We have a set of M MDPs each associated with a player p same episode length H p0 ∈
Pp : not necessarily identical. We assume that the MDPs are layered1, in that the state space be partitioned into disjoint subsets (
S1, and for every p h
Sh+1; here, we deﬁne
We denote by S := the size of the action space.
M p=1,
Mp is regarded as a task. The MDPs share the
, and initial state distribution
. The transition probabilities
Δ([0, 1]) of the players are can
[M ],
.
, ﬁnite action space be a default terminal state that is not contained in
) and reward distributions rp :
Sh)H h=1, where p0 is supported on s, a) is supported on
∈ Sh, a
· | the size of the state space, and A :=
|S|
A
S
S × A →
S
∈
SH+1 =
S
S × A →
[H], and every s
, p0, Pp, rp)
S ∪ {⊥}
, Pp(
{⊥}
∈ A
⊥
Δ(
|A|
A
∈
∈
S
S
�
�
, p=1 and (Pp)M p=1 are unknown to the players. For each episode k
Interaction process. The interaction process between the players and the environment is as follows: at the beginning, both (rp)M
[K],
[M ] independently conditioned on the interaction history up to episode k
−
∈
Mp; speciﬁcally, player p starts with state sk p0, and at every interacts with its respective MDP 1,p ∼ h,p, transitions to next state sk h,p, ak sk
[H], it chooses action ak step (layer) h h,p) and
Pp( h+1,p ∼
· | receives a stochastic immediate reward rk h,p, ak sk h,p); after all players have ﬁnished their rp( h,p ∼ k-th episode, they can communicate and share information. The goal of the players is to maximize their expected collective reward E 1, each player p
H h=1 rk h,p
K k=1
M p=1
· |
∈
∈
.
Policy and value functions. A deterministic, history-independent policy π is a mapping from to
, which can be used by a player to make decisions in its respective MDP. For player p and step h, we
S
A
��
�
�
� 1This is a standard assumption (see, e.g., [44]). It is worth noting that any episodic MDP (with possibly nonstationary transition and reward) can be converted to a layered MDP with stationary transition and reward, with the state space size being H times the size of the original state space. 2
h,p : use V π functions, respectively. They satisfy the following recurrence known as the Bellman equation:
[0, H] to denote its respective value and action-value
[0, H] and Qπ
Sh × A →
Sh → h,p : h
∀
∈
[H] : h,p(s) = Qπ
V π where we use the convention that V π h,p(s, π(s)), Qπ
H+1,p( h,p(s, a) = Rp(s, a) + (PpV π h+1,p)(s, a),
R, (Ppf )(s, a) := s,a) [ˆr] is the expected immediate reward
Sh+1 →
) = 0, and for f :
⊥ s�∈Sh+1 Pp(s� s, a)f (s�), and Rp(s, a) := Eˆr
∼ of player p. For player p and policy π, denote by V π
�
| rp(
·| p0
V π 1,p(s1) its expected reward.
For player p, we also deﬁne its optimal value function V � value function Q�
[0, H] using the Bellman optimality equation: h,p :
�
Sh →
�
[0, H] and the optimal action-0,p = Es1∼ h,p :
Q� h,p(s, a), Q� h,p(s, a) = Rp(s, a) + (PpV � h+1,p)(s, a), (1) h
∀
∈
[H] :
Sh × A →
V � h,p(s) = max
∈A a where we again use the convention that V �
H+1,p(
) = 0. For player p, denote by V � 0,p =
⊥ its optimal expected reward.
� p0
V � 1,p(s1)
Es1∼
Given a policy π, as V π the value functions (V π
[H + 1] and s every h
�
∈
We deﬁne Qπ p , V � greedy policies π� p , Q� p(s) h,p for different h’s are only deﬁned in the respective layer h,p)H h=1 and obtain a single value function V π p :
∈ Sh,
S ∪ {⊥} → h,p(s). p similarly. For player p, given its optimal action value function Q� p(s, a) is optimal with respect to argmaxa p (s) := V π
V π
Q�
∈
∈A
Mp.
Sh, we “collate”
R. Formally, for p, any of its
Q� p (s)
Suboptimality gap. For player p, we deﬁne the suboptimality gap of state-action pair (s, a) as gapp(s, a) = V � p(s, a). We deﬁne the mininum suboptimality gap of player p as gapp,min = min(s,a):gapp(s,a)>0 gapp(s, a), and the minimum suboptimality gap over all players as gapmin = minp as the set of optimal state-action pairs with respect to p.
[M ] gapp,min. For player p (s, a) : gapp(s, a) = 0
[M ], deﬁne Zp,opt :=
−
∈
∈
�
�
Performance metric. We measure the performance of the players using their collective regret, i.e., over a total of K episodes, how much extra reward they would have collected in expectation if they were executing their respective optimal policies from the beginning. Formally, suppose for each episode k, player p executes policy πk(p), then the collective regret of the players is deﬁned as:
M
K
Reg(K) =
V � 0,p −
V πk(p) 0,p
.
� p=1
�
�k=1 �
Baseline: individual STRONG-EULER. A naive baseline for multi-task RL is to let each player run a separate RL algorithm without communication. For concreteness, we choose to let each player run the state of the art STRONG-EULER algorithm [36] (see also its precursor EULER [46]), which enjoys minimax gap-independent [3, 8] and gap-dependent regret guarantees, and refer to this strategy as individual STRONG-EULER. Speciﬁcally, as it is known that STRONG-EULER has a regret of ˜O(√H 2SAK + H 4S2A), individual STRONG-EULER has a collective regret of
˜O(M √H 2SAK + M H 4S2A). In addition, by a union bound and summing up the gap-dependent regret guarantees of STRONG-EULER for the M MDPs altogether, it can be checked that with probability 1
δ, individual STRONG-EULER has a collective regret of order2
−
M SAK
δ ln
�
�




[M ]
�p
∈

�(s,a)
∈
Zp,opt


H 3 gapp,min
+
ZC
�(s,a)
∈ p,opt
H 3 gapp(s, a) 


+ M H 4S2A ln
SA gapmin (2)
.



 2The originally-stated gap-dependent regret bound of STRONG-EULER ([36], Corollary 2.1) uses a slightly different notion of suboptimality gap, which takes an extra minimum over all steps. A close examination of their proof shows that STRONG-EULER has regret bound (2) in layered MDPs. See also Remark 21 in Appendix C.4. 3
Our goal is to design multi-task RL algorithms that can achieve collective regret strictly lower than this baseline in both gap-dependent and gap-independent fashions when the tasks are similar.
Notion of similarity. Throughout this paper, we will consider the following notion of similarity between MDPs in the multi-task episodic RL setting.
Deﬁnition 1. A collection of MDPs ( (s, a)
,
∈ S × A
Mp)M p=1 is said to be �-dissimilar, if for all p, q
[M ], and
∈
Rp(s, a)
Rq(s, a)
−
Mp)M
�
�
�
�
If this happens, we call (
�-MPERL) problem instance.
�,
Pp(
�
· |
≤ s, a)
Pq(
· |
− s, a)
�1 ≤
�
H
. p=1 an �-Multi-Player Episodic Reinforcement Learning (abbrev.
Mp)M
If the MDPs in ( p=1 are 0-dissimilar, then they are identical by deﬁnition, and our interaction protocol degenerates to the concurrent RL protocol [34]. Our dissimilarity notion is complementary to those of [6, 16]: they require the MDPs to be either identical, or have well-separated parameters for at least one state-action pair; in contrast, our dissimilarity notion allows the MDPs to be nonidentical and arbitrarily close.
We have the following intuitive lemma that shows the closeness of optimal value functions of different
MDPs, in terms of the dissimilarity parameter �:
Lemma 2. If (
Q�
Q� p(s, a)
Mp)M q(s, a)
≤
−
�
�
� 3 Algorithm
�
�
� p=1 are �-dissimilar, then for every p, q
[M ], and (s, a) 2H�; consequently, gapp(s, a)
�
�
�
−
∈ gapq(s, a) 4H�.
≤
�
�
�
,
∈ S × A
We now describe our main algorithm, MULTI-TASK-EULER (Algorithm 1). Our model-based algo-rithm is built upon recent works on episodic RL that provide algorithms with sharp instance-dependent guarantees in the single task setting [46, 36]. In a nutshell, for each episode k and each player p, the algorithm performs optimistic value iteration to construct high-probability upper and lower bounds for the optimal value and action value functions V � p, and uses them to guide its exploration and decision making process. p and Q�
Empirical estimates of model parameters. For each player p, the construction of its value func-tion bound estimates relies on empirical estimates on its transition probability and expected reward function. For both estimands, we use two estimators with complementary roles, which are at two different points of the bias-variance tradeoff spectrum: one estimator uses only the player’s own data (termed individual estimate), which has large variance; the other estimator uses the data collected by all players (termed aggregate estimate), which has lower variance but can easily be biased, as transition probabilities and reward distributions are heterogeneous. Such algorithmic idea of “model transfer”, where one estimates model in one task using data collected from other tasks has appeared in prior works (e.g., [39]). Speciﬁcally, at the end of episode k, for every h
,
∈ Sh × A the algorithm maintains its empirical count of encountering (s, a) for each player p, along with its total empirical count across all players, respectively:
[H] and (s, a)
∈ k np(s, a) := 1
�l=1
� (sl h,p, al h,p) = (s, a) k
M
, n(s, a) := 1
�
�l=1 p=1
�
� (sl h,p, al h,p) = (s, a)
. (3)
�
The individual and aggregate estimates of immediate reward R(s, a) are deﬁned as:
ˆRp(s, a) := k l=1 1 (sl h,p, al h,p) = (s, a)
�
� np(s, a) rl h,p
�
, ˆR(s, a) := k l=1
M p=1 1
�
�
� 4 (sl h,p, al n(s, a) h,p) = (s, a) rl h,p
.
� (4)
Algorithm 1: MULTI-TASK-EULER
Input :Failure probability δ
Initialize: Set Vp( 1 for k = 1, 2, . . . , K do
⊥
∈ for p = 1, 2, . . . , M do (0, 1), dissimilarity parameter � 0.
) = 0 for all p in [M ], where is the only state in
⊥
≥
SH+1 ; 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19
// Construct optimal value estimates for player p for h = H, H for (s, a) 1, . . . , 1 do do
−
∈ Sh × A
Compute: ind-Qp(s, a) = ˆRp(s, a) + (ˆPpV p)(s, a) + ind-bp(s, a); ind-Q ind-bp(s, a); agg-Qp(s, a) = ˆR(s, a) + (ˆPV p)(s, a) + agg-bp(s, a); agg-bp(s, a); agg-Q (s, a) = ˆRp(s, a) + (ˆPpV p)(s, a) (s, a) = ˆR(s, a) + (ˆPV p)(s, a)
− p p
−
Update optimal action value function upper and lower bound estimates:
Qp(s, a) = min
H h + 1, ind-Qp(s, a), agg-Qp(s, a)
− 0, ind-Q
� (s, a), agg-Q for s p
Q (s, a) = max
∈ Sh do
Deﬁne πk(p)(s) = argmaxa
Update V p(s) = Qp
� p
∈A s, πk(p)(s) (s, a)
; p
�
Qp(s, a) ;
, V p(s) = Q p s, πk(p)(s)
.
;
�
// All players p interact with their respective environments, and update
�
�
�
� reward and transition estimates for p = 1, 2, . . . , M do
Player p executes policy πk(p) on h=1.
Update individual estimates of transition probability ˆPp, reward ˆRp and count np( using the ﬁrst parts of Equations (3), (4) and (5).
Mp and obtains trajectory (sk h,p, ak h,p, rk h,p)H
·
,
)
·
Update aggregate estimates of transition probability ˆP, reward ˆR and count n( second parts of Equations (3), (4) and (5).
) using the
,
·
·
Similarly, for every h
∈ aggregate estimates of transition probability as:
[H] and (s, a, s�)
∈ Sh × A × Sh+1, we also deﬁne the individual and
ˆPp(s�
| s, a) :=
� k l=1 1
� (sl h,p, al h+1,p) = (s, a, s�) h,p, sl np(s, a) h,p, al (sl k l=1
M p=1 1 h,p, sl h+1,p) = (s, a, s�)
,
� (5)
ˆP(s� s, a) :=
�
|
If n(s, a) = 0, we deﬁne ˆR(s, a) := 0 and ˆP(s�
ˆRp(s, a) := 0 and ˆPp(s�
MULTI-TASK-EULER efﬁciently in an incremental manner. s, a) := 1
|Sh+1
�
|
|
| n(s, a) s, a) := 1
�
|Sh+1
|
; and if np(s, a) = 0, we deﬁne
. The counts and reward estimates can be maintained by
.
�
Constructing value function estimates via optimistic value iteration. For each player p, based on these model parameter estimates, MULTI-TASK-EULER performs optimistic value iteration to compute the value function estimates for states at all layers (lines 3 to 15). For the terminal layer H + 1, V �
[H],
MULTI-TASK-EULER iteratively builds its value function estimates in a backward fashion. At the time of estimating values for layer h, the algorithm has already obtained optimal value es-timates for layer h + 1. Based on the Bellman optimality equation (1), MULTI-TASK-EULER
) = 0 trivially, so nothing needs to be done. For earlier layers h
H+1(
⊥
∈ 5
∈A p (s))s
∈Sh+1 ,
∈Sh+1 (lines 5 to 12). p(s, a))s
∈Sh,a
∈Sh+1 and (V p(s))s using model parameter estimates and its estimates of (V � estimates (Q� i.e., (V p(s))s
Speciﬁcally, MULTI-TASK-EULER constructs estimates of Q� in two different ways. First, it uses the individual estimates of model of player p to construct ind-Q p and ind-Qp, upper and lower bound estimates of Q� p (lines 8 and 9); this construction is reminiscent of p and ind-Qp as our opti-EULER and STRONG-EULER [46, 36], in that if we were only to use ind-Q mal action value function estimate Qp and Q p, our algorithm becomes individual STRONG-EULER.
The individual value function estimates are key to establishing MULTI-TASK-EULER’s fall-back guarantees, ensuring that it never performs worse than the individual STRONG-EULER baseline.
Second, it uses the aggregate estimate of model to construct agg-Q and agg-Qp, also upper and lower bound estimates of Q� p (lines 6 and 7); this construction is unique to the multitask learning setting, and is our new algorithmic contribution. p(s, a) for all s
∈ Sh, a
∈ A p and ind-Q
To ensure that agg-Qp and ind-Qp (resp. agg-Q p) are valid upper bounds (resp. lower bounds) of Q� p, MULTI-TASK-EULER adds bonus terms ind-bp(s, a) and agg-bp(s, a), respectively, in the optimistic value iteration process, to account for estimation error of the model estimates against the true models. Speciﬁcally, both bonus terms comprise three parts: p ind-bp(s, a) := brw np(s, a), 0
+ bprob
�
� agg-bp(s, a) := brw n(s, a), �
+ bprob
�
�
�
� where
ˆPp(
· |
ˆP(
· | s, a), np(s, a), V p, V p, 0
+ bstr
ˆPp(
� s, a), np(s, a), V p, V p, 0
· | s, a), n(s, a), V p, V p, �
�
+
,
� bstr
ˆP(
· |
�
� s, a), n(s, a), V p, V p, �
,
� brw (n, κ) := 1
κ + Θ
∧
L(n)
�� n �
, bprob q, n, V , V , κ
:= H
�
�
∧ 2κ + Θ
�
�
�
�


 vars�∼ q
V (s�)
L(n)
� n
�
+
Es�∼ q (V (s�)
�
− n
V (s�))2
L(n)
�
+
HL(n) n
�
�
�
�

,


 bstr q, n, V , V , κ
�
�
:= κ + Θ 




S Es�∼ q
� (V (s�)
− n
V (s�))2
L(n)
�
+
HSL(n) n
�
�
�
�

,



 and L(n) � ln( M SAn
The three components in the bonus terms serve for different purposes:
).
δ 1. The ﬁrst component accounts for the uncertainty in the reward estimation: with prob-ˆRp(s, a)
Rp(s, a)
−
≤ brw np(s, a), 0
, and
ˆR(s, a)
Rp(s, a)
−
≤ 2. The second component accounts for the uncertainty in estimating (PpV � p )(s, a): with proba-�
�
�
�
�
�
�
� (ˆPpV � p )(s, a)
− p )(s, a) (PpV � (PpV � p )(s, a)
�
ˆP(
�
· |
� bprob
≤
� bprob
ˆPp(
· | s, a), n(s, a), V p, V p, �
� s, a), np(s, a), V p, V p, 0
�
.
�
�
�
� ability 1
O(δ),
− n(s, a), �
. brw
� bility 1
�
O(δ),
−
� (ˆPV � p )(s, a)
�
�
− and
�
�
�
�
�
� 6
≤
�
�
�
3. The third component accounts for with probability 1 bstr optimism [36]:
ˆPp(
· |
ˆP(
� bprob
· |
� s, a), np(s, a), V p, V p, 0 s, a), n(s, a), V p, V p, �
,
�
.
� the lower order (ˆPp −
O(δ),
� (ˆP
Pp)(V p −
�
−
� terms
Pp)(V p −
V � p )(s, a)
− and to ensure strong
V � p )(s, a)
≤
≤
�
�
�
�
�
�
�
�
�
−
O(δ), both agg-Qp and ind-Qp (resp. agg-Q
Based on the above concentration inequalities and the deﬁnitions of bonus terms, it can be shown inductively that, with probability 1 p) are valid upper bounds (resp. lower bounds) of Q� p.
, Q� h+1]. By taking intersections
Finally, observe that for any (s, a) of all conﬁdence bounds of Q� p it has obtained, MULTI-TASK-EULER constructs its ﬁnal upper and lower bound estimates for Q� (line 11 to 12). Similar ideas on using data from multiple sources to construct conﬁdence intervals and guide explorations have been used by [37, 43] for multi-task noncontextual and contextual bandits. Using the relationship between the optimal value V � p (s) and and optimal action values
,
MULTI-TASK-EULER also constructs upper and lower bound estimates for V � respectively for s p(s, a) : a
� p (s), V p(s) and V p(s), (s, a) respectively, for (s, a) p(s, a), Qp(s, a) and Q p(s, a) has range [0, H
∈ Sh × A
∈ Sh×A and ind-Q
∈ A
Q�
−
� p p
∈ Sh (line 15).
Executing optimistic policies. At each episode k, for each player p, its optimal action-value function upper bound estimate Qp induces a greedy policy πk(p) : s
Qp(s, a) (line 14); the player then executes this policy at this episode to collect a new trajectory and use this to update its individual model parameter estimates. After all players ﬁnish their episode k, the algorithm also updates its aggregate model parameter estimates (lines 16 to 19) using Equations (3), (4) and (5), and continues to the next episode. argmaxa
�→
∈A 4 Performance guarantees
Before stating the guarantees of Algorithm 1, we deﬁne an instance-dependent complexity measure that characterizes the amenability to information sharing.
Deﬁnition 3. The set of subpar state-action pairs is deﬁned as: where we recall that gapp(s, a) = V �
�
I� := (s, a)
: p
[M ], gapp(s, a)
∈ S × A p (s)
∃
∈
Q� p(s, a).
−
≥ 96H�
,
�
Deﬁnition 3 generalizes the notion of subpar arms deﬁned for multi-task multi-armed bandit learn-ing [43] in two ways: ﬁrst, it is with regards to state-action pairs as opposed to actions only; second, in
RL, suboptimality gaps depend on optimal value function, which in turn depends on both immediate reward and subsequent long-term return.
To ease our later presentation, we also present the following lemma.
Lemma 4. For any (s, a) that Zp,opt = for all p, q
∈ I�, we have that: (1) for all p (s, a) : gapp(s, a) = 0
[M ], gapp(s, a)
� 1 2 gapq(s, a).
�
∈
≥
Zp,opt, where we recall is the set of optimal state-action pairs with respect to p; (2)
[M ], (s, a) /
∈
∈
The lemma follows directly from Lemma 2; its proof can be found in the Appendix along with proofs of the following theorems. Item 1 implies that any subpar state-action pair is suboptimal for all players. In other words, for every player p, the state-action space can be partitioned to three
Zp,opt)C. Item 2 implies that for any subpar (s, a), its suboptimal gaps disjoint sets: with respect to all players are within a constant of each other.
I�, Zp,opt, (
S × A
I� ∪ 4.1 Upper bounds
With the above deﬁnitions, we are now ready to present the performance guarantees of Algorithm 1.
We ﬁrst present a gap-independent collective regret bound of MULTI-TASK-EULER. 7
Theorem 5 (Gap-independent bound). If satisﬁes that with probability 1
δ,
−
M p=1 are �-dissimilar, then MULTI-TASK-EULER
Mp
�
�
Reg(K)
≤
˜O
M
�
�
H 2
C
� |
|I
K +
M H 2
|I�|
�
K + M H 4S2A
.
�
We again compare this regret upper bound with individual STRONG-EULER’s gap independent regret bound. Recall that individual STRONG-EULER guarantees that with probability 1
δ,
−
˜O
M √H 2SAK + M H 4S2A
.
Reg(K)
≤
�
We focus on the comparison on the leading terms, i.e., the √K terms. As M √H 2SAK �
K, we see that an improvement in the collective regret bound
M
K + M
H 2
H 2
|I�|
C
�
I
�
� comes from the contributions from subpar state-action pairs: the M 1
M ) improvement. Moreover, if to
MULTI-TASK-EULER provides a regret bound of lower order than individual STRONG-EULER.
K, a factor of ˜O(
SA and M
|I�|
�
M H 2
|I�|
�
I
H 2
C
�
K term is reduced
�
�
�
�
� 1,
�
�
We next present a gap-dependent upper bound on its collective regret.
�
�
�
�
�
Theorem 6 (Gap-dependent upper bound). If satisﬁes with probability 1
δ,
−
M p=1 are �-dissimilar, then MULTI-TASK-EULER
Mp
�
�
Reg(K) � ln(
M SAK
δ
)



[M ]
�p
∈
�(s,a)
∈I�

�(s,a)
∈
Zp,opt


H 3 gapp,min
+
�(s,a) (
I�∪
∈
Zp,opt)C
+
H 3 gapp(s, a) 


H 3 minp gapp(s, a)

+ ln(
M SAK
δ
)
·
M H 4S2A ln
M SA gapmin
,





 where we recall that gapp,min = min(s,a):gapp(s,a)>0 gapp(s, a), and gapmin = minp gapp,min.
Comparing this regret bound with the regret bound obtained by the individual STRONG-EULER baseline, recall that by summing over the regret guarantees of STRONG-EULER for all players
[M ], and taking a union bound over all p, individual STRONG-EULER guarantees a collective p regret bound of
∈
Reg(K) � ln(
M SAK
δ
)



[M ]
�p
∈

�(s,a)
∈
Zp,opt


H 3 gapp,min
+
�(s,a) (
I�∪
∈
Zp,opt)C
+
H 3 gapp(s, a) 


H 3 gapp(s, a)

+ ln(
M SAK
δ
)
·
M H 4S2A ln
SA gapmin
,
�(s,a)
[M ]
∈I� �p
∈
− that holds with probability 1
δ. We again focus on comparing the leading terms, i.e., the terms that have polynomial dependences on the suboptimality gaps in the above two bounds. It can be seen that an improvement in the regret bound by MULTI-TASK-EULER comes from the contributions from the
H 3 subpar state-action pairs: for each (s, a) gapp(s,a) to minp gapp(s,a) , a factor of O( 1
M ) improvement. Recent work of [44] has shown that in the single-task
H 3 setting, it is possible to replace with a sharper problem-dependent complexity gapp,min term that depends on the multiplicity of optimal state-action pairs. We leave improving the guarantee
� of Theorem 6 in a similar manner as an interesting open problem.
∈ I�, the regret bound is reduced from
Zp,opt (s,a)
�
[M ]
H 3
∈
∈ p 8
Key to the proofs of Theorems 5 and 6 is a new bound on the surplus [36] of the value function estimates. Our new surplus bound is a minimum of two terms: one depends on the usual state-action visitation counts of player p, the other depends on the task dissimilarity parameter � and the state-action visitation counts of all players. Detailed proofs can be found at Appendix C. 4.2 Lower bounds
To complement the above upper bounds, we now present gap-dependent and gap-independent regret lower bounds that also depends on our subpar state-action pair notion. Our lower bounds are inspired by regret lower bounds for episodic RL [36, 8] and multi-task bandits [43].
Theorem 7 (Gap-independent lower bound). For any A
N,
≥ and l, lC 4(S + HA), there exists some � that satisﬁes: for any algorithm Alg, there exists an �-MPERL problem instance with S states, A actions, M players and an episode length of H such that
N with l + lC = SA and l
SA, M 4H, K l, and 2, H 2, S
SA
≥
−
≥
≥
≤
∈
∈
E
�
RegAlg(K)
�
�
�
�
� 192H
I
Ω
≥
�
�
�
�
≥
M √H 2lCK + √M H 2lK
.
�
�
CK α.
We also present a gap-dependent lower bound. Before that, we ﬁrst formally deﬁne the notion of sublinear regret algorithms: for any ﬁxed �, we say that an algorithm Alg is a sublinear regret algorithm for the �-MPERL problem if there exists some C > 0 (that possibly depends on the state-action space, the number of players, and �) and α < 1 such that for all K and all �-MPERL
RegAlg(K) environments, E
Theorem 8 (Gap-dependent lower bound). Fix � with S 1), let S1 = S 2(H values that satisﬁes: (1) each Δs,a,p ∈ least one action a
∈
Δs,a,p −
≤ players and an episode length of H, such that
�
� for this problem instance, any sublinear regret algorithm Alg for the �-MPERL problem must satisfy: 2, M
N,
[M ] be any set of
×
[M ], there exists at
[0, H/48], (2) for every (s, p)
[A] such that Δs,a,p = 0, and (3) for every (s, a)
[M ],
[A] and p, q
�/4. There exists an �-MPERL problem instance with S states, A actions, M
S1 = [S1],
|Sh|
[S1] (s, a, p)
∈
∀
N, A
[S1]
×
∈
[S1]
×
∈
[S1]
×
∈ 0. For any S
Δs,a,p gapp(s, a) = Δs,a,p,
= 2 for all h 1); and let
∈ (s,a,p)
≥
[M ];
Δs,a,q 2, and
≥
[A] 2, H 2(H
[A]
≥
−
×
≥
≥
×
−
≤
−
∈
∈
�
�
�
�
�








E
RegAlg(K)
Ω ln K
≥

H 2 gapp(s, a)
+
H 2 minp gapp(s, a)


.
:
�
�
∈I
�p
∈
C (�/192H)





 gapp(s,a)>0
[M ] �(s,a)






Comparing the lower bounds with MULTI-TASK-EULER’s regret upper bounds in Theorems 5 and 6, we see that the upper and lower bounds nearly match for any constant H. When H is large, a key
I�, while the latter difference between the upper and lower bounds is that the former are in terms of are in terms of
I� with
H )—our analysis uses a clipping trick similar to [36], which may be the reason for a suboptimal
IΘ( � dependence on H. We leave closing this gap as an open question.
H ). We conjecture that our upper bounds can be improved by replacing
�(s,a)
∈I(�/192H)
IΘ( �






 5