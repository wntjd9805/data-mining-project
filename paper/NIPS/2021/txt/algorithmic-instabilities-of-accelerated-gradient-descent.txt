Abstract
We study the algorithmic stability of Nesterov’s accelerated gradient method. For convex quadratic objectives, Chen et al. [10] proved that the uniform stability of the method grows quadratically with the number of optimization steps, and conjectured that the same is true for the general convex and smooth case. We disprove this conjecture and show, for two notions of algorithmic stability (including uniform stability), that the stability of Nesterov’s accelerated method in fact deteriorates exponentially fast with the number of gradient steps. This stands in sharp contrast to the bounds in the quadratic case, but also to known results for non-accelerated gradient methods where stability typically grows linearly with the number of steps.

Introduction 1
Algorithmic stability has emerged over the last two decades as a central tool for generalization analysis of learning algorithms. While the classical approach in generalization theory originating in the PAC learning framework appeal to uniform convergence arguments, more recent progress on stochastic convex optimization models, starting with the pioneering work of Bousquet and Elisseeﬀ [6] and
Shalev-Shwartz et al. [24], has relied on stability analysis for deriving tight generalization results for convex risk minimizing algorithms.
Perhaps the most common form of algorithmic stability is the so called uniform stability [6]. Roughly, the uniform stability of a learning algorithm is the worst-case change in its output model, in terms of its loss on an arbitrary example, when replacing a single sample in the data set used for training.
Bousquet and Elisseeﬀ [6] initially used uniform stability to argue about the generalization of empirical risk minimization with strongly convex losses. Shalev-Shwartz et al. [24] revisited this concept and studied the stability eﬀect of regularization on the generalization of convex models. Their bounds were recently improved in a variety of ways [13, 14, 7] and their approach has been inﬂuential in a variety of settings (e.g., 18, 16, 9). In fact, to this day, algorithmic stability is essentially the only general approach for obtaining tight (dimension free) generalization bounds for convex optimization algorithms applied to the empirical risk (see 24, 12).
Signiﬁcant focus has been put recently on studying the stability properties of iterative optimization algorithms. Hardt et al. [17] considered stochastic gradient descent (SGD) and gave the ﬁrst bounds on its uniform stability for a convex and smooth loss function, that grow linearly with the number of optimization steps. As observed by Feldman and Vondrak [13] and Chen et al. [10], their arguments also apply with minor modiﬁcations to full-batch gradient descent (GD). Bassily et al. [5] exhibited a signiﬁcant gap in stability between the smooth and non-smooth cases, showing that non-smooth GD and SGD are inherently less stable than their smooth counterparts. Even further, algorithmic stability has also been used as an analysis technique in stochastic mini-batched iterative optimization (e.g., 25, 1), and has been proved crucial to the design and analysis of diﬀerentially private optimization algorithms [26, 4, 15], both of which focusing primarily on smooth optimization. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Having identiﬁed smoothness as key to algorithmic stability of iterative optimization methods, the following fundamental question emerges: how stable are optimal methods for smooth convex optimization? In particular, what is the algorithmic stability of the celebrated Nesterov accelerated gradient (NAG) method [22]—a cornerstone of optimal methods in convex optimization? Besides being a basic and natural question in its own right, its resolution could have important implications to the design and analysis of optimization algorithms, as well as serve to deepen our understanding of the generalization properties of iterative gradient methods. Chen et al. [10] addressed this question in the case of convex quadratic objectives and derived bounds on the uniform stability of NAG that grow quadratically with the number of gradient steps (as opposed to the linear growth known for GD).
They conjectured that similar bounds hold true more broadly, but fell short of proving this for general convex and smooth objectives. Our work is aimed at ﬁlling this gap. 1.1 Our Results
We establish tight algorithmic stability bounds for the Nesterov accelerated gradient method (NAG).
We show that, somewhat surprisingly, the uniform stability of NAG grows exponentially fast with the number of steps in the general convex and smooth setting. Namely, the uniform stability of
𝑇-steps NAG with respect to a dataset of 𝑛 examples is in general exp(Ω(𝑇))/𝑛, and in particular, after merely 𝑇 = 𝑂 (log 𝑛) steps the stability becomes the trivial Ω(1). This result demonstrates a sharp contrast between the stability of NAG in the quadratic case and in the general convex, and disproves the conjecture of Chen et al. [10] that the uniform stability of NAG in the general convex setting is 𝑂 (𝑇 2/𝑛), as in the case of a quadratic objective.
Our results in fact apply to a simpler notion of stability—one that is arguably more fundamental in the context of iterative optimization methods—which we term initialization stability. The initialization stability of an algorithm 𝐴 (formally deﬁned in Section 2 below) measures the sensitivity of 𝐴’s output to an (cid:178)-perturbation in its initialization point. For this notion, we demonstrate a construction of a smooth and convex objective function such that, for suﬃciently small (cid:178), the stability of 𝑇-steps
NAG is lower bounded by exp(Ω(𝑇))(cid:178). Here again, we exhibit a dramatic gap between the quadratic and general convex cases: for quadratic objectives, we show that the initialization stability of NAG is upper bounded by 𝑂 (𝑇 (cid:178)).
For completeness, we also prove initialization stability upper bounds in a few relevant convex optimization settings: for GD, we analyze both the smooth and non-smooth cases; for NAG, we give bounds for quadratic objectives as well as for general smooth ones. Table 1 summarizes the stability bounds we establish compared to existing bounds in the literature. Note in particular the remarkable exponential gap between the stability bounds for GD and NAG in the general smooth case, with respect to both stability deﬁnitions. Stability lower bounds for NAG are discussed in Sections 3 and 4; initialization stability upper bounds for the various settings and additional uniform stability bounds are detailed in the full version of the paper [3] .
Method
GD
GD
NAG
NAG
Setting convex, smooth convex, non-smooth convex, quadratic convex, smooth
Init. Stability Unif. Stability
Θ((cid:178))
√
Θ((cid:178) + η
𝑻)
𝑶 (𝑻(cid:178)) exp(Θ(𝑻))(cid:178)
Θ(𝑇/𝑛)
√
Θ(η
𝑇 + η𝑇/𝑛)
Θ(𝑇 2/𝑛) exp(Θ(𝑻))/𝒏
Reference
Hardt et al. [17]
Bassily et al. [5]
Chen et al. [10] (this paper)
Table 1: Stability bounds introduced in this work (in bold) compared to existing bounds. For simplicity, all bounds in the smooth case are for η = Θ(1/β). The lower bounds for NAG are presented here in a simpliﬁed form and the actual bounds exhibit a ﬂuctuation in the increase of stability; see also Fig. 2 and the precise results in Sections 3 and 4.
Finally, we remark that our focus here is on the general convex (and smooth) case, and we do not provide formal results for the strongly convex case. However, we argue that stability analysis in the latter case is not as compelling as in the general case. Indeed, a strongly convex objective admits a unique minimum, and so NAG will converge to an (cid:178)-neighborhood of this minimum in 𝑂 (log(1/(cid:178))) steps from any initialization, at which point its stability becomes 𝑂 ((cid:178)); thus, with strong convexity perturbations in initialization get quickly washed away as the algorithm rapidly converges to the unique optimum. (A similar reasoning also applies to uniform stability with strongly convex losses.) 2
1.2 Overview of Main Ideas and Techniques
We now provide some intuition to our constructions and highlight some of the key ideas leading to our results. We start by revisiting the analysis of the quadratic case which is simpler and better understood.
Why NAG is stable for quadratics: Consider a quadratic function 𝑓 with Hessian matrix 𝐻 (cid:23) 0.
For analyzing the initialization stability of NAG, let us consider two runs of the method initialized at
𝑥0, ˜𝑥0 respectively, and let (𝑥𝑡 , 𝑦𝑡 ), ( ˜𝑥𝑡 , ˜𝑦𝑡 ) denote the corresponding NAG iterates at step 𝑡. Further, let us denote by ∆𝑥
𝑡 (cid:44) 𝑥𝑡 − ˜𝑥𝑡 the diﬀerence between the two sequences of iterates. Using the update rule of NAG (see Eqs. (1) and (2) below) and the fact that for a quadratic 𝑓 , diﬀerences between gradients can be expressed as ∇ 𝑓 (𝑥) − ∇ 𝑓 (𝑥 (cid:48)) = 𝐻 (𝑥 − 𝑥 (cid:48)) for any 𝑥, 𝑥 (cid:48) ∈ ℝ𝑑, it is straightforward to show that the distance ∆𝑥
𝑡 evolves according to
This recursion can be naturally put in matrix form, leading to:
𝑡+1 = (𝐼 − η𝐻) (cid:0)(1 + γ𝑡 )∆𝑥
∆𝑥
𝑡 − γ𝑡 ∆𝑥
𝑡−1 (cid:1).
𝑡 (cid:89) (cid:19)
= (cid:18)∆𝑥
𝑡+1
∆𝑥
𝑡 (cid:18)(1 + γ𝑘 ) 𝐴 −γ𝑘 𝐴 (cid:19)
, (cid:19) (cid:18)∆𝑥 1
∆𝑥 0 0
𝐼
𝑘=1 where here 𝐴 = 𝐼 − η𝐻. Thus, for a quadratic 𝑓 , bounding the divergence (cid:107)∆𝑥
NAG sequences reduces to controlling the operator norm of the matrix product above, namely
𝑡 (cid:107) between the two (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
𝑡 (cid:89)
𝑘=1 (cid:18)(1 + γ𝑘 ) 𝐴 −γ𝑘 𝐴
𝐼 0 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
.
Remarkably, it can be shown that this norm is 𝑂 (𝑡) for any 0 (cid:22) 𝐴 (cid:22) 𝐼 and any choice of
−1 ≤ γ1, . . . , γ𝑡 ≤ 1. (This can be seen by writing the Schur decomposition of the involved matrices, as we show in the full version of the paper [3].1) As a consequence, the initialization stability of NAG for a quadratic objective 𝑓 is shown to grow only linearly with the number of steps 𝑡.
What breaks down in the general convex case: For a general convex (twice-diﬀerentiable and smooth) 𝑓 , the Hessian matrix is of course no longer ﬁxed across the execution. Assuming for simplicity the one-dimensional case, similar arguments show that the relevant operator norm is of the form (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
𝑡 (cid:89)
𝑘=1 (cid:18)(1 + γ𝑘 ) 𝐴𝑘 −γ𝑘 𝐴𝑘
𝐼 0 (cid:19)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
, where 0 ≤ 𝐴1, . . . , 𝐴𝑡 ≤ 1 are related to Hessians of 𝑓 taken at suitable points along the optimization trajectory. However, if 𝐴𝑘 are allowed to vary arbitrarily between steps, the matrix product above might explode exponentially fast, even in the one-dimensional case! Indeed, ﬁx γ𝑘 = 0.9 for all 𝑘, and set 𝐴𝑘 = 0 whenever 𝑘 mod 3 = 0 and 𝐴𝑘 = 1 otherwise; then using simple linear algebra the operator norm of interest can be shown to satisfy (cid:18)(cid:18)0 0 1 0 (cid:13) (cid:13) (cid:13) (cid:13) (cid:19) (cid:18)1.9 −0.9 (cid:19) (cid:18)1.9 −0.9 1 0 1 0 (cid:19)(cid:19) 𝑡/3(cid:13) (cid:13) (cid:13) (cid:13)
= (cid:13) (cid:13) (cid:13) (cid:13) (cid:18) 0 2.71 −1.71 0 (cid:19) 𝑡/3(cid:13) (cid:13) (cid:13) (cid:13)
≥ 1.15𝑡 .
How a hard function should look like: The exponential blowup we exhibited above hinged on a worst-case sequence 𝐴1, . . . , 𝐴𝑡 that varies signiﬁcantly between consecutive steps. It remains unclear, however, what does this imply for the actual optimization setup we care about, and whether such a sequence can be realized by Hessians of a convex and smooth function 𝑓 . Our main results essentially answer the latter question on the aﬃrmative and build upon a construction of such a function 𝑓 that directly imitates such a bad sequence.
Concretely, we generate a hard function inductively based on a running execution of NAG, where in each step we amend the construction with a “gadget” function having a piecewise-constant Hessian (that equals either 0 or the maximal β); see Fig. 1 for an illustration of this construct. The interval pieces are carefully chosen based on the NAG iterates computed so far in a way that a slightly perturbed 1Chen et al. [10] give an alternative argument based on Chebyshev polynomials. 3
Figure 1: A function (left) constructed of four instantiations of our “gadget” (right) at increasing sizes. During an interval with zero Hessian, the trajectory with the larger momentum gains distance. When reaching an interval with maximal Hessian (depicted here as iteration 𝑡), the “slow” trajectory experiences a larger gradient which gives it larger momentum and makes it become the “faster” one. execution would traverse through intervals with an appropriate pattern of Hessians that induces a behaviour similar to the one exhibited by the matrix products above, leading to an exponential blowup in the stability terms. Fig. 2 shows a simulation of the divergence between the two trajectories of NAG on the objective function we construct, illustrating how the divergence ﬂuctuates between positive and negative values, with its absolute value growing exponentially with time. More technical details on this construction can be found in Section 3. (a) construction for η = 1/(2β); (b) construction for η = 1/(10β).
Figure 2: Divergence between trajectories (in log-scale) along the optimization process for diﬀerent values of η.
At steps 𝑡 = Θ(𝑖/ηβ) (𝑖 = 1, 2, . . .) NAG experiences an exponential growth in the divergence, as it reaches an interval with maximal Hessian. The dashed lines depict our theoretical exponential lower bound.
From initialization stability to uniform stability: Finally, we employ a simple reduction to translate our results regarding initialization stability to relate to uniform stability in the context of empirical risk optimization, where one uses (full-batch) NAG to minimize the (convex, smooth) empirical risk induced by a sample 𝑆 of 𝑛 examples. Concretely, we show that by replacing a single example in 𝑆, we can arrive at a scenario where after one step of NAG on the original and modiﬁed samples the respective iterates are (cid:178) = Θ(1/𝑛) away from each other, whereas in the remaining steps both empirical risks simulate our bad function from before. Thus, we again end up with an exponential increase in the divergence between the two executions, that leads to a similar increase in the algorithmic (uniform) stability: the latter becomes as large as Ω(1) after merely 𝑇 = 𝑂 (log 𝑛) steps of full-batch NAG. The formal details of this reduction can be found in Section 4. 1.3 Discussion and Additional