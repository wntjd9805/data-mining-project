Abstract
Federated learning (FL) is a popular distributed learning framework that trains a global model through iterative communications between a central server and edge devices. Recent works have demonstrated that FL is vulnerable to model poisoning attacks. Several server-based defense approaches (e.g. robust aggre-gation) have been proposed to mitigate such attacks. However, we empirically show that under extremely strong attacks, these defensive methods fail to guar-antee the robustness of FL. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks. In this work, we propose a client-based defense, named White Blood Cell for Federated Learning (FL-WBC), which can mitigate model poisoning attacks that have already polluted the global model. The key idea of FL-WBC is to identify the parameter space where long-lasting attack effect on parameters resides and perturb that space during local training. Furthermore, we derive a certiﬁed robustness guarantee against model poisoning attacks and a convergence guarantee to FedAvg after applying our
FL-WBC. We conduct experiments on FasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model poisoning attacks. The results demon-strate that our method can effectively mitigate model poisoning attack impact on the global model within 5 communication rounds with nearly no accuracy drop under both IID and non-IID settings. Our defense is also complementary to existing server-based robust aggregation approaches and can further improve the robustness of FL under extremely strong attacks. Our code can be found at https://github.com/jeremy313/FL-WBC. 1

Introduction
Federated learning (FL) [1, 2] is a popular distributed learning approach that enables a number of edge devices to train a shared model in a federated fashion without transferring their local training data. However, recent works [3–12] show that it is easy for edge devices to conduct model poisoning attacks by manipulating local training process to pollute the global model through aggregation.
Depending on the adversarial goals, model poisoning attacks can be classiﬁed as untargeted model poisoning attacks [3–6], which aim to make the global model indiscriminately have a high error rate on any test input, or targeted model poisoning attacks [7–12], where the goal is to make the global model generate attacker-desired misclassiﬁcations for some particular test samples. Our work focuses 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
on the targeted model poisoning attacks introduced in [11, 12]. In this attack, malicious devices share a set of data points with dirty labels, and the adversarial goal is to make the global model output the same dirty labels given this set of data as inputs. Our work can be easily extended to many other model poisoning attacks (e.g., backdoor attacks), which shall be discussed in §4.
Several studies have been done to improve the robustness of FL against model poisoning attacks through robust aggregations [13–17], clipping local updates [7] and leveraging the noisy perturba-tion [7]. These defensive methods focus on only preventing the global model from being polluted by model poisoning attacks during the aggregation. However, we empirically show that these server-based defenses fail to guarantee the robustness when attacks are extremely strong. More importantly, we observe that as long as the global model is polluted, the impact of attacks on the global model will remain in subsequent rounds even if there are no subsequent attacks, and can not be mitigated by these server-based defenses. Therefore, an additional defense is needed to mitigate the poisoning attacks that cannot be eliminated by robust aggregation and will pollute the global model, which is the goal of this paper.
To achieve this goal, we ﬁrst propose a quantitative estimator named Attack Effect on Parameter (AEP). It estimates the effect of model poisoning attacks on global model parameters and infers information about the susceptibility of different instanti-ations of FL to model poisoning attacks. With our quantitative estimator, we explicitly show the long-lasting attack effect on the global model. Based on our analysis, we design a client-based defense named White Blood Cell for Federated Learning (FL-WBC), as shown in Figure 1, which can mitigate the model poisoning attacks that have already polluted the global model.
FL-WBC differs from previous server-based defenses in mitigat-ing the model poisoning attack that has already broken through the server-based defenses and polluted the global model. Thus, our client-based defense is complementary to current server-based defense and enhances the ro-bustness of FL against the model poisoning attack, especially against the extremely strong attacks that can not be mitigated during the aggregation. We evaluate our defense on Fashion-MNIST [18] and CIFAR10 [19] against the model poisoning attack [11] under IID (identically independently distributed) and non-IID settings. The results demonstrate that FL-WBC can effectively mitigate the attack effect on the global model in 1 communication round with nearly no accuracy drop under IID settings, and within 5 communication rounds for non-IID settings, respectively. We also conduct experiments by integrating the robust aggregation with FL-WBC. The results show that even though the robust aggregation is ineffective under extremely strong attacks, the attack can still be efﬁciently mitigated by applying FL-WBC.
Figure 1: Overview of FL-WBC.
Our key contributions are summarized as follows:
• To the best of our knowledge, this is the ﬁrst work to quantitatively assess the effect of model poisoning attack on the global model in FL. Based on our proposed estimator, we reveal the reason for the long-lasting effect of a model poisoning attack on the global model.
• We design a defense, which is also the ﬁrst defense to the best of our knowledge, to effectively mitigate a model poisoning attack that has already polluted the global model. We also derive a robustness guarantee in terms of AEP and a convergence guarantee to FedAvg when applying our defense.
• We evaluate our defense on Fashion-MNIST and CIFAR10 against state-of-the-art model poisoning attacks. The results show that our proposed defense can enhance the robustness of FL in an effective and efﬁcient way, i.e., our defense defends against the attack in fewer communication rounds with less model utility degradation. 2