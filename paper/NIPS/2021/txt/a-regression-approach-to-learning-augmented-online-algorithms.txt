Abstract
The emerging ﬁeld of learning-augmented online algorithms uses ML techniques to predict future input parameters and thereby improve the performance of online algorithms. Since these parameters are, in general, real-valued functions, a natural approach is to use regression techniques to make these predictions. We introduce this approach in this paper, and explore it in the context of a general online search framework that captures classic problems like (generalized) ski rental, bin packing, minimum makespan scheduling, etc. We show nearly tight bounds on the sample complexity of this regression problem, and extend our results to the agnostic setting. From a technical standpoint, we show that the key is to incorporate online optimization benchmarks in the design of the loss function for the regression problem, thereby diverging from the use of off-the-shelf regression tools with standard bounds on statistical error. 1

Introduction
A recent trend in online algorithms has seen the use of future predictions generated by ML techniques to bypass pessimistic worst-case lower bounds. A growing body of work has started to emerge in this area in the last few years addressing a broad variety of problems in online algorithms such as rent or buy, caching, metrical task systems, matching, scheduling, experts learning, stopping problems, and others (see related work for references). The vast majority of this literature is focused on using
ML predictions in online algorithms, but does not address the question of how these predictions are generated. This raises the question: what can we learn from data that will improve the performance of online algorithms? Abstractly, this question comes in two inter-dependent parts: the ﬁrst part is a learning problem where we seek to learn a function that maps the feature domain to predicted parameters, and the second part is to re-design the online algorithm to use these predictions. In this paper, we focus on the ﬁrst part of this design pipeline, namely we develop a regression approach to generating ML predictions for online algorithms. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Before delving into this question further, we note that there has been some recent research that focuses on the learnability of predicted parameters in online algorithms. Recently, Lavastida et al. [33], building on the work of Lattanzi et al. [32], took a data-driven algorithms approach to design online algorithms for scheduling and matching problems via learned weights. In this line of work, the goal is to observe sample inputs in order to learn a set of weights that facilitate better algorithms for instances from a ﬁxed distribution. In contrast, Anand et al. [4] relied on a classiﬁcation learning approach for the Ski Rental problem, where they aimed to learn a function that maps the feature set to a binary label characterizing the optimal solution. But, in general, the value of the optimal solution is a real-valued function, which motivates a regression approach to learning-augmented online algorithms that we develop in this paper.
To formalize the notion of an unknown optimal solution that we seek to learn via regression, we use the online search (ONLINESEARCH) framework. In this framework, there is as an input sequence
Σ = σ1, σ2, . . . available ofﬂine, and the actual online input is a preﬁx of this sequence ΣT =
σ1, σ2, . . . , σT , where the length of the preﬁx T is revealed online. Namely, in each online step t > 0, there are two possibilities: either the sequence ends, i.e., T = t, or the sequence continues, i.e.,
T > t. The algorithm must maintain, at all times t, a solution that is feasible for the current sequence, i.e., for the preﬁx Σt = σ1, . . . , σt. The goal is to obtain a solution that is of minimum cost among all the feasible solutions for the actual input sequence ΣT .
We will discuss applicability of the ONLINESEARCH framework in more detail in Section 1.2, but for a quick illustration now, consider the ski rental problem in this framework. In this problem, if the sequence continues on day t, then the algorithm must rent skis if it has not already bought them. In generalizations of the ski rental problem to multiple rental options, the requirement is that one of the rental options availed by the algorithm must cover day t. We will show in Section 1.2 that we can similarly model several other classic online problems in the ONLINESEARCH framework.
We use the standard notion of competitive ratio, deﬁned as the worst case ratio between the algorithm’s cost and the optimal cost, to quantify the performance of an online algorithm. For online algorithms with predictions, we follow the terminology in [41] that is now standard: we say that the consistency and robustness of an algorithm are its competitive ratios for correct predictions and for arbitrarily incorrect predictions respectively. Typically, we ﬁx consistency at 1 + (cid:15) for a hyper-parameter (cid:15) and aim to minimize robustness as a function of (cid:15).
We make some mild assumptions on the problem. First, we assume that solutions are composable, i.e., that adding feasible solutions for subsequences ensures feasibility over the entire sequence; second, that cost is monotone, i.e., the optimal cost for a subsequence is at most that for the entire sequence; and third, that the ofﬂine problem is (approximately or exactly) solvable. These assumptions hold for essentially all online problems we care for. 1.1 Our Contributions
As a warm up, we ﬁrst give an algorithm called DOUBLE for the ONLINESEARCH problem without predictions in Section 2. The DOUBLE algorithm has a competitive ratio of 4. We build on the
DOUBLE algorithm in Section 3, where we give an algorithm called PREDICT-AND-DOUBLE for the
ONLINESEARCH problem with predictions. We show that the PREDICT-AND-DOUBLE algorithm has a consistency of 1 + (cid:15) and robustness of O(1/(cid:15)), for any hyper-parameter (cid:15) > 0. We also show that this tradeoff between consistency and robustness is asymptotically tight.
Our main contributions are in Section 4. In this section, we model the question of obtaining a learning-augmented algorithm for the ONLINESEARCH problem in a regression framework. Speciﬁcally, we assume that the input comprises a feature vector x that is mapped by an unknown real-valued function f to an input for the ONLINESEARCH problem z. In the training phase, we are given a set of labeled samples of the form (x, z) from some (unknown to the algorithm) data distribution D. The goal of the learning algorithm is to produce a mapping from the feature space to algorithmic strategies for the ONLINESEARCH problem, such that when it gets an unlabeled (test) sample x from the same distribution D, the algorithmic strategy corresponding to x obtains a competitive solution for the actual input z in the test sample (that is unknown to the algorithm).
The learning algorithm employs a regression approach in the following manner. It assumes that the function f is from a hypothesis class F, and obtains an empirical minimizer in F for a carefully crafted loss function on the training samples. The design of this loss function is crucial since a bound 2
on this loss function is then shown to translate to a bound on the competitive ratio of the algorithmic strategy. (Indeed, we will show later that because of this reason, standard loss functions used in regression are inadequate for our purpose.) Finally, we use statistical learning theory for real-valued functions to bound the sample complexity of the learner that we designed.
Using the above framework, we show a sample complexity bound of O (cid:0) H·d (cid:1) for obtaining a competitive ratio of 1 + (cid:15), where H and d respectively represent the log-range of the optimal cost and a measure of the expressiveness of the function class F called its pseudo-dimension.1 We also extend this result to the so-called agnostic setting, where the function class F is no longer guaranteed to contain an exact function f that maps x to z, rather the competitive ratio is now in terms of the best function in this class that approximates f . We also prove nearly matching lower bounds for our sample complexity bounds in the two models. (cid:15)
Our framework can also be extended to the setting where the ofﬂine optimal solution is hard to compute, but there exists an algorithm with competitive ratio c given the cost of optimal solution.
In that case our algorithms gives a competitive ratio c(1 + (cid:15)), which can still be better than the competitive ratio without predictions (see examples in next subsection). 1.2 Applicability of the ONLINESEARCH framework
The ONLINESEARCH framework is applicable whenever an online algorithm beneﬁts from knowing the optimal value of the solution. Many online problems beneﬁt from this knowledge, which is sometimes called advice in the online algorithms literature. For concreteness, we give three examples of classic problems – ski rental with multiple options, online scheduling, and online bin packing – to illustrate the applicability of our framework. Our algorithm PREDICT-AND-DOUBLE (explained in more detail in section 3) successively predicts the optimal value of the solution and appends the corresponding solution to its output.
Ski Rental with Multiple Options. Generalizations of the ski rental problem with multiple options have been widely studied (e.g., [1, 34, 37, 19]), recently with ML predictions [44]. Suppose there are
V options (say coupons) at our disposal, where coupon i costs us Ci and is valid for di number of days. Given such a setup, we need to come up with a schedule: {(tk, ik), k = 1, 2 . . .} that instructs us to buy coupon ik at time tk. (The classic ski rental problem corresponds to having only two coupons C1 = 1, d1 = 1 and C2 = B, d2 → ∞.) Our ONLINESEARCH framework is applicable here: a solution that allows us to buy coupons valid time t is also a valid solution for all times s ≤ t.
Further, PREDICT-AND-DOUBLE can be implemented efﬁciently as we can compute OPT(t), for any time t using a dynamic program.
Online Scheduling. Next, we consider the classic online scheduling problem where the goal is to assign jobs arriving online to a set of identical machines so as to minimize the maximum load on any machine (called the makespan). For this algorithm, the classic list scheduling algorithm [26] has a competitive ratio of 2. A series of works [23, 14, 29, 2] improved the competitive ratio to 1.924, and currently the best known result has competitive ratio of (approx) 1.92 [20]; in fact, there are nearly matching lower bounds [25]. However, if the optimal make-span (OPT) is known, then these lower bounds can be overcome, and a signiﬁcantly better competitive ratio of 1.5 can be obtained in this setting [17] (see also [10, 31, 21, 22]). The ONLINESEARCH framework is applicable here with a slight modiﬁcation: whenever PREDICT-AND-DOUBLE tries to buy a solution corresponding to a predicted value of OPT, we execute the 1.5-approximation algorithm based on this value. The problem still satisﬁes the property that a solution for t jobs is valid for any preﬁx. We get a competitive ratio of 1.5 + O((cid:15)) that signiﬁcantly outperforms the competitive ratio of 1.92 without predictions.
Online Bin Packing. As a third example, we consider the online bin packing problem. In this problem, items arrive online and must be packed into ﬁxed-sized bins, the goal being to minimize the number of bins. (We can assume w.l.o.g., by scaling, that the bins are of unit size.) Here, it is known that the critical parameter that one needs to know/predict is not OPT but the number of items of moderate size, namely those sized between 1/2 and 2/3. If this is known, then there is a simple 1.5-competitive algorithm [5], which is not achievable without this additional knowledge.
Again, our ONLINESEARCH framework can be used to take advantage of this result. In this case, the application is not as direct, because predicting OPT does not yield the better algorithm. Nevertheless, 1Intuitively, the notion of pseudo-dimension extends that of the well-known VC dimension from binary to real-valued functions. 3
an inspection of the algorithm in [5] reveals the following strategy: The items are partitioned into three groups. The items of size ≥ 2/3 are assigned individual bins, items of size between 1/3 and 1/2 are assigned separate bins where at least two of them are assigned to each bin, and the remaining items are assigned a set of common bins. Clearly, the ﬁrst two categories can be handled online without any additional information; this means that we can deﬁne a surrogate OPT (call it OPT(cid:48)) that only captures the optimal number of bins for the common category. Note that the of prediction of OPT’ serves as a substitute for knowing the numbers of items of moderate size. Now, if OPT(cid:48) is known, then we can recover the competitive ratio of 3/2 by using a simple greedy strategy. This now allows us to use the
ONLINESEARCH framework where we predict OPT(cid:48). As earlier, the ONLINESEARCH framework can be applied with slight modiﬁcation: whenever PREDICT-AND-DOUBLE tries to buy a solution corresponding to a predicted value of OPT(cid:48), we execute the 1.5-competitive algorithm based on this value. The problem still satisﬁes the property that a solution for t items is valid for any preﬁx. 1.3 Motivation for a cognizant loss function
In this work, we explore the idea of a carefully crafted loss function that can help in making better predictions for the online decision task. To illustrate this, consider the problem of balancing the load between machines/clusters in a data center where remote users are submitting jobs. The goal is to minimize the maximum load on any machine, also called the makespan of the assignment. The optimal makespan, which we would like to predict, depends on the workload submitted by individual users who are currently active in the system. Therefore, we would like to use the user features to predict their behavior in terms of the workload submitted to the server. A typical feature vector would then be a binary vector encoding of the set of users currently active in the system, and based on this information, a learning model trained on historical behavior of the users can predict (say) a histogram of loads that these users are expected to submit, and therefore, the value of the optimal makespan.
The feature space can be richer, e.g., including contextual information like the time of the day, day of the week, etc. that are useful to more accurately predict user behavior. Irrespective of the precise learning model, the main idea in this paper is that the learner should try to optimize for competitive loss instead of standard loss functions. This is because the goal of the learner is not to accurately predict the workload of each user, but to eventually obtain the best possible makespan. For instance, a user who submits few jobs that are inconsequential to the eventual makespan need not be accurately predicted. Our technique automatically makes this adjustment in the loss function, thereby obtaining better performance on the competitive ratio. 1.4