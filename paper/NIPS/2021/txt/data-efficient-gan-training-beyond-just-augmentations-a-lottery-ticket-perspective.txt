Abstract
Training generative adversarial networks (GANs) with limited real image data generally results in deteriorated performance and collapsed models. To conquer this challenge, we are inspired by the latest observation, that one can discover independently trainable and highly sparse subnetworks (a.k.a., lottery tickets) from
GANs. Treating this as an inductive prior, we suggest a brand-new angle towards data-efﬁcient GAN training: by ﬁrst identifying the lottery ticket from the original
GAN using the small training set of real images; and then focusing on training that sparse subnetwork by re-using the same set. We ﬁnd our coordinated framework to offer orthogonal gains to existing real image data augmentation methods, and we additionally present a new feature-level augmentation that can be applied together with them. Comprehensive experiments endorse the effectiveness of our proposed framework, across various GAN architectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet, and multiple few-shot generation datasets). Codes are available at: https://github. com/VITA-Group/Ultra-Data-Efficient-GAN-Training.

Introduction 1
The quantity, diversity, and high quality of natural im-ages available in the general domain have played an essential role in the achieved breakthroughs of Gener-ative Adversarial Networks (GANs) [2–11] over the past few years. However, it could become challeng-ing or even infeasible for speciﬁc application domains to collect a sufﬁciently large-scale dataset, due to var-ious constraints on the imaging expense, subject type, image quality, privacy, copyright status, and more.
That prohibits GANs’ broader applications in these domains, e.g., for generating synthetic training data
[12]. Examples of such domains include medical im-ages, images from scientiﬁc experiments, images of rare species, or photos of a speciﬁc person or land-mark. Eliminating the need of immense datasets for
GAN training is highly demanded for those scenar-ios. Naively training GAN with scarce samples leads to overconﬁdent discriminators that overﬁt the small training data [13–15, 1]; it usually ends up with train-ing divergence and drastic performance degradation (evidenced later in Figure 2).
Figure 1: FIDs on training BigGAN on 10% train-ing data from CIFAR-100. Smaller distance to the origin indicates smaller FID/better performance.
Compared to the vanilla training baseline ((cid:72), i.e., dense model or 0% sparsity), our method’s Stage
I (•) ﬁnds highly sparse lottery tickets from the original BigGAN, with a range of sparsity up to 86.58%. Higher sparsity appears to bring better data-efﬁciency. Stage II further boosts the training of those found sparse subnetworks, by incorporat-ing existing data-level augmentation [1] and our newly proposed feature-level augmentation (•). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This paper addresses the above issue from a brand new perspective by decomposing the challenging
GAN training in limited data regimes into two sequential sub-problems: (i) ﬁnding independent train-able subnetworks (i.e., lottery tickets in GANs) [16, 17]; then (ii) training the located subnetworks, which we show is more data-efﬁcient by itself, and can further beneﬁt from aggressive augmentations (both the input data and feature levels). Either sub-problem becomes much less data-hungry to train, and the two sub-problems re-use the same small training set of real images. Although this paper focuses on tackling the data-efﬁcient training of GANs, such a coordinated framework might potentially be generalized to training other deep models with higher data efﬁciency too.
Our key enabling technique is to leverage the lottery ticket hypothesis (LTH) [18]. LTH shows the feasibility to locate highly sparse subnetworks (called “winning tickets”) that are capable of training in isolation to match or even outperform the performance of original unpruned models. Recently,
[17, 16] revealed the existence of winning ticket in GANs (called “GAN tickets”). However, none of the existing works discuss the inﬂuence of training data size on locating and training those tickets.
Our work takes one step further, and shows that one can identify the same high-quality GAN tickets even in the data-scarce regime. The found GAN tickets also serve as a sparse structural prior to solve the second sub-problem with less data, while maintaining an unimpaired trainability blessed by the
LTH assumption [18]. Figure 1 (the outer circle’s blue dots) evidences that we can identify sparse
GAN tickets that achieve superior performance than full GANs in the data-scarce scenarios.
The new lottery ticket angle complements the existing augmentation techniques [19–21], and we further show that they can be organically combined to boost performance further1. When we train the identiﬁed lottery ticket, we demonstrate its training can beneﬁt as well from the latest data-level augmentation strategies, ADA [15] and DiffAug [1]. Furthermore, we introduce a novel feature-level augmentation that can be applied in parallel to data-level. It injects adversarial perturbations into
GANs’ intermediate features to implicitly regularize both discriminator and generator. Combining the new feature-level and existing data-level augmentations in training GAN tickets leads to more stabilized training dynamics, and establishes new state-of-the-arts for data-efﬁcient GAN training.
Extensive experiments are conducted on a variety of the latest GAN architectures and datasets, which consistently validate the effectiveness of our proposal. For example, our BigGAN tickets at 36.00% and 67.24% sparsity levels reach an (FID, IS) of (23.14, 52.98) and (70.91, 7.03), on Tiny-ImageNet 64 × 64 and ImageNet 128 × 128, with 10% and 25% training data, respectively. On CIFAR-10 and
CIFAR-100, for SNGAN and BigGAN tickets at 67.24% ∼ 86.58% sparsity, our results with only 10% training data can even surpass their dense counterparts. Impressively, our method can generate high-quality images on par with other GAN transfer learning approaches, by training on as few as 100 real samples and without using any pre-training. 2