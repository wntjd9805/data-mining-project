Abstract
Recent studies show that private training data can be leaked through the gradients sharing mechanism deployed in distributed machine learning systems, such as federated learning (FL). Increasing batch size to complicate data recovery is often viewed as a promising defense strategy against data leakage. In this paper, we revisit this defense premise and propose an advanced data leakage attack with theoretical justiﬁcation to efﬁciently recover batch data from the shared aggregated gradients. We name our proposed method as catastrophic data leakage in vertical federated learning (CAFE). Comparing to existing data leakage attacks, our ex-tensive experimental results on vertical FL settings demonstrate the effectiveness of CAFE to perform large-batch data leakage attack with improved data recovery quality. We also propose a practical countermeasure to mitigate CAFE. Our results suggest that private data participated in standard FL, especially the vertical case, have a high risk of being leaked from the training gradients. Our analysis implies unprecedented and practical data leakage risks in those learning settings. The code of our work is available at https://github.com/DeRafael/CAFE. 1

Introduction
Federated learning (FL) [8, 24] is an emerging machine learning framework where a central server and multiple workers collaboratively train a machine learning model. Some existing FL methods consider the setting where each worker has data of a different set of subjects but sharing common features. This setting is also referred to data partitioned or horizontal FL (HFL). Unlike the HFL setting, in many learning scenarios, multiple workers handle data about the same set of subjects, but each has a different set of features. This case is common in ﬁnance and healthcare applications [6].
In these examples, data owners (e.g., ﬁnancial institutions and hospitals) have different records of those users in their joint user base, and so, by combining their features through FL, they can establish a more accurate model. We refer to this setting as feature-partitioned or vertical FL (VFL).
Compared with existing distributed learning paradigms, FL raises new challenges including data heterogeneity and privacy [20]. To protect data privacy, only model parameters and the change of parameters (e.g., gradients) are exchanged between server and workers [19, 15]. Recent works have studied how a malicious worker can embed backdoors or replace the global model in FL [2, 3, 27].
Furthermore, as exchanging gradients is often viewed as privacy-preserving protocols, little attention has been paid to information leakage from public shared gradients and batch identities.
In the context of data security and AI ethics, the possibility of inferring private user data from the gradients in FL has received growing interests [10, 14, 21], known as the data leakage problems. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Original
CAFE
DLG
Cosine similarity
SAPAG
BN regularizer
GC regularizer
Figure 1: Visual comparison between CAFE (our method) with the state-of-the-art data leakage attacks including DLG [32], Cosine similarity [11], SAPAG [25], BN regularzier [29] and GC regularizer [29] on Linnaeus 5 in VFL (4 workers, batch size = 40 and batch ratio = 0.05).
Previous works have made exploratory efforts on data recovery through gradients. See Section 2 and
Table 1 for details. However, existing approaches often have the limitation of scaling up large-batch data recovery and are lacking in theoretical justiﬁcation on the capability of data recovery, which may give a false sense of security that increasing the data batch size during training can prevent data leakage [30]. Some recent works provide sufﬁcient conditions for guaranteed data recovery, but the assumptions are overly restrictive and can be sometimes impractical, such as requiring the number of classes to be much larger than the number of recovered data samples [29].
To enhance scalability in data recovery and gain fundamental understanding on data leakage in VFL, in this paper we propose an advanced data leakage attack with theoretical analysis on the data recovery performance, which we call catastrophic data leakage in vertical federated learning (CAFE). As an illustration, Figure 1 demonstrates the effectiveness of CAFE for large-batch data recovery compared to existing methods. The contributions of this paper are summarized as follows.
C1) We develop a new data leakage attack named CAFE to overcome the limitation of current data leakage attacks on VFL. Leveraging the novel use of data index and internal representation alignments in VFL, CAFE is able to recover large-scale data in general VFL protocols.
C2) We provide theoretical guarantees on the recovery performance of CAFE, which permeates three steps of CAFE: (I) recovering gradients of loss with respect to the outputs of the ﬁrst fully connected (FC) layer; (II) recovering inputs to the ﬁrst FC layer; (III) recovering the original data.
C3) To mitigate the data leakage attack by CAFE, we develop a defense strategy which leverages the fake gradients and preserves the model training performance.
C4) We conduct extensive experiments on both static and dynamic VFL training settings to validate the superior data recovery performance of CAFE over state-of-the-art methods. 2