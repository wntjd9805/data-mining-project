Abstract
Prior works have found it beneﬁcial to combine provably noise-robust loss functions e.g., mean absolute error (MAE) with standard categorical loss function e.g. cross entropy (CE) to improve their learnability. Here, we propose to use Jensen-Shannon divergence as a noise-robust loss function and show that it interestingly interpolate between CE and MAE with a controllable mixing parameter. Furthermore, we make a crucial observation that CE exhibits lower consistency around noisy data points. Based on this observation, we adopt a generalized version of the Jensen-Shannon divergence for multiple distributions to encourage consistency around data points. Using this loss function, we show state-of-the-art results on both synthetic (CIFAR), and real-world (e.g. WebVision) noise with varying noise rates. 1

Introduction
Labeled datasets, even the systematically annotated ones, contain noisy labels [1]. Therefore, designing noise-robust learning algorithms are crucial for the real-world tasks. An important avenue to tackle noisy labels is to devise noise-robust loss functions [2, 3, 4, 5]. Similarly, in this work, we propose two new noise-robust loss functions based on two central observations as follows.
Observation I: Provably-robust loss functions can underﬁt the training data [2, 3, 4, 5].
Observation II: Standard networks show low consistency around noisy data points 1, see Figure 1.
We ﬁrst propose to use Jensen-Shannon divergence (JS) as a loss function, which we crucially show interpolates between the noise-robust mean absolute error (MAE) and the cross entropy (CE) that better ﬁts the data through faster convergence. Figure 2 illustrates the CE-MAE interpolation.
Regarding Observation II, we adopt the generalized version of Jensen-Shannon divergence (GJS) to encourage predictions on perturbed inputs to be consistent, see Figure 3. Notably, Jensen-Shannon divergence has previously shown promise for test-time robustness to domain shift [6], here we further argue for its training-time robustness to label noise. The key contributions of this work2 are:
• We make a novel observation that a network predictions’ consistency is reduced for noisy-labeled data when overﬁtting to noise, which motivates the use of consistency regularization.
• We propose using Jensen-Shannon divergence (JS) and its multi-distribution generaliza-tion (GJS) as loss functions for learning with noisy labels. We relate JS to loss functions that are based on the noise-robustness theory of Ghosh et al. [2]. In particular, we prove that JS generalizes CE and MAE. Furthermore, we prove that GJS generalizes JS by incorporating consistency regularization in a single principled loss function.
• We provide an extensive set of empirical evidences on several datasets, noise types and rates.
They show state-of-the-art results and give in-depth studies of the proposed losses. 1we call a network consistent around a sample (x) if it predicts the same class for x and its perturbations ( ˜x). 2implementation available at https://github.com/ErikEnglesson/GJS 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Validation Accuracy (b) Consistency Clean (c) Consistency Noisy
Figure 1: Evolution of a trained network’s consistency as it overﬁts to noise using CE loss. Here we plot the evolution of the validation accuracy (a) and network’s consistency (as measured by GJS) on clean (b) and noisy (c) examples of the training set of CIFAR-100 for varying symmetric noise rates when learning with the cross-entropy loss. The consistency of the learnt function and the accuracy closely correlate. This suggests that enforcing consistency may help avoid ﬁtting to noise.
Furthermore, the consistency is degraded more signiﬁcantly for the noisy data points. 2 Generalized Jensen-Shannon Divergence
We propose two loss functions, the Jensen-Shannon divergence (JS) and its multi-distribution generalization (GJS). In this section, we ﬁrst provide background and two observations that motivate our proposed loss functions. This is followed by deﬁnition of the losses, and then we show that
JS generalizes CE and MAE similarly to other robust loss functions. Finally, we show how GJS generalizes JS to incorporate consistency regularization into a single principled loss function. We provide proofs of all theorems, propositions, and remarks in this section in Appendix C. 2.1