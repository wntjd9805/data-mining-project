Abstract
Communication requires having a common language, a lingua franca, between agents. This language could emerge via a consensus process, but it may require many generations of trial and error. Alternatively, the lingua franca can be given by the environment, where agents ground their language in representations of the observed world. We demonstrate a simple way to ground language in learned representations, which facilitates decentralized multi-agent communication and coordination. We find that a standard representation learning algorithm – autoen-coding – is sufficient for arriving at a grounded common language. When agents broadcast these representations, they learn to understand and respond to each other’s utterances and achieve surprisingly strong task performance across a variety of multi-agent communication environments. 1

Introduction
An essential aspect of communication is that each pair of speaker and listener must share a common understanding of the symbols being used [8]. For artificial agents interacting in an environment, with a communication channel but without an agreed-upon communication protocol, this raises the question: how can meaningful communication emerge before a common language has been established?
To address this challenge, prior works have used supervised learning [18], centralized learning [14, 17, 28], or differentiable communication [6, 17, 28, 32, 41]. Yet, none of these mechanisms is representative of how communication emerges in nature, where animals and humans have evolved communication protocols without supervision and without a centralized coordinator [35]. The communication model that most closely resembles language learning in nature is a fully decentralized model, where agents’ policies are independently optimized. However, decentralized models perform poorly even in simple communication tasks [17] or with additional inductive biases [10].
We tackle this challenge by first making the following observations on why emergent communication is difficult in a decentralized multi-agent reinforcement learning setting. A key problem that prevents agents from learning meaningful communication is the lack of a common grounding in communication symbols [3, 10, 19]. In nature, the emergence of a common language is thought to be aided by physical biases and embodiment [29, 42] – we can only produce certain vocalizations, these sounds only can be heard a certain distance away, these sounds bear similarity to natural sounds in the environment, etc. – yet artificial communication protocols are not a priori grounded in aspects of the environment
Project page, code, and videos can be found at https://toruowo.github.io/marl-ae-comm/. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dynamics. This poses a severe exploration problem as the chances of a consistent protocol being found and rewarded is extremely small [17]. Moreover, before a communication protocol is found, the random utterances transmitted between agents add to the already high variance of multi-agent reinforcement learning, making the learning problem even more challenging [10, 28].
To overcome the grounding problem, an important question to ask is: do agents really need to learn language grounding from scratch through random exploration in an environment where success is determined by chance? Perhaps nature has a different answer; previous studies in cognitive science and evolutionary linguistics [20, 36, 39, 40] have provided evidence for the hypothesis that commu-nication first started from sounds whose meaning are grounded in the physical environment, then creatures adapted to make sense of those sounds and make use of them. Inspired by language learning in natural species, we propose a novel framework for grounding multi-agent communication: first ground speaking through learned representations of the world, then learn listening to interpret these grounded utterances. Surprisingly, even with the simple representation learning task of autoencoding, our approach eases the learning of communication in fully decentralized multi-agent settings and greatly improves agents’ performance in multi-agent coordination tasks that are nearly unsolvable without communication.
The contribution of our work can be summarized as follows:
• We formulate communication grounding as a representation learning problem and propose to use observation autoencoding to learn a common grounding across all agents.
• We experimentally validate that this is an effective approach for learning decentralized communica-tion in MARL settings: a communication model trained with a simple autoencoder can consistently outperform baselines across various MARL environments.
• In turn, our work highlights the need to rethink the problem of emergent communication, where we demonstrate the essential need for visual grounding. 2