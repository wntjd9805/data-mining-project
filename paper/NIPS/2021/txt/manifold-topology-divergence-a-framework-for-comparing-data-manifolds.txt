Abstract
We develop a framework for comparing data manifolds, aimed, in particular, towards the evaluation of deep generative models. We describe a novel tool,
Cross-Barcode(P,Q), that, given a pair of distributions in a high-dimensional space, tracks multiscale topology spacial discrepancies between manifolds on which the distributions are concentrated. Based on the Cross-Barcode, we introduce the
Manifold Topology Divergence score (MTop-Divergence) and apply it to assess the performance of deep generative models in various domains: images, 3D-shapes, time-series, and on different datasets: MNIST, Fashion MNIST, SVHN, CIFAR10,
FFHQ, chest X-ray images, market stock data, ShapeNet. We demonstrate that the MTop-Divergence accurately detects various degrees of mode-dropping, intra-mode collapse, mode invention, and image disturbance. Our algorithm scales well (essentially linearly) with the increase of the dimension of the ambient high-dimensional space. It is one of the ﬁrst TDA-based practical methodologies that can be applied universally to datasets of different sizes and dimensions, including the ones on which the most recent GANs in the visual domain are trained. The proposed method is domain agnostic and does not rely on pre-trained networks. 1

Introduction
Geometric perspective in working with data distributions has been pervasive in machine learning
[5, 8, 12, 10, 23, 20]. Reconstruction of the data from observing only a subset of its points has made a signiﬁcant step forward since the invention of Generative Adversarial Networks (GANs) [13]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite the exceptional success that deep generative models achieved, there still exists a longstanding challenge of good assessment of the generated samples quality and diversity [7].
For images, the Fréchet Inception Distance (FID) [16] is the most popular GAN evaluation measure.
However, FID is limited only to 2D images since it relies on pre-trained on ImageNet “Inception” network. FID unrealistically approximates point clouds by Gaussians in embedding space; also,
FID is biased [6]. Surprisingly, FID can’t be applied to compare adversarial and non-adversarial generative models since it is overly pessimistic to the latter ones [28].
The evaluation of generative models is about comparing two point clouds: the true data cloud Pdata and the model (generated) cloud Qmodel. In view of the commonly assumed Manifold Hypothesis
[5, 12], we develop a topology-based measure for comparing two manifolds: the true data manifold
Mdata and the model manifold Mmodel, by analysing samples Pdata ⊂ Mdata and Qmodel ⊂ Mmodel.
Contribution. In this work, we make the following contributions: 1. We introduce a new tool: Cross-Barcode(P, Q). For a pair of point clouds P and Q, the
Cross-Barcode(P, Q) records the differences in multiscale topology between two manifolds approximated by the point clouds; 2. We propose a new measure for comparing two data manifolds approximated by point clouds:
Manifold Topology Divergence (MTop-Div); 3. We apply the MTop-Div to evaluate performance of GANs in various domains: 2D images, 3D shapes, time-series. We show that the MTop-Div correlates well with domain-speciﬁc measures and can be used for model selection. Also it provides insights about evolution of generated data manifold during training; 4. We have compared the MTop-Div against 7 established evaluation methods: FID, discriminative score, MMD, JSD, 1-coverage, IMD and Geometry score and found that MTop-Div is able to capture subtle differences in data geometry; 5. We have essentially overcame the known TDA scalability issues and in particular have carried out the MTop-Div calculations on most recent datasets such as FFHQ, with dimensions D up to 107.
The source code is available at https://github.com/IlyaTrofimov/MTopDiv.