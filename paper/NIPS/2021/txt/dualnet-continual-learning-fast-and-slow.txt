Abstract
According to Complementary Learning Systems (CLS) theory [37] in neuroscience, humans do effective continual learning through two complementary systems: a fast learning system centered on the hippocampus for rapid learning of the specifics and individual experiences, and a slow learning system located in the neocortex for the gradual acquisition of structured knowledge about the environment. Motivated by this theory, we propose a novel continual learning framework named “DualNet", which comprises a fast learning system for supervised learning of pattern-separated representation from specific tasks and a slow learning system for unsupervised rep-resentation learning of task-agnostic general representation via a Self-Supervised
Learning (SSL) technique. The two fast and slow learning systems are complemen-tary and work seamlessly in a holistic continual learning framework. Our extensive experiments on two challenging continual learning benchmarks of CORE50 and miniImageNet show that DualNet outperforms state-of-the-art continual learning methods by a large margin. We further conduct ablation studies of different SSL ob-jectives to validate DualNet’s efficacy, robustness, and scalability. Code is publicly available at https://github.com/phquang/DualNet. 1

Introduction
Humans have the remarkable ability to learn and accumulate knowledge over their lifetime to perform different cognitive tasks. Interestingly, such a capability is attributed to the complex interactions among different interconnected brain regions [14]. One prominent model is the Complementary
Learning Systems (CLS) theory [37, 30] which suggests the brain can achieve such behaviors via two learning systems of the “hippocampus" and the “neocortex." Particularly, the hippocampus focuses on fast learning of pattern-separated representation of specific experiences. Via the memory consolidation process, the hippocampus’s memories are transferred to the neocortex over time to form a more general representation that supports long-term retention and generalization to new experiences. The two fast and slow learning systems always interact to facilitate fast learning and long-term remembering. Although deep neural networks have achieved impressive results [31], they often require having access to a large amount of i.i.d data while performing poorly on the continual learning scenarios over streams of task [19, 29, 36]. Therefore, the main focus of this study is exploring how the CLS theory can motivate a general continual learning framework with a better trade-off between alleviating catastrophic forgetting and facilitating knowledge transfer.
In literature, several continual learning strategies are inspired from the CLS theory principles, from using the episodic memory [36] to improving the representation [26, 44]. However, such techniques mostly use a single backbone to model both the the hippocampus and neocortex, which binds two representation types into the same network. Moreover, such networks are trained to minimize the supervised loss, they lack a separate and specific slow learning component that supports general representation learning. During continual learning, the representation obtained by repeatedly 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of the DualNet’ architecture. DualNet consists of (i) a slow learner (in blue) that learns representation by optimizing an SSL loss using samples from the memory, and (ii) a fast learner (in orange) that adapts the slow net’s representation for quick knowledge acquisition of labeled data. Both learners can be trained synchronously. performing supervised learning on a small amount of memory data can be prone to overfitting and may not generalize well across tasks. Consider that in continual learning, unsupervised representation [20, 40] is often more resisting to forgetting compared to supervised representation, which yields little improvements [25]; we propose to decouple representation learning from the supervised learning into two separate systems. To achieve this goal, in analogy to the slow learning system in neocortex, we propose to implement the slow general representation learning system using Self-Supervised Learning (SSL) [39]. Note that recent SSL works focus on the pre-training phase, which is not trivial to apply for continual learning as it is extensive in both storage and computational cost [28]. We argue that
SSL should be incorporated into the continual learning process while decoupling from the supervised learning phase into two separate systems. Consequently, the SSL’s slow representation is more general and can capture the intrinsic characteristics from data, which facilitates better generalization to both old and new tasks.
Inspired by the CLS theory [37], we propose DualNet (for Dual Networks, depicted in Figure 1), a novel framework for continual learning comprising two complementary learning systems: a slow learner that learns generic features via self-supervised representation learning, and a fast learner that adapts the slow learner’s features to quickly attain knowledge from labeled samples via a novel per-sample based adaptation mechanism. During the supervised learning phase, an incoming labeled sample triggers the fast learner to make predictions by querying and adapting the slow learner’s representation. Then, the incurred loss will be backpropagated through both learners to consolidate the current supervised learning pattern for long-term retention. Concurrently, the slow learner is always trained in the background by minimizing an SSL objective using only the memory data. Therefore, the slow and fast networks learning are completely synchronous, allowing
DualNet to continue to improve its representation power even in practical scenarios where labeled data are delayed [13] or even limited, which we will demonstrate in Section 4.6. Lastly, we focus on developing DualNet for the online continual learning settings [36, 2] since it is more challenging to optimize deep networks in such scenarios [51, 3]. In the batch continual learning setting [46], the model is allowed to revisit data within the current task and can achieve good representations when learning the current task.
In summary, our work makes the following contributions: 1. We propose DualNet, a novel continual learning framework comprising two key components of fast and slow learning systems, which closely models the CLS theory. 2. We propose a novel learning paradigm for DualNet to efficiently decouple the representation learning from supervised learning. Specifically, the slow learner is trained in the background with SSL to maintain a general representation. Concurrently, the fast learner is equipped with a novel adaptation mechanism to quickly capture new knowledge. Notably, unlike existing adaptation techniques, our proposed mechanism does not require the task identifiers. 3. We conduct extensive experiments to demonstrate DualNet’s efficacy, robustness to the slow learner’s objectives, and scalability to the computational resources. 2
2 Method 2.1 Setting and Notations
We consider the online continual learning setting [36, 8] over a continuum of data D = {xi, ti, yi}i, where each instance is a labeled sample {xi, yi} with an optional task identifier ti. Each labeled data sample is drawn from an underlying distribution P t(X, Y ) that represents a task and can suddenly change to P t+1, indicating a task switch. When the task identifier t is given as an input, the setting follows the task-aware setting where only the corresponding classifier is selected to make a prediction [36]. When the task identifier is not provided, the model has a shared classifier for all classes observed so far, which follows the task-free setting [7, 2]. We consider both scenarios in our experiments. A common continual learning strategy is employing an episodic memory M to store a subset of observed data and interleave them when learning the current samples [36, 9]. From M, we use M to denote a randomly sampled mini-batch, and M A, M B to denote two views of M obtained by applying two different data transformations. Lastly, we denote ϕ as the parameter of the slow network that learns general representation from the input data and θ as the parameter of the fast network that learns the transformation coefficients. 2.2 DualNet Architecture
DualNet learns the data representation independent of the task’s label, which allows for better generalization capabilities across tasks in the continual learning scenario. The model consists two main learning modules (Figure 1): (i) the slow learner is responsible for learning a general, task-agnostic representation; and (ii) the fast learner learns with labeled data from the continuum to quickly capture the new information and then consolidate the knowledge to the slow learner.
DualNet learning can be broken down into two synchronous phases. First, the self-supervised learning phase in which the slow learner optimizes a Self-Supervised Learning (SSL) objective using unlabeled data from the episodic memory M. Second, the supervised learning phase happens whenever a labeled sample arrives, which triggers the fast learner to first query the representation from the slow learner and adapt it to learn this sample. The incurred loss will be backpropagated into both learners for supervised knowledge consolidation. Additionally, the fast learner’s adaptation is per-sample-based and does not require additional information such as the task identifiers. Note that DualNet uses the same episodic memory’s budget as other methods to store the samples and their labels, but the slow learner only requires the samples while the fast learner uses both samples and their labels. 2.3 The Slow Learner
The slow learner is a standard backbone network ϕ trained to optimize an SSL loss, denoted by LSSL.
As a result, any SSL objectives can be applied in this step. However, to minimize the additional computational resources while ensuring a general representation, we only consider the SSL loss that (i) does not require additional memory unit (such as the negative queue in MoCo [23]), (ii) does not always maintain an additional copy of the network (such as BYOL [21]), and (iii) does not use handcrafted pretext losses (such as RotNet [16] or JiGEN [6]). Therefore, we consider
Barlow Twins [59], a recent state-of-the-art SSL method that achieved promising results with minimal computational overheads. Formally, Barlow Twins requires two views M A and M B by applying two different data transformations to a batch of images M sampled from the memory. The augmented data are then passed to the slow net ϕ to obtain two representations ZA and ZB. The Barlow Twins loss is defined as:
LBT ≜ (cid:88) (1 − Cii)2 + λBT (cid:88) (cid:88)
C2 ij, where λBT is a trade-off factor, and C is the cross-correlation matrix between ZA and ZB: i i j̸=i
Cij ≜ (cid:113)(cid:80) (cid:80) b zA b,izB b,j b,i)2(cid:113)(cid:80)
B(zA
B(zB b,j)2 (1) (2) with b denotes the mini-batch index and i, j are the vector dimension indices. Intuitively, by optimiz-ing the cross-correlation matrix to be identity, Barlow Twins enforces the network to learn essential information that is invariant to the distortions (unit elements on the diagonal) while eliminating the redundancy information in the data (zero element elsewhere). In our implementation, we follow the 3
Figure 2: A demonstration of the slow and fast learners’ interaction during the supervised learning or inference on a standard ResNet [22] backbone. standard practice in SSL to employ a projector on top of the slow network’s last layer to obtain the representations ZA, ZB. For supervised learning with the fast network, which will be described in
Section 2.4, we use the slow network’s last layer as the representation Z.
In most SSL methods, the LARS optimizer [58] is employed for distributed training across many devices, which takes advantage of a large amount of unlabeled data. However, in continual learning, the episodic memory only stores a small number of samples, which are always changing because of the memory updating mechanism. As a result, the data distribution in the episodic memory always drifts throughout learning, and the SSL loss in DualNet presents different challenges compared to the traditional SSL optimization. Particularly, although the SSL objective in continual learning can be easily optimized using one device, we need to quickly capture the knowledge of the currently stored samples before the newer ones replace them. In this work, we propose to optimize the slow learner using the Look-ahead optimizer [61], which performs the following updates:
˜ϕk ← ˜ϕk−1 − ϵ∇ ˜ϕk−1
ϕ ←ϕ + β( ˜ϕK − ϕ),
LBT , with ˜ϕ0 ← ϕ and k = 1, . . . , K (3) (4) where β is the Look-ahead’s learning rate and ϵ is the Look-ahead’s SGD learning rate. As a special case of K = 1, the optimization reduces to the traditional optimization of LBT using SGD. By performing K > 1 updates using a standard SGD optimizer, the look-ahead weight ˜ϕK is used to perform a momentum update for the original slow learner ϕ. As a result, the slow learner optimization can explore regions that are undiscovered by the traditional optimizer and enjoys faster training convergence [61]. Note that SSL focuses on minimizing the training loss rather than generalizing this loss to unseen samples, and the learned representation requires to be adapted to perform well on a downstream task. Therefore, such properties make the Look-ahead optimizer a more suitable choice over the standard SGD to train the slow learner.
Lastly, we emphasize that although we choose to use Barlow Twins as the SSL objective in this work,
DualNet is compatible with any existing methods in the literature, which we will explore empirically in Section 4.3. Moreover, we can always train the slow learner in the background by optimizing
Equation 1 synchronously with the continual learning of the fast learner, which we will detail in the following section. 2.4 The Fast Learner
Given a labeled sample {x, y}, the fast learner’s goal is utilizing the slow learner’s representation to quickly learn this sample via an adaptation mechanism. In this work, we propose a novel context-free adaptation mechanism by extending and improving the channel-wise transformation [42, 43] to the general continual learning setting. Particularly, instead of generating the transformation coefficients based on the task-identifier, we propose to train the fast learner to learn such coefficients from the raw pixels in the image x. Importantly, the transformation is pixel-wise instead of channel-wise to compensate for the missing input of task identifiers. Formally, let {hi}L i=1 be the feature maps from the slow learner’s layers on the image x, e.g. h1, h2, h3, h4 are outputs from four residual blocks in
ResNets [22], our goal is to obtain the adapted feature h′
L conditioned on the image x. Therefore, we 4
ml =gθ,l(h′ h′ l =hl ⊗ ml, design the fast learner as a simple CNN with L layers, and the adapted feature h′
L is obtained as l−1), with h′ 0 = x and l = 1, . . . , L (5)
∀l = 1, . . . , L, (6) where ⊗ denotes the element-wise multiplication, gθ,l denotes the l-th layer’s output from the fast network θ and has the same dimension as the corresponding slow feature hl. The final layer’s transformed feature h′
L will be fed into a classifier for prediction. Thanks to the simplicity of the transformation, the fast learner is light-weight but still can take advantage of the slow learner’s rich representation. As a result, the fast network can quickly capture knowledge in the data stream, which is suitable for online continual learning. Figure 2 illustrates the fast and slow learners’ interaction during the supervised learning or inference phase.
The Fast Learner’s Objective To further facilitate the fast learner’s knowledge acquisition during supervised learning, we also mix the current sample with previous data in the episodic memory, which is a form of experience replay (ER). Particularly, given the incoming labeled sample {x, y} and a mini-batch of memory data M belonging to a past task k, we consider the ER with a soft label loss [53] for the supervised learning phase as:
Ltr = CE(π(DualNet(x), y) + 1
|M |
|M | (cid:88) i=1
CE(π(ˆyi), yi) + λtrDKL
π (cid:18) (cid:18) ˆyi
τ (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
π (cid:18) ˆyk
τ (cid:19)(cid:19)
, (7) where CE is the cross-entropy loss , DKL is the KL-divergence, ˆy is the DualNet’s prediction, ˆyk is snapshot of the model’s logit (the fast learner’s prediction) of the corresponding sample at the end of task k, π(·) is the softmax function with temperature τ , and λtr is the trade-off factor between the soft and hard labels in the training loss. Similar to [43, 5], Equation 7 requires minimal additional memory to store the soft label ˆy in conjunction with the image x and the hard label y. 3