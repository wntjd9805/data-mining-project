Abstract
In natural language processing, most models try to learn semantic representa-tions merely from texts. The learned representations encode the “distributional semantics” but fail to connect to any knowledge about the physical world. In contrast, humans learn language by grounding concepts in perception and action and the brain encodes “grounded semantics” for cognition. Inspired by this notion and recent work in vision-language learning, we design a two-stream model for grounding language learning in vision. The model includes a VGG-based visual stream and a Bert-based language stream. The two streams merge into a joint representational space. Through cross-modal contrastive learning, the model ﬁrst learns to align visual and language representations with the MS COCO dataset.
The model further learns to retrieve visual objects with language queries through a cross-modal attention module and to infer the visual relations between the retrieved objects through a bilinear operator with the Visual Genome dataset. After training, the model’s language stream is a stand-alone language model capable of embedding concepts in a visually grounded semantic space. This semantic space manifests principal dimensions explainable with human intuition and neurobiological knowl-edge. Word embeddings in this semantic space are predictive of human-deﬁned norms of semantic features and are segregated into perceptually distinctive clusters.
Furthermore, the visually grounded language model also enables compositional language understanding based on visual knowledge and multimodal image search with queries based on images, texts, or their combinations. 1

Introduction
Humans take much longer time to name a colored word when the color and the word mismatch (e.g.,
“red” shown in green) than when they match (e.g., “red” shown in red) [1]. This effect is an example of rich psychological evidence suggesting that humans learn language by grounding meanings to knowledge about the world [2, 3]. In contrast, most models in natural language processing (NLP)
[4–7] encode “distributional semantics” [8] learned from texts only. Put yourself as machines in a thought experiment for the “Chinese Room Argument” [9]. Imagine that you have to learn Chinese from scratch as your ﬁrst language. All that you have is a Chinese-to-Chinese dictionary. You might be able to relate a word to other words based on textual distributions. It is, however, impossible to learn word meanings without any additional explanation in reference to the physical world [10].
A language model may learn concepts from texts paired with sensory data, such as images. Joint vision-language learning has been explored for image captioning [11], visual question answering
[12], and pre-training vision models with weak supervision [13, 14]. In line with these studies, we 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
train a language model and a vision model jointly to match images and texts. We further analyze the semantic space obtained with the visually grounded language model. In this space, semantic embeddings are found to be organized and clustered by visual attributes, predictive of human-deﬁned norms of semantic features, useful for compositional language understanding and cross-modal image search. We expect this visually grounded language model to also be useful for understanding the computational basis of grounded cognition [15, 16].
Figure 1: Visual grounding of natural language (see Section 3.1). The visual and language streams take an image and its caption as input, respectively. The inner-product between the visual feature maps and the contextual word embeddings forms the 3D match-map that highlights the matching between visual and language content. The similarity score calculated from the match-map (see Eq. 1) is used to evaluate the cross-modal contrastive loss. 2