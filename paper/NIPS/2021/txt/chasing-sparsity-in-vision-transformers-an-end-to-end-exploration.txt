Abstract
Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacriﬁcing the achievable accuracy. We carry out the ﬁrst-of-its-kind comprehensive exploration, on taking a uniﬁed approach of integrating sparsity in
ViTs “from end to end”. Speciﬁcally, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a ﬁxed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the ﬁnal output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside
ViTs. We further co-explore data and architecture sparsity for additional efﬁciency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain signiﬁcantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we ﬁnd that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing “free lunch”. For example, our sparsiﬁed DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32%
FLOPs and 4.40% running time savings. Our codes are available at https:
//github.com/VITA-Group/SViTE. 1

Introduction
Recent years have seen substantial efforts devoted to scaling deep networks to enormous sizes.
Parameter counts are frequently measured in billions rather than millions, with the time and ﬁnancial outlay necessary to train these models growing in concert. The trend undoubtedly continues with the recent forefront of transformers [1–3] for computer vision tasks. By leveraging self-attention, reducing weight sharing such as convolutions, and feeding massive training data, vision transformers have established many new state-of-the-art (SOTA) records in image classiﬁcation [1, 2], object de-tection [4–7], image enhancement [8, 9], and image generation [10–12]. Existing vision transformers and variants, despite the impressive empirical performance, have in general suffered from gigantic parameter-counts, heavy run-time memory usages, and tedious training. That naturally calls for the next step research of slimming their inference and training, without compromising the performance.
Model compression and efﬁcient learning are no strangers to deep learning researchers, although their exploration in the emerging vision transformer ﬁeld remains scarce [13]. Among the large variety of compression means [14], sparsity has been one of the central themes since the beginning [15]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Conventional approaches ﬁrst train dense networks, and then prune a large portion of parameters in the trained networks to zero. Those methods signiﬁcantly reduce the inference complexity. However, the price is to cost even more signiﬁcant computational resources and memory footprints at training, since they commonly require (multiple rounds of) re-training to restore the accuracy loss [15–17].
That price becomes particularly prohibitive for vision transformers, whose vanilla one-pass training is already much more tedious, slow, and unstable compared to training standard convolutional networks.
An emerging subﬁeld has explored the prospect of directly training smaller, sparse subnetworks in place of the full networks without sacriﬁcing performance. The key idea is to reuse the sparsity pattern found through pruning and train a sparse network from scratch. The seminal work of lottery ticket hypothesis (LTH) [18] demonstrated that standard dense networks contain sparse matching subnetworks (sometimes called “winning tickets”) capable of training in isolation to full accuracy.
In other words, we could have trained smaller networks from the start if only we had known which subnetworks to choose. Unfortunately, LTH requires to empirically ﬁnd these intriguing subnetworks by an iterative pruning procedure [18–27] , which still cannot get rid of the expensiveness of post-training pruning. In view of that, follow-up works reveal that sparsity patterns might emerge at the initialization [28, 29], the early stage of training [30, 31], or in dynamic forms throughout training
[32–34] by updating model parameters and architecture typologies simultaneously. These efforts shed light on the appealing prospect of “end to end” efﬁciency from training to inference, by involving sparsity throughout the full learning lifecycle.
This paper presents the ﬁrst-of-its-kind comprehensive exploration of integrating sparsity in vision transformers (ViTs) “from end to end”. With (dynamic) sparsity as the uniﬁed tool, we can improve the inference efﬁciency from both model and data perspectives, while also saving training memory costs. Our innovative efforts are unfolded along with the following three thrusts:
• From Dense to (Dynamic) Sparse: Our primary quest is to ﬁnd sparse ViTs without sacriﬁcing the achievable accuracy, and meanwhile trimming down the training memory overhead. To meet this challenging demand, we draw inspirations from the latest sparse training works [34, 35] that dynamically extract and train sparse subnetworks instead of training the full models. Sticking to a ﬁxed small parameter budget, our technique jointly optimizes model parameters and explores connectivity throughout the entire training process.
We term our ﬁrst basic approach as Sparse Vision Transformer Exploration (SViTE).
• From Unstructured to Structured: Most sparse training works [32, 33, 36–39, 38, 34, 40, 41, 35] restricted discussion to unstructured sparsity. To attain structured sparsity which is more hardware-friendly, unlike classical channel pruning available for convolutional networks, we customize a ﬁrst-order importance approximation [16, 42] to guide the prune-and-grow of self-attention heads inside ViTs. This seamlessly extends SViTE to its second variant of Structured Sparse Vision Transformer Exploration (S2ViTE).
• From Model to Data: We further conduct a uniﬁed co-exploration towards joint data and architecture sparsity. That is by plugging in a novel learnable token selector to determine the most vital patch embeddings in the current input sample. The resultant framework of
Sparse Vision Transformer Co-Exploration (SViTE+) remains to be end-to-end trainable and can gain additional efﬁciency.
Extensive experiments are conducted on ImageNet with DeiT-Tiny/Small/Base. Results of substantial computation savings and nearly undamaged accuracies consistently endorse our proposals’ effec-tiveness. Perhaps most impressively, we ﬁnd that the sparse (co-)training can even improve the ViT accuracy rather than compromising it, making sparsity a tantalizing “free lunch”. For example, apply-ing SViTE+ on DeiT-Small produces superior compressed ViTs at 50% model sparsity plus 5% data sparsity, saving 49.32% FLOPs and 4.40% running time, while attaining a surprising improvement of 0.28% accuracy; even when the data sparsity increases to 10% (the model sparsity unchanged), there is still no accuracy degradation, meanwhile saving 52.38% FLOPs and 7.63% running time. 2