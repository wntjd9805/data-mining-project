Abstract
The recovery of sparse data is at the core of many applications in machine learning and signal processing. While such problems can be tackled using (cid:96)1-regularization as in the LASSO estimator and in the Basis Pursuit approach, specialized algorithms are typically required to solve the corresponding high-dimensional non-smooth optimization for large instances. Iteratively Reweighted Least Squares (IRLS) is a widely used algorithm for this purpose due to its excellent numerical performance.
However, while existing theory is able to guarantee convergence of this algorithm to the minimizer, it does not provide a global convergence rate. In this paper, we prove that a variant of IRLS converges with a global linear rate to a sparse solution, i.e., with a linear error decrease occurring immediately from any initialization, if the measurements fulﬁll the usual null space property assumption. We support our theory by numerical experiments showing that our linear rate captures the correct dimension dependence. We anticipate that our theoretical ﬁndings will lead to new insights for many other use cases of the IRLS algorithm, such as in low-rank matrix recovery. 1

Introduction
The ﬁeld of sparse recovery deals with the problem of recovering an (approximately) sparse vector x from only few linear measurements, presented by an underdetermined system of linear equations of the form y = Ax. One approach to solve this problem is to consider the (cid:96)0-minimization under linear constraints, which is NP-hard in general [24, 54]. For computational reasons, instead of (cid:96)0-minimization, it is common practice to consider its convex relaxation min x∈RN
||x||1 subject to Ax = y, (P1)
∗kuemmerle@jhu.edu
†verdun@ma.tum.de
‡dominik.stoeger@ku.de
§The name order of the authors is alphabetical. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where A ∈ Rm×N , b ∈ Rm are given, which is referred to as (cid:96)1(-norm) minimization [20, 27, 64] or basis pursuit [18, 19] in the literature.
Unlike (cid:96)0-minimization, the optimization program (P1) is computationally tractable in general, and a close relationship of their minimizers has been recognized and well-studied in the theory of compressive sensing [12, 13, 26, 32]. In statistics and machine learning, an unconstrained variant of (P1), often called LASSO, amounts to the most well-studied tractable estimator for variable selection in high-dimensional inference [37, 48, 65]. (cid:96)1-minimization has many other applications and it was even called the modern least squares [14].
The tractability of (P1) becomes evident from the fact that it can be reformulated as a linear program
[66]. However, as many problems of interest in applications are high-dimensional and therefore challenging for standard linear programming methods, many specialized solvers for (P1) have been proposed, such as the Homotopy Method [28], primal-dual methods [15, 52], Alternating Direction
Method of Multipliers [9], Bregman iterative regularization [71] and Semismooth Newton Augmented
Lagragian Methods [46] and Iteratively Reweighted Least Squares (IRLS), the latter of which is in the focus of this paper.
Iteratively Reweighted Least Squares corresponds to a family of algorithms that minimizes non-smooth objective functions by solving a sequence of quadratic problems, with its idea going back to a method proposed by Weiszfeld for the Fermat-Weber problem [7, 69]. A variety of different problems such as robust regression in statistics [39, 53], total variation regularization in image processing
[2, 33, 55], joint learning of neural networks [72], robust subspace recovery [45] and the recovery of low-rank matrices [31, 42, 43, 51] can be solved efﬁciently by IRLS in practice, as it relies on simple linear algebra to solve the linear systems arising from the quadratic problems at each iteration, without the need of a careful initialization or intricate parameter tuning. On the other hand, the analysis of IRLS methods is typically challenging: General convergence results are often weak, and stronger convergence results are only available in particular cases; see Section 2.3 for more details.
IRLS for sparse recovery. In the sparse recovery context, the ﬁrst variants of IRLS were introduced in [34, 59] for the (cid:96)p-quasinorm minimization problem (Pp) with 0 < p ≤ 1 that is similar to (P1), but with (cid:107)x(cid:107)p instead of (cid:107)x(cid:107)1 as an objective. In [16], modiﬁcations of the method of [34, 59] using speciﬁc smoothing parameter update rules were observed to exhibit excellent numerical performance for solving (Pp), retrieving the underlying sparse vector when most of the methods fail. A useful fact is that IRLS is one of the few methods (ADMM being the other one [9]) that provides a framework to solve both constrained and unconstrained formulations of (cid:96)p-minimization problems.
A major step forward in the theoretical understanding of IRLS was achieved in [23], where the authors showed that a variant of IRLS for (P1) converges globally to an (cid:96)1-minimizer if the measurement operator A fulﬁlls the null space property of sufﬁcient order, which essentially ensures that an (cid:96)1-minimizer is actually sparse. However, since this proof relies on the existence of a convergent subsequence, their proof does not reveal any rate for global convergence. The analysis of [23] provides, furthermore, a locally linear convergence rate, but this local linear rate has the drawback that it only applies if the support of the true signal has been discovered, which is arguably the difﬁcult part of (cid:96)0-minimization—cf. Proposition 3.1 below and Section 4.1.
While several extensions and modiﬁcations of the IRLS algorithm in [23] have been proposed (see, e.g., [3, 30]), this following fundamental algorithmic question has remained unanswered:
What is the global convergence rate of the IRLS algorithm for (cid:96)1-minimization?
Our contribution. We resolve this question, formally stated in [62], and present a new IRLS algorithm that converges linearly to a sparse ground truth, starting from any initialization, as stated in
Theorem 3.2. Our algorithm returns a feasible solution with δ-accuracy, i.e., (cid:107)x∗ − xk(cid:107)1 ≤ δ, where x∗ is the underlying s-sparse vector, in k = O(N (cid:112)(log N )/m log(1/δ)) iterations. Analogous to [23], it is assumed that the measurement matrix A satisﬁes the so-called null space property
[21]. We also provide a similar result for approximately sparse vectors. Our proof relies on a novel quantiﬁcation of the descent of a carefully chosen objective function in the direction of the ground truth. Additionally, we support the theoretical claims by numerical simulations indicating that we capture the correct dimension dependence. We believe that the new analysis techniques in this paper are of independent interest and will pave the way for establishing global convergence rates for other variants of IRLS such as in low-rank matrix recovery [31]. 2
Notation. We denote the cardinality of a set I by |I| and the support of a vector x ∈ RN , i.e., the index set of its nonzero entries, by supp(x) = {j ∈ [N ] : xj (cid:54)= 0}. We call a vector s-sparse if at most s of its entries are nonzero. We denote by xI the restriction of x onto the coordinates indexed by I, and use the notation I c := [N ] \ I to denote the complement of a set I. Furthermore,
σs(x)(cid:96)1 denotes the (cid:96)1-error of the best s-term approximation of a vector x ∈ RN , i.e., σs(x)(cid:96)1 = inf{(cid:107)x − z(cid:107)1 : z ∈ RN is s-sparse}.
IRLS for sparse recovery 2
We now present a simple derivation of the Iteratively Reweighted Least Squares (IRLS) algorithm for (cid:96)1-minimization which is studied in this paper. IRLS algorithms can be interpreted as a variant of a Majorize-Minimize (MM) algorithm [63], as we will lay out in the following. It mitigates the non-smoothness of the (cid:107) · (cid:107)1-norm by using the smoothed objective function Jε : RN → R, which is deﬁned, for a given ε > 0, by
Jε(x) :=
N (cid:88) i=1 jε(xi) with jε(x) := (cid:40)|x|, (cid:16) x2
ε + ε 1 2 if |x| > ε, if |x| ≤ ε. (cid:17)
, (1)
The function Jε can be considered as a scaled Huber loss function which is widely used in robust regression analysis [40, 50]. Moreover, the function Jε is continuously differentiable and fulﬁlls
|x| ≤ jε(x) ≤ |x| + ε for each x ∈ R. Instead of minimizing the function Jε directly, the idea of IRLS is to minimize instead a suitable chosen quadratic function Qε(·, x), which majorizes
Jε such that Qε (z, x) ≥ Jε (z) for all z ∈ RN . This function is furthermore chosen such that
Qε (z, x) ≤ Jε (x). The latter inequality implies
Qε (x, x) = Jε (x) holds, which implies that min z∈Rn that by minimizing Qε(·, x), IRLS actually achieves an improvement in the value of Jε as well. More speciﬁcally, Qε (·, x) is deﬁned by
Qε(z, x) := Jε(x) + (cid:104)∇Jε(x), z − x(cid:105) + 1 2 (cid:104)(z − x), diag(wε(x))(z − x)(cid:105)
= Jε(x) + 1 2 (cid:104)z, diag(wε(x))z(cid:105) − 1 2 (cid:104)x, diag(wε(x))x(cid:105), (2) where ∇Jε(x) = (cid:32)(cid:40) xi
|xi| , xi
ε , if |xi| > ε if |xi| ≤ ε i=1 (cid:33)N is the gradient of Jε at x and the weight vector wε (x) ∈ RN is a vector of weights such that wε(x)i := [max(|xi|, ε)]−1 for i ∈ [N ]. The following lemma shows that Qε (·, ·) has indeed the above-mentioned properties. We refer to the supplementary material for a proof.
Lemma 2.1. Let ε > 0, let Jε : RN → R be deﬁned as in (1) and Qε : RN × RN → R as deﬁned in (2). Then, for any z, x ∈ RN , the following afﬁrmations hold: i. diag(wε(x))x = ∇Jε(x), ii. Qε(x, x) = Jε(x), iii. Qε(z, x) ≥ Jε(z).
As can be seen from the equality in (2), minimizing Qε(·, x) corresponds to a minimization of a (re-)weighted least squares objective (cid:104)·, diag(wε(x))·(cid:105), which lends its name to the method. Note that unlike a classical MM approach, however, IRLS comes with an update step of the smoothing parameter ε at each iteration. We provide an outline of the method in Algorithm 1. k A∗(AW −1
The weighted least squares update (3) can be computed such that xk+1 = W −1 k A∗)−1(y) with Wk = diag (wk), with the solution of the (m × m) linear system (AW −1 k A∗)z = y as a main computational step. This linear system is positive deﬁnite and suitable for the use of iterative solvers.
In [30], an analysis of how accurately the linear system of a similar IRLS method needs to be solved to ensure overall convergence. We note that for small εk, the Sherman-Woodbury formula [70] can be used so that the calculation of xk+1 boils down to solving a smaller linear system that is well-conditioned, c.f. the supplementary material for details. This numerically advantageous property is not shared by the methods of [3, 23, 30], as our smoothing update (4) is slightly different from the ones proposed in these papers. We refer to Section 2.2 for a discussion.
The update step of the smoothing parameter ε (4) for the IRLS algorithm under consideration requires an a priori estimate of the sparsity of the ground truth of the signal, a piece of information that is 3
Algorithm 1 Iteratively Reweighted Least Squares for (cid:96)1-minimization
Input: Measurement matrix A ∈ Rm×N , data vector y ∈ Rm, initial weight vector w0 ∈ RN (default: w0 = (1, 1, . . . , 1)).
Set ε0 = ∞. for k = 0, 1, 2, . . . do xk+1 := arg min (cid:104)z, diag (wk) z(cid:105) subject to Az = y, z∈RN (cid:18)
εk+1 := min
εk,
σs(xk+1)(cid:96)1
N (cid:19)
, (wk+1)i := end for return Sequence (xk)k≥1. 1 max (cid:0)|xk+1 i (cid:1)
|, εk+1 for each i ∈ [N ], (3) (4) (5) also needed by most of the methods for sparse reconstruction. In practice, an overestimation of s is not a problem for similar numerical results if the overestimation remains within small multiples of the sparsity of the signal. We note, however, that there are also versions of IRLS which do not require a-priori knowledge of s, e.g. [30, 68], as the update rule for the smoothing parameter is chosen differently. An interesting future research direction is to extend the analysis presented here to IRLS with such a smoothing parameter update.
A consequence of Lemma 2.1, step (4), the fact that ε (cid:55)→ Jε(z) is monotonously non-decreasing, and that k (cid:55)→ εk is non-increasing is that k (cid:55)→ Jε(z) is non-increasing in k. This implies that the iterates xk, xk+1 of Algorithm 1 fulﬁll
Jεk+1 (xk+1) ≤ Jεk (xk+1) ≤ Qεk (xk+1, xk) ≤ Qεk (xk, xk) = Jεk (xk). (6)
This shows in particular that the sequence (cid:8)Jεk k=0 is non-increasing. For this reason, it can be shown that each accumulation point of the sequence of iterates (xk)k≥0 is a (ﬁrst-order) stationary point of the smoothed (cid:96)1-objective Jε(·) subject to the measurement constraint imposed by A and y, where ε = limk→∞ εk (see [23, Theorem 5.3]). (cid:0)xk(cid:1)(cid:9)∞ 2.1 Null space property
As in [23], the analysis we present is based on the assumption that the measurement matrix A satisﬁes the so-called null space property [21, 36], which is a key concept in the compressed sensing literature (see, e.g., [32, Chapter 4] for an overview).
Deﬁnition 2.2. A matrix A ∈ Rm×N is said to satisfy the (cid:96)1-null space property ((cid:96)1-NSP) of order s ∈ N with constant 0 < ρs < 1 if for any set S ⊂ [N ] of cardinality |S| ≤ s, it holds that (cid:107)vS(cid:107)1 ≤ ρs(cid:107)vSc(cid:107)1, for all v ∈ ker(A).
In [32, Chapter 4], the property of Deﬁnition 2.2 was called stable null space property. The importance of the null space property is due to the fact that it gives a necessary and sufﬁcient criterion for the success of basis pursuit for sparse recovery, as the following theorem shows.
Theorem 2.3 ([32, Theorem 4.5]). Given a matrix A ∈ Rm×N , every vector x ∈ RN such that
||x||0 ≤ s is the unique solution of (P1) with Ax = y if and only if A satisﬁes the null space property of order s for some 0 < ρs < 1.
The (cid:96)1-NSP is implied by the restricted isometry property (see, e.g., [11]), which is fulﬁlled by a large class of random matrices with high probability. For example, this includes matrices with (sub-)gaussian entries and random partial Fourier matrices [6, 60]. 2.2 Existing theory
A predecessor of IRLS for the sparse recovery problem (P1), and more generally, for (cid:96)p-quasinorm minimization with 0 < p ≤ 1, is the FOCal Underdetermined System Solver (FOCUSS) as proposed by Gorodnitsky, Rao and Kreutz-Delgado [34, 59]. Asymptotic convergence of FOCUSS to a stationary point from any initialization was claimed in [59], but the proof was not entirely accurate, as 4
pointed out by [17]. One limitation of FOCUSS is that, unlike in IRLS as presented in Algorithm 1, no smoothing parameter ε is used, which leads to ill-conditioned linear systems.
To mitigate this, [16] proposed an IRLS method that uses smoothing parameters ε (such as used in
Qε deﬁned above) that are updated iteratively. It was observed that this leads to a better condition number for the linear systems to be solved in each step of IRLS and, furthermore, that this smoothing strategy has the advantage of ﬁnding sparser vectors if the weights of IRLS are chosen to minimize a non-convex (cid:96)p-quasinorm for p < 1.
Further progress for IRLS designed to minimize an (cid:96)1-norm was achieved in the seminal paper [23].
In [23], it was shown that if the measurement operator fulﬁlls a suitable (cid:96)1-null space property as in
Deﬁnition 2.2, an IRLS method with iteratively updated smoothing converges to an s-sparse solution, coinciding with the (cid:96)1-minimizer, if there exists one that is compatible with the measurements.
This method uses not exactly the update rule of (4), but rather updates the smoothing parameter such that εk+1 = min(εk, R(xk+1)s+1/N ), where R(xk+1)s+1 is the (s + 1)st-largest element of the set {|xk+1
|, j ∈ [N ]}. Furthermore, a local linear convergence rate of IRLS was established
[23, Theorem 6.1] under same conditions. j
However, the analysis of [23] has its limitations: First, there is a gap in the assumption of their convergence results between the sparsity s of a vector to be recovered and the order (cid:98)s of the
NSP of the measurement operator. Recently, this gap was circumvented in [3] with an IRLS algorithm that uses a smoothing update rule based on an (cid:96)1-norm, namely, εk+1 = min(εk, η(1 −
ρs)σs(xk+1)(cid:96)1/N ), where η ∈ (0, 1), and ρs is the NSP constant of the order s of the NSP fulﬁlled by the measurement matrix A—this rule is quite similar to the rule (4) that we use in Algorithm 1. In particular, [3, Theorem III.6] establishes convergence with local linear rate similar to [23] without the gap mentioned above. The main limitation, however, of the theory of [23] (which is shared by [3]) is that the linear convergence rate only holds locally, i.e., in a situation where the support of the sparse vector has already been identiﬁed, see also Section 3 and Section 4.1 for a discussion.
We ﬁnally mention three relevant papers for the theoretical understanding of IRLS. [5] established the correspondence between the IRLS algorithms and the Expectation-Maximization algorithm for constrained maximum likelihood estimation under a Gaussian scale mixture distribution. By doing so, they established similar results as those from [23], i.e., the global convergence of IRLS with local linear convergence rate (as can be seen from their equation (38), which similar to (7) below) but by using different techniques based on such correspondence. [62] explores the relationship of
IRLS for (cid:96)1-minimization and a slime mold dynamics, interpreting both as an instance of the same meta-algorithm. Without requiring any connection between sparse recovery and (cid:96)1-minimization, [29] shows that an IRLS-like algorithm for (P1), requires O(N 1/3 log(1/δ)/δ2/3 + log(N )/δ2) iterations to obtain a multiplicative error of 1 + δ on the minimizer ||x||1. Unlike our result Theorem 3.2, this corresponds not to a linear, but to a sublinear convergence rate. 2.3