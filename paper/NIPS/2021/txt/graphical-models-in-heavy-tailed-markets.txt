Abstract
Heavy-tailed statistical distributions have long been considered a more realistic statistical model for the data generating process in ﬁnancial markets in comparison to their Gaussian counterpart. Nonetheless, mathematical nuisances, including nonconvexities, involved in estimating graphs in heavy-tailed settings pose a sig-niﬁcant challenge to the practical design of algorithms for graph learning. In this work, we present graph learning estimators based on the Markov random ﬁeld framework that assume a Student-t data generating process. We design scalable numerical algorithms, via the alternating direction method of multipliers, to learn both connected and k-component graphs along with their theoretical convergence guarantees. The proposed methods outperform state-of-the-art benchmarks in an extensive series of practical experiments with publicly available data from the
S&P500 index, foreign exchanges, and cryptocurrencies. 1

Introduction
Graph learning frameworks are often designed based on the assumption that the observed graph signals are Gaussian distributed [1–9]. While such assumption for graphical models has found great success in many practical areas, which includes brain network analysis [10], psychological networks
[11], and single-cell sequencing [12], it inherently neglects scenarios where there may exist outliers or the underlying data is naturally heavy-tailed distributed. As a consequence, those methods often lack robustness and may not succeed in capturing a meaningful representation of the underlying graph [13].
Data from ﬁnancial instruments are well-known examples of such scenarios where heavy-tailedness and skewedness are present [14–19]. In addition, there has been a growing interest in methods for estimating graphical models in ﬁnancial markets, which hence demands the development of scalable and robust learning algorithms [20].
Perhaps one of the most prominent applications, clustering ﬁnancial time-series via graph techniques has been an active research topic [20–24]. Nonetheless, current techniques rely on the assumption that the underlying graph has a tree structure, which does bring advantages due to its hierarchical clustering properties, but also have been shown to be unstable [25–27] and not suitable when the data is not Gaussian distributed [28].
Motivated by practical challenging applications in ﬁnance, such as clustering of ﬁnancial instruments and network estimation, we investigate the problem of learning graph matrices whose structure follow that of a Laplacian matrix of an undirected weighted graph for which the data generating process is assumed to be Student-t distributed. In particular, the main contributions of this paper are as follows:
• We propose a novel formulation for learning undirected weighted graphs under the as-sumption that the data generating process is Student-t distributed. We solve the underlying 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learning problem via a carefully designed numerical algorithm based on the alternating direction method of multipliers (ADMM), along with the establishment of its theoretical convergence guarantees. We note that the proposed algorithm can be easily extended to account for additional linear constraints on the graph weights.
• We extend the proposed framework to account for heavy-tails and k-component graphs simultaneously, which enables a novel method for clustering ﬁnancial time-series.
• We present extensive practical results, with real-world data from the US stock market, foreign exchanges, and cryptocurrencies, that showcase clear advantages of including heavy-tail assumptions into graph learning frameworks when compared to state-of-the-art,
Gaussian-based methods.
Notation: Matrices (vectors) are denoted by bold, italic, capital (lowercase) roman letters like X, x.
Vectors are assumed to be column vectors. The (i, j) element of a matrix X ∈ Rn×p is denoted as
Xij. The i-th element of a vector x is denoted as xi. The i-th row of X is denoted as xi ∈ Rp×1.
Given a symmetric matrix A, λi(A) and λmax(A) denote the i-th smallest and maximum eigenvalue of A, respectively. The Moore-Penrose inverse of A is denoted as A†. The Frobenius norm of a matrix A is denoted as (cid:107)A(cid:107)F = (cid:112)tr (A(cid:62)A). The operator Diag : Rp → Rp×p creates a diagonal matrix with the elements of an input vector along its diagonal. The operator diag : Rp×p → Rp extracts the diagonal of a square matrix. For x ∈ Rp, (cid:107)x(cid:107)∞ = maxi|xi|. (x)+ denotes the projection on to the nonnegative orthant, i.e., (x)+ = max (0, x). 2