Abstract
Category-level object pose estimation aims to ﬁnd 6D object poses of previously unseen object instances from known categories without access to object CAD models. To reduce the huge amount of pose annotations needed for category-level learning, we propose for the ﬁrst time a self-supervised learning framework to estimate category-level 6D object pose from single 3D point clouds. During training, our method assumes no ground-truth pose annotations, no CAD models, and no multi-view supervision. The key to our method is to disentangle shape and pose through an invariant shape reconstruction module and an equivariant pose estimation module, empowered by SE(3) equivariant point cloud networks.
The invariant shape reconstruction module learns to perform aligned reconstruc-tions, yielding a category-level reference frame without using any annotations.
In addition, the equivariant pose estimation module achieves category-level pose estimation accuracy that is comparable to some fully supervised methods. Extensive experiments demonstrate the effectiveness of our approach on both complete and partial depth point clouds from the ModelNet40 benchmark, and on real depth point clouds from the NOCS-REAL 275 dataset. The project page with code and visualizations can be found at: dragonlong.github.io/equi-pose. 1

Introduction
Object pose estimation is a crucial computer vision task that is widely employed in robotics, human-object interaction, and augmented-reality applications. Previous work can be categorized broadly into two genres: classic instance-level 6D object pose estimation (e.g., PoseCNN [25]), which assumes the availability of exact CAD models for all object instances; and category-level 6D object pose estimation (e.g., NOCS [23]), which deﬁnes a category-level reference frame and is able to generalize to a category of object instances. The motivation underlying the category-level approach is to develop systems that can accommodate novel object instances that were unseen during training. This approach is therefore more applicable for general robotic vision systems (e.g., of a home robot) that must face complex environments, such as those encountered in everyday situations.
† Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To achieve the intra-category generalization, category-level 6D object pose estimation systems often require a signiﬁcantly larger amount of annotated training data than classic instance-level systems.
The reason for more training examples lies in the need to cover large intra-category shape variations among different object instances, in addition to the usual requirement for diversity in object poses, occlusion patterns, and environments. However, obtaining good 6D pose annotations is expensive and time-consuming. As a result, this line of research often relies heavily on synthetic training data.
In order to scale up category-level object pose estimation systems to allow for more object categories, there is a strong demand for self-supervised methods that do not require pose annotations.
Designing self-supervised methods for category-level pose estimation is very challenging. Although a few systems (e.g., Self6D [22]) have recently been proposed for self-supervised instance-level 6D object pose estimation, all of those systems have access to the CAD model of each instance during training (but without the pose annotations). These previous approaches are thus inapplicable to self-supervised category-level object pose estimation, for which no CAD model is available for any object instances during either training or testing. The lack of any CAD models and annotations further presents a formidable obstacle to the designer: the reference frame to deﬁne category-level pose is unspeciﬁed throughout the training. To the best of our knowledge, there is no existing work on fully self-supervised category-level pose estimation, probably due to this issue.
We therefore need to ﬁnd a way to allow the emergence of a category-level reference frame during training. A starting point is to consider the category-level reference frame known as Normalized
Object Coordinate Space (NOCS), which is manually deﬁned in [23]. Within the NOCS regression framework, all shapes are aligned in this space; also, dense canonical reconstruction of the visible points of the object of interest is performed. The relationship between the reference frame and canonical reconstruction inspires us to ask the following question: can a system learn to perform canonical reconstruction of object instances from the same object category, whose reconstruction space will naturally be a category-level reference frame? This canonical reconstruction presents two requirements: 1) all of the object reconstructions need to be aligned, and 2) the reconstruction should not change if the input object undergoes a change in pose. The second requirement suggests an SE(3)-invariant reconstruction of an observed object, which inspires us to consider point cloud processing networks that are SE(3)-invariant/equivariant.
Formally, given a set of point cloud transformations TA : RN ×3 → RN ×3 for A ∈ SE(3), a neural network φ : RN ×3 → F (where F is an arbitrary feature domain) is called equivariant if for each A there exists an equivariant transformation SA : F → F so that:
SA[φ(X)] = φ(TA[X]), ∀A ∈ SE(3), X ∈ RN ×3
Note that invariance is a special case of equivariance; the network is called invariant under the transformations when SA is an identity mapping.
Therefore, we propose to use an SE(3)-invariant network for shape reconstruction. To ﬁnd a self-supervision for this reconstruction task, we propose to jointly estimate the object pose P and use the predicted transformation in P to transform our reconstructions into the camera space so that a consistency loss can be formed between the proposed reconstruction and our input observation. In contrast to the pose invariance pursued by our shape reconstruction module, here the estimated 6D pose should naturally be equivariant with any SE(3) transformation applied to the input, which means that the estimated pose should undergo the same 6D transformation when the input object changes its 6D pose. We thus use an SE(3)-equivariant network for this pose estimation. Leveraging SE(3) invariance and equivariance, our proposed system is designed to disentangle shape and pose in a self-supervised manner. Note that we do not explicitly enforce shape alignment in reconstruction. In other words, our network has the freedom to choose canonical poses of each instance for reconstruction during training. Even so, we ﬁnd that our network automatically chooses to align all the instances for reconstruction (except for certain symmetry ambiguities), because the network is relatively “lazy” and such reconstruction is the easiest thing to do. Within this aligned reconstruction space, a category-level reference frame emerges without any supervision, which further deﬁnes our category-level pose that is to be estimated by our equivariant pose estimation module.
Extensive experiments show that this self-supervised disentanglement is successful for both complete and partial object observations. Our proposed method achieves very accurate 6D pose estimation on synthetic point clouds generated from ModelNet [24], and it works well on the real-world NOCS-REAL275 dataset [23]. An ablation study further shows the key role of equivariance and invariance in this self-supervised framework. 2
2