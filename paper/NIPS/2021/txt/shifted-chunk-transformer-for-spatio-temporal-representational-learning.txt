Abstract
Spatio-temporal representational learning has been widely adopted in various ﬁelds such as action recognition, video object segmentation, and action anticipation.
Previous spatio-temporal representational learning approaches primarily employ
ConvNets or sequential models, e.g., LSTM, to learn the intra-frame and inter-frame features. Recently, Transformer models have successfully dominated the study of natural language processing (NLP), image classiﬁcation, etc. However, the pure-Transformer based spatio-temporal learning can be prohibitively costly on memory and computation to extract ﬁne-grained features from a tiny patch. To tackle the training difﬁculty and enhance the spatio-temporal learning, we construct a shifted chunk Transformer with pure self-attention blocks. Leveraging the recent efﬁcient Transformer design in NLP, this shifted chunk Transformer can learn hierarchical spatio-temporal features from a local tiny patch to a global video clip. Our shifted self-attention can also effectively model complicated inter-frame variances. Furthermore, we build a clip encoder based on Transformer to model long-term temporal dependencies. We conduct thorough ablation studies to validate each component and hyper-parameters in our shifted chunk Transformer, and it outperforms previous state-of-the-art approaches on Kinetics-400, Kinetics-600,
UCF101, and HMDB51. 1

Introduction
Spatio-temporal representational learning tries to model complicated intra-frame and inter-frame re-lationships, and it is critical to various tasks such as action recognition [20], action detection [55, 52], object tracking [23], and action anticipation [21]. Deep learning based spatio-temporal representation learning approaches have been largely explored since the success of AlexNet on image classiﬁca-tion [24, 11]. Previous deep spatio-temporal learning can be mainly divided into two aspects: deep
ConvNets based methods [35, 15, 16] and deep sequential learning based methods [55, 28, 29].
Deep ConvNets based methods are primarily integrated various factorization techniques [51, 34], or a priori [16] for efﬁcient spatio-temporal learning [15]. Some works focus on extracting effec-tive spatio-temporal features [41, 8] or capturing complicated long-range dependencies [49]. Deep sequential learning based methods try to formulate the spatial and temporal relationships through advanced deep sequential models [28] or the attention mechanism [29].
On the other hand, the Transformer has become the de-facto standard for sequential learning tasks such as speech and language processing [44, 12, 54, 19]. The great success of Transformer on natural language processing (NLP) has inspired computer vision community to explore self-attention 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
.
Figure 1: The framework of the proposed shifted chunk Transformer which involves two main components, frame encoder (dark grey) and clip encoder. The frame encoder consists of N alternative blocks of image chunk self-attention (left) and shifted multi-head self-attention (MSA). structures for several vision tasks, e.g., image classiﬁcation [13, 40, 36], object detection [6], and super-resolution [33]. The main difﬁculty in pure-Transformer models for vision is that Transformers lack the inductive biases of convolutions, such as translation equivariance, and they require more data [13] or stronger regularisation [40] in the training. It is only very recently that, vision Transform (ViT), a pure Transformer architecture, has outperformed its convolutional counterparts in image classiﬁcation when pre-trained on large amounts of data [13]. However, the hurdle is aggravated when the pure-Transformer design is applied to spatio-temporal representational learning.
Recently, a few attempts have been made to design pure-Transformer structures for spatio-temporal representation learning [4, 5, 14, 2]. Simply applying Transformer to 3D video domain is compu-tationally intensive [4]. The Transformer based spatio-temporal learning methods primarily focus on designing efﬁcient variants by factorization along spatial- and temporal-dimensions [4, 5], or employing a multi-scale pyramid structure for a trade-off between the resolution and channel capacity while reducing the memory and computational cost [14]. The spatio-temporal learning capacity can be further improved by extracting more effective ﬁne-grained features through advanced and efﬁcient intra-frame and inter-frame representational learning.
In this work, we propose a novel spatio-temporal learning framework based on pure-Transformer, called shifted chunk Transformer as illustrated in Fig. 1, which extracts effective ﬁne-grained intra-frame features with a low computational complexity leveraging the recent advance of Transformer in NLP [22]. Specially, we divide each frame into several local windows called image chunks, and construct a hierarchical image chunk Transformer, which employs locality-sensitive hashing (LSH) to enhance the dot-product attention in each chunk and reduces the memory and computation consumption signiﬁcantly. To fully consider the motion effect of object, we design a robust self-attention module, shifted self-attention, which explicitly extracts correlations from nearby frames.
We further design a pure-Transformer based frame-wise attention module, clip encoder, to model the complicated inter-frame relationships with a minimal extra computational cost. Our contributions can be summarized as follows:
• We construct an image chunk self-attention to mine ﬁne-grained intra-frame features leverag-ing the recent advance of Transformer. The hierarchical image chunk Transformer employs locality-sensitive hashing (LSH) [3] to reduce the memory and computation consumption signiﬁcantly, which enables an effective spatio-temporal learning directly from a tiny patch.
• We build a shifted self-attention to fully consider the motion effect of objects, which yields effective modeling of complicated inter-frame variances in the spatio-temporal representa-2
tional learning. Furthermore, a clip encoder with a pure-Transformer structure is employed for frame-wise attention, which models complicated and long-term inter-frame relationships at a minimal extra cost.
• The shifted chunk Transformer with pure-Transformer outperforms previous state-of-the-art approaches on several action recognition benchmarks, including Kinectics-400 [20],
Kinetics-600 [7], UCF101 [39] and HMDB51 [25]. 2