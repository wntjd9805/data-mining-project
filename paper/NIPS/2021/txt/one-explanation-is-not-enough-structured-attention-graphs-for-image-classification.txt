Abstract
Saliency maps are popular tools for explaining the decisions of convolutional neural networks (CNNs) for image classiﬁcation. Typically, for each image of interest, a single saliency map is produced, which assigns weights to pixels based on their importance to the classiﬁcation. We argue that a single saliency map provides an incomplete understanding since there are often many other maps that can explain a classiﬁcation equally well. In this paper, we propose to utilize a beam search algorithm to systematically search for multiple explanations for each image. Results show that there are indeed multiple relatively localized explanations for many images. However, naively showing multiple explanations to users can be overwhelming and does not reveal their common and distinct structures. We introduce structured attention graphs (SAGs), which compactly represent sets of attention maps for an image by visualizing how different combinations of image regions impact the conﬁdence of a classiﬁer. An approach to computing a compact and representative SAG for visualization is proposed via diverse sampling. We conduct a user study comparing the use of SAGs to traditional saliency maps for answering comparative counterfactual questions about image classiﬁcations. Our results show that user accuracy is increased signiﬁcantly when presented with
SAGs compared to standard saliency map baselines. 1

Introduction
With the emergence of convolutional neural networks (CNNs) as the most successful learning paradigm for image classiﬁcation, the need for human understandable explanations of their decisions has gained prominence. Explanations lead to a deeper user understanding and trust of the neural network models, which is crucial for their deployment in safety-critical applications. They can also help identify potential causes of misclassiﬁcation. An important goal of explanation is for the users to gain a mental model of the CNNs, so that the users can understand and predict the behavior of the classiﬁer[17] in cases that have not been seen. A better mental model would lead to appropriate trust and better safeguards of the deep networks in the deployment process.
A popular line of research towards this goal has been to display attention maps, sometimes called saliency maps or heatmaps. Most approaches assign weights to image regions based on the importance of that region to the classiﬁcation decision, which is then visualized to the user. This approach implicitly assumes that a single saliency map with region-speciﬁc weights is sufﬁcient for the human to construct a reasonable mental model of the classiﬁcation decision for the particular image.
We argue that this is not always the case. Fig. 1(d-f) show three localized attention maps highlighting different regions. Each of these images, if given as input to the CNN, results in a very conﬁdent prediction of the correct category. However, this information is not apparent from a single saliency map as produced by current methods (Fig. 1(b-c)). This raises several questions: How many images have small localized explanations (i.e., attention maps) that lead to high conﬁdence predictions? Are 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Original
Image (b) Grad-CAM heatmap (c) I-GOS heatmap (d) Region 1 (98% conﬁdence) (e) Region 2 (97% conﬁdence) (f) Region 3 (93% conﬁdence)
Figure 1: An image (a) predicted as Goldﬁnch with two saliency maps (b) and (c) obtained from different approaches as explanations for the classiﬁer’s (VGGNet [27]) prediction. Each of these saliency maps creates a narrow understanding of the classiﬁer. In (d), (e) and (f), we present three diverse regions of the image that might not be deemed important by the singleton saliency maps (b) and (c), and yet are classiﬁed as the target class with high conﬁdence by the same classiﬁer
Figure 2: Example of a SAG. For the goldﬁnch image on the left, a SAG on the right is structured as a directed acyclic graph with each root node representing a minimal region of the image sufﬁcient to achieve a high conﬁdence for the classiﬁer’s prediction. Each child node is obtained by deleting a patch (denoted by red contour) from the parent, causing a drop in the classiﬁer’s conﬁdence. A signiﬁcant drop in conﬁdence implies the removed patch was of high importance to the classiﬁer.
More examples of SAGs are provided in the appendix there multiple distinct high conﬁdence explanations for each image, and if so, how to ﬁnd them?
How can we efﬁciently visualize multiple explanations to users to yield deeper insights?
The ﬁrst goal of this paper is to systematically evaluate the sizes and numbers of high-conﬁdence local attention maps of CNN image classiﬁcations. For this purpose, rather than adopting commonly used gradient-based optimization approaches, we employ discrete search algorithms to ﬁnd multiple high-conﬁdence attention maps that are distinct in their coverage.
The existence of multiple attention maps shows that CNN decisions may be more comprehensively explained with a logical structure in the form of disjunctions of conjunctions of features represented by local regions instead of a singleton saliency map. However, a signiﬁcant challenge in utilizing this as an explanation is to come up with a proper visualization to help users gain a more comprehensive mental model of the CNN. This leads us to our second contribution of the paper, Structured Attention
Graphs (SAGs) 1 , which are directed acyclic graphs over attention maps of different image regions.
The maps are connected based on containment relationships between the regions, and each map is accompanied with the prediction conﬁdence of the classiﬁcation based on the map (see Fig. 2 for an example). We propose a diverse sampling approach to select a compact and diverse set of maps for
SAG construction and visualization.
This new SAG visualization allows users to efﬁciently view information from a diverse set of maps, which serves as a novel type of explanation for CNN decisions. In particular, SAGs provide insight by decomposing local maps into sub-regions and making the common and distinct structures across maps explicit. For example, observing that the removal of a particular patch leads to a huge drop in the conﬁdence suggests that the patch might be important in that context. 1Source code for generating SAGs: https://github.com/viv92/structured-attention-graphs 2
Our visualization can also be viewed as representing a (probabilistic) Monotone Disjunctive Normal
Form (MDNF) Boolean expression, where propositional symbols correspond to primitive image regions we call ‘patches’. Each MDNF expression is a disjunction of conjunctions, where any one of the conjunctions (e.g., one of the regions in Fig. 1) is sufﬁcient for a high conﬁdent classiﬁcation.
Following [13], we call these minimal sufﬁcient explanations (MSEs). Each conjunction is true only when all the patches that correspond to its symbols are present in the image.
We conducted a large-scale user study (100 participants total) to compare SAGs to two saliency map methods. We wondered if participants can answer challenging counterfactual questions with the help of explanations , e.g., how a CNN model classiﬁes an image if parts of the image are occluded . In our user study, participants were provided two different occluded versions of the image (i.e., different parts of the image are occluded ) and asked to choose one that they think would be classiﬁed more positively. Results show that when presented with SAG, participants correctly answer signiﬁcantly more of these questions compared to the baselines, which suggests that SAGs help them build better mental models of the behavior of the classiﬁer on different subimages.
In summary, our contributions are as follows:
• With a beam search algorithm, we conducted a systematic study of the sizes and numbers of attention maps that yield high conﬁdence classiﬁcations of a CNN (VGGNet [27]) on ImageNet
[7]. We showed that the proposed beam search algorithm signiﬁcantly outperforms Grad-CAM and I-GOS in its capability to locate small attention maps to explain CNN decisions.
• We introduce Structured Attention Graphs (SAGs) as a novel representation to visualize image classiﬁcations by convolutional neural networks.
• We conducted a user study demonstrating the effectiveness of SAGs in helping users gain a deeper understanding of CNN’s decision making. 2