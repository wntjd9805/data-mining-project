Abstract
Features extracted from deep layers of classiﬁcation networks are widely used as image descriptors. Here, we exploit an unexplored property of these features: their internal dissimilarity. While small image patches are known to have similar statis-tics across image scales, it turns out that the internal distribution of deep features varies distinctively between scales. We show how this deep self dissimilarity (DSD) property can be used as a powerful visual ﬁngerprint. Particularly, we illustrate that full-reference and no-reference image quality measures derived from DSD are highly correlated with human preference. In addition, incorporating DSD as a loss function in training of image restoration networks, leads to results that are at least as photo-realistic as those obtained by GAN based methods, while not requiring adversarial training. 1

Introduction
Features extracted from deep layers of classiﬁcation networks are widely used as powerful image descriptors. These features are known to capture high level semantics [44] as well as low-level textural cues [14, 31] and are thus exploited in numerous applications, including image enhancement [24, 18, 10, 47], synthesis [52, 44, 5, 19], and editing [15, 2, 25], both in the form of per-element similarity measures (e.g. the perceptual loss [15, 18] and LPIPS [51]) and for comparing internal image distributions (e.g. the style loss [15], contextual loss [32], and projected distribution loss [10]).
In this paper, we propose to exploit a surprisingly dominant property of deep features: the dissimilarity between their internal distributions at different image scales. As opposed to small patches in pixel space, which are known to exhibit similar characteristics across scales [53, 16], here we show that deep features tend to vary signiﬁcantly for different scales of the same image. A glimpse into this phenomenon is provided in Fig. 1 for the VGG-19 network [46]. As can be seen, the network often outputs completely different classiﬁcation results for the same image at different resolutions. We show that this behavior is also characteristic of earlier stages within the network, and can therefore serve as a powerful image ﬁngerprint.
A naive approach to account for this phenomenon would be to aggregate deep feature descriptors from multiple image scales. For example, to measure discrepancy between two images, one could accumulate deep feature distribution distances, measured separately at different scales, as schemati-cally illustrated in Fig. 5 (middle) for the case of two scales. However, here we propose a different approach, which as we show, is far more powerful as a visual ﬁngerprint. Speciﬁcally, we present the deep self-dissimilarity (DSD) image descriptor, which captures differences between internal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Class variations between scales. The semantics captured by an image classiﬁer (VGG-19 in this case [46]), strongly depends on the image resolution. This phenomenon also occurs at earlier classiﬁcation layers; the deep features tend to vary signiﬁcantly when changing the input’s resolution.
We show how these dissimilarities can serve to construct full-reference and no-reference image quality measures, as well as loss functions for image restoration. feature distributions at different scales of an image. As we show, this descriptor can serve both as a full reference image ﬁdelity metric (i.e. by comparing the DSD’s of two images) and as a no-reference measure (quantifying the “naturalness” of an image according to its DSD). In both cases, our metrics are highly correlated with human perception, as we verify on user-annotated image quality assessment datasets. Furthermore, we demonstrate the effectiveness of DSD as a loss function for image restoration tasks. Our approach leads to high perceptual quality reconstructions that are at least comparable to GAN-based methods, while completely avoiding adversarial training. 2