Abstract
We consider the question of learning the natural parameters of a k-parameter mini-mal exponential family from i.i.d. samples in a computationally and statistically efﬁcient manner. We focus on the setting where the support as well as the natural parameters are appropriately bounded. While the traditional maximum likelihood estimator for this class of exponential family is consistent, asymptotically normal, and asymptotically efﬁcient, evaluating it is computationally hard. In this work, we propose a computationally efﬁcient estimator that is consistent as well as asymp-totically normal under mild conditions. We provide ﬁnite sample guarantees to achieve an ((cid:96)2) error of α in the parameter estimation with sample complexity
O(poly(k/α)) and computational complexity O(poly(k/α)). To establish these results, we show that, at the population level, our method can be viewed as the maximum likelihood estimation of a re-parameterized distribution belonging to the same class of exponential family. 1

Introduction
We are interested in the problem of learning the natural parameters of a minimal exponential family with bounded support. Consider a p-dimensional random vector x = (x1, · · · , xp) with support
X ⊂ Rp. An exponential family is a set of parametric probability distributions with probability densities of the following canonical form fx(x; θ) ∝ exp (cid:0)θT φ(x) + β(x)(cid:1), where x ∈ X is a realization of the underlying random variable x, θ ∈ Rk is the natural parameter,
φ : X → Rk is the natural statistic, k denotes the number of parameters, and β is the log base function. For representational convenience, we shall utilize the following equivalent representation of (1): (1) fx(x; Θ) ∝ exp (cid:18) (cid:10)(cid:10)Θ, Φ(x)(cid:11)(cid:11) (cid:19)
= exp (cid:18) (cid:88) i∈[k1],j∈[k2],l∈[k3] (cid:19)
Θijl × Φijl(x) (2) where Θ = [Θijl] ∈ Rk1×k2×k3 is the natural parameter, Φ = [Φijl] : X → Rk1×k2×k3 is the natural statistic, k1 × k2 × k3 − 1 = k, and (cid:10)(cid:10)Θ, Φ(x)(cid:11)(cid:11) denotes the tensor inner product, i.e., the sum of product of entries of Θ and Φ(x). An exponential family is minimal if there does not exist a nonzero tensor U ∈ Rk1×k2×k3 such that (cid:10)(cid:10)U, Φ(x)(cid:11)(cid:11) is equal to a constant for all x ∈ X .
The notion of exponential family was ﬁrst introduced by Fisher [17] and was later generalized by
Darmois [12], Koopman [30], and Pitman [40]. Exponential families play an important role in statistical inference and arise in many diverse applications for a variety of reasons: (a) they are analytically tractable, (b) they arise as the solutions to several natural optimization problems on the space of probability distributions, (c) they have robust generalization property (see [5, 2] for details). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Truncated (or bounded) exponential family, ﬁrst introduced by Hogg and Craig [20], is a set of parametric probability distributions resulting from truncating the support of an exponential family.
Truncated exponential families share the same parametric form with their non-truncated counterparts up to a normalizing constant. These distributions arise in many applications where we can observe only a truncated dataset (truncation is often imposed by during data acquisition) e.g., geolocation tracking data can only be observed up to the coverage of mobile signal, police department can often monitor crimes only within their city’s boundary.
The natural parameter Θ speciﬁes a particular distribution in the exponential family. If the natural statistic Φ and the support of x (i.e., X ) are known, then learning a distribution in the exponential family is equivalent to learning the corresponding natural parameter Θ. Despite having a long history, there has been limited progress on learning natural parameter Θ of a minimal truncated exponential family. More precisely, there is no known method (without any abstract condition) that is both computationally and statistically efﬁcient for learning natural parameter of the minimal truncated exponential family considered in this work. 1.1 Contributions
As the primary contribution of this work, we provide a computationally tractable method with statistical guarantees for learning distributions in truncated minimal exponential families. Formally, the learning task of interest is estimating the true natural parameter Θ∗ from i.i.d. samples of x obtained from fx(·; Θ∗). We focus on the setting where Θ∗ and Φ are appropriately bounded (see
Section 2). We summarize our contributions in the following two categories. 1. Computationally Tractable Estimator : Consistency, Normality, Finite Sample Guarantees.
Given n samples x(1) · · · , x(n) of x, we propose the following novel loss function to learn a distribu-tion belonging to the exponential family in (2):
Ln(Θ) = 1 n n (cid:88) t=1 exp (cid:0) − (cid:10)(cid:10)Θ, Φ(x(t))(cid:11)(cid:11)(cid:1), (3) where Φ(·) = Φ(·) − EUX [Φ(·)] with UX being the uniform distribution over X . We establish that the estimator ˆΘn obtained by minimizing Ln(Θ) over all Θ in the constraint set Λ, i.e.,
ˆΘn ∈ arg min
Θ∈Λ
Ln(Θ), (4) is consistent and (under mild further restrictions) asymptotically normal (see Theorem 4.2). We obtain an (cid:15)-optimal solution ˆΘ(cid:15),n of the convex minimization problem in (4) (i.e., Ln( ˆΘ(cid:15),n) ≤ Ln( ˆΘn) + (cid:15)) by implementing a projected gradient descent algorithm with O(poly(k1k2/(cid:15)))1 iterations (see
Lemma 3.1). Finally, we provide rigorous ﬁnite sample guarantees for ˆΘ(cid:15),n (with (cid:15) = O(α2)) to achieve an error of α (in the tensor (cid:96)2 norm) with respect to the true natural parameter Θ∗ with O(poly(k1k2/α)) samples and O(poly(k1k2/α)) computations (see Theorem 4.3). By letting certain additional structure on the natural parameter, we allow our framework to capture various constraints on the natural parameter including sparse, low-rank, sparse-plus-low-rank (see Section 2.1). 2. Connections to maximum likelihood estimation (MLE) of a re-parameterized distribution.
We establish connections between our method and the MLE of the distribution fx(·; Θ∗ − Θ). We show that the estimator that minimizes the population version of the loss function in (3) i.e., (cid:104)
L(Θ) = E exp (cid:0) − (cid:10)(cid:10)Θ, Φ(x)(cid:11)(cid:11)(cid:1)(cid:105)
. is equivalent to the estimator that minimizes the Kullback-Leibler (KL) divergence between UX (the uniform distribution on X ) and fx(·; Θ∗ − Θ) (see Theorem 4.1). Therefore, at the population level, our method can be viewed as the MLE of the parametric family fx(·; Θ∗ − Θ). We show that the KL divergence (and therefore L(Θ)) is minimized if and only if Θ = Θ∗, and this connection provides an intuitively pleasing justiﬁcation of the estimator in (4). 1We let k3 = O(1). See Section 2. 2
1.2