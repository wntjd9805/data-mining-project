Abstract
We study the problem of properly learning linear threshold functions (LTFs) in the learning from label proportions (LLP) framework. In this, the learning is on a collection of bags of feature-vectors with only the proportion of labels available for each bag.
First, we provide an algorithm that, given a collection of such bags each of size at most two whose label proportions are consistent with (i.e., the bags are satisﬁed by) an unknown LTF, efﬁciently produces an LTF that satisﬁes at least (2/5)-fraction of the bags. If all the bags are non-monochromatic (i.e., bags of size two with differently labeled feature-vectors) the algorithm satisﬁes at least (1/2)-fraction of them. For the special case of OR over the d-dimensional boolean vectors, we give an algorithm which computes an LTF achieving an additional Ω(1/d) in accuracy for the two cases.
Our main result provides evidence that these algorithmic bounds cannot be signiﬁ-cantly improved, even for learning monotone ORs using LTFs. We prove that it is NP-hard, given a collection of non-monochromatic bags which are all satisﬁed by some monotone OR, to compute any function of constantly many LTFs that satisﬁes (1/2 + ε)-fraction of the bags for any constant ε > 0. This bound is tight for the non-monochromatic bags case.
The above is in contrast to the usual supervised learning setup (i.e., unit-sized bags) in which LTFs are efﬁciently learnable to arbitrary accuracy using linear programming, and even a trivial algorithm (any LTF or its complement) achieves an accuracy of 1/2. These techniques however, fail in the LLP setting. Indeed, we show that the LLP learning of LTFs (even for the special case of monotone ORs) using LTFs dramatically increases in complexity as soon as bags of size two are allowed. Our work gives the ﬁrst inapproximability for LLP learning LTFs, and a strong complexity separation between LLP and traditional supervised learning. 1

Introduction
A linear threshold function (LTF) over the d-dimensional feature-vectors x is given by pos(g(x)) for some linear function g(x1, . . . , xd) = (cid:80)d i=1 cixi + cd+1, where pos(z) := 1{z>0}. Also known as linear classiﬁers or halfspaces, LTFs are one of the most fundamental classes studied in computational learning and lie at the core of several machine learning algorithms such as Perceptron [25, 21] and
SVM [7]. It is known [4] that LTFs are properly learnable in the supervised learning setup: given a training set of labeled feature-vectors which are consistent with some LTF, using polynomial-time linear programming one can efﬁciently ﬁnd one such LTF.
In this work we initiate an investigation into the proper learnability of LTFs in the framework of learning from label proportions (LLP). In this, instead of labels for each feature-vector, the training data consists of bags (subsets) of feature-vectors along with the proportion of labels in each bag. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Given a collection of bags with their respective label proportions which are consistent with (i.e, are satisﬁed by) some unknown feature-vector level classiﬁer, the goal is to ﬁnd a classiﬁer (from the same concept class for proper learning) satisfying the most number of bags. Such a model was formalized by Yu et. al [30] who showed bounds on the generalization error for predicting the label proportion on unseen bags.
When all bags are unit-sized this becomes the usual supervised learning in which as we noted above
LTFs are efﬁciently properly learnable. A natural question is whether this remains true even when larger bag sizes are allowed. In particular, we consider the simplest setting with bags of size at most two and study the following question: Given a collection of bags, all of size at most two and guaranteed to be satisﬁed by some unknown LTF, what is the maximum fraction of bags that can be satisﬁed using an efﬁciently computable LTF?
There naturally are two types of bags: monochromatic bags whose feature vectors have the same label, and the non-monochromatic ones whose feature-vectors have different labels and are necessarily of size two. Clearly, the monochromatic bags determine the labels of their constituent feature-vectors.
Therefore, for a collection of bags consistent with some LTF, one can efﬁciently ﬁnd an LTF using linear programming to satisfy all the monochromatic bags. However, such an algorithm is not guaranteed to satisfy any non-monochromatic bag.
In traditional supervised learning of LTFs, the trivial algorithms of taking the best of any LTF or its complement, or a random homogeneous LTF, both achieve an accuracy of half. Unfortunately, these algorithms provide no such guarantees in the LLP setting, even for bags of size at most two. At a high level, in LLP the objective is given by comparing the predicted labels of a bag’s feature-vectors with each other, unlike supervised learning where they are only compared with observed ones. This seems to qualitatively change the nature of the problem in terms of the applicable techniques.
We ﬁrst provide an algorithm that satisﬁes (in expectation) at least half of the non-monochromatic bags and one-fourths of the monochromatic ones. This can be combined with the linear programming algorithm which satisﬁes all monochromatic bags (and possibly none of the non-monochromatic ones) to obtain an algorithm which outputs an LTF satisfying (2/5)-fraction of the bags. We also show that for the special case of the feature vectors being d-dimensional boolean and the bags being consistent with an OR (boolean disjunction) the algorithmic guarantees improve by an Ω(1/d) additional factor. 1.1 Previous Work on LLP
The LLP problem naturally arises in many real world scenarios where the labels are not available individually but only as proportions for bags of feature-vectors due to privacy [28] and legal [26] reasons, high label supervision cost [6] or technical limitations of labeling mechanisms [9]. Some of the earlier works [8, 16, 22, 26] applied supervised learning methods such as MCMC, clustering, and linear classiﬁers to the LLP problem. The work of [24] assumed an exponential generative model and proved performance bounds under assumptions on the distribution of the bags, and was further generalized by [23] while the work of [29] proposed novel proportional SVM based algorithms.
Subsequently, approaches based on deep neural nets for large-scale and multi-class data [18, 10, 19], as well as bag pre-processing techniques [27] have been developed for LLP.
In contrast to the long line above of application focused research in LLP, the theoretical study of algorithmic and complexity issues in LLP has been rather limited. On the complexity side, [13] studied a one-bag version of this problem where the observed global label proportion is given over all the feature-vectors and the goal is to ﬁnd a classiﬁer to minimize the deviation from this. They showed, among other results that common concept classes such as monotone-ORs cannot be learnt in their model to arbitrary accuracy.
The LLP model which we study – of predicting the bag label proportions – was ﬁrst formalized in the work of [30] who also bounded the generalization error when taking the (bag, label-proportion)-pairs as instances sampled iid from some distribution. Their bound has the same dependence on the feature-vector classiﬁer’s VC-dimension as in supervised learning, with an additional logarithmic dependence on the bag size. While the loss function they consider is slightly different, their bound can be applied to minimizing the number of unsatisﬁed bags of size two (see Appendix E of the supplemental). In the rest of this section we deﬁne the problem of learning LTFs from label proportions, describe our results and the previous related work along with an informal description of our methods and proof techniques. 2
1.2 Problem Deﬁnition
Let X = {x1, . . . , xn} ⊆ Rd be the set of feature-vectors, and bags B = {B1, . . . , Bm} ⊆ (cid:0)X (cid:1), (cid:1)∪(cid:0)X i.e. each bag is of size at most 2. The label set is binary i.e., {0, 1}, and for each bag Bk there is a proportion σk which is the average of the labels of the vectors in the bag, satisfying σk ∈ {0, 1} if
|Bk| = 1 and σk ∈ {0, 0.5, 1} if |Bk| = 2. If σk (cid:54)= 0.5 then Bk is said to be monochromatic i.e., bags which have same labels for their feature-vectors. The remaining bags Bk necessarily of size 2 having σk = 0.5 are called non-monochromatic.
A bag Bk ∈ B is satisﬁed by some f : X → {0, 1} if (cid:0)(cid:80)
An instance of LLP-LTF is given (X, B = {Bk}m exists an LTF that satisﬁes all the bags. The goal is to ﬁnd an LTF that satisﬁes the most bags.
We denote by LLP-OR the special case when X has boolean vectors and the guaranteed LTF for satisﬁable instances is an OR. A further special case is when the guaranteed OR is monotone i.e., it has no negated variables. k=1). It is said to be satisﬁable if there x∈Bk k=1, {σk}m f (x)(cid:1) / |Bk| = σk. 1 2 1.3 Our Results
We provide the following algorithmic guarantees for LLP-LTF on bags of size at most 2.
Theorem 1.1. There is a polynomial time algorithm which given a satisﬁable instance of LLP-LTF on bags of size at most 2, computes an LTF that satisﬁes at least (2/5)-fraction of the bags. If all bags are non-monochromatic this satisﬁes at least half of the bags. For satisﬁable LLP-OR on bags of size at most 2 with d-dimensional boolean feature-vectors, there is a polynomial-time algorithm computing an LTF satisfying at least (2/5 + γ0/d)-fraction of the bags in general, and (1/2 + γ0/d)-fraction if all bags are non-monochromatic, for some absolute constant γ0 > 0.
The above theorem for LLP-LTF is proved in Sec. 2 with the proof for LLP-OR provided in Sec. 2.1. The algorithms use random hyperplane rounding of vectors, so the above guarantees are in expectation. They can however be derandomized by the sophisticated procedure given in [20] and we provide an explanation in Appendix A of the supplemental.
Our result on the intractability of LLP-OR (an thereby of LLP-LTF) is stated below.
Theorem 1.2. Given a satisﬁable instance of LLP-OR consisting only of non-monochromatic bags each of size 2 s.t. there is a monotone OR that satisﬁes all bags, it is NP-hard to ﬁnd any function of (cid:96) LTFs that satisﬁes (1/2 + δ)-fraction of the bags, for any constants (cid:96) ∈ Z+, δ > 0.
The proof follows via a reduction from a variant of the Label Cover problem. We include the guarantees of the hardness reduction as Theorem 3.3 which directly implies Theorem 1.2. However, due to lack of space we defer the formal proof of Theorem 3.3 to Appendices C and D of the supplemental, while including an informal description in Sec. 1.5. We note that Theorem 1.2 shows the optimality of the approximation factor obtained in Theorem 1.1 for non-monochromatic bags.
Previous