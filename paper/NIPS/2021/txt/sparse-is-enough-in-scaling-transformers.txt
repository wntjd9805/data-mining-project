Abstract
Large Transformer models yield impressive results on many tasks, but are expen-sive to train, or even ﬁne-tune, and so slow at decoding that their use and study becomes out of reach. We address this problem by leveraging sparsity. We study sparse variants for all layers in the Transformer and propose Scaling Transformers, a family of next generation Transformer models that use sparse layers to scale efﬁciently and perform unbatched decoding much faster than the standard Trans-former as we scale up the model size. Surprisingly, the sparse layers are enough to obtain the same perplexity as the standard Transformer with the same number of parameters. We also integrate with prior sparsity approaches to attention and enable fast inference on long sequences even with limited memory. This results in performance competitive to the state-of-the-art on long text summarization. 1

Introduction
The ﬁeld of natural language processing has seen dramatic improvements in recent years due to large neural networks based on the Transformer architecture. The original Transformer [42] signiﬁcantly advanced state-of-the-art in machine translation. BERT [7] surpassed all previous methods on question answering, language inference and other NLP tasks and was followed by a line of models like T5 [30] that further improved these results. The GPT line of models [29, 3] elevated language generation to the point that GPT-2 was invited to write short passages for the Economist and GPT-3 created whole articles almost indistinguishable from human-written ones.
The beneﬁts of this progress are undercut by the huge costs such models incur. Strubell et al. [36] estimate that training a single base BERT model costs $4k-$12k and emits as much CO2 as one passenger’s share of a 4-hour ﬂight and later Patterson et al. [27] estimate that training GPT-3 has three times as much tCO2e (metric tons of CO2 equivalent) emissions as a SF-NY round trip ﬂight.
Data and serving costs are also forbidding: a single training run of BERT, for example, processes 128B tokens, and Google Translate reportedly1 serves over 143B words per day.
With the growing popularity and size of these models, it is increasingly valuable to make them scale efﬁciently. In this work we propose Scaling Transformers with a separate sparse mechanism for the query, key, value and output layers (QKV layers for short) and combine it with sparse feedforward blocks to get a fully sparse Transformer architecture.
To quantify the computational complexity of inference in Transformer models, recall the architecture of a Transformer decoder block. It consists of three parts: a masked self-attention layer, an encoder-decoder attention layer and a feedforward block. The sizes of these layers are parameterized by dmodel and dff. The base BERT model sets dmodel = 768, the large BERT has dmodel = 1024, the largest
∗Work done while at Google Research. 1https://cutt.ly/skkFJ7a 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
baseline Transf.
+ Sparse FF
+ Sparse QKV
+ Sparse FF+QKV
Speedup baseline Transf.
+Sparse FF
+Sparse QKV
+Sparse FF+QKV
Speedup 800M
---Params Dec. time Dec. time per block 5.9ms 3.1ms 6.2ms 1.9ms 3.05x 0.581s 0.259s 0.554s 0.014s 42.5x 0.160s 0.093s 0.152s 0.061s 2.62x 3.690s 1.595s 3.154s 0.183s 20.0x 17B
---Table 1: Decoding speed (in seconds) of a single token.
For Transformer model (equivalent to T5 large with ap-proximately 800M parameters), Scaling Transformers with proposed sparsity mechanisms (FF+QKV) achieve up to 2x speedup in decoding compared to baseline dense model and 20x speedup for 17B param model.
Figure 1: Log-perplexity of Scaling Transformers (equivalent to T5 large with approximately 800M pa-rameters) on C4 dataset with proposed sparsity mecha-nisms (FF, QKV, FF+QKV) is similar to baseline dense model. Other models used in this paper are shown in grey lines; raw data is available in the appendix.
GPT-2 has dmodel = 1600 and GPT-3 reaches dmodel = 12288. For both BERT and GPT models the authors use dff = 4 dmodel. While decoding a token, the self-attention layer needs to activate four matrices of size dmodel × dmodel: one each for the queries, keys and values input to the attention and one for merging the output. In the encoder-decoder attention, the keys and values may already be cached, so only two matrices of size dmodel × dmodel are activated. The feedforward block consists of two matrices of size dmodel × dff, omitting small additional contribution of biases. The total adds up to: 4 d 2 model + 2 dmodel dff. This sum describes both the number of trainable weights of a single block and approximates well the number of ﬂoating-point operations needed for decoding a single token, except for the attention operations (discussed later). The complexity is quadratic in dmodel; for example, as dmodel increases 16-fold from base BERT to GPT-3, the complexity of a single block grows 256-fold. model + 2 d 2
In comparison Scaling Transformers use only 2dmodel model parameters in QKV layers and yield results as good as the baseline (fully dense) Transformer with the same number of parameters and complexity: 8 d 1.5 model. We were surprised that the fully sparse Scaling
Transformers are indeed enough to match the results of the baseline Transformer on the large C4 dataset [30] (Figure 1). The improvement in complexity holds not just asymptotically but yields over 2.6x speedup in wall-clock hed decoding time already for a model with 800M parameters and 20x improvement for a model with 17B parameters, as shown in Table 1. model + 4 d 1.5 model + 4 d 1.5 dmodel = 2d 1.5
√
To verify that Scaling Transformers can be used with other Transformer improvements on real tasks, we create Terraformer – a Transformer model that uses reversible layers for memory efﬁciency and sparse attention to handle long sequences. We pre-train Terraformer on the C4 dataset and ﬁne-tune it on the challenging task of summarizing arxiv articles. Terraformer yields results competitive to the state-of-the-art BigBird-Pegasus without using the Pegasus loss in pre-training (Table 5). 2