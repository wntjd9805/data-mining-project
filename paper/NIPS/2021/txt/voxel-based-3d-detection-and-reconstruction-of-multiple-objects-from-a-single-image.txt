Abstract
Inferring 3D locations and shapes of multiple objects from a single 2D image is a long-standing objective of computer vision. Most of the existing works either predict one of these 3D properties or focus on solving both for a single object. One fundamental challenge lies in how to learn an effective representation of the image that is well-suited for 3D detection and reconstruction. In this work, we propose to learn a regular grid of 3D voxel features from the input image which is aligned with 3D scene space via a 3D feature lifting operator. Based on the 3D voxel features, our novel CenterNet-3D detection head formulates the 3D detection as keypoint detection in the 3D space. Moreover, we devise an efﬁcient coarse-to-ﬁne reconstruction module, including coarse-level voxelization and a novel local PCA-SDF shape representation, which enables ﬁne detail reconstruction and one order of magnitude faster inference than prior methods. With complementary supervision from both 3D detection and reconstruction, one enables the 3D voxel features to be geometry and context preserving, beneﬁting both tasks. The effectiveness of our approach is demonstrated through 3D detection and reconstruction in single object and multiple object scenarios. Code is available at http://cvlab.cse. msu.edu/project-mdr.html. 1

Introduction
As a fundamental computer vision task, instance-level 3D scene understanding from a single image has drawn substantial attention from researchers due to its importance in applications such as robotics [1, 2], AR/VR [3] and autonomous driving [4–6]. The important 3D properties include 3D bounding box (pose, size, location) and 3D shape of object instances. In this work, we aim to design a framework to infer all these 3D properties of multiple objects from a single 2D image.
In recent years, various monocular methods are proposed to predict either 3D boxes [7–12] or 3D shapes [13–17]. However, only a few studies [18–23] consider both 3D detection and reconstruction for a total 3D scene understanding. The complexity of real-world scenarios and diverse category variations make it challenging to fully reconstruct the scene context (both semantics and geometry) at the instance level from a single image. Moreover, those methods primarily assign 3D semantic labels to pixels. Yet, such a 2D representation with depth ambiguity is insufﬁcient for 3D geometry and context reasoning. It is thus crucial to develop an effective representation of the image that is relevant to 3D geometry and spatial information for performing accurate 3D detection and reconstruction.
In light of this, attempts like grid-based representation have been made for tasks such as rendering [24], detection [25, 26], or reconstruction [27]. OFT [25] proposes to sample and transform image features into a BEV grid representation, which enables holistic reasoning of the 3D scene conﬁguration.
CaDDN [26] extends the BEV grid representation with a categorical depth prior, leading to higher 3D detection accuracies. DeepVoxels [24] and UCLID-Net [27] build voxel features by back-projecting 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Given a single image as input, our proposed approach jointly predicts 3D object bounding boxes and surfaces. 2D features to 3D space for respective rendering or single object reconstruction purposes. Inspired by this line of works, we propose a novel voxel-based 3D detection and reconstruction framework for predicting 3D bounding boxes and surfaces of multiple objects from a single image (see Fig. 1).
Speciﬁcally, we ﬁrst divide a 3D scene space into a regular grid of voxels. For each voxel, we assign 3D features by sampling from the image plane via a 2D-to-3D feature lifting operator and the known camera projection matrix. As multiple voxels can be projected to the same position, this leads to similar features along the camera ray and increased difﬁculty for downstream tasks. To remedy this, we use a positional encoding strategy to make our voxel features position-aware and more discriminative. Based on the intermediate voxel features, we carefully devise our detection and reconstruction modules. For detection, we introduce a novel CenterNet-3D detector head. Instead of formulating the 3D detection as 2D keypoint detection problem as conventional CenterNet-based methods [28, 29], each object is directly represented by its 3D keypoint. Predicting a class-speciﬁc 3D heatmap can show probabilities of 3D object centers in the pre-deﬁned voxel space, leading to improved 3D center accuracy. For reconstruction, we propose a multi-level shape representation with two components: coarse-level occupancy representation and ﬁne-level local PCA-SDF representation.
The coarse-level voxel grid represents the whole 3D scene with continuous occupancy values. At a
ﬁne level, we represent the occupied voxels with a PCA-based signed distance function (SDF) by assuming that the local shapes of different voxels are similar either within an object instance, or across different objects.
In summary, the contributions of this work include: (cid:5) We propose a novel voxel-based 3D detection and reconstruction framework, which infers the 3D locations and 3D surfaces for multiple object instances with only a 2D image as input. (cid:5) We present a novel CenterNet-3D detector, where each object is represented by its center point in a partitioned 3D grid space. CenterNet-3D avoids estimating depth directly from image features, leading to increased detection performance. (cid:5) We propose a novel local PCA-SDF shape representation, which provides ﬁner reconstruction and order of magnitude faster inference than SOTA local implicit function methods like DeepLS [30]. (cid:5) We demonstrate the superiority of our method in multiple object 3D reconstruction and detection, as well as 3D shape representation. We assemble a 3D detection and reconstruction benchmark with 18, 000 real images, annotated with 3D models and bounding boxes of 19 object categories. 2