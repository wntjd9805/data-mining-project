Abstract
Although recent advances in voice conversion have shown signiﬁcant improvement, there still remains a gap between the converted voice and target voice. A key factor that maintains this gap is the insufﬁcient decomposition of content and voice style from the source speech. This insufﬁciency leads to the converted speech containing source speech style or losing source speech content. In this paper, we present
VoiceMixer which can effectively decompose and transfer voice style through a novel information bottleneck and adversarial feedback. With self-supervised representation learning, the proposed information bottleneck can decompose the content and style with only a small loss of content information. Also, for adversarial feedback of each information, the discriminator is decomposed into content and style discriminator with self-supervision, which enable our model to achieve better generalization to the voice style of the converted speech. The experimental results show the superiority of our model in disentanglement and transfer performance, and improve audio quality by preserving content information. 1

Introduction
Voice conversion (VC) is the task of transferring the target voice style to the source speech while keeping the content information of the source speech. VC is also called voice style transfer (VST), and it shares a long history with the objective to clone someone’s voice. There is even a potential risk of usage in crime such as a voice spooﬁng (Kinnunen et al., 2012), and also in various applications in entertainment (Nachmani and Wolf, 2019), education (Sisman et al., 2020), security (Wu and Li, 2016), and voice restoring (Yamagishi et al., 2012). Although deep learning made the breakthrough in the VC domain, there still remains challenging problems for real-world application such as low audio quality or similarity to target voice style.
Usually, traditional VC systems require the same utterances for different speakers to train properly.
However, it is hard to collect such parallel data for many speakers, and extension to many-to-many VC systems becomes a laborious task. To overcome this problem, several methods have been developed.
First, generative adversarial networks (GAN) based models (Kaneko and Kameoka, 2018; Kaneko et al., 2019, 2020; Kameoka et al., 2018) use adversarial feedback with cycle-consistent loss to train with non-parallel data. However, it is hard to train these models, and they produce lacking audio quality and transfer performance. The ﬂow-based VC model, Blow (Serrà et al., 2019), is also a non-parallel VC model with normalizing ﬂows using the hyperconditioning mechanism.
Despite effort to transfer the voice style in non-parallel settings, these models are not able to sufﬁciently disentangle content and style from the source speech, and thus the converted speech
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
still contains the style of source speech. To overcome this limitation, AUTOVC (Qian et al., 2019) utilizes a simple autoencoder. The carefully designed ﬁxed-length based information bottleneck disentangles the content and style information. For better disentanglement, IDE-VC (Yuan et al., 2021) followed the AUTOVC framework with information-theoretic guidance. AdaIN-VC (Chou et al., 2019) and AGAIN-VC (Chen et al., 2020) employs the instance normalization (Ulyanov et al., 2016) to remove the global style information. Additionally, AGAIN-VC makes use of the activation function as an information bottleneck with a small size of content embedding. However, these models have a trade-off between the audio quality and the disentanglement performance. In the process of disentanglement, the loss of content information results in low audio quality with missing linguistic information. Also, they have to ﬁnd the proper size of information bottleneck heuristically.
Text transcriptions can be used to guide content embedding to learn only linguistic information (Biadsy et al., 2019; Zhang et al., 2019). These models have to be jointly trained with the text-to-speech (TTS) model to encode the linguistic information based on the attention alignment from autoregressive TTS system (Shen et al., 2018). However, they require text transcriptions for training.
Recently, self-supervised representation learning is adopted to extract important representation in speech representation learning task (Oord et al., 2018; Wang et al., 2020a). Predicting the future latent representation can make the model learn useful information without labeled data. However, such self-supervised representation learning has not yet gotten the attention in voice conversion task.
In this paper, we present VoiceMixer, which can decompose and transfer voice style through a novel similarity-based information bottleneck and adversarial feedback. We introduce self-supervised representation learning to disentangle and transfer voice style without any text transcription and additional information extracted from the external feature extractor. Self-supervised similarity based information bottleneck disentangles the content and style without effort to ﬁnd the proper downsam-pling size. Also, we propose an adversarial voice style mixup to learn the latent representation of the converted speech. We ﬁrst disentangle the discriminator into content and style discriminator. The hidden representations of generator guide each discriminator as conditional information. Through adversarial feedback of disentangled discriminators, the generator has better generalization on the converted speech. The main contributions are as follows:
• We propose the similarity-based information bottleneck with self-supervised representation learning, which can disentangle content and style with only a small loss of content informa-tion. This preservation improves the audio quality of converted speech compared to previous methods.
• For better generalization of the converted speech, we propose an adversarial voice style mixup, which learns the converted speech by adversarial feedback with self-supervised guidance, even though the converted speech does not have ground-truth audio.
• Through various subjective and objective evaluations, we demonstrate that VoiceMixer has better disentanglement and transfer performance than other baselines in both many-to-many and zero-shot voice style transfer scenarios on the real-world VCTK dataset. 2