Abstract
We derive nearly sharp bounds for the bidirectional GAN (BiGAN) estimation error under the Dudley distance between the latent joint distribution and the data joint distribution with appropriately speciﬁed architecture of the neural networks used in the model. To the best of our knowledge, this is the ﬁrst theoretical guar-antee for the bidirectional GAN learning approach. An appealing feature of our results is that they do not assume the reference and the data distributions to have the same dimensions or these distributions to have bounded support. These as-sumptions are commonly assumed in the existing convergence analysis of the unidirectional GANs but may not be satisﬁed in practice. Our results are also applicable to the Wasserstein bidirectional GAN if the target distribution is as-sumed to have a bounded support. To prove these results, we construct neural network functions that push forward an empirical distribution to another arbitrary empirical distribution on a possibly different-dimensional space. We also develop a novel decomposition of the integral probability metric for the error analysis of bidirectional GANs. These basic theoretical results are of independent interest and can be applied to other related learning problems.
∗Corresponding authors 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
Generative adversarial networks (GAN) (Goodfellow et al., 2014) is an important approach to im-plicitly learning and sampling from high-dimensional complex distributions. GANs have been shown to achieve impressive performance in many machine learning tasks (Radford et al., 2016;
Reed et al., 2016; Zhu et al., 2017; Karras et al., 2018, 2019; Brock et al., 2019). Several recent studies have generalized GANs to bidirectional generative learning, which learns an encoder map-ping the data distribution to the reference distribution simultaneously together with the generator doing reversely. These studies include the adversarial autoencoder (AAE) (Makhzani et al., 2015), bidirectional GAN (BiGAN) (Donahue et al., 2016), adversarially learned inference (ALI) (Du-moulin et al., 2016), and bidirectional generative modeling using adversarial gradient estimation (AGES) (Shen et al., 2020). A common feature of these methods is that they generalize the basic adversarial training framework of the original GAN from unidirectional to bidirectional. Dumoulin et al. (2016) showed that BiGANs make use of the joint distribution of data and latent representa-tions, which can better capture the information of data than the vanilla GANs. Comparing with the unidirectional GANs, the joint distribution matching in the training of bidirectional GANs alleviates mode dropping and encourages cycle consistency (Shen et al., 2020).
Several elegant and stimulating papers have analyzed the theoretical properties of unidirectional
GANs. Arora et al. (2017) considered the generalization error of GANs under the neural net distance.
Zhang et al. (2018) improved the generalization error bound in Arora et al. (2017). Liang (2020) studied the minimax optimal rates for learning distributions with empirical samples under Sobolev evaluation class and density class. The minimax rate is O(n−1/2 ∨ n−α+β/(2α+β)), where α and
β are the regularity parameters for Sobolev density and evaluation class, respectively. Bai et al. (2019) analyzed the estimation error of GANs under the Wasserstein distance for a special class of distributions implemented by a generator, while the discriminator is designed to guarantee zero bias. Chen et al. (2020) studies the convergence properties of GANs when both the evaluation class and the target density class are H¨older classes and derived O(n−β/(2β+d) log2 n) bound, where d is the dimension of the data distribution and α and β are the regularity parameters for H¨older density and evaluation class, respectively. While impressive progresses have been made on the theoretical understanding of GANs, there are still some drawbacks in the existing results. For example, (a) The reference distribution and the target data distribution are assumed to have the same dimen-sion, which is not the actual setting for GAN training. (b) The reference and the target data distributions are assumed to be supported on bounded sets. (c) The prefactors in the convergence rates may depend on the dimension d of the data distribution exponentially.
In practice, GANs are usually trained using a reference distributions with a lower dimension than that of the target data distribution. Indeed, an important strength of GANs is that they can model low-dimensional latent structures via using a low-dimensional reference distribution. The bounded support assumption excludes some commonly used Gaussian distributions as the reference. There-fore, strictly speaking, the existing convergence analysis results do not apply to what have been done in practice. In addition, there has been no theoretical analysis of bidirectional GANs in the literature. 1.1 Contributions
We derive nearly sharp non-asymptotic bounds for the GAN estimation error under the Dudley dis-tance between the reference joint distribution and the data joint distribution. To the best of our knowledge, this is the ﬁrst result providing theoretical guarantees for bidirectional GAN estimation error rate. We do not assume that the reference and the target data distributions have the same dimen-sion or these distributions have bounded support. Also, our results are applicable to the Wasserstein distance if the target data distribution is assumed to have a bounded support.
The main novel aspects of our work are as follows. (1) We allow the dimension of the reference distribution to be different from the dimension of the target distribution, in particular, it can be much lower than that of the target distribution. (2) We allow unbounded support for the reference distribution and the target distribution under mild conditions on the tail probabilities of the target distribution. 2
(3) We explicitly establish that the prefactors in the error bounds depend on the square root of the dimension of the target distribution. This is a signiﬁcant improvement over the exponential dependence on d in the existing works.
Moreover, we develop a novel decomposition of integral probability metric for the error analysis of bidirectional GANs. We also show that the pushforward distribution of an empirical distribution based on neural networks can perfectly approximate another arbitrary empirical distribution as long as the number of discrete points are the same.
Notation We use σ to denote the ReLU activation function in neural networks, which is σ(x) = max{x, 0}, x ∈ R. We use I to denote the identity map. Without further indication, (cid:107) · (cid:107) represents the L2 norm. For any function g, let (cid:107)g(cid:107)∞ = supx (cid:107)g(x)(cid:107). We use notation O(·) and ˜O(·) to express the order of function slightly differently, where O(·) omits the universal constant independent of d while ˜O(·) omits the constant depending on d. We use Bd 2 (a) to denote the L2 ball in Rd with center at 0 and radius a. Let g#ν be the pushforward distribution of ν by function g in the sense that g#ν(A) = ν(g−1(A)) for any measurable set A. We use ˆE to denote taking expectation with respect to the empirical distribution. 2 Bidirectional generative learning
We describe the setup of the bidirectional GAN estimation problem and present the assumptions we need in our analysis. 2.1 Bidirectional GAN estimators
Let µ be the target data distribution supported on Rd for d ≥ 1. Let ν be a reference distribution which is easy to sample from. We ﬁrst consider the case when ν is supported on R, and then extend it to Rk, where k ≥ 1 can be different from d. Usually, k (cid:28) d in practical machine learning tasks such as image generation. The goal is to learn functions g : R → Rd and e : Rd → R such that
˜g#ν = ˜e#µ, where ˜g := (g, I) and ˜e := (I, e), ˜g#ν is the pushforward distribution of ν under
˜g and ˜e#µ is the pushforward distribution of µ under ˜e. We call ˜g#ν the joint latent distribution or joint reference distribution and ˜e#µ the joint data distribution or joint target distribution. At the population level, the bidirectional GAN solves the minimax problem: (g∗, e∗, f ∗) ∈ arg min g∈G,e∈E max f ∈F
EZ∼ν[f (g(Z), Z)] − Ex∼µ[f (X, e(X))], where G, E, F are referred to as the generator class, the encoder class, and the discriminator i.i.d.∼ ν and class, respectively. Suppose we have two independent random samples Z1, . . . , Zn i.i.d.∼ µ. At the sample level, the bidirectional GAN solves the empirical version of the
X1, . . . , Xn above minimax problem: (ˆgθ, ˆeϕ, ˆfω) = arg min gθ∈GN N ,eϕ∈EN N max fω∈FN N 1 n n (cid:88) i=1 fω(gθ(Zi), Zi) − 1 n n (cid:88) j=1 fω(Xj, eϕ(Xj)), (2.1) where GN N and EN N are two classes of neural networks approximating the generator class G and the encoder class E respectively, and FN N is a class of neural networks approximating the discriminator class F. 2.2 Assumptions
We assume the target µ and the reference ν satisfy the following assumptions.
Assumption 1 (Subexponential tail). For a large n, the target distribution µ on Rd and the reference distribtuion ν on R satisﬁes the ﬁrst moment tail condition for some δ > 0, max{Eν(cid:107)Z(cid:107)1{(cid:107)Z(cid:107)>log n}, Eµ(cid:107)X(cid:107)1{(cid:107)X(cid:107)>log n}} = O(n− (log n)δ
Assumption 2 (Absolute continuity). Both the target distribution µ on Rd and the reference distri-bution ν on R are absolutely continuous with respect to the Lebesgue measure λ.
). d 3
Assumption 1 is a technical condition for dealing with the case when µ and ν are supported on
Rd and R instead of compact subsets. For distributions with bounded supports, this assumption is automatically satisﬁed. Here the factor (log n)δ ensures that the tails of µ and ν are sub-exponential, and it can be easily satisﬁed if the distributions are sub-gaussian. For the reference distribution,
Assumption 1 and 2 can be easily satisﬁed by specifying ν as some common distribution with easy-to-sample density such as Gaussian or uniform, which is usually done in the applications of GANs.
For the target distribution, Assumption 1 and 2 speciﬁes the type of distributions that are learnable by bidirectional GAN with our theoretical guarantees. Note that Assumption 1 is also necessary in our proof for bounding the generator and encoder approximation error in the sense that the results will not hold if we replace (log n)δ with 1. Assumption 2 is also necessary for Theorem 4.3 in mapping between empirical samples, which is essential in bounding generator and encoder approximation error. 2.3 Generator, encoder and discriminator classes
Let FN N := N N (W1, L1) be the discriminator class consisting of the feedforward ReLU neural networks fω : Rd+1 (cid:55)→ R with width W1 and depth L1. Similarly, let GN N := N N (W2, L2) be the generator class consisting of the feedforward ReLU neural networks gθ : R (cid:55)→ Rd with width W2 and depth L2, and EN N := N N (W3, L3) the encoder class consisting of the feedforward ReLU neural networks eϕ : Rd (cid:55)→ R with width W3 and depth L3.
The functions fω ∈ FN N have the following form: fω(x) = AL1 · σ(AL1−1 · · · σ(A1x + b1) · · · + bL1−1) + bL1 where Ai are the weight matrices with number of rows and columns no larger than the width W1, bi are the bias vectors with compatible dimensions, and σ is the ReLU activation function σ(x) = x∨0.
Similarly, functions gθ ∈ GN N and eϕ ∈ EN N have the following form:
· σ(A(cid:48)
· σ(A(cid:48)(cid:48)
L2−1 · · · σ(A(cid:48)
L3−1 · · · σ(A(cid:48)(cid:48) gθ(x) = A(cid:48)
L2 eϕ(x) = A(cid:48)(cid:48)
L3
L2−1) + b(cid:48)
L2
L3−1) + b(cid:48)(cid:48)
L3 1) · · · + b(cid:48) 1 ) · · · + b(cid:48)(cid:48) 1x + b(cid:48) 1 x + b(cid:48)(cid:48) i are the weight matrices with number of rows and columns no larger than W2 and where A(cid:48) i and A(cid:48)(cid:48)
W3, respectively, and b(cid:48)
We impose the following conditions on GN N , EN N , and FN N .
Condition 1. For any gθ ∈ GN N and eϕ ∈ EN N , we have max{(cid:107)gθ(cid:107)∞, (cid:107)eϕ(cid:107)∞} ≤ log n. i are the bias vectors with compatible dimensions. i and b(cid:48)(cid:48)
Condition 1 on GN N can be easily satisﬁed by adding an additional clipping layer (cid:96) after the original output layer, with cn,d ≡ (log n)/
√ d, (cid:96)(a) = a ∧ cn,d ∨ (−cn,d) = σ(a + cn,d) − σ(a − cn,d) − cn,d. (2.2)
We truncate the output of (cid:107)gθ(cid:107) to an increasing interval [− log n, log n] to include the whole Rd support for the evaluation function class. Condition 1 on EN N can be satisﬁed in the same manner.
This condition is technically necessary in our proof (see appendix). 3 Non-asymptotic error bounds
We characterize the bidirectional GAN solutions based on minimizing the integral probability metric (IPM, M¨uller (1997)) between two distributions µ and ν with respect to a symmetric evaluation function class F, deﬁned by dF (µ, ν) = sup f ∈F
[Eµf − Eνf ]. (3.1)
By specifying the evaluation function class F differently, we can obtain many commonly-used met-rics (Liu et al., 2017). Here we focus on the following two
• F = bounded Lipschitz function class : dF = dBL, (bounded Lipschitz (or Dudley) metric: metrizing weak convergence, Dudley (2018)),
• F = Lipschitz function class : dF = W1 (Wasserstein GAN, Arjovsky et al. (2017)). 4
We consider the estimation error under the Dudley metric dBL. Note that in the case when µ and
ν have bounded supports, the Dudley metric dBL is equivalent to the 1-Wasserstein metric W1.
Therefore, under the bounded support condition for µ and ν, all our convergence results also hold under the Wasserstein distance W1. Even if the support of µ and ν are unbounded, we can still apply the result of Lu and Lu (2020) to avoid empirical process theory and obtain an stochastic error bound under the Wasserstein distance W1. However, the result of Lu and Lu (2020) requires d prefactor. In order to make it more general, we use the empirical sub-gaussianity to obtain the processes theory to get the explicit prefactor. Also, the discriminator approximation error will be unbounded if we consider the Wasserstein distance W1. Hence, we can only consider dBL for the unbounded support case.
√
The bidirectional GAN solution (ˆgθ, ˆeϕ) in (2.1) also minimizes the distance between (˜gθ)# ˆνn and (˜eϕ)# ˆµn under dFN N min gθ∈GN N ,eϕ∈EN N dFN N ((˜gθ)# ˆνn, (˜eϕ)# ˆµn).
However, even if two distributions are close with respect to dFN N , there is no automatic guarantee that they will still be close under other metrics, for example, the Dudley or the Wasserstein distance (Arora et al., 2017). Therefore, it is natural to ask the question:
• How close are the two bidirectional GAN estimators ˆν := (ˆgθ, I)#ν and ˆµ := (I, ˆeϕ)#µ under some other stronger metrics?
We consider the IPM with the uniformly bounded 1-Lipschitz function class on Rd+1, as the evalu-ation class, which is deﬁned as, for some ﬁnite B > 0,
F 1 := (cid:8)f : Rd+1 (cid:55)→ R(cid:12) (cid:12) |f (x) − f (y)| ≤ (cid:107)x − y(cid:107), x, y ∈ Rd+1 and (cid:107)f (cid:107)∞ ≤ B(cid:9) (3.2)
In Theorem 3.1, we consider the bounded support case where dF = W1; In Theorem 3.2, we extend the result to the unbounded support case; In Theorem 3.3, we extend the result to the case where the dimension of the reference distribution is arbitrary.
We ﬁrst present a result when µ is supported on a compact subset [−M, M ]d ⊂ Rd and ν is sup-ported on [−M, M ] ⊂ R for a ﬁnite M > 0.
Theorem 3.1. Suppose that the target µ is supported on [−M, M ]d ⊂ Rd and the reference ν is supported on [−M, M ] ⊂ R for a ﬁnite M > 0, and Assumption 2 holds. Let the outputs of gθ and eϕ be within [−M, M ]d and [−M, M ] for gθ ∈ GN N and eϕ ∈ EN N , respectively. By specifying n(cid:101), W 2 the three network structures as W1L1 ≥ (cid:100) 3 L3 = C2n for some constants 12 ≤ C1, C2 ≤ 384 and properly choosing parameters, we have
EdF 1(ˆν, ˆµ) ≤ C0 2 L2 = C1dn, and W 2 d+1 (log n) dn− 1 1 d+1 ,
√
√ where C0 > 0 is a constant independent of d and n.
√ d in the error bound depends on d1/2 linearly. This is different from the existing
The prefactor C0 works where the dependence of the prefactor on d is either not clearly described or is exponential.
In high-dimensional settings with large d, this makes a substantial difference in the quality of the error bounds. These remarks apply to all the results stated below.
The next theorem deals with the case of unbounded support.
Theorem 3.2. Suppose Assumption 1 and 2 hold, and Condition 1 is satisﬁed. By specifying the 3 L3 = C2n for structures of the three network classes as W1L1 ≥ (cid:100) some constants 12 ≤ C1, C2 ≤ 384 and properly choosing parameters, we have 2 L2 = C1dn, and W 2 n(cid:101), W 2
√
EdF 1(ˆν, ˆµ) ≤ min (cid:8)C0 dn− 1 d+1 (log n)1+ 1 d+1 , Cdn− 1 d+1 log n(cid:9),
√ where C0 is a constant independent of d and n, but Cd depends on d.
Note that two methods are used in bounding stochastic errors (see appendix), which leads to two d prefactor with the cost that we have an additional log n different bounds: one with an explicit factor. Another one with an implicit prefactor but with a better log n factor. Hence, it is a tradeoff between the explicitness of prefactor and the order of log n.
√
Our next result generalizes the results to the case when the reference distribution ν is supported on
Rk for k ∈ N+. 5
Assumption 3. The target distribution µ on Rd is absolutely continuous with respect to the Lebesgue measure on Rd and the reference distribution ν on Rk is absolutely continuous with respect to the
Lebesgue measure on Rk, and k (cid:28) d.
With the above assumption, we have the following theorem providing theoretical guarantees for the validity of any dimensional reference ν.
Theorem 3.3. Suppose Assumption 1 and 3 hold, and Condition 1 is satisﬁed. By specifying gener-ator and discriminator class structure as W1L1 ≥ (cid:100) 3 L3 = C2kn for some constants 12 ≤ C1, C2 ≤ 384 and properly choosing parameters, we have 2 L2 = C1dn, and W 2 n(cid:101), W 2
√
EdF 1 (ˆν, ˆµ) ≤ min (cid:8)C0
√ dn− 1 d+k (log n)1+ 1 d+k , Cdn− 1 d+k log n(cid:9), where C0 is a constant independent of d and n, but Cd depends on d.
Note that the errors bounds established in Theorems 3.1-3.3 are tight up to a logarithmic factor, since the minimax rate measured in Wasserstein distance for learning distributions when the Lipschitz evaluation class is deﬁned on Rd is ˜O(n− 1 d ) (Liang, 2020). 4 Approximation and stochastic errors
In this section we present a novel inequality for decomposing the total error into approximation and stochastic errors and establish bounds on these errors. 4.1 Decomposition of the estimation error
Deﬁne the approximation error of a function class F to another function class H by
E(H, F) := sup h∈H inf f ∈F (cid:107)h − f (cid:107)∞.
We decompose the Dudley distance dF 1 (ˆν, ˆµ) between the latent joint distribution and the data joint distribution into four different error terms,
• the approximation error of the discriminator class FN N to F 1:
E1 = E(F 1, FN N ),
• the approximation error of the generator and encoder classes:
E2 = inf gθ∈GN N ,eϕ∈EN N sup fω∈FN N 1 n n (cid:88) (cid:16) i=1 fω(gθ(zi), zi) − fω(xi, eϕ(xi)) (cid:17)
,
• the stochastic error for the latent joint distribution ˆν:
E3 = sup fω∈F 1
Efω(ˆg(z), z) − ˆEfω(ˆg(z), z),
• the stochastic error for the latent joint distribution ˆµ:
E4 = sup fω∈F 1
ˆEfω(x, ˆe(x)) − Efω(x, ˆe(x)).
Lemma 4.1. Let (ˆgθ, ˆeϕ) be the bidirectional GAN solution in (2.1) and F 1 be the uniformly bounded 1-Lipschitz function class deﬁned in (3.2). Then the Dudley distance between the latent joint distribution ˆν = (ˆgθ, I)#ν and the data joint distribution ˆµ = (I, ˆeϕ)#µ can be decomposed as follows dF 1(ˆν, ˆµ) ≤ 2E1 + E2 + E3 + E4. (4.1)
The novel decomposition (4.1) is fundamental to our error analysis. Based on (4.1), we bound each error term on the right side of (4.1) and balance the bounds to obtain an overall bound for the bidirectional GAN estimation. 6
For proving Lemma 4.1, we introduce the following useful inequality, which states that for any two probability distributions, the difference in IPMs with two distinct evaluation classes will not exceed 2 times the approximation error between the two evaluation classes, that is, for any probability distributions µ and ν and symmetric function classes F and H, dH(µ, ν) − dF (µ, ν) ≤ 2E(H, F). (4.2)
It is easy to check that if we replace dH(µ, ν) by ˆdH(µ, ν) := sup h∈H
[ˆEµh − ˆEνh], (4.2) still holds.
Proof of Lemma 4.1. We have dF 1(ˆν, ˆµ) = sup fω∈F 1
Efω(ˆg(z), z) − Efω(x, ˆe(x))
≤ sup fω∈F 1
Efω(ˆg(z), z) − ˆEfω(ˆg(z), z) + sup fω∈F 1
ˆEfω(ˆg(z), z) − ˆEfω(x, ˆe(x))
+ sup fω∈F 1
ˆEfω(x, ˆe(x)) − Efω(x, ˆe(x))
=E3 + E4 + sup fω∈F 1
ˆEfω(ˆg(z), z) − ˆEfω(x, ˆe(x)).
Denote A := supfω∈F 1 ˆEfω(ˆg(z), z) − ˆEfω(x, ˆe(x)). By (4.2) and the optimality of the bidirec-tional GAN solutions, A satisﬁes
A = sup fω∈F 1 1 n n (cid:88) (cid:16) i=1 fω(ˆg(zi), zi) − fω(xi, ˆe(xi)) (cid:17)
≤ sup fω∈FN N 1 n n (cid:88) (cid:16) i=1 fω(ˆg(zi), zi) − fω(xi, ˆe(xi)) (cid:17)
+ 2E(F 1, FN N )
= inf gθ∈GN N ,eϕ∈EN N sup fω∈FN N
= 2E1 + E2. 1 n n (cid:88) (cid:16) i=1 fω(gθ(zi), zi) − fω(xi, eϕ(xi)) (cid:17)
+ 2E1
Note that we cannot directly apply the symmetrization technic (see appendix) to E3 and E4 since e∗ and g∗ are correlated with xi and zi. However, this problem can be solved by replacing the samples (xi, zi) in the empirical terms in E3 and E4 with ghost samples (x(cid:48) i) independent of (xi, zi) and replacing g∗ and e∗ with g∗∗ and e∗∗ which are obtained from the ghost samples, respectively. That is, we replace ˆEfω(g∗(z), z) and ˆEfω(x, e∗(x)) with ˆEfω(g∗∗(z(cid:48)), z(cid:48)) and ˆEfω(x(cid:48), e∗∗(x(cid:48))) in E3 and E4, respectively. Then we can proceed with the same proof of Lemma 4.1 and apply the sym-metrization technic to E3 and E4, since (g∗(zi), zi) and (g∗∗(z(cid:48) i) have the same distribution. To simplify the notation, we will just use ˆEfω(g∗(z), z) and ˆEfω(x, e∗(x)) to denote ˆEfω(g∗∗(z(cid:48)), z(cid:48)) and ˆEfω(x(cid:48), e∗∗(x(cid:48))) here, respectively. i), z(cid:48) i, z(cid:48) 4.2 Approximation errors
We now discuss the errors due to the discriminator approximation and the generator and encoder approximation. 4.2.1 The discriminator approximation error E1
The discriminator approximation error E1 describes how well the discriminator neural network class approximates functions from the Lipschitz class F 1. Lemma 4.2 below can be applied to obtain the neural network approximation error for Lipschitz functions. It leads to a quantitative and non-asymptotic approximation rate in terms of the width and depth of the neural networks when bounding
E1. 7
Lemma 4.2 (Shen et al. (2021)). Let f be a Lipschitz continuous function deﬁned on [−R, R]d. For arbitrary W, L ∈ N+, there exists a function ψ implemented by a ReLU feedforward neural network with width W and depth L such that
||f − ψ||∞ = O(cid:0)√ dR(W L)− 2 d (cid:1).
By Lemma 4.2 and our choice of the architecture of discriminator class FN N in the theorems, we have E1 = O(cid:0)√ d+1 log n(cid:1). Theorem 4.2 also informs about how to choose the architecture of the discriminator networks based on how small we want the approximation error E1 to be. By setting (W1L1)2 ≥ n, E1 is dominated by the stochastic terms E3 and E4. d(W1L1)− 2 4.2.2 The generator and encoder approximation error E2
The generator and encoder approximation error E2 describes how powerful the generator and en-coder classes are in pushing the empirical distributions ˆµn and ˆνn to each other. A natural question is
• Can we ﬁnd some generator and encoder neural network functions such that E2 = 0?
Most of the current literature concerning the error analysis of GANs applied the optimal transport theory (Villani, 2008) to minimize an error term similar to E2, see, for example, Chen et al. (2020).
However, the existence of the optimal transport function from R → Rd is not guaranteed. Therefore, the existing analysis of GANs can only deal with the scenario when the reference and the target data distribution are assumed to have the same dimension. This equal dimensionality assumption is not satisﬁed in the actual training of GANs or bidirectional GANs in many applications. Here, instead of using the optimal transport theory, we establish the following approximation results in Theorem 4.3, which enables us to forgo the equal dimensionality assumption.
Theorem 4.3. Suppose that ν supported on R and µ supported on Rd are both absolutely continuous w.r.t. is are i.i.d. samples from ν and µ, respectively for 1 ≤ i ≤ n. Then there exist generator and encoder neural network functions g : R (cid:55)→ Rd and e : Rd (cid:55)→ R such that g and e are inverse bijections of each other between {zi : 1 ≤ i ≤ n}
: 1 ≤ i ≤ n} up to a permutation. Moreover, such neural network functions g and and {xi e can be obtained by properly specifying W 2 3 L3 = c3n for some constant 12 ≤ c2, c3 ≤ 384. the Lebesgue measures, and z(cid:48) 2 L2 = c2dn and W 2 is and x(cid:48)
Proof. By the absolute continuity of ν and µ, all the z(cid:48) is are distinct a.s.. We can reorder z(cid:48) is from the smallest to the largest, so z1 < z2 < . . . < zn. Let zi+1/2 be any point between zi and zi+1 for i ∈ {1, 2, . . . , n − 1}. We deﬁne the continuous piece-wise linear function g : R (cid:55)→ Rd by is and x(cid:48) g(z) =


 x1 z−zi+1/2 zi−zi+1/2 xi+1 xn xi + z−zi zi+ 1 2
−zi z ≤ z1, z = (zi, zi+1/2), for i = 1, . . . , n − 1, xi+1 z ∈ [zi+1/2, zi+1], for i = 1, . . . , n − 2, z ≥ zn−1+1/2. (cid:5). Taking
By Yang et al. (2021, Lemma 3.1) , g ∈ N N (W2, L2) if n ≤ (W2 − d − 1)(cid:4) W2−d−1 n = (W2 − d − 1)(cid:4) W2−d−1 2 L2 = cdn for some constant 12 ≤ c ≤ 384. The existence of neural net function e can be constructed in the same way due to the fact that the ﬁrst coordinate of x(cid:48) (cid:5), a simple calculation shows W 2 is are distinct almost surely. (cid:5)(cid:4) L2 2 (cid:5)(cid:4) L2 2 6d 6d
When the number of point masses of the empirical distributions are relatively moderate compared with the structure of the neural nets, we can approximate empirical distributions arbitrarily well with any empirical distribution with the same number of point masses pushforwarded by the neural nets.
Theorem 4.3 provides an effective way to specify the architecture of generator and encoder classes. (cid:5) + 2,
According to this lemma, we can take n = W2−d which gives rise to W 2 3 L3 (cid:16) n. More importantly, Theorem 4.3 can be applied to (cid:5) + 2 = W3−1 2 L2/d (cid:16) W 2 (cid:4) W2−d 6d (cid:4) W3−1 6 (cid:5)(cid:4) L2 2 (cid:5)(cid:4) L3 2 2 2 8
bound E2 as follows.
E2 = inf gθ∈GN N ,eϕ∈EN N sup fω∈FN N
≤ inf gθ∈GN N sup fω∈FN N 1 n n (cid:88) (cid:16) i=1 1 n n (cid:88) (cid:16) i=1 fω(gθ(zi), zi) − fω(xi, eϕ(xi)) (cid:17) fω(gθ(zi), zi) − fω(xi, zi) (cid:17)
+ inf eϕ∈EN N sup fω∈FN N 1 n n (cid:88) (cid:16) i=1 fω(xi, zi) − fω(xi, eϕ(xi)) (cid:17)
=0.
We simply reordered z(cid:48) nated. is and x(cid:48) is as in the proof. Therefore, this error term can be perfectly elimi-4.3 Stochastic errors
The stochastic error E3 (E4) quantiﬁes how close the empirical distribution and the true latent joint distribution (data joint distribution) are with the Lipschitz class F 1 as the evaluation class under
IPM. We apply the results in the reﬁned Dudley inequality (Schreuder, 2020) in Lemma 4.4 to bound E3 and E4.
Lemma 4.4 (Reﬁned Dudley Inequality). For a symmetric function class F with supf ∈F ||f ||∞ ≤
M , we have
E[dF (ˆµn, µ)] ≤ inf 0<δ<M (cid:32) 4δ + 12
√ n (cid:90) M
δ (cid:33) (cid:112)log N ((cid:15), F, || · ||∞)d(cid:15)
.
The original Dudley inequality (Dudley, 1967; Van der Vaart and Wellner, 1996) suffers from the problem that if the covering number N ((cid:15), F, || · ||∞) increases too fast as (cid:15) goes to 0, then the upper bound will be inﬁnity, which is totally meaningless. The improved Dudley inequality circumvents such a problem by only allowing (cid:15) to integrate from δ > 0 as is shown in Lemma 4.4, which also indicates that EE3 scales with the covering number N ((cid:15), F 1, || · ||∞).
By calculating the covering number of F 1 and utilizing the reﬁned Dudley inequality, we can obtain the upper bound max{EE3, EE4} = O (cid:16)
Cdn− 1 d+1 log n ∧
√ 5