Abstract
Graph Neural Networks (GNNs) have emerged as a ﬂexible and powerful approach for learning over graphs. Despite this success, existing GNNs are constrained by their local message-passing architecture and are provably limited in their expressive power. In this work, we propose a new GNN architecture – the Neural Tree. The neural tree architecture does not perform message passing on the input graph, but on a tree-structured graph, called the H-tree, that is constructed from the input graph. Nodes in the H-tree correspond to subgraphs in the input graph, and they are reorganized in a hierarchical manner such that the parent of a node in the
H-tree always corresponds to a larger subgraph in the input graph. We show that the neural tree architecture can approximate any smooth probability distribution function over an undirected graph. We also prove that the number of parameters needed to achieve an (cid:15)-approximation of the distribution function is exponential in the treewidth of the input graph, but linear in its size. We prove that any continuous G-invariant/equivariant function can be approximated by a nonlinear combination of such probability distribution functions over G. We apply the neural tree to semi-supervised node classiﬁcation in 3D scene graphs, and show that these theoretical properties translate into signiﬁcant gains in prediction accuracy, over the more traditional GNN architectures. We also show the applicability of the neural tree architecture to citation networks with large treewidth, by using a graph sub-sampling technique. 1

Introduction
Graph-structured learning problems arise in several disciplines, including biology (e.g., molecule classiﬁcation [15]), computer vision (e.g., action recognition [19], image classiﬁcation [48], shape and pose estimation [30]), computer graphics (e.g., mesh and point cloud classiﬁcation and segmen-tation [20, 35, 41]), and social networks (e.g., fake news detection [42]), among others [9]. In this landscape, Graph Neural Networks (GNN) have gained popularity as a ﬂexible and effective approach for regression and classiﬁcation over graphs.
Despite this growing research interest, recent work has pointed out several limitations of existing GNN architectures [56, 43, 38, 8]. Local message passing GNNs are no more expressive than the Weisfeiler-Lehman (WL) graph isomorphism test [56], neither can they serve as universal approximators to all
G-invariant (equivariant) functions, i.e., functions deﬁned over a graph G that remain unchanged by (or commute with) node permutation. The work [12] proves an equivalence between the ability to do graph isomorphism testing and the ability to approximate any G-invariant function.
Various GNN architectures have been proposed, that go beyond local message passing or use tensor representations, in order to improve expressivity. Graph isomorphism testing, G-invariant/equivariant function approximation, and the generalized k-order WL (k-WL) tests have served as end objectives and guided recent progress of this inquiry. For example, k-order linear GNN [39] and k-order folklore
GNN [38] have expressive powers equivalent to k-WL and (k + 1)-WL test, respectively [3]. While 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
these architectures can theoretically approximate any G-invariant function (as k → ∞), they use k-order tensors for representations, rendering them impractical for any k > 3.
There is a need for a new way to look at constructing GNN architectures. With better end objectives to guide theoretical progress. Such an attempt can result in new and expressive GNNs that are provably tractable – if not in general, at least in reasonably constrained settings.
A GNN, by its very deﬁnition, operates on graph structured data. The graph structure of the data determines inter-dependency between nodes and their features. Probabilistic graphical models present a reasonable and well-established way of articulating and working with such inter-dependencies in the data. Prior to the advent of neural networks, inference algorithms on such graphical models were successfully applied to many real-world problems. Therefore, we pose that a GNN architecture operating on a graph should have at least the expressive power of a probabilistic graphical model, i.e., it should be able to approximate any distribution deﬁned by a probabilistic graphical model.
This is not a trivial requirement as exact inference (akin to learning the distribution or its marginals) on a probabilistic graphical model, without any structural constraints on the input graph, is known to be an NP-hard problem [13]. Even approximate inference on a probabilistic graphical model is known to be NP-hard in general [46]. A common trick to perform exact inference, consists in constructing a junction tree for an input graph and performing message passing on the junction tree instead. In the junction tree, each node corresponds to a subset of nodes of the input graph. The junction tree algorithm remains tractable for graphs with bounded treewidth, while [11] shows that treewidth is the only structural parameter, bounding which, allows for tractable inference on graphical models.
Contribution. We ﬁrst deﬁne the notion of G-compatible function and argue that approximating
G-compatible functions is equivalent to approximating any probability distribution on a probabilistic graphical model (Section 4); we also show that G-invariant/equivariant functions considered in related work can be approximated using a nonlinear combination of G-compatible functions.
We then propose a novel GNN architecture – the Neural Tree – that can approximate any G-compatible function (Section 5). Neural trees do not perform message passing on the input graph, but on a tree-structured graph, called the H-tree, that is constructed from the input graph. Each node in the
H-tree corresponds to a subgraph of the input graph. These subgraphs are arranged hierarchically in the H-tree such that the parent of a node in the H-tree always corresponds to a larger subgraph in the input graph. The leaf nodes in the H-tree correspond to singleton subsets (i.e., individual nodes) of the input graph. The H-tree is constructed by recursively computing tree decompositions of the input graph and its subgraphs, and attaching them to one another to form a hierarchy. Neural message passing on the H-tree generates representations for all the nodes and important subgraphs of the input graph.
We next prove that the neural tree architecture can approximate any smooth G-compatible function deﬁned over a given undirected graph (Section 6). We also bound the number of parameters required by a neural tree architecture to obtain an (cid:15)-approximation of an arbitrary (smooth) G-compatible function. We show that the number of parameters increases exponentially in the treewidth of the input graph, but only linearly in the input graphs size. Thus, for graphs with bounded treewidth, the neural tree can tractably approximate any smooth distribution function.
We apply the neural tree architecture for semi-supervised node classiﬁcation in 3D scene graphs and citation networks (Section 7). Our experiments on 3D scene graphs demonstrate that neural trees outperform standard, local message passing GNNs, by a large margin. Citation networks on the other hand, typically have large treewidth; therefore we make use of a recently proposed bounded treewidth graph sub-sampling algorithm [62], that sub-samples the input graph (i.e., removes edges) to reduce its treewidth to a speciﬁed number. We show that applying the neural tree architecture in conjunction with such sub-sampling algorithm makes our architecture scalable to large graphs while still preserving its advantage over traditional architectures. Our code is publically available at https://github.com/MIT-SPARK/neural_tree 2