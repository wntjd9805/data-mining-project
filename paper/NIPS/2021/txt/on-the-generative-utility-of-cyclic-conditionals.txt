Abstract
We study whether and how can we model a joint distribution p(x, z) using two conditional models p(x|z) and q(z|x) that form a cycle. This is motivated by the observation that deep generative models, in addition to a likelihood model p(x|z), often also use an inference model q(z|x) for extracting representation, but they rely on a usually uninformative prior distribution p(z) to deﬁne a joint distribution, which may render problems like posterior collapse and manifold mismatch. To explore the possibility to model a joint distribution using only p(x|z) and q(z|x), we study their compatibility and determinacy, corresponding to the existence and uniqueness of a joint distribution whose conditional distributions coincide with them. We develop a general theory for operable equivalence criteria for compatibility, and sufﬁcient conditions for determinacy. Based on the theory, we propose a novel generative modeling framework CyGen that only uses the two cyclic conditional models. We develop methods to achieve compatibility and determinacy, and to use the conditional models to ﬁt and generate data. With the prior constraint removed, CyGen better ﬁts data and captures more representative features, supported by both synthetic and real-world experiments. 1

Introduction
Deep generative models have achieved a remarkable success in the past decade for generating realistic complex data x and extracting useful representations through their latent variable z. Variational auto-encoders (VAEs) [45, 67, 14, 15, 46, 80] follow the Bayesian framework and specify a prior distribution p(z) and a likelihood model p(x|z), so that a joint distribution p(x, z) = p(z)p(x|z) is deﬁned for generative modeling (the joint induces a distribution p(x) on data). An inference model q(z|x) is also used to approximate the posterior distribution p(z|x) (derived from the joint p(x, z)), which serves for extracting representations. Other frameworks like generative adversarial nets [30, 25, 27], ﬂow-based models [24, 60, 44, 31] and diffusion-based models [74, 38, 76, 49] follow the same structure, with different choices of the conditional models p(x|z) and q(z|x) and training objectives. While for the prior p(z), there is often not much knowledge for complex data (like images, text, audio), and these models widely adopt an uninformative prior such as a standard
Gaussian. This however, introduces some side effects:
• Posterior collapse [15, 34, 64]: The standard Gaussian prior tends to squeeze q(z|x) towards the origin for all x, which degrades the representativeness of the inferred z for x and hurts downstream tasks in the latent space like classiﬁcation and clustering.
• Manifold mismatch [22, 28, 41]: Typically the likelihood model is continuous (keeps topology), so the standard Gaussian prior would restrict the modeled data distribution to a simply-connected support, which limits the capacity for ﬁtting data from a non-(simply) connected support.
While there are works trying to mitigate the two problems, they require either a strong domain knowledge [48, 41], or additional cost to learn a complicated prior model [52, 21, 79] sometimes even at the cost of inconvenient inference [59, 87].
∗Correspondence to: Chang Liu <changliu@microsoft.com>.
†Work done during an internship at Microsoft Research Asia. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
One question then naturally emerges: Can we model a joint distribution p(x, z) only using the likelihood p(x|z) and inference q(z|x) models? If we can, the limitations from specifying or learning a prior are then removed from the root. Also, the inference model q(z|x) is then no longer a struggling approximation to a predeﬁned posterior but participates in deﬁning the joint distribution (avoid “inner approximation”). Modeling conditionals is also argued to be much easier than modeling marginal or joint distributions directly [2, 9, 10]. In many cases, one may even have better knowledge on the conditionals than on the prior, e.g. shift/rotation invariance of image representations (CNNs [51] /
SphereNet [20]), and rules to extract frequency/energy features for audio [65]. It is then more natural and effective to incorporate this knowledge into the conditionals than using an uninformative prior.
In this paper, we explore such a possibility, and develop both a systematic theory and a novel generative modeling framework CyGen (Cyclic-conditional Generative model). (1) Theoretical analysis on the question amounts to two sub-problems: can two given cyclic condi-tionals correspond to a common joint, and if yes, can they determine the joint uniquely. We term them compatibility and determinacy of two conditionals, corresponding to the existence and uniqueness of a common joint. For this, we develop novel compatibility criteria and sufﬁcient conditions for determinacy. Beyond existing results, ours are operable (vs. existential [11]) and self-contained (vs. need a marginal [9, 10, 50, 32]), and are general enough to cover both continuous and discrete cases.
Our compatibility criteria are also equivalence (vs. unnecessary [1, 4–6]) conditions. The seminal book [6] makes extensive analysis for various parametric families. Besides the equivalence criteria, we also extend their general analysis beyond the product support case, and also cover the Dirac case. (2) In addition to its independent contribution, the theory also enables generative modeling using only the two cyclic conditional models, i.e. the CyGen framework. We develop methods for achieving compatibility and determinacy to make an eligible generative model, and for ﬁtting and generating data to serve as a generative model. Efﬁcient implementation techniques are designed.
Note CyGen also determines a prior implicitly; it just does not need an explicit model for the prior (vs. [52, 21, 79, 59]). We show the practical utility of CyGen in both synthetic and real-world tasks. The improved performance in downstream classiﬁcation and data generation demonstrates the advantage to mitigate the posterior collapse and manifold mismatch problems. 1.1