Abstract
Deep neural networks (DNNs) have shown to perform very well on large scale object recognition problems and lead to widespread use for real-world applications, including situations where DNN are implemented as “black boxes”. A promising approach to secure their use is to accept decisions that are likely to be correct while discarding the others. In this work, we propose DOCTOR, a simple method that aims to identify whether the prediction of a DNN classiﬁer should (or should not) be trusted so that, consequently, it would be possible to accept it or to reject it. Two scenarios are investigated: Totally Black Box (TBB) where only the soft-predictions are available and Partially Black Box (PBB) where gradient-propagation to perform input pre-processing is allowed. Empirically, we show that DOCTOR outperforms all state-of-the-art methods on various well-known images and sentiment analysis datasets. In particular, we observe a reduction of up to 4% of the false rejection rate (FRR) in the PBB scenario. DOCTOR can be applied to any pre-trained model, it does not require prior information about the underlying dataset and is as simple as the simplest available methods in the literature. 1

Introduction
With the advancement of state-of-the-art Deep Neural Networks (DNNs), there has been rapid adoption of these technologies in a broad range of applications to critical systems, such as autonomous driving vehicles or industrial robots, including–but not limited to–classiﬁcation and decision making tasks. Nevertheless, these solutions still exhibit unwanted behaviors as they tend to be overconﬁdent even in presence of wrong decisions [17]. Developing methods and tools to make these algorithms
∗These authors contributed equally to this work
†This paper is supported by the ERC project Hypatia under the European Unions Horizon 2020 research and innovation program. Grant agreement №835294.
‡This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Skłodowska-Curie grant agreement №792464. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
reliable, in particular for non-specialists who may treat them as “black boxes” with no further checks, constitutes a core challenge. Recently, the study of safety AI methods has gained ground, and many efforts have been made in several areas [7, 8, 9, 11, 26, 35, 36]. In this paper, we investigate a simple method capable of detecting whether a prediction of a classiﬁer is likely to be correct, and therefore should be trusted, or it is not, and should be rejected.
Deep learning pursues the idea of learning effective representations from the data itself by training with the implicit assumption that the test data distribution should be similar to the training data distribution. However, when applied to real-world tasks, this assumption does not hold true, leading to a signiﬁcant increase of misclassiﬁcation errors. Although classic approaches to Out-Of-Distribution (OOD) detection [1, 12, 21, 23, 33] are not directly concerned with detecting misclassiﬁcation errors, they are intended to prevent those errors indirectly by identifying potential drifts of the testing distribution. What the above ODD methods have in common with our work is that samples drawn from the in-distribution are more likely to be correctly classiﬁed than those from a different distribution. Indeed, the model’s soft-predictions for in-distribution samples tend to be generally peaky in correspondence to the correct class label while they tend to be less peaky for input samples drawn from a different distribution [12]. In general, most of these works consider white-box scenarios, where the hidden layers of the architecture are accessible or the corresponding weights are tuned during the training phase. A very effective approach to OOD detection is ODIN [23] which involves the use of temperature scaling and the addition of small perturbations to input samples. A related solution is introduced in [6] where the maximum soft-probability is called softmax response. Within this approach, the softmax response decides whether the classiﬁer is conﬁdent enough in its prediction or not. A different approach to OOD detection is given by the use of the Mahalanobis distance [14, 22], which consists in calculating how much the observed out-distribution sample deviate from the in-distribution ones but assuming the latter are given. 1.1 Summary of contributions
Our work tackles the problem of identifying whether the prediction of a classiﬁer should or should not be trusted, no matter if they are made on out or in-distribution samples, and advances the state-of-the-art in multiple ways.
• From the theoretical point of view, we derive the trade-off between two types of error probabilities: Type-I, that refers to the rejection of the classiﬁcation for an input that would be correctly classiﬁed, and Type-II, that refers to the acceptance of the classiﬁcation for an input that would not be correctly classiﬁed (Proposition 3.1). The characterization of the optimal discriminator in eq. (10) allows us to devise a feasible implementation of it, based on the softmax probability (Proposition 3.2).
• From the algorithmic point of view, inspired by our theoretical analysis, we propose DOCTOR a new discriminator (Deﬁnition 2), which yields a simple and ﬂexible framework to detect whether a decision made by a model is likely to be correct or not. We distinguish two scenarios under which DOCTOR can be deployed: Totally Black Box (TBB) where only the soft-predictions are available, hence gradient-propagation to perform input pre-processing is not allowed, and Partially Black Box (PBB) where we further allow method-speciﬁc inputs perturbations.
• From the experimental point of view, we show that DOCTOR outperforms comparable state-of-the-art methods (e.g., ODIN [23], softmax response [6] and Mahalanobis distance [22]) on datasets including both in-distribution and out-of-distribution samples, and different architectures with various expressibilities, under both TBB and PBB. A key ingredient of
DOCTOR is to fully exploit all available information contained in the soft-probabilities of the predictions (not only their maximum). 1.2