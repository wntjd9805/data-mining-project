Abstract
A deep neural network (DNN) has achieved great success in many machine learning tasks by virtue of its high expressive power. However, its prediction can be easily biased to undesirable features, which are not essential for solving the target task and are even imperceptible to a human, thereby resulting in poor generalization.
Leveraging plenty of undesirable features in out-of-distribution (OOD) examples has emerged as a potential solution for de-biasing such features, and a recent study shows that softmax-level calibration of OOD examples can successfully remove the contribution of undesirable features to the last fully-connected layer of a classiﬁer.
However, its applicability is conﬁned to the classiﬁcation task, and its impact on a DNN feature extractor is not properly investigated. In this paper, we propose
TAUFE, a novel regularizer that deactivates many undesirable features using OOD examples in the feature extraction layer and thus removes the dependency on the task-speciﬁc softmax layer. To show the task-agnostic nature of TAUFE, we rigorously validate its performance on three tasks, classiﬁcation, regression, and a mix of them, on CIFAR-10, CIFAR-100, ImageNet, CUB200, and CAR datasets.
The results demonstrate that TAUFE consistently outperforms the state-of-the-art method as well as the baselines without regularization. 1

Introduction
Undesirable features, which are informally deﬁned as those not relevant to a target task, frequently appear in training data; for example, the background is an undesirable feature for classifying animals in images. The undesirable features are mainly caused by the statistical bias in in-distribution training data. In fact, many undesirable features are statistically correlated with labels, even though they are unnecessary and sometimes even harmful for the target task [1]; for example, the “desert” background feature is correlated with “camels” because the camels frequently appear in a desert. However, such undesirable features (e.g., desert background) rather yield unreliable predictions because they are easily shifted in other unseen data (e.g., images of the camels on the road).
Meanwhile, deep neural networks (DNNs) are known to overly capture any high-frequency data components which are even imperceptible to a human [2, 3]. This property is attributed to the vulnerability of DNNs that can totally overﬁt to random labels or adversarial examples owing to their extremely high capacity [3, 4, 5, 6]. Accordingly, DNNs are easily biased toward the undesirable features as well, thereby often showing unsatisfactory generalization to unseen examples [7]. Thus, it is very important to prevent overﬁtting to the undesirable features.
In this regard, a few research efforts have been devoted to remove the negative inﬂuence of undesirable features by leveraging out-of-distribution (OOD) data [8, 9]. Under the assumption that in-distribution and OOD data share undesirable features, OOD data is treated as a useful resource to alleviate the aforementioned undesirable bias. Notably, a recent study [8] proposed a softmax-level calibration,
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In-distribution Examples
Out-of-distribution Examples
“Task-agnostic”
CNN
FC
“Task-specific”
Softmax Prob.
Uniform Prob.
Classification Task (cid:1850)(cid:3036)(cid:3041) (cid:1850)(cid:3042)(cid:3042)(cid:3031) (cid:1850)(cid:3036)(cid:3041) (cid:1850)(cid:3042)(cid:3042)(cid:3031)
CNN
FC
Softmax Prob.
Classification Task (cid:1709)
Box IoU
Regression Task
Zero 
Activations (a) Softmax-level calibration. (b) Feature-level calibration (TAUFE).
Figure 1: Comparison of softmax-level and feature-level calibrations. which assigns uniform softmax probabilities to all possible labels for all examples in OOD data.
Although this approach shows decent de-biasing performance in the classiﬁcation task, the softmax-level calibration has two limitations:
• Lack of Flexibility: The softmax-level calibration is designated only for the classiﬁcation task.
However, the bias toward undesirable features occurs in numerous tasks, such as object localization and bounding box regression. Therefore, a ﬂexible, task-agnostic approach is required for easily supporting other downstream tasks too.
• Feature Entanglement: Even desirable features can be entangled with undesirable ones by assigning the uniform softmax probability invariably to all possible labels for OOD examples.
Thus, the negative inﬂuence of the undesirable features is not perfectly removed because they still remain and affect the activation of desirable features (See § 3.3 for details).
In this paper, we propose a novel task-agnostic and feature-level calibration method, called TAUFE (Task-Agnostic Undesirable Feature dEactivation), which explicitly forces a model to produce zero values for many undesirable features in OOD examples. Differently from the softmax-level calibration that regularizes the classiﬁcation layer (Figure 1(a)), TAUFE exploits the penultimate layer right before the classiﬁcation layer and deactivates its activation only for OOD examples (Figure 1(b)). Thus,
TAUFE is applicable to any task that requires another task-speciﬁc layer other than the classiﬁcation layer, and the undesirable features are removed in the feature level without feature entanglement. The superiority of the proposed feature-level calibration over the softmax-level calibration is proven by theoretical and empirical analysis of the feature activation of the penultimate layer.
To validate its general efﬁcacy, we conducted extensive experiments through three tasks: (i) image classiﬁcation for classiﬁcation; (ii) bounding box regression for regression; and (iii) weakly supervised object localization (WSOL) for a mix of them. We tested multiple pairs of in-distribution and OOD data: CIFAR-10, CIFAR-100, ImageNet, CUB200, and CAR for in-distribution; and SVHN, LSUN, and Places365 for OOD. The experiment results demonstrate that TAUFE consistently outperforms the softmax-level calibrator [8] by up to 9.88% for classiﬁcation and by up to 8.03% for the mix of classiﬁcation and regression.
Our main contributions are summarized as follows: 1. We propose a simple yet effective method, TAUFE, to deactivate undesirable features in learning, which is easily applicable to any standard learning task with recent DNNs. 2. We provide an insight on how feature-level and softmax-level calibration differently affect feature extraction by theoretic and empirical analysis on the penultimate layer activation. 3. We validate the task-agnostic nature of TAUFE through three tasks and show its performance advantage over the state-of-the-art method. 2