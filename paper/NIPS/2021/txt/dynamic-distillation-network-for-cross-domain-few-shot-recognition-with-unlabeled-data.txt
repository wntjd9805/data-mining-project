Abstract
Most existing works in few-shot learning rely on meta-learning the network on a large base dataset which is typically from the same domain as the target dataset. We tackle the problem of cross-domain few-shot learning where there is a large shift between the base and target domain. The problem of cross-domain few-shot recog-nition with unlabeled target data is largely unaddressed in the literature. STARTUP was the first method that tackles this problem using self-training. However, it uses a fixed teacher pretrained on a labeled base dataset to create soft labels for the unlabeled target samples. As the base dataset and unlabeled dataset are from different domains, projecting the target images in the class-domain of the base dataset with a fixed pretrained model might be sub-optimal. We propose a simple dynamic distillation-based approach to facilitate unlabeled images from the nov-el/base dataset. We impose consistency regularization by calculating predictions from the weakly-augmented versions of the unlabeled images from a teacher net-work and matching it with the strongly augmented versions of the same images from a student network. The parameters of the teacher network are updated as exponential moving average of the parameters of the student network. We show that the proposed network learns representation that can be easily adapted to the target domain even though it has not been trained with target-specific classes during the pretraining phase. Our model outperforms the current state-of-the art method by 4.4% for 1-shot and 3.6% for 5-shot classification in the BSCD-FSL benchmark, and also shows competitive performance on traditional in-domain few-shot learning task. Our code is available at: https://git.io/Jilgs. 1

Introduction
The tremendous success of deep learning in visual recognition tasks is, to a great extent, attributed to the availability of large scale labeled datasets. While humans can recognize an object by looking only at a few examples, modern deep neural networks require hundreds or thousands of images for each category to achieve human-level visual recognition capability. This has led to the research on few-shot learning which aims at learning from a much smaller dataset. In a typical few-shot learning setting, there are two stages: meta-training and meta-testing. In the meta-training stage, a base dataset with labeled images is provided to train the model. In the meta-testing stage, the learned model is quickly adapted to a set of novel classes with only a few examples per class (the support set) and evaluated on a set of test images from the same novel classes (the query set). The base classes and novel classes are typically disjoint, but the images are obtained from the same domain. However, in many real world settings, training the model on a base dataset from the same domain as the target dataset is difficult and infeasible. Guo et al. [7] proposed a cross-domain few-shot benchmark, BSCD-FSL, which contains datasets from extremely different domains. In this benchmark, the meta-training is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Problem setup. (Left) In typical few-shot learning task, a model is trained on a base dataset first during meta-training stage. In meta-testing stage, a few examples from novel classes, referred to as support set, are provided, and the network predicts the categories of different samples from the same classes as support set.
The base dataset and the target dataset generally come from the same domain with disjoint categories. (Middle)
In cross-domain few-shot learning, there is a domain gap between the base dataset and the target dataset. For example, in the figure, the base dataset contains natural images from miniImageNet [29], and the target dataset consists of satellite images from EuroSAT dataset [8]. (Right) Our setting is similar to cross-domain few-shot learning setup. However, additional unlabeled images are also available during meta-training stage. Although the unlabeled dataset comes from the same domain as the target dataset, it does not contain any images either from the support set or query set. done on a labeled source dataset, and the few-shot evaluation is performed on a target dataset which is from different domain than the source dataset. The benchmark shows that traditional pretraining and finetuning outperforms more complicated meta-learning based few-shot learning methods by a significant margin.
In the real-world scenarios, the target domain should have many unlabeled images, and it might be beneficial to use the unlabeled data to learn more target domain specific representations. We hypothesize that using both labeled base data and unlabeled target data during training provides a common embedding for both base and target domain. Then the natural question could be - why not use the unlabeled target data only, it might provide more target-specific representation. One issue with this approach is that self-supervised learning generally requires a large amount of unlabeled data to work, and, as pointed out by Phoo and Hariharan [18], plain self-supervised learning struggles to outperform the naive transfer learning baseline in few-shot learning setup. Secondly, it has been shown that combining supervised and unsupervised learning during training provides more transferable representation [9]. We argue that similar conclusion holds for cross-domain few-shot learning, i.e., combining supervised and unsupervised loss provides better representation for the downstream task.
Figure 1 illustrates our experimental setup in contrast to traditional few-shot learning or cross-domain few-shot learning setup. We show that labeled images from the base dataset are still important to learn generic image features, and images from the target domain, even if unlabeled, can help developing more target domain specific representations.
Figure 2 illustrates our approach. Our goal is to train a feature extractor which will be used to evaluate few-shot learning performance on the target dataset. We propose a dynamic distillation-based approach to this end. The student network consists of an encoder fs and classifier gs, and the teacher network shares similar architecture as the student network (denoted as ft and gt). The classifier gs is a linear layer that predicts the class-logits of the samples from the base dataset. We calculate a supervised cross-entropy loss between the student’s predictions and ground-truth labels on the base dataset. For the unlabeled target data, we compute the teacher’s prediction for a weakly-augmented version of an image and the student’s prediction for a strongly augmented version of the same image, and optimize a distillation loss to match the predictions. We also apply sharpening in the teacher prediction to encourage low-entropy prediction from the student. Both the supervised loss and distillation loss are used to learn the student’s weights. The teacher network is updated as a moving average of the student network. During few-shot evaluation, we only use the student 2
encoder fs as a feature extractor, learn a classifier head on the labeled support images consisting of few examples per category, and calculate the class predictions of the query images.
Our main contributions are:
• We propose a simple method for few-shot learning across extreme domain difference.
• We use dynamic distillation based approach that uses both labeled source data and unlabeled target data to learn a better representation for the few-shot evaluation on the target domain.
• Our method significantly outperforms the current state of the art in the BSCD-FSL bench-mark with unlabeled images by 4.4% for 1-shot and 3.6% for 5-shot classification in terms of average top-1 accuracy. It even shows superior performance for in-domain few-shot classification on miniImageNet and tieredImageNet datasets. 2