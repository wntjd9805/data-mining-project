Abstract
Current theoretical results on optimization trajectories of neural networks trained by gradient descent typically have the form of rigorous but potentially loose bounds on the loss values. In the present work we take a different approach and show that the learning trajectory of a wide network in a lazy training regime can be characterized by an explicit asymptotic at large training times. Speciﬁcally, the leading term in the asymptotic expansion of the loss behaves as a power law
L(t) ∼ Ct−ξ with exponent ξ expressed only through the data dimension, the smoothness of the activation function, and the class of function being approximated.
Our results are based on spectral analysis of the integral operator representing the linearized evolution of a large network trained on the expected loss. Importantly, the techniques we employ do not require a speciﬁc form of the data distribution, for example Gaussian, thus making our ﬁndings sufﬁciently universal. 1

Introduction
A major challenge in the research of neural networks is the quantitative theoretical description of their optimization by gradient descent. At present, many aspects of network training seem to be understood rather well on a qualitative level, or admit convincing heuristic explanations, but we seem to lack tools for making reasonably accurate quantitative predictions, even for relatively simple models and data. In this sense, the theory of neural networks compares unfavorably to physics, which is also an application-driven ﬁeld but with an apparently much more successful penetration of theoretical methods. The main difﬁculty here is probably the complex structure of the data and models, which are hard to describe in terms of convenient and simple mathematical abstractions.
In recent years, a signiﬁcant progress in the theoretical analysis of gradient descent of neural networks has been associated with the limit of large networks, which can be studied using various methods from partial differential equations [26, 31], kernel methods [20, 21], spin glass theory [12], random matrix theory [29], dynamical systems [30], and other mathematical ﬁelds.
In the present work, we consider a setting of large networks and large, smoothly distributed data sets that allows us to obtain explicit leading terms in the long-term evolution of the loss under gradient descent. We are inspired by the spectral theory of singular integral operators [5], which we apply to the linearized evolution of the network. While this linearized evolution has been widely studied recently, most related research seems to focus on theoretical convergence guarantees and upper bounds for the loss values [3, 27], or on a highly symmetric problems admitting explicit solution
[35, 38]. In contrast, we focus on explicit loss evolution formulas, which we ﬁnd as power laws
L(t) ∼ Ct−ξ. (1)
We argue that the exponents ξ here exhibit some form of universality, in that they are essentially determined by the input dimension d and by the smoothness classes of the activation function and the target function. In particular, we ﬁnd that in the case of ReLU networks approximating an indicator 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The loss trajectories and spectral properties of the neural tangent kernels of shallow networks in the NTK regime. The target function (i.e., the initial displacement between the network output and the approximated function) is either generated by a Gaussian process (GP) modeled by a larger network of the same architecture, or is an indicator function of a d-dimensional ball (Ind). The data distributions µ are modeled as mixtures of 8 Gaussian distributions with random centers, and the data dimension is either d = 2 or d = 4. The solid lines show the numerically obtained values, while the dashed lines show the respective theoretical power-law asymptotics. The dataset size is
M = 104 (see Section A (SM) for further details of experiments).
Left: Loss evolution for a shallow network with width N = 3000. The scaling exponent giving the slope of the theoretical asymptotic is ξ = β d+α for Ind (see Section 5.2).
Center: Distribution of the inﬁnite network NTK eigenvalues λn. The theoretical scaling exponent is ν = 1 + 1 d (see Section 5.1). Right: Distributions of the coefﬁcient partial sums sn (see Eq. (7)).
The theoretical scaling exponent is κ = β d for GP and κ = 1 d for Ind (see Section 5.2). d+1 for GP and ξ = 1 d+α = 3 d = 3 function of some region in the d-dimensional space (a classiﬁcation problem target), the natural value of the exponent is ξ = 1 d+1 . On the other hand, in the case of target functions generated by a randomly initialized wide ReLU network, the exponent is ξ = 3 d+1 . Our approach also allows us to obtain explicit expressions for the coefﬁcient C in these cases.
The power law (1) is established using similar power laws (but with different exponents and coef-ﬁcients) for the eigenvalues of the evolution operator and for the coefﬁcients in the expansion of the target function over corresponding eigenvectors. These power laws are indeed conﬁrmed by our experiments (see Figure 1).
Our main scenario is approximation by shallow ReLU network in the NTK regime, but we also brieﬂy consider several modiﬁcations of this scenario, namely the activation functions (x+)q with q > 0, approximation by a deep network in the NTK regime, and approximation in the mean ﬁeld regime. 2