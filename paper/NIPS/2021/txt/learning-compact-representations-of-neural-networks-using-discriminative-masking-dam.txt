Abstract
A central goal in deep learning is to learn compact representations of features at every layer of a neural network, which is useful for both unsupervised representa-tion learning and structured network pruning. While there is a growing body of work in structured pruning, current state-of-the-art methods suffer from two key limitations: (i) instability during training, and (ii) need for an additional step of fine-tuning, which is resource-intensive. At the core of these limitations is the lack of a systematic approach that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve state-of-the-art performance. We present a novel single-stage structured pruning method termed DiscriminAtive Masking (DAM). The key intuition behind DAM is to discriminatively prefer some of the neurons to be refined during the training pro-cess, while gradually masking out other neurons. We show that our proposed DAM approach has remarkably good performance over a diverse range of applications in representation learning and structured pruning, including dimensionality reduction, recommendation system, graph representation learning, and structured pruning for image classification. We also theoretically show that the learning objective of
DAM is directly related to minimizing the L0 norm of the masking layer.
All of our codes and datasets are available https://github.com/jayroxis/ dam-pytorch. 1

Introduction
A central goal in deep learning is to learn compact (or sparse) representations of features at every layer of a neural network that are useful in a variety of machine learning tasks. For example, in unsupervised representation learning problems [3], there is a long-standing goal to learn low-dimensional embeddings of input features that are capable of reconstructing the original data [44, 35, 24, 33]. Similarly, in supervised learning problems, there is a growing body of work in the area of network pruning [4], where the goal is to reduce the size of modern-day neural networks (that are known to be heavily over-parameterized [7, 51, 22, 45]) so that they can be deployed in resource-constrained environments (e.g., over mobile devices) without compromising on their accuracy. From this unified view of representation learning and network pruning, the generic problem of “learning compact representations” has applications in several machine learning use-cases such as dimensionality reduction, graph representation learning, matrix factorization, and image classification.
*These authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A theoretically appealing approach for learning compact representations is to introduce regularization penalties in the learning objective of deep learning that enforce L0 sparsity of the network parameters,
θ. However, directly minimizing the L0 norm requires performing a combinatorial search over all possible subsets of weights in θ, which is computationally intractable. In practice, a common approach for enforcing sparsity is to use a continuous approximation of the L0 penalty in the learning objective, e.g., L1-based regularization (or Lasso [42]) and its variants [39, 47]. While such techniques are capable of pruning individual weights and thus reducing storage requirements, they do not offer any direct gains in inference speed since the number of features generated at the hidden layers can still be large even though the network connectivity is sparse [4]. Instead, we are interested in the area of structured network pruning for learning compact representations, where the sparsity is induced at the level of neurons by pruning features (or channels) instead of individual weights.
∈
∈
⊙
While there is a growing body of work in structured network pruning [26, 28, 43, 14], the basic structure of most state-of-the-art (SOTA) methods in this area (e.g., ChipNet [43] and NetSlim
[28]) can be described as training a learnable vector of mask
Rn, g = [g1, g2, ..., gn]⊺, which when multiplied parameters, g
Rn, results in with the features extracted at a hidden layer, h the pruned outputs of this layer, o = g h. Sparsity in the mask parameters is generally enforced using different approximations of the L0 norm of g (e.g., use of Lasso in NetSlim and use of “crispness” loss in ChipNet). Despite recent progress in this area, current SOTA in structured pruning suffer from two key limitations. First, since most methods do not explicitly minimize the L0 norm of g during pruning, they often suffer from training instabilities. In particular, most SOTA methods [28, 43] involve thresholding techniques to set small non-zero weights to zero leading to large drops in accuracy during the training process, as evidenced by our results in this paper. Second, once the pruning process is complete and we have converged at a compact network, most SOTA methods still need an additional step of fine-tuning the network in order to achieve reasonable accuracy. This is not only resource-intensive but there is also an on-going discussion on whether and how we should fine-tune [36] or rewind to initial weights
[23] or train from scratch [29], making it difficult to prefer one approach over another.
Figure 1:
Illustration of Discrimi-native Masking. The gate function shifts to the "right" during training resulting in more zeros in the mask on convergence.
At the core of these limitations is the lack of a systematic approach for structured network pruning that jointly prunes and refines weights during training in a single stage, and does not require any fine-tuning upon convergence to achieve SOTA performance. Notice that in existing methods for structured pruning, allowing every neuron j to be pruned differently using an independent mask parameter gj only increases the number of learnable (or free) parameters in the learning objective, increasing the complexity of the problem. Instead, we ask the question: “Can we leverage the intrinsic symmetry of neural network initialization to design a pruning mask using the least number of free parameters?”
G
We present a simple solution to this question by proposing a new single-stage structured pruning method that learns compact representations while training and does not require fine-tuning, termed
DiscriminAtive Masking (DAM). The basic idea of DAM is to use a monotonically increasing gate for masking every neuron in a layer that only depends on the index j = 1, 2, ..., n (or function position) of the neuron in the layer and a scalar parameter β to be learned during training (see
Figure 1). At the start of training, the gate function admits non-zero values for all neurons in the layer, allowing all neurons to be unmasked (or active). As the training progresses, the gate function gradually shifts from “left” to “right” as a result of updating β such that upon convergence, only a subset of neurons (on the extreme right) are active while all others are masked out. The key intuition behind DAM is to discriminatively prefer some of the neurons (on the right) to be refined (or re-adapted) during the training process for capturing useful features, while gradually masking out (or pruning) neurons on the left. This preferential pruning of neurons using a very simple gate function helps in regulating the number of features transmitted to the next layer.*
Contributions: We show that our proposed DAM approach has remarkably good performance over various applications, including dimensionality reduction, recommendation system, graph represen-*This is somewhat analogous to how a physical dam regulates the flow of water by shifting a movable gate. 2
tation learning, and structured pruning for image classification, achieving SOTA performance for structured pruning. This shows the versatility of DAM in learning compact representations on diverse problems and network choices, in contrast to baseline methods for structured pruning that are only developed and tested for the problem of image classification [28, 43]. Our approach is single-stage, does not require fine-tuning (and hence has lower training time), and does not suffer from training instabilities. We also theoretically show that the learning objective of DAM is directly related to minimizing the L0 norm of the discriminative mask, providing a new differentiable approximation for enforcing L0 sparsity. Our approach is also easy to implement and can be applied to any layer in a network architecture. The simplicity and effectiveness of DAM provides unique insights into the problem of learning compact representations and its relationship with preferential treatment of neurons for pruning and refining, opening novel avenues of systematic research in this rapidly growing area. 2