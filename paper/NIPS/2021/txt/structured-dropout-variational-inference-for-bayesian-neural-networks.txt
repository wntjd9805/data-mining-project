Abstract
Approximate inference in Bayesian deep networks exhibits a dilemma of how to yield high ﬁdelity posterior approximations while maintaining computational efﬁciency and scalability. We tackle this challenge by introducing a novel varia-tional structured approximation inspired by the Bayesian interpretation of Dropout regularization. Concretely, we focus on the inﬂexibility of the factorized structure in Dropout posterior and then propose an improved method called Variational
Structured Dropout (VSD). VSD employs an orthogonal transformation to learn a structured representation on the variational Gaussian noise with plausible complex-ity, and consequently induces statistical dependencies in the approximate posterior.
Theoretically, VSD successfully addresses the pathologies of previous Variational
Dropout methods and thus offers a standard Bayesian justiﬁcation. We further show that VSD induces an adaptive regularization term with several desirable properties which contribute to better generalization. Finally, we conduct extensive experiments on standard benchmarks to demonstrate the effectiveness of VSD over state-of-the-art variational methods on predictive accuracy, uncertainty estimation, and out-of-distribution detection. 1

Introduction
Bayesian Neural Networks (BNNs) [49, 63] offer a probabilistic interpretation for deep learning models by imposing a prior distribution on the weight parameters and aiming to infer a posterior distribution instead of only point estimates. By marginalizing over this posterior for prediction,
BNNs perform a procedure of ensemble learning. These principles improve the model generalization, robustness and allow for uncertainty quantiﬁcation. However, exactly computing the posterior of non-linear BNNs is infeasible and approximate inference has been devised. The core challenge is how to construct an expressive approximation for the true posterior while maintaining computational efﬁciency and scalability, especially for modern deep learning architectures.
Variational inference is a popular deterministic approximation approach to deal with this challenge.
The ﬁrst practical methods were proposed in [22, 8, 39], in which the approximate posteriors are assumed to be fully factorized distributions, also called mean-ﬁeld variational inference. In general, the mean-ﬁeld approximation family encourages several advantages in inference including computational tractability and effective optimization with the stochastic gradient-based methods.
However, it ignores the strong statistical dependencies among random weights of neural nets, leading to the inability to capture the complicated structure of the true posterior and to estimate the true model uncertainty.
∗These two authors contributed equally. †Correspondence to Son Nguyen: <v.sonnv27@vinai.io> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To overcome this limitation, many extensive studies proposed to provide posterior approximations with richer expressiveness. For instance, [47] treats the weight matrix as a whole via a matrix variate Gaussian [24] and approximates the posterior based on this parametrization. Several later works have exploited this distribution to investigate different structured representations for the variational Gaussian posterior, such as Kronecker-factored [89, 71, 72], k-tied distribution [77], non-centered or rank-1 parameterization [21, 15]. Another original idea to represent the true covariance matrix of Gaussian posterior is by employing the low-rank approximation [67, 35, 80]. For robust approximation with multimodality, [48] adopted hierarchical variational model framework [69] for inferring an implicit marginal distribution in high dimensional Bayesian setting. Despite signiﬁcant improvements in both predictive accuracy and uncertainty calibration, some of these methods incur a large computational complexity and are difﬁcult to integrate into deep convolutional networks.
Motivations. In this paper, we approach the structured posterior approximation in Bayesian neural nets from a different perspective which has been inspired by the Bayesian interpretation of Dropout training [74, 51]. More speciﬁcally, the methods proposed in [39, 20] reinterpret Dropout regular-ization as approximate inference in Bayesian deep models and base on this connection to learn a variational Dropout posterior over the weight parameters. From the literature, inference approaches based on Bayesian Dropout have shown competitive performances in terms of predictive accuracy on various tasks, even compared to the structured Bayesian methods aforementioned, but with much cheaper computational complexity. Moreover, with the solid and intriguing theories on effective regularization [81, 26, 83], generalization bound [52, 59], convergence rate and robust optimiza-tion [55, 54, 7], Dropout principle offers several potentials to further improve approximate inference in Bayesian deep networks. However, since these Bayesian Dropout methods also employed simple structures of the mean-ﬁeld family, their approximations often fail to obtain satisfactory uncertainty estimates [17]. In addition, Variational Dropout methods based on multiplicative Gaussian noise also suffer from theoretical pathologies, including improper prior leading to ill-posed true posterior, and singularity of the approximate posterior making the variational objective undeﬁned [31].
Contributions. With the above insights, we propose a novel structured variational inference frame-work, which rationally acquires complementary beneﬁts of the ﬂexible Bayesian inference and
Dropout inductive bias. Our method adopts an orthogonal approximation called Householder transfor-mation to learn a structured representation for multiplicative Gaussian noise in Variational Dropout method [39, 57]. As a consequence of the Bayesian interpretation, we go beyond the mean-ﬁeld family and obtain a variational Dropout posterior with structured covariance. Furthermore, to make our framework more expressive, we deploy a hierarchical Dropout procedure, which is equivalent to inferring a joint posterior in a hierarchical Bayesian neural nets. We name the proposed method as
Variational Structured Dropout (VSD) and summarize its advantages as follows: 1. Our structured approximation is implemented on low dimensional space of variational noise with considerable computational efﬁciency. VSD can be employed for deep CNNs in a direct way while maintaining the backpropagation in parallel and optimizing efﬁciently with gradient-based methods. 2. Especially, VSD has a standard Bayesian justiﬁcation, in which our method can overcome the critiques from the non-Bayesian perspective of previous Variational Dropout methods. Our inference framework uses a proper prior, non-singular approximate posterior and derives a tractable variational lower bound without further simpliﬁed approximation. 3. Compared with previous Bayesian Dropout methods which are relatively inﬂexible by some strict conditions, VSD is more efﬁcient on both the criteria of expressive approximation and ﬂexible hierarchical modeling. Therefore, VSD is promising to be a general-purpose approach for Bayesian inference and in particular for BNNs. 4. To reinforce the complementary advantages uniﬁed in our proposal, we also investigate the inductive biases induced by the adaptive regularization of structured Dropout noise. We further provide an interpretation that VSD implicitly facilitates the networks to converge to a local minima with smaller spectral norms and stable rank. This properties suggests better generalization and we present empirical results to support this implication. 5. Finally, we carry out extensive experiments with standard datasets and different network architec-tures to validate the effectiveness of our method on many criteria, including scalability, predictive accuracy, uncertainty calibration, and out-of-distribution detection, in comparison to popular varia-tional inference methods. 2
Notation. For a matrix A, (cid:107)A(cid:107)F and A(cid:62) denotes the Frobenius norm and the transpose matrix,
Ai: and A:j denote the i-th row and the j-th column. For an interger i, ei is the i-th standard basis, 1i ∈ Ri is the vector of all ones. The diagonal matrix with diagonal entries as the elements of a vector x is denoted by diag(x). The inner product between two matrices A and B is denoted by (cid:104)A, B(cid:105). 2