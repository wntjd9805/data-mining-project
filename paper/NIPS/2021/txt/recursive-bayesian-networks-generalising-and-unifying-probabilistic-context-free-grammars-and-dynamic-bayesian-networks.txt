Abstract
Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are widely used sequence models with complementary strengths and limi-tations. While PCFGs allow for nested hierarchical dependencies (tree structures), their latent variables (non-terminal symbols) have to be discrete. In contrast, DBNs allow for continuous latent variables, but the dependencies are strictly sequential (chain structure). Therefore, neither can be applied if the latent variables are as-sumed to be continuous and also to have a nested hierarchical dependency structure.
In this paper, we present Recursive Bayesian Networks (RBNs), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as spe-cial cases. RBNs deﬁne a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the contin-uous variables. We provide two solutions: 1) For arbitrary RBNs, we generalise inside and outside probabilities from PCFGs to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) For Gaussian
RBNs, we additionally derive an analytic approximation of the marginal data likeli-hood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The capacity and diverse applications of
RBNs are illustrated on two examples: In a quantitative evaluation on synthetic data, we demonstrate and discuss the advantage of RBNs for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In an application to musical data, we approach the unsolved problem of hierarchical music analysis from the raw note level and compare our results to expert annotations. 1

Introduction
Long-term dependencies with a nested hierarchical structure are one of the major challenges in modelling sequential data. This type of dependencies is common in many domains, such as natural
∗corresponding author, code at https://github.com/robert-lieck/RBN 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Structure
Linear/Chain
Hierarchical/Tree y l n s O e l b a i r a
V
/ e t e r c s i
D e t e r c s i
D s u o u n i t n o
C
Regular Grammars/
Hidden Markov Models
Probabilistic
Context-Free Grammars
Dynamic
Bayesian Networks
Recursive
Bayesian Networks
Figure 1: RBNs generalise PCFGs by allowing for continuous latent variables and DBNs by incorpo-rating nested hierarchical dependencies. language [1], music [2, 3], or decision making [4, 5]. Two of the most widely used probabilistic models for sequential data are probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs), both having complementary strengths.
PCFGs are well-established and widely used for modelling hierarchical long-term dependencies in symbolic data [1, 6, 7, 8, 9, 5, 10]. They generalise local (Markov) transition models by allowing for inﬁnitely many levels of nested hierarchical dependencies and a ﬂexible number of latent variables.
However, parsing methods such as the Cocke-Younger-Kasami (CYK) algorithm [11, 12, 13, 14, 6] rely on the discrete nature of the rules and variables.
In contrast, DBNs are sequential models with a ﬁxed set of random variables that reoccur at each time step [15, 16]. The variables at each time step may be discrete or continuous, latent or observed, and may have an arbitrary non-cyclic dependency structure among each other, with additional links from the previous and to the next time slice. They comprise important model classes as special cases, such as hidden Markov models (HMMs) if there is only a single discrete latent variable or linear dynamical systems if all dependencies are linear Gaussians [17, 18, 16]. However, DBNs only allow for a ﬁxed chain of Markov dependencies between time slices and cannot represent nested hierarchical structures.
In this paper, we present Recursive Bayesian Networks (RBNs), a novel class of probabilistic models that combines the strengths of PCFGs and DBNs by allowing for nested hierarchical dependencies in combination with arbitrary discrete or continuous random variables (Figure 1). Our main contributions are as follows: 1. With RBNs, we provide a uniﬁed theoretical framework for a large class of important sequence models, including PCFGs and DBNs. 2. We generalise inside and outside probabilities from PCFGs to continuous latent variables, allowing for maximum posterior (MAP) inference in arbitrary RBNs. 3. For Gaussian RBNs, we derive an analytic approximation for the marginal likelihood and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. 4. We provide a quantitative evaluation on synthetic data and an application to the challenging task of hierarchical music analysis. 1.1