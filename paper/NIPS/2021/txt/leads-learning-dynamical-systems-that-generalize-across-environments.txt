Abstract
When modeling dynamical systems from real-world data samples, the distribution of data often changes according to the environment in which they are captured, and the dynamics of the system itself vary from one environment to another.
Generalizing across environments thus challenges the conventional frameworks.
The classical settings suggest either considering data as i.i.d. and learning a single model to cover all situations or learning environment-speciﬁc models. Both are sub-optimal: the former disregards the discrepancies between environments leading to biased solutions, while the latter does not exploit their potential commonalities and is prone to scarcity problems. We propose LEADS, a novel framework that leverages the commonalities and discrepancies among known environments to improve model generalization. This is achieved with a tailored training formulation aiming at capturing common dynamics within a shared model while additional terms capture environment-speciﬁc dynamics. We ground our approach in theory, exhibiting a decrease in sample complexity w.r.t. classical alternatives. We show how theory and practice coincides on the simpliﬁed case of linear dynamics. Moreover, we instantiate this framework for neural networks and evaluate it experimentally on representative families of nonlinear dynamics. We show that this new setting can exploit knowledge extracted from environment-dependent data and improves generalization for both known and novel environments. 1

Introduction
Data-driven approaches offer an interesting alternative and complement to physical-based methods for modeling the dynamics of complex systems and are particularly promising in a wide range of settings: e.g. if the underlying dynamics are partially known or understood, if the physical model is incomplete, inaccurate, or fails to adapt to different contexts, or if external perturbation sources and forces are not modeled. The idea of deploying machine learning (ML) to model complex dynamical systems picked momentum a few years ago, relying on recent deep learning progresses and on the development of new methods targeting the evolution of temporal and spatiotemporal systems [6, 9, 7, 21, 30, 2, 37]. It is already being applied in different scientiﬁc disciplines (see e.g. [36] for a recent survey) and could help accelerate scientiﬁc discovery to address challenging domains such as climate [32] or health [12].
However, despite promising results, current developments are limited and usually postulate an idealized setting where data is abundant and the environment does not change, the so-called “i.i.d. hypothesis”. In practice, real-world data may be expensive or difﬁcult to acquire. Moreover, changes in the environment may be caused by many different factors. For example, in climate modeling, there are external forces (e.g. Coriolis) which depend on the spatial location [23]; or, in health science, parameters need to be personalized for each patient as for cardiac computational models [27]. More 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
generally, data acquisition and modeling are affected by different factors such as geographical position, sensor variability, measuring circumstances, etc. The classical paradigm either considers all the data as i.i.d. and looks for a global model, or proposes speciﬁc models for each environment. The former disregards discrepancies between the environments, thus leading to a biased solution with an averaged model which will usually perform poorly. The latter ignores the similarities between environments, thus affecting generalization performance, particularly in settings where per-environment data is limited. This is particularly problematic in dynamical settings, as small changes in initial conditions lead to trajectories not covered by the training data.
In this work, we consider a setting where it is explicitly assumed that the trajectories are collected from different environments. Note that in this setting, the i.i.d. hypothesis is removed twice: by considering the temporality of the data and by the existence of multiple environments. In many useful contexts the dynamics in each environment share similarities, while being distinct which translates into changes in the data distributions. Our objective is to leverage the similarities between environments in order to improve the modeling capacity and generalization performance, while still carefully dealing with the discrepancies across environments. This brings us to consider two research questions:
RQ1 Does modeling the differences between environments improve generalization error w.r.t. classi-cal settings: One-For-All, where a unique function is trained for all environments; and One-Per-Env., where a speciﬁc function is ﬁtted for each environment? (cf. Sec. 4 for more details)
RQ2 Is it possible to extrapolate to a novel environment that has not been seen during training?
We propose LEarning Across Dynamical Systems (LEADS), a novel learning methodology decom-posing the learned dynamics into shared and environment-speciﬁc components. The learning problem is formulated such that the shared component captures the dynamics common across environments and exploits all the available data, while the environment-speciﬁc component only models the remain-ing dynamics, i.e. those that cannot be expressed by the former, based on environment-speciﬁc data.
We show, under mild conditions, that the learning problem is well-posed, as the resulting decompo-sition exists and is unique (Sec. 2.2). We then analyze the properties of this decomposition from a sample complexity perspective. While, in general, the bounds might be too loose to be practical, a more precise study is conducted in the case of linear dynamics for which theory and practice are closer. We then instantiate this framework for more general hypothesis spaces and dynamics, lead-ing to a heuristic for the control of generalization that will be validated experimentally. Overall, we show that this framework provides better generalization properties than One-Per-Env., requiring less training data to reach the same performance level (RQ1). The shared information is also useful to extrapolate to unknown environments: the new function for this environment can be learned from very little data (RQ2). We experiment with these ideas on three representative cases (Sec. 4) where the dynamics are provided by differential equations: ODEs with the Lotka-Volterra predator-prey model, and PDEs with the Gray-Scott reaction-diffusion and the more challenging incompressible
Navier-Stokes equations. Experimental evidence conﬁrms the intuition and the theoretical ﬁndings: with a similar amount of data, the approach drastically outperforms One-For-All and One-Per-Env. settings, especially in low data regimes. Up to our knowledge, it is the ﬁrst time that generalization in multiple dynamical systems is addressed from an ML perspective1. 2 Approach 2.1 Problem setting
We consider the problem of learning models of dynamical physical processes with data acquired from a set of environments E. Throughout the paper, we will assume that the dynamics in an environment e ∈ E are deﬁned through the evolution of differential equations. This will provide in particular a clear setup for the experiments and the validation. For a given problem, we consider that the dynamics of the different environments share common factors while each environments has its own speciﬁcity, resulting in a distinct model per environment. Both the general form of the differential equations and the speciﬁc terms of each environment are assumed to be completely unknown. xe t denotes the state of the equation for environment e, taking its values from a bounded set A, with evolution term fe : A →
T A, T A being the tangent bundle of A. In other words, over a ﬁxed time interval [0, T ], we have: dxe t dt
= fe(xe t ) (1) 1Code is available at https://github.com/yuan-yin/LEADS. 2
· verifying Eq. 1, induced by a distribution of initial states xe
We assume that, for any e, fe lies in a functional vector space F. In the experiments, we will consider one ODE, in which case A ⊂ Rd, and two PDEs, in which case A is a d(cid:48)-dimensional vector ﬁeld over a bounded spatial domain S ⊂ Rd(cid:48)
. The term of the data-generating dynamical system in Eq. 1 is sampled from a distribution for each e, i.e. fe ∼ Q. From fe, we deﬁne Te, the data distribution of trajectories xe 0 ∼ P0. The data for this environment is then composed of l trajectories sampled from Te, and is denoted as ˆTe with xe,i the
· i-th trajectory. We will denote the full dataset by ˆT = (cid:83)
The classical empirical risk minimization (ERM) framework suggests to model the data dynamics either at the global level (One-For-All), taking trajectories indiscriminately from ˆT, or at the spe-ciﬁc environment level (One-Per-Env.), training one model for each ˆTe. Our aim is to formulate a new learning framework with the objective of explicitly considering the existence of different environments to improve the modeling strategy w.r.t. the classical ERM settings.
ˆTe. e∈E 2.2 LEADS framework
We decompose the dynamics into two components where f ∈ F is shared across environments and ge ∈ F is speciﬁc to the environment e, so that
∀e ∈ E, fe = f + ge (2)
Since we consider functional vector spaces, this additive hypothesis is not restrictive and such a de-composition always exists. It is also quite natural as a sum of evolution terms can be seen as the sum of the forces acting on the system. Note that the sum of two evolution terms can lead to behaviors very different from those induced by each of those terms. However, learning this decomposition from data deﬁnes an ill-posed problem: for any choice of f , there is a {ge}e∈E such that Eq. 2 is veriﬁed.
A trivial example would be f = 0 leading to a solution where each environment is ﬁtted separately.
Our core idea is that f should capture as much of the shared dynamics as is possible, while ge should focus only on the environment characteristics not captured by f . To formalize this intuition, we introduce Ω(ge), a penalization on ge, which precise deﬁnition will depend on the considered setting.
We reformulate the learning objective as the following constrained optimization problem: min f,{ge}e∈E ∈F (cid:88) e∈E
Ω(ge) subject to ∀xe,i ∈ ˆT, ∀t, dxe,i t dt
= (f + ge)(xe,i t ) (3)
Minimizing Ω aims to reduce ges’ complexity while correctly ﬁtting the dynamics of each environ-ment. This argument will be made formal in the next section. Note that f will be trained on the data from all environments contrary to ges. A key question is then to determine under which conditions the minimum in Eq. 3 is well-deﬁned. The following proposition provides an answer (proof cf. Sup. A):
Proposition 1 (Existence and Uniqueness). Assume Ω is convex, then the existence of a minimal e }e∈E ∈ F of Eq. 3 is guaranteed. Furthermore, if Ω is strictly convex, this decomposition f (cid:63), {g(cid:63) decomposition is unique.
In practice, we consider the following relaxed formulation of Eq. 3: (cid:18) 1
λ (cid:13) 2 (cid:13)
− (f + ge)(xe,i
τ ) (cid:13) (cid:13) min f,{ge}e∈E ∈F dxe,i t dt
Ω(ge) + l (cid:88) (cid:13) (cid:13) (cid:13) (cid:13) (cid:90) T (cid:88) (4) dt (cid:19) e∈E i=1 0 where f, ge are taken from a hypothesis space ˆF approximating F. λ is a regularization weight and the integral term constrains the learned f + ge to follow the observed dynamics. The form of this objective and its effective calculation will be detailed in Sec. 4.4. 3
Improving generalization with LEADS
Deﬁning an appropriate Ω is crucial for our method. In this section, we show that the generalization error should decrease with the number of environments. While the bounds might be too loose for
NNs, our analysis is shown to adequately model the decreasing trend in the linear case, linking both our intuition and our theoretical analysis with empirical evidence. This then allows us to construct an appropriate Ω for NNs. 3.1 General case
After introducing preliminary notations and deﬁnitions, we deﬁne the hypothesis spaces associated with our multiple environment framework. Considering a ﬁrst setting where all environments of interest are present at training time, we prove an upper-bound of their effective size based on the 3
covering numbers of the approximation spaces. This allows us to quantitatively control the sample complexity of our model, depending on the number of environments m and other quantities that can be considered and optimized in practice. We then consider an extension for learning on a new and unseen environment. The bounds here are inspired by ideas initially introduced in [4]. They consider multi-task classiﬁcation in vector spaces, where the task speciﬁc classiﬁers share a common feature extractor. Our extension considers sequences corresponding to dynamical trajectories, and a model with additive components instead of function composition in their case.
τ ∈ A as input, and the corresponding values of derivatives y = fe(xe
Deﬁnitions. Sample complexity theory is usually deﬁned for supervised contexts, where for a given input x we want to predict some target y. In our setting, we want to learn trajectories (xe t )0≤t≤T starting from an initial condition x0. We reformulate this problem and cast it as a standard supervised learning problem: Te being the data distribution of trajectories for environment e, as deﬁned in
· ∼ Te, and time τ ∼ Unif([0, T ]); we deﬁne system states
Sec. 2.1, let us consider a trajectory xe
τ ) ∈ T A as the associated x = xe target. We will denote Pe the underlying distribution of (x, y), and ˆPe the associated dataset of size n.
We are searching for f, ge : A → T A in an approximation function space ˆF of the original space F. Let us deﬁne ˆG ⊆ ˆF the effective function space from which the ges are sampled. Let f + ˆG := {f + g : g ∈ ˆG} be the hypothesis space generated by function pairs (f, g), with a ﬁxed f ∈ ˆF. For any h :
A → T A, the error on some test distribution Pe is given by erPe (h) = (cid:82)
A×T A(cid:107)h(x) − y(cid:107)2dPe(x, y) and the error on the training set by ˆer ˆPe (cid:107)h(x) − y(cid:107)2. (h) = 1 n (x,y)∈ ˆPe (cid:80)
LEADS sample complexity. Let CˆG(ε, ˆF) and C ˆF(ε, ˆG) denote the capacity of ˆF and ˆG at a certain scale ε > 0. Such capacity describes the approximation ability of the space. The capacity of a class of functions is deﬁned based on covering numbers, and the precise deﬁnition is provided in
Sup. B.2, Table S1. The following result is general and applies for any decomposition of the form f + ge. It states that to guarantee a given average test error, the minimal number of samples required is a function of both capacities and the number of environments m, and it provides a step towards
RQ1 (proof see Sup. B.2):
Proposition 2. Given m environments, let ε1, ε2, δ > 0, ε = ε1 + ε2. Assume the number of examples n per environment satisﬁes (cid:26) 64
ε2
+ log C ˆF
+ log CˆG (cid:18) 1 m (cid:16) ε2 16 (cid:16) ε1 16 n ≥ max (cid:17)(cid:19)
, 16
ε2
, ˆF
, ˆG log (cid:17)(cid:17) 4
δ (5) (cid:27) (cid:16) (cid:80) (cid:80) e∈E ˆer ˆPe (f + ge) + ε. e∈E erPe (f + ge) ≤ 1 m
Then with probability at least 1−δ (over the choice of training sets {ˆPe}), any learner (f +g1, . . . , f + gm) will satisfy 1 m
The contribution of ˆF to the sample complexity decreases as m increases, while that of ˆG remains the same: this is due to the fact that shared functions f have access to the data from all environments, which is not the case for ge. From this ﬁnding, one infers the basis of LEADS: when learning from several environments, to control the generalization error through the decomposition fe = f + ge, f should account for most of the complexity of fe while the complexity of ge should be controlled and minimized. We then establish an explicit link to our learning problem formulation in Eq. 3. Further in this section, we will show for linear ODEs that the optimization of Ω(ge) in Eq. 4 controls the capacity of the effective set ˆG by selecting ges that are as “simple” as possible.
As a corollary, we show that for a ﬁxed total number of samples in ˆT, the sample complexity will decrease as the number of environments increases. To see this, suppose that we have two situations corresponding to data generated respectively from m and m/b environments. The total sample 16 , ˆG)) and complexity for each case will be respectively bounded by O(log CˆG( ε1 16 , ˆG)). The latter being larger than the former, a situation with more
O(b log CˆG( ε1 environments presents a clear advantage. Fig. 4 in Sec. 4 conﬁrms this result with empirical evidence. 16 , ˆF) + m log C ˆF( ε2 16 , ˆF) + m log C ˆF( ε2
LEADS sample complexity for novel environments. Suppose that problem Eq. 3 has been solved for a set of environments E, can we use the learned model for a new environment not present in the initial training set (RQ2)? Let e(cid:48) be such a new environment, Pe(cid:48) the trajectory distribution of e(cid:48), generated from dynamics fe(cid:48) ∼ Q, and ˆPe(cid:48) an associated training set of size n(cid:48). The following results show that the number of required examples for reaching a given performance is much lower when training f + ge(cid:48) with f ﬁxed on this new environment than training another f (cid:48) + ge(cid:48) from scratch (proof see Sup. B.2). 4
Proposition 3. For all ε, δ with 0 < ε, δ < 1 if the number of samples n(cid:48) satisﬁes (cid:26) 64 4C( ε (cid:27) 16 , f + ˆG) n(cid:48) ≥ max
ε2 log
δ
, 16
ε2
, (6) then with probability at least 1 − δ (over the choice of novel training set ˆPe(cid:48)), any learner f + ge(cid:48) ∈ f + ˆG will satisfy erP (f + ge(cid:48)) + ε.
In Prop. 3 as the capacity of ˆF no longer appears, the number of required samples now depends only on the capacity of f + ˆG. This sample complexity is then smaller than learning from scratch fe(cid:48) = f + ge(cid:48) as can be seen by comparing with Prop. 2 at m = 1. e(cid:48) (f + ge(cid:48)) ≤ ˆer ˆP e(cid:48)
From the previous propositions, it is clear that the environment-speciﬁc functions ge need to be explicitly controlled. We now introduce a practical way to do that. Let ω(r, ε) be a strictly increasing function w.r.t. r such that log C ˆF(ε, ˆG) ≤ ω(r, ε), (7)
Minimizing Ω would reduce r and then the sample complexity of our model by constraining ˆG. Our goal is thus to construct such a pair (ω, Ω). In the following, we will ﬁrst show in Sec. 3.2, how one can construct a penalization term Ω based on the covering number bound for linear approximators and linear ODEs. We show with a simple use case that the generalization error obtained in practice follows the same trend as the theoretical error bound when the number of environments varies. Inspired by this result, we then propose in Sec. 3.3 an effective Ω to penalize the complexity of the neural networks ge. r = supg∈ˆG Ω(g) 3.2 Linear case: theoretical bounds correctly predict the trend of test error
Results in Sec. 3.1 provide general guidelines for our approach. We now apply them to a linear system to see how the empirical results meet the tendency predicted by theoretical bound.
Let us consider a linear ODE dxe t ) where LFe : x (cid:55)→ Fex is a linear transformation associated to the square real valued matrix Fe ∈ Md,d(R). We choose as hypothesis space the space of linear functions ˆF ⊂ L(Rd, Rd) and instantiate a linear LEADS dxe t ), LF ∈ ˆF, LGe ∈
ˆG ⊆ ˆF. As suggested in [3], we have that (proof in Sup. B.3):
Proposition 4. If for all linear maps LGe ∈ ˆG, (cid:107)G(cid:107)2 and the MSE loss function is bounded by c, then
F ≤ r, if the input space is bounded s.t. (cid:107)x(cid:107)2≤ b, dt = (LF + LGe )(xe dt = LFe (xe t t log C ˆF(ε, ˆG) ≤ (cid:100)rcd(2b)2/ε2(cid:101) log 2d2 =: ω(r, ε)
ω(r, ε) is a strictly increasing function w.r.t. r. This indicates that we can choose Ω(LG) = (cid:107)G(cid:107)F as our optimization objective in Eq. 3. The sample complexity in Eq. 5 will decrease with the size the largest possible r = supLG∈ˆG Ω(LG). The optimization process will reduce Ω(LG) until a minimum is reached. The maximum size of the effective hypothesis space is then bounded and decreases throughout training thanks to the penalty. Then in linear case Prop. 2 becomes (proof cf. Sup. B.3):
Proposition 5. If for linear maps LF ∈ ˆF, (cid:107)F (cid:107)2
F ≤ r, (cid:107)x(cid:107)2≤ b, and if the MSE loss function is bounded by c, given m environments and n samples per environment, with the probability 1 − δ, the generalization error upper bound is ε = max {(cid:112)(p + (cid:112)p2 + 4q)/2, (cid:112)16/n} where p = 64 (1−z)2 )cd(32b)2(cid:101) log 2d2 for any 0 < z < 1.
F ≤ r(cid:48), LG ∈ ˆG, (cid:107)G(cid:107)2
δ and q = 64 mz2 + r mn log 4 n (cid:100)( r(cid:48) (cid:80)
In Fig. 1, we take an instance of linear ODE deﬁned by Fe = QΛeQ(cid:62) with the diag-onal Λe speciﬁc to each environment Af-ter solving Eq. 3 we have at the optimum that Ge = Fe − F (cid:63) = Fe − 1 e(cid:48)∈E Fe(cid:48). m
Then we can take r = max{LGe } Ω(LGe) as the norm bound of ˆG when Ω(ge) is op-timized. Fig. 1 shows on the left the test error with and without penalty and the cor-responding theoretical bound on the right.
We observe that, after applying the penalty
Ω, the test error is reduced as well as the theoretical generalization bound, as indi-cated by the arrows from the dashed line to the concrete one. See Sup. B.3 for more de-tails on this experiment.
Figure 1: Test error compared with corresponding the-oretical bound. The arrows indicate the changes after applying Ω(ge) penalty. 5
3.3 Nonlinear case: instantiation for neural nets
The above linear case validates the ideas introduced in Prop. 2 and provides an instantiation guide and an intuition on the more complex nonlinear case. This motivates us to instantiate the general case by choosing an appropriate approximating space ˆF and a penalization function Ω from the generalization bounds for the corresponding space. Sup. B.4 of the Appendix contains additional details justifying those choices. For ˆF, we select the space of feed-forward neural networks with a ﬁxed architecture.
We choose the following penalty function:
Ω(ge) = (cid:107)ge(cid:107)2
∞+α(cid:107)ge(cid:107)2 (8) where (cid:107)g(cid:107)∞= ess sup|g| and (cid:107)·(cid:107)Lip is the Lipschitz semi-norm, α is a hyperparameter. This is inspired by the existing capacity bound for NNs [14] (see Sup. B.4 for details). Note that constructing tight generalization bounds for neural networks is still an open research problem [26]; however, it may still yield valuable intuitions and guide algorithm design. This heuristic is tested successfully on three different datasets with different architectures in the experiments (Sec. 4).
Lip 4 Experiments
Our experiments are conducted on three families of dynamical systems described by three broad classes of differential equations. All exhibit complex and nonlinear dynamics. The ﬁrst one is an ODE-driven system used for biological system modeling. The second one is a PDE-driven reaction-diffusion model, well-known in chemistry for its variety of spatiotemporal patterns. The third one is the more physically complex Navier-Stokes equation, expressing the physical laws of incompressible Newtonian ﬂuids. To show the general validity of our framework, we will use 3 different NN architectures (MLP, ConvNet, and Fourier Neural Operator [19]). Each architecture is well-adapted to the corresponding dynamics. This also shows that the framework is valid for a variety of approximating functions. 4.1 Dynamics, environments, and datasets
Lotka-Volterra (LV). This classical model [22] is used for describing the dynamics of interaction between a predator and a prey. The dynamics follow the ODE: du/dt = αu − βuv, dv/dt = δuv − γv with u, v the number of prey and predator, α, β, γ, δ > 0 deﬁning how the two species interact. The system state is xe
+. The initial conditions ui 0 are sampled from a uniform distribution P0. We characterize the dynamics by θ = (α/β, γ/δ) ∈ Θ. An environment e is then deﬁned by parameters θe sampled from a uniform distribution over a parameter set Θ. We then sample two sets of environment parameters: one used as training environments for RQ1, the other treated as novel environments. for RQ2. t ) ∈ R2 t = (ue t , ve 0, vi
Gray-Scott (GS). This reaction-diffusion model is famous for its complex spatiotemporal behavior given its simple equation formulation [29]. The governing PDE is:
∂u/∂t = Du∆u − uv2 + F (1 − u), ∂v/∂t = Dv∆v + uv2 − (F + k)v where the u, v represent the concentrations of two chemical components in the spatial domain S t ) ∈ R2×322 with periodic boundary conditions, the spatially discretized state at time t is xe
.
Du, Dv denote the diffusion coefﬁcients respectively for u, v, and are held constant, and F, k are the reaction parameters determining the spatio-temporal patterns of the dynamics [29]. As for the initial conditions (u0, v0) ∼ P0, we consider uniform concentrations, with 3 2-by-2 squares ﬁxed at other concentration values and positioned at uniformly sampled positions in S to trigger the reactions. An environment e is deﬁned by its parameters θe = (Fe, ke) ∈ Θ. We consider a set of θe parameters uniformly sampled from the environment distribution Q on Θ. t = (ue t , ve
+
Navier-Stokes (NS). We consider the Navier-Stokes PDE for incompressible ﬂows:
∂w/∂t = −v · ∇w + ν∆w + ξ
∇ · v = 0 where v is the velocity ﬁeld, w = ∇ × v is the vorticity, both v, w lie in a spatial domain S with periodic boundary conditions, ν is the viscosity and ξ is the constant forcing term in the domain S.
The discretized state at time t is the vorticity xe t = we
. Note that v is already contained in w.
We ﬁx ν = 10−3 across the environments. We sample the initial conditions we 0 ∼ P0 as in [19]. An environment e is deﬁned by its forcing term ξe ∈ Θξ. We uniformly sampled a set of forcing terms from Q on Θξ. t ∈ R322 6
One-Per-Env.
FT-NODE
LEADS
Ground truth
One-Per-Env.
FT-NODE
LEADS
GS
NS
GS
NS
GS
NS
GS
NS
GS
NS
GS
NS
GS
NS
Figure 2: Left: ﬁnal states for GS and NS predicted by the two best baselines (One-Per-Env. and
FT-NODE) and LEADS compared with ground truth. Different environment are arranged by row (3 in total). Right: the corresponding MAE error maps, the scale of the error map is [0, 0.6] for GS, and
[0, 0.2] for NS; darker is smaller. (See Sup. D for full sequences)
Figure 3: Test predicted trajectories in phase space with two baselines (One-Per-Env. and FT-NODE) and LEADS compared with ground truth for LV for 4 envs., one per ﬁgure from left to right. Quantity of the prey u and the predator v respectively on the horizontal and the vertical axis. Initial state is the rightmost end-point of the ﬁgures and it is common to all the trajectories.
Datasets. For training, we create two datasets for LV by simulating trajectories of K = 20 successive points with temporal resolution ∆t = 0.5. We use the ﬁrst one as a set of training dynamics to validate the LEADS framework. We choose 10 environments and simulate 8 trajectories (thus corresponding to n = 8·K data points) per environment for training. We can then easily control the number of data points and environments in experiments by taking different subsets. The second one is used to validate the improvement with LEADS while training on novel environments. We simulate 1 trajectory (n = 1·K data points) for training. We create two datasets for further validation of LEADS with GS and NS. For GS, we simulate trajectories of K = 10 steps with ∆t = 40. We choose 3 parameters and simulate 1 trajectory (n = 1·K data points) for training. For NS, we simulate trajectories of K = 10 steps with ∆t = 1. We choose 4 forcing terms and simulate 8 trajectories (n = 8·K states) for training.
For test-time evaluation, we create for each equation in each environment a test set of 32 trajectories (32·K) data points. Note that every environment dataset has the same number of trajectories and the initial conditions are ﬁxed to equal values across the environments to ensure that the data variations only come from the dynamics themselves, i.e. for the i-th trajectory in ˆPe, ∀e, xe,i 0. LV and
GS data are simulated with the DOPRI5 solver in NumPy [10, 13]. NS data is simulated with the pseudo-spectral method as in [19]. 0 = xi 4.2 Experimental settings and baselines
We validate LEADS in two settings: in the ﬁrst one all the environments in E are available at once and then f and all the ges are all trained on E. In the second one, training has been performed on
E as before, and we consider a novel environment e(cid:48) (cid:54)∈ E: the shared term f being kept ﬁxed, the approximating function fe(cid:48) = f + ge(cid:48) is trained on the data from e(cid:48) (i.e. only ge(cid:48) is modiﬁed).
All environments available at once. We introduce ﬁve baselines used for comparing with LEADS: (a) One-For-All: learning on the entire dataset ˆP over all environments with the sum of a pair of
NNs f + g, with the standard ERM principle, as in [2]. Although this is equivalent to use only one function f , we use this formulation to indicate that the number of parameters is the same for this experiment and for the LEADS ones. (b) One-Per-Env.: learning a speciﬁc function for each dataset
ˆPe. For the same reason as above, we keep the sum formulation (f + g)e. (c) Factored Tensor RNN or FT-RNN [33]: it modiﬁes the recurrent neural network to integrate a one-hot environment code into each linear transformation of the network. Instead of being encoded in a separate function ge like in LEADS, the environment appears here as an extra one-hot input for the RNN linear transformations.
This can be implemented for representative SOTA (spatio-)temporal predictors such as GRU [8] or
PredRNN [35]. (d) FT-NODE: a baseline for which the same environment encoding as FT-RNN is incorporated in a Neural ODE [7]. (e) Gradient-based Meta Learning or GBML-like method: we propose a GBML-like baseline which can directly compare to our framework. It follows the 7
Table 1: Results for LV, GS, and NS datasets, trained on m envs. with n data points per env.
NS (m = 4, n = 8 · K)
LV (m = 10, n = 1 · K)
GS (m = 3, n = 1 · K)
Method
MSE train
MSE test
MSE train
MSE test
MSE train
MSE test
One-For-All
One-Per-Env.
FT-RNN [33]
FT-NODE
GBML-like
LEADS no min.
LEADS (Ours) 4.57e-1 2.15e-5 5.29e-5 7.74e-5 3.84e-6 3.28e-6 5.74e-6 5.08±0.56 e-1 7.95±6.96 e-3 6.40±5.69 e-3 3.40±2.64 e-3 5.87±5.65 e-3 3.07±2.58 e-3 1.16±0.99 e-3 1.55e-2 8.48e-5 8.44e-6 3.51e-5 1.07e-4 7.65e-5 5.75e-5 1.43±0.15 e-2 6.43±3.42 e-3 8.19±3.09 e-3 3.86±3.36 e-3 6.01±3.62 e-3 5.53±3.43 e-3 2.08±2.88 e-3 5.17e-2 5.60e-6 7.40e-4 1.80e-4 1.39e-4 3.20e-4 1.03e-4 7.31±5.29 e-2 1.10±0.72 e-2 5.92±4.00 e-2 2.96±1.99 e-2 7.37±4.80 e-3 7.10±4.24 e-3 5.95±3.65 e-3 principle of MAML [11], by training One-For-All at ﬁrst which provides an initialization near to the given environments like GBML does, then ﬁtting it individually for each training environment. (f) LEADS no min.: ablation baseline, our proposal without the Ω(ge) penalization. A comparison with the different baselines is proposed in Table 1 for the three dynamics. For concision, we provide a selection of results corresponding to 1 training trajectory per environment for LV and GS and 8 for
NS. This is the minimal training set size for each dataset. Further experimental results when varying the number of environments from 1 to 8 are provided in Fig. 4 and Table S3 for LV.
Learning on novel environments. We consider the following training schemes with a pre-trained,
ﬁxed f : (a) Pre-trained-f -Only: only the pre-trained f is used for prediction; a sanity check to ensure that f cannot predict in any novel environment without further adaptation. (b) One-Per-Env.: training from scratch on {ˆPe(cid:48)} as One-Per-Env. in the previous section. (c) Pre-trained-f -Plus-Trained-ge: we train g on each dataset ˆPe(cid:48) based on pre-trained f , i.e. f + ge(cid:48), leaving only ge(cid:48)s adjustable. We compare the test error evolution during training for 3 schemes above for a comparison of convergence speed and performance. Results are given in Fig. 5. 4.3 Experimental results
All environments available at once. We show the results in Table 1. For LV systems, we conﬁrm
ﬁrst that the entire dataset cannot be learned properly with a single model (One-For-All) when the number of environments increases. Comparing with other baselines, our method LEADS reduces the test MSE over 85% w.r.t. One-Per-Env. and over 60% w.r.t. LEADS no min., we also cut 50%-75% of error w.r.t. other baselines. Fig. 3 shows samples of predicted trajectories in test, LEADS follows very closely the ground truth trajectory, while One-Per-Env. under-performs in most environments. We observe the same tendency for the GS and NS systems. The error is reduced by: around 2/3 (GS) and 45% (NS) w.r.t. One-Per-Env.; over 60% (GS) and 15% (NS) w.r.t. LEADS no min.; 45-75% (GS) and 15-90% (NS) w.r.t. other baselines. In Fig. 2, the ﬁnal states obtained with LEADS are qualitatively closer to the ground truth. Looking at the error maps on the right, we see that the errors are systematically reduced across all environments compared to the baselines. This shows that LEADS accumulates less errors through the integration, which suggests that LEADS alleviates overﬁtting.
We have also conducted a larger scale ex-periment on LV (Fig. 4) to analyze the be-havior of the different training approaches as the number of environments increases.
We consider three models One-For-All,
One-Per-Env. and LEADS, 1, 2, 4 and 8 environments, and for each such case, we have 4 groups of curves, corresponding to 1, 2, 4 and 8 training trajectories per envi-ronment. We summarize the main observa-tions. With One-For-All (blue), the error increases as the number of environments increases: the dynamics for each environ-ment being indeed different, this introduces an increasingly large bias, and thus the data cannot be ﬁt with one single model. The performance of One-Per-Env. (in red), for which models are trained independently for each environment, is constant as expected when the number of environments changes.
Figure 4: Test error for LV w.r.t. the number of environ-ments. We apply the models in 1 to 8 environments. 4 groups of curves correspond to models trained with 1 to 8 trajectories per env. All groups highlight the same ten-dencies: increasing One-For-All, stable One-Per-Env., and decreasing LEADS. More results of baselines meth-ods in Sup. D. 8
LEADS (green) circumvents these issues and shows that the shared characteristics among the environ-ments can be leveraged so as to improve generalization: it is particularly effective when the number of samples per environment is small. (See Sup. D for more details on the experiments and on the results).
Learning on novel environments. We demonstrate how the pre-trained dynamics can help to ﬁt a model for novel environ-ments. We took an f pre-trained by LEADS on a set of LV environments. Fig. 5 shows the evolution of the test loss during training for three systems: a f function pre-trained by LEADS on a set of LV training environ-ments, a ge function trained from scratch on the new environment and LEADS that uses a pre-trained f and learns a ge residue on this new environment. Pre-trained-f -Only alone cannot predict in any novel environments. Very fast in the training stages, Pre-trained-f -Plus-Trained-ge already surpasses the best error of the model trained from scratch (indicated with dotted line). Similar results are also observed with the GS and NS datasets (cf. Sup. D, Table S5). These empirical results clearly show that the learned shared dynamics accelerates and improves the learning in novel environments.
Figure 5: Test error evolution during training on 2 novel environments for LV. 4.4 Training and implementation details
∆t
Discussion on trajectory-based optimization. Solving the learning problem Eq. 2 in our setting, involves computing a trajectory loss (integral term in Eq. 4). However, in practice, we do not have access to the continuous trajectories at every instant t but only to a ﬁnite number of snapshots at a temporal resolution ∆t. From these discrete observed for the state values {xk∆t}0≤k≤ T trajectories, it is still possible to recover an approximate derivative dΛ using a numerical scheme Λ. The integral term for a given sample in the objective Eq. 4 would then be estimated as (cid:80)K
. This is not the best solution and we have observed much better prediction performance for all models, including the baselines, when computing the error directly on the states, using an integral formulation (cid:80)K k=1(cid:107)x(k+1)∆t − ˜x(k+1)∆t(cid:107)2, where ˜x(k+1)∆t is the solution given by a numerical solver approximating the integral xk∆t + (cid:82) (k+1)∆t (f + ge)(˜xs)ds starting from xk∆t. Comparing directly in the state space yields more accurate results for prediction as the learned network tends to correct the solver’s numerical errors, as ﬁrst highlighted in [37]. k∆t − (f + ge)(x∆tk)(cid:13) 2 (cid:13) k∆t (cid:39) dxk∆t dt (cid:13) (cid:13)dΛ k=1 k∆t
Calculating Ω. Given ﬁnite data and time, the exact inﬁnity norm and Lipschitz norm are both intractable. We opt for more practical forms in the experiments. For the inﬁnity norm, we chose to minimize the empirical norm of the output vectors on known data points, this choice is motivated in Sup. C. In practice, we found out that dividing the output norm by its input norm works better: 1 k∆t are known states in the training set. For the Lipschitz n norm, as suggested in [5], we optimize the sum of the spectral norms of the weight at each layer (cid:80)D l (cid:107)2. We use the power iteration method in [25] for fast spectral norm approximation. k∆t(cid:107)2, where the xe,i k∆t)(cid:107)2/(cid:107)xe,i i,k(cid:107)ge(xe,i l=1(cid:107)W ge (cid:80)
Implementation. We used 4-layer MLPs for LV, 4-layer ConvNets for GS and Fourier Neural
Operator (FNO) [19] for NS. For FT-RNN baseline, we adapted GRU [8] for LV and PredRNN
[35] for GS and NS. We apply the Swish function [31] as the default activation function. Networks are integrated in time with RK4 (LV, GS) or Euler (NS), using the basic back-propagation through the internals of the solver. We apply an exponential Scheduled Sampling [17] with exponent of 0.99 to stabilize the training. We use the Adam optimizer [15] with the same learning rate 10−3 and (β1, β2) = (0.9, 0.999) across the experiments. For the hyperparamters in Eq. 8, we chose respectively λ = 5 × 103, 102, 105 and α = 10−3, 10−2, 10−5 for LV, GS and NS. All experiments are performed with a single NVIDIA Titan Xp GPU. 5