Abstract
Explanation techniques that synthesize small, interpretable changes to a given im-age while producing desired changes in the model prediction have become popular for introspecting black-box models. Commonly referred to as counterfactuals, the synthesized explanations are required to contain discernible changes (for easy interpretability) while also being realistic (consistency to the data manifold). In this paper, we focus on the case where we have access only to the trained deep classiﬁer and not the actual training data. While the problem of inverting deep models to synthesize images from the training distribution has been explored, our goal is to develop a deep inversion approach to generate counterfactual explanations for a given query image. Despite their effectiveness in conditional image synthe-sis, we show that existing deep inversion methods are insufﬁcient for producing meaningful counterfactuals. We propose DISC (Deep Inversion for Synthesizing
Counterfactuals) that improves upon deep inversion by utilizing (a) stronger image priors, (b) incorporating a novel manifold consistency objective and (c) adopting a progressive optimization strategy. We ﬁnd that, in addition to producing visually meaningful explanations, the counterfactuals from DISC are effective at learning classiﬁer decision boundaries and are robust to unknown test-time corruptions. 1

Introduction
With the growing need for deploying deep black-box models into critical decision-making, there is an increased emphasis on explainability methods that can reveal intricate relationships between data signatures (e.g., image features) and predictions. In this context, the so-called counterfactual (CF) explanations [1] that synthesize small, interpretable changes to a given image while producing desired changes in model predictions to support user-speciﬁed hypotheses (e.g., progressive change in predictions) have become popular. Though counterfactual explanations provide more ﬂexibility over conventional techniques, such as feature importance estimation [2, 3, 4, 5, 6], by exploring the vicinity of a query image, an important requirement to produce meaningful counterfactuals is to produce discernible local perturbations (for easy interpretability) while being realistic (close to the underlying data manifold). Consequently, existing approaches rely extensively on pre-trained generative models to synthesize plausible counterfactuals [1, 7, 8, 9, 10]. By design, this ultimately restricts their utility to scenarios where one cannot access the training data or pre-trained generative models, for example, due to privacy requirements commonly encountered in many practical applications.
In this paper, we focus on the problem where we have access only to trained deep classiﬁers and not the actual training data or generative models. Synthesizing images from the underlying data distribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
by inverting a deep model, while not requiring access to training data, is a well investigated topic of research. For e.g., Deep Dream [11] synthesizes class-conditioned images by manipulating a noisy image directly in the space of pixels (or more formally Image Space Optimization (ISO)) constrained by image priors such as total variation [12] to regularize this ill-posed inversion. However, Deep
Dream is known to produce images that look unrealistic, often very different from the training images, thus limiting their use in practice. Consequently, Yin et al., proposed DeepInversion [13] that performs image synthesis in the latent space of a pre-trained classiﬁer (Latent Space Optimization (LSO)) and leverages layer-speciﬁc statistics (from batchnorm [14]) to constrain the images to be consistent with the training data distribution. This was showed to produce higher-quality images, particularly in the context of performing knowledge distillation [15] using the synthesized images.
Proposed Work. In contrast, this work aims to develop a deep model inversion approach that generates counterfactual explanations by exploring the vicinity of a given query image, instead of synthesizing an arbitrary realization from the entire image distribution. As illustrated in the example in Figure 1, existing deep in-version methods are ineffective when natively adopted for counterfactual generation. Due to use of weak priors, and the severely ill-posed nature of the problem, it introduces irrelevant pixel manipulations that easily satisfy the de-sired change in prediction. Hence, we propose
DISC (Deep Inversion for Synthesizing Counter-factuals) that improves upon conventional deep model inversion by utilizing: (i) stronger image priors through the use of deep image priors [16] (DIP) and implicit neural representations [17] (INR); (ii) a novel manifold consistency objective that ensures the counterfactual remains close to the underlying manifold; and (iii) a progressive optimization strategy to effectively introduce discernible, yet meaningful, changes to the query image.
Figure 1: We propose DISC, a deep model inver-sion approach for query-based CF generation. Us-ing a strong image prior (INR in this example) and our manifold consistency constraint, along with a progressive optimization strategy, DISC introduces discernible yet semantically meaningful changes (rightmost) to the query image.
From Figure 1, we ﬁnd that our approach produces meaningful image manipulations, in order to change the prediction to the smiling class, while other deep inversion strategies cannot. Using empirical studies, we show that DISC consistently produces visually meaningful explanations, and that the counterfactuals from DISC are effective at learning model decision boundaries and are robust to unknown test-time corruptions.
Our Contributions. 1. A general framework to produce counterfactuals on-the-ﬂy using deep model inversion; 2. Novel objectives to ensure consistency to the data manifold. We explore two different strate-gies based on direct error prediction [18, 19] and deterministic uncertainty estimation [20]; 3. A progressive optimization strategy to introduce discernible changes to a given query image, while satisfying the manifold consistency requirement; 4. A classiﬁer discrepancy metric to evaluate the quality of counterfactuals; 5. Empirical studies using natural image and medical image classiﬁers to demonstrate the effectiveness of DISC over a variety of baselines and ablations. 2