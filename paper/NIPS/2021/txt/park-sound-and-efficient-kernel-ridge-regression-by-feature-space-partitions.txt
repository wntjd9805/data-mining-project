Abstract
We introduce ParK, a new large-scale solver for kernel ridge regression. Our approach combines partitioning with random projections and iterative optimization to reduce space and time complexity while provably maintaining the same statistical accuracy.
In particular, constructing suitable partitions directly in the feature space rather than in the input space, we promote orthogonality between the local estimators, thus ensuring that key quantities such as local effective dimension and bias remain under control. We characterize the statistical-computational tradeoff of our model, and demonstrate the effectiveness of our method by numerical experiments on large-scale datasets. 1

Introduction
The development of provably accurate and efﬁcient algorithms for learning is key to tackle modern large-scale applications. Kernel methods [31, 32] provide a natural ground to develop this research direction. On the one hand they have sound statistical guarantees [7, 32, 33], but on the other hand their basic implementations are limited to sample size of only a few tens of thousands of points [32,
Chapter 11]. Recent years have witnessed a growing literature introducing algorithmic solutions to improve efﬁciency, but also theoretical guarantees that quantify how accuracy is affected.
We next recall a few lines of work relevant to our study. A ﬁrst line of work is based on exploiting ideas from optimization and numerical analysis. This includes for example gradient methods [37], as well as their accelerated [3], stochastic [10], preconditioned [13] and distributed [27] variants. A second line of work is based on using randomized approaches to reduce the size of the problem to be solved. This includes Nyström approximations [36], random features [25] and more generally sketching methods [1]. The theoretical properties of these methods have been recently characterized in terms of sharp statistical bounds [28, 30]. Finally, a third line of work considers different partitioning strategies to divide the estimation step in smaller subproblems. This approach is based on splitting the input space in regions where local estimators are deﬁned [23, 35, 34, 11, 24, 5]. In this context, the emphasis is typically on allowing the estimation of larger classes of functions. Another form of partitioning, called divide-and-conquer, is instead based on randomly splitting the training data to then obtain a global estimator by averaging [38, 20, 14]. In this approach, the focus is primarily on computational saving. Notably, a number of works have considered combinations of these ideas, see for example [6, 29, 8, 24, 19].
∗equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper we propose and study a local kernel algorithm, called ParK, combining partitioning with iterative optimization and sketching. Our goal is to provide an efﬁcient and accurate approximation to a global kernel ridge regression estimator. The main novelty in ParK is in the form of the considered partition, that we deﬁne in the feature space, rather than in the input space as in traditional partitioning methods. This allows to promote orthogonality between the local estimators, and thus to control the local effective dimension and the local bias. Given a partition, local kernel ridge estimators are computed using sketching and preconditioned conjugate gradient iterations [29]. From a theoretical point of view, our main contribution is characterizing the statistical properties of ParK, in terms of conditions on the partition and the choice of the hyper-parameters. Borrowing ideas from subspace clustering [12], we show that the minimal angle between suitable subspaces induced by the partition plays a crucial role. Indeed, our analysis shows that, if such an angle is sufﬁciently large, ParK can achieve the same accuracy as global kernel ridge regression estimators, with only a fraction of computations. Our theoretical results are complemented with numerical experiments on very large datasets, which show that ParK can indeed provide excellent performances, on par and often better than the best available large-scale kernel methods.
The rest of the paper is organized as follows. In Section 2 we state the problem and recall the basics of kernel ridge regression. In Section 3 we illustrate our algorithm. In Section 4 we analyze the prediction error of our method. In Section 5 we present the results of our numerical experiments. In
Section 6 we draw some conclusions and report the main limitations of our work. Additional proofs and details are collected in Appendix A. 2