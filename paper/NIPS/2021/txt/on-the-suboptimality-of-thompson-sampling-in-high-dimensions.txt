Abstract
In this paper we consider Thompson Sampling (TS) for combinatorial semi-bandits.
We demonstrate that, perhaps surprisingly, TS is sub-optimal for this problem in the sense that its regret scales exponentially in the ambient dimension, and its minimax regret scales almost linearly. This phenomenon occurs under a wide variety of assumptions including both non-linear and linear reward functions, with Bernoulli distributed rewards and uniform priors. We also show that including a ﬁxed amount of forced exploration to TS does not alleviate the problem. We complement our theoretical results with numerical results and show that in practice TS indeed can perform very poorly in some high dimensional situations. 1

Introduction
We consider the problem of combinatorial bandits with semi-bandit feedback. At time t = 1, , ..., T a learner selects a decision x(t) ∈ X where X ⊂ {0, 1}d is the set of available decisions. The environment then draws a random vector Z(t) ∈ Rd. The learner then observes Y (t) = x(t) (cid:12) Z(t), where (cid:12) denotes the Hadamard (elementwise) product. This setting is called semi bandit feedback.
We assume that (Z(t))t≥1 are i.i.d., and that Z1(t), ..., Zd(t) are independent and distributed as
Zi(t) ∼ Bernoulli(θi) for all t,i. Then the learner receives a reward f (x(t), Z(t)) where f is a known function.
The goal is to minimize the regret:
R(T, θ) = T max x∈X (cid:110)
Ef (x, Z(t)) (cid:111)
−
T (cid:88) t=1
Ef (x(t), Z(t)).
Initially θ is unknown to the learner and minimizing regret involves exploring suboptimal decisions just enough in order to identify the optimal decision. For any decision x ∈ X , deﬁne the reward gap
∆x = max x∈X
{Ef (x, Z(t))} − Ef (x, Z(t)), which is the amount of regret incurred by choosing x instead of an optimal decision x(cid:63) ∈ arg max x∈X (cid:110)
Ef (x, Z(t)) (cid:111)
, and ∆min = minx∈X :∆x>0 ∆x the minimal gap. We deﬁne m (cid:44) maxx∈X maximal decision. (cid:80)d i=1 |xi| the size of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
For this problem, an algorithm which has attracted a lot of interest is Thompson Sampling (TS), which at time t selects the decision maximizing x (cid:55)→ f (x, V (t)) where V (t) is a random variable distributed as the posterior distribution of θ knowing the information available at time t, which is
Y (1), ..., Y (t − 1). The prior distribution of θ can be chosen in various ways, the most natural being a non-informative distribution such as the uniform distribution.
TS is usually computationally simple to implement, for instance when f is linear, since it involves maximizing f over X . Also, for some problem instances it tends to perform well numerically. A particular case of interest is linear combinatorial semi-bandits where f (x, θ) = θ(cid:62)x so that the reward is a linear function of the decision.
Our contribution. We show that the regret of TS in general does not scale polynomially in the ambient dimension d. (i) We provide several examples, both for linear and non-linear combinatorial bandits, where the regret of TS does not scale polynomially in the dimension d (in fact in some cases it may scale even faster than exponentially in the dimension). In some cases, we show that one must wait for an amount of time greater than Ω(dd) for TS to perform at least as well as random choice where one simply chooses x(t) uniformly distributed in X at every round. Therefore, in high dimensions, in some instances, TS in general can perform strictly worse than random choice for all practically relevant time horizons. (ii) We show that the minimax regret of TS scales at least as Ω(T 1− 1 d ) so that it is not minimax optimal, as there exists algorithms such as CUCB and ESCB with minimax regret O(poly(d)(cid:112)T (ln T )). In fact, in high dimensions, the minimax regret of TS is almost linear. (iii) We further show that adding forced exploration as an initialization step to TS does not correct the minimax problem, so that this is not an artifact due to initialization. (iv) Using numerical experiments, we show that indeed, for reasonable time horizons, TS performs very poorly in high dimensions in some instances.
We believe that our results highlight two general characteristics of TS. First, TS tends to be much more greedy than optimistic algorithms such as ESCB and CUCB. This greedy behavior explains why the regret of TS is, in some instances, much smaller than that of optimistic algorithms. In fact it is sometimes so greedy that it misses the optimal decision. Second, TS tends to be by nature a
”risky” algorithm so that its regret exhibits very large ﬂuctuations across runs. In some cases it ﬁnds the optimal arm very quickly and with little to no regret, while in other cases it simply misses the optimal decision and performs worse than random choice.