Abstract
We provide a framework for accelerating reinforcement learning (RL) algorithms by heuristics constructed from domain knowledge or ofﬂine data. Tabula rasa
RL algorithms require environment interactions or computation that scales with the horizon of the sequential decision-making task. Using our framework, we show how heuristic-guided RL induces a much shorter-horizon subproblem that provably solves the original task. Our framework can be viewed as a horizon-based regularization for controlling bias and variance in RL under a ﬁnite interaction budget. On the theoretical side, we characterize properties of a good heuristic and its impact on RL acceleration. In particular, we introduce the novel concept of an improvable heuristic, a heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On the empirical side, we instantiate our framework to accelerate several state-of-the-art algorithms in simulated robotic control tasks and procedurally generated games. Our framework complements the rich literature on warm-starting RL with expert demonstrations or exploratory datasets, and introduces a principled method for injecting prior knowledge into RL. 1

Introduction
Many recent empirical successes of reinforcement learning (RL) require solving problems with very long decision-making horizons. OpenAI Five [1] used episodes that were 20000 timesteps on average, while AlphaStar [2] used roughly 5000 timesteps. Long-term credit assignment is a very challenging statistical problem, with the sample complexity growing quadratically (or worse) with the horizon [3].
Long horizons (or, equivalently, large discount factors) also increase RL’s computational burden, leading to slow optimization convergence [4]. This makes RL algorithms require prohibitively large amounts of interactions and compute: even with tuned hyperparameters, AlphaStar needed over 108 samples and OpenAI Five needed over 107 PFLOPS of compute.
A popular approach to mitigate the statistical and computational issues of tabula rasa RL methods is to warm-start or regularize learning with prior knowledge [1, 2, 5–10]. For instance, AlphaStar learned a policy and value function from human demonstrations and regularized the RL agent using imitation learning (IL). AWAC [9] warm-started a policy using batch policy optimization on exploratory datasets. While these approaches have been effective in different domains, none of them explicitly address RL’s complexity dependence on horizon.
In this paper, we propose a complementary regularization technique that relies on heuristic value functions, or heuristics1 for short, to effectively shorten the problem horizon faced by an online RL agent for fast learning. We call this approach Heuristic-Guided Reinforcement Learning (HuRL).
The core idea is simple: given a Markov decision process (MDP) M = (S, A, P, r, γ) and a heuristic h : S → R, we select a mixing coefﬁcient λ ∈ [0, 1] and have the agent solve a new MDP (cid:102)M = (S, A, P, (cid:101)r, (cid:101)γ) with a reshaped reward and a smaller discount (i.e. a shorter horizon): (cid:101)r(s, a) := r(s, a) + (1 − λ)γEs(cid:48)∼P (·|s,a)[h(s(cid:48))] and (cid:101)γ := λγ. (1) 1We borrow this terminology from the planning literature to refer to guesses of V ∗ in an MDP [11]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
HuRL effectively introduces horizon-based regularization that determines whether long-term value information should come from collected experiences or the heuristic. By modulating the effective horizon via λ, we trade off the bias and the complexity of solving the reshaped MDP. HuRL with
λ = 1 recovers the original problem and with λ = 0 creates an easier contextual bandit problem [12].
A heuristic h in HuRL represents a prior guess of the desired long-term return of states, which ideally is the optimal value function V ∗ of the unknown MDP M. When the heuristic h captures the state ordering of V ∗ well, conceptually, it becomes possible to make good long-term decisions by short-horizon planning or even acting greedily. How do we construct a good heuristic? In the planning literature, this is typically achieved by solving a relaxation of the original problem [13– 15]. Alternatively, one can learn it from batch data collected by exploratory behavioral policies (as in ofﬂine RL [16]) or from expert policies (as in IL [17]).2 For some dense reward problems, a zero heuristic can be effective in reducing RL complexity, as exploited by the guidance discount framework [18–23]. In this paper, we view heuristics as a uniﬁed representation of various forms of prior knowledge, such as expert demonstrations, exploratory datasets, and engineered guidance.
Although the use of heuristics to accelerate search has been popular in planning and control algorithms, e.g., A* [24], MCTS [25], and MPC [7, 26–28], its theory is less developed for settings where the
MDP is unknown. The closest work in RL is potential-based reward shaping (PBRS) [29], which reshapes the reward into ¯r(s, a) = r(s, a)+γEs(cid:48)|s,a[h(s(cid:48))]−h(s) while keeping the original discount.
PBRS can use any heuristic to reshape the reward while preserving the ordering of policies. However, giving PBRS rewards to an RL algorithm does not necessarily lead to faster learning, because the base RL algorithm would still seek to explore to resolve long-term credit assignment. HuRL allows common RL algorithms to leverage the short-horizon potential provided by a heuristic to learn faster.
In this work, we provide a theoretical foundation of HuRL to enable adopting heuristics and horizon reduction for accelerating RL, combining advances from the PBRS and the guidance discount literatures. On the theoretical side, we derive a bias-variance decomposition of HuRL’s horizon-based regularization in order to characterize the solution quality as a function of λ and h. Using this insight, we provide sufﬁcient conditions for achieving an effective trade-off, including properties required of a base RL algorithm that solves the reshaped MDP (cid:102)Mλ. Furthermore, we deﬁne the novel concept of an improvable heuristic and prove that good heuristics for HuRL can be constructed from data using existing pessimistic ofﬂine RL algorithms (such as pessimistic value iteration [30, 31]).
The effectiveness of HuRL depends on the heuristic quality, so we design HuRL to employ a sequence of mixing coefﬁcients (i.e. λs) that increases as the agent gathers more data from the environment.
Such a strategy induces a learning curriculum that enables HuRL to remain robust to non-ideal heuristics. HuRL starts off by guiding the agent’s search direction with a heuristic. As the agent becomes more experienced, it gradually removes the guidance and lets the agent directly optimize the true long-term return. We empirically validate HuRL in MuJoCo [32] robotics control problems and Procgen games [33] with various heuristics and base RL algorithms. The experimental results demonstrate the versatility and effectiveness of HuRL in accelerating RL algorithms. 2 Preliminaries 2.1 Notation
We focus on discounted inﬁnite-horizon Markov Decision Processes (MDPs) for ease of exposition.
The technique proposed here can be extended to other MDP settings.3 A discounted inﬁnite-horizon
MDP is denoted as a 5-tuple M = (S, A, P, r, γ), where S is the state space, A is the action space,
P (s(cid:48)|s, a) is the transition dynamics, r(s, a) is the reward function, and γ ∈ [0, 1) is the discount factor. Without loss of generality, we assume r : S × A → [0, 1]. We allow the state and action spaces
S and A to be either discrete or continuous. Let ∆(·) denote the space of probability distributions. A decision-making policy π is a conditional distribution π : S → ∆(A), which can be deterministic.
We deﬁne some shorthand for writing expectations: For a state distribution d ∈ ∆(S) and a function
V : S → R, we deﬁne V (d) := Es∼d[V (s)]; similarly, for a policy π and a function Q : S × A → R, we deﬁne Q(s, π) := Ea∼π(·|s)[Q(s, a)]. Lastly, we deﬁne Es(cid:48)|s,a := Es(cid:48)∼P (·|s,a). 2We consider the RL setting for imitation where we suppose the rewards of expert trajectories are available. 3The results here can be readily applied to ﬁnite-horizon MDPs; for other inﬁnite-horizon MDPs, we need further, e.g., mixing assumptions for limits to exist. 2
Central to solving MDPs are the concepts of value functions and average distributions. For a policy
π, we deﬁne its state value function V π as V π(s) := Eρπ s denotes the trajectory distribution of s0, a0, s1, . . . induced by running π starting from s0 = s. We deﬁne the state-action value function (or the Q-function) as Qπ(s, a) := r(s, a) + γEs(cid:48)|s,a[V π(s(cid:48))]. We denote the optimal policy as π∗ and its state value function as V ∗ := V π∗
. Under the assumption 1 that rewards are in [0, 1], we have V π(s), Qπ(s, a) ∈ [0, 1−γ ] for all π, s ∈ S, and a ∈ A. We denote the initial state distribution of interest as d0 ∈ ∆(S) and the state distribution of policy π at time t as dπ 0 = d0. Given d0, we deﬁne the average state distribution of a policy π as dπ := (1 − γ) (cid:80)∞ t . With a slight abuse of notation, we also write dπ(s, a) := dπ(s)π(a|s). t=0 γtr(st, at)] , where ρπ t , with dπ
[(cid:80)∞ t=0 γtdπ s 2.2 Setup: Reinforcement Learning with Heuristics
We consider RL with prior knowledge expressed in the form of a heuristic value function. The goal is to ﬁnd a policy π that has high return through interactions with an unknown MDP M, i.e., maxπ V π(d0). While the agent here does not fully know M, we suppose that, before interactions start the agent is provided with a heuristic h : S → R which the agent can query throughout learning.
The heuristic h represents a prior guess of the optimal value function V ∗ of M. Common sources of heuristics are domain knowledge as typically employed in planning, and logged data collected by exploratory or by expert behavioral policies. In the latter, a heuristic guess of V ∗ can be computed from the data by ofﬂine RL algorithms. For instance, when we have trajectories of an expert behavioral policy, Monte-Carlo regression estimate of the observed returns may be a good guess of V ∗.
Using heuristics to solve MDP problems has been popular in planning and control, but its usage is rather limited in RL. The closest provable technique in RL is PBRS [29], where the reward is modiﬁed into r(s, a) := r(s, a) + γEs(cid:48)|s,a[h(s(cid:48))] − h(s). It can be shown that this transformation does not introduce bias into the policy ordering, and therefore solving the new MDP M := (S, A, P, r, γ) would yield the same optimal policy π∗ of M.
Conceptually when the heuristic is the optimal value function h = V ∗, the agent should be able to
ﬁnd the optimal policy π∗ of M by acting myopically, as V ∗ already contains all necessary long-term information for good decision making. However, running an RL algorithm with the PBRS reward (i.e. solving M := (S, A, P, r, γ)) does not take advantage of this shortcut. To make learning efﬁcient, we need to also let the base RL algorithm know that acting greedily (i.e., using a smaller discount) with the shaped reward can yield good policies. An intuitive idea is to run the RL algorithm to
π
λ denotes the value function of π in an MDP Mλ := (S, A, P, r, λγ) maximize V
π
λ(d0) for some λ ∈ [0, 1]. However this does not always work. For example, when λ = 0, maxπ V only optimizes for the initial states d0, but obviously the agent is going to encounter other states in
M. We next propose a provably correct version, HuRL, to leverage this short-horizon insight.
π
λ(d0), where V 3 Heuristic-Guided Reinforcement Learning
We propose a general framework, HuRL, for leveraging heuristics to accelerate RL. In contrast to tabula rasa RL algorithms that attempt to directly solve the long-horizon MDP M, HuRL uses a heuristic to guide the agent in solving a sequence of short-horizon MDPs so as to amortize the complexity of long-term credit assignment. In effect, HuRL creates a heuristic-based learning curriculum to help the agent learn faster. 3.1 Algorithm
HuRL takes a reduction-based approach to realize the idea of heuristic guidance. As summarized in
Algorithm 1, HuRL takes a heuristic h : S → R and a base RL algorithm L as input, and outputs an approximately optimal policy for the original MDP M. During training, HuRL iteratively runs the base algorithm L to collect data from the MDP M and then uses the heuristic h to modify the agent’s collected experiences. Namely, in iteration n, the agent interacts with the original MDP M and saves the raw transition tuples4 Dn = {(s, a, r, s(cid:48))} (line 2). HuRL then deﬁnes a reshaped MDP (cid:102)Mn := (S, A, P, (cid:101)rn, (cid:101)γn) (line 3) by changing the rewards and lowering the discount factor: (cid:101)rn(s, a) := r(s, a) + (1 − λn)γEs(cid:48)|s,a[h(s(cid:48))] and (cid:101)γn := λnγ, (2) 4If L learns only with trajectories, we transform each tuple and assemble them to get the modiﬁed trajectory. 3
Algorithm 1 Heuristic-Guided Reinforcement Learning (HuRL)
Require: MDP M = (S, A, P, r, γ), RL algorithm L, heuristic h, mixing coefﬁcients {λn}. 1: for n = 1, . . . , N do 2: Dn ← L.CollectData(M) 3:
Get λn from {λn} and construct (cid:102)Mn = (S, A, P, (cid:101)rn, (cid:101)γn) according to (2) using h and λn
πn ← L.Train(Dn, (cid:102)Mn) 4: 5: end for 6: return πN where λn ∈ [0, 1] is the mixing coefﬁcient. The new discount (cid:101)γn effectively gives (cid:102)Mn a shorter horizon than M’s, while the heuristic h is blended into the new reward in (2) to account for the missing long-term information. We call (cid:101)γn = λnγ in (2) the guidance discount to be consistent with prior literature [20], which can be viewed in terms of our framework as using a zero heuristic.
In the last step (line 4), HuRL calls the base algorithm L to perform updates with respect to the reshaped MDP (cid:102)Mn. This is realized by 1) setting the discount factor used in L to (cid:101)γn, and 2) setting the sampled reward to r + (γ − (cid:101)γn)h(s(cid:48)) for every transition tuple (s, a, r, s(cid:48)) collected from M. We remark that the base algorithm L in line 2 always collects trajectories of lengths proportional to the original discount γ, while internally the optimization is done with a lower discount (cid:101)γn in line 4.
Over the course of training, HuRL repeats the above steps with a sequence of increasing mixing coefﬁcients {λn}. From (2) we see that as the agent interacts with the environment, the effects of the heuristic in MDP reshaping decrease and the effective horizon of the reshaped MDP increases. 3.2 HuRL as Horizon-based Regularization
We can think of HuRL as introducing a horizon-based regularization for RL, where the regularization center is deﬁned by the heuristic and its strength diminishes as the mixing coefﬁcient increases. As the agent collects more experiences, HuRL gradually removes the effects of regularization and the agent eventually optimizes for the original MDP.
HuRL’s regularization is designed to reduce learning variance, similar to the role of regularization in supervised learning. Unlike the typical weight decay imposed on function approximators (such as the agent’s policy or value networks), our proposed regularization leverages the structure of MDPs to regulate the complexity of the MDP the agent faces, which scales with the MDP’s discount factor (or, equivalently, the horizon). When the guidance discount (cid:101)γn is lower than the original discount γ (i.e. λn < 1), the reshaped MDP (cid:102)Mn given by (2) has a shorter horizon and requires fewer samples to solve. However, the reduced complexity comes at the cost of bias, because the agent is now incentivized toward maximizing the performance with respect to the heuristic rather than the original long-term returns of M. In the extreme case of λn = 0, HuRL would solve a zero-horizon contextual bandit problem with contexts (i.e. states) sampled from dπ of M. 3.3 A Toy Example
We illustrate this idea in a chain MDP environment in Fig. 1. The optimal policy π∗ for this MDP’s original γ = 0.9 always picks action →, as shown in Fig. 1b-(1), giving the optimal value V ∗ in
Fig. 1a-(2). Suppose we used a smaller guidance discount (cid:101)γ = 0.5γ to accelerate learning. This is equivalent to HuRL with a zero heuristic h = 0 and λ = 0.5. Solving this reshaped MDP yields a policy (cid:101)π∗ that acts very myopically in the original MDP, as shown in Fig. 1b-(2); the value function of (cid:101)π∗ in the original MDP is visualized in Fig. 1a-(4).
Now, suppose we use Fig. 1a-(4) as a heuristic in HuRL instead of h = 0. This is a bad choice of heuristic (Bad h) as it introduces a large bias with respect to V ∗ (cf. Fig. 1a-(2)). On the other hand, we can roll out a random policy in the original MDP and use its value function as the heuristic (Good h), shown in Fig. 1a-(3). Though the random policy has an even lower return at the initial state s = 3, it gives a better heuristic because this heuristic shares the same trend as V ∗ in Fig. 1a-(1). HuRL run with Good h and Bad h yields policies in Fig. 1b-(3,4), and the quality of the resulting solutions in the original MDP, V (cid:101)π∗
λ (d0), is reported in Fig. 1c for different λ. Observe that HuRL with a good heuristic can achieve V ∗(d0) with a much smaller horizon λ ≤ 0.5. Using a bad h does not lead to
π∗ at all when λ = 0.5 (Fig. 1b-(4)) but is guaranteed to do so when λ converges to 1. (Fig. 1b-(5)). 4
(a) Heatmap of different values. (b) Different policy behaviors. (c) HuRL with different h and λ.
Figure 1: Example of HuRL in a chain MDP. Each cell in a row in each diagram represents a state from
S = {1, . . . , 10}. The agent starts at state 3 (s0), and states 1 and 10 are absorbing (Abs in subﬁgure a-(1)).
Actions A = {←, →} move the agent left or right in the chain unless the agent is in an absorbing state. Subﬁg. a-(1) shows the reward function: r(2, ←) = 0.1, r(4, →) = −0.2, r(5, →) = 0.1, and all state-action pairs not shown in a-(1) yield r = 0. Subﬁg. a-(2) shows V ∗ for γ = 0.9. Subﬁg. a-(3) shows a good heuristic h
— V (random π). Subﬁg. a-(4) shows a bad heuristic h — V (myopic π). Subﬁg. b-(1): π∗ for V ∗ from a-(2).
Subﬁg. b-(2): ˜π∗ from HuRL with h = 0, λ = 0.5. Subﬁg. b-(3): ˜π∗ from HuRL with the good h from (a).(3) and λ = 0.5. Subﬁg. b-(4): ˜π∗ from the bad h from a-(4), λ = 0.5. Subﬁg. b-(5): ˜π∗ from the bad h and
λ = 1. Subﬁg. (c) illustrates the takeaway message: using HuRL with a good h can ﬁnd π∗ from s0 even with a small λ (see the x-axis), while HuRL with a bad h requires a much higher λ to discover π∗. 4 Theoretical Analysis
When can HuRL accelerate learning? Similar to typical regularization techniques, the horizon-based regularization of HuRL leads to a bias-variance decomposition that can be optimized for better
ﬁnite-sample performance compared to directly solving the original MDP. However, a non-trivial trade-off is possible only when the regularization can bias the learning toward a good direction. In
HuRL’s case this is determined by the heuristic, which resembles a prior we encode into learning.
In this section we provide HuRL’s theoretical foundation. We ﬁrst describe the bias-variance trade-off induced by HuRL. Then we show how suboptimality in solving the reshaped MDP translates into performance in the original MDP, and identify the assumptions HuRL needs the base RL algorithm to satisfy. In addition, we explain how HuRL relates to PBRS, and characterize the quality of heuristics and sufﬁcient conditions for constructing good heuristics from batch data using ofﬂine RL.
For clarity, we will focus on the reshaped MDP (cid:102)M = (S, A, P, (cid:101)r, (cid:101)γ) for a ﬁxed λ ∈ [0, 1], where (cid:101)r, (cid:101)γ are deﬁned in (1). We can view this MDP as the one in a single iteration of HuRL. For a policy π, we denote its state value function in (cid:102)M as (cid:101)V π, and the optimal policy and value function of (cid:102)M as (cid:101)π∗ and (cid:101)V ∗, respectively. The missing proofs of the results from this section can be found in Appendix A. 4.1 Short-Horizon Reduction: Performance Decomposition
Our main result is a performance decomposition, which characterizes how a heuristic h and subopti-mality in solving the reshaped MDP (cid:102)M relate to performance in the original MDP M.
Theorem 4.1. For any policy π, heuristic f : S → R, and mixing coefﬁcient λ ∈ [0, 1],
V ∗(d0) − V π(d0) = Regret(h, λ, π) + Bias(h, λ, π) where we deﬁne
Regret(h, λ, π) := λ (cid:16) (cid:101)V ∗(d0) − (cid:101)V π(d0) (cid:17)
Bias(h, λ, π) := (cid:16) (cid:17)
V ∗(d0) − (cid:101)V ∗(d0)
+
+ 1 − λ 1 − γ
γ(1 − λ) 1 − γ (cid:16) (cid:17) (cid:101)V ∗(dπ) − (cid:101)V π(dπ)
Es,a∼dπ Es(cid:48)|s,a (cid:104) h(s(cid:48)) − (cid:101)V ∗(s(cid:48)) (cid:105) (3) (4)
Furthermore, ∀b ∈ R, Bias(h, λ, π) = Bias(h + b, λ, π) and Regret(h, λ, π) = Regret(h + b, λ, π).
The theorem shows that suboptimality of a policy π in the original MDP M can be decomposed into 1) a bias term due to solving a reshaped MDP (cid:102)M instead of the original MDP M, and 2) a regret term (i.e. the learning variance) due to π being suboptimal in the reshaped MDP (cid:102)M. Moreover, it shows that heuristics are equivalent up to constant offsets. In other words, only the relative ordering between states that a heuristic induces matters in learning, not the absolute values. 5
Balancing the two terms trades off bias and variance in learning. Using a smaller λ replaces the long-term information with the heuristic and make the horizon of the reshaped MDP (cid:102)M shorter.
Therefore, given a ﬁnite interaction budget, the regret term in (3) can be more easily minimized, though the bias term in (4) can potentially be large if the heuristic is bad. On the contrary, with λ = 1, the bias is completely removed, as the agent solves the original MDP M directly. 4.2 Regret, Algorithm Requirement, and Relationship with PBRS
The regret term in (3) characterizes the performance gap due to π being suboptimal in the reshaped
MDP (cid:102)M, because Regret(h, λ, (cid:101)π∗) = 0 for any h and λ. For learning, we need the base RL algorithm L to ﬁnd a policy π such that the regret term in (3) is small. By the deﬁnition in (3), the base RL algorithm L is required not only to ﬁnd a policy π such that (cid:101)V ∗(s) − (cid:101)V π(s) is small for states from d0, but also for states π visits when rolling out in the original MDP M. In other words, it is insufﬁcient for the base RL algorithm to only optimize for (cid:101)V π(d0) (the performance in the reshaped MDP with respect to the initial state distribution; see Section 2.2). For example, suppose
λ = 0 and d0 concentrates on a single state s0. Then maximizing (cid:101)V π(d0) alone would only optimize
π(·|s0) and the policy π need not know how to act in other parts of the state space.
To use HuRL, we need the base algorithm to learn a policy π that has small action gaps in the reshaped MDP (cid:102)M but along trajectories in the original MDP M, as we show below. This property is satisﬁed by off-policy RL algorithms such as Q-learning [34].
Proposition 4.1. For any policy π, heuristic f : S → R and mixing coefﬁcient λ ∈ [0, 1],
Regret(h, λ, π) = −Eρπ(d0) (cid:104)(cid:80)∞ t=0 γt (cid:101)A∗(st, at) (cid:105) where ρπ(d0) denotes the trajectory distribution of running π from d0, and (cid:101)A∗(s, a) = (cid:101)r(s, a) + (cid:101)γEs(cid:48)|s,a[ (cid:101)V ∗(s(cid:48))] − (cid:101)V ∗(s) ≤ 0 is the action gap with respect to the optimal policy (cid:101)π∗ of (cid:102)M.
Another way to comprehend the regret term is through studying its dependency on λ. When λ = 1,
Regret(h, 0, π) = V ∗(d0)−V π(d0), which is identical to the policy regret in M for a ﬁxed initial dis-Es∼dπ [(cid:101)r(s, π(cid:48))−(cid:101)r(s, π)], tribution d0. On the other hand, when λ = 0, Regret(h, 0, π) = maxπ(cid:48) which is the regret of a non-stationary contextual bandit problem where the context distribution is dπ (the average state distribution of π). In general, for λ ∈ (0, 1), the regret notion mixes a short-horizon non-stationary problem and a long-horizon stationary problem. 1 1−γ
One natural question is whether the reshaped MDP (cid:102)M has a more complicated and larger value landscape than the original MDP M, because these characteristics may affect the regret rate of a base algorithm. We show that (cid:102)M preserves the value bounds and linearity of the original MDP.
Proposition 4.2. Reshaping the MDP as in (1) preserves the following characteristics: 1) If 1 1−γ ] for all π and s ∈ S. 2) If (cid:102)M is a linear MDP with h(s) ∈ [0, feature vector φ(s, a) (i.e. r(s, a) and Es(cid:48)|s,a[g(s(cid:48))] for any g can be linearly parametrized in
φ(s, a)), then (cid:102)M is also a linear MDP with feature vector φ(s, a). 1−γ ], then (cid:101)V π(s) ∈ [0, 1
On the contrary, the MDP Mλ := (S, A, P, r, λγ) in Section 2.2 does not have these properties.
We can show that Mλ is equivalent to (cid:102)M up to a PBRS transformation (i.e., ¯r(s, a) = ˜r(s, a) +
˜γEs(cid:48)|s,a[h(s(cid:48))] − h(s)). Thus, HuRL incorporates guidance discount into PBRS with nicer properties. 4.3 Bias and Heuristic Quality
The bias term in (4) characterizes suboptimality due to using a heuristic h in place of long-term state values in M. What is the best heuristic in this case? From the deﬁnition of the bias term in (4), we see that the ideal heuristic is the optimal value V ∗, as Bias(V ∗, λ, π) = 0 for all λ ∈ [0, 1]. By continuity, we can expect that if h deviates from V ∗ a little, then the bias is small.
Corollary 4.1. If inf b∈R (cid:107)h + b − V ∗(cid:107)∞ ≤ (cid:15), then Bias(h, λ, π) ≤ (1−λγ)2 (1−γ)2 (cid:15).
To better understand how the heuristic h affects the bias, we derive an upper bound on the bias by replacing the ﬁrst term in (4) with an upper bound that depends only on π∗. 6
Proposition 4.3. For g : S → R and η ∈ [0, 1], deﬁne C(π, g, η) := Eρπ(d0)
Then Bias(h, λ, π) ≤ (1 − λ)γ(C(π∗, V ∗ − h, λγ) + C(π, h − (cid:101)V ∗, γ)). (cid:2)(cid:80)∞ t=1 ηt−1g(st)(cid:3).
In Proposition 4.3, the term (1 − λ)γC(π∗, V ∗ − h, λγ) is the underestimation error of the heuristic h under the states visited by the optimal policy π∗ in the original MDP M. Therefore, to minimize the ﬁrst term in the bias, we would want the heuristic h to be large along the paths that π∗ generates.
However, Proposition 4.3 also discourages the heuristic from being arbitrarily large, because the second term in the bias in (4) (or, equivalently, the second term in Proposition 4.3) incentivizes the heuristic to underestimate the optimal value of the reshaped MDP (cid:101)V ∗. More precisely, the second term requires the heuristic to obey some form of spatial consistency. A quick intuition is the observation that if h(s) = V π(cid:48) (s) for some π(cid:48) or h(s) = 0, then h(s) ≤ (cid:101)V ∗(s) for all s ∈ S.
More generally, we show that if the heuristic is improvable with respect to the original MDP M (i.e. the heuristic value is lower than that of the max of Bellman backup), then h(s) ≤ (cid:101)V ∗(s). By
Proposition 4.3, learning with an improvable heuristic in HuRL has a much smaller bias.
Deﬁnition 4.1. Deﬁne the Bellman operator (Bh)(s, a) := r(s, a) + γEs(cid:48)|s,a[h(s(cid:48))]. A heuristic function h : S → R is said to be improvable with respect to an MDP M if maxa(Bh)(s, a) ≥ h(s).
Proposition 4.4. If h is improvable with respect to M, then (cid:101)V ∗(s) ≥ h(s), for all λ ∈ [0, 1]. 4.4 Pessimistic Heuristics are Good Heuristics
While Corollary 4.1 shows that HuRL can handle an imperfect heuristic, this result is not ideal.
The corollary depends on the (cid:96)∞ approximation error, which can be difﬁcult to control in large state spaces. Here we provide a more reﬁned sufﬁcient condition of good heuristics. We show that the concept of pessimism in the face of uncertainty provides a ﬁner mechanism for controlling the approximation error of a heuristic and would allow us to remove the (cid:96)∞-type error. This result is useful for constructing heuristics from data that does not have sufﬁcient support.
From Proposition 4.3 we see that the source of the (cid:96)∞ error is the second term in the bias upper bound, as it depends on the states that the agent’s policy visits which can change during learning.
To remove this dependency, we can use improvable heuristics (see Proposition 4.4), as they satisfy h(s) ≤ (cid:101)V ∗(s). Below we show that Bellman-consistent pessimism yields improvable heuristics.
Proposition 4.5. Suppose h(s) = Q(s, π(cid:48)) for some policy π(cid:48) and function Q : S × A → R such that Q(s, a) ≤ (Bh)(s, a), ∀s ∈ S, a ∈ A. Then h is improvable and f (s) ≤ V π(cid:48) (s) for all s ∈ S.
The Bellman-consistent pessimism in Proposition 4.5 essentially says that h is pessimistic with respect to the Bellman backup. This condition has been used as the foundation for designing pessimistic off-policy RL algorithms, such as pessimistic value iteration [30] and algorithms based on pessimistic absorbing MDPs [31]. In other words, these pessimistic algorithms can be used to construct good heuristics with small bias in Proposition 4.3 from ofﬂine data. With such a heuristic, the bias upper bound would be simply Bias(h, λ, π) ≤ (1 − λ)γC(π∗, V ∗ − h, λγ). Therefore, as long as enough batch data are sampled from a distribution that covers states that π∗ visits, these pessimistic algorithms can construct good heuristics with nearly zero bias for HuRL with high probability. 5 Experiments
We validate our framework HuRL experimentally in MuJoCo (commercial license) [32] robotics control problems and Procgen games (MIT License) [33], where soft actor critic (SAC) [35] and proximal policy optimization (PPO) [36] were used as the base RL algorithms, respectively5. The goal is to study whether HuRL can accelerate learning by shortening the horizon with heuristics. In particular, we conduct studies to investigate the effects of different heuristics and mixing coefﬁcients.
Since the main focus here is on the possibility of leveraging a given heuristic to accelerate RL algorithms, in these experiments we used vanilla techniques to construct heuristics for HuRL.
Experimentally studying the design of heuristics for a domain or a batch of data is beyond the scope of the current paper but are important future research directions. For space limitation, here we report only the results of the MuJoCo experiments. The results on Procgen games along with other experimental details can also be found in Appendix C. 5Code to replicate all experiments is available at https://github.com/microsoft/HuRL. 7
5.1 Setup
We consider four MuJoCo environments with dense rewards (Hopper-v2, HalfCheetah-v2, Humanoid-v2, and Swimmer-v2) and a sparse reward version of Reacher-v2 (denoted as Sparse-Reacher-v2)6.
We design the experiments to simulate two learning scenarios. First, we use Sparse-Reacher-v2 to simulate the setting where an engineered heuristic based on domain knowledge is available; since this is a goal reaching task, we designed a heuristic h(s) = r(s, a) − 100(cid:107)e(s) − g(s)(cid:107), where e(s) and g(s) denote the robot’s end-effector position and the goal position, respectively. Second, we use the dense reward environments to model scenarios where a batch of data collected by multiple behavioral policies is available before learning, and a heuristic is constructed by an ofﬂine policy evaluation algorithm from the batch data (see Appendix C.1 for details). In brief, we generated these behavioral policies by running SAC from scratch and saved the intermediate policies generated in training. We then use least-squares regression to ﬁt a neural network to predict empirical Monte-Carlo returns of the trajectories in the sampled batch of data. We also use behavior cloning (BC) to warm-start all RL agents based on the same batch dataset in the dense reward experiments.
The base RL algorithm here, SAC, is based on the standard implementation in Garage (MIT Li-cense) [37]. The policy and value networks are fully connected independent neural networks. The policy is Tanh-Gaussian and the value network has a linear head.
Algorithms. We compare the performance of different algorithms below. 1) BC 2) SAC 3) SAC with BC warm start (SAC w/ BC) 4) HuRL with the engineered heuristic (HuRL) 5) HuRL with a zero heuristic and BC warm start (HuRL-zero) 6) HuRL with the Monte-Carlo heuristic and BC warm start (HuRL-MC) 7) SAC with PBRS reward (and BC warm start, if applicable) (PBRS). For the
HuRL algorithms, the mixing coefﬁcient was scheduled as λn = λ0 + (1 − λ0)cω tanh(ω(n − 1)), for n = 1, . . . , N , where λ0 ∈ [0, 1], ω > 0 controls the increasing rate, and cω is a normalization constant such that λ∞ = 1 and λn ∈ [0, 1]. We chose these algorithms to study the effect of each additional warm-start component (BC and heuristics) added on top of vanilla SAC. HuRL-zero is
SAC w/ BC but with an extra λ schedule above that further lowers the discount, whereas SAC and
SAC w/ BC keep a constant discount factor.
Evaluation and Hyperparameters.
In each iteration, the RL agent has a ﬁxed sample budget for environment interactions, and its performance is measured in terms of undiscounted cumulative returns of the deterministic mean policy extracted from SAC. The hyperparameters used in the algorithms above were selected as follows. First, the learning rates and the discount factor of the base RL algorithm, SAC, were tuned for each environment. The tuned discount factor was used as the discount factor γ of the original MDP M. Fixing the hyperparameters above, we additionally tune λ0 and ω for the λ schedule of HuRL for each environment and each heuristic. Finally, after all these hyperparameters were ﬁxed, we conducted additional testing runs with 30 different random seeds and report their statistics here. Sources of randomness included the data collection process of the behavioral policies, training the heuristics from batch data, BC, and online RL. However, the behavioral policies were ﬁxed across all testing runs. We chose this hyperparameter tuning procedure to make sure that the baselines (i.e. SAC) compared in these experiments are their best versions. 5.2 Results Summary
Fig. 2 shows the results on the MuJoCo environments. Overall, we see that HuRL is able to leverage engineered and learned heuristics to signiﬁcantly improve the learning efﬁciency. This trend is consistent across all environments that we tested on.
For the sparse-reward experiments, we see that SAC and PBRS struggle to learn, while HuRL is able to converge to the optimal performance much faster. For the dense reward experiments, similarly
HuRL-MC converges much faster, though the gain in HalfCheetah-v2 is minor and it might have converged to a worse local maximum in Swimmer-v2. In addition, we see that warm-starting SAC using BC (i.e. SAC w/ BC) can improve the learning efﬁciency compared with the vanilla SAC, but using BC alone does not result in a good policy. Lastly, we see that using the zero heuristic (HuRL-zero) with extra λ-scheduling does not further improve the performance of SAC w/ BC. This comparison veriﬁes that the learned Monte-Carlo heuristic provides non-trivial information.
Interestingly, we see that applying PBRS to SAC leads to even worse performance than running SAC with the original reward. There are two reasons why SAC+PBRS is less desirable than SAC+HuRL 6The reward is zero at the goal and −1 otherwise. 8
(a) Sparse-Reacher-v2 (b) Humanoid-v2 (c) Hopper-v2 (d) Swimmer-v2 (e) HalfCheetah-v2 (f) λ0 ablation.
Figure 2: Experimental results. (a) uses an engineered heuristic for a sparse reward problem; (b)-(e) use heuristics learned from ofﬂine data and share the same legend; (e) shows ablation results of different initial λ0 in
Hopper-v2. The plots show the 25th, 50th, 75th percentiles of algorithm performance over 30 random seeds. as we discussed before: 1) PBRS changes the reward/value scales in the induced MDP, and popular
RL algorithms like SAC are very sensitive to such changes. In contrast, HuRL induces values on the same scale as we show in Proposition 4.2. 2) In HuRL, we are effectively providing the algorithm some more side-information to let SAC shorten the horizon when the heuristic is good.
The results in Fig. 2 also have another notable aspect. Because the datasets used in the dense reward experiments contain trajectories collected by a range of policies, it is likely that BC suffers from disagreement in action selection among different policies. Nonetheless, training a heuristic using a basic Monte-Carlo regression seems to be less sensitive to these conﬂicts and still results in a helpful heuristic for HuRL. One explanation can be that heuristics are only functions of states, not of states and actions, and therefore the conﬂicts are minor. Another plausible explanation is that HuRL only uses the heuristic to guide learning, and does not completely rely on it to make decisions Thus, HuRL can be more robust to the heuristic quality, or, equivalently, to the quality of prior knowledge. 5.3 Ablation: Effects of Horizon Shortening
To further verify that the acceleration in Fig. 2 is indeed due to horizon shortening, we conducted an ablation study for HuRL-MC on Hopper-v2, whose results are presented in Fig. 2f. HuRL-MC’s best λ-schedule hyperparameters on Hopper-v2, which are reﬂected in its performance in the aforementioned Fig. 2c, induced a near-constant schedule at λ = 0.95; to obtain the curves in Fig. 2f, we ran HuRL-MC with constant-λ schedules for several more λ values. Fig. 2f shows that increasing
λ above 0.98 leads to a performance drop. Since using a large λ decreases bias and makes the reshaped MDP more similar to the original MDP, we conclude that the increased learning speed on
Hopper-v2 is due to HuRL’s horizon shortening (coupled with the guidance provided by its heuristic). 6