Abstract
Imbalanced datasets are commonplace in modern machine learning problems. The presence of under-represented classes or groups with sensitive attributes results in concerns about generalization and fairness. Such concerns are further exacerbated by the fact that large capacity deep nets can perfectly ﬁt the training data and appear to achieve perfect accuracy and fairness during training, but perform poorly during test. To address these challenges, we propose AutoBalance, a bi-level optimization framework that automatically designs a training loss function to optimize a blend of accuracy and fairness-seeking objectives. Speciﬁcally, a lower-level problem trains the model weights, and an upper-level problem tunes the loss function by monitoring and optimizing the desired objective over the validation data. Our loss design enables personalized treatment for classes/groups by employing a parametric cross-entropy loss and individualized data augmentation schemes. We evaluate the beneﬁts and performance of our approach for the application scenarios of imbalanced and group-sensitive classiﬁcation. Extensive empirical evaluations demonstrate the beneﬁts of AutoBalance over state-of-the-art approaches. Our experimental ﬁndings are complemented with theoretical insights on loss function design and the beneﬁts of train-validation split. All code is available open-source. 1

Introduction
Recently, deep learning, large datasets, and the evolution of computing power have led to unprece-dented success in computer vision, and natural language processing [15, 39, 62]. This success is partially driven by the availability of high-quality datasets, built by carefully collecting a sufﬁcient number of samples for each class. In practice, real-world datasets are frequently imbalanced and exhibit long-tailed behavior, necessitating a careful treatment of the minorities [20, 53, 23]. Indeed, modern classiﬁcation tasks can involve thousands of classes, so it is perhaps intuitive that some classes should be over/under-represented compared to others. Besides class imbalance, minorities can also appear at the feature-level; for instance, the speciﬁc values of the features of an example can vary depending on that example’s membership in certain sensitive or protected groups, e.g. race, gender, disabilities (see also Figure 1a). In scenarios where imbalances are induced by heterogeneous client datasets (e.g., in the context of federated learning), addressing these imbalances can help ensure that a machine learning model works well for all clients, rather than just those that generate the majority of the training data. This rich set of applications motivate the careful treatment of imbalanced datasets.
In the imbalanced classiﬁcation literature, the recurring theme is maximizing a fairness-seeking objective, such as balanced accuracy. Unlike standard accuracy, which can be dominated by the majorities, a fairness-seeking objective seeks to promote examples from minorities, and downweigh 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
S = ST ∪ SV
SV
ST minα LSV fair (fθ) minθ LST train(fθ, α) minθ LST train(fθ, α(cid:63)) fθ(cid:63) (a) (b)
Figure 1: (a) Example group-imbalance on the Waterbirds dataset [63, 73]. Groups correspond to the distinct background types, while classes are distinct bird types. (b) Framework overview. The search phase conducts a bilevel optimization to design the optimal training loss function parameterized by α(cid:63) by minimizing the validation loss, using a train-validation split (e.g. 80%-20%). The retrain phase uses the original training data and α(cid:63) to obtain the optimal model parameters θ(cid:63). The evaluation phase predicts the test data using θ(cid:63). examples from majorities. Here, note that there is a distinction between the test and training objectives. While the overall goal is typically to maximize a non-differentiable objective such as balanced accuracy on the test set, during training, we use a differentiable proxy for this, such as weighted cross-entropy. Thus, the fundamental question of interest is:
How to design a training loss to maximize a fairness-seeking objective on the test set?
A classical answer to this question is to use a Bayes-consistent loss functions. For instance, weighted cross-entropy (e.g., each class gets a different weight, see Sec. 2) is traditionally a good choice for optimizing weighted accuracy objectives. Unfortunately, this intuition starts to break down when the training problem is overparameterized, which is a common practice in deep learning: in essence, for large capacity deep nets, the training process can perfectly ﬁt to the data, and training loss is no longer indicative of test error. In fact, recent works [8, 38] show that weighted cross-entropy has minimal beneﬁt to balanced accuracy, and instead alternative methods based on margin adjustment can be effective (namely, by ensuring that minority classes are further away from decision boundary).
These ideas led to the development of a parametric cross-entropy function (cid:96)(y, f (x)) = wy log (cid:0)1 + k(cid:54)=y elk−ly · e∆kfk(x)−∆yfy(x)(cid:1), which allows for a personalized treatment of the individual classes (cid:80) via the design parameters (wk, lk, ∆k)K k=1 [8, 38, 53, 35, 67]. Here, wk is the classical weighting term whereas lk and ∆k are additive and multiplicative logit adjustments. However, despite these developments, it is unclear how such parametric cross-entropy functions can be tuned for use for different fairness objectives, for example to tackle class or group imbalances. The works by [8, 53] provide theoretically-motivated choices for (wk, lk), while [38] argues that (wk, lk) is not as effective as ∆k in the interpolating regime of zero training error and proposes the simultaneous use of all three different parameter types. However, these works do not provide an optimized loss function that can be systematically tailored for different fairness objectives, such as balanced accuracy common in class imbalanced scenarios, or equal opportunity [23, 17] which is relevant in group-sensitive settings.
In this work, we address these shortcomings by designing the loss function within the optimization in a principled fashion, to handle different fairness-seeking objectives. Our main idea is to use bi-level optimization, where the model weights are optimized over the training data, and the loss function is automatically tuned by monitoring the validation loss. Our core intuition is that unlike training data, the validation data is difﬁcult to ﬁt and will provide a consistent estimator of the test objective.
Contributions. Based on this high-level idea, this paper takes a step towards a systematic treatment of imbalanced learning problems with contributions along several fronts: state-of-the-art performance, data augmentation, applications to different imbalance types, and theoretical intuitions. Speciﬁcally:
• We introduce AutoBalance —a bilevel optimization framework— that designs a fairness-seeking loss function by jointly training the model and the loss function hyperparameters in a systematic way (Figure 1b, Section 2). We introduce novel strategies that narrow down the search space to improve convergence and avoid overﬁtting. To further improve the performance, our design also incorporates data augmentation policies personalized to subpopulations (classes or groups). We demonstrate the beneﬁts of AutoBalance when optimizing various fairness-seeking objectives over the state-of-the-art, such as logit-adjustment (LA) [53] and label-distribution-aware margin (LDAM) [8] losses. The code is available online [42]. 2
Algorithm 1: AutoBalance via Bilevel Optimization
Input: Model fθ with weights θ, dataset S = ST ∪ SV , step sizes ηα & ηθ, # iterations t2 > t1 5 6 7 8 1 Initialize α with (cid:96)fair(·) = (cid:96)train(·; α) 2 Train θ for t1 iterations (α is ﬁxed) 3 for i ← t1 to t2 do 4
Sample training batch BT from ST ;
BT ← A(BT )
θ ← θ − ηθ∇θLBT
Sample validation batch BV from SV ;
Compute hyper-gradient ∇αLBV fair (fθ)
α ← α − ηα∇LBV train(fθ; α) fair (fθ) 9 10 end 11 Set α(cid:63) ← α, ST ← S, reset weights θ 12 Train θ for t2 iterations using α(cid:63)
// Consistent initialization
// Search Phase: Starts with warm-up
// Apply class-personalized augmentation
// via Approx. Implicit Differentiation
// Update loss function hyper-parameters
// Retraining Phase: Use all data and α(cid:63)
Result: The ﬁnal model θ(cid:63) ← θ and hyper-parameters α(cid:63)
• Extensive experiments provide several takeaways (Section 3). First, AutoBalance discovers loss functions from scratch that are consistent with theory and intuition: hyperparameters of the minority classes evolve to upweight the training loss of minority to promote them. Second, the impact of individual design parameters in the loss function is revealed, with the additive adjustment lk and multiplicative adjustment ∆k synergistically improving the fairness objective. Third, personalized data augmentation can further improve the performance over a single generic augmentation policy.
• Beyond class imbalance, we consider applications of loss function design to the group-sensitive setting (Section 4). Our experiments show that AutoBalance consistently outperforms various baselines, leading to a more efﬁcient Pareto-frontier of accuracy-fairness tradeoffs. 1.1 Problem Setup for Class Imbalance
We ﬁrst focus on the label-imbalance problem. The extension to the group-imbalanced setting (approach, algorithms, evaluations) is deferred to Section 4. Let [K] denote the set {1, . . . , K}.
Suppose we have a dataset S = (xi, yi)n i=1 sampled i.i.d. from a distribution D with input space
X and K classes. For a training example (x, y), x ∈ X is the input feature and y ∈ [K] is the output label. Let f : X → RK be a model that outputs a distribution over classes and let
ˆyf (x) = arg maxi∈[K] f (x). The standard classiﬁcation error is denoted by E(f ) = PD[y (cid:54)= ˆyf (x)].
For a loss function (cid:96)(y, ˆy) (e.g. cross-entropy), we similarly denote
Population risk: L(f ) = ED[(cid:96)(y, ˆyf (x))] and Empirical risk: LS (f ) = 1 n n (cid:88) i=1 (cid:96)(yi, ˆyf (xi)). (cid:88) Setting: Imbalanced classes. Deﬁne the frequency of the k’th class via πk = P(x,y)∼D(y = k).
Label/class-imbalance occurs when the class frequencies differ substantially, i.e., maxi∈[K] πi (cid:29) mini∈[K] πi. Let us introduce
Balanced risk: Lbal(f ) = 1
K
K (cid:88) k=1
Lk(f ) and Class-conditional risk: Lk(f ) = EDk [(cid:96)(y, ˆyf (x))].
Similarly, let Ek(f ) be the class-conditional classiﬁcation error and Ebal(f ) := (1/K) (cid:80)K k=1 Ek(f ) be the balanced error. In this setting, rather than the standard test error E(f ), our goal is to the minimize balanced error. At a high-level, we propose to do this by designing an imbalance-aware training loss that maximizes balanced validation accuracy. 2 Methods: Loss Functions, Search Space Design, and Bilevel Optimization
Our main goal in this paper is automatically designing loss functions to optimize target objectives for imbalanced learning (e.g., Settings A and B). We will employ a parametrizable family of loss functions that can be tailored to the needs of different classes or groups. Cross-entropy variations have been proposed by [44, 35, 16] to optimize balanced objectives. Our design space will utilize recent works which introduce Label-distribution-aware margin (LDAM) [8], Logit-adjustment (LA)
[53], Class-dependent temperatures (CDT) [67], Vector scaling (VS) [38] losses. Speciﬁcally, we 3
build on the following parametric loss function controlled by three vectors w, l, ∆ ∈ RK: (cid:96)(y, f (x)) = wy log (cid:0)1 + (cid:88) k(cid:54)=y elk−ly · e∆kfk(x)−∆yfy(x)(cid:1). (2.1)
Here, wy enables conventional weighted CE and ly, and ∆y are additive and multiplicative adjust-ments to the logits. This choice is same as the VS-loss introduced in [38], which borrows the ∆ term from [67] and l term from [8, 53]. [53] makes the observation that we can use l rather than w while ensuring Fisher consistency in balanced error. We make the following complementary observation.
Lemma 1 Parametric loss function (2.1) is not consistent for standard or balanced errors if there are distinct multiplicative adjustments i.e. ∆i (cid:54)= ∆j for some i, j ∈ [K].
While consistency is a desirable property, it is intuitively more critical during the earlier phase of the training where the training risk is more indicative of the test risk. In the interpolating regime of zero-training error, [38] shows that w, l can be ineffective and multiplicative ∆-adjustment can be more favorable. Our algorithm will be initialized with a consistent weighted-CE; however, we will allow the algorithm to automatically adapt to the interpolating regime by tuning l and ∆.
Proposed training loss function. For our algorithm, we will augment (2.1) with data augmentation that can be personalized to distinct classes. Let us denote the data augmentation policies by A = (Ay)K y=1 where each Ay stochastically augments an input example with label y. Additionally, we clamp ∆i with the sigmoid function σ to limit its range to (0,1) to ensure non-negativity. To this end, our loss function for the lower-level optimization (over training data) is as follows: (cid:34) (cid:33)(cid:35) (cid:32) (cid:96)train(y, x, f ; α) = − EA wy log
. (2.2) eσ(∆y)fy(Ay(x))+ly i∈[K] eσ(∆i)fi(Ay(x))+li (cid:80)
Here, α is the set of hyperparameters of the loss function that we wish to optimize, speciﬁcally
α = [w, l, ∆, param(A)]. param(A) is the parameterization of the augmentation policies (Ay)y∈[K].
Personalized data augmentation (PDA).
Remark-able beneﬁts of data augmentation techniques provide a natural motivation to investigate whether one can beneﬁt from learning class-personalized aug-mentation policies. The PDA idea relates to SMOTE
[9], where the minority class is over-sampled by creating synthetic examples.
To formalize the beneﬁts of PDA, consider a spherical augmentation strategy where Ay(x) samples a vector uniformly from an (cid:96)2-ball of radius εy around x. As visualized in Figure 2 for a linear classiﬁer, if the augmentation strengths of both classes are equal, the max-margin classiﬁer is not affected by the application of the data augmentation and remains identical. Thus, augmentation has no beneﬁt. However by applying a stronger augmentation on minority, the decision boundary is shifted to protect minority which can provably beneﬁt the balanced accuracy [38]. The following intuitive observation links the PDA to parametric loss (2.1).
Lemma 2 Consider a binary classiﬁcation task with labels 0 and 1 and a linearly separable training dataset. For any parametric loss (2.1) choices of (li, ∆i, wi)1 i=0, there exists spherical augmentation strengths for minority/majority classes so that, without regularization, optimizing the logistic loss with personalized augmentations returns the same classiﬁer as optimizing (2.1).
Figure 2: Data augmentation can shift the de-cision boundary to beneﬁt the minority class by providing a larger margin. Lemma 2 establishes an equivalence between spherical data augmentation and parametric cross-entropy loss.
This lemma is similar in ﬂavor to [31], which considers a larger uncertainty set around the minority class. But, as discussed in the appendix, Lemma 2 is relevant in the overparameterized regime whereas the approach of [31] is ineffective for separable data [51]. Algorithmically, the augmentations that we consider are much more ﬂexible than the (cid:96)p-balls of [31] and our experiments showcase the value of our approach in state-of-the-art multiclass settings. Besides, note that the (theoretical) beneﬁts of
PDA can go well-beyond Lemma 2 by leveraging the invariances [10, 14] (via rotation, translation). 2.1 Proposed Bilevel Optimization Method
We formulate the loss function design as a bilevel optimization over hyperparameters α and a hypothesis set F. We split the dataset S into training ST and validation SV sets with nT and nV 4
s e l p m a s f o r e b m u
N e u l a v r e t e m a r a p
-r e p y
H e u l a v r e t e m a r a p
-r e p y
H e u l a v r e t e m a r a p
-r e p y
H
Sorted class index (a) Train-validation sizes for CIFAR100-LT clusters
Epoch (b) Evolution of ∆ when only training ∆
Epoch (c) Evolution of l when only training l
Epoch (d) Evolution of ∆ and l when optimizing jointly
Figure 3: (a) Visualizing class clustering and train-validation split. (b), (c), (d) Evolution of loss function parameters l, ∆ over epochs for CIFAR100-LT where solid curves and dashed curves corresponds to ∆ and l respectively. We display average value of 20 classes for better visualization. Based on theory, the minority classes should be assigned a larger margin. During the initial 120 epochs, we use weighted cross-entropy training and AutoBalance kicks in after epoch 120. Observe that, AutoBalance does indeed learn larger parameters (ly, ∆y) for minority class clusters (each containing 20 classes) consistent with theoretical intuition. In all
Figures (b), (c), (d), by the end of training, the colors are ordered according to the class frequency. However, when ∆y is trained jointly with ly (Fig d), the training is more stable compared to training ∆y alone (Fig b).
Thus, besides its accuracy beneﬁts in Table 1, ly also seems to have optimization beneﬁts. examples respectively. Let Efair be the desired test-error objective. When Efair is not differentiable, we use a weighted cross-entropy (CE) loss function (cid:96)fair(y, ˆy) chosen to be consistent with Efair. For instance, Efair could be a superposition of standard and balanced classiﬁcation errors, i.e. Efair = (1 − λ)E + λEbal. Then, we simply choose (cid:96)fair = (1 − λ)CE + CEbal. The hyperparameter α aims to minimize the loss (cid:96)fair over the validation set SV and the hypothesis f ∈ F aims to minimize the training loss (2.2) as follows: min
α
LSV fair (fα) WHERE fα = arg min f ∈F
LST train(f ; α) := 1 nT nT(cid:88) i=1 (cid:96)train(yi, xi, f ; α). (2.3)
Here, Lfair/LSV fair are the test/validation risks associated with (cid:96)fair, e.g. Lfair = ED[(cid:96)fair] as in Section 1.1. Algorithm 1 summarizes our approach and highlights the key components. The training loss (cid:96)train(·; α) is also initialized to be consistent with Efair (e.g., same as (cid:96)fair). In line with the literature on bilevel optimization, we will refer to the two minimizations of the validation and training losses in (2.3) as upper and lower level optimizations, respectively.
∂α
∂α + ∂Lfair
∂θ(cid:63)
Implicit Differentiation and Warm-up training. For a loss function parameter α, the hyper-gradient can be written via the chain-rule ∂Lfair(θ(cid:63))
∂θ(cid:63)
= ∂Lfair
∂α [46]. Here, θ(cid:63) is the solution of the lower-level problem. We note that ∂Lfair/∂α = 0 since α does not appear within the upper-level loss. Also observe that ∂Lfair(θ(cid:63))/∂θ(cid:63) can be directly computed by taking the gradient. To compute ∂θ(cid:63)/∂α, we follow the recent work [46] and employ the Implicit Function Theorem (IFT).
If there exists a ﬁxed point (θ(cid:63),α(cid:63)) that satisﬁes ∂Ltrain(θ(cid:63), α(cid:63))/∂θ = 0 and regularity conditions are satisﬁed, then around α(cid:63), there exists a function θ(α) such that θ(α(cid:63)) = θ(cid:63) and we also have ∂θ
∂θ2 )−1 is usually time consuming or even impossible for modern neural networks which have millions of parameters.
To compute the hyper-gradient while avoiding extensive computation, we approximate the inverse
Hessian via the Neumann series, which is widely used for inverse Hessian estimation [43, 46]. Finally, the warm-up phase of our method (Line 2 of Algo. 1) is essential to guarantee that the IFT assumption
∂Ltrain(θ(cid:63),α(cid:63))
∂θ
∂θ∂α . However, directly computing inverse Hessian ( ∂2Ltrain
= 0 is approximately satisﬁed.
∂θ2 )−1 ∂2Ltrain
∂α = ( ∂2Ltrain
Why Bilevel Optimization? We choose differentiable optimization over alternative hyperparameter tuning methods because our hyperparameter space is continuous and potentially large (e.g. in the order of K = 8, 142 for iNaturalist). In our experiments, the runtime of our method was typically 4∼5 times that of standard training (with known hyperparameters). Intuitively, the runtime is at least twice due to our use of separate search and retraining phases. In the appendix, we also compare against alternative approaches (speciﬁcally SMAC of [26]) and found that our approach is faster and more accurate on CIFAR10-LT. 2.2 Reducing the Hyperparameter Search Space and the Beneﬁts of Validation Set
Suppose we wish to optimize the hyperparameter α = (wy, ly, ∆y)K y=1 of the parametric loss (2.1). An important challenge is the dimensionality of α, which is proportional to the number of classes K, as we need a triplet (wy, ly, ∆y) for each class. For instance, ImageNet has K = 1, 000 5
whereas iNaturalist has K = 8, 142 classes resulting in high-dimensional hyperparameters. In our experiments, we found that directly optimizing over such large spaces leads to convergence issues likely because of the difﬁculty of hypergradient estimation. Additionally, with large number of hyperparameters there is increased concern for validation overﬁtting. This is especially so for the tail classes (e.g. the smallest class in CIFAR100-LT has only 1 validation example with an 80-20% split). On the other hand, it is well-known in AutoML literature (e.g. neural architecture search [45],
AutoAugment [12]) that designing a good search space is critical for attaining faster convergence and good validation accuracy. To this end, we propose subspace-based search spaces for hyperparameters (wy, ly, ∆y). To explain the idea, consider the logit-adjustment parameters l = [l1 . . . lK] and
∆ = [∆1 . . . ∆K]. We propose representing these K dimensional vectors via K (cid:48) < K dimensional embeddings l(cid:48), ∆(cid:48) as follows
∆ = Dπ∆(cid:48) and l = Dπl(cid:48) where l(cid:48), ∆(cid:48) ∈ RK(cid:48)
.
Here, Dπ ∈ RK×K(cid:48) is a frequency-aware dictionary matrix that we design, and the range space of Dπ becomes the hyperparameter search space. In our algorithm, we cluster the classes in terms of their frequency and assign the same hyperparameter to classes with similar frequencies. To be concrete, if each cluster has size C, then K (cid:48) = (cid:100)K/C(cid:101). For CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist, we use C = {1, 10, 20, 40} respectively. In this scheme, each column d of the matrix Dπ is the indicator function of one of the K (cid:48) clusters, i.e. di = 1 if ith class is within the cluster and 0 otherwise. Clusters are pictorially illustrated in Fig. 3a. Finally, we remark that the speciﬁc hyperparameter choices of CDT [67], LA [53], and VS [38] losses in the corresponding papers, can be viewed as speciﬁc instances of the above search space design. For instance, LA-loss chooses a scalar τ and sets li = τ log(πi). This corresponds to a dictionary containing a single column d ∈ RK with entries di = log(πi).
Why is train-validation split critical? In Figure 4 we plot the balanced errors of training/validation/test datasets at each epoch of the search phase. The train-ing data is ﬁxed whereas we evaluate different val-idation set sizes. The ﬁrst ﬁnding is that training loss always overﬁts (dotted) until zero error whereas validation loss mildly overﬁts (dashed vs solid). Sec-ondly, larger validation does help improve test ac-curacy (compare solid lines). The training behavior is in line with the fact that large capacity networks can perfectly ﬁt and achieve 100% training accuracy
[18, 56, 29]. This also means that different accuracy metrics or fairness constraints can be perfectly satis-ﬁed. To truly ﬁnd a model that lies on the Pareto-front of the (accuracy, fairness) tradeoff, the optimization procedure should (approximately) evaluate on the population loss. Thus, as in Figure 4, the validation phase provides this crucial test-proxy in the overpa-rameterized setting where training error is vacuous. Following the model selection literature [32, 33], the intuition is that, as the dimensionality of the hyper-parameter α is typically smaller than the validation size nV , validation loss will not overﬁt and will be indicative of the test even if the training loss is zero. Our search space design in Section 2.2 also helps to this end by increasing the over-sampling ratio nV /dim(α) via class clustering. In the appendix, we formalize these intuitions for multi-objective problems (e.g., accuracy + fairness). Under mild assumptions, we show that a small amount of validation data is sufﬁcient to ensure that the Pareto-front of the validation risk uniformly approximates that of the test risk. Concretely, for two objectives (L1, L2), uniformly over all λ, the hyperparameter α minimizing the validation risk LSV in (2.3) also approximately minimizes the test risk Lfair(f ).
Figure 4: Train/Validation/Test errors during
CIFAR10-LT search phase with different vali-dation sizes and ﬁxed training size.
Solid: Test error
Dashed:Validation err
Dotted: Training error fair (f ) = (1 − λ)LSV 1 + λLSV
Epoch r o r r
E 2 3 Evaluations for Imbalanced Classes
In this section, we present our experiments on various datasets (CIFAR-10, CIFAR-100, iNaturalist-2018 and ImageNet) when the classes are imbalanced. The goal is to understand whether our bilevel optimization can design effective loss functions that improve balanced error Ebal on the test set. The setup is as follows. Ebal is the test objective. The validation loss Lfair is the balanced cross-entropy 6
Method
CIFAR10-LT CIFAR100-LT
ImageNet-LT iNaturalist
Cross-Entropy
LDAM loss [8]
LA loss (τ = 1) [53]
CDT loss [67]
AutoBalance: τ of LA loss
AutoBalance: l
AutoBalance: ∆
AutoBalance: ∆&l
AutoBalance: ∆&l, LA init 30.45 26.37 23.13 20.73 21.82 23.02 22.59 21.39 21.15 62.69 59.47 58.96 57.26 58.68 58.71 58.40 56.84 56.70 55.47 54.21 52.46 53.47 52.39 52.60 53.02 51.74 50.91 39.72 35.63 34.06 34.46 34.19 34.35 34.37 33.41 33.25
Table 1: Evaluations of balanced accuracy on long-tailed data. Algo. 1 with ∆&l design space and
LA initialization (bottom row) outperforms most of the baselines, across various datasets.
CEbal. We consider various designs for (cid:96)train such as individually tuning w, l, ∆ and augmentation.
We report the average result of 3 random experiments under this setup. 0/n(cid:48) 0) to the smallest class (n(cid:48)
Datasets. We follow previous works [53, 13, 8] to construct long-tailed versions of the datasets.
Speciﬁcally, for a K-class dataset, we create a long-tailed dataset by reducing the number of examples per class according to the exponential function n(cid:48) i = niµi, where ni is the original number of examples for class i, n(cid:48) i is the new number of examples per class, and µ < 1 is a scaling factor.
Then, we deﬁne the imbalance factor ρ = n(cid:48)
K, which is the ratio of the number of examples in the largest class (n(cid:48)
K). For the CIFAR10-LT and CIFAR100-LT dataset, we construct long-tailed versions of the datasets with imbalance factor ρ = 100. ImageNet-LT contains 115,846 training examples and 1,000 classes, with imbalance factor ρ = 256. iNaturalist-2018 contains 435,713 images from 8,142 classes, and the imbalance factor is ρ = 500. These choices follow that of [53]. For all datasets, we split the long-tailed training set into 80% training and 20% validation during the search phase (Figure 1b).
Implementation. In both CIFAR datasets, the lower-level optimization trains a ResNet-32 model with standard mini-batch stochastic gradient decent (SGD) using learning rate 0.1, momentum 0.9, and weight decay 1e − 4, over 300 epochs. The learning rate decays at epochs 220 and 260 with a factor 0.1. The upper-level hyper-parameter optimization computes the hyper-gradients via implicit differentiation. Because the hyper-gradient is mostly meaningful when the network achieves near zero loss (Thm 1 of [46]), we start the validation optimization after 120 epochs of the training optimization, using SGD with initial learning rate 0.05, momentum 0.9, and weight decay 1e − 4, we follow the same learning rate decay at epoch 220 and 260. For CIFAR10-LT, 20 hyper-parameters are trained, corresponding to ly and ∆y of each 10 classes. For CIFAR100-LT, ImageNet-LT, and iNaturalist, we reduce the search space with cluster sizes of 10, 20, and 40 as visualized in Figure 3(a) and as described in Section 2.2. For ImageNet-LT and iNaturalist, following previous work [53], we use
ResNet-50 and SGD for the lower and upper optimizations, For the learning rate scheduling, we use cosine scheduling starting with learning rate 0.05, and batch size 128. In searching phase, we conduct 150 epoch training with 40 epoch warm-up before the loss function design starts. For the retraining phase, we train for 90 epochs, which is the same as [53] but only due to the lack of training resources we change the batch size to 128 and adjust initial learning rate accordingly as suggested by [22].
Personalized Data Augmentation (PDA). For PDA, we utilize the AutoAugment [12] policy space and apply a bilevel search for the augmentation policy. Our approach follows existing differentiable augmentation strategies (e.g, [24]); however, we train separate policies for each class cluster to ensure that the resulting policies can adjust to class frequencies. Due to space limitations, please see supplementary materials for further details.
Results and discussion. We compared our methods with the state-of-the-art long-tail learning methods. Table 1 shows the results of our experiments where the design space is parametric CE (2.1). In the ﬁrst part of the table, we conduct experiments for three baseline methods: normal CE,
LDAM [8] and Logit Adjustment loss with temperature parameter τ = 1 [53]. The latter choice guarantees Fisher consistency. In the second part of the Table 1, we study Algo. 1 with design spaces l, ∆, and l&∆. The ﬁrst version of Algo. 1 in Table 1 tunes the LA loss parameter τ where l is parameterized by a single scalar τ as ly = τ log(πy). The next three versions of Algo. 1 consider tuning l, ∆, l&∆ respectively (Figure 3b-d shows the evolution of the l and ∆ parameters during the optimization). Finally, in last version of Algo. 1, the loss design is initialized with LA loss with τ = 1 (rather than balanced CE). The takeaway from these results is that our approach consistently leads to 7
a superior balanced accuracy objective. That said, tuning the LA loss alone is highly competitive with optimizing ∆ and l alone (in fact, strictly better for CIFAR10-LT, indicating Algo. 1 does not always converge to the optimal design). Importantly, when combining l&∆, our algorithm is able to design a better loss function and outperform all rows across all benchmarks. Finally, when the algorithm further is initialized with LA loss, the performance further improves accuracy, demonstrating that warm-starting with good designs improves performance.
Method
CIFAR10-LT CIFAR100-LT ImageNet-LT
MADAO [24]
AutoBalance: PDA
AutoBalance: ∆&l
AutoBalance: PDA, ∆&l
In Table 2, we study the beneﬁts of data augmentation, following our intu-itions from Lemma 2. We compare to the differentiable augmentation base-line of MADAO [24] which trains a single policy for the full dataset. PDA is a personalized variation of MADAO and leads to noticeable improvement across all benchmarks (most noticeably in CIFAR10-LT). More importantly, the last two lines of the table demonstrates that PDA can be synergistically combined with the parametric CE (2.1) which leads to further improvements, however, we observe that most of the improvement can be attributed to (2.1).
Table 2: The evaluations on personalized data optimization. 24.39 22.53 21.39 20.76 55.31 54.47 51.74 51.50 59.10 58.55 56.84 56.49 4 Approaches and Evaluations for Imbalanced Groups
While Section 3 focuses on the fundamental challenge of balanced error minimization, a more ambitious goal is optimizing generic fairness-seeking objectives. In this section, we study accuracy-fairness tradeoffs by examining the group-imbalanced setting. (cid:88) Setting: Imbalanced groups. For the setting with G groups, dataset is given by S = (xi, yi, gi)n i=1 where gi ∈ [G] is the group-membership. In the fairness literature, groups represent sensitive or protected attributes. For (x, y, g) ∼ D, deﬁne the group and (class, group) frequencies as
¯πj = PD(g = j), and πk,j = PD(y = k, g = j), for (k, j) ∈ [K] × [G].
The group-imbalance occurs when group or (class, group) frequencies differ, i.e., maxj∈[G] ¯πj (cid:29) minj∈[G] ¯πj or max(k,j) πk,j (cid:29) min(k,j) πk,j. A typical goal is ensuring that the prediction of the model is independent of these attributes. While many fairness metrics exist, in this work, we focus on the Difference of Equal Opportunity (DEO) [23, 17]. Our evaluations also focus on binary classiﬁcation (with labels denoted via ±) and two groups (K = G = 2). With this setup, the DEO risk is deﬁned as Ldeo(f ) = |L+,1(f ) − L+,2(f )|. Here Lk,j(f ) is the (class, group)-conditional risk evaluated on the conditional distribution of “Class k & Group j”. When both classes are equally relevant (rather than y = +1 implying a semantically positive outcome), we use the symmetric DEO:
Ldeo(f ) = |L+,1(f ) − L+,2(f )| + |L−,1(f ) − L−,2(f )|. (4.1) k=1 (cid:80)G bal(f ) = 1
KG
We will study the pareto-frontiers of the DEO (4.1), group-balanced error, and standard error. Here, (cid:80)K group-balanced risk is deﬁned as LG j=1 Lk,j(f ). Note that this deﬁnition treats each (class, group) pair as its own (sub)group. Throughout, we explicitly set the validation loss to cross-entropy for clarity, thus we use CE, CEG bal, CEdeo to refer to L, LG
In Algo. 1, we set Lfair = (1 − λval) · CE +
Validation (upper-level) loss function.
λval · CEdeo for varying 0 ≤ λval ≤ 1. The parameter λval enables a trade-off between accuracy and fairness. Within λval, we use the subscript “val” to highlight the fact that we regularize the validation objective rather than the training objective.
Group-sensitive training loss design. As ﬁrst proposed in [38], the parametric cross-entropy (CE) can be extended to (class, group) imbalance by extending hyper-parameter α to [K] × [G] variables w, l, ∆ ∈ R[K]×[G] generalizing (2.1), (2.2). This leads us to the following parametric loss function for group-sensitive classiﬁcation 1 bal, Ldeo. (cid:96)train(y, g, f (x); α) = −wyg log (cid:32) eσ(∆yg)fy(x)+lyg k∈[K] eσ(∆kg)fk(x)+lkg (cid:80) (cid:33)
. (4.2)
Here, wyg applies weighted CE, while ∆yg and lyg are logit adjustments for different (class, groups).
This loss function is used throughout the imbalanced groups experiments. 1This is a generalization to multiple classes of the proposal in [38] for binary group-sensitive classiﬁcaiton. 8
Loss function
Cross entropy (CE)
CEG bal
Group-LA loss
CEdeo 0.1 · CE + 0.9 · CEdeo (λ = 0.1)
DRO [58]
AutoBalance: Lfair with λval = 0.1
Balanced Error Worst (class, group) error DEO 33.75 20.25 29.33 25.25 26.25 6.91 4.25 23.38 20.83 22.83 19.29 20.06 16.47 15.13
Table 3: Comparison of fairness metrics for group-imbalanced experiments. The ﬁrst six rows are different training loss choices, where CEG bal, Group-LA, CEdeo, and DRO promote group fairness. The last row is Algo. 1, which designs training loss for the validation loss choice of 0.1 · CE + 0.9 · CEdeo.
We note that, DEO can be trivially minimized by always predicting the same class. To avoid this, we use a mild amount of CE loss with λval = 0.1 in Algo. 1. 43.25 36.67 40.50 35.17 31.67 32.67 30.33
Baselines. We will compare Algo. 1 with training loss functions parameterized via (1−λ)·CE+λ·Lreg.
Here Lreg is a fairness-promoting regularization. Speciﬁcally, as displayed in Table 3 and Figure 5, we will set Lreg to be CEG bal, CEdeo and Group LA. “Group LA” is a natural generalization of the LA loss to group-sensitive setting; it chooses weights wg = 1/ ¯πg to balance group frequencies and then applies logit-adjustment with τ = 1 over the classes conditioned on the group-membership.
Datasets. We experiment with the modiﬁed Waterbird dataset [58]. The goal is to correctly classify the bird type despite the spurious correlations due to the image background. The distribution of the original data is as follows. The binary classes k ∈ {−, +} correspond to {waterbird, landbird}, and the groups [G] = {1, 2} correspond to {land background, water background}. The fraction of data in each (class, group) pair is π−,2 = 0.22, π−,1 = 0.012, π+,2 = 0.038, and π+,1 = 0.73. The landbird on the water background ({+, 2}) and the waterbird on the land background ({−, 1}) are minority sub-groups within their respective classes. The test set, following [58], has equally allocated bird types on different backgrounds, i.e., π±,j = 0.25. As the test dataset is balanced, the standard classiﬁcation error E(f ) is deﬁned to be the weighted error E(f ) = πy,gEy,g(f ).
Implementation. We follow the feature extraction method from [58], where xi are 512-dimensional
ResNet18 features. When using Algo. 1, we split the original training data into 50% training and 50% validation. The search phase uses 150 epochs of warm up followed by 350 epochs of bilevel optimization. The remaining implementation details are similar to Section 3.
Results and discussion. We consider var-ious fairness-related metrics, including the worst (class, group) error, DEO Edeo(f ) and the balanced error E G bal(f ).We seek to under-stand whether AutoBalance algorithm can im-prove performance on the test set compared to the baseline training loss functions of the form (1 − λ) · CE + λ · Lreg. In Figure 5, we show the inﬂuence of the parameter λ where Lreg is chosen to be CEdeo, CEG bal, or Group-LA (each point on the plot represents a different
λ value). As we sweep across values of λ, there arises a tradeoff between standard clas-siﬁcation error E(f ) and the fairness metrics.
We observe that Algo. 1 signiﬁcantly Pareto-dominates alternative approaches, for example achieving lower DEO or balanced error for the same standard error. This demonstrates the value of automatic loss function design for a rich class of fairness-seeking objectives.
Figure 5: Waterbirds fairness-accuracy tradeoffs for parametrized loss designs (1 − λ) · CE + λ · Lreg, for different Lreg choices. Group-balanced error E G(f ) (left) and DEO Edeo(f ) (right) are plotted as a function of the misclassiﬁcation error E(f ). Algo. 1 exhibits a noticeably better tradeoff curve as it uses a DEO-based validation objective to design an optimized training loss function.
Standard classiﬁcation error
Standard classiﬁcation error d e c n n a l a
B
O
E
D r o r r e
Next, in Table 3, we solely focus on optimizing the fairness objectives (rather than standard error).
Thus, we compare against CEdeo, CEG bal, Group-LA as the baseline approaches as well as blending
CE with λ = 0.1. We also compare to the DRO approach of [58]. Finally, we display the outcome of
Algo. 1 with λval = 0.1. While DRO is competitive, similar to Figure 5, our approach outperforms all baselines for all metrics. The improvement is particularly signiﬁcant when it comes to DEO. Finally, we remark that [38] further proposed combining DRO with the group-adjusted VS-loss for improved performance. We leave the evaluation of AutoBalance for such combinations to future. 9
5