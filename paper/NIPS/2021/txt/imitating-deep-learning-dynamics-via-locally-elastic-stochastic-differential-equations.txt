Abstract
Understanding the training dynamics of deep learning models is perhaps a necessary step toward demystifying the effectiveness of these models. In particular, how do data from different classes gradually become separable in their feature spaces when training neural networks using stochastic gradient descent? In this study, we model the evolution of features during deep learning training using a set of stochastic differential equations (SDEs) that each corresponds to a training sample. As a crucial ingredient in our modeling strategy, each SDE contains a drift term that reﬂects the impact of backpropagation at an input on the features of all samples.
Our main ﬁnding uncovers a sharp phase transition phenomenon regarding the intra-class impact: if the SDEs are locally elastic [19] in the sense that the impact is more signiﬁcant on samples from the same class as the input, the features of the training data become linearly separable, meaning vanishing training loss; otherwise, the features are not separable, regardless of how long the training time is.
Moreover, in the presence of local elasticity, an analysis of our SDEs shows that the emergence of a simple geometric structure called the neural collapse of the features.
Taken together, our results shed light on the decisive role of local elasticity in the training dynamics of neural networks. We corroborate our theoretical analysis with experiments on a synthesized dataset of geometric shapes and CIFAR-10. 1

Introduction
Deep learning models have achieved signiﬁcant empirical success over the past decade across a wide spectrum of domains spanning computer vision, natural language processing, and reinforcement learning [31, 43, 48]. Despite these remarkable achievements at the empirical level, there is still much to learn about deep neural networks, as evidenced by the fact that almost all important advances concerning architecture design and optimization for deep learning are based on heuristics, without much input from a theoretical perspective [20, 11, 21, 27].
An important step toward opening these black-box models and unveiling their formidable details is to quantitatively understand the impact of backpropagation in deep learning training. While there has been a continued effort to demystify how simple optimization methods give rise to impressive generalization performance, for example, [49, 26, 4], this is by no means an easy problem, perhaps because of the daunting nonconvex nature of neural networks. Accordingly, for near-term purposes, a more practical approach is to take a phenomenological viewpoint by relating simple empirical patterns to the effectiveness of deep learning models.
In this spirit, we are interested in how data from different classes gradually become separable in their feature space by repetitively calling backpropagation. From a phenomenological viewpoint, this question can be addressed by ﬁrst analyzing the impact of a single update using a stochastic gradient on the performance of the neural networks. More precisely, imagine that the gradient is evaluated in an image of a cat, how does the hidden representation of another image—say, an image of another 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) GEOMNIST in R3. (b) GEOMNIST in R. (c) CIFAR in R3. (d) CIFAR in R.
Figure 1: Separation of features (logits). GEOMNIST dataset ((a)—(b))) and CIFAR dataset ((c)—(d)) trained using K = 3 classes on a variant of the ALEXNET model. Separation in R is done
R3, which is set to be the difference between a pair of class means by projecting the logits to ν
¯X k (T ) for a large T , where k, l are chosen heuristically. The dashed lines in (b) and (d) are simulated paths from Equation (9) using estimated (cid:98)α(t) and (cid:98)β(t). More details are given in Section 4 and Appendix D.2.
¯X l (T )
−
∈ cat or an image of a plane—evolves because of the backpropagation? Recent studies answer this question by introducing a phenomenon called local elasticity, which, roughly speaking, means that the impact is generally larger on a similar sample (an image of another cat) than on a dissimilar sample (an image of a plane) [19].
Motivated by the phenomenon of local elasticity, we propose a model that captures the interaction between different training samples during deep learning training using a set of stochastic differential equations (SDEs) that reﬂect local elasticity in neural networks. Characterizing the intra-class and inter-class effects is an essential component of our modeling strategy, each of which contains a drift term that imitates the impact of backpropagation on speciﬁc training data of all samples.
Our main ﬁnding uncovers a sharp phase transition phenomenon regarding the intra-class and inter-class impact; if the SDEs are locally elastic in the sense that the impact is more signiﬁcant on samples from the same class as the input (the intra-class effect is strictly greater than the inter-class effect), the features of the training data are guaranteed to be linearly separable, meaning vanishing training loss; otherwise, the features are not separable, no matter how long the training time is. This result provides convincing theoretical evidence for the presence of local elasticity in deep learning [19].
Our model is also quite accurate in simulating the feature dynamics of deep learning. As shown in
Figure 1 and detailed in Section 4, the dynamics of the simulated logits are reasonably close to the real dynamics of deep learning on both synthetic and real datasets, indicating a well-suited model for theoretical and practical purposes. Moreover, in the presence of local elasticity, our SDEs also predict the emergence of a simple geometric structure called neural collapse of features [38].
Taken together, our results shed light on the decisive role of local elasticity in the training dynamics of neural networks. We corroborate our theoretical analysis with experiments on a synthetic dataset of geometric shapes, as well as on CIFAR-10. The experimental evidence consistently supported our model, which provides new insights into the dynamics of deep learning training. 1.1