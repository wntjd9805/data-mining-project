Abstract
Many reinforcement learning (RL) environments consist of independent entities that interact sparsely. In such environments, RL agents have only limited inﬂuence over other entities in any particular situation. Our idea in this work is that learning can be efﬁciently guided by knowing when and what the agent can inﬂuence with its actions. To achieve this, we introduce a measure of situation-dependent causal inﬂuence based on conditional mutual information and show that it can reliably detect states of inﬂuence. We then propose several ways to integrate this measure into RL algorithms to improve exploration and off-policy learning. All modiﬁed algorithms show strong increases in data efﬁciency on robotic manipulation tasks. 1

Introduction
Reinforcement learning (RL) is a promising route towards versatile and dexterous artiﬁcial agents.
Learning from interactions can lead to robust control strategies that can cope with all the intricacies of the real world that are hard to engineer correctly. Still, many relevant tasks such as object manipulation pose signiﬁcant challenges for RL. Although impressive results have been achieved using simulation-to-real transfer [1] or heavy physical parallelization [2], training requires countless hours of interaction. Improving sample efﬁciency is thus a key concern in RL. In this paper, we approach this issue from a causal inference perspective.
When is an agent in control of its environment? An agent can only inﬂuence the environment by its actions. This seemingly trivial observation has the underappreciated aspect that the causal inﬂuence of actions is situation dependent. Consider the simple scenario of a robotic arm in front of an object on a table. Clearly, the object can only be moved when contact between the robot and object is made.
Generally, there are situations where immediate causal inﬂuence is possible, while in others, none is.
In this work, we formalize this situation-dependent nature of control and show how it can be exploited to improve the sample efﬁciency of RL agents. To this end, we derive a measure that captures the causal inﬂuence of actions on the environment and devise a practical method to compute it.
Knowing when the agent has control over an object of interest is important both from a learning and an exploration perspective. The learning algorithm should pay particular attention to these situations because (i) the robot is initially rarely in control of the object of interest, making training inefﬁcient, (ii) physical contacts are hard to model, thus require more effort to learn and (iii) these states are enabling manipulation towards further goals. But for learning to take place, the algorithm ﬁrst needs data that contains these relevant states. Thus, the agent has to take its causal inﬂuence into account already during exploration.
We propose several ways in which our measure of causal inﬂuence can be integrated into RL algorithms to address both the exploration, and the learning side. For exploration, agents can be rewarded with a bonus for visiting states of causal inﬂuence. We show that such a bonus leads the agent to quickly discover useful behavior even in the absence of task-speciﬁc rewards. Moreover, 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
our approach allows to explicitly guide the exploration to favor actions with higher predicted causal impact. This works well as an alternative to (cid:15)-greedy exploration, as we demonstrate. Finally, for learning, we propose an off-policy prioritization scheme and show that it reliably improves data efﬁciency. Each of our investigations is backed by empirical evaluations in robotic manipulation environments and demonstrates a clear improvement of the state-of-the-art with the same generic inﬂuence measure. 2