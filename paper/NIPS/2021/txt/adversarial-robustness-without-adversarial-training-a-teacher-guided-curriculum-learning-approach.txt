Abstract
Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves signiﬁcant performance gains with a little extra effort (10-20%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy.
We have performed extensive experimentation with CIFAR-10, CIFAR-100, and
TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.

Introduction 1
Ever since deep neural network (DNN) models have emerged as the de facto technique to be applied for many vision problems, adversarial robustness has emerged as a critical need. Goodfellow et al.
[1] identiﬁed this serious issue to show very different predictive behavior of a DNN model with similar looking images. Many efforts have followed since either to come up with fooling techniques
[1, 2, 3, 4, 5, 6] or defend deep models against them [3, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]. Nevertheless, both these research directions are important at this moment and require more attention. In this work, our effort lies on devising an adversarially robust training technique.
Adversarial training (AT) [3] is the most widely used method for adversarial robustness and most of the improvements have since come by adding regularizers without changing the min-max formulation.
The regularizers are added either in the inner maximization [27, 8, 15] or outer minimization [19, 14] term. Though AT-based methods are shown to perform well, they incur additional cost due to an iterative inner maximization step. Our proposed robust training method circumvents this fundamental issue.
With an objective of not following the costly min-max optimization of AT, we revisit adversarial robustness in terms of standard model training. Adversarially robust models are shown to satisfy
∗equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
better alignment between saliency and object features in [28]. We enforce such alignment through training to achieve model robustness. This is accomplished by forcing saliency of the main model to follow the object features, provided by saliency of a pre-trained reference model. We hypothesize that perturbing only the most discriminative part of an image should trigger a model to change its decision about that image. In other words, perturbing other non-important pixels shouldn’t affect model decision, and such a model can achieve adversarial robustness. This approach is achieved by progressively narrowing the object-discriminative region according to its importance in a curriculum learning sense and forcing an adversary to perturb only those pixels while changing model’s decision.
A model trained with this approach restricts the perturbation only to the object pixels when attacked.
This diminishes the range of possible perturbations and limits the attack strength, reducing the chance of the model to change its decision for the perturbed image. Being a non-iterative method, our approach not only takes considerably less time for training compared to AT, but also outperforms iterative methods (such as [3, 7, 8, 27, 16, 13, 14, 11, 12, 17, 15]) and non-iterative methods (such as
[29, 10, 9, 18]) signiﬁcantly in both natural and adversarial accuracies.
Our key contributions are summarized as below.
• We propose a non-iterative novel robust training method, which outperforms recently proposed
SOTA techniques, irrespective of their type being iterative or non-iterative, in terms of both natural and adversarial accuracies against a wide range of attacks.
• Being a non-iterative method makes it easily applicable to any large dataset to achieve an adver-sarially robust model. Our method takes 10-20% of time compared to any adversarial training technique.
• Our method attains a clean accuracy which is much closer to the performance of naturally trained models compared to other robust models.
• We perform extensive experimentation on CIFAR-10, CIFAR-100, TinyImageNet datasets and report comparative results against all other, including recently proposed, adversarial robustness techniques. We also present various studies in detail to analyze the effectiveness of our method. 2