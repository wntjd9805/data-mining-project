Abstract
Neural networks are known to be vulnerable to adversarial attacks – slight but carefully constructed perturbations of the inputs which can drastically impair the network’s performance. Many defense methods have been proposed for improving robustness of deep networks by training them on adversarially perturbed inputs.
However, these models often remain vulnerable to new types of attacks not seen during training, and even to slightly stronger versions of previously seen attacks.
In this work, we propose a novel approach to adversarial robustness, which builds upon the insights from the domain adaptation ﬁeld. Our method, called Adversarial
Feature Desensitization (AFD), aims at learning features that are invariant towards adversarial perturbations of the inputs. This is achieved through a game where we learn features that are both predictive and robust (insensitive to adversarial attacks), i.e. cannot be used to discriminate between natural and adversarial data.
Empirical results on several benchmarks demonstrate the effectiveness of the proposed approach against a wide range of attack types and attack strengths. Our code is available at https://github.com/BashivanLab/afd. 1

Introduction
When training a classiﬁer, it is common to assume that the training and test samples are drawn from the same underlying distribution. In adversarial machine learning, however, this assumption is intentionally violated by using the classiﬁer itself to perturb the samples from the original (natural) data distribution towards a new distribution over which the classiﬁer’s error rate is increased [52].
As expected, when tested on such adversarially generated input distribution, the classiﬁer severely underperforms. To date, various methods have been proposed to defend the neural networks against adversarial attacks [34, 2], additive noise patterns and corruptions [24, 25, 45], and transformations
[17]. Among these methods, two of the most successful adversarial defense methods to date are adversarial training [34], which trains the neural network with examples that are perturbed to maximize the loss on the target model, and TRADES [57], which regularizes the classiﬁer to push the decision boundary away from the data. While past adversarial defence methods have successfully improved the neural network robustness against adversarial examples, it has also been shown that these robust networks remain susceptible to even slightly larger adversarial perturbations or other forms of attacks [19, 46, 48].
In this paper, we propose to view the problem of adversarial robustness through the lens of domain adaptation, and to consider distributions of natural and adversarial images as distinct input domains 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
that a classiﬁer is expected to perform well on. We then focus our attention on learning features that are invariant under such domain shifts. Building upon domain adaptation literature [4], we use the classiﬁcation-based H∆H-divergence to quantify the distance between the natural and adversarial domains. The theory of domain adaptation allows us to formulate a bound on the adversarial classiﬁcation error (i.e. the error under the distribution of adversarial examples) in terms of the classiﬁcation error on natural images and the divergence between the natural and adversarial features.
We further propose an algorithm for minimizing the adversarial error using this bound. For this, we train a classiﬁer and a domain discriminator to respectively minimize their losses on the label classiﬁcation and domain discrimination tasks. The feature extractor is trained to minimize the label classiﬁer’s loss and maximise the discriminator’s loss. In this way, the feature extractor network is encouraged to learn features that are both predictive for the classiﬁcation task and insensitive to the adversarial attacks. The proposed setup is conceptually similar to prior work in adversarial domain adaptation [18, 53], where domain-invariant features are learned through an adversarial game between the domain discriminator and a feature extractor network.
This setup is similar to the adversarial learning paradigm widely used in image generation and transformation [20, 28, 60], unsupervised and semi-supervised learning [39], video prediction [35, 31], active learning [47], and continual learning [16]. Some prior work have also considered adversarial learning to tackle the problem of adversarial examples [54, 36, 9, 8]. These methods used generative models to learn the distribution of the adversarial images[54, 36], or to learn the distribution of input gradients[9, 8]. Unlike our method which learns a discriminator function between distributions of adversarial and natural features and updates the feature extractor to reduce the discriminability of those distributions.
The main contributions of this work are as follows:
• We apply domain-adaptation theory to the problem of adversarial robustness; this allows to bound the adversarial error in terms of the error on the natural inputs and the divergence between the feature (representation) distributions of adversarial and natural domains.
• Aiming to minimize this bound, we propose a method which learns adversarially robust features that are both predictive and insensitive to adversarial attacks, i.e. cannot be used to discriminate between natural and adversarial data.
• We empirically demonstrate the effectiveness of the proposed method in learning robust models against a wide range of attack types and attack strengths, and show that our proposed approach often signiﬁcantly outperforms most previous defense methods. 2