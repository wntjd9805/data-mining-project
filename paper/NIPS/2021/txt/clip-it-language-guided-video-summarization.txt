Abstract
A generic video summary is an abridged version of a video that conveys the whole story and features the most important scenes. Yet the importance of scenes in a video is often subjective, and users should have the option of customizing the summary by using natural language to specify what is important to them. Further, existing models for fully automatic generic summarization have not exploited available language models, which can serve as an effective prior for saliency. This work introduces CLIP-It, a single framework for addressing both generic and query-focused video summarization, typically approached separately in the literature. We propose a language-guided multimodal transformer that learns to score frames in a video based on their importance relative to one another and their correlation with a user-deﬁned query (for query-focused summarization) or an automatically generated dense video caption (for generic video summarization). Our model can be extended to the unsupervised setting by training without ground-truth supervision.
We outperform baselines and prior work by a signiﬁcant margin on both standard video summarization datasets (TVSum and SumMe) and a query-focused video summarization dataset (QFVS). Particularly, we achieve large improvements in the transfer setting, attesting to our method’s strong generalization capabilities.

Introduction 1
An effective video summary captures the essence of the video and provides a quick overview as an alternative to viewing the whole video; it should be succinct yet representative of the entire video.
Summarizing videos has many use cases - for example, viewers on YouTube may want to watch a 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
short summary of the video to assess its relevance. While a generic summary is useful for capturing the important scenes in a video, it is even more practical if the summary can be customized by the user. As seen in Fig. 1, users should be able to indicate the concepts of the video they would like to see in the summary using natural language queries.
Generic video summarization datasets such as TVSum [36] and SumMe [7] provide ground-truth annotations in the form of frame or shot-level importance scores speciﬁed by multiple annotators.
Several learning-based methods reduce the task to a frame-wise score prediction problem. Sequence labeling methods [5, 42, 43, 44, 18] model variable-range dependencies between frames but fail to capture relationships across all frames simultaneously. While attention [3] and graph based [26] methods address this partly, they disregard ordering of the input frames which is useful when predicting scores. Moreover, these methods use only visual cues to produce the summaries and cannot be customized with a natural language input. Another line of work, query-focused video summarization [32], allows the users to customize the summary by specifying a query containing two concepts (eg., food and drinks). However, in their work the query can only be chosen from a ﬁxed set of predeﬁned concepts which limits the ﬂexibility for the user.
It is important to note that efforts in generic and query-focused video summarization have so far been disjoint, with no single method for both. Our key innovation is to unify these tasks in one language-guided framework. We introduce CLIP-It, a multimodal summarization model which takes two inputs, a video and a natural language text, and synthesizes a summary video conditioned on the text. In case of generic video summarization, the natural language text is a video description obtained using an off-the-shelf dense video captioning method. Alternatively, in the case of query-focused video summarization, the language input is a user-deﬁned query. Unlike existing generic methods which only use visual cues, we show that adding language as an input leads to the “discovery” of relevant concepts and actions resulting in better summaries. Our method uses a Transformer with positional encoding, which can attend to all frames at once (unlike LSTM based methods) and also keep track of the ordering of frames (unlike graph based methods). In contrast to existing query-focused methods [32] which only allow keyword based queries, we aim to enable open-ended natural language queries for users to customize video summaries. For example, as seen in Fig. 1, using our method users can specify long and descriptive queries such as, “All water bodies such as lakes, rivers, and waterfalls” which is not possible with previous methods.
Given an input video, CLIP-It generates a video summary guided by either a user-deﬁned natural
It uses a Language-Guided Attention head language query or a system generated description. to compute a joint representation of the image and language embeddings, and a Frame-Scoring
Transformer to assign scores to individual frames in the video using the fused representations.
Following [43, 42], the summary video is constructed from high scoring frames by converting frame-level scores to shot-level scores and using knapsack algorithm to ﬁt the maximum number of high scoring shots in a timed window. Fig. 1 shows an output from our method. Given an hour long video of a national park tour, we generate a 2 minute generic summary comprising of all the important scenes in the video. Given the two language queries, our method picks the matching keyframes in both cases. We can train CLIP-It without ground-truth supervision by leveraging the reconstruction and diversity losses [9, 31]. For generic video summarization, we evaluate our approach on the standard benchmarks, TVSum and SumMe. We achieve performance improvement on F1 score of nearly 3% in the supervised setting and 4% in the unsupervised setting on both datasets. We show large gains (5%) in the Transfer setting, where CLIP-It is trained and evaluated on disjoint sets of data. For the query-focused scenario we evaluate on the QFVS dataset [33], where we also achieve state-of-the-art results.
To summarize our contributions, we introduce CLIP-It, a language-guided model that uniﬁes generic and query-focused video summarization. Our approach uses language conditioning in the form of off-the-shelf video descriptions (for generic summarization) or user-deﬁned natural language queries (for query-focused summarization). We show that the Transformer design enables effective contextualization across frames, beneﬁting our tasks. We also demonstrate the impact of language guidance on generic summarization. Finally, we establish the new state-of-the-art on both generic and query-focused datasets in supervised and unsupervised settings. 2