Abstract
In this paper, we apply the self-attention from the state-of-the-art Transformer in
Attention Is All You Need [88] for the ﬁrst time to a data-driven operator learning problem related to partial differential equations. An effort is put together to explain the heuristics of, and to improve the efﬁcacy of the attention mechanism. By employing the operator approximation theory in Hilbert spaces, it is demonstrated for the ﬁrst time that the softmax normalization in the scaled dot-product attention is sufﬁcient but not necessary. Without softmax, the approximation capacity of a linearized Transformer variant can be proved to be comparable to a Petrov-Galerkin projection layer-wise, and the estimate is independent with respect to the sequence length. A new layer normalization scheme mimicking the Petrov-Galerkin projec-tion is proposed to allow a scaling to propagate through attention layers, which helps the model achieve remarkable accuracy in operator learning tasks with unnor-malized data. Finally, we present three operator learning experiments, including the viscid Burgers’ equation, an interface Darcy ﬂow, and an inverse interface coefﬁcient identiﬁcation problem. The newly proposed simple attention-based operator learner, Galerkin Transformer, shows signiﬁcant improvements in both training cost and evaluation accuracy over its softmax-normalized counterparts. 1

Introduction
Partial differential equations (PDEs) arise from almost every multiphysics and biological systems, from the interaction of atoms to the merge of galaxies, from the formation of cells to the change of climate. Scientists and engineers have been working on approximating the governing PDEs of these physical systems for centuries. The emergence of the computer-aided simulation facilitates a cost-friendly way to study these challenging problems. Traditional methods, such as ﬁnite el-ement/difference [20, 22], spectral methods [12], etc., leverage a discrete structure to reduce an inﬁnite dimensional operator map to a ﬁnite dimensional approximation problem. Meanwhile, in the
ﬁeld practice of many scientiﬁc disciplines, substantial data for PDE-governed phenomena available on discrete grids enable modern black-box models like Physics-Informed Neural Network (PINN)
[71, 62, 49] to exploit measurements on collocation points to approximate PDE solutions.
Nonetheless, for traditional methods or data-driven function learners such as PINN, given a PDE, the focus is to approximate a single instance, for example, solving for an approximated solution for one coefﬁcient with a ﬁxed boundary condition. A slight change to this coefﬁcient invokes a potentially expensive re-training of any data-driven function learners. In contrast, an operator learner aims to learn a map between inﬁnite-dimensional function spaces, which is much more difﬁcult yet rewarding. A well-trained operator learner can evaluate many instances without re-training or collocation points, thus saving valuable resources, and poses itself as a more efﬁcient approach in the long run. Data-driven resolution-invariant operator learning is a booming new research direction
[60, 5, 56, 64, 90, 57, 61, 91, 37, 74]. The pioneering model, DeepONet [60], attributes architecturally to a universal approximation theorem for operators [18]. Fourier Neural Operator (FNO) [57] notably 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
shows an awing state-of-the-art performance outclassing classic models such as the one in [100] by orders of magnitudes in certain benchmarks.
Under a supervised setting, an operator learner is trained with the operator’s input functions and their responses to the inputs as targets. Since both functions are sampled at discrete grid points, this is a special case of a seq2seq problem [81]. The current state-of-the-art seq2seq model is the Transformer ﬁrst introduced in [88]. As the heart and soul of the Transformer, the scaled dot-product attention mechanism is capable of unearthing the hidden structure of an operator by capturing long-range interactions. Inspired by many insightful pioneering work in Transformers
[50, 19, 75, 84, 96, 97, 95, 59, 76, 66], we have modiﬁed the attention mechanism minimally yet in a mathematically profound manner to better serve the purpose of operator learning.
Among our new Hilbert space-inspired adaptations of the scaled dot-product attention, the ﬁrst and foremost change is: no softmax, or the approximation thereof. In the vanilla attention [88], the softmax succeeding the matrix multiplication convexiﬁes the weights for combining different positions’ latent representations, which is regarded as an indispensable ingredient in the positive kernel interpretation of the attention mechanism [84]. However, softmax acts globally in the sequence length dimension for each row of the attention matrix, and further adds to the quadratic complexity of the attention in the classic Transformer. Theory-wise, instead of viewing “row ≈ word” in the
Natural Language Processing (NLP) tradition, the columns of the query/keys/values are seen as sampling of functions in Hilbert spaces on discretized grids. Thus, taking the softmax away allows us to verify a discrete Ladyzhenskaya–Babuška–Brezzi (LBB) condition, which further amounts to the proof that the newly proposed Galerkin-type attention can explicitly represent a Petrov-Galerkin projection, and this approximation capacity is independent of the sequence length (Theorem 4.3).
Numerically, the softmax-free models save valuable computational resources, outperforming the ones with the softmax in terms of training FLOP and memory consumption (Section 5). Yet in an ablation study, the training becomes unstable for softmax-free models (Table 8). To remedy this, a new
Galerkin projection-type layer normalization scheme is proposed to act as a cheap diagonal alternative to the normalizations explicitly derived in the proof of the Petrov-Galerkin interpretation (equation (40)). Since a learnable scaling can now be propagated through the encoder layers, the attention-based operator learner with this new layer normalization scheme exhibits better comprehension of certain physical properties associated with the PDEs such as the energy decay. Combining with other approximation theory-inspired tricks including a diagonally dominant rescaled initialization for the projection matrices and a layer-wise enrichment of the positional encodings, the evaluation accuracies in various operator learning tasks are boosted by a signiﬁcant amount.
Main contributions. The main contributions of this work are summarized as follows.
• Attention without softmax. We propose a new simple self-attention operator and its linear variant without the softmax normalization. Two new interpretations are offered, together with the approximation capacity of the linear variant proved comparable to a Petrov-Galerkin projection.
• Operator learner for PDEs. We combine the newly proposed attention operators with the current best state-of-the-art operator learner Fourier Neural Operator (FNO) [57] to signiﬁcantly improve its evaluation accuracy in PDE solution operator learning benchmark problems. Moreover, the new model is capable of recovering coefﬁcients based on noisy measurements that traditional methods or FNO cannot accomplish.
• Experimental results. We present three benchmark problems to show that operator learners using the newly proposed attentions are superior in computational/memory efﬁciency, as well as in accuracy versus those with the conventional softmax normalization. The PyTorch codes to reproduce our results are available as an open-source software. 1 2