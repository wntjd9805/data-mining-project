Abstract
In collaborative machine learning (CML), multiple agents pool their resources (e.g., data) together for a common learning task. In realistic CML settings where the agents are self-interested and not altruistic, they may be unwilling to share data or model information without adequate rewards. Furthermore, as the data/model information shared by the agents may differ in quality, designing rewards which are fair to them is important so that they would not feel exploited nor discouraged from sharing. In this paper, we adopt federated learning as the CML paradigm, propose a novel cosine gradient Shapley value (CGSV) to fairly evaluate the expected marginal contribution of each agent’s uploaded model parameter update/gradient without needing an auxiliary validation dataset, and based on the CGSV, design a novel training-time gradient reward mechanism with a fairness guarantee by sparsifying the aggregated parameter update/gradient downloaded from the server as reward to each agent such that its resulting quality is commensurate to that of the agent’s uploaded parameter update/gradient. We empirically demonstrate the effectiveness of our fair gradient reward mechanism on multiple benchmark datasets in terms of fairness, predictive performance, and time overhead. 1

Introduction
In collaborative machine learning (CML), multiple agents (e.g., researchers, organizations, compa-nies) pool their resources (e.g., data) together for a common learning task. It spans a wide variety of real-world applications such as digital healthcare [49], clinical trial research [13, 23], wake word detection for smart voice assistants [27], and next word prediction on mobile devices [15].
Federated learning (FL) provides a natural paradigm of CML [18, 29, 41, 43, 57, 62]. In FL, the agents perform local model training (e.g., using stochastic gradient descent) and share their resulting model parameter updates/gradients via a trusted server [40, 56, 59]. An important distinction of our work here from the standard FL literature is that the agents are self-interested and hence not necessarily cooperative like the worker nodes in distributed learning. The implication is that to achieve competitive predictive performance for the learning task, it is imperative to incentivize/reward the agents for contributing/sharing high-quality information in the form of model parameter up-dates/gradients [47, 48, 52].
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our work here adopts FL as the CML paradigm for designing a fair reward mechanism such that the (self-interested) agents who contribute more would not feel exploited but be rewarded commensurately. This is often regarded as fairness in cooperative game theory [42], mechanism design [4], and computational social choice [11]. To design such a fair reward mechanism, we need to address three main questions:
Firstly, what is a suitable notion of fairness? The Shapley value (SV) [50] from cooperative game theory is an appealing choice and has been used in ML [14] and FL [54, 56]. However, existing
SV-based works [19, 37, 54, 56] typically require the availability of (and all agents to agree on) an auxiliary validation dataset and signiﬁcant time overhead from evaluating the agents’ contributions in the form of SVs and the resulting model training. To overcome these difﬁculties, we propose to instead exploit the alignment (speciﬁcally, cosine similarity) of an agent’s uploaded/contributed model parameter update/gradient vector (or that aggregated over some agents) to that aggregated over all agents (hence measuring its quality/value and circumventing the need for a validation dataset [12, 52]) for devising our proposed cosine gradient Shapley value (CGSV) (Sec. 3.2) which can be efﬁciently approximated with a bounded error (Sec. 3.3).
Secondly, what is the choice of reward? Various choices such as monetary rewards from a pre-allocated budget [65, 66] or the total revenue generated from the collaboration through FL [9, 10] have been proposed. Though it may seem natural to consider monetary rewards, it is not obvious how a common denomination between money and data/gradients [1, 46] can be readily established, which makes it challenging to apply these works in practice. Instead, we propose to consider the aggregated parameter updates/gradients downloaded from the server as rewards to the agents.
Finally, how can the gradient reward mechanism ensure fairness? Our proposed mechanism exploits a sparsifying gradient trick (Sec. 3.4) for controlling the quality of the aggregated parameter up-date/gradient downloaded from the server as reward to each agent at training time (rather than post hoc [48, 52, 65]) such that its quality is commensurate to that of the agent’s uploaded/contributed parameter update/gradient [2, 7]. Consequently, an agent who uploads/contributes higher-quality parameter updates/gradients over the entire training process should eventually be rewarded with converged model parameters whose resulting training loss (and hence predictive performance) is closer to that of the server, as demonstrated in our fairness guarantee (Sec. 3.5) [52].
In summary, the contributions of our work here to CML and FL include the following:
• We propose a novel cosine gradient Shapley value (CGSV) (Sec. 3.2) to fairly evaluate the expected marginal contribution of each agent’s uploaded model parameter update/gradient without needing an auxiliary validation dataset and present an efﬁcient approximation of CGSV with a bounded error (Sec. 3.3).
• Based on the approximate CGSV, we design a novel training-time gradient reward mechanism (Sec. 3.4) with a fairness guarantee (Sec. 3.5) by exploiting the trick of sparsifying the aggregated parameter update/gradient downloaded from the server as reward to each agent such that its resulting quality is commensurate to that of the agent’s uploaded/contributed parameter update/gradient.
• We empirically demonstrate the effectiveness of our fair gradient reward mechanism on multiple benchmark datasets in terms of fairness, predictive performance, and time overhead (Sec. 4). 2