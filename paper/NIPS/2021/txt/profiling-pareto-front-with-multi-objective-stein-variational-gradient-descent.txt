Abstract
Finding diverse and representative Pareto solutions from the Pareto front is a key challenge in multi-objective optimization (MOO). In this work, we propose a novel gradient-based algorithm for proﬁling Pareto front by using Stein variational gradient descent (SVGD). We also provide a counterpart of our method based on
Langevin dynamics. Our methods iteratively update a set of points in a parallel fashion to push them towards the Pareto front using multiple gradient descent, while encouraging the diversity between the particles by using the repulsive force mechanism in SVGD, or diffusion noise in Langevin dynamics. Compared with existing gradient-based methods that require predeﬁned preference functions, our method can work efﬁciently in high dimensional problems, and can obtain more diverse solutions evenly distributed in the Pareto front. Moreover, our methods are theoretically guaranteed to converge to the Pareto front. We demonstrate the effectiveness of our method, especially the SVGD algorithm, through extensive experiments, showing its superiority over existing gradient-based algorithms. 1

Introduction
Many scientiﬁc and engineering problems involve optimizing multiple conﬂicting objectives [5, 28, 4, 24], including, for example, designing wireless sensors [12], building electric power systems [30], and training neural networks with multiple tasks [35]. With multiple conﬂicting objectives, it is impossible to ﬁnd a single solution that optimizes all the objectives simultaneously. Instead, it is essential to ﬁnd a set of diverse solutions in the Pareto front that represent different preferences on the different objective functions, so that the users can have a global view of how the optimal trade-off of the different objectives look like and select the solution according to their own preference.
Unfortunately, proﬁling the Pareto fronts casts a key computational challenge, especially for high dimensional problems [5, 12, 28]. Traditionally, a large literature has been devoted to developing black-box, derivative-free algorithms that are suitable for black-box optimization, such as these based on evolutionary algorithms [8] and Bayesian optimization [25, 1, 39]. However, the black-box algorithms tend to be expensive and can only be applied to small scale problems due to the lack of gradient information. Gradient-based MOO algorithms have been catching attention only recently, which include mainly multiple gradient descent (MGD) based methods [10, 22, 27]. However,
∗Code is available at https://github.com/gnobitab/MultiObjectiveSampling. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the results of these methods are still dissatisfying in ﬁnding diverse and evenly distributed Pareto solutions in complex problems.
We introduce Stein variational gradient descent (SVGD) [20, 19, 21] and Langevin dynamics [37] as efﬁcient approaches for proﬁling Pareto fronts. These methods iteratively evolve a group of particles to represent the target distribution. The main difference of the two algorithms is how they distribute points. The Langevin dynamics uses stochastic noises to perturb the particle trajectories, so they can visit different areas of the Pareto front. On contrast, SVGD is a deterministic sampling algorithm that pushes the particles to high probability regions using gradient information, while enforcing diversity between the particles using a repulsive force.
In this work, we propose to a simple approach to integrate SVGD and Langevin Dynamics with
MGD to draw samples from the Pareto front. Theoretical analyses are provided for both algorithms to understand their limiting distributions and their convergence speed. One challenge with MGD based sampling is that the limiting distributions do not admit explicit formulations. This is mainly because the forcing from MGD is in general not the gradient of any function. However, we can show its non-gradient component is orthogonal to a large class of functions. Assuming each objective function is strongly convex and regular, we can also show the limiting distributions concentrate on the Pareto front. Moreover, we can show the two algorithms converge to good solutions of MOO with O(1/t) and linear rate.
We test our methods on a variety of tasks, ranging from low-dimensional optimization to multi-task neural network optimization. On all the tasks tested, our method can obtain diverse and high quality
Pareto solutions that distribute evenly on the Pareto front, without predeﬁned preference vectors.
Quantitatively, we substantially outperform PF-SMG and EPO with respect to the hypervolume metric. 2