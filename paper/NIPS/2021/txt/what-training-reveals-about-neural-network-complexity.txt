Abstract
This work explores the Benevolent Training Hypothesis (BTH) which argues that the complexity of the function a deep neural network (NN) is learning can be deduced by its training dynamics. Our analysis provides evidence for BTH by relating the NN’s Lipschitz constant at different regions of the input space with the behavior of the stochastic training procedure. We ﬁrst observe that the Lipschitz constant close to the training data affects various aspects of the parameter trajectory, with more complex networks having a longer trajectory, bigger variance, and often veering further from their initialization. We then show that NNs whose 1st layer bias is trained more steadily (i.e., slowly and with little variation) have bounded complexity even in regions of the input space that are far from any training point.
Finally, we ﬁnd that steady training with Dropout implies a training- and data-dependent generalization bound that grows poly-logarithmically with the number of parameters. Overall, our results support the intuition that good training behavior can be a useful bias towards good generalization. 1

Introduction
Though neural networks (NNs) trained on relatively small datasets can generalize well, when em-ploying them on unfamiliar tasks signiﬁcant trial and error may be needed to select an architecture that does not overﬁt [1]. Could it be possible that NN designers favor architectures that can be easily trained and this biases them towards models with better generalization?
In the heart of this question lies what we refer to as the “Benevolent Training Hypothesis” (BTH), which argues that the behavior of the training procedure can be used as an indicator of the complexity of the function a NN is learning. Some empirical evidence for BTH already exists: (a) It has been observed that the training is becoming more tedious for high frequency directions in the input space [2] and that low frequencies are learned ﬁrst [3]. (b) Training also slows down the more images/labels are corrupted [4], e.g., the Inception [5] architecture is 3.5× slower to train when used to predict random labels than real ones. (c) Finally, Arpit et al. [6] noticed that the loss is more sensitive with respect to speciﬁc training points when the network is memorizing data and that training slows down faster as the NN size decreases when the data contain noise.
From the theory side, it is known that the training of shallow networks converges faster for more separable classes [7] and slower when ﬁtting random labels [8]. In addition, the stability [9] of stochastic gradient descent (SGD) implies that (under assumptions) NNs that can be trained with a small number of iterations provably generalize [10, 11]. Intuitively, since each gradient update conveys limited information, a NN that sees each training point few times (typically one or two) will not learn enough about the training set to overﬁt. Despite the elegance of this claim, the provided explanation does not necessarily account for what is observed in practice, where NNs trained for thousands of epochs can generalize even without rapidly decaying learning rates. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Our ﬁndings connect training dynamics and NN complexity by showing that the trajectory of the 1st layer bias reﬂects the NN’s Lipschitz constant near (and far from) the training data: the bias of higher complexity NNs exhibits a longer trajectory and varies more at the end of the training. 1.1 Quantifying NN complexity
This work takes a further step towards theoretically grounding the BTH by characterizing the relationship between the SGD trajectory and the complexity of the learned function. We study neural networks with ReLU activations, i.e., parametric piece-wise linear functions. Though many works measure the complexity of these networks via their maximum number of linear regions [12–15], it is suspected that the average NN behavior is far from the extremal constructions usually employed theoretically [16].
We instead focus on the Lipschitz continuity of a NN at different regions of its input. For networks equipped with ReLU activations, the Lipschitz constant in a region is simply the norm of the gradient at any point within it. The distribution of Lipschitz constants presents a natural way to quantify the complexity of NNs. Crucially, NNs with a bounded Lipschitz constant can generalize beyond the training data, a phenomenon that has been demonstrated both theoretically [17–19] and empirically [20]. The generalization bounds in question grow with the Lipschitz constant and the intrinsic dimensionality of the data manifold, but not necessarily with the number of parameters1, which renders them ideal for the study of overparameterized networks. 1.2 Main ﬁndings: connecting training behavior and neural network complexity
We link training dynamics and NN complexity close and far from the training data (see Figure 1).
NN complexity close to the training data. Section 4 commences with a simple observation: SGD updates the 1st layer bias more quickly if the learned function has a large Lipschitz constant near a sampled data point. This implies that the length of the bias trajectory grows linearly with the
Lipschitz constant of the NN on its linear regions that contain training data (Theorem 1). Based on this insight, we deduce that (a) near convergence, the parameters of more complex NNs vary more across successive SGD iterations (Corollary 2), and (b) the distance of the trained network to initialization is small if the learned NN has a low complexity (near training data) throughout its training, with the ﬁrst few high-error epochs playing a dominant role (Corollary 3).
NN complexity far from the training data. Section 5 focuses on the relationship between training and the Lipschitz constant in empty regions of the input space, i.e., linear regions of the NN that do not contain training points. We ﬁrst show that the Lipschitz constants in empty regions are linked with those of regions containing training points (Theorem 2). Our analysis implies that NNs whose parameters are updated more slowly during training have bounded complexity in a larger portion of the input space. We then demonstrate how training NNs with Dropout enables us to grasp more information about the properties of the learned function and, as such, to yield tighter estimates for the global Lipschitz constant. Our ﬁndings yield a data- and training-dependent generalization bound 1While the Lipschitz constant is typically upper bounded by the product of spectral norms of the layer weight matrices (thus yielding an exponential dependency on the depth), the product-of-norms bound is known to be far from the real Lipschitz constant [21, 22] 2
that features a poly-logarithmic dependence on the number of parameters and depth (Theorem 3).
On the contrary, in typical NN generalization bounds the number of samples needs to grow nearly linearly with the number of parameters [23–25] or exponentially with depth [26–30].
All proofs can be found in Appendix B, whereas Appendices A and C contain additional empirical and theoretical results, respectively. 2