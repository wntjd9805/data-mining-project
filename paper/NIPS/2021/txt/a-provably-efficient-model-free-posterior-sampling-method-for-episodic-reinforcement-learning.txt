Abstract
Thompson Sampling is one of the most effective methods for contextual bandits and has been generalized to posterior sampling for certain MDP settings. However, existing posterior sampling methods for reinforcement learning are limited by being model-based or lack worst-case theoretical guarantees beyond linear MDPs. This paper proposes a new model-free formulation of posterior sampling that applies to more general episodic reinforcement learning problems with theoretical guarantees.
We introduce novel proof techniques to show that under suitable conditions, the worst-case regret of our posterior sampling method matches the best known results of optimization based methods. In the linear MDP setting with dimension, the regret of our algorithm scales linearly with the dimension as compared to a quadratic dependence of the existing posterior sampling-based exploration algorithms. 1

Introduction
A key challenge in reinforcement learning problems is to balance exploitation and exploration. The goal is to make decisions that are currently expected to yield high reward and that help identify less known but potentially better alternate decisions. In the special case of contextual bandit problems, this trade-off is well understood and one of the most effective and widely used algorithms is Thompson sampling [Thompson, 1933]. Thompson sampling is a Bayesian approach that maintains a posterior distribution of each arm being optimal for the given context. At each round, the algorithm samples an action from this distribution and updates the posterior with the new observation. The popularity of
Thompson sampling stems from strong empirical performance [Li et al., 2010], as well as competitive theoretical guarantees in the form of Bayesian [Russo and Van Roy, 2014] and frequentist regret bounds [Kaufmann et al., 2012].
The results in the contextual bandit setting have motivated several adaptations of Thompson sampling to the more challenging Markov decision process (MDP) setting. Most common are model-based adaptations such as PSRL [Strens, 2000, Osband et al., 2013, Agrawal and Jia, 2017] or BOSS
[Asmuth et al., 2012], which maintain a posterior distributions over MDP models. These algorithms determine their current policy by sampling a model from this posterior and computing the optimal policy for it. The beneﬁt of maintaining a posterior over models instead of the optimal policy or optimal value function directly is that posterior updates can be more easily derived and algorithms are easier to analyze. However, model-based approaches are limited to small-scale problems where a realizable model class of moderate size is available and where computing the optimal policy of a model is computationally tractable. This rules out many practical problems where the observations are rich, e.g. images or text. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
There are also model-free posterior sampling algorithms that are inspired by Thompson sampling.
These aim to overcome the limitation of model-based algorithms by only requiring a value-function class and possibly weaker assumptions on the MDP model. Several algorithms have been proposed
[e.g. Osband et al., 2016a, Fortunato et al., 2017, Osband et al., 2018] with good empirical per-formance but with no theoretical performance guarantees. A notable exception is the randomized least-squares value iteration (RLSVI) algorithm by Osband et al. [2016b] that admits frequentist regret bounds in tabular [Russo, 2019] and linear Markiov decision processes [Zanette et al., 2020a].
However, to the best of our knowledge, no such results are available beyond the linear setting.
In contrast, there has been impressive recent progress in developing and analyzing provably efﬁcient algorithms for more general problem classes based on the OFU (optimism in the face of uncer-tainty) principle. These works show that OFU-based algorithm can learn a good policy with small sample-complexity or regret as long as the value-function class and MDP satisﬁes general structural assumptions. Those assumptions include bounded Bellman rank [Jiang et al., 2017], low inherent
Bellman error [Zanette et al., 2020b], small Eluder dimension [Wang et al., 2020] or Bellman-Eluder dimension [Jin et al., 2021]. This raises the question of whether OFU-based algorithms are inherently more suitable for such settings or whether it is possible to achieve similar results with a model-free posterior sampling approach. In this work, we answer this question by analyzing a posterior sampling algorithm that works with a Q-function class and admits worst-case regret guarantees under general structural assumptions. Our main contributions are:
• We derive a model-free posterior sampling algorithm for reinforcement learning in general
Markov decision processes and value function classes.
• We introduce a new proof technique for analyzing posterior sampling with optimistic priors.
• We prove that this algorithm achieves near-optimal worst-case regret bounds that match the regret of OFU-based algorithms and improve the best known regret bounds for posterior sampling approaches. 1.1 Further