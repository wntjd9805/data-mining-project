Abstract
We challenge a common assumption underlying most supervised deep learning: that a model makes a prediction depending only on its parameters and the features of a single input. To this end, we introduce a general-purpose deep learning architecture that takes as input the entire dataset instead of processing one datapoint at a time. Our approach uses self-attention to reason about relationships between datapoints explicitly, which can be seen as realizing non-parametric models using parametric attention mechanisms. However, unlike conventional non-parametric models, we let the model learn end-to-end from the data how to make use of other datapoints for prediction. Empirically, our models solve cross-datapoint lookup and complex reasoning tasks unsolvable by traditional deep learning models. We show highly competitive results on tabular data, early results on CIFAR-10, and give insight into how the model makes use of the interactions between points. 1

Introduction
From CNNs [57] to Transformers [90], most of supervised deep learning relies on parametric (x1, y1), . . . , (xn, yn) modeling: models learn parameters θ from a set of training data
}
{ to maximize training likelihoods p(y to target values y
.
∈ Y
∈ X
At test time, they then make a prediction p(y∗ x∗; θ) that depends only on those parameters θ and the test input x∗. That is, parametric models do not consider direct dependencies between datapoints. x; θ) mapping from features x
Dtrain =
|
|
This paper challenges parametric modeling as the dominant paradigm in deep learning. Based on the same end-to-end learning motivations that underpin deep learning itself, we consider giving models the additional ﬂexibility of using training data directly when making predictions p(y∗
Dtrain; θ).
Concretely, we introduce Non-Parametric Transformers (NPTs): a general deep learning architec-ture that takes the entire dataset as input and predicts by explicitly learning interactions between datapoints (Fig. 1). NPTs leverage both parametric and non-parametric predictive mechanisms, with the use of end-to-end training allowing the model to naturally learn from the data how to balance the two. Namely, instead of just learning predictive functions from the features to the targets of independent datapoints, NPTs can also learn to reason about general relationships between inputs.
We use multi-head self-attention [4, 59, 90] to model relationships between datapoints and construct x∗,
|
∗Equal Contribution. Correspondence to {jannik.kossen, neil.band}@cs.ox.ac.uk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: NPTs learn direct interactions between datapoints. (a) Input data: predict masked target entry [?] for datapoint Xi. (b) Notation from §2. (c) Parametric models predict only from the features of the given input. (d) NPTs predict by modeling relationships between all points in the dataset. a training objective for NPTs with a stochastic masking mechanism inspired by self-supervised reconstruction tasks in natural language processing [24]. We show that these models learn to look up information from other datapoints and capture the causal mechanism generating the data in semi-synthetic settings. However, unlike conventional non-parametric models, NPTs are not forced to only make predictions in this way: they can also use the power of ordinary parametric deep learning.
| x∗,