Abstract
We employ a toolset — dubbed Dr. Frankenstein — to analyse the similarity of representations in deep neural networks. With this toolset, we aim to match the activations on given layers of two trained neural networks by joining them with a stitching layer. We demonstrate that the inner representations emerging in deep convolutional neural networks with the same architecture but different initializations can be matched with a surprisingly high degree of accuracy even with a single, afﬁne stitching layer. We choose the stitching layer from several possible classes of linear transformations and investigate their performance and properties. The task of matching representations is closely related to notions of similarity. Using this toolset, we also provide a novel viewpoint on the current line of research regarding similarity indices of neural network representations: the perspective of the performance on a task. 1

Introduction
A central topic in the analysis of deep neural networks is the investigation of the learned representa-tions. A good understanding of the features learned on inner layers provides means to analyse and advance deep learning systems. A fundamental question in this line of inquiry is “when are two representations similar?” We contribute to this line of research by introducing a novel approach to study the above question. The key idea of our work is to ask the following, somewhat different question: “in what way are two representations similar, once we know that they are similar?” In this paper, we present a conceptual framework and a methodology that allows to meaningfully pose and study this question. For this purpose, we will distinguish two approaches to deﬁne similarity: representational similarity and functional similarity.
Representational similarity Representational similarity notions deﬁne similarity via computing statistics on two different data embeddings that capture appropriate geometrical or statistical prop-erties. There exist several statistical measures, [Raghu et al., 2017, Morcos et al., 2018, Kornblith et al., 2019] each of which serves as a different notion of similarity. These measures have been used successfully to obtain valuable insights about the inner workings of deep neural networks [Nguyen et al., 2021, Mirzadeh et al., 2021, Neyshabur et al., 2020, Wu et al., 2020].
Functional similarity While representational similarity concerns the data embeddings, in contrast, functional similarity concerns the functions that produce and process these embeddings, i.e., parts of the function compositions that the neural networks realize. With these elements, we can ask new types of questions that we were not able to ask using solely the data embeddings. For example, we can ask the following: “can network B achieve its task using the representations of network A?” 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our paper focuses on this speciﬁc theme. In particular, we investigate this by taking the activations of network A at a given layer, transform it with an afﬁne map, then use the result as an input of the same layer in network B. In other words, we stitch together the two networks with an afﬁne stitching layer.
This technique ﬁrst appeared in Lenc and Vedaldi [2019] to study the invariance and equivariance properties of convolutional networks. Evaluating the performance of this combined network provides an alternative viewpoint on similarity of representations, the viewpoint of functional similarity.
We investigate this novel perspective of reﬂecting on representational similarity through the lens of functional similarity. A brief outline of this work1 is the following:
• After a detailed introduction to model stitching (Section 3) and presenting two types of stitching methods (Section 4), we empirically demonstrate that trained convolutional networks with the same architecture but different initializations can be stitched together, even with a single, afﬁne stitching layer in many cases without signiﬁcant performance loss (Section 5). We will refer to this compatibility property as the ‘matchability’ of representations.
• Observing the matchability of representations as a quite robust property of common vision models, we have a wide range of examples in our hands when we know that representations are functionally similar. This permits us to study the relation between representational similarity measures and functional similarity. In our experiments, we show that the values of similarity indices are not necessarily indicative of task performance on a stitched network, and perhaps more surprisingly, high-performance stitchings can have differences in the values of similarity indices such as Centered
Kernel Alignment (CKA) [Cortes et al., 2012, Kornblith et al., 2019]. This also reﬂects on phenomena experienced by the ‘end-users’ of these indices. For instance, in the context of continual learning Mirzadeh et al. [2021] observe that CKA remains constant, while the accuracy on previous tasks drops drastically (Section 6).
• Constraining the space of transformations in the stitching layer provides means to analyse how the information is organised in the representations. As an example of this probing methodology, we study bottleneck stitching layers, and show that when doing SVD in the representational space, the magnitudes of the principal components are not in direct correspondence with the information content of the latent directions (Section 7). As SVD is commonly used with embeddings, and there are representational similarity measures, for example SVCCA [Raghu et al., 2017], that use SVD as an ingredient, these results might be of interest to both theorists and practitioners.
• The transformation matrices of well-performing stitching layers capture how representations are functionally similar. This provides the opportunity to empirically investigate “in what way are two representations similar, once we know that they are similar?”. As a ﬁrst step on this avenue, we present results regarding the uniqueness and sparsity properties of the stitching matrices (Section 8). 2