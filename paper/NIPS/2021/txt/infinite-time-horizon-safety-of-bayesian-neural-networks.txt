Abstract
Bayesian neural networks (BNNs) place distributions over the weights of a neural network to model uncertainty in the data and the network’s prediction. We consider the problem of verifying safety when running a Bayesian neural network policy in a feedback loop with inﬁnite time horizon systems. Compared to the existing sampling-based approaches, which are inapplicable to the inﬁnite time horizon setting, we train a separate deterministic neural network that serves as an inﬁnite time horizon safety certiﬁcate. In particular, we show that the certiﬁcate network guarantees the safety of the system over a subset of the BNN weight posterior’s support. Our method ﬁrst computes a safe weight set and then alters the BNN’s weight posterior to reject samples outside this set. Moreover, we show how to extend our approach to a safe-exploration reinforcement learning setting, in order to avoid unsafe trajectories during the training of the policy. We evaluate our approach on a series of reinforcement learning benchmarks, including non-Lyapunovian safety speciﬁcations. 1

Introduction
The success of deep neural networks (DNNs) across different domains has created the desire to apply them in safety-critical applications such as autonomous vehicles [28, 30] and healthcare systems [41].
The fundamental challenge for the deployment of DNNs in these domains is certifying their safety [3].
Thus, formal safety veriﬁcation of DNNs in isolation and closed control loops [26, 23, 17, 43, 13] has become an active research topic.
Bayesian neural networks (BNNs) are a family of neural networks that place distributions over their weights [36]. This allows learning uncertainty in the data and the network’s prediction, while preserving the strong modelling capabilities of DNNs [32]. In particular, BNNs can learn arbitrary data distributions from much simpler (e.g. Gaussian) weight distributions. This makes BNNs very appealing for robotic and medical applications [33] where uncertainty is a central component of data.
Despite the large body of literature on verifying safety of DNNs, the formal safety veriﬁcation of
BNNs has received less attention. Notably, [8, 45, 34] have proposed sampling-based techniques for obtaining probabilistic guarantees about BNNs. Although these approaches provide some insight
∗Equal Contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
into BNN safety, they suffer from two key limitations. First, sampling provides only bounds on the probability of the BNN’s safety which is insufﬁcient for systems with critical safety implications.
For instance, having an autonomous vehicle with a 99.9% safety guarantee is still insufﬁcient for deployment if millions of vehicles are deployed. Second, samples can only simulate the system for a
ﬁnite time, making it impossible to reason about the system’s safety over an unbounded time horizon.
In this work, we study the safety veriﬁcation problem for BNN policies in safety-critical sys-tems over the inﬁnite time horizon. Formally, we consider discrete-time closed-loop systems deﬁned by a dynamical system and a BNN pol-icy. Given a set of initial states and a set of unsafe (or bad) states, the goal of the safety veriﬁcation problem is to verify that no system execution starting in an initial state can reach an unsafe state. Unlike existing literature which considers probability of safety, we verify sure safety, i.e. safety of every system execution of the system. In particular, we present a method for computing safe weight sets for which every system execution is safe as long as the BNN samples its weights from this set.
Figure 1: BNNs are typically unsafe by default.
Top ﬁgure: The posterior of a typical BNN has unbounded support, resulting in a non-zero proba-bility of producing an unsafe action. Bottom ﬁgure:
Restricting the support of the weight distributions via rejection sampling ensures BNN safety.
Our approach to restrict the support of the weight distribution is necessary as BNNs with
Gaussian weight priors typically produce out-put posteriors with unbounded support. Conse-quently, there is a low but non-zero probability for the output variable to lie in an unsafe region, see Figure 1. This implies that BNNs are usually unsafe by default. We therefore consider the more general problem of computing safe weight sets. Verifying that a weight set is safe allows re-calibrating the BNN policy by rejecting unsafe weight samples in order to guarantee safety.
As most BNNs employ uni-modal weight priors, e.g. Gaussians, we naturally adopt weight sets in the form of products of intervals centered at the means of the BNN’s weight distributions. To verify safety of a weight set, we search for a safety certiﬁcate in the form of a safe positive invariant (also known as safe inductive invariant). A safe positive invariant is a set of system states that contains all initial states, is closed under the system dynamics and does not contain any unsafe state.
The key advantage of using safe positive invariants is that their existence implies the inﬁnite time horizon safety. We parametrize safe positive invariant candidates by (deterministic) neural networks that classify system states for determining set inclusion. Moreover, we phrase the search for an invariant as a learning problem. A separated veriﬁer module then checks if a candidate is indeed a safe positive invariant by checking the required properties via constraint solving. In case the veriﬁer
ﬁnds a counterexample demonstrating that the candidate violates the safe positive invariant condition, we re-train the candidate on the found counterexample. We repeat this procedure until the veriﬁer concludes that the candidate is a safe positive invariant ensuring that the system is safe.
The safe weight set obtained by our method can be used for safe exploration reinforcement learning.
In particular, generating rollouts during learning by sampling from the safe weight set allows an exploration of the environment while ensuring safety. Moreover, projecting the (mean) weights onto the safe weight set after each gradient update further ensures that the improved policy stays safe.
Contributions Our contributions can be summarized as follows: 1. We deﬁne a safety veriﬁcation problem for BNN policies which overcomes the unbounded posterior issue by computing and verifying safe weight sets. The problem generalizes the sure safety veriﬁcation of BNNs and solving it allows re-calibrating BNN policies via rejection sampling to guarantee safety. 2. We introduce a method for computing safe weight sets in BNN policies in the form of products of intervals around the BNN weights’ means. To verify safety of a weight set, our novel algorithm learns a safe positive invariant in the form of a deterministic neural network. 3. We evaluate our methodology on a series of benchmark applications, including non-linear systems and non-Lyapunovian safety speciﬁcations. 2
2