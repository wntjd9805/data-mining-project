Abstract
Unsupervised domain adaptation aims to align a labeled source domain and an unlabeled target domain, but it requires to access the source data which often raises concerns in data privacy, data portability and data transmission efﬁciency. We study unsupervised model adaptation (UMA), or called Unsupervised Domain Adapta-tion without Source Data, an alternative setting that aims to adapt source-trained models towards target distributions without accessing source data. To this end, we design an innovative historical contrastive learning (HCL) technique that exploits historical source hypothesis to make up for the absence of source data in UMA.
HCL addresses the UMA challenge from two perspectives. First, it introduces historical contrastive instance discrimination (HCID) that learns from target sam-ples by contrasting their embeddings which are generated by the currently adapted model and the historical models. With the historical models, HCID encourages
UMA to learn instance-discriminative target representations while preserving the source hypothesis. Second, it introduces historical contrastive category discrimi-nation (HCCD) that pseudo-labels target samples to learn category-discriminative target representations. Speciﬁcally, HCCD re-weights pseudo labels according to their prediction consistency across the current and historical models. Extensive experiments show that HCL outperforms and state-of-the-art methods consistently across a variety of visual tasks and setups. 1

Introduction
Deep neural networks (DNNs) [28, 73, 23] have achieved great success in various computer vision tasks [8, 55, 60, 59, 28, 73, 23] but often generalize poorly to new domains due to the inter-domain discrepancy [1]. Unsupervised domain adaptation (UDA) [78, 51, 76, 64, 66, 79, 77, 103, 102, 25, 71, 44, 26, 86] addresses the inter-domain discrepancy by aligning the source and target data distributions, but it requires to access the source-domain data which often raises concerns in data privacy, data portability, and data transmission efﬁciency.
In this work, we study unsupervised model adaptation (UMA), an alternative setting that aims to adapt source-trained models to ﬁt target data distribution without accessing the source-domain data.
Under the UMA setting, the only information carried forward is a portable source-trained model which is usually much smaller than the source-domain data and can be transmitted more efﬁciently
[45, 42, 43, 72, 48] as illustrated in Table 1. Beyond that, the UMA setting also alleviates the concern of data privacy and intellectual property effectively. On the other hand, the absence of the labeled source-domain data makes domain adaptation much more challenging and susceptible to collapse.
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Source data have much larger sizes than source-trained models.
Semantic segmentation Object detection
Image classiﬁcation
Storage size (MB)
GTA5
SYNTHIA
Cityscapes
Source dataset
Source-trained model 62, 873.6 179.1 22, 323.2 179.1 12, 697.6 553.4
VisDA17 7, 884.8 172.6
To this end, we develop historical contrastive learning (HCL) that aims to make up for the absence of source data by adapting the source-trained model to ﬁt target data distribution without forgetting source hypothesis, as illustrated in Fig. 1. HCL addresses the UMA challenge from two perspectives.
First, it introduces historical contrastive instance discrimination (HCID) that learns target samples by comparing their embeddings generated by the current model (as queries) and those generated by historical models (as keys): a query is pulled close to its positive keys while pushed apart from its negative keys. HCID can thus be viewed as a new type of instance contrastive learning for the task of UMA with historical models, which learns instance-discriminative target representations without forgetting source-domain hypothesis. Second, it introduces historical contrastive category discrimination (HCCD) that pseudo-labels target samples for learning category-discriminative target representations. Speciﬁcally, HCCD re-weights the pseudo labels according to their consistency across the current and historical models.
The proposed HCL tackles UMA with three desirable features: 1) It introduces historical contrast and achieves UMA without forgetting source hypothesis; 2) The HCID works at instance level, which encourages to learn instance-discriminative target representations that generalize well to unseen data [98]; 3) The HCCD works at category level (i.e., output space) which encourages to learn category-discriminative target representation that is well aligned with the objective of down-stream tasks.
The contributions of this work can be summarized in three aspects. First, we investigate memory-based learning for unsupervised model adaptation that learns discriminative representations for unlabeled target data without forgetting source hypothesis. To the best of our knowledge, this is the ﬁrst work that explores memory-based learning for the task of UMA. Second, we design historical contrastive learning which introduces historical contrastive instance discrimination and category discrimination, the latter is naturally aligned with the objective of UMA. Third, extensive experiments show that the proposed historical contrastive learning outperforms state-of-the-art methods consistently across a variety of visual tasks and setups. 2