Abstract
Lookahead search has been a critical component of recent AI successes, such as in the games of chess, go, and poker. However, the search methods used in these games, and in many other settings, are tabular. Tabular search methods do not scale well with the size of the search space, and this problem is exacerbated by stochasticity and partial observability. In this work we replace tabular search with online model-based ﬁne-tuning of a policy neural network via reinforcement learn-ing, and show that this approach outperforms state-of-the-art search algorithms in benchmark settings. In particular, we use our search algorithm to achieve a new state-of-the-art result in self-play Hanabi, and show the generality of our algorithm by also showing that it outperforms tabular search in the Atari game Ms. Pacman. 1

Introduction
Lookahead search has been a key component of successful AI systems in sequential decision-making problems. For example, in order to achieve superhuman performance in go, chess and shogi, Alp-haZero leveraged Monte Carlo tree search (MCTS) [38]. MuZero extended this even further to Atari games, again using MCTS [32]. Without MCTS, AlphaZero performs below a top human level, and more generally no superhuman Go bot has yet been developed that does not use some form of
MCTS. Similarly, search algorithms were a critical component of AI successes in backgammon [45], chess [10], poker [27, 7, 8], and Hanabi [22]. However, even though different search algorithms were used in each domain, all of them were tabular search algorithms, i.e., a distinct policy was computed for each state encountered during search, without any function approximation to generalize between similar states.
While tabular search has achieved great success, particularly in perfect-information deterministic environments, its applicability is clearly limited. For example, in the popular partially observ-able stochastic AI benchmark game Hanabi [5], one-step lookahead search involves a search over about 500 possible next states. However, searching over all two-step joint policies would require a search over 2020 states, which is clearly intractable for tabular search. Additionally, unlike perfect-information deterministic games where it is only necessary to search over a tiny fraction of the next several moves, partial observability and stochasticity make it impossible to limit the search to a tiny subset of all possible states. Fortunately, many of these states are extremely similar, so a search algorithm can in theory beneﬁt by generalizing between similar states. This is the motivation for our non-tabular search algorithm.
∗Equal Contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper we take inspiration from related research in continuous control environments that use non-tabular planning algorithms to improve performance at inference time [47, 25, 1]. These meth-ods leverage ﬁnite-horizon model-based rollouts. Speciﬁcally, we replace tabular search with ﬁne-tuning of the policy network at inference time. We show that with this approach we are able to achieve state-of-the-art performance in Hanabi. Speciﬁcally, our method is able to search multiple moves ahead and discover joint deviations, which in general is intractable using tabular search. We also show the generality of our approach by showing that it outperforms Monte Carlo tree search in deterministic and stochastic versions of the Atari game Ms. Pacman. 2