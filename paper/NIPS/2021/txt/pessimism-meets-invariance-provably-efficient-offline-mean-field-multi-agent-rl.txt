Abstract
Mean-Field Multi-Agent Reinforcement Learning (MF-MARL) is attractive in the applications involving a large population of homogeneous agents, as it exploits the permutation invariance of agents and avoids the curse of many agents. Most existing results only focus on online settings, in which agents can interact with the environment during training. In some applications such as social welfare optimization, however, the interaction during training can be prohibitive or even unethical in the societal systems. To bridge such a gap, we propose a SAFARI (peSsimistic meAn-Field vAlue iteRatIon) algorithm for off-line MF-MARL, which only requires a handful of pre-collected experience data. Theoretically, under a weak coverage assumption that the experience dataset contains enough information about the optimal policy, we prove that for an episodic mean-ﬁeld MDP with a horizon H and N training trajectories, SAFARI attains a sub-optimality gap (H 2de↵ /pN ), where de↵ is the effective dimension of the function class of for parameterizing the value function, but independent on the number of agents.
Numerical experiments are provided.
O 1

Introduction
Signiﬁcant progress has been made towards multi-agent reinforcement learning (MARL) for many prominent sequential decision making problems, such as social welfare optimization (Leibo et al., 2017), ﬂeet control of autonomous vehicles (Shalev-Shwartz et al., 2016) and playing multiplayer online battle arena (MOBA) games (Berner et al., 2019). As the joint state and action space scales exponentially with the number of agents, however, MARL becomes computationally expensive. One remedy is the mean-ﬁeld regime when an extremely large number of homogenous agents are involved, e.g., social welfare optimization. The effect of each agent on the overall multi-agent system can become inﬁnitesimal, and therefore all agents can be considered interchangeable/indistinguishable (Yang et al., 2018; Carmona et al., 2019; Li et al., 2021). Accordingly, the interaction among agents can be captured by some mean-ﬁeld quantity such as the empirical distribution of states, and therefore each agent only needs to ﬁnd the best response to the so-called “mean-ﬁeld state”, which avoids the curse of many agents.
Most existing results on mean-ﬁeld MARL (MF-MARL) are for the online setting (Yang et al., 2018;
Zhang et al., 2019), where the agents can interact with the environment during training. However, such interaction during training can be prohibitive for some important applications (Leibo et al., 2017;
Mandel et al., 2014; Jaques et al., 2019; Levine et al., 2020). Taking social welfare optimization as an example, repeatedly conducting social experiments on human being can be unaffordable or even unethical in the societal systems. Therefore, we can only consider the ofﬂine settings, i.e., we learn the optimal policy based on some pre-collected experience data (Levine et al., 2020). Unfortunately,
⇤Extension to online setting is provided in a longer technical report version, which is available upon request. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
existing ofﬂine reinforcement learning (RL) algorithms and theories all focus on the single agent settings, and no algorithms and theories have been developed for MARL under the ofﬂine settings, regardless of the mean-ﬁeld regime or not.
To bridge such a critical gap, we propose the ﬁrst pessimistic algorithm – named SAFARI (peSsimistic meAn-Field vAlue iteRatIon) for mean-ﬁeld MARL, which can provably achieve sample efﬁciency under the ofﬂine setting. Our proposed algorithm contains two important components: (1) To incorporate the permutation invariance of the homogenous agents, we adopt a RKHS (Reproducing
Kernel Hilbert Space) mean-embedding approach for approximating value functions, which avoids the exponential blowup of the agents’ state and action spaces; (2) We develop an uncertainty quantiﬁer, and integrate it into the value iteration procedure as the penalty function. Such a penalty function can effectively screen the “spuriously correlated trajectories”, i.e., which possibly happen to appear in the experience data, but are actually unrelated to the optimal policy, but by chance induce large cumulative rewards and hence may potentially mislead the learned policy.
Theoretically, we establish a data-dependent upper bound on the suboptimality of SAFARI for MF-MARL without the stringent assumptions on the sufﬁcient coverage of the experience data (e.g., ﬁnite concentrability coefﬁcients (Chen and Jiang, 2019) or uniformly lower bounded densities of visitation measure (Yin et al., 2020)). More speciﬁcally, we only assume that the experience data of N training trajectories contains enough information about the optimal policy. Then we prove that for an episodic (H 2de↵ /pN ),
MF-MARL problem with a horizon H, SAFARI attains a sub-optimality gap of where de↵ is the effective dimension of the function class (RKHS) for parameterizing the value function and independent on the number of agents. In addition to the ofﬂine settings, our SAFARI algorithm can also be extended to MF-MARL under the online setting (OMPPO algorithm), which is of independent interest. Details are provided in a longer technical report version, which is available upon request.
O
The rest of this paper is organized as follows: Section 2 reviews related work on mean-ﬁeld multi-agent reinforcement learning and ofﬂine reinforcement learning for the single agent settings; Section 3 introduces our problem setup of the mean-ﬁeld MARL regime; Section 4 introduces our proposed
SAFARI algorithm; Section 5 establishes the theoretical guarantees for SAFARI; Section 6 presents numerical experiments on the multi-agent particle cooperative navigation scenario; Section 7 draws a brief conclusion. 2