Abstract
Inferring representations of 3D scenes from 2D observations is a fundamental prob-lem of computer graphics, computer vision, and artiﬁcial intelligence. Emerging 3D-structured neural scene representations are a promising approach to 3D scene understanding. In this work, we propose a novel neural scene representation, Light
Field Networks or LFNs, which represent both geometry and appearance of the underlying 3D scene in a 360-degree, four-dimensional light ﬁeld parameterized via a neural network. Rendering a ray from an LFN requires only a single network evaluation, as opposed to hundreds of evaluations per ray for ray-marching or volumetric based renderers in 3D-structured neural scene representations. In the setting of simple scenes, we leverage meta-learning to learn a prior over LFNs that enables multi-view consistent light ﬁeld reconstruction from as little as a single image observation. This results in dramatic reductions in time and memory complexity, and enables real-time rendering. The cost of storing a 360-degree light ﬁeld via an LFN is two orders of magnitude lower than conventional methods such as the Lumigraph. Utilizing the analytical differentiability of neural implicit representations and a novel parameterization of light space, we further demonstrate the extraction of sparse depth maps from LFNs. 1

Introduction
A fundamental problem across computer graphics, computer vision, and artiﬁcial intelligence is to infer a representation of a scene’s 3D shape and appearance given impoverished observations such as 2D images of the scene. Recent contributions have advanced the state of the art for this problem signiﬁcantly. First, neural implicit representations have enabled efﬁcient representation of local 3D scene properties by mapping a 3D coordinate to local properties of the 3D scene at that coordinate [1– 6]. Second, differentiable neural renderers allow for the inference of these representations given only 2D image observations [3, 4]. Finally, leveraging meta-learning approaches such as hypernetworks or gradient-based meta-learning has enabled the learning of distributions of 3D scenes, and therefore reconstruction given only a single image observation [3]. This has enabled a number of applications, such as novel view synthesis [7, 3, 6], 3D reconstruction [5, 3] semantic segmentation [8, 9], and
SLAM [10]. However, 3D-structured neural scene representations come with a major limitation:
Their rendering is prohibitively expensive, on the order of tens of seconds for a single 256 × 256 image for state-of-the-art approaches. In particular, parameterizing the scene in 3D space necessitates
∗These authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the discovery of surfaces along camera rays during rendering. This can be solved either by encoding geometry as a level set of an occupancy or signed distance function, or via volumetric rendering, which solves an alpha-compositing problem along each ray. Either approach, however, requires tens or even hundreds of evaluations of the 3D neural scene representation in order to render a single camera ray.
We propose a novel neural scene representation, dubbed Light Field Networks or LFNs. Instead of encoding a scene in 3D space, Light Field Networks encode a scene by directly mapping an oriented camera ray in the four dimensional space of light rays to the radiance observed by that ray. This obviates the need to query opacity and RGB at 3D locations along a ray or to ray-march towards the level set of a signed distance function, speeding up rendering by three orders of magnitude compared to volumetric methods. In addition to directly encoding appearance, we demonstrate that LFNs encode information about scene geometry in their derivatives. Utilizing the unique ﬂexibility of neural
ﬁeld representations, we introduce the use of Plücker coordinates to parameterize 360-degree light
ﬁelds, which allow for storage of a-priori unbounded scenes and admit a simple expression for the depth as an analytical function of an LFN. Using this relationship, we demonstrate the computation of geometry in the form of sparse depth maps. While 3D-structured neural scene representations are multi-view consistent by design, parameterizing a scene in light space does not come with this guarantee: the additional degree of freedom enables rays that view the same 3D point to change appearance across viewpoints. For the setting of simple scenes, we demonstrate that this challenge can be overcome by learning a prior over 4D light ﬁelds in a meta-learning framework. We benchmark with current state-of-the-art approaches for single-shot novel view synthesis, and demonstrate that
LFNs compare favorably with globally conditioned 3D-structured representations, while accelerating rendering and reducing memory consumption by orders of magnitude.
In summary, we make the following contributions: 1. We propose Light Field Networks (LFNs), a novel neural scene representation that directly parameterizes the light ﬁeld of a 3D scene via a neural network, enabling real-time rendering and vast reduction in memory utilization. 2. We demonstrate that we may leverage 6-dimensional Plücker coordinates as a parameteri-zation of light ﬁelds, despite their apparent overparameterization of the 4D space of rays, thereby enabling continuous, 360-degree light ﬁelds. 3. By embedding LFNs in a meta-learning framework, we demonstrate light ﬁeld reconstruction and novel view synthesis of simple scenes from sparse 2D image supervision only. 4. We demonstrate that inferred LFNs encode both appearance and geometry of the underlying 3D scenes by extracting sparse depth maps from the derivatives of LFNs, leveraging their analytical differentiability.
Scope. The proposed method is currently constrained to the reconstruction of simple scenes, such as single objects and simple room-scale scenes, in line with recent work on learning generative models in this regime [3, 11]. 2