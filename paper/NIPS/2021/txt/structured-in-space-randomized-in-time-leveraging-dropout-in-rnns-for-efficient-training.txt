Abstract
Recurrent Neural Networks (RNNs), more speciﬁcally their Long Short-Term
Memory (LSTM) variants, have been widely used as a deep learning tool for tackling sequence-based learning tasks in text and speech. Training of such LSTM applications is computationally intensive due to the recurrent nature of hidden state computation that repeats for each time step. While sparsity in Deep Neural
Nets has been widely seen as an opportunity for reducing computation time in both training and inference phases, the usage of non-ReLU activation in LSTM
RNNs renders the opportunities for such dynamic sparsity associated with neuron activation and gradient values to be limited or non-existent. In this work, we identify dropout induced sparsity for LSTMs as a suitable mode of computation reduction. Dropout is a widely used regularization mechanism, which randomly drops computed neuron values during each iteration of training. We propose to structure dropout patterns, by dropping out the same set of physical neurons within a batch, resulting in column (row) level hidden state sparsity, which are well amenable to computation reduction at run-time in general-purpose SIMD hardware as well as systolic arrays. We provide a detailed analysis of how the dropout-induced sparsity propagates through the different stages of network training and how it can be leveraged in each stage. More importantly, our proposed approach works as a direct replacement for existing dropout-based application settings. We conduct our experiments for three representative NLP tasks: language modelling on the PTB dataset, OpenNMT based machine translation using the IWSLT De-En and En-Vi datasets, and named entity recognition sequence labelling using the
CoNLL-2003 shared task. We demonstrate that our proposed approach can be used to translate dropout-based computation reduction into reduced training time, with improvement ranging from 1.23× to 1.64×, without sacriﬁcing the target metric. 1

Introduction
Recurrent Neural Networks (RNNs) are an important class of machine learning approaches for analyzing sequential data including language, speech, time-series data, etc. Given an input training sequence, RNNs cells pass information from the previous time-step to the current time-step, in the form of a hidden-state, which helps the network to learn inherent temporal constructs. To further enhance the ability to learn from past information, Long Short-Term Memory(LSTM) [Hochreiter and Schmidhuber, 1997] cells have been proposed, which consist of multiple internal gates and a cell-state in addition to the hidden-state. Such LSTM cell-based RNNs have been successfully deployed in a variety of application domains, e.g., language modelling [Mikolov and Zweig, 2012], automatic speech recognition [Hannun et al., 2014, Amodei et al., 2016, Graves et al., 2013], sentiment analysis [Ma et al., 2018], and machine translation [Luong et al., 2015]. However, training such RNNs is a time- and compute-intensive task. This is because, for each input sequence, a 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
given RNN is logically unrolled into a sequence equal to the sequence length, and computation is performed following the dependencies. Once the training loss is calculated, it is back-propagated over the sequence in the reverse direction to compute the weight gradients, which is known as back-propagation through time (BPTT) [Werbos, 1990].
One of the promising ways of reducing the effective amount of computation is to exploit sparsity present in data structures. Sparsity refers to the fraction of zero-valued operands to the total size of the operands involved in a computation step. Thus, efﬁciently skipping zero-valued operands from computation leads to faster training time and energy efﬁciency. Convolutional Neural Networks (CNNs), owing to their prominent use of ReLU [Nair and Hinton, 2010] as the activation function, generate signiﬁcant dynamic sparsity associated with the activation and neuron gradient values, which can be leveraged during training. However, due to the non-ReLU activation functions, e.g., tanh and sigmoid, dynamic sparsity in LSTM based networks is not inherent in nature. Such activation functions do not directly induce zero values, and hence the opportunity of exploiting sparsity is only limited to thresholding and approximation-based approaches [Gupta et al., 2019]. Although such techniques have been shown to work relatively well during the inference phase (after the network is fully trained), training still relies on high precision arithmetic operations. Also, the derivatives of the tanh and sigmoid functions do not lend themselves to sparsity as in ReLU. For the same reasons, the back-propagated gradient values are not sparse either, thus ruling out the opportunity for sparsity exploitation during the backward pass.
Motivated by these observations, in this work, we explore the opportunity of using the dropout-induced zero values as a source of sparsity in LSTMs. Dropout, a widely used regularizing technique, works by randomly dropping activation values in the forward pass. Since the dropped activation map becomes sparse in nature, we can take advantage of this sparsity towards achieving computation speed-up. However, such dropout masks are sampled randomly, which creates bottlenecks in accelerating the resultant sparse computation in a general-purpose SIMD hardware owing to the storage and memory access overheads. Therefore, we propose to structure such dropout patterns which makes it amenable for hardware acceleration. We distinguish between sparsity types, namely, input and output sparsity, depending on whether the input operands are zero-valued or an output operand is going to be zero. Since the dropout mask can be sampled ahead of time, exploiting output sparsity may seem straightforward. However, applying it naively to the hidden state can result in unintended consequences. This is because the sparsity also propagates to the cell states due to the inherent computational dependencies, resulting in possible loss of functional performance(e.g. accuracy).
Further, we carefully analyze the computational ﬂow in the backward pass of LSTM to identify sparse operands. We also extend the sparsity technique to SIMD GPU-based matrix-multiplication using shared memory, and show that that our proposed approach is exploitable in existing software based techniques. Our proposed sparsity pattern is also well-suited to be leveraged in systolic array-based computations, resulting in energy-efﬁciency beneﬁts of dense matrix operations combined with high-performance sparse operations. In summary, this paper makes the following key contributions:
• We outline a generic framework for applying dropout in the context of LSTMs, considering uniformity of dropout patterns within a batch and across time steps. In addition to commonly used non-recurrent(NR) direction, we also extend dropout structural pattern to recurrent (NR+RH) directions to maximize the scope for potential training speedup.
• We provide detailed opportunities for exploiting the uniform sparsity in forward as well as backward passes of LSTMs based on sparsity propagation during training. We also identify the sparsity types based on input/output sparsity, and leverage it towards faster computation operation.
• We conduct extensive experiments with three representative applications including Language
Modelling, Machine Translation, and Named Entity Recognition to evaluate the general applica-bility and performance beneﬁts of our proposed approach. Our results show that uniform dropout pattern within a batch and random in time steps achieves signiﬁcant training speedup ranging from 1.23x-1.64x, while not compromising on the target accuracy metric. 2