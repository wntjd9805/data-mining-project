Abstract
We initiate an investigation of private sampling from distributions. Given a dataset with n independent observations from an unknown distribution P , a sampling algorithm must output a single observation from a distribution that is close in total variation distance to P while satisfying differential privacy. Sampling abstracts the goal of generating small amounts of realistic-looking data. We provide tight upper and lower bounds for the dataset size needed for this task for three natural families of distributions: arbitrary distributions on {1, . . . , k}, arbitrary product distributions on {0, 1}d, and product distributions on on {0, 1}d with bias in each coordinate bounded away from 0 and 1. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer observations than learning a description of P nonprivately; in other regimes, however, private sampling proves to be as difﬁcult as private learning. Notably, for some classes of distributions, the overhead in the number of observations needed for private learning compared to non-private learning is completely captured by the number of observations needed for private sampling. 1

Introduction
Statistical machine learning models trained on sensitive data are now widely deployed in domains such as education, ﬁnance, criminal justice, medicine, and public health. The personal data used to train such models are more detailed than ever, and there is a growing awareness of the privacy risks that come with their use. Differential privacy is a standard for conﬁdentiality that is now well studied and increasingly deployed.
Differentially private algorithms ensure that whatever is learned about an individual from the algo-rithm’s output would be roughly the same whether or not that individual’s record was actually part of the input dataset. This requirement entails a strong guarantee, but limits the design of algorithms signiﬁcantly. As a result, there is a substantial line of work on developing good differentially private methodology for learning and statistical estimation tasks, and on understanding how the sample size required for speciﬁc tasks increases relative to unconstrained algorithms. A typical task investigated in this area is distribution learning: informally, given records drawn i.i.d. from an unknown distribution
P , the algorithm aims to produce a description of a distribution Q that is close to P .
However, often the task at hand requires much less than full-ﬂedged learning. We may simply need to generate a small amount of data that has the same distributional properties as the population, or perhaps simply “looks plausible” for the population. For example, one might need realistic data for debugging a software program or for getting a quick idea of the range of values in a dataset. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we study a basic problem that captures these seemingly less stringent aims. Informally, the goal is to design a sampling algorithm that, given a dataset with n observations drawn i.i.d. from a distribution P , generates a single observation from a distribution Q that is close to P .
To formulate the problem precisely, consider a randomized algorithm A : U n → U that takes as input a dataset of n records from some universe U and outputs a single element of U. Given a distribution P on U, let X = (X1, ...Xn) be a random dataset with entries drawn i.i.d. from P . Let A(X) denote the random variable corresponding to the output of the algorithm A on input X. This random variable depends on both the selection of entries of X from P and the coins of A. Let QA,P denote the distribution of A(X), so that
QA,P (z) =
Pr
X ∼i.i.d.P coins of A (A(X) = z) = (cid:88) (cid:16)
Pr(X = x) · Pr (A(x) = z) coins of A (cid:17)
. x∈U n
The dataset size n is a parameter of A and thus deﬁned implicitly. We measure the closeness between the input and output distributions in total variation distance, denoted dT V .
Deﬁnition 1.1 (Accuracy of sampling [4]). A sampler A is α-accurate on a distribution P if dT V (QA,P , P ) ≤ α.
A sampler is α-accurate on a class C of distributions if it is α-accurate on every P in C.
The class C in the accuracy deﬁnition above effectively encodes what the sampler is allowed to assume about P . For example, C might include all distributions on k elements or all product distributions on
{0, 1}d (that is, distributions on d-bit strings for which the individual binary entries are independent).
Our aim in formulating Deﬁnition 1.1 was to capture the weakest reasonable task that abstracts the goal of generating data with the same distributional properties as the input data. Without any privacy constraints, this task is trivial to achieve: an algorithm that simply outputs its ﬁrst input record will sample from exactly the distribution P , so the interesting problem is to generate a sample of size m > n when given only n observations (as in the work of Axelrod et al. [4], which inspired our investigation). However, a differentially private algorithm cannot, in general, simply output one of its records in the clear. Even producing a single correctly distributed sample is a non-trivial task.
To understand the task and compare our results to existing work, it is helpful to contrast our deﬁnition of sampling with the more stringent goal of learning a distribution. A learning algorithm gets input in the same format as a sampling algorithm. We state a deﬁnition of distribution learning formulated with a single parameter α that captures both the distance between distributions and the failure probability.
Deﬁnition 1.2 (Distribution learning). An algorithm B learns a distribution class C to within error α if, given a dataset consisting of independent observations from a distribution P ∈ C, algorithm B outputs a description of a distribution that satisﬁes (cid:0)dT V (B(X), P ) ≤ α(cid:1) ≥ 1 − α.
Pr
X ∼i.i.d.P coins of B
An algorithm B that learns class C to within error α immediately yields a 2α-accurate sampler for class C: the sampler ﬁrst runs B to get a distribution ˆP and then generates a single sample from ˆP .
Thus, it is instructive to compare bounds on the dataset size required for sampling to known results on the sample complexity of distribution learning. Lower bounds for sampling imply lower bounds for all more stringent tasks, including learning, whereas separations between the complexity of sampling and that of learning suggest settings where weakening the learning objective might be productive.
Differential privacy Differential privacy is a constraint on the algorithm A that processes a dataset.
It does not rely on any distributional property of the data. We generally use uppercase letters (e.g.,
X = (X1, ..., Xn)) when viewing the data as a random variable, and lowercase symbols (e.g., x = (x1, ..., xn)) when treating the data as ﬁxed. Two datasets x, x(cid:48) ∈ U n are neighbors if they differ in at most one entry. If each entry corresponds to the data of one person, then neighboring datasets differ by replacing one person’s data with an alternate record. Informally, differential privacy requires that an algorithm’s output distributions are similar on all pairs of neighboring datasets.
Deﬁnition 1.3 (Differential Privacy [15, 14]). A randomized algorithm A : U n → Y is (ε, δ)-differentially private (in short, (ε, δ)-DP) if for every pair of neighboring datasets x, x(cid:48) ∈ U n and for all subsets Y ⊆ Y,
Pr[A(x) ∈ Y ] ≤ eε · Pr[A(x(cid:48)) ∈ Y ] + δ. 2
Table 1: Sample complexity of sampling and estimation tasks. Our negative results hold for (ε, δ)-differential privacy when δ < 1/n. In this table, ε ≤ 1 and δ = 1/nc for constant c > 1.
Nonprivate learning (ε, δ)-DP sampling (this work)
Distributions on [k] (cid:19)
Θ (cid:18) k
α2 (cid:18) k
αε
Theorems 1.4, 1.5
Θ (cid:19)
Product distributions on {0, 1}d (cid:19)
˜O (cid:18) d
αε
Theorem 1.6 (ε, δ)-DP learning
Θ (cid:18) k
α2 + (cid:19) k
αε
[11]
Ω(d)
Theorem 1.7 Ω (cid:18) d d
α2
αε
˜Θ
+ (cid:18) d
α2
Θ
Product distributions with pj ∈ [ 1 (cid:19) 3 , 2 3 ] (cid:33)
˜O (cid:32) √ d
ε
+ log d
α
Theorem 1.8 (cid:16)√ (cid:17) d/ε (cid:19)
[17, 8]
Theorem 1.9
For ε ≤ 1 and δ < 1/n, this guarantee implies, roughly, that one can learn the same things about any given individual from the output of the algorithm as one could had their data not been used in the computation [19]. When δ = 0, the guarantee is referred to as pure differential privacy. 1.1 Our results
We initiate a systematic investigation of the sample complexity of differentially private sampling. We provide upper and lower bounds for the sample size needed for this task for three natural families of distributions: arbitrary distributions on [k] = {1, . . . , k}, arbitrary product distributions on {0, 1}d, and product distributions on {0, 1}d with bounded bias. We demonstrate that, in some parameter regimes, private sampling requires asymptotically fewer samples than learning a description of P nonprivately; in other regimes, however, sampling proves to be as difﬁcult as private learning.
Our results are summarized in Table 1. Proofs of all results are included in the supplementary material. For simplicity, the table and our informal discussions focus on (ε, δ)-differential privacy (Deﬁnition 1.3) in the setting1 where δ = 1/nc for some constant c > 1. (cid:1)
Let Ck be the class of k-ary distributions (that is, distributions on [k]). We show that n = Θ(cid:0) k observations are necessary and sufﬁcient for differentially private sampling from Ck in the worst case.
Theorem 1.4. For all k ≥ 2, ε > 0, and α ∈ (0, 1), there exists an (ε, 0)-DP sampler that is
α-accurate on the distribution class Ck for datasets of size n = O( k
Theorem 1.5. For all k ≥ 2, n ∈ N, α ∈ (0, 1 5000n ], if there is an ((cid:15), δ)-DP sampler that is α-accurate on the distribution class Ck on datasets of size n, then n = Ω(cid:0) k 50 ], ε ∈ (0, 1], and δ ∈ (0,
αε ). (cid:1).
αε 1
αε
The second major class we consider consists of products of d Bernoulli distributions, for d ∈ N. We denote this class by B⊗d. Each distribution in B⊗d is described by a vector (p1, ..., pd) ∈ [0, 1]d of d probabilities, called biases; a single observation in {0, 1}d is generated by ﬂipping d independent coins with respective probabilities p1, ..., pd of heads. We show that n = ˜Θ(cid:0)d(cid:1) observations are necessary and sufﬁcient for differentially private sampling from B⊗d in the worst case.
Theorem 1.6. For all d, n ∈ N and (cid:15), δ, α ∈ (0, 1), there exists an ((cid:15), δ)-DP sampler that is
α-accurate on the distribution class B⊗d for datasets of size n = ˜O(cid:0) d (cid:1), assuming log(1/δ) = poly(log n). (cid:3),
Theorem 1.7. For all sufﬁciently small α > 0, and for all d, n ∈ N, ε ∈ (0, 1], and δ ∈ (cid:2)0, if there is an (ε, δ)-DP sampler that is α-accurate on the distribution class B⊗d on datasets of size n, then n = Ω(d). 1 5000n
α(cid:15)
Finally, we give better samplers and matching lower bounds for Bernoulli distributions and, more generally, products of Bernoulli distributions, with bias bounded away from 0 and 1. For simplicity, 1This setting precludes trivial solutions (which are possible when δ = Ω(1/n)), but allows us to treat factors of log(1/δ) as logarithmic in n and absorb them in ˜O expressions. 3
3 , 2 we consider distributions with bias pj ∈ [ 1 3 ] in each coordinate j ∈ [d]. For this class, we show d/ε, signiﬁcantly that differentially private sampling can be performed with datasets of size roughly smaller than in the general case. Curiously, the accuracy parameter α has almost no effect on the sample complexity. For Bernoulli distributions with bounded bias, we achieve this with pure differential privacy, that is, with δ = 0. For products of Bernoulli distributions, we need δ > 0.
Theorem 1.8. For all d ∈ N and (cid:15), δ, α ∈ (0, 1), there exists an (ε, δ)-DP sampler that is α-accurate on the class of products of d Bernoulli distributions with biases in (cid:2) 1 (cid:3) for datasets of size
ε + log 1 3 , 2
. When d = 1, the sampler has δ = 0 and n = O( 1
+ log d
α n = O d log(1/δ)
α ).
√
√ (cid:16) (cid:17) 3
ε
Theorem 1.9. For all sufﬁciently small α > 0, and for all d, n ∈ N, ε ∈ (0, 1], and δ ∈ [0, 1 100n ], if there exists an (ε, δ)-DP sampler that is α-accurate on the class of products of d Bernoulli distributions with biases in (cid:2) 1 (cid:3) on datasets of size n, then n = Ω( d/ε).
√ 3 , 2 3
Implications Our results show that the sample complexity of private sampling can differ substan-tially from that of private learning (for which known bounds are stated in Table 1). In some settings, sampling is much easier than learning: for example, for products of Bernoulli distributions with d instead bounded biases, private sampling has a lower dependence on the dimension (speciﬁcally, of d) and essentially no dependence on α. Even for arbitrary biases or arbitrary k-ary distributions, private sampling is easier when α (cid:28) ε. In other settings, however, private sampling can be as hard as private learning: e.g., for (cid:15) ≤ α, the worst-case complexity of sampling and learning k-ary distributions and product distributions is the same.
√
A more subtle point is that, in settings where private sampling is as hard as private learning, sampling accounts for the entire cost of privacy in learning. Speciﬁcally, the optimal sample complexity (cid:1) (e.g., see [3, of differentially private learning for arbitrary k-ary distributions is n = Θ(cid:0) k
Theorem 13]). This bound is the sum of two terms: the sample complexity of nonprivate learning (cid:1)). One interpretation of our (Θ(cid:0) k
α2 result that private sampling requires n = Θ(cid:0) k (cid:1) observations is that the extra privacy term in the complexity of learning can be explained by the complexity of privately generating a single sample with approximately correct distribution. (cid:1)) plus a term to account for the privacy constraint (Θ(cid:0) k
α2 + k
αε
αε
αε
Another implication of our results is that, in some settings, the distributions that are hardest for learning—nonprivate or private—are the easiest for sampling, and vice versa. Consider the simple case of Bernoulli distributions (i.e., product distributions with d = 1). The “hard” instances for nonprivate learning to within error α are distributions with bias p = 1±α 2 , but private sampling is easiest in that parameter regime. In contrast, the “hard” instances in our Ω( 1
αε ) lower bound for Bernoulli distributions have bias 10α, that is, close to 0 as opposed to close to 1/2. A simple variance argument shows that nonprivate learning is easy in that parameter regime, requiring only
O( 1
α ) observations. Similarly, for product distributions, we show that the complexity of private sampling is only ˜Θ( d) when biases are bounded away from 0 and 1. For the same class, however, the complexity of private and nonprivate learning is Θ(d).
√
Our ﬁnal point is that our lower bounds for k-ary distributions and general product distributions only require that the sampler generate a value in the support of the distribution with high probability. They thus apply to a weaker problem, similar in spirit to the interior point problem that forms the basis of lower bounds for private learning of thresholds [9].
Taken together, our results show that studying the complexity of generating a single sample helps us understand the true source of difﬁculty for certain tasks and sheds light on when we might be able to engage in nontrivial statistical tasks with very little data.
Limitations of our results and open questions Our work raises many questions about the com-plexity of private sampling. First, our upper bounds achieve only the minimal goal of generating a single observation. In most settings, one would presumably want to generate several such samples.
One can do so by repeating our algorithms several times on disjoint subsamples, but in general this is not the best way to proceed. Second, we study only three classes of distributions. It is likely that the picture of what is possible for many classes is more complex and nuanced. It would be interesting, for example, to study private sampling for Gaussian distributions, since they demonstrate intriguing data/accuracy tradeoffs for nonprivate sampling [4]. 4
Societal impact We study the feasibility of basic inference under privacy constraints. Our work is motivated by societal concerns, but focused on fundamental theoretical limits. We do not anticipate direct practical impact. 1.2 An overview of our proofs and techniques
For both algorithms and lower bounds, our results require the development of new techniques. On the algorithmic side, we take advantage of the fact that sampling algorithms need only be correct on average over samples drawn from a given distribution. One useful observation that underlies our positive results is that sampling based on an unbiased estimate ˆP of a probability distribution P (in the sense that E[ ˆP (u)] = P (u) for all elements u in the universe U, where the expectation is taken over the randomness in the dataset and the coins of the algorithm) has zero error, even though the learning error, e.g., dT V ( ˆP , P ) might be large. For product distributions with bounded biases, we also exploit the randomness of the sampling process itself to gain privacy without explicitly adding noise.
For negative results, we cannot generally use existing lower bounds for learning or estimation, because of a fundamental obstacle. The basic framework used in proving most lower bounds on sample complexity of learning problems is based on identiﬁability: to show that a large sample is required to learn class C, one ﬁrst ﬁnds a set of distributions P1, ...., Pt in the class C that are far apart from each other and then shows that the output of any sufﬁciently accurate learning algorithm allows an outside observer to determine exactly which distribution in the collection generated the data. The ﬁnal step is to show that algorithms in a given family (say, differentially private algorithms with a certain sample size) cannot reliably identify the correct distribution. This general approach is embodied in recipes such as Fano’s, Assouad’s, and Le Cam’s methods from classical statistics (see, e.g., [3] for a summary of these methods and their differentially private analogues). For many sampling problems, the identiﬁability approach breaks down: a single observation is almost never enough to identify the originating distribution.
One of our approaches to proving lower bounds is to leverage ways in which the algorithm’s output directly signals a failure to sample correctly. For instance, our lower bound for k-ary distribution relies on the fact that an α-accurate sampler must produce a value in the support of the true distribution P with high probability. Another approach is to reduce from other distribution (sampling or estimation) problems. For example, our lower bound for product distributions with bounded biases is obtained via a reduction from an estimation problem, by observing that a small number of samples from a nearby distribution sufﬁces for a very weak estimate of the underlying distribution’s attribute biases.
We break down our discussion of techniques according to the speciﬁc distribution classes we consider.
Distributions on [k] For the class of distributions on [k], Theorem 1.4 shows that α-accurate (ε, 0)-differently private sampling can be performed with a dataset of size O( k
αε ). Our private sampler computes, for each j ∈ [k], the proportion ˆPj of its dataset that is equal to j, adds Laplace noise to each count, uses L1 projection to obtain a valid vector of probabilities ˜P = ( ˜P1, . . . , ˜Pk), and ﬁnally outputs an element of [k] sampled according to ˜P .
Theorem 1.5 provides a matching lower bound on n that holds for all (ε, δ)-differentially private algorithms with δ = o(1/n). We prove our lower bound separately for Bernoulli distributions and for discrete distributions with support size k ≥ 3, using different analyses. For Bernoulli distributions, we
ﬁrst exploit the group privacy for differentially private algorithms and the fact that the sampler must be accurate for the Bernoulli distribution Ber(0) to show that, on input with t ones, a differentially private sampler outputs 1 with probability at most 2αeεt. Then we consider P = Ber(10α). We use
α-accuracy in conjunction with group privacy to give a lower and an upper bound on the probability of the output being 1 when the input is drawn i.i.d. from P . This allows us to relate the parameters involved in order to obtain the desired lower bound on n.
The lower bound for distributions on [k] with k ≥ 3 is more involved. We start by identifying general properties of samplers that allow us to restrict our attention to relatively simple algorithms. First, we observe that every sampler can be converted to a Poisson algorithm, that is, an algorithm that, instead of receiving an input of a ﬁxed size, gets an input with the number of records that follows a
Poisson distribution. This observation allows us to use a standard technique called Poissonization that 5
makes the empirical frequencies of different elements independent. Next, we observe that privacy for samplers can be easily ampliﬁed, so that we can assume w.l.o.g. that ε is small. Finally, we observe that every sampler for the class of k-ary distributions can be converted to a frequency-count-based algorithm. A sampler is frequency-count-based if the probability it outputs a speciﬁc element depends only on the number of occurrences of this element in its input and the frequency counts2 of the input (that is, the number of elements that occur zero times, once, twice, thrice, and so on).
Frequency-count-based algorithms have been studied for a long time in the context of understanding properties of distributions (see, e.g., [5, 6, 20]).
Equipped with the three observations, we restrict our attention to Poisson, frequency-count-based algorithms, with small ε in the privacy guarantee. In contrast to our lower bound for Bernoulli samplers, we show that when the support size is at least 3 and the dataset size, n, is too small, the sampler is likely to output an element outside of the support of the input distribution P. Here, we exploit group privacy, which implies that the probability that a sampler outputs a speciﬁc element which appears j times in its input differs by at most a factor of eεj from the probability that it outputs a speciﬁc element that does not appear in the input. Then we consider a distribution P that has most of its mass (speciﬁcally, 1 − O(α)) on a special element, and the remaining mass spread uniformly among half of the remaining domain elements. That is, P is a mixture of a unit distribution on the special element and a uniform distribution on half of the remaining elements. We show that, when the dataset size is too small, the sampler is nearly equally likely to output any non-special element.
But it has to output non-special elements with probability Ω(α) to be α-accurate. This means that when the database size is too small, the sampler outputs a non-special element outside the support of
P with probability Ω(α). The details of the proof are quite technical and appear in Section 2.
Product distributions Our private sampler for product distributions over {0, 1}d, used to prove
Theorem 1.6, builds on the recursive private preconditioning procedure designed by Kamath et al. [17] for learning this distribution class. In our case, the sampler gets a dataset of size which is asymptotically smaller than necessary for learning this distribution class in some important parameter regimes. Let (p1, . . . , pd) be the attribute biases for the product distribution P from which the data is drawn. For simplicity, assume w.l.o.g. that all the marginal biases pj are less than 1/2. The main idea in [17] is that smaller biases have lower sensitivity in the following sense: if we know that a set of attributes has biases pj that are all at most some bound u then, since the data is generated from a product distribution, the number of ones in those attributes should at most 2ud with high probability. We can enforce this bound on the number of ones in those coordinates by truncating each row appropriately, and thus learn the biases of those coordinates to higher accuracy than we knew before. Building on that idea, we partition the input into smaller datatsets and run our algorithm in rounds, each using fresh data, a different truncation threshold, and noise of different magnitude.
Our algorithm consists of two phases. In the bucketing phase, we use half of the dataset and the technique of [17] to get rough multiplicative estimates of all biases pj except the very small ones (where pj < 1/d). This allows us to partition the coordinates into log(d) buckets, where biases within each bucket are similar. We show this crude estimation only requires n = ˜O(d/αε). In the sampling phase, we use the buckets to generate our output sample. For each bucket, we can get a fresh estimate of the biases using the other half of the dataset and, again, the technique from [17] to scale the noise proportionally to the upper bound on the biases that bucket. These estimates are essentially unbiased. Flipping d coins independently according to the estimated biases produces an observation with essentially the correct distribution.
The proof of our lower bound for general product distributions proceeds via reduction from sam-pling of k-ary distributions for k = d. In contrast, lower bounds for private learning of product distributions rely on ﬁngerprinting codes (building on the framework of Bun et al. [10]). Although
ﬁngerprinting codes are indeed useful when reasoning about samplers for distributions with bounded biases (discussed below), our approach relies, instead, on the fact that samplers must distinguish coordinates with bias 0 from coordinates with small bias. Speciﬁcally, given a distribution P on [2k] that is uniform on a subset of [2k] of size k, we deﬁne a product distribution P (cid:48) on {0, 1}2k with biases pj = P (j) = 1 k for j ∈ S and pj = 0 otherwise. We use Poissonization and coupling between
Poisson and binomial distributions to design a reduction that, given observations from P , ﬁrst creates an appropriately distributed sample of almost the same size drawn according to P (cid:48), then runs a 2The vector of frequency counts is called a ﬁngerprint or a histogram in previous work. 6
hypothetical sampler for product distributions to get a vector in {0, 1}2k (drawn roughly according to P (cid:48)), and ﬁnally converts that vector back to a single element of [2k] distributed roughly as P .
The details are subtle since most draws from P (cid:48) will not have exactly one nonzero element—this complicates conversion in both directions.
Distributions with bounded bias
Interestingly, our sampler for product distributions with bounded bias does not directly add noise to data. It performs the following step independently for each attribute: compute the empirical mean of the sample in that coordinate, obtain the clipped mean by rounding the empirical mean to the interval [1/4, 3/4], and sample a bit according to the clipped mean. The key idea in the analysis of accuracy is that, conditioned on no rounding (that is, the empirical mean already being in the required interval), the new bit is sampled from the correct distribution, and rounding occurs with small probability. We argue that this sampler is (4/n, 0)-differentially private for the case when d = 1. For larger d, the sampler is a composition of d differentially private algorithms, and the main bound follows from compositions theorems (and conversions between different variants of differential privacy).
The lower bound for this class proceeds by a reduction from the following weak estimation problem:
Given samples from a product distribution with biases p1, p2, ..., pd, output estimates ˜p1, ..., ˜pd such that each ˜pi is within additive error 1 20 , with probability at least 1 − 1 20 of pi, that is, |˜pi − pi| ≤ 1 20 (where 1 20 is just a sufﬁciently small constant). This problem is known to require datasets of size
√
˜Ω( d) for differentially private algorithms [10]. However, nonprivate algorithms require only O(1) data records to solve this same problem! We can thus reduce from estimation to sampling with very little overhead: Given a private sampler, we run it a constant number of times on disjoint datasets to obtain enough samples to (nonprivately) estimate each of the pi’s. The nonprivate estimation is a postprocessing of an output of a differentially private algorithm, so the overall algorithm is differentially private. Some care is required in the reduction, since we must ensure that the nonprivate estimation algorithm is robust (i.e., works even when the samples are only close in TV distance to the correct distribution) and that the lower bound of [10] applies even when the biases pi lie in [ 1 3 , 2 3 ]. 1.3