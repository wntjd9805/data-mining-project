Abstract
Policy networks are a central feature of deep reinforcement learning (RL) algo-rithms for continuous control, enabling the estimation and sampling of high-value actions. From the variational inference perspective on RL, policy networks, when used with entropy or KL regularization, are a form of amortized optimization, opti-mizing network parameters rather than the policy distributions directly. However, direct amortized mappings can yield suboptimal policy estimates and restricted distributions, limiting performance and exploration. Given this perspective, we con-sider the more ﬂexible class of iterative amortized optimizers. We demonstrate that the resulting technique, iterative amortized policy optimization, yields performance improvements over direct amortization on benchmark continuous control tasks.
Accompanying code: github.com/joelouismarino/variational_rl. 1

Introduction
Reinforcement learning (RL) algorithms involve policy evaluation and policy optimization [73].
Given a policy, one can estimate the value for each state or state-action pair following that policy, and given a value estimate, one can improve the policy to maximize the value. This latter procedure, policy optimization, can be challenging in continuous control due to instability and poor asymptotic performance. In deep RL, where policies over continuous actions are often parameterized by deep networks, such issues are typically tackled using regularization from previous policies [67, 68] or by maximizing policy entropy [57, 23]. These techniques can be interpreted as variational inference
[51], using optimization to infer a policy that yields high expected return while satisfying prior policy constraints. This smooths the optimization landscape, improving stability and performance [3].
However, one subtlety arises: when used with entropy or KL regularization, policy networks perform amortized optimization [26]. That is, rather than optimizing the action distribution, e.g., mean and variance, many deep RL algorithms, such as soft actor-critic (SAC) [31, 32], instead optimize a network to output these parameters, learning to optimize the policy. Typically, this is implemented as a direct mapping from states to action distribution parameters. While such direct amortization schemes have improved the efﬁciency of variational inference as “encoder” networks [44, 64, 56], they also suffer from several drawbacks: 1) they tend to provide suboptimal estimates [20, 43, 55], yielding a so-called “amortization gap” in performance [20], 2) they are restricted to a single estimate
[27], thereby limiting exploration, and 3) they cannot generalize to new objectives, unlike, e.g., gradient-based [36] or gradient-free optimizers [66].
Inspired by techniques and improvements from variational inference, we investigate iterative amor-tized policy optimization. Iterative amortization [55] uses gradients or errors to iteratively update the parameters of a distribution. Unlike direct amortization, which receives gradients only after
∗Now at DeepMind, London, UK. Correspondence to josephmarino@deepmind.com. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
outputting the distribution, iterative amortization uses these gradients online, thereby learning to iteratively optimize. In generative modeling settings, iterative amortization empirically outperforms direct amortization [55, 54] and can ﬁnd multiple modes of the optimization landscape [27].
The contributions of this paper are as follows:
• We propose iterative amortized policy optimization, exploiting a new, fruitful connection between amortized variational inference and policy optimization.
• Using the suite of MuJoCo environments [78, 12], we demonstrate performance improve-ments over direct amortized policies, as well as more complex ﬂow-based policies.
• We demonstrate novel beneﬁts of this amortization technique: improved accuracy, providing multiple policy estimates, and generalizing to new objectives. 2