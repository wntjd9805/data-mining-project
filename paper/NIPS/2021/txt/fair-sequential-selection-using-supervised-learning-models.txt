Abstract
We consider a selection problem where sequentially arrived applicants apply for a limited number of positions/jobs. At each time step, a decision maker accepts or rejects the given applicant using a pre-trained supervised learning model until all the vacant positions are filled. In this paper, we discuss whether the fairness notions (e.g., equal opportunity, statistical parity, etc.) that are commonly used in classifica-tion problems are suitable for the sequential selection problems. In particular, we show that even with a pre-trained model that satisfies the common fairness notions, the selection outcomes may still be biased against certain demographic groups.
This observation implies that the fairness notions used in classification problems are not suitable for a selection problem where the applicants compete for a limited number of positions. We introduce a new fairness notion, “Equal Selection (ES),” suitable for sequential selection problems and propose a post-processing approach to satisfy the ES fairness notion. We also consider a setting where the applicants have privacy concerns, and the decision maker only has access to the noisy version of sensitive attributes. In this setting, we can show that the perfect ES fairness can still be attained under certain conditions. 1

Introduction
Machine learning (ML) techniques have been increasingly used for automated decision-making in high-stake applications such as criminal justice, loan application, face recognition surveillance, etc.
While the hope is to improve societal outcomes with these ML models, they may inflict harm by being biased against certain demographic groups. For example, companies such as IBM, Amazon, and
Microsoft had to stop sales of their face recognition surveillance technology to the police in summer 2020 because of the significant racial bias [1, 2]. COMPAS (Correctional Offender Management
Profiling for Alternative Sanctions), a decision support tool widely used by courts across the United
States to predict the recidivism risk of defendants, is biased against African Americans [3]. In lending, the Apple card application system has shown gender biases by assigning a lower credit limit to females than their male counterparts [4].
To measure and remedy the unfairness issues in ML, various fairness notions have been proposed.
They can be roughly classified into two categories [5]: 1) Individual fairness: it implies that similar individuals should be treated similarly [6, 7, 8]. 2) Group fairness: it requires that certain statistical measures to be equal across different groups [9, 10, 11, 12, 13, 14, 15]. In this work, we mainly focus on the notions of group fairness. We consider a sequential selection problem where a set of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
applicants compete for limited positions and sequentially enter the decision-making system.1 At each time step, a decision maker accepts or rejects an applicant until m positions are filled. Each applicant can be either qualified or unqualified and has some features related to its qualification state. While applicants’ true qualification states are hidden to the decision maker, their features are observable.
We assume the decision maker has access to a pre-trained supervised learning model, which maps each applicant’s features to a predicted qualification state (qualified or unqualified) or a qualification score indicating the applicant’s likelihood of being qualified. Decisions are then made based on these qualification states/scores. Note that this pre-trained model can possibly be biased or satisfy certain group fairness notions (e.g., equal opportunity, statistical parity, etc.).
To make a fair selection with respect to multiple demographic groups, each applicant’s group membership (sensitive attribute) is often required. However, in many scenarios, such information can be applicants’ private information, and applicants may be concerned about revealing them to the decision maker. As such, we further consider a scenario where instead of the true sensitive attribute, each applicant only reveals a noisy version of the sensitive attribute to the decision maker. We adopt the notion of local differential privacy [16] to measure the applicant’s privacy. This notion has been widely used by researchers [17, 18, 19] and has been implemented by Apple, Google, Uber, etc.
In this paper, we say the decision is fair if the probability that each position is filled by a qualified applicant from one demographic group is the same as the probability of filling the position by a qualified applicant from the other demographic group. We call this notion equal selection (ES). We first consider the case where the decision maker has access to the applicants’ true sensitive attributes.
With no limit on the number of available positions (i.e., no competition), our problem can be cast as classification, and statistical parity and equal opportunity constraints are suitable for finding qualified applicants. However, when the number of acceptances is limited (e.g., job application, college admission, award nomination), we can show that the decisions made based on a pre-trained model satisfying statistical parity or equal opportunity fairness may still result in discrimination against a demographic group. It implies that the fairness notions (i.e., statistical parity and equal opportunity) defined for classification problems, are not suitable for sequential selection problems with the limited number of acceptances. We then propose a post-processing method by solving a linear program, which can find a predictor satisfying the ES fairness notion. Our contributions can be summarized as follows, 1. We introduce Equal Selection (ES), a fairness notion suitable for the sequential selection problems which ensures diversity among the selected applicants.To the best of our knowledge, this is the first work that studies the fairness issue in sequential selection problems. 2. We show that decisions made based on a pre-trained model satisfying statistical parity or equal opportunity fairness notion may still lead to an unfair and undesirable selection outcome. To address this issue, we use the ES fairness notion and introduce a post-processing approach which solves a linear program and is applicable to any pre-trained model. 3. We also consider a scenario where the applicants have privacy concerns and only report the differentially private version of sensitive attributes. We show that the perfect ES fairness is still attainable even when applicants’ sensitive attributes are differentially private. 4. The experiments on real-world datasets validate the theoretical results.