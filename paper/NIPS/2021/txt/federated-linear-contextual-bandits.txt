Abstract
This paper presents a novel federated linear contextual bandits model, where indi-vidual clients face different K-armed stochastic bandits coupled through common global parameters. By leveraging the geometric structure of the linear rewards, a collaborative algorithm called Fed-PE is proposed to cope with the heterogeneity across clients without exchanging local feature vectors or raw data. Fed-PE relies on a novel multi-client G-optimal design, and achieves near-optimal regrets for both disjoint and shared parameter cases with logarithmic communication costs. In addition, a new concept called collinearly-dependent policies is introduced, based on which a tight minimax regret lower bound for the disjoint parameter case is derived. Experiments demonstrate the effectiveness of the proposed algorithms on both synthetic and real-world datasets. 1

Introduction
Federated learning (FL) (McMahan et al., 2017) is an emerging distributed machine learning (ML) paradigm where massive number of clients collaboratively learn a shared prediction model while keeping all the training data on local devices. Compared with standard centralized machine learning,
FL has the following characteristics (Kairouz et al., 2021):
• Heterogeneous local datasets. The local datasets, which are often generated at edge devices, are likely drawn from non-independent and identically distributed (non-IID) distributions.
• Communication efﬁciency. The communication cost scales with the number of clients, which is one of the primary bottlenecks of FL. It is critical to minimize the communication cost while maintaining the learning accuracy.
• Privacy. FL protects local data privacy by only sharing model updates instead of the raw data.
While the main focus of the state-of-the-art FL is on the supervised learning setting, recently, a few researchers begin to extend FL to the multi-armed bandits (MAB) framework (Lai and Robbins, 1985; Auer et al., 2002; Bubeck and Cesa-Bianchi, 2012; Agrawal and Goyal, 2012, 2013a). In the canonical setting of MAB, a player chooses to play one arm from a set of arms at each time slot.
An arm, if played, will offer a reward that is drawn from its distribution which is unknown to the player. With all previous observations, the player needs to decide which arm to pull each time in order to maximize the cumulative reward. MAB thus represents an online learning model that naturally captures the intrinsic exploration-exploitation tradeoff in many sequential decision-making problems.
Extending FL to the MAB framework is naturally motivated by a corpus of applications, such as recommender systems, clinical trials, and cognitive radio. In those applications, the sequential decision making involves multiple clients and is distributed by nature. While classical MAB models assume immediate access to the sequentially generated data at the learning agent, under the new 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
realm of FL, local datasets can be stored and analyzed at the clients, thus reducing the communication load and potentially protecting the data privacy.
Despite the potential beneﬁts of FL, the sequential decision making and bandit feedback bring new challenges to the design of FL algorithms in the MAB setting. Different from the supervised learning setting where static datasets are collected beforehand, under the MAB setting, data is generated sequentially as decisions are made, actions are taken, and observations are collected. In order to maximize the cumulative reward and minimize the corresponding learning regret, it thus requires sophisticated coordination of the actions of the clients. The heterogeneous reward distributions across clients make the coordination process even more convoluted and challenging. Besides, the data privacy and communication efﬁciency requirements result in signiﬁcant challenges for efﬁcient information exchange and aggregation between local clients and the central server.
In this work, we attempt to address those challenges in a federated linear contextual bandits framework.
This particular problem is motivated by the following exemplary applications.
• Personalized content recommendation. For content (arm) recommendation in web-services, user engagement (reward) depends on the proﬁle of a user (context). The central server may deploy a recommender system on each user’ local device (client) in order to personalize recommendations without knowing the personal proﬁle or behavior of the user.
• Personalized online education. In order to maximize students performances (reward) in online learning, the education platform (central server) needs to personalize teaching methods (arms) based on the characteristics of individual students (context). With the online learning software installed at local devices (client), it is desirable to personalize the learning experiences without allowing the platform to access students’ characteristics or scores.
In those examples, the reward of pulling the same arm at different clients follows different distributions dependent on the context as in contextual bandits (Auer, 2003; Langford and Zhang, 2008). We note that conventional contextual bandits is deﬁned with respect to a single player, where the time-varying context can be interpreted as different incoming user proﬁles. In contrast, we consider a multi-client model, where each client is associated with a ﬁxed user proﬁle. The variation of contexts is captured over clients as opposed to over time. Although the set of clients remains ﬁxed through the learning process, the reward of pulling the same arm still varies across clients. Such a model naturally takes data heterogeneity into consideration. Besides, we adopt a linear reward model, which has been widely studied in contextual bandits (Li et al., 2010; Agrawal and Goyal, 2013b).
Main contributions. Our main contributions are summarized as follows.
First, we propose a new federated linear contextual bandits model that takes the diverse user pref-erences and data heterogeneity into consideration. Such a model naturally bridges local stochastic bandits with linear contextual bandits, and is well poised to capture the tradeoffs between communi-cation efﬁciency and learning performances in the federated bandits setting.
Second, we design a novel algorithm named Fed-PE and further develop its variants to solve the federated linear contextual bandits problem. Under Fed-PE, clients only upload their local estimates of the global parameters without sharing their local feature vectors or raw observations. It not only keeps the personal information private, but also reduces the upload cost. We explicitly show that Fed-PE and its variants achieve near-optimal regret performances for both disjoint and shared parameter cases with logarithmic communication costs.
Third, we generalize the G-optimal design from the single-player setting (Lattimore and Szepesvári, 2020) to the multi-client setting. We develop a block coordinate ascent algorithm to solve the generalized G-optimal design efﬁciently with convergence guarantees. Such a multi-client G-optimal design plays a vital role in Fed-PE, and may ﬁnd broad applications in related multi-agent setups.
Finally, we introduce a novel concept called collinearly-dependent policy and show that the cele-brated LinUCB type of policies (Li et al., 2010), Thompson sampling based policies with Gaussian priors (Agrawal and Goyal, 2013b), and least squared estimation based policies, such as Fed-PE, are all in this category. By utilizing the property of collinearly-dependent policies, we are able to characterize a tight minimax regret lower bound in the disjoint parameter setting. We believe that this concept may be of independent interest for the study of bandits with linear rewards. 2
Table 1: Performance comparison
Model
Linear
Linear contextual (shared parameter)
Linear contextual (disjoint parameter)
Algorithm
DELB
FedUCB1
Fed-PE (this work)
Lower bound
Centralized2
Fed-PE (this work)
Lower bound (this work)
Regret
O(d(cid:112)M T log(T ))
√
O( dM T log T )
O((cid:112)dM T log(KM T ))
√
Ω( dM T )
O((cid:112)dKM T log3(KM T ))
O((cid:112)dKM T log(KM T ))
√
Ω( dKM T )
Communication cost
O((M d + d log log d) log T )
O(M d2 log T )
O(M (d2 + dK) log T )
N/A
O(M d2KT )
O(M d2K log T )
N/A
M : number of clients; K: number of arms; T : time horizon; d: ambient dimension of the feature vectors. x(cid:124)V x. The range of a matrix A, denoted
Notations. Throughout this paper, we use (cid:107)x(cid:107)V to denote by range(A), is the subspace spanned by the column vectors of A. We use A† and Det(A) to denote the pseudo-inverse and pseudo-determinant of square matrix A, respectively. The speciﬁc deﬁnitions can be found in Appendix B of the supplementary material.
√ 2