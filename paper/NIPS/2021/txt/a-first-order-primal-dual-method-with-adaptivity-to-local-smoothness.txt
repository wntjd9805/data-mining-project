Abstract (cid:105) −
Ax, y (cid:104)
We consider the problem of ﬁnding a saddle point for the convex-concave objective g∗(y), where f is a convex function with locally minx maxy f (x) +
Lipschitz gradient and g is convex and possibly non-smooth. We propose an adaptive version of the Condat-V˜u algorithm, which alternates between primal gradient steps and dual proximal steps. The method achieves stepsize adaptivity and the norm of recently computed gradients through a simple rule involving (k−1) ergodic convergence rate. of f . Under standard assumptions, we prove an
Furthermore, when f is also locally strongly convex and A has full row rank we show that our method converges with a linear rate. Numerical experiments are provided for illustrating the practical performance of the algorithm.
O
A (cid:107) (cid:107) 1

Introduction
In this paper we study a particular instance of the composite minimization problem where f and g are convex, proper and lower-semicontinuous (l.s.c.), and A is a linear operator. min x∈X f (x) + g(Ax), (1)
Problems of the form (1) have been studied in the literature under various assumptions on f and g.
A is proximal-friendly and f is L-smooth, the objective is
For the particular instances where g suitable for applying forward-backward splitting algorithms like the Proximal Gradient algorithm and its accelerated counterpart [Nesterov, 2013, Beck and Teboulle, 2009]. In general, however, the proximal operator of g
A is not easily computable and in such cases a popular approach is to decouple A and g by reformulating problem (1) as a convex-concave saddle-point problem:
◦
◦ min x∈X max y∈Y (cid:104)
Ax, y
+ f (x) g∗(y), (cid:105) where g∗ denotes the Fenchel conjugate of g. Objective (2) is typically addressed by primal-dual splitting algorithms which, under strong duality, can recover the solution to the original problem (1).
In the particular case when f and g are proximal-friendly and possibly non-smooth, a very popular method is the Primal-Dual Hybrid Gradient proposed in [Chambolle and Pock, 2011], which was further extended to handle an additional L-smooth component with the Condat-V˜u algorithm [Condat, 2013, V˜u, 2013]. Convergence rates for the latter are studied in [Chambolle and Pock, 2016a].
− (2)
Together, these classes of algorithms cover a broad range of problems in diverse ﬁelds such as signal processing, machine learning, inverse problems, telecommunications and many others. As a result, a great amount of research effort has gone into addressing practical concerns such as robustness 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to inexact oracles, acceleration and automation of stepsize selection. For a comprehensive list of examples and theoretical details we refer the reader to review papers [Combettes and Pesquet, 2011,
Parikh and Boyd, 2014, Komodakis and Pesquet, 2015, Chambolle and Pock, 2016b]. In this work, we focus on the line of investigation studying stepsize regime automation for primal-dual algorithms targeting problem (2).
A
In their basic form, primal-dual methods require as input stepsize parameters belonging to a designated interval of stability, which depends on problem speciﬁc constants like the global smoothness parameter
L and
. Dependence on such constants is undesirable because they may be costly to compute and oftentimes one can only access upper-bound estimates, thus leading to overly-conservative stepsizes and slower convergence. Moreover, the need to know L for setting the stepsizes prevents these methods from being applied to functions which are not globally smooth. (cid:107) (cid:107)
Consequently, recent efforts have gone towards devising methods with adaptive stepsizes [Goldstein et al., 2013, 2015, Malitsky and Pock, 2018, Pedregosa and Gidel, 2018]. These approaches resort to linesearch for ﬁnding good stepsizes at every iteration, and exhibit improved practical performance.
It thus appears that better convergence comes at the cost of an indeterminate number of extra steps spent in subprocedures aimed at ﬁnding appropriate stepsizes. f is locally Lipschitz continuous and
In this work, we study problem (2) under the assumption that g is proximal-friendly. To illustrate the motivation of our framework, we take a prototypical example in image processing:
∇ min x 1 2 (cid:107)
Kx b 2 + λ
− (cid:107)
Dx (cid:107)2,1 , K
∈ (cid:107)
Rm×d, D
R2d×d,
∈ where x is an image, K is a problem-speciﬁc measurement operator, b is the vector of (possibly noisy) observations and D is the discrete gradient operator and the regularization term represents the isotropic TV norm. In order to apply any of the aforementioned primal-dual algorithms, one needs to
ﬁrst choose how to decouple the linear operators. There are three options: decoupling with respect to
K leaves us with having to compute the proximal operator of the TV norm for the primal step, which is an iterative procedure [Chambolle, 2004]. Decoupling D implies performing gradient steps on f , since in general its proximal operator is not efﬁcient. Finally, decoupling with respect to both implies increasing the dimensionality of the dual variable to m + 2d, which is problematic for large d and m.
The sensible choice is the second one (i.e., decoupling D), and the question we seek to answer with this work is:
Does there exist a method for solving (2) that adapts to the local problem geometry without resorting to linesearch?
Our contribution is to propose a ﬁrst-order primal-dual scheme that answers this question in the afﬁrmative and is accompanied by theoretical convergence guarantees. Using standard analysis f is locally Lipschitz and g is techniques we show an ergodic convergence of proximal-friendly, and a linear convergence rate for the case when f is in addition locally strongly convex and A has full row rank. We provide numerical experiments for sparse logistic regression and image inpainting, as well as use our method as a heuristic for TV-regularized nonconvex phase retrieval. (k−1) when
∇
O
The rest of the paper is structured as follows: Section 2 provides details about related work; Section 3 introduces notation, along with technical preliminaries and assumptions to be used in our analysis;
Section 4 reports the main theoretical results alongside partial proofs; ﬁnally, partial numerical results are provided in Section 5 with the rest being deferred to the appendix due to lack of space. 2