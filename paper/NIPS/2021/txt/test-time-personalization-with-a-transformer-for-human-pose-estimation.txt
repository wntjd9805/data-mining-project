Abstract
We propose to personalize a 2D human pose estimator given a set of test images of a person without using any manual annotations. While there is a signiﬁcant advancement in human pose estimation, it is still very challenging for a model to generalize to different unknown environments and unseen persons. Instead of using a ﬁxed model for every test case, we adapt our pose estimator during test time to exploit person-speciﬁc information. We ﬁrst train our model on diverse data with both a supervised and a self-supervised pose estimation objectives jointly.
We use a Transformer model to build a transformation between the self-supervised keypoints and the supervised keypoints. During test time, we personalize and adapt our model by ﬁne-tuning with the self-supervised objective. The pose is then improved by transforming the updated self-supervised keypoints. We experiment with multiple datasets and show signiﬁcant improvements on pose estimations with our self-supervised personalization. Project page with code is available at https://liyz15.github.io/TTP/.

Introduction 1
Recent years have witnessed a large advancement in human pose estimation. A lot of efforts have been spent on learning a generic deep network on large-scale human pose datasets to handle diverse appearance changes [59, 64, 7, 15, 43]. Instead of learning a generic model, another line of research is to personalize and customize human pose estimation for a single subject [10]. For a speciﬁc person, we can usually have a long video (e.g., instructional videos, news videos) or multiple photos from personal devices. With these data, we can adapt the model to capture the person-speciﬁc features for improving pose estimation and handling occlusion and unusual poses. However, the cost of labeling large-scale data for just one person is high and unrealistic.
In this paper, we propose to personalize human pose estimation with unlabeled video data during test time, namely, Test-Time Personalization. Our setting falls in the general paradigm of Test-Time
Adaptation [58, 35, 61, 69], where a generic model is ﬁrst trained with diverse data, and then it is
ﬁne-tuned to adapt to a speciﬁc instance during test time without using human supervision. This allows the model to generalize to out-of-distribution data and preserves privacy when training is distributed. Speciﬁcally, Sun et al. [58] propose to generalize image classiﬁcation by performing joint training with a semantic classiﬁcation task and a self-supervised image rotation prediction task [18]. During inference, the shared network representation is ﬁne-tuned on the test instance with the self-supervisory signal for adaptation. While the empirical result is encouraging, it is unclear how
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Test-Time Personalization. Our model is ﬁrstly trained on diverse data with both super-vised and self-supervised keypoint estimation tasks. During test time, we personalize the model using only the self-supervised target in single person domain and then predict with the updated model. During Test-Time Personalization, no continuous data is required but only unlabeled samples belonging to the same person are needed. Our method boosts performance at test time without costly labeling or sacriﬁcing privacy. the rotation prediction task can help image classiﬁcation, and what is the relation between two tasks besides sharing the same feature backbone.
Going beyond feature sharing with two distinct tasks, we introduce to perform joint supervised and self-supervised human keypoint estimation [26] tasks where the supervised keypoint outputs are directly transformed from the self-supervised keypoints using a Transformer [60]. In this way, when
ﬁne-tuning with the self-supervised task in test time, the supervised pose estimation can be improved by transforming from the improved self-supervised keypoints.
We adapt the self-supervised keypoint estimation task proposed by Jakab et al. [26]. The task is built on the assumption that the human usually maintains the appearance but changes poses across time in a video. Given a video frame, it trains a network to extract a tight bottleneck in the form of sparse spatial heatmaps, which only contain pose information without appearance. The training objective is to reconstruct the same frame by combining the bottleneck heatmaps and the appearance feature extracted from another frame. Note while this framework can extract keypoints to represent the human structure, they are not aligned with the semantic keypoints deﬁned in human pose estimation. Building on this model, we add an extra keypoint estimation objective which is trained with human supervision.
Instead of simply sharing features between two objectives as [58], we train a Transformer model on top of the feature backbone to extract the relation and afﬁnity matrix between the self-supervised keypoint heatmap and the supervised keypoint heatmap. We then use the afﬁnity matrix to transform the self-supervised keypoints as the supervised keypoint outputs. With our Transformer design, it not only increases the correlation between two tasks when training but also improves Test-Time
Personalization as changing one output will directly contribute to the the output of another task.
We perform our experiments with multiple human pose estimation datasets including Human 3.6M [24], Penn Action [71], and BBC Pose [8] datasets. As shown in Figure 1, our Test-Time
Personalization can perform on frames that continuously exist in a video and also with frames that are non-continuous as long as they are for the same person. We show that by using our approach for personalizing human pose estimation in test time, we achieve signiﬁcant improvements over baselines in all datasets. More interestingly, the performance of our method improves with more video frames appearing online for the same person during test time. 2