Abstract
The increasing computational requirements of deep neural networks (DNNs) have led to signiﬁcant interest in obtaining DNN models that are sparse, yet accurate.
Recent work has investigated the even harder case of sparse training, where the
DNN weights are, for as much as possible, already sparse to reduce computational costs during training. Existing sparse training methods are often empirical and can have lower accuracy relative to the dense baseline. In this paper, we present a general approach called Alternating Compressed/DeCompressed (AC/DC) training of DNNs, demonstrate convergence for a variant of the algorithm, and show that AC/DC outperforms existing sparse training methods in accuracy at similar computational budgets; at high sparsity levels, AC/DC even outperforms existing methods that rely on accurate pre-trained dense models. An important property of
AC/DC is that it allows co-training of dense and sparse models, yielding accurate sparse–dense model pairs at the end of the training process. This is useful in practice, where compressed variants may be desirable for deployment in resource-constrained settings without re-doing the entire training ﬂow, and also provides us with insights into the accuracy gap between dense and compressed models. The code is available at: https://github.com/IST-DASLab/ACDC. 1

Introduction
The tremendous progress made by deep neural networks in solving diverse tasks has driven signiﬁcant research and industry interest in deploying efﬁcient versions of these models. To this end, entire fami-lies of model compression methods have been developed, such as pruning [29] and quantization [22], which are now accompanied by hardware and software support [55, 8, 11, 43, 23].
Neural network pruning, which is the focus of this paper, is the compression method with arguably the longest history [38]. The basic goal of pruning is to obtain neural networks for which many connections are removed by being set to zero, while maintaining the network’s accuracy. A myriad pruning methods have been proposed—please see [29] for an in-depth survey—and it is currently understood that many popular networks can be compressed by more than an order of magnitude, in terms of their number of connections, without signiﬁcant accuracy loss.
Many accurate pruning methods require a fully-accurate, dense variant of the model, from which weights are subsequently removed. A shortcoming of this approach is the fact that the memory and computational savings due to compression are only available for the inference, post-training phase, and not during training itself. This distinction becomes important especially for large-scale modern models, which can have millions or even billions of parameters, and for which fully-dense training can have high computational and even non-trivial environmental costs [53].
One approach to address this issue is sparse training, which essentially aims to remove connections from the neural network as early as possible during training, while still matching, or at least approx-imating, the accuracy of the fully-dense model. For example, the RigL technique [16] randomly
∗Correspondence to: Alexandra Peste <alexandra.peste@ist.ac.at> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
removes a large fraction of connections early in training, and then proceeds to optimize over the sparse support, providing savings due to sparse back-propagation. Periodically, the method re-introduces some of the weights during the training process, based on a combination of heuristics, which requires taking full gradients. These works, as well as many recent sparse training approaches [4, 44, 32], which we cover in detail in the next section, have shown empirically that non-trivial computational savings, usually measured in theoretical FLOPs, can be obtained using sparse training, and that the optimization process can be fairly robust to sparsiﬁcation of the support.
At the same time, this line of work still leaves intriguing open questions. The ﬁrst is theoretical: to our knowledge, none of the methods optimizing over sparse support, and hence providing training speed-up, have been shown to have convergence guarantees. The second is practical, and concerns a deeper understanding of the relationship between the densely-trained model, and the sparsely-trained one. Speciﬁcally, (1) most existing sparse training methods still leave a non-negligible accuracy gap, relative to dense training, or even post-training sparsiﬁcation; and (2) most existing work on sparsity requires signiﬁcant changes to the training ﬂow, and focuses on maximizing global accuracy metrics; thus, we lack understanding when it comes to co-training sparse and dense models, as well as with respect to correlations between sparse and dense models at the level of individual predictions.
Contributions. In this paper, we take a step towards addressing these questions. We investigate a general hybrid approach for sparse training of neural networks, which we call Alternating Compressed
/ DeCompressed (AC/DC) training. AC/DC performs co-training of sparse and dense models, and can return both an accurate sparse model, and a dense model, which can recover the dense baseline accuracy via ﬁne-tuning. We show that a variant of AC/DC ensures convergence for general non-convex but smooth objectives, under analytic assumptions. Extensive experimental results show that it provides state-of-the-art accuracy among sparse training techniques at comparable training budgets, and can even outperform post-training sparsiﬁcation approaches when applied at high sparsities.
AC/DC builds on the classic iterative hard thresholding (IHT) family of methods for sparse recov-ery [6]. As the name suggests, AC/DC works by alternating the standard dense training phases with sparse phases where optimization is performed exclusively over a ﬁxed sparse support, and a subset of the weights and their gradients are ﬁxed at zero, leading to computational savings. (This is in contrast to error feedback algorithms, e.g. [9, 40] which require computing fully-dense gradients, even though the weights themselves may be sparse.) The process uses the same hyper-parameters, including the number of epochs, as regular training, and the frequency and length of the phases can be safely set to standard values, e.g. 5–10 epochs. We ensure that training ends on a sparse phase, and return the resulting sparse model, as well as the last dense model obtained at the end of a dense phase. This dense model may be additionally ﬁne-tuned for a short period, leading to a more accurate dense-ﬁnetuned model, which we usually ﬁnd to match the accuracy of the dense baseline.
We emphasize that algorithms alternating sparse and dense training phases for deep neural networks have been previously investigated [33, 25], but with the different goal on using sparsity as a regularizer to obtain more accurate dense models. Relative to these works, our goals are two-fold: we aim to produce highly-accurate, highly-sparse models, but also to maximize the fraction of training time for which optimization is performed over a sparse support, leading to computational savings. Further, we are the ﬁrst to provide convergence guarantees for variants of this approach.
We perform an extensive empirical investigation, showing that AC/DC provides consistently good results on a wide range of models and tasks (ResNet [28] and MobileNets [30] on the ImageNet [49]
/ CIFAR [36] datasets, and Transformers [56, 10] on WikiText [42]), under standard values of the training hyper-parameters. Speciﬁcally, when executed on the same number of training epochs, our method outperforms all previous sparse training methods in terms of the accuracy of the resulting sparse model, often by signiﬁcant margins. This comes at the cost of slightly higher theoretical computational cost relative to prior sparse training methods, although AC/DC usually reduces training
FLOPs to 45–65% of the dense baseline. AC/DC is also close to the accuracy of state-of-the-art post-training pruning methods [37, 52] at medium sparsities (80% and 90%); surprisingly, it outperforms them in terms of accuracy, at higher sparsities. In addition, AC/DC is ﬂexible with respect to the structure of the “sparse projection” applied at each compressed step: we illustrate this by obtaining semi-structured pruned models using the 2:4 sparsity pattern efﬁciently supported by new NVIDIA hardware [43]. Further, we show that the resulting sparse models can provide signiﬁcant real-world speedups for DNN inference on CPUs [12].
An interesting feature of AC/DC is that it allows for accurate dense/sparse co-training of models.
Speciﬁcally, at medium sparsity levels (80% and 90%), the method allows the co-trained dense 2
model to recover the dense baseline accuracy via a short ﬁne-tuning period. In addition, dense/sparse co-training provides us with a lens into the training dynamics, in particular relative to the sample-level accuracy of the two models, but also in terms of the dynamics of the sparsity masks. Speciﬁcally, we observe that co-trained sparse/dense pairs have higher sample-level agreement than sparse/dense pairs obtained via post-training pruning, and that weight masks still change later in training.
Additionally, we probe the accuracy differences between sparse and dense models, by examining their “memorization” capacity [60]. For this, we perform dense/sparse co-training in a setting where a small number of valid training samples have corrupted labels, and examine how these samples are classiﬁed during dense and sparse phases, respectively. We observe that the sparse model is less able to “memorize” the corrupted labels, and instead often classiﬁes the corrupted samples to their true (correct) class. By contrast, during dense phases model can easily “memorize” the corrupted labels. (Please see Figure 2b for an illustration.) This suggests that one reason for the higher accuracy of dense models is their ability to “memorize” hard-to-classify samples. 2