Abstract
Image-based virtual try-on is one of the most promising applications of human-centric image generation due to its tremendous real-world potential. Yet, as most try-on approaches ﬁt in-shop garments onto a target person, they require the labori-ous and restrictive construction of a paired training dataset, severely limiting their scalability. While a few recent works attempt to transfer garments directly from one person to another, alleviating the need to collect paired datasets, their performance is impacted by the lack of paired (supervised) information. In particular, disen-tangling style and spatial information of the garment becomes a challenge, which existing methods either address by requiring auxiliary data or extensive online opti-mization procedures, thereby still inhibiting their scalability. To achieve a scalable virtual try-on system that can transfer arbitrary garments between a source and a target person in an unsupervised manner, we thus propose a texture-preserving end-to-end network, the PAtch-routed SpaTially-Adaptive GAN (PASTA-GAN), that facilitates real-world unpaired virtual try-on. Speciﬁcally, to disentangle the style and spatial information of each garment, PASTA-GAN consists of an inno-vative patch-routed disentanglement module for successfully retaining garment texture and shape characteristics. Guided by the source person keypoints, the patch-routed disentanglement module ﬁrst decouples garments into normalized patches, thus eliminating the inherent spatial information of the garment, and then reconstructs the normalized patches to the warped garment complying with the target person pose. Given the warped garment, PASTA-GAN further introduces novel spatially-adaptive residual blocks that guide the generator to synthesize more realistic garment details. Extensive comparisons with paired and unpaired approaches demonstrate the superiority of PASTA-GAN, highlighting its ability to generate high-quality try-on images when faced with a large variety of garments (e.g. vests, shirts, pants), taking a crucial step towards real-world scalable try-on. 1

Introduction
Image-based virtual try-on, the process of computationally transferring a garment onto a particular person in a query image, is one of the most promising applications of human-centric image generation with the potential to revolutionize shopping experiences and reduce purchase returns. However, to fully exploit its potential, scalable solutions are required that can leverage easily accessible training data, handle arbitrary garments, and provide efﬁcient inference results. Unfortunately, to date, most existing methods [35, 38, 12, 7, 37, 9, 10, 4, 36, 39] rely on paired training data, i.e., a person image
∗Xiaodan Liang is the corresponding author. Our code will be available at PASTA-GAN. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Example virtual try-on results from our PASTA-GAN, which is ﬂexible for various try-on scenarios, e.g., garment transfer for the upper body, the lower body, and the full body. and its corresponding in-shop garment, leading to laborious data-collection processes. Furthermore, these methods are unable to exchange garments directly between two person images, thus largely limiting their application scenarios and raising the need for unpaired solutions to ensure scalability.
While unpaired solutions have recently started to emerge, performing virtual try-on in an unsupervised setting is extremely challenging and tends to affect the visual quality of the try-on results. Speciﬁcally, without access to the paired data, these models are usually trained by reconstructing the same person image, which is prone to over-ﬁtting, and thus underperform when handling garment transfer during testing. The performance discrepancy is mainly reﬂected in the garment synthesis results, in particular the shape and texture, which we argue is caused by the entanglement of the garment style and spatial representations in the synthesis network during the reconstruction process.
While this is not a problem for the traditional paired try-on approaches [35, 12, 37, 10], which avoid this problem and preserve the garment characteristics by utilizing a supervised warping network to obtain the warped garment in target shape, this is not possible in the unpaired setting due to the lack of warped ground truth. The few works that do attempt to achieve unpaired virtual try-on, therefore, choose to circumvent this problem by either relying on person images in various poses for feature disentanglement [23, 33, 32, 31, 1, 5], which again leads to a laborious data-collection process, or require extensive online optimization procedures [25, 17] to obtain ﬁne-grain details of the original garments, harming the inference efﬁciency. However, none of the existing unpaired try-on methods consider the problem of coupled style and spatial garment information directly, which is crucial to obtain accurate garment transfer results in the unpaired and unsupervised virtual try-on scenario.
In this paper, to tackle the essential challenges mentioned above, we propose a novel PAtch-routed
SpaTially-Adaptive GAN, named PASTA-GAN, a scalable solution to the unpaired try-on task.
Our PASTA-GAN can precisely synthesize garment shape and style (see Fig. 1) by introducing a patch-routed disentanglement module that decouples the garment style and spatial features, as well as a novel spatially-adaptive residual module to mitigate the problem of feature misalignment.
The innovation of our PASTA-GAN includes three aspects: First, by separating the garments into normalized patches with the inherent spatial information largely reduced, the patch-routed disen-tanglement module encourages the style encoder to learn spatial-agnostic garment features. These features enable the synthesis network to generate images with accurate garment style regardless of varying spatial garment information. Second, given the target human pose, the normalized patches can be easily reconstructed to the warped garment complying with the target shape, without requiring a warping network or a 3D human model. Finally, the spatially-adaptive residual module extracts the warped garment feature and adaptively inpaints the region that is misaligned with the target garment shape. Thereafter, the inpainted warped garment features are embedded into the intermediate layer of the synthesis network, guiding the network to generate try-on results with realistic garment texture.
We collect a scalable UnPaired virtual Try-on (UPT) dataset and conduct extensive experiments on the UPT dataset and two existing try-on benchmark datasets (i.e., the DeepFashion [21] and the
MPV [6] datasets). Experiment results demonstrate that our unsupervised PASTA-GAN outperforms both the previous unpaired and paired try-on approaches. 2
Figure 2: Overview of the inference process. (a) Given the source and target images of person (Is, It), we can extract the source garment Gs, the source pose Js, and the target pose Jt. The three are then sent to the patch-routed disentanglement module to yield the normalized garment patches Pn and the warped garment Gt. (b) The modiﬁed conditional StyleGAN2 ﬁrst collaboratively exploits the disentangled style code w, projected from Pn, and the person identity feature fid, encoded from target head and pose (Ht, Jt), to synthesize the coarse try-on result ˜It in the style synthesis branch along with the target garment mask Mg. It then leverages the warped garment feature fg in the texture synthesis branch to generate the ﬁnal try-on result I (cid:48) t. (cid:48) 2