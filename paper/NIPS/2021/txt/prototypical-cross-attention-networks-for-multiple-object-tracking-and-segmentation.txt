Abstract
Multiple object tracking and segmentation requires detecting, tracking, and seg-menting objects belonging to a set of given classes. Most approaches only exploit the temporal dimension to address the association problem, while relying on sin-gle frame predictions for the segmentation mask itself. We propose Prototypical
Cross-Attention Network (PCAN), capable of leveraging rich spatio-temporal in-formation for online multiple object tracking and segmentation. PCAN first distills a space-time memory into a set of prototypes and then employs cross-attention to retrieve rich information from the past frames. To segment each object, PCAN adopts a prototypical appearance module to learn a set of contrastive foreground and background prototypes, which are then propagated over time. Extensive exper-iments demonstrate that PCAN outperforms current video instance tracking and segmentation competition winners on both Youtube-VIS and BDD100K datasets, and shows efficacy to both one-stage and two-stage segmentation frameworks.
Code and video resources are available at http://vis.xyz/pub/pcan. 1

Introduction
Multiple object tracking and segmentation (MOTS), also known as Video Instance Segmentation (VIS), is an important problem with many real-world applications, including autonomous driving [10, 26] and video analysis [4, 46]. The task involves tracking and segmenting all objects within a video from a given set of semantic classes. We are witnessing rapidly growing research interest on MOTS thanks to the introduction of large scale benchmarks [46, 50, 37]. State-of-the-art methods [46, 5, 37, 29] for MOTS mainly follow the tracking-by-detection paradigm, where objects are first detected and segmented in individual frames and then associated over time.
Although methods based on the popular tracking-by-detection philosophy have shown promising results, temporal modeling is limited to the object association phase [46, 5, 22] and only between two adjacent frames [37, 18]. On the other hand, the temporal dimension carries rich information about the scene. The information encoded in multiple temporal views of an object has the potential of improving the quality of predicted segmentation, localization, and categories. However, effectively and efficiently leveraging the rich temporal information remains a challenge. While sequential modeling has been applied for video processing [40, 41, 9, 28, 12], these methods generally operate directly on the high-resolution deep features, requiring large computational and memory consumption, which greatly limits their use.
We propose a Prototypical Cross-Attention Module, termed PCAM, to leverage temporal information for multiple object tracking and segmentation. As illustrated in Figure 1, the module first distills spatio-temporal information into condensed prototypes using clustering based on Expectation Maximization.
The resulting prototypes, composed of Gaussian Components, yield a rich and generalizable yet compact representation of the past visual features. Given a deep feature embedding of the current 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: We propose Prototypical Cross-Attention Network for MOTS, which first condenses the space-time memory and high-resolution frame embeddings into frame-level and instance-level prototypes. These are then employed to retrieve rich temporal information from past frames by our efficient prototypical cross-attention operation. frame, PCAM then employs prototypical cross-attention to read relevant information from prior frames.
Based on the noise-reduced clustered video features information, we further develop a Prototypical
Cross-Attention Network (PCAN) for MOTS, that integrates the general PCAM at two stages in the network: on the frame-level and instance-level. The former reconstructs and aligns temporal past frame features with current frame, while the instance level integrates specific information about each object in the video. For robustness to object appearance change, PCAN represents each object instance by learning sets of contrastive foreground and background prototypes, which are propagated in an online manner. With a limited number of prototypes for each instance or frame, PCAN efficiently performs long-range feature aggregation and propagation in a video with linear complexity.
Consequently, our PCAN outperforms standard non-local attention [40] and video transformer [41] on both the large-scale Youtube-VIS and BDD100K MOTS benchmarks.
Our main contributions are summarized as follows: (i) We introduce the PCAN module for efficiently utilizing long-term spatio-temporal video information. (ii) We develop a MOTS approach that employs PCAN on frame and instance-level. (iii) We further represent the appearance of each video tracklet with contrastive foreground and background prototypes, which are propagated over time. (iv)
We extensively analyze our approach. Our PCAN outperforms previous approaches on the challenging self-driving dataset BDD100K [50] and the semantically diverse YouTube-VIS dataset [46]. 2