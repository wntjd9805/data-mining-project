Abstract
Since visual perception can give rich information beyond text descriptions for world understanding, there has been increasing interest in leveraging visual grounding for language learning. Recently, vokenization [68] has attracted attention by using the predictions of a text-to-image retrieval model as labels for language model supervision. Despite its success, the method suffers from approximation error of using finite image labels and the lack of vocabulary diversity of a small image-text dataset. To overcome these limitations, we present VIDLANKD, a video-language knowledge distillation method for improving language understanding. We train a multi-modal teacher model on a video-text dataset, and then transfer its knowledge to a student language model with a text dataset. To avoid approximation error, we propose to use different knowledge distillation objectives. In addition, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. In our experiments, VIDLANKD achieves consistent improvements over text-only language models and vokenization models, on several downstream language un-derstanding tasks including GLUE, SQuAD, and SWAG. We also demonstrate the improved world knowledge, physical reasoning, and temporal reasoning capabil-ities of our model by evaluating on the GLUE-diagnostics, PIQA, and TRACIE datasets. Lastly, we present comprehensive ablation studies as well as visualiza-tions of the learned text-to-video grounding results of our teacher and student language models.1 1

Introduction
Language learning can be aided by grounded visual cues, as they provide powerful signals for modeling a vastness of experiences in the world that cannot be documented by text alone [5; 29; 4].
While the recent trend of large-scale language model pretraining indirectly provides some world knowledge from text, most large text corpora (e.g., Wikipedia) do not provide enough multi-modal grounding information. Previous works have explored multiple ways of grounding language to visual information such as constructing a common vector space [38; 7] and supervising the model with token-wise generated vision labels [68]. However, the widely-used image-text datasets (e.g.,
MS COCO [48]) are much smaller than text-only corpora in terms of word counts and vocabulary diversity for language learning.
The recent method of ‘vokenization’ [68] is a promising initial step towards addressing this problem by supervising language models with weakly-aligned vision-language groundings. Firstly, an image-text matching model retrieves a corresponding image to each text token in a sentence. Then a language model learns to predict the selected image (called ‘voken’) for each text token. This can be seen as a knowledge distillation (KD) process [33] from a vision-language grounding model to a language 1Code and models: https://github.com/zinengtang/VidLanKD 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
Figure 1: Overview of the proposed VIDLANKD method. We first pretrain a teacher language model on a multi-modal dataset (Sec. 3.2). Then we distill the knowledge of the teacher model (weights frozen) to a student language model on a text dataset (Sec. 3.3). model. Although the voken classification task helps the language model to improve on natural language understanding (NLU) tasks, there exist several limitations: (1) images cannot faithfully convey word meanings that require more activity-based and physical commonsense knowledge. (2) the voken supervision suffers from approximation/quantization error of the text-to-image retrieval.
To address these problems, we propose a novel Video-and-Language Knowledge Distillation method, named VIDLANKD. Our teacher model consists of a video encoder and a language encoder. They are jointly trained with a video-language contrastive learning objective and a masked language modeling (MLM) objective on a multi-modal dataset (see Fig. 1). Then, we transfer the knowledge of the frozen teacher language encoder to a student language model by minimizing the distance between contextualized text representations of two models on a text dataset. For this, we propose to use different KD objectives including neuron selectivity transfer (NST) [34] and contrastive representation distillation (CRD) [71] that avoid the approximation error from voken assignments
[68]. For cross-modal pretraining of our teacher model, we use HowTo100M [54], a large-scale video dataset which has more diverse vocabulary and richer world commonsense (e.g., physical and temporal) knowledge compared to MS COCO image dataset.
In our experiments, student language models learned with the proposed video-language KD objectives outperform the baseline text-pretrained language models and the models distilled with vokeniza-tion, on several diverse natural language understanding benchmarks including GLUE [73], SQuAD
[61], and SWAG [79]. We also show comprehensive ablation studies on video encoders, student
KD objectives, teacher pretraining objectives, and video vs. image-based pretraining. Further-more, we empirically illustrate that our model successfully learns linguistic world knowledge and physical/temporal commonsense abilities from video, by showing improved performances on the
GLUE-diagnostics [73], PIQA [6], and TRACIE [82] datasets.
Overall, our contributions are: (1) a novel cross-modal knowledge distillation method for improving natural language understanding, (2) using rich video-text data which can overcome the limitations of image vokenization, (3) empirical improvements on several language understanding benchmarks and studying different knowledge distillation methods, and (4) analysis on linguistic/physical/temporal knowledge learned from videos and ablation studies on the effectiveness of proposed components. 2