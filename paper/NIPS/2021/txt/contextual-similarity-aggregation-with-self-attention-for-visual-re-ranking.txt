Abstract
In content-based image retrieval, the ﬁrst-round retrieval result by simple visual feature comparison may be unsatisfactory, which can be reﬁned by visual re-ranking techniques. In image retrieval, it is observed that the contextual similarity among the top-ranked images is an important clue to distinguish the semantic relevance.
Inspired by this observation, in this paper, we propose a visual re-ranking method by contextual similarity aggregation with self-attention. In our approach, for each image in the top-K ranking list, we represent it into an afﬁnity feature vector by comparing it with a set of anchor images. Then, the afﬁnity features of the top-K images are reﬁned by aggregating the contextual information with a transformer encoder. Finally, the afﬁnity features are used to recalculate the similarity scores between the query and the top-K images for re-ranking of the latter. To further improve the robustness of our re-ranking model and enhance the performance of our method, a new data augmentation scheme is designed. Since our re-ranking model is not directly involved with the visual feature used in the initial retrieval, it is ready to be applied to retrieval result lists obtained from various retrieval algorithms. We conduct comprehensive experiments on four benchmark datasets to demonstrate the generality and effectiveness of our proposed visual re-ranking method. 1

Introduction
In instance image retrieval, the goal is to efﬁciently identify images containing the same object or describing the same scene with the query image from a large corpus of images. Towards this goal, many works have emerged in recent years [42, 21, 36, 26, 24]. With the development of deep learning, a great number of methods leverage convolutional neural network (CNN) as feature extractor [38, 3, 44, 4, 14, 34, 25, 35, 45] to replace or combine with the classic SIFT feature [23].
Generally, the performance of the initial retrieval results by simple comparison of visual feature may not be satisfactory. To reﬁne it, many visual re-ranking techniques have been proposed [9, 19, 48].
Popular visual re-ranking techniques include query expansion, geometric context veriﬁcation, kNN-based re-ranking, and diffusion-based methods, etc. Query Expansion (QE) [9, 10, 39, 34, 30] computes an average feature of the query and the top-ranked images to update the original query
*The ﬁrst and second authors contribute equally to this work.
†Corresponding authors: Min Wang and Wengang Zhou
Code will be released at https://github.com/MCC-WH/CSA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
feature, which is then used for a second-round retrieval. Geometric Context Veriﬁcation (GCV) [30, 12, 20, 2] leverages the geometric context of local features to remove the false matches in the original results. Besides, if two images belong to the k-reciprocal nearest neighbors (kNN) of each other, these two images have a high probability of being relevant. Based on this observation, lots of k-Nearest Neighbors (kNN) based re-ranking methods thus emerge and achieve outstanding performance [19, 48, 50]. Diffusion-based methods [51, 11, 18] consider the global similarity of all images and perform similarity propagation iteratively. Most of the above methods do not involve training, thus can be quickly equipped with various features. The recent learning-based methods such as LAttQE [16] and GSS [22], achieve better performance but require training a speciﬁc model for each kind of feature.
Different from the methods mentioned above, in this paper, we propose a novel visual re-ranking method by contextual information aggregation. The initial retrieval results generated by feature comparison only consider pairwise image similarity, but ignore the rich contextual information contained in the ranking list. As illustrated in [29], if two images are mutually relevant, they share similar distances from a set of anchor images. Thus, we directly select the top-L retrieval results as anchors for each query to fully model the contextual information in the ranking list. We deﬁne an afﬁnity feature for each image in top-K candidates by computing the similarity between it and the anchor images. To further promote the re-ranking quality, we propose a new data augmentation method.
Moreover, inspired by Query Expansion (QE) which employs the information of top-ranked images to update the query feature, we propose to update afﬁnity features of each top-K candidates by aggregating the afﬁnity features of other candidates to promote the re-ranking performance with transformer encoder [46]. For each image in the top-K candidates, our re-ranking model dynamically aggregates the afﬁnity features of other candidates based on the importance between candidates by computing their similarities using the afﬁnity features. The output of our re-ranking model can be regarded as the reﬁned afﬁnity features for top-K candidates which contain more contextual information.
To train the re-ranking model, two loss functions are introduced. Firstly, we use a contrastive loss to restrain the updated afﬁnity features so that the relevant images have large similarity, and vice versa.
Besides, we exploit a Mean Squared Error (MSE) loss to reserve the information in original afﬁnity features by restricting the difference between the original afﬁnity features and the reﬁned afﬁnity features. During the inference time, we compute the afﬁnity features for the top-K candidates and utilize the transformer encoder to reﬁne the features. Then we re-rank these candidates by computing the cosine similarity between the reﬁned afﬁnity features. The rank of images outside the top-K ranking list remains unchanged.
Note that our re-ranking model is not directly involved with the original visual feature. Instead, it computes the afﬁnity features for top-K images, which serve as the input in our method. Therefore, it can be combined with various existing image retrieval algorithms including representation learning and re-ranking methods, and further enhance the retrieval performance with low computational overhead. We conduct comprehensive experiments on four benchmark datasets to prove the generality and effectiveness of our proposed visual re-ranking method. Besides, the time and memory complexity of our re-ranking method is lower than the state-of-the-art re-ranking methods. 2