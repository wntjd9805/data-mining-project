Abstract
We present a new perspective of achieving image synthesis by viewing this task as a visual token generation problem. Different from existing paradigms that directly synthesize a full image from a single input (e.g., a latent code), the new formulation enables a ﬂexible local manipulation for different image regions, which makes it possible to learn content-aware and ﬁne-grained style control for image synthesis.
Speciﬁcally, it takes as input a sequence of latent tokens to predict the visual tokens for synthesizing an image. Under this perspective, we propose a token-based generator (i.e.,TokenGAN). Particularly, the TokenGAN inputs two semantically different visual tokens, i.e., the learned constant content tokens and the style tokens from the latent space. Given a sequence of style tokens, the TokenGAN is able to control the image synthesis by assigning the styles to the content tokens by attention mechanism with a Transformer. We conduct extensive experiments and show that the proposed TokenGAN has achieved state-of-the-art results on several widely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with different resolutions. In particular, the generator is able to synthesize high-ﬁdelity images with 1024 × 1024 size, dispensing with convolutions entirely. 1

Introduction
Unconditional image synthesis generates images from latent codes by adversarial training [10, 16, 19, 25, 27, 33, 50]. Recent advances have been achieved by a style-based generator architecture in terms of both the visual quality and resolution of generated images [26, 27, 28, 37, 52]. In particular, the style-based generator has been widely used in many other generative tasks, including facial editing
[9, 40], style transfer [1, 38], image super-resolution [17, 31], and image inpainting [2, 52].
The key to the success of the style-based generator lies in the learning of the style control based on the intermediate latent space W [27, 28]. Instead of feeding the input latent code z ∈ Z through the input layer only (Figure1-a), the style-based generator maps the input z to an intermediate latent space w ∈ W, which then controls the “style” of the image at each layer via adaptive instance normalization (AdaIN [21]) (Figure 1-b). It has been demonstrated that such a design allows a less entangled representation learning in W, leading to better generative image modeling [12, 23, 27, 40].
Despite the promising results, the style-based generator can suffer from the style control via AdaIN operation [28, 52]. Speciﬁcally, the style control is content-independent. It “washes away” the original information of features by normalization and assigns new styles decided by the latent codes regardless of the image/feature content. Besides, the style code w affects the entire image by scaling and biasing complete feature maps with a single value via AdaIN operation [35, 44, 54]. Such an imposed single style over multiple image regions can inevitably result in entangled representation of
∗This work was done while Yanhong Zeng was a research intern at Microsoft Research Asia. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of different generators. (a) A traditional generator [16] feeds a single latent z as input to control the image synthesis. (b) A style-based generator [27] maps z to an intermediate latent space w ∈ W to control the styles of the content c via AdaIN [21]. (c) Our token-based generator starts from a sequence of content tokens {c1, · · · , cm} and controls each content token with a set of style tokens {s1, · · · , sn} ∈ S by attention mechanism with a visual Transformer. M denotes the mapping network and G denotes the generator network. different image variations (e.g., hairstyle, facial expression) [28, 44, 52]. These limitations for image modeling can even lead to visible artifacts in the synthesized results (e.g., droplet artifacts [27]).
To get rid of the issues caused by StyleGAN’s style modeling, we introduce a new perspective that views image synthesis as a visual token generation problem. The visual token is a popular representation of an image patch with a predeﬁned size and position [6, 7, 13]; and has shown an impressive superiority in various tasks with the development of Transformer models [6, 13, 43, 46].
Inspired by the appealing property of the token-based representation, we propose to achieve image synthesis by visual token generation. Speciﬁcally, it takes as input a sequence of latent tokens to predict the visual tokens of an image. Such a token-based representation enables a ﬂexible local manipulation for different image regions, which makes it possible to learn content-aware and
ﬁne-grained style control for image synthesis.
Under this new paradigm, we design a token-based generator, i.e., TokenGAN, for the visual token generation problem. Speciﬁcally, we consider two different types of input tokens in the generator, i.e., the content tokens and the style tokens. The content tokens are learned as the constant input in the generator network and the style tokens are projected from a learned intermediate latent space (Figure1-c). Given a sequence of style tokens, the TokenGAN learns to control the visual token generation by rendering each content token with related style tokens according to their semantics. In particular, since the Transformer has been veriﬁed to be effective in sequence modeling in a broad range of tasks [6, 43], we adopt a generator network architecture from a visual Transformer to model the relations between the content tokens and the style tokens. Through such a content-dependent style modeling by the attention mechanism in Transformer, the TokenGAN is able to achieve content-aware and ﬁne-grained style learning for image synthesis.
We conduct both quantitative comparisons and qualitative analysis on several unconditional image generation benchmarks. The results show that the token-based generator has achieved comparable results to the state-of-the-art in image synthesis. We summarize our contributions as below:
• We propose a new perspective of achieving image synthesis by visual token generation.
Such a token-based representation enables ﬂexible local manipulation for different image regions, leading to a better image modeling.
• We propose a token-based generator (i.e., TokenGAN) for the visual token generation.
Speciﬁcally, the TokenGAN introduces the style tokens and the content tokens. It adopts a
Transformer-based network for content-aware style modeling.
• We show extensive experiments (quantitative and qualitative comparisons, study on style editing, image inversion and image interpolation) to verify the effectiveness of the token-based generator. Speciﬁcally, the token-based generator is able to synthesize high-ﬁdelity 1024 × 1024 images without any convolutions in the generator. 2