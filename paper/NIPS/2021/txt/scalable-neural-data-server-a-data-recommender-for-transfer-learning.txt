Abstract
Absence of large-scale labeled data in the practitioner’s target domain can be a bottleneck to applying machine learning algorithms in practice. Transfer learning is a popular strategy for leveraging additional data to improve the downstream performance, but ﬁnding the most relevant data to transfer from can be challenging.
Neural Data Server (NDS) [45], a search engine that recommends relevant data for a given downstream task, has been previously proposed to address this problem. NDS uses a mixture of experts trained on data sources to estimate similarity between each source and the downstream task. Thus, the computational cost to each user grows with the number of sources. To address these issues, we propose Scalable
Neural Data Server (SNDS), a large-scale search engine that can theoretically index thousands of datasets to serve relevant ML data to end users. SNDS trains the mixture of experts on intermediary datasets during initialization, and represents both data sources and downstream tasks by their proximity to the intermediary datasets. As such, computational cost incurred by SNDS users remains ﬁxed as new datasets are added to the server. We validate SNDS on a plethora of real world tasks and ﬁnd that data recommended by SNDS improves downstream task performance over baselines. We also demonstrate the scalability of SNDS by showing its ability to select relevant data for transfer outside of the natural image setting. 1

Introduction
In recent years, machine learning (ML) applications have taken a foothold in many ﬁelds. Method-ological and computational advances have shown a trend in performance improvements achieved by using larger and larger training datasets, conﬁrming that data is the fuel of modern machine learning.
This fuel, however, is not free of cost. While the raw data itself (such as images, text, etc.) can be relatively easy to collect, annotating the data is a labour intensive endeavour. A popular and effective approach to reduce the need of labeling in a target application domain is to leverage existing datasets via transfer learning. Transfer learning is the re-purposing of ML models trained on a source dataset towards a different target task [22]. The performance of transfer learning is predicated on the relevance of the source to the target [46, 36]. Although there are numerous datasets available through various data sharing platforms [41, 5, 38, 20], ﬁnding the right dataset that will most beneﬁt transfer-learning performance on the target domain is not a simple problem.
Consider a startup looking to train a debris detector for use on a road cleaning robot, and suppose that they have collected a small, labeled dataset. They are looking for data with which to augment their training set. Ideally, they would ﬁnd a dataset of roadside debris that is captured from a sensor similar to their own. However, keyword searches for “roadside debris”, would likely ﬁnd data geared towards autonomous driving, which might be from one of the many sensor types (rgb, lidar, radar).
Searching for their speciﬁc sensor type might lead to data from similar setups, but of different objects.
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Scalable Neural Data Server.
SNDS is a scalable data recommendation sys-tem for transfer learning data. It can theoreti-cally index thousands of datasets by using a pool of experts, trained once on intermediary datasets, and used for data matching. SNDS uses these intermediary datasets to compute similarities between the consumer’s data and the server’s datasets. Similarities are com-puted on the user’s side with a computation cost that does not grow with the size of the server’s data. User’s data privacy is preserved.
Rather than using keywords, an ideal system would ﬁnd relevant data based on the training set itself.
In this work, we envision a data search engine that indexes source data from data providers, and recommends relevant transfer learning data to consumers who have a target task.
Neural DataServer (NDS) [45] has been proposed as a data recommendation system for transfer learning. Given a target task (as represented by a target dataset), it uses expert models trained on splits of source data to make predictions on the target, which is in turn used as a measure of similarity between the source and the target. Directly indexing a large number of datasets with NDS is expensive, as it would require each data provider to upload a trained expert, and any data consumer looking to select data for purchase must download and run all of the trained experts in order to evaluate similarity. Hence, for M data providers in the server, the computational and bandwidth cost to the data consumer scales linearly with M .
At the same time, using an expert model per data provider in NDS ensures that the data recom-mendations are highly relevant to the consumer. A data search engine that operates in a large-scale setting must be able to preserve the quality of recommendations, while disentangling the consumer’s computational cost from the number of providers. The system should also preserve the data privacy properties from NDS, where the client’s data is never exchanged with the server.
We propose Scalable Neural Data Server (SNDS), a search engine for data recommendation at scale.
This is achieved by using an intermediary, public dataset to train a ﬁxed number of experts, and a similarity measure that evaluates the relevance of the server’s data for a given consumer via these experts. Compared to NDS, the computational cost of SNDS is constant for a given consumer with respect to the M data providers, and so is the bandwidth cost between the consumer and the server.
We validate SNDS experimentally and ﬁnd that 1) SNDS generalizes in the sense that it recommends useful data for downstream tasks that are not directly represented by the intermediary datasets, 2)
SNDS improves performance on downstream tasks over baselines, and is competitive with NDS, 3)
SNDS is able to generalize to domains that are very dissimilar to the intermediary datasets. 2