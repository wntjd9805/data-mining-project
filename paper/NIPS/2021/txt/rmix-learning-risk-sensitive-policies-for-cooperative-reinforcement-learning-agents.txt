Abstract
Current value-based multi-agent reinforcement learning methods optimize indi-vidual Q values to guide individuals’ behaviours via centralized training with decentralized execution (CTDE). However, such expected, i.e., risk-neutral, Q value is not sufﬁcient even with CTDE due to the randomness of rewards and the uncertainty in environments, which causes the failure of these methods to train coordinating agents in complex environments. To address these issues, we propose RMIX, a novel cooperative MARL method with the Conditional Value at Risk (CVaR) measure over the learned distributions of individuals’ Q values.
Speciﬁcally, we ﬁrst learn the return distributions of individuals to analytically calculate CVaR for decentralized execution. Then, to handle the temporal nature of the stochastic outcomes during executions, we propose a dynamic risk level predictor for risk level tuning. Finally, we optimize the CVaR policies with CVaR values used to estimate the target in TD error during centralized training and the
CVaR values are used as auxiliary local rewards to update the local distribution via Quantile Regression loss. Empirically, we show that our method outperforms many state-of-the-art methods on various multi-agent risk-sensitive navigation scenarios and challenging StarCraft II cooperative tasks, demonstrating enhanced coordination and revealing improved sample efﬁciency. 1

Introduction
Reinforcement learning (RL) has made remarkable advances in many domains, including arcade video games [28], complex continuous robot control [21] and the game of Go [40]. Recently, many researchers put their efforts to extend the RL methods into multi-agent systems (MASs), such as urban systems [41], coordination of robot swarms [16] and real-time strategy (RTS) video games [50].
Centralized training with decentralized execution (CTDE) [30] has drawn enormous attention via training policies of each agent with access to global trajectories in a centralized way and executing actions given only the local observations of each agent in a decentralized way. Empowered by CTDE, several multi-agent RL (MARL) methods, including value-based and policy gradient-based, are proposed [10, 43, 35, 42]. These MARL methods propose decomposition techniques to factorize the global Q value either by structural constraints or by estimating state-values or inter-agent weights to conduct the global Q value estimation [10, 43, 35, 42, 53, 54].
Despite the merits, most of these works focus on decomposing the global Q value into individual Q values with different constraints and network architectures, but ignore the fact that such expected, i.e., risk-neutral, Q value is not sufﬁcient as optimistic actions executed by some agents can impede the team coordination such as imprudent actions in hostage rescue operations, which causes the failure of
∗Correspondence to qiuw0008@e.ntu.edu.sg 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
these methods to train coordinating agents in complex environments. Speciﬁcally, these methods only learn the expected values over returns [35] and do not handle the high variance caused by events with extremely high/low rewards to agents but at small probabilities, which cause the inaccurate/insufﬁcient estimations of the future returns. Therefore, instead of expected values, learning distributions of future returns, i.e., Q values, are more useful for agents to make decisions. Even further, given that the environment is nonstationary from the perspective of each agent, decision-making over the agent’s return distribution takes events of potential return into account, which makes agents able to address uncertainties in the environment compared with simply taking the expected values for execution.
However, current MARL methods do not extensively investigate these aspects.
Motivated by the previous reasons, we intend to extend the risk-sensitive RL [5, 18, 56, 3] (“Risk” refers to the uncertainty of future outcomes [8]) to MARL settings, where risk-sensitive RL optimizes policies with a risk measure, such as variance, power formula measure value at risk (VaR) and conditional value at risk (CVaR). Among these risk measures, CVaR has been gaining popularity due to both theoretical and computational advantages [36, 38]. However, there are two main obstacles: (i) most of the previous works focus on risk-neutral or static risk level in the single-agent settings, ignoring the randomness of reward and the temporal structure of agents’ trajectories [8, 47, 24, 18]; (ii) many methods use risk measures over Q values for policy execution without getting the risk measure values used in policy optimization in temporal difference (TD) learning, which causes the global value factorization on expected individual values to have sub-optimal behaviours in MARL.
In this paper, we propose RMIX, a novel cooperative risk-sensitive MARL method. Speciﬁcally, our contributions are in three folds: (i) We ﬁrst learn the return distributions of individuals by using Dirac
Delta functions in order to analytically calculate CVaR for decentralized execution. The resulting
CVaR values at each time step are used as policies for each agent via arg max operation; (ii) We then propose a dynamic risk level predictor for CVaR calculation to handle the temporal nature of stochastic outcomes as well as tune the risk level during executions. The dynamic risk level predictor measures the discrepancy between the embedding of current individual return distributions and the embedding of historical return distributions. The dynamic risk levels are agent-speciﬁc and observation-wise; (iii) As our method focuses on optimizing the CVaR policies via CTDE, we ﬁnally optimize CVaR policies with CVaR values as target estimators in TD error via centralized training and CVaR values are used as auxiliary local rewards to update local return distributions via Quantile Regression loss.
These also allow our method to achieve temporally extended exploration and enhanced temporal coordination, which are keys to solving complex multi-agent tasks. Empirically, we show that RMIX outperforms many state-of-the-art methods on various multi-agent risk-sensitive navigation scenarios and challenging StarCraft II cooperative tasks, demonstrating enhanced coordination and revealing improved sample efﬁciency. 2 Preliminaries and