Abstract
Large pretrained language models (LMs) like BERT have improved performance in many disparate natural language processing (NLP) tasks. However, ﬁne tuning such models requires a large number of training examples for each target task.
Simultaneously, many realistic NLP problems are "few shot", without a sufﬁciently large training set. In this work, we propose a novel conditional neural process-based approach for few-shot text classiﬁcation that learns to transfer from other diverse tasks with rich annotation. Our key idea is to represent each task using gradient information from a base model and to train an adaptation network that modulates a text classiﬁer conditioned on the task representation. While previous task-aware few-shot learners represent tasks by input encoding, our novel task representation is more powerful, as the gradient captures input-output relationships of a task.
Experimental results show that our approach outperforms traditional ﬁne-tuning, sequential transfer learning, and state-of-the-art meta learning approaches on a collection of diverse few-shot tasks. We further conducted analysis and ablations to justify our design choices. 1

Introduction
Transformer-based pretrained large-scale language models (LMs) have achieved tremendous success on many NLP tasks [15, 34], but require a large number of in-domain labeled examples for ﬁne-tuning [54]. One approach to alleviate that issue is to ﬁne-tune the model on an intermediate (source) task that is related to the target task. While previous work has focused on this setting for a single source task [33, 50], the problem of transferring from a set of source tasks has not been thoroughly considered. Wang et al. [51] showed that a naïve combination of multiple source tasks may negatively impact target task performance. Additionally, transfer learning methods typically consider the setting where more than a medium (N>100) number of training examples are available for both the source and target tasks.
In this work we develop a method to improve a large pre-trained LM for few-shot text classiﬁcation problems by transferring from multiple source tasks. While it is possible to directly adapt recent advances in meta-learning developed for few-shot image classiﬁcation, a unique challenge in the
NLP setting is that source tasks do no share the same task structure. Namely, on standard few-shot image classiﬁcation benchmarks, the training tasks are sampled from a “single” larger dataset, and the label space contains the same task structure for all tasks. In contrast, in text classiﬁcation tasks, the set of source tasks available during training can range from sentiment analysis to grammatical acceptability judgment. Different source tasks could not only be different in terms of input domain, but also their task structure (i.e. label semantics, and number of output labels).
This challenging problem requires resistance to overﬁtting due to its few-shot nature and more task-speciﬁc adaptation due to the distinct nature among tasks. Fine-tuning based approaches are known to suffer from the overﬁtting issue and approaches like MAML [17] and prototypical networks 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(ProtoNet) [41] were not designed to meta-learn from such a diverse collection of tasks. CNAP [37], a conditional neural process [18], explicitly designed for learning from heterogeneous task distributions by adding a task conditional adaptation mechanism is better suited for solving tasks with a shifted task distribution. We design our approach based on the CNAP framework. Compared with CNAP, we use pretrained transformers for text sequence classiﬁcation instead of convolutional networks for image classiﬁcation. We insert adapter modules [21] into a pretrained LM to ensure efﬁciency and maximal parameter-sharing, and learn to modulate those adapter parameters conditioned on tasks, while CNAP learns to adapt the whole feature extractor. Additionally, we use gradient information for task representation as opposed to average input encoding in CNAP, since gradients can capture information from both input and ouput space.
In summary, our main contributions and ﬁndings are:
• We propose the use of gradients as features to represent tasks under a model-based meta-learning framework. The gradient features are used to modulate a base learner for rapid generalization on diverse few-shot text classiﬁcation tasks.
• We use pre-trained BERT with adapter modules [21] as the feature extractor and show that it works better than using only the BERT model on few-shot tasks, while also being more efﬁcient to train as it has much fewer parameters to learn.
• We compare our approach with traditional ﬁne-tuning, sequential transfer learning, and state-of-the-art meta-learning approaches on a collection of diverse few-shot text classiﬁcation tasks used in [5] and three newly added tasks, and show that our approach achieves the best performance. Our codes are publicly available1. 2