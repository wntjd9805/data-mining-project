Abstract
Spiking neural networks (SNNs) have emerged as a biology-inspired method mimicking the spiking nature of brain neurons. This biomimicry derives SNNs’ energy efﬁciency of inference on neuromorphic hardware. However, it also causes an intrinsic disadvantage in training high-performing SNNs from scratch since the discrete spike prohibits the gradient calculation. To overcome this issue, the surrogate gradient (SG) approach has been proposed as a continuous relaxation.
Yet the heuristic choice of SG leaves it vacant how the SG beneﬁts the SNN training. In this work, we ﬁrst theoretically study the gradient descent problem in SNN training and introduce ﬁnite difference gradient to quantitatively analyze the training behavior of SNN. Based on the introduced ﬁnite difference gradient, we propose a new family of Differentiable Spike (Dspike) functions that can adaptively evolve during training to ﬁnd the optimal shape and smoothness for gradient estimation. Extensive experiments over several popular network structures show that training SNN with Dspike consistently outperforms the state-of-the-art training methods. For example, on the CIFAR10-DVS classiﬁcation task, we can train a spiking ResNet-18 and achieve 75.4% top-1 accuracy with 10 time steps. 1

Introduction
Recently, spiking neural networks (SNNs) have received increasing attention due to their biology-inspired neural behavior and efﬁcient computation. SNNs deal with binary spike information and therefore enjoy the advantage of multiplication-free inference. Neuromorphic hardware such as
TrueNorth [1] and Loihi [2] demonstrates that SNNs can save energy by orders of magnitude. Hybrid architecture like Tianjic [3] suggests its potential power for general intelligence when integrated with traditional artiﬁcial infrastructure. However, the bio-mimicry also causes an intrinsic disadvantage in training high-performing SNNs from scratch due to the discrete spike. As a result, obtaining a high-performing SNN has long been a critical problem limiting SNN’s deployment in practice.
To be more speciﬁc, the appealing advantage of the multiplication-free computation in SNN is not recognized in its training [4]. Modern ML frameworks such as Pytorch [5], JAX [6], and
Tensorﬂow [7] do not provide efﬁcient and general instructions or high-level functions to accelerate the convolution with 0/1 spike activation. Moreover, the binary activation in SNN produces all-or-nothing gradients [8] that are incompatible with typical gradient-based optimization methods for efﬁcient neural network training. To circumvent this problem, many works adopt surrogate gradients [8] (typically the derivative of a soft-relaxed function) to replace the spike non-linear gradient. Although the heuristic choice of SG is beneﬁcial to enable the gradient descent in SNN training, the theoretical soundness of SG has not been justiﬁed so far. What’s more, the gradient descent in SNN training may follow different updating rules from ANN. In fact, since the true gradient
∗Equal Contributions. (cid:0)Corresponding author. Primarily supported by NSF-61876032. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overall workﬂow of the proposed algorithm. We use a general, ﬂexible Dspike function and control its shape with ﬁnite difference method to facilitate the surrogate gradient descent in training SNN. value of spike activation does not exist, the adoption of SG raises a new question of how to replace the ∞-gradient adaptively through the training process for a good updating strategy of parameters.
In this work, we study this specialized problem of adaptive gradient estimation in SNN training. We
ﬁrst revisit the gradient descent algorithm with the ﬁnite difference method to accurately depict the loss landscape of adopting a surrogate gradient for the non-differentiable spike ﬁring function. Then we further propose the Differentiable Spike (Dspike) function, which can adaptively change its shape and capture the direction of ﬁnite difference gradients. With such adaptive surrogate gradients, we establish an efﬁcient algorithm of training SNN with large-scale models. The overall workﬂow of our algorithm is visualized in Fig. 1.
Our contributions can be summarized as follows:
• We identify the problem of adaptively estimating gradient in SNN training and propose a novel analysis framework for SNN’s surrogate gradient, through which we are able to not only explain why it works well but also indicate how we can improve its adaptivity to data through training.
• We also introduce the novel Dspike function built from the hyperbolic tangent function. The Dspike function can learn its shape accordingly by matching it with the ﬁnite difference gradient.
• Extensive experiments on both static and dynamic datasets show that our method is highly effective and efﬁcient. For example, our model can achieve 75.4% top-1 accuracy on the CIFAR-DVS. Our
SNN also only consumes 7.8% energy of the corresponding ANN on CIFAR-10. 2