Abstract
Reliable yet efﬁcient evaluation of generalisation performance of a proposed archi-tecture is crucial to the success of neural architecture search (NAS). Traditional approaches face a variety of limitations: training each architecture to completion is prohibitively expensive, early stopped validation accuracy may correlate poorly with fully trained performance, and model-based estimators require large train-ing sets. We instead propose to estimate the ﬁnal test performance based on a simple measure of training speed. Our estimator is theoretically motivated by the connection between generalisation and training speed, and is also inspired by the reformulation of a PAC-Bayes bound under the Bayesian setting. Our model-free estimator is simple, efﬁcient, and cheap to implement, and does not require hyperparameter-tuning or surrogate training before deployment. We demonstrate on various NAS search spaces that our estimator consistently outperforms other alternatives in achieving better correlation with the true test performance rankings.
We further show that our estimator can be easily incorporated into both query-based and one-shot NAS methods to improve the speed or quality of the search. 1

Introduction
Reliably estimating the generalisation performance of a proposed architecture is crucial to the success of Neural Architecture Search (NAS) but has always been a major bottleneck in NAS algorithms [13].
The traditional approach of training each architecture for a large number of epochs and evaluating it on validation data (full training) provides a reliable performance measure, but requires prohibitively large computational resources on the order of thousands of GPU days [55, 39, 56, 38, 13]. This motivates the development of methods for speeding up performance estimation to make NAS practical for limited computing budgets.
A popular simple approach is early-stopping, which offers a low-ﬁdelity approximation of generalisa-tion performance by training for fewer epochs [27, 14, 25]. However, if we stop training early and evaluate the set of models on validation data, their relative performance ranking may not correlate well with the performance ranking of the fully-trained models [53], i.e. their relative performance on the test set after the entire training budget has been used. Another line of work focuses on learning curve extrapolation [10, 23, 3], which trains a surrogate model to predict the ﬁnal generalisation performance based on the initial learning curve or meta-features of the architecture. However, the training of the surrogate often requires hundreds of fully evaluated architectures to achieve satisfac-tory extrapolation performance and the hyper-parameters of the surrogate also need to be optimised.
Alternatively, the idea of weight sharing is adopted in one-shot NAS methods to speed up evaluation
∗Equal contribution. Correspondence to robin@robots.ox.ac.uk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[36, 28, 47]. Despite leading to signiﬁcant cost-saving, weight sharing heavily underestimates the true performance of good architectures and is unreliable in predicting the relative ranking among architectures [49, 52]. A more recent group of estimators claim to be zero-cost [32, 1]. Yet, their performance is often not competitive with the state of the art, inconsistent across tasks and cannot be further improved with greater training budgets.
In view of the above limitations, we propose a simple model-free method, Training Speed Estimation (TSE), which provides a reliable yet computationally cheap estimate of the generalisation performance ranking of a set of architectures. Our method is inspired by recent empirical and theoretical results linking training speed and generalisation [17, 30] and measures the training speed of an architecture by summing the training losses of the commonly-used SGD optimiser during training. We empirically show that our estimator can outperform strong existing approaches to predict the relative performance ranking among architectures, and can remain effective for a variety of search spaces and datasets.
Moreover, we verify its usefulness under different NAS settings and ﬁnd it can speed up query-based
NAS approaches signiﬁcantly as well improve the performance of one-shot and differentiable NAS.
Our code is available at https://github.com/rubinxin/TSE. 2 Method
Motivation The theoretical relationship between training speed and generalisation is described in a number of existing works. Stability-based generalisation bounds for SGD [17, 29] bound the generalisation gap of a model based on the number of optimisation steps used to train it. These bounds predict that models which train faster obtain a lower worst-case generalisation error. In networks of sufﬁcient width, a neural tangent kernel-inspired complexity measure can bound both the worst-case generalisation gap and the rate of convergence of (full-batch) gradient descent [2, 6].
However, these bounds cannot distinguish between models that are trained for the same number of steps but attain near-zero loss at different rates as they ignore the training trajectory.
We instead draw inspiration from another approach which incorporates properties of the trajectory taken during SGD, seen in the information-theoretic generalisation bounds of [33] and [34]. These bounds depend on the variance of gradients computed during training, a quantity which provides a
ﬁrst-order approximation of training speed by quantifying how well gradient updates computed for one mini-batch generalize to other points in the training set. In view of this link, the empirical and theoretical ﬁndings in recent work [15, 43], which show that a notion of the variance of gradients over the training data is correlated with generalisation, become another piece of supporting evidence for training speed as a measure of generalisation.
Finally, [30] prove that, in the setting of linear models and inﬁnitely wide deep models performing
Bayesian updates, the marginal likelihood, which is a theoretically-justiﬁed tool for model selection in Bayesian learning, can be bounded by a notion of training speed. This notion of training speed is deﬁned as a sum of negative log predictive likelihoods, terms that resemble the losses on new data points seen by the model during an online learning procedure. Maximising the marginal likelihood is also equivalent to minimising a PAC-Bayesian bound on the generalisation error of a model, as shown by [16]. In particular, this suggests that using the TSE approach for model selection in Bayesian models is equivalent to minimizing an estimate of a PAC-Bayes bound. Because the NAS settings we consider in our experiments use neural networks rather than Bayesian models and due to space constraints, we defer the statement and proof of this result to Appendix A.
Training Speed Estimation The results described above suggest that leveraging a notion of training speed may beneﬁt model selection procedures in NAS. Many such notions exist in the generalisation literature: [18] count the number of optimisation steps needed to attain a loss below a speciﬁed threshold, while [17] consider the total number of optimisation steps taken. Both measures are strong predictors of generalisation after training, yet neither is suitable for NAS, where we seek to stop training as early as possible if the model is not promising.
We draw inspiration from the Bayesian perspective and the PAC-Bayesian bound discussed above and present an alternate estimator of training speed that amounts to the area under the model’s training curve. Models that train quickly attain a low loss after few training steps, and so will have a lower area-under-curve than those which train slowly. This addresses the shortcomings of the previous two methods as it is able to distinguish models both early and late in training. 2
Deﬁnition 1 (Training Speed Estimator). Let (cid:96) denote a loss function, fθ(x) the output of a neural network f with input x and parameters θ, and let θt,i denote the parameters of the network after t epochs and i mini-batches of SGD. After training the network for T epochs2, we sum the training losses collected so far to get the following Training Speed Estimate (TSE):
TSE = (cid:34)
T (cid:88) t=1 1
B
B (cid:88) i=1 (cid:35) (cid:96) (cid:0)fθt,i(Xi), yi (cid:1) (1) where l is the training loss of a mini-batch (Xi, yi) at epoch t and B is the number of training steps within an epoch.
This estimator weights the losses accumulated during every epoch equally. However, recent work suggests that training dynamics of neural networks in the very early epochs are often unstable and not always informative of properties of the converged networks [24]. Therefore, we hypothesise that an estimator of network training speed that assigns higher weights to later epochs may exhibit a better correlation with the true generalisation performance of the ﬁnal trained network. On the other hand, it is common for neural networks to overﬁt on their training data and reach near-zero loss after sufﬁcient optimisation steps, so attempting to measure training speed solely based on the epochs near the end of training will be difﬁcult and likely suffer degraded performance on model selection.
To verify whether it is beneﬁcial to ignore or downplay the information from early epochs of training, we propose two variants of our estimator. The ﬁrst, TSE-E, treats the ﬁrst few epochs as a burn-in phase for θt,i to converge to a stable distribution P (θ) and starts the sum from epoch t = T − E + 1 instead of t = 1. In the case where E = 1, we start the sum at t = T and our estimator corresponds to the sum over training losses within the most recent epoch t = T .
TSE-E =
T (cid:88) t=T −E+1 (cid:34) 1
B
B (cid:88) i=1 (cid:35) (cid:96) (cid:0)fθt,i(Xi), yi (cid:1)
,
TSE-EMA =
T (cid:88) t=1
γT −t (cid:34) 1
B
B (cid:88) i=1 (cid:35) (cid:96) (cid:0)fθt,i(Xi), yi (cid:1)
The second, TSE-EMA, does not completely discard the information from the early training trajectory but takes an exponential moving average of the sum of training losses with γ = 0.9, thus assigning higher weight to the sum of losses obtained in later training epochs.
We empirically show in Section 4.2 that our proposed TSE and its variants (TSE-E and TSE-EMA), despite their simple form, can reliably estimate the generalisation performance of neural architectures with a very small training budget, can remain effective for a large range of training epochs, and are robust to the choice of hyperparameters such as the summation window E and the decay rate
γ. However, our estimator is not meant to replace the validation accuracy at the end of training or when the user can afford large training budget. In those settings, validation accuracy remains as the gold standard for evaluating the true test performance of architectures. Ours is just a speedy performance estimator for NAS, aimed at giving an indication early in training about an architecture’s generalisation potential under a ﬁxed training set-up.
Our choice of using the training loss, instead of the validation loss, to measure training speed is an important component of the proposed method. While it is possible to formulate an alternative estimator, which sums the validation losses of a model early in training, this estimator would no longer be measuring training speed. In particular, such an estimator would not capture the generalisation of gradient updates from one mini-batch to later mini-batches in the data to the same extent as TSE does. Indeed, we hypothesise that once the optimisation process has reached a local minimum, the sum over validation losses more closely resembles a variance-reduction technique that estimates the expected loss over parameters sampled via noisy SGD steps around this minimum. We show in
Figure 1 and Appendix C that our proposed sum over training losses (TSE) outperforms the sum over validation losses (SoVL) in ranking models in agreement with their true test performance. 3