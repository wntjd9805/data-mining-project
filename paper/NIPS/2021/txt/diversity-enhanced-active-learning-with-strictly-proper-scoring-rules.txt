Abstract
We study acquisition functions for active learning (AL) for text classiﬁcation.
The Expected Loss Reduction (ELR) method focuses on a Bayesian estimate of the reduction in classiﬁcation error, recently updated with Mean Objective
Cost of Uncertainty (MOCU). We convert the ELR framework to estimate the increase in (strictly proper) scores like log probability or negative mean square error, which we call Bayesian Estimate of Mean Proper Scores (BEMPS2). We also prove convergence results borrowing techniques used with MOCU. In order to allow better experimentation with the new acquisition functions, we develop a complementary batch AL algorithm, which encourages diversity in the vector of expected changes in scores for unlabelled data. To allow high performance text classiﬁers, we combine ensembling and dynamic validation set construction on pretrained language models. Extensive experimental evaluation then explores how these different acquisition functions perform. The results show that the use of mean square error and log probability with BEMPS yields robust acquisition functions, which consistently outperform the others tested. 1

Introduction
Classiﬁcation has extensive uses and deep learning has substantially improved its performance, but a major hurdle for its use is the paucity of labelled or annotated data. The data labelling process performed by domain experts is expensive and tedious to produce, especially in the medical ﬁeld, where due to lack of expertise and privacy issues, annotation costs are time-consuming and expensive
[11]. Active Learning (AL) is an approach to speeding up learning by judiciously selecting data to be annotated [27]. AL is perhaps the simplest of all human-in-the-loop learning approaches, because the only interaction is the (expert) human providing a class label, yet for even in the simplest of tasks, classiﬁcation, a general theory of AL is not agreed on.
Moreover, in practical situations, one needs to consider many issues when designing an AL system with deep learning: the expense to retrain deep neural networks and the use of validation data for training [31], transformer language models [16], batch mode AL with diversity [22], and consideration of expert capabilities and costs [8, 43]. Also important in experimental work is the need for realistic labelling sizes, with practitioners we work with saying expert annotation may allow a budget of up to 1000 data points, rarely more. Using large batch sizes (e.g., 1000 in [1, 20]) can thus be impractical.
Our fundamental research contribution is to suggest what makes a good acquisition function for the uncertainty component, without any batching. While there are many recent methods looking at the uncertainty diversity trade-off for batch AL [22], few recently have focused on understanding the uncertainty side alone. A substantial advance is a recent theoretical framework, Mean Objective
∗Corresponding author 2Our implementation of BEMPS can be downloaded from https://github.com/davidtw999/BEMPS. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Cost of Uncertainty (MOCU) [42] that provides a convergence proof. As our ﬁrst contribution, we convert their expected loss reduction framework to an AL model using strictly proper scoring rules or Bregman divergences [9] instead of classiﬁcation errors. MOCU required manipulations of the expected error, resulting in weighted-MOCU (WMOCU), in order to achieve convergence and avoid getting stuck in error bands. Strictly proper scoring rules naturally avoid this problem by generalising expected errors to expected scores. Using strictly proper scoring rules means better calibrated classiﬁers are rewarded. The scoring rules go beyond simple minimum errors of WMOCU and can be adapted to different kinds of inference tasks (e.g., different utilities, precision-recall trade-offs, etc.). This property is preferable and beneﬁcial for applications such as medical domains where actual errors become less relevant for an inference task.
In order to evaluate the new acquisition functions we use text classiﬁcation, which is our target application domain. For realistic evaluation, we want to use near state of the art systems, which means using pretrained language models with validation sets [24], and neural network ensembles [13].
Ensembling also doubles as a heuristic technique to yield estimates of model uncertainty and posterior probabilities. Coming up with a simple approach to combine ensembling and validations sets is our second research contribution. The importance of these combinations for AL has been noted [40, 25].
For further batch comparisons, we then bring back diversity into the research, suggesting a way to naturally complement our new family of acquisition functions with a method to achieve diversity, our third research contribution. Extensive experiments with a comprehensive set of ablation studies on four text classiﬁcation datasets show that our BEMPS-based AL model consistently outperforms recent techniques like WMOCU and BADGE, although we explicitly exclude recent semi-supervised
AL methods because they represent an unfair comparison against strictly supervised learning. 2