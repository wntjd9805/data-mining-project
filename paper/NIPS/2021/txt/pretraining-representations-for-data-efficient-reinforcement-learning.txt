Abstract
Data efﬁciency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then ﬁnetuned on a small amount of task-speciﬁc data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach signiﬁcantly surpasses prior work combining ofﬂine representation pretraining with task-speciﬁc ﬁnetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data – approaching human-level performance and data-efﬁciency on Atari in our best setting. We provide code associated with this work at https://github.com/mila-iqia/SGI. 1

Introduction
Deep reinforcement learning (RL) methods often focus on training networks tabula rasa from random initializations without using any prior knowledge about the environment. In contrast, humans rely a great deal on visual and dynamics priors about the world to perform decision making efﬁciently (Dubey et al., 2018; Lake et al., 2017). Thus, it is not surprising that RL algorithms which learn tabula rasa suffer from severe overﬁtting (Zhang et al., 2018) and poor sample efﬁciency compared to humans (Tsividis et al., 2017).
Self-supervised learning (SSL) provides a promising approach to learning useful priors from past data or experiences. SSL methods leverage unlabelled data to learn strong representations, which can be used to bootstrap learning on downstream tasks. Pretraining with self-supervised learning has been shown to be quite successful in vision (Hénaff et al., 2019; Grill et al., 2020; Chen et al., 2020a) and language (Devlin et al., 2019; Brown et al., 2020) settings.
Pretraining can also be used in an RL context to learn priors over representations or dynamics. One approach to pretraining for RL is to train agents to explore their environment in an unsupervised fashion, forcing them to learn useful skills and representations (Hansen et al., 2020; Liu & Abbeel, 2021; Campos et al., 2021). Unfortunately, current unsupervised exploration methods require months or years of real-time experience, which may be impractical for real-world systems with limits and costs to interaction — agents cannot be run faster than real-time, may require signiﬁcant human oversight for safety, and can be expensive to run in parallel. It is thus important to develop pretraining methods that work with practical quantities of data, and ideally that can be applied ofﬂine to ﬁxed datasets collected from prior experiments or expert demonstrations (as in Stooke et al., 2021).
∗{max.schwarzer, nitarshan}@mila.quebec 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A schematic diagram showing our two stage pretrain-then-ﬁnetune method. All unsuper-vised training losses and task-speciﬁc RL use the shared torso on the left.
To this end, we propose to use a combination of self-supervised objectives for representation learning on ofﬂine data, requiring orders of magnitude less pretraining data than existing methods, while approaching human-level data-efﬁciency when ﬁnetuned on downstream tasks. We summarize our work below:
RL-aligned representation learning objectives: We propose to pretrain representations using a combination of latent dynamics modeling, unsupervised goal-conditioned reinforcement learning, and inverse dynamics modeling – with the intuition that a collection of such objectives can capture more information about the dynamical and temporal aspects of the environment of interest than any single objective. We refer to our method as SGI (SPR, Goal-conditioned RL and Inverse modeling).
Signiﬁcant advances for data-efﬁciency on Atari: SGI’s combination of objectives performs better than any in isolation and signiﬁcantly improves performance over previous representation pretraining baselines such as ATC (Stooke et al., 2021). Our results are competitive with exploration-based approaches such as APT or VISR (Liu & Abbeel, 2021; Hansen et al., 2020), which require two to three orders of magnitude more pretraining data and the ability to interact with the environment during training, while SGI can learn with a small ofﬂine dataset of exploration data.
Scaling with data quality and model size: SGI’s performance also scales with data quality and quantity, increasing as data comes from better-performing or more-exploratory policies. Additionally, we ﬁnd that SGI’s performance scales robustly with model size; while larger models are unstable or bring limited beneﬁts in standard RL, SGI pretraining allows their ﬁnetuning performance to signiﬁcantly exceed that of smaller networks.
We assume familiarity with RL in the following sections (with a brief overview in Appendix A). 2 Representation Learning Objectives
A wide range of SSL objectives have been proposed for RL which leverage various aspects of the structure available in agent interactions. For example, the temporal dynamics of an environment can be exploited to create a forward prediction task (e.g., Gelada et al., 2019; Guo et al., 2018; Stooke et al., 2021; Schwarzer et al., 2021) in which an agent is trained to predict its immediate future observations, perhaps conditioned on a sequence of actions to perform.
Structure in RL goes far beyond forward dynamics, however. We propose to combine multiple representation learning objectives, covering different agent-centric and temporal aspects of the MDP.
Based on the intuition that knowledge of an environment is best represented in multiple ways (Hessel et al., 2021; Degris & Modayil, 2012), we expect this to outperform monolithic representation learning methods such as temporal contrastive learning (e.g., Stooke et al., 2021). In deciding which 2
tasks to use, we consider questions an adequate representation should be able to answer about its environment, including:
• If I take action a in state s, what state s(cid:48) am I likely to see next?
• If I transitioned from state s to state s(cid:48), what action a did I take?
• What action a would I take in state s so that I reach another state s(cid:48) as soon as possible
Note that none of these questions are tied to task reward, allowing them to be answered in a fully-unsupervised fashion. Additionally, these are questions about the environment, and not any speciﬁc policy, allowing them to be used in ofﬂine pretraining with arbitrary behavioral policies.
In general, the ﬁrst question may be answered by forward dynamics modeling, which as mentioned above is well-established in RL. The second question corresponds to inverse dynamics modeling, which has also been commonly used in the past (Lesort et al., 2018). The third question corresponds to self-supervised goal-conditioned reinforcement learning which has the advantage of being structurally similar to the downstream target task, as both require learning to control the environment.
To facilitate their joint use, we formulate these objectives so that they operate in the latent representa-tion space provided by a shared encoder. We provide an overview of these components in Figure 1 and describe them in greater detail below; we also provide detailed pseudocode in Appendix D. 2.1 Self-Predictive Representations
SPR (Schwarzer et al., 2021) is a representation learning algorithm developed for data-efﬁcient reinforcement learning. SPR learns a latent-space transition model, directly predicting representations of future states without reconstruction or negative samples. As in its base algorithm, Rainbow (Hessel et al., 2018), SPR learns a convolutional encoder, denoted as fo, which produces representations of states as zt = fo(st). SPR then uses a dynamics model h to recursively estimate the representations of future states, as ˆzt+k+1 = h(ˆzt+k, at+k), beginning from ˆzt (cid:44) zt. These representations are projected to a lower-dimensional space by a projection function po to produce ˆyt+k (cid:44) po(ˆzt+k).
Simultaneously, SPR uses a target encoder fm to produce target representations ˜zt+k (cid:44) fm(st+k), which are further projected by a target projection function pm to produce ˜yt+k (cid:44) pm(˜zt+k). SPR then maximizes the cosine similarity between these predictions and targets, using a learned linear prediction function q to translate from ˆy to ˜y:
LSPR
θ (st:t+K, at:t+K) = −
K (cid:88) k=1 q(ˆyt+k) · ˜yt+k
||q(ˆyt+k)||2 · ||˜yt+k||2
. (1)
The parameters of these target modules θm are deﬁned as an exponential moving average of the parameters θo of fo and po: θm = τ θm + (1 − τ )θo. 2.2 Goal-Conditioned Reinforcement Learning
Inspired by works such as Dabney et al. (2021) that show that modeling many different value functions is a useful representation learning objective, we propose to augment SPR with an unsupervised goal-conditioned reinforcement learning objective. We deﬁne goals g to be normalized vectors of the same size as the output of the agent’s convolutional encoder (3,136- or 4,704-dimensional vectors, for the architectures we consider). We use these goals to annotate transitions with synthetic rewards, and train a modiﬁed version of Rainbow (Hessel et al., 2018) to estimate Q(st, a, g), the expected return from taking action a in state st to reach goal g if optimal actions are taken in subsequent states.
We select goals using a scheme inspired by hindsight experience replay (Andrychowicz et al., 2017), seeking to generate goal vectors that are both semantically meaningful and highly diverse. As in hindsight experience replay, we begin by sampling a state from another trajectory or the future of the current trajectory. However, we take the additional step of applying stochastic noise to encourage goals to lie somewhat off of the current representation manifold. We provide details in Appendix C.2. 2.3
Inverse Dynamics Modeling
We propose to use an inverse dynamics modeling task (Lesort et al., 2018), in which the model is trained to predict at from st and st+1. Because this is a classiﬁcation task (in discrete control) or 3
regression task (continuous control), it is naturally not prone to representational collapse, which may complement and stabilize our other objectives. We directly integrate inverse modeling into the rollout structure of SPR, modeling p(at+k|ˆyt+k, ˜yt+k+1) for each k ∈ (0, . . . , K−1), using a two-layer
MLP trained by cross-entropy. 3