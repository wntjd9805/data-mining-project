Abstract
We present an approach to contrastive representation learning for semantic segmentation. Our approach leverages the representational power of existing feature extractors to ﬁnd corresponding regions across images. These cross-image correspondences are used as auxiliary labels to guide the pixel-level selection of positive and negative samples for more effective contrastive learning in semantic segmentation. We show that auxiliary labels can be generated from a variety of feature extractors, ranging from image classiﬁcation networks that have been trained using unsupervised contrastive learning to segmentation models that have been trained on a small amount of labeled data. We additionally introduce a novel metric for rapidly judging the quality of a given auxiliary-labeling strategy, and empirically analyze various factors that inﬂuence the performance of contrastive learning for semantic segmentation. We demonstrate the effectiveness of our method both in the low-data as well as the high-data regime on various datasets. Our experiments show that contrastive learning with our auxiliary-labeling approach consistently boosts semantic segmentation accuracy when compared to standard
ImageNet pre-training and outperforms existing approaches of contrastive and semi-supervised semantic segmentation. 1

Introduction
Training semantic segmentation models typically requires a large amount of densely annotated images. Producing such dense annotations for an adequate number of images at sufﬁcient quality is an extremely laborious and costly task [17]. Self-supervised and weakly-supervised learning techniques offer the possibility to signiﬁcantly reduce the manual labeling effort that is required to train high-performance machine learning models. Contrastive learning [14, 22] has recently emerged as a promising technique for self-supervised representation learning that can leverage large amounts of unlabeled data for augmenting and extending existing labeled datasets without costly manual annotation. However, recent works on contrastive learning primarily focused on image/instance-level learning. Few works have studied contrastive learning in dense image labeling tasks, such as semantic segmentation [50, 51], as dense pixel-level segmentation poses unique challenges.
At its core, self-supervised contrastive learning is rooted in the availability of training samples that can be grouped automatically into positive pairs of similar concepts, and negative pairs of dissimilar concepts. The notion of similarity thereby depends strongly on the task at hand.
The appropriate selection of such pairs is of paramount importance for the performance of the resulting models. While it is often straightforward to generate negative pairs, automatically generating positive pairs can present a formidable challenge that requires intimate knowledge about the data and task at hand. Image augmentations are both simple and effective to generate positive pairs for image/instance-level learning [14, 15] but are too coarse-grained to learn representations at a pixel level.
∗Equal advising. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Pseudo-labels for semi-supervised learning (b) Auxiliary labels for our contrastive learning
Figure 1: Pseudo-labels vs. auxiliary labels. (a) Semi-supervised semantic segmentation requires pseudo-labels to be compatible with the ground truth labels. (b) Our method employs auxiliary labels to ﬁnd corresponding regions across images. These correspondences do not need to match the ground truth classes exactly as we leverage contrastive learning.
Recent works [51,56] adapt the idea of using image augmentations to generate positive pairs for more
ﬁne-grained pixel-level representation learning. While surprisingly effective, image augmentations have only a limited capability of covering the full visual spectrum of how two semantically related pixels or patches can look. Mid and high-level information such as context, texture, large visual shifts inside a semantic class (for example, the class "car" may comprise a wide range of different shapes and colors), and even projective distortions due to the imaging process, may be extremely relevant but cannot be simulated realistically with augmentations alone. As a consequence, methods that exclusively rely on augmentations are limited in the contrastive information they can provide to downstream tasks. A different line of work thus exploits ground truth semantic labels to establish correspondences between images to generate more diverse and informative positive pairings [50, 52].
However, the need for labeled data impedes scaling these approaches to the massive amounts of data required to fully reap the beneﬁts of contrastive learning [14, 22].
In this work, we explore the generation of auxiliary labels to establish ﬁne-grained, pixel-level correspondences across images to select contrastive pairs. We distinguish auxiliary labels from the more commonly employed pseudo-labels [66], as auxiliary labels need not represent the same classes as the ground truth labels. This concept is illustrated in Figure 1. To this end, we leverage feature embeddings that we generate with existing feature extractors. The feature extractors have either been trained using coarse image-level supervision, on a small set of pixel-level labeled data, or using image-level, self-supervised contrastive learning approaches [16, 22]. We cluster feature embeddings and assign auxiliary labels to individual pixels based on cluster membership. We also introduce a simple, yet effective, voting strategy to spatially regularize the auxiliary label maps. The auxiliary labels tend to cluster into semantically and perceptually meaningful groups which can be used to establish cross-image correspondences. This allows us to generate a diverse and realistic set of positive pairs that can be used for ﬁne-grained contrastive visual representation learning. An example that illustrates our approach is shown in Figure 2.
We additionally introduce a proxy metric that allows us to quickly assess the quality of any auxiliary labeling without the need to train a model. Finally, we present a comprehensive experimental
Figure 2: Comparison of augmentation-based contrastive learning (left) to our proposed cross-image contrastive learning with auxiliary labels (right). For an anchor image, prior work [51, 56] samples positive correspondences only within augmentations of the same image. Our method additionally establishes correspondences to other images based on matching auxiliary labels. 2
evaluation of various factors and hyper-parameters that inﬂuence the performance of contrastive training in semantic segmentation.
Our experiments across three datasets and two network architectures show that contrastive learning with auxiliary labels consistently boosts semantic segmentation performance when compared to standard ImageNet pre-training and outperforms existing approaches for contrastive and semi-supervised semantic segmentation. 2