Abstract
Contrastive learning (CL) can learn generalizable feature representations and achieve state-of-the-art performance of downstream tasks by ﬁnetuning a linear classiﬁer on top of it. However, as adversarial robustness becomes vital in image classiﬁcation, it remains unclear whether or not CL is able to preserve robustness to downstream tasks. The main challenge is that in the ‘self-supervised pretraining
+ supervised ﬁnetuning’ paradigm, adversarial robustness is easily forgotten due to a learning task mismatch from pretraining to ﬁnetuning. We call such challenge
‘cross-task robustness transferability’. To address the above problem, in this paper we revisit and advance CL principles through the lens of robustness enhancement.
We show that (1) the design of contrastive views matters: High-frequency com-ponents of images are beneﬁcial to improving model robustness; (2) Augmenting
CL with pseudo-supervision stimulus (e.g., resorting to feature clustering) helps preserve robustness without forgetting. Equipped with our new designs, we pro-pose ADVCL, a novel adversarial contrastive pretraining framework. We show that
ADVCL is able to enhance cross-task robustness transferability without loss of model accuracy and ﬁnetuning efﬁciency. With a thorough experimental study, we demonstrate that ADVCL outperforms the state-of-the-art self-supervised robust learning methods across multiple datasets (CIFAR-10, CIFAR-100 and STL-10) and ﬁnetuning schemes (linear evaluation and full model ﬁnetuning). Code is available at https://github.com/LijieFan/AdvCL. 1

Introduction
Image classiﬁcation has been revolutionized by convolutional neural networks (CNNs). In spite of
CNNs’ generalization power, the lack of adversarial robustness has shown to be a main weakness that gives rise to security concerns in high-stakes applications when CNNs are applied, e.g., face recognition, medical image classiﬁcation, surveillance, and autonomous driving [1–5]. The brittleness of CNNs can be easily manifested by generating tiny input perturbations to completely alter the models’ decision. Such input perturbations and corresponding perturbed inputs are referred to as adversarial perturbations and adversarial examples (or attacks), respectively [6–10].
One of the most powerful defensive schemes against adversarial attacks is adversarial training (AT)
[11], built upon a two-player game in which an ‘attacker’ crafts input perturbations to maximize the training objective for worst-case robustness, and a ‘defender’ minimizes the maximum loss for an improved robust model against these attacks. However, AT and its many variants using min-max optimization [12–21] were restricted to supervised learning as true labels of training data are required 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
for both supervised classiﬁer and attack generator (that ensures misclassiﬁcation). The recent work
[22–24] demonstrated that with a properly-designed attacker’s objective, AT-type defenses can be generalized to the semi-supervised setting, and showed that the incorporation of additional unlabeled data could further improve adversarial robustness in image classiﬁcation. Such an extension from supervised to semi-supervised defenses further inspires us to ask whether there exist unsupervised defenses that can eliminate the prerequisite of labeled data but improve model robustness.
Some very recent literature [25–29] started tackling the problem of adversarial defense through the lens of self-supervised learning. Examples include augmenting a supervised task with an unsupervised
‘pretext’ task for which ground-truth label is available for ‘free’ [25, 26], or robustifying unsupervised representation learning based only on a pretext task and then ﬁnetuning the learned representations over downstream supervised tasks [27–29]. The latter scenario is of primary interest to us as a defense can then be performed at the pretraining stage without needing any label information. Meanwhile, self-supervised contrastive learning (CL) has been outstandingly successful in the ﬁeld of representation learning: It can surpass a supervised learning counterpart on downstream image classiﬁcation tasks in standard accuracy [30–34]. Different from conventional self-supervised learning methods [35],
CL, e.g., SimCLR [30], enforces instance discrimination by exploring multiple views of the same data and treating every instance under a speciﬁc view as a class of its own [36].
The most relevant work to ours is [27, 28], which integrated adversarial training with CL.
However, the achieved adversarial robustness at downstream tasks largely relies on the use of advanced ﬁnetuning techniques, either ad-versarial full ﬁnetuning [27] or adversarial lin-ear ﬁnetuning [28]. Different from [27, 28], we ask: (Q) How to accomplish robustness enhance-ment using CL without losing its ﬁnetuning ef-ﬁciency, e.g., via a standard linear ﬁnetuner?
)
% ( y c a r u c c
A t s u b o
R 50 45 40 35 30 25 20 15
SLF 
Improvement
AFF 
Improvement
SLF
AFF
Supervised
AP-DPE
RoCL
ACL
ADVCL (Ours) 81 84 82 83 79 80 78 77
Standard Accuracy (%)
Our work attempts to make a rigorous and comprehensive study on addressing the above question. We ﬁnd that self-supervised learn-ing (including the state-of-the-art CL) suffers a new robustness challenge that we call ‘cross-task robustness transferability’, which was largely overlooked in the previous work. That is, there exists a task mismatch from pretrain-ing to ﬁnetuning (e.g., from CL to supervised classiﬁcation) so that adversarial robustness is not able to transfer across tasks even if pretraining datasets and ﬁnetuning datasets are drawn from the same distribution. Dif-ferent from supervised/semi-supervised learn-ing, this is a characteristic behavior of self-supervision when being adapted to robust learning. As shown in Figure 1, our work advances CL in the adversarial context and the pro-posed method outperforms all state-of-the-art baseline methods, leading to a substantial improvement in both robust accuracy and standard accuracy using either the lightweight standard linear ﬁnetuning or end-to-end adversarial full ﬁnetuning.
Figure 1: Summary of performance for various robust pretraining methods on CIFAR-10. The covered baseline methods include AP-DPE [26], RoCL [28], ACL [27] and supervised adversarial training (AT) [11]. Upper-right indi-cates better performance with respect to (w.r.t.) standard accuracy and robust accuracy (under PGD attack with 20 steps and 8/255 `
-norm perturbation strength). Different colors represent different pretraining methods, and differ-ent shapes represent different ﬁnetuning settings. Circles (l) indicates Standard Linear Finetuning (SLF), and Di-amonds (u) indicates Adversarial Full Finetuning (AFF).
Our method (ADVCL, red circle/diamond) has the best per-formance across ﬁnetuning settings. Similar improvement could be observed under Auto-Attacks, and we provide the visualization in the appendix. 1
Contributions Our main contributions are summarized below.
∂ We propose ADVCL, a uniﬁed adversarial CL framework, and propose to use original adversarial examples and high-frequency data components to create robustness-aware and generalization-aware views of unlabeled data.
∑ We propose to generate proper pseudo-supervision stimulus for ADVCL to improve cross-task robustness transferability. Different from existing self-supervised defenses aided with labeled data
[27], we generate pseudo-labels of unlabeled data based on their clustering information. 2  
∏ We conduct a thorough experimental study and show that ADVCL achieves state-of-the-art robust accuracies under both PGD attacks [11] and Auto-Attacks [37] using only standard linear ﬁnetuning.
For example, in the case of Auto-Attack (the most powerful threat model) with 8/255 `
-norm perturbation strength under ResNet-18, we achieve 3.44% and 3.45% robustness improvement on
CIFAR-10 and CIFAR-100 over existing self-supervised methods. We also justify the effectiveness of ADVCL in different attack setups, dataset transferring, model explanation, and loss landscape smoothness. 1 2