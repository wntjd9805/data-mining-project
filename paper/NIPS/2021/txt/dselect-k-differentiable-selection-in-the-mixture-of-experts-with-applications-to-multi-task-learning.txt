Abstract
The Mixture-of-Experts (MoE) architecture is showing promising results in improv-ing parameter sharing in multi-task learning (MTL) and in scaling high-capacity neural networks. State-of-the-art MoE models use a trainable “sparse gate” to select a subset of the experts for each input example. While conceptually appealing, existing sparse gates, such as Top-k, are not smooth. The lack of smoothness can lead to convergence and statistical performance issues when training with gradient-based methods. In this paper, we develop DSelect-k: a continuously differentiable and sparse gate for MoE, based on a novel binary encoding formulation. The gate can be trained using ﬁrst-order methods, such as stochastic gradient descent, and offers explicit control over the number of experts to select. We demonstrate the effectiveness of DSelect-k on both synthetic and real MTL datasets with up to 128 tasks. Our experiments indicate that DSelect-k can achieve statistically sig-niﬁcant improvements in prediction and expert selection over popular MoE gates.
Notably, on a real-world, large-scale recommender system, DSelect-k achieves over 22% improvement in predictive performance compared to Top-k. We provide an open-source implementation of DSelect-k1. 1

Introduction
The Mixture of Experts (MoE) [14] is the basis of many state-of-the-art deep learning models. For example, MoE-based layers are being used to perform efﬁcient computation in high-capacity neural networks and to improve parameter sharing in multi-task learning (MTL) [32, 22, 21]. In its simplest form, a MoE consists of a set of experts (neural networks) and a trainable gate. The gate assigns weights to the experts on a per-example basis, and the MoE outputs a weighted combination of the experts. This per-example weighting mechanism allows experts to specialize in different partitions of the input space, which has the potential to improve predictive performance and interpretability. In
Figure 1 (left), we show an example of a simple MoE architecture that can be used as a standalone learner or as a layer in a neural network.
The literature on the MoE has traditionally focused on softmax-based gates, in which all experts are assigned nonzero weights [17]. To enhance the computational efﬁciency and interpretability of
MoE models, recent works use sparse gates that assign nonzero weights to only a small subset of the experts [1, 32, 28, 21]. Existing sparse gates are not differentiable, and reinforcement learning algorithms are commonly used for training [1, 28]. In an exciting work, [32] introduced a new sparse gate (Top-k gate) and proposed training it using stochastic gradient descent (SGD). The ability to train the gate using SGD is appealing because it enables end-to-end training. However, the Top-k gate is not continuous, which can lead to convergence issues in SGD that affect statistical performance (as we demonstrate in our experiments). 1https://github.com/google-research/google-research/tree/master/dselect_k_moe 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (Left): An example of a MoE that can be used as a standalone learner or layer in a neural network. Here “Ei” denotes the i-th expert. (Right): A multi-gate MoE for learning two tasks simultaneously. “Task i NN” is a neural network that generates the output of Task i.
In this paper, we introduce DSelect-k: a continuously differentiable and sparse gate for MoE. Given a user-speciﬁed parameter k, the gate selects at most k out of the n experts. This explicit control over sparsity leads to a cardinality-constrained optimization problem, which is computationally challenging. To circumvent this challenge, we propose a novel, unconstrained reformulation that is equivalent to the original problem. The reformulated problem uses a binary encoding scheme to implicitly enforce the cardinality constraint. We demonstrate that by carefully smoothing the binary encoding variables, the reformulated problem can be effectively optimized using ﬁrst-order methods such as SGD. DSelect-k has a unique advantage over existing methods in terms of compactness and computational efﬁciency. The number of parameters used by DSelect-k is logarithmic in the number of experts, as opposed to linear in existing gates such as Top-k. Moreover, DSelect-k’s output can be computed efﬁciently via a simple, closed-form expression. In contrast, state-of-the-art differentiable methods for stochastic k-subset selection and Top-k relaxations, such as [25, 39]2, require solving an optimization subproblem (for each input example) to compute the gate’s output.
DSelect-k supports two gating mechanisms: per-example and static. Per-example gating is the classical gating technique used in MoE models, in which the weights assigned to the experts are a function of the input example [14, 32]. In static gating, a subset of experts is selected and the corresponding weights do not depend on the input [28]. Based on our experiments, each gating mechanism can outperform the other in certain settings. Thus, we study both mechanisms and advocate for experimenting with each.
MTL is an important area where MoE models in general, and our gate in particular, can be useful.
The goal of MTL is to learn multiple tasks simultaneously by using a shared model. Compared to the usual single task learning, MTL can achieve better generalization performance through exploiting task relationships [4]. One key problem in MTL is how to share model parameters between tasks [30].
For instance, sharing parameters between unrelated tasks can potentially degrade performance. The multi-gate MoE [22] is a ﬂexible architecture that allows for learning what to share between tasks.
Figure 1 (right) shows an example of a multi-gate MoE, in the simple case of two tasks. Here, each task has its own gate that adaptively controls the extent of parameter sharing. In our experiments, we study the effectiveness of DSelect-k in the context of the multi-gate MoE.
Contributions: On a high-level, our main contribution is DSelect-k: a new continuously differ-entiable and sparse gate for MoE, which can be directly trained using ﬁrst-order methods. Our technical contributions can be summarized as follows. (i) The gate selects (at most) k out of the n experts, where k is a user-speciﬁed parameter. This leads to a challenging, cardinality-constrained optimization problem. To deal with this challenge, we develop a novel, unconstrained reformulation, and we prove that it is equivalent to the original problem. The reformulation uses a binary encoding scheme that implicitly imposes the cardinality constraint using learnable binary codes. (ii) To make the unconstrained reformulation smooth, we relax and smooth the binary variables. We demonstrate that, with careful initialization and regularization, the resulting problem can be optimized with
ﬁrst-order methods such as SGD. (iii) We carry out a series of experiments on synthetic and real MTL datasets, which show that our gate is competitive with state-of-the-art gates in terms of parameter sharing and predictive performance. (iv) We provide an open-source implementation of DSelect-k. 2These methods were not designed speciﬁcally for the MoE. 2
1.1