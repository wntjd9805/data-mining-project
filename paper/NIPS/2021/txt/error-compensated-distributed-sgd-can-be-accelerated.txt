Abstract
Gradient compression is a recent and increasingly popular technique for reducing the communication cost in distributed training of large-scale machine learning models. In this work we focus on developing efficient distributed methods that can work for any compressor satisfying a certain contraction property, which includes both unbiased (after appropriate scaling) and biased compressors such as RandK and TopK. Applied naively, gradient compression introduces errors that either slow down convergence or lead to divergence. A popular technique designed to tackle this issue is error compensation/error feedback. Due to the difficulties associated with analyzing biased compressors, it is not known whether gradient compression with error compensation can be combined with acceleration. In this work, we show for the first time that error compensated gradient compression methods can be accelerated. In particular, we propose and study the error compensated loopless Katyusha method, and establish an accelerated linear convergence rate under standard assumptions. We show through numerical experiments that the proposed method converges with substantially fewer communication rounds than previous error compensated algorithms. 1

Introduction
When training very large scale supervised machine learning problems, such as those arising in the context of federated learning [Koneˇcný et al., 2016b, McMahan et al., 2017, Koneˇcný et al., 2016a] (see also recent surveys [Li et al., 2019, Kairouz, 2019]), distributed algorithms are indispensable. In such settings, communication is generally much slower than (local) computation, which makes it the key bottleneck in the design of efficient distributed systems. There are several ways to tackle this issue, including reliance on large mini-batches [Goyal et al., 2017, You et al., 2017], asynchronous learning [Agarwal and Duchi, 2011, Lian et al., 2015, Recht et al., 2011], local updates [Ma et al., 2017, Stich, 2020, Khaled et al., 2020, Hanzely and Richtárik, 2020, Woodworth et al., 2020] and communication compression (e.g., quantization and sparsification) [Alistarh et al., 2017, Bernstein et al., 2018, Mishchenko et al., 2019, Seide et al., 2014, Wen et al., 2017].
Communication compression. In this work, we focus on the last of these techniques: communication compression. The key idea here is to apply a lossy compression transformation/operator to the messages before they are communicated so as to save on communication time. While compression reduces the communicated bits in each communication round, it introduces errors, and this generally leads to an increase in the number of communication rounds needed to find a solution of any predefined accuracy. Still, compression has been found useful in practice, as the trade-off often seems to prefer compression to no compression.
∗King Abdullah University of Science and Technology, Thuwal, Saudi Arabia
†King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Moscow Institute of Physics and Technology, Dolgoprudny, Russia
‡Hong Kong University of Science and Technology, Hong Kong 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Contractive and unbiased compressors. There are two large families of such compression operators: contraction and unbiased compressors [Beznosikov et al., 2020]. A (possibly) randomized map
Q : Rd → Rd is called a contraction compressor if there exists a constant 0 < δ ≤ 1 such that
E (cid:2)∥x − Q(x)∥2(cid:3) ≤ (1 − δ)∥x∥2,
∀x ∈ Rd. (1)
Further, we say that a randomized map ˜Q : Rd → Rd is an unbiased compressor if there exists a constant ω ≥ 0 such that (cid:105) (cid:104) ˜Q(x)
≤ (ω + 1)∥x∥2,
= x and E
∥ ˜Q(x)∥2(cid:105)
∀x ∈ Rd.
E (cid:104) (2) 1
ω+1
It is well known that (see, e.g., [Beznosikov et al., 2020]) after appropriate scaling, any unbiased compressor satisfying (2) becomes a contraction compressor. Indeed, it is easy to verify that for any
˜Q is a contraction compressor satisfying (1) with δ = 1/(ω+1). Thus we can
˜Q satisfying (2), construct corresponding contraction compressor from any unbiased compressor. However, there are empirically very powerful contractive compressors, such as TopK [Alistarh et al., 2018] (described below), which do not arise this way, and because of this, the class of contractive compressors is important in practice. For illustration purposes, we now define two canonical examples of contraction compressors. Let 1 ≤ K ≤ d. The TopK compressor is defined as (TopK(x))π(i) = (cid:26) (x)π(i) 0 if i ≤ K, otherwise, where π is a permutation of {1, 2, ..., d} such that (|x|)π(i) ≥ (|x|)π(i+1) for i = 1, ..., d − 1. The
RandK compressor is defined as (RandK(x))i = (cid:26) (x)i 0 if i ∈ S, otherwise, where S is chosen uniformly from the set of all K element subsets of {1, 2, ..., d}. For TopK and RandK compressors, we have δ ≥ K/d [Stich et al., 2018]. Some frequently used unbiased compressors include random dithering [Alistarh et al., 2017], random sparsification [Stich et al., 2018], and natural compression [Horváth et al., 2019b]. For more examples of contraction and unbiased compressors, we refer the reader to [Beznosikov et al., 2020].