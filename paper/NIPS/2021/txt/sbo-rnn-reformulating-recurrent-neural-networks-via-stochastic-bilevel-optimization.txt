Abstract
In this paper we consider the training stability of recurrent neural networks (RNNs), and propose a family of RNNs, namely SBO-RNN, that can be formulated using stochastic bilevel optimization (SBO). With the help of stochastic gradient descent (SGD), we manage to convert the SBO problem into an RNN where the feedforward and backpropagation solve the lower and upper-level optimization for learning hidden states and their hyperparameters, respectively. We prove that under mild conditions there is no vanishing or exploding gradient in training SBO-RNN.
Empirically we demonstrate our approach with superior performance on several benchmark datasets, with fewer parameters, less training data, and much faster convergence. Code is available at https://zhang-vislab.github.io. 1

Introduction
Training Stability in RNNs. Recurrent neural networks (RNNs) have achieved signiﬁcant success in learning complex patterns for sequential input data. Due to the repeatability of network weights in the chain rule when computing gradients, vanishing or exploding gradients often occur in training RNNs, i.e., the gradient magnitudes either too small or too large, leading to the well-known critical training stability issue [Pascanu et al., 2013a]. To see this, suppose that we are optimizing the following RNN: (1)
E(x,y)∈X ×Y (cid:96)(hT , y; ω), s.t. ht = f (ht−1, xt; θ), ∀xt ∈ x, ∀t ∈ [T ], min
ω,θ where (x, y) ∈ X × Y denotes the training data with samples x = {x1, · · · , xT } ⊆ Rd and label y, ht ∈ RD denotes the hidden state variable at the t-th time step with a predeﬁned initial h0, (cid:96), f denote the loss and nonconvex transition functions parametrized by ω, θ, respectively, and E denotes the expectation operation. The gradient of (cid:96) w.r.t. θ per data in the RNN can be computed as follows: (cid:19)
∂(cid:96)
∂θ
=
∂(cid:96)
∂hT
· (cid:88) 1≤t≤T (cid:18) ∂hT
∂ht
∂ht
∂θ
, where
∂hT
∂ht
= (cid:89) t<k≤T
∂hk
∂hk−1
. (2)
The training challenge mainly comes from ∂hT
∂ht (cid:107) ∂hT
∂ht (cid:107) → +∞ leading to exploding gradients, where (cid:107) · (cid:107) denotes the magnitude of a gradient.
, i.e., (cid:107) ∂hT
∂ht (cid:107) → 0 leading to vanishing gradients and
Penalty/Lagrangian Methods. In fact, the learning problem in Eq. 1 is a constrained optimization problem with recursive equality constraints. To solve it, an intuitive idea is to relax it to an uncon-strained optimization problem using, for instance, penalty methods [Zhang and Brand, 2017] or (augmented) Lagrangian methods [Gu et al., 2020] that lift the constraints into the objective function.
As demonstration, we list a relaxation of Eq. 1 using a penalty method as follows:
 min
ω,θ,{ht}
E(x,y)∈X ×Y
(cid:96)(hT , y; ω) + (cid:88) t∈[T ]
λt (cid:107)ht − f (ht−1, xt; θ)(cid:107)2

 , (3) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where λt ≥ 0, ∀t denotes a penalty coefﬁcient. Now the gradient w.r.t. θ is only dependent on the second term, i.e., least squares, with no training stability issue. Such relaxation has modiﬁed the RNN network architectures that leads to new challenges in such methods for RNN training, for instance:
• The size of variables {ht} increases linearly w.r.t. the sequence length T , leading to large memory.
• The learned parameters ω, θ may not work well in the original RNN at test time, due to the penalty.
How to solve these challenges in the relaxed training approaches still remains an open question [Gu et al., 2020].
Bilevel Optimization (BO) & Ordinary Differential Equations (ODEs). Alternatively, based on a similar idea of penalty methods, a trivial yet equivalent modiﬁcation of Eq. 1 is listed as Eq. 4 below, which essentially deﬁnes a bilevel optimization problem where one optimization problem is embedded within the other (e.g., [Vicente and Calamai, 1994, Sinha et al., 2017]): min
ω,θ
≡ min
ω,θ
E(x,y)∈X ×Y (cid:96)(hT , y; ω), s.t. ht = arg min h (cid:107)h − f (ht−1, xt; θ)(cid:107)2, ∀t ∈ [T ]
E(x,y)∈X ×Y (cid:96)(hT , y; ω), s.t. ˙ht = ht − f (ht−1, xt; θ), ∀t ∈ [T ], (4) (5) where ˙ht denotes the change rate of variable ht over time in the gradient ﬂow, and ˙ht = 0 when ht converges to a ﬁxed point, if exists. In fact, some of recent ODE based RNNs have similar transition functions to Eq. 5. For instance, Kag et al. [2020] proposed an incremental RNN (iRNN) with a transition function of −β ˙ht = α (ht + ht−1) − f (ht + ht−1, xt; θ) , ht(0) = 0, and proved that there is no vanishing/exploding gradient in training iRNN. As we can see, the key difference between iRNN and Eq. 5 is that the current hidden state variable ht is introduced into function f , leading to a sequence of updates for ht through variable discretization. However, to the best of our knowledge, so far no work has been proposed to formulate RNNs using BO. Recently BO regains attention in meta-learning. For instance, Franceschi et al. [2018] proposed a framework based on bilevel programming to unify gradient-based hyperparameter optimization and meta-learning. Mounsaveng et al. [2021] proposed using online BO to tune the hyperparameters for data augmentation.
Our Contributions. In contrast to the literature, in this paper we consider the training of RNNs itself as a bilevel optimization problem. Speciﬁcally, we propose a new family of RNNs, namely
SBO-RNN, that can be formulated using stochastic bilevel optimization (SBO), where we consider hidden state vectors as intermediate auxiliary variables that depend on the RNN model parameters.
These auxiliary variables are optimized in the lower-level problem of SBO, and the model parameters as well as the predictor are learned by optimizing the outer objective function. We utilize stochastic gradient descent (SGD) and its momentum variants to solve such SBO problems, where in return the optimization process of the lower-level problem can be (approximately) represented by SBO-RNN.
We also prove that under mild conditions, our SBO-RNN can avoid vanishing or exploding gradients in training by selecting proper learning rates for the lower-level problem. We conduct comprehensive experiments on several benchmark datasets to evaluate our approach, and achieve superior results.
In summary, our key contributions in this paper are listed as follows:
• We propose a new family of RNNs, namely SBO-RNN, that are the ﬁrst in the literature, to the best of our knowledge, to formulate RNNs using stochastic bilevel optimization for streaming data.
• We prove that our networks manage to obtain good training stability by selecting proper learning rates for the lower-level problem to avoid vanishing and exploding gradients.
• We demonstrate superior performance empirically with fewer parameters, less training data, and much faster convergence. 2