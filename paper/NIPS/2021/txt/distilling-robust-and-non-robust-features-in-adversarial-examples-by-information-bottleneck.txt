Abstract
Adversarial examples, generated by carefully crafted perturbation, have attracted considerable attention in research ﬁelds. Recent works have argued that the exis-tence of the robust and non-robust features is a primary cause of the adversarial examples, and investigated their internal interactions in the feature space. In this paper, we propose a way of explicitly distilling feature representation into the ro-bust and non-robust features, using Information Bottleneck. Speciﬁcally, we inject noise variation to each feature unit and evaluate the information ﬂow in the feature representation to dichotomize feature units either robust or non-robust, based on the noise variation magnitude. Through comprehensive experiments, we demonstrate that the distilled features are highly correlated with adversarial prediction, and they have human-perceptible semantic information by themselves. Furthermore, we present an attack mechanism intensifying the gradient of non-robust features that is directly related to the model prediction, and validate its effectiveness of breaking model robustness. 1

Introduction
Deep neural networks (DNNs) have achieved remarkable performances in a wide variety of machine learning tasks. Despite the breakthrough outcomes, DNNs are easily fooled from adversarial attacks, with crafted perturbations [1, 2, 3, 4, 5, 6, 7]. These perturbations are imperceptible to human eyes, but simply adding them to clean images (i.e., adversarial examples) can effectively deceive classiﬁers.
Such a vulnerability affects security problems [8, 9, 10, 11], bringing in the weak reliability of DNNs.
Previous works have broadly investigated the reason for the widespread of such adversarial examples.
Goodfellow et al. [2] have argued that adversarial vulnerability is induced from the excessive linearity nature of DNNs in high-dimensional spaces. Several works [1, 12] have regarded the primary cause of the adversarial examples as statistical variation with aberrations in data manifold. Schmidt et al. [13] have suggested that the pervasiveness of the examples should not be considered as a drawback of training methods for DNNs, since the available dataset may not be large enough to train them robustly.
In recent years, Tsipras et al. [14] have suggested an intriguing analysis that the disagreement between standard and adversarial accuracy stems from differently trained feature representation. In this literature, Ilyas et al. [15] further have demonstrated the adversarial examples are inevitable results of standard supervised training and arisen from well-generalized features in the dataset. They have analyzed the adversarial examples are originated from brittle and unintelligible features (i.e., non-robust features) that are arbitrarily manipulated with the imperceptible noise, and shown that the
∗Equal contribution. † Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
robust features still can provide precise accuracy even in the existence of adversarial perturbation.
They have argued that the non-robust features cannot show reliable accuracy in the adversarial setting and could provoke incomprehensible properties.
Nonetheless, the underlying reason for the existence and pervasiveness of adversarial examples cannot derive common consensus in the research ﬁeld and still remains unclear [16]. To clarify where the adversarial brittleness truly comes from, we need to ﬁgure out how the robust and non-robust features in data manifold subtly manipulate feature representation and fool model prediction, by directly handling them in the feature space. To address it, we propose a way to precisely distill intermediate features into robust and non-robust features by employing Information Bottleneck (IB) [17, 18, 19]. In the sense that semantic information is included in the units of intermediate feature representation [20, 21, 22, 23], we utilize the bottleneck to regulate the information ﬂow in the feature space by explicitly adding noise to the feature units. Then, we estimate how each feature unit contaminated with the noise affects model prediction with assigned information. Based on the prediction sensitivity of the noise intervention, we assort the feature units either robust or brittle, and disentangle the feature representation into robust or non-robust features, respectively.
Through extensive analysis of the distilled features, we corroborate that the pervasiveness of the adversarial brittleness is derived from the non-robust features, and they have a high correlation with the adversarial prediction. In addition, in order to understand the semantic information of distilled features, we directly visualize them in the feature space and provide their visual interpretation.
Consequently, we reveal that both of the robust and non-robust features indeed have semantic information in terms of human-perception by themselves. Based on our observation, we theoretically describe the negative impact of the non-robust features for the model prediction and introduce an approach of amplifying the gradients of non-robust features to break the model prediction.
In this paper, our contributions can be summarized into three-fold as follows:
• We propose a novel way to explicitly distill intermediate features into the robust and non-robust features using Information Bottleneck, and interpret the disentangled features in terms of human-perception by directly visualizing them in the feature space.
• By analyzing how the distilled features affect the intermediate feature representation under adversarial perturbation, we demonstrate that the non-robust features are highly correlated with the adversarial prediction.
• We present an attack mechanism manipulating the non-robust features by strengthening their gradients, and validate its effectiveness of breaking model prediction. 2 Distilling Robust and Non-robust Features in Intermediate Feature Space
Problem Setup and Notations. Let X denote clean images and Y denote (one-hot encoded) target labels corresponding to the clean images. Then, adversarial examples Xadv can be created by the
E(X,Y )[L(f (X + δ), Y )], where δ denotes an adversarial perturbation, following equations: max and L denotes a pre-deﬁned loss for machine learning tasks. The adversarial examples can be made by Xadv = X + δ. When a given model f is adversarially trained against PGD attack [7], it can be written as follows:
δ min w max (cid:107)δ(cid:107)∞≤γ
E(X,Y ) [L (f (X + δ), Y )] , (1) where w represents the parameters of f , which are learned to be robust against adversarial attacks.
Here, (cid:107)·(cid:107)∞ ≤ γ describes L∞ norm, and γ-ball means the perturbation magnitude. In this paper, we adversarially train the model f on γ = 0.03 for the standard adversarial attack. Note that once adversarially trained, the parameters of the model f are no longer covered.1
As notation of variables that we will use in this paper, Z and ¯Z indicate the intermediate features of the model f such that Z = fl(X) and ¯Z = fl(Xadv), where fl(·) describes l-th layer outputs of the given model. Similarly, fl+(·) represents subsequent network after the l-th layer, thus intermediate features can be propagated to the last output layer, such that ˆY = fl+(Z) and ˆYadv = fl+( ¯Z). The 1Previous works [14, 15] have demonstrated that the distinction between the robust and non-robust features arises in adversarial settings. In the sense that adversarially trained networks learn robust representation [24], we set the robust classiﬁer as default. Please see the analysis of the standard training in Appendix F. 2
Figure 1: Diagrams of our Information Bottleneck (IB), minimizing LCE + βLI to ﬁnd noise variation σ that can estimate information ﬂow in the intermediate features Z. Here, σz represents nature feature variation of intermediate features Z for each unit, and (cid:15) indicates Gaussian noise sampled from N (0, I). More implementation details are described in Appendix A. model f can be expressed as f = fl+ ◦ fl, satisfying ˆY = f (X) for the given clean images X. Also,
ˆYadv = f (Xadv) is denoted by model propagation of the adversarial examples Xadv. Note that we designate l-th layer as the last convolutional layer, and regard l+ as the rest of the layers in the model. 2.1
Information Bottleneck for Distilling Informative Features
In adversarial settings, we focus on separating robust and non-robust features in the intermediate layer.
Recall that robust features are literally robust on the noise (variation) and invariant to the existence of the adversarial perturbation, but non-robust features are not. Our approach aims to distill feature units that affect target prediction under the noise perturbation in the intermediate feature space.
We follow that the semantic information is inherently included in the feature units of DNNs [20, 22, 25]. From this perspective, we utilize Information Bottleneck (IB) to distill the robust and non-robust features on the given intermediate features Z. Information Bottleneck [17, 18] proposed to encode maximally informative representation for target labels, restraining input information, concurrently.
Using the bottleneck, we suggest a way to assess feature importance and quantify information ﬂow for the target prediction. The objective function of IB can be written as follows: max
Z
I(Z, Y ) − βI(Z, X), (2) where I denotes mutual information, and β represents the degree of restraining input information.
The ﬁrst term I(Z, Y ) allows the intermediate features Z to be predictive on the target label Y , and the second term I(Z, X) encourages Z to compress the information of the given images X in the bottleneck. Here, the second term requires a true feature probability p(Z) = (cid:82)
X p(Z | X)p(X)dX to expand it, but it is computationally intractable due to a high dimensional dependency of the dataset probability p(X). Thus, several works [18, 19] modiﬁed the IB’s objective function to make it possible to learn DNNs without the true feature probability as follows: min LCE + βLI (see
Appendix B). In this formulation, LCE indicates cross-entropy loss, and LI represents information loss computed by KL divergence [26] between a feature likelihood p(Z | X) and an approximate feature probability q(Z). It is radically a closed-form approximation for the true feature probability p(Z).
Firstly, we deliberately inject noise variation into Z to estimate the prediction sensitivity of each feature unit along the noise intervention. To do so, we newly design an approximate feature probability using noise variation σ, such that qσ(Z) = N (fl(X), σ2). Then, we sample random variables from qσ(Z) and deﬁne informative features ZI as follows:
ZI = fl(X) + σ · (cid:15), (3) where ZI ∼ qσ(Z). Note that the operator · denotes Hadamard product, and (cid:15) stands for Gaussian noise sampled from N (0, I). Here, the noise variation measures a correlation between intermediate features and model prediction based on the fact that robustness means high correlation on model prediction and non-robustness are opposite [14, 15] in adversarial settings. Since the correlation
E(X,Y )[Y · f (X)] in output layer can be expressed as a variance measure Cov(Y, f (X)), we consider the correlation in intermediate layer as the noise variation. From this perspective, if a feature unit is 3
highly predictive despite large noise variation (high correlation), the unit can robustly predict target labels, while a non-robust unit cannot.
Once the informative features ZI are acquired from the noise variation σ, we propagate ZI to the last output layer and estimate the feature importance of each unit for model prediction. Then, we deal with the information loss LI in order to alleviate feature heterogeneity between Z and ZI . Through aforementioned modiﬁcation [18, 19], our objective function can be written as follows: min
σ
LX (σ) = −Y log fl+(fl(X) + σ · (cid:15)) (cid:125) (cid:124) (cid:123)(cid:122)
LCE
,
+β DKL[p(Z | X) || qσ(Z)] (cid:125) (cid:124) (cid:123)(cid:122)
LI (4) (cid:80)C k=1[ where the feature likelihood p(Z | X) is set to N (fl(X), σ2 z ). Here, σz indicates inherent feature variation of the intermediate features Z for each unit. The second term LI makes the informative features ZI resemble the intermediate features Z, while minimizing the cross-entropy LCE. This
σ2 second term can be written as LI = 1 zk
− 1], where k denotes an index of
σ2 2 k the noise variation σ = [σ1, σ2, · · · , σC] (the optimizing parameters) and the feature variation
σz = [σz1 , σz2, · · · , σzC ] (the given parameters). Here, only of the variation σ is updated such that
σ ← σ − ∂
∂σ LX (σ). In brief, we summarize overall procedure of our bottleneck concept in Fig. 1.
Moreover, we mention that β in Eq. (4) controls the amount of information that ﬂows into the feature representation. Speciﬁcally, when β is set to zero, IB loss is equivalent to cross-entropy loss, which means that ZI can accommodate even unimportant features to predict target labels. In contrast, excessively large β only focuses on compressing input information, thus IB may cannot ﬁlter out important features to predict target labels. Accordingly, we empirically control β to distill informative features ZI based on the noise variation σ (Please see section 3.4 for analysis of information ﬂow).
+ log σ2 k
σ2 zk 2.2 Separating Informative Features by Tolerance of Feature Variation
After optimizing the informative features ZI , we compare the noise variation σ for the informative feature units and dichotomize each unit either robust or non-robust based on their prediction sensitivity.
We set the criterion for comparison as T = max(σ2 z ). Here, T represents the maximum tolerance of the noise variation. It is a reasonable choice to set T as a criterion, because it indicates the maximal variation with respect to the changes of the given image X, in the feature space. In the following procedure, we explicitly disentangle intermediate features Z into the robust Zr and non-robust features Znr.
Firstly, once the noise variation σ is larger than the maximum tolerance T in speciﬁc units, it indicates that their corresponding features are highly predictive on the model prediction, despite the noise intervention. Thus, we deﬁne their conjunction as robust features. On the other hand, if the variation of a speciﬁc unit is smaller than T , their corresponding features can be represented as non-robust features. This is because the small variation behaves as a strict restriction to retain model prediction of target labels. We assume that once a strong adversarial perturbation comes in, the feature variation of non-robust features becomes to be larger than acceptable tolerance, thereby easily breaking the model classiﬁer and leading to misclassiﬁed prediction. In this respect, we sort the noise variation according to their magnitude, and cluster them by assigning robust or non-robust channel indexes to each feature unit. The robust channel index, ir = [ir1, ir2 , · · · , irC ] can be computed as follows: irk = 1(σ2 k > T ) = (cid:26)1 σ2 > T 0 σ2 ≤ T
, (5) where 1(·) represents the indicator function. The non-robust channel index inr is simply reversed from the robust channel index such that inrk = 1 − irk . Then, we estimate robust features Zr by multiplying the robust channel index to the intermediate features element-wisely such that Zr = ir ·Z.
Similarly, non-robust features Znr are presented as Znr = inr · Z. In this way, the intermediate features Z are fully disentangled into the two types of feature representation satisfying Z = Zr + Znr.
To sum it up, we regard the robust features Zr that have the larger noise variation as invariant features from the adversarial perturbation. On the other hand, non-robust features Znr are considered as easily manipulated features, which harmonize the smaller noise variation. Now, we analyze their impacts to the robustness by expanding the model prediction of ZI to Taylor approximation (see Appendix C.) 4
Table 1: Classiﬁcation accuracy of model performance attacked by FGSM [2], PGD [7], and CW [4] on VGG-16 [29] and WRN-28-10 [30], adversarially trained with γ = 0.03 for CIFAR-10, SVHN, and Tiny-ImageNet. We selectively propagate each feature (i.e., intermediate features (Int.), robust (R.), and non-robust features (NR.)) to measure classiﬁcation accuracy.
Model Example
VGG
WRN
Clean
FGSM [2]
PGD [7]
CW [4]
Clean
FGSM [2]
PGD [7]
CW [4]
CIFAR-10
SVHN
Tiny-ImageNet
Int. Acc R. Acc NR. Acc Int. Acc R. Acc NR. Acc Int. Acc R. Acc NR. Acc 99.76 98.72 97.91 99.60 99.44 97.90 96.33 97.58 99.87 99.58 99.38 99.85 98.66 96.80 96.61 97.74 90.35 63.71 48.92 33.26 93.53 73.93 61.09 40.61 34.82 22.82 20.64 13.66 44.67 31.65 29.44 17.28 57.09 40.12 32.18 16.75 70.43 53.96 45.79 22.78 79.73 51.28 44.71 40.32 82.56 56.43 51.63 45.47 83.11 77.74 77.59 75.69 96.35 91.58 90.37 95.78 33.98 17.45 16.13 12.00 43.13 20.38 18.84 13.51 7.50 4.94 4.69 4.03 6.07 3.01 2.83 2.06 (a) Clean Examples (b) PGD Examples
Figure 2: The result of t-SNE plot [31] in CIFAR-10 dataset for VGG-16 network. Each cluster indicates high-dimensional distributions of feature representation for 10 object labels in CIFAR-10 dataset. Additional t-SNE results for other adversarial attacks are illustrated in Appendix D. with its convergence of local minima [27, 28] as follows: fl+(fl(X) + σ · (cid:15)) = fl+(fl(X) + σr · (cid:15)) + (cid:20) ∂
∂σr (cid:124) fl+(fl(X) + σr · (cid:15))
σnr
, (6) (cid:21)T (cid:123)(cid:122)
∆ (cid:125) where robust noise variation σr = ir · σ and non-robust noise variation σnr = inr · σ. Since the variation of the robust features does not degrade the model prediction, when σnr is small enough, the erroneous term ∆ in Eq. (6) closes to zero. That is, the model retains having signiﬁcant robustness against the adversarial perturbation interrupting robust channel index in Z. Conversely, once we force
σnr to increase, its output becomes inaccurate for the robust prediction (i.e., ﬁrst term in Eq. (6)) due to high ∆. Here, we theoretically demonstrate how the brittleness of non-robust features affects the model robustness. We will thoroughly analyze the properties of the two distilled features by empirically showing the robustness of Zr and brittleness of Znr in the following sections. 3 Analysis of Distilled Features and Visual Interpretation 3.1 Property of Distilled Feature Units under Adversarial Perturbation
After we distill the robust and non-robust features using the bottleneck concept, our next question is
How will the target prediction change under the adversarial attacks? In our posit, if the bottleneck successfully disentangles the robust and non-robust channel index (i.e., ir and inr) from the given examples, we should identify the consequential classiﬁcation accuracy changes under the adversarial perturbation. That is, after applying the robust index to the attacked feature representation, the selected adversarial features with ir denoted by ¯Zr (i.e., ¯Zr = ir · ¯Z) should have invariant accuracy changes for the target labels. Here, ir is robust channel index obtained from the given clean examples X, and ¯Z represents the intermediate features of the adversarial examples, which means ¯Z = fl(Xadv).
Contrarily, the selected features satisfying ¯Znr = inr · ¯Z will show inaccurate accuracy due to their brittleness under the existence of adversarial perturbation. 5
(a) Clean Examples (b) PGD Examples
Figure 3: Feature visualization [25] for the intermediate feature (Int.), robust feature (R.), and non-robust feature (NR.). The class labels under each image indicate the predicted results of the corresponding features propagated by fl+(·). Note that the visualization of the non-robust features displays semantic similarity of the misclassiﬁed classes of the adversarial examples. Please see more visualization results in Appendix E.
In Table 1, we analyze evaluation results of the disentangled features under standard attack algo-rithms [2, 4, 7] in publicly available datasets [32, 33, 34]. As aforementioned, we apply the robust and non-robust channel index optimized from the clean examples to the adversarial features ¯Z, and estimate their accuracy (i.e., fl+( ¯Zr) and fl+( ¯Znr)). As in the table, ¯Zr still shows constant robust accuracy regardless of the adversarial perturbation, even in the high-conﬁdence adversarial attack [4].
On the other hand, we can ﬁnd that the classiﬁcation accuracy of ¯Znr steeply degrades as the attacks get stronger, which coincides with the properties of the robust and non-robust features. To further support our experiments, we illustrate the correlation between the disentangled features and true labels, using 2D t-SNE plot [31]. In the case of clean examples as in Fig. 2(a), the robust features
Zr exhibit separable clusters on the target labels, while the non-robust features Znr show a partially disorganized tendency. When adversarial perturbation [7] exists, the attacked features ¯Znr represents more collapsed t-SNE visualization as shown in Fig. 2(b). Notably, we can observe that ¯Zr still sustain highly clustered results even in the attacked condition. 3.2 Feature Visualization of Robust and Non-robust Features
We have identiﬁed the existence of the robust and non-robust features using the bottleneck. Then, we wonder about a way of interpreting the semantic information in the feature space with respect to human-perception. Analyzing semantic representation of DNNs is a wide research area to understand their decision [20, 22, 35].
In adversarial settings, several studies [14, 36] argued that a robust classiﬁer has more meaningful (i.e., perceptually-aligned) loss gradients in the input space. Engstrom et al. [24] further endeavored to interpret robust feature representation using feature visualization [25, 37, 38]. In this manner, we explore whether the disentangled feature representation from the bottleneck indeed has human-perceptible information in the intermediate feature space.
Feature visualization is an optimization-based method that maximizes speciﬁc activation of feature units [38, 39], such that X (cid:48) = argmaxX (al i(X)), where al i(·) indicates feature activation of i-th unit in the l-th layer. To understand what conjunction of the robust and non-robust feature units truly interacts with the target labels, we optimize each distilled feature and create their visual explanations.
We adopt direct visualization method [25] that has various regularization techniques (e.g., frequency penalization and transformations) to yield better representative visual quality.
We employ ia nr for the adversarial examples Xadv, which is obtained by optimizing LXadv (σ) instead of Eq. (4). Then, we deﬁne ¯Z a nr as robust and non-robust features of the adversarial examples, satisfying ¯Z a nr · ¯Z. After distilling robust and non-robust features with their corresponding index, we maximize the selected feature unit activation, respectively. The feature visualization results of distilled features are illustrated in Fig. 3. r and ¯Z a nr = ia r · ¯Z and ¯Z a r and ia r = ia 6
Table 2: The prediction accuracy of the non-robust features ˆZ a nr for attacked labels ˆYadv. The input ˆZ a nr is the non-robust features of the corresponding attack methods. To clearly show the correlation between ad-versarial examples and the non-robust features, we evaluate the accuracy under the condition of success-fully attacked examples (i.e., Y (cid:54)= ˆYadv).
Attack
FGSM [2]
PGD [7]
CW [4]
CIFAR-10
SVHN
Tiny-ImageNet
VGG WRN VGG WRN VGG WRN 60.82 92.78 63.64 93.43 56.84 93.72 94.35 95.06 94.42 96.13 96.44 97.65 63.39 65.84 55.68 94.90 96.21 95.75
Figure 4: An example of highly correlated ad-versarial prediction with NR. Both ¯Yadv and
Z a nr output same prediction "Bell Pepper".
As in the ﬁgure, we can clearly recognize the semantic information of true labels Y and attacked predictions ˆYadv in the intermediate features (Z and ¯Z). Interestingly, what we can observe is: (i) the distilled features have semantic information by themselves and maintain their information, even under the adversarial perturbation, (ii) when the adversarial perturbation exists, the brittleness of non-robust features is intensiﬁed and reﬂected onto ¯Z. Thus, the visualization of ¯Z and ¯Z a nr looks similar, and they manipulate the target prediction to same adversarial prediction. Unlike the previous work [15] that has argued the non-robust features solely have incomprehensible property, the visualization of the distilled features from our bottleneck represent recognizable outputs even for the adversarial examples, and provides a decisive key to interpret the cause of adversarial examples. 3.3 Adversarial Prediction is Highly Correlated with Non-robust Features
We have observed that the non-robust features optimized from the bottleneck are brittle and easily manipulated under the adversarial perturbation, while robust features maintain substantial prediction results for the target labels. Then, if the primary cause of the adversarial examples indeed belongs to non-robust features, it is natural to examine the correlation between the classiﬁcation outputs of the non-robust features and the adversarial prediction induced by adversarial attacks. Accordingly, we identically apply our IB loss on the adversarial examples Xadv [2, 7, 4], and ﬁnd their corresponding robust and non-robust channel index using Eq. (5). nr for the attacked labels ˆYadv that can be written as follows: ˆY a
To enlighten the correlation of non-robust features and the adversarial prediction, we evaluate the model prediction of ¯Z a nr).
We set the condition of ˆYadv as successfully attacked labels (i.e., Y (cid:54)= f (Xadv)) to deﬁnitely show nr of non-robust features and the adversarial prediction ˆYadv the relationship between the prediction ˆY a of adversarial examples. In Table 2, we summarize the accuracy of ˆY a nr for the successfully attacked label ˆYadv in standard attack methods. Generally, we can observe that the non-robust features are highly predictive on ˆYadv in standard low dimensional datasets such as CIFAR-10 and SVHN. Even in a large dataset (i.e., Tiny-ImageNet), the non-robust features are remarkably correlated with the attacked prediction. A brief explanation of the highly correlated example is described in Fig. 4. nr = fl+( ¯Z a 3.4 Bottleneck Controls Information Flow of Robust and Non-robust Features
In this analysis, we will investigate how the bottleneck affects the information ﬂow of the robust and non-robust features and clarify their relation. Recall that the bottleneck reﬁnes informative features from the given image samples, and β regulates the total amount of the information that ﬂows into Z.
We will compare classiﬁcation accuracy for the robust and non-robust features along β value and analyze the changes of information ﬂow assigned to each disentangled feature.
In Fig. 5, as β value increases, the accuracy of the robust features is getting higher and decreases after a speciﬁc threshold. As theoretically mentioned in 2.1, we can infer that a suitable choice of the β can ﬁlter out robust feature units in the intermediate layer. For the excessive β value, we can observe that the bottleneck cannot accurately disentangle adversarial features. For example, the accuracy of the robust and non-robust features are reversed after β = 5.0 in the particular networks of CIFAR-10 and SVHN datasets. In addition, as in Fig. 5(a) and (b), the accuracy of the non-robust features progressively increases. Such results indicate a few robust feature units that are not distilled 7
Table 3: Comparing attack performance for FGSM [2], BIM [40], PGD [7], CW [4], AutoAttack (AA) [41], FAB [42], and non-robust feature attack denoted by NRF. We adversarially train VGG-16 and WRN-28-10 on L∞ norm γ = 0.03 perturbation for CIFAR-10, SVHN, and Tiny-ImageNet with
PGD adversarial training [7] (ADV) and advanced defense methods: TRADES [43] and MART [44].
Dataset
Method
VGG-16
WRN-28-10
CIFAR-10
SVHN
Tiny-ImageNet
Clean FGSM BIM PGD CW AA FAB NRF Clean FGSM BIM PGD CW AA FAB NRF 52.8 51.6 45.5 49.8 49.0 17.1
ADV 79.7 55.0 53.9 46.7 52.4 49.8 26.8
TRADES 78.2 56.0 54.7 46.5 52.8 50.2 19.6 73.5
MART 64.8 61.1 40.7 55.5 56.6 13.4
ADV 90.4 63.9 60.4 42.0 55.0 55.4 10.1
TRADES 90.4 64.5 61.1 42.3 55.4 56.0 8.1 90.5
MART 19.3 18.8 13.5 18.1 14.2 5.3
ADV 34.0 25.6 25.2 17.4 24.4 17.7 9.6
TRADES 38.7 26.1 25.7 17.5 25.0 17.8 9.9 38.4
MART 46.5 44.7 40.3 42.0 40.9 27.4 82.6 51.7 50.9 43.0 49.5 46.3 31.2 83.0 52.2 51.7 42.2 50.6 45.1 31.4 83.4 52.1 48.8 33.3 39.9 41.6 12.6 93.5 59.0 57.0 44.8 53.5 50.0 14.3 93.9 59.7 57.8 46.4 53.0 47.0 16.1 94.1 16.5 16.1 12.0 15.4 12.2 6.7 43.1 19.1 18.7 13.9 17.8 13.3 7.8 47.2 19.5 19.1 14.1 18.3 14.7 9.2 48.5 56.4 57.9 59.0 73.9 72.9 73.0 20.4 26.7 27.4 51.3 54.5 54.2 63.7 65.3 65.1 17.5 20.1 20.6 (a) VGG-16 (c) # of Assigned Channels
Figure 5: The accuracy of robust and non-robust features along β value, and the number of assigned channels in the l-th feature representation. Note that each color of the lines in (c) corresponds to the same color in the bar plots of (a) and (b). The total number of channels is equivalent to the size of C. (b) WRN-28-10 from IB are gradually ﬂowing into the non-robust side and dichotomized as non-robust units, thus producing more higher accuracy. It coincides with the analysis of the number of the assigned robust and non-robust channels in Fig. 5(c). We can observe that the number of robust channels constantly diminishes, whereas that of the non-robust channels increases. It indicates the bottleneck delicately weighs the information ﬂow that should be precisely assigned to the robust or non-robust units.
From our analysis section, in conclusion, we have demonstrated the existence of the distilled features using the bottleneck: the robust and non-robust features in the intermediate feature representation.
In addition, we have revealed that easily manipulated property of non-robust features is the primary cause of adversarial examples through their high correlation with the adversarial prediction. Based on the fact that the non-robust features break the model prediction, we will suggest an effective way of enhancing the adversarial attack, utilizing the gradient of non-robust features in section 4. 4 Amplifying Brittleness of Non-Robust Features for Attack
In this section, we intentionally increase the non-robust noise variation σnr to make model classiﬁer fooled based on Eq. (6). Note that increasing σnr induces high erroneous term ∆, thereby going to deviate from the robust prediction. However, this variation σnr is merely an optimizing parameter that cannot be controlled manually. Thus, in order to secondarily have the effect of enlarging σnr, we alternatively utilize the gradients of the non-robust features directly connected with the model 8
Table 4: Comparison of classiﬁcation accuracy for adversarial examples generated by maximizing (↑) or minimizing (↓) the magnitude of robust (Gr) or non-robust feature gradients (Gnr).
Dataset
Method
VGG-16
WRN-28-10
CIFAR-10
SVHN
Tiny-ImageNet
Clean (cid:107)Gnr(cid:107)2 ↑ (cid:107)Gnr(cid:107)2 ↓ (cid:107)Gr(cid:107)2 ↑ (cid:107)Gr(cid:107)2 ↓ Clean (cid:107)Gnr(cid:107)2 ↑ (cid:107)Gnr(cid:107)2 ↓ (cid:107)Gr(cid:107)2 ↑ (cid:107)Gr(cid:107)2 ↓ 79.7
ADV
TRADES 78.2 73.5
MART
ADV 90.4
TRADES 90.4
MART 90.4 34.0
ADV
TRADES 38.7 38.4
MART 27.4 31.2 31.4 12.6 14.3 16.1 6.7 7.8 9.2 74.8 73.9 74.3 88.0 88.1 87.7 38.9 40.9 41.3 17.1 26.8 19.6 13.4 10.1 8.1 5.3 9.6 9.9 74.6 77.1 69.1 71.5 82.3 84.9 29.0 33.8 32.5 35.9 38.2 39.6 20.8 27.3 31.6 9.7 11.8 13.4 67.5 71.6 63.8 71.9 68.4 66.2 26.1 30.7 29.0 79.5 79.9 79.4 93.4 93.3 91.5 39.9 44.4 45.7 82.6 83.0 83.4 93.5 93.9 94.1 43.1 47.2 48.5 28.9 30.3 24.5 15.6 13.2 9.0 15.5 16.9 17.2 prediction. The gradients of non-robust features in adversarial examples can be described as follows:
Gnr =
∂
∂ ¯Znr
Lbase(f (X + δ), Y ), (7) where we deﬁne a baseline loss as Lbase(f (X), Y ) = (cid:107)δ(cid:107)2 + c · max(max i(cid:54)=Y (f (X)i) − f (X)i, 0), instead of cross-entropy loss due to its empirical effectiveness of attack performance [4]. In addition, we use a technique of changes of variables [4] from δ to w for generating an imperceptible yet powerful perturbation, such that δ = 1 2 (tanh(w) + 1) − X. It serves to smooth out projected gradient descent that clips prematurely to prevent adversarial examples falling into the extreme image domain.
¯Z by chain
To compute the gradient practically, we ﬁrstly calculate ∂ rule. Here, the latter gradient equals to non-robust channel index inr, because the intermediate features of adversarial examples can be re-written as: ¯Z = ir · ¯Zr + inr · ¯Znr, and the derivative of ¯Z over ¯Znr equals to inr. Thus, the gradients of the non-robust features Gnr can be simpliﬁed as inr · ∂
∂ ¯Z Lbase. Using the gradient, we suggest an attack to non-robust features (NRF) by optimizing the following objective:
∂ ¯Z Lbase, and multiply it to
∂
∂ ¯Znr min
δ
Lbase(f (X + δ), Y ) − (cid:13) (cid:13) (cid:13) (cid:13) inr ·
∂
∂ ¯Z
Lbase (cid:13) (cid:13) (cid:13) (cid:13)2
. (8)
In Table 3, NRF shows more effective attack performance than the other standard adversarial attacks in [7, 43, 44], since NRF strengthens the gradients of non-robust features that contains the same effect of increasing σnr to disturb accurate model prediction.
Moreover, we conduct an ablation study on the gradients of robust and non-robust features to probe their inﬂuence of prediction changes in Table 4. As expected, maximizing Gnr shows more effective attack performance than minimizing it. It is because maximizing Gnr has an alternative effect of increasing the non-robust noise variation σnr (i.e., large erroneous term ∆ ↑) in Eq. (6). Whereas, manipulating the gradients of robust features Gr shows less attack performance than controlling Gnr, since it cannot directly handle brittle features in the intermediate feature representation. Especially, it seems difﬁcult to break model prediction, while weakening the gradient of robust features, whose noise variation σr have invariance of target prediction even under the adversarial perturbation. 5