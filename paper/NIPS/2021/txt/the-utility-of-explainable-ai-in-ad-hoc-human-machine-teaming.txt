Abstract
Recent advances in machine learning have led to growing interest in Explainable
AI (xAI) to enable humans to gain insight into the decision-making of machine learning models. Despite this recent interest, the utility of xAI techniques has not yet been characterized in human-machine teaming. Importantly, xAI offers the promise of enhancing team situational awareness (SA) and shared mental model development, which are the key characteristics of effective human-machine teams.
Rapidly developing such mental models is especially critical in ad hoc human-machine teaming, where agents do not have a priori knowledge of others’ decision-making strategies. In this paper, we present two novel human-subject experiments quantifying the beneﬁts of deploying xAI techniques within a human-machine teaming scenario. First, we show that xAI techniques can support SA (p < 0.05).
Second, we examine how different SA levels induced via a collaborative AI policy abstraction affect ad hoc human-machine teaming performance. Importantly, we
ﬁnd that the beneﬁts of xAI are not universal, as there is a strong dependence on the composition of the human-machine team. Novices beneﬁt from xAI providing increased SA (p < 0.05) but are susceptible to cognitive overhead (p < 0.05). On the other hand, expert performance degrades with the addition of xAI-based support (p < 0.05), indicating that the cost of paying attention to the xAI outweighs the beneﬁts obtained from being provided additional information to enhance SA. Our results demonstrate that researchers must deliberately design and deploy the right xAI techniques in the right scenario by carefully considering human-machine team composition and how the xAI method augments SA. 1

Introduction
Collaborative robots (i.e., "cobots") and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity, enhancing safety, and im-proving the quality of our lives [16, 22]. In the envisage of ubiquitous cobots, these agents will dynamically interact with a wide variety of people in dynamic and novel contexts. Ad hoc teaming characterizes this type of scenario, where multiple unacquainted agents (in this case, humans and cobots) with varying capabilities must collectively collaborate to accomplish a shared goal [33, 52].
Ad hoc teaming presents a signiﬁcant challenge in that agents are unaware of the capabilities and behaviors of other agents, and lack the opportunity to develop a team identity, shared mental models, and trust [9, 10, 57]. For example, the human may not be aware of the cobot’s possible actions
DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. This material is based upon work supported by the United States Air Force under Air Force Contract No. FA8702-15-D-0001.
Any opinions, ﬁndings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reﬂect the views of the United States Air Force. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: This ﬁgure displays an overview of our experimentation in relation to the Observe-Orient-Decide-Act (OODA) loop. On the left, we display the human-machine teaming interaction with both agents taking actions and the cobot outputting a policy explanation to the human teammate. On the right-hand side, we display the two questions assessed by our human-subjects experiments. and proclivities towards speciﬁc activities, limiting her ability to coordinate and plan effectively
[35, 48]. Furthermore, this lack of understanding negatively impacts the user’s ability to perform situational analysis, impeding a key component of the Observe-Orient-Decide-Act (OODA) loop
[7]. For a human teammate to maintain situational awareness (SA) and effectively make decisions in human-machine teaming, the human must maintain an internal model of the cobot’s behavior.
However, to develop such an understanding of the cobot, the human may have to constantly observe or monitor the cobot’s behavior, a costly and tedious process.
Effective collaboration in teaming arises from the ability for team members to coordinate their actions by understanding both the capabilities and decision-making criterion of their teammates [37, 47].
For human-human teams, Sebanz et al. [45] states that successful joint coordination among agents depends on the abilities to share representations, to predict other agents’ actions, and to integrate the effects of these action predictions. Similarly, we believe these ﬁndings should correlate in human-machine teaming. Explainable AI (xAI) techniques, utilizing abstractions or explanations that provide the user insight into the AI’s rationale, strengths and weaknesses, and expected behavior
[21], can supply the human teammate a representation of the cobot’s behavior policy and may assist in the human teammate’s ability to predict and develop a collaboration plan. Furthermore, effective human-machine teaming requires the ability for team members to develop a shared mental model (i.e., team members share common expectations about the team coordination strategy, the outcomes of individual strategies, and the individual roles in achieving the team’s objective [34])
[35]. xAI techniques offer the promise of enhancing team situational awareness, shared mental model development, and human-machine teaming performance.
Recent work in the machine learning community on xAI has emphasized the importance of in-terpretability, and post-hoc explainability in enhancing the prevalence of machine learning-based approaches [43]. Other work has even explored simulatability [40, 49], assessing a human’s ability to observe a model (e.g., a decision tree, which lends itself to interpretability [14]) and be able to produce the correct output given an input feature. Augmenting machine learning-based systems with some form of intepretability or simulatability can enable these systems to gain human trust [3, 25, 38], an essential quality in high-performance teaming [28]. While prior work has provided approaches for explaining machine behavior through natural language [42], interpretable decision trees [40], and attention-based focusing[55], the utility of collaborative agents augmented with explainable AI techniques in human-machine teaming has not been explored.
In this work, we present two novel human-subjects studies to quantify the utility of xAI in human-machine teaming. We assess the ability for human teammates to gain improved SA through the augmentation of xAI techniques and quantify the subjective and objective impact of xAI-supported
SA on human-machine team ﬂuency. We ﬁrst assess if xAI can support the different levels of SA
[15] by assessing how different abstractions of the AI’s policy support SA within a human-machine 2
teaming scenario. Second, we study the effect of augmenting cobots with online xAI and assess how different abstractions of the AI’s policy affect ad hoc human-machine teaming performance. In
Figure 1, we present an overview of our experimentation in relation to the Observe-Orient-Decide-Act (OODA) loop. We provide the following contributions: 1. We design and conduct a study relating different abstractions of the cobot’s policy to their induced situational awareness levels, measuring how different explanations can help a human perceive the current environment (Level 1), comprehend the AI’s decision-making model (Level 2), and project into the future to develop a collaboration plan (Level 3). Our results show that xAI techniques can support situational awareness (p < 0.05). 2. We design and conduct an ad hoc human-machine teaming study assessing how online xAI-based support, generated via cobot abstractions, and the human’s ability to process higher levels of information affect teaming performance. We ﬁnd novices beneﬁt from xAI-based support (p < 0.05) but are susceptible to information overload from more involved xAI abstractions (p < 0.05). Expert performance, on the other hand, degrades with the addition of xAI-based support (p < 0.05), indicating that the cost of paying attention to the explanation outweighs the beneﬁts obtained from generating an accurate mental model of the cobot’s behavior. 2