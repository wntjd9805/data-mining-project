Abstract
Modern minimax problems, such as generative adversarial network and adversarial training, are often under a nonconvex-nonconcave setting, and developing an efﬁcient method for such setting is of interest. Recently, two variants of the extragradient (EG) method are studied in that direction. First, a two-time-scale variant of the EG, named EG+, was proposed under a smooth structured nonconvex-nonconcave setting, with a slow O(1/k) rate on the squared gradient norm, where k denotes the number of iterations. Second, another variant of EG with an anchoring technique, named extra anchored gradient (EAG), was studied under a smooth convex-concave setting, yielding a fast O(1/k2) rate on the squared gradient norm. Built upon EG+ and EAG, this paper proposes a two-time-scale EG with anchoring, named fast extragradient (FEG), that has a fast O(1/k2) rate on the squared gradient norm for smooth structured nonconvex-nonconcave problems; the corresponding saddle-gradient operator satisﬁes the negative comonotonicity condition. This paper further develops its backtracking line-search version, named
FEG-A, for the case where the problem parameters are not available. The stochastic analysis of FEG is also provided. 1

Introduction
Recently, nonconvex-nonconcave minimax problems have received an increased attention in the optimization community and the machine learning community due to their applications to generative adversarial network [10] and adversarial training [27]. In this paper, we consider a smooth structured nonconvex-nonconcave minimax problem: min x∈Rdx max y∈Rdy f (x, y), (1) where f : Rdx × Rdy → R is smooth and is possibly nonconvex in x for ﬁxed y, and possibly nonconcave in y for ﬁxed x; the saddle-gradient operator F := (∇xf, −∇yf ) satisﬁes the negative comonotonicity [1]. We construct an efﬁcient (ﬁrst-order) method, using a saddle gradient operator
F for ﬁnding a ﬁrst-order stationary point of the problem (1).
So far little is known under the nonconvex-nonconcave setting, compared to the convex-concave setting. Recent works [4, 7, 22, 24, 26, 42, 44] studied extragradient-type methods [19, 39] for minimax problems under various structured nonconvex-nonconcave settings. In other words, they consider various non-monotone conditions on F , such as the Minty variational inequality (MVI) condition [4], the weak MVI condition [7], and the negative comonotonicity [1].1 Among them, this 35th Conference on Neural Information Processing Systems (NeurIPS 2021). 1Relations between the conditions on F considered in this paper is summarized in Figure 1.
paper focuses on the negative comonotonicity condition for a Lipschitz continuous F . To the best of our knowledge, the following two-time-scale variant of the extragradient method, named EG+: zk+1/2 = zk −
αk
β
F zk, zk+1 = zk − αkF zk+1/2, (EG+) is the only known (explicit)2 method, using F , that converges under the considered setting3 [7], where zk := (xk, yk). The EG+, however, has a slow O(1/k) rate on the squared gradient norm.
Note that a similar two-time-scale approach has been found to stabilize the stochastic extragradient method with unbounded noise variance [14].
Meanwhile, under the smooth convex-concave setting, recent works [6, 17, 21, 40, 43] suggest that
Halpern-type [12] (or anchoring) methods, performing a convex combination of an initial point z0 and the last updated point zk at each iteration, has a fast O(1/k2) rate in terms of the squared gradient
In particular, [43] developed the following anchoring variant of the extragradient method, norm. named extra anchored gradient (EAG): zk+1/2 = zk + βk(z0 − zk) − αkF zk, zk+1 = zk + βk(z0 − zk) − αkF zk+1/2. (EAG)
This is the ﬁrst (explicit) method with a fast O(1/k2) rate on the squared gradient norm, when F satisﬁes both the Lipschitz continuity and the monotonicity. [43] also showed that such O(1/k2) rate is optimal for ﬁrst-order methods using a Lipschitz continuous and monotone F .
Built upon both EG+ and EAG, this paper studies the following class of two-time-scale anchored extragradient methods, named fast extragradient (FEG): zk+1/2 = zk + βk(z0 − zk) − (1 − βk)(αk + 2ρk)F zk, zk+1 = zk + βk(z0 − zk) − αkF zk+1/2 − (1 − βk)2ρkF zk. (Class FEG)
Note that (Class FEG) reuses the F zk term in the zk+1 update, unlike the standard extragradient-type methods, which we found essential for handling the negative comonotonicity condition. We leave further understanding the use of F zk and the formulation of (Class FEG) as future work. The proposed FEG method (with appropriately chosen step coefﬁcients αk, βk and ρk discussed later) has an O(1/k2) rate on the squared gradient norm, under the Lipschitz continuity and the negative comonotonicity conditions on F . To the best of our knowledge, this is the ﬁrst accelerated method under the nonconvex-nonconcave setting. The FEG also has value under the smooth convex-concave setting. First, when F is Lipschitz continuous and monotone, the rate bound of FEG is about 27/4 times smaller than that of EAG. Also note that the rate bound of FEG is only about four times larger than the O(1/k2) lower complexity bound of ﬁrst-order methods under such setting [43], further closing the gap between the lower and upper complexity bounds. Second, when F is cocoercive,
FEG has a rate faster than that of a version of Halpern iteration [12] in [6].
We also develop an adaptive variant of FEG, named FEG-A, which updates its parameters, αk and ρk in (Class FEG), adaptively using a backtracking line-search [2, 25, 31]. FEG requires the knowledge of the two problem parameters for the Lipschitz continuity and the comonotonicity of F . However, those global parameters can be conservative, and in practice, they are even usually unknown. For such cases, the FEG-A adaptively and locally estimates the problem parameters, while preserving the fast rate O(1/k2) on the squared gradient norm for smooth structured nonconvex-nonconcave minimax problems.
Lastly, we study a stochastic version of FEG, named S-FEG, which uses an unbiased stochastic estimate of F z, i.e., ˜F z = F z + ξ, instead of F z in FEG, where ξ denotes a stochastic noise. For a
Lipschitz continuous and monotone F , we provide a convergence analysis in terms of the expected squared gradient norm. In speciﬁc, we show that the S-FEG is stable with a rate O(1/k2) + O((cid:15)), when the noise variance decreases in the order of O((cid:15)/k), while being unstable otherwise due to 2A proximal point method converges under the negative comonotonicity [1, 18], but such implicit method is not preferable over explicit methods in practice due to its implicit nature. 3The EG+ was originally shown to work under the weak MVI condition of F , which is weaker than the negative comonotonicity. 2
error accumulation. This is similar to the convergence behavior of a stochastic version of Nesterov’s fast gradient method [35, 36], observed in [5], for smooth convex minimization.
Our main contributions are summarized as follows.
• We propose the FEG method that has an accelerated convergence rate O(1/k2) on the squared gradient norm for smooth structured nonconvex-nonconcave minimax problems.
• We present that the FEG method has a rate faster than those of the EAG and the Halpern iteration for smooth convex-concave problems.
• We construct a backtracking line-search version of FEG, named FEG-A, for the case where the Lipschitz constant and comonotonicity parameters of F are unavailable.
• We analyze a stochastic version of FEG, named S-FEG, for smooth convex-concave prob-lems. 2