Abstract
The recent success of transformer models in language, such as BERT, has motivated the use of such architectures for multi-modal feature learning and tasks. However, most multi-modal variants (e.g., ViLBERT) have limited themselves to visual-linguistic data. Relatively few have explored its use in audio-visual modalities, and none, to our knowledge, illustrate them in the context of granular audio-visual detection or segmentation tasks such as sound source separation and localization.
In this work, we introduce TriBERT – a transformer-based architecture, inspired by
ViLBERT, which enables contextual feature learning across three modalities: vision, pose, and audio, with the use of ﬂexible co-attention. The use of pose keypoints is inspired by recent works that illustrate that such representations can signiﬁcantly boost performance in many audio-visual scenarios where often one or more persons are responsible for the sound explicitly (e.g., talking) or implicitly (e.g., sound produced as a function of human manipulating an object). From a technical perspective, as part of the TriBERT architecture, we introduce a learned visual tokenization scheme based on spatial attention and leverage weak-supervision to allow granular cross-modal interactions for visual and pose modalities. Further, we supplement learning with sound-source separation loss formulated across all three streams. We pre-train our model on the large MUSIC21 dataset and demonstrate improved performance in audio-visual sound source separation on that dataset as well as other datasets through ﬁne-tuning. In addition, we show that the learned
TriBERT representations are generic and signiﬁcantly improve performance on other audio-visual tasks such as cross-modal audio-visual-pose retrieval by as much as 66.7% in top-1 accuracy. 1

Introduction
Multi-modal audio-visual learning [57], which explores and leverages the relationship between visual and auditory modalities, has started to emerge as an important sub-ﬁeld of machine learning and computer vision. Examples of typical tasks include: audio-visual separation and localization, where the goal is to segment sounds produced by individual objects in an audio and/or to localize those objects in a visual scene [15, 16, 42, 55]; and audio-visual correspondence, where the goal is often audio-visual retrieval [23, 47, 53]. Notably, some of the most recent audio-visual methods [15] leverage human pose keypoints, or landmarks, as an intermediate or contextual representation. This tends to improve the overall performance of sound separation, as pose and motion are important cues for characterising both the type of instrument being played and, potentially, over time, the rhythm of the individual piece [15]. It can also serve as an intermediate representation when generating video from acoustic signals [8, 44] for example. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Most of the existing architectures tend to extract features from the necessary modalities using pre-trained backbones (e.g., CNNs applied to video frames [55], object regions [16], and audio spec-trograms; and/or graph CNN for human pose [15]) and then construct problem-speciﬁc architectures that often utilize simple late fusion for cross-modal integration in decoding (e.g., to produce spectro-gram masks [15, 16, 55]). This is contrary to current trends in other multi-modal problem domains, where over the past few years, approaches have largely consolidated around generic multi-modal feature learning architectures that are task agnostic to produce contextualized feature representations and then ﬁne-tune those representations to a variety of tasks (e.g., visual question answering (VQA) or reasoning (VCR)) and datasets. Examples of such architectures include ViLBERT [33], VL-BERT
[46], and Unicoder-VL [31], all designed speciﬁcally for visual-linguistic tasks.
Audio-visual representation learning has, in comparison, received much less attention. Most prior works [51] assume a single sound source per video and rely on audio-visual alignment objectives.
Exceptions include [39], which relies on proposal mechanisms and multiple-instance learning [49] or co-clustering [25]. These approaches tend to integrate multi-modal features extracted using pre-trained feature extractors (e.g., CNNs) at a somewhat shallow level. The very recent variants
[6, 28, 35] leverage transformers for audio-visual representation learning through simple classiﬁcation
[6] and self-supervised [28] or contrastive [35] learning objectives while only illustrating performance on video-level audio-visual action classiﬁcation. To the best of our knowledge, no audio-visual representation learning approach to date has explored pose as one of the constituent modalities; nor has shown that feature integration and contextualization at a hierarchy of levels, as is the case for
BERT-like architectures, can lead to improvements on granular audio-visual tasks such as audio-visual sound source separation.
To address the aforementioned limitations, we formulate a human-centric audio-visual representation learning architecture, inspired by ViLBERT [33] and other transformer-based designs, with an explicit goal of improving the state-of-the-art in audio-visual sound source separation. Our transformer model takes three streams of information: video, audio, and (pose) keypoints and co-attends among those three modalities to arrive at enriched representations that can then be used for the ﬁnal audio-visual sound separation task. We illustrate that these representations are general and also improve performance on other auxiliary tasks (e.g., forms of cross-modal audio-visual-pose retrieval). From a technical perspective, unlike ViLBERT and others, our model does not rely on global frame-wise features nor an external proposal mechanism. Instead, we leverage a learned attention to form visual tokens, akin to [42], and leverage weakly supervised objectives that avoid single sound-source assumptions for learning. In addition, we introduce spectrogram mask prediction as one of our pre-training tasks to enable the network to better learn task-speciﬁc contextualized features.
Contributions: Foremost, we introduce a tri-modal VilBERT-inspired model, which we call TriBERT, that co-attends among visual, pose keypoint, and audio modalities to produce highly contextualized representations. We show that these representations, obtained by optimizing the model with respect to uni-modal (weakly-supervised) classiﬁcation and sound separation pretraining objectives, produce features that improve audio-visual sound source separation at large and also work well on other downstream tasks. Further, to avoid reliance on the image proposal mechanisms, we formulate tokenization in the image stream in terms of learned attentional pooling, which is learned jointly. This alleviates the need for externally trained detection mechanisms, such as Faster R-CNN and variants.
We illustrate competitive performance on a number of granular audio-visual tasks both by using the
TriBERT model directly, using it as a feature extractor, or by ﬁne-tuning it. 2