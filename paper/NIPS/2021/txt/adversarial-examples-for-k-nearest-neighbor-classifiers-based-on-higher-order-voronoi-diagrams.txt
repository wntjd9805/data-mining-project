Abstract
Adversarial examples are a widely studied phenomenon in machine learning models.
While most of the attention has been focused on neural networks, other practical models also suffer from this issue. In this work, we propose an algorithm for evaluating the adversarial robustness of k-nearest neighbor classiﬁcation, i.e.,
ﬁnding a minimum-norm adversarial example. Diverging from previous proposals, we propose the ﬁrst geometric approach by performing a search that expands outwards from a given input point. On a high level, the search radius expands to the nearby higher-order Voronoi cells until we ﬁnd a cell that classiﬁes differently from the input point. To scale the algorithm to a large k, we introduce approximation steps that ﬁnd perturbation with smaller norm, compared to the baselines, in a variety of datasets. Furthermore, we analyze the structural properties of a dataset where our approach outperforms the competition. 1

Introduction
It is well-known that machine learning models can easily be misled by maliciously crafted inputs, called adversarial examples, which are generated by adding a tiny perturbation to test samples [Biggio et al., 2013, Szegedy et al., 2013, Goodfellow et al., 2015]. Adversarial examples are often studied in the context of neural networks, leaving the problem largely unexplored for other classiﬁers. The k-nearest neighbor, or simply k-NN, classiﬁer is a simple yet widely used model in various applications such as data mining, recommendation, and anomaly detection systems where interpretability and simplicity are preferred [Wu et al., 2008]. This non-parametric classiﬁer does not require a training phase and has a well-understood and elegant geometric foundation [Okabe et al., 1992]. k-NN is also an active area of research with lots of developments in popular libraries like Google’s ScaNN [Guo et al., 2020] and Facebook’s FAISS [Johnson et al., 2017]. In recent works [Papernot and McDaniel, 2018, Dubey et al., 2019, Sitawarin and Wagner, 2019b], k-NN is combined with neural networks to enhance the robustness and the interpretability. Despite the importance of k-NN, there are only a handful of works that study its robustness [Wang et al., 2018, Yang et al., 2020, Wang et al., 2019,
Sitawarin and Wagner, 2019a, 2020].
The ﬁrst step towards evaluating the robustness of a classiﬁer is to generate an adversarial example that is close to a given test point, under some deﬁnition of closeness. In the case of k-NN where k > 1, this problem is challenging as the computation involves high-dimensional polytopes that satisfy a set of geometric properties, the so-called Voronoi cells. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we propose the GeoAdEx algorithm for ﬁnding adversarial examples for k-NN classiﬁers.
GeoAdEx is the ﬁrst algorithm that exploits the underpinning geometry of the k-NN classiﬁer.
Speciﬁcally, our approach performs a principled search on the underlying high-order Voronoi diagram by expanding the search radius around the test point until it ﬁnds an adversarial, or simply incorrect, classiﬁcation. The geometric foundation of GeoAdEx allows us to locate nearby adversarial cells that the other attacks typically miss. GeoAdEx follows the footsteps of the work by Jordan et al. [2019] who exploited the geometry of neural networks for verifying the outputs. Our algorithmic approach stands in sharp contrast to previous attacks for k-NN [Yang et al., 2020, Wang et al., 2019] which reﬁne the exhaustive approach by heuristic-based ﬁltering.
Furthermore, we introduce optimizations and approximations to the main algorithm, and as a result, the experiments show that GeoAdEx discovers the smallest adversarial distance compared to all of the baselines in the vast majority of our experiments for k ∈ {3, 5, 7}. GeoAdEx ﬁnds up to 25% smaller mean adversarial distance compared to the second best attack. We note that one inherent shortcoming of our geometric approach is the increased computation time, an aspect that can be further improved.
Finally, we present experiments that demonstrate that GeoAdEx performs signiﬁcantly better when points from different classes are “mixed" together, i.e., there is no clear separation between classes.
On a high level, such a setup generates a more intricate spatial tessellation with nearby cells that alternate classes. In this challenging case, GeoAdEx performs up to 5× better than the baselines. 2