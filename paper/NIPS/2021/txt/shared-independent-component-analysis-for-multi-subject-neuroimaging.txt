Abstract
We consider shared response modeling, a multi-view learning problem where one wants to identify common components from multiple datasets or views. We introduce Shared Independent Component Analysis (ShICA) that models each view as a linear transform of shared independent components contaminated by additive Gaussian noise. We show that this model is identiﬁable if the components are either non-Gaussian or have enough diversity in noise variances. We then show that in some cases multi-set canonical correlation analysis can recover the correct unmixing matrices, but that even a small amount of sampling noise makes Multiset
CCA fail. To solve this problem, we propose to use joint diagonalization after
Multiset CCA, leading to a new approach called ShICA-J. We show via simulations that ShICA-J leads to improved results while being very fast to ﬁt. While ShICA-J is based on second-order statistics, we further propose to leverage non-Gaussianity of the components using a maximum-likelihood method, ShICA-ML, that is both more accurate and more costly. Further, ShICA comes with a principled method for shared components estimation. Finally, we provide empirical evidence on fMRI and MEG datasets that ShICA yields more accurate estimation of the components than alternatives. 1

Introduction
In many data science problems, data are available through different views. Generally, the views represent different measurement modalities such as audio and video, or the same text that may be available in different languages. Our main interest here is neuroimaging where recordings are made from multiple subjects. In particular, it is of interest to ﬁnd common patterns or responses that are shared between subjects when they receive the same stimulation or perform the same cognitive task
[16, 51].
A popular line of work to perform such shared response modeling is group Independent Component
Analysis (ICA) methods. The fastest methods [14, 58] are among the most popular, yet they are not grounded on principled probabilistic models for the multiview setting. More principled approaches exist [51, 27], but they do not model subject-speciﬁc deviations from the shared response. However, such deviations are expected in most neuroimaging settings, as the magnitude of the response may differ from subject to subject [46], as may any noise due to heartbeats, respiratory artefacts or head 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
movements [39]. Furthermore, most GroupICA methods are typically unable to separate components whose density is close to a Gaussian.
Independent vector analysis (IVA) [36, 5] is a powerful framework where components are independent within views but each component of a given view can depend on the corresponding component in other views. However, current implementations such as IVA-L [36], IVA-G [5], IVA-L-SOS [11],
IVA-GGD [7] or IVA with Kotz distribution [6] estimate only the view-speciﬁc components, and do not model or extract a shared response which is the main focus in this work.
On the other hand, the shared response model [16] is a popular approach to perform shared response modeling, yet it imposes orthogonality constrains that are restrictive and not biologically plausible.
In this work we introduce Shared ICA (ShICA), where each view is modeled as a linear transform of shared independent components contaminated by additive Gaussian noise. ShICA allows the principled extraction of the shared components (or responses) in addition to view-speciﬁc components.
Since it is based on a statistically sound noise model, it enables optimal inference (minimum mean square error, MMSE) of the shared responses.
Let us note that ShICA is no longer the method of choice when the concept of common response is either not useful or not applicable. Nevertheless, we believe that the ability to extract a common response is an important feature in most contexts because it highlights a stereotypical brain response to a stimulus. Moreover, ﬁnding commonality between subjects reduces often unwanted inter-subject variability.
The paper is organized as follows. We ﬁrst analyse the theoretical properties of the ShICA model, before providing inference algorithms. We exhibit necessary and sufﬁcient conditions for the ShICA model to be identiﬁable (previous work only shows local identiﬁability [7]), in the presence of
Gaussian or non-Gaussian components. We then use Multiset CCA to ﬁt the model when all the components are assumed to be Gaussian. We exhibit necessary and sufﬁcient conditions for Multiset
CCA to be able to recover the unmixing matrices (previous work only gives sufﬁcient conditions [38]).
In addition, we provide instances of the problem where Multiset CCA cannot recover the mixing matrices while the model is identiﬁable. We next point out a practical problem : even a small sampling noise can lead to large error in the estimation of unmixing matrices when Multiset CCA is used. To address this issue and recover the correct unmixing matrices, we propose to apply joint diagonalization to the result of Multiset CCA yielding a new method called ShICA-J. We further introduce ShICA-ML, a maximum likelihood estimator of ShICA that models non-Gaussian components using a Gaussian mixture model. While ShICA-ML yields more accurate components,
ShICA-J is signiﬁcantly faster and offers a great initialization to ShICA-ML. Experiments on fMRI and MEG data demonstrate that the method outperforms existing GroupICA and IVA methods. 2 Shared ICA (ShICA): an identiﬁable multi-view model
Notation We write vectors in bold letter v and scalars in lower case a. Upper case letters M are used to denote matrices. We denote |M | the absolute value of the determinant of M . x ∼ N (µ, Σ) means that x ∈ Rk follows a multivariate normal distribution of mean µ ∈ Rk and covariance
Σ ∈ Rk×k. The j, j entry of a diagonal matrix Σi is denoted Σij, the j entry of yi is denoted yij.
Lastly, δ is the Kronecker delta.
Model Deﬁnition In the following, x1, . . . , xm ∈ Rp denote the m observed random vectors obtained from the m different views. We posit the following generative model, called Shared ICA (ShICA): for i = 1 . . . m xi = Ai(s + ni) (1) where s ∈ Rp contains the latent variables called shared components, A1, . . . , Am ∈ Rp×p are the invertible mixing matrices, and ni ∈ Rp are individual noises. The individual noises model both the deviations of a view from the mean —i.e. individual differences— and measurement noise.
Importantly, we explicitly model both the shared components and the individual differences in a probabilistic framework to enable an optimal inference of the parameters and the responses.
We assume that the shared components are statistically independent, and that the individual noises are Gaussian and independent from the shared components: p(s) = (cid:81)p j=1 p(sj) and ni ∼ N (0, Σi), where the matrices Σi are assumed diagonal and positive. Without loss of generality, components 2
are assumed to have unit variance E[ss(cid:62)] = Ip. We further assume that there are at least 3 views: m ≥ 3.
In contrast to almost all existing works, we assume that some components (possibly all of them) may be Gaussian, and denote G the set of Gaussian components: sj ∼ N (0, 1) for j ∈ G. The other components are non-Gaussian: for j /∈ G, sj is non-Gaussian.
Identiﬁability The parameters of the model are Θ = (A1, . . . , Am, Σ1, . . . , Σm). We are inter-ested in the identiﬁability of this model: given observations x1, . . . , xm generated with parameters
Θ, are there some other Θ(cid:48) that may generate the same observations? Let us consider the following assumption that requires that the individual noises for Gaussian components are sufﬁciently diverse:
Assumption 1 (Noise diversity in Gaussian components). For all j, j(cid:48) ∈ G, j (cid:54)= j(cid:48), the sequences (Σij)i=1...m and (Σij(cid:48))i=1...m are different where Σij is the j, j entry of Σi
It is readily seen that there is one trivial set of indeterminacies in the problem: if P ∈ Rp×p is a sign and permutation matrix (i.e. a matrix which has one ±1 coefﬁcient on each row and column, and 0’s elsewhere) the parameters (A1P, . . . , AmP, P (cid:62)Σ1P, . . . , P (cid:62)ΣmP ) also generate x1, . . . , xm. The following theorem shows that under the above assumption, these are the only indeterminacies of the problem.
Theorem 1 (Identiﬁability). We make Assumption 1. We let Θ(cid:48) = (A(cid:48) m) another set of parameters, and assume that they also generate x1, . . . , xm. Then, there exists a sign and permutation matrix P such that for all i, A(cid:48) 1, . . . , A(cid:48) 1, . . . , Σ(cid:48) m, Σ(cid:48) i = AiP , and Σ(cid:48) i = P (cid:62)ΣiP .
The proof is in Appendix A.1. Identiﬁability in the Gaussian case is a consequence of the identiﬁability results in [59] and in the general case, local identiﬁability results can be derived from the work of [7].
However local identiﬁability only shows that for a given set of parameters there exists a neighborhood in which no other set of parameters can generate the same observations [52]. In contrast, the proof of
Theorem 1 shows global identiﬁability.
Theorem 1 shows that the task of recovering the parameters from the observations is a well-posed problem, under the sufﬁcient condition of Assumption 1. We also note that Assumption 1 is necessary for identiﬁability. For instance, if j and j(cid:48) are two Gaussian components such that Σij = Σij(cid:48) for all i, then a global rotation of the components j, j(cid:48) yields the same covariance matrices. The current work assumes m ≥ 3, in appendix B we give an identiﬁability result for m = 2, under stronger conditions. 3 Estimation of components with noise diversity via joint-diagonalization
We now consider the computational problem of efﬁcient parameter inference. This section considers components with noise diversity, while the next section deals with non-Gaussian components. 3.1 Parameter estimation with Multiset CCA j ] = Ai(Ip + δijΣi)A(cid:62) j
If we assume that the components are all Gaussian, the covariance of the observations given by
Cij = E[xix(cid:62) are sufﬁcient statistics and methods using only second order information, like Multiset CCA, are candidates to estimate the parameters of the model. Consider the matrix C ∈ Rpm×pm containing m × m blocks of size p × p such that the block i, j is given by Cij.
Consider the matrix D identical to C excepts that the non-diagonal blocks are ﬁlled with zeros:




C =


C11
...
Cm1
. . . C1m
...
. . .
. . . Cmm

 , D =


C11
... 0 0
...
. . .
. . .
. . . Cmm

 .
Generalized CCA consists of the following generalized eigenvalue problem:
Cu = λDu, λ > 0, u ∈ Rpm . (2) (3)
Consider the matrix U = [u1, . . . , up] ∈ Rmp×p formed by concatenating the p leading eigenvectors of the previous problem ranked in decreasing eigenvalue order. Then, consider U to be formed of m 3
blocks of size p × p stacked vertically and deﬁne (W i)(cid:62) to be the i-th block. These m matrices are the output of Multiset CCA. We also denote λ1 ≥ · · · ≥ λp the p leading eigenvalues of the problem.
An application of the results of [38] shows that Multiset CCA recovers the mixing matrices of ShICA under some assumptions.
Proposition 1 (Sufﬁcient condition for solving ShICA via Multiset CCA [38]). Let rijk = (1 +
Σik)− 1 2 . Assume that (rijk)k is non-increasing. Assume that the maximum eigenvalue
νk of matrix R(k) of general element (rijk)ij is such that νk = λk . Assume that λ1 . . . λp are distinct.
Then, there exists scale matrices Γi such that Wi = ΓiA−1 2 (1 + Σjk)− 1 for all i. i
This proposition gives a sufﬁcient condition for solving ShICA with Multiset CCA. It needs a particular structure for the noise covariances as well as speciﬁc ordering for the eigenvalues. The next theorem shows that we only need λ1 . . . λp to be distinct for Multiset CCA to solve ShICA:
Assumption 2 (Unique eigenvalues). λ1 . . . λp are distinct.
Theorem 2. We only make Assumption 2. Then, there exists a permutation matrix P and scale matrices Γi such that Wi = P ΓiA−1 for all i. i
The proof is in Appendix A.2. This theorem means that solving the generalized eigenvalue problem (3) allows to recover the mixing matrices up to a scaling and permutation: this form of generalized CCA recovers the parameters of the statistical model. Note that Assumption 2 is also a necessary condition.
Indeed, if two eigenvalues are identical, the eigenvalue problem is not uniquely determined.
We have two different Assumptions, 1 and 2, the ﬁrst of which guarantees theoretical identiﬁability as per Theorem 1 and the second guarantees consistent estimation by Multiset CCA as per Theorem 2.
Next we will discuss their connections, and show some limitations of the Multiset CCA approach. To begin with, we have the following result about the eigenvalues of the problem (3) and the Σij.
Proposition 2. For j ≤ p, let λj the largest solution of (cid:80)m are the p largest eigenvalues of problem (3).
= 1. Then, λ1, . . . , λp 1
λj (1+Σij )−Σij i=1
It is easy to see that we then have λ1, . . . , λp greater than 1, while the remaining eigenvalues are lower than 1. From this proposition, two things appear clearly. First, Assumption 2 implies Assumption 1.
Indeed, if the λj’s are distinct, then the sequences (Σij)i must also be different from the previous proposition. This is expected as from Theorem 2, Assumption 2 implies identiﬁability, which in turn implies Assumption 1.
Prop. 2 also allows us to derive cases where Assumption 1 holds but not Assumption 2. The following
Proposition gives a simple case where the model is identiﬁable but it cannot be solved using Multiset
CCA:
Proposition 3. Assume that for two integers j, j(cid:48), the sequence (Σij)i is a permutation of (Σij(cid:48))i, i.e. that there exists a permutation of {1, . . . , p}, π, such that for all i, Σij = Σπ(i)j(cid:48). Then, λj = λj(cid:48).
In this setting, Assumption 1 holds so ShICA is identiﬁable, while Assumption 2 does not hold, so
Multiset CCA cannot recover the unmixing matrices. 3.2 Sampling noise and improved estimation with joint diagonalization
The consistency theory for Multiset CCA developed above is conducted under the assumption that the covariances Cij are the true covariances of the model, and not approximations obtained from observed samples. In practice, however, a serious limitation of Multiset CCA is that even a slight error of estimation on the covariances, due to “sampling noise”, can yield a large error in the estimation of the unmixing matrices, as will be shown next.
We begin with an empirical illustration. We take m = 3, p = 2, and Σi such that λ1 = 2+ε and λ2 = 2 for ε > 0. In this way, we can control the eigen-gap of the problem, ε. We take Wi the outputs of
Multiset CCA applied to the true covariances Cij. Then, we generate a perturbation ∆ = δ · S, where
S is a random positive symmetric pm×pm matrix of norm 1, and δ > 0 controls the scale of the pertur-bation. We take ∆ij the p×p block of ∆ in position (i, j), and ˜Wi the output of Multiset CCA applied to the covariances Cij + ∆ij. We ﬁnally compute the sum of the Amari distance between the Wi and
˜Wi: the Amari distance measures how close the two matrices are, up to scale and permutation [4]. 4
Fig 1 displays the median Amari distance over 100 random repetitions, as the perturbation scale δ increases. The different curves correspond to different values of the eigen-gap ε. We see clearly that the robustness of Multiset CCA critically depends on the eigen-gap, and when it is small, even a small perturbation of the input (due, for instance, to sampling noise) leads to large estimation errors.
This problem is very general and well studied [53]: the mapping from matrices to (generalized) eigenvectors is highly non-smooth. However, the gist of our method is that the span of the leading p eigenvectors is smooth, as long as there is a large enough gap between λp and λp+1.
For our speciﬁc problem we have the following bounds, derived from Prop. 2.
Proposition 4. We let σmax = maxij Σij and σmin = minij Σij. Then, λp ≥ 1 + m−1
, while λp+1 ≤ 1 − 1+σmax 1 1+σmin
.
Figure 1: Amari distance between true mixing matrices and estimates of Multi-set CCA when covariances are perturbed.
Different solid curves correspond to dif-ferent eigen-gaps. The black dotted line shows the chance level. When the gap is small, a small perturbation can lead to complete mixing. Joint-diagonalization (colored dotted lines) ﬁxes the problem.
As a consequence, we have λp − λp+1 ≥ m−1 1+σmax
+
: the gap between these eigenvalues 1 1+σmin increases with m, and decreases with the noise power. m 1+σmax
≥
In this setting, when the magnitude of the perturbation ∆ is smaller than λp − λp+1, [53] indicates that Span([W1, . . . , Wm](cid:62)) (cid:39) Span([ ˜W1, . . . , ˜Wm](cid:62)), where [W1, . . . , Wm](cid:62) ∈ Rpm×p is the vertical concatenation of the Wi’s. In turn, this shows that there exists a matrix Q ∈ Rp×p such that
Wi (cid:39) Q ˜Wi for all i. (4)
We propose to use joint-diagonalization to recover the matrix Q. Given the ˜Wi’s, we con-i , where ˜Cii is the contaminated covari-sider the set of symmetric matrices ˜Ki = ˜Wi ˜Cii ˜W (cid:62) ance of xi. Following Eq. (4), we have Q ˜KiQ(cid:62) = Wi ˜CiiW (cid:62) i , and using Theorem 2, we have i ΓiP (cid:62). Since ˜Cii is close to Cii = Ai(Ip + Σi)A(cid:62)
Q ˜KiQ(cid:62) = P ΓiA−1 i , the matrix
P ΓiA−1 i ΓiP (cid:62) is almost diagonal. In other words, the matrix Q is an approximate diago-nalizer of the ˜Ki’s, and we approximate Q by joint-diagonalization of the ˜Ki’s. In Fig 1, we see that this procedure mitigates the problems of multiset-CCA, and gets uniformly better performance regardless of the eigen-gap. In practice, we use a fast joint-diagonalization algorithm [1] to minimize a joint-diagonalization criterion for positive symmetric matrices [48]. The estimated unmixing matrices Ui = Q ˜Wi correspond to the true unmixing matrices only up to some scaling which may be different from subject to subject: the information that the components are of unit variance is lost. As a consequence, naive averaging of the recovered components may lead to inconsistant estimation. We now describe a procedure to recover the correct scale of the individual components across subjects.
˜CiiA−(cid:62)
˜CiiA−(cid:62) i i
Algorithm 1 ShICA-J
Input : Covariances ˜Cij = E[xix(cid:62) j ] ( ˜Wi)i ← MultisetCCA(( ˜Cij)ij)
Q ← JointDiag(( ˜Wi ˜Cii ˜W (cid:62) i )i)
Γij ← Q ˜Wi ˜CijW (cid:62) (Φi)i ← Scaling((Γij)ij)
Return : Unmixing matrices (ΦiQ ˜Wi)i. j Q(cid:62)
Scale estimation We form the matrices Γij =
Ui ˜CijU (cid:62) j . In order to estimate the scalings, we solve (cid:80) i(cid:54)=j (cid:107)Φidiag(Γij)Φj −Ip(cid:107)2
F where the Φi min(Φi) are diagonal matrices. This function is readily min-imized with respect to one of the Φi by the formula
Φi = j diag(Yij )2 (derivations in Appendix 20).
We then iterate the previous formula over i until con-vergence. The ﬁnal estimates of the unmixing ma-trices are given by (ΦiUi)m i=1. The full procedure, called ShICA-J, is summarized in Algorithm 1. j(cid:54)=i Φj diag(Yij ) j(cid:54)=i Φ2 (cid:80) (cid:80) 3.3 Estimation of noise covariances
In practice, it is important to estimate noise covari-ances Σi in order to take advantage of the fact that 5
some views are noisier than others. As it is well known in classical factor analysis, modelling noise variances allows the model to virtually discard variables, or subjects, that are particularly noisy.
Using the ShICA model with Gaussian components, we derive an estimate for the noise covariances directly from maximum likelihood. We use an expectation-maximization (EM) algorithm, which is especially fast because noise updates are in closed-form. Following derivations given in appendix D.1, the sufﬁcient statistics in the E-step are given by
E[s|x] = (cid:32) m (cid:88) i=1
Σ−1 i + I (cid:33)−1 m (cid:88) i=1 (cid:0)Σ−1 i yi (cid:1)
V[s|x] = ( m (cid:88) i=1
Σ−1 i + I)−1 (5)
Incorporating the M-step we get the following updates that only depend on the covariance matrices:
Σi ← diag( ˆCii − 2V[s|x] (cid:80)m
ˆCji + V[s|x] (cid:80)m
V[s|x] + V[s|x]) (cid:80)m (cid:16) (cid:17)
Σ−1 j
ˆCjlΣ−1 l j=1 Σ−1 j j=1 l=1 4 ShICA-ML: Maximum likelihood for non-Gaussian components
ShICA-J only uses second order statistics. However, the ShICA model (1) allows for non-Gaussian components. We now propose an algorithm for ﬁtting the ShICA model that combines covariance information with non-Gaussianity in the estimation to optimally separate both Gaussian and non-Gaussian components. We estimate the parameters by maximum likelihood. Since most non-Gaussian components in real data are super-Gaussian [21, 13], we assume that the non-Gaussian components s have the super-Gaussian density (cid:0)N (sj; 0, 1 p(sj) = 1 2
We propose to maximize the log-likelihood using a generalized EM [41, 22]. Derivations are available in Appendix E. Like in the previous section, the E-step is in closed-form yielding the following sufﬁcient statistics: 2 ) + N (sj; 0, 3 2 )(cid:1) .
E[sj|x] = (cid:80)
α∈{ 1 (cid:80) 2 , 3
α¯yj 2 } θα
α+ ¯Σj
α∈{0.5,1.5} θα and V[sj|x] = (cid:80)
α∈{ 1 (cid:80) 2 , 3
¯Σj α 2 } θα
α+ ¯Σj
α∈{0.5,1.5} θα (6) (cid:80) i Σ−1 ij yij (cid:80) i Σ−1 ij i Σ−1 and ¯Σj = ((cid:80) (cid:92)
HWi a,b,c,d = δadδbc + δacδbd where θα = N (¯yj; 0, ¯Σj + α), ¯yj = ij )−1 with yi = Wixi. Noise updates are in closed-form and given by: Σi ← diag((yi−E[s|x])(yi−E[s|x])(cid:62)+V[s|x]). However, no closed-form is available for the updates of unmixing matrices. We therefore perform quasi-Newton updates given by Wi ← (I − ρ( (cid:100)HWi)−1GWi)Wi where ρ ∈ R is chosen by backtracking line-(yib)2 search, is an approximation of the Hessian of the negative complete
Σia likelihood and GWi = −I + (Σi)−1(yi − E[s|x])(yi)(cid:62) is the gradient.
We alternate between computing the statistics E[s|x], V[s|x] (E-step) and updates of parameters Σi and Wi for i = 1 . . . m (M-step). Let us highlight that our EM algorithm and in particular the E-step resembles the one used in [40]. However because they assume noise on the sensors and not on the components, their formula for E[s|x] involves a sum with 2p terms whereas we have only 2 terms.
The resulting method is called ShICA-ML.
Minimum mean square error estimates in ShICA In ShICA-J as well as in ShICA-ML, we have a closed-form for the expected components given the data E[s|x], shown in equation (5) and (6) respectively. This provides minimum mean square error estimates of the shared components, and is an important beneﬁt of explicitly modelling shared components in a probabilistic framework. 5