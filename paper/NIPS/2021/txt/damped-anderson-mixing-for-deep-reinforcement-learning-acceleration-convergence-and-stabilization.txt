Abstract
Anderson mixing has been heuristically applied to reinforcement learning (RL) algorithms for accelerating convergence and improving the sampling efﬁciency of deep RL. Despite its heuristic improvement of convergence, a rigorous mathe-matical justiﬁcation for the beneﬁts of Anderson mixing in RL has not yet been put forward. In this paper, we provide deeper insights into a class of acceleration schemes built on Anderson mixing that improve the convergence of deep RL al-gorithms. Our main results establish a connection between Anderson mixing and quasi-Newton methods and prove that Anderson mixing increases the convergence radius of policy iteration schemes by an extra contraction factor. The key focus of the analysis roots in the ﬁxed-point iteration nature of RL. We further propose a stabilization strategy by introducing a stable regularization term in Anderson mix-ing and a differentiable, non-expansive MellowMax operator that can allow both faster convergence and more stable behavior. Extensive experiments demonstrate that our proposed method enhances the convergence, stability, and performance of
RL algorithms. 1

Introduction
In reinforcement learning (RL) [28], an agent seeks an optimal policy in a sequential decision-making process. Deep RL has recently achieved signiﬁcant improvements in a variety of challenging tasks, including game playing [20, 26, 18] and robust navigation [19]. A ﬂurry of state-of-the-art algorithms have been proposed, including Deep Q-Learning (DQN) [20] and variants such as
Double-DQN [13], Dueling-DQN [31], Deep Deterministic Policy Gradient (DDPG) [16], Soft
Actor-Critic [12] and distributional RL algorithms [5, 17, 33], all of which have successfully solved end-to-end decision-making problems such as playing Atari games. However, the slow convergence and sample inefﬁciency of RL algorithms still hinders the progress of RL research, particularly in high-dimensional state spaces where deep neural network are used as function approximators, making learning in real physical worlds impractical.
To address these issues, various acceleration strategies have been proposed, including the classical
Gauss-Seidel Value Iteration [23] and Jacobi Value Iteration [24]. Another popular branch of tech-niques accelerates RL by leveraging historical data. Interpolation methods such as Average-DQN [2]
∗Equal contributions in alphabetical order
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
have been widely used in ﬁrst-order optimization problems [6] and have been proven to converge faster than vanilla gradient methods. As an effective multi-step interpolation method, Anderson mix-ing [30, 9], also known as Anderson acceleration, has attracted great attention from RL researchers.
The insight underpinning of Anderson acceleration is that RL [28] is intrinsically linked to ﬁxed-point iterations: the optimal value function is the ﬁxed point of the Bellman optimality operator. These
ﬁxed-points are computed recursively by repeatedly applying an operator of interest [11]. Anderson mixing is a general method to accelerate ﬁxed-point iterations [30] and has been successfully applied to ﬁelds, such as the computational chemistry [22] or electronic structure computation [1]. In particu-lar, Anderson acceleration leverages the m previous estimates in order to ﬁnd a better estimate in a
ﬁxed-point iteration. To compute the mixing coefﬁcients in Anderson iteration, it searches for a point with a minimal residual within the subspace spanned by these estimates. It is thus natural to explore the efﬁcacy of Anderson acceleration in RL settings.
Several works [15, 25] have attempted to apply Anderson acceleration to reinforcement learning.
Anderson mixing was ﬁrst applied to value iteration in [11, 15] and resulted in signiﬁcant conver-gence improvements. Regularized Anderson acceleration [25] was recently proposed to further accelerate convergence and enhance the ﬁnal performance of state-of-the-art RL algorithms in various experiments. However, previous applications of Anderson acceleration were typically heuristic: con-sequently, these empirical improvements in convergence have so far lacked a rigorous mathematical justiﬁcation.
In this paper, we provide deeper insights into Anderson acceleration in reinforcement learning by establishing its connection with quasi-Newton methods for policy iteration and improved conver-gence guarantees under the assumptions that the Bellman operator is differential and non-expansive.
MellowMax operator is adopted to replace the max operator in policy iteration to simultaneously guarantee faster convergence of value function and reduce the estimated gradient variance to yield sta-bilization. In addition, we analyze the stability properties of Anderson acceleration in policy iteration and propose a stable regularization to further enhance the stability. These key two factors, i.e., the stable regularization and the theoretically-inspired MellowMax operator, are the basis for our Stable
Anderson Acceleration (Stable AA) method. Finally, our experimental results on various Atari games demonstrate that our Stable AA method enjoys faster convergence and achieves better performance relative to existing Anderson acceleration baselines. Our work provides a uniﬁed analytic framework that illuminates Anderson acceleration for reinforcement learning algorithms from the perspectives of acceleration, convergence, and stabilization. 2 Acceleration and Convergence Analysis of Anderson Acceleration on RL
We ﬁrst present the notion of Anderson acceleration in the reinforcement learning and then provide deeper insights into the acceleration if affords by establishing a connection with quasi-Newton methods. Finally, a theoretical convergence analysis is provided to demonstrate that Anderson acceleration can increase the convergence radius of policy iteration by an extra contraction factor.