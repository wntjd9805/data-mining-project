Abstract
When solving two-player zero-sum games, multi-agent reinforcement learning (MARL) algorithms often create populations of agents where, at each iteration, a new agent is discovered as the best response to a mixture over the opponent population. Within such a process, the update rules of "who to compete with" (i.e., the opponent mixture) and "how to beat them" (i.e., ﬁnding best responses) are underpinned by manually developed game theoretical principles such as ﬁctitious play and Double Oracle. In this paper1, we introduce a novel framework—Neural
Auto-Curricula (NAC)—that leverages meta-gradient descent to automate the discovery of the learning update rule without explicit human design. Speciﬁcally, we parameterise the opponent selection module by neural networks and the best-response module by optimisation subroutines, and update their parameters solely via interaction with the game engine, where both players aim to minimise their exploitability. Surprisingly, even without human design, the discovered MARL algorithms achieve competitive or even better performance with the state-of-the-art population-based game solvers (e.g., PSRO) on Games of Skill, differentiable
Lotto, non-transitive Mixture Games, Iterated Matching Pennies, and Kuhn Poker.
Additionally, we show that NAC is able to generalise from small games to large games, for example training on Kuhn Poker and outperforming PSRO on Leduc
Poker. Our work inspires a promising future direction to discover general MARL algorithms solely from data. 1

Introduction
Two-player zero-sum games have been a central interest of the recent development of multi-agent reinforcement learning (MARL) [60, 2, 53]. In solving such games, a MARL agent has a clear objective: to minimise the worst case performance, its exploitability, against any potential oppo-nents. When both agents achieve zero exploitability, they reach a Nash equilibrium (NE) [36], a classical solution concept from Game Theory [34, 10]. Even though this objective is straightforward, developing effective algorithms to optimise such an objective often requires tremendous human efforts. One effective approach is through iterative methods, where players iteratively expand a population of agents (a.k.a auto-curricula [26]) where at each iteration, a new agent is trained and added to the player’s strategy pool. However, within the auto-curricula process, it is often non-trivial to design effective update rules of "who to compete with" (i.e., opponent selections) and "how to beat them" (i.e., ﬁnding best responses). The problem becomes more challenging when one considers additional requirements such as generating agents that have behavioural diversity [58, 37, 3, 28] or generalisation ability [47, 38]. 1
⇤Equal contributions. †Corresponding to <yaodong.yang@pku.edu.cn>. Code released at https:// github.com/waterhorse1/NAC 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
One effective approach to designing auto-curricula is to follow game theoretical principles such as
ﬁctitious play [5] and Double Oracle (DO) methods [33, 11]. For example, in the DO dynamics, each player starts from playing a sub-game of the original game where only a restricted set of strategies are available; then, at each iteration, each player will add a best-response strategy, by training against a NE mixture of opponent models at the current sub-game, into its strategy pool. When an exact best response is hard to compute, approximation methods such as reinforcement learning (RL) methods are often applied [25]. One of the potential downsides of this approach is, under approximate best response methods, one no longer maintains the theoretical properties of DO and a worthwhile avenue of exploration is to ﬁnd auto-curricula that are more conducive to these approximation solutions.
Apart from following game theoretical principles, an appealing alternative approach one can consider is to automatically discover auto-curricula from data generated by playing games, which can be formulated as a meta-learning problem [13]. However, it remains an open challenge whether it is feasible to discover fundamental concepts (e.g., NE) entirely based on data. If we can show that it is possible to discover fundamental concepts from scratch, it opens the avenue to trying to discover fundamentally new concepts at a potentially rapid pace. Although encouraging results have been reported in single-agent RL settings showing that it is possible to meta-learn RL update rules, such as temporal difference learning [38, 56, 57], and the learned update rules can generalise to unseen tasks, we believe discovering auto-curricula in multi-agent cases is particularly hard. Two reasons for this are that the discovered auto-curriculum itself directly affects the development of the agent population, and each agent involves an entire training process, which complicates the meta-learning process.
Albeit challenging, we believe a method capable of discovering auto-curricula without explicit game theoretic knowledge can potentially open up entirely new approaches to MARL. As a result, this paper initiates the study on discovering general-purpose MARL algorithms in two-player zero-sum games.
Speciﬁcally, our goal is to develop an algorithm that learns its own objective (i.e., the auto-curricula) solely from environment interaction, and we offer a meta-learning framework that achieves such a goal. Our solution framework – Neural Auto-Curriculum (NAC) – has two promising properties.
Firstly, it does not rely on human-designed knowledge about game theoretic principles, but instead lets the meta-learner decide what the meta-solution concept (i.e., who to compete with) should be during training. This property means that our meta-learner shares the ability of game theoretic principles in being able to accurately evaluate the policies in the population. Secondly, by taking the best-response computation into consideration, NAC can end to end offer more suitable auto-curricula within the approximate best-response scenario compared with previous approaches; this is particularly important since an exact best-response oracle is not always available in practice.
Our empirical results show that NAC can discover meaningful solution concepts alike NE, and based on that build effective auto-curricula in training agent populations. In multiple different environments, the discovered auto-curriculum achieves the same performance or better than that of PSRO methods
[25, 3]. We additionally evaluate the ability of our discovered meta-solvers to generalise to unseen games of a similar type (e.g., training on Kuhn Poker and testing on Leduc Poker), and show that the auto-curricula found on a simple environment is able to generalise to a more difﬁcult one. To the best of our knowledge, this is the ﬁrst work that demonstrates the possibility of discovering an entire auto-curriculum in solving two-player zero-sum games, and that the rule discovered from simple domains can be competitive with human-designed algorithms on challenging benchmarks. 2