Abstract
We study the problem of zeroth-order (black-box) optimization of a Lipschitz function f deﬁned on a compact subset X of Rd, with the additional constraint that algorithms must certify the accuracy of their recommendations. We characterize the optimal number of evaluations of any Lipschitz function f to ﬁnd and certify an approximate maximizer of f at accuracy ε. Under a weak assumption on X , this optimal sample complexity is shown to be nearly proportional to the integral (cid:82)
X dx/(max(f ) − f (x) + ε)d. This result, which was only (and partially) known in dimension d = 1, solves an open problem dating back to 1991. In terms of techniques, our upper bound relies on a packing bound by Bouttier et al. (2020) for the Piyavskii-Shubert algorithm that we link to the above integral. We also show that a certiﬁed version of the computationally tractable DOO algorithm matches these packing and integral bounds. Our instance-dependent lower bound differs from traditional worst-case lower bounds in the Lipschitz setting and relies on a local worst-case analysis that could likely prove useful for other learning tasks. 1

Introduction
The problem of optimizing a black-box function f with as few evaluations of f as possible arises in many scientiﬁc and industrial ﬁelds such as computer experiments (Jones et al., 1998; Richet et al., 2013) or automatic selection of hyperparameters in machine learning (Bergstra et al., 2011). For safety-critical applications, e.g., in aircraft or nuclear engineering, using sample-efﬁcient methods is not enough. Certifying the accuracy of the output of the optimization method can be a crucial additional requirement (Vanaret et al., 2013). As a concrete example, Azzimonti et al. (2021) describe a black-box function in nuclear engineering whose output is a k-effective multiplication factor, for which a higher value corresponds to a higher nuclear hazard. Certifying the optimization error is a way to certify the worst-case k-effective factor, which may be required by safety authorities.
In this paper, we formally study the problem of ﬁnding and certifying an ε-approximate maximizer of a Lipschitz function f of d variables and characterize the optimal number of evaluations of any such function f to achieve this goal. We start by formally deﬁning the setting. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1 Setting: Zeroth-order Lipschitz Optimization with Error Certiﬁcates
Let f : X → R be a function on a compact non-empty subset X of Rd and x(cid:63) ∈ X a maximizer. (cid:12)f (x) − f (y)(cid:12)
Lipschitz assumption. We assume that f is Lipschitz with respect to a norm (cid:107)·(cid:107), that is, there exists L ≥ 0 such that (cid:12) (cid:12) ≤ L (cid:107)x − y(cid:107) for all x, y ∈ X . Furthermore, we assume such a Lipschitz bound L to be known. Even though the smallest Lipschitz constant Lip(f ) := min{L(cid:48) ≥ 0 : f is L(cid:48)-Lipschitz} is well deﬁned mathematically, it is rarely known exactly in practical black-box problems. As a theoretical curiosity, we will brieﬂy discuss the case L = Lip(f ) (i.e., when the best
Lipschitz constant of the unknown black-box function f is known exactly) in Section 4, but for most of our results, we will make the following more realistic assumption.
Assumption 1. For some known Lipschitz constant L, the function f : X → R belongs to
FL := (cid:8)g : X → R | g is Lipschitz and Lip(g) < L(cid:9) . (1)
The Lipschitzness of f implies the weaker property that f (x(cid:63)) − f (x) ≤ L (cid:107)x(cid:63) − x(cid:107) for all x ∈ X , sometimes referred to as Lipschitzness around a maximizer x(cid:63) ∈ X . Although this is not the focus of our work, we will mention when our results hold under this weaker assumption.
Online learning protocol. We study the case in which f is black-box, i.e., except for the a priori knowledge of L, we can only access f by sequentially querying its values at a sequence x1, x2, . . . ∈ X of points of our choice. At every round n ≥ 1, the query point xn can be chosen as a deterministic function of the values f (x1), . . . , f (xn−1) observed so far. At the end of round n, using all the values f (x1), . . . , f (xn), the learner outputs two quantities:
• a recommendation x(cid:63) regret): max(f ) − f (x(cid:63) n); n ∈ X , with the goal of minimizing the optimization error (a.k.a. simple
• an error certiﬁcate ξn ≥ 0, with the constraint to correctly upper bound the optimization error for any L-Lipschitz function f : X → R, i.e., so that max(f ) − f (x(cid:63) n) ≤ ξn.
We call certiﬁed algorithm any algorithm for choosing such a sequence (xn, x(cid:63)
Our goal is to quantify the smallest number of evaluations of f that certiﬁed algorithms need in order to ﬁnd and certify an approximate maximizer of f at accuracy ε. This objective motivates the following deﬁnition. For any accuracy ε > 0, we deﬁne the sample complexity (that could also be called query complexity) of a certiﬁed algorithm A for an L-Lipschitz function f as n, ξn)n≥1.
σ(A, f, ε) := inf(cid:8)n ≥ 1 : ξn ≤ ε(cid:9) ∈ {1, 2, . . .} ∪ {+∞} . (2)
This corresponds to the ﬁrst time when we can stop the algorithm while being sure to have an
ε-optimal recommendation x(cid:63) n. 1.2 Main Contributions and Outline of the Paper
The main result of this paper is a tight characterization (up to a log factor) of the optimal sample complexity of certiﬁed algorithms in any dimension d ≥ 1, solving a three-decade old open problem raised by Hansen et al. (1991). More precisely, we prove the following instance-dependent upper and lower bounds, which we later state formally in Theorem 3 of Section 4 (see also discussions therein, as well as Propositions 2 and 3 for the limit case L = Lip(f )).
Theorem (Informal statement). Under a mild geometric assumption on X , there exists a computa-tionally tractable algorithm A (e.g., c.DOO, Algorithm 1) such that, for some constants Cd, cd > 0 (depending exponentially on the dimension d), any Lipschitz function f ∈ FL (see (1)) and any accuracy ε,
σ(A, f, ε) ≤ Cd (cid:90)
X dx (cid:0)f (x(cid:63)) − f (x) + ε(cid:1)d , (3) while any certiﬁed algorithm A(cid:48) must satisfy, for all f ∈ FL, and c ≈ cd(1 − Lip(f )/L)d/ log(1/ε), (cid:90) c
X dx (cid:0)f (x(cid:63)) − f (x) + ε(cid:1)d ≤ σ(A(cid:48), f, ε) . (4)
In particular, this result extends to any dimension d ≥ 1 the upper bound proportional to (cid:82) 1 0 dx/(f (x(cid:63)) − f (x) + ε) that Hansen et al. (1991) derived in dimension d = 1 using arguments speciﬁc to the geometry of the real line. 2
Detailed contributions and outline of the paper. We make the following contributions.
• As a warmup, we show in Section 2 how to add error certiﬁcates to the DOO algorithm (well-known in the more classical zeroth-order Lipschitz optimization setting without error certiﬁcates, see Perevozchikov 1990; Munos 2011). We then upper bound its sample complexity by the quantity SC(f, ε) deﬁned in (5) below. This bound matches a recent bound derived by Bouttier et al. (2020) for a computationally much more expensive algorithm. In passing, we also slightly improve the packing arguments that Munos (2011) used in the non-certiﬁed setting.
• In Section 3 we show that, under a mild geometric assumption on X , the complexity measure
, which implies
SC(f, ε) is actually proportional to the integral (cid:82) (3) above. This extends the bound of Hansen et al. (1991) (d = 1) to any dimension d.
X dx/(cid:0)f (x(cid:63)) − f (x) + ε(cid:1)d
• Finally, in Section 4, we prove the instance-dependent lower bound (4), which differs from traditional worst-case lower bounds in the Lipschitz setting. Our proof relies on a local worst-case analysis that could likely prove useful for other learning tasks.
Some of the proofs are deferred to the Supplementary Material, where we also recall useful results on packing and covering numbers (Section A), as well as provide a slightly improved sample complexity bound on the DOO algorithm in the more classical non-certiﬁed setting (Section E). 1.3