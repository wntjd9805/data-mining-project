Abstract
In this work, we establish risk bounds for the Empirical Risk Minimization (ERM) with both dependent and heavy-tailed data-generating processes. We do so by extending the seminal works [Men15, Men18] on the analysis of ERM with heavy-tailed but independent and identically distributed observations, to the strictly sta-tionary exponentially β-mixing case. Our analysis is based on explicitly controlling the multiplier process arising from the interaction between the noise and the func-tion evaluations on inputs. It allows for the interaction to be even polynomially heavy-tailed, which covers a significantly large class of heavy-tailed models be-yond what is analyzed in the learning theory literature. We illustrate our results by deriving rates of convergence for the high-dimensional linear regression problem with dependent and heavy-tailed data. 1

Introduction
Given a random vector (X, Y ) ∈ Rd × R, with joint distribution (X, Y ) ∼ π, and a class of closed, convex set of functions F ⊂ L2(π), the objective in statistical learning theory is to find the best function in the set F that maps the input X to the target Y . The quality of this mapping is measured by a user-defined loss function ℓ : R → R+ ∪ {0}. The most well-studied approach for the above task is that of risk minimization, where the best function is defined as the one that minimizes the expected loss over the set F: f ∗ = argmin f ∈F
P ℓf := argmin f ∈F
Eπ [ℓ (f (X) − Y )] .
The above problem requires the knowledge of the distribution π which is typically unknown in practice. However, we are usually given observations Zi = (Xi, Yi) for i = 1, . . . , N , from the distribution π which leads to the Empirical Risk Minimization (ERM) procedure defined as
ˆf = argmin f ∈F
PN ℓf := argmin f ∈F 1
N
N (cid:88) i=1
ℓ (f (Xi) − Yi) .
The convergence of the empirical risk minimizer ˆf to the true risk minimizer f ∗ is typically analyzed by considering the underlying empirical process, a topic which dates back to the seminal work of [VC71]; see also [VDVW96, vdG00, BBM05, Kol06, Kol11]. In a representative analysis in this
∗Department of Statistics, University of California, Davis.abroy@ucdavis.edu. Research of this author was supported in part by NSF TRIPODS grant CCF-1934568
†Department of Statistics, University of California, Davis. kbala@ucdavis.edu. Research of this author was supported in part by UC Davis CeDAR (Center for Data Science and Artificial Intelligence Research)
Innovative Data Science Seed Funding Program.
‡Department of Computer Science and Department of Statistical Sciences at the University of Toronto, and
Vector Institute. erdogdu@cs.toronto.edu. Research of this author was supported in part by NSERC Grant
[2019-06167], Connaught New Researcher Award, CIFAR AI Chairs program, and CIFAR AI Catalyst grant 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
setting, a majority of the works assume the observations Zi are generated independent and identically distributed (iid) from π, and the analysis is based on uniform concentration. However, there are important limitations associated with this approach, particularly due to the (Talagrand’s) contraction principle which naturally requires a Lipschitz loss function (see, for example, [LT13, Corollary 3.17] or [Kol11, Theorem 2.3]). As a result, in order to work with standard (unbounded) loss functions such as squared-error loss or Huber loss, it is generally assumed that the range of f ∈ F is uniformly bounded and/or the noise ξ := Y − f (X) is also uniformly bounded π-almost surely.
Several attempts have been made in the literature to overcome the limitations of the standard ERM analysis. A significant progress was made by Mendelson [Men15, Men18], who proposed the so-called learning without concentration framework for analyzing ERM procedures with unbounded noise or loss functions. The approach is based on a combination of small-ball type assumption on the input samples Xi, along with developing multiplier empirical process inequalities under weaker moment assumptions. We refer the interested reader, for example, to [Men17b, Men17a,
LM18, LRS15, GM20] for details. The aforementioned works, while relaxing the prior analysis of
ERM to handle heavy-tailed data-generating process (DGP), still require the more stringent iid assumption for their analysis. This restricts the practical applicability of the developed theoretical results significantly. Indeed, heavy-tailed and dependent data appear naturally in various practical learning scenarios [BF89, JM01, DKBR07]; however, theoretical guarantees are still missing.
Our Contributions: Aiming to fill the above gap, we analyze ERM with convex loss functions (that are locally strongly-convex around the origin) when the DGP is both heavy-tailed and non-iid. We do so by extending the small-ball technique of [Men15, Men18] to the strictly stationary exponentially
β-mixing data. In the iid case, the interaction between the noise and the inputs is handled by an analysis based on multiplier empirical process. However, developing similar techniques in the non-iid case is fundamentally restrictive due to the limitations of the analysis based on empirical process.
We side-step this issue for the non-iid case by directly making assumptions on the interaction, which allows for it to be either exponentially or polynomially heavy-tailed. For the exponentially heavy-tailed interactions, we leverage the concentration inequalities developed by [MPR11]. For the polynomially heavy-tailed case, we develop new concentration inequalities extending the recent work [BMdlP20] to β-mixing random variables. We illustrate our results in the context of ERM with sparse linear function class and stationary β-mixing DGP under both squared and Huber loss.
Motivation: A natural question arises in this context: Why study ERM with convex loss functions when the DGP is heavy-tailed? Firstly, convex loss functions cover a large class of robust loss function that are tailored to deal with the heavy-tailed behavior present in the noise and/or input data. Some examples include the Huber loss [Hub92], conditional value-at-risk [RU02, RS06, MGW20, SY20] and the so-called spectral risk measures [Ace02, HH21]. While there exist studies for nonconvex loss functions suited for heavy-tailed input data (for example, [Loh17]), such analyses are mostly in a model-based setting and focus on estimation error. Secondly, while alternatives to ERM have also been proposed and analyzed in the literature for the iid case (with the most prominent one being the median-of-means framework and its variants [MM19, LM19, LL20, BM21]), it is not immediately clear how to extend such methods to the dependent DGP that we consider in this paper. We view our work as taking the first step in developing risk bounds for statistical learning when the DGP is both heavy-tailed and dependent.