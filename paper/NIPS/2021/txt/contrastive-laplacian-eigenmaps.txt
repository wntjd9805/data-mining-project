Abstract
Graph contrastive learning attracts/disperses node representations for similar/dis-similar node pairs under some notion of similarity. It may be combined with a low-dimensional embedding of nodes to preserve intrinsic and structural properties of a graph. In this paper, we extend the celebrated Laplacian Eigenmaps with contrastive learning, and call them COntrastive Laplacian EigenmapS (COLES).
Starting from a GAN-inspired contrastive formulation, we show that the Jensen-Shannon divergence underlying many contrastive graph embedding models fails under disjoint positive and negative distributions, which may naturally emerge during sampling in the contrastive setting. In contrast, we demonstrate analyt-ically that COLES essentially minimizes a surrogate of Wasserstein distance, which is known to cope well under disjoint distributions. Moreover, we show that the loss of COLES belongs to the family of so-called block-contrastive losses, previously shown to be superior compared to pair-wise losses typically used by contrastive methods. We show on popular benchmarks/backbones that COLES offers favourable accuracy/scalability compared to DeepWalk, GCN, Graph2Gauss,
DGI and GRACE baselines. 1

Introduction
Celebrated graph embedding methods, including Laplacian Eigenmaps [5] and IsoMap [42], reduce the dimensionality of the data by assuming that it lies on a low-dimensional manifold. The objective functions used in studies [5, 42] model the pairwise node similarity [7] by encouraging the embeddings of nodes to lie close in the embedding space if the nodes are closely related. In other words, such penalties do not guarantee that unrelated graph nodes are separated from each other in the embedding space. For instance, Elastic Embedding [8] uses data-driven afﬁnities for the so-called local distance term and the data-independent repulsion term.
In contrast, modern graph embedding models, often uniﬁed under the Sampled Noise Contrastive
Estimation (SampledNCE) framework [33, 28] and extended to graph learning [41, 15, 50], enjoy contrastive objectives. By maximizing the mutual information between patch representations and high-level summaries of the graph, Deep Graph Infomax (DGI) [43] is a contrastive method. GraphSAGE
[15] minimizes/maximizes distances between so-called positive/negative pairs, respectively. It relies on the inner product passed through the sigmoid non-linearity, which we argue below as suboptimal.
Thus, we propose a new COntrastive Laplacian EigenmapS (COLES) framework for unsuper-vised network embedding. COLES, derived from SampledNCE framework [33, 28], realizes the negative sampling strategy for Laplacian Eigenmaps. Our general objective is given as:
Θ∗ = arg max
Θ
Tr(fΘ(X)(cid:62)∆WfΘ(X)) + βΩ(fΘ(X)). (1)
X ∈ Rn×d in Eq. (1) is the node feature matrix with d feature dimensions given n nodes, fΘ(X) ∈
Rn×d(cid:48) is an output of a chosen Graph Neural Network backbone (embeddings to optimize) with the
*The corresponding author.
Code: https://github.com/allenhaozhu/COLES. 35th Conference on Neural Information Processing Systems (NeurIPS 2021). 1
(a) (b)
Figure 1: Densities of dot-product scores (cid:104)v, u(cid:105) and (cid:104)v, u(cid:48)(cid:105) (red and blue curves) between the anchor/positive embedding and the anchor/negative embedding (GCN contrastive setting). Left/right
ﬁgures use two distinct minibatches sampled on Cora. With the small overlap of distributions, many contrastive methods relying on the JS divergence may underperform (see Section 4.1 for details). feature dimension d(cid:48), Θ denotes network parameters, whereas ∆W ∈ Sn
+ is the difference between the degree-normalized positive and negative adjacency matrices which represent the data graph and some negative graph capturing negative links for contrastive learning. Moreover, β ≥ 0 controls the regularization term Ω(·) whose role is to constrain the (cid:96)2 norm of network outputs or encourage the so-called incoherence [36] between column vectors. Section 3.1 presents COLES for the Linear
Graph Network (LGN) family, in which we take special interest due to their simplicity and agility.
By building upon previous studies [28, 2, 48], we show that COLES can be derived by reformulating
SampledNCE into Wasserstein GAN using a GAN-inspired contrastive formulation. This result has a profound impact on the performance of COLES, as the standard contrastive approaches based on
SampledNCE strategy (i.e., GraphSAGE [15]) turn out to utilize the Jensen-Shannon divergence, which yields log 2 constant and vanishing gradients for disjoint distributions of positive and negative sampled pairs used for contrastive learning. Figure 1 shows two examples of such nearly disjoint distributions. In contrast, COLES by design avoids the sigmoid in favour of the Radial Basis Function (RBF) non-linearity. We show that such a choice coincides with a surrogate of Wasserstein distance, which is known for its robustness under poor overlap of distributions, leading to the good performance of COLES. Moreover, we also show that the loss of COLES belongs to the family of so-called block-contrastive losses, which were shown to be superior compared to pair-wise losses [3]. In summary, our contributions are threefold: i. We derive COLES, a reformulation of the Laplacian Eigenmaps into a contrastive setting, based on the SampledNCE framework [33, 28]. ii. By using a formulation inspired by GAN, we show that COLES essentially minimizes a surrogate of Wasserstein distance, as opposed to the Jensen-Shannon (JS) divergence emerging in traditional contrastive learning. Speciﬁcally, by showing the Lipschitz continuous nature of our formulation, we prove that our formulation enjoys the Kantorovich-Rubinstein duality for the Wasserstein distance. iii. We show COLES enjoys a block-contrastive loss known to outperform pair-wise losses [3].
Novelty. We propose a simple way to obtain contrastive parametric graph embeddings which works with numerous backbones. For instance, we obtain spectral graph embeddings by combining COLES with SGC [49] and S2GC [61], which is solved by the SVD decomposition. 2 Preliminaries
Notations. Let G = (V, E) be a simple, connected and undirected graph with n = |V | nodes and m = |E| edges. Let i ∈ {1, · · · , n} be the node index of G, and dj be the degree of node j of G. Let
W be the adjacency matrix, and D be the diagonal matrix containing degrees of nodes. Moreover, let X ∈ Rn×d denote the node feature matrix where each node v is associated with a feature vector xv ∈ Rd. Let the normalized graph Laplacian matrix be deﬁned as L = I − D−1/2 (cid:99)WD−1/2 ∈ Sn
+, a symmetric positive semi-deﬁnite matrix. Finally, scalars and vectors are denoted by lowercase regular and bold fonts, respectively. Matrices are denoted by uppercase bold fonts. 2
2.1 Negative Sampling
SampledNCE [14, 33, 28], a contrastive learning framework, is used by numerous works [41, 15, 50].
Let pd(u|v) and pn(u(cid:48)|v) be the so-called data and negative distributions given the so-called anchor node v, where u and u(cid:48) denote the node for a positive and negative sample, respectively. Let pd(v) be the anchor distribution. Given some loss components sΘ(v, u) and ˜sΘ(v, u(cid:48)) whose role is to evaluate the similarity for pairs (v, u) and (v, u(cid:48)), the contrastive loss is typically given as:
J(Θ) = Ev∼pd(v) (cid:2)Eu∼pd(u|v)sΘ(v, u) + ηEu(cid:48)∼pn(u(cid:48)|v)˜sΘ(v, u(cid:48))(cid:3) , (2) where η > 0 controls the impact of negative sampling. Let u ∈ Rd(cid:48) be the embedding of the node u obtained with an encoder fΘ(xu) given parameters Θ, where xu ∈ Rd is the initial node feature vector. Let u(cid:48) ∈ Rd(cid:48) be embeddings of nodes u(cid:48) and v, accordingly. Let sΘ(u, v) = log σ(u(cid:62)v) and ˜sΘ(u(cid:48), v) = log(1 − σ(u(cid:48)(cid:62)v)), where σ(·) is the sigmoid function.
Subsequently, one obtains the contrastive objective (to be maximized), employed by LINE [41],
REFINE [60], GraphSAGE [15] and many other methods according to Yang et al. [50]: (cid:2)Eu∼pd(u|v) log σ(u(cid:62)v) + ηEu(cid:48)∼pn(u(cid:48)|v) log σ(−u(cid:48)(cid:62)v)(cid:3) .
J(Θ) = Ev∼pd(v) and v ∈ Rd(cid:48) (3)
In what follows, we argue that the choice of sigmoid for σ(·) leads to negative consequences. Thus, we derive COLES under a different choice of sΘ(v, u) and ˜sΘ(v, u(cid:48)). 3 Methodology
In what follows, we depart from the above setting of (typical) contrastive sampling, which results in a derivation of our COntrastive Laplacian EigenmapS (COLES). 3.1 Contrastive Laplacian Eigenmaps
Instead of log-sigmoid used in sΘ(v, u) and ˜sΘ(v, u(cid:48)) of Eq. (3), let us substitute sΘ(v, u) = log exp(u(cid:62)v) = u(cid:62)v and ˜sΘ(v, u(cid:48)) = log exp(−u(cid:48)(cid:62)v) = −u(cid:48)(cid:62)v into Eq. (2), which yields:
J(Θ) = Ev∼pd(v) (4)
We assume that variables of the above objective (to maximize) can be constrained (e.g., by the (cid:96)2 norms to prevent ill-posed solutions) and represented by degree-normalized adjacency matrices. Next, we cast Eq. (4) into the objective of COLES (refer to our Suppl. Material for derivations): (cid:2)Eu∼pd(u|v)(u(cid:62)v) + ηEu(cid:48)∼pn(u(cid:48)|v) (cid:0)−u(cid:48)(cid:62)v(cid:1)(cid:3) .
Y∗ = arg min
Tr(Y(cid:62)LY) −
Y, s.t. Y(cid:62)Y=I
η(cid:48)
κ
κ (cid:88) k=1
Tr(Y(cid:62)L(−) k Y)
= arg max
Tr(Y(cid:62)∆WY) where ∆W =W(+) −
Y, s.t. Y(cid:62)Y=I (5)
η(cid:48)
κ
κ (cid:88) k=1
W(−) k
, k contain the embedding vectors, L(−) and L = I−W(+). The scalar 0 ≤ η(cid:48) ≤ 1 ensures that L− η(cid:48)
κ and the rows of matrix Y ∈ Rn×d(cid:48) randomly generated degree-normalized Laplacian matrices capturing the negative sampling, L(−)
I−W(−) truncate the negative spectrum instead) and controls the impact of W(−)
We note that COLES minimizes over the standard Laplacian Eigenmap while maximizing over the randomized Laplacian Eigenmap, which alleviates the lack of negative sampling in the original
Laplacian Eigenmaps. However, unlike Laplacian Eigenmaps, we do not optimize over free variables
Y but over the network parameters, as in Eq. (1) and (6). Clearly, if η(cid:48) = 0 and Y are free variables,
Eq. (5) reduces to standard Laplacian Eigenmaps [5]: Y∗ = arg minY, s.t. Y(cid:62)Y=I Tr(Y(cid:62)LY). for k = 1, · · · , κ are k =
+ (one could k=1 L(−)
. k ∈ Sn (cid:80)η(cid:48) k k
COLES for Linear Graph Networks. In what follows, we are especially interested in the lightweight family of LGNs such as SGC [49] and S2GC [61] (APPNP [23] with the linear activation could be another choice) whose COLES-based objective can be reformulated as:
P∗ = arg max
Tr(PX(cid:62)F(cid:62)∆WFXP(cid:62)). (6)
P, s.t. PP(cid:62)=I 3
F ∈ Rn×n in Eq. (6) is the so-called spectral ﬁlter operating on the (degree-normalized) graph adjacency matrix, and P ∈ Rd(cid:48)×d is a unitary projection matrix such that 0 < d(cid:48) < d. The solution to
Eq. (6) can be readily obtained by solving the generalized eigenvalue problem X(cid:62)F(cid:62)∆WFXp =
λp (an SVD on a small d × d matrix (X(cid:62)F(cid:62)∆WFX) ∈ Sd
+). This step results in a matrix of embeddings fP(X) = FXP(cid:62) ∈ Rn×d(cid:48) for supervised training. Based on given a degree-normalized graph adjacency matrix W ∈ Rn×n, the spectral ﬁlters for SGC and S2GC are given as WK(cid:48) and
αI + 1−α k=1 Wk. Here, integer K (cid:48) ≥ 1 and scalar α ≥ 0 are the number of layers and the
K(cid:48) importance of self-loop. Note that Eq. (6) is related to Locality Preserving Projections [17] if η(cid:48) = 0.
Note also that enforcing the orthogonality constraints in Eq. (6) coincides with the SVD-based solution described above. In contrast, the more general form of COLES in Eq. (1) requires the regularization or constraints (depending on the backbone) imposed on minibatches i ∈ B e.g., we
Θ(Xi)fΘ(Xi) − I(cid:107)2 used the soft penalty Ω(fΘ(Xi)) = (cid:107)f (cid:62)
F . (cid:80)K(cid:48)
COLES (Stiefel). investigate:
Inspired by the Locality Preserving Projections [17] and Eq. (6), we also (P∗, Θ∗) = arg max
P,Θ, s.t. PP(cid:62)=I
Tr(Pf (cid:62)
Θ(X)∆WfΘ(X)P(cid:62)), (7) solved on the Stiefel manifold by GeoTorch [29]. The embed. is: fP(X) = fΘ(X)P(cid:62) ∈ Rn×d(cid:48)
. 4 Theoretical Analysis 4.1 COLES is Wasserstein-based Contrastive Learning
By casting the positive and negative distributions of SampledNCE as the real and generated data distributions of GAN, the key idea of this analysis is to (i) cast the traditional contrastive loss in Eq. (3) (used by LINE [41], GraphSAGE [15] and other methods [50]) as a GAN framework, and show this corresponds to the use of JS divergence and (ii) cast the objective of COLES in Eq. (4) as a GAN framework, and show it corresponds to the use of a surrogate of Wasserstein distance. The latter outcome is preferable under the vanishing overlap of two distributions, as the JS divergence yields log(2) constant and vanishing gradients. The Wasserstein distance suffer less from this issue.
For simplicity, consider the embedding v of the anchor node is given. An embedding vector u is sampled from the ‘real’ distribution pr(u) = pd(u | v), and u(cid:48) is sampled from the ‘generator’ distribution pg(u) = pn (u(cid:48) | v). Following Arjovsky et al. [2] and Weng [48], one arrives at a
GAN-inspired formulation which depends on the choice of ‘discriminator’ D(u): (cid:90) (cid:18) u max
Θ pr(u) log(D(u)) + pg(u) log(1 − D(u)) du ≤ 2JS (pr(cid:107)pg) − 2 log 2, (8) (cid:19) where JS (pr(cid:107)pg) denotes the Jensen-Shannon (JS) divergence. If D(u) is completely free, then the optimal D∗(u) which maximizes the left-hand-side (LHS) of Eq. (8) is D(u) = pr(u)/(pr(u) + pg(u)). Plugging D∗ back into the LHS, we get the right-hand-side (RHS) of the inequality. In our setting, the case pg ∼ pr means that negative sampling yields hard negatives, that is, negative and positive samples are very similar. Hence, this family of embedding techniques try to optimally discriminate pr and pg in the embedding space.
The above analysis shows that traditional contrastive losses are bounded by the JS divergence.
Regardless of the choice of D(u), if the support of the density pr and the support of pg are disjoint (e.g., positive and negative samples in the minibatch of the SGD optimization), the JS divergence yields zero and vanishing gradients. If the ‘discriminator’ is set to D(u) = σ(u(cid:62)v), the objective in
Eq. (8) becomes exactly Eq. (3). By noting ∂ log σ(u(cid:62)v)/∂u = σ(u(cid:62)v)v, the gradient is likely to vanish due to the scalar σ(u(cid:62)v) and does not contribute to learning of network parameters. Figure 1 shows densities of x = u(cid:62)v and x = u(cid:48)(cid:62)v for pr and pd estimated by the Parzen window on two sampled minibatches of contrastive GCN. Clearly, these distributions are approximately disjoint.
Compared with the JS divergence, the Wasserstein distance considers the metric structure of the embedding space: inf
γ∼Π(pr,pg) where Π(pr, pg) is the set of joint distributions with marginals pr(u) and pg(u(cid:48)).
E(u,u(cid:48))∼γ(cid:107)u − u(cid:48)(cid:107)1, (9) 4
By the Kantorovich-Rubinstein duality [45], the optimal transport problem for COLES can be equivalently expressed as: sup g: K(g)≤1 (cid:0)Eu∼pr [g(u)] − Eu(cid:48)∼pg [g(u(cid:48))](cid:1)
≥ max
Θ (cid:2)Eu∼pd(u|v)(u(cid:62)v) + Eu(cid:48)∼pn(u(cid:48)|v)(−u(cid:48)(cid:62)v)(cid:3) , (10) under a drawn anchor v ∼ pd(v), where K(g) means the Lipschitz constant, and supreme is taken over all 1-Lipschitz functions (or equivalently, all K-Lipschitz functions.)
The “≥” is because g(u) is chosen to the speciﬁc form gv(u) = u(cid:62)v, where v is parameterized by a graph neural network with parameters Θ. Optimizing over the neural network parameters Θ can enumerate a subset of functions which satisﬁes the Lipschitz constant K.
Lipschitz continuity of COLES. In order to assure the Lipschitz continuity of COLES, let individual embeddings be stacked row-wise into a matrix and (cid:96)2-norm normalized along rows, or along columns.
Given v (the reference node), the following holds:
|u(cid:62)v − u(cid:48)(cid:62)v| ≤ (cid:107)v(cid:107)max(cid:107)u − u(cid:48)(cid:107)1, where K = maxv (cid:107)v(cid:107)max (≤ 1 in the case of either sphere embedding or the constraint Y(cid:62)Y = I of the COLES formula in Eq. (5)). Thus, the function g(u) = u(cid:62)v is Lipschitz with constant K. 4.2 COLES enjoys the Block-contrastive Loss
We notice that COLES leverages an access to blocks of similar data, rather than just individual pairs in the loss function. To this end, we resort to the Prop. 6.2 of Arora et al. [3], which shows that for family of functions F whose (cid:107)f (·)(cid:107) ≤ R for some R > 0, a block-contrastive loss Lblock is always bounded by a pairwise-contrastive loss Lun , that is, Lblock un (f ) ≤ Lun (f ). To that end, Arora et al.
[3] also show that as block-contrastive losses achieve lower minima than their pairwise-contrastive counterparts, they also enjoy better generalization. un
We show that COLES is a block-contrastive loss, which explains its good performance. Following Eq. (4), for a given embedding v = fΘ(xv), and b embeddings ui = fΘ(xui) and u(cid:48)
) drawn according to pd(u | v) and pn(u(cid:48) | v), we have (note minus preceding eq. as here we minimize): i = fΘ(xu(cid:48) i
−Eu∼pd(u|v)(u(cid:62)v)+Eu(cid:48)∼pn(u(cid:48)|v) (cid:0)−u(cid:48)(cid:62)v(cid:1) = −v(cid:62) (cid:18) (cid:80) i ui b
− (cid:19) (cid:80) i u(cid:48) i b(cid:48)
= −v(cid:62)(µ+ −µ−), (11) where µ+ and µ− are positive and negative block summaries of sampled nodes. Looking at Eq. (5), it is straightforward to simply expand (cid:80) (i,j)∈E (cid:107)yi − yj(cid:107)2 2 ∆Wij to see that each index i will act as a selector of anchors, whereas index j will loop over positive and negative samples taking into account their connectivity to i captured by ∆Wij. We provide this expansion in the Suppl. Material. 4.3 Geometric Interpretation.
Below, we analyze COLES through the lens of Alignment and Uniformity on the Hypersphere of Wang and Isola [47]. To this end, we decompose our objective into the so-called alignment and uniformity losses. Firstly, Mikolov et al. [33] have shown that SampledNCE with the sigmoid non-linearity is a practical approximation of SoftMax contrastive loss, the latter suffering poor scalability w.r.t. the count of negative samples. For this reason, many contrastive approaches (DeepWalk, GraphSAGE,
DGI, Graph2Gauss, etc.) adopt SampledNCE rather than SoftMax (GRACE) framework.
Wang and Isola [47] have decomposed the SoftMax contrastive loss into Lalign and Lumiform [47]:
L(u, v, N ) = Lalign(u, v) + Luniform(u(cid:48), v, N ) = − log eu(cid:62)v eu(cid:62)v + (cid:80) u(cid:48)∈N eu(cid:48)(cid:62)v (12) where N is a sampled subset of negative samples, u and v are node indexes of so-called positive sample and anchor embeddings, u and v. Let (cid:104)u, u(cid:105) = (cid:104)u(cid:48), u(cid:48)(cid:105) = (cid:104)v, v(cid:105) = τ 2(τ acts as the so-called u‡∈N ∪{u} eu‡(cid:62)v, that is, Luniform is temperature). Moreover, Lalign = −(cid:104)u, v(cid:105) and Luniform = log (cid:80) 5
a logarithm of an arithmetic mean of RBF responses over the subset N ∪ {u}. Of course, computing the total loss L requires drawing u and v from the graph and summing over multiple Lalign(u, v) and
L(cid:48) uniform(u, v, N ) but we skip this step and the argument variables of loss functions for brevity.
COLES can be decomposed into Lalign and Lumiform [47] as follows: log e−u(cid:48)(cid:62)v = − log uniform = − log eu(cid:62)v −
Lalign + L(cid:48) (cid:88) 1
|N | u(cid:48)∈N eu(cid:62)v (cid:0)Πu(cid:48)∈N eu(cid:48)(cid:62)v(cid:1) 1
Πu(cid:48)∈N eu(cid:48)(cid:62)v(cid:17) 1
|N |
|N | (cid:16)
, (13) uniform = log where Lalign remains the same with SoftMax but L(cid:48) is in fact a logarithm of the geometric mean of RBF responses over the subset N . Thus, our loss can be seen as the ratio of geometric means over RBF functions. Several authors (e.g., Gonzalez [12]) noted that the geometric mean helps smooth out the Gaussian noise under the i.i.d. uniform sampling while loosing less information than the arithmetic mean. The geometric mean enjoys better conﬁdence intervals the arithmetic mean given a small number of samples. As we sample few negative nodes for efﬁcacy, we expect the geometric mean is more reliable. Eq. (12) and (13) are just two speciﬁc cases of a generalized loss:
Lalign + L(cid:48)(cid:48) uniform = − log (14) where Mp(·) in L(cid:48)(cid:48) is the so-called generalized mean. We introduce Mp(·) into the denominator of Eq. (14) but it can be also introduced in the numerator. We investigate the geometric (p = 0), arithmetic (p = 1), harmonic (p = −1) and quadratic (p = 2) means. uniform = log Mp 1 eu(cid:48) (cid:62)v, · · · , eu(cid:48)(cid:62) (cid:16) eu(cid:62)v (cid:62)v, · · · , eu(cid:48)(cid:62)
|N |v(cid:17) ,
Mp (cid:16) 1 eu(cid:48)
|N |v(cid:17) 5