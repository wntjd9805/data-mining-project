Abstract
Robust principal component analysis (RPCA) is a critical tool in modern machine learning, which detects outliers in the task of low-rank matrix reconstruction. In this paper, we propose a scalable and learnable non-convex approach for high-dimensional RPCA problems, which we call Learned Robust PCA (LRPCA).
LRPCA is highly efﬁcient, and its free parameters can be effectively learned to optimize via deep unfolding. Moreover, we extend deep unfolding from ﬁnite itera-tions to inﬁnite iterations via a novel feedforward-recurrent-mixed neural network model. We establish the recovery guarantee of LRPCA under mild assumptions for
RPCA. Numerical experiments show that LRPCA outperforms the state-of-the-art
RPCA algorithms, such as ScaledGD and AltProj, on both synthetic datasets and real-world applications. 1

Introduction
Over the last decade, robust principal component analysis (RPCA), one of the fundamental dimension reduction techniques, has received intensive investigations from theoretical and empirical perspectives
[1–13]. RPCA also plays a key role in a wide range of machine learning tasks, such as video background subtraction [14], singing-voice separation [15], face modeling [16], image alignment
[17] , feature identiﬁcation [18], community detection [19], fault detection [20], and NMR spectrum recovery [21]. While the standard principal component analysis (PCA) is known for its high sensitivity to outliers, RPCA is designed to enhance the robustness of PCA when outliers are present. In this paper, we consider the following RPCA setting: given an observed corrupted data matrix
Y = X(cid:63) + S(cid:63) ∈ Rn1×n2, (1) where X(cid:63) is a rank-r data matrix and S(cid:63) is a sparse outlier matrix, reconstruct X(cid:63) and S(cid:63) simultane-ously from Y .
One of the main challenges of designing an RPCA algorithm is to avoid high computational costs.
Inspired by deep unfolded sparse coding [22], some recent works [23–25] have successfully extended deep unfolding techniques to RPCA and achieved noticeable accelerations in certain applications.
∗Authors are listed in alphabetic order. Correspondence shall be addressed to H.Q. Cai. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Speciﬁcally, by parameterizing a classic RPCA algorithm and unfolding it as a feedforward neural network (FNN), one can improve its performance by learning the parameters of the FNN through backpropagation. Since RPCA problems of a speciﬁc application often share similar key properties (e.g., rank, incoherence, and amount of outliers), the parameters learned from training examples that share the properties can lead to superior performance. Nevertheless, all existing learning-based approaches call an expensive step of singular value thresholding (SVT) [26] at every iteration during both training and inference. SVT involves a full or truncated singular value decomposition (SVD), which costs from O(n3) to O(n2r) ﬂops2 with some large hidden constant in front3. Thus, these approaches are not scalable to high-dimensional RPCA problems. Another issue is that the existing approaches only learn the parameters for a ﬁnite number of iterations. If a user targets at a speciﬁc accuracy of recovery, then the prior knowledge of the unfolded algorithm is required for choosing the number of unfolded layers. Moreover, if the user desires better accuracy later, one may have to restart the learning process to obtain parameters of the extra iterations.
We must ask the questions: “Can one design a highly efﬁcient and easy-to-learn method for high-dimensional RPCA problems?” and “Do we have to restrict ourselves to ﬁnite-iteration unfolding (or a
ﬁxed-layer neural network)?” In this paper, we aim to answer these questions by proposing a scalable and learnable approach for high-dimensional RPCA problems, which has a ﬂexible feedforward-recurrent-mixed neural network model for potentially inﬁnite-iteration unfolding. Consequently, our approach can satisfy arbitrary accuracy without relearning the parameters.