Abstract
Although machine learning models trained on massive data have led to break-throughs in several areas, their deployment in privacy-sensitive domains remains limited due to restricted access to data. Generative models trained with privacy constraints on private data can sidestep this challenge, providing indirect access to private data instead. We propose DP-Sinkhorn, a novel optimal transport-based generative method for learning data distributions from private data with differen-tial privacy. DP-Sinkhorn minimizes the Sinkhorn divergence, a computationally efﬁcient approximation to the exact optimal transport distance, between the model and data in a differentially private manner and uses a novel technique for control-ling the bias-variance trade-off of gradient estimates. Unlike existing approaches for training differentially private generative models, which are mostly based on generative adversarial networks, we do not rely on adversarial objectives, which are notoriously difﬁcult to optimize, especially in the presence of noise imposed by privacy constraints. Hence, DP-Sinkhorn is easy to train and deploy. Ex-perimentally, we improve upon the state-of-the-art on multiple image modeling benchmarks and show differentially private synthesis of informative RGB images.
Project page: https://nv-tlabs.github.io/DP-Sinkhorn. 1

Introduction
Modern machine learning (ML) algorithms and their practical applications (e.g. recommender sys-tems [1], personalized medicine [2], face recognition [3], speech synthesis [4], etc.) have become increasingly data hungry and the use of personal data is often a necessity. Consequently, the impor-tance of privacy protection has become apparent to both the public and academia.
Differential privacy (DP) is a rigorous deﬁnition of privacy that quantiﬁes the amount of information leaked by a user, participating in a data release [5, 6]. The degree of privacy protection is represented by the privacy budget. DP was originally designed for answering queries to statistical databases. In a typical setting, a data analyst (party wanting to use data; e.g. a healthcare company) sends a query to a data curator (party in charge of safekeeping the database; e.g. a hospital), who makes the query on the database and replies with a semi-random answer that preserves privacy. Responding to each new query incurs a privacy cost. If the analyst has multiple queries, the curator must subdivide the privacy budget to spend on each query. Once the budget is depleted, the curator can no longer respond to queries, preventing the analyst from performing new, unanticipated tasks with the database.
Generative models can be applied as a general and ﬂexible data-sharing medium [7, 8], sidestepping the above problems. In this scenario, the curator ﬁrst encodes private data into a generative model;
∗Work done during internship at NVIDIA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).            
then, the model is shared with the analyst, who can use it to synthesize similar yet different data from the training data. This data can be used in any way desired, such as for data analysis or to train speciﬁc ML models. Unanticipated novel tasks can be accommodated without repeatedly interacting with the curator, since the analyst can easily generate additional synthetic data as required.
Furthermore, it has been observed that generative models can reveal critical information about their training data [9, 10]. For example, Webster et al. [9] found that modern GANs trained on images of faces produce examples that greatly resemble their training data, thereby leaking private informa-tion. Hence, the generative model must be learnt with privacy constraints to protect the privacy of individuals contributing to the database.
Differentially private learning of generative models has been studied mostly using generative adver-sarial networks (GANs) [7, 11, 12, 13, 14]. While GANs in the non-private setting can synthesize complex data such as high deﬁnition images [15, 16], their application in the private setting is chal-lenging. This is in part because GANs suffer from training instabilities [17, 18], which can be exacerbated by adding noise to the GAN’s gradients during training, a common technique to im-plement DP. Hence, GANs typically require careful hyperparameter tuning. This goes against the principle of privacy, where repeated access to data need to be avoided [19].
In this paper, we propose DP-Sinkhorn, a novel method to train differentially private generative models using a semi-debiased Sinkhorn loss. DP-Sinkhorn is based on the framework of optimal transport (OT), where the problem of learning a generative model is framed as minimizing the op-timal transport distance, a type of Wasserstein distance, between the generator-induced distribution and the real data distribution [20, 21]. DP-Sinkhorn approximates the exact OT distance in the pri-mal space using the Sinkhorn iteration method [22]. Furthermore, we propose a novel semi-debiased
Sinkhorn loss to optimally control the bias-variance trade-off when estimating gradients of this OT distance in the privacy preserving setting. Since our approach does not rely on adversarial compo-nents, it avoids any training instabilities and removes the need for early stopping (stopping before catastrophic divergence of GANs, as done, for example, in [15]). This makes our method easy to train and deploy in practice. To the best of our knowledge, DP-Sinkhorn is the ﬁrst fully OT-based approach for differentially private generative modeling.
In summary, we make the following contributions: (i) We propose DP-Sinkhorn, a ﬂexible and robust optimal transport-based framework for training differentially private generative models. (ii)
We demonstrate a novel technique to ﬁnely control the bias-variance trade-off of gradient estimates when using the Sinkhorn loss. (iii) Beneﬁting from these technical innovations, we achieve state-of-the-art performance on widely used image modeling benchmarks for varying privacy budgets, both in terms of image quality (as measured by FID) and downstream image classiﬁcation accuracy.
Finally, we present informative RGB images generated under strict differential privacy without the use of public data, with image quality surpassing that of concurrent works. 2