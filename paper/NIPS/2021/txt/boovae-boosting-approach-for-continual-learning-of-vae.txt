Abstract
Variational autoencoder (VAE) is a deep generative model for unsupervised learn-ing, allowing to encode observations into the meaningful latent space. VAE is prone to catastrophic forgetting when tasks arrive sequentially, and only the data for the current one is available. We address this problem of continual learning for
VAEs. It is known that the choice of the prior distribution over the latent space is crucial for VAE in the non-continual setting. We argue that it can also be helpful to avoid catastrophic forgetting. We learn the approximation of the aggregated posterior as a prior for each task. This approximation is parametrised as an additive mixture of distributions induced by encoder evaluated at trainable pseudo-inputs.
We use a greedy boosting-like approach with entropy regularisation to learn the components. This method encourages components diversity, which is essential as we aim at memorising the current task with the fewest components possible. Based on the learnable prior, we introduce an end-to-end approach for continual learning of VAEs and provide empirical studies on commonly used benchmarks (MNIST,
Fashion MNIST, NotMNIST) and CelebA datasets. For each dataset, the proposed method avoids catastrophic forgetting in a fully automatic way. 1

Introduction
Variational Autoencoders (VAEs) (Kingma & Welling, 2013) are deep generative models used in various domains (Lee et al., 2017; Zhou et al., 2020). The VAE model consists of deep neural networks (DNNs): an encoder (inference network) and decoder (generative network). DNNs are known to reduce their quality on previously learned tasks when trained on data from a new task. Several directions to address this problem of catastrophic forgetting were suggested. But this phenomenon is mainly considered without attention to the speciﬁc properties of the VAE. We want to discuss current approaches to continual learning and formulate requirements for the ideal solution.
Dynamic architecture approach adds task-speciﬁc last layers (multi-heads) to encoder and decoder for each task (Rusu et al., 2016; Nguyen et al., 2018; Li & Hoiem, 2018). We suppose that practical applications of VAE require the common latent space, which is violated in multi-heads. Using multi-heads requires deciding which head to apply to the new data and when to expand the architecture.
They reduce reuse of similarities between tasks. Hence, we suppose that the approach should keep the static architecture for both encoder and decoder.
The generative replay (Shin et al., 2017; Rao et al., 2019) uses a "teacher" generative model for generating "fake" data that mimics former training examples. Then the "student" model is trained on joint "fake" and new data. This approach is conceptually simple, model-agnostic and overcomes forgetting. However, these beneﬁts come with a computational price. We need to retrain model while generating the dataset from all the past tasks, asses samples quality and the task-balance. Thus the approach should avoid generative replay).
⇤These authors contributed equally
†The work was done while the author was at the Skoltech, Moscow, 2019 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Weight penalty approach (Liu et al., 2018; Kirkpatrick et al., 2017) forms the trust-region around the optimum of the previous task for protecting parameters. This approach preserves the architecture and avoids generative replay. However, for DNNs a change in the weights is a poor proxy for the difference in the outputs (Benjamin et al., 2018). It is an even more critical issue for VAE model as it consists of a pair DNNs: encoder and decoder. Thus the approach should link the data-space and the latent-space.
We propose a novel continual learning approach for the VAEs. For each task, we expand current prior to get the approximation of an aggregated posterior over the whole data. We parametrise approximation as an additive mixture of distributions induced by encoder evaluated at trainable pseudo-inputs. These pseudo-inputs link the data-space and latent-space and help to memorise knowledge about past tasks. The problem of matching the aggregated posterior is ill-posed since we observe only its empirical version. As a solution, we use a greedy boosting-like approach with entropy regularisation. This method encourages components in the learned approximation to be diverse, which is essential as we aim at memorising the current task with the fewest components possible. The proposed approach is orthogonal to other methods mentioned above and can be applied in combination with them. Our main contributions are the following:
• We relate the approximation of optimal prior, the aggregated posterior and the continual learning task for VAE model. We ﬁnd optimal additive perturbation in order to approximate optimal prior distribution. We derive the algorithm of effective approximation of the optimal prior for the continual learning framework.
• We use this result and present Boosting Approach for Continual Learning of VAE (BooVAE), a framework for training VAE models in the continual framework with static architecture.
• We empirically validate the proposed algorithm on commonly used benchmarks (MNIST,
Fashion-MNIST, NotMNIST) and CelebA for disjoint sequential image generation tasks.
The proposed generative model could be efﬁciently used in a generative replay for discrimi-native models. We train both generative and discriminative models incrementally, avoiding retraining the generative model for each task from scratch or storing several generative models. We provide code at https : //github.com/AKuzina/BooVAE. 2