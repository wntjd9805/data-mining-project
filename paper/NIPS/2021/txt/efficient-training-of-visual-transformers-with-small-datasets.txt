Abstract
Visual Transformers (VTs) are emerging as an architectural paradigm alternative to
Convolutional networks (CNNs). Differently from CNNs, VTs can capture global relations between image elements and they potentially have a larger representation capacity. However, the lack of the typical convolutional inductive bias makes these models more data hungry than common CNNs. In fact, some local properties of the visual domain which are embedded in the CNN architectural design, in VTs should be learned from samples. In this paper, we empirically analyse different VTs, comparing their robustness in a small training set regime, and we show that, despite having a comparable accuracy when trained on ImageNet, their performance on smaller datasets can be largely different. Moreover, we propose an auxiliary self-supervised task which can extract additional information from images with only a negligible computational overhead. This task encourages the VTs to learn spatial relations within an image and makes the VT training much more robust when training data is scarce. Our task is used jointly with the standard (supervised) training and it does not depend on speciﬁc architectural choices, thus it can be easily plugged in the existing VTs. Using an extensive evaluation with different
VTs and datasets, we show that our method can improve (sometimes dramatically) the ﬁnal accuracy of the VTs. Our code is available at: https://github.com/ yhlleo/VTs-Drloc. 1

Introduction
Visual Transformers (VTs) are progressively emerging architectures in computer vision as an alter-native to standard Convolutional Neural Networks (CNNs), and they have already been applied to many tasks, such as image classiﬁcation [17, 53, 61, 35, 58, 60, 33, 59], object detection [4, 66, 14], segmentation [50], tracking [36], image generation [30, 28] and 3D data processing [65], to mention a few. These architectures are inspired by the well known Transformer [55], which is the de facto standard in Natural Language Processing (NLP) [15, 45], and one of their appealing properties is the possibility to develop a uniﬁed information-processing paradigm for both visual and textual domains. A pioneering work in this direction is ViT [17], in which an image is split using a grid of non-overlapping patches, and each patch is linearly projected in the input embedding space, so obtaining a "token". After that, all the tokens are processed by a series of multi-head attention and feed-forward layers, similarly to how (word) tokens are processed in NLP Transformers.
∗Work done as intern at the Tencent AI Lab. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A clear advantage of VTs is the possibility for the network to use the attention layers to model global relations between tokens, and this is the main difference with respect to CNNs, where the receptive
ﬁeld of the convolutional kernels locally limits the type of relations which can be learned. However, this increased representation capacity comes at a price, which is the lack of the typical CNN inductive biases, based on exploiting the locality, the translation invariance and the hierarchical structure of visual information [35, 58, 60]. As a result, VTs need a lot of data for training, usually more than what is necessary to standard CNNs [17]. For instance, ViT is trained with JFT-300M [17], a (proprietary) huge dataset of 303 million (weakly) labeled high-resolution images, and performs worse than
ResNets [24] with similar capacity when trained on ImageNet-1K (∼ 1.3 million samples [48]). This is likely due to the fact that ViT needs to learn some local proprieties of the visual data using more samples than a CNN, while the latter embeds these properties in its architectural design [47].
To alleviate this problem, a second generation of VTs has very recently been independently proposed by different groups [61, 35, 58, 60, 59, 33, 28]. A common idea behind these works is to mix convolutional layers with attention layers, in such a way providing a local inductive bias to the VT.
These hybrid architectures enjoy the advantages of both paradigms: attention layers model long-range dependencies, while convolutional operations can emphasize the local properties of the image content.
The empirical results shown in most of these works demonstrate that these second-generation VTs can be trained on ImageNet outperforming similar-size ResNets on this dataset [61, 35, 58, 60, 59, 33].
However, it is still not clear what is the behaviour of these networks when trained on medium-small datasets. In fact, from an application point of view, most of the computer vision tasks cannot rely on (supervised) datasets whose size is comparable with (or larger than) ImageNet.
In this paper, we compare to each other different second-generation VTs by either training them from scratch or ﬁne-tuning them on medium-small datasets, and we empirically show that, despite their ImageNet results are basically on par with each other, their classiﬁcation accuracy with smaller datasets largely varies. We also compare VTs with same capacity ResNets, and we show that, in most cases, VTs can match the ResNet accuracy when trained with small datasets. Moreover, we propose to use an auxiliary self-supervised pretext task and a corresponding loss function to regularize training in a small training set or few epochs regime. Speciﬁcally, the proposed task is based on (unsupervised) learning the spatial relations between the output token embeddings. Given an image, we densely sample random pairs from the ﬁnal embedding grid, and, for each pair, we ask the network to guess the corresponding geometric distance. To solve this task, the network needs to encode both local and contextual information in each embedding. In fact, without local information, embeddings representing different input image patches cannot be distinguished the one from the others, while, without contextual information (aggregated using the attention layers), the task may be ambiguous.
Our task is inspired by ELECTRA [12], in which the (NLP) pretext task is densely deﬁned for each output embedding (Section 2). Clark et al. [12] show that their task is more sample-efﬁcient than commonly used NLP pretext tasks, and this gain is particularly strong with small-capacity models or relatively smaller training sets. Similarly, we exploit the fact that an image is represented by a
VT using multiple token embeddings, and we use their relative distances to deﬁne a localization task over a subset of all the possible embedding pairs. This way, for a single image forward pass, we can compare many embedding pairs with each other, and average our localization loss over all of them. Thus, our task is drastically different from those multi-crop strategies proposed, for instance, in SwAV [6], which need to independently forward each input patch through the network. Moreover, differently from "ordering" based tasks [42], we can deﬁne pairwise distances on a large grid without modeling all the possible permutations (more details in Section 2).
Since our auxiliary task is self-supervised, our dense relative localization loss (Ldrloc) does not require additional annotation, and we use it jointly with the standard (supervised) cross-entropy as a regularization of the VT training. Ldrloc is very easy-to-be-reproduced and, despite this simplicity, it can largely boost the accuracy of the VTs, especially when the VT is either trained from scratch on a small dataset, or ﬁne-tuned on a dataset with a large domain-shift with respect to the pretraining
ImageNet dataset. In our empirical analysis, based on different training scenarios, a variable amount of training data and different VT architectures, Ldrloc has always improved the results of the tested baselines, sometimes boosting the ﬁnal accuracy of tens of points (and up to 45 points).
In summary, our main contributions are: 1. We empirically compare to each other different VTs, showing that their behaviour largely differs when trained with small datasets or few training epochs. 2
2. We propose a relative localization auxiliary task for VT training regularization. 3. Using an extensive empirical analysis, we show that this task is beneﬁcial to speed-up training and improve the generalization ability of different VTs, independently of their speciﬁc architectural design or application task. 2