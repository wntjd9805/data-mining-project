Abstract
We introduce regularized Frank-Wolfe, a general and effective algorithm for in-ference and learning of dense conditional random ﬁelds (CRFs). The algorithm optimizes a nonconvex continuous relaxation of the CRF inference problem using vanilla Frank-Wolfe with approximate updates, which are equivalent to minimizing a regularized energy function. Our proposed method is a generalization of existing algorithms such as mean ﬁeld or concave-convex procedure. This perspective not only offers a uniﬁed analysis of these algorithms, but also allows an easy way of exploring different variants that potentially yield better performance. We illustrate this in our empirical results on standard semantic segmentation datasets, where several instantiations of our regularized Frank-Wolfe outperform mean ﬁeld infer-ence, both as a standalone component and as an end-to-end trainable layer in a neural network. We also show that dense CRFs, coupled with our new algorithms, produce signiﬁcant improvements over strong CNN baselines. 1

Introduction
Fully-connected or dense conditional random ﬁelds (CRFs) [34]—combined with strong pixel-level classiﬁers such as a convolutional neural network (CNN) [25, 42]—have been a highly-successful paradigm for semantic segmentation. Top-performing systems on the PASCAL VOC benchmark [22] used to include a CRF as either a post-processing step [13, 14, 15, 20, 43, 44, 45] or a trainable component [4, 49, 64, 68, 73, 76]. However, as CNNs got stronger, the improvements that CRFs brought decreased over time, and as a result they fell out of favor since 2017 [45].
In this paper, we revisit dense CRFs with two contributions. First, on the theoretical side, we propose regularized Frank-Wolfe, a new class of algorithms for inference and learning of CRFs that perform better than the popular mean ﬁeld [34, 35, 58]—the method of choice in the aforementioned works. Regularized Frank-Wolfe optimizes a nonconvex continuous relaxation of the CRF inference problem (§2) by performing approximate conditional-gradient updates (§3.1), which is equivalent to minimizing a regularized energy using the generalized Frank-Wolfe method [53] (§3.2). Several of its instantiations lead to new algorithms that have not been studied before in the MAP inference literature (§3.3). Moreover, we show that it also includes several existing methods, including mean ﬁeld and the concave-convex procedure [75], as special cases (§3.4). This generalized perspective allows a uniﬁed analysis of all these old and new algorithms in a single framework (§4). In particular, we show that (1/pk) for suitable stepsize schemes, and in certain they achieve a sublinear rate of convergence (1/k) (§4.1). cases (such as strongly-convex regularizer or concave energy) this can be improved to
Furthermore, we provide a tightness analysis for the resulting nonconvex relaxation of the regularized energy, recovering some existing tightness results [10, 41, 61] as special cases (§4.2). The proposed algorithms are easy to implement, converge quickly in practice, and have (sub)differentiable iterates.
Such properties are important for successful gradient-based learning via backpropagation [47, 62].
O
O 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our second contribution lies on the practical side. In addition to mean ﬁeld and regularized Frank-Wolfe variants, we re-implement several existing ﬁrst-order inference methods [39, 41, 48]—those that are amenable to gradient-based learning—for comparison. Remarkably, we ﬁnd that dense
CRFs can still achieve important improvements over the strong DeepLabv3+ [17] CNN model for all these solvers (§5). In particular, our best variant of regularized Frank-Wolfe achieves a mean intersection-over-union (mIoU) score of 88.0 on the PASCAL VOC test set (§5.3), improving over
DeepLabv3+. We hope that these encouraging results could attract interest from the community in considering dense CRFs (again) for tasks such as semantic segmentation. Our source code is made publicly available under the GNU general public license for this purpose.1 2