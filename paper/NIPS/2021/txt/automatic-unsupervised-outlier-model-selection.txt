Abstract
Given an unsupervised outlier detection task on a new dataset, how can we au-tomatically select a good outlier detection algorithm and its hyperparameter(s) (collectively called a model)? In this work, we tackle the unsupervised outlier model selection (UOMS) problem, and propose METAOD, a principled, data-driven approach to UOMS based on meta-learning. The UOMS problem is notoriously challenging, as compared to model selection for classiﬁcation and clustering, since (i) model evaluation is infeasible due to the lack of hold-out data with labels, and (ii) model comparison is infeasible due to the lack of a universal objective function. METAOD capitalizes on the performances of a large body of detection models on historical outlier detection benchmark datasets, and carries over this prior experience to automatically select an effective model to be employed on a new dataset without any labels, model evaluations or model comparisons. To capture task similarity within our meta-learning framework, we introduce specialized meta-features that quantify outlying characteristics of a dataset. Extensive experiments show that selecting a model by METAOD signiﬁcantly outperforms no model selection (e.g. always using the same popular model or the ensemble of many) as well as other meta-learning techniques that we tailored for UOMS. Moreover upon (meta-)training, METAOD is extremely efﬁcient at test time; selecting from a large pool of 300+ models takes less than 1 second for a new task. We open-source1
METAOD and our meta-learning database for practical use and to foster further research on the UOMS problem. 1

Introduction
The lack of a universal learning model that performs well on all problem instances is well recognized
[53]. Therefore, effort has been directed toward building a toolbox of various models and algorithms, which has given rise to the problem of algorithm selection and hyperparameter tuning (i.e., model selection). The same problem applies to outlier detection (OD); a long list of detectors has been developed in the last decades [2], with no universal “winners” [8].
In supervised learning, model selection can be done via performance evaluation of each trained model on labeled hold-out data. In contrast, unsupervised OD does not have access to any labels, nor is there a universal objective function that could guide model selection (cf. clustering where a loss function enables model comparison). Unsupervised model selection for OD is challenging exactly because both model evaluation and comparison are not feasible—which renders any trial-and-error techniques like grid search or iterative strategies like Bayesian hyperparameter optimization [57] inapplicable.
Consequently, there has been no principled work on unsupervised outlier model selection—rather, the choice of a model for a new task (or dataset) remains “a black art”. A typical approach is to use popular OD algorithms, like LOF [6] and iForest [31] (often with default hyperparameters) which are shown to be competitive on average on many benchmark datasets. However, as noted earlier, none of 1Code available at URL: https://github.com/yzhao062/UOMS 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
these methods can universally outperform others on all tasks [8]. We argue that model selection is exactly how one can “break the performance ceiling” for OD.
In this work, we tackle the unsupervised outlier model selection (UOMS) problem systematically. To that end, we introduce (to the best of our knowledge) the ﬁrst UOMS approach that selects an effective model to be employed on a new detection task without any model evaluation (using labels) or model comparison (via loss criteria). Our proposed method, called METAOD, is based on meta-learning, and stands on the prior performances of a large collection of existing detection models on an extensive corpora of historical outlier detection benchmark datasets. In a nutshell, the idea is to estimate a candidate model’s performance on the new task (with no labels) based on its prior performance on similar historical tasks. We remark that METAOD is strictly a model selection technique – that picks one model (a detector and its associated hyperparameter(s)) from a pool of (existing) candidate models – and not yet-another outlier detection algorithm itself.
In leveraging meta-learning, we establish a connection between the UOMS problem and the cold-start problem in collaborative ﬁltering (CF), where the new task in UOMS is akin to a new user in CF (with no available evaluations, hence cold-start) and the model space is analogous to the item-set.
Differently, OD necessitates the identiﬁcation of a single best model (i.e., top-1 rank), whereas CF typically operates in a top-k setting. In CF, future recommendations can be improved based on user feedback which is not applicable to OD. Moreover, METAOD requires the effective learning of task similarities based on characteristic dataset features (namely, meta-features) that capture the outlying properties within a dataset, whereas user features (location, age, etc.) in CF may be readily available.
In summary, the key contributions of this work include the following:
• First Approach to Unsupervised Outlier Model Selection: We propose METAOD, (to our knowledge) the ﬁrst effort on unsupervised model selection for OD tasks. Notably, given a new dataset (i.e., at test time), it does not rely on any ground-truth labels for model evaluation or any loss or heuristic criterion for model comparison. METAOD stands on meta-learning in principle, and historical collections of outlier models and benchmark datasets in practice.
• Problem Formulation: We establish a correspondence between UOMS and CF under cold-start, where the new task “better likes” a model that performs better on similar historical tasks.
• Specialized Meta-features: We design novel meta-features to capture the outlying characteristics in a dataset toward effectively quantifying task similarity speciﬁcally among OD tasks.
• Effectiveness and Efﬁciency: Through extensive experiments on two benchmark testbeds that we have constructed, we show that selecting a model by METAOD for each given task signiﬁcantly outperforms always using a popular model like iForest, as well as other possible meta-learning approaches that we tailored for UOMS. Moreover, METAOD incurs negligible run-time overhead (<1 second) at test time.
• Open-source Platform: We open-source1 METAOD and our meta-learning database for the community to use it for UOMS in practice, and to extend it with new datasets and models. We expect the growth of the database would make meta-learning based approaches, like METAOD, more powerful and also help foster further research on this new direction to an important problem. 2