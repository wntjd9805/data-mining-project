Abstract
Following tremendous success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens, i.e. words or image patches, and enables ﬂexible modelling of image data beyond the local interactions of convolutions. This ﬂexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a “transposed” version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efﬁcient processing of high-resolution images. Our cross-covariance image transformer (XCiT) – built upon XCA – combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including (self-supervised) image classiﬁcation on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k. 1

Introduction
Transformers architectures [68] have provided quantitative and qualitative breakthroughs in speech and natural language processing (NLP). After a few attempts to incorporate wide-range self-attention in vision architectures [71, 82], Dosovitskiy et al. [21] established transformers as a viable architecture for learning visual representations, reporting competitive results for image classiﬁcation while relying on large-scale pre-training. Touvron et al. [64] have shown on par or better accuracy/throughput compared to strong convolutional baselines such as EfﬁcientNets [58] when training transformers on ImageNet-1k using extensive data augmentation and improved training schemes. Promising results have been obtained for other vision tasks, including image retrieval [22], object detection and semantic segmentation [44, 70, 81, 83], as well as video understanding [2, 7, 23].
One major drawback of transformers is the time and memory complexity of the core self-attention operation, that increases quadratically with the number of input tokens, or similarly number of (w2h2), which is patches in computer vision. For w prohibitive for most tasks involving high-resolution images, such as object detection and segmentation.
Various strategies have been proposed to alleviate this complexity, for instance using approximate forms of self-attention [44, 81], or pyramidal architectures which progressively downsample the feature maps [70]. However, none of the existing solutions are fully satisfactory, as they either trade complexity for accuracy, or their complexity remains excessive for processing very large images. h images, this translates to a complexity of
O
⇥
We replace the self-attention, as originally introduced by Vaswani et al. [68], with a “transposed” attention that we denote as “cross-covariance attention” (XCA). Cross-covariance attention substi-Code: https://github.com/facebookresearch/xcit 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
<latexit sha1_base64="1CEKzsCFOnoRXAG5s9zr9j/7SaY=">AAAEAnicvVNLb9NAEHYTHsW8WrggcVnRREoEaZPyFBJSUaSIqlGUAG0jZdNovd4kq6x33d1x28iyuPBXuHAAIa78Cm78G9Z2QH1wZiWvx9/MfN/MrNcLBTdQr/9aKhQvXb5ydfmae/3GzVu3V1bv7BkVacp2qRJK9z1imOCS7QIHwfqhZiTwBNv3Zs3Uv3/EtOFKvod5yIYBmUg+5pSAhUarhXvlEg4ITCkR8euksvOoV0WvEAZ2AvE7NYaAnCRYsDFUcDglElQQYx0JNnhcD2EYP7F7Ej+1W2/nAIMK0QY2hxpifzRLEqz5ZArVEsbuGRnMZfblefHb5CDuYOABM6iT5JE9+8pTpgTinSQj3sBAotIfsJeDpbO8ozgvvN9M/lcrpyXPtWXzFo35o8PElkq1MqZG1RHRnEjKUKXfrCICwGR6Gi9dt9xWlhZ1CdAp2pbANKGpC1Xa3e2qVX5oaVqM+bWW0sdE+6jD4FjpGaq0Wh0b0CZzpjtKB27ZE4rOaj4nEyUtZ79Z+6vklrkMI0CgZkwaS9vOCy25/SZ/j0RKMlpZq6/Xs4UuGo2FseYsVne08hP7ikaBlaCCGDNoZGMlGjgVLHFxZFhI6IxM2MCakljBYZz9wgkqW8RHY6XtIwFl6OmMmATGzAPPRqYTNud9Kfgv3yCC8YthnLXLJM2FxpGwraP0PiCfa0ZBzK1BqOa2VkSnJB27vTWuHULjfMsXjb3N9caz9UZvc22rvhjHsnPfeeBUnIbz3Nly3jhdZ9ehhQ+FT4Uvha/Fj8XPxW/F73loYWmRc9c5s4o/fgP7cUw7</latexit>
XCiT layer
<latexit sha1_base64="eruebNSaeV04bSIvV8Oyx0QMgAk=">AAAD9nicvVNLb9NAEHYdHsUUaOHIZUUSKREkTcpTSEhFkSKqRFECtI2UTaP1epOsYu+6u+O2keUfwoUDCHHlt3Dj37C2A+qDMyt5Pf5m5vtmZr1u6HMNjcavNbtw7fqNm+u3nNsbd+7e29y6f6BlpCjbp9KXaugSzXwu2D5w8NkwVIwErs8O3UUr9R+eMKW5FB9hGbJxQGaCTzklYKDJlr1RLuGAwJwSP36bVDpPBlX0BmFgZxB/kFMIyFmCfTaFCg7nRIAMYqwin42eNkIYx8/MnsTPzTboHGGQIdrG+lhB7E0WSYIVn82hWsLYuSCDuci+XDd+nxzFPQw8YBr1kjxyYF55ypxA3Eky4m0MJCr9AQc5WLrIO4nzwoet5H+1cl7yUlsmb9WYNzlOTKlUSa1rVJ4QxYmgDFWGrSoiAEykp/HaccpdaWhRnwCdoz0BTBGaulCl29+rGuXHhqbNmFdrS3VKlId6DE6lWqBKu90zAV2yZKonVeCUXV/SRc3jZCaF4Ry2an+VnDIXYQQI5IIJ7ZS6eZ2lyWaxUW9kC101miujaK1Wf7L5E3uSRoGhpT7RetTMRkkUcOqzxMGRZiGhCzJjI2MKYlTGcfbbJqhsEA9NpTKPAJSh5zNiEmi9DFwTmU5VX/al4L98owimr8Zx1iITNBeaRr5pF6V3AHlcMQr+0hiEKm5qRXRO0lGbm+KYITQvt3zVONipN1/Um4Od4m5jNY5166H1yKpYTeultWu9s/rWvkVtbX+yv9hfC2eFz4Vvhe95qL22ynlgXViFH78Bn7pH/w==</latexit>L
⇥
<latexit sha1_base64="KPZvdnRo7kwadB5MhbdNDPu+nJk=">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>+
<latexit sha1_base64="h7pdr1AQwdjbtyG3A29ciWZxZzE=">AAADtXicvVJbb9owFE5hly67lG6Pe7EGlUBbKbBLp0mTOlVCq1oh2NYWCVN04jhgkcSpfdIWWfmFe9vb/s0cYFKhe96R7Bx/5/7leEkoNDYavzcKxXv3HzzcfOQ+fvL02VZp+/mZlqli/JTJUKq+B5qHIuanKDDk/URxiLyQn3vTw9x+fsWVFjL+gbOEDyMYxyIQDNBCo+2NnzsVGgFOGITmS1Y9ftOrkc+EIr9B810GGMFNRkMeYJUmE4hRRoaqNOSDt40Eh+advTPz3l694wuKMiF7VF8qNP5ommVUifEEaxVK3ZUyVMTzl+eZb9mF6VAUEdekky08e/azCJkAmuNsnniPIqSVv2BvAVZW847MovH+Yfa/Rrldcm0sG7cczB9dZrZVpqTWu0xegRIQM06q/cMaAUQe53/jk+vunEiblnQB2YQcxcgVsNxEqifdo5qt/Lritjn3d9tSXYPySYfjtVRTUm23O7VRqdyoN+ZC7irNpVJ2ltIdlX5RX7I0svVZCFoPmnMmQKFgIc9cmmqeAJvCmA+sGoMdZmjmW5eRHYv4JJDKnhjJHL0dYSDSehZ51jMnRa/bcvBftkGKwcehEXGSWmLYolCQhgQlyVeY+EJxhuHMKsCUsL0SNoGcKbvoriWhuT7yXeWsVW9+qDd7rfJBY0nHpvPSeeVUnaaz7xw4X52uc+qwQqvQL0DBK+4Xh0W/GCxcCxvLmBfOihTlH4JSL/Q=</latexit>
Feed-Forward Network (FFN)
<latexit sha1_base64="FoAHg4HMSRcRayfjk7hi/qD5hAw=">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>LayerNorm
<latexit sha1_base64="KPZvdnRo7kwadB5MhbdNDPu+nJk=">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>+
<latexit sha1_base64="8/rbFhaoAyt80VM2uVIEw0doO78=">AAADlHicvVJba9swFHbsXbrs0mSDvexFLCkksKVOd+kYG6SEwUpKSbalDUSpkRUlFrEtVzouDcJ/aD9nb/s3k50MmnTPOyD56DvX7/j4ScgVuO7vku3cuXvv/s6D8sNHj5/sVqpPz5RIJWVDKkIhRz5RLOQxGwKHkI0SyUjkh+zcX3Rz+/kVk4qL+AcsEzaJyDzmM04JGMirln7u1XFEIKAk1EdZo/dq0ESfEQZ2Dfq7mEFErjMcshk0cBKQGESksUxDNn7jJjDRb82d6XfmGvQuMIgE7WN1KUFPvUWWYcnnATTrGJc3ymAeFy/f19+yC32KgUdModNs5Tkwn1VIQED3siLxPgaS1v+CgxVY38zr6VXjo272v6jcLLlFy8StiU29y8y0SqVQ6jUVV0RyElOGGqNuExEAFud/42O5fCJMVtQnQAN0HAOThOYW1DjpHze9Ss1tuYWg20p7rdSstfS9yi88FTSNTHoaEqXG7YIokcBpyLIyThVLCF2QORsbNSam14kulipDewaZopmQ5sSACvRmhCaRUsvIN545Z7Vty8F/2cYpzD5MNI+T1PCmq0KzNEQgUL6haMoloxAujUKo5KZXRAOST8LscdkMob1N+bZydtBqv2+1Bwe1jrsex471wnppNay2dWh1rK9W3xpa1K7ah3bHPnKeO5+crvNl5WqX1jHPrA1xTv8AZJQlDg==</latexit>
Local Patch Interaction (LPI)
<latexit sha1_base64="FoAHg4HMSRcRayfjk7hi/qD5hAw=">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>LayerNorm
<latexit sha1_base64="KPZvdnRo7kwadB5MhbdNDPu+nJk=">AAADmXicvVJbb9MwFE5bLiPcOpB42YtFU6kV0CXjKgTSpgq00WlqGdsq1V3kuG5rNYkz+2RaZeU38V9449/gpEVaO545kp3j71y/kxMkIVfgur9L5cqt23fubtyz7z94+OhxdfPJqRKppOyEilDIfkAUC3nMToBDyPqJZCQKQnYWzNq5/eySScVF/APmCRtGZBLzMacEDORvln7WHRwRmFIS6r2s0XnZa6LPCAO7An0sxhCRqwyHbAwNnExJDCLSWKYhG7x2ExjqN+bO9Ftz9TrnGESCtrG6kKBH/izLsOSTKTQdjO2VMpjHxSsI9PfsXB9h4BFT6ChbePbMZxEyJaA7WZF4GwNJnb9gbwE6q3l9vWi8387+F5XrJddombglsZF/kZlWqRRKvaLikkhOYspQo99uIgLA4vxvfLTt+qEwaVGXAJ2igxiYJDQ3ocZh96BpOy8cv1pzW24h6KbiLZWatZSuX/2FR4KmkalBQ6LUwCvYEgmchiyzcapYQuiMTNjAqDExDQ91sVkZqhtkhMZCmhMDKtDrEZpESs2jwHjmxNW6LQf/ZRukMP4w1DxOUkOeLgqN0xCBQPmaohGXjEI4NwqhkpteEZ2SfBpmmW0zBG+d8k3ldKflvWt5vZ3arrscx4a1ZT23GpZnvbd2rX2ra51YtPys/Kn8pfy1slXZq+xXvi1cy6VlzFNrRSrHfwAfuCXi</latexit>+
<latexit sha1_base64="yeUADHGIyi00gAsuWK+ziUGW3+s=">AAAEBHicvVNLb9NAEHYTHsW8WrjBZUUTKRF9JOV5QWoVKaJqFCXQR6RsGq3Xm2SV9a67O24bWT5w4a9w4QBCXPkR3Pg3rO2A+uDMSl6Pv5n5Zr5ZrxcKbqBW+7VQKF67fuPm4i339p279+4vLT84MCrSlO1TJZTuecQwwSXbBw6C9ULNSOAJduhNG6n/8IRpw5Xcg1nIBgEZSz7ilICFhsuFR+USDghMKBHxdlLZXe1W0RuEgZ1B/F6NICBnCRZsBBUcTogEFcRYR4L1n9VCGMTP7Z7EL+zW3T3CoEK0gc2xhtgfTpMEaz6eQLWEsXuhDOYy+/K8+F1yFLcx8IAZ1E7yyK595SkTAvFukhFvYCBR6Q/YzcHSRd5hnDfeayT/S8r5kpdk2by5MH94nJTchlbGrDXUCdGcSMrQNgCT6TmgSq+xXXXdcktZVtQhQCdoRwLThOb+Vmenags/tYKbjPlrTaVPifZRm8Gp0lNUaTbbNqBFZky3lQ7csicUna75nIyVtJy2gFvmMowAgZoyaSxbK2/PcvYafA+JNHkV/YWHSyu19Vq20FWjPjdWnPnqDJd+Yl/RKLCiqCDG9OvZYIkGTgVLXBwZFhI6JWPWt6Yktsogzn7iBJUt4qOR0vaRgDL0fEZMAmNmgWcj0xmby74U/JevH8Ho9SDOpDNJ80KjSNgxoPRGIJ9rRkHMrEGo5rZXRCcknby9N64dQv2y5KvGweZ6/eV6vbu5srU6H8ei89h54lScuvPK2XLeOh1n36GFD4VPhS+Fr8WPxc/Fb8XveWhhYZ7z0Lmwij9+A261S44=</latexit>
Cross-Covariance Attention (XCA)
<latexit sha1_base64="FoAHg4HMSRcRayfjk7hi/qD5hAw=">AAADwHicvVJba9swFHaTXTrv0nZ73ItYU0jY2ibdlUGhWyCstIRkW9tAlIZjWalFZMuRjttmwn9yD4P9m8lJBr3seQckH33n/vkEqRQG6/XfS6Xynbv37i8/8B8+evxkZXXt6bFRmWb8iCmpdC8Aw6VI+BEKlLyXag5xIPlJMG4W9pNzro1QyXecpnwQw1kiRoIBOmi4tvRro0JjwIiBtJ/y6sGrbo3sEor8Eu03NcIYLnMq+QirNI0gQRVbqjPJ+6/rKQ7sG3fn9q27ugenFFVKtqmZaLThcJznVIuzCGsVSv1rZahIZq8gsF/zU9umKGJuSDufe3bdZx4SAdqDfJZ4myJklb9gdw5Wrucd2nnjvWb+v0a5WvLGWC5uMVg4nOSuVaaVMZtMnYMWkDBOqr1mjQAiT4q/8dH3Nw6VS0s6gCwi+wlyDawwkephZ7/mKr90aVqch5stpS9Ah6TN8ULpMam2Wu2afwhTrttKx8PV9fpWfSbkttJYKOveQjrD1Z80VCyLXStMgjH9xowU0CiY5LlPM8NTYGM4432nJuDmGtjZAuZkwyEhGSntToJkhl6NsBAbM40D51nwY27aCvBftn6Gow8DK5I0cxyxeaFRJgkqUmwzCYXmDOXUKcC0cL0SFkFBmtt535HQuDnybeV4Z6vxbqvR3Vnfqy/oWPaeey+8qtfw3nt73hev4x15rLRbYiVZisufy1FZlSdz19LSIuaZd03KP/4AC440Kg==</latexit>LayerNorm
<latexit sha1_base64="LeZ03nPOmmTYMNuojoWcyqxFVTk=">AAAD63icvVNbb9MwFM5SLiPcNnjkxWKt1Aq6teMqJKRBpYppU9UC2yrVXeU4bmM1sTP7ZFtl5S/wwgMI8cof4o1/g5MWtAvPWIpzci7fOd9nx08irqHR+LXklq5cvXZ9+YZ389btO3dXVu/ta5kqyvaojKTq+0SziAu2Bxwi1k8UI7EfsQN/2srjB8dMaS7FR5glbBiTieBjTglY12jVXaqUcUwgpCQyb7LqzuNeDb1GGNgpmA9yDDE5zXDExlDFSUgEyNhglUZs8KSRwNA8tXtmntmtt3OIQSZoA+sjBSYYTbMMKz4JoVbG2DvXBnNRfPm+eZ8dmg4GHjONOtk8s2df85KQgNnJCuANDCQt/3H25s7yedyRmQ/eb2X/i8rZlhdo2boFsWB0lNlRqZJa16k8JooTQRmq9ls1RACYyE/jledVdqWFRV0CNETbApgiNA+h6m53u2Y7P7IwbcaCeluqE6IC1GFwItUUVdvtjk3YJTOmOlLFXuVtJOm0HnAykcJi9lv1v508LpIUEMgpE3q0stZYbxQLXTaaC2PNWazuaOUnDiRNY4tFI6L1oFnoRxRwGrHMw6lmCaFTMmEDawpiJRia4q5mqGI9ARpLZR8BqPCerTAk1noW+zYzl1JfjOXOf8UGKYxfDk1BjAk6bzROI0sS5RcfBVwxCtHMGoQqbmdFNCS5vvb38KwIzYuULxv7m+vN5+vN3ubaVmMhx7LzwHnoVJ2m88LZct45XWfPoW7ofnK/uF9Lcelz6Vvp+zzVXVrU3HfOrdKP3+CFRDA=</latexit>input tokens
<latexit sha1_base64="jtdbbq1umaFiw3sS5idIFzHKLag=">AAAEanicvVNba9swFHaTbOu8dWs72Bh9EWsCCW3SpLu+DFoCYSUhJOstEKdBlpVERJZc6bhtMIb9xr3tF+xlP2JynLHe2OMElo4/fef2HeQGnGmoVn8sZbK5Bw8fLT+2nzxdefZ8dW39RMtQEXpMJJeq52JNORP0GBhw2gsUxb7L6ak7rSf3pxdUaSbFEcwCOvDxWLARIxgMNFzLfCvkHR/DhGAe7cfF5na3hD4jB+gVRIdyBD6+ih1OR1B0ggkWIP3IUSGn/bfVAAbRO7PH0XuzdZtnDsgA7Tj6XEHkDadx7Cg2nkAp7zj2jTQOE/M/142+xmdR2wHmU43accrsmiN1mWCImvE88I4DOMz/AbspmL8Zdxilhffq8f9q5XrKW20Zv0Vj3vA8NqXWldS6XJcXWDEsCEX7AFQkg0DFXn2/ZB9SPirjv+AJ1pdYMEQBYV4p2XahJU1a1MFAJuhAAFWYpNRW56BkKtsyaRqUeuWGVJdYeahN4VKqKSo2Gm1DaOEZVW2pfLvgckmmZY/hsRQmpinALjARhIBATqnQJlorrd/E7NXZEeKJ8za6BuebyDSN7htmolrekLvoHxQjy3B1s1qpzhe6a9QWxqa1WJ3h6nfHkyT0jUiEY637tfn4sAJGOI1tJ9Q0wGSKx7RvTIFNqkE0fyoxKhjEQyOpzCcAzdHrHhH2tZ75rmEmBevbdwl4310/hNGnQTTXjwqSJhqF3GiJkneHPKYoAT4zBiaKmVoRmeBkfOZ12kaE2u2W7xonu5Xah0qtu7u5t72QY9nasN5YRatmfbT2rC9Wxzq2SOZndiX7Mvsq+yu3nnud20ipmaWFzwvrxsrlfwPJ7Gtm</latexit>
Self-attention (Vaswani et al.)
<latexit sha1_base64="2zVyOUg3FwZMgvsI1cBI4DvtOPQ=">AAACbHicbVFNbxMxEPVu+Sjho2nhAKqQLJKiVKrCbqGlF6QiLki9NIK0leIQeR1vYsUfW3sWNbL2xD/kxk/gwm/A2eRQWkbyzNObN/L4OSukcJAkv6J47c7de/fXHzQePnr8ZKO5uXXmTGkZ7zMjjb3IqONSaN4HAZJfFJZTlUl+ns0+Lfrn37l1wuivMC/4UNGJFrlgFAI1av5oE0Vhyqj0H6vOyV5vF3/ABPgV+C8mB0WvKiJ5Dh1STKkGozyxpeSDt0kBQ/8u5MofhNQ7+UbAFPgNcZcW/Hg0qypixWQKu21CGjvtXig1WAqv6dqjZivpJnXg2yBdgRZaxemo+ZOMDSsV18AkdW6Q1ttQC4JJXjVI6XhB2YxO+CBATRV3Q1+bVeGdwIxxbmw4GnDNXp/wVDk3V1lQLpxxN3sL8n+9QQn50dALXZTANVtelJcSg8EL5/FYWM5AzgOgzIqwK2ZTaimD8D+NYEJ688m3wdl+Nz3spr391vHRyo51tI1eoQ5K0Xt0jD6jU9RHDP2ONqLn0YvoT/ws3o5fLqVxtJp5iv6J+PVfmRq6EQ==</latexit>
A (K, Q) = Softmax
<latexit sha1_base64="TgWpT0wMMIg4lLuzyVhnYotJxzk=">AAADUnictVJLbxMxEPYmPEooNIUjF4skUiqhNFug7QWpqBekSFUDpI0Upyuv481a8T5qz6JGln8jEuLCD+HCAXA2QZQUjozk8edvZjwPTZhLoaHb/eJVqrdu37m7ca92f/PBw6369qMznRWK8QHLZKaGIdVcipQPQIDkw1xxmoSSn4ez44X9/ANXWmTpe5jnfJzQaSoiwSg4Ktj24laTJBRiRqV5bdu9Z/0d/AoT4Fdg3mURJPTKEskjaJM8pilkiSGqkHz0vJvD2Lxw2pqXTvV7FwSyHO8SfanATIKZtUSJaQw7TUJq17MQkZavMDRv7YU5ISASrvGJXTi2mn13lYDEFEzPlv/uEqBF8xfZX5Ll+/e/gVnWPTy2/7GTf6Zca8vFrRqbBJe2GdQb3U63FHwT+CvQQCs5DeqfyCRjRcJTYJJqPfLLOqkCwSS3NVJonlM2o1M+cjClLtXYlCthccsxExxlyp0UcMlejzA00XqehM5zUbJety3Iv9lGBUSHYyPSvACesmWiqJAYMrzYLzwRijOQcwcoU8LVillMFWXgtrDmhuCvt3wTnO11/P2O399rHB2uxrGBnqCnqI18dICO0Bt0igaIeR+9r95370flc+Vb1atWl64VbxXzGP0h1c2fJZESdQ==</latexit>
A 2
RN
⇥
N
 
 
 
 
 
 
<latexit sha1_base64="Iru02VIUgZo2sFV5BD2xOmnMFVk=">AAACbHicbVHLbhMxFPVMeZTwaHgsQBWSRSYolaow0xboBqmIDVI3jSBtpThEHseTWPFjat9BjaxZ8Yfs+AQ2fAPOY1FaruR7j849V74+zkspHKTpryjeuHX7zt3Ne437Dx4+2mo+fnLqTGUZ7zMjjT3PqeNSaN4HAZKfl5ZTlUt+ls8+Lfpn37l1wuivMC/5UNGJFoVgFAI1av5oJ0RRmDIq/ce6c7zb28EfMAF+Cf6LKUDRy5pIXkCHlFOqwShPbCX5YD8tYegPQq7925B6x98ImBK/Ie7Cgh+PZnVNrJhMYSchpNFOeqEEkKx0V2TJqNlKu+ky8E2QrUELreNk1PxJxoZVimtgkjo3yJbLUAuCSV43SOV4SdmMTvggQE0Vd0O/NKvG7cCMcWFsOBrwkr064alybq7yoFwY4673FuT/eoMKisOhF7qsgGu2uqioJAaDF87jsbCcgZwHQJkVYVfMptRSBuF/GsGE7PqTb4LTvW72rpv19lpHh2s7NtE2eoU6KEPv0RH6jE5QHzH0O9qKnkcvoj/xs3g7frmSxtF65in6J+LXfwGBHroR</latexit>
K >/pdk
<latexit sha1_base64="5rjicXqAdnEdJFdxF5d1qKWfYLY=">AAACbHicbVHLbhMxFPVMeZTwaHgsQBWSRSYolaow0xboBqmIDVI3jSBtpThEHseTWPFjat9BjaxZ8Yfs+AQ2fAPOY1FaruR7j849V74+zkspHKTpryjeuHX7zt3Ne437Dx4+2mo+fnLqTGUZ7zMjjT3PqeNSaN4HAZKfl5ZTlUt+ls8+Lfpn37l1wuivMC/5UNGJFoVgFAI1av5oJ0RRmDIq/ce6c7zb28EfMAF+Cf6LKUDRy5pIXkCHlFOqwShPbCX5YD8tYegPQq7925B6x98ImBK/Ie7Cgh+PZnVNrJhMYSchpJH0Qg61nax0V2TJqNlKu+ky8E2QrUELreNk1PxJxoZVimtgkjo3yJbLUAuCSV43SOV4SdmMTvggQE0Vd0O/NKvG7cCMcWFsOBrwkr064alybq7yoFwY4673FuT/eoMKisOhF7qsgGu2uqioJAaDF87jsbCcgZwHQJkVYVfMptRSBuF/GsGE7PqTb4LTvW72rpv19lpHh2s7NtE2eoU6KEPv0RH6jE5QHzH0O9qKnkcvoj/xs3g7frmSxtF65in6J+LXfwGCCboR</latexit>Q
<latexit sha1_base64="9edzzRL4uMWnIOw+vcyPEh06C+k=">AAAEanicvVNba9swFHaTbOu8dWs72Bh9EasDCWvSpLu+DFoCYaUhJOtlgSgNsqwkIrLlSsdtgzHsN+5tv2Av+xGT44z1xh4nsHT86Tu37yA3FFxDrfZjKZcv3Lv/YPmh/ejxypOnq2vrJ1pGirJjKoVUPZdoJnjAjoGDYL1QMeK7gn11p430/us5U5rL4AhmIRv4ZBzwEacEDDRcy30rOtgnMKFExHtJ6WCrW0afEAZ2CfGhHIFPLhMs2AhKOJyQAKQfYxUJ1n9TC2EQvzV7Er8zW/fgFIMM0TbWZwpibzhNEqz4eAJlB2P7WhrMg/mf68ZfktO4jYH7TKN2kjG75shcJgTig2QeeBsDiZw/YDcDnetxh3FWeK+R/K9Wrqa80ZbxWzTmDc8Sx24oqXWlIc+J4iSgDO0BsCCdAyr1Gntlu3jIxKhC/qInRF+QgCMGiIhq2baLLWnSog4BOkH7ATBFaEZtdfZNAOe1UaTJmFdpSnVBlIfaDC6kmqJSs9k2hBaZMdWWyreLrpB0WvE4GcvAxDQV2EUehBEgkFMWaBOtldVvYvYa/AiJ1HkLXYGdA2SaRncNM1XNMeQu+gfFyDJc3axVa/OFbhv1hbFpLVZnuPode5JGvhGJCqJ1vz4fH1HAqWCJjSPNQkKnZMz6xgyISTWI508lQUWDeGgklfkCQHP0qkdMfK1nvmuYacH65l0K3nXXj2D0cRDP9WMBzRKNImG0ROm7Qx5XjIKYGYNQxU2tiE5IOj7zOm0jQv1my7eNk51q/X213t3Z3N1ayLFsbVivrJJVtz5Yu9Znq2MdWzT3M7+Sf55/kf9VWC+8LGxk1NzSwueZdW0VnN/Qhmtm</latexit>
Cross-Covariance Attention (XCA)
<latexit sha1_base64="zU2dNxLFyIwFvh8H/IjCuhUNVes=">AAAC33ictVJLbxMxEPYur7I8GuDIxSKJlEoo7JZXL0hFvSD10gjSRorDyut4Eytee7FnUSNrL1w4gBBX/hY3/ghnnE0OoeXKSB5/+uYbzXjGWSmFhTj+FYRXrl67fmPnZnTr9p27u61790+trgzjQ6alNqOMWi6F4kMQIPmoNJwWmeRn2eJoFT/7yI0VWr2DZcknBZ0pkQtGwVNp63e3QwoKc0ale133jh8P9vArTICfg3urcyjoeU0kz6FHyjlVoAtHTCX5+GlcwsQ98752z70bHL8noEv8hNgPBtw0XdQ1MWI2h70OIVG3M/BXA9bCLV0n2u4hdevqo6P6//WTttpxP24MXwbJBrTRxk7S1k8y1awquAImqbXjpKlHDQgmeR2RyvKSsgWd8bGHihbcTlyznxp3PTPFuTb+KMANu53haGHtssi8cjUHezG2Iv8VG1eQH0ycUGUFXLF1obySGDReLRtPheEM5NIDyozwvWI2p4Yy8F8i8kNILj75Mjjd7ycv+slgv314sBnHDnqIHqEeStBLdIjeoBM0RCwgwafgS/A1pOHn8Fv4fS0Ng03OA/SXhT/+AEZZ55A=</latexit>
AXC(K, Q) = Softmax
AXC 2
Rdk⇥ dq
<latexit sha1_base64="LAiCoX4t7SbGeC9gI1mbitiJdNY=">AAADUnictVJLbxMxEPYmPEookMKRi0VSKZVQmi2vXpCKekGKVDVA2khxuvI63qwV76P2LGpk+TciIS78EC4cAGc3SCSFIyN5/PmbtzVhLoWGXu+rV6vfuHnr9tadxt3te/cfNHcenumsUIwPWSYzNQqp5lKkfAgCJB/litMklPw8nB8v7ecfudIiSz/AIueThM5SEQlGwVHBjhfvtklCIWZUmje203862MOvMQF+BeZ9FkFCryyRPIIOyWOaQpYYogrJx896OUzMc6eteeHUoH9BIMvxPtGXCsw0mFtLlJjFsNcmpLFWhoi0fIWheWcvzAkBkXCNT2zlOXBXFRJTMH1bJt4nQIv2b3JQke31vIGpGh8d2/84yr8qbkzlwlZzTYNL2w6arV63Vwq+DvwVaKGVnAbNz2SasSLhKTBJtR77ZZtUgWCS2wYpNM8pm9MZHzuYUldqYsqVsHjXMVMcZcqdFHDJ/hlhaKL1Igmd57JlvWlbkn+zjQuIDidGpHkBPGVVoaiQGDK83C88FYozkAsHKFPC9YpZTBVl4Law4T7B3xz5Ojg76Povu/7goHV0uPqOLfQYPUEd5KNX6Ai9RadoiJj3yfvm/fB+1r7Uvte9er1yrXmrmEdoTerbvwD5khJ1</latexit>
 
 
 
 
 
K
<latexit sha1_base64="PNIauBE2MLCA7j7XkFwTzTjbG/0=">AAAESXicvVNLb9NAEHaTAsU82sKRy4omUiL6SMrzgtQqUkSVKEqgj0jZ1FqvN8kqa6+7O24bWf57XLhx4z9w4QBCnFjbQfQljqzk3fE3M9+8NG4ouIZa7ctCobh46/adpbv2vfsPHi6vrD461DJSlB1QKaTqu0QzwQN2ABwE64eKEd8V7MidNlL90SlTmstgH2YhG/pkHPARpwQM5KwWnHIJ+wQmlIh4N6m01ntV9BZhYOcQf5Aj8Ml5ggUbQQWHExKA9GOsIsEGz2shDOMX5k7il+bqtY4xyBBtYX2iIPacaZJgxccTqJYwti+FwTzI/lw3fp8cxx0M3GcadZLcsmee3GVCIG4lGfEWBhKV/oC9HCxd5nXiPPF+I/lfpVwMeaUs4zcvzHNOEpNqQ0mtNxrylChOAsrQLgAL0kGgSr+xW7XtclsaWtQlQCdoLwCmCM317e5e1UR+ZmiajHkbTanOiPJQh8GZVFNUaTY7xqBNZkx1pPLtsisknW54nIxlYDhNALvMgzACBHLKAm3Y2nl+hrPf4PtIpM7r6C9caiFTE7ppVmlTSsa2h/5hYqp2VtZqm7XsoOtCfS6sWfPTdVY+Y0/SyDeNoYJoPahn0yEKOBUssXGkWUjolIzZwIgBMaGGcbYJCSobxEMjqcwXAMrQix4x8bWe+a6xTBPWV3UpeJNuEMHozTDO2scCmgcaRcK0EqVrhTyuGAUxMwKhiptcEZ2QdHpm+WzThPrVkq8Lh9ub9Veb9d722s76vB1L1hPrqVWx6tZra8d6Z3WtA4sWPha+Fr4XfhQ/Fb8VfxZ/5aaFhbnPY+vSWSz+Bh0uZPw=</latexit>
<latexit sha1_base64="mzzZzEa2n4o+HNrlONK9hYkgqHE=">AAAC73ictVJLbxMxEPYuFEp4NIUjF4tspFRC6W559YJU1AtSLo0gbaQ4rBzHm7XiXW/tWdTI2j/BhQMIceXvcOPf4N3k0AdXRvL48zczms9jzwopDIThH8+/dXvrzt3te637Dx4+2mnvPj41qtSMj5iSSo9n1HApcj4CAZKPC81pNpP8bLY8ruNnn7k2QuUfYVXwaUYXuUgEo+CoeNfb6gYko5AyKu27qjd4PtzDbzEBfgH2g0ogoxcVkTyBHilSmoPKLNGl5JMXYQFT+9L5yr5ybjj4REAVeJ+Ycw12Hi+rimixSGEvIKTVDYZucyAgKQU7qJrsfQK0DOrg1eMlSbFdixkfV/9PXtzuhP2wMXwTRBvQQRs7idu/yVyxMuM5MEmNmURNP6pBMMmrFikNLyhb0gWfOJjTjJupbd6rwl3HzHGitFs54Ia9XGFpZswqm7nMeg7meqwm/xWblJAcTq3IixJ4ztaNklJiULh+fDwXmjOQKwco08JpxSylmjJwX6TlhhBdv/JNcHrQj173o+FB5+hwM45t9BQ9Qz0UoTfoCL1HJ2iEmCe9L94377t/7n/1f/g/16m+t6l5gq6Y/+svFvnrxg==</latexit> ˆK >/⌧
 
<latexit sha1_base64="2F7+uxwf9NhMKztOTe242WTCH2g=">AAAC8HictVJLbxMxEPYuUMryaApHLhZJpFRC6W559YJU1AtSLl1B2khxWHkdb9aK94E9ixpZ+yu4cAChXvtzuPFvcHZzoC1XRvL40zczms8zjkspNPj+b8e9dfvO1t3te979Bw8f7XR2H5/qolKMj1khCzWJqeZS5HwMAiSflIrTLJb8LF4er+NnX7jSosg/wqrks4wucpEIRsFS0a6z1e+RjELKqDTv6sHoebiH32IC/BzMhyKBjJ7XRPIEBqRMaQ5FZoiqJJ++8EuYmZfW1+aVdeHoE4GixPtEf1Zg5tGyrokSixT2eoR4/V5orwaQlIIZ1U36PgFa9byWC1uu513RFJlWzeS4/n/6ok7XH/qN4Zsg2IAu2thJ1PlF5gWrMp4Dk1TradD0owoEk7z2SKV5SdmSLvjUwpxmXM9Ms7Aa9y0zx0mh7MkBN+zfFYZmWq+y2Gau56Cvx9bkv2LTCpLDmRF5WQHPWdsoqSSGAq+3j+dCcQZyZQFlSlitmKVUUQb2j3h2CMH1J98EpwfD4PUwCA+6R4ebcWyjp+gZGqAAvUFH6D06QWPEnMz56nx3frjK/eb+dC/aVNfZ1DxBV8y9/APgy+yM</latexit> ˆQ>
RN dk , Q
⇥
RN 2 2
 
 
 
 
 
 
 
 
 
 
 
  dq
⇥
Figure 1: Our XCiT layer consists of three main blocks, each preceded by LayerNorm and followed by a residual connection: (i) the core cross-covariance attention (XCA) operation, (ii) the local patch interaction (LPI) module, and (iii) a feed-forward network (FFN). By transposing the query-key interaction, the computational complexity of XCA is linear in the number of data elements N , rather than quadratic as in conventional self-attention. tutes the explicit full pairwise interaction between tokens by self-attention among features, where the attention map is derived from the cross-covariance matrix computed over the key and query projections of the token features. Importantly, XCA has a linear complexity in the number of patches.
To construct our Cross-Covariance Image Transformers (XCiT), we combine XCA with local patch interaction modules that rely on efﬁcient depth-wise convolutions and point-wise feedforward net-works commonly used in transformers, see Figure 1. XCA can be regarded as a form of a dynamic 1 1 convolution, which multiplies all tokens with the same data-dependent weight matrix. We
ﬁnd that the performance of our XCA layer can be further improved by applying it on blocks of channels, rather than directly mixing all channels together. This “block-diagonal” shape of XCA further reduces the computational complexity with a factor linear in the number of blocks.
⇥
Given its linear complexity in the number of tokens, XCiT can efﬁciently process images with more than thousand pixels in each dimension. Notably, our experiments show that XCiT does not compromise the accuracy and achieves similar results to DeiT [64] and CaiT [67] in comparable settings. Moreover, for dense prediction tasks such as object detection and image segmentation, our models outperform popular ResNet [28] backbones as well as the recent transformer-based models [44, 70, 81]. Finally, we also successfully apply XCiT to the self-supervised feature learning using DINO [12], and demonstrate improved performance compared to a DeiT-based backbone [64].
Overall, we summarize our contributions as follows:
• We introduce cross-covariance attention (XCA), which provides a “transposed” alternative to conventional self-attention, attending over channels instead of tokens. Its complexity is linear in the number of tokens, allowing for efﬁcient processing of high-resolution images, see Figure 2.
• XCA attends to a ﬁxed number of channels, irrespective of the number of tokens. As a result, our models are signiﬁcantly more robust to changes in image resolution at test time, and are therefore more amenable to process variable-size images.
• For image classiﬁcation, we demonstrate that our models are on par with state-of-the-art vision transformers for multiple model sizes using a simple columnar architecture, i.e., in which we keep the resolution constant across layers. In particular, our XCiT-L24 model achieves 86.0% top-1 accuracy on ImageNet, outperforming its CaiT-M24 [67] and NFNet-F2 [10] counterparts with comparable numbers of parameters.
• For dense prediction tasks with high-resolution images, our models outperform ResNet and multiple transformer-based backbones. On the COCO benchmark, we achieve a strong performance of 48.5% and 43.7% mAP for object detection and instance segmentation respectively. Moreover, we report 48.4% mIoU for semantic segmentation on the ADE20k benchmark, outperforming the state-of-the-art Swin Transformer [44] backbones across all comparable model sizes.
• Finally, our XCiT model is highly effective in self-supervised learning setups, achieving 80.9% top-1 accuracy on ImageNet-1k using DINO [12]. 2
2