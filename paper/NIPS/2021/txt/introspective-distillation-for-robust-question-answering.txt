Abstract
Question answering (QA) models are well-known to exploit data bias, e.g., the language prior in visual QA and the position bias in reading comprehension. Recent debiasing methods achieve good out-of-distribution (OOD) generalizability with a considerable sacrifice of the in-distribution (ID) performance. Therefore, they are only applicable in domains where the test distribution is known in advance. In this paper, we present a novel debiasing method called Introspective Distillation (IntroD) to make the best of both worlds for QA. Our key technical contribution is to blend the inductive bias of OOD and ID by introspecting whether a training sample fits in the factual ID world or the counterfactual OOD one. Experiments on visual QA datasets VQA v2, VQA-CP, and reading comprehension dataset SQuAD demonstrate that our proposed IntroD maintains the competitive OOD performance compared to other debiasing methods, while sacrificing little or even achieving better ID performance compared to the non-debiasing ones. 1

Introduction
Question answering (QA), which requires machines to answer questions given a context, is one of the most fundamental AI tasks. Popular contexts are vision (e.g., image for VQA [5]) and natural language (e.g., pas-sage for extractive QA [27]). A common observation is that QA models prefer to over-exploit the training bias, which bypasses the context comprehension for a short-cut answer. For example, by only using the linguistic correlations between questions and answers, VQA mod-els can answer most questions correctly [16, 2, 5, 20].
Similarly, extractive QA models may use the spurious positional cues to locate the answer in the passage [22]. As a result, QA models that have already achieved strong in-distribution (ID) performance may inevitably fail in out-of-distribution (OOD) test scenarios, regardless of the scale of training data and models [14, 22, 37].
Figure 1: Recent debiasing methods achieve high OOD accuracy with the sacri-fice of ID accuracy. Our proposed IntroD makes the best of both worlds.
Recently, several debiasing methods aim to close the gap between the ID and OOD performances [6, 11, 7, 25]. However, many of them hold the assumption that the training and test distributions are very different or even reversed, e.g., if there are more “yes” answers in training, there must be more
“no” answers in testing. As a result, these methods encounter a severe performance drop under the
ID evaluation, although they significantly outperform non-debiasing baselines in terms of OOD performance. An interesting observation from Figure 1 is that non-debiasing methods (circles) obtain high ID but low OOD performance, while debiasing methods (squares) achieve high OOD but low
ID performance. This observation motivates us to ask: can we make the best of both worlds? 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: Illustration of our proposed introspection. (a) When ID inductive bias dominates the learning, the student model should listen more to the OOD-aware knowledge. (c) When OOD inductive bias dominates the learning, the student model listens more to the ID-aware knowledge. (b) When learning is fair, the student model listens to both teachers equally. The areas in the “ID” and “OOD” bars represent the proportion of the predicted probability. The areas in the “Ratio” bars represent the proportion of the introspective weights (Eq. (3)).
In this paper, we take a step forward to building robust QA models that achieve strong performances in both ID and ODD evaluations. We point out that if the model is over-exploiting the bias in one world, the performance in the other one would be significantly degraded. Therefore, the “best of both” model should be fair with the inductive bias in either world. To this end, we present a simple yet effective training paradigm—Introspective Distillation (IntroD)—to blend the inductive bias of both worlds fairly. Suppose that we have two expert teacher models: ID-teacher and OOD-teacher, each of which captures the ID or OOD inductive bias and represents the corresponding world. Figure 2 illustrates three cases about how an introspective student learns from the two very different teachers.
Case 1: if ID-bias > OOD-bias, then ID-teacher < OOD-teacher. ID inductive bias dominates the learning, and the student should listen more to OOD-teacher. This case occurs when ID-teacher has a low training loss while OOD-teacher has a high one. As shown in Figure 2 (a), it is hard for QA models to conclude whether the oven is electric or not without additional context. Due to the inductive bias in the training data, i.e., most questions starting with “is” are answered by “yes”,
ID-teacher concludes with over-confidence while OOD-teacher does not.
Case 2: if ID-bias < OOD-bias, then ID-teacher > OOD-teacher. OOD inductive bias dominates the learning, and the student should listen more to ID-teacher. This case occurs when ID-teacher has a high training loss while OOD-teacher has a low one. As shown in Figure 2 (c), there are at least two older men, one in a blue shirt selling fruits and one in a white shirt walking in the crowd.
Therefore, both “blue” and “white” should be correct. However, as most training questions starting with “what color” are labeled by “white” answer, the bias of “OOD should be different from ID” enforces OOD-teacher to downplay “white” unfairly while ID-teacher does not.
Case 3: if ID ≈ OOD, then ID-teacher ≈ OOD-teacher. Learning is fair and the student should listen to both teachers equally. This case occurs when the training losses of the two are close. As shown in
Figure 2 (b), the ID-teacher and OOD-teacher produce similar predictions.
The above introspection can be represented as a blended knowledge of the two teachers, which is distilled to the student model [18]. Yet, an unsolved challenge is how to obtain the “oracle” teachers, especially the OOD-teacher, because the OOD distribution is unseen in training, not mentioning to train a teacher model. Thanks to the recent causality-based approach [25], we can approximate the
OOD-teacher using a causal model that imagines the unseen world by counterfactual reasoning.
Without loss of generality, we take visual QA and extractive QA as case studies. Experiments on
VQA-CP [2], VQA v2 [16], and SQuAD [27] validate the effectiveness of our proposed IntroD.
Interestingly, extensive ablations demonstrate that the success of IntroD is indeed from the causal introspection but not from the simple ensemble. 2