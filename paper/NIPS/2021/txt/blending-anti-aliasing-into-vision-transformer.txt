Abstract
The transformer architectures, based on self-attention mechanism and convolution-free design, recently found superior performance and booming applications in computer vision. However, the discontinuous patch-wise tokenization process implicitly introduces jagged artifacts into attention maps, arising the traditional problem of aliasing for vision transformers. Aliasing effect occurs when discrete patterns are used to produce high frequency or continuous information, resulting in the indistinguishable distortions. Recent researches have found that modern convo-lution networks still suffer from this phenomenon. In this work, we analyze the uncharted problem of aliasing in vision transformer and explore to incorporate anti-aliasing properties. Speciﬁcally, we propose a plug-and-play Aliasing-Reduction
Module (ARM) to alleviate the aforementioned issue. We investigate the effective-ness and generalization of the proposed method across multiple tasks and various vision transformer families. This lightweight design consistently attains a clear boost over several famous structures. Furthermore, our module also improves data efﬁciency and robustness of vision transformers1. 1

Introduction
Transformers have led to impressive breakthroughs in language understanding, dominating a wide range of natural language processing (NLP) tasks. Meanwhile, continuous researches strive to leverage transformers for vision tasks, revolutionizing the conventional inductive bias in convolutional neural networks (CNNs). After a big leap made by vision transformer (ViT) [1], promising results on vision tasks have recently emerged [2–4], demonstrating a possibility of using transformers as a primary backbone for vision applications.
While appealing advantages such as long-range context modeling and parameter efﬁciency are introduced by self-attention, this mechanism also brings an inevitable aliasing issue to vision transformers. Aliasing [5] traditionally refers to the phenomenon that high-frequency signals become indistinguishable when undersampled [6]. This effect occurs when discrete patterns are utilized to capture a more continuous signal, resulting in frequency ambiguity. In terms of ViT [1], images are split into non-overlapping patches during tokenization, which are then fed into transformer blocks.
Compare to the more “continuous” representation of image, tokenization and self-attention performed on its discontinuous patch embeddings can be regarded as subsampling operations. While alleviating computational costs, these operations leave a side-effect which leads to possible aliasing.
A simple solution to mitigate aliasing phenomenon is to increase the sampling rate. Similar properties emerge in vision transformers, where overlapped tokens [7] and smaller patch sizes [1, 8] lead to improved performance. As increased sampling rates lead to quadratic computation costs, we hereby conjecture that proper anti-aliasing ﬁlters that integrated into the “attending” process could also provide a ﬁx. A potential concern is that the general purpose of anti-aliasing: to introduce
∗Work done during an internship at Amazon. 1Code is made available at https://github.com/amazon-research/anti-aliasing-transformer. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
“smoothness” to signals, might be contradictory with the goal of self-attention: to capture more signiﬁcant and high-frequency features in contrast. How would traditional anti-aliasing techniques inﬂuence vision transformers? And what if we equip these state-of-the-art transformer architectures with suitable anti-aliasing options?
Figure 1: Visualization of attention maps from pre-trained DeiT-B [9]. The attention maps are visualized by averaging heads in the selected block. The masks are obtained by thresholding the self-attention maps following
DINO [8]. The second row shows the results after applying our anti-aliasing module to the ﬁrst transformer block. The earlier aliasing effect on the ﬁsh tail region makes the subsequent layers focus on the incorrect high frequency and make the wrong prediction. In contrast, the employed anti-aliasing maintains the overall smoothness, leading follow-up layers to capture more continuous features along the real semantic contour.
To this end, we design the Aliasing Reduction Module (ARM) for vision transformers, which consists of an anti-aliasing ﬁlter and an external modulation module. Extensive investigations are conducted primarily, including the ﬁltering choices such as the basic Gaussian blurring ﬁlters in [10] and learnable convolutional ﬁlters, as well as different placements of these ﬁlters. Notably, based on the observations that the “jagged” phenomenon varies across images and locations, we propose an adaptive ﬁltering method which draws inspirations from dictionary learning [11]. Speciﬁcally, a lightweight aliasing estimation network is constructed to predict the combination coefﬁcients of the pre-deﬁned low pass ﬁlter bank adaptively. Furthermore, the position of integrating our Aliasing
Reduction Module in highly-modularized transformers is crucial. The model beneﬁts from an early anti-aliasing operation right after the self-attention computation, which is distinctive from the observations in CNNs [10, 12]. As elaborated in Figure 1, the module brings perceptually smoother attention, which leads to better feature localization and eventually correct prediction. By integrating
ARM to state-of-the-art architectures, we consistently ﬁnd a considerable margin of improvement.
To summarize, our contributions are:
• We investigate anti-aliasing techniques for vision transformers, and propose the Aliasing
Reduction Module (ARM), which is compatible with most existing architectures.
• We tailor various anti-aliasing strategies and different placements of aliasing reduction, which are distinctive from the observations in CNN structures.
• Experiments show that our simple yet effective design boosts sophisticated state-of-the-art vision transformers. Furthermore, we observe stronger generalization, robustness, and data-efﬁciency brought by our anti-aliasing strategy. 2