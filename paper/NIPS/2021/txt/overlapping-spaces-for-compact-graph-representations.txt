Abstract
Various non-trivial spaces are becoming popular for embedding structured data such as graphs, texts, or images. Following spherical and hyperbolic spaces, more general product spaces have been proposed. However, searching for the best con-ﬁguration of a product space is a resource-intensive procedure, which reduces the practical applicability of the idea. We generalize the concept of product space and introduce an overlapping space that does not have the conﬁguration search problem.
The main idea is to allow subsets of coordinates to be shared between spaces of different types (Euclidean, hyperbolic, spherical). As a result, we often need fewer coordinates to store the objects. Additionally, we propose an optimization algo-rithm that automatically learns the optimal conﬁguration. Our experiments conﬁrm that overlapping spaces outperform the competitors in graph embedding tasks with different evaluation metrics. We also perform an empirical analysis in a realistic information retrieval setup, where we compare all spaces by incorporating them into DSSM. In this case, the proposed overlapping space consistently achieves nearly optimal results without any conﬁguration tuning. This allows for reducing training time, which can be essential in large-scale applications. 1

Introduction
Building vector representations of various objects is one of the central tasks of machine learning.
Word embeddings such as Glove [22] and Word2Vec [19] are widely used in natural language processing; a similar Prod2Vec [7] approach is used in recommendation systems. There are many algorithms proposed for graph embeddings, e.g., Node2Vec [8] and DeepWalk [23]. Recommendation systems often construct embeddings of a bipartite graph that describes interactions between users and items [10].
For a long time, embeddings were considered exclusively in Rn. However, the hyperbolic space was shown to be more suitable for graph, word, and image representations due to the underlying hierarchical structure [12, 20, 21, 26]. Going beyond spaces of constant curvature, a recent study [9] proposed product spaces, which combine several copies of Euclidean, spherical, and hyperbolic spaces. While these spaces demonstrate promising results, the optimal signature (types of combined spaces and their dimensions) has to be chosen via brute force, which may not be acceptable in large-scale applications.
In this paper, we propose a more general metric space called overlapping space (OS) together with an optimization algorithm that trains signature simultaneously with embedding allowing us to avoid brute-forcing. The main idea is to allow coordinates to be shared between different spaces, which signiﬁcantly reduces the number of coordinates needed. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Importantly, the proposed overlapping space can further be enhanced by adding non-metric approaches such as weighted inner product [13] as additional similarity measures complementing metric ones.
Thus, we obtain a ﬂexible hybrid measure OS-Mixed that is no longer a metric space. Our experiments show that in some cases, non-metric measures outperform metric ones. The proposed OS-Mixed has advantages of both worlds and thus achieves superior performance.
To validate the usefulness of the proposed overlapping space, we provide an extensive empirical evaluation for the task of graph embedding, where we consider both distortion-based (i.e., preserving distances) and ranking-based (i.e., preserving neighbors) objectives. In both cases, the proposed measure outperforms the competitors. We also compare the spaces in information retrieval and recommendation tasks, for which we apply them to train embeddings via DSSM [11]. Our method works comparable to the best product spaces tested in these cases, while it does not require brute-forcing for the best signature. Thus, using the overlapping space may signiﬁcantly reduce the training time, which can be crucial in large-scale applications. 2