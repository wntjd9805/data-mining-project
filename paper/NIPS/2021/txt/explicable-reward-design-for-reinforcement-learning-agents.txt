Abstract
We study the design of explicable reward functions for a reinforcement learning agent while guaranteeing that an optimal policy induced by the function belongs to a set of target policies. By being explicable, we seek to capture two properties: (a) informativeness so that the rewards speed up the agent’s convergence, and (b) sparseness as a proxy for ease of interpretability of the rewards. The key challenge is that higher informativeness typically requires dense rewards for many learning tasks, and existing techniques do not allow one to balance these two properties appropriately.
In this paper, we investigate the problem from the perspective of discrete optimization and introduce a novel framework, EXPRD, to design explicable reward functions. EXPRD builds upon an informativeness criterion that captures the (sub-)optimality of target policies at different time horizons in terms of actions taken from any given starting state. We provide a mathematical analysis of
EXPRD, and show its connections to existing reward design techniques, including potential-based reward shaping. Experimental results on two navigation tasks demonstrate the effectiveness of EXPRD in designing explicable reward functions. 1

Introduction
A reward function plays the central role during the learning/training process of a reinforcement learning (RL) agent. Given a “task” the agent is expected to perform (i.e., the desired learning outcome), there are typically many different reward speciﬁcations under which an optimal policy has the same performance guarantees on the task. This freedom in choosing the reward function, in turn, leads to the fundamental question of reward design: What are different criteria that one should consider in designing a reward function for the agent, apart from the agent’s ﬁnal output policy? [1–3].
One of the important criteria is informativeness, capturing that the rewards should speed up the agent’s convergence [1–6]. For instance, a major challenge faced by an RL agent is because of delayed rewards during training; in the worst-case, the agent’s convergence is slowed down exponentially w.r.t. the time horizon of delay [7]. In this case, we seek to design a new reward function that reduces this time horizon of delay while guaranteeing that any optimal policy induced by the designed function is also optimal under the original reward function [3]. The classical technique of potential-based reward shaping (when applied with appropriate state potentials) indeed allows us to reduce this time horizon of delay to 1; see [3, 8] and Section 2. With 1, it means that globally optimal actions for any state are also myopically optimal, thereby making the agent’s learning process trivial.
While informativeness is an important criterion, it is not the only criterion to consider when designing rewards for many practical applications. Another natural criterion to consider is sparseness as a proxy for ease of interpretability of the rewards. There are several practical settings where sparseness and interpretability of rewards are important, as discussed next. The ﬁrst motivating application is when 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
rewards are designed for human learners who are learning to perform sequential tasks, for instance, in pedagogical applications such as educational games [9], virtual reality-based training simulators [10, 11], and solving open-ended problems (e.g., block-based visual programming [12]). In this context, tasks can be challenging for novice learners and a teacher agent can assist these learners by designing explicable rewards associated with these tasks. The second motivating application is when rewards are designed for complex compositional tasks in the robotics domain that involve reward speciﬁcations in terms of logic, automata, or subgoals [13, 14]—these speciﬁcations induce a form of sparsity structure on the underlying reward function. The third motivating application is related to defense against reward-poisoning attacks in RL (see [15–19]) by designing structured and sparse reward functions that are easy to debug/verify. Beyond these practical settings, many naturally occurring reward functions in real-life tasks are inherently sparse and interpretable, further motivating the need to distill these proper-ties in the automated reward design process. The key challenge is that higher informativeness typically requires dense rewards for many learning tasks – for instance, the above-mentioned potential-based shaped rewards that achieve a time horizon of 1 would require most of the states be associated with some real-valued reward (see Sections 2 and 4). To this end, an important research question that we seek to address is: How to balance these two criteria of informativeness and sparseness in the reward design process while guaranteeing an optimality criterion on policies induced by the reward function?
In this paper, we formalize the problem of designing explicable reward functions, focusing on the criteria of informativeness and sparseness. We investigate this problem from an expert/teacher’s point of view who has full domain knowledge (in this case, an original reward function along with optimal policies induced by the original function), and seeks to design a new reward function for the agent— see Figure 1 and further discussion in Section 5 on expert-driven vs. agent-driven reward design. We tackle the problem from the perspective of discrete optimization and introduce a novel framework,
EXPRD, to design reward functions. EXPRD allows us to appropriately balance informativeness and sparseness while guaranteeing that an optimal policy induced by the function belongs to a set of target policies. EXPRD builds upon an informativeness criterion that captures the (sub-)optimality of target policies at different time horizons from any given starting state. Our main contributions are:1
I. We formulate the problem of explicable reward functions to balance the two important criteria of informativeness and sparseness in the reward design process. (Sections 2 and 3.1)
II. We propose a novel optimization framework, EXPRD, to design reward functions. As part of this framework, we introduce a new criterion capturing informativeness of reward functions that is amenable to optimization techniques and is of independent interest. (Sections 3.2 and 3.3)
III. We provide a detailed mathematical analysis of EXPRD and show its connections to popular techniques, including potential-based reward shaping. (Sections 3.3 and 3.4)
IV. We provide a practical extension to apply our framework to large state spaces. We perform extensive experiments on two navigation tasks to demonstrate the effectiveness of EXPRD in designing explicable reward functions. (Sections 3.5 and 4) 2 Problem Setup
Environment. An environment is deﬁned as a Markov Decision Process (MDP) M := (S, A, T, γ, R), where the set of states and actions are denoted by S and A respectively. T :
S × S × A → [0, 1] captures the state transition dynamics, i.e., T (s(cid:48) | s, a) denotes the probability of landing in state s(cid:48) by taking action a from state s. Here, γ is the discounting factor. The underlying reward function is given by R : S × A → [−Rmax, Rmax], for some Rmax > 0. We interchangeably represent the reward function by a vector R ∈ R|S|·|A|, whose (s |A| + a)-th entry is given by
R (s, a). We deﬁne the support of R as supp(R) := {s : s ∈ S, R (s, a) (cid:54)= 0 for some a ∈ A}, and the (cid:96)0-norm of R as (cid:107)R(cid:107)0 := |supp(R)|.
Preliminaries and deﬁnitions. We denote a stochastic policy π : S → ∆ (A) as a mapping from a state to a probability distribution over actions, and a deterministic policy π : S → A as a mapping from a state to an action. For any policy π, the state value function V π
∞ and the action value function
Qπ t=0 γtR(st, at)|s0 = s, T, π] and Qπ t=0 γtrt|s0 = s, a0 = a, T, π]. Further, the optimal value functions are given by V ∗
∞ (s, a). There always exists a
∞ in the MDP M are deﬁned as follows respectively: V π
∞ (s, a) = E [(cid:80)∞
∞ (s, a) = supπ Qπ
∞ (s) = supπ V π
∞ (s) = E [(cid:80)∞
∞ (s) and Q∗ 1Github repo: https://github.com/adishs/neurips2021_explicable-reward-design_code. 2
Task speciﬁed as an
MDP M with given reward function R
Teacher computes
∗ optimal Q
∞ and Π w.r.t. R
∗
Teacher designs a new explicable reward function (cid:98)R
RL agent learns an optimal (cid:98)π∗ ∈ (cid:98)Π∗ w.r.t. (cid:98)R (a) (b) (c) (d)
Figure 1: Illustration of the explicable reward design problem in terms of a task speciﬁed through MDP
M , an RL agent whose objective is to perform this task, and a teacher/expert whose objective is to help this RL agent. (a) MDP M with a given reward function R specifying the task the RL agent is expected
∗ to perform; (b) The teacher computes optimal action value function Q
∞ along with the set of optimal policies Π w.r.t. R; (c) The teacher designs a new explicable reward function (cid:98)R for the RL agent; (d) The RL agent trains using the designed reward (cid:98)R and outputs a policy (cid:98)π∗ from the set of optimal policies (cid:98)Π∗ w.r.t. (cid:98)R. Our framework designs an explicable reward function (cid:98)R with three properties: invariance, informativeness, and sparseness; see main text for formal deﬁnitions of these properties.
∗ deterministic stationary policy π that achieves the optimal value function simultaneously for all s ∈ S [7, 20], and we denote all such deterministic optimal policies by the set Π∗ :=
{π : S → A s.t. V π
∞ (s) , ∀s ∈ S}. From here onwards, we focus on deterministic policies unless stated otherwise. For any π and R, we deﬁne the following quantities that capture the ∞-step (global) optimality gap and the 0-step (myopic) optimality gap of action a at state s, respectively:
∞ (s) = V ∗
∞(s) − Q∗
∞(s, a) = V ∗ 0 (s, a) := Qπ
∞(s, a), and δπ 0 (s, π(s)) − Qπ
∞(s, π(s)) − Qπ
∞(s, a) = 0, ∀s ∈ S, a ∈ Π∗ s. 0 (s, a) = R (s, a) is the 0-step action value function of policy π. The δπ 0 (s, a), ∀s ∈ S, a ∈ A,
∞(s, a) values
∞(s, a); however, this is not 0 (s, a) values in general. For any state s ∈ S and a set of policies Π, we deﬁne
δπ
∞(s, a) := Qπ where Qπ are same for all π ∈ Π∗, and we denote it by δ∗ the case with δπ
Πs := {a : a = π(s), π ∈ Π}. Then, we have that δ∗
Explicable reward design. Figure 1 presents an illustration of the explicable reward design problem that we formalize below. A task is speciﬁed as an MDP M with a given goal-based reward function R where R has non-zero rewards only on goal states G ⊆ S, i.e., R (s, a) = 0, ∀s ∈ S\G, a ∈ A. Many naturally occurring tasks (see Section 1 for motivating applications) are goal-based and challenging for learning an optimal policy when the state space S is very large. In this paper, we study the following explicable reward design problem from an expert/teacher’s point of view: Given R and the corresponding optimal policy set Π w.r.t. R as the input, the teacher designs a new reward function (cid:98)R with criteria of informativeness and sparseness while guaranteeing an invariance requirement (these properties are formalized in Section 3). Informally, the invariance requirement is that any induced by R.2 optimal policy learned using the new reward (cid:98)R belongs to the optimal policy set Π
Typical techniques for reward design and issues. Given a set of important states (subgoals) in the environment, one could design a handcrafted reward function (cid:98)RCRAFT by assigning non-zero reward values only to these states. Even though this simple approach produces a reward function with a speciﬁed sparsity level, it often fails to satisfy the invariance requirement. In particular, there are some well-known “reward bugs” that can arise in this approach and mislead the agent into learning sub-optimal policies (see [2, 3]). In the seminal work [3], the authors introduced the potential-based reward shaping (PBRS) method to alleviate this issue. The reward function produced by the PBRS method with optimal value function V
∗
∞ under R as the potential function is deﬁned as follows:
∗
∞ (s(cid:48)) − V
T (s(cid:48) | s, a) · V
∗
∞ (s) . (cid:98)RPBRS (s, a) := R (s, a) + γ (cid:88) (1)
∗
∗ s(cid:48)∈S
The set of optimal policies (cid:98)Π∗ induced by (cid:98)RPBRS is exactly equal to the set of optimal policies Π induced by R since (cid:98)δπ
[3]. In addition, for any state s ∈ S, globally
∗ optimal actions Π
∞(s, a) for all π ∈ Π
[3, 8] – this leads to a dramatic speed-up in the learning process. However, the potential-based reward shaping produces dense reward function which is less interpretable (see Section 4).
∗ s ⊆ A under R are also myopically optimal under (cid:98)RPBRS since (cid:98)δπ
∗
∞(s, a) for all π ∈ Π
∞(s, a) = δ 0 (s, a) = δ
∗
∗
∗ 2In the rest of the paper, the quantities deﬁned corresponding to R := R are denoted by an overline, e.g.,
∗
∞; the quantities deﬁned corresponding to and the ∞-step optimality gaps by δ
∗ the optimal policy set by Π
R := (cid:98)R are denoted by a widehat, e.g., the optimal policy set by (cid:98)Π∗. 3
3 Our Reward Design Framework EXPRD
In Sections 3.1, 3.2, and 3.3, we propose an optimization formulation and a greedy solution for the explicable reward design problem. In Section 3.4, we provide a theoretical analysis of our greedy solution. In Section 3.5, we provide a practical extension to apply our framework to large state spaces. 3.1 Discrete Optimization Formulation
Given R and the corresponding optimal policy set Π tion framework (EXPRD) to design an explicable reward function (cid:98)R (see Figure 1).
, we systematically develop a discrete optimiza-∗
Sparseness, informativeness, and invariance. The sparseness of the reward function (cid:98)R is captured by supp( (cid:98)R). In Section 3.2, we formalize an informativeness criterion I( (cid:98)R) of (cid:98)R that captures how hard/easy it is to learn an optimal behavior induced by (cid:98)R. We explicitly enforce the invariance
∗ requirement (see Section 2) for the new reward (cid:98)R by choosing a set of candidate policies Π† ⊆ Π
, and satisfying the following (Bellman-optimality) conditions:
Qπ†
∞ (s, a) = (cid:98)R(s, a) + γ (cid:88)
T (s(cid:48)|s, a) · Qπ†
∞ (s(cid:48), π†(s(cid:48))),
∀a ∈ A, s ∈ S, π† ∈ Π† (C.1)
Qπ†
∞ (s, π†(s)) ≥ Qπ†
∞ (s, a) + δ s(cid:48)∈S
∗
∞(s),
∀a ∈ A\Π
∗ s, s ∈ S, π† ∈ Π†, (C.2) s
∗
δ
∗
∞(s) := mina∈A\Π∗
∗
∞(s, a), ∀s ∈ S.3 The above conditions guarantee that any optimal where δ
∗ policy induced by (cid:98)R is also optimal under R, i.e., Π† ⊆ (cid:98)Π∗ ⊆ Π is used to reduce the number of constraints. Note that for the potential-based shaped reward (cid:98)RPBRS, we have (cid:98)Π∗ = Π
Maximizing informativeness for a given set of important states. When a domain expert provides us a set of important states (subgoals) in the environment [21–24], we want to use this set in a principled way to design a reward (cid:98)R, while avoiding the “reward bugs” that can arise from hand-crafted rewards (cid:98)RCRAFT. To this end, for any given set of subgoals Z ⊆ S\G, we optimize the informativeness criterion I(R) while satisfying the invariance requirement:
. Here, the set Π† ⊆ Π
∗
. g(Z) := max
R:supp(R)⊆Z∪G
I(R) subject to conditions (C.1) − (C.2) with (cid:98)R replaced by R hold
|R (s, a)| ≤ Rmax, ∀s ∈ S, a ∈ A. (P1)
Let R(Z) denote the R that maximizes g(Z). Let R ⊆ R|S|·|A| be a constraint set on R that captures only the conditions (C.1) − (C.2) and the Rmax bound.
Jointly ﬁnding subgoals along with maximizing informativeness. Based on (P1), we propose the following discrete optimization formulation that allows us to select a set of important states (of size
B) and design a reward function that maximizes informativeness automatically: max
Z:Z⊆S\G,|Z|≤B g(Z). (P2)
We can incorporate prior knowledge about the quality of subgoals using a set function D : 2S → R (we assume D to be a submodular function [25]). Finally, the full EXPRD formulation is given by: max
Z:Z⊆S\G,|Z|≤B g(Z) + λ · D(Z ∪ G), for some λ ≥ 0. (P3)
We study the problems (P1), (P2), and (P3) in the following subsections.
∗ 3Note that the true action values Q
∞ are used in the conditions (C.1) − (C.2) to obtain the terms δ
∗
∞(s, a),
∗
∗ s, and Π†. However, when we only have an approximate estimate of Q
A\Π
∞, we can adapt (C.1) − (C.2) appropriately with approximate versions of δ
∗
∗ s, and Π†.
∞(s, a), A\Π 4
3.2
Informativeness Criterion
∗
∞(s, a) = δ
Understanding the informativeness of a reward function is an important problem, and several works have investigated it [4, 5, 26–28]. Our goal is to deﬁne an informativeness criterion that is amenable to optimization techniques. As noted in Section 2, for any policy π ∈ Π
, 0-step and ∞-step optimality gaps induced by (cid:98)RPBRS are all equal to ∞-step optimality gaps induced by R, i.e.,
∗ (cid:98)δπ 0 (s, a) = (cid:98)δπ
∞(s, a). For any reward function R, one could ask how much these two quantities could differ, and even consider the intermediate cases between 0-step and ∞-step optimality.
Inspired by the h-step optimality notions studied in [4, 26], we deﬁne the h-step action value (cid:105) function of any policy π as Qπ
, and it satisﬁes h(s, a) = R(s, a) + γ (cid:80) h−1(s(cid:48), π(s(cid:48))). the following recursive relationship: Qπ
Let H be a set of horizons for which we want to maximize informativeness. For any policy π and reward function R, we deﬁne the following quantity that captures the h-step optimality gap of action a at state s: δπ h(s, a), ∀s ∈ S, a ∈ A, h ∈ H. Later, in the proof of Proposi-h (s, a) = (cid:10)wh;(s,a), R(cid:11) for some vector wh;(s,a) ∈ tion 2, we show that δπ
R|S|·|A|. Interestingly, the following proposition states that, for any policy π ∈ Π and any h, the h-step optimality gap induced by (cid:98)RPBRS given in (1) is equal to the ∞-step optimality gap induced by R:
Proposition 1. The goal-based reward function R, and the potential-based shaped reward function (cid:98)RPBRS given in (1) satisfy the following: (cid:98)δπ t=0 γtR(st, at)|s0 = s, a0 = a, T, π s(cid:48)∈S T (s(cid:48)|s, a) · Qπ
∗
∞(s, a), ∀s ∈ S, a ∈ A, π ∈ Π h (s, a) is linear in R, i.e., δπ h(s, π(s)) − Qπ h (s, a) := Qπ h (s, a) = E h (s, a) = δ
, h ∈ H. (cid:104)(cid:80)h
∗
∗
Let (cid:96) : R → R be a monotonically non-decreasing concave function. Then, based on the h-step optimality gaps, we deﬁne the informativeness criterion of the reward R as follows: (cid:88) (cid:88) (cid:88) (cid:88)
I(cid:96)(R) := (cid:96)(δπ† h (s, a)).
π†∈Π† h∈H s∈S a∈A\Π∗ s
From here onwards, we let I be I(cid:96) in the problem (P1). As an example for (cid:96), we consider the negated
∗ hinge loss given by (cid:96)hg(δ(s, a)) := − max(0, δ
∞(s, a) − δ(s, a)). By Proposition 1, we have that
I(cid:96)hg ( (cid:98)RPBRS) = 0, and I(cid:96)hg (R) ≤ 0 for any other R, i.e., (cid:98)RPBRS achieves the maximum value of I(cid:96)hg . 3.3
Iterative Greedy Algorithm
First, we show that the problem (P1) can be efﬁciently solved using the standard concave optimization methods to ﬁnd R(Z) for any given Z ⊆ S\G:
Proposition 2. For any given Z ⊆ S\G, the problem (P1) is a concave optimization problem in
R ∈ R|S|·|A| with linear constraints. Further, the feasible set of the problem (P1) is non-empty.
Then, inspired by the Forward Stepwise Selection method from [29], we propose an iterative greedy solution (see Algorithm 1) to solve the problems (P2) and (P3). To compute the incremental gain at each step, we would need to solve the concave optimization problem (P1) for different values of Z.
The problem (P1) has |S| · |A| optimization variables and O(|S| · |A| · (cid:12) (cid:12) · |H|) constraints. (cid:12)Π†(cid:12)
Algorithm 1 Iterative Greedy Algorithm for EXPRD 1: Input: MDP M := (cid:0)S, A, T, γ, R(cid:1), δ 2: Initialize: Z0 ← ∅ 3: for k = 1, 2, . . . , B do 4: 5: 6: Output: ZB and the corresponding optimal reward function R(ZB ). zk ← arg maxz∈S\Zk−1
Zk ← Zk−1 ∪ {zk}
∗
∞(s, a) values, sets Π
, Π
∗
† g(Zk−1∪{z})+λ·D(Zk−1∪G ∪{z})−g(Zk−1)−λ·D(Zk−1∪G)
, G, H, sparsity budget B 3.4 Theoretical Analysis
Here, we provide guarantees for the solution returned by our Algorithm 1. Below, we give an overview of the main technical ideas, and leave a detailed discussion along with proofs in the 5
Appendix. For some µ ≥ 0, let I reg criterion. We deﬁne a normalized set function f : 2S → R as follows: (cid:96) (R) := I(cid:96)(R) − µ (cid:107)R(cid:107)2 2 be the regularized informativeness f (Z) = max
R:supp(R)⊆Z∪G,R∈R (I reg (cid:96) (R) − I reg (cid:96) (R(∅))) + λ · (D(Z ∪ G) − D(G)), (2)
B
B and f OPT
B be the set selected by our Algorithm 1 and Z OPT where R(∅) = arg maxR:supp(R)⊆G,R∈R I reg (cid:96) (R). Note that the regularized variant (I(cid:96) replaced by I reg (cid:96) ) of the optimization problem (P3) is equivalent to maxZ:Z⊆S\G,|Z|≤B f (Z). For a given sparsity budget B, let Z Greedy be the optimal set that maximizes the regularized variant of problem (P3). The corresponding f values of these sets are denoted by f Greedy respectively; in the following, we are interested in comparing these two values. The problem (P3) is closely related to the subset selection problem studied in [29] with a twist of an additional constraint set R (see the discussion after (P1)), making the theoretical analysis more challenging. Inspired by the analysis in [29], we need to prove a weak form of submodularity [25, 30] for f (since D is already a submodular function, we need to prove this for the case when λ = 0). To this end, we require the regularized informativeness criterion I reg to (cid:96) satisfy certain structural assumptions. First, we deﬁne the restricted strongly concavity and restricted smoothness notions of a function that are used in our analysis.
Deﬁnition 1 (Restricted Strong Concavity, Restricted Smoothness [31]). A function L : R|S|·|A| → R is said to be restricted strong concave with parameter mΩ and restricted smooth with parameter MΩ on a domain Ω ⊂ R|S|·|A| × R|S|·|A| if for all (x, y) ∈ Ω:
B mΩ 2 (cid:107)y − x(cid:107)2
− 2 ≥ L (y) − L (x) − (cid:104)∇L (x), y − x(cid:105) ≥ −
For any integer k, we deﬁne the following two sets: Ωk := {(x, y) : (cid:107)x(cid:107)0 ≤ k, (cid:107)y(cid:107)0 ≤ k, (cid:107)x − y(cid:107)0 ≤ k, x, y ∈ R}, and ˜Ωk := {(x, y) : (cid:107)x(cid:107)0 ≤ k, (cid:107)y(cid:107)0 ≤ k, (cid:107)x − y(cid:107)0 ≤ 1, x, y ∈ R}. Let mk := mΩk and Mk := MΩk (similarly we deﬁne ˜mk and ˜Mk). (cid:107)y − x(cid:107)2 2 .
MΩ 2
When there is no R ∈ R constraint in (2), informativeness criterion is sufﬁcient to prove the weak submodularity of f [29]:
Assumption 1. The regularized informativeness criterion I reg (cid:96) and M2B+|G|-restricted smooth on Ω2B+|G|. the following assumption on the regularized is m2B+|G|-restricted strongly concave
However, due to the additional R ∈ R constraint, we need to enforce further requirements on I reg (cid:96) formally captured in Assumption 2 provided in the Appendix; here, we discuss these requirements informally. Let Z be any set such that Z ⊆ S\G, and ∇I reg (cid:96) (R(Z)) be the gradient of the regularized informativeness criterion at the optimal reward R(Z). Then, we need to ensure the following: (i) the (cid:96)2-norm of the projection of ∇I reg (cid:96) (R(Z)) on (Z ∪ G) is upper-bounded, captured by dopt max; (ii) the (cid:96)2-norm of the projection of ∇I reg (cid:96) (R(Z)) on any j ∈ S\(Z ∪ G) is lower-bounded, captured by dnon min; and (iii) the components of the optimal reward R(Z) outside (Z ∪ G) do not lie in the boundary of R, captured by κ. Then, by using Assumption 1 and Assumption 2 (see Appendix), we prove the weak submodularity of f . Finally, by applying Theorem 3 from [29], we obtain the following theorem: satisﬁes Assumption 1 and Assumption 2 requirements. Then, we have f Greedy
Theorem 1. Let I reg (cid:96) (1 − e−γ) f OPT (dnon
≥
B
·
, where γ = κ·m2B+|G|
M2B+|G|
B min)2
+(dnon min)2 . (dopt max)2
We provide Assumption 2 and a detailed proof of the theorem in the Appendix. 3.5 Extension to Large State Spaces using State Abstractions
This section presents an extension of our EXPRD framework that is scalable to large state spaces by leveraging the techniques from state abstraction literature [32–34]. We use an abstraction
φ : S → Xφ, which is a mapping from high-dimensional state space S to a low-dimensional latent space Xφ. Let φ−1(x) := {s ∈ S : φ(s) = x} , ∀x ∈ Xφ, and M := (cid:0)S, A, T, γ, R(cid:1). We propose the following pipeline: 1. By using M and φ, we construct an abstract MDP M φ = (cid:0)Xφ, A, Tφ, γ, Rφ (cid:1) as follows, s(cid:48)∈φ−1(x(cid:48)) T (s(cid:48)|s, a), and Rφ(x, a) = (cid:80)
∀x, x(cid:48) ∈ Xφ, a ∈ A: Tφ(x(cid:48)|x, a) = 1
|φ−1(x)| s∈φ−1(x) R(s, a). We compute the set of optimal policies Π 1
|φ−1(x)| s∈φ−1(x) (cid:80) (cid:80)
∗
φ for the MDP M φ. 6
2. We run our EXPRD framework on M φ with Π† = Π 3. We deﬁne the reward function (cid:98)R on the state space S as follows: (cid:98)R(s, a) = (cid:98)Rφ(φ(s), a).
∗
φ, and the resulting reward is denoted (cid:98)Rφ.
By assuming certain structural conditions on φ formalized in the Appendix, we can show that any opti-mal policy induced by the above reward (cid:98)R acts nearly optimal w.r.t. R. This pipeline can be extended to continuous state space as well, similar to [34–36]. We provide more details in the Appendix. 4 Experimental Evaluation
In this section, we evaluate EXPRD on two environments: ROOMSNAVENV (Section 4.1) and
LINEKEYNAVENV (Section 4.2). ROOMSNAVENV corresponds to a navigation task in a grid-world where the agent has to learn a policy to quickly reach the goal location in one of four rooms, starting from an initial location. Even though this environment has a small state space, it provides a very rich and an intuitive problem setting to validate different reward design techniques, and variants of
ROOMSNAVENV have been used extensively in the literature [14, 21, 22, 37–40]. LINEKEYNAVENV corresponds to a navigation task in a one-dimensional space where the agent has to ﬁrst pick the key and then reach the goal. The agent’s location in this environment is represented as a point on a line segment. Given the large state space representation, it is computationally challenging to apply the reward design technique from Section 3.3 and we use the state abstraction-based extension of our framework from Section 3.5. This environment is inspired by variants of navigation tasks in the literature where an agent needs to perform sub-tasks [3, 41]. We give an overview of main results here, and provide a more detailed description of the setup and additional results in the Appendix. 4.1 Evaluation on ROOMSNAVENV
ROOMSNAVENV (Figure 2). We represent the environment as an MDP with S states each corresponding to cells in the grid-world indicating the agent’s current location (shown as “blue-circle”). Goal (shown as “green-star”) is located at the top-right corner cell. The agent can take four actions given by A :=
{“up”, “left”, “down”, “right”}. An action takes the agent to the neighbouring cell represented by the direction of the action; how-ever, if there is a wall (shown as “brown-segment”), the agent stays at the current location. Furthermore, when an agent takes an action a ∈ A, there is prand probability that an action a(cid:48) ∈ A \ {a} will be executed instead of a. In addition to these walls, there are a few terminal walls (shown as “thick-red-segment”) that terminates the episode—at the bottom-left corner cell, “left” and “down” actions terminate; at the top-right corner cell, “right” action terminates. The agent gets a reward of Rmax after it has navigated to the goal and then takes a “right” action (i.e., only one state-action pair has a reward); note that this action also terminates the episode. The reward is 0 for all other state-action pairs and there is a discount factor γ.
This MDP has |S| = 49 and |A| = 4; we set prand = 0.1, Rmax = 10, and γ = 0.95 in our evaluation.
Figure 2: ROOMSNAVENV
Techniques evaluated. We consider the following baselines: (i) (cid:98)RORIG := R, which simply represents default reward function, (ii) (cid:98)RPBRS obtained via the PBRS technique with the optimal
∗ value function V
∞ w.r.t. R (see Section 2), (iii) (cid:98)RCRAFT that we design manually (see Section 2 and description below), and (iv) (cid:98)RPBRS-CRAFT(B=5) obtained via the PBRS technique with the optimal
∗
∞ [42].4 To design (cid:98)RCRAFT, we ﬁrst hand-crafted a set value function w.r.t. (cid:98)RCRAFT instead of V function D that assigns scores to the states in the MDP, e.g., the scores are higher for the four entry points in the rooms. In general, one could learn such D automatically using the techniques from [21–24]—see full details about D in the Appendix. Then, for a ﬁxed budget B, we pick the top B states according to the scoring by D and assign a reward of +1 for optimal actions and −1 for others. For the evaluation, we use B = 5 and denote the function as (cid:98)RCRAFT(B=5). Note that apart from B states, (cid:98)RCRAFT(B=5) also has a reward assigned for the goal state taken from R. 4The reward shaping method in [42] is based on the PBRS technique and leads to dense reward functions.
However, their method is more practical as it does not require solving the original task w.r.t. R. 7
(a) Convergence (b) (cid:98)RORIG (c) (cid:98)RPBRS (d) (cid:98)REXPRD(B=5,λ=0)
Figure 3: Results for ROOMSNAVENV. (a) shows convergence in performance of the agent w.r.t. training episodes. Here, performance is measured as the expected reward per episode computed using
R; note that the x-axis is exponential in scale. (b-d) visualize the designed reward functions (cid:98)RORIG, (cid:98)RPBRS, and (cid:98)REXPRD(B=5,λ=0). These plots illustrate reward values for all combinations of S × A shown as four 7×7 grids corresponding to different actions. Blue color represents positive reward, red color represents negative reward, and the magnitude of the reward is indicated by color intensity. As an example, consider “right” action grid for (cid:98)RORIG in (b) where the dark blue color in the corner indicates the goal. To increase the color contrast, we clipped rewards in the range [−4, +4] for this visualization even though the designed rewards are in the range [−10, +10]. See Section 4.1 for details.
The reward functions (cid:98)REXPRD designed by our EXPRD framework are parameterized by budget B and hyperparameter λ. For λ, we consider two extreme settings: (a) λ = 0 where the problem (P3) reduces to (P2), and (b) λ → ∞ where the problem (P3) reduces to (P1) corresponding to the reward design with subgoals pre-selected by the function D. We use the same function D that we used for (cid:98)RCRAFT above. For budget B, we consider values from {3, 5, |S|}. In particular, we evaluate the following reward functions: (cid:98)REXPRD(B=5,λ→∞), (cid:98)REXPRD(B=3,λ=0), (cid:98)REXPRD(B=5,λ=0), and (cid:98)REXPRD(B=|S|,λ=0).
For the evaluation in this section, we use the following parameter choices for EXPRD: H =
{1, 4, 8, 16, 32}, (cid:96) is the negated hinge loss (cid:96)hg, and Π† contains only one policy from Π
Results. We use standard Q-learning method for the agent with a learning rate 0.5 and exploration factor 0.1 [7]. During training, the agent receives rewards based on (cid:98)R, however, is evaluated based on R. A training episode ends when the maximum steps (set to 50) is reached or an agent’s action terminates the episode. All the results are reported as average over 40 runs and convergence plots show mean with standard error bars. The convergence behavior in Figure 3a demonstrates the effectiveness of the reward functions designed by our EXPRD framework.5 Note that (cid:98)RCRAFT(B=5) leads to sub-optimal behavior due to “reward bugs” (see Section 2), whereas (cid:98)REXPRD(B=5,λ→∞)
ﬁxes this issue using the same set of subgoals. EXPRD leads to good performance even without domain knowledge (i.e., when λ = 0), e.g., the performance corresponding to (cid:98)REXPRD(B=3,λ=0) is comparable to that of (cid:98)REXPRD(B=5,λ→∞). The visualizations of (cid:98)RORIG, (cid:98)RPBRS, and (cid:98)REXPRD(B=5,λ=0) in Figures 3b, 3c, and 3d highlight the trade-offs in terms of sparseness and interpretability of the reward functions. The reward function (cid:98)REXPRD(B=5,λ=0) designed by our EXPRD framework provides a good balance in terms of convergence performance while maintaining high sparseness.
Additional visualizations and results are provided in the Appendix.
∗
. 4.2 Evaluation on LINEKEYNAVENV
LINEKEYNAVENV (Figure 4). We represent the environment as an
MDP with S states corresponding to the agent’s status comprising of the current location (shown as “blue-circle” and is a point x in [0, 1]) and a binary ﬂag whether the agent has acquired a key (shown as
“cyan-bolt”). Goal (shown as “green-star”) is available in locations on the segment [0.9, 1], and the key is available in locations on the segment [0.1, 0.2]. The agent can take three actions given by
A := {“left”, “right”, , “pick”}. “pick” action does not change the agent’s location, however, when
Figure 4: LINEKEYNAVENV 5As we discussed in Sections 1 and 2, (cid:98)RPBRS designed using V
∗
∞ makes the agent’s learning process trivial. 8
(a) Convergence (b) (cid:98)RORIG (c) (cid:98)RPBRS (d) (cid:98)REXPRD(B=5,λ=0)
Figure 5: Results for LINEKEYNAVENV. (a) shows convergence in performance of the agent w.r.t. training episodes. Here, performance is measured as the expected reward per episode computed using R. (b-d) visualize the designed reward functions (cid:98)RORIG, (cid:98)RPBRS, and (cid:98)REXPRD(B=5,λ=0). These plots illustrate reward values for all combination of triplets, i.e., agent’s location on the segment
[0.0, 1.0] (shown as horizontal bar), agent’s status whether it has acquired key or not (indicated as
‘K’ or ‘-’), and three actions (indicated as ‘l’ for “left”, ‘r’ for “right”, ‘p’ for “pick”). We use a color representation similar to Figure 3, and we clipped rewards in the range [−3, +3] to increase the color contrast for this visualization. As an example, consider ‘rK’ bar for (cid:98)RORIG in (b) where the dark blue color on the segment [0.9, 1] indicate the locations with goal. See Section 4.2 for details. executed in locations with availability of the key, the agent acquires the key; if agent already had a key, the action does not affect the status. A move action of “left” or “right” takes the agent from the current location in the direction of move with the dynamics of the ﬁnal location captured by two hyperparameters (∆a,1, ∆a,2); for instance, with current location x and action “left”, the new location x(cid:48) is sampled uniformly among locations from (x − ∆a,1 − ∆a,2) to (x − ∆a,1 + ∆a,2).
Similar to ROOMSNAVENV, the agent’s move action is not applied if the new location crosses the wall, and there is prand probability of a random action. The agent gets a reward of Rmax after it has navigated to the goal locations after acquiring the key and then takes a “right” action; note that this action also terminates the episode. The reward is 0 elsewhere and there is a discount factor γ. We set prand = 0.1, Rmax = 10, γ = 0.95, ∆a,1 = 0.075 and ∆a,2 = 0.01.
Techniques evaluated. The baseline (cid:98)RORIG := R represents the default reward function. We evaluate the variants of (cid:98)RPBRS and (cid:98)REXPRD using an abstraction. For a given hyperparameter α ∈ (0, 1), the set of possible locations X are obtained by α-level discretization of the line segment from 0.0 to 1.0, leading to a 1/α set of locations. For the abstraction φ associated with this discretization [43], the abstract MDP M φ (see Section 3.5) has |Xφ| = 2/α and |A| = 3. We use α = 0.05. We compute the optimal state value function in the abstract MDP M φ, lift it to the original state space via φ, and use the lifted value function as the potential to design (cid:98)RPBRS [35]. We follow the pipeline in Section 3.5 to design (cid:98)REXPRD – in the subroutine, we run EXPRD on M φ for a budget B = 5 and a full budget B =
|Xφ|; we set λ = 0. For other parameters (H, (cid:96), and Π†), we use the same choices as in Section 4.1.
Results. The agent uses Q-learning method in the original MDP M by using a ﬁne-grained dis-cretization of the state space; rest of the method’s parameters are same as in Section 4.1. All the results are reported as average over 40 runs and convergence plots show mean with standard error bars. Figure 5a demonstrates that all three designed reward functions— (cid:98)RPBRS, (cid:98)REXPRD(B=5,λ=0), (cid:98)REXPRD(B=|Xφ|,λ=0)—substantially improves the convergence, whereas the agent is not able to learn under (cid:98)RORIG. Based on the visualizations in Figures 5b, 5c, and 5d, (cid:98)REXPRD(B=5,λ=0) provides a good balance between convergence and sparseness. Interestingly, (cid:98)REXPRD(B=5,λ=0) assigned a high posi-tive reward for the “pick” action when the agent is in the locations with key (see ‘p-’ bar in Figure 5d). 5