Abstract
While safe reinforcement learning (RL) holds great promise for many practical ap-plications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such speciﬁcations demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we ﬁrst introduce HAZARD-WORLD, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints speciﬁed in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HAZARDWORLD, we show that our method achieves higher rewards (up to 11x) and fewer constraint violations (by 1.8x) compared to existing approaches.
However, in terms of absolute performance, HAZARDWORLD still poses signiﬁcant challenges for agents to learn efﬁciently, motivating the need for future work.1 1

Introduction
Although reinforcement learning (RL) has shown promise in several simulated domains such as games [1, 2, 3] and autonomous navigation [4, 5], deploying RL in real-world scenarios remains challenging [6]. In particular, real-world RL requires ensuring the safety of the agent and its sur-roundings, which means accounting for constraints during training that are orthogonal to maximizing rewards. For example, a cleaning robot must be careful to not knock the television over, even if the television lies on the optimal path to cleaning the house.
Safe RL tackles these challenges with algorithms that maximize rewards while simultaneously minimizing constraint violations during exploration [7, 8, 9, 10, 11, 12, 13, 14, 15, 16]. However, these algorithms have two key limitations that prevent their widespread use. First, they require us
∗ Equal contribution. 1Code and data are available at https://github.com/princeton-nlp/SRL-NLC 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to provide constraints in mathematical or logical forms, which calls for speciﬁc domain expertise.
Second, a policy trained with a speciﬁc set of constraints cannot be transferred easily to learn new tasks with the same set of constraints, since current approaches do not maintain an explicit notion of constraints separate from reward-maximizing policies. This means one would have to retrain the policy (with constraints) from scratch.
We consider the use of natural language to spec-ify constraints (which are orthogonal to rewards) on learning. Human languages provide an intu-itive and easily-accessible medium for describ-ing constraints–not just for machine learning ex-perts or system developers, but also for potential end users interacting with agents such as house-hold robots. Consider the environment in Fig. 1 for example. Instead of expressing a constraint as (cid:80)T t=0 1st∈lava · 1(cid:54)∃st(cid:48) ∈water, t(cid:48)∈[0,1,...,t−1] = 0, one could simply say “Do not visit the lava be-fore visiting the water”. The challenge of course, lies in training the RL agent to accurately inter-pret and adhere to the textual constraints as it learns a policy for the task.
Figure 1: Learning to navigate with language constraints. The ﬁgure shows (1) a third-person view of the environment (red dotted square box), (2) three types of language constraints, (3) items which provide rewards when collected. During safety training, the agent learns to interpret textual constraints while learning the task (i.e., collect re-wards). During safety evaluation, the agent learns a new task with different rewards while following the constraints and minimizing violations.
To study this problem, we ﬁrst create HAZ-ARDWORLD, a collection of grid-world and robotics environments for safe RL with textual constraints (Fig. 1). HAZARDWORLD consists of separate ‘safety training’ and ‘safety evalu-ation’ sets, with disjoint sets of reward func-tions and textual constraints between training and evaluation. To do well on HAZARDWORLD, an agent has to learn to interpret textual con-straints during safety training and safely adhere to any provided constraints while picking up new tasks during the safety evaluation phase. Built on existing RL software frameworks [17, 18], HAZ-ARDWORLD consists of navigation and object collection tasks with diverse, crowdsourced, free-form text specifying three kinds of constraints: (1) budgetary constraints that limit the frequency of being in unsafe states, (2) relational constraints that specify unsafe states in relation to surrounding entities, and (3) sequential constraints that activate certain states to be unsafe based on past events (e.g.,
“Make sure you don’t walk on water after walking on grass”). Our setup differs from instruction following [19, 20, 21, 22, 23, 24] in two ways. First, instructions specify what to do, while textual constraints only inform the agent on what not to do, independent of maximizing rewards. Second, learning textual constraints is a means for ensuring safe exploration while adapting to a new reward function.
In order to demonstrate learning under this setting, we develop Policy Optimization with Language
COnstraints (POLCO), where we disentangle the representation learning for textual constraints from policy learning. Our model ﬁrst uses a constraint interpreter to encode language constraints into representations of forbidden states. Next, a policy network operates on these representations and state observations to produce actions. Factorizing the model in this manner allows the agent to retain its constraint comprehension capabilities while modifying its policy network to learn new tasks.
Experiments demonstrate that our approach achieves higher rewards (up to 11x) while maintaining lower constraint violations (up to 1.8x) compared to several baselines on two different domains within
HAZARDWORLD. Nevertheless, HAZARDWORLD remains far from being solved, especially in tasks with high-dimensional observations, complex textual constraints and those requiring high-level planning or memory-based systems. 2
2