Abstract
Unsupervised domain adaptation, as a prevalent transfer learning setting, spans many real-world applications. With the increasing representational power and applicability of neural networks, state-of-the-art domain adaptation methods make use of deep architectures to map the input features X to a latent representation
Z that has the same marginal distribution across domains. This has been shown to be insufficient for generating optimal representation for classification, and to find conditionally invariant representations, usually strong assumptions are needed. We provide reasoning why when the supports of the source and target data from overlap, any map of X that is fixed across domains may not be suitable for domain adaptation via invariant features. Furthermore, we develop an efficient technique in which the optimal map from X to Z also takes domain-specific information as input, in addition to the features X. By using the property of minimal changes of causal mechanisms across domains, our model also takes into account the domain-specific information to ensure that the latent representation Z does not discard valuable information about Y . We demonstrate the efficacy of our method via synthetic and real-world data experiments. The code is available at: https://github.com/DMIRLAB-Group/DSAN. 1

Introduction k , yS )mS k=1 and xT
Unsupervised domain adaptation (UDA) is a common setting for supervised learning, in which the labeled training and unlabeled test data come from different distributions. More formally, given features X ∈ Rd and labels Y ∈ R, we observe labeled source and unlabeled target domain instances, represented by (xS , yS ) = (xS k=1 respectively, where P S (X, Y ) ̸=
P T (X, Y ), and mS and mT are the number of observations in the source and target domains, respectively. The main challenge of domain adaptation is to use the given source domain observations for learning a predictor that will perform well in the target domain. To do so, the procedure needs to make use of some similarities between the two domains. One way to formalize this is by making assumptions about how the joint distribution joint P (X, Y ) changes across domains. For example, in the well-studied setting of covariate shift [31, 44, 20, 35, 2, 8], the marginal distribution P (X) changes while the conditional distribution P (Y |X) (i.e. the optimal predictor) is shared across domains. k = (xT )mT
However, in many real-world applications, P (Y |X) can also change, and this requires making use of further assumptions. One such assumption is that the factorization P (X, Y ) = P (Y )P (X|Y ) allows for addressing the changes in P (Y ) and P (X|Y ) independently in a situation where their
∗These authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
respective changes are simple and easier to capture [48, 30]. In this particular setting, the problem is generally broken down into: (1) Target shift: P (Y ) changes across domains while P (X|Y ) stay the same [34, 21, 29, 48] (2) Conditional shift: P (X|Y ) changes across domains but P (Y ) stays the same [48, 26, 24]. (3) Conditional-target shift: Both P (X|Y ) and P (Y ) change independently across domains - the most general setting under this generating process assumption [48, 6]. Then, assumptions about the changes of the factors of the joint distrubution can be made so that the problem is solvable, such as location-scale transformation [48, 26], or that the changing parameters lie on a low-dimensional manifold [33], and algorithms can be designed to enforce these constraints and make use of them for prediction in the target domain.
Another fruitful view of the problem is through the lens of representation learning, due to wide-spread applicability of neural architectures for many real-world problems. In particular, state of the art deep learning techniques harness the high representational capacity of neural networks to transform the input data into a latent feature representation which is predictive of the target variable Y in the source domain, and has the same distribution across domains. Formally, this means learning a function (encoder) ϕ : X → Z from the input space to a latent space Z, such that P S(Z) = P T (Z).
At the same time, a function h : Z → Y can be learnt to minimize the risk in the labeled source domain [1]. The hope is then, that the overall function g := h ◦ ϕ will have low prediction risk in the target domain. However, the above-described theoretical and methodological framework does not guarantee that the learnt representation Z will have any relevant information for predicting Y in the target domain. Namely, one can easily have a situation in which the learnt representation Z is marginally invariant (P S(Z) = P T (Z)), but not conditionally invariant (P S(Z|Y ) ̸= P T (Z|Y )), as discussed in [53]. This means that the learnt function g := h ◦ ϕ can have very good prediction performance in the source domain, but generalize very poorly to the target domain. the same encoding function ϕ(X) across
In many methods, domains is used to learn invariant latent representations. This enjoys computational benefits and makes the learning proce-dure relatively simple, and the vast majority of approaches ([12, 22, 25, 23, 17] among many) employ this technique. However, in certain situations, the same encoding function across domains cannot learn a marginally invariant representation that is optimal for classification in the target domain. There are certain studies ([37, 38, 4]) which implement domain-specific encoders ϕS and ϕT .
Unfortunately, such methods suffer from the following drawbacks: (1) the exact motivation behind having separate encoders for each domain is not clear; (2) including two separate encoders may be inefficient because it greatly increases the number of parameters that we need to learn; (3) in the field of UDA based on invariant representation learning, there is still no principled way to guarantee that the learned marginally invariant representation Z has sufficient structural (semantic) information, and has the potential to be conditionally invariant.
Figure 1: The underlying data-generating process under condi-tional shift, of the observed vari-ables Y and X, and the latent vari-able Z. θX represents the chang-ing parameters of P (X|Y ) across domains.
In this paper we assume the setting of conditional shift, and we make use of the data-generating process to: (i) justify the use of two separate encoding functions in order to infer the latent representation, (ii) implement the two encoding functions more efficiently, and (iii) constrain the latent representation Z to have meaningful structure which is useful for prediction in the target domain. In Section 2, we first motivate the use of two separate encoders to infer Z, via rigorous treatment and an illustrative example. Subsequently, in Chapter 3, we introduce an efficient way to implement two separate functions for inferring Z. In Chapter 3 we shall also introduce a principled way to ensure that the latent representation Z contains useful information for prediction, and finally, in Chapter 4 we provide empirical evaluation. 1.1