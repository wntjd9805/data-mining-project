Abstract
The ﬁeld of Deep Learning is rich with empirical evidence of human-like perfor-mance on a variety of prediction tasks. However, despite these successes, the recent
Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020 competition
[1] suggests that there is a need for more robust and efﬁcient measures of network generalization. In this work, we propose a new framework for evaluating the generalization capabilities of trained networks. We use perturbation response (PR) curves that capture the accuracy change of a given network as a function of varying levels of training sample perturbation. From these PR curves, we derive novel statistics that capture generalization capability. Speciﬁcally, we introduce two new measures for accurately predicting generalization gaps: the Gi-score and Pal-score, which are inspired by the Gini coefﬁcient and Palma ratio (measures of income in-equality), that accurately predict generalization gaps. Using our framework applied to intra and inter-class sample mixup, we attain better predictive scores than the current state-of-the-art measures on a majority of tasks in the PGDL competition.
In addition, we show that our framework and the proposed statistics can be used to capture to what extent a trained network is invariant to a given parametric input transformation, such as rotation or translation. Therefore, these generalization gap prediction statistics also provide a useful means for selecting optimal network architectures and hyperparameters that are invariant to a certain perturbation. 1

Introduction
Neural networks have produced state-of-the-art and human-like performance across a variety of tasks.
This rapid progress has led to wider-spread adoption and deployment. Given their prevalence and increasing applications, it is important to estimate how well a trained net will generalize. Additionally, speciﬁc tasks often require models to be invariant to certain transformations or perturbations of the data. This can be achieved either through data augmentation that changes the underlying statistics of the training sets or through inductive architectural biases, such as translation invariance that is inherent in convolutional neural networks. It is important as well to understand how and when a network has been able to learn task-dependent invariances.
Various attempts at bounding and predicting neural network generalization are well summarized and analyzed in the recent survey [2]. While both theoretical and empirical progress has been made, there remains a gap in the literature for an efﬁcient and intuitive measure that can predict generalization given a trained network and its corresponding data post hoc. Aiming to ﬁll this gap, the recent
Predicting Generalization in Deep Learning (PGDL) NeurIPS 2020 competition [1] encouraged participants to provide complexity measures that would take into account network weights and training data to predict generalization gaps, i.e., the difference between training and test set accuracy.
In this work, we propose a new framework that presents progress towards this goal. Our methodology consists of ﬁrst building an estimate of how the accuracy of a network changes as a function of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
varying levels of perturbation present in training samples. To do so, we evaluate a trained network’s accuracy on a subset of the training dataset that has been perturbed to some degree. Using multiple observations of accuracy vs. perturbation magnitude, we develop a perturbation response (PR) curve for each model. From the PR curves, we derive two new measures called the Gi-score and the Pal-score, which compare a given network’s PR curve to that of an idealized network that is unaffected by all perturbation magnitudes. When applying our framework to inter and intra class
Mixup [3] perturbations, we are able to achieve better generalization prediction scores on a majority of the tasks than the current state-of-the-art proposal from the PGDL competition. Because our framework can be applied to any parametric perturbation, we also demonstrate how it can be used to predict the degree to which a network has learned to be invariant to a given perturbation. 2