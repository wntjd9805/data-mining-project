Abstract
Inspired by human intelligence, the research on online continual learning aims to push the limits of the machine learning models to constantly learn from sequentially encountered tasks, with the data from each task being observed in an online fashion.
Though recent studies have achieved remarkable progress in improving the online continual learning performance empowered by the deep neural networks-based models, many of today’s approaches still suffer a lot from catastrophic forgetting, a persistent challenge for continual learning. In this paper, we present a novel method which attempts to mitigate catastrophic forgetting in online continual learning from a new perspective, i.e., neuron calibration. In particular, we model the neurons in the deep neural networks-based models as calibrated units under a general formulation. Then we formalize a learning framework to effectively train the calibrated model, where neuron calibration could give ubiquitous beneﬁt to balance the stability and plasticity of online continual learning algorithms through inﬂuencing both their forward inference path and backward optimization path.
Our proposed formulation for neuron calibration is lightweight and applicable to general feed-forward neural networks-based models. We perform extensive experiments to evaluate our method on four benchmark continual learning datasets.
The results show that neuron calibration plays a vital role in improving online continual learning performance and our method could substantially improve the state-of-the-art performance on all the evaluated datasets. 1

Introduction
While humans and animals exhibit remarkable ability to deal with new tasks by effectively adapting their acquired knowledge without forgetting the previously learned skills, in stark contrast, it is challenging for artiﬁcial learning systems to effectively deal with continuously inquiring tasks.
When switching to a new task from some previously learned ones, there is sometimes a signiﬁcant drop in performance, where such a phenomenon is also referred to as catastrophic forgetting. To overcome such an issue, the research on continual learning has emerged, which deﬁnes the learning protocol for the scenarios when the training data of different tasks procedurally arrives in a sequential order [20, 23, 28, 35]. The catastrophic forgetting problem is rooted in the model training process, as the gradients to train the model parameters encode great amount of information about the dynamically shifting data distribution seen by the model, which could keep changing throughout the entire continual learning period.
In this paper, we consider a challenging task which poses a more restrictive requirement on the conventional continual learning setting, i.e., online continual learning [1]. Inspired by many real life scenarios, online continual learning requires the data for each training task to appear as a (one-pass) stream of samples. To tackle such challenges, we focus on the replay-based approach [19, 34], which 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
grants the model with limited access to the data from past tasks for the rehearsal of past experience.
To mitigate the catastrophic forgetting issue, such methods leverage the replay memory to consolidate the past knowledge in various forms, such as raw data [1] and past gradients [27, 32], to facilitate effective transfer of knowledge. Though the research community has also started to pay attention on a number of non replay-based directions to tackle the continual learning problems, such as leveraging energy-based models [14] and neuron modulation approaches [15], the replay-based methods still play a pivot role in continual learning with state-of-the-art performance in many challenging scenarios.
However, despite of the promise in their performance, the replay-based attempts easily lead to a data-imbalance issue, which is also formally referred to as the stability-plasticity dilemma [21]. In particular, high plasticity refers to the case when old experience is drastically forgotten, and high stability refers to the case that too much attention has been paid on stabilizing the previously learned knowledge so that the learning hinders the acquisition of knowledge on the new tasks. Lose of balance between plasticity and stability would deteriorate the performance of continual learning.
The existing replay-based approaches have taken into account of different perspectives of the model training process to remedy the stability-plasticity dilemma, such as regularizing the parameter change during training [4, 12, 23], selective memory storage or replay [1, 11], Bayesian and variational
Bayesian training [4, 12, 22], and task-speciﬁc parameterization of the model [25, 36]. In this paper, we tackle the problem from a novel angle that is distinct to all the aforementioned attempts, i.e., seeking a better balance between stability and plasticity with neuron calibration. Speciﬁcally, we refer to neuron calibration as a process of mathematically adjusting the transformation functions in various layers of deep neural networks. Considering that besides the task setting, catastrophic forgetting in contemporary deep learning-based continual learning models is also closely related to the vulnerability of deep neural networks, our proposed neuron calibration approach aims to regularize the parameter update against catastrophic forgetting via posing a trainable soft mask on the parameters, which then inﬂuences both the model inference process and the model training process through the forward inference path and the backward optimization path. Our work is inspired by the earlier works that seek optimal model generalization through calibrating the neural networks parameters or labels [9, 10], as well as a recent continual learning work that learns task-speciﬁc calibrations for retaining task-speciﬁc parameters without memory rehearsal [36]. Compared to those works, the neuron calibration approach we propose shares an enjoyable property of being task-agnostic in terms of parameter sharing among the tasks. That is, instead of reserving task-speciﬁc parameters for preserving the task knowledge against forgetting, we train a shared calibrated model where we interleave data from different task distributions to effectively optimize the model.
The contributions of our work are three-fold: (i) we introduce a general and light-weight neuron calibration approach to tackle task-incremental online continual learning problems where the models are formulated as feed-forward deep neural networks-based function approximations; (ii) we for-mulate a novel task-incremental learning paradigm to train the calibrated model with an interleaved optimization scheme to achieve a better balance between stability and plasticity; (iii) we show through extensive empirical experiments that our proposed method could outperform the state-of-the-art con-tinual learning algorithms as well as the related layer calibration approach with signiﬁcant margins on all the evaluation datasets. 2