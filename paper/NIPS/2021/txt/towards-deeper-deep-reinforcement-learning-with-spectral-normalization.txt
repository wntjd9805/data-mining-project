Abstract
In computer vision and natural language processing, innovations in model architec-ture that increase model capacity have reliably translated into gains in performance.
In stark contrast with this trend, state-of-the-art reinforcement learning (RL) algo-rithms often use small MLPs, and gains in performance typically originate from algorithmic innovations. It is natural to hypothesize that small datasets in RL necessitate simple models to avoid overﬁtting; however, this hypothesis is untested.
In this paper we investigate how RL agents are affected by exchanging the small
MLPs with larger modern networks with skip connections and normalization, fo-cusing speciﬁcally on actor-critic algorithms. We empirically verify that naïvely adopting such architectures leads to instabilities and poor performance, likely contributing to the popularity of simple models in practice. However, we show that dataset size is not the limiting factor, and instead argue that instability from taking gradients through the critic is the culprit. We demonstrate that spectral normalization (SN) can mitigate this issue and enable stable training with large modern architectures. After smoothing with SN, larger models yield signiﬁcant performance improvements — suggesting that more “easy” gains may be had by focusing on model architectures in addition to algorithmic innovations. 1

Introduction
In computer vision and natural language processing (NLP), competitive models are growing increas-ingly large, and researchers now train billion-parameter models [13, 36]. The earliest neural networks were often shallow [40] with performance dropping for excessively deep models [29]. However, ever since the introduction of batch normalization [31] and residual connections [29], performance has improved more or less monotonically with model scale [64]. As a result, competitive models in computer vision and NLP are growing ever larger [10, 56], and further architectural innovations are continuously researched [14].
In stark contrast with this trend, state-of-the-art (SOTA) reinforcement learning (RL) agents often rely on small feedforward networks [35, 37, 39] and performance gains typically originate from algorithmic innovations such as novel loss functions [58, 62, 68] rather than increasing model capacity.
Indeed, a recent large-scale study has shown that large networks can harm performance in RL [2].
It is natural to suspect that high-capacity models might overﬁt in the low-sample regime common in RL evaluation. Imagenet contains over a million unique images whereas RL is often evaluated in contexts with fewer environment samples [37, 39]. However, to date, this overﬁtting hypothesis remains largely untested.
To address this, we study the effects of using larger modern architectures in RL. By modern archi-tectures we mean networks with high capacity, facilitated by normalization layers [6, 31] and skip connections [29]. We thus depart from the trend in RL of treating networks as black-box function
∗Correspondence to: Johan Bjorck <njb225@cornell.edu> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
approximators. To limit the scope and compute requirements we focus on continuous control from pixels [65] with two actor-critic agents based on the Soft Actor-Critic (SAC) [25, 35] and Deep
Deterministic Policy Gradients (DDPG) [43, 71] algorithms. Actor-critic methods form the basis of many SOTA algorithms for continuous control [35, 38, 39]. As can be expected, we demonstrate that naïvely adopting modern architectures leads to poor performance, supporting the idea that RL does not beneﬁt from larger models. However, we show that the issue is not necessarily overﬁtting, but instead, that training becomes unstable with deeper modern networks. We hypothesize that taking the gradient of the actor through the critic network creates exploding gradients [53] for deeper networks.
We connect this setup with generative adversarial networks (GANs) [22], and propose to use a simple smoothing technique from the GAN literature to stabilize training: spectral normalization [47].
We demonstrate that this simple strategy allows the training of larger modern networks in RL without instability. With these ﬁxes, we can improve upon state-of-the-art RL agents on competitive continu-ous control benchmarks, demonstrating that improvements in network architecture can dramatically affect performance in RL. We also provide performance experiments showing that such scaling can be relatively cheap in terms of memory and compute time in RL from pixels. Our work suggests that model scaling is complementary to algorithmic innovations and that this simple strategy should not be overlooked. We summarize our contributions as follows:
• We verify empirically that large modern networks fail for two competitive actor-critic agents.
We demonstrate dramatic instabilities during training, which casts doubt on overﬁtting being responsible.
• We argue that taking the gradients through the critic is the cause of this instability. To combat this problem we propose to adopt spectral normalization [47] from the GAN literature.
• We demonstrate that this simple smoothing method enables the use of large modern networks and leads to signiﬁcant improvements for SOTA methods on hard continuous control tasks.
We further provide evidence that this strategy is computationally cheap in RL from pixels. 2