Abstract
Large scale training requires massive parallelism to ﬁnish the training within a reasonable amount of time. To support massive parallelism, large batch training is the key enabler but often at the cost of generalization performance. Existing works explore adaptive batching or hand-tuned static large batching, in order to strike a balance between the computational efﬁciency and the performance. However, these methods can provide only coarse-grained adaption (e.g., at a epoch level) due to the intrinsic expensive calculation or hand tuning requirements. In this paper, we propose a fully automated and lightweight adaptive batching methodology to enable
ﬁne-grained batch size adaption (e.g., at a mini-batch level) that can achieve state-of-the-art performance with record breaking batch sizes. The core component of our method is a lightweight yet efﬁcient representation of the critical gradient noise information. We open-source the proposed methodology by providing a plugin tool that supports mainstream machine learning frameworks. Extensive evaluations on popular benchmarks (e.g., CIFAR10, ImageNet, and BERT-Large) demonstrate that the proposed methodology outperforms state-of-the-art methodologies using adaptive batching approaches or hand-tuned static strategies in both performance and batch size. Particularly, we achieve a new state-of-the-art batch size of 78k in
BERT-Large pretraining with SQuAD score 90.69 compared to 90.58 reported in previous state-of-the-art with 59k batch size.

Introduction 1
Recent years have witnessed the success of deep learning in a variety of applications, such as computer vision [1] and natural language processing [2, 3]. To push the performance boundaries, deep learning models in many applications have become increasingly complicated, and training such models requires large amounts of data as well as high cost. Taking GPT3 [3] (an autoregressive language model that uses deep learning to produce human-like text) as an example, the state-of-the-art model has 175 billion parameters and is trained on 300 billion data, which costs $12 million for a single training run [4]. Therefore, it is of critical importance to study scalable and efﬁcient training for large scale machine learning [5, 6].
The core idea of large scale machine learning is to parallelize model training across vast amounts of computing resources to speed up the training process. However, the overhead of the model synchronization due to massive parallelism in training can be the bottleneck for large scale training.
To reduce the model synchronization overhead, large-batch training is a commonly employed strategy that can reduce the synchronization frequency. Taking BERT training as an example, increasing batch size from 256 to 4k can increase the training speed 2.5x faster on a 16-GPU cluster. OpenAI uses a batch size of 3.2 million to train GPT3 over a supercomputing cluster [3], which accelerates the training speed by approximately over 500x compared to sample-by-sample run on a 16-GPU cluster. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Despite the beneﬁts of large batch size, large batch size may degrade the model generalization performance [7, 8, 9]. To address this issue, existing works [10, 11, 12] demonstrate that a carefully tuned learning rate scheduling can help to mitigate such performance loss caused by large batch size. However, manually tuning learning rate requires signiﬁcant amounts of efforts, which limits its potential use for wide adoption. [13, 14] develop automated learning rate tuning methods for large batch size, which, however, rely on users to set appropriate batch sizes. [15] develops adaptive batching using second order information, aiming to reduce the generalization impact due to large batch sizes. [16] uses the gradient variance information to guide the batch size and learning rate conﬁgurations. Due to the high cost of analyzing the gradient variance and the second order information, existing methods can adapt the batch size only at a coarse-grained level (i.e., at the end of each epoch). It is worth noting that within each epoch, the variance of gradient may change swiftly and signiﬁcantly [17, 18] (also see Section 4.1). Therefore, the course-grained methods cannot ﬁnd a suitable batch size to reduce the performance loss, and thus miss the opportunity to achieve larger batch sizes with less performance impact.
To address the above challenges, we propose a ﬁne-grained batch size adaption approach SimiGrad to optimize the batch size at the iteration level. SimiGrad is driven by a novel gradient similarity mea-surement that can accurately capture the gradient variance information in a lightweight manner. We use the proposed gradient similarity measurement to derive the optimized batch size and learning rate with an automated algorithm. We integrate SimiGrad into mainstream machine learning frameworks by providing an open-sourced plugin tool.
We evaluate SimiGrad via extensive experiments using popular benchmarks, such as CIFAR10,
ImageNet, and BERT-Large. The experimental results demonstrate that SimiGrad can consistently outperform state-of-the-art methods in both the average batch size and the model generalization performance. Particularly, we achieve a new state-of-the-art batch size of 78k in BERT-Large pre-training with the SQuAD score 90.69, compared to 90.58 reported in the state-of-the-art LAMB[19] with 59k batch size. We also achieve better Pareto frontier for the trade-off between the batch size and the model performance, compared with state-of-the-art methods.
We summarize our main contributions as follows:
• We develop an accurate gradient similarity measurement for capturing the gradient variance at the iteration level. This measurement can be directly inferred from the gradients of replicas with low computation overhead.
• We develop a fully automated ﬁne-grained adaptive batching algorithm that can identify the optimal batch size and learning rate using the gradient similarity measurement.
• We integrate SimiGrad into mainstream machine learning frameworks and open-source it 1.
• SimiGrad enables a record breaking large batch size of 77k for BERT-Large pretraining. With very large batch sizes, SimiGrad is able to achieve better performance than state-of-the-art methods. 2