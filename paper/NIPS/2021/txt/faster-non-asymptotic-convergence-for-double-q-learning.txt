Abstract
Double Q-learning (Hasselt, 2010) has gained signiﬁcant success in practice due to its effectiveness in overcoming the overestimation issue of Q-learning. However, the theoretical understanding of double Q-learning is rather limited. The only existing ﬁnite-time analysis was recently established in (Xiong et al., 2020), where the polynomial learning rate adopted in the analysis typically yields a slower con-vergence rate. This paper tackles the more challenging case of a constant learning rate, and develops new analytical tools that improve the existing convergence rate by orders of magnitude. Speciﬁcally, we show that synchronous double Q-learning attains an (cid:15)-accurate global optimum with a time complexity of ˜Ω
, and (cid:16) the asynchronous algorithm achieves a time complexity of ˜Ω
, where
D is the cardinality of the state-action space, γ is the discount factor, and L is a parameter related to the sampling strategy for asynchronous double Q-learning.
These results improve the existing convergence rate by the order of magnitude in terms of its dependence on all major parameters ((cid:15), 1 − γ, D, L). This paper presents a substantial step toward the full understanding of the fast convergence of double-Q learning. (1−γ)7(cid:15)2 (cid:17)
L (1−γ)7(cid:15)2 (cid:16) ln D (cid:17) 1

Introduction
Double Q-learning, proposed in Hasselt (2010), is a widely used model-free reinforcement learning (RL) algorithm in practice for learning an optimal policy (Zhang et al., 2018a,b; Hessel et al., 2018).
Compared to the vanilla Q-learning proposed in Watkins and Dayan (1992), double Q-learning uses two Q-estimators with their roles randomly selected at each iteration, respectively for estimating the maximal Q-function value and updating the Q-function. In this way, the overestimation of the Q-function in the vanilla Q-learning can be effectively mitigated, especially when the reward is random or prone to errors (Hasselt, 2010; Hasselt et al., 2016; Xiong et al., 2020). Moreover, double Q-learning has been shown to have the desired performance in both ﬁnite state-action space setting (Hasselt, 2010) and inﬁnite setting (Hasselt et al., 2016) where it successfully improved the performance of deep Q-network (DQN), and thus inspired more variants (Zhang et al., 2017;
Abed-alguni and Ottom, 2018) subsequently.
In parallel to its empirical success in practice, the theoretical convergence properties of double
Q-learning has also been explored. Its asymptotic convergence was ﬁrst established in Hasselt (2010).
The asymptotic mean-square error for double Q-learning was studied in Weng et al. (2020b) under the assumption that the algorithm converges to a unique optimal policy. More recently, in Xiong et al. (2020), the ﬁnite-time convergence rate has been established for double Q-learning with a polynomial learning rate α = 1/tω, ω ∈ (0, 1). Under such a choice of the learning rate, Xiong et al. (2020) showed that double Q-learning attains an (cid:15)-accurate optimal Q-function at a time complexity approaching to but never reaching Ω( 1 1 1−γ . (cid:15)2 ) at the cost of an asymptotically large exponent of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison of time complexity for synchronous and asynchronous double Q-learning.
SyncDQ
Xiong et al. (2020)
This work
AsyncDQ
Xiong et al. (2020)
This work
Stepsize 1 tω , ω ∈ ( 1 3 , 1) (cid:15)2(1−γ)6
Stepsize 1 tω , ω ∈ ( 1 3 , 1) (cid:18)
ω = 1 − η → 1 (cid:16) 1
˜Ω 1−γ 1 (cid:15)2+η ∨ (cid:1)
˜Ω (cid:0) 1 (cid:15)2
η (cid:19) (cid:17) 1
Time complexity†
ω = 6/7 (cid:18)
˜Ω 1 (1−γ)7 (cid:16)
˜Ω 1 (1−γ)7(cid:15)2
Time complexity † (cid:17)7(cid:19)(cid:19) (cid:16) ln 1 1−γ (cid:18) 1 (cid:15)3.5 ∨ (cid:17) (cid:18)
ω = 1 − η → 1 (cid:17) 1
˜Ω (cid:16) 1 1−γ
η (cid:19)
ω = 6/7 (cid:18)
˜Ω 1 (1−γ)7 (cid:18) (cid:17)7(cid:19)(cid:19) (cid:16) ln 1 1−γ
ω = 2/3
˜Ω (cid:16) L6(ln L)1.5 (1−γ)9(cid:15)3 (cid:17) (cid:15)2(1−γ)6
˜Ω (cid:0) 1 (cid:15)2 7 , and ω = 2 3 optimize the dependence of time complexity on (cid:15), 1 − γ, and L 1 (1−γ)7(cid:15)2
L (1−γ)7(cid:15)2
˜Ω
˜Ω (cid:16) (cid:17) (cid:16) 1 (cid:15)2+η ∨ (cid:1) 1 (cid:15)3.5 ∨ (cid:17)
† The choices ω → 1, ω = 6 in Xiong et al. (2020), respectively. In addition, we denote a ∨ b = max{a, b}.
However, a polynomial learning rate typically does not offer the best possible convergence rate, as having been shown for RL algorithms that a constant learning rate achieves better convergence bounds (Beck and Srikant, 2012; Bhandari et al., 2018; Li et al., 2020). Therefore, a natural question arises as follows:
Can a constant learning rate improve the convergence rate of double Q-learning by orders of magnitude? If yes, does it also improve the dependence of the convergence rate on other important parameters of the Markov decision process (MDP) such as the discount factor and the cardinality of the state-action space?
In this paper, we develop novel analysis techniques and provide afﬁrmative answers to the above questions. 1.1 Our contributions
This paper establishes sharper ﬁnite-time bounds for double Q-learning with constant learning rates, which improve the existing bounds by orders of magnitude. Our result hence encourages to apply the constant stepsize in practice to attain better convergence performance. (cid:16) ln D
For synchronous double Q-learning, where all state-action pairs are visited at each iteration, we show that with constant learning rates αt ≡ α ∈ (0, 1), the algorithm converges to an (cid:15)-accurate global (cid:17) optimum with a time complexity of ˜Ω
, where γ is the discount factor and D = |S||A| is the cardinality of the ﬁnite state-action space. As a comparison, for the (cid:15)-dominated regime (with relatively small γ), we show that double Q-learning attains an (cid:15)-accurate optimal Q-function with a time complexity of Ω( 1 (cid:15)2 ), whereas the result in Xiong et al. (2020) (see Table 1) does not exactly reach Ω( 1 (cid:15)2 ) and its approaching to such an order (η := 1 − ω → 0) is at an additional cost of an asymptotically large exponent on 1 1−γ . For the (1 − γ)-dominated regime, our result improves on that in Xiong et al. (2020) (which has been optimized in the dependence on 1 − γ in Table 1 by (1−γ)7(cid:15)2 (cid:18)(cid:16)
O ln 1 1−γ (cid:17)7(cid:19)
. (cid:17) (cid:16)
L (1−γ)7(cid:15)2
For asynchronous double Q-learning, where only one state-action pair is visited at each iteration via a single sample trajectory, we show that the algorithm attains an (cid:15)-accurate global optimum with a time complexity of ˜Ω
, where L denotes the covering time (see (15)), and depends on the sampling strategy. As illustrated in Table 1, our result improves upon that in Xiong et al. (2020) order-wisely in terms of its dependence on (cid:15) and 1 − γ as well as on L by at least O (cid:0)L5(cid:1).
Technically, our ﬁnite-time analysis approach here is very different yet more direct than the techniques in Xiong et al. (2020). Our goal is still to bound the convergence error via a pair of nested stochastic approximation (SA) recursions, where the outer SA captures the learning error dynamics between 2
one Q-estimator and the global optimum and the inner SA captures the error propagation between the two Q-estimators. Rather than constructing two block-wisely decreasing bounds for the nested
SAs as in Xiong et al. (2020), which appears challenging if not infeasible under the constant learning rate, we devise new analysis techniques to directly bound both the inner and outer error dynamics either per iteration (for synchronous sampling) or per frame of constant iterations (for asynchronous sampling). We then treat the output of the inner SA as an noise term in the outer SA, and combine the two bounds to establish the ﬁnite-time error bound of the learning error. 1.2