Abstract
Extreme multi-label text classiﬁcation (XMC) seeks to ﬁnd relevant labels from an extreme large label collection for a given text input. Many real-world applications can be formulated as XMC problems, such as recommendation systems, document tagging and semantic search. Recently, transformer based XMC methods, such as X-Transformer and LightXML, have shown signiﬁcant improvement over other XMC methods. Despite leveraging pre-trained transformer models for text representation, the ﬁne-tuning procedure of transformer models on large label space still has lengthy computational time even with powerful GPUs. In this paper, we propose a novel recursive approach, XR-Transformer to accelerate the procedure through recursively ﬁne-tuning transformer models on a series of multi-resolution objectives related to the original XMC objective function. Empirical results show that XR-Transformer takes signiﬁcantly less training time compared to other transformer-based XMC models while yielding better state-of-the-art results. In particular, on the public Amazon-3M dataset with 3 million labels, XR-Transformer is not only 20x faster than X-Transformer but also improves the Precision@1 from 51% to 54%. Our code is publicly available at https://github.com/amzn/pecos. 1

Introduction
Many real-world applications such as open-domain question answering [1, 2], e-commerce dynamic search advertising [3, 4], and semantic matching [5], can be formulated as an extreme multi-label text classiﬁcation (XMC) problem: given a text input, predict relevant labels from an enormous label collection of size L. In these applications, L ranges from tens of thousands to millions, which makes it very challenging to design XMC models that are both accurate and efﬁcient to train. Recent works such as Parabel [3], Bonsai [6], XR-Linear [7] and AttentionXML [8], exploit the correlations among the labels to generate label partitions or hierarchical label trees (HLTs) which can be used to shortlist candidate labels to be considered during training and inference. While these methods are scalable in terms of the size of the label collection, most of them rely only on statistical representations (such as bag-of-words) or pooling from pre-generated token embeddings (such as word2vec) to vectorize text inputs.
In light of the recent success of deep pretrained transformers models such as BERT [9], XLNet [10] and RoBerta [11] in various NLP applications, X-Transformer [12] and LightXML [13] propose to
ﬁne-tune pre-trained transformer models on XMC tasks to obtain new state-of-the-art results over the aforementioned approaches. Although transformers are able to better capture semantic meaning of textual inputs than statistical representations, text truncation is often needed in practice to reduce GPU memory footprint and maintain model efﬁciency. For example, X-Transformer truncates input texts 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to contain the ﬁrst 128 tokens before feeding it into transformer models. Efﬁciency of transformer
ﬁne-tuning poses another challenge for XMC applications. Directly ﬁne-tuning transformer models on the original XMC task with a very large label collection is infeasible as both the training time and the memory consumption are linear in L. In order to alleviate this, both X-Transformer and
LightXML adopt a similar approach to group L labels into K clusters of roughly equal size denoted by B and ﬁne-tune transformers on the task to identify relevant label clusters (instead of labels
L, then both the training time and the memory requirement themselves). If B ≈
√ of the ﬁne-tuning can be reduced to O(
L) from O(L). However, as pointed out in [8], the model performance would deteriorate due to the information loss from label aggregation. Thus, both X-Transformer and LightXML still choose a small constant B ( ≤ 100) as the size of the label clusters.
As a result, transformers are still ﬁne-tuned on a task with around L/100 clusters, which leads to a much longer training time compared with non-transformer based models. For example, it takes
X-Transformer 23 and 25 days respectively to train on Amazon-3M and Wiki-500K even with 8
Nvidia V100 GPUs.
L and K ≈
√
√
To address these issues, we propose XR-Transformer, an XMC architecture that leverages pre-trained transformer models and has much smaller training cost compared to other transformer-based XMC methods. Motivated by the multi-resolution learning in image generation [14–16] and curriculum learning [17], we formulate the XMC problem as a series of sub-problems with multi-resolution label signals and recursively ﬁne-tune the pre-trained transformer on the coarse-to-ﬁne objectives. In this paper, our contributions are as follows:
• We propose XR-Transformer, a transformer based framework for extreme multi-label text classiﬁcation where the pre-trained transformer is recursively ﬁne-tuned on a series of easy-to-hard training objectives deﬁned by a hierarchical label tree. This allows the transformers to be quickly ﬁne-tuned for a XMC problem with a very large number label collection progressively.
• To get better text representation and mitigate the information loss in text truncation for transformers, we take into account statistical text features in addition to the transformer text embeddings in our model. Also, a cost sensitive learning scheme by label aggregation is proposed to introduce richer information on the coarsiﬁed labels.
• We conduct experiments on 6 public XMC benchmarking datasets and our model takes signiﬁcantly lower training time compared to other transformer-based XMC models to yield better state-of-the-art results. For example, we improve the state-of-the-art Prec@1 result on Amazon-3M established by X-Transformer from 51.20% to 54.04% while reducing the required training time from 23 days to 29 hours using the same hardware. 2