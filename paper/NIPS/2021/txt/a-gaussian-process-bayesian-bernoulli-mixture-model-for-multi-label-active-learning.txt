Abstract
Multi-label classification (MLC) allows complex dependencies among labels, mak-ing it more suitable to model many real-world problems. However, data anno-tation for training MLC models becomes much more labor-intensive due to the correlated (hence non-exclusive) labels and a potentially large and sparse label space. We propose to conduct multi-label active learning (ML-AL) through a novel integrated Gaussian Process-Bayesian Bernoulli Mixture model (GP-B2M) to accurately quantify a data sample’s overall contribution to a correlated label space and choose the most informative samples for cost-effective annotation. In particular, the B2M encodes label correlations using a Bayesian Bernoulli mixture of label clusters, where each mixture component corresponds to a global pattern of label correlations. To tackle highly sparse labels under AL, the B2M is further integrated with a predictive GP to connect data features as an effective inductive bias and achieve a feature-component-label mapping. The GP predicts coefficients of mixture components that help to recover the final set of labels of a data sample.
A novel auxiliary variable based variational inference algorithm is developed to tackle the non-conjugacy introduced along with the mapping process for efficient end-to-end posterior inference. The model also outputs a predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. A principled sampling function is designed accordingly to naturally capture both the feature uncertainty (through GP) and label covariance (through B2M) for effective data sampling. Experiments on real-world multi-label datasets demonstrate the state-of-the-art AL performance of the proposed model. 1

Introduction
In multi-label classification (MLC), each data instance may be associated with more than one label.
Such a rich representation of labels can encode more complex data-label distributions that arise in many real-world problems [1, 2, 3, 4]. As a simple yet powerful tool, binary relevance machines (BRMs) transform an MLC problem into multiple binary problems and train independent binary classifiers for each label [5]. Such a transformation gives BRMs the flexibility to leverage state-of-art binary classifiers (e.g., deep neural networks and SVMs). However, applying BRMs to a correlated and potentially large label space poses key challenges. First, many real-world multi-label datasets contain a large number of labels. Training one predictor per label incurs a prohibitive cost. Second, despite an overall large label space, each data instance is usually assigned limited labels. Many labels are relatively rare and their appearances depend not only on the features but also the occurrence of other labels. Predicting these “complex” labels directly using independent binary classifiers is fundamentally difficult due to the limited positive data instances and weaker direct dependency on the features. Correlations among labels provide important auxiliary information to enhance multi-label
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
(a) (b) (c)
Figure 1: (a) Labels with geometric correlation (G1-G8); (b) Labels with cardinality (C1,C2), overlapping (O1-O4), and exclusive (and hierarchical) dependencies (E1-E4); (c) Definition of label correlation and learned mixture components. prediction [6, 7, 8]. However, these models heavily rely on the training data that exhibit these important label correlations. Considering the high cost in annotating a multi-label dataset, it is critical to choose the most informative data samples for cost-effective data annotation.
In this paper, we propose a novel Gaussian Process-Bayesian Bernoulli Mixture (GP-B2M) model to achieve cost-effective sampling for multi-label active learning (ML-AL). In ML-AL, since labels are not mutually exclusive as in the single label setting, all the labels should be considered collectively when designing an active sampling function so that a data sample’s overall contribution to the entire label space can be accurately measured. However, since only limited training data instances are available for an ML-AL model, how to accurately model label correlations and hence quantify a data sample’s overall informativeness using very sparse labels under AL poses a grand challenge. Existing efforts that explicitly model label correlations usually focus on limited types of correlations such as pairwise [3, 9], conditional [10, 11], or full correlation in a subset of labels [12, 13]. Consequently, those methods may miss some important label correlations. Label correlations can also be captured through a latent embedding [7, 8]. While these methods can scale to a large label space, they usually require a decent number of training labels to compute an accurate embedding, making them less suitable for ML-AL. Furthermore, the learned embedding has no semantic meanings, which cannot be used to interpret the discovered label dependencies 2.
The proposed GP-B2M model addresses the limitations of existing methods to fundamentally advance
ML-AL. In particular, the B2M encodes label correlations using a Bayesian Bernoulli mixture of label clusters. Since labels are highly sparse in ML-AL, a predictive GP is further integrated to learn a distribution of mixture coefficients that connect data features with the label clusters. Thus, the label clusters can be regarded as a global pattern of label co-occurrences discovered from both the training labels and data features to address label sparsity. In this novel feature-component-label mapping, data features serve as an inductive bias to learn accurate mixture components of labels, where data samples with similar features should be mapped to similar mixture components, which in turn lead to a similar set of labels. Such an inductive bias allows the discovery of label relationships from limited labels with the support of feature relationships, which is essential for a sparse label space in ML-AL.
Figure 1 shows three mixture components learned from synthetic data designed with complex label correlations, including geometric, cardinality, overlapping, and hierarchical (see Figure 1 (c) for definitions). For example, the hierarchical labels (E1 − E4) show some interesting but quite complicated correlations that may exist in many real-world data. If E1 represents a common disease and E2, E3 represent some less common ones that may co-occur with E1 (30% of the time), then
E4 corresponds to a rather rare disease that only co-occurs with E3 but not E2. In Figure 1 (c), both components 1and 2 show a high chance of E4. It is also clear that while both E1 and E3 are very likely to appear in these components, the chance of seeing E2 is much lower, reflecting its exclusive relationship with E3. Interestingly, while both components 1 and 2 cover E4, they also show complementary information. In component 1, E4 mostly appears in the non-overlapping regions, which is indicated by smaller O1 and O4; whereas in component 2, it’s more likely to 2Correlation sometimes refers to the degree to which a pair of random variables are linearly related. However, in the broadest sense, both correlation and dependence refer to any statistical relationship between two random variables, so they are used exchangeably in the rest of the paper. 2
occur in overlapping regions O1 and O4. Finally, in component 3, it is less likely to observe E4.
Consequently, the chance to see E2 is significantly higher.
The above example demonstrates that the learned mixture components accurately capture complex label correlations that are critical for active data sampling. They are also interpretable, which can help to unveil important relationships among labels. Our key contribution is threefold: (i) We propose a novel GP-Bayesian Bernoulli mixture model to discover meaningful label correlations from limited labels by encoding the inductive bias from data features as Bayesian priors to learn from both labels and features while ensuring consistency. (ii) We introduce a set of auxiliary latent variables to achieve a fully conjugate feature-component-label mapping in the Bayesian model to support efficient end-to-end posterior inference. The number of mixture components is also dynamically adjusted during Bayesian inference so that the model complexity is automatically calibrated according to the size of training data, which is critical for AL. (iii) The model outputs the predictive distribution that provides both the label prediction and their correlations in the form of a label covariance matrix. We design a novel active sampling function that integrates both feature uncertainty and label covariance to quantify a data sample’s overall contribution to a correlated label space. Extensive experiments on both synthetic and real-world multi-label data and comparison with competitive models demonstrate that the proposed GP-B2M achieves the state-of-the-art active learning performance. 2