Abstract
Collaborating with humans requires rapidly adapting to their individual strengths, weaknesses, and preferences. Unfortunately, most standard multi-agent reinforce-ment learning techniques, such as self-play (SP) or population play (PP), produce agents that overﬁt to their training partners and do not generalize well to humans.
Alternatively, researchers can collect human data, train a human model using behav-ioral cloning, and then use that model to train “human-aware” agents (“behavioral cloning play”, or BCP). While such an approach can improve the generalization of agents to new human co-players, it involves the onerous and expensive step of collecting large amounts of human data ﬁrst. Here, we study the problem of how to train agents that collaborate well with human partners without using human data.
We argue that the crux of the problem is to produce a diverse set of training part-ners. Drawing inspiration from successful multi-agent approaches in competitive domains, we ﬁnd that a surprisingly simple approach is highly effective. We train our agent partner as the best response to a population of self-play agents and their past checkpoints taken throughout training, a method we call Fictitious Co-Play (FCP). Our experiments focus on a two-player collaborative cooking simulator that has recently been proposed as a challenge problem for coordination with humans.
We ﬁnd that FCP agents score signiﬁcantly higher than SP, PP, and BCP when paired with novel agent and human partners. Furthermore, humans also report a strong subjective preference to partnering with FCP agents over all baselines. 1

Introduction
Generating agents which collaborate with novel partners is a longstanding challenge for Artiﬁcial
Intelligence (AI) [4, 16, 37, 52]. Achieving ad-hoc, zero-shot coordination [31, 66] is especially important in situations where an AI must generalize to novel human partners [6, 61]. Many successful approaches have employed human models, either constructed explicitly [14, 35, 53] or learnt implicitly
[12, 60]. By contrast, recent work in competitive domains has shown that it is possible to reach human-level using model-free reinforcement learning (RL) without human data, via self-play [8, 9, 63, 64].
This begs the question: Can model-free RL without human data generate agents that can collaborate with novel humans?
We seek an answer to this question in the space of common-payoff games, where all agents work towards a shared goal and receive the same reward. Self-play (SP), in which an agent learns from repeated games played against copies of itself, does not produce agents that generalize well to novel co-players [10, 11, 21, 44]. Intuitively, this is because agents trained in self-play only ever need to coordinate with themselves, and so make for brittle and stubborn collaborators with new partners who act differently. Population play (PP) trains a population of agents, all of whom interact with each other [39]. While PP can generate agents capable of cooperation with humans in competitive team games [34], it still fails to produce robust partners for novel humans in pure common-payoff settings
⇤Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(cid:1)(cid:156)(cid:143)(cid:181)(cid:210)(cid:303)(cid:210)(cid:200)(cid:121)(cid:161)(cid:181)(cid:161)(cid:181)(cid:156)(cid:303)(cid:161)(cid:181)(cid:303) (cid:225)(cid:121)(cid:200)(cid:161)(cid:186)(cid:214)(cid:204)(cid:303)(cid:197)(cid:186)(cid:197)(cid:214)(cid:174)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:204) (cid:117)(cid:143)(cid:200)(cid:186)(cid:286)(cid:204)(cid:160)(cid:186)(cid:210)(cid:303)(cid:134)(cid:186)(cid:186)(cid:200)(cid:139)(cid:161)(cid:181)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181) (cid:226)(cid:161)(cid:210)(cid:160)(cid:303)(cid:181)(cid:186)(cid:225)(cid:143)(cid:174)(cid:303)(cid:121)(cid:156)(cid:143)(cid:181)(cid:210)(cid:303)(cid:197)(cid:121)(cid:200)(cid:210)(cid:181)(cid:143)(cid:200)(cid:204) (cid:117)(cid:143)(cid:200)(cid:186)(cid:286)(cid:204)(cid:160)(cid:186)(cid:210)(cid:303)(cid:134)(cid:186)(cid:186)(cid:200)(cid:139)(cid:161)(cid:181)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:303)(cid:226)(cid:161)(cid:210)(cid:160)(cid:303)(cid:181)(cid:186)(cid:225)(cid:143)(cid:174)(cid:303)(cid:160)(cid:214)(cid:180)(cid:121)(cid:181)(cid:303)(cid:197)(cid:121)(cid:200)(cid:210)(cid:181)(cid:143)(cid:200)(cid:204)(cid:303) (cid:121)(cid:181)(cid:139)(cid:303)(cid:143)(cid:174)(cid:161)(cid:134)(cid:161)(cid:210)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:303)(cid:186)(cid:155)(cid:303)(cid:160)(cid:214)(cid:180)(cid:121)(cid:181)(cid:303)(cid:197)(cid:200)(cid:143)(cid:155)(cid:143)(cid:200)(cid:143)(cid:181)(cid:134)(cid:143)(cid:204)(cid:303)(cid:186)(cid:225)(cid:143)(cid:200)(cid:303)(cid:121)(cid:156)(cid:143)(cid:181)(cid:210)(cid:204) (cid:76)(cid:200)(cid:161)(cid:225)(cid:121)(cid:210)(cid:143)(cid:303)(cid:344)(cid:303)(cid:14)(cid:186)(cid:181)(cid:242)(cid:139)(cid:143)(cid:181)(cid:210)(cid:161)(cid:121)(cid:174) (cid:83)(cid:143)(cid:174)(cid:155)(cid:286)(cid:197)(cid:174)(cid:121)(cid:232)(cid:303) (cid:280)(cid:83)(cid:76)(cid:281) (cid:76)(cid:186)(cid:197)(cid:214)(cid:174)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:286)(cid:197)(cid:174)(cid:121)(cid:232) (cid:280)(cid:76)(cid:76)(cid:281) (cid:83)(cid:76) (cid:76)(cid:76) (cid:76)(cid:76) (cid:76)(cid:76) (cid:76)(cid:76) (cid:83)(cid:76) (cid:83)(cid:76) (cid:76)(cid:76) (cid:39) (cid:83)(cid:76) (cid:76)(cid:76) (cid:39) (cid:105)(cid:160)(cid:161)(cid:134)(cid:160)(cid:303)(cid:121)(cid:156)(cid:143)(cid:181)(cid:210)(cid:303) (cid:139)(cid:161)(cid:139)(cid:303)(cid:232)(cid:186)(cid:214)(cid:303)(cid:197)(cid:200)(cid:143)(cid:155)(cid:143)(cid:200)(cid:272) (cid:83)(cid:76) (cid:76)(cid:76)
Figure 1: In this work, we evaluate a variety of agent training methods (Section 2) in zero-shot coordination with agents (Section 4). We then run a human-agent collaborative study designed to elicit human preferences over agents (Section 5).
[12]. PP in common-payoff settings naturally encourages agents to play the same way, reducing strategic diversity and producing agents not so different from self-play [24].
Our approach starts with the intuition that the key to producing robust agent collaborators is exposure to diverse training partners. We ﬁnd that a surprisingly simple strategy is effective in generating sufﬁcient diversity. We train N self-play agents varying only their random seed for neural network initialization. Periodically during training, we save agent “checkpoints” representing their strategy at that point in time. Then, we train an agent partner as the best-response to both the fully-trained agents and their past checkpoints. The different checkpoints simulate different skill levels, and the different random seeds simulate breaking symmetries in different ways. We refer to this agent training procedure as Fictitious Co-Play (FCP) for its relationship to ﬁctitious self-play [7, 27, 28, 69].
We evaluate FCP in a fully-observable two-player common-payoff collaborative cooking simulator.
Based on the game Overcooked [25], it has recently been proposed as a coordination challenge for
AI [12, 50, 70]. State-of-the-art performance in producing agents capable of generalization to novel humans was achieved in [12] via behavioral cloning (BC) of human data. More precisely, BC was used to produce models that can stand in as human proxies during training in simulation, a method we call behavioral cloning play (BCP). We demonstrate that FCP outperforms BCP in generalizing to both novel agent and human partners, and that humans express a signiﬁcant preference for partnering with FCP over BCP. Our method avoids the cost and potential privacy concerns of collecting human data for training, while achieving better outcomes for humans at test time.
We summarize the novel contributions of this paper as follows: 1. We propose Fictitious Co-Play (FCP) to train agents capable of zero-shot coordination with humans (Section 2.1). 2. We demonstrate that FCP agents generalize better than SP, PP, and BCP in zero-shot coordination with a variety of held-out agents (Section 4.2). 3. We propose a rigorous human-agent interaction study with behavioral analysis and partici-pant feedback (Section 5.1). 4. We demonstrate that FCP signiﬁcantly outperforms the BCP state-of-the-art, both in task score and in human partner preference (Section 5.2). 2 Methods 2.1 Fictitious Co-Play (FCP)
Diverse training conditions have been shown to make agents more robust, from environmental variations (i.e. domain randomization [54, 56, 67]) to heterogeneity in training partners [69]. We seek to train agents that are robust partners for humans in common-payoff games, and so extend this line of work to that setting.
One important challenge in collaborating with novel partners is dealing with symmetries [31]. For example, two agents A and B facing each other may move past each other by A going left and B going right, or vice versa. Both are valid solutions, but a good agent partner will adaptively switch between 2
(cid:79)(cid:143)(cid:161)(cid:181)(cid:155)(cid:186)(cid:200)(cid:134)(cid:143)(cid:180)(cid:143)(cid:181)(cid:210)(cid:303)(cid:174)(cid:143)(cid:121)(cid:200)(cid:181)(cid:161)(cid:181)(cid:156)(cid:303)(cid:280)(cid:79)(cid:53)(cid:281) (cid:13)(cid:143)(cid:160)(cid:121)(cid:225)(cid:161)(cid:186)(cid:200)(cid:121)(cid:174)(cid:303)(cid:134)(cid:174)(cid:186)(cid:181)(cid:161)(cid:181)(cid:156)(cid:303)(cid:280)(cid:13)(cid:14)(cid:281) (cid:39)(cid:214)(cid:180)(cid:121)(cid:181)(cid:286)(cid:160)(cid:214)(cid:180)(cid:121)(cid:181) (cid:139)(cid:121)(cid:210)(cid:121)(cid:303)(cid:134)(cid:186)(cid:174)(cid:174)(cid:143)(cid:134)(cid:210)(cid:161)(cid:186)(cid:181) (cid:39) (cid:39) (cid:210)(cid:320)(cid:248) (cid:210)(cid:320)(cid:249) (cid:210)(cid:320)(cid:250) (cid:210)(cid:320)(cid:248) (cid:210)(cid:320)(cid:249) (cid:210)(cid:320)(cid:250) (cid:76)(cid:200)(cid:161)(cid:225)(cid:121)(cid:210)(cid:143)(cid:303)(cid:344)(cid:303)(cid:14)(cid:186)(cid:181)(cid:242)(cid:139)(cid:143)(cid:181)(cid:210)(cid:161)(cid:121)(cid:174) (cid:83)(cid:143)(cid:174)(cid:155)(cid:286)(cid:197)(cid:174)(cid:121)(cid:232) (cid:280)(cid:83)(cid:76)(cid:281) (cid:76)(cid:186)(cid:197)(cid:214)(cid:174)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:286)(cid:197)(cid:174)(cid:121)(cid:232)(cid:303) (cid:280)(cid:76)(cid:76)(cid:281) (cid:13)(cid:143)(cid:160)(cid:121)(cid:225)(cid:161)(cid:186)(cid:200)(cid:121)(cid:174)(cid:303)(cid:134)(cid:174)(cid:186)(cid:181)(cid:161)(cid:181)(cid:156)(cid:303)(cid:197)(cid:174)(cid:121)(cid:232) (cid:280)(cid:13)(cid:14)(cid:76)(cid:281) (cid:34)(cid:161)(cid:134)(cid:210)(cid:161)(cid:210)(cid:161)(cid:186)(cid:214)(cid:204)(cid:303)(cid:134)(cid:186)(cid:286)(cid:197)(cid:174)(cid:121)(cid:232) (cid:280)(cid:34)(cid:14)(cid:76)(cid:281)
Figure 2: The four agent training methods we evaluate in this work. Self-play (SP) where an agent learns with itself, population-play (PP) where a population of agents are co-trained together, and behavioral cloning play (BCP) where data from human games is used to create a behaviorally cloned agent with which an RL agent is then trained. In our method, Fictitious Co-Play (FCP), N self-play agents are trained independently and checkpointed throughout training. An agent is then trained to best respond to the entire population of SP agents and their checkpoints. these conventions if a human clearly prefers one over the other. A second important challenge is dealing with variations in skill level. Good agent partners should be able to assist both highly-skilled partners, as well as partners who are still learning.
Fictitious co-play (FCP) is a simple two-stage approach for training agents that overcomes both of these challenges (Figure 2, right). In the ﬁrst stage, we train a diverse pool of partners. To allow the pool to represent different symmetry breaking conventions, we train N partner agents in self-play. Since these partners are trained independently, they can arrive at different arbitrary conventions for breaking symmetries. To allow the pool to represent different skill levels, we use multiple checkpoints of each self-play partner throughout training. The ﬁnal checkpoint represents a fully-trained “skillful” partner, while earlier checkpoints represent less skilled partners. Notably, by using multiple checkpoints per partner, this additional diversity in skill incurs no extra training cost.
In the second stage, we train an FCP agent as the best response to the pool of diverse partners created in the ﬁrst stage. Importantly, the partner parameters are frozen and thus FCP must learn to adapt to partners, rather than expect partners to adapt to it. In this way, FCP agents are prepared to follow the lead of human partners, and learn a general policy across a range of strategies and skills. We call our method “ﬁctitious” co-play for its relationship to ﬁctitious self-play in which competitive agents are trained with past checkpoints (in that case, to avoid strategy cycling) [7, 27, 28, 39, 69]. 2.2 Baselines and ablations
We compare FCP agents to the three baseline training methods listed below, each varying only in their set of training partners, with the RL algorithm and architecture consistent across all agents: 1. Self-play (SP), where agents learn solely through interaction with themselves. 2. Population-play (PP), where a population of agents are co-trained through random pairings. 3. Behavioral cloning play (BCP), where an agent is trained with a BC model of a human [12].
We also evaluate three variations on FCP to better understand the conditions for its success:
  1. To test the importance of including past checkpoints in training, we evaluate an ablation of FCP in which agents are trained only with the converged checkpoints of their partners (FCP
T for “FCP minus time”). 2. To test whether FCP would beneﬁt from additional diversity in its partner population, we evaluate an augmentation of FCP in which the population of SP partners varies not just in random seed, but also in architecture (FCP+A for “FCP plus architectural variation”). 3. To test whether architectural variation can serve as a full replacement for playing with past checkpoints, we evaluate the combination of both modiﬁcations (FCP
 
T,+A). 3
2.3 Environment
Following prior work on zero-shot coordination in human-agent interaction, we study the Overcooked environment (see Figure 3) [12, 13, 38, 50, 70]. We draw particular inspiration from the environment in Carroll et al. [12]. For full details, see Appendix A.
In this environment, players are placed into a gridworld kitchen as chefs and tasked with delivering as many cooked dishes of tomato soup as possible within an episode. This involves a series of sequential high-level actions to which both players can contribute: collecting tomatoes, depositing them into cooking pots, letting the tomatoes cook into soup, collecting a dish, getting the soup, and delivering it. Upon a successful delivery, both players are rewarded equally.
To effectively complete the task, players must learn to navigate the kitchen and interact with objects in the correct order, all while maintaining awareness of their partner’s behavior to coordinate with them.
This environment therefore presents the challenges of both movement and strategic coordination.
Each player observes an egocentric RGB view of the world, and at every step can perform one of six actions: stand still, move {up, down, left, right}, interact. The behavior of interact varies based on the cell which the player is facing (e.g. place tomato on counter). (cid:76)(cid:200)(cid:161)(cid:225)(cid:121)(cid:210)(cid:143)(cid:303)(cid:344)(cid:303)(cid:14)(cid:186)(cid:181)(cid:242)(cid:139)(cid:143)(cid:181)(cid:210)(cid:161)(cid:121)(cid:174) (cid:14)(cid:160)(cid:143)(cid:155)(cid:204) (cid:14)(cid:186)(cid:186)(cid:172)(cid:161)(cid:181)(cid:156)(cid:303)(cid:197)(cid:186)(cid:210)(cid:204) (cid:19)(cid:143)(cid:174)(cid:161)(cid:225)(cid:143)(cid:200)(cid:232) (cid:174)(cid:186)(cid:134)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181) (cid:14)(cid:186)(cid:174)(cid:174)(cid:143)(cid:134)(cid:210)(cid:303)(cid:121)(cid:303)(cid:210)(cid:186)(cid:180)(cid:121)(cid:210)(cid:186)(cid:303)(cid:121)(cid:181)(cid:139)(cid:303)(cid:197)(cid:174)(cid:121)(cid:134)(cid:143)(cid:303)(cid:161)(cid:181)(cid:210)(cid:186)(cid:303)(cid:134)(cid:186)(cid:186)(cid:172)(cid:161)(cid:181)(cid:156)(cid:303)(cid:197)(cid:186)(cid:210) (cid:14)(cid:186)(cid:214)(cid:181)(cid:210)(cid:143)(cid:200) (cid:19)(cid:161)(cid:204)(cid:160) (cid:204)(cid:210)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181) (cid:89)(cid:186)(cid:180)(cid:121)(cid:210)(cid:186)(cid:303) (cid:204)(cid:210)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:204) (cid:79)(cid:143)(cid:197)(cid:143)(cid:121)(cid:210)(cid:303)(cid:210)(cid:226)(cid:161)(cid:134)(cid:143)(cid:303)(cid:180)(cid:186)(cid:200)(cid:143)(cid:266)(cid:303)(cid:210)(cid:160)(cid:143)(cid:181)(cid:303)(cid:226)(cid:121)(cid:161)(cid:210)(cid:303)(cid:155)(cid:186)(cid:200)(cid:303)(cid:204)(cid:186)(cid:214)(cid:197)(cid:303)(cid:210)(cid:186)(cid:303)(cid:134)(cid:186)(cid:186)(cid:172) (cid:14)(cid:186)(cid:174)(cid:174)(cid:143)(cid:134)(cid:210)(cid:303)(cid:139)(cid:161)(cid:204)(cid:160)(cid:266)(cid:303)(cid:210)(cid:160)(cid:143)(cid:181)(cid:303)(cid:156)(cid:143)(cid:210)(cid:303)(cid:204)(cid:186)(cid:214)(cid:197)(cid:303)(cid:155)(cid:200)(cid:186)(cid:180)(cid:303)(cid:134)(cid:186)(cid:186)(cid:172)(cid:161)(cid:181)(cid:156)(cid:303)(cid:197)(cid:186)(cid:210) (cid:19)(cid:143)(cid:174)(cid:161)(cid:225)(cid:143)(cid:200)(cid:303)(cid:204)(cid:186)(cid:214)(cid:197)(cid:303)(cid:210)(cid:186)(cid:303)(cid:139)(cid:143)(cid:174)(cid:161)(cid:225)(cid:143)(cid:200)(cid:232)(cid:303)(cid:204)(cid:210)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181) (cid:76)(cid:200)(cid:161)(cid:225)(cid:121)(cid:210)(cid:143)(cid:303)(cid:344)(cid:303)(cid:14)(cid:186)(cid:181)(cid:242)(cid:139)(cid:143)(cid:181)(cid:210)(cid:161)(cid:121)(cid:174) (cid:19)(cid:143)(cid:174)(cid:161)(cid:225)(cid:143)(cid:200)(cid:143)(cid:139)(cid:270) (cid:316)(cid:303)(cid:156)(cid:174)(cid:186)(cid:133)(cid:121)(cid:174)(cid:303)(cid:200)(cid:143)(cid:226)(cid:121)(cid:200)(cid:139)
Figure 3: The Overcooked environment: a two-player common-payoff game in which players must coordinate to cook and deliver soup. (cid:14)(cid:200)(cid:121)(cid:180)(cid:197)(cid:143)(cid:139)(cid:303)(cid:79)(cid:186)(cid:186)(cid:180) (cid:1)(cid:204)(cid:232)(cid:180)(cid:180)(cid:143)(cid:210)(cid:200)(cid:161)(cid:134)(cid:303)(cid:1)(cid:139)(cid:225)(cid:121)(cid:181)(cid:210)(cid:121)(cid:156)(cid:143)(cid:204) (cid:14)(cid:186)(cid:186)(cid:200)(cid:139)(cid:161)(cid:181)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)(cid:303)(cid:79)(cid:161)(cid:181)(cid:156) (cid:14)(cid:186)(cid:214)(cid:181)(cid:210)(cid:143)(cid:200)(cid:303)(cid:14)(cid:161)(cid:200)(cid:134)(cid:214)(cid:161)(cid:210) (cid:34)(cid:186)(cid:200)(cid:134)(cid:143)(cid:139)(cid:303)(cid:14)(cid:186)(cid:186)(cid:200)(cid:139)(cid:161)(cid:181)(cid:121)(cid:210)(cid:161)(cid:186)(cid:181)
Figure 4: Layouts: the kitchens which agents and humans play in, each emphasizing different coordination strategies. Highlighted in bold are the terms used to refer to each in the rest of this paper. 2.4
Implementation details
Here we highlight several key implementation details for our training methods. For full details, including the architectures, hyperparameters, and compute used, please see Appendix B.
For our reinforcement learning agents, we use the V-MPO [65] algorithm along with a ResNet [26] plus LSTM [29] architecture which we found led to optimal behavior across all layouts. Agents are trained using a distributed set of environments running in parallel [17], each sampling two agents from the training population to play together every episode.
Both PP and FCP are trained with a population size of N = 32 agents which are sampled uniformly.
For FCP, we use 3 checkpoints for each agent, therefore incurring no additional training burden: (1) at initialization (i.e. a low-skilled agent), (2) at the end of training (i.e. a fully-trained expert agent), and (3) at the middle of training, deﬁned as when the agent reaches 50% of its ﬁnal reward (i.e. an average-skilled agent). When varying architecture for the training partners of the FCP+A and 4
 
T,+A variants, we vary whether the partners use memory (i.e. LSTM vs not) and the width
FCP of their policy and value networks (i.e. 16 vs 256). In total, we train 8 agents for each of the 4 combinations, leaving the total population size of N = 32 unchanged, ensuring a fair comparison.
To train agents via behavioral cloning [58], we use the open-source Acme [30] to learn a policy from human gameplay data. Speciﬁcally, we collected 5 human-human trajectories of length 1200 time steps for each of the 5 layouts, resulting in 60k total environment steps. We divide this data in half and train two BC agents: (1) a partner for training a BCP agent, and (2) a “human proxy” partner for agent-agent evaluation. Following Carroll et al. [12], we use a set of feature-based observations for the agents (as opposed to RGB) and generate comparable results: performance is higher on 3 layouts (asymmetric, cramped, and ring) but poorer on the other 2 (circuit and forced). 3