Abstract
Deep Neural Networks (DNNs) are known to be vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To tackle this, adversarial training is currently the most effective defense method, by augmenting the training set with adversarial samples generated on the Ô¨Çy. Interestingly, we discover for the Ô¨Årst time that there exist subnetworks with inborn robustness, matching or sur-passing the robust accuracy of the adversarially trained networks with com-parable model sizes, within randomly initialized networks without any model training, indicating that adversarial training on model weights is not indispensable towards adversarial robustness. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efÔ¨Åcient. Distinct from the popular lottery ticket hypothesis, neither the original dense networks nor the identiÔ¨Åed RSTs need to be trained. To validate and understand this fascinating Ô¨Ånding, we further conduct extensive experiments to study the existence and properties of RSTs under different models, datasets, sparsity patterns, and attacks, drawing insights regarding the rela-tionship between DNNs‚Äô robustness and their initialization/overparameterization.
Furthermore, we identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. We believe our Ô¨Åndings about RSTs have opened up a new perspective to study model robustness and extend the lottery ticket hypothesis. Our codes are available at: https://github.com/RICE-EIC/Robust-Scratch-Ticket. 1

Introduction
There has been an enormous interest in deploying deep neural networks (DNNs) into numerous real-world applications requiring strict security. Meanwhile, DNNs are vulnerable to adversarial attacks, i.e., an imperceptible perturbation to the input can mislead DNNs trained on clean images into making erroneous predictions. To enhance DNNs‚Äô robustness, adversarial training augmenting the training set with adversarial samples is commonly regarded as the most effective defense method.
Nevertheless, adversarial training is time-consuming as each stochastic gradient descent (SGD) iteration requires multiple gradient computations to produce adversarial images. In fact, its actual slowdown factor over standard DNN training depends on the number of gradient steps used for adversarial example generation, which can result in a 3‚àº30 times longer training time [1].
In this work, we ask an intriguing question: ‚ÄúCan we Ô¨Ånd robust subnetworks within randomly initial-ized networks without any training‚Äù? This question not only has meaningful practical implication 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
but also potential theoretical ones. If the answer is yes, it will shed light on new methods towards robust DNNs, e.g., adversarial training might not be indispensable towards adversarial robustness; furthermore, the existence of such robust subnetworks will extend the recently discovered lottery ticket hypothesis (LTH) [2], which articulates that neural networks contain sparse subnetworks that can be effectively trained from scratch when their weights being reset to the original initialization, and thus our understanding towards robust DNNs. In particular, we make the following contributions:
‚Ä¢ We discover for the Ô¨Årst time that there exist subnetworks with inborn robustness, matching or surpassing the robust accuracy of adversarially trained networks with comparable model sizes, within randomly initialized networks without any model training. We name such subnetworks Robust Scratch Tickets (RSTs), which are also by nature efÔ¨Åcient. Distinct from the popular LTH, neither the original dense networks nor the identiÔ¨Åed RSTs need to be trained. For example, RSTs identiÔ¨Åed from a randomly initialized ResNet101 achieve a 3.56%/4.31% and 1.22%/4.43% higher robust/natural accuracy than the adversarially trained dense ResNet18 with a comparable model size, under a perturbation strength of ùúñ = 2 and
ùúñ = 4, respectively.
‚Ä¢ We propose a general method to search for RSTs within randomly initialized networks, and conduct extensive experiments to identify and study the consistent existence and proper-ties of RSTs under different DNN models, datasets, sparsity patterns, and attack methods, drawing insights regarding the relationship between DNNs‚Äô robustness and their initializa-tion/overparameterization. Our Ô¨Åndings on RSTs‚Äô existence and properties have opened up a new perspective for studying DNN robustness and can be viewed as a complement to LTH.
‚Ä¢ We identify the poor adversarial transferability between RSTs of different sparsity ratios drawn from the same randomly initialized dense network, and propose a Random RST
Switch (R2S) technique, which randomly switches between different RSTs, as a novel defense method built on top of RSTs. While an adversarially trained network shared among datasets suffers from degraded performance, our R2S enables the use of one random dense network for effective defense across different datasets. 2