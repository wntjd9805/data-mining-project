Abstract
Recent research has shown that graph neural networks (GNNs) can learn policies for locomotion control that are as effective as a typical multi-layer perceptron (MLP), with superior transfer and multi-task performance [55, 20]. However, results have so far been limited to training on small agents, with the performance of
GNNs deteriorating rapidly as the number of sensors and actuators grows. A key motivation for the use of GNNs in the supervised learning setting is their applicabil-ity to large graphs, but this beneﬁt has not yet been realised for locomotion control.
We show that poor scaling in GNNs is a result of increasingly unstable policy up-dates, caused by overﬁtting in parts of the network during training. To combat this, we introduce SNOWFLAKE, a GNN training method for high-dimensional continu-ous control that freezes parameters in selected parts of the network. SNOWFLAKE signiﬁcantly boosts the performance of GNNs for locomotion control on large agents, now matching the performance of MLPs while offering superior transfer properties. 1

Introduction
Whereas many traditional machine learning models operate on sequential or Euclidean (grid-like) data representations, GNNs allow for graph-structured inputs. GNNs have yielded breakthroughs in a variety of complex domains, including drug discovery [33, 50], fraud detection [56], computer vision [49, 43], and particle physics [18].
GNNs have also been successfully applied to reinforcement learning (RL), with promising results on locomotion control tasks with small state and action spaces. Not only are GNN policies as effective as MLPs on certain training tasks, but when a trained policy is transferred to another similar task,
GNNs signiﬁcantly outperform MLPs [55, 20]. This is largely due to the capacity of a single GNN to operate over arbitrary graph topologies (patterns of connectivity between nodes) and sizes without modiﬁcation. However, so far GNNs in RL have only shown competitive performance with MLPs on lower-dimensional locomotion control tasks. For higher-dimensional tasks, one must therefore choose between superior training task performance (MLPs) and superior transfer performance (GNNs).
This paper investigates the factors underlying poor GNN scaling and introduces a method to combat them. We begin with an analysis of the GNN-based NERVENET architecture [55], which we choose for its strong zero-shot transfer performance. We show that optimisation updates for the GNN
⇤Corresponding author. Now at Graphcore, Bristol
†Now at Waymo, Oxford 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
policy have a tendency to cause excessive changes in policy space, leading to performance degrading.
To combat this, current state-of-the-art algorithms [46, 48, 1] employ trust region-like constraints, inspired by natural gradients [2, 23], that limit the change in policy for each update. We outline how this policy instability can be framed as a form of overﬁtting—a problem GNN architectures like
NERVENET are known to suffer from in supervised learning, and show that parameter regularisation (a standard remedy for overﬁtting) leads to a small improvement in NERVENET performance.
We then investigate which structures in the GNN contribute most to this overﬁtting, by applying different learning rates to different parts of the network. Surprisingly, the best performance is attained when training with a learning rate of zero in the parts of the GNN architecture that encode, decode, and propagate messages in the graph, in effect training only the part that updates node representations.
We use this approach as the basis of our method, SNOWFLAKE, which freezes the parameters of particular operations within the GNN to their initialised values, keeping them ﬁxed throughout training while updating the non-frozen parameters as before. This simple technique enables GNN policies to be trained much more effectively in high-dimensional environments.
Experimentally, we show that applying SNOWFLAKE to NERVENET dramatically improves asymp-totic performance and sample complexity on such tasks. We also demonstrate that a policy trained using SNOWFLAKE exhibits improved zero-shot transfer compared to regular NERVENET or MLPs on high-dimensional tasks. 2