Abstract
Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efﬁcient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations. 1

Introduction
Learning from high-dimensional data is known to be statistically intractable without strong assump-tions on the problem. A canonical example is learning Lipschitz functions, which generally requires a number of samples exponential in the dimension due to the curse of dimensionality (e.g., [31]). Many high-dimensional machine learning problems involve highly structured data such as images, text, or graphs, and may exhibit invariance to certain transformations of the input data, such as permutations, translations or rotations, and near invariance to small deformations. More precisely, if X is the high-dimensional data domain, and G is a set of transformations σ : X → X , the learning task can be alleviated if one knows in advance that the target function f varies smoothly under transformations in
G: |f (σ · x) − f (x)| is uniformly small over x ∈ X for σ ∈ G.
To further motivate this property, it is useful to view the data domain X as a space of signals
X = L2(Ω; R) deﬁned over a geometric domain Ω, such as a 2d grid. The set of transformations G can then be articulated in terms of Ω rather than X , a much simpler geometric object, and then lifted into X by composition: if σ : Ω → Ω, and x ∈ X then (σ · x)(u) := x(σ−1(u)) for every u ∈ Ω.
The smoothness to transformations can thus be interpreted as a form of geometric stability.
In this paper, we quantify the sample complexity gains brought by geometric stability. Concretely, we consider target functions f deﬁned on the sphere X = Sd−1 in d dimensions with ﬁnite L2(Sd−1) norm. In this case, we view the geometric domain as the discrete 1d grid Ω = [1, . . . , d], and consider geometric transformations G as subsets of the symmetric group of permutations of d
∗Center for Data Science, New York University.
†Courant Institute for Mathematical Sciences, New York University.
‡Center for Data Science and Courant Institute for Mathematical Sciences, New York University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
elements. Given a set G (not necessarily a group), we consider the smoothing operator given by SGf (x) = 1
σ∈G f (σ · x) for f ∈ L2(Sd−1), and assume that our target function f is
|G| geometrically stable, in the sense that f = SGg for some g ∈ L2(Sd−1). In words, the smoothing operator SG replaces the prediction f (x) by the average over transformations of x. In particular, functions which are invariant under the action of σ ∈ G, namely
P f (σ · x) = f (x),
σ ∈ G, x ∈ Sd−1, (1) are also stable, with f = SGf .
Building on the recent work [23], we proceed by studying harmonic decompositions of such functions using spherical harmonics [14], which generalize Fourier series on the circle to higher dimensions.
This allows us to obtain rates of approximation for invariant and geometrically stable functions with varying levels of smoothness, and to study the generalization properties of invariant kernel methods using kernels deﬁned on the sphere. Speciﬁcally, our main contributions are:
• By comparing spectral properties of usual kernels on the sphere with invariant ones, we ﬁnd that the latter provide improvements in sample complexity by a factor of the order of the size of the group when the sample size is large enough (Section 3).
• We study how this improvement factor varies with sample size, in terms of the structure of the group and on spectral properties of the permutation matrices it contains (Section 4).
• We extend the invariance analysis to geometrically stable functions, establishing similar gains in sample complexity that depend on the size of the transformation subset (Section 5).
Our proofs rely on comparing the dimension of invariant and non-invariant spherical harmonics at a given degree, and showing that their ratio decays to the inverse group size as the degree tends to inﬁnity. In contrast to [23], we consider the dimension to be ﬁxed and study non-parametric rates of convergence for potentially non-smooth target functions and general groups of permutations, while they consider a different regime in high dimension and focus on invariance to translation groups.