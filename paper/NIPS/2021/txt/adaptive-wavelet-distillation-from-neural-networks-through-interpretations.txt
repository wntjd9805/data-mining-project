Abstract
Recent deep-learning models have achieved impressive prediction performance, but often sacriﬁce interpretability and computational efﬁciency. Interpretability is crucial in many disciplines, such as science and medicine, where models must be carefully vetted or where interpretation is the goal itself. Moreover, interpretable models are concise and often yield computational efﬁciency. Here, we propose adaptive wavelet distillation (AWD), a method which aims to distill information from a trained neural network into a wavelet transform. Speciﬁcally, AWD pe-nalizes feature attributions of a neural network in the wavelet domain to learn an effective multi-resolution wavelet transform. The resulting model is highly predic-tive, concise, computationally efﬁcient, and has properties (such as a multi-scale structure) which make it easy to interpret. In close collaboration with domain experts, we showcase how AWD addresses challenges in two real-world settings: cosmological parameter inference and molecular-partner prediction. In both cases,
AWD yields a scientiﬁcally interpretable and concise model which gives predictive performance better than state-of-the-art neural networks. Moreover, AWD identi-ﬁes predictive features that are scientiﬁcally meaningful in the context of respective domains. All code and models are released in a full-ﬂedged package available on
Github. 1 1

Introduction
Recent advancements in deep learning have led to impressive increases in predictive performance.
However, the inability to interpret deep neural networks (DNNs) has led them to be characterized as black boxes. It is often critical that models are inherently interpretable [1–3], particularly in high-stakes applications such as medicine, biology, and policy-making. In these cases, interpretations which are relevant to a particular domain/audience [3] can ensure that models behave reasonably, identify when models will make errors, and make the models more amenable to inspection and improvement by domain experts. Moreover, interpretable models tend to be faster and more computationally efﬁcient than large neural networks. 1￿ github.com/Yu-Group/adaptive-wavelets 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
One promising approach to constructing interpretable models without sacriﬁcing prediction perfor-mance is model distillation. Model distillation [4–6] transfers the knowledge in one model (i.e., the teacher), into another model (i.e., the student), where the student model often has desirable properties, such as being more interpretable than the teacher model. Recent works have considered distilling a
DNN into inherently interpretable models such as a decision tree [7–9] or a global additive model [10], with some success. Here, we consider distilling a DNN into a learnable wavelet transform, which is a powerful tool to describe signals both in time (spatial) and frequency domains that has found numerous successful applications in physical and biomedical sciences.
Wavelet  transform !
X
!
!-1
"$
Pre-trained 
DNN
"#
DNN attributions for wavelet coefficients
Figure 1: Adaptive wavelet distillation uses attributions from a trained DNN to improve its wavelet transform, while satisfying constraints for reconstruction error and wavelet constraints. See Eq. 8 for the precise formulation of the optimization objective.
Wavelets have many properties amenable to interpretation: they can form an orthogonal basis, identify a sparse representation of a signal, and tile different frequencies and spatial locations (and sometimes rotations), allowing for multiresolution analysis. Most previous work has focused on hand-designed wavelets for different scenarios rather than wavelets which adapt to given data. Recent work has explored wavelets which adapt to an input data distribution, under the name optimized wavelets or adaptive wavelets [11–19]. Moreover, some work has used wavelets as part of the underlying structure of a neural network, as in wavelet networks / wavelet neural networks [20–25], or the scattering transform [26, 27]. However, none of them utilize wavelets for interpretable model distillation.
Fig 1 outlines Adaptive Wavelet Distillation (AWD), our approach for distilling a wavelet transform from a trained DNN. A key novelty of AWD is that it uses attributions from a trained DNN to improve the learned wavelets;2 this incorporates information not just about the input signals, as is done in previous work, but also about the target variable and the inductive biases present in the DNN.3
This paper deviates signiﬁcantly from a typical NeurIPS paper. While there has been an explosion of work in “interpretable machine learning” [28], there has been very limited development and grounding of these methods in the context of a particular problem and audience. This has led to much confusion about how to develop and evaluate interpretation methods [29, 30]; in fact, a major part of the issue is that interpretability cannot be properly deﬁned without the context of a particular problem and audience [3]. As interpretability and scientiﬁc machine learning enter a new era, researchers must ground themselves in real-world problems and work closely with domain experts.
This paper focuses on scientiﬁc machine learning—providing insight for a particular scientiﬁc au-dience into a chosen scientiﬁc problem— and from its outset, was designed to solve a particularly challenging cosmology problem in close collaboration with cosmologists. We showcase how AWD can inform relevant features in a fundamental problem in cosmology: inferring cosmological parame-ters from weak gravitational lensing convergence maps.4 In this case, AWD identiﬁes high-intensity peaks in the convergence maps and yields an easily interpretable model which outperforms state-of-the-art neural networks in terms of prediction performance. We next ﬁnd that AWD successfully provides prediction improvements in another scientiﬁc application (now in collaboration with cell-biology experts): molecular-partner prediction. In this case, AWD allows us to vet that the model’s use of clathrin corresponds to our domain knowledge about how clathrin must build up slowly then fall in order to predict a successful event. In both cases, the wavelet models from AWD concisely explains model behavior using extremely few parameters (e.g. 10), while also extracting compressed 2By attributions, we mean feature importance scores given input data and a pre-trained DNN. 3Though we focus on DNNs, AWD works for any black-box models for which we can attain attributions. 4For the purpose of this work, we work with simulated lensing maps. 2
representations of the input in comparison to a standard wavelet model. We hope that the depth and grounding of the scientiﬁc problems in this work can spur further interpretability research in real-world problems, where interpretability can be evaluated by and enrich domain knowledge, beyond benchmark data contexts such as MNIST [31] where the need for interpretability is less cogent. 2