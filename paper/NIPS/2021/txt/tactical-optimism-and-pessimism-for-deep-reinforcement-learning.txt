Abstract
In recent years, deep off-policy actor-critic algorithms have become a dominant approach to reinforcement learning for continuous control. One of the primary drivers of this improved performance is the use of pessimistic value updates to address function approximation errors, which previously led to disappointing performance. However, a direct consequence of pessimism is reduced exploration, running counter to theoretical support for the efﬁcacy of optimism in the face of uncertainty. So which approach is best? In this work, we show that the most effective degree of optimism can vary both across tasks and over the course of learning. Inspired by this insight, we introduce a novel deep actor-critic framework,
Tactical Optimistic and Pessimistic (TOP) estimation, which switches between optimistic and pessimistic value learning online. This is achieved by formulating the selection as a multi-arm bandit problem. We show in a series of continuous control tasks that TOP outperforms existing methods which rely on a ﬁxed degree of optimism, setting a new state of the art in challenging pixel-based environments.
Since our changes are simple to implement, we believe these insights can easily be incorporated into a multitude of off-policy algorithms. 1

Introduction
Reinforcement learning (RL) has begun to show signiﬁcant empirical success in recent years, with value function approximation via deep neural networks playing a fundamental role in this success [37, 49, 5]. However, this success has been achieved in a relatively narrow set of problem domains, and an emerging set of challenges arises when one considers placing RL systems in larger systems. In particular, the use of function approximators can lead to a positive bias in value computation [53], and therefore systems that surround the learner do not receive an honest assessment of that value.
One can attempt to turn this vice into a virtue, by appealing to a general form of the optimism-under-uncertainty principle—overestimation of the expected reward can trigger exploration of states and actions that would otherwise not be explored. Such exploration can be dangerous, however, if there is not a clear understanding of the nature of the overestimation.
⇤Work mostly completed at the Gatsby Unit. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This tension has not been resolved in the recent literature on RL approaches to continuous-control problems. On the one hand, some authors seek to correct the overestimation, for example by using the minimum of two value estimates as a form of approximate lower bound [20]. This approach can be seen as a form of pessimism with respect to the current value function. On the other hand,
[14] have argued that the inherent optimism of approximate value estimates is actually useful for encouraging exploration of the environment and/or action space. Interestingly, both sides have used their respective positions to derive state-of-the-art algorithms. How can this be, if their views are seemingly opposed? Our key hypothesis is the following:
The degree of estimation bias, and subsequent efﬁcacy of an optimistic strategy, varies as a function of the environment, the stage of optimization, and the overall context in which a learner is embedded.
This hypothesis motivates us to view optimism/pessimism as a spectrum and to investigate procedures that actively move along that spectrum during the learning process. We operationalize this idea by measuring two forms of uncertainty that arise during learning: aleatoric uncertainty and epistemic uncertainty. These notions of uncertainty, and their measurement, are discussed in detail in Section 5.1.
We then further aim to control the effects of these two kinds of uncertainty, making the following learning-theoretic assertion:
When the level of bias is unknown, an adaptive strategy can be highly effective.
In this work, we investigate these hypotheses via the development of a new framework for value estimation in deep RL that we refer to as Tactical Optimism and Pessimism (TOP). This approach acknowledges the inherent uncertainty in the level of estimation bias present, and rather than adopt a blanket optimistic or pessimistic strategy, it estimates the optimal approach on the ﬂy, by formulating the optimism/pessimism dilemma as a multi-armed bandit problem. Furthermore, TOP explicitly isolates the aleatoric and epistemic uncertainty by representing the environmental return using a distributional critic and model uncertainty with an ensemble. The overall concept is summarized in
Figure 1.
We show in a series of experiments that not only does the efﬁcacy of optimism indeed vary as we suggest, but TOP is able to capture the best of both worlds, achieving a new state of the art for challenging continuous control problems.
Our main contributions are as follows:
• Our work shows that the efﬁcacy of optimism for a ﬁxed function approximator varies across environments and during training for reinforcement learning with function approximation.
• We propose a novel framework for value estimation, Tactical Optimism and Pessimism (TOP), which learns to balance optimistic and pessimistic value estimation online. TOP frames the choice of the degree of optimism or pessimism as a multi-armed bandit problem.
• Our experiments demonstrate that these insights, which require only simple changes to popular algorithms, lead to state-of-the-art results on both state- and pixel-based control. 2