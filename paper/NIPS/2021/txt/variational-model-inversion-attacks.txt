Abstract
Given the ubiquity of deep neural networks, it is important that these models do not reveal information about sensitive data that they have been trained on. In model inversion attacks, a malicious user attempts to recover the private dataset used to train a supervised neural network. A successful model inversion attack should generate realistic and diverse samples that accurately describe each of the classes in the private dataset. In this work, we provide a probabilistic interpretation of model inversion attacks, and formulate a variational objective that accounts for both diversity and accuracy. In order to optimize this variational objective, we choose a variational family defined in the code space of a deep generative model, trained on a public auxiliary dataset that shares some structural similarity with the target dataset. Empirically, our method substantially improves performance in terms of target attack accuracy, sample realism, and diversity on datasets of faces and chest X-ray images. 1

Introduction
Thanks to recent advances, deep neural networks are now widely used in applications including facial recognition [Parkhi et al., 2015], intelligent automated assistants, and personalized medicine [Consor-tium, 2009; Sconce et al., 2005]. Powerful deep neural networks are trained on large datasets that could contain sensitive information about individuals. Publishers release only the trained models (e.g., on TensorFlow Hub [TensorFlow]), but not the original dataset to protect the privacy of individuals whose information is in the training set. Regrettably, the published model can leak information about the original training set [Geiping et al., 2020; Nasr et al., 2019; Shokri et al., 2017]. Attacks that exploit such properties of machine learning models are privacy attacks [Rigaki and Garcia, 2020].
Other real-world scenarios where an attacker can access models trained on private datasets include interaction with APIs (e.g., Amazon Rekognition [AWS]), or in the growing paradigm of federated learning [Bonawitz et al., 2019; Yang et al., 2019a], where a malicious insider can try to steal data from other data centers [Hitaj et al., 2017]. Kaissis et al. [2020] provides an overview of how these privacy concerns are hindering the widespread application of deep learning in the medical domain.
This paper focuses on the model inversion (MI) attack, a type of privacy attack that tries to recover the training set given access only to a trained classifier [Chen et al., 2020; Fredrikson et al., 2015, 2014;
Hidano et al., 2017; Khosravy et al., 2020; Yang et al., 2019b; Zhang et al., 2020]. The classifier under attack is referred to as the “target classifier” (see Figure 1). An important relevant application concerns identity recognition from face images, since facial recognition is a common approach to biometric identification [Galbally et al., 2014; Rathgeb and Uhl, 2011]. Once an attacker successfully reconstructs images of the faces of individuals in the private training set, they can use the stolen identity to break into otherwise secure systems.
Early studies of MI attacks focused on attacking classifiers whose inputs are tabular data [Fredrikson et al., 2015, 2014; Hidano et al., 2017] by directly performing inference in the input space. However, the same methods fail when the target classifier processes high-dimensional inputs, e.g., in the above 35th Conference on Neural Information Processing Systems (NeurIPS 2021), held virtually.
Figure 1: In model inversion attacks, the attacker tries to recover the original dataset used to train a classifier with access to only the trained classifier. In the two figures on the right, each row corresponds to a private identity – an individual whose images are not available to the attacker. The first five identities (not cherry-picked) were visualized. On the left are real data samples, and on the right are generated samples from our proposed method. example of a face recognizer. Recent methods cope with this high-dimensional search space by performing the attack in the latent space of a neural network generator [Khosravy et al., 2020; Yang et al., 2019b; Zhang et al., 2020]. Introducing a generator works well in practice, but this success is not theoretically understood. In this paper, we frame MI attacks as a variational inference (VI) [Blei et al., 2017; Jordan et al., 1999; Kingma and Welling, 2013] problem, and derive a practical objective.
This view allows us to justify existing approaches based on a deep generative model, and identify missing ingredients in them. Empirically, we find that our proposed objective leads to significant improvements: more accurate and diverse attack images, which are quantified using standard sample quality metrics (i.e., FID [Heusel et al., 2017], precision, and recall [Kynkäänniemi et al., 2019;
Naeem et al., 2020]).
In summary, our contributions and findings are:
• We view the model inversion (MI) attack problem as a variational inference (VI) problem.
This view allows us to derive a practical objective based on a statistical divergence, and provides a unified framework for analyzing existing methods.
• We provide an implementation of our framework using a set of deep normalizing flows [Dinh et al., 2014, 2016; Kingma and Dhariwal, 2018] in the extended latent space of a Style-GAN [Karras et al., 2020]. This implementation can leverage the hierarchy of learned representations, and perform targeted attacks while preserving diversity.
• Empirically, on the CelebA [Liu et al., 2015], and ChestX-ray [Wang et al., 2017] datasets, our method results in higher attack accuracy, and more diverse generation compared to existing methods. We provide thorough comparative experiments to understand which components contribute to improved target accuracy, sample realism, and sample diversity, and detail the benefits of the VI perspective.
Code can be found at https://github.com/wangkua1/vmi. 2 Problem Setup: Model Inversion Attack
In the problem of model inversion (MI) attack, the attacker has access to a “target classifier”: pTAR(y|x) : Rd → ∆C−1 where ∆C−1 to denote the (C − 1)-simplex, representing the C-dimensional probability where C is the number of classes. This target classifier is trained on the private target dataset, DTAR = {xi, yi}NTAR i=1 where x ∈ Rd and y ∈ {1, 2, ..., C}. We use pTAR(y|x) to denote the given target classifier, which is an approximation of the true conditional probability pTAR(y|x) of the underlying data generating distribution.
Goal. Given a target classifier pTAR(y|x), we wish to approximate the class conditional distribution pTAR(x|y) without having access to the private training set DTAR. 2
A good model inversion attack should approximate the Bayes posterior pTAR(x|y) ∝ pTAR(y|x)pTAR(x) well, and allow the attacker to generate realistic, accurate, and diverse samples. Notice, our goal shifts slightly from recovering the exact instances in the training set to recovering the data generating distribution of the training data. Let’s consider a scenario where the attacker intends to fake a victim’s identity. A good security system might test the realism of a sequence of inputs [Holland and Komogortsev, 2013]. More generally, faking someone’s identity requires passing a two-sample test [Holland and Komogortsev, 2013; Lehmann and Romano, 2006], which means the ability to generate samples from pTAR(x|y) is necessary. Another scenario can be an attacker trying to steal private/proprietary information. For example, a malicious insider in a federated learning setting can try to steal information from another datacenter by accessing the shared model [Hitaj et al., 2017]. In such a scenario, a good pTAR(x|y) model can reveal not only the distinguishing features of the data, but also its variations. 3