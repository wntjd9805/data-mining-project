Abstract
Local treatment effects are a common quantity found throughout the empirical sciences that measure the treatment effect among those who comply with what they are assigned. Most of the literature is focused on estimating the average of such quantity, which is called the “local average treatment effect (LATE)” [31]).
In this work, we study how to estimate the density of the local treatment effect, which is naturally more informative than its average. Specifically, we develop two families of methods for this task, namely, kernel-smoothing and model-based approaches. The kernel-smoothing-based approach estimates the density through some smooth kernel functions. The model-based approach estimates the density by projecting it onto a finite-dimensional density class. For both approaches, we derive the corresponding double/debiased machine learning-based estimators [13].
We further study the asymptotic convergence rates of the estimators and show that they are robust to the biases in nuisance function estimation. The use of the proposed methods is illustrated through both synthetic and a real dataset called 401(k). 1

Introduction
Controlled experimentation is one powerful tool used throughout the empirical sciences to infer the effect of a certain treatment on a given outcome. The idea is to randomize the treatment assignment so as to neutralize the effect of unobserved confounders. However, in some practical settings, it may be challenging to ascertain that individuals who are selected for treatment will follow their recommendations. Issues of non-compliance and unmeasured confounding are quite common and lead to the non-identification of treatment effects in many real-world cases [29, 50, 32, 56].
An approach known as instrumental variables (IVs) has been proposed to try to circumvent this issue
[68]. The idea is to find a set of variables (possibly singleton) that are not the target of the analysis by itself but that will help to control for the unobserved confounding between the treatment and the outcome. In particular, IVs are special variables that (i) are correlated with the treatment, (ii) do not directly influence the outcome, and (iii) are not affected by certain unmeasured confounders.
For concreteness, consider a study of the effect of 401(k) participation (X) on the distribution of net financial assets (Y ) [2]. This setting is represented in the causal graph in Fig. 1. Note that a dashed-bidirected arrow exists between X and Y , which in graphical language represents unobserved confounding affecting both X and Y . The variable Z in this model represents the eligibility of 401(k).
We note that Z qualifies as an instrument in this case – (i) it does affect the participation of 401(k) (X) and (ii) has no direct influence on the net financial asset (Y ), (iii) is not affected by unmeasured confounders between X and Y . The variable W represents observed covariates (e.g., gender, age, ethnicity, income, family size).
We are interested in the particular setting where only individuals who were offered the treatment may have access to it [31]. For instance, in the case of 401(k) participation (X = 1), only eligible 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: Densities of outcome Y among compliers under the treatment X = 1. All densities have a mean 0 and a variance 2. individuals (Z = 1) would be allowed to join the program. This assumption is known in the literature as monotonicity, which rules out the possibility that any units would respond contrary to the instrument. Under monotonicity, the causal effect in the subpopulation whose actual treatment
X coincides with the assigned treatment Z (called compliers) is identifiable [31, 2]. The average treatment effect (ATE) for the compliers is called ‘Local ATE’ (LATE) (or Complier average causal effects, CACE) [31].
The most common quantification of these effects in IV settings found in practice is the average (e.g.,
LATE). The average is certainly an informative summary; however, it may fail to capture significant differences in the causal distributions of the outcome. For instance, consider Fig. 2 that shows the densities of outcomes Y under treatments X = 1 among compliers which are generated from samples drawn from four synthetic data generating processes represented by the IV graph in Fig. 1 (further discussed in Sec. 5). All of the four distributions have the same mean 0 and variance 2. However, the difference in the LTE distributions is self-evident.
Most of the prior work on quantifying distributions of treat-ment effects focuses on estimating cumulative distribution functions (CDFs) or quantiles, and little attention has been given to estimating densities (refer to Sec. 1.1 for further comparison). As a complement to CDFs, densities have vari-ous advantages, including a more interpretable visualization of the distribution and generative capability of producing samples. One challenge with estimating densities is that n-rate while CDFs are pathwise-differentiable and enjoy estimators (n is the size of data), densities are not (i.e., they are non-regular), and therefore possess no influence n-rate estimators without approximations [7, functions nor
Ch. 3].
√
√
W
Z
X
Y
Figure 1: A causal graph for the IV setting. Bidirected arrows encode un-measured confounders.
In this paper, our goal is to provide methods to estimate densities of local treatment effects in IV settings under the monotonicity assumption. We develop two families of methods for this task based on kernel-smoothing and model-based approximations. The former smooths the density by convolution with a kernel function; the latter projects the density onto a finite-dimensional density class based on a distributional distance measure. For both approaches, we construct double/debiased machine learning (DML) style density estimators [43, 54, 52, 70, 13]. We analyze the asymptotic n-rate) even convergence properties of the estimators, showing that they can converge fast (i.e., when nuisance estimates converge slowly (e.g., n−1/4 rate) (a property called ‘debiasedness’1). We illustrate the proposed methods on synthetic and real data.
√ 1.1