Abstract
A challenging aspect of the bandit problem is that a stochastic reward is observed only for the chosen arm and the rewards of other arms remain missing. The dependence of the arm choice on the past context and reward pairs compounds the complexity of regret analysis. We propose a novel multi-armed contextual bandit algorithm called Doubly Robust (DR) Thompson Sampling employing the doubly-robust estimator used in missing data literature to Thompson Sampling with contexts (LinTS). Different from previous works relying on missing data techniques (Dimakopoulou et al. [2019], Kim and Paik [2019]), the proposed algorithm is designed to allow a novel additive regret decomposition leading to an improved regret bound with the order of ˜O(φ−2
T ), where φ2 is the minimum eigenvalue of the covariance matrix of contexts. This is the ﬁrst regret bound of
LinTS using φ2 without the dimension of the context, d. Applying the relationship between φ2 and d, the regret bound of the proposed algorithm is ˜O(d
T ) in d. A many practical scenarios, improving the bound of LinTS by a factor of beneﬁt of the proposed method is that it utilizes all the context data, chosen or not chosen, thus allowing to circumvent the technical deﬁnition of unsaturated arms used in theoretical analysis of LinTS. Empirical studies show the advantage of the proposed algorithm over LinTS.
√
√
√ 1

Introduction
Contextual bandit has been popular in sequential decision tasks such as news article recommendation systems. In bandit problems, the learner sequentially pulls one arm among multiple arms and receives random rewards on each round of time. While not knowing the compensation mechanisms of rewards, the learner should make his/her decision to maximize the cumulative sum of rewards. In the course of gaining information about the compensation mechanisms through feedback, the learner should carefully balance between exploitation, pulling the best arm based on information accumulated so far, and exploration, pulling the arm that will assist in future choices, although it does not seem to be the best option at the moment. Therefore in the bandit problem, estimation or learning is an important element besides decision making. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: The shaded data are used in complete record analysis (left) and DR method (right) under multi-armed contextual bandit settings. The contexts, rewards and DR imputing values are denoted by X, Y , and Y DR, respectively. The question mark refers to the missing reward of unchosen arms. t = 1 t = 2 t = 1 t = 2
Arm 1 X1(1)
Arm 2 X2(1)
Arm 3 Xa1 (1)
Arm 4 X4(1)
?
?
X1(2)
Xa2 (2)
Ya1 (1) X3(2)
X4(2)
?
?
Ya2 (2)
?
?
Arm 1 X1(1)
Arm 2 X2(1)
Arm 3 Xa1 (1)
Arm 4 X4(1)
Y DR (1) X1(2) 1
Y DR (1) Xa2 (2) 2
Y DR a1 (1) X3(2)
Y DR (1) X4(2) 4
Y DR (2) 1
Y DR a2 (2)
Y DR (2) 3
Y DR (2) 4
A challenging aspect of estimation in the bandit problem is that a stochastic reward is observed only for the chosen arm. Consequently, only the context and reward pair of the chosen arm is used for estimation, which causes dependency of the context data at the round on the past contexts and rewards. To handle this difﬁculty, we view bandit problems as missing data problems. The ﬁrst step in handling missing data is to deﬁne full, observed, and missing data. In bandit settings, full data consist of rewards and contexts of all arms; observed data consist of full contexts for all arms and the reward for the chosen arm; missing data consist of the rewards for the arms that are not chosen. Typical estimation procedures require both rewards and contexts pairs to be observed, and the observed contexts from the unselected are discarded (see Table 1). The analysis based on the completely observed pairs only is called complete record analysis. Most stochastic bandit algorithms utilize estimates based on complete record analysis. Estimators from complete record analysis are known to be inefﬁcient. In bandit setting, using the observed data whose probability of observation depends on previous rewards requires special theoretical treatment.
There are two main approaches to missing data: imputation and inverse probability weighting (IPW).
Imputation is to ﬁll in the predicted value of missing data from a speciﬁed model, and IPW is to use the observed records only but weight them by the inverse of the observation probability. The doubly robust (DR) method [Robins et al., 1994, Bang and Robins, 2005] is a combination of imputation and IPW tools. We provide a review of missing data and DR methods in supplementary materials.
The robustness against model misspeciﬁcation in missing data settings is insigniﬁcant in the bandit setting since the probability of observation or allocation to an arm is known. The merit of the DR method in the bandit setting is its ability to employ all the contexts including unselected arms.
We propose a novel multi-armed contextual bandit algorithm called Doubly Robust Thompson
Sampling (DRTS) that applies the DR technique used in missing data literature to Thompson Sampling with linear contextual bandits (LinTS). The main thrust of DRTS is to utilize contexts information for all arms, not just chosen arms. By using the unselected, yet observed contexts, along with a novel algorithmic device, the proposed algorithm renders a unique regret decomposition which leads to a novel regret bound without resorting to the technical deﬁnition of unsaturated arms used by Agrawal and Goyal [2014]. Since categorizing the arms into saturated vs. unsaturated plays a critical role in
T ) bound of the cumulative regret in many costing extra practical occasions compared to ˜O(d3/2 d, by circumventing it, we prove a ˜O(d
T ) shown in Agrawal and Goyal [2014].
√
√
√
The main contributions of this paper are as follows.
√
• We propose a novel contextual bandit algorithm that improves the cumulative regret bound d (Theorem 1) in many practical scenarios (Section 4.1). This of LinTS by a factor of improvement is attained mainly by deﬁning a novel set called super-unsaturated arms, that is utilizable due to the proposed estimator and resampling technique adopted in the algorithm.
• We provide a novel estimation error bound of the proposed estimator (Theorem 3) which depends on the minimum eigenvalue of the covariance matrix of the contexts from all arms without d.
• We develop a novel dimension-free concentration inequality for sub-Gaussian vector martingale (Lemma 4) and use it in deriving our regret bound in place of the self-normalized theorem by Abbasi-Yadkori et al. [2011].
• We develop a novel concentration inequality for the bounded matrix martingale (Lemma 6) which improves the existing result (Proposition 5) by removing the dependency on d in the bound. Lemma 6 also allows eliminating the forced sampling phases required in some bandit algorithms relying on Proposition 5 [Amani et al., 2019, Bastani and Bayati, 2020]. 2
All missing proofs are in supplementary materials. 2