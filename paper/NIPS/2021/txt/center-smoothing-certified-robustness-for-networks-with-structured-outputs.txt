Abstract
The study of provable adversarial robustness has mostly been limited to classi-ﬁcation tasks and models with one-dimensional real-valued outputs. We extend the scope of certiﬁable robustness to problems with more general and structured outputs like sets, images, language, etc. We model the output space as a metric space under a distance/similarity function, such as intersection-over-union, per-ceptual similarity, total variation distance, etc. Such models are used in many machine learning problems like image segmentation, object detection, generative models, image/audio-to-text systems, etc. Based on a robustness technique called randomized smoothing, our center smoothing procedure can produce models with the guarantee that the change in the output, as measured by the distance metric, remains small for any norm-bounded adversarial perturbation of the input. We apply our method to create certiﬁably robust models with disparate output spaces – from sets to images – and show that it yields meaningful certiﬁcates without signiﬁcantly degrading the performance of the base model. 1

Introduction
The study of adversarial robustness in machine learning (ML) has gained a lot of attention ever since deep neural networks (DNNs) have been demonstrated to be vulnerable to adversarial attacks. These attacks are generated by making tiny perturbations of the input that can completely alter a model’s predictions [56, 46, 23, 35]. They can signiﬁcantly degrade the performance of a model, like an image classiﬁer, and make it output almost any class of the attacker’s choice. However, these attacks are not limited just to classiﬁcation problems. They have also been shown to exist for DNNs with structured outputs like text, images, probability distributions, sets, etc. For instance, automatic speech recognition systems can be attacked with 100% success rate to output any phrase of the attackers choice [10]. Similar attacks can cause neural image captioning systems to produce speciﬁc target captions with high success-rate [11]. Quality of image segmentation models have been shown to degrade severely under adversarial attacks [2, 27, 30]. Facial recognition systems can be deceived to evade detection, impersonate authorized individuals and even render them completely ineffective
[59, 55, 20]. Image reconstruction models have been targeted to introduce unwanted artefacts or miss important details, such as tumors in MRI scans, through adversarial inputs [1, 50, 8, 12].
Super-resolution systems can be made to generate distorted images that can in turn deteriorate the performance of subsequent tasks that rely on the high-resolution outputs [14, 63]. Deep neural network based policies in reinforcement learning problems also have been shown to succumb to imperceptible perturbations in the state observations [21, 29, 4, 48]. Such widespread presence of adversarial attacks is concerning as it threatens the use of deep neural networks in critical systems, such as facial recognition, self-driving vehicles, medical diagnosis, etc., where safety, security and reliability are of utmost importance. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Adversarial defenses have mostly focused on classiﬁcation tasks [34, 6, 26, 17, 44, 25, 22]. Certiﬁed defenses based on convex-relaxation [61, 49, 53, 13, 54], interval-bound propagation [24, 28, 18, 47] and randomized smoothing [15, 36, 42, 51] that guarantee that the predicted class will remain the same in a certiﬁed region around the input point have also been studied. Compared to empirical robustness methods that are often shown to be broken by stronger attacks [9, 3, 58], procedures with provable robustness guarantees are of special importance to the study of robustness in ML as their guarantees hold regardless of improvements in attack strategies. Among these approaches, certiﬁed defenses based on randomized smoothing have been show to scale up to high-dimensional inputs, such as images, and does not need to make assumptions about the underlying model. The robustness certiﬁcates produced by these defenses are probabilistic, meaning that they hold with high probability and not absolute certainty.
Unlike classiﬁcation problems, where certiﬁcates guarantee that the predicted class remains un-changed under bounded-size perturbations, it is not immediately obvious what the goal of robustness should be for problems with structured outputs like images, text, sets, etc. While accuracy is the standard quality measure for classiﬁcation, more complex tasks may require other quality met-rics like total variation for images, intersection over union for object localization, earth-mover distance for distributions, etc. In general, neural networks can be cast as functions of the type f : Rk → (M, d) which map a k dimensional real-valued space into a metric space M with distance function d : M × M → R≥0. In this work, we design a randomized smoothing based technique to obtain provable robustness for functions of this type with minimal assumptions on the distance metric d. We generate a robust version ¯f such that the change in its output, as measured by d, is small for a small change in its input. More formally, given an input x and an (cid:96)2-perturbation size (cid:15)1, we produce a value (cid:15)2 with the guarantee that, with high probability,
∀x(cid:48) s.t. (cid:107)x − x(cid:48)(cid:107)2 ≤ (cid:15)1, d( ¯f (x), ¯f (x(cid:48))) ≤ (cid:15)2.
Our contributions: We develop center smoothing, a procedure to make functions like f provably ro-bust against adversarial attacks. For a given input x, center smoothing samples a collection of points in the neighborhood of x using a Gaussian smooth-ing distribution, computes the function f on each of these points and returns the center of the smallest ball enclosing at least half the points in the output space (see ﬁgure 1). Computing the minimum enclosing ball in the output space is equivalent to solving the 1-center problem with outliers (hence the name of our procedure), which is an NP-complete problem for a general metric [52]. We approximate it by comput-ing the point that has the smallest median distance to all the other points in the sample. We show that the output of the smoothed function is robust to input per-turbations of bounded (cid:96)2-size. We restrict the input perturbations to be inside an (cid:96)2-ball as the main focus of this work is on the output space of f . However, our method does not critically rely on the (cid:96)2 threat model or Gaussian smoothing noise, and can be adapted to other perturbations types and smoothing distributions. Although we deﬁne the output space as a metric, our proofs only require the symmetry property and triangle inequality to hold. Thus, center smoothing can also be applied to pseudometric distances that need not satisfy the identity of indiscernibles. Many distances deﬁned for images, such as total variation, cosine distance, perceptual distances, etc., fall under this category. Center smoothing steps outside the world of (cid:96)p metrics, and certiﬁes robustness in metrics like IoU/Jaccard distance for object localization, and total-variation, which is a good measure of perceptual similarity for images. In our experiments, we show that this method can produce meaningful certiﬁcates for a wide variety of output metrics without signiﬁcantly compromising the quality of the base model.
Figure 1: Center smoothing.