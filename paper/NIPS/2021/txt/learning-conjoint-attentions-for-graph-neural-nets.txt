Abstract
In this paper, we present Conjoint Attentions (CAs), a class of novel learning-to-attend strategies for graph neural networks (GNNs). Besides considering the layer-wise node features propagated within the GNN, CAs can additionally in-corporate various structural interventions, such as node cluster embedding, and higher-order structural correlations that can be learned outside of GNN, when computing attention scores. The node features that are regarded as signiﬁcant by the conjoint criteria are therefore more likely to be propagated in the GNN. Given the novel Conjoint Attention strategies, we then propose Graph conjoint attention networks (CATs) that can learn representations embedded with signiﬁcant latent features deemed by the Conjoint Attentions. Besides, we theoretically validate the discriminative capacity of CATs. CATs utilizing the proposed Conjoint At-tention strategies have been extensively tested in well-established benchmarking datasets and comprehensively compared with state-of-the-art baselines. The ob-tained notable performance demonstrates the effectiveness of the proposed Conjoint
Attentions. 1

Introduction
Graph neural networks (GNNs) have shown much success in the learning of graph structured data.
Amongst these noteworthy GNNs, attention-based GNNs [33] have drawn increasing interest lately, and have been applied to solve a plethora of real-world problems competently, including node classiﬁcation [17, 33], image segmentation [35], and social recommendations [30].
Empirical attention mechanisms adopted by GNNs aim to leverage the node features (node embed-dings) to compute the normalized correlations between pairs of nodes that are observed to connect.
Treating normalized correlations (attention scores/coefﬁcients) as the relative weights between node pairs, attention-based GNN typically performs a weighted sum of node features which are subse-quently propagated to higher layers. Compared with other GNNs, especially those that aggregate node features with predeﬁned strategies [1, 17, 19], attention-based GNNs provide a dynamical way for feature aggregation, which enables highly correlated features from neighboring nodes to be propagated in the multi-layer neural architecture. Representations that embed with multi-layer correlated features are consequently learned by attention-based GNNs, and can be used for various downstream tasks.
Though effective, present empirical graph attention has several shortcomings when aggregating node features. First, the computation of attention coefﬁcients is limited solely to the correlations of internal factors, i.e., layer-wise node features within the neural nets. External factors such as cluster structure and higher-order structural similarities, which comprise heterogeneous node-node relevance have remained underexplored to be positively incorporated into the computation of more purposeful attention scores. Second, the empirical attention heavily leaning on the node features may cause 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
over-ﬁtting in the training stage of neural nets [34]. The predictive power of attention-based GNNs is consequently limited.
To overcome the mentioned challenges, in this paper, we propose a class of generic graph attention mechanisms, dubbed here as Conjoint Attentions (CAs). Given CAs, we construct Graph conjoint attention networks (CATs) for different downstream analytical tasks. Different from previous graph attentions, CAs are able to ﬂexibly compute the attention coefﬁcients by not solely relying on layer-wise node embeddings, but also allowing the incorporation of purposeful interventions brought by factors external to the neural net, e.g., node cluster embeddings. With this, CATs are able to learn representations from features that are found as signiﬁcant by diverse criteria, thus increasing the corresponding predictive power. The main contributions of the paper are summarized as follows.
• We propose Conjoint Attentions (CAs) for GNNs. Different from popular graph attentions that rely solely on node features, CAs are able to incorporate heterogeneous learnable factors that can be internal and/or external to the neural net to compute purposeful and more appropriate attention coefﬁcients. The learning capability and hence performance of
CA-based GNNs is thereby enhanced with the proposed novel attention mechanisms.
• For the ﬁrst time, we theoretically analyze the expressive power of graph attention lay-ers considering heterogeneous factors for node feature aggregation, and the discriminant capacity of such attention layers, i.e., CA layers is validated.
• Given CA layers, we build and demonstrate the potential of Graph conjoint attention networks (CATs) for various learning tasks. The proposed CATs are comprehensively investigated on established and extensive benchmarking datasets with comparison studies to a number of state-of-the-art baselines. The notable results obtained are presented to verify and validate the effectiveness of the newly proposed attention mechanisms. 2