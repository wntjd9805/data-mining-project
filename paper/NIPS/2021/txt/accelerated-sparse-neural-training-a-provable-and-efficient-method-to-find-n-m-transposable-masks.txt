Abstract
Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we ﬁrst suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested
N : M ﬁne-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N : M ﬁne-grained block sparsity allows acceler-ation in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable ﬁne-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of ﬁnding the optimal transposable-mask as a minimum-cost ﬂow problem. Additionally, to speed up the minimum-cost
ﬂow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching be-tween different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N : M ﬁne-grained block sparsity model with little to no training. A reference implementation can be found at https:
//github.com/papers-submission/structured_transposable_masks. 1

Introduction
Deep neural networks (DNNs) have established themselves as the ﬁrst-choice tool for a wide range of applications, including computer vision and natural language processing. However, their impressive performance comes at a price of extensive infrastructure costs — as state-of-the-art DNNs may contain trillions of parameters [10] and require thousands of petaﬂops [6] for the training process.
For this reason, compression of DNNs training and inference process is a research topic of paramount
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
importance in both academia and industry. The main techniques of compression include quantization
[2, 34], knowledge distillation [17], and pruning [15, 23].
Pruning DNNs is one of the most popular and widely studied methods to improve DNN resource efﬁciency. The different pruning methods can be categorized into two different groups: unstructured and structured pruning. While the former can achieve a very high compression ratio, it usually fails in reducing the computational footprint in modern hardware. In contrast, structured pruning methods, such as block [41] or ﬁlter [23] pruning, are more hardware friendly. Unfortunately, these methods usually fail to keep the original accuracy for high compression ratios [37]. Finding an optimal structured sparsity pattern is still an ongoing research topic.
Recently, Nvidia [36] announced the A100 GPU, containing sparse tensor cores which are able to accelerate ﬁne-grained sparse matrix multiplication. The sparse tensor cores in A100 enable a 2x acceleration of regular matrix multiplication in DNNs, Y = W X, where W and X are weight and input matrices, respectively. The only requirement is that W would have a ﬁne-grained 2:4 sparsity structure, i.e. out of every four contiguous elements in W , two are pruned. Nvidia [36] suggested a two-fold scheme for pruning a pretrained dense model: (a) Deﬁne a ﬁne-grained 2:4 ﬁxed mask, and (b) retrain with the masked weights using original training schedule. Indeed, the Nvidia [36] approach is very appealing for the common case where a pretrained dense model is given.
While the Nvidia [36] method works well on many models, a pretrained model is not always given. In those cases, one has to ﬁrst train a dense model and only then try to prune it. To alleviate this demand,
Zhou et al. [43] suggested a method that trains from scratch a model with N : M ﬁne-grained mask, using a sparse-reﬁned straight-through estimator (SR-STE). Similarly to the quantization-aware-training methods [18], they maintain a dense copy of the weights and prune it in every iteration, passing the gradients using the straight-through estimator [5]. Since the mask dynamically changes while training, they suggest adding an extra weight decay on the masked (i.e. pruned) elements to reduce the mask changes during the training process. As opposed to Evci et al. [9] that aims to reduce memory footprint for sparse training from scratch, Zhou et al. [43] only eliminates the need to train a dense model before pruning it.
Figure 1: High-level overview of the different questions and their corresponding solutions proposed in this work. Motivated by understanding ﬁne-grained sparsity we ﬁrst suggest a measure to rank different sparsity mask, then we suggest a method to accelerate training with ﬁne-grained sparsity and ﬁnally propose a method to change the ﬁne-grained mask without re-training.
Motivated by these promising results, our goal here is to answer three remaining questions: (Fig. 1) 1. How to rank different types of sparsity masks? We suggest a new measure called “mask diversity", which is the ﬁrst to connect mask constraints and network accuracy (Section 3). 2. Can ﬁne-grained sparsity masks accelerate training? We start by observing both the forward and the backward matrix-multiplications involving the weight matrix W . Since the backward pass requires using the transposed matrix W T :
∂Loss
∂X
= W T ·
∂Loss
∂Y
, (1) and in general W T does not have an N : M ﬁne-grained sparsity structure (even if W has this structure), the methods suggested in [43, 36] accelerate only the forward pass matrix-multiplication, 2
Y = W X. Consequently, the current methods only utilize in part the sparse tensor cores to accelerate training. We propose a novel N : M transposable-ﬁne-grained sparsity mask, where the same mask can be used for both forward and backward passes (Fig. 2). We focus on accelerating sparse training in the two settings detailed above: (a) starting from a pretrained model, and (b) starting from scratch. For (a) we derive a novel algorithm to determine the optimal transposable-mask using a reduction to a min-cost ﬂow problem. For (b) we devise an approximation algorithm with an (almost) linear (in input-size) time complexity that produces a mask whose (cid:96)1 norm is within a factor of 2 from the optimal mask. We show the effectiveness of both methods (Section 4). (a) (b)
Figure 2: (a): A 4:8 structured pruning mask, as in Zhou et al. [43], Nvidia [36], capable of accelerating with sparse tensors core only the forward pass. (b): The suggested 4:8 transposable structured pruning mask capable of accelerating with sparse tensors core the forward and backward passes. 3. Can we change the type of sparsity mask structure without re-training? Different hardware devices can support different types of ﬁne-grained sparsity masks. Therefore, we suggest the
"Adaprune" method, which converts between types of sparsity masks (even from unstructured masks) without the need of re-training, and almost no degradation in accuracy (Section 5). 2