Abstract
Deep neural networks are susceptible to adversarially crafted, small and impercep-tible changes in the natural inputs. The most effective defense mechanism against these examples is adversarial training which constructs adversarial examples during training by iterative maximization of loss. The model is then trained to minimize the loss on these constructed examples. This min-max optimization requires more data, larger capacity models, and additional computing resources. It also degrades the standard generalization performance of a model. Can we achieve robustness more efﬁciently? In this work, we explore this question from the perspective of knowledge transfer. First, we theoretically show the transferability of robustness from an adversarially trained teacher model to a student model with the help of mixup augmentation. Second, we propose a novel robustness transfer method called
Mixup-Based Activated Channel Maps (MixACM) Transfer. MixACM transfers robustness from a robust teacher to a student by matching activated channel maps generated without expensive adversarial perturbations. Finally, extensive experi-ments on multiple datasets and different learning scenarios show our method can transfer robustness while also improving generalization on natural images. 1

Introduction
Deep learning models have achieved impressive performance on a wide variety of challenging tasks such as image recognition, natural language generation, game playing, etc. However, these models are susceptible to even small changes in the input space. It is possible to craft tiny changes in the input such that a model classiﬁes unaltered input correctly but classiﬁes the same input incorrectly after small and visually imperceptible perturbations [71, 27]. These altered inputs are known as adversarial examples, and this vulnerability of the state-of-the-art models has raised serious concerns [41, 58, 10, 47].
Many defense mechanisms have been proposed to train deep models to be robust against such adversarial perturbations. These techniques include defensive distillation [57], gradient regularization
[29, 58, 62], model compression [21, 45], activation pruning [23, 59], adversarial training [49], etc. Among them, Adversarial Training (AT) is one general strategy that is the most effective [4].
∗Equal Contribution.
†This work was carried out at Huawei Noah’s Ark Lab. The webpage for the project is available at: awais-rauf.github.io/MixACM
‡Corresponding Author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In general, adversarial training is a kind of data augmentation technique that trains a model on examples augmented with adversarial changes [49]. The adversarial training consists of an inner, iterative maximization loop to augment natural examples with adversarial perturbations, and an outer minimization loop similar to normal training. Many different methods have been introduced to improve robustness [76, 15, 52, 91, 79, 66, 94, 78, 7, 81, 39, 44, 56], but all of them are fundamentally based on the principle of training on adversarially augmented examples.
Adversarial training is more challenging compared with normal training. Better robust generalization requires larger capacity models [49, 53, 80]. Even over-parameterized models that can easily ﬁt data for normal training [92] may have insufﬁcient capacity to ﬁt adversarially augmented data
[97]. The sample complexity of adversarial training can be signiﬁcantly higher than normal training, and it requires more labeled [64] or unlabeled data [76, 15, 52, 91]. The inner maximization for the generation of adversarial perturbations for adversarial training is signiﬁcantly more expensive computationally as it requires iterative gradient steps with respect to the inputs (e.g., adversarial training takes ∼7x more time compared with normal training). Adversarial training also degrades the performance of a model on natural examples signiﬁcantly [53, 94].
Can we attain robustness more efﬁciently, i.e., with less data, small capacity models, without extra back-propagation steps, and at no signiﬁcant sacriﬁce of clean accuracy? For normal training, knowledge transfer is one possible paradigm for the efﬁcient training of deep neural networks. In normal knowledge transfer, a pre-trained teacher model is leveraged to efﬁciently train a student model on the same or similar dataset [70]. However, knowledge transfer methods are designed to transfer features related to normal generalization which may be at odds with robustness [74, 38].
A recent line of work has explored transferring or distilling robustness from pre-trained models
[32, 67, 25, 16]. Although these techniques are effective, they also require extra gradient computation and may not work in some cases.
In this work, we argue that a better approach for robustness transfer is to distill intermediate features of a robust teacher, generated on mixup examples. Our proposed approach does not require any additional gradient computation and works well with smaller models and fewer data samples. It can also effectively transfer robustness across models and datasets.
We begin with the theoretical analysis and show that adversarial loss of a student model can be bounded with two terms: natural loss and distance between the student and teacher model on mixup examples. To minimize the distance between robust teacher and student, we proposed a new distillation method. Our proposed distillation method is based on channel-wise activation analysis of robustness by Bai et al. [7] and other studies showing that adversarially trained models learn fundamentally different features [38, 74, 99].
Channels of a convolutional neural network learn different disentangled representations which, when combined, describe speciﬁc semantic concepts [8]. Samples of different classes activate different channels for an intermediate layer. However, adversarial examples make these channels activate more uniformly thereby destroying class-related information. Adversarial training solves this issue by forcing a similar channel-activation pattern for normal and adversarial examples [7]. Our proposed distillation method generates activated channel maps from a robust teacher which has already learned robust channel-activation patterns. The student, then, is forced to match these activated channel maps.
We have conducted extensive experiments to show the effectiveness of our method using various datasets and under different learning settings. We start by exhibiting the ability of our method to transfer robustness without requiring adversarial examples. We then show that our method can distill robustness from large pre-trained models to smaller models and it can transfer robustness across datasets. We also show that our method is capable to robustify a model even with a small number of examples. Concisely, our contributions are as follows: 1. We show that adversarial loss of a student model can be bounded with the distance between robust teacher and student model on mixup examples. 2. We propose a new method to transfer robustness from the intermediate features of robust pre-trained models. 3. We have demonstrated effectiveness of our method with experiments on CIFAR-10, CIFAR-100, and ImageNet datasets. We showed that MixACM can achieve adversarial robustness comparable to the state-of-the-art methods without generating adversarial examples. Our method also improves clean accuracy signiﬁcantly. 2
2