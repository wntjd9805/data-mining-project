Abstract
Active learning has proven to be useful for minimizing labeling costs by selecting the most informative samples. However, existing active learning methods do not work well in realistic scenarios such as imbalance or rare classes, out-of-distribution data in the unlabeled set, and redundancy. In this work, we propose SIMILAR (Submodular Information Measures based actIve LeARning), a uniﬁed active learning framework using recently proposed submodular information measures (SIM) as acquisition functions. We argue that SIMILAR not only works in standard active learning but also easily extends to the realistic settings considered above and acts as a one-stop solution for active learning that is scalable to large real-world datasets. Empirically, we show that SIMILAR signiﬁcantly outperforms existing active learning algorithms by as much as ≈ 5% − 18% in the case of rare classes and ≈ 5% − 10% in the case of out-of-distribution data on several image classiﬁcation tasks like CIFAR-10, MNIST, and ImageNet. SIMILAR is available as a part of the DISTIL toolkit: https://github.com/decile-team/distil. 1

Introduction
Deep neural networks (DNNs) have had a lot of success in a wide variety of domains. However, they require large labeled datasets which are often taxing, time-consuming, and expensive to obtain. Active learning (AL) [12, 13, 39, 3, 9] is a promising approach to solve this problem. It aims to select the most informative data points from an unlabeled dataset to be labeled in an adaptive manner with a human in the loop. The goal of AL is to achieve maximum accuracy of the model while minimizing the number of data points required to be labeled.
Current AL methods have been tested in rel-atively simple, clean, and balanced datasets.
However, real-world datasets are not clean and have a number of characteristics that makes learning from them challenging [10, 46, 47, 38, 1, 8]. Firstly, these real-world datasets are im-balanced, and some classes are very rare (e.g., Fig 1(a)). Examples of this imbalance are medical
Figure 1: Motivating scenarios for realistic active learning: (a) rare classes: digits 5 and 8 are rare; (b) redundancy: digits 0 and 1 are redundant; (c) out-of-distribution (OOD): letters A, R, B, F in digit classiﬁcation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
imaging domains where the cancerous images are rare. Secondly, real-world data has a lot of redun-dancy (e.g., Fig 1(b)). This redundancy is more prominent in datasets that are created by sampling frames from videos (e.g., footage from a car driving on a freeway or surveillance camera footage).
Thirdly, it is common to have out-of-distribution (OOD) (e.g., Fig 1(c)) data, where some part of the unlabeled data is not of concern to the task at hand. Given the amount of unlabeled data, it is not realistic to assume that these datasets can be cleaned manually; hence, it is the need of the hour to have active learning methods that are robust to such scenarios. We show that current AL approaches (including the state-of-the-art approach BADGE [3]) do not work well in the presence of the dataset biases described above. In this work, we address the following question: Can a machine learning model be trained using a single uniﬁed active learning framework that works for a broad spectrum of realistic scenarios? As a solution, we propose SIMILAR1, a uniﬁed active learning framework which enables active learning for many realistic scenarios like rare classes, out-of-distribution (OOD) data, and redundancy. 1.1