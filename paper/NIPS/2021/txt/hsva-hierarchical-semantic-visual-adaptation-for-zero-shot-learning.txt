Abstract
Zero-shot learning (ZSL) tackles the unseen class recognition problem, transferring semantic knowledge from seen classes to unseen ones. Typically, to guarantee desirable knowledge transfer, a common (latent) space is adopted for associating the visual and semantic domains in ZSL. However, existing common space learning methods align the semantic and visual domains by merely mitigating distribution disagreement through one-step adaptation. This strategy is usually ineffective due to the heterogeneous nature of the feature representations in the two domains, which intrinsically contain both distribution and structure variations. To address this and advance ZSL, we propose a novel hierarchical semantic-visual adaptation (HSVA) framework. Speciﬁcally, HSVA aligns the semantic and visual domains by adopting a hierarchical two-step adaptation, i.e., structure adaptation and distribu-tion adaptation. In the structure adaptation step, we take two task-speciﬁc encoders to encode the source data (visual domain) and the target data (semantic domain) into a structure-aligned common space. To this end, a supervised adversarial dis-crepancy (SAD) learning is proposed to adversarially minimize the discrepancy between the predictions of two task-speciﬁc classiﬁers, thus making the visual and semantic feature manifolds more closely aligned. In the distribution adaptation step, we directly minimize the Wasserstein distance between the latent multivari-ate Gaussian distributions to align the visual and semantic distributions using a common encoder. Finally, the structure and distribution adaptation are derived in a uniﬁed framework under two partially-aligned variational autoencoders. Extensive experiments on four benchmark datasets demonstrate that HSVA achieves superior performance on both conventional and generalized ZSL. The code is available at https://github.com/shiming-chen/HSVA . 1

Introduction
Over the past decade, signiﬁcant progress has been made in zero-shot learning (ZSL), which aims to recognize new classes during learning by exploiting the intrinsic semantic relatedness between seen and unseen categories [1, 2, 3]. Inspired by the way humans learn unknown concepts, side-information (e.g., attributes [4], word vectors [5], and sentences [6]) related to seen/unseen classes is employed to guarantee knowledge transfer between the seen/unseen data. Common space learning is one typical method for representing the relationship between the visual and semantic domains for
ZSL, which is key to dealing with the knowledge transfer [7]. However, existing common space
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
(a) One-step adaptation (b) Hierarchical adaptation (c) t-SNE visualization of features for CADA-VAE (d) t-SNE visualization of features for HSVA
Figure 1: Illustration of HSVA. Common space learning methods learn domain-aligned feature representations for semantic and visual domains in latent embedding space with semantic-visual adaptation. However, the heterogeneous feature representations of the semantic and visual domains vary in both distribution and structure. (a) One-step adaptation focuses on distribution alignment between visual and semantic domains, neglecting structure variation. This causes the semantic and visual distributions to be located in different manifolds, resulting in the misclassiﬁcation of some samples. (b) Hierarchical adaptation, in contrast, can learn an intrinsic and discriminative common space for semantic and visual feature representations by adopting sequential structure adaptation and distribution adaptation. We provide t-SNE visualizations [14] of features learned by (c) CADA-VAE
[11] and (d) our HSVA on 10 classes from AWA2 (the "o" and "x" indicate visual and semantic features, respectively, and different colors denote different classes). Best viewed in color. learning methods align the distributions of the semantic and visual domains with one-step adaptation
[8, 9, 7, 10, 11, 12, 13], while neglecting the fact that the heterogeneous feature representations of distinct semantic and visual domains have both distribution and structure variations. Thus, their performance is limited. In this work, we propose a novel common space learning formulation that leverages a hierarchical semantic-visual adaptation to learn an intrinsic common space for a better alignment of the two heterogeneous representation domains.
The heterogeneous feature representations of the semantic and visual domains are distinct [15, 16].
Although one-step adaptation learns distribution-aligned feature representations for the semantic and visual domains using two different mapping models (e.g., encoders) and distribution alignment constraints (e.g., maximum mean discrepancy (MMD)), the two distributions are located in different manifolds since the one-step adaptation does not consider the manifold structure relationship between visual and semantic representations, as shown in Figure 1 (a) and (c). If we take the Euclidean distance or manifold distance [17, 18] to measure the relationship between different classes, the classiﬁer inevitably misclassiﬁes some samples, thus leading to inferior ZSL performance. As such, mapping the semantic and visual domains into an intrinsic and desirable common space for conducting an effective ZSL is highly necessary.
To learn structure- and distribution-aligned feature representations for distinct visual and semantic domains, we propose a hierarchical semantic-visual adaptation (HSVA) framework to learn an intrinsic common space that contains the essential multi-modal information associated with unseen classes (Figure 1 (b) and (d)). HSVA aligns the semantic and visual domains with structure adaptation (SA) and distribution adaptation (DA) in a uniﬁed framework, under two partially-aligned variational autoencoders. In the SA, we use two task-speciﬁc encoders to encode source data (visual domain) and target data (semantic domain), respectively, into a structure-aligned space, which is learned through supervised adversarial discrepancy (SAD) learning to minimize the discrepancy between 2
the predictions of the two task-speciﬁc classiﬁers. This encourages the visual and semantic feature manifolds to be closer to each other, thus aligning the manifold structure variation. In the DA, we map the structure-aligned features into a distribution-aligned common space using a common encoder, which is optimized by minimizing the Wasserstein distance between the latent multivariate
Gaussian distributions of the visual and semantic domains. The common encoder preserves the structure-aligned representations in the DA. Thus, HSVA can learn a structure and distribution-aligned common space for visual and semantic feature representations, which is better than the distribution-aligned feature representations learned by existing common space learning methods
[8, 9, 7, 10, 11, 19]. To the best of our knowledge, this is the ﬁrst work that leverages hierarchical semantic-visual adaptation to address heterogeneous feature alignment in ZSL. Following [11], we conduct extensive experiments on four challenging benchmark datasets, i.e., CUB, SUN, AWA1, and AWA2, under both conventional and generalized ZSL settings. The proposed HSVA achieves consistent improvement over the existing common space learning methods on all datasets. We show qualitatively that the structural variation is important for the interaction between visual and semantic domains. Moreover, we show signiﬁcantly better common space for visual and semantic feature representations than the existing common space learning methods, e.g., CADA-VAE [11]. 2