Abstract
One of the main challenges in model-based reinforcement learning (RL) is to decide which aspects of the environment should be modeled. The value-equivalence (VE) principle proposes a simple answer to this question: a model should capture the aspects of the environment that are relevant for value-based planning. Technically,
VE distinguishes models based on a set of policies and a set of functions: a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the correct result when applied to the functions. As the number of policies and functions increase, the set of VE models shrinks, eventually collapsing to a single point corresponding to a perfect model. A fundamental question underlying the VE principle is thus how to select the smallest sets of policies and functions that are sufﬁcient for planning. In this paper we take an important step towards answering this question. We start by generalizing the concept of VE to order-k counterparts deﬁned with respect to k applications of the Bellman operator. This leads to a family of VE classes that increase in size as k → ∞. In the limit, all functions become value functions, and we have a special instantiation of VE which we call proper VE or simply PVE. Unlike VE, the PVE class may contain multiple models even in the limit when all value functions are used. Crucially, all these models are sufﬁcient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment. We construct a loss function for learning PVE models and argue that popular algorithms such as MuZero can be understood as minimizing an upper bound for this loss. We leverage this connection to propose a modiﬁcation to MuZero and show that it can lead to improved performance in practice. 1

Introduction
It has long been argued that, in order for reinforcement learning (RL) agents to solve truly complex tasks, they must build a model of the environment that allows for counterfactual reasoning [29]. Since representing the world in all its complexity is a hopeless endeavor, especially under capacity con-straints, the agent must be able to ignore aspects of the environment that are irrelevant for its purposes.
This is the premise behind the value equivalence (VE) principle, which provides a formalism for focusing on the aspects of the environment that are crucial for value-based planning [17].
VE distinguishes models based on a set of policies and a set of real-valued scalar functions of state (henceforth, just functions). Roughly, a model is said to be VE to the environment if the Bellman operators it induces for the policies yield the same result as the environment’s Bellman operators when applied to the functions. The policies and functions thus become a “language” to specify which parts of the environment a model should capture. As the number of policies and functions increase the requirements on the model become more stringent, which is to say that the class of VE models shrinks. In the limit, the VE class collapses to a single point corresponding to a perfect model. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Although this result is reassuring, in practice we want to stop short of collapsing—after all, at this point the agent is no longer ignoring irrelevant aspects of the environment.
A fundamental question is thus how to select the smallest sets of policies and functions such that a resulting VE model is sufﬁcient for planning. In this paper we take an important additional step in this direction: we show that the VE principle can be formulated with respect to value functions only.
This result drastically reduces the space of functions that must be considered by VE, as in general only a small fraction of the set of all functions will qualify as value functions in a given environment.
Since every policy has an associated value function, this new formulation of VE removes the need for selecting functions, only requiring policies. We name our new formulation proper value equivalence (PVE) to emphasize its explicit use of value functions.
PVE has several desirable properties. Unlike with VE, the class of PVE models does not collapse to a singleton in the limit. This means that, even if all value functions are used, we generally end up with multiple PVE models—which can be beneﬁcial if some of these are easier to learn or represent than others. Crucially, all of these models are sufﬁcient for planning, meaning that they will yield an optimal policy despite the fact that they may ignore many aspects of the environment.
Finally, we make more precise Grimm et al.’s [17] suggestion that the VE principle may help explain the good empirical performance of several modern algorithms [38, 33, 24, 12, 30]. Speciﬁcally, we show that, with mild assumptions, minimizing the loss of the MuZero algorithm [31] can be understood as minimizing a PVE error. We then leverage this connection to suggest a modiﬁcation to
MuZero and show a small but signiﬁcant improvement in the Atari Learning Environment [3]. 2