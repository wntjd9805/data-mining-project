Abstract
Computer-aided design (CAD) is the most widely used modeling approach for technical design. The typical starting point in these designs is 2D sketches which can later be extruded and combined to obtain complex three-dimensional assem-blies. Such sketches are typically composed of parametric primitives, such as points, lines, and circular arcs, augmented with geometric constraints linking the primitives, such as coincidence, parallelism, or orthogonality. Sketches can be represented as graphs, with the primitives as nodes and the constraints as edges.
Training a model to automatically generate CAD sketches can enable several novel workﬂows, but is challenging due to the complexity of the graphs and the hetero-geneity of the primitives and constraints. In particular, each type of primitive and constraint may require a record of different size and parameter types. We propose
SketchGen as a generative model based on a transformer architecture to address the heterogeneity problem by carefully designing a sequential language for the primitives and constraints that allows distinguishing between different primitive or constraint types and their parameters, while encouraging our model to re-use information across related parameters, encoding shared structure. A particular highlight of our work is the ability to produce primitives linked via constraints that enables the ﬁnal output to be further regularized via a constraint solver. We evaluate our model by demonstrating constraint prediction for given sets of primitives and full sketch generation from scratch, showing that our approach signiﬁcantly out performs the state-of-the-art in CAD sketch generation. 1

Introduction
Computer Aided Design (CAD) tools are popular for modeling in industrial design and engineering.
The employed representations are popular due to their compactness, precision, simplicity, and the fact that they are directly ‘understood’ by many fabrication procedures.
CAD models are essentially a collection of primitives (e.g., lines, arcs) that are held together by a set of relations (e.g., collinear, parallel, tangent). Complex shapes are then formed by a sequence of CAD operations, forming a base shape in 2D that is subsequently lifted to 3D using extrusion operations. Thus, at the core, most CAD models are coplanar constrained sketches, i.e., a sequence of planar curves linked by constraints. For example, CAD models in some of the largest open-sourced datasets like Fusion360 [37] and SketchGraphs [27] are thus structured, or sketches drawn in algebraic systems like Cinderella [1]. Such sequences are not only useful for shape creation, but also facilitate easy editing and design exploration, as relations are preserved when individual elements are edited.
CAD representations are heterogeneous. First, different instructions have their unique sets of parame-ters, with different parameter counts, data types, and semantics. Second, constraint speciﬁcations involve links (i.e., pointers) to existing primitives or parts of primitives. Further, the same ﬁnal shape 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
can be equivalently realized with different sets of primitives and constraints. Thus, CAD models neither come in regular structures (e.g., image grids) nor can they be easily encoded using a ﬁxed length encoding. The heterogeneous nature of CAD models makes it difﬁcult to adopt existing deep learning frameworks to train generative models. One easy-to-code solution is to simply ignore the constraints and rasterize the primitives into an image. While such a representation is easy to train, the output is either in the image domain, or needs to be converted to primitives via some form of differential rendering. In either case, the constraints and the regularization parametric primitives provide is lost, and the conversion to primitives is likely to be unreliable. A slightly better option is to add padding to the primitive and constraint representations, and then treat them as ﬁxed-length encodings for each primitive/constraint. However this only hides the difﬁculty, as the content of the representation is still heterogeneous, the generator now additionally has to learn the length of the padding that needs to be added, and the increased length is inefﬁcient and becomes unworkable for complex CAD models.
We introduce SketchGen as a generative framework for learning a distribution of constrained sketches.
Our main observation is that the key to working with a heterogeneous representation is a well-structured language. We develop a language for sketches that is endowed with a simple syntax, where each sketch is represented as a sequence of tokens. We explicitly use the syntax tree of a sequence as additional guidance for our generative model. Our architecture makes use of transformers to capture long range dependencies and outputs both primitives along with their constraints. We can aggressively quantize/approximate the continuous primitive parameters in our sequences as long as the inter-primitive constraints can be correctly generated. A sampled graph can then be converted, if desired, to a physical sketch by solving a constrained optimization using traditional solvers, removing errors that were introduced by the quantization.
We evaluate SketchGen on one of the largest publicly-available constrained sketch datasets Sketch-Graph [27]. We compare with a set of existing and contemporary works, and report noticeable improvement in the quality of the generated sketches. We also present ablation studies to evaluate the efﬁcacy of our various design choices. 2