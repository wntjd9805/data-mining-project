Abstract
We study the problem of learning control policies for complex tasks given by logical speciﬁcations. Recent approaches automatically generate a reward function from a given speciﬁcation and use a suitable reinforcement learning algorithm to learn a policy that maximizes the expected reward. These approaches, however, scale poorly to complex tasks that require high-level planning. In this work, we develop a compositional learning approach, called DIRL, that interleaves high-level planning and reinforcement learning. First, DIRL encodes the speciﬁcation as an abstract graph; intuitively, vertices and edges of the graph correspond to regions of the state space and simpler sub-tasks, respectively. Our approach then incorporates reinforcement learning to learn neural network policies for each edge (sub-task) within a Dijkstra-style planning algorithm to compute a high-level plan in the graph. An evaluation of the proposed approach on a set of challenging control benchmarks with continuous state and action spaces demonstrates that it outperforms state-of-the-art baselines. 1

Introduction
Reinforcement learning (RL) is a promising approach to automatically learning control policies for continuous control tasks—e.g., for challenging tasks such as walking [11] and grasping [6], control of multi-agent systems [31, 22], and control from visual inputs [28]. A key challenge facing RL is the difﬁculty in specifying the goal. Typically, RL algorithms require the user to provide a reward function that encodes the desired task. However, for complex, long-horizon tasks, providing a suitable reward function can be a daunting task, requiring the user to manually compose rewards for individual subtasks. Poor reward functions can make it hard for the RL algorithm to achieve the goal; e.g., it can result in reward hacking [3], where the agent learns to optimize rewards without achieving the goal.
Recent work has proposed a number of high-level languages for specifying RL tasks [5, 29, 24, 34, 19].
A key feature of these approaches is that they enable the user to specify tasks compositionally—i.e., the user can independently specify a set of short-term subgoals, and then ask the robot to perform a complex task that involves achieving some of these subgoals. Existing approaches for learning from high-level speciﬁcations typically generate a reward function, which is then used by an off-the-shelf
RL algorithm to learn a policy. Recent works based on Reward Machines [19, 35] have proposed
RL algorithms that exploit the structure of the speciﬁcation to improve learning. However, these algorithms are based on model-free RL at both the high- and low-levels instead of model-based
RL. Model-free RL has been shown to outperform model-based approaches on low-level control tasks [10]; however, at the high-level, it is unable to exploit the large amount of available structure.
Thus, these approaches scale poorly to long horizon tasks involving complex decision making. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Left: The 9-rooms environment, with initial region S0 in the bottom-left, an obstacle O in the middle-left, and three subgoal regions S1, S2, S3 in the remaining corners. Middle top: A user-provided speciﬁcation φex. Middle bottom: The abstract graph Gex DIRL constructs for φex.
Right: Learning curves for our approach and some baselines; x-axis is number of steps and y-axis is probability of achieving φex.
We propose DIRL, a novel compositional RL algorithm that leverages the structure in the speciﬁcation to decompose the policy synthesis problem into a high-level planning problem and a set of low-level control problems. Then, it interleaves model-based high-level planning with model-free RL to compute a policy that tries to maximize the probability of satisfying the speciﬁcation. In more detail, our algorithm begins by converting the user-provided speciﬁcation into an abstract graph whose edges encode the subtasks, and whose vertices encode regions of the state space where each subtask is considered achieved. Then, it uses a Djikstra-style forward graph search algorithm to compute a sequence of subtasks for achieving the speciﬁcation, aiming to maximize the success probability.
Rather than compute a policy to achieve each subtask beforehand, it constructs them on-the-ﬂy for a subtask as soon as Djikstra’s algorithm requires the cost of that subtask.
We empirically evaluate1 our approach on a “rooms environment” (with continuous state and action spaces), where a 2D agent must navigate a set of rooms to achieve the speciﬁcation, as well as a challenging “fetch environment” where the goal is to use a robot arm to manipulate a block to achieve the speciﬁcation. We demonstrate that DIRL signiﬁcantly outperforms state-of-the-art deep RL algorithms for learning policies from speciﬁcations, such as SPECTRL, TLTL, QRM and HRM, as well as a state-of-the-art hierarchical RL algorithm, R-AVI, that uses state abstractions, as the complexity of the speciﬁcation increases. In particular, by exploiting the structure of the speciﬁcation to decouple high-level planning and low-level control, the sample complexity of DIRL scales roughly linearly in the size of the speciﬁcation, whereas the baselines quickly degrade in performance. Our results demonstrate that DIRL is capable of learning to perform complex tasks in challenging continuous control environments. In summary, our contributions are as follows:
• A novel compositional algorithm to learn policies for continuous domains from complex high-level speciﬁcations that interleaves high-level model-based planning with low-level
RL.
• A theoretical analysis of our algorithm showing that it aims to maximize a lower bound on the satisfaction probability of the speciﬁcation.
• An empirical evaluation demonstrating that our algorithm outperforms several state-of-the-art algorithms for learning from high-level speciﬁcations.
Motivating example. Consider an RL-agent in the environment of interconnected rooms in Figure 1.
The agent is initially in the blue box, and their goal is to navigate to either the top-left room S1 or the bottom-right room S2, followed by the top-right room S3, all the while avoiding the red block O.
This goal is formally captured by the SPECTRL speciﬁcation φex (middle top). This speciﬁcation is comprised of four simpler RL subtasks—namely, navigating between the corner rooms while avoiding the obstacle. Our approach, DIRL, leverages this structure to improve learning. First, based on the speciﬁcation alone, it constructs the abstract graph Gex (see middle bottom) whose vertices represent the initial region and the three subgoal regions, and the edges correspond to subtasks (labeled with a safety constraint that must be satisﬁed).
However, Gex by itself is insufﬁcient to determine the optimal path—e.g., it does not know that there is no path leading directly from S2 to S3, which is a property of the environment. These differences 1Our implementation is available at https://github.com/keyshor/dirl. 2
can be represented as (a priori unknown) edge costs in Gex. At a high level, DIRL trains a policy
πe for each edge e in Gex, and sets the cost of e to be c(e; πe) = − log P (e; πe), where P (e; πe) is the probability that πe succeeds in achieving e. For instance, for the edge S0 → S1, πe is trained to reach S1 from a random state in S0 while avoiding O. Then, a na¨ıve strategy for identifying the optimal path is to (i) train a policy πe for each edge e, (ii) use it to estimate the edge cost c(e; πe), and (iii) run Djikstra’s algorithm with these costs.
One challenge is that πe depends on the initial states used in its training—e.g., training πe for e = S1 → S3 requires a distribution over S1. Using the wrong distribution can lead to poor performance due to distribution shift; furthermore, training a policy for all edges may unnecessarily waste effort training policies for unimportant edges. To address these challenges, DIRL interweaves training policies with the execution of Djikstra’s algorithm, only training πe once Djikstra’s algorithm requires the cost of edge e. This strategy enables DIRL to scale to complex tasks; in our example, it quickly learns a policy that satisﬁes the speciﬁcation with high probability. These design choices are validated empirically—as shown in Figure 1, DIRL quickly learns to achieve the speciﬁcation, whereas it is beyond the reach of existing approaches.