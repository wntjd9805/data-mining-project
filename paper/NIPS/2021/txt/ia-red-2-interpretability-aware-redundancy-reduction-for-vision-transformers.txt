Abstract
The self-attention-based model, transformer, is recently becoming the leading backbone in the field of computer vision. In spite of the impressive success made by transformers in a variety of vision tasks, it still suffers from heavy computation and intensive memory costs. To address this limitation, this paper presents an
Interpretability-Aware REDundancy REDuction framework (IA-RED2). We start by observing a large amount of redundant computation, mainly spent on uncor-related input patches, and then introduce an interpretable module to dynamically and gracefully drop these redundant patches. This novel framework is then ex-tended to a hierarchical structure, where uncorrelated tokens at different stages are gradually removed, resulting in a considerable shrinkage of computational cost. We include extensive experiments on both image and video tasks, where our method could deliver up to 1.4× speed-up for state-of-the-art models like
DeiT [53] and TimeSformer [3], by only sacrificing less than 0.7% accuracy. More importantly, contrary to other acceleration approaches, our method is inherently interpretable with substantial visual evidence, making vision transformer closer to a more human-understandable architecture while being lighter. We demonstrate that the interpretability that naturally emerged in our framework can outperform the raw attention learned by the original visual transformer, as well as those gener-ated by off-the-shelf interpretation methods, with both qualitative and quantitative results. Project Page: http://people.csail.mit.edu/bpan/ia-red/. 1

Introduction
Transformer, a self-attention-based architecture processing sequential input without any recurrent or convolutional operations, has set off a storm in the computer vision literature recently. By dividing the input image into a series of patches and then tokenizing them with linear transformation, the transformer can effectively process the visual data in different modalities [13, 53, 54, 28, 3, 17, 66].
Despite its versatility, the transformer is always deeply troubled with inefficient computation and its vague interpretability. The vision transformer suffers heavy computational costs, especially when the input sequence is long. As the attention module in the vision transformer computes the fully-connected relations among all of the input patches, the computational cost is then quadratic with regard to the length of the input sequence. On the other hand, previous works [6, 8] have already shown the vulnerable interpretability of the original vision transformer, where the raw attention comes from the architecture sometimes fails to perceive the informative region of the input images.
Recently, more designs of vision transformer architecture [34, 65, 18, 56, 14, 9, 3] are proposed to get higher accuracy with less computational cost. Although these methods anchor good trade-offs between efficiency and accuracy, their compression makes the vision transformer even more lack interpretability. Most of these methods assume that the input sequences are sampled from a regular visual input in a fixed shape rule, and thus their network architectures are not flexible as well, which makes the vision transformer (1) no longer able to process the input sequence with arbitrary length as the architecture is designed for a specific input shape; (2) neither model-agnostic nor task agnostic anymore; or (3) neglect the fact that the model redundancy is also input-dependant. We yet argue that 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Two examples of redundancy reduction in vision transformers. Our proposed multi-head interpreters serve as a model-agnostic module which are built on top of the existing transformer-based backbones for different tasks, including image recognition and video action recognition. there is no inherent tension between efficiency and interpretability, and achieving them both does not have to pay design flexibility as a price. Indeed, starting from the philosophy of Occam’s razor, the law of parsimony, or always pursuing more compact solutions when possible, is always treated as a rule-of-thumb for pursing interpretability, especially in complicated fitting problems [21].
This paper aims to seek the win-win between efficiency and interpretability while keeping the flexibility and versatility of the original vision transformer. We propose a novel Interpretability-Aware
REDundancy REDuction (IA-RED2) framework for reducing the redundancy of vision transformers.
The key mechanism that IA-RED2 uses to increase efficiency is to dynamically drop some less informative patches in the original input sequence so that the length of the input sequence could be reduced. While the original vision transformer tokenizes all of the input patches, it neglects the fact that some of the input patches are redundant and such redundancy is input-dependant (see from
Figure 1). As the computational complexity of the attention module is quadratically linear to the input sequence length, the effect of reducing input sequence length would be magnified in the amount of the computation. Motivated by this, we leverage the idea of dynamic inference [39, 37, 38, 61, 57], and adopt a policy network (referred to as multi-head interpreter) to decide which patches are uninformative and then discard them. Our proposed method is inherently interpretability-aware as the policy network learns to discriminate which region is crucial for the final prediction results.
To summarize, the main contributions of our work includes: (1) We propose IA-RED2, the first interpretability-aware redundancy reduction framework for vision transformer. (2) Our IA-RED2 framework is one of the first input-dependent dynamic inference framework for vision transformer, which adaptively decides the patch tokens to compute per input instance. (3) IA-RED2 is both model-agnostic and task-agnostic. We conduct experiments with IA-RED2 framework spanning different tasks, including image recognition and action recognition, and different models, including
DeiT [53], TimeSformer [3]. (4) We attain promising interpretable results (shown in Figure 3) over baselines, with a 1.4× acceleration over DeiT on image recognition tasks, and a 4× acceleration over TimeSformer on video action recognition task while largely maintaining the accuracy. We also provide both qualitative results regarding interpretability with heatmaps by our method and those from other baseline methods like raw attention, MemNet [29]; as well as the quantitative comparison with current state-of-the-art model interpretability methods, such as GradCAM [44], on
ImageNet-Segmentation [16] dataset with the weakly-supervised image segmentation task. 2