Abstract
Optimal transport maps between two probability distributions µ and ν on Rd have found extensive applications in both machine learning and statistics. In practice, these maps need to be estimated from data sampled according to µ and ν. Plug-in estimators are perhaps most popular in estimating transport maps in the ﬁeld of computational optimal transport. In this paper, we provide a comprehensive analysis of the rates of convergences for general plug-in estimators deﬁned via barycentric projections. Our main contribution is a new stability estimate for barycentric projections which proceeds under minimal smoothness assumptions and can be used to analyze general plug-in estimators. We illustrate the usefulness of this stability estimate by ﬁrst providing rates of convergence for the natural discrete-discrete and semi-discrete estimators of optimal transport maps. We then use the same stability estimate to show that, under additional smoothness assumptions of
Sobolev type or Besov type, kernel smoothed or wavelet based plug-in estimators respectively speed up the rates of convergence and signiﬁcantly mitigate the curse of dimensionality suffered by the natural discrete-discrete/semi-discrete estimators.
As a by-product of our analysis, we also obtain faster rates of convergence for plug-in estimators of W2(µ, ν), the Wasserstein distance between µ and ν, under the aforementioned smoothness assumptions, thereby complementing recent results in Chizat et al. (2020). Finally, we illustrate the applicability of our results in obtaining rates of convergence for Wasserstein barycenter between two probability distributions and obtaining asymptotic detection thresholds for some recent optimal-transport based tests of independence.

Introduction 1
Given two random variables X ∼ µ and Y ∼ ν, where µ, ν are probability measures on Rd, d ≥ 1, the problem of ﬁnding a “nice" map T0(·) such that T0(X) ∼ ν has numerous applications in machine learning such as domain adaptation and data integration [34, 35, 38, 48, 61, 112], dimension reduction [12, 66, 90], generative models [60, 81, 88, 110], to name a few. Of particular interest is the case when T0(·) is obtained by minimizing a cost function, a line of work initiated by
Gaspard Monge [97] in 1781 (see (1.1) below), in which case T0(·) is termed an optimal transport (OT) map and has applications in shape matching/transfer problems [29, 47, 107, 121], Bayesian statistics [46, 75, 80, 108], econometrics [15, 28, 45, 50, 54], nonparametric statistical inference [39– 41, 113, 114]; also see [111, 128, 129] for book-length treatments on the subject. In this paper, we will focus on the OT map obtained using the standard squared Euclidean cost function, i.e.,
T0 := argmin
T :T #µ=ν
E(cid:107)X − T (X)(cid:107)2, (1.1) where T #µ = ν means T (X) ∼ ν for X ∼ µ. The estimation of T0 has attracted a lot of interest in recent years due to its myriad applications (as stated above) and interesting geometrical properties (see [19, 56, 91] and Deﬁnition 1.1 below). In practice, the main hurdle in constructing estimators for
T0 is that the explicit forms of the measures µ, ν are unknown; instead only random samples
X1, . . . , Xm ∼ µ and
Y1, . . . , Yn ∼ ν 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
are available. A natural strategy in this scenario is to estimate T0 using (cid:101)Tm,n, where (cid:101)Tm,n is computed as in (1.1) with µ and ν replaced by (cid:101)µm and (cid:101)νn which are empirical approximations of µ and ν based on X1, . . . , Xm and Y1, . . . , Yn respectively (see Deﬁnition 1.2). Such estimators are often called plug-in estimators and have been used extensively; see [7, 30, 67, 93, 94, 102, 116].
The main goal of this paper is to study the rates of convergence of general plug-in estimators of T0 under a uniﬁed framework. We show that when (cid:101)µm and (cid:101)νn are chosen as (cid:98)µm and (cid:98)νn respectively, where (cid:98)µm and (cid:98)νn are the standard empirical distributions supported on m and n atoms, i.e., (cid:98)µm := 1 m m (cid:88) i=1
δXi and (cid:98)νn := 1 n n (cid:88) j=1
δYj , (1.2) (cid:101)Tm,n (appropriately deﬁned using Deﬁnition 1.2) converges at a rate of m−2/d + n−2/d for d ≥ 4 in the sense of (1.8). This rate happens to be minimax optimal under minimal smoothness assumptions (see [72, Theorem 6]) but suffers from the curse of dimensionality. We next show that, if µ and
ν are known to admit sufﬁciently smooth densities, it is possible to apply kernel or wavelet based smoothing techniques on (cid:98)µm and (cid:98)νn to obtain plug-in estimators that mitigate the aforementioned curse of dimensionality.
Our next contribution pertains to the estimation of W 2 2 (µ, ν) (the squared Wasserstein distance), see (1.3) below, a quantity of independent interest in statistics and machine learning with applications in structured prediction [51, 89], image analysis [18, 59], nonparametric testing [16, 106], generative modeling [10, 96], etc. In this paper, we also obtain rates of convergence for plug-in estimators
W 2 2 ((cid:101)µm, (cid:101)νn) of W 2 2 (µ, ν). We show that kernel smoothing (cid:98)µm and (cid:98)νn can be used to obtain plug-in estimators of W 2 2 (µ, ν) that mitigate the curse of dimensionality as opposed to a direct plug-in approach using (cid:98)µm and (cid:98)νn (as used in [30, Theorem 2]). This provides an answer to the open question of estimating W 2 2 (µ, ν) when µ, ν admit smooth densities laid out in [30]. 1.1