Abstract
In typical few-shot learning, each task is not equipped with enough data to be learned in isolation. To cope with such data scarcity, meta-representation learn-ing methods train across many related tasks to ﬁnd a shared (lower-dimensional) representation of the data where all tasks can be solved accurately. It is hypothe-sized that any new arriving tasks can be rapidly trained on this low-dimensional representation using only a few samples. Despite the practical successes of this approach, its statistical and computational properties are less understood. Recent theoretical studies either provide a highly suboptimal statistical error, or require many samples for every task, which is infeasible in the few-shot learning setting.
Moreover, the prescribed algorithms in these studies have little resemblance to those used in practice or they are computationally intractable. To understand and explain the success of popular meta-representation learning approaches such as
ANIL [43], MetaOptNet [36], R2D2 [9], and OML [33], we study a alternating gradient-descent minimization (AltMinGD) method (and its variant alternating min-imization (AltMin) in the Appendix) which underlies the aforementioned methods.
For a simple but canonical setting of shared linear representations, we show that
AltMinGD achieves nearly-optimal estimation error, requiring only Ω(polylog d) samples per task. This agrees with the observed efﬁcacy of this algorithm in the practical few-shot learning scenarios. 1

Introduction
Common real world tasks follow a long tailed distribution where most of the tasks only have a small number of labeled examples [51]. Collecting more clean labels is often costly (e.g., medical imaging).
As each task does not have enough examples to be learned in isolation under this few-shot learning scenario, meta-learning attempts to jointly learn across a large number of tasks to exploit some structural similarities among those tasks.
One popular approach is to learn a shared representation, where new arriving tasks can be solved accurately [45]. The premise is that (i) there is a shared low-dimensional representation fU (x) ∈ Rr represented by a task-independent meta-parameter U and (ii) a simple linear model of (cid:104)vi, fU (x)(cid:105) can make accurate prediction on the i-th task with a task-speciﬁc parameter vi. Once the representation fU has been learnt, we can rapidly adapt to new arriving tasks as the representation dimension r is much smaller than the dimension d of the input data. This approach is becoming increasingly popular with a growing list of recent applications [33, 36, 9, 42, 43, 27, 47, 14, 13, 18] and has been empirically shown to achieve the state-of-the-art performances on benchmark few-shot learning datasets [47, 14, 43].
∗†thekump2@illinois.edu, ‡prajain@google.com, ¶pnetrapalli@google.com (Part of the work was done while at Google Research India), and ±sewoong@cs.washington.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
These successes rely on a simple but effective training algorithm which alternately updates U and
{vi} which we call AltMinGD (Alternating Minimization and Gradient Descent). Suppose we are given t tasks, and the i-th task is associated with a dataset {(x(i) j=1 of size m. In this paper, we closely follow the formulation of [47], which solves for a function fU : Rd → Rr (typically a deep neural network) and a task-speciﬁc linear model vi ∈ Rr on a choice of a loss (cid:96)(·, ·): j ∈ Rd, y(i) j )}m min
U (cid:110) (cid:88) i∈[t] min vi∈Rr (cid:88) j∈[m] (cid:96)((cid:104)vi, fU (x(i) j )(cid:105), y(i) j ) (cid:111)
, (1) by alternately applying a (stochastic) gradient descent step of U in the outer loop (for given vi’s) and numerically ﬁnding the optimal solution vi in the inner loop (for a given U ). Several closely related algorithms have been proposed, including separating training-set used for the inner loop and the validation-set used for the outer-loop [43, 36, 9, 5], early stopping the inner-loop [33], applying to datasets with imbalanced data sizes [42, 14], and proposing new architectures and regularizers [27].
There is an increasing list [47, 14, 43] of numerical evidences showing that these meta representation learning improves upon competing approaches including MAML [20] and its variants [23, 32, 39].
Further, [43] provides experimental evidences that shared representation is the dominant component in the efﬁcacy of MAML [20], even though MAML does not explicitly seek a shared representation.
In this paper, we analyze the computational and statistical properties of AltMinGD and its variant
AltMin under the simple but canonical setting of learning a shared linear representation for linear regression tasks [48]. The fundamental question of interest is: as the number of tasks grow, does
AltMinGD learn the underlying r-dimensional shared representation (subspace) more accurately, and consequently make more accurate predictions on new tasks? This question is critical in explaining the empirical success in few-shot learning where the number of tasks in the training set is large while each of those tasks is data starved. Further, in settings like crowdsourcing or bioinformatics, collecting more data on new tasks is easier than collecting more data on existing tasks.
Contributions. We analyze the widely adopted AltMinGD and prove a nearly optimal error rate. We show that AltMinGD requires only m = Ω(log t + log log(1/ε)) samples per task to achieve an error of ε in estimating the representation U when we have a large enough number t of tasks in the training data and assuming a constant dimensionality r = O(1) of the representation. Under this condition,
AltMinGD achieves an error decaying as (cid:101)O(σ(cid:112)d/mt), which nearly matches the fundamental lower bound. Together, these analyses imply that AltMinGD is able to compensate for having only a few samples per task (small m) by having many few-shot tasks (large t), signiﬁcantly improving the state-of-the-art (see Table 1). Note that the log log(1/ε) dependence of m is hidden in the (cid:101)Ω notation and is not explicitly visible from our main theorems or the table. A ﬁne grained analysis showing this dependence is provided in Theorem 9 in Appendix C.
We follow the proof strategy of alternating minimization algorithms for matrix sensing [30, 38], but there are important differences making the analysis challenging. First, the meta-learning dataset does not satisfy Restricted Isometry Property (RIP) central in the existing matrix sensing analysis, and hence none of the technical lemmas can be directly applied. We leverage on the task diversity property in Assumption 2, to prove all necessary concentration bounds. Next, there is an inherent asymmetry in the problem; we require accurate estimation of U for generalization to new arriving tasks (which is the primary goal of meta-learning), but we do not necessarily require accurate estimation of vi’s. We exploit this to ensure accurate estimation of U with a small m.
Our analysis of AltMinGD leads to a fundamental theoretical question: is the condition m = Ω(log t) necessary? We introduce a variation AltMinGD-S, which at each iteration selects a subset of tasks that are well-behaved (covering the r-dimensional subspace of current estimated U ) and uses (the empirical risk of) only those tasks in the update. While log t dependence is unavoidable if we require all t tasks to be well-behaved, ensuring a large fraction to be well-behaved requires smaller m. When the noise is sufﬁciently small with variance O(1/ log t), we show that AltMinGD-S requires only m = Ω(log log(1/ε)) (with no dependence on t) to estimate the shared representation accurately.
Inspired by a long line of successes in matrix completion and matrix sensing [30], we also analyze a variation of AltMinGD that alternately applies minimization for U and {vi} updates, which we call
AltMin, and prove a slightly improved guarantees at an extra computational cost of a factor of dr2.
Notations: [n] = {1, 2, . . . , n}. (cid:107)A(cid:107) and (cid:107)A(cid:107)F denote the spectral and Frobenius norms of a matrix
A. (cid:104)A, B(cid:105) denotes the inner product. A† is the Moore-Penrose pseudoinverse. x ∼ N (0, Id×d) means 2
that x is a d dimensional standard isotropic Gaussian random vector. (cid:101)O, (cid:101)Ω and (cid:101)Θ hide logarithmic terms in dimension d, rank r, tolerance ε and other problems parameters. 1.1