Abstract
We study the ofﬂine reinforcement learning (ofﬂine RL) problem, where the goal is to learn a reward-maximizing policy in an unknown Markov Decision Process (MDP) using the data coming from a policy µ. In particular, we consider the sample complexity problems of ofﬂine RL for ﬁnite-horizon MDPs. Prior works study this problem based on different data-coverage assumptions, and their learning guarantees are expressed by the covering coefﬁcients which lack the explicit
In this work, we analyze the Adaptive characterization of system quantities.
Pessimistic Value Iteration (APVI) algorithm and derive the suboptimality upper bound that nearly matches

 (cid:115)
O

H (cid:88) (cid:88) h=1 sh,ah dπ(cid:63) h (sh, ah) (V (cid:63)
VarPsh,ah dµ h(sh, ah) h+1 + rh) (cid:114) 1 n
 . (1) h(sh, ah) > 0 if dπ(cid:63)
In complementary, we also prove a per-instance information-theoretical lower bound under the weak assumption that dµ h (sh, ah) > 0. Differ-ent from the previous minimax lower bounds, the per-instance lower bound (via local minimaxity) is a much stronger criterion as it applies to individual instances separately. Here π(cid:63) is a optimal policy, µ is the behavior policy and dµ h is the marginal state-action probability. We call (1) the intrinsic ofﬂine reinforcement learning bound since it directly implies all the existing optimal results: minimax rate under uniform data-coverage assumption, horizon-free setting, single policy concentrability, and the tight problem-dependent results. Later, we extend the result to the assumption-free regime (where we make no assumption on µ) and obtain the assumption-free intrinsic bound. Due to its generic form, we believe the intrinsic bound could help illuminate what makes a speciﬁc problem hard and reveal the fundamental challenges in ofﬂine RL. 1

Introduction
In ofﬂine reinforcement learning (ofﬂine RL Levine et al. [2020], Lange et al. [2012]), the goal is to learn a reward-maximizing policy in an unknown environment (Markov Decision Process or
MDP) using the historical data coming from a (ﬁxed) behavior policy µ. Unlike online RL, where the agent can keep interacting with the environment and gain new feedback by exploring unvisited state-action space, ofﬂine RL usually populates when such online interplays are expensive or even unethical. Due to its nature of without the access to interact with the MDP model (which causes the distributional mismatches), most of the literature that study the sample complexity / provable efﬁciency of ofﬂine RL (e.g. Le et al. [2019], Chen and Jiang [2019], Xie and Jiang [2020, 2021], 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Yin et al. [2021a,b], Ren et al. [2021], Rashidinejad et al. [2021], Xie et al. [2021b]) rely on making different data-coverage assumptions for making the problem learnable and provide the near-optimal worst-case performance bounds that depend on their data-coverage coefﬁcients. Those results are valuable in general as they do not depend on the structure of the particular problem, therefore, remain valid even for pathological MDPs. But is this good enough?
In practice, the empirical performances of ofﬂine reinforcement learning (e.g. Gulcehre et al. [2020],
Fu et al. [2020, 2021], Janner et al. [2021]) are often far better than what those non-adaptive / problem-independent bounds would indicate. Although empirical evidence can help explain why we may observe better or worse performances on different MDPs, a systematic understanding of what types of decision processes and what kinds of behavior policies are inherently easier or more challenging for ofﬂine RL is lacking. Besides, despite the fact that a non-adaptive bound can learn even the pathological examples within the assumption family, there is no guarantee for the instances outside the family. However, practical ofﬂine reinforcement learning problems are usually beyond the scope of certain data-coverage assumptions, which limits the applicability of those results. Can we make as few assumptions as possible? Or even more, what can we guarantee when no assumption is made about ofﬂine learning?
Those motivate us to derive the provably efﬁcient bounds that are adaptive to the individual instances but only require minimal assumptions so they can be widely applied in most cases. Ideally, such bounds should characterize the system structures of the speciﬁc problems, hold even for peculiar instances that do not satisfy the standard data-coverage assumptions, and recover the worst-case guarantees when the assumptions are satisﬁed. As mentioned in Zanette and Brunskill [2019], a fully adaptive characterization in RL is important as it might bring considerable saving in the time spent designing domain-speciﬁc RL solutions and in training a human expert to judge and recognize the complexity of different problems. 1.1 Our contribution
In this work, we provide the analysis for the adaptive pessimistic value iteration (APVI) (Algorithm 1) with ﬁnite horizon time-inhomogeneous (non-stationary) MDPs and derive a strong adaptive bound that is near-optimal under the weak assumption dµ h (sh, ah) > 0 (Theorem 4.1).
Speciﬁcally, our bound (quantity (1)) explicitly depends on the marginal importance ratios (between the optimal policy π(cid:63) and the behavior policy µ) and the per-step conditional variances. In addition, we provide an instance-dependent (local minimax) lower bound (Theorem 4.3) to certify (1) is nearly optimal at the instance level for ofﬂine learning and call it the intrinsic ofﬂine learning bound. The intrinsic bound has the following consequences. h(sh, ah) > 0 if dπ(cid:63)
• In the non-adaptive / worst-case regime (4.1-4.3), the intrinsic bound implies (cid:101)O(H 3/dm(cid:15)2) complexity under the uniform data-coverage 2.1, ˜O(H 3SC (cid:63)/(cid:15)2) complexity under the single policy concentrability assumption 2.3 and (cid:101)O(H/dm(cid:15)2) complexity when the sum of rewards is bounded by 1. All of those are optimal in their respectively regimes [Yin et al., 2021a, Rashidinejad et al., 2021, Xie et al., 2021b, Ren et al., 2021];
• In the adaptive domain (4.4), the intrinsic bound implies the tight problem-dependent counterpart of Zanette and Brunskill [2019], yields ˜O(H 3/ndm) fast convergence in the deterministic systems, has improved complexity in the partially deterministic systems and a family of highly mixing problems, and remains optimal when reducing to the tabular contextual bandits.
Beyond the above, due to the generic form of the intrinsic bound, we could come up with as many problem instances (that are of our interests) as possible and study their properties. In this sense, the intrinsic bound helps illuminate the fundamental nature of ofﬂine RL.
Furthermore, as a step towards assumption-free ofﬂine reinforcement learning, we build a modiﬁed
AVPI and obtain an adaptive bound that could characterize the suboptimality gap in the state-action space that is agnostic to the behavior policy (Theorem 5.1). To the best of our knowledge, all of these results are the ﬁrst of its kinds. 2
1.2