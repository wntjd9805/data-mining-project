Abstract
Recent advances in the video question answering (i.e., VideoQA) task have achieved strong success by following the paradigm of ﬁne-tuning each clip-text pair independently on the pretrained transformer-based model via supervised learning.
Intuitively, multiple samples (i.e., clips) should be interdependent to capture similar visual and key semantic information in the same video. To consider the interdepen-dent knowledge between contextual clips into the network inference, we propose a Siamese Sampling and Reasoning (SiaSamRea) approach, which consists of a siamese sampling mechanism to generate sparse and similar clips (i.e., siamese clips) from the same video, and a novel reasoning strategy for integrating the inter-dependent knowledge between contextual clips into the network. The reasoning strategy contains two modules: (1) siamese knowledge generation to learn the inter-relationship among clips; (2) siamese knowledge reasoning to produce the reﬁned soft label by propagating the weights of inter-relationship to the predicted candidates of all clips. Finally, our SiaSamRea can endow the current multimodal reasoning paradigm with the ability of learning from inside via the guidance of soft labels. Extensive experiments demonstrate our SiaSamRea achieves state-of-the-art performance on ﬁve VideoQA benchmarks, e.g., a signiﬁcant +2.1% gain on MSRVTT-QA, +2.9% on MSVD-QA, +1.0% on ActivityNet-QA, +1.8% on
How2QA and +4.3% (action) on TGIF-QA. 1

Introduction
By inferring the correct answers for video-based questions, video question answering (VideoQA) has attracted increasing research attention due to its huge application potential, as a fundamen-tal technique for vision-to-language reasoning.
The task involves acquisition and manipulation of spatio-temporal visual representations guided by the compositional semantics of the linguistic clues [32, 15, 21, 34]. Existing works can roughly be divided into two aspects. One aspect is to ex-plore a powerful multimodal transformer-based net-work [22, 2, 34, 45] trained on large-scale datasets (e.g., COCO Captions [3] and HowTo100M [29]).
∗Corresponding Author: xiaon6@mail.sysu.edu.cn
Figure 1: Different sampling mechanisms for video frames. (a) Traditional methods use dense clip features from full-length videos. (b) A re-cent approach [22] suggests sparsely sampled clips for end-to-end learning. (c) Our siamese sampling to generate similar semantic clips . 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 2: (a) Traditional multimodal learning uses dense sampling for video frames and extracts individual clip-text features via ofﬂine encoders. (b) Sparse based multimodal learning beneﬁts from sparsely sampled clips (independent) and raw text tokens for end-to-end modeling. (c) Self-driven based multimodal learning utilizes cross-relationship between anchor clip (vanchor) and siamese clips (v1, v2, v3) as knowledge to lead the model to create soft labels for self-supervised learning.
Note that the modules in (c) using same color mean the weights are shared.
The other aspect aims at exploring the structure reasoning for semantic alignment between vision and language (e.g., Hierarchical Reasoning [21], Heterogeneous Graph Alignment [15, 46]and Object
Relation Reasoning [16]). Both of them solely consider each clip-text pair separately and ignore the correlation between contextual clips in the same video.
By further analyzing the existing multimodal learning paradigm and showing their difference in
Figure 2, we observe that current methods suffer from a key drawback: every clip-text pair is regarded as individual and independent during the training. Such drawback overlooks the rich interaction of contextual clips from the same video2. We believe that the internal interaction information from same videos can be helpful for further enhancing the network learning. Hence, to provide a remedy to this dilemma, we present a self-driven Siamese Sampling and Reasoning (SiaSamRea) framework learning from inside, by using the internal contextual semantics of interdependent video-aware data (e.g., clips) from the same video in the training process.
First of all, our SiaSamRea (in Figure 3) consists of two key parts: (1) a siamese sampling shown in
Figure 1 (c) to extract multiple similar clips from the same video, which is motivated by ClipBert [22]; (2) a reasoning strategy named self-driven based multimodal learning as shown in Figure 2 (c). The strategy contains two modules: (i) a siamese knowledge generation module to calculate the correlation matrix between the anchor clip-text pair (e.g., fanchor) and siamese clip-text pairs (e.g., f1, f2, f3); (ii) a siamese knowledge reasoning module to produce soft labels for self-supervised reﬁnement during the training. Finally, the labels are applied as auxiliary training supervision to enhance the network.
Compared with previous sampling mechanism, our siamese sampling as shown in Figure 1 (c) not only is sparse but also is able to generate multiple similar clips for constructing their internal relationships. Speciﬁcally, our siamese sampling captures clips at different start frames with same interval time in the same video, which can constrain the global semantic of each clip to be similar (i.e., each clip can represent the consistent video content from a global perspective).
Furthermore, to fully utilize the siamese clips, we explore a new reasoning strategy namely self-driven based multimodal learning as shown in Figure 2 (c). There are three steps in the strategy for using the interaction of internal clips from the same video into the training. At the 1st step, the anchor clip and siamese clips are obtained by using sparse sampling and siamese sampling, respectively. Then the two types of clips individually cooperated with the text are fed into the model to extract clip-text features, including anchor clip-text feature (e.g., vanchor) and siamese clip-text features (e.g., v1, v2, v3). At the 2nd step, the internal contextual interaction3 between the anchor clip-text feature and siamese 2We call the contextual clips from the same video as internal clips containing anchor and siamese clips 3The internal contextual interaction is regarded as siamese knowledge in our paper. 2
clip-text features is calculated via our siamese knowledge generation module. At the 3rd step, a siamese knowledge reasoning module is proposed to use the siamese knowledge applying to several predicted candidates (e.g., panchor, p1, p2, p3) for adaptively reasoning out the reﬁned soft label.
Here, the siamese knowledge is applied for adaptively reasoning. Because it is hard to distinguish which candidate is critical to the accuracy from several predictions during the training. Finally, we use the reﬁned soft label to further distill our model for high-quality representation generation.
Our contributions are three-fold: (i) We propose a novel end-to-end framework named SiaSamRea for learning from inside on VideoQA task, by using siamese sampling and reasoning to integrate the interdependent semantics of clips from the same video into the training process. (ii) A novel reasoning strategy is carefully designed for building the soft guidance from the interdependent knowledge between internal clips, which consists of a siamese knowledge generation module and a siamese knowledge reasoning module. (iii) Experiments on ﬁve commonly-used VideoQA benchmarks show the superior ability of our SiaSamRea and demonstrate the effectiveness of our proposed components.
Not that our method only teaches the network with interdependent knowledge during the training, which does not bring any extra burden (e.g., computation, memory and parameters) in the inference. 2