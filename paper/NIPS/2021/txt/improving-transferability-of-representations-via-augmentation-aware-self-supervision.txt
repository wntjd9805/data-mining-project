Abstract
Recent unsupervised representation learning methods have shown to be effective in a range of vision tasks by learning representations invariant to data augmentations such as random cropping and color jittering. However, such invariance could be harmful to downstream tasks if they rely on the characteristics of the data augmen-tations, e.g., location- or color-sensitive. This is not an issue just for unsupervised learning; we found that this occurs even in supervised learning because it also learns to predict the same label for all augmented samples of an instance. To avoid such failures and obtain more generalizable representations, we suggest to optimize an auxiliary self-supervised loss, coined AugSelf, that learns the difference of augmentation parameters (e.g., cropping positions, color adjustment intensities) between two randomly augmented samples. Our intuition is that AugSelf encour-ages to preserve augmentation-aware information in learned representations, which could be beneﬁcial for their transferability. Furthermore, AugSelf can easily be incorporated into recent state-of-the-art representation learning methods with a negligible additional training cost. Extensive experiments demonstrate that our simple idea consistently improves the transferability of representations learned by supervised and unsupervised methods in various transfer learning scenarios. The code is available at https://github.com/hankook/AugSelf. 1

Introduction
Unsupervised representation learning has recently shown a remarkable success in various domains, e.g., computer vision [1, 2, 3], natural language [4, 5], code [6], reinforcement learning [7, 8, 9], and graphs [10]. The representations pretrained with a large number of unlabeled data have achieved outstanding performance in various downstream tasks, by either training task-speciﬁc layers on top of the model while freezing it or ﬁne-tuning the entire model.
In the vision domain, the recent state-of-the-art methods [1, 2, 11, 12, 13] learn representations to be invariant to a pre-deﬁned set of augmentations. The choice of the augmentations plays a crucial role in representation learning [2, 14, 15, 16]. A common choice is a combination of random cropping, horizontal ﬂipping, color jittering, grayscaling, and Gaussian blurring. With this choice, learned representations are invariant to color and positional information in images; in other words, the representations lose such information.
On the contrary, there have also been attempts to learn representations by designing pretext tasks that keep such information in augmentations, e.g., predicting positional relations between two patches of
∗Work done while at University of Michigan. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of the proposed method, AugSelf, that learns augmentation-aware information by predicting the difference between two augmentation parameters ω1 and ω2. Here, x is an original image, v = tω(x) is an augmented sample by an augmentation tω, f is a feature extractor such as
ResNet [20], and g is a classiﬁer for supervised learning or a projection MLP head for the recent unsupervised learning methods [1, 2, 12, 13]. an image [17], solving jigsaw puzzles [18], or predicting color information from a gray image [19].
These results show the importance of augmentation-speciﬁc information for representation learning, and inspire us to explore the following research questions: when is learning invariance to a given set of augmentations harmful to representation learning? and, how to prevent the loss in the recent unsupervised learning methods?
Contribution. We ﬁrst found that learning representations with an augmentation-invariant objective might hurt its performance in downstream tasks that rely on information related to the augmentations.
For example, learning invariance against strong color augmentations forces the representations to con-tain less color information (see Figure 2a). Hence, it degrades the performance of the representations in color-sensitive downstream tasks such as the Flowers classiﬁcation task [21] (see Figure 2b).
To prevent this information loss and obtain more generalizable representations, we propose an auxiliary self-supervised loss, coined AugSelf, that learns the difference of augmentation parameters between the two augmented samples (or views) as shown in Figure 1. For example, in the case of random cropping, AugSelf learns to predict the difference of cropping positions of two randomly cropped views. We found that AugSelf encourages the self-supervised representation learning methods, such as SimCLR [2] and SimSiam [13], to preserve augmentation-aware information (see Figure 2a) that could be useful for downstream tasks. Furthermore, AugSelf can easily be incorporated into the recent unsupervised representation learning methods [1, 2, 12, 13] with a negligible additional training cost, which is for training an auxiliary prediction head φ in Figure 1.
Somewhat interestingly, we also found that optimizing the auxiliary loss, AugSelf, can even improve the transferability of representations learned under the standard supervised representation learning scenarios [22]. This is because supervised learning also forces invariance, i.e., assigns the same label, for all augmented samples (of the same instance), and AugSelf can help to keep augmentation-aware knowledge in the learned representations.
We demonstrate the effectiveness of AugSelf under extensive transfer learning experiments: AugSelf improves (a) two unsupervised representation learning methods, MoCo [1] and SimSiam [13], in 20 of 22 tested scenarios; (b) supervised pretraining in 9 of 11 (see Table 1). Furthermore, we found that
AugSelf is also effective under few-shot learning setups (see Table 2).
Remark that learning augmentation-invariant representations has been a common practice for both su-pervised and unsupervised representation learning frameworks, while the importance of augmentation-awareness is less emphasized. We hope that our work could inspire researchers to rethink the under-explored aspect and provide a new angle in representation learning. 2 Preliminaries: Augmentation-invariant representation learning
In this section, we review the recent unsupervised representation learning methods [1, 2, 11, 12, 13] that learn representations by optimizing augmentation-invariant objectives. Formally, let x be an 2
(a) Mutual information (b) STL10→Flowers (c) STL10→Food
Figure 2: (a) Changes of mutual information, i.e., INCE(C; z), between color information C(x) and the representation z = f (x) pretrained on STL10 [23] with varying the color jittering strength s. The pretrained representations are evaluated in color-sensitive benchmarks, (b) Flowers [21] and (c) Food
[24], by the linear evaluation protocol [25]. image, tω be an augmentation function parameterized by an augmentation parameter ω, v = tω(x) be the augmented sample (or view) of x by tω, and f be a CNN feature extractor, such as ResNet [20].
Generally speaking, the methods encourage the representations f (v1) and f (v2) to be invariant to the two randomly augmented views v1 = tω1(x) and v2 = tω2(x), i.e., f (v1) f (v2) for ω1, ω2 ∼
Ω
≈ where Ω is a pre-deﬁned augmentation parameter distribution. We now describe the recent methods one by one brieﬂy. For simplicity, we here omit the projection MLP head g(
) which is widely used
· in the methods (see Figure 1).
Instance contrastive learning approaches [1, 2, 26] minimize the distance between an anchor f (tω1(x)) and its positive sample f (tω2 (x)), while maximizing the distance between the anchor f (tω1(x)) and its negative sample f (tω3(x(cid:48))). Since contrastive learning performance depends on the number of negative samples, a memory bank [26], a large batch [2], or a momentum network with a representation queue [1] has been utilized.
Clustering approaches [11, 27, 28] encourage two representations f (tω1 (x)) and f (tω2(x)) to be assigned into the same cluster, in other words, the distance between them will be minimized.
Negative-free methods [12, 13] learn to predict the representation f (v1) of a view v1 = tω1(x) 2 from another view v2 = tω2(x). For example, SimSiam [13] minimizes 2 (cid:107) where h is an MLP and sg is the stop-gradient operation. In these methods, if h is optimal, then h(f (v2)) = Eω1∼Ω[f (v1)]; thus, the expectation of the objective can be rewritten as Varω∼Ω(f (v)).
Therefore, the methods can be considered as learning invariance with respect to the augmentations. sg(f (v1)) h(f (v2))
− (cid:107)
≈ f (tω1(x)) f (tω2 (x)). y(cid:48) exp(c(cid:62) y f (t(x)))/ (cid:80)
Supervised learning approaches [22] also learn augmentation-invariant representations. Since they often maximize exp(c(cid:62) y(cid:48)f (t(x))) where cy is the prototype vector of the label y, f (t(x)) is concentrated to cy, i.e., cy ≈
These approaches encourage representations f (x) to contain shared (i.e., augmentation-invariant) information between tω1(x) and tω2(x) and discard other information [15]. For example, if tω
Ω, f (x) will be changes color information, then to satisfy f (tω1(x)) = f (tω2(x)) for any ω1, ω2 ∼ learned to contain no (or less) color information. To verify this, we pretrain ResNet-18 [20] on STL10
[29] using SimSiam [13] with varying the strength s of the color jittering augmentation. To measure the mutual information between representations and color information, we use the InfoNCE loss [30].
We here simply encode color information as RGB color histograms of an image. As shown in Figure 2a, using stronger color augmentations leads to color-relevant information loss. In classiﬁcation on
Flowers [21] and Food [24], which is color-sensitive, the learned representations containing less color information result in lower performance as shown in Figure 2b and 2c, respectively. This observation emphasizes the importance of learning augmentation-aware information in transfer learning scenarios. 3 Auxiliary augmentation-aware self-supervision
In this section, we introduce auxiliary augmentation-aware self-supervision, coined AugSelf, which encourages to preserve augmentation-aware information for generalizable representation learning. To 3
Figure 3: Examples of the commonly-used augmentations and their parameters ωaug. be speciﬁc, we add an auxiliary self-supervision loss, which learns to predict the difference between augmentation parameters of two randomly augmented views, into existing augmentation-invariant representation learning methods [1, 2, 11, 12, 13]. We ﬁrst describe a general form of our auxiliary loss, and then speciﬁc forms for various augmentations. For conciseness, let θ be the collection of all parameters in the model.
Since an augmentation function tω is typically a composition of different types of augmentations, the augmentation parameter ω can be written as ω = (ωaug)aug∈A where is the set of augmentations crop, flip}), and ωaug is an augmentation-speciﬁc parameter (e.g., used in pretraining (e.g.,
{
ωcrop decides how to crop an image). Then, given two randomly augmented views v1 = tω1 (x) and v2 = tω2 (x), the AugSelf objective is as follows:
A
A
=
LAugSelf(x, ω1, ω2; θ) = (cid:88) aug∈AAugSelf Laug (cid:0)φaug
θ (fθ(v1), fθ(v2)), ωaug diff (cid:1),
θ is a 3-layer MLP for ωaug is the set of augmentations for augmentation-aware learning, ωaug where diff is the
AAugSelf ⊆ A difference between two augmentation-speciﬁc parameters ωaug
Laug is an augmentation-, speciﬁc loss, and φaug diff prediction. This design allows us to incorporate
AugSelf into the recent state-of-the-art unsupervised learning methods [1, 2, 11, 12, 13] with a negligible additional training cost. For example, the objective of SimSiam [13] with AugSelf can
· LAugSelf(x, ω1, ω2; θ), where λ be written as
LSimSiam(x, ω1, ω2; θ) + λ is a hyperparameter for balancing losses. Remark that the total objective
Ltotal encourages the shared representation f (x) to learn both augmentation-invariant and augmentation-aware features.
Hence, the learned representation f (x) also can be useful in various downstream (e.g., augmentation-sensitive) tasks.2
Ltotal(x, ω1, ω2; θ) = and ωaug 1 2
In this paper, we mainly focus on the commonly-used augmentations in the recent unsupervised representation learning methods [1, 2, 11, 12, 13]: random cropping, random horizontal ﬂipping, color jittering, and Gaussian blurring; however, we remark that different types of augmentations can be incorporated into AugSelf (see Section 4.2). In the following, we elaborate on the details of ωaug and
Laug for each augmentation. The examples of ωaug are illustrated in Figure 3. 1 −
ωcrop 2 diff = ωcrop
[0, 1]4. Then, we use (cid:96)2 loss for
Random cropping. The random cropping is the most popular augmentation in vision tasks. A cropping parameter ωcrop contains the center position and cropping size. We normalize the values by the height and width of the original image x, i.e., ωcrop
Lcrop and set ωcrop
.
Random horizontal ﬂipping. A ﬂipping parameter ωflip
ﬂipped or not. Since it is discrete, we use the binary cross-entropy loss for 1[ωflip
Color jittering. The color jittering augmentation adjusts brightness, contrast, saturation, and hue of an input image in a random order. For each adjustment, its intensity is uniformly sampled from a pre-deﬁned interval. We normalize all intensities into [0, 1], i.e., ωcolor
[0, 1]4. Similarly to
Lcolor and set ωcolor cropping, we use (cid:96)2 loss for indicates the image is horizontally diff =
Lflip and set ωflip diff = ωcolor 1 = ωflip 1 −
ωcolor 2
∈ { 0, 1
∈
∈
].
} 2
. 2We observe that our augmentation-aware objective LAugSelf does not interfere with learning the augmentation-invariant objective, e.g., LSimSiam. This allows f (x) to learn augmentation-aware information with a negligible loss of augmentation-invariant information. A detailed discussion is provided in the supplementary material. 4
Table 1: Linear evaluation accuracy (%) of ResNet-50 [20] and ResNet-18 pretrained on ImageNet100
[31, 32] and STL10 [23], respectively. Bold entries are the best of each baseline.
Method
CIFAR10 CIFAR100
Food MIT67
Pets
Flowers Caltech101
Cars Aircraft DTD SUN397
SimSiam
+ AugSelf (ours)
MoCo v2
+ AugSelf (ours)
Supervised
+ AugSelf (ours)
SimSiam
+ AugSelf (ours)
MoCo v2
+ AugSelf (ours) 86.89 88.80 84.60 85.26 86.16 86.06 82.35 82.76 81.18 82.45
ImageNet100-pretrained ResNet-50 61.48 65.63 59.37 60.78 53.89 55.84 33.99 41.58 33.69 36.91 65.75 67.76 61.64 63.36 52.91 54.63 74.69 76.34 70.08 73.46 73.50 74.81 88.06 90.70 82.43 85.70 76.09 78.22
STL10-pretrained ResNet-18 39.15 45.67 39.01 41.67 44.90 48.42 42.34 43.80 59.19 72.18 61.01 66.96 84.13 85.30 77.25 78.93 77.53 77.47 66.33 72.75 64.15 66.02 66.33 70.27 61.60 63.90 62.70 63.77 54.90 58.65 53.75 57.17 48.20 47.52 33.86 37.35 30.61 31.26 16.85 21.17 16.09 17.53 48.63 49.76 41.21 39.47 36.78 38.02 26.06 33.17 26.63 28.02 65.11 67.29 64.47 66.22 61.91 62.07 42.57 47.02 41.20 45.21 50.60 52.28 46.50 48.52 40.59 41.49 29.05 34.14 28.50 30.93
Gaussian blurring. This blurring operation is widely used in unsupervised representation learning.
The Gaussian ﬁlter is constructed by a single parameter, standard deviation σ = ωblur. We also normalize the parameter into [0, 1]. Then, we use (cid:96)2 loss for diff = ωblur
ωblur 2
.
Lblur and set ωblur 1 − 4 Experiments
Setup. We pretrain the standard ResNet-18 [20] and ResNet-50 on STL10 [23] and ImageNet1003
[31, 32], respectively. We use two recent unsupervised representation learning methods as baselines for pretraining: a contrastive method, MoCo v2 [1, 14], and a non-contrastive method, SimSiam
[13]. For STL10 and ImageNet100, we pretrain networks for 200 and 500 epochs with a batch size of 256, respectively. For supervised pretraining, we pretrain ResNet-50 for 100 epochs with a batch size of 128 on ImageNet100.4 For augmentations, we use random cropping, ﬂipping, color jittering, grayscaling, and Gaussian blurring following Chen and He [13]. In this section, our AugSelf predicts random cropping and color jittering parameters, i.e.,
, unless otherwise
} stated. We set λ = 1.0 for STL10 and λ = 0.5 for ImageNet100. The other details and the sensitivity analysis to the hyperparameter λ are provided in the supplementary material. For ablation study (Section 4.2), we only use STL10-pretrained models. crop, color
{
AAugSelf = 4.1 Main results
∼
Linear evaluation in various downstream tasks. We evaluate the pretrained networks in down-stream classiﬁcation tasks on 11 datasets: CIFAR10/100 [29], Food [24], MIT67 [36], Pets [37],
Flowers [21], Caltech101 [38], Cars [39], Aircraft [40], DTD [41], and SUN397 [42]. They contain roughly 1k 70k training images. We follow the linear evaluation protocol [25]. The detailed in-formation of datasets and experimental settings is described in the supplementary material. Table 1 shows the transfer learning results in the various downstream tasks. Our AugSelf consistently improves (a) the recent unsupervised representation learning methods, SimSiam [13] and MoCo
[13], in 10 out of 11 downstream tasks; and (b) supervised pretraining in 9 out of 11 downstream tasks. These consistent improvements imply that our method encourages to learn more generalizable representations.
Few-shot classiﬁcation. We also evaluate the pretrained networks on various few-shot learning benchmarks: FC100 [33], Caltech-UCSD Birds (CUB200) [43], and Plant Disease [35]. Note that
CUB200 and Plant Disease benchmarks require low-level features such as color information of birds and leaves, respectively, to detect their ﬁne-grained labels. They are widely used in cross-domain few-shot settings [44, 45]. For few-shot learning, we perform logistic regression using the frozen representations f (x) without ﬁne-tuning. Table 2 shows the few-shot learning performance of 5-way 1-shot and 5-way 5-shot tasks. As shown in the table, our AugSelf improves the performance of
SimSiam [13] and MoCo [1] in all cases with a large margin. For example, for plant disease detection 3ImageNet100 is a 100-category subset of ImageNet [31]. We use the same split following Tian et al. [32]. 4We do not experiment supervised pretraining on STL10, as it has only 5k labeled training samples, which is not enough for pretraining a good representation. 5
Table 2: Few-shot classiﬁcation accuracy (%) with 95% conﬁdence intervals averaged over 2000 episodes on FC100 [33], CUB200 [34], and Plant Disease [35]. (N, K) denotes N -way K-shot tasks.
Bold entries are the best of each group.
Method (5, 1) (5, 5) (5, 1) (5, 5) (5, 1) (5, 5)
FC100
CUB200
Plant Disease
ImageNet100-pretrained ResNet-50
SimSiam
+ AugSelf (ours)
MoCo v2
+ AugSelf (ours)
Supervised
+ AugSelf (ours) 36.19±0.36 39.37±0.40 31.67±0.33 35.02±0.36 33.15±0.33 34.70±0.35 50.36±0.38 55.27±0.38 43.88±0.38 48.77±0.39 46.59±0.37 48.89±0.38 45.56±0.47 48.08±0.47 41.67±0.47 44.17±0.48 46.57±0.48 47.58±0.48 62.48±0.48 66.27±0.46 56.92±0.47 57.35±0.48 63.69±0.46 65.31±0.45 75.72±0.46 77.93±0.46 65.73±0.49 71.80±0.47 68.95±0.47 70.82±0.46 89.94±0.31 91.52±0.29 84.98±0.36 87.81±0.33 88.77±0.30 89.77±0.29
STL10-pretrained ResNet-18
SimSiam
+ AugSelf (ours)
MoCo v2
+ AugSelf (ours) 36.72±0.35 40.68±0.39 35.69±0.34 39.66±0.39 51.49±0.36 56.26±0.38 49.26±0.36 55.58±0.39 37.97±0.43 41.60±0.42 37.62±0.42 38.33±0.41 50.61±0.45 56.33±0.44 50.71±0.44 51.93±0.44 58.13±0.50 62.85±0.49 57.87±0.48 60.78±0.50 75.98±0.40 81.14±0.37 75.98±0.40 78.76±0.38
Table 3: Linear evaluation accuracy (%) under the same setup following Xiao et al. [16]. The augmentations in the brackets of LooC [16] indicate which augmentation-aware information is learned. N is the number of required augmented samples for each instance, that reﬂects the effective training batch size. ∗ indicates that the numbers are reported in [16]. The numbers in the brackets show the accuracy gains compared to each baseline.
Method
MoCo∗ [1]
LooC∗ [16] (color)
LooC∗ [16] (rotation)
LooC∗ [16] (color, rotation)
MoCo [1]
MoCo [1] + AugSelf (ours)
SimSiam [13]
SimSiam [13] + AugSelf (ours)
N ImageNet100 CUB200
Flowers (5-shot)
Flowers (10-shot) 2 3 3 4 2 2 2 2 81.0 81.1 (+0.1) 80.2 (-0.8) 79.2 (-1.8) 81.0 82.4 (+1.4) 81.6 82.6 (+1.0) 36.7 40.1 (+3.4) 38.8 (+2.1) 39.6 (+2.9) 67.9±0.5 68.2±0.6 (+0.3) 70.1±0.4 (+2.2) 70.9±0.3 (+3.0) 32.2 37.0 (+4.8) 78.5±0.3 81.7±0.2 (+3.2) 38.4 45.3 (+6.9) 83.6±0.3 86.4±0.2 (+2.8) 77.3±0.1 77.6±0.1 (+0.3) 79.3±0.1 (+2.0) 80.8±0.2 (+3.5) 81.2±0.3 84.5±0.2 (+3.3) 85.9±0.2 88.3±0.1 (+2.4)
[35], we obtain up to 6.07% accuracy gain in 5-way 1-shot tasks. These results show that our method is also effective in such transfer learning scenarios.
Comparison with LooC. Recently, Xiao et al. [16] propose LooC that learns augmentation-aware representations via multiple augmentation-speciﬁc contrastive learning objectives. Table 3 shows head-to-head comparisons under the same evaluation setup following Xiao et al. [16].5 As shown in the table, our AugSelf has two advantages over LooC: (a) AugSelf requires the same number of augmented samples compared to the baseline unsupervised representation learning methods while
LooC requires more, such that AugSelf does not increase the computational cost; (b) AugSelf can be incorporated with non-contrastive methods e.g., SimSiam [13], and SimSiam with AugSelf outperforms LooC in all cases.
Object localization. We also evaluate representations in an object localization task (i.e., bounding box prediction) that requires positional information. We experiment on CUB200 [46] and solve linear regression using representations pretrained by SimSiam [13] without or with our method. Table 4 reports (cid:96)2 errors of bounding box predictions and Figure 4 shows the examples of the predictions.
These results demonstrate that AugSelf is capable of learning positional information.
Retrieval. Figure 5 shows the retrieval results using pretrained models. For this experiment, we use the Flowers [21] and Cars [39] datasets and ﬁnd top-4 nearest neighbors based on the cosine 5Since LooC’s code is currently not publicly available, we reproduced the MoCo baseline as reported in the sixth row in Table 3: we obtained the same ImageNet100 result, but different ones for CUB200 and Flowers. 6
Method
SimSiam
+ AugSelf (ours)
MoCo
+ AugSelf (ours)
Supervised
+ AugSelf (ours)
Error 0.00462 0.00335 0.00487 0.00429 0.00520 0.00473
Table 4: (cid:96)2 errors of bounding box predictions on CUB200.
Figure 4: Examples of bounding box predictions on CUB200. Blue and red boxes are ground-truth and model prediction, respectively. (a) SimSiam (b) SimSiam + AugSelf (ours)
Figure 5: Top-4 nearest neighbors based on the cosine similarity using representations f (x) learned by (a) SimSiam [13] or (b) SimSiam with AugSelf (ours). similarity between representations f (x) where f is the pretrained ResNet-50 on ImageNet100. As shown in the ﬁgure, the representations learned by AugSelf are more color-sensitive. 4.2 Ablation study
∈ { crop, flip, color, blur
Laug for each aug
Effect of augmentation prediction tasks. We ﬁrst evaluate the proposed augmentation prediction tasks one by one without incorporating invariance-learning methods. More speciﬁcally, we pretrain fθ using only
. Remark that training objectives
} are different but we use the same set of augmentations. Table 5 shows the transfer learning results in various downstream tasks. We observe that solving horizontal ﬂipping and Gaussian blurring prediction tasks results in worse or similar performance to a random initialized network in various downstream tasks, i.e., the augmentations do not contain task-relevant information. However, solving random cropping and color jittering prediction tasks signiﬁcantly outperforms the random initialization in all downstream tasks. Furthermore, surprisingly, the color jittering prediction task achieves competitive performance in the Flowers [21] dataset compared to a recent state-of-the-art method, SimSiam [13]. These results show that augmentation-aware information are task-relevant and learning such information could be important in downstream tasks.
Based on the above observations, we incorporate random cropping and color jittering prediction tasks into SimSiam [13] when pretraining. More speciﬁcally, we optimize
LSimSiam + λcropLcrop +
. The transfer learning results are reported in Table 6. As
λcolorLcolor where λcrop, λcolor ∈ { shown in the table, each self-supervision task improves SimSiam consistently (and often signiﬁcantly) in various downstream tasks. For example, the color jittering prediction task improves SimSiam by 6.33% and 11.89% in Food [24] and Flowers [21] benchmarks, respectively. When incorporating both tasks simultaneously, we achieve further improvements in almost all the downstream tasks.
Furthermore, as shown in Figure 2, our AugSelf preserves augmentation-aware information as much as possible; hence our gain is consistent regardless of the strength of color jittering augmentation. 0, 1
}
Different augmentations. We conﬁrm that our method can allow to use other strong augmentations: rotation, which rotates an image by 0◦, 90◦, 180◦, 270◦ degrees randomly; and solarization, which inverts each pixel value when the value is larger than a randomly sampled threshold. Based on
, we additionally apply each the default augmentation setting, i.e.,
} crop, color
{
AAugSelf = 7
Table 5: Linear evaluation accuracy (%) of ResNet-18 [20] pretrained by each augmentation prediction task without other methods such as SimSiam [13]. We report SimSiam [13] results as reference. Bold entries are larger than the random initialization.
Pretraining objective
STL10 CIFAR10 CIFAR100
Food MIT67
Pets
Flowers
Random Init
Lcrop
Lflip
Lcolor
Lblur
SimSiam [13] 42.72 68.28 46.45 61.14 48.26 85.19 47.45 70.78 53.80 63.39 46.60 82.35 23.73 43.44 24.89 40.38 20.44 54.90 11.54 22.26 9.69 28.02 8.73 33.99 12.29 26.17 11.99 25.35 11.87 39.15 12.94 27.68 10.71 24.49 13.07 44.90 26.06 38.21 13.04 54.42 17.20 59.19
Table 6: Linear evaluation accuracy (%) of ResNet-18 [20] pretrained by SimSiam [13] with various combinations of our augmentation prediction tasks. Bold entries are the best of each task.
AAugSelf
∅ crop
}
{ color
{
} crop, color
{
}
STL10 CIFAR10 CIFAR100
Food MIT67
Pets
Flowers 85.19 85.98 85.55 85.70 82.35 82.82 82.90 82.76 54.90 55.78 58.11 58.65 33.99 35.68 40.32 41.58 39.15 43.21 43.56 45.67 44.90 47.10 47.85 48.42 59.19 62.05 71.08 72.18
[
− diff ∈ { 0, 1, 2, 3
} augmentation with a probability of 0.5. We also evaluate the effectiveness of augmentation prediction tasks for rotation and solarization. Note that we formulate the rotation prediction as a 4-way classiﬁcation task (i.e., ωrot
) and the solarization prediction as a regression task (i.e.,
ωsol 1, 1]). As shown in Table 7, we obtain consistent gains across various downstream tasks diff ∈ even if stronger augmentations are applied. Furthermore, in the case of rotation, we observe that our augmentation prediction task tries to prevent the performance degradation from learning invariance to rotations. For example, in CIFAR100 [29], the baseline loses 4.66% accuracy (54.90% 50.24%) when using rotations, but ours does only 0.37% (58.65% 58.28%). These results show that our
AugSelf is less sensitive to the choice of augmentations. We believe that this robustness would be useful in future research on representation learning with strong augmentations.
→
→
Table 8: Linear evaluation accuracy in augmentation-aware pretext tasks.
Solving geometric and color-related pretext tasks. To val-idate that our AugSelf is capable of learning augmentation-aware information, we try to solve two pretext tasks requiring the information: 4-way rotation (0◦, 90◦, 180◦, 270◦) and 6-way color channel permutation (RGB, RBG, . . ., BGR) classiﬁcation tasks. We note that the baseline (SimSiam) and our method (SimSiam+AugSelf) do not observe rotated or color-permuted samples in the pretraining phase. We train a linear classiﬁer on top of pretrained representation without ﬁnetuning for each task. As reported in Table 8, our AugSelf solves the pretext tasks well even without their prior knowledge in pretraining; these results validate that our method learns augmentation-aware information.
Rotation Color perm
SimSiam
+ AugSelf 24.66 60.49 59.11 64.61
Method
Compatibility with other methods. While we mainly focus on SimSiam [13] and MoCo [1] in the previous section, our AugSelf can be incorporated into other unsupervised learning methods,
SimCLR [2], BYOL [12], and SwAV [11]. Table 9 shows the consistent and signiﬁcant gains by
AugSelf across all methods and downstream tasks. 5