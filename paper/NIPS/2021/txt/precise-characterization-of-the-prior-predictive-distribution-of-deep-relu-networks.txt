Abstract
Recent works on Bayesian neural networks (BNNs) have highlighted the need to better understand the implications of using Gaussian priors in combination with the compositional structure of the network architecture. Similar in spirit to the kind of analysis that has been developed to devise better initialization schemes for neural networks (cf. He- or Xavier initialization), we derive a precise characterization of the prior predictive distribution of ﬁnite-width ReLU networks with Gaussian weights. While theoretical results have been obtained for their heavy-tailedness, the full characterization of the prior predictive distribution (i.e. its density, CDF and moments), remained unknown prior to this work. Our analysis, based on the
Meijer-G function, allows us to quantify the inﬂuence of architectural choices such as the width or depth of the network on the resulting shape of the prior predictive distribution. We also formally connect our results to previous work in the inﬁnite width setting, demonstrating that the moments of the distribution converge to those of a normal log-normal mixture in the inﬁnite depth limit. Finally, our results provide valuable guidance on prior design: for instance, controlling the predictive variance with depth- and width-informed priors on the weights of the network. 1

Introduction
It is well known that standard neural networks initialized with Gaussian weights tend to Gaussian processes (Rasmussen, 2003) in the inﬁnite width limit (Neal, 1996; Lee et al., 2018; de G. Matthews et al., 2018), coined neural network Gaussian process (NNGP) in the literature. Although the NNGP has been derived for a number of architectures, such as convolutional (Novak et al., 2019; Garriga-Alonso et al., 2019), recurrent (Yang, 2019) and attention mechanisms (Hron et al., 2020), little is known about the ﬁnite width case.
The reason why the inﬁnite width limit is relatively tractable to study is that uncorrelated but dependent units of intermediate layers become normally distributed due to the central limit theorem (CLT) and as a result, independent. In the ﬁnite width case, zero correlation does not imply independence, rendering the analysis far more involved as we will outline in this paper.
One of the main motivations for our work is to better understand the implications of using Gaussian priors in combination with the compositional structure of the network architecture. As argued by
Wilson and Izmailov (2020); Wilson (2020), the prior over parameters does not carry a meaningful 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
interpretation; the prior that ultimately matters is the prior predictive distribution that is induced when a prior over parameters is combined with a neural architecture (Wilson and Izmailov, 2020;
Wilson, 2020).
Studying the properties of this prior predictive distribution is not an easy task, the main reason being the compositional structure of a neural network, which ultimately boils down to products of random matrices with a given (non-linear) activation function. The main tools to study such products are the
Mellin transform and the Meijer-G function (Meijer, 1936; Springer and Thompson, 1970; Mathai, 1993; Stojanac et al., 2017), both of which will be leveraged in this work to gain theoretical insights into the inner workings of BNN priors.
Contributions Our results provide an important step towards understanding the interplay between architectural choices and the distributional properties of the prior predictive distribution, in particular:
• We characterize the prior predictive density of ﬁnite-width ReLU networks of any depth through the framework of Meijer-G functions.
• We draw analytic insights about the shape of the distribution by studying its moments and the resulting heavy-tailedness. We disentangle the roles of width and depth, demonstrating how deeper networks become more and more heavy-tailed, while wider networks induce more Gaussian-like distributions.
• We connect our work to the inﬁnite width setting by recovering and extending prior results (Lee et al., 2018; Matthews et al., 2018) to the inﬁnite depth limit. We describe the resulting distribution in terms of its moments and match it to a normal log-normal mixture (Yang, 2008), empirically providing an excellent ﬁt even in the non-asymptotic regime.
• Finally, we introduce generalized He priors, where a desired variance can be directly speciﬁed in the function space. This allows the practitioner to make an interpretable choice for the variance, instead of implicitly tuning it through the speciﬁcation of each layer variance.
The rest of the paper is organized as follows: in Section 3.1, we introduce the relevant notation for the neural network that will be analyzed. We describe the prior works of Lee et al. (2018); Matthews et al. (2018) in more detail in Section 3.2 . Then, in Section 3.3, we introduce the Meijer-G function and the necessary mathematical tools. In Section 4 we derive the probability density function for a linear network of any depth and extend these results to ReLU networks, which represents the key contribution of our work. In Section 5, we present several consequences of our analysis, including an extension of the inﬁnite width setting to inﬁnite depth as well as precise characterizations of the heavy-tailedness in the ﬁnite regime. Finally, in Section 6, we show how one can design an architecture and a prior over the weights to achieve a desired prior predictive variance. 2