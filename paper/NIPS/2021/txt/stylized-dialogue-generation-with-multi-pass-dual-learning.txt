Abstract
Stylized dialogue generation, which aims to generate a given-style response for an input context, plays a vital role in intelligent dialogue systems. Considering there is no parallel data between the contexts and the responses of target style S1, existing works mainly use back translation to generate stylized synthetic data for training, where the data about context, target style S1 and an intermediate style S0 is used. However, the interaction among these texts is not fully exploited, and the pseudo contexts are not adequately modeled. To overcome the above difﬁculties, we propose multi-pass dual learning (MPDL), which leverages the duality among the context, response of style S1 and response of style S0. MPDL builds mappings among the above three domains, where the context should be reconstructed by the MPDL framework, and the reconstruction error is used as the training signal.
To evaluate the quality of synthetic data, we also introduce discriminators that effectively measure how a pseudo sequence matches the speciﬁc domain, and the evaluation result is used as the weight for that data. Evaluation results indicate that our method obtains signiﬁcant improvement over previous baselines. 1

Introduction
In recent years, dialogue systems have attracted a surge of research interest and achieved great success [1, 2, 3, 4]. The dialogue generation task can be formulated as a sequence-to-sequence learning problem [5, 6], where the source sequence is the given context, and the target sequence is the response. Stylized dialogue generation, which automatically generates response following the desired style, obtains signiﬁcant progress in open-domain dialogue systems. The styles could be formal/informal, modern/Shakespearean, serious/humor, etc.
A core challenge of this task is the lack of parallel training data between the contexts and responses of desired styles (denoted as style S1), which limits the performance of models. A natural solution is to establish the pseudo pairs between contexts and responses of style S1 using unsupervised or semi-supervised learning. The parallel data between contexts and responses of style S0 (denoted as style S0) is leveraged. Note the conversations corpus (context, style S0) are relatively easy to obtain.
After that, the stylized dialogue model can be trained on the synthetic data. Speciﬁcally, Su et al. [7] propose a diversifying dialogue generation model based on iterative back translation [8]. Zheng et al. [9] propose a style routing approach with a joint training process, where an inverse model with style embeddings is used to generate pseudo pairs and train the stylized dialogue generation model.
∗corresponding authors: Rui Yan (ruiyan@ruc.edu.cn) and Dongyan Zhao (zhaody@pku.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Although the above methods achieved success in the stylized dialogue generation, there still exists lots of room to improve. First, the interaction between context, style S0 and style S1 is not fully exploited. An example is that a pseudo context can be explicitly obtained by a two-hop process,
S1 → S0 →pseudo context instead of using style embedding only. Second, in the aforementioned methods, the quality of the pseudo contexts generated through back translation is not properly modeled. By manually checking the generated data, we ﬁnd that there are quite a few pseudo contexts that contain irrelevant contents of the response. Several common patterns in the pseudo contexts and responses include “I don’t know”, “I’m so happy”, etc. In previous methods, each pseudo data is treated equally regardless of their quality, which introduces bias to the training set and hurts the performances of model.
In this paper, we propose a new framework, multi-pass dual learning (MPDL), for stylized dialogue generation. Inspired by dual learning [10] that can automatically extract information through two dual tasks in an unsupervised manner, we extend it to a multi-pass version. In MPDL, there are three dual tasks in total, including context↔style S1, context↔style S0 and style S0 ↔style S1.
The unlabeled texts in style S1 and contexts are leveraged. We also introduce two discriminators to evaluate the quality of the generated data. The discriminators are used to measure the similarity of generated data to the corresponding domains. The similarity is used as the weight of the synthetic data for training, by which we can adaptively use the unlabeled data. We conduct experiments with benchmarks of stylized response generation in formal English and Shakespearean English. We achieved state-of-the-art results in terms of both automatic evaluation and manual judgment.
Our contributions are summarized as follows: (1) We propose multi-pass dual learning (MPDL) framework for stylized dialogue response generation, that can effectively leverage the unlabeled data; (2) Compared with standard dual learning, we introduce two discriminators to evaluate the quality of the pseudo parallel data. This is a new attempt for the general dual learning framework; (3) We provide a new dataset for this task and set several benchmarks using our method; (4) We empirically verify the effectiveness of MPDL on two datasets with formal and Shakespearean response generation. 2