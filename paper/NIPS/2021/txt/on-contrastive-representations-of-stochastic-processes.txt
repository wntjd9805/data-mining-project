Abstract
Learning representations of stochastic processes is an emerging problem in ma-chine learning with applications from meta-learning to physical object models to time series. Typical methods rely on exact reconstruction of observations, but this approach breaks down as observations become high-dimensional or noise distributions become complex. To address this, we propose a unifying framework for learning contrastive representations of stochastic processes (CRESP) that does away with exact reconstruction. We dissect potential use cases for stochastic process representations, and propose methods that accommodate each. Empir-ically, we show that our methods are effective for learning representations of periodic functions, 3D objects and dynamical processes. Our methods tolerate noisy high-dimensional observations better than traditional approaches, and the learned representations transfer to a range of downstream tasks. 1

Introduction
Table 1: Example stochastic processes with covariate space X and observation space Y.
Y
R
R
X n o i t c n u f
Illustration
The stochastic process (Doob, 1953; Parzen, 1999) is a powerful mathematical abstraction used in biology (Bressloff, 2014), chemistry (van Kampen, 1992), physics (Jacobs, 2010), ﬁnance (Steele, 2012) and other ﬁelds. The simplest incarnation of a stochas-tic process is a random function R → R, such as a
Gaussian Process (MacKay, 2003), that can be used to describe a real-valued signal indexed by time or space. Extending to random functions from R to another space, stochastic processes can model time-dependent phenomena like queuing (Grimmett and
Stirzaker, 2020) and diffusion (Itô et al., 2012). In meta-learning, the stochastic process can be used to describe few-shot learning tasks—mappings from im-ages to class labels (Vinyals et al., 2016)—and image completion tasks—mappings from pixel locations to
RGB values (Garnelo et al., 2018a). In computer vision, 2D views of 3D objects can be seen as obser-vations of a stochastic process indexed by the space of possible viewpoints (Eslami et al., 2018; Milden-hall et al., 2020). Videos can be seen as samples from a time-indexed stochastic process with 2D image observations (Zelnik-Manor and Irani, 2001).
Images
Images
SE(3) t c e j b o
D 3
R3
Z2 o e d i
V l l
ﬁ
-n i e g a m
R
D 1
I
∗Equal contribution. Author ordering determined by coin ﬂip. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Machine learning algorithms that operate on data generated from stochastic processes are therefore in high demand. We assume that we have access to only a small set of covariate–observation pairs
{(xi, yi)C i=1} from different realizations of the underlying stochastic process. This might correspond to a few views of a 3D object, or a few snapshots of a dynamical system evolving in time. Whilst conventional deep learning thrives when there is a large quantity of i.i.d. data available (Lake et al., 2017), allowing us to learn a fresh model for each realization of the stochastic process, when the context size is small it makes sense to use data from other realizations to build up prior knowledge about the domain which can aid learning on new realizations (Reed et al., 2018; Garnelo et al., 2018a). i=1}, these methods provide a predictive distribution q (cid:0)y(cid:63)|x(cid:63), (xi, yi)C
Traditional methods for learning from stochastic processes, including the Gaussian Process family (MacKay, 2003; Rasmussen, 2003) and the Neural Process family (Garnelo et al., 2018a,b; Eslami et al., 2018), learn to reconstruct a realization of the process from a given context. That is, given a (cid:1) for context set {(xi, yi)C the observation that would be obtained from this realization of the process at any target covariate x(cid:63). These methods use an explicit likelihood for q, typically a Gaussian distribution. Whilst this can work well when y(cid:63) is low-dimensional and unimodal, it is a restrictive assumption. For example, (cid:1) samples a high-dimensional image with colour distortion, traditional when p (cid:0)y(cid:63)|x(cid:63), (xi, yi)C methods must learn to perform conditional image generation, a notably challenging task (van den
Oord et al., 2016; Chrysos and Panagakis, 2021). i=1 i=1
In this paper, we do away with the explicit likelihood requirement for learning from stochastic processes. Our ﬁrst insight is that, for a range of important downstream tasks, exact reconstruction is not necessary to obtain good performance. Indeed, whilst y may be high-dimensional, the downstream target label or feature (cid:96) may be simpler. We consider two distinct settings for (cid:96) ∈ L. The ﬁrst is a downstream task that depends on the covariate x ∈ X , formally a second process X → L that covaries with the ﬁrst. For example, (cid:96)(x) could represent a class label or annotation for each video frame. The second is a downstream task that depends on the entire process realization, such as a single label for a 3D object. In both cases, we assume that we have limited labelled data, so we are in a semi-supervised setting (Zhu, 2005).
To solve problems of this nature, we propose a general framework for Contrastive Representations of
Stochastic Processes (CRESP). At its core, CRESP consists of a ﬂexible encoder network architecture for contexts {(xi, yi)C 1 } that unites transformer encoders of sets (Vaswani et al., 2017; Parmar et al., 2018) with convolutional encoders (LeCun et al., 1989) for observations that are images. To account for the two kinds of downstream task that may of interest, we propose a targeted variant of CRESP that learns a representations depending on the context and a target covariate x(cid:63), and an untargeted variant that learns one representation of the context. To train our encoder, we take our inspiration from recent advances in contrastive learning (Bachman et al., 2019; Chen et al., 2020) which have so far focused on representations of single observations, typically images. We deﬁne a variant of the InfoNCE objective (van den Oord et al., 2018) for contexts sampled from stochastic processes, allowing us to avoid training objectives that necessitate exact reconstruction. Rather than attempting pixel-perfect reconstruction, then, CRESP solves a self-supervised task in representation space.
The CRESP framework uniﬁes and extends recent work, building on function contrastive learning (FCLR) (Gondal et al., 2021) by considering targeted as well as untargeted representations and using self-attention in place of mean-pool aggregation. We develop on noise contrastive meta-learning (Ton et al., 2021) by focusing on downstream tasks rather than multi-modal reconstruction, replacing conditional mean embeddings with neural representations and using a simpler training objective.
We evaluate CRESP on sinusoidal functions, 3D objects, and dynamical processes with high-dimensional observations. We empirically show that our methods can handle high-dimensional observations with naturalistic distortion, unlike explicit likelihood methods, and our representations lead to improved data efﬁciency compared to supervised learning. CRESP performs well on a range of downstream tasks, both targeted and untargeted, outperforming existing methods across the board.
Our code is publicly available at github.com/ae-foster/cresp. 2