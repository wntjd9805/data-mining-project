Abstract
Image classiﬁers are typically scored on their test set accuracy, but high accuracy can mask a subtle type of model failure. We ﬁnd that high scoring convolutional neural networks (CNNs) on popular benchmarks exhibit troubling pathologies that allow them to display high accuracy even in the absence of semantically salient features. When a model provides a high-conﬁdence decision without salient supporting input features, we say the classiﬁer has overinterpreted its input, ﬁnding too much class-evidence in patterns that appear nonsensical to humans. Here, we demonstrate that neural networks trained on CIFAR-10 and ImageNet suffer from overinterpretation, and we ﬁnd models on CIFAR-10 make conﬁdent predictions even when 95% of input images are masked and humans cannot discern salient features in the remaining pixel-subsets. We introduce Batched Gradient SIS, a new method for discovering sufﬁcient input subsets for complex datasets, and use this method to show the sufﬁciency of border pixels in ImageNet for training and testing. Although these patterns portend potential model fragility in real-world deployment, they are in fact valid statistical patterns of the benchmark that alone sufﬁce to attain high test accuracy. Unlike adversarial examples, overinterpretation relies upon unmodiﬁed image pixels. We ﬁnd ensembling and input dropout can each help mitigate overinterpretation. 1

Introduction
Well-founded decisions by machine learning (ML) systems are critical for high-stakes applications such as autonomous vehicles and medical diagnosis. Pathologies in models and their respective training datasets can result in unintended behavior during deployment if the systems are confronted with novel situations. For example, a medical image classiﬁer for cancer detection attained high accuracy in benchmark test data, but was found to base decisions upon presence of rulers in an image (present when dermatologists already suspected cancer) [1]. We deﬁne model overinterpretation to occur when a classiﬁer ﬁnds strong class-evidence in regions of an image that contain no semantically salient features. Overinterpretation is related to overﬁtting, but overﬁtting can be diagnosed via reduced test accuracy. Overinterpretation can stem from true statistical signals in the underlying dataset distribution that happen to arise from particular properties of the data source (e.g., derma-tologists’ rulers). Thus, overinterpretation can be harder to diagnose as it admits decisions that are made by statistically valid criteria, and models that use such criteria can excel at benchmarks. We demonstrate overinterpretation occurs with unmodiﬁed subsets of the original images. In contrast to adversarial examples that modify images with extra information, overinterpretation is based on real patterns already present in the training data that also generalize to the test distribution. Hidden statistical signals of benchmark datasets can result in models that overinterpret or do not generalize to new data from a different distribution. Computer vision (CV) research relies on datasets like
CIFAR-10 [2] and ImageNet [3] to provide standardized performance benchmarks. Here, we analyze the overinterpretation of popular CNN architectures on these benchmarks to characterize pathologies. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Revealing overinterpretation requires a systematic way to identify which features are used by a model to reach its decision. Feature attribution is addressed by a large number of interpretability methods, although they propose differing explanations for the decisions of a model. One natural explanation for image classiﬁcation lies in the set of pixels that is sufﬁcient for the model to make a conﬁdent prediction, even in the absence of information about the rest of the image. In the example of the medical image classiﬁer for cancer detection, one might identify the pathological behavior by ﬁnding pixels depicting the ruler alone sufﬁce for the model to conﬁdently output the same classiﬁcations.
This idea of Sufﬁcient Input Subsets (SIS) has been proposed to help humans interpret the decisions of black-box models [4]. An SIS subset is a minimal subset of features (e.g., pixels) that sufﬁces to yield a class probability above a certain threshold with all other features masked.
We demonstrate that classiﬁers trained on CIFAR-10 and ImageNet can base their decisions on
SIS subsets that contain few pixels and lack human understandable semantic content. Nevertheless, these SIS subsets contain statistical signals that generalize across the benchmark data distribution, and we are able to train classiﬁers on CIFAR-10 images missing 95% of their pixels and ImageNet images missing 90% of their pixels with minimal loss of test accuracy. Thus, these benchmarks contain inherent statistical shortcuts that classiﬁers optimized for accuracy can learn to exploit, instead of learning more complex semantic relationships between the image pixels and the assigned class label. While recent work suggests adversarially robust models base their predictions on more semantically meaningful features [5], we ﬁnd these models suffer from overinterpretation as well.
As we subsequently show, overinterpretation is not only a conceptual issue, but can actually harm overall classiﬁer performance in practice. We ﬁnd model ensembling and input dropout partially mitigate overinterpretation, increasing the semantic content of the resulting SIS subsets. However, this mitigation is not a substitute for better training data, and we ﬁnd that overinterpretation is a statistical property of common benchmarks. Intriguingly, the number of pixels in the SIS rationale behind a particular classiﬁcation is often indicative of whether the image is correctly classiﬁed.
It may seem unnatural to use an interpretability method that produces feature attributions that look uninterpretable. However, we do not want to bias extracted rationales towards human visual priors when analyzing a model’s pathologies, but rather faithfully report the features used by a model. To our knowledge, this is the ﬁrst analysis showing one can extract nonsensical features from CIFAR-10 and ImageNet that intuitively should be insufﬁcient or irrelevant for a conﬁdent prediction, yet are alone sufﬁcient to train classiﬁers with minimal loss of performance. Our contributions include:
• We discover the pathology of overinterpretation and ﬁnd it is a common failure mode of ML models, which latch onto non-salient but statistically valid signals in datasets (Section 4.1).
• We introduce Batched Gradient SIS, a new masking algorithm to scale SIS to high-dimensional inputs and apply it to characterize overinterpretation on ImageNet (Section 3.2).
• We provide a pipeline for detecting overinterpretation by masking over 90% of each image, demonstrating minimal loss of test accuracy, and establish lack of saliency in these patterns through human accuracy evaluations (Sections 3.3, 4.2, 4.3).
• We show misclassiﬁcations often rely on smaller and more spurious feature subsets suggest-ing overinterpretation is a serious practical issue (Section 4.4).
• We identify two strategies for mitigating overinterpretation (Section 4.5). We demonstrate that overinterpretation is caused by spurious statistical signals in training data, and thus training data must be carefully curated to eliminate overinterpretation artifacts.
Code for this paper is available at: https://github.com/gifford-lab/overinterpretation. 2