Abstract
Although the optimization objectives for learning neural networks are highly non-convex, gradient-based methods have been wildly successful at learning neural networks in practice. This juxtaposition has led to a number of recent studies on provable guarantees for neural networks trained by gradient descent. Unfortunately, the techniques in these works are often highly speciﬁc to the particular setup in each problem, making it difﬁcult to generalize across different settings. To address this drawback in the literature, we propose a uniﬁed non-convex optimization framework for the analysis of neural network training. We introduce the notions of proxy convexity and proxy Polyak-Lojasiewicz (PL) inequalities, which are satisﬁed if the original objective function induces a proxy objective function that is implicitly minimized when using gradient methods. We show that stochastic gradient descent (SGD) on objectives satisfying proxy convexity or the proxy PL inequality leads to efﬁcient guarantees for proxy objective functions. We further show that many existing guarantees for neural networks trained by gradient descent can be uniﬁed through proxy convexity and proxy PL inequalities. 1

Introduction
Understanding the ability of gradient-based stochastic optimization algorithms to ﬁnd good minima of non-convex objective functions has become an especially important problem due to the success of stochastic gradient descent (SGD) in learning deep neural networks. Although there exist non-convex objective functions and domains for which SGD will necessarily lead to sub-optimal local minima, it appears that for many problems of interest in deep learning, across domains as varied as natural language and images, these worst-case situations do not arise. Indeed, a number of recent works have developed provable guarantees for GD and SGD when used for objective functions deﬁned in terms of neural networks over certain distributions, despite the non-convexity of the underlying optimization problem [5, 3, 6, 22, 14, 16]. To date, however, there has not been a framework which could unify the variegated approaches for guarantees in these settings.
In this work, we introduce the notion of proxy convexity and demonstrate that many existing provable guarantees for learning with neural networks trained by gradient-based optimization fall into a problem satisfying proxy convexity. First, let us deﬁne the learning problem over a distribution D, where the goal is to minimize the expected loss min w∈W
F (w) := Ez∼Df (w; z), (1.1) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where W is a parameter domain and f (· ; z) is a loss function. We are interested in guarantees using the online SGD algorithm, which uses a set of i.i.d. samples zt wt+1 = wt − η∇f (wt; zt), where η > 0 is a ﬁxed learning rate. We now introduce the ﬁrst notion of proxy convexity we will consider in the paper.
Deﬁnition 1.1 (Proxy convexity). We say that a function f : Rp → R satisﬁes (g, h)-proxy convexity if there exist functions g, h : Rp → R such that for all w, v ∈ Rp, (cid:104)∇f (w), w − v(cid:105) ≥ g(w) − h(v). i.i.d.∼ D and has updates given by
Clearly, every convex function f satisﬁes (f, f )-proxy convexity. We next introduce the analogy of proxy convexity for the Polyak–Łojasiewicz (PL) inequality [23].
Deﬁnition 1.2 (g-proxy, ξ-optimal PL inequality). We say that a function f : Rp → R satisﬁes a g-proxy, ξ-optimal Polyak–Łojasiewicz inequality with parameters α ∈ (0, 2] and µ > 0 (in short, f satisﬁes the (g, ξ, α, µ)-PL inequality) if there exists a function g : Rp → R and scalars ξ ∈ R,
µ > 0 such that for all w ∈ Rp, (cid:107)∇f (w)(cid:107)α ≥ 1 2
µ (g(w) − ξ) .
As we shall see below, the proxy PL inequality is a natural extension of the standard PL inequality.
Our main contributions are as follows. 1. When f satisﬁes (g, h)-proxy convexity, and f is either Lipschitz or satisﬁes a particular smooth-ness assumption, then for any norm bound R > 0, the online SGD algorithm run for polynomial (in 1/ε and R) number of iterations satisﬁes the following in expectation over z1, . . . , zT ∼ DT , min t<T
Ez∼Dg(wt; z) ≤ min (cid:107)w(cid:107)≤R
Ez∼Dh(w; z) + ε. 2. When f satisﬁes a (g, ξ, α, µ)-proxy PL inequality and has Lipschitz gradients, SGD run for a polynomial (in 1/ε) number of iterations satisﬁes the following in expectation over z1, . . . , zT ∼
DT ,
Ez∼Dg(wt; z) ≤ ξ + ε. min t<T 3. We demonstrate that many previous guarantees for neural networks trained by gradient descent can be uniﬁed in the framework of proxy convexity.
As we will describe in more detail below, if a loss function (cid:96) is (g, h)-proxy convex or satisﬁes a g-proxy PL inequality, then the optimization problem is straightforward and the crux of the problem then becomes connecting guarantees for the proxy g with approximate guarantees for f .
Notation. We use uppercase letters to refer to matrices, and lowercase letters will either refer to vectors or scalars depending on the context. For vectors w, we use (cid:107)w(cid:107) to refer to the Euclidean norm, and for matrices W we use (cid:107)W (cid:107) to refer to the Frobenius norm. We use the standard O(·),
Ω(·) notations to hide universal constants, with ˜O(·) and ˜Ω(·) additionally hiding logarithmic factors. 2 Proxy Convexity in Comparison to Other Non-convex Optimization
Frameworks
In this section, we describe how proxy convexity and proxy PL-inequalities relate to other notions in non-convex optimization. In Section 6, we will discuss additional related work. First, recall that a function f is (g, h)-proxy convex if there exist functions g and h such that for all w, v, (cid:104)∇f (w), w − v(cid:105) ≥ g(w) − h(v).
One notion from the non-convex optimization literature that is related to our notion of proxy convexity is that of invexity [19]. A function f is invex if it is differentiable and there exists a vector-valued function k(w, v) such that for any w, v, (cid:104)∇f (w), k(w, v)(cid:105) ≥ f (w) − f (v). 2
It has been shown that a smooth function f is invex if and only if every stationary point of f is a global minimum [10]. However, for many problems of interest involving neural networks, it is not the case that every stationary point will be a global optimum, which makes invexity a less appealing framework for understanding neural networks. Indeed, we shall see in Example 4.2 below that if one considers the problem of learning a single ReLU neuron x (cid:55)→ max(0, (cid:104)w, x(cid:105)) under the squared loss, it is not hard to see that there exist stationary points which are not global minima (e.g., w = 0 assuming the convention σ(cid:48)(0) = 0). By contrast, we shall see that the single ReLU neuron does satisfy a form of proxy convexity that enables SGD to ﬁnd approximately (but not globally) optimal minima. Thus even the simplest neural networks induce objective functions which are proxy convex and non-invex. We shall see in Example 3.3 that proxy convexity appears in the objective functions induced by wide and deep neural networks as well.
To understand how the proxy PL inequality compares to other notions in the optimization literature, recall that an objective function f satisﬁes the standard PL inequality [33, 27] if there exists µ > 0 such that (cid:107)∇f (w)(cid:107)2 ≥
[f (w) − f ∗]
∀w,
µ 2 where f ∗ = minw f (w). Clearly, any stationary point of an objective satisfying the standard PL inequality is globally optimal. Thus, the presence of local minima among stationary points in neural network objectives makes the standard PL inequality suffer from the same drawbacks that invexity does for understanding neural networks trained by gradient descent. This further applies to any of the conditions which are known to imply the PL inequality, like weak strong convexity, the restricted secant inequality, and the error bound condition (Karimi et al. [23]).1
In comparison, the (g, ξ, α, µ)-proxy PL inequality is satisﬁed if there exists a function g and constants
ξ > 0, α ∈ (0, 2] and µ > 0 such that (cid:107)∇f (w)(cid:107)α ≥
µ 2
[g(w) − ξ]
∀w.
It is clear that if a function f satisﬁes the standard PL inequality, then it satisﬁes the (f, f ∗, 2, µ) proxy PL inequality. Stationary points w0 of objective functions satisfying the proxy PL inequality have (cid:107)∇f (w0)(cid:107) = 0 which imply g(w0) ≤ ξ. In the case that g = f , the slack error term ξ allows for the proxy PL inequality framework to accommodate the possibility that stationary points may not be globally optimal (i.e. have objective value f ∗ = minw f (w)), but could be approximately optimal by, for example, having objective value at most ξ = C · f ∗ or ξ = C · f ∗ for some constant C ≥ 1.
When g (cid:54)= f , the proxy PL inequality allows for the possibility of analyzing a proxy loss function g which is implicitly minimized when using gradient-based optimization of the objective f .
√
At a high level, proxy convexity and the proxy PL inequality are well-suited to situations where stationary points may not be globally optimal, but may be approximately optimal with respect to a related optimization objective. The proxy convexity framework allows for one to realize this through developing problem-speciﬁc analyses that connect the proxy objective g to the original objective f .
As we shall see below, rich function classes like neural networks are often more easily analyzed by considering a proxy objective function that naturally appears when one analyzes the gradient of the loss.
Finally, we note that [25] introduced a different generalization of the PL inequality, namely the PL∗ and PL∗
ε inequality, which relaxes the standard PL inequality deﬁnition so that the PL condition only needs to hold on a subset of the domain. In particular, a function f : Rp → R satisﬁes the PL∗ inequality on a set S ⊂ Rp if there exists µ > 0 such that (cid:107)∇f (w)(cid:107)2 ≥ µf (w) ∀w ∈ S.
ε inequality on S if there exists a set S and ε ≥ 0 such that the PL∗
Likewise, f satisﬁes the PL∗ inequality holds on the set Sε = {w ∈ S : f (w) ≥ ε}. One can see that if f satisﬁes the PL∗
ε inequality on S, then the function g(w) := f (w) + ε satisﬁes the g-proxy, ε-optimal PL inequality on S.
We wish to emphasize the differences in the framing and motivation of the PL∗
ε inequality by [25] and that of proxy convexity and the proxy PL inequality in this paper. [25] focus on the geometry of 1Karimi et al. [23] shows that these conditions imply the PL inequality under the assumption that the objective function has Lipschitz-continuous gradients. 3
optimization in the overparameterized setting where one has a ﬁxed set of samples {(xi, yi)}n i=1 and a parametric model class g(x; w) (for w ∈ Rp, p > n) and the goal is to solve g(xi; w) = yi for all i ∈ [n]. In this setting one can view the optimization problem as a nonlinear least squares system with p unknowns and n equations, and [25] use geometric arguments to show that when p > n the
PL∗ condition is satisﬁed throughout most of the domain. They extend the PL∗ condition to the PL∗
ε condition with the motivation that in underparameterized settings, or when performing early stopping, there may not exist interpolating solutions. By contrast, we focus on the stochastic optimization setting, where the goal is to minimize the expected loss over some distribution; it is unclear how geometric arguments for minimizing the training loss can lead to generalization guarantees in the overparameterized setting, especially when considering online SGD where samples are observed one-by-one and ‘overparameterization’ has a less clear meaning. Furthermore, in this work we are not primarily interested in understanding how overparameterization affects optimization. Rather, our aim is to develop a framework that allows for formal characterizations of optimization problems where stationary points are not globally optimal with respect to the original objective but are approximately optimal with respect to proxy objective functions. We will demonstrate below that such a framework can help unify a number of works on learning with neural networks trained by gradient descent. 3 Proxy PL Inequality Implies Proxy Objective Guarantees
In this section, we show that for loss functions satisfying a proxy PL inequality, SGD2 efﬁciently minimizes the proxy. We leave the proofs for Section 5.
Theorem 3.1. Suppose F (w) = Ez∼Df (w; z) where f (· ; z) satisﬁes the (g(· ; z), ξ(z), α, µ)-proxy
PL inequality for some function g(· ; z) : Rp → R for each z. Denote by G(w) := Ez∼Dg(w; z).
Assume that f is non-negative and has L2-Lipschitz gradients. Then for any ε > 0, provided
η < 1/L2, online SGD with ﬁxed step size η and run for T = 2η−1(µε/2)−2/αf (w0; z0) iterations results in the following guarantee in expectation over z0, . . . , zT −1 ∼ DT ,
G(wt) ≤ Ez∼Dξ(z) + ε. min t<T (3.1)
To get a feel for how a proxy PL inequality might be useful for learning neural networks, consider a classiﬁcation problem with labels y ∈ {±1}, and suppose that N (W ; x) is a neural network function. A standard approach for learning neural networks is to minimize the cross-entropy loss (cid:96)(yN (W ; x)) = log (cid:0)1 + exp(−yN (W ; x))(cid:1) using gradient descent. Using the variational form of the norm, we have (cid:107)∇(cid:96)(yN (W ; x))(cid:107) = sup (cid:107)U (cid:107)=1 (cid:104)∇(cid:96)(yN (W ; x)), U (cid:105)
≥ −(cid:96)(cid:48)(yN (W ; x)) · y(cid:104)∇N (W ; x), V (cid:105), (3.2) where V is any matrix satisfying (cid:107)V (cid:107) = 1. Now, although the function −(cid:96)(cid:48) is not an upper bound for (cid:96) (indeed, −(cid:96)(cid:48) < (cid:96)), it is an upper bound for a constant multiple of the zero-one loss, and can thus serve as a proxy for the classiﬁcation error.3 This is because for convex and decreasing losses (cid:96), the function −(cid:96)(cid:48) is non-negative and decreasing, and hence by Markov’s inequality,
P(y (cid:54)= sgn(N (W ; x))) = P(y · N (W ; x) < 0)
= P(−(cid:96)(cid:48)(yN (w; x)) > −(cid:96)(cid:48)(0))
≤ 1
−(cid:96)(cid:48)(0)
E[−(cid:96)(cid:48)(yN (W ; x))].
Thus, if one can bound the population risk under −(cid:96)(cid:48), one has a bound for the classiﬁcation error.
Indeed, this property has been used in a number of recent works on neural networks [6, 13, 22, 16].
This lets the −(cid:96)(cid:48) term in (3.2) represent the desired proxy g in the deﬁnition of the (g, ξ, α, µ)-proxy
PL inequality. Thus, for neural network classiﬁcation problems, the problem of showing the neural network has small classiﬁcation error is reduced to constructing a matrix V that allows for the quantity y(cid:104)∇N (W ; x), V (cid:105) to be large and non-negative. The quantity y(cid:104)∇N (W ; x), V (cid:105) can be thought of 2We focus on online SGD in this paper for simplicity. Analogous optimization guarantees would hold for other variants of gradient descent that utilize samples in batches. 3In fact, the function z (cid:55)→ [(cid:96)(cid:48)(z)]2 is also a proxy for the classiﬁcation error; see [15, Appendix A]. 4
as a margin function that is large when the gradient of the neural network loss points in a good direction. Although we shall see below that in some instances one can derive a lower bound for y(cid:104)∇N (W ; x), V (cid:105) that holds for all W , x, and y, a more general approach would be to show that along the gradient descent trajectory W (t), a lower bound for y(cid:104)∇N (W (t); x), V (cid:105) holds.4
In the remainder of this section, we will show that a number of recent works on learning neural networks with gradient descent utilized proxy PL inequalities. In our ﬁrst example, we consider recent work by Charles and Papailiopoulos [7] that directly used a (standard) PL inequality.
Example 3.2 (Standard PL inequality for single leaky ReLU neurons and deep linear networks).
Charles and Papailiopoulos [7] showed that the standard PL inequality holds in two distinct settings.
The ﬁrst is that of a single leaky ReLU neuron x (cid:55)→ σ((cid:104)w, x(cid:105)), where σ(z) = max(cσz, z) for cσ (cid:54)= 0. They showed that if smin(X) is the smallest singular value of the matrix X ∈ Rn×d of n samples, then for a λ-strongly convex loss (cid:96), the loss f (w) = (cid:96)(σ((cid:104)w, x(cid:105))) satisﬁes the standard
µ-PL inequality, i.e., the (f, f ∗, 2, µ)-proxy PL inequality for µ = λsmin(X)2c2
σ (Charles and
Papailiopoulos [7, Theorem 4.1]).
The same authors also showed that under certain conditions the standard PL inequality holds when the neural network takes the form N (W ; x) = WL · · · W1x and the loss is the squared loss, f (W ) = 1/2(y − N (W ; x))2.
In particular, they showed that if smin(Wi) ≥ τ > 0 throughout the gradient descent trajectory, then f satisﬁes the standard µ-PL inequality for
µ = Lτ 2L−2/ (cid:13)
The standard PL inequality has been used by a number of other authors in the deep learning theory literature, see e.g. Xie et al. [36, Theorem 1], Hardt and Ma [20, Eq. 2.3], Zhou and Liang [40,
Theorem 1], Shamir [35, Theorem 3]. (cid:13)(XX (cid:62))−1X(cid:13) 2
F (Charles and Papailiopoulos [7, Theorem 4.5]). (cid:13)
In our next example, we show that a proxy PL inequality holds for deep neural networks in the neural tangent kernel (NTK) regime.
Example 3.3 (Proxy PL inequality for deep neural networks in NTK regime). Consider the class of deep, L-hidden-layer ReLU networks, either with or without residual connections:
N1(W ; x) = σ(W (1)x), Nl(W ; x) = slNl−1(W ; x) + σ(W (l)Nl−1(W ; x)), l = 2, . . . , L,
N (W ; x) = m (cid:88) j=1 aj[NL(W ; x)]j, where sl = 0 for fully-connected networks and sl = 1 for residual networks. Cao and Gu [6,
Theorem 4.2], Frei, Cao, and Gu [13, Lemma 4.3], and Zou et al. [41, Lemma B.5] have shown that under certain distributional assumptions and provided the iterates of gradient descent stay close to their intialization, one can guarantee that for the cross-entropy loss f (W ; (x, y)) = (cid:96)(yN (W ; x)), (cid:107)∇f (W ; (x, y))(cid:107) ≥ C1 · −(cid:96)(cid:48)(yN (W ; x)). (3.3)
By deﬁning g(W ; z) = −(cid:96)(cid:48)(yN (W ; x)), the loss f satisﬁes the (g, 0, 1, 2C1)-proxy PL inequality.
Since the ReLU is not smooth, the loss f will not have Lipschitz gradients, and thus a direct application of Theorem 3.1 is not possible. Instead, the authors show that in the NTK regime, the loss obeys a type of semi-smoothness that still allows for an analysis simliar to that of Theorem 3.1.
Example 3.4 (Proxy PL inequality for one-hidden-layer networks outside NTK regime). Consider a one-hidden-layer network with activation function σ, m (cid:88)
N (W ; (x, y)) = ajσ((cid:104)wj, x(cid:105)), (3.4) j=1 where the second layer weights {aj}m j=1 are randomly initialized and ﬁxed at initialization, but the
{wj}m j=1 are trained. Assume σ satisﬁes σ(cid:48)(z) ≥ cσ > 0 for all z (e.g., the leaky ReLU activation).
Frei, Cao, and Gu have shown [16, Lemma 3.1] that there exists a matrix V ∈ Rm×d with (cid:107)V (cid:107)F = 1 such that for distributions satisfying anti-concentration, for any x, y and W , y(cid:104)∇N (W ; x), V (cid:105) ≥ C1[cσ − ξ(x, y)], 4Although our results as stated would not immediately apply in this setting, the proof would be the same up to trivial modiﬁcations. 5
where E[ξ(x, y)] = O( over D. Thus, when f (W ; (x, y)) = (cid:96)(yN (W ; (x, y)) is the cross-entropy loss,
OPT) where OPT is the best classiﬁcation error achieved by a halfspace
√ (cid:107)∇f (W ; (x, y))(cid:107) = sup (cid:104)∇f (W ; (x, y)), Z(cid:105) (cid:107)Z(cid:107)F =1
≥ −(cid:96)(cid:48)(yN (W ; x)) · y(cid:104)∇N (W ; x), V (cid:105)
≥ C1cσ · [−(cid:96)(cid:48)(yN (W ; x)) − c−1
As in Example 3.3, by deﬁning g(W ; z) = −(cid:96)(cid:48)(yN (W ; x)), (g, c−1
Lipschitz gradients for some constant L2 > 0, Theorem 3.1 shows that the loss f satisﬁes the
σ ξ(x, y), 1, 2C1cσ)-proxy PL inequality. Thus, provided we can show that f (·; z) has L2-σ ξ(x, y)]. min t<T
P(x,y)(y (cid:54)= sgn(N (W (t); x))) ≤ min t<T
≤ 2C2c−1
σ
√
E(x,y)∼D[−(cid:96)(cid:48)(yN (W (t); x)]
|(cid:96)(cid:48)(0)|
E(x,y)∼Dξ(x, y) + ε
= O(
OPT) + ε.
Provided σ is such that σ(cid:48) is continuous and differentiable, then f has L2-Lipschitz gradients and thus the guarantees will follow. In particular, this analysis follows if σ is any smoothed approximation to the leaky ReLU which satisﬁes σ(cid:48)(z) ≥ cσ > 0.
Note that the above optimization analysis is an original contribution of this work as we utilize a completely different proof technique than that of [16]. In that paper, the authors utilize a Perceptron-style proof technique that analyzes the correlation (cid:104)W (t), V (cid:105) of the weights found by gradient descent and a reference matrix V . Their proof relies crucially on the homogeneity of the (non-smooth) leaky
ReLU activation, namely that zσ(cid:48)(z) = σ(z) for z ∈ R, and cannot accommodate more general smooth activations. By contrast, the proxy PL inequality proof technique in this example relies upon the smoothness of the activation function and is more similar to smoothness-based analyses of gradient descent. 4 Proxy Convexity Implies Proxy Objective Guarantees
In this section, we show that if f satisﬁes (g, h)-proxy convexity, we can guarantee that by minimizing f with gradient descent, we ﬁnd a hypothesis for which g(w) is at least as small as the best norm-bounded predictor as measured by the loss h. We present two versions of our result: one that relies upon fewer assumptions on the loss f but needs a small step size, and another that requires a proxy smoothness assumption on f but allows for a constant step size. The proofs for the theorem are given in Section 5.
Theorem 4.1. Suppose that F (w) := Ez∼Df (w; z) and f (·; z) is (g(·; z), h(·; z))-proxy convex for each z. Denote H(w) := Ez∼Dh(w; z) and G(w) := Ez∼Dg(w; z). (a) Assume there exists L1 > 0 such that for all w, Ez∼D[(cid:107)∇f (w; z)(cid:107)2] ≤ L2 1. Then for any v ∈ Rp and any ε > 0, performing online SGD on F (w) from an arbitrary initialization w0 with for T = η−1ε−1 (cid:107)w0 − v(cid:107)2 iterations implies that, in expectation over
ﬁxed step size η ≤ εL−2 (z0, . . . , zT −1) ∼ DT , 1 min t<T
G(wt) ≤ H(v) + ε. (b) Assume there exists L2 > 0 such that for all w, Ez∼D[(cid:107)∇f (w; z)(cid:107)2] ≤ 2L2Ez∼Dg(w; z). Then for any v ∈ Rp and any ε > 0, performing online SGD on F (w) from an arbitrary initialization 2 /2 for T = η−1ε−1 (cid:107)w0 − v(cid:107)2 implies that, in expectation over with ﬁxed step size η ≤ L−1 (z0, . . . , zT −1) ∼ DT , min t<T
G(wt) ≤ (1 + 2ηL2)H(v) + ε.
In order for (g, h)-proxy convexity to be useful, there must be a way to relate guarantees for g into guarantees for the desired objective function f . In the remainder of this section, we will discuss two neural network learning problems which are non-convex and yet satisfy proxy convexity which leads to generalization guarantees. Our ﬁrst example is the problem of learning a neural network with a single nonlinear unit. 6
Example 4.2 (Single neuron satisﬁes proxy convexity). Consider the problem of learning a single neuron x (cid:55)→ σ((cid:104)w, x(cid:105)) under the squared loss, where σ is the ReLU activation. The objective function of interest is
F (w) = E(x,y)∼D 1/2(σ((cid:104)w, x(cid:105)) − y)2.
Denote
F (w).
F ∗ := min (cid:107)w(cid:107)≤1
It is known that F is non-convex [37]. Under the assumption that learning sparse parities with noise is computationally hard, it is known that no polynomial time algorithm can achieve risk F ∗ exactly; moreover, it is known that (unconditionally) the standard gradient descent algorithm cannot achieve risk F ∗ [18].5 However, Frei, Cao, and Gu [14] showed that although F is non-convex and no algorithm can achieve risk F ∗, F does satisfy a form of proxy convexity that allows for gradient
F ∗). They showed that the loss function descent to achieve risk O(
√ satisﬁes (g, h)-proxy convexity, where f (w; (x, y)) = 1/2(σ((cid:104)w, x(cid:105)) − y)2 g(w; (x, y)) = 2 [σ((cid:104)w, x(cid:105)) − σ((cid:104)v∗, x(cid:105))]2 σ(cid:48)((cid:104)w, x(cid:105)), h(v; (x, y)) = |σ((cid:104)v, x(cid:105)) − y| = (cid:112)2f (v; (x, y), where v∗ is the population risk minimizer of F (w) (see their Eq. (3.13)). Moreover, they showed (see their Eq. (3.9)) (cid:107)∇f (w; z)(cid:107)2 ≤ 8g(w; z).
Thus Theorem 4.1(b) implies that SGD with step size η ≤ 1/8 and T = 2η−1ε−1 (cid:107)w0 − v∗(cid:107)2 iterations will ﬁnd a point wt satisfying (cid:104) (cid:105)
G(wt) = 2E(x,y) (σ((cid:104)wt, x(cid:105)) − σ((cid:104)v∗, x(cid:105)))2 σ(cid:48)((cid:104)wt, x(cid:105))
≤ (1 + 8η)H(v∗) + ε
≤ (1 + 8η)E|σ((cid:104)v∗, x(cid:105)) − y| + ε
≤ (1 + 8η)(cid:112)E[(σ((cid:104)v∗, x(cid:105)) − y)2] + ε
= O(
F ∗).
√
√
F ∗) implies
The authors then show that under some distributional assumptions on D, G(wt) = O(
F ∗) [14, Lemma 3.5]. Thus, the optimization problem for F induces a proxy convex
F (wt) = O( optimization problem deﬁned in terms of G which yields guarantees for G in terms of H, and this in turn leads to approximate optimality guarantees for the original objective F .
√
In our next example, we show that a number of works on learning one-hidden-layer ReLU networks in the neural tangent kernel regime [21] can be cast as problems satisfying proxy convexity.
Example 4.3 (Proxy convexity for one-hidden-layer ReLU neural networks in the NTK regime).
Consider the class of one-hidden-layer ReLU networks consisting of m neurons,
N (W ; (x, y)) = m (cid:88) ajσ((cid:104)wj, x(cid:105)), j=1 where the {aj}m j=1 are randomly initialized and ﬁxed at initialization, but the {wj}m j=1 are trained.
Suppose we consider a binary classiﬁcation problem, where y ∈ {±1} and we minimize the cross-entropy loss,
F (W ) = E(x,y)∼Df (W ; (x, y)),
Ji and Telgarsky [22, Proof of Lemma 2.6] showed that there exists a function ˜h(a, W, V ; (x, y)) such that the iterates of gradient descent satisfy f (W ; (x, y)) = (cid:96)(cid:0)yN (W ; (x, y))(cid:1), (cid:96)(z) = log(1+exp(−z)). (cid:104)∇f (W ; (x, y)), w − v(cid:105) ≥ f (W ; (x, y)) − ˜h(a, W, V ; (x, y)). 5This stands in contrast to learning a single leaky ReLU neuron x (cid:55)→ max(αx, x) for α (cid:54)= 0, which as we showed in Example 3.2 can be solved using much simpler techniques. 7
Under the assumption that the iterates of gradient descent stay close to the initialization (i.e., the neural tangent kernel regime), they show that ˜h(a, W, V ; (x, y)) ≤ ε under distributional assumptions, and thus F (w) will satisfy (f, ˜h ≡ ε)-proxy convexity. The cross entropy loss satisﬁes [(cid:96)(cid:48)(z)]2 ≤ (cid:96)(z), and thus we can apply Theorem 4.1(b) to get guarantees of the form mint<T F (wt) ≤ ε for the cross-entropy loss F (W ) of SGD-trained neural networks in the NTK regime.
In another problem of learning one-hidden-layer networks, Allen-Zhu, Li, and Liang [3, Proof of
Lemma B.4] show that there exists a proxy loss function g(a, W ; (x, y)) such that provided the neural network weights stay close to their initialized values, f (a, W ; (x, y)) satisﬁes (g, g + ε) proxy convexity. Again using that the cross-entropy loss satisﬁes [(cid:96)(cid:48)(z)]2 ≤ (cid:96)(z), Theorem 4.1(b) shows that SGD-trained neural networks in the NTK regime satisfy mint<T G(Wt) ≤ minV G(V ) + ε.
They further show that the proxy loss g is close to the cross entropy loss, i.e. |E[g(a, W ; (x, y))] −
E[f (a, W ; (x, y))]| ≤ ε, implying a bound of the form mint<T F (Wt) ≤ minV F (V ) + ε. 5 Proof of the Main Results
In this section we provide the proofs of the theorems given in Sections 3 and 4.
We ﬁrst give the proof of Theorem 3.1 which provides guarantees for learning with objectives satisfying proxy PL inequalities.
Proof of Theorem 3.1. Since f has L2-Lipschitz gradients, we have for any w, w(cid:48) and ﬁxed z, f (w; z) ≤ f (w(cid:48); z) + (cid:104)∇f (w(cid:48); z), w − w(cid:48)(cid:105) +
L2 2 (cid:107)w − w(cid:48)(cid:107)2 .
Taking w = wt+1, w(cid:48) = wt, and z = zt, f (wt+1; zt) ≤ f (wt; zt) − η (cid:107)∇f (wt; zt)(cid:107)2 +
η2L2 2 (cid:107)∇f (wt; zt)(cid:107)2
= f (wt; zt) − η [1 − ηL2/2] (cid:107)∇f (wt; zt)(cid:107)2 .
Since η < 1/L2, we have (1 − ηL2/2)−1 ≤ 2, and thus we can rearrange the above to get 1
η(1 − ηL2/2) 2
η
[f (wt; zt) − f (wt+1; zt)].
[f (wt; zt) − f (wt+1; zt)] (cid:107)∇f (wt; zt)(cid:107)2 ≤
≤ (5.1) (5.2)
Summing the above from t = 0 to t = T − 1 and using that f is non-negative, we get 1
T
T −1 (cid:88) t=0 (cid:107)∇f (wt; zt)(cid:107)2 ≤ 2f (w0; z0)
ηT
.
Using the deﬁnition of proxy PL inequality, this implies 1
T
T −1 (cid:88) (µ/2)2/α(g(wt; zt) − ξ(zt))2/α ≤ t=0 2f (w0; z0)
ηT
.
Taking the minimum over t < T and re-arranging terms, this means
Therefore, we have (g(wt; zt) − ξ(zt))2/α ≤ min t<T 2f (w0; z0)
ηT (µ/2)2/α
. min t<T g(wt; zt) ≤ ξ(zt) + (cid:18) 2f (w0; z0)
ηT
· 2
µ (cid:19)α/2
.
Taking T = 2η−1f (w0; z0)(µε/2)−2/α and taking expectations over z0, . . . , zT −1, we get (3.1).
We next prove guarantees for SGD when the objective satisﬁes proxy convexity. 8
Proof of Theorem 4.1. By the deﬁnition of proxy convexity, (cid:107)wt − v(cid:107)2 − (cid:107)wt+1 − v(cid:107)2 = 2η(cid:104)∇f (wt; zt), wt − v(cid:105) − η2 (cid:107)∇f (wt; zt)(cid:107)2
≥ 2η[g(wt; zt) − h(v; zt)] − η2 (cid:107)∇f (wt; zt)(cid:107)2 .
Conditional on the values of z0, . . . , zt−1 (and hence on the value of wt), taking expectations of both sides with respect to zt ∼ D results in (cid:107)wt − v(cid:107)2 − Ezt∼D (cid:107)wt+1 − v(cid:107)2 ≥ 2η[G(wt) − H(v)] − η2Ezt∼D (cid:107)∇f (wt; zt)(cid:107)2 . (5.3)
For case (a), this results in (cid:107)wt − v(cid:107)2 − Ezt∼D (cid:107)wt+1 − v(cid:107)2 ≥ 2η[G(wt) − H(v) − η/2L2 1].
Dividing both sides by 2ηT and summing from t = 0, . . . , T − 1, we get 1
T
T −1 (cid:88) t=0
G(wt) ≤ 1
T
T −1 (cid:88) t=0
H(v) +
ηL2 1 2
+ (cid:107)w0 − v(cid:107)2 − EzT −1∼D (cid:107)wT − v(cid:107)2 2ηT
.
Taking expectations over z0:t = (z0, . . . , zt−1) ∼ Dt, we get
Ez0:t∼DtG(wt) ≤ min t<T 1
T
T −1 (cid:88) t=0
Ez0:t∼DtG(wt) ≤ H(v) +
ηL2 1 2
+ (cid:107)w0 − v(cid:107)2 2ηT
.
In particular, for η ≤ εL−2 1 and T = η−1ε−1 (cid:107)w0 − v(cid:107)2, we get
Ez0:t∼DtG(wt) ≤ H(v) + ε. min t<T
For case (b), (5.3) becomes (cid:107)wt − v(cid:107)2 − Ezt∼D (cid:107)wt+1 − v(cid:107)2 ≥ 2η[G(wt) − H(v) − ηL2G(wt)]
= 2η [(1 − ηL2)G(wt) − H(v)] .
T −1 (cid:88)
Dividing both sides by 2ηT (1 − ηL2) and summing from t = 0, . . . , T − 1, we get (cid:107)w0 − v(cid:107)2 − EzT −1∼D (cid:107)wT − v(cid:107)2 2ηT (1 − ηL2) (1 + 2ηL2) (cid:107)w0 − v(cid:107)2 2ηT
≤ (1 + 2ηL2)H(v) + 1 1 − ηL2
G(wt) ≤
H(v) + 1
T t=0
, where in the last line we have used that η ≤ L−1 expectations over z0:t = (z0, . . . , zt−1) ∼ Dt, we get 2 /2 and that 1/(1 − x) ≤ 1 + 2x on [0, 1/2]. Taking
Ez0:t∼DtG(wt) ≤ min t<T 1
T
T −1 (cid:88) t=0
Ez0:t∼DtG(wt)
≤ (1 + 2ηL2)H(v) + 2 (cid:107)w0 − v(cid:107)2 2ηT
.
In particular, for T = η−1ε−1 (cid:107)w0 − v(cid:107)2, we get
Ez0:t∼DtG(wt) ≤ (1 + 2ηL2)H(v) + ε. min t<T
We note that under additional assumptions on D and the loss function, we could improve the results from holding in expectation over the draws of the sample to high probability guarantees by using standard concentration arguments. This is easily done when the objective function satisﬁes proxy convexity: we can make a slight modiﬁcation to the proof of Theorem 4.1 to argue inductively that 1+ε, we have that that (cid:107)wt − v(cid:107)2−(cid:107)wt+1 − v(cid:107)2 ≥ ε. until we reach a point with G(wt) ≤ H(v)+ηL2
This implies that the norm of the predictors remain bounded throughout the trajectory of gradient descent until we reach the desired point with G(wt) ≤ H(v) + ηL2 1 + ε, which can then be used in
Rademacher complexity-type arguments [4]. This type of argument was previously used by e.g., Frei et al. [14]. 9
6 Additional