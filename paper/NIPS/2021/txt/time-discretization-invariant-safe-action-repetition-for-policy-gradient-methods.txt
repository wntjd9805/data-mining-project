Abstract
In reinforcement learning, continuous time is often discretized by a time scale
δ, to which the resulting performance is known to be highly sensitive. In this work, we seek to ﬁnd a δ-invariant algorithm for policy gradient (PG) methods, which performs well regardless of the value of δ. We ﬁrst identify the underly-ing reasons that cause PG methods to fail as δ 0, proving that the variance of the PG estimator can diverge to inﬁnity in stochastic environments under a certain assumption of stochasticity. While durative actions or action repetition can be employed to have δ-invariance, previous action repetition methods can-not immediately react to unexpected situations in stochastic environments. We thus propose a novel δ-invariant method named Safe Action Repetition (SAR) applicable to any existing PG algorithm. SAR can handle the stochasticity of environments by adaptively reacting to changes in states during action repetition.
We empirically show that our method is not only δ-invariant but also robust to stochasticity, outperforming previous δ-invariant approaches on eight MuJoCo environments with both deterministic and stochastic settings. Our code is available at https://vision.snu.ac.kr/projects/sar.
→ 1

Introduction
Deep reinforcement learning (RL) has demonstrated phenomenal achievements in a wide array of tasks, including superhuman game-playing [19, 27] and controlling complex robots [8, 13]. Most
RL algorithms are based on an Markov Decision Process (MDP), which is a discrete-time control process for the iteration of observing a state and performing an action. However, numerous real-world problems such as robotic manipulation and autonomous driving are deﬁned in continuous time, which does not directly ﬁt the MDP setting. To ﬁll this gap, continuous time is often discretized by a discretization time scale δ, where the RL agent makes a decision at every δ. It has been shown that
RL algorithms are greatly sensitive to this hyperparameter [1, 36]. For instance, altering δ via frame skipping leads to drastic performance differences [1, 4]. Indeed, an excessively high δ precludes the agent from making ﬁne-grained decisions, which is likely to cause performance degradation.
On average, the agent could perform equally well or better with a lower δ than with a higher δ, since the agent can make decisions more frequently. However, Baird [2] and Tallec et al. [36] theoretically 0 as the action-value (Q) function collapses to the proved that the standard Q-learning fails when δ
→ state-value (V) function, eliminating preferences between actions. As will be shown in Section 4.1, policy gradient (PG) methods fail as well with an inﬁnitesimal δ for the following three reasons: (1)
The variance of the gradient estimator explodes. (2) Exploration ranges may become highly limited. (3) Inﬁnitely many decision steps are required. The latter two also apply to Q-learning methods.
Therefore, it is generally required to differently set an appropriate δ for each continuous environment.
Indeed, continuous control environments in MuJoCo [37] have different discretization time scales from one another, ranging from 0.008s (Hopper) to 0.05s (InvertedDoublePendulum). However, such 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
tuning of δ could be burdensome when applying RL algorithms to new environments, considering its signiﬁcant inﬂuence on performance [1, 36]. Furthermore, even if the optimal δ is found in simulation, trained policies may not be transferable to real-world settings, since physical sensors often have their unique sampling frequencies.
There have been proposed some methods for robustness to discretization of time scales. δ-invariant methods could bring several advantages: (1) It obviates the need for tuning δ on each continuous control environment. (2) They can achieve better performance by utilizing more ﬁne-grained control with a low δ. (3) Using a policy with adaptive decision frequencies (as a variant of δ-invariant policies), the agent can efﬁciently take actions only when necessary, which could expedite training without losing agility. Tallec et al. [36] introduced an algorithm based on Advantage Updating [2], which can make existing Q-learning methods (e.g., DQN [18] and DDPG [16]) invariant to δ by preventing them from Q-function collapse. For PG methods, Munos [21] and Wawrzynski [38] proposed methods that can cope with ﬁne time discretization. However, these methods either assume access to the gradient of the reward function or require an inﬁnite number of decision steps (or 0, both of which could hinder its application to real-world environments. training steps) when δ
We aim at proposing an efﬁcient δ-invariant approach applicable to existing PG methods such as PPO
[30], TRPO [28] and A2C [20]. One straightforward approach may be to take durative actions by making policies produce both actions and their durations. Such an approach is practically equivalent to prior work on action repetition whose policies output both actions and the number of action repetitions [15, 31], since continuous control environments such as MuJoCo often already provide discretized time scales. However, prior approaches to action repetition possess some limitations. For example, there is no way to stop repeating a chosen action during a repetition period, which means that they are not capable of immediately handling unexpected events in stochastic environments. This could lead to catastrophic failure in some real-world settings such as autonomous driving.
→
We thus propose an alternative approach named Safe Action Repetition (SAR) with the key idea of repeating an action until the agent exits its safe region. Our policy produces both an action and a safe region in the state space, only within which the chosen action is repeated. SAR enables any PG algorithm to not only be δ-invariant but also be robust to stochasticity such as unexpected events in the environment, because such situations lead the agent’s state to be outside of the safe region, immediately causing the cease of the current action. We apply the proposed method to several
PG algorithms and empirically show that SAR indeed exhibits δ-invariance on various MuJoCo environments and outperforms baselines on both deterministic and stochastic settings.
Our contributions can be summarized as follows:
• We ﬁrst provide a more general proof on the variance explosion of the PG estimator, which 0. We then show that temporally extended is the main reason why PG methods fail as δ actions can resolve the failure mode of PG algorithms with a low δ.
→
• We introduce a novel δ-invariant method named SAR applicable to any PG method on continuous control domains. To the best of our knowledge, this is the ﬁrst action repetition (or durative action) method that repeats an action based on the agent’s state, rather than a precomputed action duration. As a result, SAR can cope with unexpected situations in stochastic environments, which existing action repetition methods cannot handle.
• We apply SAR to three PG methods, PPO, TRPO and A2C, and empirically demonstrate that our SAR method is mostly invariant to δ on eight MuJoCo environments. We also verify its robustness to stochasticity via three different stochastic settings on each MuJoCo environment. Our method also outperforms previous δ-invariant approaches such as FiGAR-C [31] and DAU [36] on those settings. 2