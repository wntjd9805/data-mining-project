Abstractions for Deep Reinforcement Learning
Cameron Allen∗
Brown University
Neev Parikh
Brown University
Omer Gottesman
Brown University
George Konidaris
Brown University
Abstract
A fundamental assumption of reinforcement learning in Markov decision processes (MDPs) is that the relevant decision process is, in fact, Markov. However, when
MDPs have rich observations, agents typically learn by way of an abstract state representation, and such representations are not guaranteed to preserve the Markov property. We introduce a novel set of conditions and prove that they are sufﬁcient for learning a Markov abstract state representation. We then describe a practical training procedure that combines inverse model estimation and temporal contrastive learning to learn an abstraction that approximately satisﬁes these conditions. Our novel training objective is compatible with both online and ofﬂine training: it does not require a reward signal, but agents can capitalize on reward information when available. We empirically evaluate our approach on a visual gridworld domain and a set of continuous control benchmarks. Our approach learns representations that capture the underlying structure of the domain and lead to improved sample efﬁciency over state-of-the-art deep reinforcement learning with visual features— often matching or exceeding the performance achieved with hand-designed compact state information. 1

Introduction
Reinforcement learning (RL) in Markov decision processes with rich observations requires a suitable state representation. Typically, such representations are learned implicitly as a byproduct of doing deep RL. However, in domains where precise and succinct expert state information is available, agents trained on such expert state features usually outperform agents trained on rich observations.
Much recent work (Shelhamer et al., 2016; Pathak et al., 2017; Ha & Schmidhuber, 2018; Gelada et al., 2019; Yarats et al., 2019; Kaiser et al., 2020; Laskin et al., 2020a,b; Zhang et al., 2021) has sought to close this representation gap by incorporating a wide range of representation-learning objectives that help the agent learn abstract representations with various desirable properties.
Perhaps the most obvious property to incentivize in a state representation is the Markov property, which holds if and only if the representation contains enough information to accurately characterize the rewards and transition dynamics of the decision process. Markov decision processes (MDPs) have this property by deﬁnition, and most reinforcement learning algorithms depend on having Markov state representations. For instance, the ubiquitous objective of learning a stationary, state-dependent optimal policy that speciﬁes how the agent should behave is only appropriate for Markov states.
But learned abstract state representations are not necessarily Markov, even when built on top of
MDPs. This is due to the fact that abstraction necessarily throws away information. Discard too much information, and the resulting representation cannot accurately characterize the environment. Discard too little, and agents will fail to close the representation gap. Abstraction must balance between
∗Please send any correspondence to Cameron Allen <csal@brown.edu>. Code repository available at https://github.com/camall3n/markov-state-abstractions. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (Left) A 6×6 visual gridworld domain with hidden state s and unknown sensor σ, where an abstraction function φ maps each high-dimensional observed state x to a lower-dimensional abstract state z (orange circle). (Right) Our Markov abstraction training architecture. A shared encoder φ maps ground states x, x(cid:48) to abstract states z, z(cid:48), which are inputs to an inverse dynamics model I and a contrastive model D that discriminates between real and fake state transitions. The agent’s policy π depends only on the current abstract state. ignoring irrelevant information and preserving what is important for decision making. If reward feedback is available, an agent can use it to determine which state information is relevant to the task at hand. Alternatively, if the agent can predict raw environment observations from learned abstract states, then all available information is preserved (along with the Markov property). However, these approaches are impractical when rewards are sparse or non-existent, or observations are sufﬁciently complex.
We introduce a new approach to learning Markov state abstractions. We begin by deﬁning a set of theoretical conditions that are sufﬁcient for an abstraction to retain the Markov property. We next show that these conditions are approximately satisﬁed by simultaneously training an inverse model to predict the action distribution that explains two consecutive states, and a discriminator to determine whether two given states were in fact consecutive. Our combined training objective (architecture shown in Fig. 1, right) supports learning Markov abstract representations without requiring reward information or observation prediction.
Our method is effective for learning Markov state abstractions that are highly beneﬁcial for decision making. We perform evaluations in two settings with rich (visual) observations: a gridworld naviga-tion task (Fig. 1, left) and a set of continuous control benchmarks. In the gridworld, we construct an abstract representation ofﬂine—without access to reward feedback—that captures the underlying structure of the domain and fully closes the representation gap between visual and expert features.
In the control benchmarks, we combine our training objective (online) with traditional RL, where it leads to a signiﬁcant performance improvement over state-of-the-art visual representation learning. 2