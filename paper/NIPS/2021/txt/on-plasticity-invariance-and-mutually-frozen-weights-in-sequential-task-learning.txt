Abstract
Plastic neural networks have the ability to adapt to new tasks. However, in a continual learning setting, the conﬁguration of parameters learned in previous tasks can severely reduce the adaptability to future tasks. In particular, we show that, when using weight decay, weights in successive layers of a deep network may become “mutually frozen”. This has a double effect: on the one hand, it makes the network updates more invariant to nuisance factors, providing a useful bias for future tasks. On the other hand, it can prevent the network from learning new tasks that require signiﬁcantly different features. In this context, we ﬁnd that the local input sensitivity of a deep model is correlated with its ability to adapt, thus leading to an intriguing trade-off between adaptability and invariance when training a deep model more than once. We then show that a simple intervention that “resets” the mutually frozen connections can improve transfer learning on a variety of visual classiﬁcation tasks. The efﬁcacy of “resetting” itself depends on the size of the target dataset and the difference of the pre-training and target domains, allowing us to achieve state-of-the-art results on some datasets. 1

Introduction
Lifelong learning [1, 2] holds both the promise of beneﬁtting from past experiences and the challenge of having to continually adapt to new problem settings. The characteristics of this intriguing sequential learning problem make it a balancing act between retaining knowledge and adapting to new experiences referred to as the stability-plasticity dilemma [3]. Similar to a coach or teacher, we want to understand when a sequence of tasks helps or hinders further learning. Clearly, some pre-training tasks are highly beneﬁcial [4–7] while others can hurt performance [8, 9].
The existing literature on pre-training and curriculum learning sheds some light on the desired characteristics of sequential learning. In some cases, following a sequence of learning tasks can lead to better results than simply training on the target task from scratch [10]. Work by Achille and
Soatto [8] contrasts this picture by highlighting that neural networks can suffer from pre-training on some tasks. In an extreme case, when learning data is corrupted for a sufﬁciently long period, even switching back to clean data does not allow the neural network to recover its original performance.
Contribution: We formalize the basic ingredients of sequential task learning, introduce mutually frozen weights and establish a connection between plasticity and invariance of deep networks. To understand mutually frozen weights, we investigate their weight update dynamics, show that such frozen weights are different from regular sparse weights that are not connected to other sparse weights and that frozen weights can lead to a decrease in performance when retraining on a new task. Based on this analysis, we show that applying an intervention which “resets” frozen weights can improve retraining performance on a number of tasks as long as sufﬁciently many samples are available for retraining and even achieve state-of-the-art results on FashionMNIST image classiﬁcation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
“Frozen weights” impede retraining on future tasks
Pre-training can create “frozen weights” f2 f3
Model path (f0, . . . , fﬁnal) f0 f1 (⌧ 1,! 1) (⌧ 2,! 2) (⌧ 3,! 3)
Learning path
T
M
Figure 1: In the context of the negative pre-training effect, e.g. when pre-training on blurred images before training on unblurred images [8], we explore the effect of “frozen weights”, which can hinder retraining in a sequential learning setting, on the generalization performance of neural networks . A
), composed of a data point on the learning manifold distribution of input and labels and a loss function, and a learning process ! which determines how a model f is adapted given a task ⌧ . The learning path determines model path changes from initial to ﬁnal trained model fﬁnal. We focus on the effect of frozen weights and the plasticity-invariance trade-off that emerges when sequentially training a deep network with weight decay. is a supervised learning task ⌧ := (p(x, y),
M
L
T 2