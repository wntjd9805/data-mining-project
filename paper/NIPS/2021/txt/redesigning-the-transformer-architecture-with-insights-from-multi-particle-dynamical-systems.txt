Abstract
The Transformer and its variants have been proven to be efﬁcient sequence learners in many different domains. Despite their staggering success, a critical issue has been the enormous number of parameters that must be trained (ranging from 107 to 1011) along with the quadratic complexity of dot-product attention. In this work, we investigate the problem of approximating the two central components of the
Transformer — multi-head self-attention and point-wise feed-forward transforma-tion, with reduced parameter space and computational complexity. We build upon recent developments in analyzing deep neural networks as numerical solvers of ordinary differential equations. Taking advantage of an analogy between Trans-former stages and the evolution of a dynamical system of multiple interacting particles, we formulate a temporal evolution scheme, TransEvolve, to bypass costly dot-product attention over multiple stacked layers. We perform exhaus-tive experiments with TransEvolve on well-known encoder-decoder as well as encoder-only tasks. We observe that the degree of approximation (or inversely, the degree of parameter reduction) has different effects on the performance, de-pending on the task. While in the encoder-decoder regime, TransEvolve delivers performances comparable to the original Transformer, in encoder-only tasks it con-sistently outperforms Transformer along with several subsequent variants. Code is available in: https://github.com/LCS2-IIITD/TransEvolve. 1

Introduction
Neural networks have evolved from early feed-forward and convolutional networks, to recurrent networks, to very deep and wide ‘Transformer’ networks based on attention [Vaswani et al., 2017].
Transformers and their enhancements, such as BERT [Devlin et al., 2019], T5 [Raffel et al., 2020] and GPT [Brown et al., 2020] are, by now, the default choice in many language applications. Both their training data and model sizes are massive. BERT-base has 110 million parameters. BERT-large, which often leads to better task performance, has 345 million parameters. GPT-3 has 175 billion trained parameters. Larger BERT models already approach the limits of smaller GPUs. GPT-3 is outside the resource capabilities of most research groups. Training these gargantuan models is even more challenging, with signiﬁcant energy requirements and carbon emissions [Strubell et al., 2020].
In response, a growing community of researchers is focusing on post-facto reduction of model sizes, which can help with the deployment of pre-trained models in low-resource environments. However, training complexity is also critically important. A promising recent approach to faster training uses a 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
way of viewing layers of attention as solving ordinary differential equations (ODEs) deﬁned over a dynamical system of interacting particles [Lu et al., 2019, Vuckovic et al., 2020]. We pursue that line of work.
Simulating particle interactions over time has a correspondence to ‘executing’ successive layers of the
Transformer network. In the forward pass at successive layers, the self-attention and position-wise feed-forward operations of Transformer correspond to computing the new particle states from the previous ones. However, the numeric function learned by the i-th attention layer has zero knowledge regarding the one learned by the (i − 1)-th layer. This is counter-intuitive due to the fact that the whole evolution is temporal in nature, and this independence leads to growing numbers of trainable parameters and computing steps. We seek to develop time-evolution functionals from the initial condition alone. Such maps can then approximate the underlying ODE from parametric functions of time (the analog of network depth) and do not require computing self-attention over and over.
We propose such a scheme, leading to a network/method we call TransEvolve. It can be used for both encoder-decoder and encoder-only applications. We experiment on several tasks: neural machine translation, whole-sequence classiﬁcation, and long sequence analysis with different degrees of time-evolution. TransEvolve outperforms Transformer base model on WMT 2014 English-to-French translation by 1.4 BLEU score while using 10% fewer trainable parameters. On all the encoder-only tasks, TransEvolve outperforms Transformer, as well as several strong baselines, with 50% fewer trainable parameters and more than 3× training speedup. 2