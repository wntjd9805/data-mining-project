Abstract
Most of the existing neural video compression methods adopt the predictive coding framework, which ﬁrst generates the predicted frame and then encodes its residue with the current frame. However, as for compression ratio, predictive coding is only a sub-optimal solution as it uses simple subtraction operation to remove the redundancy across frames. In this paper, we propose a deep contextual video compression framework to enable a paradigm shift from predictive coding to conditional coding. In particular, we try to answer the following questions: how to deﬁne, use, and learn condition under a deep video compression framework. To tap the potential of conditional coding, we propose using feature domain context as condition. This enables us to leverage the high dimension context to carry rich information to both the encoder and the decoder, which helps reconstruct the high-frequency contents for higher video quality. Our framework is also extensible, in which the condition can be ﬂexibly designed. Experiments show that our method can signiﬁcantly outperform the previous state-of-the-art (SOTA) deep video compression methods. When compared with x265 using veryslow preset, we can achieve 26.0% bitrate saving for 1080P standard test videos. The codes are at https://github.com/DeepMC-DCVC/DCVC. 1

Introduction
From H.261 [1] developed in 1988 to the just released H.266 [2] in 2020, all traditional video coding standards are based on a predictive coding paradigm, where the predicted frame is ﬁrst generated by handcrafted modules and then the residue between the current frame and the predicted frame is encoded and decoded. Recently, many deep learning (DL)-based video compression methods [3–11] also adopt the predictive coding framework to encode the residue, where all handcrafted modules are merely replaced by neural networks.
Encoding residue is a simple yet efﬁcient manner for video compression, considering the strong temporal correlations among frames. However, residue coding is not optimal to encode the current frame xt given the predicted frame ˜xt, because it only uses handcrafted subtraction operation to remove the redundancy across frames. The entropy of residue coding is greater than or equal to that of conditional coding [12]: H(xt − ˜xt) ≥ H(xt|˜xt), where H represents the Shannon entropy.
Theoretically, one pixel in frame xt correlates to all the pixels in the previous decoded frames and the pixels already been decoded in xt. For traditional video codec, it is impossible to use the handcrafted rules to explicitly explore the correlation by taking all of them into consideration due to the huge space.
Thus, residue coding is widely adopted as a special extremely simpliﬁed case of conditional coding, with the very strong assumption that the current pixel only has the correlation with the predicted pixel.
DL opens the door to automatically explore correlations in a huge space. Considering the success of
DL in image compression [13, 14], which just uses autoencoder to explore correlation in image, why not use network to build the conditional coding-based autoencoder to explore correlation in video rather than restricting our vision into residue coding? 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Paradigm shift from residue coding-based framework to conditional coding-based frame-work. xt is the current frame. ˆxt and ˆxt−1 are the current and previous decoded frames. The orange dashed line means that the context is also used for entropy modeling.
When we design the conditional coding-based solution, a series of questions naturally come up:
What is condition? How to use condition? And how to learn condition? Technically speaking, condition can be anything that may be helpful to compress the current frame. The predicted frame can be used as condition but it is not necessary to restrict it as the only representation of condition.
Thus, we deﬁne the condition as learnable contextual features with arbitrary dimensions. Along this idea, we propose a deep contextual video compression (DCVC) framework to utilize condition in a uniﬁed, simple, yet efﬁcient approach. The diagram of our DCVC framework is shown in Fig. 1.
The contextual information is used as part of the input of contextual encoder, contextual decoder, as well as the entropy model. In particular, beneﬁting from the temporal prior provided by context, the entropy model itself is temporally adaptive, resulting in a richer and more accurate model. As for how to learn condition, we propose using motion estimation and motion compensation (MEMC) at feature domain. The MEMC can guide the model where to extract useful context. Experimental results demonstrate the effectiveness of the proposed DCVC. For 1080p standard test videos, our
DCVC can achieve 26.0% bitrate saving over x265 using veryslow preset, and 16.4% bitrate saving over previous SOTA DL-based model DVCPro [4].
Actually, the concept of conditional coding has appeared in [15, 16, 12, 17]. However, these works are only designed for partial module (e.g., only entropy model or encoder) or need handcrafted operations to ﬁlter which content should be conditionally coded. By contrast, our framework is a more comprehensive solution which considers all of encoding, decoding, and entropy modeling.
In addition, the proposed DCVC is an extensible conditional coding-based framework, where the condition can be ﬂexibly designed. Although this paper proposes using feature domain MEMC to generate contextual features and demonstrates its effectiveness, we still think it is an open question worth further investigation for higher compression ratio.
Our main contributions are four-folded:
• We design a deep contextual video compression framework based on conditional coding.
The deﬁnition, usage, and learning manner of condition are all innovative. Our method can achieve higher compression ratio than previous residue coding-based methods.
• We propose a simple yet efﬁcient approach using context to help the encoding, decoding, as well as the entropy modeling. For entropy modeling, we design a model which utilizes spatial-temporal correlation for higher compression ratio or only utilizes temporal correlation for fast speed.
• We deﬁne the condition as the context in feature domain. The context with higher dimensions can provide richer information to help reconstruct the high frequency contents.
• Our framework is extensible. There exists great potential in boosting compression ratio by better deﬁning, using, and learning the condition. 2
2