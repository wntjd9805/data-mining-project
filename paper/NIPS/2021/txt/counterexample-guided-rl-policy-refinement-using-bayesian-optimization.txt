Abstract
Constructing Reinforcement Learning (RL) policies that adhere to safety require-ments is an emerging ﬁeld of study. RL agents learn via trial and error with an objective to optimize a reward signal. Often policies that are designed to accumu-late rewards do not satisfy safety speciﬁcations. We present a methodology for counterexample guided reﬁnement of a trained RL policy against a given safety speciﬁcation. Our approach has two main components. The ﬁrst component is an approach to discover failure trajectories using Bayesian Optimization over multiple parameters of uncertainty from a policy learnt in a model-free setting. The second component selectively modiﬁes the failure points of the policy using gradient-based updates. The approach has been tested on several RL environments, and we demon-strate that the policy can be made to respect the safety speciﬁcations through such targeted changes. 1

Introduction
Classical Reinforcement Learning (RL) is designed to perceive the environment and act towards maximizing a long term reward. These algorithms have shown superhuman performances in games like GO [10], motivating their use in safety-critical domains like autonomous driving [23, 20, 16].
These policies are mostly learnt in simulation environments such as TORCS [22] or CARLA [14] for accelerated learning. Therefore they are susceptible to perform poorly in the real world where domain uncertainty arises due to noisy data. Also, the choice of a RL policy is determined by the design of the reward signal, which if not constructed properly, may lead the agent towards unintended or even harmful behaviour. Hence the need for constructing policies that account for risks arising due to uncertainty and do not violate safety speciﬁcations.
Safe RL aims to learn policies that maximize the expectation of the return while respecting safety speciﬁcations [17]. The existing literature on safe RL is broadly divided into three schools of thought namely, 1) Transforming the optimization criteria by factoring in costs derived from safety constraints [1], 2) Using external knowledge in the form of a teacher or a safety shield to replace unsafe actions chosen by the policy during exploration / deployment [2], and 3) Establishing the existence of an inductive safety invariant such as a Lyapunov function [11] or a control barrier function [25]. Establishing such an invariant is often intractable for Neural Network controllers that use many parameters and work on high-dimensional state spaces.
In this work, we propose a counterexample guided policy reﬁnement method for verifying and correcting a policy that has already been optimized and calibrated. Failure traces may occur in such a policy due to the following reasons: a) Failure trajectories are extremely rare and do not contribute signiﬁcantly enough to reduce the expected reward, b) Uncertainty in the environment
∗Code is available online at https://github.com/britig/policy-reﬁnement-bo 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
which had not been accounted for during policy training. Our aim is to progressively make the system safer by ﬁnding and eliminating failure traces with targeted policy updates. Counterexample guided reﬁnement techniques have been well studied in the formal methods community for program veriﬁcation [9] and reachability analysis of hybrid systems [3], but not so far in the context of RL.
We study the proposed methodology on environments that work over continuous space and continuous or discrete actions. The proposed methodology is divided into two steps: 1. Given a policy πold, learnt for optimizing reward in a given environment, we test it against parameters with uncertainties and a set of objective functions ϕ derived from the negation of the given safety criteria. Multiple counter-example traces or failure trajectories are uncovered from πold using Bayesian Optimization. 2. Using the failure trajectories we selectively do a gradient update on πold to construct a new policy πnew, that excludes the counterexample traces under the given domain uncertainties.
We speciﬁcally work with a policy gradient RL algorithm, Proximal Policy Optimization (PPO), which directly optimizes the learnt policy using gradient updates.
These steps may be repeated multiple times. We show that such targeted updates ensure that πnew is safer than πold, but not signiﬁcantly different in terms of reward.
The paper is organized as follows. Section 2 outlines related work, Section 3 presents the methodology to uncover failure trajectories, Section 4 presents the methodology of policy reﬁnement, Section 5 presents case studies on several RL environments, and Section 6 provides concluding remarks. 2