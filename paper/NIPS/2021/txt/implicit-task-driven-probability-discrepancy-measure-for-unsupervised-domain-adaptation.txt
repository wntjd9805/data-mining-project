Abstract
Probability discrepancy measure is a fundamental construct for numerous machine learning models such as weakly supervised learning and generative modeling.
However, most measures overlook the fact that the distributions are not the end-product of learning, but are the input of a downstream predictor. Therefore, it is important to warp the probability discrepancy measure towards the end tasks, and towards this goal, we propose a new bi-level optimization based approach so that the two distributions are compared not uniformly against the entire hypothesis space, but only with respect to the optimal predictor for the downstream end task. When applied to margin disparity discrepancy and contrastive domain discrepancy, our method signiﬁcantly improves the performance in unsupervised domain adaptation, and enjoys a much more principled training process. 1

Introduction
Discrepancy measures on two distributions underpin a large variety of machine learning tasks, and have been studied extensively since the dawn of modern probability [1]. For example, in generative models, such a measure is applied to align the generated distribution with the empirical one, and prevalent examples include 1) the f -divergence that admits a convenient variational form hence can be effectively evaluated via sample-based adversarial optimization [2, 3]; 2) integral probability metric [IPM, 4] that seeks the largest discrepancy in function expectation over a reproducing kernel
Hilbert space (RKHS) [MMD GAN, 5–7], 1-Lipschitz continuous functions [Wasserstein GAN, 8, 9], or unit L2 norm functions [Fisher GAN, 10], etc.
In domain adaptation [DA, 11, 12], probability discrepancy is also the key construct in the feature adaptation approach, where a feature extractor φ is sought to align the source and target distributions transformed by φ [13, 14]. The aforementioned measures can be applied directly in this context.
It has been long noted that the discrepancy should be tailored to the function class of interest, e.g., those for which we would like to compute expectations. This principle has been applied to density estimation [15] amongst others, where the RKHS is selected to match the downstream task such as image categorization based on the compressed pixel distribution. Naturally this motivation can be easily implemented in IPMs by customizing the generating function space.
However such tailoring remains oblivious to the loss and available labels of the end task. Intuitively, if the latent features in DA are to be used for classiﬁcation, then whether the loss is AUC or F-score should ideally inﬂuence the probability discrepancy. The seminal H∆H-divergence is designed for classiﬁcation accuracy [16], with a few extensions to Bayesian and other losses [17–20]. Despite being data-dependent, however, they are unsupervised without accounting for the available labels.
Likewise, if a generative model is used to augment data so as to improve segmentation accuracy
[21], then the adversarial network in GANs should not only be able to distinguish between real and synthetic, but also “align", in an appropriate sense, with the segmentation labels at hand. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Warping probability discrepancies towards a task has been lightly touched in unsupervised DA (UDA). [22] trains two classiﬁers that not only boost the source-domain accuracy, but also maximally disagree on the target domain. Unfortunately, it is only formulated as a procedure, not a probability discrepancy. Most relevant to our work is the margin disparity discrepancy [MDD, 23], which is based on the H∆H-divergence where two ﬁctitious classiﬁers h and h(cid:48) are jointly optimized to maximally reveal the two distributions’ difference. [23] took the key insight that h can be tied with the source-domain predictor, and can thus be optimized to simultaneously reduce the source-domain risk and the H∆H-divergence. However, in spite of its effectiveness in both theory and practice, we discover that the speciﬁc formulation conﬂicts with the H∆H-divergence — the latter tries to maximize over h so as to promote the divergence, while MDD tries to minimize it (Section 3). This undermines the power of MDD in distinguishing two distributions as illustrated in Figure 1. Flipping the sign and min/max cannot resolve the issue.
Our ﬁrst contribution, therefore, is to develop a new task-driven discrepancy framework that over-comes this obstacle. The key inspiration is that MDD relies on the pseudo-label in the target domain (i.e., speculation of their labels based, e.g., on the source-domain head), and this is also the case for some other measures such as the contrastive domain discrepancy [CDD, 24], which promotes the proximity between the class mean of the two domains for each class, and pushes apart the mean of different classes. Such a commonality motivates us to generate the target-domain pseudo-label based on the optimal source-domain classiﬁer h∗. In MDD, this provides a natural substitute for the
ﬁctitious classiﬁer h (Section 3.2) which no longer needs to be optimized over, thereby solving the aforementioned problem. As our second contribution, we extend this strategy to CDD in Section 4.
The overall formulation becomes a bi-level optimization solvable by implicit differentiation (hence the modiﬁer “implicit” in the method’s name).
We note in passing that pseudo-label is commonly used in self-training for UDA [25–28]. However, most methods require various reﬁnements of it in order to mitigate its inaccuracy due to distributional shift [29]. Examples include label sharpening [30], entropy reweighting [31], cycle training [29]. We instead directly use the output of f ∗ as the pseudo-label in probability discrepancy, outperforming state of the art on a range of datasets (Section 6).
UDA has recently received considerable interest, and most algorithms rely on ad-hoc heuristics; we will mention a few below. Many of them require perusing the code and conﬁguration script.
As such, our main goal is not to develop yet another highly engineered model that performs better, but to present a principled formulation solvable by off-the-shelf optimizers. Although our implicit task-driven discrepancy can be straightforwardly applied to generative models, we deem it a better use of space to fully demonstrate its power in UDA. Such a probability discrepancy can also be easily extended to measure (conditional) independence, which has witnessed immediate application in fair and disentangled representation learning [32–35]. 2 Preliminaries
In UDA, there is a source domain and a target domain, and they are respectively represented as a joint distribution S and T on an input-output space X × Y. We will denote their marginal distributions via subscripts, e.g., Sx and Ty. The Y domain can be multiclass with labels [C] := {1, 2, . . . , C}. We are provided with labeled examples in the source domain, denoted as an empirical distribution ˜S. On the target domain, however, we can only access unlabeled examples, i.e., an empirical distribution
˜Tx which only encompasses the input part of an empirical distribution ˜T . In short, let the empirical distributions consist of {xs j=1 for the source and target domains respectively. i=1 and {xt i }ns j}nt i , ys
The goal of UDA is to ﬁnd a classiﬁer that predicts well on the target domain T . This is often referred to as inductive learning, while, in contrast, transductive learning is only concerned with the prediction on the empirical distribution ˜T , whose feature component ˜Tx is available at training time.
The classiﬁcation model, shared by both domains, consists of a feature extractor (e.g., ResNet) parameterized by φ and a head hθ parameterized by θ. Letting (cid:96) be the loss over the ground-truth label y and the prediction hθ(φ(x)), we seek the φ and θ that minimize the target-domain risk
E(x,y)∼T (cid:96)(y, hθ(φ(x))), or its empirical counterpart E (x,y)∼ ˜T (cid:96)(y, hθ(φ(x))). (1)
In order to leverage the labeled data from the source domain and the unlabeled target-domain data, the feature adaption approach enforces low empirical risk on the source domain (thanks to the availability 2
of labels there) and encourages that the source domain distribution, after being transformed by the feature extractor φ, “aligns” well with that of the target domain [13, 14, 36, 37]. This is achieved by min
φ,θ (x,y)∼ ˜S (cid:96)(y, hθ(φ(x))) + d(φ# ˜Sx, φ# ˜Tx),
E (2) where φ# ˜Sx is the pushforward distribution of ˜Sx, and d denotes some discrepancy measure between two distributions. The intuition is that by “mixing” the latent distributions across the two domains through φ, the favorable accuracy of hθ on the source domain can be transferred to the target domain.
For simplicity, we will denote P := φ#Sx and ˜P := φ# ˜Sx, and explicitize its dependency on φ by writing Pφ whenever necessary. With z = φ(x), we can derive a conditional distribution of y given z based on S, and we denote it as Sy|z. Similarly, let Q := φ#Tx, ˜Q := φ# ˜Tx, and deﬁne
Ty|z analogously. 3
Implicit Task-Driven Margin Disparity Discrepancy
There has been a plethora of research on sample-based discrepancy measure between two distributions.
Examples include maximum mean discrepancy [MMD, 38], and (neural) variational optimization
[39] which effectively subsumes a number of adversarial learning based measures [2, 14].
However, these methods are oblivious to the subsequent tasks that are based on P and Q. For example, UDA can be aimed to classify well on these distributions. In domain-adversarial neural networks [DANN, 14], the discrepancy between P and Q is measured via the Jensen-Shannon divergence, reformulated in an adversarial objective as in the generative adversarial network [GAN, 40]. Moreover, MMD simply measures the largest possible difference in the function expectation over P and Q:
MMD(P, Q) := (cid:104) sup f ∈H:(cid:107)f (cid:107)H≤1
E x∼P f (x) − E x∼Q (cid:105) f (x)
= (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
E x∼P k(x, ·) − E x∼Q k(x, ·) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)H
, (3) where H is the reproducing kernel Hilbert space (RKHS) induced by a kernel k. Obviously, it does not take into account whether f is used for classiﬁcation or regression. The celebrated H∆H-divergence addresses this problem by focusing on binary classiﬁcation [16, Lemma 3]: dH∆H(P, Q) := max h∈H max h(cid:48)∈H
D(h, h(cid:48), P, Q), (4) where D(h, h(cid:48), P, Q) := |EP [[sign ◦h(cid:48) (cid:54)= sign ◦h]] − EQ[[sign ◦h(cid:48) (cid:54)= sign ◦h]]| .
Here sign ◦h applies the sign function on the output of h. H is a hypothesis space (not necessarily an
RKHS), and [[·]] is the Iverson bracket that evaluates to 1 if · is true, and 0 otherwise. However, it still does not concern the label of the data (i.e., align only in an unsupervised fashion). To warp the measure to the end-task in a data-dependent fashion, [23] proposed the margin disparity discrepancy (MDD), which improved upon [22] by formulating a principled objective function instead of a heuristic procedure. According to Equation 24 of [23], MDD employs (cid:27) (5) (cid:26) dMDD(P, Q) = min h∈H
R(h; P ) + max h(cid:48)∈H
D(h, h(cid:48), P, Q)
, where R(h; P ) := Ez∼P, y|z∼Sy|z (cid:96)(h(z), y) + reg(h) is the regularized risk, (6) (7) and the 0-1 loss in D can be replaced by smooth surrogates such as hinge loss or cross-entropy loss.
Here reg is any standard regularizer applied in regularized risk minimization, e.g., (cid:96)2 norm. The underlying insight is that when comparing P and Q, one only needs to consider those h that predict well on the (labeled) source domain, while leaving h(cid:48) to reveal the maximum discrepancy between P and Q. Similar ideas have been leveraged in [22, 41]. 3.1 Conﬂict between MDD and H∆H-divergence
Unfortunately, dMDD turns out conﬂicting with the spirit of H∆H-divergence in an important way.
Note that h is maximized in D as in (4), while it is minimized in dMDD as in (6). This raises a natural question: can the distribution discrepancy be sufﬁciently revealed when maxh is replaced by minh in the deﬁnition of D, i.e., d min
H∆H(P, Q) := min h∈H max h(cid:48)∈H
D(h, h(cid:48), P, Q). (8) 3
Figure 1: An example showing that changing maxh into minh undercuts the power of discriminating two distributions. Here the source distribution P has two blue clusters, and the target distribution Q consists of two red clusters. The location of h in (a) makes maxh(cid:48)∈H D(h, h(cid:48), P, Q) = 0, meaning that the new discrepancy d min
H∆H(P, Q) cannot distinguish the two distributions. In contrast, the h in (b) makes maxh(cid:48)∈H D(h, h(cid:48), P, Q) = 1, implying that the original dH∆H(P, Q) can distinguish.
It turns out such a change does undermine the discriminative power, and an example is illustrated in
Figure 1. Here both the source and target domains have two separate clusters, and the hypothesis space is the horizontal or vertical half spaces (i.e., decision stumps). Sub-ﬁgure (a) shows that the
H∆H is attained at the horizontal line, and it is easy to check that no matter where h(cid:48) minimum h in d min is placed, D(h, h(cid:48), P, Q) = 0. In contrast, the h and h(cid:48) shown in (b) attain D(h, h(cid:48), P, Q) = 1. So changing maximization of h into minimization caused signiﬁcant loss in the discrimination power. A more detailed discussion in conjunction with R as in (6) is available in Appendix A. 3.2 A new implicit task-driven MDD
Flipping back the optimization of h turns out far more involved that it appears. It cannot be achieved by simply changing minh into maxh in (6) with the source domain risk negated: (cid:110) max h∈H
− R(h; P ) + max h(cid:48)∈H
D(h, h(cid:48), P, Q) (cid:111)
, (9)
This is because P and Q indeed depend on the feature extractor φ. If we next minimize dMDD(P, Q) over φ, then it implicitly promotes the source domain risk. If dMDD(P, Q) is instead maximized over
φ, then φ would attempt to increase the discrepancy D. With a few trials, it becomes clear that the same issue persists in other combinations of ﬂipping sign or min/max.
Our ﬁrst contribution, hence, is to resolve this issue by turning dMDD into a constrained formulation: max (10) h(cid:48)∈H max h∈H:R(h;P )≤λ
D(h, h(cid:48), P, Q), where λ is some pre-speciﬁed cap of loss. Constraining the performance of a classiﬁer is quite commonly used in, e.g., gradient episodic memory to combat catastrophic forgetting [GEM, 42, 43].
However, GEM only solves a linear approximation instead of the exact problem, and it is arguably difﬁcult to differentiate through for optimizing φ in (10). Therefore, we ﬁnally develop a bi-level optimization by setting h to the optimal one for the source domain, and then using it in the discrepancy measure. We call it i-MDD because it will rely on implicit differentiation for training. The overall training objective can be written as:1 min
φ di-MDD( ˜Pφ, ˜Qφ) + αR(h∗; ˜Pφ) where di-MDD( ˜Pφ, ˜Qφ) := max h(cid:48)∈H
D(h∗, h(cid:48), ˜Pφ, ˜Qφ), (11) h∗ := arg min h∈H
R(h; ˜Pφ). (12)
Here α > 0 is a tradeoff parameter. If we do not include R(h; ˜Pφ) in the objective, then the feature φ would receive no incentive to reduce the source-domain risk. This term in the objective function does not necessitate new implicit differentiation, because h∗ is exactly the minimizer of R(h; ˜Pφ). The architecture of i-MDD is shown in Figure 2, in comparison with MDD. 1One might wonder why the max over h in (10) is turned into min over h in (12). This is because λ is set to minh R(h; P ). Analogously, maximizing f (x, y) over (x − 1)2 ≤ 0 is equivalent to evaluating f (1, y), because 0 is the minimum of (x − 1)2 attained at x = 1. Or to bear more resemblance to (11) and (12), it is equal to f (x∗, y) where x∗ = argminx (x − 1)2. 4
Figure 2: Illustration of i-MDD and MDD. The h∗, fed into di-MDD in i-MDD, is the minimizer of R. 3.3 Practical discussions: differentiable surrogates
Since the 0-1 loss in D is not amenable to differentiable training, we follow [23] to morph it into the cross-entropy loss (CE). In particular, suppose h outputs a C dimensional logit vector, and p = softmax(h). Similarly, p(cid:48) = softmax(h(cid:48)). Then the standard CE(p(cid:48), p) = − (cid:80) i ≥ 0.
To combat exploding or vanishing gradient, [3] proposed a modiﬁed CE: MCE(p(cid:48), p) = (cid:80) i pi log(1− p(cid:48) i) ≤ 0. Then [23] adopts the approximation
D(h, h(cid:48), ˜Pφ, ˜Qφ) ≈ E ˜Qφ
MCE(p(cid:48), ind ◦ p) − γE ˜Pφ
CE(p(cid:48), ind ◦ p), i pi log p(cid:48) (γ > 0) (13) where ind : RC → {0, 1}C is the indicator function mapping a vector v to the i∗-th canonical vector with i∗ = arg maxi vi. In practice, the formulation has two issues. First, the right-hand side of (13) is unbounded from below, making it possible for φ (the minimizing variable) to push it to the negative inﬁnity when solved by stochastic saddle-point optimization. As a result, the implementation of
[23] tuned the step size delicately. Secondly, the indicator function ind blocks the backpropagation through the branch of h, jeopardizing the proper optimization. We tried removing the indicator function but observed negative inﬁnity even after ﬁnely tuning the step size.
In contrast, our new i-MDD is immune to these issues, where (13) is used without including the indicator function. In our experiment, we observed that the head hθ only needs to be linear in order to achieve state-of-the-art performance. This provided considerable convenience because the optimization for h∗ in (12) can be accomplished very efﬁciently with high precision by convex solvers such as LIBLINEAR [44]. Similarly, it is clear that h∗ does not depend on h(cid:48), but on ˜Pφ only (i.e.,
φ). Therefore, we can ﬁrst solve h∗ in (12), and then ﬁx it when solving h(cid:48) in (11), which results in another convex problem thanks to the linearity of h(cid:48). These conveniences signiﬁcantly beneﬁt computation and convergence properties.
Although MDD can forgo the stochastic saddle-point optimization and also evaluate dMDD exactly, the inner joint maximization over h and h(cid:48) leads to a non-concave function, hence impairing the precision of backpropagation. Even if the indicator function is imposed and optimization is only over h(cid:48), we found a linear h(cid:48) was insufﬁcient to deliver accurate predictions. 3.4 Bi-level optimization
Bi-level optimization has recently received intensive study [45–48], and they can be easily applied to i-MDD. Thanks to the linearity of h and h(cid:48), the backpropagation can be performed in a closed form. Denote the ultimate objective value in (11) as J. Letting zs j), we only need to derive new strategies to compute ∂J/∂zs j, based on which backpropagation through the feature extractor will be standard. Towards this end, most of the implicit differentiation approaches rely on multiplying a given vector to the Hessian of the loss (cid:96) in (12) with respect to h
[45]. Interestingly, for linear multi-class classiﬁers with cross-entropy loss, the formula has already been derived by [49, Appendix D], and we quote their results in Appendix B for completeness, along with the detailed analysis of computational complexity. i and ∂J/∂zt i = φ(xs j = φ(xt i ) and zt
To summarize, the crux of i-MDD is to replace the h in the H∆H-divergence by the optimal source domain classiﬁer h∗ under the current φ. This is in line with the pseudo-label approach and h∗ can be applied to the target domain to provide a soft label. Indeed this principle can be applied to other class-aware discrepancy measures, and our next contribution is to warp the contrastive domain discrepancy [CDD, 24] towards the end task. 5
4 Task-driven Contrastive Domain Discrepancy
Underpinning CDD is the hard pseudo-label ˆyt
[24] adopted clustering on zt the mean of the source domain zs
Then the discrepancy between ˜P and ˜Q is deﬁned as (distilled from Equations 3 and 4 in [24]) (cid:13) 2 (cid:13)
H j ∈ [C] assigned to each target domain example zt j. j, where each class corresponds to a cluster, and its center is initialized by j belongs to at convergence. j is set to the cluster that zt dCDD( ˜P , ˜Q) = 1
C i . Naturally, ˆyt 1
C(C−1) c − µt c(cid:48) c − µt c (cid:13) (cid:13)µs (cid:13) (cid:13)µs (cid:13) 2 (cid:13)
H
− β · (cid:88) (cid:88)
, c∈[C] (cid:124) (cid:123)(cid:122) intra-class discrepancy (cid:125) c(cid:54)=c(cid:48) (cid:124) (cid:123)(cid:122) inter-class discrepancy (cid:125) c := mean{k(zs where µs c := mean{k(zt
µt i , ·) : i ∈ [ns] and ys j, ·) : j ∈ [nt] and ˆyt i = c}, j = c},
∀ c ∈ [C]
∀ c ∈ [C]. (14) (15) (16)
Here β > 0 is a tradeoff coefﬁent. The underlying motivation is to align the class-wise center between source and target domains (the intra-class discrepancy), and push apart the centers of different classes (the inter-class discrepancy). Although the source-domain label is used to initialize clustering, the prediction head h is not involved in dCDD, hence not sufﬁciently driven by the end task.
In addition, a number of heuristics are required for CDD to perform well. Firstly, after clustering, only the target-domain examples that are close to the center are included to compute the mean
µt c. This introduces one hyperparameter to tune. Secondly, domain speciﬁc batch-normalization is required. Finally, the bandwidth of the RBF kernel needs to be learned for each pair of (c, c(cid:48)) in the implementation. To remove all these nuisances and formulate a principled optimization, we next warp CDD towards tasks based on bi-level optimization. 4.1
Implicit task-driven CDD
Our key insight is that the head h∗ in (12) constitutes a natural source of pseudo-label that is superior to clustering. Firstly, h∗ is uniquely determined thanks to the convexity originating from the linearity of h. Moreover, clustering is a “procedure" which is not amenable to differentiation despite some recent progress in reversible learning [46]. In contrast, differentiation through h∗ is straightforward as discussed above.
This intuition can be directly implemented by redeﬁning the class centers in the target domain based on the h∗-induced soft pseudo-label for each example zt j) produces the C-dimensional logit (unnormalized score) for the C classes, and the softmax of it yields a C-dimensional probability vector pt j, whose c-th element encodes the probability of belonging to class c. Accordingly, we can morph the target-domain center µt nt(cid:88) j. Recall h∗(zt c into nt(cid:88) (cid:44)(cid:32) (cid:33) (pt j)c
, where pt j = softmax(h(zt j)) ∈ RC.
µt c(h) := (pt j)c zt j 10−6 + (17) j=1 j=1
Note the kernel k is removed and we directly used zt in case all examples are unlikely to belong to class c. To summarize, our training objective is j. We also added a small smoothing factor 10−6 di-CDD( ˜Pφ, ˜Qφ) + αR(h∗; ˜Pφ) min
φ (18) where di-CDD( ˜Pφ, ˜Qφ) := 1
C (cid:88) (cid:13) (cid:13)µs c − µt c(h∗)(cid:13) 2
H − β (cid:13) 1
C(C − 1) (cid:88) c(cid:54)=c(cid:48) (cid:107)µs c − µs c(cid:48)(cid:107)2
H (19)
R(h; ˜Pφ). (20) c∈[C] h∗ := arg min h∈H c − µt c(cid:48)) into within source domain only (µs
It is clearly identical to i-MDD in (11) except that the di-MDD is replaced by di-CDD. Compared with dCDD in (14), we slightly changed the inter-class term from between source and target domains (µs c − µs c(cid:48)). This simpliﬁes optimization because the centers of the source domain do not depend on h∗. Meanwhile, different classes are still pushed apart in both domains because 1) it is enforced on the source domain, and 2) the source domain centers µs c are aligned with those of the target domain µt c(h∗). Backpropagation and bi-level optimization are similar to i-MDD, with even reduced complexity as no optimization (over h(cid:48)) is involved in di-CDD. 6
4.2 Cache-augmented training
It was noted in [24] that the limited size of mini-batch may leave only a small number of examples for each class (or even none), especially when there are many classes. This hampers the computation of class means. They thus resorted to a class-aware sampling strategy where only a subset of classes are picked at each iteration, and samples are drawn only for these classes. This again relies on the result of clustering for the target domain, exacerbating the fallout of not backpropagating through it.
To address this issue, we followed [50, 51] by caching the latent representations z in the most recent iterations via a circular queue for each class. This allows the class means to be computed more accurately, and the backpropagation is still conducted only on the current mini-batch examples.
We emphasize that our overall optimization remains principled even with cache augmentation, an observation that has not been made in literature to the best of our knowledge. Since φ is updated with a small step size and only a small number of latest iterations are cached, the continuity of the algorithm ensures that the z computed from a stale φ is still close to the value if it were computed with the latest φ. As a result, the bias of the gradient can be bounded linearly by the step size times the staleness (i.e., the length of the queue / mini-batch size). We relegate the details to Appendix C. 5