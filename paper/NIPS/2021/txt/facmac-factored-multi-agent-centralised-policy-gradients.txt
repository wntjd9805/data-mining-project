Abstract
We propose FACtored Multi-Agent Centralised policy gradients (FACMAC), a new method for cooperative multi-agent reinforcement learning in both discrete and con-tinuous action spaces. Like MADDPG, a popular multi-agent actor-critic method, our approach uses deep deterministic policy gradients to learn policies. However,
FACMAC learns a centralised but factored critic, which combines per-agent util-ities into the joint action-value function via a non-linear monotonic function, as in QMIX, a popular multi-agent Q-learning algorithm. However, unlike QMIX, there are no inherent constraints on factoring the critic. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased rep-resentational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics. In addition, FACMAC uses a cen-tralised policy gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG.
This allows for more coordinated policy changes and fully reaps the beneﬁts of a centralised critic. We evaluate FACMAC on variants of the multi-agent particle environments, a novel multi-agent MuJoCo benchmark, and a challenging set of
StarCraft II micromanagement tasks. Empirical results demonstrate FACMAC’s superior performance over MADDPG and other baselines on all three domains. 1

Introduction
Signiﬁcant progress has been made in cooperative multi-agent reinforcement learning (MARL) under the paradigm of centralised training with decentralised execution (CTDE) [27, 18] in recent years, both in value-based [41, 31, 37, 32, 44, 30] and actor-critic [23, 9, 36, 13, 8] approaches. Most popular multi-agent actor-critic methods such as COMA [9] and MADDPG [23] learn a centralised critic with decentralised actors. The critic is centralised to make use of all available information (i.e., it can condition on the global state and the joint action) to estimate the joint action-value function
Qtot, unlike a decentralised critic that estimates the local action-value function Qa based only on individual observations and actions for each agent a.3 Even though the joint action-value function
∗Equal contribution. Correspondence to: Bei Peng <bei.peng@liverpool.ac.uk>
†Work done while the authors were at the University of Oxford. 3COMA learns a single centralised critic for all cooperative agents due to parameter sharing. For each agent the critic has different inputs and can thus output different values for the same state and joint action. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
these actor-critic methods can represent is not restricted, in practice they signiﬁcantly underperform value-based methods like QMIX [31] on the challenging StarCraft Multi-Agent Challenge (SMAC)
[35] benchmark [33, 32].
In this paper, we propose a novel approach called FACtored Multi-Agent Centralised policy gradients (FACMAC), which works for both discrete and continuous cooperative multi-agent tasks. Like
MADDPG, our approach uses deep deterministic policy gradients [20] to learn decentralised policies.
However, FACMAC learns a single centralised but factored critic, which factors the joint action-value function Qtot into per-agent utilities Qa that are combined via a non-linear monotonic function, as in the popular Q-learning algorithm QMIX [31]. While the critic used in COMA and MADDPG is also centralised, it is monolithic rather than factored.4 Compared to learning a monolithic critic, our factored critic can potentially scale better to tasks with a larger number of agents and/or actions. In addition, in contrast to other value-based approaches such as QMIX, there are no inherent constraints on factoring the critic. This allows us to employ rich value factorisations, including nonmonotonic ones, that value-based methods cannot directly use without forfeiting decentralisability or introducing other signiﬁcant algorithmic changes. We thus also employ a nonmonotonic factorisation and empirically demonstrate that its increased representational capacity allows it to solve some tasks that cannot be solved with monolithic, or monotonically factored critics.
In MADDPG, a separate policy gradient is derived for each agent individually, which optimises its policy assuming all other agents’ actions are ﬁxed. This could cause the agents to converge to sub-optimal policies in which no single agent wishes to change its action unilaterally. In FACMAC, we use a new centralised gradient estimator that optimises over the entire joint action space, rather than optimising over each agent’s action space separately as in MADDPG. The agents’ policies are thus trained as a single joint-action policy, which can enable learning of more coordinated behaviour, as well as the ability to escape sub-optimal solutions. The centralised gradient estimator fully reaps the beneﬁts of learning a centralised critic, by not implicitly marginalising over the actions of the other agents in the policy-gradient update. The gradient estimator used in MADDPG is also known to be vulnerable to relative overgeneralisation [47]. To overcome this issue, in our centralised gradient estimator, we sample all actions from all agents’ current policies when evaluating the joint action-value function. We empirically show that MADDPG can quickly get stuck in local optima in a simple continuous matrix game, whereas our centralised gradient estimator ﬁnds the optimal policy.
While Lyu et al. [24] recently show that merely using a centralised critic (with per-agent gradients that optimise over each agent’s actions separately) does not necessarily lead to better coordination between agents, our centralised gradient estimator re-establishes the value of using centralised critics.
Most recent works on continuous MARL focus on evaluating their algorithms on the multi-agent particle environments [23], which feature a simple two-dimensional world with some basic simulated physics. To demonstrate FACMAC’s scalability to more complex continuous domains and to stimulate more progress in continuous MARL, we introduce Multi-Agent MuJoCo (MAMuJoCo), a new, comprehensive benchmark suite that allows the study of decentralised continuous control. Based on the popular single-agent MuJoCo benchmark [5], MAMuJoCo features a wide variety of novel robotic control tasks in which multiple agents within a single robot have to solve a task cooperatively.
We evaluate FACMAC on variants of the multi-agent particle environments [23] and our novel
MAMuJoCo benchmark, which both feature continuous action spaces, and the challenging SMAC benchmark [35], which features discrete action spaces. Empirical results demonstrate FACMAC’s superior performance over MADDPG and other baselines on all three domains.
In particular,
FACMAC scales better when the number of agents (and/or actions) and the complexity of the task increases. Results on SMAC show that FACMAC signiﬁcantly outperforms stochastic DOP [46], which recently claimed to be the ﬁrst multi-agent actor-critic method to outperform state-of-the-art valued-based methods on SMAC, in all scenarios we tested. Moreover, our ablations and additional experiments demonstrate the advantages of both factoring the critic and using our centralised gradient estimator. We show that, compared to learning a monolithic critic, learning a factored critic can: 1) better take advantage of the centralised gradient estimator to optimise the agent policies when the
In MADDPG, each agent learns its own centralised critic, as it is designed for general multi-agent learning problems, including cooperative, competitive, and mixed settings. 4We use “centralised and monolithic critic” and “monolithic critic” interchangeably to refer to the centralised critic used in COMA and MADDPG, and “centralised but factored critic” and “factored critic” interchangeably to refer to the critic used in our approach. 2
number of agents and/or actions is large, and 2) leverage a nonmonotonic factorisation to solve tasks that cannot be solved with monolithic or monotonically factored critics. 2