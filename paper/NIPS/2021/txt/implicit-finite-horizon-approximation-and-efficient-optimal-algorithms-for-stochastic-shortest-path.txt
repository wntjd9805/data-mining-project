Abstract
We introduce a generic template for developing regret minimization algorithms in the Stochastic Shortest Path (SSP) model, which achieves minimax optimal regret as long as certain properties are ensured. The key of our analysis is a new technique called implicit ﬁnite-horizon approximation, which approximates the SSP model by a ﬁnite-horizon counterpart only in the analysis without explicit implementation.
Using this template, we develop two new algorithms: the ﬁrst one is model-free (the ﬁrst in the literature to our knowledge) and minimax optimal under strictly positive costs; the second one is model-based and minimax optimal even with zero-cost state-action pairs, matching the best existing result from [Tarbouriech et al., 2021b]. Importantly, both algorithms admit highly sparse updates, making them computationally more efﬁcient than all existing algorithms. Moreover, both can be made completely parameter-free. 1

Introduction
We study the Stochastic Shortest Path (SSP) model, where an agent aims to reach a goal state with minimum cost in a stochastic environment. SSP is well-suited for modeling many real-world applications, such as robotic manipulation, car navigation, and others. Although it is widely studied empirically (e.g., [Andrychowicz et al., 2017, Nasiriany et al., 2019]) and in optimal control theory (e.g., [Bertsekas and Tsitsiklis, 1991, Bertsekas and Yu, 2013]), it has received less attention under the regret minimization setting where a learner needs to learn the environment and improve her policy on-the-ﬂy through repeated interaction. Speciﬁcally, the problem proceeds in K episodes. In each episode, the learner starts at a ﬁxed initial state, sequentially takes action, suffers some cost, and transits to the next state, until reaching a predeﬁned goal state. The performance of the learner is measured by her regret, which is the difference between her total costs and that of the best policy.
Tarbouriech et al. [2020a] develop the ﬁrst regret minimization algorithm for SSP with a regret bound of ˜O(D3/2S(cid:112)AK/cmin), where D is the diameter, S is the number of states, A is the number of actions, and cmin is the minimum cost among all state-action pairs. Cohen et al. [2020] improve over their results and give a near optimal regret bound of ˜O(B(cid:63)S
AK), where B(cid:63) ≤ D is the largest expected cost of the optimal policy starting from any state. Even more recently, Cohen et al.
[2021] achieve minimax regret of ˜O(B(cid:63)
SAK) through a ﬁnite-horizon reduction technique, and concurrently Tarbouriech et al. [2021b] also propose minimax optimal and parameter-free algorithms.
Notably, all existing algorithms are model-based with space complexity Ω(S2A). Moreover, they all
√
√ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
update the learner’s policy through full-planning (a term taken from [Efroni et al., 2019]), incurring a relatively high time complexity.
In this work, we further advance the state-of-the-art by proposing a generic template for regret minimization algorithms in SSP (Algorithm 1), which achieves minimax optimal regret as long as some properties are ensured. By instantiating our template differently, we make the following two key algorithmic contributions:
• In Section 4, we develop the ﬁrst model-free SSP algorithm called LCB-ADVANTAGE-SSP (Algorithm 2). Similar to most model-free reinforcement learning algorithms, LCB-ADVANTAGE-SSP does not estimate the transition directly, enjoys a space complexity of ˜O(SA), and also takes only O (1) time to update certain statistics in each step, making it a highly efﬁcient algorithm.
It achieves a regret bound of ˜O(B(cid:63)
SAK + B5 min), which is minimax optimal when cmin > 0. Moreover, it can be made parameter-free without worsening the regret bound. (cid:63)S2A/c4
√
√
• In Section 5, we develop another simple model-based algorithm called SVI-SSP (Algorithm 3), which achieves minimax regret ˜O(B(cid:63)
SAK + B(cid:63)S2A) even when cmin = 0, matching the best existing result by Tarbouriech et al. [2021b].1 Notably, compared to their algorithm (as well as other model-based algorithms), SVI-SSP is computationally much more efﬁcient since it updates each state-action pair only logarithmically many times, and each update only performs one-step planning (again, a term taken from [Efroni et al., 2019]) as opposed to full-planning (such as value iteration or extended value iteration); see more concrete time complexity comparisons in Section 5.
SVI-SSP can also be made parameter-free following the idea of [Tarbouriech et al., 2021b].
We include a summary of regret bounds of all existing SSP algorithms as well as more complexity comparisons in Appendix A.
Techniques Our main technical contribution is a new analysis framework called implicit ﬁnite-horizon approximation (Section 3), which is the key to analyze algorithms developed from our template. The high level idea is to approximate an SSP instance by a ﬁnite-horizon counterpart.
However, the approximation only happens in the analysis, a key difference compared to [Chen et al., 2021, Chen and Luo, 2021, Cohen et al., 2021] that explicitly implement such an approximation in their algorithms. As a result, our method not only avoids blowing up the space complexity by a factor of the horizon, but also allows one to derive a horizon-free regret bound (more explanation to follow).
In order to achieve the minimax optimal regret, our model-free algorithm LCB-ADVANTAGE-SSP uses a key variance reduction idea via a reference-advantage decomposition by [Zhang et al., 2020b].
However, crucial distinctions exist. For example, we update the reference value function more frequently instead of only one time, which helps reduce the sample complexity and improve the lower-order term in the regret bound. We also maintain an empirical upper bound on the value function in a doubling manner, which is the key to eventually make the algorithm parameter-free. On the other hand, for our model-based algorithm SVI-SSP, we adopt a special Bernstein-style bonus term and bound the learner’s total variance via recursion, taking inspiration from [Tarbouriech et al., 2021b, Zhang et al., 2020a].
Empirical Evaluation We support our theoretical ﬁndings with experiments in Appendix H. Our model-free algorithm demonstrates a better convergence rate compared to vanilla Q learning with naive (cid:15)-greedy exploration. Our model-based algorithm has competitive performance compared to other model-based algorithms, while spending the least amount of time in updates.