Abstract
Neural Transducer (e.g., RNN-T) has been widely used in automatic speech recog-nition (ASR) due to its capabilities of efﬁciently modeling monotonic alignments between input and output sequences and naturally supporting streaming inputs.
Considering that monotonic alignments are also critical to text to speech (TTS) synthesis and streaming TTS is also an important application scenario, in this work, we explore the possibility of applying Transducer to TTS and more. However, it is challenging because it is difﬁcult to trade off the emission (continuous mel-spectrogram prediction) probability and transition (ASR Transducer predicts blank token to indicate transition to next input) probability when calculating the output probability lattice in Transducer, and it is not easy to learn the alignments between text and speech through the output probability lattice. We propose SpeechTrans-ducer (Speech-T for short), a Transformer based Transducer model that 1) uses a new forward algorithm to separate the transition prediction from the continuous mel-spectrogram prediction when calculating the output probability lattice, and uses a diagonal constraint in the probability lattice to help the alignment learning; 2) supports both full-sentence or streaming TTS by adjusting the look-ahead context; and 3) further supports both TTS and ASR together for the ﬁrst time, which enjoys several advantages including fewer parameters as well as streaming synthesis and recognition in a single model. Experiments on LJSpeech datasets demonstrate that
Speech-T 1) is more robust than the attention based autoregressive TTS model due to its inherent monotonic alignments between text and speech; 2) naturally supports streaming TTS with good voice quality; and 3) enjoys the beneﬁt of joint modeling TTS and ASR in a single network. 1

Introduction
Transducer [7] is a sequence-to-sequence model widely used in automatic speech recognition (ASR) [19, 17, 36, 28]. ASR Transducer consists of a speech encoder that converts an input speech sequence to hidden representations, a text encoder that processes already generated text tokens autoregressively, and a joint network that predicts the next text token. Transducer networks are trained to maximize the alignment probability between speech and text sequences, with the help of a forward algorithm on the output probability lattice (see Figure 1 for more details). The alignments 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learned in the output probability lattice of the Transducer are strictly monotonic, thus well modeling monotonic alignments between speech and text sequences in ASR. By restricting the speech encoder only to see the previous speech frames, Transducer naturally supports streaming inputs and is the one of the most popular solutions for streaming ASR [36, 12, 33, 3].
Although Transducer is originally designed for ASR, clearly, its advantages of modeling mono-tonic alignments between inputs and outputs and supporting streaming inputs perfectly match the requirements of text to speech (TTS) synthesis [26, 30, 22, 1, 6, 16, 13, 20, 21, 11]. First, learning alignments between text and speech is important in TTS [26], and different approaches such as atten-tion mechanisms [22, 8, 25, 13] or duration prediction [20, 21, 11, 4] have been leveraged to model the alignments. However, attention mechanisms suffer from unstable alignments with word skipping and repeating issues [16, 20, 8], and duration prediction suffers from additional complication because of the training of duration predictor and training-inference mismatch caused by the predictor [20, 21, 11].
Thus, the inherent monotonic alignments in Transducer can be a quite competitive solution for TTS over previous methods. Second, streaming TTS (or incremental TTS) [14, 5, 23], which aims to support streaming text inputs, can greatly save the synthesis latency and can be widely used in online scenarios such as conversations. Unfortunately, existing solutions for streaming TTS [14] mostly rely on the argmax operation of the encoder-decoder attention values to decide the end of speech frame corresponding to an input word, which is not accurate and robust. Transducer can naturally support streaming inputs and can be a better solution compared with previous methods for streaming TTS.
Therefore, we explore the possibility of applying Transducer for TTS in this work.
However, applying Transducer to TTS is challenging due to the distinctive characteristics of speech (e.g., mel-spectrograms) generation in TTS compared with the text token generation in ASR. First, there are two actions in the output probability lattice of Transducer [7, 32]: emission that predicts a text token and transition that predicts a blank token to indicate null outputs in current step and the transition to the next input speech frame [7]. In ASR Transducer, the blank token is added into the token vocabulary, which means that the emission (token prediction) and transition (blank prediction) can be modeled in a uniﬁed probability distribution through the softmax on all the vocabulary tokens. However, mel-spectrograms in TTS are continuous, which makes it hard to trade off the two probabilities when calculating the output probability lattice in TTS Transducer. As shown in our experiments, ill tradeoff between the two probabilities causes the failure of alignment learning.
Second, while predicting current mel-spectrogram, previous mel-spectrograms can provide enough information as they are very similar due to the continuity between consecutive mel-spectrograms [22, 2], which may lead to copying the previous frame instead of learning information from the text encoder and thus harms alignment learning. Moreover, Transducer learns the appropriate alignment path from huge candidate paths in the probability lattice, which further causes difﬁculties in alignment learning.
In this work, we propose a Transformer [29] based Transducer model, named SpeechTransducer (Speech-T for short), for TTS. First, we design a lazy forward algorithm that separates transition pre-diction from mel-spectrogram prediction when deriving the loss function of Transducer. Speciﬁcally, this lazy forward algorithm on the probability lattice only calculates the probability of transition/non-transition (using a binary classiﬁcation), without the probability of mel-spectrogram prediction.
Second, we introduce a diagonal constraint [25, 16, 2] in the probability lattice to assist alignment learning, which ignores the alignment paths that deviate too much from the diagonal. The intuitive idea behind this design is that the alignments between text and speech should be monotonic, and thus lie in the diagonal region in the probability lattice. Third, by adjusting the text encoder of Speech-T to look the whole context or only the previous context, it can support full-sentence TTS or streaming
TTS. Last but not the least, we further extend Speech-T to support both TTS and ASR together for the ﬁrst time, which enjoys several advantages including fewer parameters as well as streaming synthesis and recognition in a single model. Experiment results on LJSpeech datasets demonstrate that Speech-T 1) is more robust than attention based autoregressive TTS models due to its inherent monotonic alignments between text and speech, 2) can naturally support streaming TTS with good voice quality, and 3) enjoys the beneﬁt of joint modeling TTS and ASR in a single network.
The main contributions of this work are summarized as follows:
• To leverage the advantages of Transducer (i.e., monotonic alignment modeling and streaming input supporting), we propose SpeechTransducer for TTS. To the best of our knowledge, we are the ﬁrst to achieve competitive voice quality in TTS and support streaming TTS using Transducer. 2
• We design a lazy forward algorithm to trade off the two probabilities of mel-spectrogram prediction and transition prediction, and design a diagonal constraint to ensure the alignment learning between text and speech, to ensure the quality of SpeechTransducer.
• We further extend SpeechTransducer to support both TTS and ASR at the same time, which enjoys several advantages including compact modeling (fewer parameters), streaming synthesis and recognition, self-ranking, etc. To the best of our knowledge, we are the ﬁrst to successfully perform the two tasks with a single model. 2