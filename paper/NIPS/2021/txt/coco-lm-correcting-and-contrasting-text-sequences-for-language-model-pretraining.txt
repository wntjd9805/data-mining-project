Abstract
We present a self-supervised learning framework, COCO-LM, that pretrains Lan-guage Models by COrrecting and COntrasting corrupted text sequences. Following
ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The ﬁrst token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive
Learning, is to align text sequences originated from the same source input while en-suring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efﬁciency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points. 1

Introduction
Pretrained language models (PLMs) have reshaped the way AI systems process natural language [11, 36, 39, 40]. Before task-speciﬁc training, it is now a common practice to ﬁrst pretrain the deep neural networks, often Transformers [53], via a self-supervised token-level language modeling task [29, 31, 40]. Whether it is autoregressive [39], permutational [62], or masked language modeling (MLM) [11], the Transformer networks are pretrained to recover some omitted tokens using the rest of input texts. Then the language semantics captured during pretraining are conveyed to downstream tasks via the pretrained Transformer parameters [5, 8, 44].
Recent research [14, 16, 25, 43] observed several challenges in this self-supervised learning frame-work. One challenge is its efﬁciency. After pretrained for a while with the standard token-level language modeling, the networks have already captured the basic language patterns, making a large fraction of pretraining signals no longer informative. Linear improvement in the model effectiveness often requires exponentially more pretraining compute and parameters [25], which is unsustainable.
Another challenge is the anisotropy of text representations from pretrained models. The sequence representations from many pretrained models are quite irregular [30, 43] and require dedicated
ﬁne-tuning approaches to be useful in sequence-level applications [32, 60].
Clark et al. [7] proposed a new pretraining strategy, ELECTRA, that uses an auxiliary language model (“generator”) to replace tokens in input texts and pretrains the main Transformer (“discriminator”) to
∗Part of this work was done while Yu was interning at Microsoft. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
detect replaced tokens. This improves the pretraining efﬁciency and effectiveness, but pretraining via binary classiﬁcation hinders the model’s usage on applications requiring language modeling capability (e.g., prompt-based learning [15, 28, 46]). It could further distort the representation space as the Transformers are pretrained to output the same “non-replacement” label for all actual tokens.
In this paper, we present a new self-supervised learning approach, COCO-LM, that pretrains Lan-guage Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary model to corrupt the input texts, upon which it intro-duces two new pretraining tasks for the main Transformer, one at token level and one at sequence level. The token-level task, corrective language modeling (CLM), pretrains the main Transformer to detect and correct the tokens in the corrupted sequences. It uses a multi-task setup to combine the beneﬁts of replaced token detection and language modeling. The sequence-level task, sequence contrastive learning (SCL), pretrains the model to align text sequences originated from the same source sequence and enforce uniformity of the representation space.
In our experiments on GLUE [54] and SQuAD [41] benchmarks, COCO-LM not only outperforms state-of-the-art pretraining approaches in effectiveness, but also signiﬁcantly improves the pretraining efﬁciency. Under the same setting, COCO-LM matches the MNLI accuracy of RoBERTa and
ELECTRA with 60% and 50% of their GPU hours in pretraining, respectively. When pretrained with the same number of steps, COCO-LM outperforms the previous best models by 1+ GLUE average points under the standard base/large-sized model evaluations. With 367 million parameters, COCO-LMLarge++ reaches the MNLI accuracy of Megatron3.9B [49], one of the largest BERT-style model with 3.9 billion parameters. Our analyses provide further insights on the advantage of CLM in learning token representations and its effectiveness in prompted-based ﬁne-tuning, as well as the beneﬁt of
SCL in ensuring alignment and uniformity in the representation space for better generalization1. 2