Abstract
Actor-critic methods are widely used in ofﬂine reinforcement learning practice, but are not so well-understood theoretically. We propose a new ofﬂine actor-critic algorithm that naturally incorporates the pessimism principle, leading to several key advantages compared to the state of the art. The algorithm can operate when the Bellman evaluation operator is closed with respect to the action value func-tion of the actor’s policies; this is a more general setting than the low-rank MDP model. Despite the added generality, the procedure is computationally tractable as it involves the solution of a sequence of second-order programs. We prove an upper bound on the suboptimality gap of the policy returned by the procedure that depends on the data coverage of any arbitrary, possibly data dependent comparator policy. The achievable guarantee is complemented with a minimax lower bound that is matching up to logarithmic factors. 1

Introduction
The problem of learning a near-optimal policy is a core challenge in reinforcement learning (RL).
In many settings, it is beneﬁcial to be able to learn a good policy using only a pre-collected set of data, without further exploration with the environment; this problem is known as ofﬂine or batch policy learning. The ofﬂine setting has unique challenges due to the incomplete information about the Markov decision process (MDP) encoded in the available dataset. For example, due to maxi-mization bias, a naive ofﬂine algorithm can return a policy with a severely overestimated value. In order to avoid such undesirable behavior, researchers have introduced the idea of pessimism under uncertainty, and there is now a growing literature (e.g., Liu et al. (2020); Jin et al. (2020b); Buck-man et al. (2020); Kumar et al. (2019); Kidambi et al. (2020); Yu et al. (2020)) on different ways in which pessimism can be incorporated. See Appendix B for additional references and discussion of this body of work.
At a high level, incorporating pessimism prevents algorithms from settling down on uncertain poli-cies whose value might be misleadingly high under the current dataset due to statistical errors. By using pessimism, uncertain policies are penalized in such a way that only those policies robust to statistical errors are returned. The principle can be implemented in at least two different ways: (a) by penalizing policies that are far from the one that generated the dataset; or (b) by penalizing the value functions of policies not well covered by the dataset. In this paper, we take the latter avenue.
⇤This work was fully completed while Andrea Zanette was a PhD candidate at Stanford University. Future updates of this work will be available at https://arxiv.org/abs/2108.08812 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1 Overview and our contributions
Implementing pessimism with function approximation is challenging for several reasons. First, un-certainty must be estimated with particular care. On one hand, underestimating it can fail to correct the coverage problem. On the other hand, overestimating it leads to policies that are too conserva-tive and thus underperform. Second, the incorporation of pessimism may introduce complex, higher order perturbations into the value function class handled by the algorithm. Similar issues can arise when adding optimistic bonuses in the exploration. The increased complexity of the function class often requires additional assumptions on the model, because the new class needs to interact “nicely” with the Bellman operator. Prior art on pessimism with function approximation has by-passed this problem by making strong model assumptions, such as low-rank transitions Jin et al. (2020b) or algorithm-speciﬁc assumptions Liu et al. (2020).
Actor-critic methods: Most past theoretical work on ofﬂine reinforcement learning on ﬁnding with high probability the policy with the highest performance has focused on algorithms that are either model or value-based2 Liu et al. (2020); Jin et al. (2020b); Buckman et al. (2020); Kidambi et al. (2020); Yu et al. (2020); these often incorporate pessimism into the estimates of the policy performance. Actor-critic methods are a hybrid class of methods that mitigate some deﬁciencies of methods that are either purely policy or purely value-based Konda and Tsitsiklis (2000, 2003);
Heess et al. (2015); Haarnoja et al. (2017, 2018); in modern RL, they are widely used in practice (e.g., Levine et al. (2020); Wu et al. (2019, 2021); Kumar et al. (2019, 2020)). An actor-critic method generally consists of an actor that changes the policy in order to maximize its value as estimated by the critic. Given their popularity, it is natural to ask the following question: do actor-critic methods provably offer any advantage in ofﬂine RL? The main contribution of this paper is to give a positive answer to this question: by separating the policy optimization from the policy evaluation, both tasks become simpler to design and the pessimism principle can be incorporated more naturally.
Contributions: More speciﬁcally, we study the problem of policy learning using linear function approximation in the ofﬂine setting. We assume that we are given a batch data set
, in which each sample consists of a quadruple. The ﬁrst two components are the state-action pair, corresponding to the state in which a given action was taken, and the last two components correspond to a noisy observation of the reward, and a successor state drawn from the appropriate transition function.
Our theory allows for a very general dependence structure among the the state-action pairs in these samples; when the data set is ordered according to how the samples were collected (which need not be related to a trajectory), we allow the state-action pair at any given instant to depend on all past samples. This set-up allows from data collected from arbitrary policies, mixtures of policies, generative models or even in adversarial manner.
D
D
Given such a data set, our objective is to ﬁnd the policy that performs best in the face of uncertainty.
In particular, we need to account for the fact that the optimal policy ⇡⇤ for the underlying MDP may not be well covered by the dataset
, in which case the associated uncertainty would be prohibitive.
In order to achieve this goal, we design an actor-critic procedure that iteratively optimizes a lower bound on the value of the optimal policy. Suppose that we are interested in optimizing the value function at some given initial s1. Our strategy works as follows: for any given policy ⇡, we construct (⇡) of “statistically plausible” MDPs, and use them to deﬁne a simple second-order a family cone program. By solving this convex program, we obtain value function estimate V ⇡
M (s1) = (⇡)—is guaranteed to be
M (s1) that—for an appropriately constructed family arg minM a lower bound on the true value function of ⇡ in the unknown MDP that generated the dataset. Given a procedure for producing such lower bounds, it is then natural to maximize these lower bounds over some family ⇧ of policies. This combination leads to the saddle-point problem (⇡) V ⇡
M
M 2M max
⇧
⇡ 2 min 2M (⇡)
M
V ⇡
M (s1). (1)
Note that actor-critic methods ﬁt naturally in this framework: the critic provides a pessimistic eval-uation of any given policy ⇡, and the actor solves the outer maximization problem over policies. 2Exceptions to this include importance-sampling based approaches to selecting among a ﬁnite set of policies (e.g. Mandel et al. (2014); Thomas et al. (2015, 2019)); however, such approaches have focused on operating without a Markov assumption and inherently provide much looser guarantees than the ones we and others consider for the Markov setting. 2
This decoupling lends itself to a computationally tractable implementation, along with an analysis of the procedure. In particular, we show that the actor’s sequence of estimated policies enjoys online learning-style guarantees with respect to a sequence of pessimistic MDPs implicitly identiﬁed by the critic.
The way in which we introduce pessimism is a second key component of the algorithmic framework.
In particular, in line with our previous paper Zanette et al. (2020b), we do so without enlarging the prescribed classes of functions and policies. We do so by a direct perturbation of the value functions examined by the critic; there is no addition of pessimistic bonuses or absorbing states. Since the class of value functions is not altered, this method has two main advantages. First, there are no additional model assumptions compared to the standard—that is non-pessimistic—version of the actor-critic method. Second, the complexity of the underlying classes is not increased, thereby allowing us to construct tight conﬁdence intervals and estimation error bounds that are minimax optimal up to logarithmic factors.
The remainder of this paper is organized as follows. We begin in Section 2 with background on
MDPS, and then introduce the modeling assumptions that underlie the analysis of this paper. In
Section 3, we introduce the algorithm studied in this paper, namely the Pessimistic Actor Critic for Learning without Exploration (for short, PACLE) algorithm. Section 4 provides statements of our main results and discussion of their consequences, including an upper bound on the PACLE algorithm in Theorem 1, and a minimax lower bound in Theorem 2. In Section A, we provide an outline of the proof of Theorem 1, with various technical details as well as the proof of Theorem 2 deferred to the appendices. We conclude with a discussion in Section 5. 1.2 Notation r x 2
Rd
For the reader’s convenience, we summarize here some notation used throughout the paper. We let
R in dimension d; we simply denote the Euclidean ball of radius r
Bd(r) = x
{
Rd, we use [x]i to denote its when there is no possibility of confusion. For a vector x write
B ith component. We use the
O notation to denote an upper bound that holds up to constants and log factors in the input parameters ( 1
  , d, H). The notation . means an upper bound that holds up to a constant, with an analogous deﬁnition for &. k2  2 2
|k
} e 2