Abstract
Modern neural interfaces allow access to the activity of up to a million neurons within brain circuits. However, bandwidth limits often create a trade-off between greater spatial sampling (more channels or pixels) and the temporal frequency of sampling. Here we demonstrate that it is possible to obtain spatio-temporal super-resolution in neuronal time series by exploiting relationships among neurons, embedded in latent low-dimensional population dynamics. Our novel neural net-work training strategy, selective backpropagation through time (SBTT), enables learning of deep generative models of latent dynamics from data in which the set of observed variables changes at each time step. The resulting models are able to infer activity for missing samples by combining observations with learned latent dynamics. We test SBTT applied to sequential autoencoders and demonstrate more efﬁcient and higher-ﬁdelity characterization of neural population dynamics in electrophysiological and calcium imaging data. In electrophysiology, SBTT enables accurate inference of neuronal population dynamics with lower interface bandwidths, providing an avenue to signiﬁcant power savings for implanted neu-roelectronic interfaces. In applications to two-photon calcium imaging, SBTT accurately uncovers high-frequency temporal structure underlying neural popula-tion activity, substantially outperforming the current state-of-the-art. Finally, we demonstrate that performance could be further improved by using limited, high-bandwidth sampling to pretrain dynamics models, and then using SBTT to adapt these models for sparsely-sampled data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
* Contributed equally. Corresponding authors: fzhu23@emory.edu, {asedler,chethan}@gatech.edu
1

Introduction
Modern systems neuroscientists have access to the activity of many thousands to potentially millions of neurons via multi-photon calcium imaging and high-density silicon probes [1–4]. Such interfaces provide a qualitatively different picture of brain activity than was achievable even a decade ago.
However, neural interfaces increasingly face a trade-off – the number of neurons that can be accessed (capacity) is often far greater than the number that is simultaneously monitored (bandwidth). For example, with 2-photon calcium imaging (2p; Fig. 1a, top), hundreds to thousands of neurons are serially scanned by a laser that traverses the ﬁeld of view, resulting in different neurons being sampled at different times within an imaging frame. As a consequence, a trade-off exists between the size of the ﬁeld-of-view (and hence the number of neurons monitored), the sampling frequency, and the signal-to-noise with which each neuron is sampled. Whereas current analysis methods treat 2p data as if all neurons within a ﬁeld-of-view were sampled at the same time at the imaging frame rate, the fact that each neuron is sampled at staggered, known times within the frame could be employed to increase the time resolution.
Electrophysiological interfaces face similar trade-offs (Fig. 1a, bottom). With groundbreaking high-density probes such as Neuropixels and Neuroseeker [3–5], simultaneous monitoring of all recording sites is either not currently possible or limits the signal-to-noise ratio, so users typically monitor a selected subset of sites within a given recording session. For example, Neuropixels 2.0 probes contain up to 5120 electrodes, 384 of which can be recorded simultaneously [4]. In other situations, power constraints might make it preferable to restrict the number of channels that are simultaneously monitored, such as in wireless or fully-implanted applications where battery life and heat dissipation are key challenges [6–8]. As newer interfacing strategies provide a pathway to hundreds of thousands of channels for revolutionary brain-machine interfaces [9, 10], neural data processing strategies that can leverage dynamic deployment of recording bandwidth might allow substantial power savings.
Solutions to these space-time trade-offs may come from the structure of neural activity itself. A large body of work suggests that the activity of individual neurons within a large population is not independent, but instead is coordinated through a lower-dimensional, latent state that evolves with stereotyped temporal structure (Fig. 1b). We can represent the state at time t as a vector xt ∈ RD that evolves according to dynamics captured by a function f such that xt+1 ≈ f (xt). Rather than
Figure 1: Exploiting space-time trade-offs in neural interfaces using SBTT. (a) In 2-photon calcium imaging (top), individual neurons are serially scanned at a low frame rate, resulting in staggered sample times. In modern electrophysiological recordings (bottom), bandwidth or power constraints prevent simultaneous monitoring of all recording sites. (b) Observed neuronal activity reﬂects latent, low-dimensional dynamics (captured by the function f ). (c) SBTT applied to a sequential autoencoder for inferring latent dynamics from neural population activity. 2
directly observing the latent state xt, we observe neural activity that we represent as yt ∈ RN , where yt ≈ h(xt) for some function h. Due both to the fact that f imposes a signiﬁcant amount of structure on the trajectory of the xt’s and the fact that we typically expect the dimension D of xt to be far smaller than the number of possible observations N , one might expect that it should be possible to estimate the xt’s without observing every neuron at every time step (i.e., measuring only some of the elements of each yt), just as we generally infer latent states from only a fraction of the neurons in a given area. If so, principled exploitation of the space-time trade-off of neural interfaces might achieve higher-ﬁdelity or more bandwidth-efﬁcient characterization of neural population activity.
To our knowledge, no methods have demonstrated inference of dynamics from data in which the set of neurons being monitored changes dynamically at short intervals. To address this challenge, we introduce selective backpropagation through time (SBTT; Fig. 1c), a method to train deep generative models of latent dynamics from data where the identity of observed variables varies from sample to sample. Here we explore applications of SBTT to state space modeling of neural population activity that obeys low-dimensional dynamics.
This paper is organized as follows. Section 2 provides an overview of related work. Section 3 details
SBTT and its integration with sequential autoencoders for modeling neural population dynamics.
Section 4 demonstrates the effectiveness of this solution in achieving more efﬁcient and higher-ﬁdelity inference of latent dynamics in applications to electrophysiological and calcium imaging data. 2