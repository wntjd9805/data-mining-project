Abstract
While agents trained by Reinforcement Learning (RL) can solve increasingly chal-lenging tasks directly from visual observations, generalizing learned skills to novel environments remains very challenging. Extensive use of data augmentation is a promising technique for improving generalization in RL, but it is often found to decrease sample efﬁciency and can even lead to divergence. In this paper, we in-vestigate causes of instability when using data augmentation in common off-policy
RL algorithms. We identify two problems, both rooted in high-variance Q-targets.
Based on our ﬁndings, we propose a simple yet effective technique for stabilizing this class of algorithms under augmentation. We perform extensive empirical evaluation of image-based RL using both ConvNets and Vision Transformers (ViT) on a family of benchmarks based on DeepMind Control Suite, as well as in robotic manipulation tasks. Our method greatly improves stability and sample efﬁciency of ConvNets under augmentation, and achieves generalization results competitive with state-of-the-art methods for image-based RL in environments with unseen visuals. We further show that our method scales to RL with ViT-based architectures, and that data augmentation may be especially important in this setting.† 1

Introduction
Reinforcement Learning (RL) from visual observations has achieved tremendous success in various applications such as video-games [43, 4, 70], robotic manipulation [37], and autonomous navigation
[42, 83]. However, it is still very challenging for current methods to generalize the learned skills to novel environments, and policies trained by RL can easily overﬁt to the training environment [81, 13], especially for high-dimensional observation spaces such as images [8, 58].
Increasing the variability in training data via domain randomization [66, 50] and data augmenta-tion [57, 35, 33, 51] has demonstrated encouraging results for learning policies invariant to changes in environment observations. Speciﬁcally, recent works on data augmentation [35, 33] both show im-provements in sample efﬁciency from simple cropping and translation augmentations, but the studies also conclude that additional data augmentation in fact decrease sample efﬁciency and even cause divergence. While these augmentations have the potential to improve generalization, the increasingly varied data makes the optimization more challenging and risks instability. Unlike supervised learning, balancing the trade-off between stability and generalization in RL requires substantial trial and error.
In this paper, we illuminate causes of instability when applying data augmentation to common off-policy RL algorithms [43, 38, 15, 18]. Based on our ﬁndings, we provide an intuitive method for stabilizing this class of algorithms under use of strong data augmentation. Speciﬁcally, we ﬁnd two main causes of instability in previous work’s application of data augmentation: (i) indiscriminate application of data augmentation resulting in high-variance Q-targets; and (ii) that Q-value estimation strictly from augmented data results in over-regularization.
†Website and code is available at: https://nicklashansen.github.io/SVEA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
To address these problems, we propose SVEA: Stabilized Q-Value Estimation under Augmentation, a simple yet effective framework for data augmentation in off-policy RL that greatly improves stability of Q-value estimation. Our method consists of the following three components: Firstly, by only applying augmentation in Q-value estimation of the current state, without augmenting Q-targets used for bootstrapping, SVEA circumvents erroneous bootstrapping caused by data augmentation;
Secondly, we formulate a modiﬁed Q-objective that optimizes Q-value estimation jointly over both augmented and unaugmented copies of the observations; Lastly, for SVEA implemented with an actor-critic algorithm, we optimize the actor strictly on unaugmented data, and instead learn a generalizable policy indirectly through parameter-sharing. Our framework can be implemented efﬁciently without additional forward passes nor introducing additional learnable parameters.
We perform extensive empirical evaluation on the DeepMind Control Suite [64] and extensions of it, including the DMControl Generalization Benchmark [21] and the Distracting Control Suite [60], as well as a set of robotic manipulation tasks. Our method greatly improve Q-value estimation with ConvNets under a set of strong data augmentations, and achieves sample efﬁciency, asymptotic performance, and generalization that is competitive or better than previous state-of-the-art methods in all tasks considered, at a lower computational cost. Finally, we show that our method scales to RL with Vision Transformers (ViT) [10]. We ﬁnd that ViT-based architectures are especially prone to overﬁtting, and data augmentation may therefore be a key component for large-scale RL. 2