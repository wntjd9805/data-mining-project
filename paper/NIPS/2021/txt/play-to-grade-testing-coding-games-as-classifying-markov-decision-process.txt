Abstract
Contemporary coding education often presents students with the task of develop-ing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are im-possible to grade by traditional unit tests. In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Deci-sion Processes (MDPs). Each student’s program fully speciﬁes an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken.
We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classiﬁer to determine membership: Play to
Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research. 1

Introduction
The need for high quality education at scale is of critical importance [1]. While delivery of content for millions of students is possible, providing feedback – a cornerstone of education – remains an open challenge. The quality of an online education platform depends on the feedback it can provide to its students. However contemporary coding education has a clear limitation. Students are able to get automatic feedback only up until they start writing interactive programs. When a student authors a program that requires user interaction, e.g. where a user interacts with the student’s program using a mouse (such as a game), or by clicking on button (such as in a graphical user interface) it becomes exceedingly difﬁcult to grade automatically. Even for well deﬁned challenges, if the user has any creative discretion, or the problem involves any randomness, the task of automatically assessing the work is daunting. This is especially true as coding has become more popular and feedback is required for many students. One popular intro to programming platform, Code.org has over 61 million enrolled students [2]. For their many interactive assignments they have no ability to even identify if a student has a working solution.
Why is providing feedback to interactive programs so hard? The standard way to provide feedback is through human labor. Teachers need to interact with each student’s program for 20 seconds to 10 minutes in order to grade. This may seem small, but for the quantity of unique programs on a platform such as Code.org that amounts to approximately 9.5 years of human effort to provide feedback for a single assignment. Code.org has tried crowdsourcing feedback from hundreds of thousands of teachers which has fallen short for similar reasons: labelling is hard and undesirable work [3]. Interactive assignments can not be auto graded as running the program requires dynamic
∗anie@stanford.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
input. Moreover, heavily scaffolded assignments – ones that force students to program their interactive programs in a way that enables standard testing – are considered to be a sub-par learning experience for students and are not common in contemporary classrooms.
There is a long history of work towards providing feedback to students working on open ended assignments which has shown that automatically providing feedback based on code text is a hard machine learning problem [4, 5, 6, 7, 8, 9]. This is true for many reasons. Even for introductory level computer science education, homework datasets have statistical distributions with heavy tails similar to natural language [10]. Moreover, these distributions tend to be highly discontinuous – two solutions which are only slightly different in text can be very different in its behavior. As such, approaches to grading which rely only on reading the text of a students code end up being as complex as understanding a passage of natural language [11]. Finally, because feedback is necessary for the ﬁrst student working on an assignment, not just the millionth, and new assignments are often introduced, the challenge is inherently few shot.
In this work we show that an algorithm that learns to interact with a student’s assignment, to play with the student work, can enable new capacity for understanding student work. A collaborative system which can simultaneously learn to understand what type of behavior is undesirable, as well as play student’s work to actively trigger these undesirable behaviors, is the crucial ﬁrst step to developing automatic and intelligent feedback for massive online coding education.
Our contributions:
• We introduce the reinforcement learning challenge of Play to Grade.
• We propose a collaborative algorithm where an agent simultaneously learns to play a game and recognize what states are bug inducing states.
• Our solution obtains 93.4%-94.0% accuracy on a real-world coding dataset provided by Code.org.
We gained a 19-25 percentage point improvement over grading programs via code text.
• We release a Bounce dataset of student submissions with ground truth bug labels and an OpenAI-Gym compatible environment to support further research: https://github.com/windweller/ play-to-grade. 1.1