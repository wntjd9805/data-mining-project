Abstract
Humans perceive the world by concurrently processing and fusing high-dimensional inputs from multiple modalities such as vision and audio. Machine perception models, in stark contrast, are typically modality-specific and optimised for unimodal benchmarks, and hence late-stage fusion of final representations or predictions from each modality (‘late-fusion’) is still a dominant paradigm for multimodal video classification. Instead, we introduce a novel transformer based architecture that uses ‘fusion bottlenecks’ for modality fusion at multiple layers.
Compared to traditional pairwise self-attention, our model forces information be-tween different modalities to pass through a small number of bottleneck latents, requiring the model to collate and condense relevant information in each modality and share what is necessary. We find that such a strategy improves fusion per-formance, at the same time reducing computational cost. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple audio-visual classi-fication benchmarks including Audioset, Epic-Kitchens and VGGSound. All code and models will be released. 1

Introduction
Simultaneous multimodal sensations are a crucial enabler of human perceptual learning [50]. For artificial learning systems, however, designing a unified model for modality fusion is challenging due to a number of factors: (i) variations in learning dynamics between modalities [56], (ii) different noise topologies, with some modality streams containing more information for the task at hand than others, as well as (iii) specialised input representations. The difference in input representations between audio and vision is particularly stark – many state of the art audio classification methods rely on short term Fourier analysis to produce log-mel spectrograms, often using them as inputs to
CNN architectures designed for images [26, 48]. These time-frequency representations have different distributions to images – multiple acoustic objects can have energy at the same frequency, and the translation invariances of CNNs may no longer be a desired property (while an acoustic object can be shifted in time, a shift in frequency could alter the meaning entirely). In contrast, the visual stream in a video is three-dimensional (two spatial and one temporal), and while different spatial regions of the image correspond to different objects, there is the unique challenge of high redundancy across multiple frames. Hence input representations, and consequently neural network architectures and benchmarks tend to vary wildly for different modalities. For simplicity, the dominant paradigm for multimodal fusion therefore often consists of an ad-hoc scheme that involves integrating separate audio and visual networks via their output representations or scores i.e. ‘late-fusion’ [22, 44].
In this work, we present a new transformer based model for audiovisual fusion in video. Despite originally being proposed for NLP tasks, there has been recent interest in transformers [54] as universal perceptual models [29], due to their ability to model dense correlations between tokens, at the same time making few assumptions about their inputs (and because continuous perceptual inputs can be tokenised). By dividing dense continuous signals into patches and rasterising them 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Cross-modal Fusion. Unlike late fusion (left), where no cross-modal information is exchanged in the model until after the classifier, we investigate two pathways for the exchange of cross-modal information. The first is via standard pairwise self attention across all hidden units in a layer, but applied only to later layers in the model – mid fusion (middle, left). We also propose the use of ‘fusion bottlenecks’ (middle, right) that restrict attention flow within a layer through tight latent units. Both forms of restriction can be applied in conjunction (Bottleneck Mid Fusion) for optimal performance (right). We show B = 2 bottleneck units and 3 hidden units per modality. Grey boxes indicate tokens that receive attention flow from both audio and video tokens. to 1D tokens, transformers have been shown to perform competitively for image (ViT [16]) and video classification (ViViT [6]), and more recently, audio classification (AST [23]). Because these models are able to elegantly handle variable length sequences, a natural first extension would be to feed in a sequence of both visual and auditory patches to a transformer, with minimal changes to the architecture. This ‘early fusion’ model allows free attention flow between different spatial and temporal regions in the image, as well as across frequency and time in the audio spectrogram. While theoretically appealing, we hypothesise that full pairwise attention at all layers of the model is not necessary because audio and visual inputs contain dense, fine-grained information, much of which is redundant. This is particularly the case for video, as shown by the performance of ‘factorised’ versions of [6]. Such a model would also not scale well to longer videos due to the quadratic complexity of pairwise attention with token sequence length. To mitigate this, we propose two methods to restrict the flow of attention in our model. The first follows from a common paradigm in multimodal learning, which is to restrict cross-modal flow to later layers of the network, allowing early layers to specialise in learning and extracting unimodal patterns. Henceforth this is is referred to as ‘mid fusion’ (Fig. 1, middle left), where the layer at which cross-modal interactions are introduced is called the ‘fusion layer’. The two extreme versions of this are ‘early fusion’ (all layers are cross-modal) and ‘late fusion’ (all are unimodal) which we compare to as a baselines. Our second idea (and main contribution), is to restrict cross-modal attention flow between tokens within a layer. We do this by allowing free attention flow within a modality, but force our model to collate and ‘condense’ information from each modality before sharing it with the other. The core idea is to introduce a small set of latent fusion units that form an ‘attention bottleneck’, through which cross-modal interactions within a layer must pass.
We demonstrate that this ‘bottlenecked’ version, which we name Multimodal Bottleneck Transformer (MBT), outperforms or matches its unrestricted counterpart, but with lower computational cost.
Concretely, we make the following contributions: (i) We propose a new architecture (MBT) for audiovisual fusion. Our model restricts the flow of cross-modal information between latent units through tight fusion ‘bottlenecks’, that force the model to collect and ‘condense’ the most relevant inputs in each modality (and therefore share only that which is necessary with the other modality).
This avoids the quadratic scaling cost of full pairwise attention, and leads to performance gains with less compute; (ii) We apply MBT to image and spectogram patches (Fig. 2), and explore a number of ablations related to the fusion layer, the sampling of inputs and data size; and finally (iii) We set the new state-of-the-art for video classification across a number of popular audio-visual benchmarks, including AudioSet [21], Epic-Kitchens100 [12] and VGGSound [10]. On the Audioset dataset, we outperform the current state of the art by 5.9 mAP (12.7% relative improvement). 2