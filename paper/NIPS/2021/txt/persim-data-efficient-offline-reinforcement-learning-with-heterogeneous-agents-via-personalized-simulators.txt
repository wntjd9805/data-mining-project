Abstract
We consider ofﬂine reinforcement learning (RL) with heterogeneous agents under severe data scarcity, i.e., we only observe a single historical trajectory for every agent under an unknown, potentially sub-optimal policy. We ﬁnd that the performance of state-of-the-art ofﬂine and model-based RL methods degrade signiﬁcantly given such limited data availability, even for commonly perceived “solved” benchmark settings such as “MountainCar” and “CartPole”. To address this challenge, we propose PerSim, a model-based ofﬂine RL approach which ﬁrst learns a personalized simulator for each agent by collectively using the historical trajectories across all agents, prior to learning a policy. We do so by positing that the transition dynamics across agents can be represented as a latent function of latent factors associated with agents, states, and actions; subsequently, we theoretically establish that this function is well-approximated by a “low-rank” decomposition of separable agent, state, and action latent functions. This representation suggests a simple, regularized neural network architecture to effectively learn the transition dynamics per agent, even with scarce, ofﬂine data. We perform extensive experiments across several benchmark environments and RL methods. The consistent improvement of our approach, measured in terms of both state dynamics prediction and eventual reward, conﬁrms the efﬁcacy of our framework in leveraging limited historical data to simultaneously learn personalized policies across agents. 1

Introduction
Reinforcement learning (RL) coupled with expressive deep neural networks has now become a generic yet powerful solution for learning complex decision-making policies for an agent of interest; it provides the key algorithmic foundation underpinning recent successes such as game solving [34, 45, 44] and robotics [30, 22]. However, many state-of-the-art RL methods are data hungry and require the ability to query samples at will, which is infeasible for numerous settings such as healthcare, autonomous driving, and socio-economic systems. As a result, there has been a rapidly growing literature on “ofﬂine
RL” [31, 24, 32, 15], which focuses on leveraging existing datasets to learn decision-making policies.
Within ofﬂine RL, we consider a regime of severe data scarcity: there are multiple agents and for each agent, we only observe a single historical trajectory generated under an unknown, potentially sub-optimal policy; further, the agents are heterogeneous, i.e., each agent has unique state transition dy-namics. Importantly, the characteristics of the agents that make their transition dynamics heterogeneous are latent. Using such limited ofﬂine data to simultaneously learn a good “personalized” policy for each agent is a challenging setting that has received limited attention. Below, we use a prevalent example 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) MountainCar (b) CartPole
Figure 1: t-SNE visualization of the learned latent factors for the 500 heterogeneous agents. Colors indicate the value of the modiﬁed parameters in each environment (e.g., gravity in MountainCar). There is an informative low-dimensional manifold induced by the agent-speciﬁc latent factors, and there is a natural direction on the manifold along which the parameters that characterize the heterogeneity vary continuously and smoothly. (c) HalfCheetah from healthcare to motivate and argue that tackling such a challenge is an important and necessary step towards building personalized decision-making engines via RL in a variety of important applications.
A Motivating Example. Consider a pre-existing clinical dataset of patients (agents). Our goal is to design a personalized treatment plan (policy) for each patient moving forward. Notable challenges include the following: First, each patient only provides a single trajectory of their medical history. Second, each patient is heterogeneous in that they may have a varied response for a given treatment under similar medical conditions; further, the underlying reason for this heterogeneous response across patients is likely unknown and thus not recorded in the dataset. Third, in the absence of an accurate personalized
“forecasting” model for a patient’s medical outcome given a treatment plan, the treatment assigned is likely to be sub-optimal. This is particularly true for complicated medical conditions like T-Cell
Lymphoma. We aim to address the challenges laid out above (ofﬂine scarce data, heterogeneity, and sub-optimal policies) so as to develop a personalized “forecasting” model for each patient under any given treatment plan. Doing so will then naturally enable ideal personalized treatment policies for each patient.
Key question. Tackling a scenario like the one described above in a principled manner is the focus of this work. In particular, we seek to answer the following question:
“Can we leverage scarce, ofﬂine data collected across heterogeneous agents under unknown, sub-optimal policies to learn a personalized policy for each agent?”
Our Contributions. As our main contribution, we answer this question in the afﬁrmative by developing a structured framework to tackle this challenging yet meaningful setting. Next, we summarize the main methodological, theoretical, algorithmic, and experimental contributions in our proposed framework.
Methodological—personalized simulators. We propose a novel methodological framework, PerSim, to learn a policy given the data availability described above. Taking inspiration from the model-based
RL literature, our approach is to ﬁrst build a personalized simulator for each agent. Speciﬁcally, we use the ofﬂine data collectively available across heterogeneous agents to learn the unique transition dynamics for each agent. We do this without requiring access to the covariates or features that drive the heterogeneity amongst the agents. Having constructed a personalized simulator, we then learn a personalized decision-making policy separately for each agent by simply using online model predictive control (MPC) [16, 9].
Theoretical—learning across agents. As alluded to earlier, the challenge in building a personalized simulator for each agent is that we only have access to a single ofﬂine trajectory for any given agent.
Hence, each agent likely explores a very small subset of the entire state-action space. However, by viewing the trajectories across the multitude of agents collectively, we potentially have access to a relatively larger and more diverse ofﬂine dataset that covers a much richer subset of the state-action space. Still, any approach that augments the data of an agent in this manner must address the possibly large heterogeneity amongst the agents, which is challenging as we do not observe the characteristics that make agents heterogeneous. Inspired by the literature on collaborative ﬁltering for recommendation systems, we posit that the transition dynamics across agents can be represented as a latent function of latent factors associated with agents, states, and actions. In doing so, we establish that this function is well-approximated by a “low-rank” decomposition of separable agent, state, and action latent functions (Theorem 1). Hence, for any ﬁnite sampling of the state and action spaces, accurate model learning for each agent with ofﬂine data—generated from any policy—can be reduced to estimating a low-rank tensor corresponding to agents, states, and actions. As such, low-rank tensors can provide a useful algorithmic lens to enable model learning with ofﬂine data in RL and we hope this work leads to further research studying the relationship between these seemingly disparate ﬁelds. 2
Algorithmic—regularizing via a latent factor model. As a consequence of our low-rank representation result, we propose a natural neural network architecture that respects the constraints induced by the factorization across agents, states, and actions (Section 4). It is this principled structure, which accounts for agent heterogeneity and regularizes the model learning, that ensures the success of our approach despite access to only scarce and heterogeneous data. Further, we propose a natural extension of our framework which generalizes to unseen agents, i.e., agents that are not observed in the ofﬂine data.
Experimental—extensive benchmarking. Using standard environments from OpenAI Gym, we ex-tensively benchmark PerSim against four state-of-the-art methods: a model-based online RL method (CaDM) [29], two model-free ofﬂine RL method (BCQ and CQL) [15, 25], and a model-based ofﬂine
RL method (MOReL) [23]. Below, we highlight six conclusions we reach from our experiments. (i) Despite access to only a single trajectory from each agent (and no access to the covariates that drive agent heterogeneity), PerSim produces accurate personalized simulators for each agent. (ii)
All benchmarking algorithms perform sub-optimally for the data availability we consider, even on simple baseline environments such as MountainCar and Cartpole, which are traditionally considered to be “solved”. (iii) PerSim is able to robustly extrapolate outside the policy used to generate the ofﬂine dataset, even if the policy is highly sub-optimal (e.g., actions are sampled uniformly at random). (iv) To corroborate our latent factor representation, we ﬁnd that across all environments, the learned agent-speciﬁc latent factors correspond very closely with the latent source of heterogeneity amongst agents; we re-emphasize this is despite PerSim not getting access to the agent covariates. (v) We ﬁnd that augmenting the training data of an ofﬂine model-free method (e.g., BCQ) with PerSim-generated synthetic trajectories results in a signiﬁcantly better average reward. (vi) As an ablation study, if we decrease the number of observed trajectories, PerSim consistently achieves a higher reward than the other baselines across most agents, indicating its robustness to data scarcity. For a visual depiction of the conclusions, see Figure 1 for the learned latent agent factors and Figure 2 for the relative prediction accuracy of the learned model using PerSim versus [29].