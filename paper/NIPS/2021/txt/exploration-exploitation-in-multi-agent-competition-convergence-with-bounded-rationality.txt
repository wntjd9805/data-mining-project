Abstract
The interplay between exploration and exploitation in competitive multi-agent learning is still far from being well understood. Motivated by this, we study smooth
Q-learning, a prototypical learning model that explicitly captures the balance between game rewards and exploration costs. We show that Q-learning always converges to the unique quantal-response equilibrium (QRE), the standard solution concept for games under bounded rationality, in weighted zero-sum polymatrix games with heterogeneous learning agents using positive exploration rates. Com-plementing recent results about convergence in weighted potential games [16, 34], we show that fast convergence of Q-learning in competitive settings obtains regard-less of the number of agents and without any need for parameter fine-tuning. As showcased by our experiments in network zero-sum games, these theoretical results provide the necessary guarantees for an algorithmic approach to the currently open problem of equilibrium selection in competitive multi-agent settings. 1

Introduction
Zero-sum games and variants thereof are arguably amongst the most well studied settings in game theory. Indeed much attention has focused on the class of strictly competitive games [4], i.e., two player games such that when both players change their mixed strategies, then either there is no change in the expected payoffs, or one of the two expected payoffs increases and the other decreases.1
According to Aumann [4], “Strictly competitive games constitute one of the few areas in game theory, and indeed in the social sciences, where a fairly sharp, unique prediction is made." The unique prediction, of course, refers to the min-max solution and the resulting values guaranteed to both agents due to the classic work of von Neumann [53].
Unfortunately, when we move away from the safe haven of two-agent strictly competitive games, a lot of these regularities disappear. For example, in multi-agent variants of zero-sum and strictly competitive games, several critical aspects of the min-max theorem collapse [12]. Critically, Nash
Equilibrium (NE) payoffs need not be unique. In fact, there can be continua of equilibria with the payoff range of different agents corresponding to positive measure sets. Furthermore, NE strategies need not be exchangeable (i.e., mixing-matching strategies from different Nash profiles does not lead to a Nash) nor max-min. Thus, network competition is not only significantly harder, but poses qualitatively different questions than two-agent competition.
Nevertheless, and despite the intense recent interest inspired by Artificial Intelligence (AI) and
Machine Learning (ML) applications such as generative adversarial networks and actor-critic systems to understand learning dynamics in zero-sum games and even network variants thereof [19, 22, 27], so far, there has been no systematic study of how agents should deal with uncertainty of the resulting 1In fact, as recent work has established these strictly competitive games are formally equivalent to weighted zero-sum games, i.e., affine transformations of zero-sum games [1]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
payoffs in such games. In these settings, the use of purely optimization-driven, regret-minimizing algorithms is no longer equally attractive as in the two-player case. The multiplicity of equilibria and the lack of unique value give rise to a non-trivial problem of equilibrium selection and learning agents face the fundamental dilemma between exploration and exploitation [15, 41, 11, 59]. These considerations drive our motivating question:
Are there exploration-exploitation dynamics that provably converge in networks of strictly competitive games? How do they behave in settings with multiple, payoff diverse Nash equilibria?
Model and Results. We study a well-known smooth variant of Q-learning [55, 54], with softmax or Boltzmann exploration, one of the most fundamental models of exploration-exploitation in multi-agent systems (MAS), termed Boltzmann or smooth Q-learning [51, 48]. Informally (see Section 3 for the rigorous definition), each agent k updates their choice distribution x = (xi) according to the rule ˙xi/xi = (ri − ¯r) − Tk(ln xi − (cid:80) j xj ln xj), where ri, ¯r denote agent k’s rewards from action i and average rewards, respectively, given all other agents’ actions and Tk is agent k’s exploration rate.
In our main result, we show convergence of Q-learning to Quantal Response Equilibria (QRE), the prototypical extension of Nash equilibria for games with bounded rationality [36], in multi-agent/network generalizations of strictly competitive games [13, 12]. As long as all exploration rates are positive, we prove via a global Lyapunov argument that the Q-learning dynamic converges pointwise to a unique QRE regardless of initial conditions and regardless of the number of the Nash equilibria of the original network game (Theorem 4.1). Related to the above, we demonstrate how exploration by all agents leads to equilibrium selection. Thus, this long-standing open problem ([32, 45, 43]) becomes tractable in practice due to the theoretical guarantees of fast convergence to
QRE that we provide in Theorem 4.1 for this class of competitive games. In fact, Theorem 4.1 is in some sense tight, as there exist network competitive settings whose dynamics lead to limit cycles if not all of the agents are performing exploration (see experiments and discussion in Section 5).
Other