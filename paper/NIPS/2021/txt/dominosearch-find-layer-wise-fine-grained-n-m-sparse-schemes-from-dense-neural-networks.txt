Abstract
Neural pruning is a widely-used compression technique for Deep Neural Networks (DNNs). Recent innovations in Hardware Architectures (e.g. Nvidia Ampere
Sparse Tensor Core) and N:M ﬁne-grained Sparse Neural Network algorithms (i.e. every M-weights contains N non-zero values) reveal a promising research line of neural pruning. However, the existing N:M algorithms only address the challenge of how to train N:M sparse neural networks in a uniform fashion (i.e. every layer has the same N:M sparsity) and suffer from a signiﬁcant accuracy drop for high sparsity (i.e. when sparsity > 80%). To tackle this problem, we present a novel technique –
DominoSearch to ﬁnd mixed N:M sparsity schemes from pre-trained dense deep neural networks to achieve higher accuracy than the uniform-sparsity scheme with equivalent complexity constraints (e.g. model size or FLOPs). For instance, for the same model size with 2.1M parameters (87.5% sparsity), our layer-wise N:M sparse ResNet18 outperforms its uniform counterpart by 2.1% top-1 accuracy, on the large-scale ImageNet dataset. For the same computational complexity of 227M
FLOPs, our layer-wise sparse ResNet18 outperforms the uniform one by 1.3% top-1 accuracy. Furthermore, our layer-wise ﬁne-grained N:M sparse ResNet50 achieves 76.7% top-1 accuracy with 5.0M parameters. This is competitive to the results achieved by layer-wise unstructured sparsity that is believed to be the upper-bound of Neural Network pruning with respect to the accuracy-sparsity trade-off. We believe that our work can build a strong baseline for further sparse DNN research and encourage future hardware-algorithm co-design work. Our code and models are publicly available at https://github.com/NM-sparsity/DominoSearch. 1

Introduction
Modern deep neural networks (DNNs) achieve remarkable success in various tasks such as computer vision and language processing at a cost of memory footprint, computation, energy and storage. These have all become bottlenecks of deploying DNNs on commodity hardware for real-world applications.
To address these challenges, many model compression techniques have been proposed to optimize
DNN models. Neural network pruning is one promising technique orthogonal to quantization [1–3], network architecture search [4–6] and knowledge distillation [7–9].
Recent innovations in hardware architectures (e.g. Nvidia Ampere Sparse Tensor Core [10]) and
N:M ﬁne-grained sparse DNN pruning algorithms [11, 12] reveal a promising research line of neural
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
pruning that may achieve a high sparse ratio while maintaining a regular sparse structure. On the other hand, traditional unstructured sparsity can achieve a higher sparse ratio but cannot maintain a regular sparse structure, which makes it difﬁcult for acceleration [13]. Traditional structured sparsity can achieve regular sparse structure but may not achieve high compression ratio[11]. Figure 1 illustrates traditional unstructured/structured pruning and N:M ﬁne-grained structured pruning.
Figure 1: (a) Unstructured pruning treats every weight in the dense matrix individually which will produce a sparse matrix with irregular non-zero data. (b) Coarse-grained pruning treats a ﬁlter/channel as an atomic component and can produce a regular sparse matrix. (c) Fine-grained N:M pruning performs unstructured pruning inside each group (group size M = 4 in this example) while it enforces an additional constraint that there are at most N non-zero values to be kept.
Applying uniform ﬁne-grained N:M sparsity training, or ﬁne-tuning to DNNs, will lead to a sub-optimal solution because layers are treated equally. Modern DNNs tend to have a number of layers with a highly non-uniform distribution of redundancy. This well-known phenomenon has been exploited by recent unstructured pruning methods [14–17] that have achieved higher accuracy than the uniform counterparts, especially for relatively high sparsity. However, the existing methods mainly focus on unstructured pruning that treats the weights within a layer individually and does not have ﬁne-grained position constraints, which cannot be used for ﬁne-grained N:M sparsity. As a consequence, a novel method for layer-wise ﬁne-grained N:M sparsity is needed.
Furthermore, sparsity acceleration relies on the underlying software-architecture system of the platform. We can make a reasonable assumption that there will be more but not arbitrary N:M schemes supported in future software-architecture designs. Although the ﬂexibility of N and M is still an open system design research question, we assume a policy of ﬁxed M and ﬂexible N in this paper because it can provide ﬁxed window size (i.e. size = M) for sparse weight matrix compression.
Motivated by above-mentioned challenges, this work introduces an efﬁcient framework to ﬁnd a layer-wise ﬁne-grained N:M scheme with pre-deﬁned N:M candidates which achieves higher accuracy compared to the uniform counterpart with equivalent complexity constraint (e.g. model size or
FLOPs).
We propose DominoSearch, an iterative algorithm that ﬁnds layer-wise ﬁne-grained sparse schemes from pre-trained dense weights. This is achieved using a magnitude-based criterion [14, 18] for the pruning selection threshold, in combination with a weight penalty to drive values towards the threshold. DominoSearch also uses a layer-wise penalty factor to balance the heuristic layer-wise redundancy of parameters[15] and layer-wise computational complexity (e.g. FLOPs). As indicated by the name Domino, the N value from the N:M ratio decrements during the search phase with the weight penalty acting as momentum.
Due to the layer-wise sparsity, our framework is able to achieve state-of-the-art (SOTA) accuracy-model size trade-offs when compared with unstructured sparsity and accuracy-FLOPs trade-offs when compared with structured sparsity. Figure 2 demonstrates the advantages of layer-wise N:M sparsity under both scenarios on ResNet50 [19]. Details can be found in Table 2.
The major contributions of our paper can be summarized as follows: (1) We present DominoSearch -A novel and easy-to-use framework to search the layer-wise sparse schemes from pre-trained dense weights with a speciﬁed model complexity constraint. Our method does not rely on expensive design space exploration such as Network Architecture Search (NAS) or Reinforcement Learning (RL). 2
Figure 2: ResNet50 on ImageNet: Compare top-1 Accuracy of our layer-wise N:M sparse models with SOTA structured/unstructured sparsity under various FLOPs/model sizes.
To the best of our knowledge, this is the ﬁrst work on layer-wise ﬁne-grained N:M sparsity. (2)
We propose a layer-wise penalty factor to balance the parameters redundancy that does not depend on input tensor/image size and complexity that depends on input tensor/image size (e.g. FLOPs).
The computational complexity measurement FLOPs used in this paper can easily be replaced with latency, throughput or energy consumption when deployed on a speciﬁc platform. (3) We evaluate our solution on the large scale ImageNet dataset with both heavy ResNets [19] and compact RegNets [20], and the smaller scale CIFAR100 dataset with ResNet56. Results of ResNet50 on ImageNet show that layer-wise N:M sparsity searched by our framework achieves SOTA accuracy-compression trade-off under ﬁne-grained N:M sparsity. 2