Abstract
Vision transformer (ViT) models exhibit substandard optimizability. In particular, they are sensitive to the choice of optimizer (AdamW vs. SGD), optimizer hyperpa-rameters, and training schedule length. In comparison, modern convolutional neural networks are easier to optimize. Why is this the case? In this work, we conjecture that the issue lies with the patchify stem of ViT models, which is implemented by a stride-p p×p convolution (p = 16 by default) applied to the input image. This large-kernel plus large-stride convolution runs counter to typical design choices of convolutional layers in neural networks. To test whether this atypical design choice causes an issue, we analyze the optimization behavior of ViT models with their original patchify stem versus a simple counterpart where we replace the ViT stem by a small number of stacked stride-two 3×3 convolutions. While the vast majority of computation in the two ViT designs is identical, we ﬁnd that this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the ﬁnal model accuracy.
Using a convolutional stem in ViT dramatically increases optimization stability and also improves peak performance (by ∼1-2% top-1 accuracy on ImageNet-1k), while maintaining ﬂops and runtime. The improvement can be observed across the wide spectrum of model complexities (from 1G to 36G ﬂops) and dataset scales (from ImageNet-1k to ImageNet-21k). These ﬁndings lead us to recommend using a standard, lightweight convolutional stem for ViT models in this regime as a more robust architectural choice compared to the original ViT model design. 1

Introduction
Vision transformer (ViT) models [13] offer an alternative design paradigm to convolutional neural networks (CNNs) [24]. ViTs replace the inductive bias towards local processing inherent in con-volutions with global processing performed by multi-headed self-attention [43]. The hope is that this design has the potential to improve performance on vision tasks, akin to the trends observed in natural language processing [11]. While investigating this conjecture, researchers face another unexpected difference between ViTs and CNNs: ViT models exhibit substandard optimizability.
ViTs are sensitive to the choice of optimizer [41] (AdamW [27] vs. SGD), to the selection of dataset speciﬁc learning hyperparameters [13, 41], to training schedule length, to network depth [42], etc.
These issues render former training recipes and intuitions ineffective and impede research.
Convolutional neural networks, in contrast, are exceptionally easy and robust to optimize. Simple training recipes based on SGD, basic data augmentation, and standard hyperparameter values have been widely used for years [19]. Why does this difference exist between ViT and CNN models? In this paper we hypothesize that the issues lies primarily in the early visual processing performed by
ViT. ViT “patchiﬁes” the input image into p×p non-overlapping patches to form the transformer encoder’s input set. This patchify stem is implemented as a stride-p p×p convolution, with p = 16 as a default value. This large-kernel plus large-stride convolution runs counter to the typical design 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Early convolutions help transformers see better: We hypothesize that the substandard optimizability of ViT models compared to CNNs primarily arises from the early visual processing performed by its patchify stem, which is implemented by a non-overlapping stride-p p×p convolution, with p = 16 by default. We minimally replace the patchify stem in ViT with a standard convolutional stem of only ∼5 convolutions that has approximately the same complexity as a single transformer block. We reduce the number of transformer blocks by one (i.e., L − 1 vs. L) to maintain parity in ﬂops, parameters, and runtime. We refer to the resulting model as ViTC and the original ViT as
ViTP . The vast majority of computation performed by these two models is identical, yet surprisingly we observe that ViTC (i) converges faster, (ii) enables, for the ﬁrst time, the use of either AdamW or SGD without a signiﬁcant accuracy drop, (iii) shows greater stability to learning rate and weight decay choice, and (iv) yields improvements in ImageNet top-1 error allowing ViTC to outperform state-of-the-art CNNs, whereas ViTP does not. choices used in CNNs, where best-practices have converged to a small stack of stride-two 3×3 kernels as the network’s stem (e.g., [30, 36, 39]).
To test this hypothesis, we minimally change the early visual processing of ViT by replacing its patchify stem with a standard convolutional stem consisting of only ∼5 convolutions, see Figure 1.
To compensate for the small addition in ﬂops, we remove one transformer block to maintain parity in
ﬂops and runtime. We observe that even though the vast majority of the computation in the two ViT designs is identical, this small change in early visual processing results in markedly different training behavior in terms of the sensitivity to optimization settings as well as the ﬁnal model accuracy.
In extensive experiments we show that replacing the ViT patchify stem with a more standard convolutional stem (i) allows ViT to converge faster (§5.1), (ii) enables, for the ﬁrst time, the use of either AdamW or SGD without a signiﬁcant drop in accuracy (§5.2), (iii) brings ViT’s stability w.r.t. learning rate and weight decay closer to that of modern CNNs (§5.3), and (iv) yields improvements in ImageNet [10] top-1 error of ∼1-2 percentage points (§6). We consistently observe these improvements across a wide spectrum of model complexities (from 1G ﬂops to 36G ﬂops) and dataset scales (ImageNet-1k to ImageNet-21k).
These results show that injecting some convolutional inductive bias into ViTs can be beneﬁcial under commonly studied settings. We did not observe evidence that the hard locality constraint in early layers hampers the representational capacity of the network, as might be feared [9]. In fact we observed the opposite, as ImageNet results improve even with larger-scale models and larger-scale data when using a convolution stem. Moreover, under carefully controlled comparisons, we ﬁnd that
ViTs are only able to surpass state-of-the-art CNNs when equipped with a convolutional stem (§6).
We conjecture that restricting convolutions in ViT to early visual processing may be a crucial design choice that strikes a balance between (hard) inductive biases and the representation learning ability of transformer blocks. Evidence comes by comparison to the “hybrid ViT” presented in [13], which uses 40 convolutional layers (most of a ResNet-50) and shows no improvement over the default ViT.
This perspective resonates with the ﬁndings of [9], who observe that early transformer blocks prefer to learn more local attention patterns than later blocks. Finally we note that exploring the design of hybrid CNN/ViT models is not a goal of this work; rather we demonstrate that simply using a minimal convolutional stem with ViT is sufﬁcient to dramatically change its optimization behavior.
In summary, the ﬁndings presented in this paper lead us to recommend using a standard, lightweight convolutional stem for ViT models in the analyzed dataset scale and model complexity spectrum as a more robust and higher performing architectural choice compared to the original ViT model design. 2
2