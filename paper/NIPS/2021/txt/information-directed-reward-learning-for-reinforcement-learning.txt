Abstract
For many reinforcement learning (RL) applications, specifying a reward is difﬁcult.
This paper considers an RL setting where the agent obtains information about the reward only by querying an expert that can, for example, evaluate individual states or provide binary preferences over trajectories. From such expensive feedback, we aim to learn a model of the reward that allows standard RL algorithms to achieve high expected returns with as few expert queries as possible. To this end, we propose Information Directed Reward Learning (IDRL), which uses a
Bayesian model of the reward and selects queries that maximize the information gain about the difference in return between plausibly optimal policies. In contrast to prior active reward learning methods designed for speciﬁc types of queries, IDRL naturally accommodates different query types. Moreover, it achieves similar or better performance with signiﬁcantly fewer queries by shifting the focus from reducing the reward approximation error to improving the policy induced by the reward model. We support our ﬁndings with extensive evaluations in multiple environments and with different query types. 1

Introduction
Reinforcement learning (RL; Sutton and Barto, 2018) casts the problem of learning to perform complex tasks by interacting with an environment as an optimization problem where the learning agent aims to maximize its expected cumulative reward. Despite the remarkable successes of
RL (e.g., Mnih et al., 2015; Silver et al., 2016), specifying reward functions that capture complex tasks is still an open problem. A promising approach is to learn a reward function from human feedback (e.g., Christiano et al., 2017). However, since human feedback is expensive, active reward learning aims to minimize the number of queries. Prior work often focuses on approximating the reward function uniformly well. However, this may not be aligned with the original goal of RL: ﬁnding an optimal policy, as Figure 1 shows. Moreover, prior work is often tailored to
∗Work done while at Microsoft Research Cambridge. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
π1
π2
) + ˆr(
ˆG(π1) = ˆr(
ˆG(π2) = ˆr(
) + ˆr(
ˆG(π1) − ˆG(π2) = ˆr(
)
)
) − ˆr(
) argmax
, q∈{ ,
, }
I( ˆG(π1) − ˆG(π2), (q, ˆy)) (cid:123)(cid:122) (cid:125) (cid:124)
IDRL objective
= { ,
}
T = 4
Figure 1: The robot wants to collect food for a human. It can only move 4 timesteps in the gridworld, cannot pass through the black walls, and collecting more food is always better. The robot does not know the human’s preferences, but it can ask for food ratings. Common active learning methods aim to learn
In contrast, IDRL considers only the reward uniformly well, and would query all items similarly often. the two plausibly optimal policies π1 and π2. Since both policies collect the cherry, and do not collect the pear, the robot only needs to learn about the apple and the corn. IDRL can solve the task with 2 queries instead of 4. speciﬁc types of queries, such as comparisons of two trajectories (e.g., Sadigh et al., 2017) or numerical evaluations of trajectories (e.g., Daniel et al., 2015), limiting its applicability.
Contributions. We propose Information Directed Reward Learning (IDRL), a general active reward learning approach for learning a model of the reward function from expensive feedback with the goal of ﬁnding a good policy rather than uniformly reducing the model’s error. IDRL can use arbitrary
Bayesian reward models and arbitrary types of queries (Section 4), making it more general than existing methods. We describe an exact and efﬁcient implementation of IDRL using Gaussian process (GP) reward models (Section 5) and different types of queries, and an approximation of IDRL that uses a deep neural network reward model and a state-of-the-art policy gradient algorithm to learn from comparison queries (Section 6). We evaluate IDRL extensively in simulated environments (Section 7), including a driving task and high-dimensional continuous control tasks in the MuJoCo simulator, and show that both implementations signiﬁcantly outperform prior methods. 2