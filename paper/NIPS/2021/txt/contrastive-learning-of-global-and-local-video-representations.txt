Abstract
Contrastive learning has delivered impressive results for various tasks in the self-supervised regime. However, existing approaches optimize for learning represen-tations speciﬁc to downstream scenarios, i.e., global representations suitable for tasks such as classiﬁcation or local representations for tasks such as detection and localization. While they produce satisfactory results in the intended downstream scenarios, they often fail to generalize to tasks that they were not originally designed for. In this work, we propose to learn video representations that generalize to both the tasks which require global semantic information (e.g., classiﬁcation) and the tasks that require local ﬁne-grained spatio-temporal information (e.g., localization).
We achieve this by optimizing two contrastive objectives that together encour-age our model to learn global-local visual information given audio signals. We show that the two objectives mutually improve the generalizability of the learned global-local representations, signiﬁcantly outperforming their disjointly learned counterparts. We demonstrate our approach on various tasks including action/sound classiﬁcation, lip reading, deepfake detection, event and sound localization.1 1

Introduction
Recent years have seen a surge of interest in contrastive self-supervised learning (CSL) [59, 38, 35, 16] to obtain representations that generalize to various downstream scenarios. In CSL, the choice of
“contrasting views” plays a crucial role because the learned representations capture information shared between different views by maximizing mutual information between them [8]. This makes it critical to design contrastive objectives with the “right” contrasting views tailored for the intended downstream scenarios [77], which has been the focus of many recent works [38, 10, 45, 74, 84, 83].
The progress made so far provides important insights for understanding how to select optimal contrasting views for a given task [77]. However, the current paradigm of designing CSL approaches speciﬁc to any intended (global or local) downstream scenarios could be suboptimal, as in the real-world case the downstream scenarios are generally unknown in advance. This not only limits the generalizability of the learned representations, evaluating the approaches solely on the intended scenarios could produce misleading conclusions. Although existing approaches achieve impressive results in their intended downstream tasks, they often fail to generalize to tasks that they were not originally designed for, e.g., as we show later in our experiments, global representations do not generalize well to tasks such as lip reading [22, 21] which require local spatio-temporal information.
Motivated by this, we take an orthogonal direction to the current CSL approaches: We aim to learn representations agnostic to the types of downstream scenarios and generalize to both the scenarios that require global representations (e.g., classiﬁcation) and scenarios that require local representations (e.g., localization). We focus on learning video representations using the natural 1https://github.com/yunyikristy/global_local 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
audio-visual correspondence as the primary self-supervisory signal. In this scenario, the notion of global/local representations is intertwined in space and time; we can obtain representations that are spatially global or local, and also temporally global or local. However, most existing video
CSL approaches optimize for only global spatio-temporal representations and demonstrate them on audio/visual video classiﬁcation tasks [60, 46, 55, 61]. Part of the difﬁculty here is that formulating a contrastive objective for local representations is not straightforward because of the one-to-many relationship in audio-visual correspondences, i.e., spatially, multiple pixel regions can contribute to the sound in the corresponding audio, and temporally, multiple temporal slices of audio can map to a single video frame due to sampling rate differences. This hinders the development of CSL for local video representations useful for tasks such as sound source separation and lipreading.
In this paper, we present an approach for learning global-local video representations in the CSL frame-work. We design two cross-modal contrastive objectives that collaboratively capture information shared between audio and visual signals. An important aspect of our approach is the factoriza-tion of the spatio-temporal feature space into a spatially-local/temporally-global subspace and a spatially-global/temporally-local subspace, where each of the two contrastive objectives are deﬁned in, respectively; see Fig. 1. The explicit space-time factorization helps each contrastive objective focus on capturing either spatially-local or temporally-local information and thus facilitates learning complementary features from audio-visual correspondence more effectively than in the original spatio-temporal space. Furthermore, we deﬁne both objectives in the multiple instance learning framework [24, 52] to handle the one-to-many relationship between audio and visual signals. This helps the model learn representations without knowing ﬁne-grained audio-visual correspondence.
We evaluate our approach on various downstream tasks that need local spatio-temporal information, i.e., lip reading [22, 21, 3], deep-fake detection [25] and audio-visual event localization [76], and also discriminative tasks that needs global information, i.e., audio/visual video classiﬁcation [71, 47, 63, 41]. We show that the same pretrained model successfully generalizes to all our scenarios without having to re-pretrain it using different objectives and/or datasets. Furthermore, we demonstrate that the two contrastive objectives mutually beneﬁts each other and helps improve the generalizability of both global and local representations. To the best of our knowledge, our work is the ﬁrst to demonstrate a CSL approach that learns video representations that generalize to both global and local video understanding scenarios. 2