Abstract
We consider the problem of ofﬂine reinforcement learning (RL) — a well-motivated setting of RL that aims at policy optimization using only historical data. Despite its wide applicability, theoretical understandings of ofﬂine RL, such as its optimal sample complexity, remain largely open even in basic settings such as tabular
Markov Decision Processes (MDPs). In this paper, we propose Off-Policy Double
Variance Reduction (OPDVR), a new variance reduction based algorithm for ofﬂine
RL. Our main result shows that OPDVR provably identiﬁes an (cid:15)-optimal policy with (cid:101)O(H 2/dm(cid:15)2) episodes of ofﬂine data in the ﬁnite-horizon stationary transition setting, where H is the horizon length and dm is the minimal marginal state-action distribution induced by the behavior policy. This improves over the best known upper bound by a factor of H. Moreover, we establish an information-theoretic lower bound of Ω(H 2/dm(cid:15)2) which certiﬁes that OPDVR is optimal up to logarithmic factors. Lastly, we show that OPDVR also achieves rate-optimal sample complexity under alternative settings such as the ﬁnite-horizon MDPs with non-stationary transitions and the inﬁnite horizon MDPs with discounted rewards. 1

Introduction
Ofﬂine reinforcement learning (ofﬂine RL) aims at learning the near-optimal policy by using a static ofﬂine dataset that is collected by a certain behavior policy µ [Lange et al., 2012]. As ofﬂine RL agent has no access to interact with the environment, it is more widely applicable to problems where online interaction is infeasible, e.g. when trials-and-errors are expensive (robotics, education), risky (autonomous driving) or even unethical (healthcare) [see,e.g., a recent survey Levine et al., 2020].
Despite its practical signiﬁcance, a precise theoretical understanding of ofﬂine RL has been lacking.
Previous sample complexity bounds for RL has primarily focused on the online setting [Azar et al., 2017, Jin et al., 2018, Zanette and Brunskill, 2019, Simchowitz and Jamieson, 2019, Efroni et al., 2019, Cai et al., 2019] or the generative model (simulator) setting [Azar et al., 2013, Sidford et al., 2018a,b, Yang and Wang, 2019, Agarwal et al., 2020, Wainwright, 2019, Lattimore and Szepesvari, 2019], both of which assuming interactive access to the environment and not applicable to ofﬂine RL.
On the other hand, the sample complexity of ofﬂine RL remains unsettled even for environments with
ﬁnitely many state and actions, a.k.a, the tabular MDPs (Markov Decision Processes). One major line of work is concerned with the off-policy evaluation (OPE) problem [Li et al., 2015, Jiang and Li, 2016, Liu et al., 2018, Kallus and Uehara, 2019a,b, Uehara and Jiang, 2019, Xie et al., 2019, Yin and
Wang, 2020, Duan and Wang, 2020]. These works provide sample complexity bounds for evaluating the performance of a ﬁxed policy, and do not imply guarantees for policy optimization. Another line of work studies the sample complexity of ofﬂine policy optimization in conjunction with function 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison of sample complexities for tabular ofﬂine RL interpretation.
Method/Analysis
BFVT [Xie and Jiang, 2020a]
Setting
∞-horizon
MBS-PI/QI [Liu et al., 2020b]
∞-horizon
∞-horizon
Le et al. [2019]
∞-horizon
FQI [Chen and Jiang, 2019]
MSBO/MABO [Xie and Jiang, 2020b] ∞-horizon
OPEMA [Yin et al., 2021]
OPDVR (Section 3)
OPDVR (Section 4)
OPDVR (Section 4.2)
Assumptions only realizability + MDP concentrabilityb completeness+bounded den-sity estimation error
Full Concentrability
Full Concentrability
Full Concentrability
Full Concentrability
Sample complexitya
˜O((1 − γ)−8C 2/(cid:15)4)
˜O((1 − γ)−8C 2/(cid:15)2)
˜O((1 − γ)−6βµ/(cid:15)2)
˜O((1 − γ)−6C/(cid:15)2) (cid:101)O((1 − γ)−4Cµ/(cid:15)2) (cid:101)O(H 3/dm(cid:15)2) (cid:101)O(H 3/dm(cid:15)2) (cid:101)O(H 2/dm(cid:15)2) (cid:101)O((1 − γ)−3/dm(cid:15)2)
H-horizon non-stationary
H-horizon non-stationary Weak Coverage
H-horizon stationary
Weak Coverage
∞-horizon
Weak Coverage a Number of episodes in the ﬁnite horizon setting and number of steps in the inﬁnite horizon. b βµ, C, Cµ, 1/dm are the concentrability-type coefﬁcients that measure the state-action coverage. See
Assumption 2.1 and also Section F.2 for discussions. approximation [Chen and Jiang, 2019, Xie and Jiang, 2020b,a, Jin et al., 2020]. These results apply to ofﬂine RL with general function classes, but when specialized to the tabular setting, they give rather loose sample complexity bounds with suboptimal dependencies on various parameters 1.
The recent work of Yin et al. [2021] showed that the optimal sample complexity for ﬁnding an (cid:15)-optimal policy in ofﬂine RL is (cid:101)O(H 3/dm(cid:15)2) in the ﬁnite-horizon non-stationarysetting (with matching upper and lower bounds), where H is the horizon length and dm is a constant related to the data coverage of the behavior policy in the given MDP. However, the optimal sample complexity in alternative settings such as stationary transition or inﬁnite-horizon settings remains unknown. Further, the (cid:101)O(H 3/dm(cid:15)2) sample complexity is achieved by an off-policy evaluation + uniform convergence type algorithm; other more practical algorithms including (stochastic) optimal planning algorithms such as Q-Learning are not well understood in ofﬂine RL.
Our Contributions In this paper, we propose an algorithm OPDVR (Off-Policy Doubled Variance
Reduction) for ofﬂine reinforcement learning based on an extension of the variance reduction technique initiated in [Sidford et al., 2018a, Yang and Wang, 2019]. OPDVR performs stochastic (minibatch style) value iterations using the available ofﬂine data, and can be seen as a version of stochastic optimal planning that interpolates value iteration and Q-learning. Our main contributions are summarized as follows.
• We show that OPDVR ﬁnds an (cid:15)-optimal policy with high probability using (cid:101)O(H 2/dm(cid:15)2) episodes of ofﬂine data (Section 4.1). This improves upon the best known sample complexity by an H factor and to the best of our knowledge is the ﬁrst that achieves an O(H 2) horizon dependence, thus separating the stationary case with the non-stationary case for ofﬂine RL.
• We establish a sample (episode) complexity lower bound Ω(H 2/dm(cid:15)2) for ofﬂine RL in the ﬁnite-horizon stationary setting (Theorem 4.2), showing that the sample complexity of
OPDVR is optimal up to logarithmic factors.
• In the ﬁnite-horizon non-stationary setting, and inﬁnite horizon γ-discounted setting, we show that OPDVR achieves (cid:101)O(H 3/dm(cid:15)2) sample (episode) complexity (Section 3) and (cid:101)O((1 − γ)−3/dm(cid:15)2) sample complexity (Section 4.2) respectively. They are both optimal up to logarithmic factors and our inﬁnite-horizon result improves over the best known results, e.g., those derived for the ﬁtted Q-iteration style algorithms [Xie and Jiang, 2020b].
• On the technical end, our algorithm presents a sharp analysis of ofﬂine RL with stationary transitions, and, importantly, the use of the doubling technique to resolve the initialization dependence defect which fails to make the original variance reduction algorithm of [Sidford et al., 2018a] to be optimal, see Appendix F.4. Running [Sidford et al., 2018a] may not yield the desired accuracy as they stated and our result is robust in preserving the optimality.