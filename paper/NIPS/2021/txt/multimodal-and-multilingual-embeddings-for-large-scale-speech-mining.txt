Abstract
We present an approach to encode a speech signal into a ﬁxed-size representation which minimizes the cosine loss with the existing massively multilingual LASER text embedding space. Sentences are close in this embedding space, independently of their language and modality, either text or audio. Using a similarity metric in that multimodal embedding space, we perform mining of audio in German,
French, Spanish and English from Librivox against billions of sentences from
Common Crawl. This yielded more than twenty thousand hours of aligned speech translations. To evaluate the automatically mined speech/text corpora, we train neural speech translation systems for several languages pairs. Adding the mined data, achieves signiﬁcant improvements in the BLEU score on the CoVoST2 and the MUST-C test sets with respect to a very competitive baseline.
Our approach can also be used to directly perform speech-to-speech mining, with-out the need to ﬁrst transcribe or translate the data. We obtain more than one thousand three hundred hours of aligned speech in French, German, Spanish and
English. This speech corpus has the potential to boost research in speech-to-speech translation which suffers from scarcity of natural end-to-end training data. All the mined multimodal corpora will be made freely available. 1

Introduction
Neural approaches have evolved to the de-facto standard in machine translation (MT), namely text-to-text and speech-to-text translation (Berard et al., 2016; Weiss et al., 2017), and more recently speech-to-speech translation (Jia et al., 2019). While there is very promising research on unsupervised
MT, e.g. (Lample et al., 2018; Artetxe et al., 2017), or initialization of parts of the MT system by approaches trained in a self-supervised manner, labeled data remains extremely useful to achieve best performance. This labeled data is either used to directly train the MT system end-to-end, if enough data is available, or to ﬁne-tune an MT system which parts have been initialized with some self-supervised methods. This type of approaches is commonly used in speech-to-text translation, see for example (Tran et al., 2020; Li et al., 2020).
In the case of text-to-text (t2t) machine translation, the labeled data consists of source/target sentence pairs, commonly named bitexts or parallel data. There are several international organisations which produce human translation in several languages, best known are probably the European Commission and the United Nations. These human translations have been collected and are distributed to the MT community, e.g. the well known Europarl (Koehn, 2005) and UN corpora (Ziemski et al., 2016).
There is also a large body of research which aims in ﬁnding existing translation in huge collections of monolingual texts, which are abundant on the the Internet for many languages. This bitext mining has provided huge amounts of parallel data, in particular the ParaCrawl project (Esplà et al., 2019) or the CCMatrix initiative (Schwenk et al., 2021). This data enabled training a multilingual t2t 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
MT systems for hundred languages (Fan et al., 2020). We are not aware of any attempts to mine for speech translation data.
Training speech-to-text (s2t) MT systems requires audio in the source language and its translation in the target language. To the best of our knowledge, this type of data is mainly generated by humans volunteers: speech is ﬁrst transcribed and then translated into a another language – a time and labour extensive process. When translations are not provided at the sentence level, forced alignments are necessary. These methodologies helped creating well-known speech translation corpora, like Must-C (Di Gangi et al., 2019a), CoVoST (Wang et al., 2020a) and EuroparlST (Iranzo-Sánchez et al., 2020).
Finally, speech-to-speech translation (s2s) is faced with extreme scarcity of labeled data, i.e. speech in the source language and the spoken translation in the target language. Previous work on speech-to-speech translation often use synthesized speech targets to overcome this lack of labeled data. We are only aware of one s2s training corpus (Wang et al., 2021) which provides speech-to-speech alignments for 16 source languages with 5 target languages. However, the target aligned speech is oral interpretation of the source speech. The quality of these oral interpretations for speech-to-speech translation have not been evaluated yet.
In this work, we build on recent advances to perform bitext mining based on a similarity measure in a ﬁxed-size sentence embedding space (Schwenk, 2018; Artetxe and Schwenk, 2019; Feng et al., 2020). The underlying idea of those approaches is to learn a multilingual sentence embedding with the property that sentences with similar meaning, in the same or a different language, are close in the embedding space. Then, mining for translations can be performed by calculating the cosine distance, or a margin-based criterion, in this multilingual embedding space for a large collection of sentences in several languages. Sentence pairs with a distance below a threshold are considered to be parallel.
We extend this idea to the speech modality and present a ﬁxed-size embedding of a speech input, of arbitrary length. Our current audio encoder supports speech in ﬁve languages, namely English,
French, Spanish, German and Russian. We apply a teacher-student training approach which yields speech embeddings which are compatible with an existing multilingual text encoder, namely the freely available LASER encoder. This means that we can mine speech input against the 80 languages which are supported by LASER, as used in the bitext mining project CCMatrix.
The contributions of this paper are as follows:
• We explore several approaches to learn multimodal (speech/text) representations for several languages;
• We use these multimodal embeddings to mine more than twenty thousand hours of speech in four languages from Librivox against texts from Common Crawl;
• We add the automatically mined data to train speech-to-text translation systems and obtain signiﬁcant improvements with respect to a very strong baseline;
• We also provide a proof of concept that our approach can be used to directly perform speech-to-speech mining, without the need to transcribe and translate the input speech;
The paper is structured as follows. In the next section, we ﬁrst summarize related work. In Section 3 we describe our approach in detail, followed by a detailed experimental evaluation in Section 4. The paper concludes with a summary and directions of future research. 2