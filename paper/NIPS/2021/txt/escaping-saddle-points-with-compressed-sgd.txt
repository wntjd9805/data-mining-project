Abstract
Stochastic gradient descent (SGD) is a prevalent optimization technique for large-scale distributed machine learning. While SGD computation can be efﬁciently divided between multiple machines, communication typically becomes a bottleneck in the distributed setting. Gradient compression methods can be used to alleviate this problem, and a recent line of work shows that SGD augmented with gradient compression converges to an ε-ﬁrst-order stationary point. In this paper we extend these results to convergence to an ε-second-order stationary point (ε-SOSP), which is to the best of our knowledge the ﬁrst result of this type. In addition, we show that, when the stochastic gradient is not Lipschitz, compressed SGD with RAN-DOMK compressor converges to an ε-SOSP with the same number of iterations as uncompressed SGD [25], while improving the total communication by a factor of
˜Θ( dε−3/4), where d is the dimension of the optimization problem. We present additional results for the cases when the compressor is arbitrary and when the stochastic gradient is Lipschitz.
√ 1

Introduction
Stochastic Gradient Descent (SGD) and its variants are the main workhorses of modern machine learning. Distributed implementations of SGD on a cluster of machines with a central server and a large number of workers are frequently used in practice due to the massive size of the data. In distributed SGD each machine holds a copy of the model and the computation proceeds in rounds.
In every round, each worker ﬁnds a stochastic gradient based on its batch of examples, the server averages these stochastic gradients to obtain the gradient of the entire batch, makes an SGD step, and broadcasts the updated model parameters to the workers. With a large number of workers, computation parallelizes efﬁciently while communication becomes the main bottleneck [12, 38], since each worker needs to send its gradients to the server and receive the updated model parameters.
Common solutions for this problem include: local SGD and its variants, when each machine performs multiple local steps before communication [36]; decentralized architectures which allow pairwise communication between the workers [30] and gradient compression, when a compressed version of the gradient is communicated instead of the full gradient [6, 37, 27]. In this work, we consider the latter approach, which we refer to as compressed SGD.
Most machine learning models can be described by a d-dimensional vector of parameters x and the model quality can be estimated as a function f (x). Hence optimization of the model parameters can be cast a minimization problem minx f (x), where f : Rd → R is a continuous function, which can be optimized using continuous optimization techniques, such as SGD. Fast convergence of compressed
SGD to a ﬁrst-order stationary point (FOSP, (cid:107)∇f (x)(cid:107) < ε) was shown recently for various gradient compression schemes [6, 37, 27, 23, 2]. However, even an exact FOSP can be either a local minimum, a saddle point or a local maximum. While local minima often correspond to good solutions in machine learning applications [21, 39, 7], saddle points and local maxima are always suboptimal and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
it is important for an optimization algorithm to avoid converging to them. In particular, [13] show that for neural networks many local minima are almost optimal, but the corresponding loss functions have a combinatorial explosion in the number of saddle points. Furthermore, [15] show that saddle points can signiﬁcantly slow down SGD convergence and hence it is important to be able to escape from them efﬁciently.
Since ﬁnding a local minimum is NP-hard in general [5], a common relaxation of this requirement is to ﬁnd an approximate second-order stationary point (SOSP), i.e. a point with a small gradi-ent norm ((cid:107)∇f (x)(cid:107) < ε) and the smallest (negative) eigenvalue being small in absolute value (λmin(∇2f (x)) > −εH ). When f has ρ-Lipschitz Hessian (i.e. (cid:107)∇2f (x) − ∇2f (y)(cid:107) ≤ ρ(cid:107)x − y(cid:107)
ρε [32], and such approximate SOSP is commonly referred for all x, y), a standard choice of εH is as an ε-SOSP. While second-order optimization methods allow one to escape saddle points, such methods are typically substantially more expensive computationally. A line of work originating with the breakthrough of [20] shows that ﬁrst-order methods can escape saddle points when perturbations are added at certain iterations. In particular, a follow-up [25] show that SGD converges to an ε-SOSP in an almost optimal number of iterations.
√
In this paper, we show that even compressed SGD can efﬁciently converge to an ε-SOSP. To the best of our knowledge, this is the ﬁrst result showing convergence of compressed methods to a second-order stationary point. 1.1