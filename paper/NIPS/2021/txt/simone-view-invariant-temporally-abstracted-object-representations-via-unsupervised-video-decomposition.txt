Abstracted
Object Representations via Unsupervised Video
Decomposition
Rishabh Kabra1, Daniel Zoran1, Goker Erdogan1, Loic Matthey1
Antonia Creswell1, Matthew Botvinick1, Alexander Lerchner1, Christopher P. Burgess2∗ 1DeepMind, 2Wayve, ∗Work done at DeepMind
{rkabra, danielzoran, gokererdogan, lmatthey, tonicreswell, botvinick, lerchner}@deepmind.com, chrisburgess@wayve.ai
Abstract
To help agents reason about scenes in terms of their building blocks, we wish to extract the compositional structure of any given scene (in particular, the con-ﬁguration and characteristics of objects comprising the scene). This problem is especially difﬁcult when scene structure needs to be inferred while also estimating the agent’s location/viewpoint, as the two variables jointly give rise to the agent’s observations. We present an unsupervised variational approach to this problem.
Leveraging the shared structure that exists across different scenes, our model learns to infer two sets of latent representations from RGB video input: a set of "object" latents, corresponding to the time-invariant, object-level contents of the scene, as well as a set of "frame" latents, corresponding to global time-varying elements such as viewpoint. This factorization of latents allows our model, SIMONe, to represent object attributes in an allocentric manner which does not depend on viewpoint.
Moreover, it allows us to disentangle object dynamics and summarize their trajec-tories as time-abstracted, view-invariant, per-object properties. We demonstrate these capabilities, as well as the model’s performance in terms of view synthesis and instance segmentation, across three procedurally generated video datasets. 1

Introduction
The problem of unsupervised visual scene understanding has become an increasingly central topic in machine learning [1, 2]. The attention is merited by potential gains to reasoning, autonomous navigation, and myriad tasks. However, within the current literature, different studies frame the problem in different ways. One approach aims to decompose images into component objects and object features, supporting (among other things) generation of alternative data that permits insertion, deletion, or repositioning of individual objects [3–6]. Another approach aims at a very different form of decomposition—between allocentric scene structure and a variable viewpoint—supporting generation of views of a scene from new vantage points [7–9] and, if not supplied as input, estimation of camera pose [10]. Although there is work pursuing both of these approaches concurrently in the supervised setting [11–13], very few previous studies have approached the combined challenge in the unsupervised case. In this work, we introduce the Sequence-Integrating Multi-Object Net (SIMONe), a model which pursues that goal of object-level and viewpoint-level scene decomposition and synthesis without supervision. SIMONe is designed to handle these challenges without privileged information concerning camera pose, and in dynamic scenes.
Given a video of a scene our model is able to decouple scene structure from viewpoint information (see Figure 1). To do so, it utilizes video-based cues, and a structured latent space which separates 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Decomposition (A): SIMONe factorizes a scene sequence X into scene content (“object latents,” constant across the sequence) and view/global content (“frame latents,” one per frame) without supervision. Its spatio-temporal attention-based inference naturally allows stable object tracking (e.g. the green sphere is assigned the same segment across frames). Recomposition (B):
Object latents of a given sequence X can be recomposed with the frame latents of a different (i.i.d.) sequence X(cid:48) to generate a consistent rendering of the same scene (i.e. objects and their properties, relative arrangements, and segmentation assignments) from entirely different viewpoints. Notice that both camera pose and lighting are transferred, as evidenced by the wall corners in the background and the shadows of the green sphere. time-invariant per-object features from time-varying global features. These features are inferred using a transformer-based network which integrates information jointly across space and time.
Second, our method seeks to summarize objects’ dynamics. It learns to disentangle not only static object attributes (and their 2D spatial masks), but also object trajectories, without any prior notion of these objects, from videos alone. The learnt trajectory features are temporally abstract and per object; they are captured independently of the dynamics of camera pose, which being a global property, is captured in the model’s per-frame (time-varying) latents.1
Our model thus advances the state of the art in unsupervised, object-centric scene understanding by satisfying the following desiderata: (1) decomposition of multi-object scenes from RGB videos alone; (2) handling of changing camera pose, and simultaneous inference of scene contents and viewpoint from correlated views (i.e. sequential observations of a moving agent); (3) learning of structure across diverse scene instances (i.e. procedurally sampled contents); (4) object representations which summarize static object attributes like color or shape, view-dissociated properties like position or size, as well as time-abstracted trajectory features like direction of motion; (5) no explicit assumptions of 3D geometry, no explicit dynamics model, no specialized renderer, and few a priori modeling assumptions about the objects being studied; and (6) simple, scalable modules (for inference and rendering) to enable large-scale use. 2