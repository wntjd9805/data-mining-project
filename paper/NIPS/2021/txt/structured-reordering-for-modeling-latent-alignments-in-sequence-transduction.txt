Abstract
Despite success in many domains, neural models struggle in settings where train and test examples are drawn from different distributions. In particular, in contrast to humans, conventional sequence-to-sequence (seq2seq) models fail to generalize systematically, i.e., interpret sentences representing novel combinations of concepts (e.g., text segments) seen in training. Traditional grammar formalisms excel in such settings by implicitly encoding alignments between input and output segments, but are hard to scale and maintain. Instead of engineering a grammar, we directly model segment-to-segment alignments as discrete structured latent variables within a neural seq2seq model. To efﬁciently explore the large space of alignments, we introduce a reorder-ﬁrst align-later framework whose central component is a neural reordering module producing separable permutations. We present an efﬁcient dynamic programming algorithm performing exact marginal and MAP inference of separable permutations, and, thus, enabling end-to-end differentiable training of our model. The resulting seq2seq model exhibits better systematic generalization than standard models on synthetic problems and NLP tasks (i.e., semantic parsing and machine translation). 1

Introduction
Recent advances in deep learning have led to ma-jor progress in many domains, with neural models sometimes achieving or even surpassing human perfor-mance [49]. However, these methods often struggle in out-of-distribution (ood) settings where train and test ex-amples are drawn from different distributions. In partic-ular, unlike humans, conventional sequence-to-sequence (seq2seq) models, widely used in natural language pro-cessing (NLP), fail to generalize systematically [4, 27, 28], i.e., correctly interpret sentences representing novel combi-nations of concepts seen in training. Our goal is to provide a mechanism for encouraging systematic generalization in seq2seq models.
To get an intuition about our method, consider the semantic parsing task shown in Figure 1. A learner needs to map a natural language (NL) utterance to a program which can then be executed on a knowledge base. To process the test utterance, the learner needs to ﬁrst decompose it into two segments previously observed in training (shown in green and blue), and then combine their corresponding program fragments to create a new
Figure 1: A semantic parser needs to generalize to test examples which con-tain segments from multiple training ex-amples (shown in green and blue). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
program. Current seq2seq models fail in this systematic generalization setting [12, 24]. In contrast, traditional grammar formalisms decompose correspondences between utterances and programs into compositional mappings of substructures [46], enabling grammar-based parsers to recombine rules acquired during training, as needed for systematic generalization. Grammars have proven essential in statistical semantic parsing in the pre-neural era [51, 57], and have gained renewed interest now as a means of achieving systematic generalization [18, 41]. However, grammars are hard to create and maintain (e.g., requiring grammar engineering or grammar induction stages) and do not scale well to NLP problems beyond semantic parsing (e.g., machine translation). In this work, we argue that the key property of grammar-based models, giving rise to their improved ood performance, is that a grammar implicitly encodes alignments between input and output segments. For example, in Figure 1, the expected segment-level alignments are ‘the length → len’ and ‘the longest river → longest(river(all))’. The encoded alignments allow for explicit decomposition of input and output into segments, and consistent mapping between input and output segments. In contrast, decision rules employed by conventional seq2seq models do not exhibit such properties. For example, recent work [15] shows that primitive units such as words are usually inconsistently mapped across different contexts, preventing these models from generalizing primitive units to new contexts. Instead of developing a full-ﬂedged grammar-based method, we directly model segment-level alignments as structured latent variables. The resulting alignment-driven seq2seq model remains end-to-end differentiable, and, in principle, applicable to any sequence transduction problem.
Modeling segment-level alignments requires simultaneously inducing a segmentation of input and output sequences and discovering correspondences between the input and output segments. While segment-level alignments have been previously incorporated in neural models [50, 55], to maintain tractability, these approaches support only monotonic alignments. The monotonicity assumption is reasonable for certain tasks (e.g., summarization), but it is generally overly restrictive (e.g., consider semantic parsing and machine translation). To relax this assumption, we complement monotonic alignments with an extra reordering step. That is, we ﬁrst permute the source sequence so that segments within the reordered sequence can be aligned monotonically to segments of the target sequence. Coupling latent permutations with monotonic alignments dramatically increases the space of admissible segment alignments.
The space of general permutations is exceedingly large, so, to allow for efﬁcient training, we restrict ourselves to separable permutations [5]. We model separable permutations as hierarchical reordering of segments using permutation trees. This hierarchical way of modeling permutations reﬂects the hierarchical nature of language and hence is arguably more appropriate than ‘ﬂat’ alternatives [33].
Interestingly, recent studies [45, 47] demonstrated that separable permutations are sufﬁcient for capturing the variability of permutations in linguistic constructions across natural languages, providing further motivation for our modeling choice.
Simply marginalizing over all possible separable permutations remains intractable. Instead, inspired by recent work on modeling latent discrete structures [9, 14], we introduce a continuous relaxation of the reordering problem. The key ingredients of the relaxation are two inference strategies: marginal inference, which yields the expected permutation under a distribution; MAP inference, which returns the most probable permutation. In this work, we propose efﬁcient dynamic programming algorithms to perform exact marginal and MAP inference with separable permutations, resulting in effective differentiable neural modules producing relaxed separable permutations. By plugging these modules into an existing module supporting monotonic segment alignments [55], we obtain end-to-end differentiable seq2seq models, supporting non-monotonic segment-level alignments.
In summary, our contributions are:
• A general seq2seq model for NLP tasks that accounts for latent non-monotonic segment-level alignments.
• Novel and efﬁcient algorithms for exact marginal and MAP inference with separable permu-tations, allowing for end-to-end training using a continuous relaxation.1
• Experiments on synthetic problems and NLP tasks (semantic parsing and machine transla-tion) showing that modeling segment alignments is beneﬁcial for systematic generalization. 1Our code and data are available at https://github.com/berlino/tensor2struct-public. 2
2