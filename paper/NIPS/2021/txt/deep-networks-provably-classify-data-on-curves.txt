Abstract
Data with low-dimensional nonlinear structure are ubiquitous in engineering and scientiﬁc problems. We study a model problem with such structure—a binary classiﬁcation task that uses a deep fully-connected neural network to classify data drawn from two disjoint smooth curves on the unit sphere. Aside from mild regularity conditions, we place no restrictions on the conﬁguration of the curves.
We prove that when (i) the network depth is large relative to certain geometric properties that set the difﬁculty of the problem and (ii) the network width and number of samples are polynomial in the depth, randomly-initialized gradient descent quickly learns to correctly classify all points on the two curves with high probability. To our knowledge, this is the ﬁrst generalization guarantee for deep networks with nonlinear data that depends only on intrinsic data properties. Our analysis proceeds by a reduction to dynamics in the neural tangent kernel (NTK) regime, where the network depth plays the role of a ﬁtting resource in solving the classiﬁcation problem. In particular, via ﬁne-grained control of the decay properties of the NTK, we demonstrate that when the network is sufﬁciently deep, the NTK can be locally approximated by a translationally invariant operator on the manifolds and stably inverted over smooth functions, which guarantees convergence and generalization. 1

Introduction
In applied machine learning, engineering, and the sciences, we are frequently confronted with the problem of identifying low-dimensional structure in high-dimensional data. In certain well-structured data sets, identifying a good low-dimensional model is the principal task: examples include convolutional sparse models in microscopy [43] and neuroscience [10, 16], and low-rank models in collaborative ﬁltering [7, 8]. Even more complicated datasets from problems such as image classiﬁcation exhibit some form of low-dimensionality: recent experiments estimate the effective dimension of CIFAR-10 as 26 and the effective dimension of ImageNet as 43 [61]. The variability in these datasets can be thought of as comprising two parts: a “probabilistic” variability induced by the distribution of geometries associated with a given class, and a “geometric” variability associated with physical nuisances such as pose and illumination. The former is challenging to model analytically; virtually all progress on this issue has come through the introduction of large datasets and high-capacity learning machines. The latter induces a much cleaner analytical structure: transformations of a given image lie near a low-dimensional submanifold of the image space (Figure 1). The celebrated successes of convolutional neural networks in image classiﬁcation seem to derive from their ability to simultaneously handle both types of variability. Studying how neural networks compute with data lying near a low-dimensional manifold is an essential step towards understanding how neural 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
networks achieve invariance to continuous transformations of the image domain, and towards the longer term goal of developing a more comprehensive mathematical understanding of how neural networks compute with real data. At the same time, in some scientiﬁc and engineering problems, classifying manifold-structured data is the goal—one example is in gravitational wave astronomy [22, 30], where the goal is to distinguish true events from noise, and the events are generated by relatively simple physical systems with only a few degrees of freedom.
Figure 1: Low-dimensional structure in image data and the two curves problem. Left: Manifold structure in natural images arises due to invariance of the label to continuous domain transformations such as translations and rotations. Right: The two curve problem. We train a neural network to classify points sampled from a density ρ on the submanifolds of the unit sphere. We illustrate the angle injectivity radius ∆ and curvature 1/κ. These parameters help to control the difﬁculty of the problem: problems with smaller separation and larger curvature are more readily separated with deeper networks.
M+,
M−
Motivated by these long term goals, in this paper we study the multiple manifold problem (Figure 1), a mathematical model problem in which we are presented with a ﬁnite set of labeled samples lying on disjoint low-dimensional submanifolds of a high-dimensional space, and the goal is to correctly classify every point on each of the submanifolds—a strong form of generalization. The central mathematical question is how the structure of the data (properties of the manifolds such as dimension, curvature, and separation) inﬂuences the resources (data samples, and network depth and width) required to guarantee generalization. Our main contribution is the ﬁrst end-to-end analysis of this problem for a nontrivial class of manifolds: one-dimensional smooth curves that are non-intersecting, cusp-free, and without antipodal pairs of points. Subject to these constraints, the curves can be oriented essentially arbitrarily (say, non-linearly-separably, as in Figure 1), and the hypotheses of our results depend only on architectural resources and intrinsic geometric properties of the data. To our knowledge, this is the ﬁrst generalization result for training a deep nonlinear network to classify structured data that makes no a-priori assumptions about the representation capacity of the network or about properties of the network after training.
Our analysis proceeds in the neural tangent kernel (NTK) regime of training, where the network is wide enough to guarantee that gradient descent can make large changes in the network output while making relatively small changes to the network weights. This approach is inspired by the recent work
[57], which reduces the analysis of generalization in the one-dimensional multiple manifold problem to an auxiliary problem called the certiﬁcate problem. Solving the certiﬁcate problem amounts to proving that the target label function lies near the stable range of the NTK. The existence of certiﬁcates (and more generally, the conditions under which practically-trained neural networks can
ﬁt structured data) is open, except for a few very simple geometries which we will review below—in particular, [57] leaves this question completely open. Our technical contribution is to show that setting the network depth sufﬁciently large relative to intrinsic properties of the data guarantees the existence of a certiﬁcate (Theorem 3.1), resolving the one-dimensional case of the multiple manifold problem for a broad class of curves (Theorem 3.2). This leads in turn to a novel perspective on the role of the network depth as a ﬁtting resource in the classiﬁcation problem, which is inaccessible to shallow networks. 2
1.1