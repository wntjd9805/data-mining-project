Abstract
The rapid increase in sizes of state-of-the-art DNN models, and consequently the increase in the compute and memory requirements of model training, has led to the development of many execution schemes such as data parallelism, pipeline model parallelism, tensor (intra-layer) model parallelism, and various memory-saving optimizations. However, no prior work has tackled the highly complex problem of optimally partitioning the DNN computation graph across many accelerators while combining all these parallelism modes and optimizations. In this work, we introduce Piper, an efﬁcient optimization algorithm for this problem that is based on a two-level dynamic programming approach. Our two-level approach is driven by the insight that being given tensor-parallelization techniques for individual layers (e.g., Megatron-LM’s splits for transformer layers) signiﬁcantly reduces the search space and makes the global problem tractable, compared to considering tensor-parallel conﬁgurations for the entire DNN operator graph. 1

Introduction
Deep Neural Network (DNN) models have grown exponentially in size over the past two decades; consequently, modern DNNs are extremely computationally expensive. For example, a state-of-the-art language model, GPT-3 [2], has an astounding 175 billion parameters, requiring 314 ZettaFLOPS (3.14 × 1023 FLOPS) of compute.
This trend of computationally expensive models with working-set sizes for training and inference that far exceed the memory capacities of individual DNN accelerators (e.g., GPUs, TPUs, FPGAs,
ASICs) has reinvigorated the study of parallel training techniques that split DNN model state across different devices. In the “modern era”, such model-parallel training techniques trace their roots back to AlexNet [14] and early inﬂuential systems such as DistBelief [6] and Project Adam [3].
Recent advances in model-parallel training include tensor (intra-layer) model parallelism [22], where individual operators of the model are partitioned over multiple workers, and pipeline model parallelism that combines inter-layer model parallelism with pipelining [17, 18, 11]. Various hybrid parallelism techniques that combine tensor, pipeline, and data parallelism across operators [13, 19] have also been developed to more efﬁciently train a wider range of models. Additionally, memory-saving optimizations like activation recomputation, which trade off throughput for memory, are also being used [18] to scale training to larger models such as GPT-3.
Combining these dimensions, however, is non-trivial [19], since each dimension has trade-offs with respect to computational efﬁciency, amount of communication, and memory footprint. Given the importance of efﬁcient model-parallel training (and inference [7, 4]), partitioning a model across 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
devices has thus received recent interest, evolving from a manual process driven by human experts to using algorithms for automated device placement of DNN operators. Focusing on algorithms, some take a black-box approach using learning-based techniques [16, 15, 8, 1, 26, 21, 25]; others build a cost model and algorithmically solve an optimization problem [12, 17, 18, 13, 23]. However, each of the prior algorithmic approaches tackle a slightly different subset of parallelization techniques, system constraints, and DNN optimizations; for example, FlexFlow [13] does not consider pipelining,
PipeDream [17] does not consider memory-saving optimizations that are useful for training large models, and PipeDream and PipeDream-2BW [18] both do not consider tensor parallelism.
Our contribution.
In a ﬁrst, this paper covers a broad search space of parallelization and optimiza-tion techniques while determining placement of DNN layers on the provided hardware deployment (a given number of accelerators of a speciﬁc type with certain performance characteristics, memory ca-pacity, and interconnect bandwidth). Speciﬁcally, our optimization algorithm, called Piper, considers three different dimensions of parallelism (data, tensor model, and pipeline model parallelism), and a general framework of memory-saving optimizations that trade off throughput for lower memory footprint. In this work, we consider pipeline parallelism schedules without pipeline ﬂushes [17, 18].
We leave extensions to pipeline parallelism schedules with ﬂushes [11, 19] to future work.
The focus of our work is on algorithms for DNN partitioning across devices to maximize throughput (for training and large-batch inference of DNN models). Piper uses non-trivial combinations of data, tensor, and pipeline parallelism, as well as memory-saving optimizations such as activation recomputation, carefully chosen for each stage of the partition. Piper supports general DAG (Directed
Acyclic Graph) model topologies and is agnostic of the type of DNN accelerator used. Piper is given a target “safe” batch size (e.g., 1920 for large GPT-style models [20]) and returns a training conﬁguration with this batch size. It is efﬁcient both in theory and in practice.
In Section 5, we evaluate Piper on real-world DNN proﬁles, and study the effects of combining the various parallelism modes and memory-saving optimizations on performance. We ﬁnd that Piper’s large search space combined with its principled algorithmic approach allows it to ﬁnd non-trivial high-quality parallelization conﬁgurations for each given problem instance. Piper compares favorably to planners from prior work (PipeDream, PipeDream-2BW). For example, we ﬁnd (as expected) that in many settings, pipeline model parallelism combined with per-shard data parallelism is enough to obtain high throughput; however, tensor parallelism is also required when one needs a batch size smaller than the number of devices, as well as in highly memory-constrained settings. Piper’s ability to form heterogeneous stages also gives it an advantage over prior work. 2