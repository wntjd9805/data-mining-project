Abstract
Random Fourier features provide a way to tackle large-scale machine learning problems with kernel methods. Their slow Monte Carlo convergence rate has motivated the research of deterministic Fourier features whose approximation error can decrease exponentially in the number of basis functions. However, due to their tensor product extension to multiple dimensions, these methods suffer heavily from the curse of dimensionality, limiting their applicability to one, two or three-dimensional scenarios. In our approach we overcome said curse of dimensionality by exploiting the tensor product structure of deterministic Fourier features, which enables us to represent the model parameters as a low-rank tensor decomposition.
We derive a monotonically converging block coordinate descent algorithm with linear complexity in both the sample size and the dimensionality of the inputs for a regularized squared loss function, allowing to learn a parsimonious model in decomposed form using deterministic Fourier features. We demonstrate by means of numerical experiments how our low-rank tensor approach obtains the same per-formance of the corresponding nonparametric model, consistently outperforming random Fourier features. 1

Introduction
Kernel methods such as Support Vector Machines and Gaussian Processes are commonly used to tackle problems such as classiﬁcation, regression and dimensionality reduction. Since they can be universal function approximators [13], kernel methods have received renewed attention in the last few years and have shown equivalent or superior performance to Neural Networks [22, 26, 11].
The main idea behind kernel methods is to lift the data into a higher-dimensional (or even inﬁnite-dimensional) Reproducing Kernel Hilbert Space H by means of a feature map φ (·) : X → H.
Considering then the pairwise similarities between the mapped data allows to tackle problems which are highly nonlinear in the original sample space X , but linear in H. This can be done equivalently by considering a kernel function k (·, ·) : X × X → R such that (cid:104)φ (x) , φ (x(cid:48))(cid:105) = k (x, x(cid:48)) and performing thus said mapping implicitly. Although effective at learning nonlinear patterns in the data, kernel methods are known to scale poorly as the number of data points N increases. For example, when considering Kernel Ridge Regression (KRR), Gaussian Process Regression (GPR) [33] or
Least-Squares Support Vector Machine (LS-SVM) [41, 42] training usually consist in inverting the Gram matrix kij = k (xi, xj), which encodes the pairwise relation between all data. As a consequence, the associated storage complexity is O (cid:0)N 2(cid:1) and the computational complexity is
O (cid:0)N 3(cid:1), rendering these methods unfeasible for large data. In order to lower the computational cost, data dependent methods approximate the kernel function by means of M data dependent basis functions. Due to their reduced-rank formulation, the computational complexity is reduced to O (cid:0)N M 2(cid:1) for N (cid:29) M . However, e.g. for the Nyström method [46], the convergence rate is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
√
M) [7] limiting its effectiveness. As the name suggests, data independent methods only of O (1/ approximate the kernel function by M data independent basis functions. A good example is the celebrated Random Fourier Features approach by Rahimi and Recht [31], where the authors propose for stationary kernels a low-dimensional random mapping z (·) : RD → RM such that k (x, x(cid:48)) = (cid:104)φ (x) , φ (x(cid:48))(cid:105) ≈ (cid:104)z (x) , z (x(cid:48))(cid:105) . (1)
As in the case of data dependent methods, the reduced-rank formulation allows for a computational complexity of O (cid:0)N M 2(cid:1) for N (cid:29) M . Probabilistic error bounds on the approximation are provided which result in a convergence rate of O (1/
M), which is again the Monte Carlo rate.
√
Improvements in this sense were achieved by considering deterministic features resulting from dense quadrature methods [5, 25, 36] and kernel eigenfunctions [16, 39]. These methods are able to achieve exponentially decreasing upper bounds on uniform convergence guarantees when certain conditions are met. However, for a D-dimensional input space these methods take the tensor product of D vectors, resulting in an exponential increase in the number of basis functions and thus of model weights, effectively limiting the applicability of deterministic features to low-dimensional data. In this work we consider deterministic features. In order to take advantage of the tensor product structure which arises when mapping the inputs to H, we represent the weights as a low-rank tensor. This allows us to learn the inter-modal relations in the tensor product of (low-dimensional) Hilbert spaces, avoiding the exponential computational and storage complexities in D. In this way we are able to obtain a linear computational complexity in both the number of samples and in the input dimension during training, without having to resort to the use of sparse grids or additive modeling of kernels.
The main contribution of this work is in lifting the curse of dimensionality affecting deterministic
Fourier features by modeling the weights as a low-rank tensor. This enables the efﬁcient solution of large-scale and high-dimensional kernel learning problems. We derive an iterative algorithm under the exemplifying case of regularized squared loss and test it on regression and classiﬁcation problems. 2