Abstract
In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame t may be entirely unrelated to what is found at that location in frame t + k. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers—trajectory attention—that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos.
While these ideas are useful in a range of settings, we apply them to the speciﬁc task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something–Something V2, and Epic-Kitchens datasets.
Code and models are available at: https://github.com/facebookresearch/
Motionformer. 1

Introduction
Transformers [75] have become a popular architecture across NLP [32], vision [20] and speech [5].
The self-attention mechanism in the transformer works well for different types of data and across domains. However, its generic nature and its lack of inductive biases also mean that transformers typically require extremely large amounts of data for training [56, 9], or aggressive domain-speciﬁc augmentations [71]. This is particularly true for video data, for which transformers are also appli-cable [50], but where statistical inefﬁciencies are exacerbated. While videos carry rich temporal information, they can also contain redundant spatial information from neighboring frames. Vanilla self-attention applied to videos compares pairs of image patches extracted at all possible spatial locations and frames. This can lead it to focus on the redundant spatial information rather than the temporal information, as we show by comparing normalization strategies in our experiments.
We therefore contribute a variant of self-attention, called trajectory attention, which is better able to characterize the temporal information contained in videos. For the analysis of still images,
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Trajectory attention. In this sequence of frames from the Kinetics-400 dataset, depicting the action ‘kicking soccer ball’, the ball does not remain stationary with respect to the camera, but instead moves to different locations in each frame. Trajectory attention aims to share information along the motion path of the ball, a more natural inductive bias for video data than pooling axially along the temporal dimension or over the entire space-time feature volume. This allows the network to aggregate information from multiple views of the ball, to reason about its motion characteristics, and to be less sensitive to camera motion. spatial locality is perhaps the most important inductive bias, motivating the design of convolutional networks [42] and the use of spatial encodings in vision transformers [20]. This is a direct consequence of the local structure of the physical world: points that belong to the same 3D object tend to project to pixels that are close to each other in the image. By studying the correlation of nearby pixels, we can thus learn about the objects.
Videos are similar, except that 3D points move over time, thus projecting on different parts of the image along certain 2D trajectories. Existing video transformer methods [8, 3, 50] disregard these trajectories, pooling information over the entire 3D space-time feature volume [3, 50], or pooling axially across the temporal dimension [8]. We contend that pooling along motion trajectories would provide a more natural inductive bias for video data, allowing the network to aggregate information from multiple views of the same object or region, to reason about how the object or region is moving (for example, the linear and angular velocities), and to be invariant to camera motion.
We leverage attention itself as a mechanism to ﬁnd these trajectories. This is inspired by methods such as RAFT [70], which showed that excellent estimates of optical ﬂow can be obtained from the correlation volume obtained by comparing local features across space and time. We observe that the joint attention mechanism for video transformers computes such a correlation volume as an intermediate result. However, subsequent processing collapses the volume without consideration for its particular structure. In this work, we seek instead to use the correlation volume to guide the network to pool information along motion paths.
We also note that visual transformers operate on image patches which, differently from individual pixels, cannot be assumed to correspond to individual 3D points and thus to move along simple 1D trajectories. For example, in Figure 1, depicting the action ‘kicking soccer ball’, the ball spans up to four patches, depending on the speciﬁc video frame. Furthermore, these patches contain a mix of foreground (the ball) and background objects, thus at least two distinct motions. Fortunately, we are not forced to select a single putative motion: the attention mechanism allows us to assemble a motion feature from all relevant ‘ball regions’.
Inspired by Nyströmformer [85], we also propose a principled approximation to self-attention,
Orthoformer. Our approximation sets state-of-the-art performance on the recent Long Range Arena (LRA) benchmark [69] for evaluating efﬁcient attention approximations and generalizes beyond the video domain to long text and high resolution images, with lower FLOPS and memory requirements compared to alternatives, Nyströmformer and Performer [15]. Combining our approximation with trajectory attention allows us to signiﬁcantly improve its computational and memory efﬁciency. With our contributions, we set state-of-the-art results on four video action recognition benchmarks. 2
2