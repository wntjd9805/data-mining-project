Abstract
As large-scale language model pretraining pushes the state-of-the-art in text gen-eration, recent work has turned to controlling attributes of the text such models generate. While modifying the pretrained models via ﬁne-tuning remains the popu-lar approach, it incurs a signiﬁcant computational cost and can be infeasible due to lack of appropriate data. As an alternative, we propose MUCOCO—a ﬂexible and modular algorithm for controllable inference from pretrained models. We formu-late the decoding process as an optimization problem which allows for multiple attributes we aim to control to be easily incorporated as differentiable constraints to the optimization. By relaxing this discrete optimization to a continuous one, we make use of Lagrangian multipliers and gradient-descent based techniques to generate the desired text. We evaluate our approach on controllable machine translation and style transfer with multiple sentence-level attributes and observe signiﬁcant improvements over baselines.1 1

Introduction
Recent advances in language models [11, 51, 52] trained on large-scale web text corpora have led to great improvements in state-of-the-art on many natural language processing (NLP) tasks including the ability to generate increasingly coherent text [3]. However, once such models are trained, they are prone to degeneration [66] and biased, non-factual outputs [16, 45] as it is difﬁcult to control the characteristics or attributes of the generated text without architectural modiﬁcations [26, 28, 35] and
ﬁne-tuning the models on attribute-speciﬁc corpora [29, 7]. This can be even more challenging if multiple attributes are involved as labeled data for each combination of attributes can be difﬁcult to obtain.
We focus on controlled text generation where the goal is to decode from a text generation model such that the outputs satisfy certain constraints, which the model was not necessarily trained on. For example, given a dialogue generation model, additionally constraining the generated responses to be polite, although the model was not optimized for politeness during training. Recent works address this problem with left-to-right autoregressive decoding, and modify the vocabulary distribution at every step directly using classiﬁers or language models trained on attribute speciﬁc corpora [72, 40, 39], or indirectly via backpropagating gradients through model activations [8]. While exhibiting high level of attribute control, by design these methods can only work with categorical attributes (typically only one attribute) and condition only on the left context while decoding. Additionally, they often require several heuristics to work and are prone to adversarial outputs [64]. 1The code is available at https://github.com/Sachin19/mucoco 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To address these concerns, we propose the following decoding algorithm. Given a pretrained language model, we posit decoding from it as an optimization problem. First, we relax this discrete optimization problem to a continuous one by representing each token as a simplex on the target vocabulary [19].
This allows us to use continuous optimization techniques like gradient-descent considering each token distribution as parameters; while keeping the language model’s parameters ﬁxed (§2). Second, we represent each target attribute to control as a differentiable function. We formulate controllable decoding as a multi-objective optimization problem, with maximizing the log-probability of the language model as well as target attributes as objectives. To make this optimization feasible via gradient-descent, we repurpose it to a constraint optimization problem and solve the dual using the modiﬁed differential method of multipliers [48]. We call the algorithm MUCOCO, for incorporating multiple constraints through continuous optimization.
We validate MUCOCO on three conditional text generation tasks with different types of sentence level constraints: (1) Adding formality and cross-lingual similarity in a machine translation model; (2) Ensuring transfer and content-preservation in a style-transfer model, and ﬁnally (3) Incorporating multiple styles and attributes (e.g., formality, sentiment magnitude, writer’s age group) in a para-phrasing model. With automatic as well as human evaluations we ﬁnd that our proposed method outperforms strong baselines. 2 MUCOCO: Constrained Decoding as Multi-Objective Optimization
For a given language generation task, let G model the conditional probability p(y|x) of the output sequence y = y1, . . . , yn, given the input sequence x = x1, . . . , xn. This model can be parameterized using any differentiable architecture like Transformers [62] or LSTMs [20] and trained with any loss function [12, 30], either autoregressively or non-autoregressively [17]. Traditionally, given an input x, decoding from such a model requires ﬁnding the output sequence with the highest probability or the lowest negative log-probability, y∗ = arg miny∈Y − log P (y|x). Here Y is the set of all possible output sequences. In practice, searching Y to ﬁnd the highest probability generation is intractable as the space of possible sequences grows exponentially with sequence length and has also been shown to produce undesirable solutions [60]. In most prior work, simple heuristics like beam search, or sampling have been adopted to ﬁnd approximate solutions, where the text is generated one token at a time (usually left to right) with the output of step t being fed to the input at step t + 1.
In this work, however, given G and an input sequence x, we are interested in ﬁnding an output sequence y that not only maximizes the output probability but also optimizes multiple objectives deﬁned over x and y. More formally, we seek to ﬁnd a y that minimizes all of the following objectives y∗ = arg min y∈Y (− log p(y|x), f1(y), . . . , fu(y), g1(x, y), . . . , gv(x, y)) (1)
Here each fi is a function deﬁned over the output sequence y, for example, the negative log-probability of an attribute (e.g., formality) classiﬁer we want the output sequence to satisfy. And each gj is a function deﬁned over both the input and output sequence, for example, semantic similarity between x and y [55]. We assume all fi and gj are differentiable. This is a multi-objective optimization with several possible solutions.
Since there are many objectives to minimize, a left-to-right decoding strategy like beam search or sampling will simply not work due to several reasons. First, the objectives fi and gj are sentence-level and hard to deﬁne accurately only on generated left-context [72, 40]. Even if we are able to deﬁne them, as we add more objectives this process becomes very computationally expensive. Following prior work [19, 50], we formulate this as a continuous optimization process instead of a standard discrete one, and then use standard algorithms for continuous optimization (like gradient descent) for decoding. We maintain a soft-representation of the sequence y, ˜y = (˜y1, . . . , ˜yn), where each
˜yk ∈ ∆V is a simplex over the target vocabulary of size V , representing the probability of the k-th token. To decode a sentence, we initialize each ˜yi uniformly over V , and treat the entire output sentence as the parameters for gradient descent keeping the parameters of G, fi, gj ﬁxed. After gradient descent has converged, we generate discrete text by selecting the token with the highest probability in ˜yk. We provide more details on the optimization procedure in §2.2. 2
To make optimization feasible, a multi-objective problem generally yields itself to the following formulation: arg min y
−α log p(y|x) + u (cid:88) i=1
λifi(y) + v (cid:88) j=1
µjgj(x, y), (2) for some statically or dynamically computed weights λi and µj for each i and j, where α + (cid:80) (cid:80) i λi + j µj = 1. Although this weighted summation formulation is intuitively appealing, it typically requires an expensive grid-search over the various scalings or use of a heuristic [25, 6, 18]. Fur-thermore, this formulation by deﬁnition assumes a trade-off between the different objectives by essentially assigning an importance weight to each of them. This problem is further exacerbated when different objectives have widely varying scales2 with smaller scale objectives just getting ignored.
More concretely, a multi-objective formulation as we deﬁne in (1) admits several possible “optimal” solutions also known as the Pareto set [9]. The image of the Pareto set is called the Pareto front. Since we deﬁne all objectives using neural networks, the Pareto front in our case is non-convex, where linear combinations of objectives are shown to be unsuccessful in ﬁnding good solutions [36, 37, 10] (see ﬁgure 1 for an example). (a) Linear combinations. (b) MUCOCO without damp. (c) MUCOCO
Figure 1: Loss curves for gradient descent for different conﬁgurations for an example of machine translation with a cross-lingual semantic similarity constraint (XSIM < 0.15). For each experiment, we do 100 steps of gradient descent (for clarity, we plot the loss values for every 10 steps). See §3.2 for detailed results. Left: In all cases one of the objectives is favored while the other fails to decrease. Middle: We observe ﬂuctuations in the two losses. Right: The losses decrease much more smoothly leading to a better minimum.
Ideally, our goal is a tunable optimization algorithm that ﬁnds solutions on the Pareto front, i.e., every solution on the Pareto front should have a hyperparameter value for which the optimization algorithm
ﬁnds that solution. In order to achieve this, we reframe our optimization problem as a Lagrangian optimization problem instead. We choose one of the losses as the primary objective and consider other losses as constraints. The goal is to minimize the primary loss subject to the secondary losses, each below a threshold value. More formally, arg min y
− log P (y|x) subject to fi(y) ≤ (cid:15)i, i ∈ {1, · · · , u} gj(x, y) ≤ ξj, j ∈ {1, · · · , v}.
Here (cid:15)i and ξj are tunable hyperparameters whose values’ change can result in different solutions on the Pareto front. This formulation leads to an intuitive interpretation of the decoding process that the generated text from the model G should satisfy the constraints while being as faithful to the primary objective as much as possible.3 Consequently, the Lagrangian we end up with looks similar to our original total loss linearly combined as in (2) given by
L(y, λ1, . . . , λu, µ1, ...µv) = − log p(y|x) − u (cid:88) i=1
λi((cid:15)i − fi(y)) − v (cid:88) j=1
µj(ξj − gj(x, y)) (3) 2For example, classiﬁer log-probabilities are in (0, inf) while sentence similarities usually lie in [0,1]. 3For example, deﬁning fi(y) = p(a|y) as the probability of a desired attribute a in y leads to a natural threshold of fi(y) > 0.5. For a well-calibrated fi, an even higher threshold could be used for inducing highly indicative features of a in y. 3
where λi, µj are Lagrange multipliers, and an optimal output y∗ can be obtained as y∗ = arg miny maxλi≥0,µi≥0 L(y, λi, µi). However, the traditional method of solving the dual func-tion to ﬁnd λi, µj that matches (cid:15)i, ξj, respectively, again leads to a linear trade-off between the various objectives. When the Pareto front is non-convex as in our case, with gradient-descent, the constraints can be ignored and we still cannot always ﬁnd optimal solutions by tuning (cid:15)i, ξj [48]. 2.1 Modiﬁed Differential Method of Multipliers
The fundamental issue in both linear combination of objectives and solving the dual is that ﬁxed scalings λi and µi (manually pre-determined or obtained by solving the dual) do not work well with gradient descent to minimize for y. Following prior work on differential method of multipliers [48], we propose to use a single gradient descent to optimize for both Lagrangian multipliers and y simultaneously as follows: y(t) = y(t−1) − η1∇yL, λt i = λt−1 i + η2∇λiL, µt i = µt−1 i + η2∇µiL. (4)
We follow the gradient of L downwards for the y (descent) and upwards for the multipliers (ascent) while making sure that the multipliers remain positive (by setting the multipliers to 0 whenever they become negative). Intuitively, this algorithm works by increasing the value of the multiplier with each gradient step as long as the constraint is violated. But when the constraint is suddenly satisﬁed and the multiplier is still large, it might take a number of gradient steps before the gradient descent pushes it to 0, thus causing the solution to be pushed further away from the constraint. As soon as the multipliers become 0 (or negative), the constraint is ignored and the process continues. However when the optimization hits the constraint again, this whole cycle repeats, resulting in “oscillations”.
We introduce a dampening parameter to each of the multipliers to reduce these oscillations (again following Platt and Barr [48]) and update the Lagrangian as follows:
L(y, λi, µj) = − log p(y|x) − u (cid:88) i=1 (λi − ζi)((cid:15)i − fi(y)) − v (cid:88) (µj − νj)(ξj − gj(x, y)), (5) j=1 where ζi = d ∗ stop-gradient((cid:15)i − fi(y)), νj = d ∗ stop-gradient(ξj − gj(x, y)) and d is a hyperparameter. d does not affect the ﬁnal y, just how quickly the algorithm converges to it (We use d = 1 in all experiments). stop-gradient(·) indicates that the argument is detached from the computational graph and does not contribute to the gradient computation. When a constraint is not satisﬁed ((cid:15)i − fi(y) < 0, hence ζi < 0), the dampening parameter ζi being negative incurs higher penalty on the violation than when not using any dampening, without actually increasing the value of
λi too much. But when the constraint is satisﬁed, it helps quickly reduce the value of penalty being incurred on the constraint while the multiplier converges to 0. 2.2 Optimization: Exponentiated Gradient Descent
Our goal is to generate a sequence of discrete symbols y = y1, . . . , yT , where yk is from the target vocabulary. To make continuous optimization like gradient descent feasible, we adopt a soft-relaxation [19] to represent each yk as a probability simplex, ˜yk ∈ ∆V (i.e. 0 ≤ ˜ykl ≤ 1 and (cid:80)|V | l=1 ˜ykl = 1). Intuitively, it gives the probability of each token in the vocabulary. To compute the loss L during forward pass, we ﬁrst convert ˜yk to a one-hot vector ˆyk via a straight through estimator [2]. This allows gradients to be applied to ˜yk during the backward pass. More formally,
ˆyk = one-hot(arg max ˜yk) − stop-gradient(˜yk) + ˜yk. During the forward pass, the input embedding tables corresponding to G and each of the constraints’ models receive a one-hot vector ˆyk at each step k, and the input embedding is computed as a weighted-sum of the embedding weights.
But in the backward pass, the gradients are applied to ˜yk.4
This relaxation, however, adds another constraint to the objective L that each parameter ˜yk should be a simplex. We use exponentiated gradient descent [27, 19] to solve this problem which modiﬁes k ∝ ˜y(t−1) the gradient-descent update shown in (4) as: ˜y(t) exp(−η1∇˜yk L). After every descent step,
˜y(t) k is normalized to make it a simplex. k 4Unlike prior work [19, 50, 59], we do not feed ˜yi directly to the model as in our early experiments we found that it leads to slow convergence. 4
2.3 Preventing adversarial solutions: Annealing the thresholds
Finally, it is well known that most neural network based models are not robust to noise and in fact gradient-based methods have been used to generate adversarial examples for text classiﬁers [59].
We ﬁnd in our early experiments that using these models to deﬁne constraints can also lead to such cases where the constraints are rapidly satisﬁed but the generated sentences are disﬂuent. To prevent this issue, we introduce an annealing schedule [47] during the gradient descent where we start with relaxed thresholds (cid:15)i, ξj such that they are all satisﬁed and only the primary loss − log p(y|x) is active. As the optimization progresses, we gradually decrease the value of the thresholds causing the constraints to get violated resulting in the optimization gradually shifting to updating y to satisfy them. The exact schedule we use is described in the next section.
The ﬁnal decoding algorithm we use in all our experiments is described in the Appendix algorithm 1. 3 Experimental Setup
We evaluate MUCOCO on the following controlled generation tasks: reinforcing target style in text generated by a style transfer model §3.1 and adding formality to a machine translation model (§3.2).
Additionally, we conduct a qualitative analysis of rewriting a product review to adhere to multiple expected attributes like formality, sentiment magnitude, and age group of the author (§4). These tasks include constraints corresponding to both expected attributes in the target sentence (like formality) as well as both source and target sentences (like semantic similarity) with up to 6 constraints per task.
Implementation Details For a given sentence length T , we initialize each simplex ˜y1, . . . , ˜yT uniformly over the vocabulary. We use exponentiated descent learning rate of η1 = 50 for y and ascent learning rate of η2 = 2.0 for the multipliers, and run the optimization for 100 steps. Given all intermediate solutions y(t), we choose the one which satisﬁes the constraints and has the minimum value of the primary objective. For each constraint, we use the following annealing schedule: we start with an initial value and linearly decrease it at step 40 until it reaches the desired value at step 80, after which we keep it constant. Additionally, since the length of the target sequence is not known in advance, we ﬁrst greedily decode from G till the end-of-sentence token is generated resulting in a sequence of length L. We then use our approach for each T ∈ {L − 5, . . . , L + 5} and choose the one which (a) satisﬁes all the constraints and (b) has the minimum value of the primary objective.
However, this optimization objective is highly non-convex and may get stuck in a local minimum where constraints are not satisﬁed. If none or partial constraints are satisﬁed, we choose the output based on (b). 3.1 Style Transfer
We begin with a style-transfer task, a task aiming to faithfully and ﬂuently rewrite a given sentence such that a desired writing style is reﬂected in the generation. This task has been widely studied [22, 58, 29, among others] and differs from related tasks like sentiment transfer [61, 33, 34] where ﬂipping the sentiment usually comes at the cost of changing meaning.
Style transfer is usually evaluated across three dimensions: (1) does the output sentence conform to the expected style; (2) does the output sentence preserve the input’s meaning; and (3) is the generated sentence ﬂuent. Most prior work in style transfer focused on devising training objectives serving as proxy for the desired outcomes, for example, back-translation [49, 33] or paraphrasing [29] for content preservation and language modeling for style and ﬂuency. But depending on training algorithm and available data, there is often an observed trade-off between transfer and content-preservation [49, 33].
To that end, we add the desired attributes via explicit constraints when decoding from an existing style transfer model.
More speciﬁcally, we consider the task of informal to formal transfer [53] with the state-of-the-art unsupervised model STRAP from Krishna et al. [29]. This model is trained in an unsupervised fashion by (1) generating a pseudo-parallel corpus by paraphrasing each formal sentence in the training set (which results in a demotion of stylistic attributes), and (2) training an inverse-paraphrase model to translate paraphrases back to the original formal style. At test time, given an informal input sentence x, the model ﬁrst generates its paraphrase z, then using an inverse-paraphrase model to generate the output ˆy. We train this model by ﬁne-tuning GPT2 (345M) [51] with the GYAFC Corpus 5
(Entertainment/Music domain; around 50K formal sentences) [53] and evaluate it on the provided test set containing 1312 informal sentences. Krishna et al. [29] report best results with greedy decoding.
In MUCOCO we modify the decoding algorithm by considering the negative log-probability of y given z according to the model as the primary objective, and incorporate the following constraints:
Formality: We train a binary classiﬁer pFORMAL(y) by ﬁne-tuning GPT2 on the same GYAFC training corpus, following default hyperparameter choices provided in HuggingFace [70]. This classiﬁer outputs the formality probability of a sentence y. We add this output as a constraint to the decoder as
− log(pFORMAL(y)) < − log(0.5). In other words, the constraint is satisﬁed if the classiﬁer assigns at least 0.5 probability of the output y being formal. We initialize the threshold to 10.0 which is later annealed to − log(0.5).
Semantic Similarity: Since the baseline style-transfer model takes as input the paraphrase z and not the original text x, it is susceptible to losing some of the original content in x while generating y. To ensure content preservation we incorporate two kinds of objectives: (1) USIM(x, y) = cosine(M (x), M (y)) [55] where M outputs a continuous vector representation of a given sentence.
Similarity between x and y is measured by cosine similarity of their respective representations. (2) WMD(x, y) takes as input bags of word embeddings of the two sentences and computes the
Word Mover’s Distance between them [32]. This distance is computed by solving a linear program.
We adapt the alternating optimization procedure described in [31] to make this loss differentiable through the program. Intuitively, while USIM computes similarity between sentences taking context into account, it can be less robust to certain missing or repeating tokens, whereas WMD measures lexical overlap between input sentences acting as a proxy for coverage. We discuss the two losses in more detail in Appendix D. To compute the thresholds for constrained optimization, we compute the average value of the two functions on the development set in the same corpus. We use USIM ≤ 0.15 and WMD ≤ 0.4 as the ﬁnal constraints (with initial threshold values of 2.0 for each).
Baselines and Evaluation Metrics We compare MUCOCO with the following baselines:
NO-CONSTRAINTS: We decode directly from the model greedily without any constraints. This replicates the best result reported by Krishna et al. [29]. We do not use continuous optimization to do unconstrained decoding as it has been shown to perform similarly to left-to-right decoding in prior work [19].
FUDGE: Introduced by Yang and Klein [72], this method decodes in an autoregressive manner.
It modiﬁes the output vocabulary distribution at every step by interpolating the language model probability with that of a formality classiﬁer. This classiﬁer is trained to predict the probability of entire sentence being formal given only a preﬁx (we train it similarly to pFORMAL(y) by ﬁne-tuning
GPT2). This method only works with categorical features like formality and is not extensible to constraints like semantic similarity. We decode using the hyperparameters recommended in [72].
To show the beneﬁts of the constrained optimization setup, we show additional comparisons with a linear combination of objectives in Appendix C
Following the baseline model Krishna et al. [29], we evaluate the generated sentences with the following metrics: (a) ﬂuency or grammatical wellformedness measured by the accuracy of a
RoBERTa-based classiﬁer model [41] trained on CoLA [65], averaged over all outputs, (b) transfer: measured by a RoBERTa-based classiﬁer model [41] trained on the GYAFC training corpus, and
ﬁnally (c) WSIM [68], a subword embedding based similarity model trained on a large-scale para-phrase corpus which performs well on STS benchmarks [4] as well. We measure this metric both with respect to the input and the provided references.5 In addition, we also report USIM.
Results The style transfer results are summarized in table 1. If we only incorporate a formality constraint, we observe that compared to FUDGE our method signiﬁcantly improves transfer accuracy at the expense of content preservation. Adding semantic similarity constraints on the other hand improves both transfer as well as content preservation with the largest gains achieved when all the constraints are considered together. Qualitative analysis shows that MUCOCO’s outputs are typically more ﬂuent and have stronger formality signals, but all of the models are prone to propagating errors from the paraphrasing model (see examples in the Appendix table 4). 5Each input sentence has 4 references, we choose the highest WSIM value to compute the average. 6
Method
Constraint
Fluency Transfer
None
STRAP
FORMAL(y)
FUDGE
MUCOCO FORMAL(y)
MUCOCO USIM(x, y)
MUCOCO USIM(x, y), WMD(x, y)
MUCOCO SIM(x, y), WMD(x, y), FORMAL(y) 91% 90% 89% 92% 92% 93% 78% 85% 93% 85% 87% 92%
Content
Preservation (w.r.t. input)
Content
Preservation (w.r.t. ref)
WSIM USIM WSIM USIM 0.69 0.71 0.67 0.71 0.73 0.71 0.77 0.77 0.75 0.78 0.79 0.79 0.72 0.73 0.72 0.74 0.77 0.75 0.80 0.81 0.78 0.81 0.86 0.84
Table 1: Automatic evaluation of ﬂuency, formality transfer, and content preservation for informal-to-formal style transfer models. 3.2 Style-controlled Machine Translation
We now evaluate MUCOCO in the task of formality transfer in machine translation. Given a trained
MT model, decoding is often done using beam search and the highest probability beam candidate is chosen as the ﬁnal output. Prior work has explored adding rule-based or heuristic constraints such as length penalty or coverage [71] to rerank beam candidates, and adding lexical constraints like penalizing n-gram repetitions [21]. In this experiment, we target sentence-level constraints which are otherwise difﬁcult to incorporate in a left-to-right decoding process. Given a trained MT model and the source text x, we use negative log-probability of the translation y under the MT model as our primary objective and incorporate the following constraints for decoding in different combinations:
Cross-lingual Similarity Similar to USIM, we deﬁne XSIM(x, y) = cosine(CM (x), CM (y)), where CM is a multilingual encoder trained by distilling a monolingual model like M described ear-lier [56]. More details of training are available in the Appendix D. Averaging across the development set, we use 0.2 as the threshold for the constraint.
Formality Unlike style transfer, where the goal is to rewrite text in the desired style, here we seek to generate translations in a desired style directly from an MT model which was not explicitly trained to conform to a speciﬁc style. We train a classiﬁer pFORMAL(y) similarly to one described in previous section by ﬁne-tuning GPT2, but with a different input-embedding table to match the vocabulary of the decoder of the MT model. Again, we use log pFORMAL(y) > log(0.5) as the constraint.
Baselines and Evaluation Metrics We compare MUCOCO with the following two baselines:
BEAMSEARCH: We decode directly from the translation model with a beam search of size 5.
FUDGE [72]: deﬁned similarly as in the style transfer task but trained to match the decoder vocabulary.
As mentioned before, FUDGE only works with categorical attributes like formality and is not easily extensible to constraints like cross-lingual similarity. We use the recommended hyperparameters by
Yang and Klein [72] for decoding.
In Yang and Klein [72], the authors also compare FUDGE with other baselines such as PPLM [8] and
BEAMSEARCH followed by style transfer. They show that FUDGE vastly outperforms these baselines.
Hence, we only show comparisons with FUDGE in this work. We evaluate along the following metrics: (a) BLEU [46]: a standard metric for evaluating MT, (b) BERTScore [75]: an embedding-based metric which is more robust to changes in surface forms of the words than BLEU. (b) transfer: the same RoBERTa-based formality classiﬁer as in our style transfer experiments. We also report XSIM, the constraint we use for decoding.
We experiment with French to English translation with a subset of the OpenSubtitles test set [38] containing 1360 sentence pairs.6 This test set contains informal spoken language for both source and target. For the primary objective, we use the Marian Transformer based French (fr) to English (en) model [24] through Huggingface. We summarize the results of this experiment in table 2 with selected examples in the Appendix table 5. 6We create this subset by ﬁltering the original test set to contain only sentence pairs for which beam search translations are classiﬁed as informal. 7
Method
Constraint
BLEU BertScore Formality(%)
XSIM
BEAMSEARCH None
MUCOCO
XSIM(x, y)
FUDGE
MUCOCO
MUCOCO
FORMAL(y)
FORMAL(y)
FORMAL(y), XSIM(x, y) 42.1 42.7 39.2 37.5 39.8 0.932 0.939 0.922 0.913 0.935 0% 4% 6% 30% 23% 0.85 0.88 0.83 0.83 0.86
Table 2: Results of style-controlled machine translation experiments.
Results By just using a cross-lingual similarity metric without modifying the model at all, we observe
+0.6 improvement in BLEU score as well as BERTScore. Adding a formality constraint leads to considerable gain in formality of the outputs with a drop in BLEU; using both XSIM and FORMAL helps recover some of the drop. The drop in BLEU is unsurprising: since BLEU is a surface-level metric it naturally penalizes the translations that are rephrased to conform to formality constraints.
Indeed, as shown in table 5, adding a formality constraint leads to changes in sentence structure and vocabulary. On the other hand, we see improvements in BERTScore which is an embedding-based metric, more robust to paraphrasing.
To further validate our results, we conduct a human evaluation of the generated translations. We randomly sample 100 source sentences and their translations generated by beam search and MUCOCO with both FORMAL and XSIM constraints. Two annotators (highly proﬁcient in French and English) to rank the translations on faithfulness (is the source meaning reﬂected in the translation?) and formality.
The options are randomized. On the translation pairs where both annotators agree (79 out of 100), the ones generated by our method were favored by annotators 37% percent of the time, while beam search translations were favored only 18% of the time, and 21% translations were equally favored. 4 Discussion
Simultaneously controlling several attributes One of the main advantages of our proposed ap-proach is its ﬂexibility to introduce any number of constraints (as long as they are differentiable) to the decoding objective. To illustrate this advantage we consider the following problem: given a sentence annotated with following attributes: age group of the author, formality, and sentiment magnitude, rewrite it such that any chosen combination of the attributes are modiﬁed while keeping the others ﬁxed and the content preserved [42, 33]. For our primary objective, we use a inverse-paraphrasing model as deﬁned in §3.1 which we train on a corpus of Yelp Reviews7 [49]. First, we paraphrase each sentence in the corpus as described in Krishna et al. [29] creating a pseudo-parallel corpus (of reviews and their paraphrases) and train G as an inverse-paraphrase model to translate the paraphrases back to the original reviews. We use USIM and WMD for semantic similarity constraints and three classiﬁers for (a) age group of the author (binary; < 30 years or > 30 years); (b) formality of the review (binary: informal or formal); (c) sentiment magnitude (ﬁve-class classiﬁer ratings of 1 to 5). Here we focus on sentiment ampliﬁcation rather than transfer. That is, changing the 4-star rating of an input to 5 (or 2 to 1). Details of the classiﬁers and the data used are provided in Appendix D.2.8
Table 6 shows examples of generated sentences with different combinations of attribute values. We do not focus on sentiment transfer in this setting (e.g. changing a 1-star review to 5-star review) because it also changes the meaning of the utterance making semantic similarity and sentiment constraints incompatible with each other where satisfying one violates the other.
Finding other solutions on the Pareto front As described in §2, the thresholds (cid:15), ξ are tunable hyperparameters that allow us to ﬁnd different solutions on the Pareto front. In our experiments so far, based on expected outcomes and how the constraints are deﬁned, we showed results with only one threshold for each constraint. For example, ideally for a well-calibrated text classiﬁer based constraint, this technique should be able to ﬁnd solutions for any probability as threshold, but most neural-network based classiﬁers are not well-calibrated and predict the highest probability output as the label, hence a natural threshold for binary-classiﬁers is a label probability > 0.5. In Appendix 7This corpus is sentence-tokenized and lowercased with 2.2M sentences not labeled for any attributes. 8Due to lack of an established benchmark for this task and due to many possible combinations of attributes, we do not report quantitative results. 8
table 8, we show how the outputs change if we modify this threshold to different values. We observe that in most cases the optimization converges to generate words more commonly associated with formality. On the other hand, semantic similarity between two sentences is even harder to deﬁne, is less robust to noise, and varies with writing styles of the input sentences. As shown, increasing this threshold for semantic similarity can lead to repetitions and disﬂuency.
Speed and memory requirements The presented decoding algorithm treats each token in the output sequence y as a parameter for gradient-descent which involves multiple forward and backward passes through the primary generative model G as well as attribute models. Given an expected sequence length L, it optimizes L × V parameters which is both memory and time intensive compared to left-to-right decoding. For example, on a single GeForce RTX 2080 Ti (12GB) on which we run all presented experiments, with a batch size of 1, our approach takes approximately 90 minutes on average to decode around 1200 sentences compared to around 20 minutes for FUDGE [72] with a single constraint. For reference, unconstrained beam-search takes 2-5 minutes. Given enough GPU capacity, however, this approach can easily be extended to larger-batches to improve decoding speed.
We do not conduct this experiment due to limited available resources. Using 16-bit ﬂoating point operations, this can further be improved. Another way of improving memory efﬁciency would be to optimize not for tokens directly but instead optimize for token embeddings [30]. This formulation also removes the requirement for all the models to share a vocabulary. We plan to investigate this in future work. Finally, given the capability of this approach to incorporate multiple constraints, it can also be used to generate pseudo-parallel data with different attribute combinations which then could be used to train supervised models for attributes for interest resulting in faster models at inference. 5 Ethical considerations
Language generation is a growing research area, and state-of-the-art techniques are still not powerful enough to facilitate ﬁne-grained control over generated content. In the current form, large language models have the potential to generate harmful and biased language. For example, language generators are prone to generating toxic [15] and non-factual content [45], especially when used maliciously [63, 64, 74]. Controlled text generation techniques can be used to mitigate many such problematic biases already encoded in large language models [16, 1, 40]. They also have many other positive use-cases, for example, anonymizing personal attributes in written text [54], and even aiding authors in avoiding implicit biases in their writing [44, 14]. However, none of the existing approaches, including ours, can sufﬁciently address these issues yet.
We also caution that there are additional risks of adversarial applications of controlled text generation research. The same algorithms that help us control for content preservation and mitigate biases can be used maliciously, to generate misinformation, incorporate pernicious biases, target speciﬁc individuals to inﬂuence public opinion and seed polarization via manipulating the generated content.
For example, when style transfer techniques are used in conjunction with users’ personal attributes such as gender, they can amplify harmful social biases. We thus opted not to include gender transfer in our experiments.
Nevertheless, these issues should not discourage the scientiﬁc exploration that will advance the state-of-the-art in many positive usages of controlled text generation, including in machine translation, question answering, summarization, dialogue, etc. In parallel, future research should focus on developing better defense methods against mis-using these models maliciously, in a way that could cause societal harms [74]. 6