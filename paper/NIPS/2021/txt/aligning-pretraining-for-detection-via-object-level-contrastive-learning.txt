Abstract
Image-level contrastive representation learning has proven to be highly effective as a generic model for transfer learning. Such generality for transfer learning, however, sacriﬁces speciﬁcity if we are interested in a certain downstream task.
We argue that this could be sub-optimal and thus advocate a design principle which encourages alignment between the self-supervised pretext task and the downstream task. In this paper, we follow this principle with a pretraining method speciﬁcally designed for the task of object detection. We attain alignment in the following three aspects: 1) object-level representations are introduced via selective search bounding boxes as object proposals; 2) the pretraining network architecture incorporates the same dedicated modules used in the detection pipeline (e.g. FPN); 3) the pretraining is equipped with object detection properties such as object-level translation invariance and scale invariance. Our method, called
Selective Object COntrastive learning (SoCo), achieves state-of-the-art results for transfer performance on COCO detection using a Mask R-CNN framework. Code is available at https://github.com/hologerry/SoCo. 1

Introduction
Pretraining and ﬁnetuning has been the dominant paradigm of training deep neural networks in computer vision. Downstream tasks usually leverage pretrained weights learned on large labeled datasets such as ImageNet [1] for initialization. As a result, supervised ImageNet pretraining has been prevalent throughout the ﬁeld. Recently, self-supervised pretraining [2, 3, 4, 5, 6, 7, 8, 9] has achieved considerable progress and alleviated the dependency on labeled data. These methods aim to learn generic visual representations for various downstream tasks by means of image-level pretext tasks, such as instance discrimination. Some recent works [10, 11, 12, 13] observe that the image-level representations are sub-optimal for dense prediction tasks such as object detection and semantic segmentation. A potential reason is that image-level pretraining may overﬁt to holistic representations and fail to learn properties that are important outside of image classiﬁcation.
The goal of this work is to develop self-supervised pretraining that is aligned to object detection. In object detection, bounding boxes are widely adopted as the representation for objects. Translation and scale invariance for object detection are reﬂected by the location and size of the bounding boxes.
An obvious representation gap exists between image-level pretraining and the object-level bounding boxes of object detection.
Motivated by this, we present an object-level self-supervised pretraining framework, called Selective
Object COntrastive learning (SoCo), speciﬁcally for the downstream task of object detection. To introduce object-level representations into pretraining, SoCo utilizes off-the-shelf selective search [14] to generate object proposals. Different from prior image-level contrastive learning methods which
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
treat the whole image as an instance, SoCo treats each object proposal in the image as an independent instance. This enables us to design a new pretext task for learning object-level visual representations with properties that are compatible with object detection. Speciﬁcally, SoCo constructs object-level views where the scales and locations of the same object instance are augmented. Contrastive learning follows to maximize the similarity of the object across augmented views.
The introduction of the object-level representation also allows us to further bridge the gap in network architecture between pretraining and ﬁnetuning. Object detection often involves dedicated modules, e.g., feature pyramid network (FPN) [15], and special-purpose sub-networks, e.g., R-CNN head [16, 17]. In contrast to image-level contrastive learning methods where only feature backbones are pretrained and transferred, SoCo performs pretraining over all the network modules used in detectors.
As a result, all layers of the detectors can be well-initialized.
Experimentally, the proposed SoCo achieves state-of-the-art transfer performance from ImageNet to
COCO. Concretely, by using Mask R-CNN with an R50-FPN backbone, it obtains 43.2 APbb / 38.4
APmk on COCO with a 1× schedule, which are +4.3 APbb / +3.0 APmk better than the supervised pretraining baseline, and 44.3 APbb / 39.6 APmk on COCO with a 2× schedule, which are +3.0 APbb
/ +2.3 APmk better than the supervised pretraining baseline. When transferred to Mask R-CNN with an R50-C4 backbone, it achieves 40.9 APbb / 35.3 APmk and 42.0 APbb / 36.3 APmk on the 1× and 2× schedule, respectively. 2