Abstract
We present a novel approach to unsupervised learning for video object segmenta-tion (VOS). Unlike previous work, our formulation allows to learn dense feature representations directly in a fully convolutional regime. We rely on uniform grid sampling to extract a set of anchors and train our model to disambiguate between them on both inter- and intra-video levels. However, a naive scheme to train such a model results in a degenerate solution. We propose to prevent this with a simple regularisation scheme, accommodating the equivariance property of the segmentation task to similarity transformations. Our training objective admits efficient implementation and exhibits fast training convergence. On established
VOS benchmarks, our approach exceeds the segmentation accuracy of previous work despite using significantly less training data and compute power. 1

Introduction
Unsupervised learning of visual representations has recently made considerable and expeditious advances, in part already outperforming even supervised feature learning methods [8, 11, 14]. Most of these works, however, require substantial computational resources [8, 11], and only few accommodate one of the most ubiquitous types of visual data: videos [13, 33]. In contrast to image sets, video data embeds ample information about typical transformations occurring in nature. Exploiting such cues may allow systems to learn more task-relevant invariances, instead of relying only on hand-engineered data augmentation [30]. In this work, we propose to learn such invariances with a novel framework for the task of video object segmentation (VOS). In contrast to previous works [17, 19], we learn dense representations efficiently in a fully convolutional manner, previously considered prone to degenerate solutions [17]. This is achieved by leveraging the assumption of feature equivariance to similarity transformations (e. g., multi-scale cropping) applied to the input frame.
Fig. 1 presents an overview of our approach. For each video sequence, we designate one of the frames as the reference frame. We sample a set of features, produced from the reference frame by a fully convolutional network, on a spatially uniform grid, and refer to them as anchors. Using a contrastive formulation [12], our approach learns to represent the temporally proximate frames to the reference in terms of these anchors. Importantly, this representation is learned to be equivariant to similarity transformations. Towards this goal, we devise a self-training objective [20] that generates pseudo labels by leveraging the second, transformed view of the original video sequence. These pseudo labels contain dominant assignments of the features from the second view to the anchors. Following the self-training mechanism, we learn to assign the features in the original view consistently with the transformed view. Our self-supervised loss further disambiguates the anchors themselves spatially and between independent video sequences, while also ensuring their transformation equivariance.
Implementing this process in a novel framework, we attain a new state of the art in video segmentation accuracy, with compelling computational and data efficiency. Our framework can be trained on
Code (Apache-2.0 License) available at https://github.com/visinf/dense-ulearn-vos. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
reference frame anchors embedding space video-level discrimination anchor affinity self-supervision soft assignment hard assignment similarity transform view I video A view II video B spatial discrimination
Figure 1: Illustration of the main idea. We learn dense visual feature representations in an unsupervised way by (i) discriminating the features both spatially and at the video level, and (ii) embedding video representations in terms of a temporally persistent set of video-specific anchors.
We devise a simple and effective approach that sidesteps trivial solutions by exploiting feature equivariance to similarity transformations of the input frame, which allows to efficiently learn dense representations in a fully convolutional manner. a single commodity GPU, yet achieves higher segmentation accuracy than previous work, while requiring orders of magnitude less data. Improving the accuracy even further with more data, our approach also exhibits favourable scalability in a more comprehensive testing scenario, which has not yet been shown in prior work [17, 38]. 2