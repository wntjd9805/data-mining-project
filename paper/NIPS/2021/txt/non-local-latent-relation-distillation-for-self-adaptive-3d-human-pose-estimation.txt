Abstract
Available 3D human pose estimation approaches leverage different forms of strong (2D/3D pose) or weak (multi-view or depth) paired supervision. Barring synthetic or in-studio domains, acquiring such supervision for each new target environment is highly inconvenient. To this end, we cast 3D pose learning as a self-supervised adaptation problem that aims to transfer the task knowledge from a labeled source domain to a completely unpaired target. We propose to infer image-to-pose via image-to-latent and latent-to-pose where the latter two explicit mappings viz. is a pre-learned decoder obtained from a prior-enforcing generative adversarial auto-encoder. Next, we introduce relation distillation as a means to align the unpaired cross-modal samples i.e. the unpaired target videos and unpaired 3D pose sequences. To this end, we propose a new set of non-local relations in order to characterize long-range latent pose interactions unlike general contrastive relations where positive couplings are limited to a local neighborhood structure. Further, we provide an objective way to quantify non-localness in order to select the most effective relation set. We evaluate different self-adaptation settings and demonstrate state-of-the-art 3D human pose estimation performance on standard benchmarks.1

Introduction 1
Human pose estimation systems have garnered immense attention due to their innumerable applica-tions [55, 19, 85]. The successes of such systems are mostly driven by large-scale datasets containing images with paired 3D pose annotations [25]. Unlike 2D joint landmarks, annotating a 3D pose requires body-worn sensors or multi-camera structure-from-motion setup [71] which is challenging to install outdoors. Hence, the available datasets are either captured in constrained laboratory settings or are limited in size and diversity. Unsurprisingly, models trained on such datasets exhibit poor cross-dataset generalization. Addressing this, several approaches [11, 77, 56, 34] resort to weakly-supervised methods that rely on images with paired 2D landmark annotations. Certain methods require additional supervision such as depth [81, 11] or multi-view image pairs [66, 33]. However, they still suffer from dataset-bias due to their strong reliance on some form of paired supervision.
In this work, we digress from any form of paired supervision or auxiliary cues (multi-view or depth) thereby avoiding the curse of dataset-bias. We thus introduce a self-supervised domain adaptation framework for 3D human pose estimation (Fig. 1). In the proposed setting, we consider access to three different datasets. First, a labeled source dataset obtained from graphics-based synthetic environments such as SURREAL [79] or in-studio datasets such as Human3.6M [25]. Second, unlabeled video sequences from the target domain. Third, a set of unpaired 3D pose sequences.
Following this, image-to-pose inference is carried out via two explicit mappings i.e., image-to-latent
CNN followed by a latent-to-pose network. Here, the latent-to-pose network is pre-learned as a decoder of a prior-enforcing generative adversarial auto-encoder [47] (AAE) in order to restrict the 1Webpage: https://sites.google.com/view/sa3dhp 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
pose predictions within the plausibility limits. We follow the same to pre-learn a generative motion embedding via a recurrent AAE setup which operates on the latent pose space instead of the raw pose sequences. Both pose and motion embeddings are trained solely on the unpaired 3D pose dataset.
Next, we prepare image-to-latent CNN by supervising on the labeled source dataset. After ﬁnishing the above pre-learning steps, our prime objective is to train or adapt only the image-to-latent CNN so that it can perform well on the unlabeled target domain samples. In order to align the output manifold of the image-to-latent with the pre-learned latent pose manifold (in the absence of cross-modal pairs), one must formalize innovative ways to represent the dark-knowledge based on which the student (i.e. image-to-latent) output can align to that of the teacher (i.e. pose-to-latent).
Several studies [18, 20] on human cognitive development advocate that, in a self-supervised paradigm, new knowledge is acquired by relating entities based on some semantic rule. Motivated by this, we explore different ways of formalizing inter-entity relations. A relation can be characterized as lower order or higher order based on the number of entities that participate in deﬁning a relationship tuple.
For instance, in contrastive learning [57, 12], a pose space triplet relation expresses a coupling of just 3 pose entities, and is thus considered a lower order relation. However, a similar contrastive triplet deﬁned in the hierarchical motion space (i.e. temporal pose sequences of ﬁxed sequence length T ) would couple 3T pose entities, thus is considered a higher order relation.
Next, we adapt the target-speciﬁc image-to-latent by minimizing the relational energies de-rived from the contrastive relations. Unlike in prior-arts [12, 51] where the output embedding is learned from scratch, we are restricted to op-erate on a pre-learned output pose embedding.
Consequently, the model often converges to a de-generate solution exhibiting instance-level mis-alignment [46]. We realize that the positive coupling in contrastive relations is limited to local neighborhood structures resulting in sub-optimal alignment. This motivates us to develop a new set of relations that would express positive coupling of diverse non-local relations (beyond the structural neighborhood), thereby character-izing long-range latent pose interactions in a much effective manner.
Figure 1: We align samples from unpaired pose (or motion) and unpaired images (or videos) at a shared latent pose space by distilling higher order (associ-ating multiple instance via motion) non-local (e.g. flip-backward) relations. Relations are equivalent to a form of data-interlinking as done in knowledge-graphs.
We deﬁne tangible non-local relations separately in the pose and motion space that are categorized under a) lower order non-local relations and b) higher order non-local relations, respectively. For instance, “pose-flip” is a lower order non-local relation which associates anchor poses with their left-right ﬂipped versions. Similarly, “flip-backward” is a higher order non-local relation which couples anchor pose-sequences with their ﬂip-backward counterpart which is obtained via temporal reversal (backwards) of the individually ﬂipped frames. Here, the corresponding relational energy is devised via latent space relation networks. These are essentially frozen neural networks that are trained to regress the latent embedding of the relational counterpart given latent embedding of the anchor as input. In a nutshell, the relational energies aim to preserve the equivariance of higher order spatio-temporal relations between the two modalities as a means to perform the cross-modal alignment. It turns out that, among various relations, relations coupling the most diverse non-local samples result in a better cross-modal alignment. We perform extensive experiments to validate the efﬁcacy of our approach and demonstrate superior generalizability on samples from in-the-wild environments. We summarize our contributions as follows:
• The proposed solution for self-adapting 3D human pose model involves cross-modal align-ment between the unpaired samples from the input and output modalities, via relation distillation. Highly non-local instances are associated using novel relation networks to speciﬁcally cater to the instance-level misalignment.
• We provide insights to select the most effective non-local relations. This involves quantifying non-localness of a relation as the average latent-distance between the coupled entities.
• We evaluate different self-adaptation settings and demonstrate state-of-the-art 3D human pose estimation performance against the available semi-supervised and weakly-supervised prior arts on Human3.6M [25], MPI-INF-3DHP [49], and 3DPW [80]. 2
2