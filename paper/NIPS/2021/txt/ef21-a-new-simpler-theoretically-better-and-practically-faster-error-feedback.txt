Abstract
Error feedback (EF), also known as error compensation, is an immensely popular convergence stabilization mechanism in the context of distributed training of super-vised machine learning models enhanced by the use of contractive communication compression mechanisms, such as Top-𝑘. First proposed by Seide et al. [2014] as a heuristic, EF resisted any theoretical understanding until recently [Stich et al., 2018, Alistarh et al., 2018]. While these early breakthroughs were followed by a steady stream of works offering various improvements and generalizations, the current theoretical understanding of EF is still very limited. Indeed, to the best of our knowledge, all existing analyses either i) apply to the single node setting only, ii) rely on very strong and often unreasonable assumptions, such as global bound-edness of the gradients, or iterate-dependent assumptions that cannot be checked a-priori and may not hold in practice, or iii) circumvent these issues via the intro-duction of additional unbiased compressors, which increase the communication cost. In this work we ﬁx all these deﬁciencies by proposing and analyzing a new EF mechanism, which we call EF21, which consistently and substantially outperforms
EF in practice. Moreover, our theoretical analysis relies on standard assumptions only, works in the distributed heterogeneous data setting, and leads to better and more meaningful rates. In particular, we prove that EF21 enjoys a fast 𝒪(1/𝑇 ) convergence rate for smooth nonconvex problems, beating the previous bound of
𝒪(1/𝑇 2/3), which was shown under a strong bounded gradients assumption. We further improve this to a fast linear rate for Polyak-Lojasiewicz functions, which is the ﬁrst linear convergence result for an error feedback method not relying on unbiased compressors. Since EF has a large number of applications where it reigns supreme, we believe that our 2021 variant, EF21, can have a large impact on the practice of communication efﬁcient distributed learning. 1

Introduction
In order to obtain state-of-the-art performance, modern machine learning models rely on elaborate architectures, need to be trained on data sets of enormous sizes, and involve a very large number of parameters. Some of the most successful models are heavily over-parameterized, which means that they involve more parameters than the number of available training data points [Arora et al., 2018].
Naturally, these circumstances should inform the design of optimization methods that could be most efﬁcient to perform the training.
First, the reliance on sophisticated model architectures, as opposed to simple linear models, generally leads to nonconvex optimization problems, which are more challenging than convex problems [Jain and Kar, 2017]. Second, the need for very large training data sizes necessitates the use of distributed
*King Abdullah University of Science and Technology, Thuwal, Saudi Arabia.
†This paper was written while Ilyas Fatkhullin was an intern at KAUST. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
computing [Verbraeken et al., 2019]. Due to its enormous size, the data needs to be partitioned across a number of machines able to work in parallel. Typically, for further efﬁciency gains, each such machine further parallelizes its local computations using one or more hardware accelerators.
Third, the very large number of parameters describing these models exerts an extra stress on the communication links used to exchange model updates among the machines. These links are typically slow compared to the speed at which computation takes place, and communication often forms the bottleneck of distributed systems even in less extreme situations than over-parameterized training where the number of parameters, and hence the nominal size of communicated messages, can be truly staggering. For this reason, modern efﬁcient optimization methods typically employ elaborate lossy communication compression techniques to reduce the size of the communicated messages.
Due to the above reasons, in this paper we are interested in solving the nonconvex distributed optimization problem
[︃
𝑓 (𝑥) def= min
𝑥∈R𝑑 1
𝑛
𝑛
∑︁
𝑖=1
]︃
𝑓𝑖(𝑥)
, (1) where 𝑥 ∈ R𝑑 represents the parameters of a machine learning model we wish to train, 𝑛 is the number of workers/nodes/machines, and 𝑓𝑖(𝑥) is the loss of model 𝑥 on the data stored on node 𝑖.
We speciﬁcally focus on the development of new and more efﬁcient communication efﬁcient ﬁrst-order methods for solving (1) utilizing biased compression operators, with a special emphasis on clean convergence analysis which removes the strong and often unrealistic assumptions, such as the bounded gradient assumption, which are currently needed to analyze such methods (see Table 1).
The remainder of the paper is organized as follows. In Section 2 we describe the key concepts, results and open problems that form the motivation for our work, and summarize our main contributions.
Our main theoretical results are presented in Section 4. In Section 4.4 we establish a connection between EF and EF21. Finally, experimental results are described in Section 5. 2