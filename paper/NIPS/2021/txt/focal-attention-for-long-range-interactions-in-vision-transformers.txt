Abstract
Recently, Vision Transformer and its variants have shown great promise on various computer vision tasks. The ability of capturing local and global visual dependencies through self-attention is the key to its success. However, this also brings challenges due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection). Many recent works have attempted to reduce the cost and improve model performance by applying either coarse-grained global attention or ﬁne-grained local attention. However, both approaches cripple the modeling power of the original self-attention mechanism of multi-layer Transformers, leading to sub-optimal solutions. In this paper, we present focal attention, a new attention mechanism that incorporates both ﬁne-grained local and coarse-grained global interactions. In this new mechanism, each token attends its closest surrounding tokens at ﬁne granularity and the tokens far away at coarse granularity, and thus can capture both short- and long-range visual dependencies efﬁciently and effectively.
With focal attention, we build a new variant of Vision Transformer models, called
Focal Transformers, which achieve superior performance over the state-of-the-art (SoTA) Vision Transformers on a range of public image classiﬁcation and object detection benchmarks. In particular, our Focal Transformer models with a moderate size of 51.1M and a large size of 89.8M achieve 83.6% and 84.0%
Top-1 accuracy, respectively, on ImageNet classiﬁcation at 224 × 224. When employed as the backbones, Focal Transformers achieve consistent and substantial improvements over the current SoTA Swin Transformers [43] across 6 different object detection methods. Our largest Focal Transformer yields 58.7/59.0 box mAPs and 50.9/51.3 mask mAPs on COCO mini-val/test-dev, and 55.4 mIoU on
ADE20K for semantic segmentation, creating new SoTA on three of the most challenging computer vision tasks. Our code is available at: https://github. com/microsoft/Focal-Transformer. 1

Introduction
Nowadays, Transformer [57] has become a prevalent model architecture in natural language pro-cessing (NLP) [20, 6]. In the light of its success in NLP, there is an increasing effort on adapt-ing it to computer vision (CV) [47, 50]. Since its promise ﬁrstly demonstrated in Vision Trans-former (ViT) [21], we have witnessed a ﬂourish of full-Transformer models for image classiﬁca-tion [55, 60, 64, 43, 76, 56], object detection [8, 85, 79, 18] and semantic segmentation [58, 62].
Beyond these static image tasks, it has also been applied on various temporal understanding tasks, such as action recognition [40, 78, 10], object tracking [13, 59], scene ﬂow estimation [38].
The self-attention mechanism is arguably the key component that differentiates Transformers from the widely used convolutional neural networks (CNNs) [37] in computer vision. At each Transformer layer, self-attention enables global content-dependent interactions among different image regions for 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Left: Visualization of the attention maps of the three heads at the given query patch (blue) in the ﬁrst layer of the DeiT-Tiny model [55]. Right: An illustrative depiction of focal attention mechanism. Three granularity levels are used to compose the attention region for the blue query. modeling short- and long-range dependencies, respectively. Through the visualization of full self-attention results1, we indeed observe that self-attention learns to attend local surroundings (like CNNs) and the global contexts at the same time, as illustrated in Fig. 1 (Left). Nevertheless, when dealing with high-resolution vision tasks such as object detection or segmentation, an efﬁcient implementation of a global and ﬁne-grained self-attention becomes non-trivial due to the quadratic computational cost with respect to the number of tokens in feature maps. Recent works have alternatively exploited either a coarse-grained global self-attention [60, 64] or a ﬁne-grained local self-attention [43, 76, 56], for the sake of reducing the computational cost. However, both approaches cripple the power of the original full self-attention i.e., the ability to simultaneously capture local and global visual dependencies.
In this paper, we present a new attention mechanism to capture both short- and long-range interactions in Transformer layers for high-resolution input images. Considering that the visual dependencies between the nearby (local) regions are usually much stronger than the dependencies between the regions that are far away, we perform the ﬁne-grained attention only in local regions while the coarse-grained attention globally. As depicted in Fig. 1 (Right), a query token in the feature map attends its closest local surroundings at the ﬁnest granularity as itself. However, when it goes to the regions far away, it attends to summarized tokens to capture coarse-grained visual dependencies. We call this new mechanism focal attention, as each token attends the others in a focal manner. We will show in this study that focal attention allows to effectively model visual dependencies among all regions covering the whole high-resolution feature maps while introducing much less number of tokens in the computation than that in the standard self-attention mechanism.
Equipped with focal attention, a series of Focal Transformers are developed and validated via a comprehensive empirical study across three core vision tasks, including image classiﬁcation, object detection and segmentation. Results show that Focal Transformers consistently outperform the SoTA
Vision Transformers across various settings (i.e., in model sizes and complexities). Notably, the small
Focal Transformer with 51.1M parameters achieves 83.6% top-1 accuracy on ImageNet-1K, and the base model with 89.8M parameters obtains 84.0% top-1 accuracy. In the ﬁne-tuning experiments for object detection, Focal Transformers consistently outperform the SoTA Swin Transformers [43] across six popular object detection methods. Our largest Focal Transformer model achieves 59.0 box mAP and 51.3 mask mAP on COCO test-dev for object detection and instance segmentation, respectively, and 55.4 mIoU on ADE20K for semantic segmentation. These results demonstrate that focal attention is highly effective in modeling the global interactions in Vision Transformers. 2