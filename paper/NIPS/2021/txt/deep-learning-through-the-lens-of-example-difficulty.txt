Abstract
Existing work on understanding deep learning often employs measures that com-press all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difﬁculty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relation-ships between the prediction depth of a given input and the model’s uncertainty, conﬁdence, accuracy and speed of learning for that data point. We further cate-gorize difﬁcult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this under-standing allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions ﬁrst. 1

Introduction
Much of the existing work on understanding deep learning “integrates out” the data, viewing the inductive bias of the model, or the properties of the optimizer as central to the success of the approach.
Examples of such work include studies of eigenvalues of the Hessian and the geometry of the loss landscape (Ghorbani et al., 2019; Yao et al., 2020; Sagun et al., 2016; Li et al., 2018; Pennington and Bahri, 2017; Sagun et al., 2018), studies of margin and effective generalization measures (Long and Sedghi, 2019; Unterthiner et al., 2020; Jiang et al., 2020, 2018; Kawaguchi et al., 2017) and mean-ﬁeld studies of stochastic optimization (Smith et al., 2021; Stephan et al., 2017; Smith and Le, 2018). However, in practice, we are rarely concerned with only the average behavior of a model.
One pathway to understanding the principles that govern how deep models process data is to study the properties of deep models for data points with different “amounts” or “types” of example difﬁculty.
There are a number of deﬁnitions of example difﬁculty in the literature (E.g. see Carlini et al. (2019); Hooker et al. (2019); Lalor et al. (2018); Agarwal and Hooker (2020)). Two are particularly relevant to this work. Firstly, the probability of predicting the ground truth label for an example, when that example is omitted from the training set (Jiang et al., 2021), which represents a statistical view of example difﬁculty. Secondly, the difﬁculty of learning an example, parameterized by the earliest training iteration after which the model predicts the ground truth class for that example in all subsequent iterations (Toneva et al., 2019). This measure represents a learning view of example difﬁculty 2.
∗Work completed as part of the Google AI Residency Program 2We expand on other notions of example difﬁculty in Appendix B. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
These notions suffer from two fundamental limitations. While early-exit strategies in computer vision (Teerapittayanon et al., 2016; Huang et al., 2018) and NLP (Dehghani et al., 2018; Liu et al., 2020b; Schwartz et al., 2020; Xin et al., 2020) suggest predictions for easier examples require less computation, the above example difﬁculty notions do not encapsulate the processing of data inside a given converged model. Moreover, existing notions of example difﬁculty (E.g. Carlini et al. (2019)) provide a one-dimensional view of difﬁculty which can not distinguish between examples that are difﬁcult for different reasons.
In this paper, we take a signiﬁcant step towards resolving the above shortcomings. To take the processing of the data into account we propose a new measure of example difﬁculty, the prediction depth, which is determined from the hidden embeddings. To escape the one-dimensional view of difﬁculty, we introduce three distinct difﬁculty types by relating the hidden embeddings for an input to high-level concepts about example difﬁculty: “Does this example look mislabeled?”; “Is classifying this example only easy if the label is given?”; “Is this example ambiguous both with and without its label?”. Furthermore, we show how this enhanced notion of example difﬁculty can unify our understanding of several seemingly unrelated phenomena in deep learning. We hope that the results presented in this work will aid the development of models that capture heteroscedastic uncertainty, our understanding of how deep networks respond to distributional shift, and the advancement of curriculum learning approaches and machine learning fairness. These connections are discussed in
Section 5.
Contributions Our main contributions are as follows:
• We introduce a measure of computational example difﬁculty: the prediction depth (PD). The prediction depth, illustrated in Figure 1, represents the number of hidden layers after which the network’s ﬁnal prediction is already (effectively) determined (Section 2).
• We show that the prediction depth is larger for examples that visually appear to be more difﬁcult, and that prediction depth is consistent between architectures and random seeds (Section 2.2).
• Our empirical investigation reveals that prediction depth appears to establish a linear lower bound on the consistency of a prediction. We further show that predictions are on average more accurate for validation points with small prediction depths (Section 3.1).
• We demonstrate that ﬁnal predictions for data points that converge earlier during training are typically determined in earlier layers which establishes a correspondence between the training history of the network and the processing of data in the hidden layers (Section 3.2).
• We show that both the adversarial input margin and the output margin are larger for examples with smaller prediction depths. We further design an intervention to reduce the output margin of a network and show that this leads to predictions being made only in the latest hidden layers (Section 3.3).
• We identify three extreme forms of example difﬁculty by considering the prediction depth in the training and validation splits independently and demonstrate how a simple algorithm that uses the hidden embeddings in one middle layer to make predictions can lead to dramatic improvements in accuracy for inputs that strongly exhibit a speciﬁc form of example difﬁculty (Section 4).
• We use our results to present a coherent picture of deep learning that uniﬁes four seemingly unrelated deep learning phenomena: early layers generalize while later layers memorize; networks converge from input layer towards output layer; easy examples are learned ﬁrst and networks present simpler functions earlier in training (Section 5).
Experimental Setup: To ensure that our results are robust to the choice of architectures and datasets, we report empirical ﬁndings for ResNet18 (He et al., 2016), VGG16 (Simonyan and Zisserman, 2015) and MLP architectures trained on CIFAR10, CIFAR100 (Krizhevsky et al., 2009), Fashion MNIST (FMNIST) (Xiao et al., 2017) and SVHN (Netzer et al., 2011) datasets. All models were trained using
SGD with momentum. Our MLP comprises 7 hidden layers of width 2048 with ReLU activations.
Details of the datasets, architectures, and hyperparameters used can be found in Appendix A.