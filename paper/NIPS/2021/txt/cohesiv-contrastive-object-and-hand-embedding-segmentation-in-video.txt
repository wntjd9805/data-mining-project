Abstract
In this paper we learn to segment hands and hand-held objects from motion. Our system takes a single RGB image and hand location as input to segment the hand and hand-held object. For learning, we generate responsibility maps that show how well a hand’s motion explains other pixels’ motion in video. We use these responsibility maps as pseudo-labels to train a weakly-supervised neural network using an attention-based similarity loss and contrastive loss. Our system outperforms alternate methods, achieving good performance on the 100DOH,
EPIC-KITCHENS, and HO3D datasets. 1

Introduction
We invite you to pick up an object in your vicinity and bring it towards you. As you hold the object and move your hand, the object moves coherently, and is roughly rigidly attached to a coordinate frame in your hand. While your adult brain does not need this signal and can readily differentiate the object in your hand from both your hand and the background even if you hold your hand still, how did you learn to do this? One answer [42] is that as a human you have time locked modalities of your own hand’s conﬁguration via proprioception and vision, and your hand and the object share a
“common fate” [47]. The goal of this paper is to operationalize this idea by learning to segment hand and in-hand objects (irrespective of name) from a single image by learning from video data.
This goal poses many challenges for current computer vision. While there has been a longstanding interest in learning segmentation and object individuation from motion cues in vision [41, 34, 11], the general case of being broadly able to segment everything has not made substantial progress. While there has, of course, been substantial progress on instance segmentation for particular categories (for instance MaskRCNN [21]) powered by large segmentation datasets [28], the space of categories that one can pick up is vast. Indeed, recent work that aims to reconstruct in-hand objects via segmentation losses [7] reconstructs a few objects by ﬁnding category correspondences (e.g., tennis racket masks are used for knives). At the same time, while there is work on understanding hands, including contact state [32], and detecting boxes around held generic objects [40], there is no work on segmenting objects, much less work that learns from video data. In sum – segmenting lots of generic objects is still challenging, and segmenting generic objects in contact is no exception.
Our approach makes progress on this challenging problem by assuming a small amount of knowledge about humans. We assume we can estimate landmarks on hands [37], identify whether they are holding something [40], as well as identify which pixels belong to people [23]. This small amount of information guides understanding of the the deluge of information from optical ﬂow [43]. Rather than segmenting the full 3D motions of 3D objects from the ﬂow, we instead only have to identify whether the ﬂow is better explained by a moving hand or a background. Moreover, by knowing about an ubiquitous object (the hand), we show we can learn about many less ubiquitous objects.
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We implement our solution with a network, named COHESIV, that takes an RGB image and one 2D location on a hand as inputs to segment the hand and the object that the hand is interacting with (Section 3). At test time, COHESIV maps the input image to an embedding space with a CNN [38]; this embedding is processed with lightweight heads to produce per-pixel detectors at each hand, and feature maps for objects that can be queried to produce a heatmap. At training time, we use the motion with nearby frames to derive signal. We deﬁne a responsibility map for a moving hand as the relative goodness of ﬁt on optical ﬂow for a planar motion model in comparison to a background model. These responsibilities power two losses: a similarity loss that directly supervises per-hand segments; a contrastive loss [13] that encourages in-group afﬁnity as well as separates embeddings among people, objects, and background via pseudo-labels.
We train and validate on video data of humans engaged in complex behaviors using subsets of the 100 Days of Hands (100DOH) [40], EPIC-KITCHENS-55 (EPICK) [9, 10], and HO3D [18] datasets.
We compare with alternate methods that range from fully-supervised bounding boxes [40] to basic motion cues from optical ﬂow [43], to saliency [51]. We show that our weakly-supervised method is comparable to the supervised bounding box detector method, while outperforming ﬂow and saliency methods. 2