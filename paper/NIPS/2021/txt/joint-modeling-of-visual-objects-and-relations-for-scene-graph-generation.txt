Abstract
An in-depth scene understanding usually requires recognizing all the objects and their relations in an image, encoded as a scene graph. Most existing approaches for scene graph generation ﬁrst independently recognize each object and then predict their relations independently. Though these approaches are very efﬁcient, they ignore the dependency between different objects as well as between their relations.
In this paper, we propose a principled approach to jointly predict the entire scene graph by fully capturing the dependency between different objects and between their relations. Speciﬁcally, we establish a uniﬁed conditional random ﬁeld (CRF) to model the joint distribution of all the objects and their relations in a scene graph. We carefully design the potential functions to enable relational reasoning among different objects according to knowledge graph embedding methods. We further propose an efﬁcient and effective algorithm for inference based on mean-ﬁeld variational inference, in which we ﬁrst provide a warm initialization by independently predicting the objects and their relations according to the current model, followed by a few iterations of relational reasoning. Experimental results on both the relationship retrieval and zero-shot relationship retrieval tasks prove the efﬁciency and efﬁcacy of our proposed approach. 1

Introduction
Modern object recognition [32, 10, 35] and detection [28, 27, 57] systems excel at the perception of visual objects, which has signiﬁcantly boosted many industrial applications such as intelligent surveillance [18, 49] and autonomous driving [23, 38]. To have a deeper understanding of a visual scene, detecting and recognizing the objects in the scene is however insufﬁcient. Instead, a compre-hensive cognition of visual objects and their relationships is more desirable. Scene Graph Generation (SGG) [13] is a natural way to achieve this goal, in which a graph incorporating all objects and their relations within a scene image is derived to represent its semantic structure.
Most previous works for SGG [48, 55, 53, 36, 4, 37] usually ﬁrst independently predict different objects in a scene and then predict their relations independently. In practice, though such methods are very efﬁcient, they ignore the dependency between different objects and between the relations of different object pairs. For example, a car could frequently co-occur with a street, and the relation eating could always appear along with the relation sitting on. Modeling such dependency could be very important for accurate scene graph prediction, especially for rare objects and relations. There are indeed some recent works [6, 5] along this direction. For example, Dai et al. [6] explored the triplet-level label dependency among a head object, a tail object and their relation. These methods have shown very promising results, while they only explored the limited dependency within a triplet.
How to capture the full dependency between different objects and between their relations within a whole scene graph remains very challenging and unexplored. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To attain such a goal, in this paper, we propose a principled approach called Joint Modeling for
Scene Graph Generation (JM-SGG) to predict the whole scene graph by jointly capturing all the label dependency within it, i.e. the dependency between different objects and their relations and also the interdependency between them. Speciﬁcally, we model the joint distribution of all objects and relations in a scene graph with the conditional random ﬁeld (CRF) framework [17]. To ﬂexibly model the joint distribution, the key is to deﬁne effective potential functions on both nodes (i.e. objects) and edges (i.e. relations between objects). We deﬁne the potential functions on objects according to the object representations extracted by existing neural network based object detector. It is however nontrivial to design effective potential functions on edges, since these potential functions have to capture the relation between two objects in an edge and meanwhile allow relational reasoning among different edges, which models the dependency among the relations on various edges. Inspired by the existing work of knowledge graph embedding [20], which represents entities and relations in the same embedding space and performs relational reasoning in that space, we deﬁne our potential functions according to the knowledge graph embedding method and hence allow efﬁcient relational reasoning between different object pairs in a scene graph.
Such a fully expressive model also brings challenges to both learning and inference due to the complicated structures between different random variables in the CRF, i.e. objects and their relations.
We therefore further propose an efﬁcient and effective inference algorithm based on mean-ﬁeld variational inference, which is able to assist the gradient estimation for learning and derive the most likely scene graph for test. Traditional mean-ﬁeld methods usually suffer from the problem of slow convergence. Instead of starting from a randomly initialized variational distribution as in traditional mean-ﬁeld methods, we propose to initialize the variational distribution, i.e. the marginal distribution of each object and each relation, with a factorized tweak of JM-SGG model, and then perform a few iterations of message passing induced by the ﬁxed-point optimality condition of mean ﬁeld to reﬁne the variational distribution, which allows our approach to enjoy both good precision and efﬁciency.
To summarize, in this paper, we make the following contributions:
• We propose Joint Modeling for Scene Graph Generation (JM-SGG) which is a fully expressive model that can capture all the label dependency in a whole scene graph.
• We propose a principled mean-ﬁeld variational inference algorithm to enable the efﬁcient learning and inference of JM-SGG model.
• We verify the superior performance of our method on both relationship retrieval and zero-shot relationship retrieval tasks under various settings and metrics. Also, we illustrate the efﬁciency and efﬁcacy of the proposed inference algorithm by thorough analytical experiments. 2