Abstract
High-resolution satellite imagery has proven useful for a broad range of tasks, in-cluding measurement of global human population, local economic livelihoods, and biodiversity, among many others. Unfortunately, high-resolution imagery is both infrequently collected and expensive to purchase, making it hard to efﬁciently and effectively scale these downstream tasks over both time and space. We propose a new conditional pixel synthesis model that uses abundant, low-cost, low-resolution imagery to generate accurate high-resolution imagery at locations and times in which it is unavailable. We show that our model attains photo-realistic sample quality and outperforms competing baselines on a key downstream task – object counting – particularly in geographic locations where conditions on the ground are changing rapidly. 1

Introduction
Recent advancements in satellite technology have enabled granular insight into the evolution of human activity on the planet’s surface. Multiple satellite sensors now collect imagery with spatial resolution less than 1m, and this high-resolution (HR) imagery can provide sufﬁcient information for various ﬁne-grained tasks such as post-disaster building damage estimation, poverty prediction, and crop phenotyping [15, 3, 41]. Unfortunately, HR imagery is captured infrequently over much of the planet’s surface (once a year or less), especially in developing countries where it is arguably most needed, and was historically captured even more rarely (once or twice a decade) [7]. Even when available, HR imagery is prohibitively expensive to purchase in large quantities. These limitations often result in an inability to scale promising HR algorithms and apply them to questions of broad social importance. Meanwhile, multiple sources of publicly-available satellite imagery now provide sub-weekly coverage at global scale, albeit at lower spatial resolution (e.g. 10m resolution for
Sentinel-2). Unfortunately, such coarse spatial resolution renders small objects like residential buildings, swimming pools, and cars unrecognizable.
Figure 1: Given a 10m low resolution (LR) image from 2016 and a 1m high resolution (HR) image from 2018, we generate a photo-realistic and accurate HR image for 2016. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In the last few years, thanks to advances in deep learning and generative models, we have seen great progress in image processing tasks such as image colorization [43], denoising [6, 35], inpainting
[35, 27], and super-resolution [11, 21, 16]. Furthermore, pixel synthesis models such as neural radiance ﬁeld (NeRF) [26] have demonstrated great potential for generating realistic and accurate scenes from different viewpoints. Motivated by these successes and the need for high-resolution images, we ask whether it is possible to synthesize high-resolution satellite images using deep generative models. For a given time and location, can we generate a high-resolution image by interpolating the available low-resolution and high-resolution images collected over time?
To address this question, we propose a conditional pixel synthesis model that leverages the ﬁne-grained spatial information in HR images and the abundant temporal availability of LR images to create the desired synthetic HR images of the target location and time. Inspired by the recent development of pixel synthesis models pioneered by the NeRF model [26, 40, 2], each pixel in the output images is generated conditionally independently by a perceptron-based generator given the encoded input image features associated with the pixel, the positional embedding of its spatial-temporal coordinates, and a random vector. Instead of learning to adapt to different viewing directions in a single 3D scene [26], our model learns to interpolate across the time dimension for different geo-locations with the two multi-resolution satellite image time series.
To demonstrate the effectiveness of our model, we collect a large-scale paired satellite image dataset of residential neighborhoods in Texas using high-resolution NAIP (National Agriculture Imagery
Program, 1m GSD) and low-resolution Sentinel-2 (10m GSD) imagery. This dataset consists of scenes in which housing construction occurred between 2014 and 2017 in major metropolitan areas of Texas, with construction veriﬁed using CoreLogic tax and deed data. These scenes thus provide a rapidly changing environment on which to assess model performance. As a separate test, we also pair
HR images (0.3m to 1m GSD) from the Functional Map of the World (fMoW) dataset [9] crop ﬁeld category with images from Sentinel-2.
To evaluate our model’s performance, we compare to state-of-the-art methods, including super-resolution models. Our model outperforms all competing models in sample quality on both datasets measured by both standard image quality assessment metrics and human perception (see example in
Figure 1). Our model also achieves 0.92 and 0.62 Pearson’s r2 in reconstructing the correct numbers of buildings and swimming pools respectively in the images, outperforming other models in these tasks. Results suggest our model’s potential to scale to downstream tasks that use these object counts as input, including societally-important tasks such as population measurement, poverty prediction, and humanitarian assessment [7, 3]. 2