Abstract
We present and analyze an algorithm for optimizing smooth and convex or strongly convex objectives using minibatch stochastic gradient estimates. The algorithm is optimal with respect to its dependence on both the minibatch size and minimum expected loss simultaneously. This improves over the optimal method of Lan [17], which is insensitive to the minimum expected loss; over the optimistic acceleration of Cotter et al. [10], which has suboptimal dependence on the minibatch size; and over the algorithm of Liu and Belkin [19], which is limited to least squares problems and is also similarly suboptimal with respect to the minibatch size. Applied to interpolation learning, the improvement over Cotter et al. and Liu and Belkin translates to a linear, rather than square-root, parallelization speedup. 1

Introduction
The massive scale of many modern machine learning models and datasets give rise to complex, high-dimensional training objectives that can be very computationally expensive to optimize. To reduce the computational cost, it is therefore important to devise optimization algorithms that can leverage parallelism to reduce the amount of time needed to train. Stochastic ﬁrst-order methods, which use stochastic estimates of the gradient of the training objective, are by far the most common approach and these methods can be directly improved by using minibatch stochastic gradient estimates, which are easy to parallelize across multiple computing cores or devices. Accordingly, we propose and analyze an optimal accelerated minibatch stochastic gradient descent algorithm.
Our analysis exploits that while training machine learning models, it is typically possible to drive the loss either all the way to zero, or at least very close to zero, and the performance of our algorithm improves as the minimum value of the loss approaches zero. This property has multiple names in different contexts. In learning theory, it is common to show fast rates under the assumption of
“realizability”—meaning the data can be ﬁt perfectly by the model, i.e. the loss can be driven to zero.
Even when the problem is not exactly realizable, it is sometimes possible to derive “optimistic rates” that interpolate between fast rates for realizable learning and the slower agnostic rates [33]. In the context of “interpolation learning,” there has recently been great interest in understanding training
“overparametrized” models—which have many more parameters than there are training examples, generally meaning that many settings of the parameters would attain zero training loss—both in terms of optimization [2–4, 9, 15] and generalization [6, 35, 38]. Finally, in the optimization literature, there have been efforts to prove optimistic rates depending on the minimum value of the objective or on the variance of the stochastic gradients a the minimizer [10, 19–21, 23, 31]. Regardless of the name, these ideas are all based on the same fundamental concept of exploiting the fact that the minimum value of the objective is nearly zero. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our contributions
In Section 3, we present and analyze an accelerated minibatch SGD algorithm for optimizing smooth and convex objectives using minibatch stochastic gradient estimates. Our method closely resembles the methods of Lan [17] and Cotter et al. [10], but with different stepsizes and momentum parameters, and a tighter analysis. Importantly, our algorithm enjoys a linear speedup in the minibatch size all the way up to a critical threshold beyond which larger minibatches do not help. In contrast, Lan and
Cotter et al.’s bounds have a worse, sublinear speedup and a correspondingly higher critical threshold.
In Section 4, we show that a modiﬁed version of our algorithm can attain substantially faster convergence when the objective satisﬁes a certain quadratic growth condition, which is a relaxation of strong convexity. As part of our analysis, we simplify and generalize a restarting technique
[13, 18, 24, 26, 29, 30], which we show amounts to a reduction from strongly convex optimization to convex optimization. The reduction in the other direction is a well-known tool in optimization analysis, but our result shows that it goes both ways and it may be of more general interest.
In Sections 5 and 6, we prove that our methods are optimal with respect to both the minibatch size and also the minimum value of the loss in the settings we consider. We then explain how our guarantees demonstrate a linear speedup in the minibatch size, which improves over a sublinear speedup in previous work [10, 19].
Finally, in Section 7, we extend our results to a related setting where the bound on the minimum value of the loss is replaced by a bound on the variance of the stochastic gradients at the optimum
[as in, e.g. 8, 14, 21, 23, 31]. Under this condition, we establish the optimal error achievable by any learning rule, including non-ﬁrst-order methods, and we show that the optimal convergence rate is nevertheless achieved by SGD—a ﬁrst-order method without acceleration. Further, we show that the accelerated optimization rate, T −2, is unattainable using minibatches of size 1, but with larger minibatches, our accelerated minibatch SGD method can match the optimal error of SGD using a substantially smaller parallel runtime but the same number of samples. 2 Setting and