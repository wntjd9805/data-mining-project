Abstract
Path planning, the problem of efﬁciently discovering high-reward trajectories, often requires optimizing a high-dimensional and multimodal reward function. Pop-ular approaches like CEM [37] and CMA-ES [16] greedily focus on promising regions of the search space and may get trapped in local maxima. DOO [31] and
VOOT [22] balance exploration and exploitation, but use space partitioning strate-gies independent of the reward function to be optimized. Recently, LaMCTS [45] empirically learns to partition the search space in a reward-sensitive manner for black-box optimization. In this paper, we develop a novel formal regret analysis for when and why such an adaptive region partitioning scheme works. We also propose a new path planning method LaP3 which improves the function value estimation within each sub-region, and uses a latent representation of the search space. Empir-ically, LaP3 outperforms existing path planning methods in 2D navigation tasks, especially in the presence of difﬁcult-to-escape local optima, and shows beneﬁts when plugged into the planning components of model-based RL such as PETS [7].
These gains transfer to highly multimodal real-world tasks, where we outperform strong baselines in compiler phase ordering by up to 39% on average across 9 tasks, and in molecular design by up to 0.4 on properties on a 0-1 scale. Code is available at https://github.com/yangkevin2/neurips2021-lap3. 1

Introduction
Path planning has been used extensively in many applications, ranging from reinforcement learning [7, 13, 14] and robotics [27, 35, 26] to biology [24], chemistry [40], material design [21], and compiler optimization [42]. The goal is to ﬁnd the most rewarding trajectory (i.e., state-action sequence) x = (s0, a0, s1, . . . , sn) in the search space Ω: x∗ = arg maxx∈Ω f (x), where f (x) is the reward.
In this work, we focus on deterministic path planning problems with long trajectories x, and dis-continuous and/or multimodal reward functions f . Such high-dimensional non-convex optimization problems exist in many real domains, both continuous and discrete. While we could always ﬁnd near-optimal x by random sampling given an inﬁnite query budget, in practice we prefer a sample-efﬁcient method that achieves high-reward trajectories with fewer queries of the reward function f .
While global methods like Bayesian Optimization (BO) [3] may struggle with limited samples and high-dimensional spaces, classic approaches like CEM [37] and CMA-ES [16] learn a local model around promising trajectories. For example, CEM tracks a population of trajectories and repeatedly re-samples its population according to the highest-performing trajectories from the previous generation.
On the other hand, such a focus can trap CEM in local optima, as conﬁrmed empirically (Sec. 5).
Other recent approaches, such as VOOT [22] and DOO [31], use a (recursive) region partitioning scheme: they split the search space Ω into sub-regions Ω = Ω1 ∪ . . . ∪ Ωk, then invest more samples into promising sub-regions while continuing to explore other regions via an upper conﬁdence bound 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(UCB). While such exploration-exploitation procedures adaptively focus on promising sub-regions and lead to sub-linear regret and optimality guarantees, their region partition procedure is manually designed by humans and remains non-adaptive. For example, DOO partitions the space with uniform axis-aligned grids and VOOT with Voronoi cells, both independent of the reward f to be optimized.
Recently, Wang et al. proposed LaNAS [46] and LaMCTS [45], which adaptively partition the search regions based on sampled function values, and focus on good regions. They achieve strong empirical performance on Neural Architecture Search (NAS) and black-box optimization, outperforming many existing methods including evolutionary algorithms and BO. Notably, in recent NeurIPS’20 black-box optimization challenges, two teams that use variants of LaMCTS ranked 3rd [38] and 8th [23].
In this paper, we provide a simple theoretical analysis of LaMCTS to reveal the underlying principles of adaptive region partitioning, an analysis missing in the original work. Based on this analysis, we propose Latent Space Partitions for Path Planning (LaP3), a novel optimization technique for path-planning. Unlike LaMCTS, LaP3 uses a latent representation of the search space. Additionally, we use the maximum (instead of the mean) as the node score to improve sample efﬁciency, veriﬁed empirically in Sec. 5.3. Both changes are motivated by our theoretical analysis.
We verify LaP3 on several challenging path-planning tasks, including 2D navigation environments from past work with difﬁcult-to-escape local optima, and real-world planning problems in compiler optimization and molecular design. In all tasks, LaP3 demonstrates substantially stronger exploration ability to escape from local optima compared to several baselines including CEM, CMA-ES and
VOOT. On compiler phase ordering, we achieve on average 39% and 31% speedup in execution cycles comparing to -O3 optimization and OpenTuner [1], two widely used optimization techniques in compilers. On molecular design, LaP3 outperforms all of our baselines in generating molecules with high values of desirable properties, beating the best baseline in average property value by up to 0.4 on properties in a [0, 1] range. Additionally, extensive ablation studies show factors that affect the quality of planning and verify the theoretical analysis.
LaP3 is a general planning technique and can be readily plugged into existing algorithms with path planning components. For example, we apply LaP3 to PETS [7] in model-based RL and observe substantially improved performance for high-dimensional continuous control and navigation, compared to CEM as used in the original PETS framework. 2 Latent Space Monte Carlo Tree Search (LaMCTS)
LaMCTS [45] is recently proposed to solve black-box optimization problems x∗ = arg maxx f (x) via recursively learning f -dependent region partitions. Fig. 1 and Alg. 1 show the details of LaMCTS as well as our proposed approach LaP3 (formally introduced in Sec. 4) for comparison.
Algorithm 1 LaP3 Pseudocode for Path Planning. Improvements over LaMCTS in green. 1: Input: Number of rounds T , Environment Oracle: f (x), Dataset D, Sampling Latent Model h(x),
Partitioning Latent Model s(x). 2: Parameters: Initial #samples Ninit, Re-partitioning interval Npar, Node partition threshold Nthres, UCB parameter Cp. 3: Pre-train h(·) on D when D (cid:54)= ∅. 4: Set region partition V0 = {Ω}. 5: Draw Ninit samples uniformly from S0 = {(xi, f (xi))}Ninit 6: for t = 0, . . . , T − Ninit − 1 do if t divides Npar then 7: 8: 9: 10: 11: end if for k := root, k /∈ Vleaf do i=1 ⊂ Ω.
Train/ﬁne-tune latent model h(·) using samples St ∪ D (Eqn. ??).
Re-learn region partition Vt ← Partition(Ω, St, Nthres, s(·)) in latent space Φs of s(·). 12: k ← arg max
Ωc∈child(Ωk) bc, where bc :=

 (cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24)(cid:24) 1 n(Ωc) (cid:88) f (xi) max xi∈Ωc xi∈Ωc f (xi) + Cp (cid:113) 2 log n(Ωk) n(Ωc)

. end for
Initialize CMA-ES using encodings of St ∩ Ωk via h(·). Here Ωk is the chosen leaf sub-region.
St ← St−1 ∪ {(xt, f (xt))}, where xt is drawn from CMA-ES and decoded via h−1(·). 13: 14: 15: 16: end for 2
Figure 1: LaP3 extends LaMCTS [45] to path planning. (a) Starting from a search space Ω, both LaP3 and
LaMCTS ﬁrst draw a few samples x ∈ Ω, then learn to partition Ω into a sub-region Ω1 with good samples (high f (x)) and a sub-region Ω2 with bad samples (low f (x)). Compared to LaMCTS, LaP3 uses a latent space and reduces the dimensionality of the search space. (b) Sampling follows the learned recursive space partition, focusing on good regions while still exploring bad regions using UCB. LaP3 uses the maximum of the sampled value in a region (maxxi∈Ω f (xi)) as the node value, while LaMCTS uses the mean. (c) Upon reaching a leaf, new data points are sampled within the region and the space partition is relearned.
LaMCTS starts with Ninit random samples of the entire search space Ω (line 5 in Alg. 1). For a region Ωk, let n(Ωk) be the number of samples within. LaMCTS dictates that, if n(Ωk) ≥ Nthres, then Ωk is partitioned into disjoint sub-regions Ωk = Ωgood ∪ Ωbad as its children (Fig. 1(a)-(b), line 9 in Alg. 1, the function Partition). Intuitively, Ωgood contains promising samples with high f , while Ωbad contains samples with low f . Unlike DOO and VOOT, such a partition is learned using
St ∩ Ωk, our samples so far in the region, and is thus dependent on the function f to be optimized.
Given tree-structured sub-regions, new samples are mostly drawn from promising regions and occasionally from other regions for exploration. This is achieved by Monte Carlo Tree Search (MCTS) [4] (line 11-13): at each tree branching, the UCB score b is computed to balance exploration and exploitation (line 12). Then the subregion with highest UCB score is selected (e.g., it may have high f and/or low n). This is done recursively until a leaf sub-region Ω(cid:48) is reached. Then a new sample x is drawn from Ω(cid:48) (line 15) either uniformly, or from a local model constructed by an existing optimizer (e.g., TuRBO [10], CMA-ES [16]), in which case LaMCTS becomes a meta-algorithm.
When more samples are collected, regions are further partitioned and the tree gets deeper.
Finally, the function Partition in Alg. 1 is deﬁned as follows: ﬁrst a 2-class K-means on (x, f (x)) is used to create positive/negative sample groups. Next, a SVM classiﬁer is used to learn the decision boundary (hence the partition), so that samples with high f (x) fall into Ωgood, and samples with low f (x) fall into Ωbad (Fig. 1(a)). See Appendix A for the pseudo code. The partition boundary can also be re-learned after more samples are collected (line 9). 3 A Theoretical Understanding of Space Partitioning
While LaMCTS [45] shows strong empirical performance, it contains several components with no clear theoretical justiﬁcation. Here we attempt to give a formal regret analysis when sub-regions
{Ωk} are ﬁxed and all at the same tree level, and the function f is deterministic. We leave further analysis of tree node splitting and evolution of hierarchical structure to future work.
Despite the drastic simpliﬁcation, our regret bound still shows why an f -dependent region partition is helpful. By showing that a better regret bound can be achieved by a clever region partition as empirically used in the Partition function in Alg. 1, we justify the design of LaMCTS. Furthermore, our analysis suggests several empirical improvements over LaMCTS and motivates the design of
LaP3, which outperforms multiple classic approaches on hard path planning problems. 3.1 Regret Analysis with Fixed Sub-Regions
We consider the following setting. Suppose we have K d-dimensional regions {Ωk}K k=1, and nt(Ωk) is the visitation count at iteration t. The global optimum x∗ resides in some unknown region Ωk∗ . At each iteration t, we visit a region Ωk, sample (uniformly or otherwise) a data point xt ∈ Ωk, and retrieve its deterministic function value ft = f (xt). In each region Ωk, deﬁne x∗ k := arg maxx∈Ωk f (x) and the maximal value g∗(Ωk) = f (x∗ k). The maximal value so far at iteration t is gt(Ωk) = maxt(cid:48)≤t f (xt(cid:48)). It is clear that gt ≤ g∗ and gt → g∗ when t → +∞. 3
Figure 2: Theoretical understanding of space partitioning. (a) Deﬁnition of (zk, ck)-diluted region Ωk (Def. 1). (b) Partition of region Ωk into good region Ωk1 and bad region Ωk2. Optimal solution x∗ ∈ Ωk1. (c) After space partitioning, Fk is split into Fk1 and Fk2. The good region Fk1 has much smaller ck1 while the bad region has much larger best-to-optimality gap ∆k2. As a result, the expected total regret decreases.
We deﬁne the conﬁdence bound rt = rt(Ωk) so that with high probability, the following holds: gt(Ωk) ≥ g∗(Ωk) − rt(Ωk)
At iteration t, we pick region kt to sample based on the upper conﬁdence bound: kt = arg maxk gt(Ωk) + rt(Ωk). Many different conﬁdence bounds can be applied; for convenience in this analysis, we use the “ground truth” bound from the cumulative density function (CDF) of f within the region Ωk (Please check Appendix B for all proofs):
Lemma 1. Let Fk(y) := P [f (x) ≤ g∗(Ωk) − y|x ∈ Ωk] be a strictly decreasing function, and let rk,t(Ωk) := F −1 (cid:0)δ1/nt(Ωk)(cid:1). Then Eqn. 1 holds with probability 1 − δ. (1) k k is the inverse function of Fk and randomness arises from sampling within Ωk. Since Fk is exists and is also strictly decreasing. By deﬁnition, Fk ∈ [0, 1],
Here F −1 a strictly decreasing function, F −1
Fk(0) = 1 and F −1 k (1) = 0. We then deﬁne the dilution of each region as follows:
Deﬁnition 1 ((zk, ck)-dilution). A region Ωk is (zk, ck)-diluted if there exist zk, ck such that Fk(y) ≤ 1 − (y/ck)d for y ∈ [0, ck(1 − zk)1/d], where zk is the smallest Fk(y) to make the inequality hold. k
The intuition for dilution for a given region, as depicted in Fig. 2(a), is that all but zk fraction of the region has function value close to the maximum, with "close" deﬁned based on ck (smaller ck implies a stricter deﬁnition of “close”). Obviously if Ωk is (zk, ck)-diluted then it is (z(cid:48) k)-diluted for any c(cid:48) k ≥ ck and z(cid:48) k ≥ zk. Therefore, we often look for the smallest zk and ck to satisfy the condition. If a region Ωk has small ck and zk, we say it is highly concentrated. For example, if f (x) is mostly constant within a region, then ck is very small since Fk(y) drops to 0 very quickly. In such a case, most of the region’s function values are concentrated near the maximum, making it easier to optimize. k, c(cid:48)
While the deﬁnition of concentration may be abstract, we show it is implied by Lipschitz continuity:
Corollary 1. If a region Ωk is Lk-Lipschitz continuous, i.e., |f (x)−f (x(cid:48))| ≤ Lk(cid:107)x−x(cid:48)(cid:107)2, and there d(cid:112) ˜Vk)-diluted. k, (cid:15)0) ⊆ Ωk, then with uniform sampling, Ωk is (1 − (cid:15)d exists an (cid:15)0-ball B(x∗ 0
Here ˜Vk := Vk/V0 is the relative volume with respect to the unit sphere volume V0.
˜V −1 k
, Lk
Typically, a smoother function (with small Lk) and large (cid:15)0 yield a less diluted (and more concentrated) region. However, the concept of dilution (Def. 1) is much broader. For example, if we shufﬂe function values within Ωk, Lipschitz continuity is likely to break but Def. 1 still holds.
Now we will bound the total regret. Let Rt(at) := f ∗ − gt(Ωat) ≥ 0 be the regret of pick-ing Ωat and R(T ) := (cid:80)T t=1 Rt(at) be the total regret, where T is the total number of sam-ples (queries to f ). Deﬁne the gap of each region ∆k := f ∗ − g∗(Ωk) and split the region indices into Kgood := {k : ∆k ≤ ∆0} and Kbad := {k : ∆k ≥ ∆0} by a threshold ∆0. (cid:16)(cid:80) (cid:17)1/d cd k and Cbad := (cid:0)(cid:80) (cid:1)1/d cd k k∈Kgood
Cgood := are the (cid:96)d-norms of the ck in these two sets. Finally, M := supx∈Ω f (x) − inf x∈Ω f (x) is the maximal gap between function values.
Treating each region Ωk as an arm and applying a regret analysis similar to multi-arm bandits [41], we obtain the following theorem:
Theorem 1. Suppose all {Ωk} are (zk, ck)-diluted with zk ≤ η/T 3 for some η > 0. The total expected regret E [R(T )] = O
T d−1 ln T + M (Cbad/∆0)d ln T + KM η/T
Cgood k∈Kbad
√ (cid:105) (cid:104)
. d 4
3.2
Implications of Theorem 1
The effect of space partitioning. Reducing {ck} results in a smaller regret R(T ). Thus if we can partition Ωk into two sub-regions Ωk1 and Ωk2 such that the good partition Ωk1 has smaller ck1 < ck and the bad partition Ωk2 has larger ∆k2 > ∆0 and falls into Kbad, then we can improve the regret bound (Fig. 2(b)-(c)). This coincides with the Partition function of LaMCTS very well: it samples a few points in Ωk, and trains a classiﬁer to separate high f from low f . On the other hand, if we partition a region Ωk randomly, e.g., each f (x) is assigned to either Ωk1 or Ωk2 at random, then statistically Fk1 = Fk2 = Fk and ck1 = ck2 = ck, which increases the regret bound. Therefore, the partition needs to be informed by data that have already been sampled within the region Ωk.
Recursive region partitioning. In Theorem 1, we assume all regions {Ωk} have ﬁxed ck and zk, so the bound breaks for large enough T (as η/T 3 eventually becomes smaller than any ﬁxed zk).
However, as LaMCTS conducts further internal partitioning within Ωk, its ck and zk keep shrinking with more samples T . If each split leads to slightly fewer bad f (i.e., lighter “tail”), with the ratio being γ < 1, then by the deﬁnition of CDF, zk is the probability mass of the tail and thus zk ∼ γ−T /Npar . This would yield zk ≤ η/T 3 for all T , since γ−T decays faster than 1/T 3 and
Theorem 1 would hold for all T . See Appendix F.2 for empirical veriﬁcation of decaying zk. 3.3