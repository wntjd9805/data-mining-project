Abstract
Social recommendation has shown promising improvements over traditional sys-tems since it leverages social correlation data as an additional input. Most existing works assume that all data are available to the recommendation platform. However, in practice, user-item interaction data (e.g., rating) and user-user social data are usually generated by different platforms, both of which contain sensitive infor-mation. Therefore, How to perform secure and efﬁcient social recommendation across different platforms, where the data are highly-sparse in nature remains an important challenge. In this work, we bring secure computation techniques into social recommendation, and propose S3Rec, a sparsity-aware secure cross-platform social recommendation framework. As a result, S3Rec can not only improve the recommendation performance of the rating platform by incorporating the sparse social data on the social platform, but also protect data privacy of both platforms.
Moreover, to further improve model training efﬁciency, we propose two secure sparse matrix multiplication protocols based on homomorphic encryption and private information retrieval. Our experiments on two benchmark datasets demon-strate that S3Rec improves the computation time and communication size of the state-of-the-art model by about 40× and 423× in average, respectively. 1

Introduction
The recent advances of social recommendation have achieved remarkable performances in recommen-dation tasks [12, 28]. Unlike traditional methods, social recommendation leverages user-item rating data (e.g. from Netﬂix) with user-user social data (e.g. from Facebook) to facilitate model training.
The intuition behind this setup is that Facebook’s social data is much better than Netﬂix’s social data in both quantity and quality, and those social data at Facebook can help to improve Netﬂix’s recommendation performance. However, the cross-platform nature, the high sparsity and sensitivity of recommendation/social data make social recommendation hard-to-deploy in the real world [5]. In summary, the main problem we are facing is,
How to perform secure and efﬁcient social recommendation across different platforms, where the data are highly-sparse in nature?
Speciﬁcally, we focus on the problem of collaborative social recommendation in the two-party model, where one party (denoted as P0) is a rating platform that holds user-item rating data, and the other party (denoted as P1) is a social platform that holds user-user social data. We also assume that the adversaries are semi-honest, which is commonly used in the secure computation literature [9]. That 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is to say, the adversary will not deviate from the pre-deﬁned protocol, but will try to learn as much information as possible from its received messages.
Choices of privacy enhancing techniques. Currently, many anonymization techniques have been used in publishing recommendation data, such as k-anonymity and differential privacy [11]. On the other hand, cryptographic methods like secure multiparty computation (MPC) [11] and homomorphic encryption (HE) have been proposed to enable calculation on the protected data. Since k-anonymity has been demonstrated risky in practice (e.g., the re-identiﬁcation attack on Netﬂix Prize dataset
[22]), and differential privacy introduces random noises to the dataset which eventually affects model accuracy [10, 30], we consider they are not the ideal choice for our framework. Instead, we choose a combination of cryptographic tools (i.e., MPC and HE, but mainly MPC) which allows multiple parties to jointly compute a function depending on their private inputs while providing security guarantees.
Choices of social recommendation model. In literature, many social recommendation models have been proposed [8, 18, 27] using matrix factorization or neural networks. Existing MPC-based neural network protocols [21, 29] usually suffer from accuracy loss and inefﬁciency due to their approximation of non-linear operations. Especially for the case of social recommendation, training data could exceed to millions, and this makes NN-based model a less ideal choice. Therefore, we choose the classic social recommendation model, Soreg [18], as a typical example, and present how to build a secure and efﬁcient version of Soreg under cross-domain social recommendation scenario.
Dealing with sparse data in secure machine learning. One important property of social recom-mendation data is its high sparsity. Take LibraryThing dataset [32] for example, its social matrix density is less than 0.02%. Recently, Schoppmann et al. introduced the ROOM framework [26] for secure computation over sparse data. However, their solution only works on column-sparse or row-sparse data, and in addition, it requires secure matrix multiplication protocol (for instance, based on Beaver’s multiplication triple). Chen et al. proposed a secure protocol for a sparse matrix multiplies a dense matrix [6], which combines homomorphic encryption and secret sharing, but it only works well when the dense matrix is small. Different from their work, in this paper, we propose a PIR-based matrix multiplication which does not reply on pre-generated correlated randomness.
Our framework. In this paper, we propose S3Rec, a sparsity-aware secure cross-platform social recommendation framework. Starting with the classic Soreg model, we observe that the training process of Soreg involves two types of calculation terms: (1) the rating term which could be calculated by P0 locally, and (2) the social term which needs to be calculated by P0 and P1 collaboratively.
Therefore, the key to S3Rec is designing secure and efﬁcient protocols for calculating the social term.
To begin with, we ﬁrst let both parties perform local calculation. Then both parties invoke a secure social term calculation protocol and let P0 ﬁnally receive the plaintext social term, and update the model accordingly. In this way, the security of our protocol relies signiﬁcantly on the secure social term calculation protocol (for simplicity, we refer this protocol as the ‘ST-MPC’ protocol), and we propose a secure instantiation and prove its security. Similarly, the efﬁciency of S3Rec relies heavily on the performance of ST-MPC, and at the core, it relies on the efﬁciency of a matrix multiplication protocol. The naïve secure matrix multiplication protocol is traditionally evaluated through Beaver’s triples [3], and has O(km2) asymptotic communication complexity, where k is the dimension of latent factors and m is the number of users. To improve the communication efﬁciency, we propose two secure sparse matrix multiplication protocols for ST-MPC, based on two sparsity settings: (1) insensitive sparsity, which is a weaker variant of matrix multiplication where we assume both parties know the locations of non-zero values in the sparse matrix, and (2) sensitive sparsity, which is also a weaker variant of matrix multiplication, but stronger than (1), and we assume ‘only’ the number of zeros is public. Nevertheless, we present secure constructions for MatrixMul in both cases by leveraging two cryptography primitives called Private Information Retrieval (PIR) [1] and
Homomorphic Encryption (HE) [24]. PIR can hide the locations of the non-zero values in the sparse matrix while HE enables additions and multiplications on ciphertexts. To this end, we drop the communication complexity of secure MatrixMul to O(km) for the insensitive sparsity case and to
O(αkm) for the sensitive sparsity case, where α denotes the density of user social matrix.
Summary of our experimental results. We conduct experiments on two popularly used dataset, i.e., Epinions [19] and LibraryThing [32]. The results demonstrate that (1) S3Rec achieves the same 2
performance as existing social recommendation models, and (2) S3Rec improves the computation time and communication size of the state-of-the-art (SeSoRec) by about 40× and 423× in average.
Contributions. We summarize our main contributions below: (1) We propose S3Rec, a privacy-preserving cross-platform social recommendation framework, which relies on a general protocol for calculating the social term securely; (2) We propose two secure sparse matrix multiplication protocols based on different sparsity visibility, i.e., insensitive sparsity and sensitive sparsity. We prove that both protocols are secure under semi-honest adversaries; and (3) We empirically evaluate the performance of S3Rec on benchmark datasets. 2 Tools and Recommendation Model
Notation. We use [n] to denote the set {1, ..., n}, and |x| to denote the bit length of x. In terms of
MPC, we denote a secret shared value of x in ZN as
, where N is a positive integer. Also, we let 1 ∈ ZN . We also use (cid:74) x (cid:74)
← to denote the assignment of variables, e.g., x ← 4. 0 denote P0’s share, and (cid:75) 1 denote P1’s share, where x (cid:75) x (cid:74) 0 +
= x x x (cid:75) (cid:75) (cid:74) (cid:74) (cid:75) (cid:74) (cid:75) 2.1 Tools
In this section, we introduce several secure computation tools used in our work.
Multi-Party Computation (MPC). MPC is a cryptographic tool which enables multiple par-ties (say, n parties) to jointly compute a function f (x1, ..., xn), where xi is i-th party’s private in-put. MPC protocols ensure that, at the end of the protocol, parties eventually learn nothing but their own input and the function output. MPC has been widely-used in secure machine learn-ing systems such as PrivColl [31] and CrypT-Flow [15], most of which support a wide range of linear (e.g. addition, multiplication) and non-linear functions (e.g. equality test, comparison).
Here, we present three popular MPC protocols (addition, multiplication, and matrix multiplica-tion), which we will use later in our protocol, x
Add( (cid:74) x
Mul( (cid:74) (cid:75) y
,
): Take two shares as inputs from (cid:74) (cid:75) both parties, Pb∈{0,1} locally calculate y and return b. (cid:75)
): Take two shares as inputs from then evaluate using
, (cid:74) (cid:75) both parties,
Beaver’s Triples [3]. b + x y (cid:74) (cid:75) (cid:74) (cid:75)
MatrixMul(X, Y) (Ofﬂine) Generate km2 Beaver’s triples
← Shr(xi,j)
← Shr(yi,j)
= 0, zi,j (cid:74) (cid:75) (cid:75) (cid:75) xi,j (cid:74) yi,j (cid:74) foreach i ∈ [k], j ∈ [m], let foreach a ∈ [m], b ∈ [m],
← Mul(
← Add( 1 : ∀xi,j ∈ X, P0 invokes 2 : ∀yi,j ∈ Y, P1 invokes 3 : 4 : 5 : 6 : 7 : 8 : endfor return 9 : tmp (cid:74) (cid:75) zi,j (cid:75) (cid:74) endfor xi,a (cid:74) tmp (cid:74)
, (cid:75)
, (cid:75)
Z (cid:74) (cid:75) yb,j (cid:74) zi,j (cid:74)
) (cid:75)
) (cid:75)
Figure 1: Secure matrix multiplication protocol, where Shr is a secret sharing algorithm.
Homomorphic Encryption (HE) scheme. HE is essentially a speciﬁc type of encryption scheme which allows manipulation on encrypted data. More speciﬁcally, HE involves a key pair (pk, sk), where the public key pk is used for encryption and the secret key sk is used for decryption. In this work, we use an additive HE scheme (i.e., Paillier [24]) which allows the following operations:
Encpk(x) ⊕ Encpk(y): addition between two ciphertexts, returns z = Encpk(x + y);
Encpk(x) ⊗ y: multiplication between a ciphertext and a plaintext, returns z = Encpk(x · y).
Private Information Retrieval (PIR). Now, we introduce single-server PIR [1]. In this setting, we assume there is a server and a client, where the server holds a database DB = {d1, ..., dn} with n elements, and the client wants to retrieve DBi while hiding the query index i from the server. Roughly, a PIR protocol consists of a tuple of algorithm (PIR.Query, PIR.Response, PIR.Extract). First, the client generates a query q ← PIR.Query(i) from an index i, and then sends query q to the server.
The server then is able to generate a response r ← PIR.Response(DB, q) based on the query and 3
database DB, and returns r to the client. Finally, the client extracts the result from server’s response
DBi ← PIR.Extract(r).
Client
Server q ← PIR.Query(i) q r
DBi ← PIR.Extract(r) r ← PIR.Response(DB, q)
Figure 2: An overview of Private Information Retrieval (PIR). 2.2 Recommendation model
Recall that we assume there are two platforms, a rating platform P0, and a social platform P1. We assume P0 holds a private rating matrix R ∈ Rm×n, and P1 holds a private user social matrix
S ∈ Rm×m, where n and m denote the number of items and their common users, respectively. Also, we denote the user latent factor matrix as U ∈ Rk×m and item latent factor matrix as V ∈ Rk×n, where k is the dimension of latent factors. We further deﬁne an indication matrix I ∈ Rm×n, where
Ii,j denotes whether user i has rated item j.
Existing work [27] summarizes factorization based social recommendation models as the combination of a “basic factorization model” and a “social information model”. To date, different kinds of social information models have been proposed [18, 14], and their common intuition is that users with social relations tend to have similar preferences. In this work, we focus on the classic social recommendation model, i.e., Soreg [18], which aims to learn U and V by minimizing the following objective function, m (cid:88) n (cid:88) i=1 j=1 1 2
Ii,j (cid:0)ri,j − u∗,i
T v∗,j (cid:1)2
+
λ 2 m (cid:88) i=1 (cid:107)u∗,i(cid:107)2
F +
λ 2 n (cid:88) j=1 (cid:107)v∗,j(cid:107)2
F +
γ 2 m (cid:88) m (cid:88) i=1 f =1 si,f (cid:107)u∗,i − u∗,f (cid:107)2
F , (1) where the ﬁrst term is the basic factorization model, the last term is the social information model, and the middle two terms are regularizers, (cid:107) · (cid:107)2
F is the Frobenius norm, λ and γ are hyper-parameters. If we denote D ∈ Rm×m as a diagonal matrix with diagonal element db = (cid:80)m c=1 sb,c and E ∈ Rm×m as a diagonal matrix with diagonal element ei = (cid:80)m b=1 sb,i. The gradients of L in Eq. (1) with respect to U and V are, (cid:18)(cid:16) (cid:17)T (cid:19)
= −V
R − UT V
◦ I
+ λU
+ (2)
γ 2 (cid:124)
U(DT + ET ) − γUST (cid:125) (cid:123)(cid:122)
Social term: computed byP0 and P1 collaboratively
, (cid:124) (cid:123)(cid:122)
Rating term: computed byP0 locally (cid:125)
∂L
∂U (cid:18)(cid:16)
R − UT V (cid:17)T
= −U (cid:19)
◦ I
+ λV
∂L
∂V (cid:124) (cid:123)(cid:122)
Rating term: computed byP0 locally (cid:125)
. (3) 3 Framework
We summarize our proposed S3Rec framework in Figure 3. To begin with, we assume that party
P0 holds the rating matrix R and P1 holds the social matrix S. At ﬁrst, P0 randomly initializes
U ←$ Rk×m and V ←$ Rk×n. Then, for each iteration (while the model dose not coverage), we let
P0 and P1 jointly evaluate the social term deﬁned in Eq 2. P0 then locally calculates the rating term in Eq 2 and Eq 3, as well as ∂L/∂U and ∂L/∂V. Party P0 then locally updates U and V accordingly and ends the iteration.
Communication efﬁciency. In our framework, the only communication between two parties occurs in the ST-MPC protocol. Since we choose additive secret sharing, the Add protocol contains only local computation, we claim that the communication efﬁciency of S3Rec signiﬁcantly relies on the efﬁciency of matrix multiplication protocol. We give a popular MatrixMul protocol in Figure 4
Global Parameter: Regularization strength γ, and learning rate θ.
Input: Private rating matrix R from platform P0, private user social matrix S from platform P1.
Output: Platform P0 receives the user latent matrix U and item latent matrix V. 1 : Platform P0 initializes U and V, 2 : while not coverage, 3 : 4 : 5 : 6 : 7 : endwhile 8 :
P0 and P1 securely calculate the social term ←
P0 locally computes the rating terms
P0 locally updates U by U ← U − θ · ∂L/∂U
P0 locally updates V by V ← V − θ · ∂L/∂V return U and V to platform P0
ST-MPC(γ, U, D, E, S) 1 : 2 : 3 : 4 : (cid:75)
← MatrixMul(γU/2, DT + ET )
R0 (cid:74)
← MatrixMul(−γU, ST )
R1 (cid:75) (cid:74)
)
R1
,
R0
← Add(
R (cid:75) (cid:74) (cid:75) (cid:74) (cid:74)
) to P0 return Rec(
R (cid:75) (cid:74) (cid:75)
Figure 3: Our proposed S3Rec framework, where MatrixMul stands for secure matrix multiplication protocol, Add stands for secure add protocol, Rec stands for reconstruction protocol for secret sharing. 1 and analyze its efﬁciency in our framework. The protocol in Figure 1 requires km2 log2 N bit online communication, where m is the number of users and k is the dimension of latent factors. As for the usual case where the number of users is ≈ 104, k = 10, and logN = 64, one invocation of
MatrixMul protocol would have a total communication of around 7.4GB. Considering 100 iterations of our framework, this leads to ≈ 1491GB communication, which is impractical. Fortunately, the social matrices (D, E, and S) are highly sparse in social recommendation. In the following section, we propose a PIR-based sparse matrix multiplication protocol with better communication efﬁciency. 3.1 Secure sparse matrix multiplication
Essentially, any matrix could be represented by a value vector and a location vector, where the value vector contains all non-zero values and the location vector contains locations of those values. That is, m2 , vy ∈ Rt), where t is a sparse matrix Y ∈ Rm×m can be represented by a pair of vectors (ly ∈ Nt the number of non-zero values in Y.
Dense-sparse matrix multiplication. Considering the case where X ∈ Rk×m is the dense matrix from P0 and Y ∈ Rm×m is the sparse matrix from P1. Now we consider the following two cases.
Case 1: insensitive sparsity, i.e., insensitive ly and sensitive vy. This refers to the case where the locations of zero values are public or contain no sensitive information. Take the social matrices (D and E) for example, both of them are diagonal, and thus the location vector is insensitive while the value vector is still sensitive.
ﬁlter with ly xi,∗
P0
P1
Tx
Tx(i)
Many Mul() and Add()
Ty
Ty(j) yj,∗
ﬁlter with ly
Figure 4: Matrix multiplication with insensitive sparsity.
Our protocol mainly works as follows. First, P0 and P1 parse X and Y into two tables Tx and Ty separately, where the value set of each bin in Tx is a subset of one row in X, that is, Tx(i) ⊆ xi,∗. Similarly, bin set in Ty is a subset of one column in Y,
Ty(i) ⊆ y∗,i. The intuition behind is to use bins to contain only the necessary values needed to calculate the output value (which means ﬁlter out the zero multiplies in each bin). Take the ﬁrst bin for example (that is, Tx(0) and Ty(0)), for j ∈ [m], Tx(0) contains all x0,j where yj,0 is a non-zero value, and Ty(0) contains all non-zero yj,0. In order to get the ﬁnal result, we perform the secure inner product protocol on Tx(0) and Ty(0), and denote the result as z0,0
. We show the high level idea in Figure 4. By doing this, our protocol concretely consumes (cid:75) (cid:74) k|ly| Beaver’s triples and therefore has O(k|ly|) online communication complexity. Figure 5 shows the technical details of our proposed protocol for case 1. For Line 1 in ST-MPC (Figure 3), clearly both parties know that D and E are diagonal matrices, that is, |ly| = m. Therefore, our proposed protocol in Figure 4 can drop the complexity from O(km2) to O(km). 5
MatrixMul(X, Y) with insensitive sparsity (Ofﬂine) Generate km2 Beaver’s triples for j ∈ [k] do 1 : ∀(i, j) ∈ ly, P1 pushes yi,j into Ty(j) 2 : 3 : 4 : 5 : 6 : 7 : 8 : 9 : 10 : endfor
Both parties let
P0 invokes zi,j (cid:74) (cid:75) endfor zi,j (cid:74)
← Shr(v), P1 invokes u (cid:74) v (cid:74)
= Add(Mul( zi,j (cid:74)
), (cid:75) v (cid:74)
) (cid:75)
, (cid:75) (cid:75) (cid:75)
P1 lets Ty = ∅
∀a ∈ [m], b ∈ [m], if (i, a) ∈ ly, P0 pushes xa,b into Tx(a) for j ∈ [m] do
= 0, then, for all values v ∈ Tx(i), u ∈ Ty(j)
← Shr(u) v (cid:74) (cid:75)
MatrixMul(X, Y) with sensitive sparsity (Ofﬂine) P0 generates an additive HE key pair (pk, sk) , then sends pk to P1 1 : ∀i ∈ [k], j ∈ [m], P0 lets ei,j = Enc(pk, xi,j), and lets E be the encrypted matrix 2 : ∀(i, j) ∈ ly, P1 pushes yi,j into Ty(j), also, P1 invokes qi,j ← PIR.Query(i + jk) 3 : P1 sends the query set (denoted as q) to P0 4 : ∀qi,j ∈ q, P0 invokes ri,j ← PIR.Response(E, qi,j). 5 : P0 sends the response set (denoted as r) to P1 6 : ∀ri,j ∈ r, P1 invokes ei,j ← PIR.Extract(ri,j) and pushes ei,j to T (cid:48) 7 : 8 : for i ∈ [k], j ∈ [m] do e(i) e(i), u ∈ Ty(j), P0 invokes βi,j = v ⊗ u ⊕ βi,j
P0 lets βi,j = Encpk(0)
∀v ∈ T (cid:48)
P0 samples random numbers gi,j ←$ Zδ, then letsβi,j = gi,j ⊕ βi,j zi,j
P0 sends βi,j to P1, then lets (cid:74) 9 : 10 : 11 : 12 : endfor 13 : P0decrypts all receving messages and lets 14 : zi,j (cid:74) 1 = Decsk(βi,j) (cid:75) 0 = −gi,j (cid:75) return
Z (cid:74) (cid:75)
Figure 5: Dense-sparse MatrixMul(X, Y) with insensitive and sensitive sparsity protocols, where we have X ∈ Rk×m, Y ∈ Rm×m.
Lemma 1. The ﬁrst protocol in Figure 5 is secure against semi-honest adversary if we assume the existence of secure addition and multiplication semi-honest MPC protocols.
Proof. Please ﬁnd the proof in the Technical Appendix.
Case 2: sensitive sparsity, i.e., sensitive ly and sensitive vy. For a more general case, where both the location vector and the value vector contain sensitive information. Take the social matrix S for instance, its location vector indicates the existence of a social relation between two users, its value vector further shows the strength of their relation, and both of which are sensitive.
In this case, both the dense matrix X and the entire sparse matrix Y are sensitive. Following the idea in case 1, the matrix multiplication protocol should ﬁrst generate Tx, Ty according to vx, vy and ly, and then perform the inner product multiplication for each aligned bins in Tx, Ty. Still, P1 can generate
Ty according to its own inputs vy, ly. However, P0 cannot generate Tx directly, since vx is kept by itself while ly is held by P1. We make a communication and computation trade-off by leveraging PIR techniques, and as a result, our PIR-based approach has lower concrete communication, and overall is faster than the baseline protocol.
We show the high-level idea of our PIR-based protocol in Figure 6. The intuition behind is to let P1 obliviously ﬁlter each bin in Tx since both value vector and location vector are sensitive. In summary, 6
ﬁrst P0 encrypts all the values in Tx, the encrypted table is denoted as Te. Then P1 and P0 invoke PIR protocol, where P0 acts as server and sets Te as PIR database, P1 acts as client and parses ly to many
PIR queries. At the end of PIR protocol, P1 receives the encrypted and ﬁltered table T (cid:48) e. Afterwards
P1 performs secure inner product evaluation. By doing this, the communication complexity drops from O(km2) to O(αkm), compared with the simple solution. The details of our protocol are shown in Figure 5. For Line 2 in ST-MPC (Figure 3), the social matrix (S) is sparse in nature, and thus our proposed protocol in Figure 6 can signiﬁcantly improve its efﬁciency. In summary, with our proposed two secure MatrixMul protocols, one can securely calculate the social term efﬁciently. For instance, again considering the social recommendation with ≈ 104 users, our proposal only requires a total of
≈ 3.6GB communication for each iteration.
Lemma 2. The second protocol in Figure 5 is secure against semi-honest adversary with the leakage of |ly| if we assume the existence of a secure PIR protocol.
Proof. Please ﬁnd the proof in the Technical Appendix. 3.2 Security discussions of the social term
P0
PIR encrypt matrix E ← Enc(X)
In S3Rec, two parties jointly calculate the social term γU(DT +
ET )/2 − γUST and then reveal the social term to P0 (see
Eq. (2)). The security of S3Rec relies on whether P0 can resolve the social matrix ST given its own inputs U and the social term.
We claim that this is difﬁcult because, the number of equations
T (#epoch, 100 in our experiments) is much smaller than that of the variables n (#user, much more than 100 in practice), which indicates that there are inﬁnite solutions for this. In practice,
T < n can be easily satisﬁed for both the social platform and the rating platform. The reasons are two-folds. First, our proposed framework is secure against a semi-honest adversary (which is a popular threat model in the secure computation literature), i.e., both platforms will strictly follow the protocol execution. Second, the number of items whose size/scale is usually large and publicly-known in practice. Therefore, both platforms can agree on an iteration number T such that T < n, before running our proposed framework. Each platform can shut down the program if it reaches the pre-deﬁned number of iterations. Moreover, the reveal of the social term to P0 could be avoided by taking the whole model training procedure as an MPC functionality and designing a complicated protocol for it. Inevitably, such protocol introduces impractical communication costs, and we leave how to solve this efﬁciently as a future work.
Figure 6: Matrix multiplication with sensitive sparsity.
Many ⊕ and ⊗
ﬁlter with ly
T (cid:48) e(i)
Ty(j) yj,∗
P1
T (cid:48) e
Ty ly 4 Experiments
Our experiments intend to answer the following questions. Q1: How do the social recommendation models using both rating data on P0 and social data on P1 outperform the model that only uses rating data on P0 (Section 4)? Q2: How does our model perform compared with SeSoRec (Section 4)? Q3:
How does the social data sparsity affect the performance of SeSoRec and our model (Section 4)?
Implementation and setup. We run our experiments on a machine with 4-Core 2.4GHz Intel Core i5 with 16G memory, we compile our program using a modern C++ compiler (with support for C++ standard 17). In addition, our tests were run in a local network, with ≈ 3ms network latency. For additive HE scheme, we choose the implementation of libpaillier1. Also, we use Seal-PIR2 with same parameter setting as the original paper [1]. For security, we choose 128-bit computational security and 40-bit statistical security as recommended by NIST [2]. Similarly we leverage the generic ABY library3 to implement SeSoRec [5] and MPC building blocks such as addition, multiplication, and truncation. In particular, we choose 64-bit secret sharing in all our experiments. 1libpaillier: http://acsc.cs.utexas.edu/libpaillier/, GPL license 2Seal-PIR: https://github.com/microsoft/SealPIR, MIT license 3ABY: https://github.com/encryptogroup/ABY, LGPL license 7
Dataset. We choose two popular benchmark datasets to evaluate the performance of our proposed model, i.e., Epinions [19] and LibraryThing (Lthing) [32], both of which are popularly used for evaluating social recommendation tasks. Following existing work [5], we remove the users and items that have less than 15 interactions for both datasets. We summarize the statistics of both datasets after process in Table 1. Notice that we assume users’ rating data are located at P0, users’ social data are located at P1, and P0 and P1 share the same user set.
Table 1: Dataset statistics.
Dataset
Epinions
Lthing
#user
#item
#rating rating density
#social relation social density 11,500 15,039 7,596 14,957 283,319 529,992 0.32% 0.24% 275,117 44,710 0.21% 0.02%
Comparison Methods. We compare S3Rec with the following classic and state-of-the-art models: – MF [20] is a classic matrix factorization model that only uses rating data on P0, i.e., when γ = 0 for S3Rec. – Soreg [18] is a classic social recommendation model, which does not consider data privacy and assumes both rating data and social data are available on P0. – SeSoRec [5] tries to solve the privacy-preserving cross-platform social recommendation problem, but suffers from security and efﬁciency problem.
Hyper-parameters. For all the model, during comparison, we set k = 10. We tune learning rate θ and regularizer parameter λ in {10−3, 10−2, ..., 101} to achieve their best values. We also report the effect of K on model performance.
Metrics. We will evaluate both accuracy and efﬁciency of our proposed model. For accuracy, we choose Root Mean Square Error (RMSE) as the evaluation metric, since ratings range in [0, 5]. For efﬁciency, we report the computation time (in seconds) and the communication size between P0 and
P1 (in gigabytes), if has, for all the models. We use ﬁve-fold cross-validation during experiments.
Performance Comparison. We ﬁrst compare the model performances in terms of accuracy (RMSE) and efﬁciency (total time and communication). Table 2 shows the time and communication for each epoch, where time is shown in seconds, and communication is shown in GB.
From those Tables, we ﬁnd that: (1) the use of social information can indeed improve the recom-mendation performance of the rating platform, e.g., 1.193 vs. 1.062 and 0.927 vs. 0.098 in terms of
RMSE on Epinions and Lthing, respectively. This result is consistent with existing work from [18, 5]; (2) despite the same RMSE as SeSoRec and Soreg, S3Rec signiﬁcantly improves the efﬁciency of
SeSoRec, especially on the more sparse Lthing dataset, reducing the total time for one epoch from around 4.5 hours to around 4.5 minutes, and reducing the total communication from nearly 1.3TB to around 2.2GB. This yields an improvement of 18.57× faster, and 224.8× less communication on
Epinions and 61.37× faster and 620.2× less communication on Lthing, respectively.
Effect of Social Data Sparsity. Next, we try to study the effect of social data sparsity on training efﬁciency. In order to do this, we sample the social relation of both datasets with a rate of 0.8, 0.6, and 0.4. As the result, the RMSEs of both SeSoRec and S3Rec decrease to 1.0932, 1.1373, 1.1751 on Epinions dataset, and 0.9112, 0.9187, 0.9210 on Lthing dataset. The rational behind is that recommendation performance decreases with the number of social relations. We also report the efﬁciency of both models on Epinions and Lthing datasets in Table 3. From it, we can ﬁnd that the computation time and communication size of SeSoRec are constant no mater what the sample rate is.
In contrast, the computation time and communication size of S3Rec decrease linearly with sample rate. This result beneﬁts from that S3Rec can deal with sparse social data with our proposed sparse matrix multiplication protocols.
Effect of k. For efﬁciency, we report the running time and communication size of SeSoRec and
PriorRec w.r.t k in Table 4, where we use the Epinions dataset. From it, we can get that in average,
S3Rec improves SeSoRec 18.6x in terms of total running time and 225x in terms of communication.
More speciﬁcally, we observe that (1) the total running time of both SeSoRec and PriorRec increase 8
Table 2: Comparison results of different models in terms of model accuracy (in RMSE), running time (in seconds), and communication size (in GB), on Epinions and Lthing datasets.
Models
RMSE
Ofﬂine Time
Total Time
Ofﬂine Comm.
Total Comm.
MF 1.193
-3.846
--Epinions dataset
SeSoRec
Soreg 1.062
-40.50
--1.062 7,271 7,799 788.3 798.6
S3Rec 1.062 10.86 419.9 0 3.552
Lthing dataset
MF
Soreg
SeSoRec 0.927
-9.596
--0.908
-57.76
--0.908 14,450 16,084 1,348 1,365
S3Rec 0.908 8.912 262.1 0 2.201
Table 3: Comparison results by varying social data sparsity on Epinions and Lthing datasets.
Metric
Models
Total time (Seconds)
Total communication (GB)
SesoRec
S3Rec (Improvement)
SesoRec
S3Rec (Improvement)
Epinions 0.6 7,799 381.2 (20.46x) 798 3.29 (243x) 0.8 7,799 401.8 (19.41x) 798 3.46 (231x) 0.4 7,799 366.3 (21.29x) 798 3.12 (255x) 0.4 16,084 194 (82.91x) 1,366 1.62 (843x)
Lthing 0.6 16,084 217 (74.12x) 1,366 1.82 (751x) 0.8 16,084 238 (67.58x) 1,366 2.01 (680x)
Table 4: Effect of k on running time and communication size on Epinions dataset.
Models
Ofﬂine Time
Total Time
Ofﬂine Comm.
Total Comm. k = 10 7,271 7,799 788.3 798.6
SeSoRec k = 15 12,651 13,565 1,182 1,198 k = 20 17,676 19,585 1,577 1,597 k = 10 10.86 419.9 0 3.552
S3Rec k = 15 9.667 449.6 0 3.552 k = 20 9.815 527.4 0. 3.552 with k, but the increase rate of S3Rec is slower than that of SeSoRec; (2) the communication size of SeSoRec increases with k, in contrast, the communication size of S3Rec is constant. This result demonstrates that our proposed S3Rec has better scalability than SeSoRec in terms of both running time and communication size. 5