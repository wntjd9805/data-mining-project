Abstract
Despite a series of recent successes in reinforcement learning (RL), many RL algorithms remain sensitive to hyperparameters. As such, there has recently been interest in the ﬁeld of AutoRL, which seeks to automate design decisions to create more general algorithms. Recent work suggests that population based approaches may be effective AutoRL algorithms, by learning hyperparameter schedules on the ﬂy. In particular, the PB2 algorithm is able to achieve strong performance in
RL tasks by formulating online hyperparameter optimization as time varying GP-bandit problem, while also providing theoretical guarantees. However, PB2 is only designed to work for continuous hyperparameters, which severely limits its utility in practice. In this paper we introduce a new (provably) efﬁcient hierarchical approach for optimizing both continuous and categorical variables, using a new time-varying bandit algorithm speciﬁcally designed for the population based training regime.
We evaluate our approach on the challenging Procgen benchmark, where we show that explicitly modelling dependence between data augmentation and other hyperparameters improves generalization. 1

Introduction
Reinforcement Learning (RL [75]) is a paradigm whereby agents learn to make sequential decisions through trial and error. In the past few years there have been a series of breakthroughs using RL in games [70, 44, 6] and robotics [50, 33], which have led to a surge of interest in the machine learning community. Beyond these examples, RL offers the potential to impact vast swathes of society, from autonomous vehicles to robotic applications in healthcare and industry.
Despite the promising results, RL is notoriously difﬁcult to use in practice.
In particular, RL algorithms are incredibly sensitive to hyperparameters [23, 2, 18, 27, 9, 47], with careful tuning often the difference between success and failure. Furthermore, as new and more complex algorithms are introduced, the search space continues to grow [2]. As a result, it is often challenging to reproduce RL results given the signiﬁcant number of moving parts in modern algorithms [2]. This makes it almost impossible to apply these methods in novel settings, where optimal hyperparameters are unknown.
On the other hand, the capability of current methods may be understated, as better conﬁgurations could boost performance [10, 84].
In this work we focus on the recent impressive results for Population Based Training (PBT, [29, 39]), which has demonstrated strong performance in a variety of prominent RL settings [65, 84, 43, 19, 28].
PBT works by training agents in parallel, with an evolutionary outer loop optimizing hyperparameters, periodically replacing weaker agents with perturbations of stronger ones. However, since PBT relies on random search to explore the hyperparameter space, it requires a large population size which can be prohibitively expensive for small and even medium-sized labs.
úCorrespondence to jackph@robots.ox.ac.uk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: The components of related approaches, and their relative trade-offs.
Algorithm
Population Based? Efﬁcient Continuous? Efﬁcient Categorical?
PBT/PBA [29, 25]
PB2 [52]
CoCaBO [63]
This work 3 3 7 3 7 3 3 3 7 7 3 3
In order to achieve similar success with a smaller computational budget, the recent Population Based
Bandits (PB2, [52]) algorithm improved sample efﬁciency by introducing a probabilistic exploration step, backed by theoretical guarantees. Unfortunately, a key limitation of PB2 is that it only addresses the problem of efﬁciently selecting continuous hyperparameters, inheriting the random search method for categorical variables from the original PBT. This is not only inefﬁcient but also crucially ignores potential dependence between continuous and categorical variables. Since the Gaussian Process (GP,
[58]) model is completely unaware of the changing categories, it is unable to differentiate between trials that may have completely different categorical hyperparameters. In this paper we introduce a new hierarchical approach to PB2, which can efﬁciently model both continuous and categorical variables, with theoretical guarantees. Our main contributions are the following:
Technical: We introduce a new PB2 explore step which can efﬁciently choose between both categori-cal and continuous hyperparameters in a population based training setup. In particular, we propose a new time-varying batch multi-armed bandit algorithm, and introduce two hierarchical algorithms which condition on the selected categorical variables. We show our new approach achieves sublinear regret, extending the results for the continuous case with PB2.
Practical: We scale our approach to test generalization on the Procgen benchmark [13], an active area of research. We demonstrate improved performance when explicitly modelling dependence between data augmentation type and continuous hyperparameters (such as learning rate) vs. baselines using random search to select the data augmentation. 2