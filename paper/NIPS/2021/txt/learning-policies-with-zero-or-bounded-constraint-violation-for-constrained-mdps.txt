Abstract
√
√
We address the issue of safety in reinforcement learning. We pose the problem in an episodic framework of a constrained Markov decision process. Existing results have shown that it is possible to achieve a reward regret of ˜O(
K) while allowing an ˜O(
K) constraint violation in K episodes. A critical question that arises is whether it is possible to keep the constraint violation even smaller. We show that when a strictly safe policy is known, then one can conﬁne the system to zero constraint violation with arbitrarily high probability while keeping the reward regret of order ˜O(
K). The algorithm which does so employs the principle of optimistic pessimism in the face of uncertainty to achieve safe exploration. When no strictly safe policy is known, though one is known to exist, then it is possible to restrict the system to bounded constraint violation with arbitrarily high probability.
This is shown to be realized by a primal-dual algorithm with an optimistic primal estimate and a pessimistic dual update.
√ 1

Introduction
Reinforcement learning (RL) addresses the problem of learning an optimal control policy that maximizes the expected cumulative reward while interacting with an unknown environment [25].
Standard RL algorithms typically focus only on maximizing a single objective. However, in many real-world applications, the control policy learned by an RL algorithm has to additionally satisfy stringent safety constraints [9, 4]. For example, an autonomous vehicle may need to reach its destination in the minimum possible time without violating safety constraints such as crossing the middle of the road. The Constrained Markov Decision Process (CMDP) [2, 12] formalism, where one seeks to maximize a reward while satisfying safety constraints, is a standard approach for modeling the necessary safety criteria of a control problem via constraints on cumulative costs.
Several policy-gradient-based algorithms have been proposed to solve CMDPs. Lagrangian-based methods [26, 24, 21, 16] formulate the CMDP problem as a saddle-point problem and optimize it via primal-dual methods, while Constrained Policy Optimization [1, 29] (inspired by the trust region policy optimization [23]) computes new dual variables from scratch at each update to maintain constraints during learning. Although these algorithms provide ways to learn an optimal policy, performance guarantees about reward regret, safety violation or sample complexity are rare.
One class of RL algorithms for which performance guarantees are available follow the principle of Optimism in the Face of Uncertainty (OFU) [8, 10, 22], and provide an ˜O(
K) guarantee
√
∗The ﬁrst two authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Regret and constraint violation comparisons for algorithms on episodic CMDPs
Algorithm
OPDOP [8]
OptCMDP [10] 3
OptCMDP-bonus [10] 3
OptDual-CMDP [10] 3
OptPrimalDual-CMDP [10] 3
C-UCRL [30] 4
OptPess-LP
OptPess-PrimalDual
Regret2
˜O(H 3(cid:112)|S|2|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(T 3 4 )
˜O( H 3
τ −c0
˜O( H 3
τ −c0 (cid:112)|S|3|A|K) (cid:112)|S|3|A|K) O(1) 5 0
Constraint violation 2
˜O(H 3(cid:112)|S|2|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K)
˜O(H 2(cid:112)|S|3|A|K) 0
√ for the reward regret, where K is the number of episodes. However, these algorithms also have
˜O(
K) safety violations. Such signiﬁcant violation of the safety constraints during learning may be unacceptable in many safety-critical real-world applications such as the control of autonomous vehicles or power systems. These applications demand a class of safe RL algorithms that can provably guarantee safety during learning. With this goal in mind, we aim to answer the following open theoretical question in this paper:
Can we design safe RL algorithms that can achieve an ˜O(
K) regret with respect to the performance objective, while guaranteeing zero or bounded safety constraint violation with arbitrarily high probability?
√
We answer the above question afﬁrmatively by proposing two algorithms and establishing their stringent safety performance during learning. Our focus is on the tabular episodic constrained RL setting (unknown transition probabilities, rewards, and costs). The key idea behind both algorithms is a concept used earlier for safe exploration in constrained bandits [20, 17], which we call “Optimistic
Pessimism in the Face of Uncertainty (OPFU)” here. The optimistic aspect incentivizes the algorithm for using exploration policies that can visit new state-action pairs, while the pessimistic aspect disincentivizes the algorithm from using exploration policies that can violate safety constraints. By carefully balancing optimism and pessimism, the proposed algorithms guarantee zero or bounded safety constraint violation during learning while achieving an ˜O(
K) regret with respect to the reward objective.
√
The two algorithms address two different classes of the safe learning problem: whether a strictly safe policy is known a priori or not. The resulting exploration strategies are very different in the two cases. 1. OptPess-LP Algorithm: This algorithm assumes the prior knowledge of a strictly safe policy. It ensures zero safety constraint violation during learning with high probability and utilizes the linear programming (LP) approach for solving a CMDP problem. The algorithm (cid:112)|S|3|A|K) with respect to the performance objective, achieves a reward regret of ˜O( H 3
τ −c0 where H is the number of steps per episode, τ is the given constraint on safety violation, c0 is the known safety constraint value of a strictly safe policy π0, and |S| and |A| are the number of states and actions respectively. 2. OptPess-PrimalDual Algorithm: This algorithm addresses the case where no strictly safe policy, but a feasible strictly safe cost is known. By allowing a bounded (in K) safety cost, it opens up space for exploration. The OptPess-PrimalDual algorithm avoids linear programming and its attendant complexity and exploits the primal-dual approach for solving a CMDP problem. The proposed approach improves the computational tractability, while 2This table is presented for K ≥ poly(|S|, |A|, H), with polynomial terms independent of K omitted. 3Efroni et al. [10] use N , the maximum number of non-zero transition probabilities across the entire state-action space, in their regret and constraint violation analysis. For consistency, we use |S|2|A| to bound N . 4Zheng et al. [30] assumes a known transition kernel and analyzes regret in the long-term average setting. 5The detailed constraint violation is O
, which is independent of
C (cid:48)(cid:48)H + H 2(cid:112)|S|3|A|C (cid:48)(cid:48) log(C (cid:48)(cid:48)/δ(cid:48)) (cid:17) (cid:16)
K. Here, δ(cid:48) = δ/(16|S|2|A|H) and C (cid:48)(cid:48) = O( H4|S|3|A| (τ −c0)2 log H4|S|3|A| (τ −c0)2δ(cid:48) ). 2
ensuring a bounded safety constraint violation during learning and a reward regret of
˜O( H 3
τ −c0 (cid:112)|S|3|A|K) with respect to the objective.
Compared with the other methods listed in Table 1, though the proposed algorithms have an additional
H/(τ − c0) or (cid:112)|S|/(τ − c0) factor in the regret bounds, they are able to reduce the constraint violation to zero or constant with high probability. This improvement in safety can be extremely important for many mission-critical applications. 1.1