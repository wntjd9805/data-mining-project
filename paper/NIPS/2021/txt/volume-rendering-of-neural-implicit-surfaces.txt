Abstract
Neural volume rendering became increasingly popular recently due to its success in synthesizing novel views of a scene from a sparse set of input images. So far, the geometry learned by neural volume rendering techniques was modeled using a generic density function. Furthermore, the geometry itself was extracted using an arbitrary level set of the density function leading to a noisy, often low ﬁdelity reconstruction. The goal of this paper is to improve geometry representation and reconstruction in neural volume rendering. We achieve that by modeling the vol-ume density as a function of the geometry. This is in contrast to previous work modeling the geometry as a function of the volume density. In more detail, we deﬁne the volume density function as Laplace’s cumulative distribution function (CDF) applied to a signed distance function (SDF) representation. This simple density representation has three beneﬁts: (i) it provides a useful inductive bias to the geometry learned in the neural volume rendering process; (ii) it facilitates a bound on the opacity approximation error, leading to an accurate sampling of the viewing ray. Accurate sampling is important to provide a precise coupling of geometry and radiance; and (iii) it allows efﬁcient unsupervised disentanglement of shape and appearance in volume rendering. Applying this new density repre-sentation to challenging scene multiview datasets produced high quality geometry reconstructions, outperforming relevant baselines. Furthermore, switching shape and appearance between scenes is possible due to the disentanglement of the two. 1

Introduction
Volume rendering [18] is a set of techniques that renders volume density in radiance ﬁelds by the so called volume rendering integral. It has recently been shown that representing both the density and radiance ﬁelds as neural networks can lead to excellent prediction of novel views by learning only from a sparse set of input images. This neural volume rendering approach, presented in [21] and developed by its follow-ups [34, 2] approximates the integral as alpha-composition in a differentiable way, allowing to learn simultaneously both from input images. Although this coupling indeed leads to good generalization of novel viewing directions, the density part is not as successful in faithfully predicting the scene’s actual geometry, often producing noisy, low ﬁdelity geometry approximation.
We propose VolSDF to devise a different model for the density in neural volume rendering, leading to better approximation of the scene’s geometry while maintaining the quality of view synthesis.
The key idea is to represent the density as a function of the signed distance to the scene’s surface, see Figure 1. Such density function enjoys several beneﬁts. First, it guarantees the existence of a well-deﬁned surface that generates the density. This provides a useful inductive bias for disentangling density and radiance ﬁelds, which in turn provides a more accurate geometry approximation. Second, we show this density formulation allows bounding the approximation error of the opacity along rays.
This bound is used to sample the viewing ray so to provide a faithful coupling of density and radiance
ﬁeld in the volume rendering integral. E.g., without such a bound the computed radiance along a ray (pixel color) can potentially miss or extend surface parts leading to incorrect radiance approximation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: VolSDF: given a set of input images (left) we learn a volumetric density (center-left, sliced) deﬁned by a signed distance function (center-right, sliced) to produce a neural rendering (right). This deﬁnition of density facilitates high quality geometry reconstruction (gray surfaces, middle).
A closely related line of research, often referred to as neural implicit surfaces [22, 38, 14], have been focusing on representing the scene’s geometry implicitly using a neural network, making the surface rendering process differentiable. The main drawback of these methods is their requirement of masks that separate objects from the background. Also, learning to render surfaces directly tends to grow extraneous parts due to optimization problems, which are avoided by volume rendering. In a sense, our work combines the best of both worlds: volume rendering with neural implicit surfaces.
We demonstrate the efﬁcacy of VolSDF by reconstructing surfaces from the DTU [12] and Blended-MVS [37] datasets. VolSDF produces more accurate surface reconstructions compared to NeRF [21] and NeRF++ [39], and comparable reconstruction compared to IDR [38], while avoiding the use of object masks. Furthermore, we show disentanglement results with our method, i.e., switching the density and radiance ﬁelds of different scenes, which is shown to fail in NeRF-based models. 2