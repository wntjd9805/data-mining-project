Abstract
Interpreting Deep Reinforcement Learning (DRL) models is important to enhance trust and comply with transparency regulations. Existing methods typically explain a DRL model by visualizing the importance of low-level input features with super-pixels, attentions, or saliency maps. Our approach provides an interpretation based on high-level latent object features derived from a disentangled representation. We propose a Represent And Mimic (RAMi) framework for training 1) an identiﬁable latent representation to capture the independent factors of variation for the objects and 2) a mimic tree that extracts the causal impact of the latent features on DRL action values. To jointly optimize both the ﬁdelity and the simplicity of a mimic tree, we derive a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. Based on this objective, we describe a
Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to ﬁnd the IB-optimal mimic tree. Experiments show that our mimic tree achieves strong approximation performance with signiﬁcantly fewer nodes than baseline models. We demonstrate the interpretability of our mimic tree by showing latent traversals, decision rules, causal impacts, and human evaluation results. 1

Introduction
Deep neural networks have enabled Reinforcement Learning (RL) agents to extract relevant features from image observations and achieve human-level control [1, 2, 3] by modeling action-value functions.
Despite their promising performance, the learned knowledge remains implicit in these black-box neural structures, which hinders understanding the importance of input features and how they inﬂuence decisions. Most previous DRL interpretations aimed to visualize attention masks [4, 5] or saliency maps [6, 7] for input states. These interpretations are commonly based on entangled raw features in high-dimensional input space, and some recent studies [8, 9, 10] showed that the generated attention masks are inconsistent for local samples in that very different attention distributions can yield equivalent predictions. Moreover, these point-wise importance maps do not identify the underlying causal relationships between target variables and complex inputs.
An alternative approach for generating post-hoc global explanations is mimic learning [11], which allows distilling the knowledge from an opaque DRL model to a transparent tree model. Some recent works [12, 13] built linear model trees to imitate the DRL action-value functions. These trees are learned for the high-dimensional input space; therefore they require numerous splits for building a tree with promising regression performance. The large tree size increases the complexity of their interpretations and makes it difﬁcult to generate human-understandable decision rules. Previous models [14, 15] constrained the tree complexity by empirically setting some hyper-parameters (e.g, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
maximum depth), which, however, limits the mimic performance. It is unclear how to jointly optimize the simplicity and the ﬁdelity of tree models to ﬁnd a "sweet-spot".
To ﬁnd an attractive trade-off between simplicity and ﬁdelity, we develop a Represent And Mimic (RAMi) framework based on the Information Bottleneck (IB) principle [16]. The goal of IB is learning a bottleneck representation to compress input signals while maximizing the information about a target variable, which deﬁnes an information-theoretic objective that views mimic learning as a data compression process. Our novel IB objective facilitates learning a mimic tree as the bottleneck to approximate the action advantages from the DRL model with a minimum number of splits. To learn IB-optimal mimic trees for DRL interpretation, RAMi integrates two components. 1) An Identiﬁable Multi-Object Network (IMONet) that detects objects from high-dimensional input space and learns an object representation to embed the objects with a limited number of latent variables. This representation is a) well-disentangled, meaning that each variable independently captures one factor of variation in a detected object, b) identiﬁable, allowing the mimic tree to uniquely extract the underlying causal relation from DRL models, and c) interpretable, in order to generate understandable DRL explanations. The interpretability is demonstrated with illustrative examples of latent traversals and human evaluation. 2) A Monte Carlo Regression Tree Search (MCRTS) algorithm that learns mimic trees based on the latent features from the object representation. MCRTS incorporates the IB principle into its search heuristic by deriving a Minimum Description Length (MDL) objective. Unlike previous tree learners that deterministically select a split by evaluating only its local inﬂuence, MCRTS looks ahead to global mimic performance and generates a compact distribution of candidate trees with explorations.
Our empirical evaluation shows that IMONet+MCRTS achieves a promising mimic performance with signiﬁcantly fewer splits than other baseline models and an important reduction in the size of tree interpretation. To demonstrate how the learned mimic tree makes its target action-advantage function interpretable we show causal relations and results of counterfactual interventions can be extracted from it. We validate the tree interpretations by 1) comparison with previous interpretable
RL models, and 2) a survey that collects human evaluations.
Contributions. 1) We propose a RAMi framework that enables representing the object information with our novel IMONet and searching for the optimal mimic tree with the MCRTS algorithm. 2) We derive an information-theoretic IB-MDL objective that incorporates both the ﬁdelity and simplicity for mimic tree learning. 3) We introduce our method of leveraging the mimic tree to compute feature importance and extract causal relations from a DRL model. 2 Represent and Mimic Framework
We propose a Represent And Mimic (RAMi) framework based on the Information Bottleneck (IB) principle. RAMi separately interprets the feature extraction and the decision making process of a
DRL model with 1) an interpretable latent representation and 2) a transparent mimic tree. 2.1 Mimic Learning for DRL
RAMi applies mimic learning [11] to learn post-hoc interpretations for a DRL model by transferring its knowledge to a mimic tree φ. To facilitate the knowledge distillation, mimic learners utilize soft-outputs from deep models as targets to supervise the training process of mimic trees. DRL models typically compute value functions (V (s) or Q(s, a)) to estimate the expected cumulative rewards at state s. These value functions explicitly inﬂuence the RL decisions by controlling the policy gradient: Q(s, a)∇π(a|s) [17] or determining the action: ˆa = arg maxa Q(s, a) [18]. Compared to the policy π(a|s), value functions evaluate candidate actions or states more directly. Based on the value functions, one can compute an advantage value for each action by subtracting a baseline: y = Q(s, a) − V (s)1, which creates unbiased action-speciﬁc evaluations with less variance. Our mimic learner utilizes action advantages as the mimic targets, allowing us to understand when an action can outperform others by a certain advantage. 1Action advantages can be deﬁned as V (s(cid:48)) + r − V (s) if Q function is unknown (e.g., in A3C). 2
2.2
IB Method for Representing and Mimicking
We derive an objective for our RAMi framework based on the Information Bottleneck (IB) princi-ple [16]. An ideal mimic tree achieves a promising approximation performance (ﬁdelity) with a minimum number of splits (simplicity). The IB objective naturally integrates both goals, by encourag-ing a bottleneck representation to compress the input signals X and preserve as much of the relevant information about mimic targets Y (action advantages) as possible. The IB objective is: (cid:105) (cid:104)
Iω(Φ, Y ) − λIω(Φ, X) max
ω (1) where I denotes the mutual information, and λ is a Lagrange multiplier. −Iω(Φ, X) controls how much the mimic model compresses the input data, and Iω(Φ, Y ) measures how well the mimic model preserves information about targets2. The IB principle deﬁnes what we mean by a good representation, in terms of the fundamental trade-off between having a concise representation and one with good mimic performance [19]. However, in practice, it is difﬁcult to learn a mimic tree from high-dimensional and entangled raw inputs (e.g., images and text embeddings), so we learn a latent object representation from input space and then build a mimic tree upon latent features. Inspired by the deep variational IB [19], The objective for our RAMi framework is given by the following result.
Theorem 1 Consider a dataset of size N with input features x and prediction target y. Let Lq(yn) =
− log q(yn|φ) denote the description length for encoding the target yn, let Lp(φ) = − log p0(φ) denote the description length for encoding the mimic model φ. Optimizing the IB objective is equivalent to maximizing its lower bound, which is: 1
N
N (cid:88) (cid:110) n=1
Eq(z|xn)[log p(xn|z)] − λDKL[q(z|xn)(cid:107)p0(z)]−
Eq(z|xn)
Lq(yn) + λLp(φ)
Eq(φ|z) (cid:17) (cid:16) (cid:104)
− λH[q(φ|z)] (cid:105)(cid:111) (2)
= ELBo objective + IB-MDL objective + Entropy Regularizer
The proof can be found in the appendix. This lower bound deﬁnes two sub-objectives: 1) the Evidence
Lower Bound (ELBo) objective enables learning an object representation for inputs q(Z|X) with our
IMONet (Section 3). 2) The IB-MDL objective, for which we design a MCRTS algorithm (Section 4).
MCRTS learns a compact distribution over mimic trees q(Φ|Z) where we select the optimal mimic tree φ∗ to achieve a "sweet-spot" between ﬁdelity and simplicity. 3 Learning Object Representation
This section introduces the Identiﬁable Multi-Object Network (IMONet). The observed inputs from the RL environment are state-action-reward triplets xn = [sn, an, rn]. Actions and rewards are often well-disentangled in low dimensions, so our IMONet learns a disentangled latent representation for capturing the object features from states. We 1) introduce the key properties of IMONet that enable extraction of causal relationships, 2) describe the model structure and 3) discuss the interpretability for the learned latent representation. 3.1 Disentangled Representation for Causal Interpretation
The high-dimensional state features from the RL environment are often entangled (e.g., highly correlated pixels in an image), which makes causal inference from the raw input space impossible.
IMONet utilizes a Variational Auto-Encoder (VAE) framework [20] that converts state features to a disentangled latent representation (i.e., p(Z) = (cid:81)D d=1 p(Zd)). After optimizing a generative model for the observed states that utilizes the latent representations, the independent latent variables capture complete state information. The independence implies that there is no (unobserved or observed) confounder between two latent variables [21, 22]. This property enables mimic learners to model causal relations between latent variables Z and the target variable Y following a Causal
Decision Tree [23]. (Section 5.2 shows examples of causal relations). Since the latent representation encodes complex RL states with few latent variables, the efﬁciency of causal inference is improved signiﬁcantly. 2X, Y , Z, and Φ are random variables; x, y, z, and φ are instances of random variables, so p(X) represents a distribution while p(x) deﬁnes a probability. Fω(·) is the functional parameterized by ω. 3
3.2
Identiﬁability of Latent Representation
Identiﬁability is one of the most fundamental properties for uniquely identifying the causal relations between inputs and targets. If a latent representation is unidentiﬁable, there exists multiple distri-butions with the same generative performance, and the variables between two distributions can be completely entangled, meaning that they develop inconsistent factorizations for the same inputs [24] (see Section E.1 in appendix for examples). With unidentiﬁable representations, the mimic learner can return many different causal relations between the latent and target variables.
To better understand this problem, we deﬁne identiﬁability in the unsupervised representation learning setting as follows: p(Z) is identiﬁable up to equivalence ∼B, if ∀s ∈ S, p(s) = (cid:82) p(s|z)p(z)dz
= (cid:82) p(s|z(cid:48))p(z(cid:48))dz =⇒ ∃B, c s.t. T (p(Z)) = BT (p(Z (cid:48))) + c, where function T computes sufﬁcient statistics of a distribution, c is a vector, and B is an invertible matrix (that represents a bijective mapping). If B is a permutation matrix, we have identﬁability up to a permutation, otherwise up to scaling [25]. Previous work [24] proved that without including an inductive bias (e.g., speciﬁc model designs or additional observations), it is impossible to learn identiﬁable representations without supervising signals. To ensure the identiﬁability of the latent object representation, IMONet 1) utilizes a Multi-Object Network (MONet) [26] structure that is specially designed to capture variations of objects, and 2) employs a conditionally factored prior p(Z|A, R) = (cid:81) d p(Zd|A, R) where each variable Zd has a univariate Gaussian distribution. Following an Identiﬁable VAE (IVAE) design
[25], IMONet learns a conditional approximate posterior q(Z|S, A, R) as the state representation.
Figure 1: Mean Correlation Coefﬁcients (MCC) for the examined variational encoders (including VAE, Condi-tional VAE, MONet and IMONet) trained in the Flappy Bird (left), Space Invaders (middle), and Assault (right) environments. We report the mean±variance MCC scores computed from three independent runs.
Evaluation. We evaluate the identiﬁability of IMONet by comparing its Mean Correlation Coefﬁcient (MCC) with other variational encoders. MCC measures whether it is possible to identify latent variables from one model with variables from another model (up to point-wise transformations).
To compute MCC, we 1) independently train two encoders based on the same structure, 2) pair latent variables from the encoders, by solving a linear sum assignment problem for the best overall correlation, 3) average the correlation coefﬁcients between paired latent variables at different training steps. The results (Figure 1) demonstrate that conditioning variables and an object network can consistently improve the identiﬁability of latent representations, whereas VAEs show no evidence of identiﬁability. This ﬁnding is consistent with [24, 25]. 3.3 Model Implementation
IMONet decomposes a state s into object-level components with spatial masks (m1, ..., mK) from a multi-layer convolutional attention network (mk indicates whether the kth object is present in each pixel, see examples in Figure 2). Conditioning on mk, r, and a, IMONet independently encodes each component with an encoder-decoder Conditional VAE (CVAE). To train IMONet, we decompose the
ELBo from our IB objective (Theorem 1) into an object-oriented variational lower bound: log
K (cid:88) k=1 mkpd(s|zk) − βDKL (cid:16) K (cid:89) k=1 qenc(zk|s, a, r, mk)(cid:107)p(zk|a, r) (cid:17)
−
K (cid:88) k=1 (cid:16)
λDKL q(mk|s, a, r)(cid:107)pd(mk|zk) (3) (cid:17) where 1) the ﬁrst term is a decoder log-likelihood that deﬁnes the reconstruction performance for a mixtures of decoder outputs. The state decoder pd(s|zk) is implemented by deconvolution layers. 2)
The second term is the KL-divergence (KLD) between the conditional approximate posteriors for 4
each component and a conditional prior. The approximate posterior is implemented by: q] = ψq[ψconv(s, mk), ψmlp(a, r)] qenc(z|s, a, r, mk) := N (µq, diag(σ2 q)) s.t. [µp, σ2 (4)
ψmlp and ψq are multi-layer perceptrons while ψconv denotes a convolutional network. The prior p(z|a, r) is implemented by another Gaussian function without ψconv. 3) The last term deﬁnes the KLD between the mask generated by the attention network and the mask reconstructed by the
CVAE. Our attention network q(mk|s, a, r) is implemented by U-Net [27], which is implemented a layer-by-layer convolutional neural network with instance normalization, followed by a sigmoid function that transfers logistics to attentions. The mask decoder pd(mk|zk) applies the same structure as our state decoder.
Learning an object representation for the RL states is applicable in practice since 1) a RL state can typically be represented by a limited number of objects, and 2) the agent can interact with the RL environment to generate sufﬁcient data for learning such a representation. Object-Oriented
RL [28, 29, 30] has demonstrated that an agent can master RL games by modeling the object dynamics, so we utilize features from the object representation as the inputs to mimic learners: we sample latent features zk from each component posterior qenc(Zk|S = s, A = a, R = r, M = mk), concatenate them with the conditioning values, and feed the entire vector [z1,...,K, a, r] to MCRTS . 3.4
Interpreting Latent Variables
IMONet learns a symbolic abstraction of state space by representing object variations (e.g., shapes and locations) with latent variables. A latent variable Zk,d (the dth variable for the kth object) captures an independent factor of object variations. A common approach to reveal the information captured by Zk,d is latent traversing [26, 31, 32]: we randomly choose images and inspect how the reconstruction components change as Zk,d is traversed from (empirical) minimum to maximum values.
Figure 2 illustrates the objects captured by attention masks from IMONet and shows two examples of latent traversals in a ﬂappy bird environment. We include a human evaluation to demonstrate the interpretability of latent variables (Section 5.2). The appendix includes more examples (Section E.3 ).
Figure 2: Visualizations for IMONet outputs. IMONet decomposes a state sreal into three objects with masks m1 (for the background), m2 (for pillars) and m3 (for the bird), where white/dark colors mark captured/uncaptured regions. The generations from the decoder are ˆs1, ˆs2, and ˆs3. We show two latent traversals [33] for interpreting
Z3,1 (the 1st variable for the 3rd object) and Z2,3 (the 3rd variable for the 2nd object). Z3,1 captures the vertical distance between the bird and a pillar, and Z2,3 captures the length of the left pillar (highlighted by blue frames). 4 Learning Mimic Tree Interpretations
This section introduces the approach to infer a mimic tree with the IB-MDL objective (in Theorem 1) and a Monte Carlo Regression Tree Search (MCRTS) algorithm to ﬁnd the optimal tree. 4.1
Inferring Mimic Trees with IB-MDL
We deﬁne Lp(φ) and Lq(yn) in the IB-MDL objective for mimic tree inference. The goal of MDL is to ﬁnd regularity and simplicity in the data by viewing learning as a compression problem. There are two agents with complete copies of the latent features zn, but only the sender agent knows the target labels yn. The sender agents transmits to the receiver agent a description of the missing labels using as few bits as possible; we use a mimic tree to encode the information. The optimal mimic tree is 5
deﬁned to be the one that enables sending the minimum description length of (a) encoding the tree structure Lp(φ), and (b) encoding the exceptions Lq(yn) at each leaf node. (a) Encoding Tree Structure: We convert the binary tree structure to a string that records the splits (f ) and leaf predictions (ˆy) (e.g., 1,f0,1,f1, 0, ˆy0, 0, ˆy1, 0, ˆy2 where we mark splits and leaves with 1 and 0) with depth-ﬁrst search [34]. The next proposition gives the encoding cost Lp(φ).
Proposition 1 Given a regression tree with L splits, the total cost (in bits) of describing the tree structure with the string encoding method is: (2L − 1)2 2 (L − 1) 1
L 3
L 2L − 1
+ (2L − 1)H(
Lp(φ) = log
) + O(L−1) (5) 2 (b) Encoding the Exceptions: Traditional MDL tree algorithms [35] are proposed for classiﬁcation trees to handle discrete labels. Their exception-encoding methods are infeasible with continuous predictions, so we utilize an alternative approach that models the distribution of action advantages at a leaf node i with a Gaussian distribution N (ˆytree and ˆσ2 i model the prediction and the scale of exception at the leaf i respectively. Inspired by the variance reduction objective in classic regression trees, we utilize the log-variance to measure the cost of encoding exceptions, so Ep(yn)[Lq(yn)] := log total data size and the number of data points on the ith leaf node respectively. i (φ)). Given a mimic tree φ, ˆytree where N and N leaf (cid:104) (cid:80)L+1 i=0 denote the
N ˆσ2 (φ), ˆσ2
N leaf i (cid:105) i i i i 4.2 MCRTS Implementation
MCRTS (Figure 3) takes latent fea-tures zn ∈ RK×D (K and D de-note the number of objects and latent size respectively) from our IMONet and the conditioning values an, rn as inputs. It explores different splits by maintaining multiple candidate mimic trees that are trained to approx-imate action advantages Y . To initi-ate a tree search, MCRTS stores all the instances (data points) in a cell cell((cid:104)z1, a1, r1; y1(cid:105), (cid:104)z2, a2, r2; y2(cid:105), (cid:104)z3, a3, r3; y3(cid:105), . . . ) at the root node.
An edge in the MCRTS search tree represents a split f in the mimic tree.
This binary split passes the instances from a parent cell to two partition cells in child nodes. Each MCRTS edge records a number of visits N V (J, f ) and an estimate QM C(J, f ) of the expected splitting inﬂuence. Each node J contains a series of partition cells constructed by splits from the root node: J := {cell1((cid:104)z1, a1, r1; y1(cid:105), (cid:104)z3, a3, r3; y3(cid:105), . . . ), cell2((cid:104)z2, a2, r2; y2(cid:105), (cid:104)z4, a4, r4; y4(cid:105), . . . ), cell3(. . . )}. MCRTS learns a compact distribution of mimic trees p(Φ|Z). A mimic tree φ ∈ Φ can be extracted from MCRTS by following a path from the root to a leaf node Jl, so φ := {f0 . . . fl−1, Jl}. A partition celli in Jl deﬁnes a leaf node in the mimic tree φ, and the leaf prediction is ˆytree
Figure 3: MCRTS structure, where a tree node records partition cells and an edge stores a number of visits N V and an estimate
: Zd < v denotes the kth split at layer l, which checks
Q(J, f ). f k l whether Zd is smaller than a value v. The layer number l equals the number of splits in a path. ¬fl denotes splitting ends.
. Appendix (E.1) provides a mimic tree extraction example.
=(cid:80) y/N leaf i i y∈celli
Searching: MCRTS implements a tree search by running M plays from a starting node Js (which is initialized to be the root node and updated after each move) with root parallelization [36]. At the mth play, MCRTS implements four phases including: 1) Selection: Traverse the tree from Js to a leaf node by selecting the split fl,m at each layer to maximize the Upper Conﬁdence Bound (UCB) [37]: (cid:104)
QM C
√ log(m−1) (cid:105) fl,m = arg maxf m−1(Jl, f ) + cpuct where cpuct controls the scale of exploration. Similar to the entropy regularizer (Theorem 1), a large cpuct prevents over-frequent visits to a node. 2) Evaluation: Evaluate the selected leaf
Jleaf with reward: rM C(Jleaf ) = −Lq(yn) − λLp(φ). 3) Expansion: Expand the leaf node
N Vm−1(Jl,f )+1 (6) 6
Table 1: Mimic performance. We use the Variance Reduction (VR) and Variance Reduction Per Leaf (VR-PL) metrics since they are common regression objectives and optimized by the IB-MDL objective. Results for other regression metrics (Root Mean Square Error and Mean Absolute Error) and the corresponding variances are recorded in Table B.2 in appendix. w+ indicates that each leaf node has an extra linear regression model. We omit the results for the raw-input-based MCRTS since it is computational intractable.
Method
Cart
VIPER
M5-RT
M5-MT
GM-LMT
VR-LMT
VAE+CART
VAE+VIPER
VAE+GM-LMT
VAE+VR-LMT
VAE+MCRTS
IMONet+CART
IMONet+VIPER
IMONet+GM-LMT
IMONet+VR-LMT
IMONet+MCRTS
VR 8.51E-2 8.57E-2 9.59E-2 9.56E-2 8.99E-2 8.46E-2 7.25E-2 7.63E-2 6.35E-2 7.95E-2 7.83E-2 8.23E-2 8.50E-2 7.87E-2 8.21E-2 8.53E-2
Flappy Bird
VR-PL 8.43E-5 1.88E-4 8.37E-5 1.55E-4 2.99E-4 5.36E-4 3.44E-4 5.32E-4 3.51E-4 5.12E-4 1.27E-3 4.02E-4 4.48E-4 3.74E-4 7.16E-4 1.37E-3
Leaf 1007 453 1144 612w+ 303w+ 157w+ 212 143 180w+ 154w+ 61 204 191 212w+ 115w+ 62
Space Invaders
VR-PL 7.02E-5 8.80E-5 2.92E-5 1.23E-5 8.32E-5 1.61E-4 7.86E-5 9.89E-5 2.75E-4 2.08E-4 5.66E-4 1.38E-4 1.69E-4 3.23E-4 3.79E-4 7.08E-4
VR 4.96E-2 4.63E-2 4.54E-2 1.60E-2 2.07E-2 2.65E-2 3.99E-2 4.12E-2 3.39E-2 3.52E-2 4.82E-2 5.21E-2 5.26E-2 4.79E-2 4.54E-2 5.37E-2
Leaf 705 525 1558 1303w+ 249w+ 166w+ 507 417 123w+ 171w+ 85 375 313 149w+ 120w+ 76
VR 4.79E-2 5.28E-2 4.37E-2 3.42E-2 5.55E-2 5.80E-2 5.15E-2 4.57E-2 4.20E-2 5.10E-2 6.58E-2 5.67E-2 6.05E-2 5.45E-2 6.03E-2 7.53E-2
Assault
VR-PL 7.46E-5 8.09E-5 2.73E-5 2.54E-5 1.83E-4 1.98E-4 1.16E-4 1.29E-4 1.44E-5 1.99E-4 7.75E-4 1.81E-4 1.90E-4 2.15E-4 2.27E-4 9.07E-4
Leaf 642 653 1605 1351w+ 307 w+ 291 w+ 448 356 293w+ 258w+ 85 315 319 256w+ 268w+ 83 m = (QM C with G (the maximum exploration width) child nodes. 4) Back Up: Update the action-values:
QM C m−1N Vm−1 + rM C)/(N Vm−1 + 1) and increment the visit count: N Vm = N Vm−1 + 1 on all the traversed edges.
Move: After M plays, we move Js to a child node by selecting the split ˜fs = arg maxfk∈F1...g(m)
N Vm(Js, fk) and setting Js to a child node ˜Js+1 (connected by the edge ˜fs). The next play will start from the new starting node ˜Js+1. It allows MCRTS to prune the sub-optimal nodes [38]. 5 Empirical Evaluation
We evaluate the mimic performance and demonstrate the interpretability of the mimic tree.
Environment and Running Settings: We study the Flappy Bird, Space Invaders, and Assaults environments. Flappy Bird is a procedural game, where the game states are randomly generated at each episode. Space Invaders and Assaults are commonly studied Atari games from the Gym toolkit [39]. (Check data generation details and model hyper-parameters in Appendix).
Baseline Models: We compare previous tree-based mimic learners. The ﬁrst baseline model is a
Classiﬁcation and Regression Tree (CART ) [40]. We include VIPER [41] as our second baseline by replacing its policy (decision) tree with a regression tree that imitates action values with a Q-dagger algorithm. The third baseline is the M5 [42] regression tree training algorithm. M5 constructs a piecewise constant tree by pruning the sub-optimal nodes. We include M5 with the Regression-Tree option (M5-RT) and the Model-Tree option (M5-MT). M5-MT builds a linear regression model for the instances at each leaf node while M5-RT maintains only a constant value. The last baseline is a Linear Model Tree (LMT) [12] for interpreting action values. A recent work [13] explored different heuristics for selecting splitting features, including Variance Reduction (VR-LMT) and
Gaussian Mixture clustering (GM-LMT). The aforementioned models are directly learnt from raw input space (states and actions) [14]. The aforementioned models were previously learnt directly from raw input space (states and actions). In this study of latent representation, we evaluate their regression performance based on the latent features from both VAE and IMONet (Check appendix for the implementation details of baseline models). 5.1 Fidelity versus Simplicity
This experiment evaluates mimic trees by their 1) ﬁdelity: how well a mimic model approximates the DRL model [12] and 2) simplicity: the size of these nonparametric trees. We divide the dataset (50k) into training (80%), validation (10%), and testing (10%) sets and generate 5 independent runs.
Fidelity and simplicity are evaluated by the mimic performance and the number of leaves respectively. 7
Table 1 shows the regression performance. MCRTS+IMONet achieves a promising regression performance with signiﬁcantly fewer leaves than other baseline models. This is because 1) IMONet learns a concise object representation from RL states and this representation outperforms that modelled by an unidentiﬁable VAE. 2) MCRTS selects splits to maximize the regression performance at a global level, yielding a mimic tree with better ﬁdelity. Apart from these obvious advantages, we
ﬁnd that some latent features may be indistinguishable for predicting action values, and this explains why some trees have better regression performance by building a large tree from raw inputs, while the large tree size compromises their interpretability.
Leaf-by-Leaf Regression Performance: To study the regression efﬁciency, we evaluate the per-leaf regression performance of examined models based on the latent features from IMONet. Figure 4 illustrates the performance of examined mimic trees when we constrain their number of leaves.
MCRTS achieves a leading regression performance, as it selects a split by looking ahead to the future cumulative rewards instead of its local inﬂuence. Another key observation is that a well-explored split from MCRTS can beat a greedy split with extra linear regressors at leaf nodes (e.g., LMTs), allowing MCRTS to substantially improve the learning efﬁciency.
Figure 4: Leaf-by-leaf tree regression results based on latent features from IMONet in the Flappy Bird (left),
Space Invaders (middle), and Assault (right) environments. 5.2
Illustrative Examples of Interpretable Mimic Trees
We demonstrate the interpretability of our mimic tree by 1) illustrating the extracted rules and causal relations, 2) comparing with previous RL interpretations and 3) conducting human evaluations. This section uses the Flappy Bird environment as an example, where a DRL agent earns rewards by controlling a bird to pass pillars.
Figure 5: Mimic tree. fl : Zk,d < v indicates the lth tree split is at Zk,d (dth variable for the kth object) with a splitting value v. In the decision rules (b&c), solid / dash lines indicate tree path / causal relations.
Causal Explanations: Our mimic tree φ captures the inﬂuence of latent features on action advantages y. Figure 5 (a) illustrates the top three layers of the mimic tree. We can interpret the inﬂuence of a split by visualizing the expected latent values after this split. For example, after implementing the split f1, we obtain two child nodes (cell2,1 and cell2,2). We compute the expected latent values for the instances on these nodes (En∈cell(Zn)) and visualize the 3rd object with the decoder (since
Z3,1 captures the variation of the 3rd object). These visualizations capture the difference of object information after splitting. They show that the instances on cell2,2 have shorter (vertical) distance 8
between the bird and the upper pillar, and thus their advantages for the action ’down’ are larger. We verify this observation by sampling an instance from both child nodes respectively. This observation is consistent with game rules since performing the action ’down’ when the bird approaches the upper pillar can prevent potential crashes and thus has a larger advantage. (a) Causal Relation Extraction. Since there is no (observed/unobserved) confounder between latent variables (section 3.1), our mimic tree can capture causal relations between splitting features and predictions. Similar to Causal Decision Trees (CDTs) [23], a tree split f in our mimic tree represents a context-speciﬁc causal relationship between the splitting condition f and the action advantage ˆytree
.
The context is a series of value assignments of the latent features along the path from the root to the parent node of f . For example, the decision rule in Figure 5(b) states that having Z3,4 ≥ 0.71 is casually related to an action advantage of 0.23 when the action is ’down’ and Z3,1 ≥ 0.12 (context). i (b) Counterfactual Analysis. Leveraging the generation ability of the IMONet decoder, our mimic tree enables counterfactual analysis to answer “what if” questions: we show the inﬂuence on tree prediction by intervening to set the value of Zk,d equal to some particular value, For example, in
Figure 5(c), we compute Ftree[f0, f1, . . . , fL, X (cid:48), Y (cid:48)
θ|do(Z3,1 = 0)]. To achieve this, 1) we set Z3,1 to 0 for all input latent vectors and get z(cid:48). 2) The IMONet decoder computes pd(x(cid:48)|z(cid:48)) and the DRL model predicts Y (cid:48)
θ with the generated observations X (cid:48). 3) The mimic tree follows previous splits to construct tree predictions (the splits on the intervened variables are removed). The difference of tree prediction before and after the intervention (0.23 versus 0.11) reveals the inﬂuence of do(Z3,1 = 0).
Comparison to other interpretations. We compare other interpretable DRL models based on 1)
Super-pixels [12] (Figure 5d) that highlight the pixels used as splitting features in a regression tree, 2) Saliency Map [6] (Figure 5e) which is generated by perturbing the pixels within a region and evaluating their impact on target outputs, and 3) Attention Mask [4] (Figure 5f) (from the convolutional attention network) that represents the importance of pixels on model outputs. These traditional models used the raw images as input during training. The learned interpretations visualize the pixels that are important for decisions. They generally agree that the distance between the bird and the pillar is most inﬂuential. This observation is consistent with rules extracted by our method (Figure 5a&b) since this distance determines how likely the bird can pass the pillars to win more rewards.
Human Evaluation. We evaluate the interpretability of the aforementioned methods with RL practitioners from four universities. We ﬁrst introduce the saliency map, mimic trees based on super-pixels and latent variables, and then ask the participants to 1) describe the information captured by these methods and 2) rank the corresponding interpretations. To eliminate potential bias, participants are interviewed independently, and they are not told which methods are the baselines or the proposed model (check Section D in appendix for further details). We present a summary of the results by discussing the two aspects of interpretability regarding our method: (a) Interpretability of Latent Variables. After observing the latent traversals, all (12/12) participants correctly recognized the physical meaning of the latent variables whereas 58%(7/12) and 33.3%(4/12) participants managed to recognize the features captured by saliency maps and super-pixels due to the inconsistency of local samples and the complexity of tree paths. (b) Interpretability of mimic trees. 83% (10/12) participants preferred the mimic tree based on latent features since its tree path captures globally-consistent knowledge and its tree size is smaller than that of the super-pixel tree (62 versus 1007, check Table 1). The rest voted for the saliency map because they preferred straight-forward explanations. 5.3 Limitations
Computational Complexity. Since MCRTS conducts multiple simulations to explore and generate the global-optimal mimic tree, it generally consumes more computational resources than classic re-gression. We compare the computational complexity of our method and the baselines in the appendix.
Although IMONet compresses high-dimensional inputs into latent vectors and we parallelize the tree search in MCRTS, empirically, it takes MCRTS a longer to determine a split.
Guarantee for Interpretability. It is generally hard to theoretically justify or numerically quantity the level of interpretability, so we utilize illustrative examples and human evaluation to empirically 9
show the interpretability. Although previous works demonstrated interpretability of latent features [26, 31, 32], there is no guarantee that these features always have a clear meaning across all RL games. 6