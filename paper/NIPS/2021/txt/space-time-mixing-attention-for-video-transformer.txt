Abstract
This paper is on video recognition using Transformers. Very recent attempts in this area have demonstrated promising results in terms of recognition accuracy, yet they have been also shown to induce, in many cases, signiﬁcant computational overheads due to the additional modelling of the temporal information. In this work, we propose a Video Transformer model the complexity of which scales linearly with the number of frames in the video sequence and hence induces no overhead compared to an image-based Transformer model. To achieve this, our model makes two approximations to the full space-time attention used in
Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer’s depth to obtain full temporal coverage of the video sequence. (b) It uses efﬁcient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. We also show how to integrate 2 very lightweight mechanisms for global temporal-only attention which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model produces very high recognition accuracy on the most popular video recognition datasets while at the same time being signiﬁcantly more efﬁcient than other Video Transformer models.
Code for our method is made available here. 1

Introduction
Video recognition – in analogy to image recognition – refers to the problem of recognizing events of interest in video sequences such as human activities. Following the tremendous success of
Transformers in sequential data, speciﬁcally in Natural Language Processing (NLP) [39, 5], Vision
Transformers were very recently shown to outperform CNNs for image recognition too [48, 13, 35], signaling a paradigm shift on how visual understanding models should be constructed. In light of this, in this paper, we propose a Video Transformer model as an appealing and promising solution for improving the accuracy of video recognition models.
A direct, natural extension of Vision Transformers to the spatio-temporal domain is to perform the self-attention jointly across all S spatial locations and T temporal locations. Full space-time attention though has complexity O(T 2S2) making such a model computationally heavy and, hence, impractical even when compared with the 3D-based convolutional models. As such, our aim is to exploit the temporal information present in video streams while minimizing the computational burden within the
Transformer framework for efﬁcient video recognition. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Full space-time atten-tion: O(T 2S2) (b) Spatial-only attention:
O(T S2) (c) TimeSformer [3] and
[1]:
ViViT (Model 3)
O(T 2S + T S2) (d) Ours: O(T S2)
Figure 1: Different approaches to space-time self-attention for video recognition. In all cases, the key locations that the query vector, located at the center of the grid in red, attends are shown in orange.
Unlike prior work, our key vector is constructed by mixing information from tokens located at the same spatial location within a local temporal window. Our method then performs self-attention with these tokens. Note that our mechanism allows for an efﬁcient approximation of local space-time attention at no extra cost.
A baseline solution to this problem is to consider spatial-only attention followed by temporal averaging, which has complexity O(T S2). Similar attempts to reduce the cost of full space-time attention have been recently proposed in [3, 1]. These methods have demonstrated promising results in terms of video recognition accuracy, yet they have been also shown to induce, in most of the cases, signiﬁcant computational overheads compared to the baseline (spatial-only) method due to the additional modelling of the temporal information.
Our main contribution in this paper is a Video Transformer model that has complexity O(T S2) and, hence, is as efﬁcient as the baseline model, yet, as our results show, it outperforms re-cently/concurrently proposed work [3, 1] in terms of efﬁciency (i.e. accuracy/FLOP) by signiﬁcant margins. To achieve this our model makes two approximations to the full space-time attention used in Video Transformers: (a) It restricts time attention to a local temporal window and capitalizes on the Transformer’s depth to obtain full temporal coverage of the video sequence. (b) It uses efﬁcient space-time mixing to attend jointly spatial and temporal locations without inducing any additional cost on top of a spatial-only attention model. Fig. 1 shows the proposed approximation to space-time attention. We also show how to integrate two very lightweight mechanisms for global temporal-only attention, which provide additional accuracy improvements at minimal computational cost. We demonstrate that our model is surprisingly effective in terms of capturing long-term dependencies and producing very high recognition accuracy on the most popular video recognition datasets, including
Something-Something-v2 [17], Kinetics [4] and Epic Kitchens [9], while at the same time being signiﬁcantly more efﬁcient than other Video Transformer models. 2