Abstract
Tensor network (TN) methods have been a key ingredient of advances in condensed matter physics and have recently sparked interest in the machine learning commu-nity for their ability to compactly represent very high-dimensional objects. TN methods can for example be used to efﬁciently learn linear models in exponen-tially large feature spaces [56]. In this work, we derive upper and lower bounds on the VC-dimension and pseudo-dimension of a large class of TN models for classiﬁcation, regression and completion. Our upper bounds hold for linear models parameterized by arbitrary TN structures, and we derive lower bounds for common tensor decomposition models (CP, Tensor Train, Tensor Ring and Tucker) showing the tightness of our general upper bound. These results are used to derive a gener-alization bound which can be applied to classiﬁcation with low-rank matrices as well as linear classiﬁers based on any of the commonly used tensor decomposition models. As a corollary of our results, we obtain a bound on the VC-dimension of the matrix product state classiﬁer introduced in [56] as a function of the so-called bond dimension (i.e. tensor train rank), which answers an open problem listed by
Cirac, Garre-Rubio and Pérez-García in [13]. 1

Introduction
Tensor networks (TNs) have emerged in the quantum physics community as a mean to compactly represent wave functions of large quantum systems [45, 5, 52]. Their introduction in physics can be traced back to the work of Penrose [47] and Feynman [15]. Akin to matrix factorization, TN methods rely on factorizing a high-order tensor into small factors and have recently gained interest from the machine learning community for their ability to efﬁciently represent and perform operations on very high-dimensional data and high-order tensors. They have been for example successfully used for compressing models [43, 69, 42, 29, 70], developing new insights on the expressiveness of deep neural networks [14, 31] and designing novel approaches to supervised [56, 18] and unsupervised [55, 25, 39] learning. Most of these approaches leverage the fact that TN can be used to efﬁciently parameterize high-dimensional linear maps, which is appealing from two perspectives: it makes it possible to learn models in exponentially large feature spaces and it acts as a regularizer, controlling the capacity of the class of hypotheses considered for learning.
While the expressive power of TN models has been studied recently [17, 2], the focus has mainly been on the representation capacity of TN models, but not on their ability to generalize in the context of supervised learning tasks. In this work, we study the generalization ability of TN models by deriving lower and upper bounds on the VC-dimension and pseudo-dimension of TN models commonly used for classiﬁcation, completion and regression, from which bounds on the generalization gap of TN models can be derived. Using the general framework of tensor networks, we derive a general upper bound for models parameterized by arbitrary TN structures, which applies to all commonly used tensor decomposition models [20] such as CP [27], Tucker [59] and tensor train (TT)) [46], as well 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
as more sophisticated structures including hierarchical Tucker [19, 23], tensor ring (TR) [73] and projected entangled state pairs (PEPS) [60].
Our analysis proceeds mainly in two steps. First, we formally deﬁne the notion of TN learning model by disentangling the underlying graph structure of a TN from its parameters (the core tensors, or factors, involved in the decomposition). This allows us to deﬁne, in a conceptually simple way, the
HG corresponding to the family of linear models whose weights are represented hypothesis class using an arbitrary TN structure G. We then proceed to deriving upper bounds on the VC/pseudo-dimension and generalization error of the class
HG. These bounds follow from a classical result from Warren [66] which was previously used to obtain generalization bounds for neural networks [3], matrix completion [54] and tensor completion [41]. The bounds we derive naturally relate the capacity
HG to the underlying graph structure G through the number of nodes and effective number of of parameters of the TN. To assess the tightness of our general upper bound, we derive lower bounds for particular TN structures (rank-one, CP, Tucker, TT and TR). These lower bounds show that, for completion, regression and classiﬁcation, our general upper bound is tight up to a log factor for rank-one, TT and TR tensors, and is tight up to a constant for matrices. Lastly, as a corollary of our results, we obtain a bound on the VC-dimension of the tensor train classiﬁer introduced in [56], which answers one of the open problems listed by Cirac, Garre-Rubio and Pérez-García in [13].