Abstract
Graph neural networks (GNNs) have recently emerged as revolutionary technolo-gies for machine learning tasks on graphs. In GNNs, the graph structure is generally incorporated with node representation via the message passing scheme, making the explanation much more challenging. Given a trained GNN model, a GNN explainer aims to identify a most inﬂuential subgraph to interpret the prediction of an instance (e.g., a node or a graph), which is essentially a combinatorial optimization prob-lem over graph. The existing works solve this problem by continuous relaxation or search-based heuristics. But they suffer from key issues such as violation of message passing and hand-crafted heuristics, leading to inferior interpretability. To address these issues, we propose a RL-enhanced GNN explainer, RG-Explainer, which consists of three main components: starting point selection, iterative graph generation and stopping criteria learning. RG-Explainer could construct a con-nected explanatory subgraph by sequentially adding nodes from the boundary of the current generated graph, which is consistent with the message passing scheme.
Further, we design an effective seed locator to select the starting point, and learn stopping criteria to generate superior explanations. Extensive experiments on both synthetic and real datasets show that RG-Explainer outperforms state-of-the-art
GNN explainers. Moreover, RG-Explainer can be applied in the inductive setting, demonstrating its better generalization ability. 1

Introduction
Graph Neural Networks (GNNs) extend neural network models on ubiquitous graph data via utilizing the message passing scheme to incorporate graph structures with node features. They have achieved state-of-the-art performance not only in classic machine learning tasks on graphs, e.g., node classiﬁ-cation [10, 28], link prediction [38], and graph classiﬁcation [33], but also in reasoning tasks, e.g., intuitive physics [4], mathematical reasoning [24], and IQ tests [3]. Similar to most deep learning methods, one major limitation of GNNs is the lack of the interpretability for the predicted results; a post-hoc analysis is usually needed to explain the results.
To enhance the interpretability of GNNs, a line of works [34, 17, 30, 37, 31] focused on developing
GNN explainers. The goal of GNN explainers is to identify a most inﬂuential subgraph structure to interpret the predicted label of an instance (e.g., a node or a graph). It can be generally formulated as 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
an optimization problem that maximizes the mutual information between the predicted results and the distribution of relevant subgraphs under some size constraints.
The pioneering works, e.g., GNNExplainer [34] and PGExplainer [17], attempt to solve the opti-mization problem with continuous relaxation. These methods optimize a soft mask matrix for edges, and select the important nodes/edges by the threshold. However, they cannot guarantee that nodes and edges in the output subgraph are connected. Thus, their explanatory subgraphs cannot explicitly visualize the message passing paths. Besides, they consider the importance of each edge indepen-dently, and ignore the interactions among selected nodes and edges. Some recent works, such as
SubgraphX [37] and Causal Screening [31], design the search criteria and use search-based methods to solve the optimization problem. Due to the combinatorial property of searching explanatory graph structures, it is difﬁcult to design a general hand-crafted search criterion. These criteria are limited on speciﬁc situations and thus not widely applicable.
To address these issues, we propose RG-Explainer, which adopts reinforcement learning to explain
GNNs’ predictions. Our framework is inspired by classic combinatorial optimization solvers, which consists of three crucial steps: starting point selection, iterative graph generation and stopping criteria learning. These three components work together to generate an explanatory graph that interprets the predicted label of a given node/graph instance, as we elaborate next.
Firstly, starting point selection selects the most important node as the seed node in the instance. If the task is to interpret the prediction of a speciﬁc node label, then the most important node refers to the node itself. To explain a graph label, we design a seed locator to learn the node that inﬂuences the graph label the most. Iterative graph generation is the key module in our method, which generates the nodes in the explanatory graph sequentially. Speciﬁcally, we add an inﬂuential node (action) from the neighbors based on the current generated graph (state) at each step. It explicitly guarantees the connectivity of the generated graph. The generation process is controlled by the reward, i.e., the mutual information between the original predicted label and the label made by the generated graph.
To ensure a compact and meaningful explanatory graph, we also involve some constraints into the reward, such as size loss, radius penalty and similarity loss. Finally, stopping criteria are learned to further avoid generating very large explanatory graphs.
Furthermore, our method has better generalization ability and can be applied in both transductive and inductive setting. Different from the search-based methods, we learn the heuristics from the data automatically. A well-trained RG-Explainer can infer the explanations of instances which are not involved in the training phase.
We conduct extensive experiments on both synthetic and real-world datasets to show that the proposed
RG-Explainer can achieve superior performance compared to state-of-the-art GNN explainers. In particular, our visualization results further demonstrate the better intepretability of our method. 2