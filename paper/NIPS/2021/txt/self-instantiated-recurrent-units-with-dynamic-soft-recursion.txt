Abstract
While standard recurrent neural networks explicitly impose a chain structure on different forms of data, they do not have an explicit bias towards recursive self-instantiation where the extent of recursion is dynamic. Given diverse and even growing data modalities (e.g., logic, algorithmic input and output, music, code, images, and language) that can be expressed in sequences and may beneﬁt from more architectural ﬂexibility, we propose the self-instantiated recurrent unit (Self-IRU) with a novel inductive bias towards dynamic soft recursion. On one hand, the
Self-IRU is characterized by recursive self-instantiation via its gating functions, i.e., gating mechanisms of the Self-IRU are controlled by instances of the Self-IRU itself, which are repeatedly invoked in a recursive fashion. On the other hand, the extent of the Self-IRU recursion is controlled by gates whose values are between 0 and 1 and may vary across the temporal dimension of sequences, enabling dynamic soft recursion depth at each time step. The architectural ﬂexibility and effectiveness of our proposed approach are demonstrated across multiple data modalities. For example, the Self-IRU achieves state-of-the-art performance on the logical inference dataset [Bowman et al., 2014] even when comparing with competitive models that have access to ground-truth syntactic information. 1

Introduction
Models based on the notion of recurrence have enjoyed pervasive impact across various applica-tions. In particular, most effective recurrent neural networks (RNNs) operate with gating functions.
Such gating functions not only ameliorate vanishing gradient issues when modeling and capturing long-range dependencies, but also beneﬁt from ﬁne-grained control over temporal composition for sequences [Hochreiter and Schmidhuber, 1997, Cho et al., 2014].
With diverse and even growing data modalities (e.g., logic, algorithmic input and output, music, code, images, and language) that can be expressed in sequences and may beneﬁt from more architectural
ﬂexibility, recurrent neural networks that only explicitly impose a chain structure on such data but lack an explicit bias towards recursive self-instantiation may be limiting. For example, their gating functions are typically static across the temporal dimension of sequences. In view of such, this paper aims at studying an inductive bias towards recursive self-instantiation where the extent of recursion is dynamic at different time steps.
We propose a novel recurrent unit whose gating functions are repeatedly controlled by instances of the recurrent unit itself. Our proposed model is called the self-instantiated recurrent unit (Self-IRU), where self-instantiation indicates modeling via the own instances of the model itself in a recursive fashion. Speciﬁcally, two gates of the Self-IRU are controlled by Self-IRU instances. Biologically,
∗Work was done at NTU. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
this design is motivated by the prefrontal cortex/basal ganglia working memory indirection [Kriete et al., 2013]. For example, a child Self-IRU instance drives the gating for outputting from its parent
Self-IRU instance.
Our proposed Self-IRU is also characterized by the dynamically controlled recursion depths. Specif-ically, we design a dynamic soft recursion mechanism, which softly learns the depth of recursive self-instantiation on a per-time-step basis. More concretely, certain gates are reserved to control the extent of the Self-IRU recursion. Since values of these gates are between 0 and 1 and may vary across the temporal dimension, they make dynamic soft recursion depth at each time step possible, which could lead to more architectural ﬂexibility across diverse data modalities.
This design of the Self-IRU is mainly inspired by the adaptive computation time (ACT) [Graves, 2016] that learns the number of computational steps between an input and an output and recursive neural networks that operate on directed acyclic graphs. On one hand, the Self-IRU is reminiscent of the ACT, albeit operated at the parameter level. While seemingly similar, the Self-IRU and ACT are very different in the context of what the objective is. Speciﬁcally, the goal of the Self-IRU is to dynamically expand the parameters of the model, not dynamically decide how long to deliberate on input tokens in a sequence. On the other hand, the Self-IRU marries the beneﬁt of recursive reasoning with recurrent models. However, in contrast to recursive neural networks, the Self-IRU is neither concerned with syntax-guided composition [Tai et al., 2015, Socher et al., 2013, Dyer et al., 2016,
Wang and Pan, 2020] nor unsupervised grammar induction [Shen et al., 2017, Choi et al., 2018,
Yogatama et al., 2016, Havrylov et al., 2019].
Our Contributions All in all, sequences are fundamentally native to the world, so the design of effective inductive biases for data in this form has far-reaching beneﬁts across a diverse range of real-world applications. Our main contributions are outlined below:
• We propose the self-instantiated recurrent unit (Self-IRU). It is distinctly characterized by a novel inductive bias towards modeling via the own instances of the unit itself in a recursive fashion, where the extent of recursion is dynamically learned across the temporal dimension of sequences.
• We evaluate the Self-IRU on a wide spectrum of sequence modeling tasks across multiple modalities: logical inference, sorting, tree traversal, music modeling, semantic parsing, code generation, and pixel-wise sequential image classiﬁcation. Overall, the empirical results demonstrate architectural ﬂexibility and effectiveness of the Self-IRU. For example, the
Self-IRU achieves state-of-the-art performance on the logical inference dataset [Bowman et al., 2014] even when comparing with competitive models that have access to ground-truth syntactic information.
Notation For readability, all vectors and matrices are denoted by lowercase and uppercase bold letters such as x and X, respectively. When a scalar is added to a vector, the addition is applied element-wise [Zhang et al., 2021]. 2 Method
This section introduces the proposed Self-IRU. The Self-IRU is fundamentally a recurrent model, but distinguishes itself in that the gating functions that control compositions over time are recursively modeled by instances of the Self-IRU itself, where the extent of recursion is dynamic. In the following, we begin with the model architecture that can recursively self-instantiate. Then we detail its key components such as how dynamic soft recursion is enabled. 2.1 Self-Instantiation
Given an input sequence of tokens x1, . . . , xT , the Self-IRU transforms them into hidden states throughout all the time steps: h1, . . . , hT . Denoting by L the user-speciﬁed maximum recursion depth, the hidden state at time step t is ht = Self-IRU(L)(xt, h(L) t−1), 2
Figure 1: The self-instantiated recurrent unit (Self-IRU) model architecture. Circles represent gates that control information ﬂow from dotted lines, and squares represent transformations or operators. where Self-IRU(L) is an instance of the Self-IRU model at recursion depth L and h(L) state at time step t 0 t−1 is a hidden 1 and recursion depth L. In general, a Self-IRU instance at any recursion depth
L returns a hidden state for that depth:
− l
≤
≤
Self-IRU(l)(xt, h(l) t−1) = h(l) t
, which involves the following computation: (cid:16) (cid:16) f (l) t = σ o(l) t = σ z(l) t = tanh t = f (l) c(l) t = o(l) h(l) t (cid:12) t (cid:12) t Self-IRU(l−1)(xt, h(l−1)
α(n) t−1 ) + (1 t Self-IRU(l−1)(xt, h(l−1)
β(n) (cid:16) (cid:17) t−1 ) + (1 z (xt, h(l)
F (l) t−1) f (l) t )
− z(l) t (cid:12) c(l) t−1 + (1 c(l) t + xt, (cid:17)
)F (l) f (xt, h(l) o (xt, h(l) t−1) (cid:17) t−1)
)F (l)
α(n) t
β(n) t
−
− (2.1) (2.2) (2.3) (2.4) (2.5) denotes the element-wise multiplication, σ denotes the sigmoid function, scalars α(n) and (cid:12) are soft depth gates at time step t and recursion node n in the unrolled recursion paths, and F (l) f , are base transformations at recursion depth l. Without losing sight of the big picture, where
β(n) t
F (l) o , and F (l) z we will provide more details of such soft depth gates and base transformations later. t
On a high level, Figure 1 depicts the Self-IRU model architecture. We highlight that two gating functions of a Self-IRU, the forget gate f (l) in (2.2), are recursively controlled by instances of the Self-IRU itself. Therefore, we call both the forget and output gates the self-instantiation gates. The base case (l = 0) for self-instantiation gates is (xt, h(0) (xt, h(0) in (2.1) and the output gate o(l) t and o(0) f (0) t = σ (cid:17) t−1) t = σ
F (0) o t−1)
F (0) f (cid:17) (cid:16) (cid:16)
. t
At each recursion depth l, the candidate memory cell z(l) t at time step t is computed in (2.3). Then t and the memory cell c(l) in (2.4), the forget gate f (l) t−1 at the t previous time step to produce the memory cell c(l) t at the current time step t. As illustrated by the bottom arrow starting from xt in Figure 1, the output gating in (2.5) also adds a skip connection from residual networks to facilitate gradient ﬂow throughout the recursive self-instantiation of the
Self-IRU [He et al., 2016]. controls the information ﬂow from z(l) 2.2 Dynamic Soft Recursion
Now let us detail the soft depth gates α(n) in (2.1) and (2.2) for time step t and recursion t node n in the unrolled recursion paths. The index n is used to distinguish nodes at different positions and β(n) t 3
and β(n)
Figure 2: Soft depth gates α(n) for time step t and recursion node n (denoting F and R as the left child and the right child, respectively) control the extent of the Self-IRU recursion. The extent is indicated by greyscale of any node at the beginning of an arrow along an unrolled recursion path.
These gates are between 0 and 1 and may vary across the temporal dimension, enabling dynamic soft recursion depth at each time step (here maximum depth L = 2). t t in the recursion tree (e.g., in Figure 2) that is determined by the maximum recursion depth L. We propose learning them in a data-driven fashion. Speciﬁcally, we parameterize α(n) and β(n) t with t
α(n) t = σ(F (n)
α (xt)) and β(n) t = σ(F (n)
β (xt)),
∗
∗
α, β
R (
∗ ∈ {
∗ xt + b(n) (xt) = W(n)
∈ both learned from data. where F (n)
∗ parameters b(n)
Together with the sigmoid function σ, these simple linear transformations of the input token xt are applied dynamically at each time step t across the input sequence. Moreover, as shown in (2.1) and (2.2), mathematically 0 < α(n) t < 1 control the extent of recursion at each recursion node n, enabling soft depth along any unrolled recursive self-instantiation path. Thus, α(n) are called the soft depth gates.
) with weight parameters W(n)
} and β(n) and bias
, β(n)
∗ t t t
Putting these together, Figure 2 unrolls the recursive self-instantiation paths at two consecutive time steps to illustrate dynamic soft recursion depth. Speciﬁcally, the “softness” is indicated by greyscale of any node at the beginning of an arrow along an unrolled recursion path. In sharp contrast to multi-layer RNNs, Self-IRUs enable tree structures of self-instantiation, where the extent of recursion is dynamic (to be visualized in Section 3.7). 2.3 Base Transformations in (2.1), F (l) o
At any recursion depth l, F (l) f input xt and the hidden state h(l) units (e.g., LSTM): at recursion depth l, for
∗ (xt, h(l)
F (l)
∗ ∈ { f, o, z we have
}
∗ (xt, h(l) t−1) = RNN(l) t−1). in (2.2), and F (l) z in (2.3) are base transformations of the t−1. For example, we can model base transformations using RNN
Alternatively, we may also model base transformations with linear layers that only transform the input xt using learnable weight parameters W(l)
∗ and bias parameters b(l)
∗ for f, o, z
∗ ∈ {
:
}
∗ (xt) = W(l)
F (l)
∗ xt + b(l)
∗ .
The Self-IRU is agnostic to the choice of base transformations and we will evaluate different choices in the experiments. We will discuss how the Self-IRU can be useful as a (parallel) non-autoregressive model and connects to other recurrent models in the supplementary material. 3 Experiments
To demonstrate the architectural ﬂexibility and effectiveness, we evaluate Self-IRUs on a wide range of publicly available benchmarks, perform ablation studies on the maximum recursion depth and base transformations, and analyze dynamics of soft depth gates. 4
3.1 Pixel-wise Sequential Image Classiﬁcation
The sequential pixel-wise image classiﬁcation problem treats pixels in images as sequences. We use the well-established pixel-wise MNIST and CIFAR-10 datasets.
Table 1: Experimental results (accuracy) on the pixel-wise sequential image classiﬁcation task.
Model
Independently R-RNN [Li et al., 2018a] r-LSTM with Aux Loss [Trinh et al., 2018]
Transformer (self-attention) [Trinh et al., 2018]
TrellisNet [Bai et al., 2018b] (reported)
TrellisNet [Bai et al., 2018b] (our run)
Self-IRU
#Params MNIST CIFAR-10 99.00 98.52 98.90 99.20 97.59 99.04
-72.20 62.20 73.42 55.83 73.01
---8.0M 8.0M 0.9M
Table 1 reports the results of Self-IRUs against independently recurrent RNNs [Li et al., 2018a], r-LSTMs with aux loss [Trinh et al., 2018], Transformers (self-attention) [Trinh et al., 2018], and
TrellisNets [Bai et al., 2018b]. On both the MNIST and CIFAR-10 datasets, the Self-IRU outperforms most of the other investigated baseline models. For the only exception, parameters of the Self-IRU are only about 1/8 of those of the TrellisNet [Bai et al., 2018b] while still achieving comparable performance. This supports that the Self-IRU is a reasonably competitive sequence encoder. 3.2 Logical Inference
We experiment for the logical inference task on the standard dataset2 proposed by Bowman et al.
[2014]. This classiﬁcation task is to determine the semantic equivalence of two statements expressed with logic operators such as not, and, and or. As per prior work [Shen et al., 2018], the model is trained on sequences with 6 or fewer operations and evaluated on sequences of 6 to 12 operations.
Table 2: Experimental results (accuracy) on the logical inference task (symbol denotes models with access to ground-truth syntax). The baseline results are reported from [Shen et al., 2018]. The
Self-IRU achieves state-of-the-art performance.
†
Model
Tree-LSTM† [Tai et al., 2015]
LSTM [Bowman et al., 2014]
RRNet [Jacob et al., 2018]
ON-LSTM [Shen et al., 2018]
Self-IRU
#Operations
= 7 = 8 = 9 = 10 = 11 = 12 87.0 93.0 69.0 88.0 71.0 84.0 76.0 91.0 88.0 97.0 86.0 71.0 72.0 78.0 90.0 89.0 78.0 74.0 81.0 92.0 87.0 80.0 78.0 86.0 93.0 90.0 85.0 81.0 87.0 95.0
We compare Self-IRUs with Tree-LSTMs [Tai et al., 2015], LSTMs [Bowman et al., 2014], RR-Nets [Jacob et al., 2018], and ordered-neuron (ON-) LSTMs [Shen et al., 2018] based on the common experimental setting in these works. Table 2 reports our results on the logical inference task. The
Self-IRU is a strong and competitive model on this task, outperforming ON-LSTM by a wide margin (+12% on the longest number of operations). Notably, the Self-IRU achieves state-of-the-art per-formance on this dataset even when comparing with Tree-LSTMs that have access to ground-truth syntactic information. 3.3 Sorting and Tree Traversal
We also evaluate Self-IRUs on two algorithmic tasks that are solvable by recursion: sorting and tree traversal. In sorting, the input to the model is a sequence of integers. The correct output is the sorted sequence of integers. Since mapping sorted inputs to outputs can be implemented in a recursive fashion, we evaluate the Self-IRU’s ability to model recursively structured sequence data.
An example input-output pair would be 9, 1, 10, 5, 3 1, 3, 5, 9, 10. We evaluate on sequence length m =
→ 5, 10
{
.
} 2https://github.com/sleepinyourhat/vector-entailment. 5
In the tree traversal problem, we construct a binary tree of maximum depth n. The goal is to generate the postorder tree traversal given the inorder and preorder traversal of the tree. This is known to arrive at only one unique solution. The constructed trees have random sparsity where trees can grow up to maximum depth n. Hence, these trees can have varying depths (models can solve the task entirely when trees are ﬁxed and full). We concatenate the postorder and inorder sequences, delimited by a special token. We evaluate on maximum depth n
, we ensure that each tree traversal has at least 10 tokens. For n = 10, we ensure that each path has at least 15 tokens.
An example input-output pair would be 13, 15, 4, 7, 5, X, 13, 4, 15, 5, 7 5, 8
∈ {
} 7, 15, 13, 4, 5.
. For n
} 3, 4, 5, 8, 10
∈ {
We frame sorting and tree traversal as sequence-to-sequence [Sutskever et al., 2014] tasks and evaluate models with measures of exact match (EM) accuracy and perplexity (PPL). We use a standard encoder-decoder architecture with attention [Bahdanau et al., 2014], and vary the encoder module with BiLSTMs, stacked BiLSTMs, and ordered-neuron (ON-) LSTMs [Shen et al., 2018].
→
Table 3: Experimental results on the sorting and tree traversal tasks.
SORTING m = 5 m = 10
EM
Model
BiLSTM 79.9
Stacked BiLSTM 83.4 90.8
ON-LSTM 92.2
Self-IRU
PPL 1.2 1.2 1.1 1.1
EM 78.9 88.0 87.4 90.6
PPL 1.2 1.1 1.1 1.1 n = 3
EM PPL 1.0 100 1.0 100 1.0 100 1.0 100 n = 4
TREE TRAVERSAL n = 5
EM 96.9 98.0 81.0 98.4
PPL 2.4 1.0 1.4 1.0
EM 60.3 63.4 55.7 63.4
PPL 2.4 2.5 2.8 1.8 n = 8
EM PPL 30.6 5.6 5.9 99.9 52.3 5.5 20.4 5.6 n = 10
EM 2.2 2.8 2.7 2.8
PPL 132.0 225.1 173.2 119.0
Table 3 reports our results on the sorting and tree traversal tasks. In fact, all the models solve the tree traversal task with n = 3. However, the task gets increasingly harder with a greater maximum possible depth and largely still remains a challenge for neural models today. On one hand, stacked
BiLSTMs always perform better than BiLSTMs and ON-LSTMs occasionally perform worse than standard BiLSTMs on tree traversal, while for the sorting task ON-LSTMs perform much better than standard BiLSTMs. On the other hand, the relative performance of the Self-IRU is generally better than any of these baselines, especially pertaining to perplexity. 3.4 Music Modeling
Moreover, we evaluate the Self-IRU on the polyphonic music modeling task, i.e., generative modeling of musical sequences. We use three well-established datasets: Nottingham, JSB Chorales, and Piano
Midi [Boulanger-Lewandowski et al., 2012]. The inputs are 88-bit (88 piano keys) sequences.
Table 4: Experimental results (negative log-likelihood) on the music modeling task.
Model
GRU [Chung et al., 2014]
LSTM [Song et al., 2019]
G2-LSTM [Li et al., 2018b]
B-LSTM [Song et al., 2019]
TCN [Bai et al., 2018a] (reported)
TCN [Bai et al., 2018a] (our run)
Self-IRU
Nottingham JSB Piano Midi 8.54 8.61 8.67 8.30 8.10 8.13 8.12 8.82 7.99 8.18 7.55
-7.53 7.49 3.13 3.25 3.21 3.16 3.07 2.95 2.88
Table 4 compares the Self-IRU with a wide range of published works: GRU [Chung et al., 2014],
LSTM [Song et al., 2019], G2-LSTM [Li et al., 2018b], B-LSTM [Song et al., 2019], and TCN [Bai et al., 2018a]. The Self-IRU achieves the best performance on the Nottingham and Piano midi datasets.
It also achieves competitive performance on the JSB Chorales dataset, only underperforming the state-of-the-art by 0.02 negative log-likelihood. 3.5 Semantic Parsing and Code Generation
We further evaluate Self-IRUs on the semantic parsing (the Geo, Atis, and Jobs datasets) and code generation (the Django dataset) tasks. They are mainly concerned with learning to parse and generate structured data. We run our experiments on the publicly released source code3 of [Yin and Neubig, 3https://github.com/pcyin/tranX 6
2018], replacing the recurrent decoder with our Self-IRU decoder (TranX + Self-IRU). We only replace the recurrent decoder since our early experiments showed that varying the encoder did not yield any beneﬁts in performance. Overall, our hyperparameter details strictly follow the codebase of [Yin and Neubig, 2018], i.e., we run every model from their codebase as it is.
Table 5: Experimental results (accuracy) on the semantic parsing (the Geo, Atis, and Jobs datasets) and code generation tasks (the Django dataset).
Model
Seq2Tree [Dong and Lapata, 2016]
LPN [Ling et al., 2016]
NMT [Neubig, 2015]
YN17 [Yin and Neubig, 2017]
ASN [Rabinovich et al., 2017]
ASN + Supv. Attn. [Rabinovich et al., 2017]
TranX [Yin and Neubig, 2018] (reported in code)
TranX [Yin and Neubig, 2018] (our run)
TranX + Self-IRU
Geo Atis 84.6 87.1
------85.3 85.7 85.9 87.1 88.6 87.7 87.5 87.5 88.4 88.6
Jobs Django
------90.0 90.0 90.7 31.5 62.3 45.1 71.6
--77.2 76.7 78.3
Table 5 reports the experimental results in comparison with the other competitive baselines such as Seq2Tree [Dong and Lapata, 2016], LPN [Ling et al., 2016], NMT [Neubig, 2015], YN17 [Yin and Neubig, 2017], ASN (with and without supervised attention) [Rabinovich et al., 2017], and
TranX [Yin and Neubig, 2018]. We observe that TranX + Self-IRU outperforms all the other approaches, achieving state-of-the-art performance. On the code generation task, TranX + Self-IRU outperforms TranX by +1.6% and
+1% on all the semantic parsing tasks. More importantly, the performance gain over the base TranX method allows us to observe the ablative beneﬁt of the
Self-IRU that is achieved by only varying the recurrent decoder.
≈ 3.6 Ablation Studies of the Maximum Recursion Depth and Base Transformations
Table 6: Ablation studies of the maximum recursion depth and base transformation on the semantic parsing (SP) and code generation (CG) tasks.
Table 6 presents ablation studies of the maximum recursion depth (Section 2.1) and base transfor-mations (Section 2.3) of Self-IRUs. The results are based on the semantic parsing (Atis) and code generation (Django) tasks.
We can see that their optimal choice is task dependent: (i) on the semantic parsing task, using the linear layer performs better than the LSTM for base transfor-mations; (ii) conversely, the linear transformation performs worse than the LSTM on the code generation task.
Max Depth Base Transformations
Linear 1
Linear 2
Linear 3
LSTM 1
LSTM 2
CG 77.56 77.62 76.84 78.33 77.39
SP 88.40 88.21 87.80 86.61 85.93
On the whole, we also observe this across the other tasks in the experiments. Table 7 re-ports their optimal combinations for diverse tasks in the experi-ments, where the maximum re-cursion depth is evaluated on
L = 0, 1, 2, 3
. As we can
}
{ tell from different optimal com-binations in Table 7, choices of the maximum recursion depth and base transformations of Self-IRUs depend on tasks.
Table 7: The optimal maximum recursion depth and base transfor-mations for different tasks in the experiments.
Task
Image classiﬁcation
Logical inference
Tree traversal
Sorting
Music modeling
Semantic parsing
Code generation
Max Depth Base Transformations 1 2 1 1 2 1 1
LSTM
LSTM
LSTM
LSTM
Linear
Linear
LSTM 7
3.7 Analysis of Soft Depth Gates
Besides the task-speciﬁc maximum recursion depth and base transformations, empirical effectiveness of Self-IRUs may also be explained by the modeling ﬂexibility via the inductive bias towards dynamic soft recursion (Section 2.2). We will analyze in two aspects below. (a) Initial (CIFAR-10) (b) Epoch 10 (CIFAR-10) (c) Initial (MNIST) (d) Epoch 10 (MNIST)
Figure 3: Soft depth gate values at initialization and training epoch 10 on the CIFAR-10 and MNIST datasets. the Self-IRU
First, during training, has the ﬂexibility of building data-dependent recursive patterns of self-instantiation. Figure 3 displays values of all the soft depth gates at all the three recursion depths on the CIFAR-10 and MNIST datasets, depicting how the recursive pattern of the Self-IRU is updated during training. For differ-ent datasets, the Self-IRU also ﬂexibly learns to construct different soft recur-sive (via soft depth gates of values be-tween 0 and 1) patterns.
Second, we want to ﬁnd out whether the Self-IRU has the ﬂexibility of softly learning the recursion depth on a per-time-step basis via the inductive bias to-wards dynamic soft recursion. Figure 4 visualizes such patterns (i) for pixel-wise sequential image classiﬁcation on the CIFAR-10 and MNIST datasets and (ii) for music modeling on the Notting-ham dataset. Notably, all the datasets have very diverse temporal composi-tions of recursive patterns. More con-cretely, the soft depth gate values ﬂuc-tuate aggressively on the CIFAR-10 dataset (consisting of color images) in
Figure 4a while remaining more stable for music modeling in Figure 4c. More-over, these soft depth gate values remain totally constant on the MNIST dataset (consisting of much simpler grayscale images) in Figure 4b. These provide compelling empirical evidence for the architectural ﬂexibility of Self-IRUs: they can adjust the dynamic construc-tion adaptively and can even revert to static recursion over time if necessary (such as for simpler tasks). (a) Image classiﬁcation (CIFAR-10) (b) Image classiﬁcation (MNIST) (c) Music modeling (Nottingham)
Figure 4: Soft depth gate values across the temporal dimen-sion. “L” and “R” denote α(n)
, respectively (e.g., t
“LLR” denotes the node at the end of the unrolled recursive path α(n) and β(n)
α(n)
). t
β(n) t t → t → 8
The dynamic soft recursion pattern is made more intriguing by observing how the “softness” alters on the CIFAR-10 and Nottingham datasets. From Figure 4c we observe that the soft recursion pattern of the model changes in a rhythmic fashion, in line with our intuition of musical data. When dealing with pixel information, the recursive pattern in Figure 4a changes adaptively according to the more complex color-image information. Though these empirical results are intuitive, a better understanding of such behaviors may beneﬁt from theoretical or biological perspectives in the future. 4