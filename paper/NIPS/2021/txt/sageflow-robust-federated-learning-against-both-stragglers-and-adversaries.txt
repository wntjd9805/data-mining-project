Abstract
While federated learning (FL) allows efﬁcient model training with local data at edge devices, among major issues still to be resolved are: slow devices known as stragglers and malicious attacks launched by adversaries. While the presence of both of these issues raises serious concerns in practical FL systems, no known schemes or combinations of schemes effectively address them at the same time.
We propose Sageﬂow, staleness-aware grouping with entropy-based ﬁltering and loss-weighted averaging, to handle both stragglers and adversaries simultaneously.
Model grouping and weighting according to staleness (arrival delay) provides robustness against stragglers, while entropy-based ﬁltering and loss-weighted averaging, working in a highly complementary fashion at each grouping stage, counter a wide range of adversary attacks. A theoretical bound is established to provide key insights into the convergence behavior of Sageﬂow. Extensive experimental results show that Sageﬂow outperforms various existing methods aiming to handle stragglers/adversaries. 1

Introduction
Large volumes of data collected at various edge devices (i.e., smart phones) are valuable resources in training a model with a good performance. Federated learning [18, 12, 7] is a promising direction for large-scale learning, which enables training of a global model with less privacy concerns. However, among major issues that need to be addressed in current federated learning (FL) systems are the devices called stragglers that are considerably slower than the average and the adversaries that enforce a various form of attacks.
Regarding the ﬁrst issue, simply waiting for all the stragglers at each global round can signiﬁcantly slow down the overall training process. To handle stragglers, asynchronous FL schemes [15, 22, 25, 21, 14] update the global model every time the server receives a local model from each device; especially in FedAsync [25], the global model is updated asynchronously according to the device’s staleness t − τ , the time difference between the current round t and the past round τ when the device
ﬁrst received the global model. While the asynchronous schemes are highly effective in handling stragglers, the one-by-one update nature does not lend itself well for integration with established aggregation methods to combat the second issue, the adversaries.
There are different forms of adversarial attacks that signiﬁcantly degrade current FL systems. In untargeted attacks, an attacker can poison the updated model at the devices before it is sent to the
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
server (model poisoning) [3, 9] or can poison the datasets of each device (data poisoning) [2, 16]. In targeted attacks (or backdoor attacks) [4, 1, 20], the adversaries cause the model to misclassify the targeted subtasks only, while not degrading the overall test accuracy that much. Robust federated averaging (RFA) of [19], a well-known method proposed to handle adversaries in FL, employs geometric-median-based aggregation to provide a fair level of protection. Other various aggregation schemes (e.g., Multi-Krum) also successfully handle adversaries in distributed learning [3, 28, 5].
Unfortunately, however, the performance of these methods are substantially degraded when the portion of adversaries is large. The presence of stragglers can drive the attack ratio higher (e.g., by ignoring stragglers), signiﬁcantly degrading the performance of current aggregation schemes.
While the presence of both stragglers and adversaries raises signiﬁcant concerns in practical FL, to our knowledge, there are currently no existing methods or known combinations of ideas that can effectively handle these two issues simultaneously.
Main contributions. We propose Sageﬂow, staleness-aware grouping with entropy-based ﬁltering and loss-weighted averaging, a robust FL strategy which can handle both stragglers and adversaries at the same time. Targeting the straggler issue, our strategy is to perform periodic global aggregation while allowing the results sent from stragglers to be aggregated in later rounds. In each global round, we take advantage of the results sent from stragglers by ﬁrst grouping the models that come from the same initial models (i.e., same staleness), to obtain a group representative model. Then, we aggregate the representative models of all groups based on their staleness, to obtain the global model. Our periodic aggregation strategy is not only effective in neutralizing stragglers but also provides a great platform for countering adversaries, as discussed below.
Targeting each grouping stage of our straggler-mitigating idea, we propose an intra-group defense strategy which is based on our entropy-based ﬁltering and loss-weighted averaging. The entropy
ﬁltering ﬁrst ﬁlters out the models with high entropies, i.e., outliers, and the loss-weighted averaging of the survived models enables model aggregation according to the measured qualities of the received local models. These two methods work in a highly complementary fashion to effectively counter a wide range of adversarial attacks in each grouping stage. Here, in computing the entropy and loss of each received model, we utilize public data that we assume to be available at the server. In fact, the utilization of public data is not a new idea, as seen in recent FL setups of [30, 27, 11]. This is generally a reasonable setup since data centers typically have some collected data that can be accessed by public. For example, different types of anonymous medical data are often available for public research in various countries. We show later via experiments that only a very small amount of public data is necessary at the server (1-2% of the entire dataset, which is comparable to the amount of local data at a single device) to successfully combat adversaries. Our main contributions are as follows:
• We propose Sageﬂow, handling both stragglers/adversaries simultaneously in FL, via a novel staleness-aware grouping combined with entropy ﬁltering and loss-weighted averaging.
• We derive the theoretical bound for Sageﬂow based on key parameters and provide insights into the convergence behavior.
• Experimental results on different datasets show that Sageﬂow outperforms various combina-tions of straggler/adversary defense methods using only a small portion of public data.