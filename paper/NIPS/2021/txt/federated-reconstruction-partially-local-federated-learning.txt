Abstract
Personalization methods in federated learning aim to balance the beneﬁts of feder-ated and local training for data availability, communication cost, and robustness to client heterogeneity. Approaches that require clients to communicate all model parameters can be undesirable due to privacy and communication constraints. Other approaches require always-available or stateful clients, impractical in large-scale cross-device settings. We introduce Federated Reconstruction, the ﬁrst model-agnostic framework for partially local federated learning suitable for training and inference at scale. We motivate the framework via a connection to model-agnostic meta learning, empirically demonstrate its performance over existing approaches for collaborative ﬁltering and next word prediction, and release an open-source library for evaluating approaches in this setting. We also describe the successful deployment of this approach at scale for federated collaborative ﬁltering in a mobile keyboard application. 1

Introduction
Federated learning is a machine learning setting in which distributed clients solve a learning objective on sensitive data via communication with a coordinating server [44]. Typically, clients collaborate to train a single global model under an objective that combines heterogeneous local client objectives.
For example, clients may collaborate to train a next word prediction model for a mobile keyboard application without sharing sensitive typing data with other clients or a centralized server [28].
This paradigm has been scaled to production and deployed in cross-device settings [3, 28, 56] and cross-silo settings [11, 13].
However, training a fully global federated model may not always be ideal due to heterogeneity in clients’ data distributions. Yu et al. [58] show that global models can perform worse than purely local (non-federated) models for many clients (e.g., those with many training examples). Moreover, in some settings privacy constraints completely prohibit fully global federated training. For instance, for models with user-speciﬁc embeddings, such as matrix factorization models for collaborative ﬁltering
[37], naively training a global federated model involves sending updates to user embeddings on the server, directly revealing potentially sensitive individual preferences [21, 47].
To address this, we explore partially local federated learning. In this setting, models are partitioned into global g and local parameters l such that local parameters never leave client devices. This enables training on sensitive user-speciﬁc parameters as in the collaborative ﬁltering setting, and we show it can also improve robustness to client data heterogeneity and communication cost for other settings, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Schematic of Federated Reconstruction. Model variables are partitioned into global and local variables. For every round t, each participating client i is sent the current global variables, uses them to reconstruct its own local variables, and then updates its copy of the global variables. The server aggregates updates to only the global variables across clients. since we are effectively interpolating between local and federated training. Previous works have looked at similar settings [4, 41]. Importantly, these approaches cannot realistically be applied at scale in cross-device settings because they assume clients are stateful or always-available: in practice, clients are sampled from an enormous population with unreliable availability, so approaches that rely on repeated sampling of the same stateful clients are impractical (Kairouz et al. [34] [Table 1]). Other work has demonstrated that stateful federated algorithms in partial participation regimes can perform worse than stateless algorithms due to the state becoming "stale" [48]. Previous methods also do not enable inference on new clients unseen during training, preventing real-world deployment.
These limitations motivate a new method for partially local federated learning, balancing the beneﬁts of federated aggregation and local training. This approach should be: 1. Model-agnostic: works with any model. 2. Scalable: compatible with large-scale cross-device training with partial participation. 3. Practical for inference: new clients can perform inference. 4. Fast: clients can quickly adapt local parameters to their personal data.
In this work, we propose combining federated training of global parameters with reconstruction of local parameters (see Figure 1). We show that our method relaxes the statefulness requirement of previous work and enables fast personalization for unseen clients without additional communication, even for models without user-speciﬁc embeddings.
Our contributions: We make the following key contributions:
•
•
•
•
•
Introduce a model-agnostic framework for training partially local and partially global models, satisfying the above criteria. We propose a practical algorithm instantiating this framework (FEDRECON).
Justify the algorithm via a connection to model-agnostic meta learning (see Section 4.2), showing that FEDRECON naturally leads to fast reconstruction at test time (see Table 1).
Demonstrate FEDRECON’s empirical performance over existing approaches for applications in collaborative ﬁltering and next word prediction, showing that our method outperforms standard centralized and federated training in performance on unseen clients (see Table 1), enables fast adaptation to clients’ personal data (see Figure 3), and matches the performance of other federated personalization techniques with less communication (see Figure 2).
Release an open-source library for evaluating algorithms across tasks in this setting.1
Describe the successful deployment of this approach at scale for collaborative ﬁltering in a real-world mobile keyboard application (see Section 7). 2