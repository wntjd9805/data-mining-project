Abstract
This paper investigates how to realize better and more efficient embedding learn-ing to tackle the semi-supervised video object segmentation under challenging multi-object scenarios. The state-of-the-art methods learn to decode features with a single positive object and thus have to match and segment each target separately under multi-object scenarios, consuming multiple times computing resources. To solve the problem, we propose an Associating Objects with Transformers (AOT) approach to match and decode multiple objects uniformly. In detail, AOT em-ploys an identification mechanism to associate multiple targets into the same high-dimensional embedding space. Thus, we can simultaneously process multiple objects’ matching and segmentation decoding as efficiently as processing a single object. For sufficiently modeling multi-object association, a Long Short-Term
Transformer is designed for constructing hierarchical matching and propagation.
We conduct extensive experiments on both multi-object and single-object bench-marks to examine AOT variant networks with different complexities. Particularly, our R50-AOT-L outperforms all the state-of-the-art competitors on three popu-lar benchmarks, i.e., YouTube-VOS (84.1% J &F), DAVIS 2017 (84.9%), and
DAVIS 2016 (91.1%), while keeping more than 3× faster multi-object run-time.
Meanwhile, our AOT-T can maintain real-time multi-object speed on the above benchmarks. Based on AOT, we ranked 1st in the 3rd Large-scale VOS Challenge. 1

Introduction
Video Object Segmentation (VOS) is a fundamental task in video understanding with many potential applications, including augmented reality [25] and self-driving cars [52]. The goal of semi-supervised
VOS, the main task in this paper, is to track and segment object(s) across an entire video sequence based on the object mask(s) given at the first frame.
Thanks to the recent advance of deep neural networks, many deep learning based VOS algorithms have been proposed recently and achieved promising performance. STM [26] and its following works [34, 23] leverage a memory network to store and read the target features of predicted past frames and apply a non-local attention mechanism to match the target in the current frame. FEELVOS [41] and CFBI [50, 51] utilize global and local matching mechanisms to match target pixels or patches from both the first and the previous frames to the current frame.
Even though the above methods have achieved significant progress, the above methods learn to decode scene features that contain a single positive object. Thus under a multi-object scenario, they have to match each object independently and ensemble all the single-object predictions into a multi-object segmentation, as shown in Fig. 1a. Such a post-ensemble manner eases network architectures’ design 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Post-ensemble (b) Associating objects (ours) (c) Comparison
Figure 1: VOS methods (e.g., [50, 34]) process multi-object scenarios in a post-ensemble manner (a).
In contrast, our AOT associates all the objects uniformly (b), leading to better efficiency (c). since the networks are not required to adapt the parameters or structures for different object numbers.
However, modeling multiple objects independently, instead of uniformly, is inefficient in exploring multi-object contextual information to learn a more robust feature representation for VOS. In addition, processing multiple objects separately yet in parallel requires multiple times the amount of GPU memory and computation for processing a single object. This problem restricts the training and application of VOS under multi-object scenarios, especially when computing resources are limited.
To solve the problem, Fig. 1b demonstrates a feasible approach to associate and decode multiple objects uniformly in an end-to-end framework. Hence, we propose an Associating Objects with
Transformers (AOT) approach to match and decode multiple targets uniformly. First, an identification mechanism is proposed to assign each target a unique identity and embed multiple targets into the same feature space. Hence, the network can learn the association or correlation among all the targets. Moreover, the multi-object segmentation can be directly decoded by utilizing assigned identity information. Second, a Long Short-Term Transformer (LSTT) is designed for constructing hierarchical object matching and propagation. Each LSTT block utilizes a long-term attention for matching with the first frame’s embedding and a short-term attention for matching with several nearby frames’ embeddings. Compared to the methods [26, 34] utilizing only one attention layer, we found hierarchical attention structures are more effective in associating multiple objects.
We conduct extensive experiments on two popular multi-object benchmarks for VOS, i.e., YouTube-VOS [48] and DAVIS 2017 [31], to validate the effectiveness and efficiency of the proposed AOT.
Even using the light-weight Mobilenet-V2 [33] as the backbone encoder, the AOT variant networks achieve superior performance on the validation 2018 & 2019 splits of the large-scale YouTube-VOS (ours, J &F 82.6∼ 84.5% & 82.2∼ 84.5%) while keeping more than 2× faster multi-object run-time (27.1∼ 9.3FPS) compared to the state-of-the-art competitors (e.g., CFBI [50], 81.4% & 81.0%, 3.4FPS). We also achieve new state-of-the-art performance on both the DAVIS-2017 validation (85.4%) and testing (81.2%) splits. Moreover, AOT is effective under single-object scenarios as well and outperforms previous methods on DAVIS 2016 [30] (92.0%), a popular single-object benchmark. Besides, our smallest variant, AOT-T, can maintain real-time multi-object speed on all above benchmarks (51.4FPS on 480p videos). Particularly, AOT ranked 1st in the Track 1 (Video
Object Segmentation) of the 3rd Large-scale Video Object Segmentation Challenge.
Overall, our contributions are summarized as follows:
• We propose an identification mechanism to associate and decode multiple targets uniformly for
VOS. For the first time, multi-object training and inference can be efficient as single-object ones, as demonstrated in Fig. 1c.
• Based on the identification mechanism, we design a new efficient VOS framework, i.e., Long Short-Term Transformer (LSTT), for constructing hierarchical multi-object matching and propagation.
LSTT achieves superior performance on VOS benchmarks [48, 31, 30] while maintaining better efficiency than previous state-of-the-art methods. To the best of our knowledge, LSTT is the first hierarchical framework for object matching and propagation by applying transformers [39] to VOS. 2