Abstract
We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reﬂect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being reﬁned, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a ﬂat and monolithic latent space towards a transparent design that encourages efﬁciency, controllability and interpretability.
We demonstrate GANformer2’s strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging
COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model’s disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects’ depths and dependencies, and up to the ﬁnal high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/ dorarad/gansformer for model implementation. 1

Introduction
Drawing, the practice behind human visual and artistic expression, can essentially be deﬁned as an iterative process. It starts from an initial outline, with just a few strokes that specify the spatial arrangement and overall layout, and is then gradually reﬁned and embellished with color, depth and richness, until a vivid picture eventually emerges. These initial schematic sketches can serve as an abstract scene representation that is very concise, yet highly expressive: several lines are enough to delineate three dimensional structures, account for perspective and proportion, convey shapes and geometry, and even imply semantic information [52, 63, 67]. A large body of research in psychology highlights the importance of sketching in stimulating creative discovery [40, 58], fostering cognitive development [27], and facilitating problem solving [26, 44]. In fact, a large variety of generative and
RL tasks either in the visual [5], textual [22, 78] or symbolic modalities [2], can beneﬁt from the same strategy – prepare a high-level plan ﬁrst, and then carry out the details.
At the heart of this hierarchical strategy stands the principle of compositionality – where the meaning of the whole can be derived from those of its constituents. Indeed, the world around us is highly structured. Our environment consists of a varied collection of objects, tightly interconnected through dependencies of all sorts: from close-by to long-range, and from physical and dynamic to abstract and semantic [10, 66]. As pointed out by prior literature [24, 51, 55, 57], compositional representations are pivotal to human intelligence, supporting our capabilities of reasoning [32], planning [53], learning [54] and imagination [6]. Realizing this principle, and explicitly capturing the objects and elements composing the scene, is thereby a desirable goal, that can make the generative process more explainable, controllable, versatile and efﬁcient. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The GANformer2 model has a set of latent variables that represent the objects and entities within the generated scene. It proceeds through two stages, planning and execution: ﬁrst iteratively drafting a high-level layout of the scene’s spatial structure, and then reﬁning and translating it into a photo-realistic image. Each latent corresponds to a different layout’s segment, controlling its structure during the planning, and its style during the execution. On the discriminator side, we introduce new structural losses for compositional scene generation.
Yet, the vast majority of generative models do not directly reﬂect this compositionality so intrinsic to visual scenes. Rather, most networks, and GANs in particular, aim to generate the entire scene all at once from a single monolithic latent space, through a largely opaque transformation. Consequently, while they excel in producing stunning, strikingly-realistic portrayals of faces, sole centralized objects, or natural scenery, they still struggle to faithfully mimic richer or densely-packed scenes, as those common to everyday contexts, falling short of mastering their nuances and intricacies [5, 50, 76].
Likewise, while there is some evidence for property disentanglement within their latent space to independent axes of variation at the global scale [28, 36, 64, 71], most existing GANs fail to provide controllability at the level of the individual object or local region. Understanding of the mechanisms by which these models give rise to an output image, and the transparency of their synthesis process, thereby remain rather limited.
Motivated to alleviate these shortcomings and make generative modeling more compositional, inter-pretable and controllable, we propose GANformer2, a structured object-oriented transformer that decouples the visual synthesis task into two stages: planning and execution. We begin by sampling a set of latent variables, corresponding to the objects and entities that compose the scene. Then, at the planning stage, we transform the latents into a schematic layout – a scene representation that depicts the object segments, reﬂecting their shapes, positions, semantic classes and relative ordering.
Its construction occurs in an iterative manner, to capture the dependencies and interactions between different objects. Next, at the execution stage, the layout is translated into the ﬁnal image, with the latents cooperatively guiding the content and style of their respective segments through bipartite attention [33]. This approach marks a shift towards a more natural process of image generation, in which objects and the relations among them are explicitly modeled, while the initial sketch is successively tweaked and reﬁned to ultimately produce a rich photo-realistic scene.
We study the model’s behavior and performance through an extensive set of experiments, where it attains state-of-the-art results in both conditional and unconditional synthesis, as measured along metrics of ﬁdelity, diversity and semantic consistency. GANformer2 reaches high versatility, demon-strated through multiple datasets of challenging simulated and real-world scenes. Further analysis illustrates its capacity to manipulate chosen objects and properties in an independent manner, achiev-ing both spatial disentanglement and separation between structure and style. We notably observe amodal completion of occluded objects, likely driven by the sequential nature of the computation.
Meanwhile, inspection of the produced layouts and their components shed more light upon the syn-thesis of each resulting scene, substantiating the model’s transparency and explainability. Overall, by integrating strong compositional structure into the model, we can move in the right direction towards multiple favorable properties: increasing robustness for rich and diverse visual domains, enhancing controllability over individual objects, and improving the generative process’s interpretability. 2