Abstract
We propose a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essen-tially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More speciﬁcally, the regularizer in the variational setting is parametrized by a deep neural network and learned simultane-ously with the unrolled reconstruction operator. The variational problem is then initialized with the output of the reconstruction network and solved iteratively till convergence. Notably, it takes signiﬁcantly fewer iterations to converge as com-pared to variational methods, thanks to the excellent initialization obtained via the unrolled operator. The resulting approach combines the computational efﬁciency of end-to-end unrolled reconstruction with the well-posedness and noise-stability guarantees of the variational setting. Moreover, we demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that our approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with state-of-the-art supervised data-driven reconstruction approaches. 1

Introduction
Inverse problems are ubiquitous in imaging applications, wherein one seeks to recover an unknown model parameter x ∈ X from its incomplete and noisy measurement, given by yδ = A(x) + e ∈ Y.
Here, the forward operator A : X → Y models the measurement process in the absence of noise, and e, with (cid:107)e(cid:107)2 ≤ δ, denotes the measurement noise. For example, in computed tomography (CT), the forward operator computes line integrals of x over a predetermined set of lines in R3 and the goal is to reconstruct x from its projections along these lines. Without any further information about x, inverse problems are typically ill-posed, meaning that there could be several possible reconstructions that are consistent with the measured data, even without any noise.
The variational framework circumvents ill-posedness by encoding prior knowledge about x via a regularization functional R : X → R. In the variational setting, one solves
LY(yδ, A(x)) + λ R(x), min x∈X (1) where LY : Y × Y → R+ measures data-ﬁdelity and R penalizes undesirable or unlikely solutions.
The penalty λ > 0 balances the regularization strength with the ﬁdelity of the reconstruction. The variational problem (1) is said to be well-posed if it has a unique solution varying continuously in yδ. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The success of deep learning in recent years has led to a surge of data-driven approaches for solving inverse problems [5], especially in imaging applications. These methods come broadly in two ﬂavors: (i) end-to-end trained models that aim to directly map the measurement or a model-based noisy reconstruction to the corresponding true parameter [11, 2] and (ii) learned regularization methods that seek to ﬁnd a data-adaptive regularizer instead of handcrafting it [15, 17, 13]. Techniques in both categories have their relative advantages and demerits. Speciﬁcally, end-to-end approaches offer fast reconstruction of astounding quality, but lack in terms of theoretical guarantees and need supervised data (i.e., pairs of input and target images) for training. On the contrary, learned regularization methods inherit the provable well-posedness properties of the variational setting and can be trained on unpaired training data, however the reconstruction entails solving a high-dimensional optimization problem, which is often slow and computationally demanding.
Our work derives ideas from learned optimization and adversarial machine learning, and makes an attempt to combine the best features of both aforementioned paradigms.
In particular, the proposed method offers the ﬂexibility of training on unpaired samples, produces fast reconstructions comparable to end-to-end supervised methods in quality, while enjoying the well-posedness and stability guarantees of the learned regularization framework. We ﬁrst provide an overview of the recent literature on data-driven solutions for inverse problems before detailing our contributions. 1.1