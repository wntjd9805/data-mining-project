Abstract
It is a commonly held belief that enforcing invariance improves generalisation.
Although this approach enjoys widespread popularity, it is only very recently that a rigorous theoretical demonstration of this benefit has been established. In this work we build on the function space perspective of Elesedy and Zaidi [8] to derive a strictly non-zero generalisation benefit of incorporating invariance in kernel ridge regression when the target is invariant to the action of a compact group. We study invariance enforced by feature averaging and find that generalisation is governed by a notion of effective dimension that arises from the interplay between the kernel and the group. In building towards this result, we find that the action of the group induces an orthogonal decomposition of both the reproducing kernel Hilbert space and its kernel, which may be of interest in its own right. 1

Introduction
Recently, there has been significant interest in models that are invariant to the action of a group on their inputs. It is believed that engineering models in this way improves sample efficiency and generalisation. Intuitively, if a task has an invariance, then a model that is constructed to be invariant ahead of time should require fewer examples to generalise than one that must learn to be invariant.
Indeed, there are many application domains, such as fundamental physics or medical imaging, in which the invariance is known a priori [29, 32]. Although this intuition is certainly not new (e.g. [33]), it has inspired much recent work (for instance, see [36, 15]).
However, while implementations and practical applications abound, until very recently a rigorous theoretical justification for invariance was missing. As pointed out in [8], many prior works such as [28, 24] provide only worst-case guarantees on the performance of invariant algorithms. It follows that these results do not rule out the possibility of modern training algorithms automatically favouring invariant models, irrespective of the choice of architecture. Steps towards a more concrete theory of the benefit of invariance have been taken by [8, 20] and our work is a continuation along the path set by [8].
In this work we provide a precise characterisation of the generalisation benefit of invariance in kernel ridge regression. In contrast to [28, 24], this proves a provably strict generalisation benefit for invariant, feature-averaged models. In deriving this result, we provide insights into the structure of reproducing kernel Hilbert spaces in relation to invariant functions that we believe will be useful for analysing invariance in other kernel algorithms.
The use of feature averaging to produce invariant predictors enjoys both theoretical and practical success [17, 9]. For the purposes of this work, feature averaging is defined as training a model as normal (according to any algorithm) and then transforming the learned model to be invariant.
This transformation is done by orbit-averaging, which means projecting the model on the space of invariant functions using the operator O introduced in Section 2.3.
Kernel methods have a long been a mainstay of machine learning (see [30, Section 4.7] for a brief historical overview). Kernels can be viewed as mapping the input data into a potentially infinite 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dimensional feature space, which allows for analytically tractable inference with non-linear predictors.
While modern machine learning practice is dominated by neural networks, kernels remain at the core of much of modern theory. The most notable instance of this is the theory surrounding the neural tangent kernel [11], which states that the functions realised by an infinitely wide neural network belong to a reproducing kernel Hilbert space (RKHS) with a kernel determined by the network architecture. This relation has led to many results on the theory of optimisation and generalisation of wide neural networks (e.g. [14, 3]). In the same vein, via the NTK, we believe the results of this paper can be extended to study wide, invariant neural networks. 1.1 Summary of Contributions
This paper builds towards a precise characterisation of the benefit of incorporating invariance in kernel ridge regression by feature averaging.
Lemma 3, given in Section 3, forms the basis of our work, showing that the action of the group G on the input space induces an orthogonal decomposition of the RKHS H as
H = H ⊕ H⊥ where each term is an RKHS and H consists of all of the invariant functions in H. We stress that, while the main results of this paper concern kernel ridge regression, Lemma 3 holds regardless of training algorithm and could be used to explore invariance in other kernel methods.
Our main results are given in Section 4 and we outline them here. We define the generalisation gap
∆(f, f ′) for two predictors f, f ′ as the difference in their test errors. If ∆(f, f ′) > 0 then f ′ has strictly better test performance than f . Theorem 5 describes ∆(f, f ′) for f being the solution to kernel ridge regression and f ′ its invariant (feature averaged) version and shows that it is positive when the target is invariant.
More specifically, let X ∼ µ where µ is G-invariant and Y = f ∗(X) + ξ with f ∗ G-invariant and
E[ξ] = 0, E[ξ2] = σ2 < ∞. Let f be the solution to kernel ridge regression with kernel k and regularisation parameter ρ > 0 on n i.i.d. training examples {(Xi, Yi) ∼ (X, Y ) : i = 1, . . . , n} and let f ′ be its feature averaged version. Our main result, Theorem 5, says that
E[∆(f, f ′)] ≥
σ2 dimeff(H⊥) + E
√ n)2 nMk + ρ/ (
√ where Mk = supx k(x, x) < ∞, E ≥ 0 describes the approximation errors and dimeff(H⊥) is the effective dimension of the RKHS H⊥. For an RKHS H with kernel k the effective dimension is defined by (cid:90) dimeff(H) = k(x, y)2 dµ(x) dµ(y).
X where X = supp µ. We return to this quantity at various points in the paper.
It is important to note that the use of the feature averaged predictor f ′ as a comparator is without loss of generality. Any other predictor f ′′ that has test risk not larger than f ′ would satisfy the above bound, simply because this means ∆(f ′, f ′′) ≥ 0 so ∆(f, f ′′) = ∆(f, f ′) + ∆(f ′, f ′′) ≥ ∆(f, f ′).1
Finally, for intuition, in Theorem 7 we specialise Theorem 5 to the linear setting and compute the bound exactly. Assumptions and technical conditions are given in Section 2 along with an outline of the ideas of Elesedy and Zaidi [8] on which we build.