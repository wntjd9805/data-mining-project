Abstract
Many concepts have been proposed for meta learning with neural networks (NNs), e.g., NNs that learn to reprogram fast weights, Hebbian plasticity, learned learning rules, and meta recurrent NNs. Our Variable Shared Meta Learning (VSML) uniﬁes the above and demonstrates that simple weight-sharing and sparsity in an NN is sufﬁcient to express powerful learning algorithms (LAs) in a reusable fashion.
A simple implementation of VSML where the weights of a neural network are replaced by tiny LSTMs allows for implementing the backpropagation LA solely by running in forward-mode. It can even meta learn new LAs that differ from online backpropagation and generalize to datasets outside of the meta training distribution without explicit gradient calculation. Introspection reveals that our meta learned
LAs learn through fast association in a way that is qualitatively different from gradient descent. 1

Introduction
The shift from standard machine learning to meta learning involves learning the learning algorithm (LA) itself, reducing the burden on the human designer to craft useful learning algorithms [43].
Recent meta learning has primarily focused on generalization from training tasks to similar test tasks, e.g., few-shot learning [11], or from training environments to similar test environments [17]. This contrasts human-engineered LAs that generalize across a wide range of datasets or environments.
Without such generalization, meta learned LAs can not entirely replace human-engineered variants.
Recent work demonstrated that meta learning can also successfully generate more general LAs that generalize across wide spectra of environments [20, 1, 31], e.g., from toy environments to
Mujoco and Atari. Unfortunately, however, many recent approaches still rely on a large number of human-designed and unmodiﬁable inner-loop components such as backpropagation.
Is it possible to implement modiﬁable versions of backpropagation or related algorithms as part of the end-to-end differentiable activation dynamics of a neural net (NN), instead of inserting them as separate ﬁxed routines? Here we propose the Variable Shared Meta Learning (VSML) principle for this purpose. It introduces a novel way of using sparsity and weight-sharing in NNs for meta learning. We build on the arguably simplest neural meta learner, the meta recurrent neural network (Meta RNN) [16, 10, 56], by replacing the weights of a neural network with tiny
LSTMs. The resulting system can be viewed as many RNNs passing messages to each other, or as one big RNN with a sparse shared weight matrix, or as a system learning each neuron’s functionality and its LA. VSML generalizes the principle behind end-to-end differentiable fast weight programmers [45, 46, 3, 41], hyper networks [14], learned learning rules [4, 13, 33], and hebbian-like synaptic plasticity [44, 46, 25, 26, 30]. Our mechanism, VSML, can implement backpropagation solely in the forward-dynamics of an RNN. Consequently, it enables meta-optimization of backprop-like algorithms. We envision a future where novel methods of credit assignment can be meta learned while still generalizing across vastly different tasks. This may lead to improvements in sample efﬁciency, memory efﬁciency, continual learning, and others. As a ﬁrst step, our system meta 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learns online LAs from scratch that frequently learn faster than gradient descent and generalize to datasets outside of the meta training distribution (e.g., from MNIST to Fashion MNIST). Our VSML
RNN is the ﬁrst neural meta learner without hard-coded backpropagation that shows such strong generalization. 2