Abstract
Combinatorial Optimization (CO) has been a long-standing challenging research topic featured by its NP-hard nature. Traditionally such problems are approximately solved with heuristic algorithms which are usually fast but may sacriﬁce the solution quality. Currently, machine learning for combinatorial optimization (MLCO) has become a trending research topic, but most existing MLCO methods treat CO as a single-level optimization by directly learning the end-to-end solutions, which are hard to scale up and mostly limited by the capacity of ML models given the high complexity of CO. In this paper, we propose a hybrid approach to combine the best of the two worlds, in which a bi-level framework is developed with an upper-level learning method to optimize the graph (e.g. add, delete or modify edges in a graph), fused with a lower-level heuristic algorithm solving on the optimized graph. Such a bi-level approach simpliﬁes the learning on the original hard CO and can effectively mitigate the demand for model capacity. The experiments and results on several popular CO problems like Directed Acyclic Graph scheduling,
Graph Edit Distance and Hamiltonian Cycle Problem show its effectiveness over manually designed heuristics and single-level learning methods. Code available at https://github.com/Thinklab-SJTU/PPO-BiHyb. 1

Introduction
Combinatorial Optimization (CO) is a family of long-standing optimization problems. A large portion of CO problems is NP-hard due to the combinatorial nature, raising challenges for traditional (exact) solvers on even medium-sized problems. Heuristic algorithms are often adopted to approximately solve CO problems within an acceptable time, and there is a growing trend adopting modern data-driven approaches to solve CO problems that achieve better and faster results [30].
The major line of works solving CO with machine learning (ML) is single-level [8, 25, 29, 30, 39, 41, 57, 59, 64], where the prediction of ML module lies in the solution space, assuming the model has enough capacity learning the input-output mapping of the CO problem. However, achieving such an assumption is non-trivial, leading to the following two aspects of challenges. On the one hand, it is challenging to design a model with enough capacity with limited computational resources, and existing models are usually tailored for speciﬁc problems which require heavy trail-and-error [25, 57, 59]. On the other hand, training such a heavy model requires either supervision from high-quality labels [31, 57, 60] which are infeasible to obtain for large-sized problems due to the NP-hard nature, or reinforcement learning (RL) [8, 30, 38, 39] which might be unstable due to the challenges of large action space and sparse reward especially for large-sized problems [52].
∗Part of the work was done while the ﬁrst author was working as an intern at Ant Group.
†Junchi Yan is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
An alternative approach resorts to a hybrid machine learning and traditional optimization pipeline [11, 18, 26, 31, 51, 60, 61] hoping to utilize the power of traditional optimization methods. However, designing a general hybrid MLCO approach is still non-trivial, as existing methods [11, 60] usually require domain-speciﬁc knowledge for the model design. It is again challenging to obtain high-quality supervision labels, and existing methods are based on either problem-speciﬁc surrogate labels [18, 31, 60], or learned with RL while the challenges of RL still exist [11, 51].
In this paper, we propose a general hybrid MLCO approach over graphs. We ﬁrst reduce the complexity of deep learning model by reformulating the original CO into a bi-level optimization, whose objective is to minimize the long-term upper-level objective by optimizing the graph structure, and the lower-level problem is handled by an existing heuristic. We resort to RL and the traditional heuristic can be absorbed as part of the environment, and it is shown that the sparse reward issue is mitigated for the resulting RL problem compared to previous RL-based methods [8, 30, 38, 39].
Speciﬁcally, our model is built with standard building blocks: the input graph is encoded by Graph
Convolutional Network (GCN) [32], and the actor and critic modules are based on ResNet blocks [22] and attention models [55]. All modules are learned with the Proximal Policy Optimization (PPO) algorithm [48]. The contributions of this paper include:
• To combine the best of the two worlds, we propose a general hybrid approach that integrates traditional heuristic solvers with machine learning algorithm.
• We propose a bi-level optimization formulation for learning to solve CO on graphs. The upper-level optimization adopts a reinforcement learning agent to adaptively modify the graphs, while the lower-level optimization involves traditional learning-free heuristics to solve combinatorial optimization tasks on the modiﬁed graphs. Our approach does not require ground truth labels.
• The experiments for graphs up to thousands of nodes on several popular tasks such as Directed
Acrylic Graph scheduling (DAG scheduling), Graph Edit Distance (GED), and Hamiltonian Cycle
Problem (HCP) show that our method notably surpasses both traditional learning-free heuristics and single-level learning method. Our method generalizes well to graphs of different sizes while having comparable overhead to the single-level learning methods. 2