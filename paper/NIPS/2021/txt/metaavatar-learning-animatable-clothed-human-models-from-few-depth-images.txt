Abstract
In this paper, we aim to create generalizable and controllable neural signed distance
ﬁelds (SDFs) that represent clothed humans from monocular depth observations.
Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from dif-ferent sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difﬁculty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computation-ally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs.
The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to ﬁne-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the ﬁrst to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. 1

Introduction
Representing clothed humans as neural implicit functions is a rising research topic in the computer vision community. Earlier works in this direction address geometric reconstruction of clothed humans from static monocular images [35, 36, 63, 64], RGBD videos [37, 38, 71, 78, 80] or sparse point clouds [12] as direct extensions of neural implicit functions for rigid objects [11, 45, 46, 52]. More recent works advocate to learn shapes in a canonical pose [7, 27, 75] in order to not only handle 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Given as few as 8 monocular depth images and their SMPL ﬁttings, our meta-learned model yields a controllable neural SDF in 2 minutes which synthesizes realistic cloth deformations for unseen body poses. Here we show results of two different subjects wearing different clothes. reconstruction, but also build controllable neural avatars from sensor inputs. However, these works do not model pose-dependent cloth deformation, limiting their realism.
On the other hand, traditional parametric human body models [41, 50, 54, 77] can represent pose-dependent soft tissue deformations of minimally-clothed human bodies. Several recent meth-ods [13, 48] proposed to learn neural implicit functions to approximate such parametric models from watertight meshes. However, they cannot be straightforwardly extended to model clothed humans.
SCANimate [65] proposed to learn canonicalized dynamic neural Signed Distance Fields (SDFs) controlled by human pose inputs and trained with Implicit Geometric Regularization (IGR [21]), thus circumventing the requirement of watertight meshes. However, SCANimate works only on dense full-body scans with accurate surface normals and further requires expensive per-subject/cloth-type training. These factors limit the applicability of SCANimate for building personalized human avatars from commodity RGBD sensors.
Contrary to all the aforementioned works, we propose to use meta-learning to effectively incorporate priors of dynamic neural SDFs of clothed humans, thus enabling fast ﬁne-tuning (few minutes) for generating new avatars given only a few monocular depth images of unseen clothed humans as inputs. More speciﬁcally, we build upon recently proposed ideas of meta-learned initialization for implicit representations [67, 72] to enable fast ﬁne-tuning. Similar to [67], we represent a speciﬁc category of objects (in our case, clothed human bodies in the canonical pose) with a neural implicit function and use meta-learning algorithms such as [16, 49] to learn a meta-model. However, unlike [67, 72], where the implicit functions are designed for static reconstruction, we target the generation of dynamic neural SDFs that are controllable by user-speciﬁed body poses. We observe that directly conditioning neural implicit functions (represented as a multi-layer perceptron) on body poses lacks the expressiveness to capture high-frequency details of diverse cloth types, and hence propose to meta-learn a hypernetwork [25] that predicts the parameters of the neural implicit function.
Overall, the proposed approach, which we name MetaAvatar, yields controllable neural SDFs with dynamic surfaces in minutes via fast ﬁne-tuning, given only a few depth observations of an unseen clothed human and the underlying SMPL [41] ﬁttings (Fig. 1) as inputs. Code and data are public at https://neuralbodies.github.io/metavatar/. 2