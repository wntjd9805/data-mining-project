Abstract
Variational autoencoders model high-dimensional data by positing low-dimensional latent variables that are mapped through a ﬂexible distribution parametrized by a neural network. Unfortunately, variational autoencoders often suffer from posterior collapse: the posterior of the latent variables is equal to its prior, rendering the variational autoencoder useless as a means to produce meaningful representations.
Existing approaches to posterior collapse often attribute it to the use of neural networks or optimization issues due to variational approximation. In this paper, we consider posterior collapse as a problem of latent variable non-identiﬁability.
We prove that the posterior collapses if and only if the latent variables are non-identiﬁable in the generative model. This fact implies that posterior collapse is not a phenomenon speciﬁc to the use of ﬂexible distributions or approximate inference. Rather, it can occur in classical probabilistic models even with exact inference, which we also demonstrate. Based on these results, we propose a class of latent-identiﬁable variational autoencoders, deep generative models which enforce identiﬁability without sacriﬁcing ﬂexibility. This model class resolves the problem of latent variable non-identiﬁability by leveraging bijective Brenier maps and parameterizing them with input convex neural networks, without special variational inference objectives or optimization tricks. Across synthetic and real datasets, latent-identiﬁable variational autoencoders outperform existing methods in mitigating posterior collapse and providing meaningful representations of the data. 1

Introduction
Variational autoencoders (VAE) are powerful generative models for high-dimensional data [28, 46].
Their key idea is to combine the inference principles of probabilistic modeling with the ﬂexibility of neural networks. In a VAE, each datapoint is independently generated by a low-dimensional latent variable drawn from a prior, then mapped to a ﬂexible distribution parametrized by a neural network.
Unfortunately, VAE often suffer from posterior collapse, an important and widely studied phe-nomenon where the posterior of the latent variables is equal to prior [6, 8, 38, 62]. This phenomenon is also known as latent variable collapse, KL vanishing, and over-pruning. Posterior collapse ren-ders the VAE useless to produce meaningful representations, in so much as its per-datapoint latent variables all have the exact same posterior.
Posterior collapse is commonly observed in the VAE whose generative model is highly ﬂexible, leading to the common speculation that posterior collapse occurs because VAE involve ﬂexible neural networks in the generative model [11], or because it uses variational inference [59]. Based on these hypotheses, many of the proposed strategies for mitigating posterior collapse thus focus on modifying the variational inference objective (e.g. [44]), designing special optimization schemes for variational inference in VAE (e.g. [18, 25, 32]), or limiting the capacity of the generative model (e.g. [6, 16, 60].) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we consider posterior collapse as a problem of latent variable non-identiﬁability.
We prove that posterior collapse occurs if and only if the latent variable is non-identiﬁable in the generative model, which loosely means the likelihood function does not depend on the latent variable [40, 42, 56]. Below, we formally establish this equivalence by appealing to recent results in
Bayesian non-identiﬁability [40, 42, 43, 49, 58].
More broadly, the relationship between posterior collapse and latent variable non-identiﬁability implies that posterior collapse is not a phenomenon speciﬁc to the use of neural networks or variational inference. Rather, it can also occur in classical probabilistic models ﬁtted with exact inference methods, such as Gaussian mixture models and probabilistic principal component analysis (PPCA).
This relationship also leads to a new perspective on existing methods for avoiding posterior collapse, such as the delta-VAE [44] or the Ø-VAE [19]. These methods heuristically adjust the approximate inference procedure embedded in the optimization of the model parameters. Though originally motivated by the goal of patching the variational objective, the results here suggest that these adjustments are useful because they help avoid parameters at which the latent variable is non-identiﬁable and, consequently, avoid posterior collapse.
The relationship between posterior collapse and non-identiﬁability points to a direct solution to the problem: we must make the latent variable identiﬁable. To this end, we propose latent-identiﬁable
VAE, a class of VAE that is as ﬂexible as classical VAE while also being identiﬁable. Latent-identiﬁable VAE resolves the latent variable non-identiﬁability by leveraging Brenier maps [36, 39] and parameterizing them with input-convex neural networks [2, 35]. Inference on identiﬁable VAE uses the standard variational inference objective, without special modiﬁcations or optimization tricks.
Across synthetic and real datasets, we show that identiﬁable VAE mitigates posterior collapse without sacriﬁcing ﬁdelity to the data.