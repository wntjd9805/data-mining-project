Abstract
Compressive phase retrieval is a popular variant of the standard compressive sensing problem in which the measurements only contain magnitude information.
In this paper, motivated by recent advances in deep generative models, we provide recovery guarantees with near-optimal sample complexity for phase retrieval with generative priors. We ﬁrst show that when using i.i.d. Gaussian measurements and an L-Lipschitz continuous generative model with bounded k-dimensional inputs, roughly O(k log L) samples sufﬁce to guarantee that any signal minimizing an amplitude-based empirical loss function is close to the true signal. Attaining this sample complexity with a practical algorithm remains a difﬁcult challenge, and
ﬁnding a good initialization for gradient-based methods has been observed to pose a major bottleneck. To partially address this, we further show that roughly O(k log L) samples ensure sufﬁcient closeness between the underlying signal and any globally optimal solution to an optimization problem designed for spectral initialization (though ﬁnding such a solution may still be challenging). We also adapt this result to sparse phase retrieval, and show that O(s log n) samples are sufﬁcient for a similar guarantee when the underlying signal is s-sparse and n-dimensional, matching an information-theoretic lower bound. While these guarantees do not directly correspond to a practical algorithm, we propose a practical spectral initialization method motivated by our ﬁndings, and experimentally observe performance gains over various existing spectral initialization methods for sparse phase retrieval. 1

Introduction
In this paper, we consider the (real-valued) phase retrieval problem, which aims to recover a signal x ∈ Rn from noisy magnitude-only measurements: yi = |(cid:104)ai, x(cid:105)| + ηi, (1) where ai ∈ Rn is the i-th sensing vector, and ηi represents additive noise. This problem arises naturally in areas such as diffraction imaging, X-ray crystallography, microscopy, optics and astron-omy [8], where it is often difﬁcult or even impossible to observe the linear measurements directly, and one can only record the magnitudes or intensities (squared magnitudes). i = 1, 2, . . . , m,
In many real applications, to reduce the required number of measurements, it is of interest to exploit structure in the signal being estimated. In particular, for applications related to signal processing and imaging, it is well-known that the underlying signal typically admits a sparse or approximately sparse 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
representation in some known basis [40]. Motivated by this, and considering the popularity of the standard compressive sensing (CS) problem [16], the sparse phase retrieval problem has attracted signiﬁcant research interest.
Moreover, inspired by the successful applications of deep generative models in many ﬁelds [15], recently, a new perspective of CS has emerged, for which the sparsity assumption is replaced by a generative model assumption. That is, instead of assuming sparsity, the signal is assumed to be close to the range of a generative model [5]. In addition to the theoretical developments in [5], the authors also provide impressive numerical results showing that for some imaging applications, using generative priors can signiﬁcantly reduce the required number of measurements (e.g., by a factor of 5 to 10) for recovering the signal up to a given accuracy. Follow-up works of [5] include [58, 22, 45, 67, 12, 29], just to name a few.
In this paper, we focus on providing recovery guarantees with near-optimal sample complexity bounds (e.g., optimal up to constant factors) for the compressive phase retrieval problem, considering both sparse and generative priors. 1.1