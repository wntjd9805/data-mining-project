Abstract
Neural module networks (NMN) are a popular approach for solving multi-modal tasks such as visual question answering (VQA) and visual referring expression recognition (REF). A key limitation in prior implementations of NMN is that the neural modules do not effectively capture the association between the visual input and the relevant neighbourhood context of the textual input. This limits their generalizability. For instance, NMN fail to understand new concepts such as “yellow sphere to the left" even when it is a combination of known concepts from train data: “blue sphere", “yellow cube", and “metallic cube to the left". In this paper, we address this limitation by introducing a language-guided adaptive convolution layer (LG-Conv) into NMN, in which the ﬁlter weights of convolutions are explicitly multiplied with a spatially varying language-guided kernel. Our model allows the neural module to adaptively co-attend over potential objects of interest from the visual and textual inputs. Extensive experiments on VQA and
REF tasks demonstrate the effectiveness of our approach. Additionally, we propose a new challenging out-of-distribution test split for REF task, which we call C3-Ref+, for explicitly evaluating the NMN’s ability to generalize well to adversarial perturbations and unseen combinations of known concepts. Experiments on C3-Ref+ further demonstrate the generalization capabilities of our approach. 1

Introduction
Visual question answering (VQA) [11, 8, 7] and visual referring expression recognition (REF) [37, 27] are fundamental language-to-vision matching tasks that have several downstream applications such as robot navigation, image retrieval, and natural language interfaces [42, 50, 4, 3, 39]. The high-In the level goal of these tasks is to perform joint reasoning over visual and textual queries. recent years, neural module networks (NMN) [10, 20, 33] attracted increasing attention due to their superior performance on these tasks [33, 26]. Brieﬂy, NMN models learn to parse textual queries as executable programs composed of learnable neural modules. Each of these modules implements a single step of reasoning (e.g. count, filter, compare) and are dynamically assembled to perform multi-step reasoning over text. In addition to the good performance, NMN also provide high model interpretability thanks to their transparent, hierarchical and semantically motivated architecture [45, 5, 6, 32].
Despite great success, the current NMN implementations require a large amount of training data and are less effective in generalizing to unseen but known language constructs [29, 13, 51]. For example,
NMN fail to understand new concepts such as “yellow sphere to the left" that are constructed using a combinations of known concepts from train data such as “blue sphere", “yellow cube", and “metallic 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: An example from the CLEVR-Ref+ dataset. Existing NMN implementations only provide the visual features (vm) as inputs to the neural modules. In this work, we additionally condition each module on textual expression (q) by replacing the standard convolution layers with content adaptive convolution layers LG-Conv which modify the convolution by explicitly multiplying the
ﬁlter weights (W ) with a spatially varying language-guided kernel G. (cid:78) denotes element-wise multiplication and (cid:76) denotes summation. cube to the left". One of the main reasons for this is that the neural modules in existing works either use a shallow, indirect language guidance [40, 19, 2] or pre-deﬁne the textual inputs in the module instantiation [26, 33], ignoring the rich correlations among the visual inputs and the relevant context from the textual inputs. For example, the neural module that ﬁlters based on the object size,
“filter_size(smallest)”, needs to localize a tiny sphere or a medium-sized sphere in the image depending on the object relationships in the expression (e.g. “the smallest thing among the spheres" vs. “the metallic sphere smaller than all the large cylinders") and the different sizes of spheres and cylinders available in its visual input. We believe that explicitly conditioning the neural modules on the joint textual and visual context helps in inferring robust visiolinguistic relationships which further enhances the compositional reasoning skills.
In this work, we address the aforementioned issues by explicitly providing the relevant objects and relationships in the textual expression to neural modules. To do this, as shown in Figure 1, we replace the standard convolution operations in the neural modules with a novel language-guided adaptive convolution operation, which we call LG-Conv. More speciﬁcally, the ﬁlter weights W of
LG-Conv are explicitly multiplied with a spatially varying language-guided kernel G, which allows the module to adaptively co-attend over potential objects of interest from the visual input and textual input by altering the convolution. Although content-adaptive convolutions [24, 15, 44] are used in several vision tasks, we are not aware of any prior works that does this ﬁlter adaptation using language as guidance. We propose two novel and effective methods namely, bi-salient attentional guidance (BiSAtt) network and co-salient attentional guidance guidance (CoSAtt) network to learn the guidance kernel G from textual and visual inputs.
We conduct extensive experiments on VQA and REF tasks using CLEVR [25], CLOSURE [13], and CLEVR-Ref+ [33] datasets. On the recently released VQA benchmark CLOSURE [13], our approach signiﬁcantly outperforms all the previous works with 11.6% improvements in accuracy. On the REF benchmark CLEVR-Ref+ [33], we outperform competing approaches by as much as +9.8% accuracy on single-referent split (S-Ref) and +4.7% on full-referent split (F-Ref), suggesting the importance of language-guidance. Most signiﬁcant gains with S-Ref, which consists of only 30% of the training data in CLEVR-Ref+, demonstrate the superior generalization of our model in learning 2
Figure 2: (a) Architecture of neural module (m) in existing NMN [26] consuming a visual input vm; (b) Our proposed architecture replacing Conv layers with content adaptive convolution layers guided by the input image I, input query q and parameterized textual input marg. from fewer training samples1. We further evaluate the generalization capabilities of our approach by collecting a new dataset consisting of unseen compositions and contrasting samples for CLEVR-Ref+ benchmark [17], and call our new dataset C3-Ref+.
Our key contributions are summarized as follows: 1. We propose a novel language-guided adaptive convolution layer for NMN that guide modules in adaptively selecting informative visiolinguistic relationships and in attending to relevant objects of interest from the visual and textual inputs; 2. We demonstrate the superiority of our approach by achieving new state-of-the-art results on multiple tasks and benchmarks; 3. We introduce a new benchmark to explicitly test the model’s ability to generalize to adver-sarial perturbations and novel compositions of concepts unseen during training. We show that our model is more robust and generalizable compared to previous approaches. 2