Abstract
We study k-median clustering under the sequential no-substitution setting. In this setting, a data stream is sequentially observed, and some of the points are selected by the algorithm as cluster centers. However, a point can be selected as a center only immediately after it is observed, before observing the next point. In addition, a selected center cannot be substituted later. We give the ﬁrst algorithm for this setting that obtains a constant approximation factor on the optimal cost under a random arrival order, an exponential improvement over previous work. This is also the ﬁrst constant approximation guarantee that holds without any structural assumptions on the input data. Moreover, the number of selected centers is only quasi-linear in k. Our algorithm and analysis are based on a careful cost estimation that avoids outliers, a new concept of a linear bin division, and a multiscale approach to center selection. 1

Introduction
Clustering is a fundamental unsupervised learning task used for various applications, such as anomaly detection [Leung and Leckie, 2005], recommender systems [Shepitsen et al., 2008] and cancer diagnosis [Zheng et al., 2014]. In recent years, the problem of sequential clustering has been actively studied, motivated by applications in which data arrives sequentially, such as online recommender systems [Nasraoui et al., 2007] and online community detection [Aggarwal, 2003].
In this work, we study k-median clustering in the sequential no-substitution setting, a term ﬁrst introduced in Hess and Sabato [2020]. In this setting, a stream of data points is sequentially observed, and some of these points are selected by the algorithm as cluster centers. However, a point can be selected as a center only immediately after it is observed, before observing the next point. In addition, a selected center cannot be substituted later. This setting is motivated by applications in which center selection is mapped to a real-world irreversible action. For instance, consider a stream of website users, where the goal is to instantaneously identify users that serve as social cluster centers and provide them with a promotional gift while they are still on the website. As another example, consider recruiting representative participants for a clinical trial out of a stream of incoming patients.
∗This work was done while the author was at the University of California San Diego. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The goal in the no-substitution k-median setting is to obtain a near-optimal k-median cost value, while selecting a number of centers that is as close as possible to k. For an adversarially ordered stream, it has been shown [Moshkovitz, 2021] that an algorithm that selects a number of centers that is sublinear in the stream length cannot obtain a constant approximation guarantee, unless some structural assumptions are imposed on the input data.
In this work, we show that in contrast, when the stream order is random, a constant approximation guarantee can be obtained without imposing structural assumptions on the data. Previous works on the random-order setting provide either an algorithm with an almost-constant approximation assuming a bounded metric space [Hess and Sabato, 2020] or an algorithm with an approximation factor that is exponential in k [Moshkovitz, 2021]. Thus, our guarantees are stronger and more general than the previous guarantees. Moreover, the number of centers selected by our algorithm is only quasi-linear in k and does not depend on the stream length.
Main result. We propose a new algorithm, MUNSC (Multiscale No-Substitution Clustering), that obtains a constant approximation factor with probability 1 − δ over the random order of the stream, while selecting only O(k log2(k/δ)) centers. MUNSC operates by executing several instances of a new center-selection procedure, called SelectProc, where each instance is applied to a stream preﬁx of a different scale. SelectProc decides which centers to select using a novel technique, in which a small stream preﬁx is used to estimate a truncated version of the optimal cost of the entire stream. This truncated version ignores outliers, and so can be estimated reliably. The value of the estimate is used to determine which centers to select from the rest of the stream. The multiscale use of SelectProc by MUNSC allows MUNSC to select only a quasi-linear number of centers.
As part of our analysis, we propose a new concept of a linear bin division, which allows a high-probability association of the costs of stream subsets, while selecting a number of centers that is independent of the stream length. This improves a previous construction of Meyerson et al. [2004].
The guarantees for MUNSC are provided in our main theorem, Theorem 4.1, stated in Section 4. Table 1 below compares our guarantees to previous works; See Section 2 for additional details. We note that our algorithm and guarantees are easily extendable to k-means clustering, as well as to any other clustering objective that satisﬁes the weak triangle inequality. MUNSC uses as a black box an ofﬂine k-median approximation algorithm with a constant approximation factor. Whenever the ofﬂine algorithm is efﬁcient [e.g., Charikar et al., 2002, Li and Svensson, 2016], then so is MUNSC.
Limitations. The proposed algorithm is the ﬁrst constant-approximation no-substitution clustering algorithm that makes no structural assumptions on the input data set. The only limitation is the assumption that the input order is random. This assumption is useful in cases where there is no adversary and any order is as likely, as in the example applications mentioned above. Moreover, a random order is a standard assumption in many streaming settings, satisﬁed also whenever the input stream is drawn i.i.d. from a distribution. In some cases, the randomness assumption might hold only approximately. In these cases, we expect a graceful degradation of the guarantees. We leave for future work a comprehensive treatment of input orders that are neither random nor adversarial. We expect the techniques of the current work to serve as a cornerstone in such a treatment.
Paper structure. We discuss related work in Section 2. The formal setting and notations are deﬁned in Section 3. The algorithm and its guarantees are given in Section 4. In Section 5, we outline the proof of the main result. Some parts of the proof are deferred to appendices, which can be found in the supplementary material. We conclude with a discussion in Section 6. 2