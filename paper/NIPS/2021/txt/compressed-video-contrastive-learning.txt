Abstract
This work concerns self-supervised video representation learning (SSVRL), one topic that has received much attention recently. Since videos are storage-intensive and contain a rich source of visual content, models designed for SSVRL are expected to be storage- and computation-efﬁcient, as well as effective. However, most existing methods only focus on one of the two objectives, failing to consider both at the same time. In this work, for the ﬁrst time, the seemingly contradictory goals are simultaneously achieved by exploiting compressed videos and capturing mutual information between two input streams. Speciﬁcally, a novel Motion Vector based Cross Guidance Contrastive learning approach (MVCGC) is proposed. For storage and computation efﬁciency, we choose to directly decode RGB frames and motion vectors (that resemble low-resolution optical ﬂows) from compressed videos on-the-ﬂy. To enhance the representation ability of the motion vectors, hence the effectiveness of our method, we design a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss, where motion vectors can take supervision signals from RGB frames and vice versa. Comprehensive experiments on two downstream tasks show that our MVCGC yields new state-of-the-art while being signiﬁcantly more efﬁcient than its competitors. 1

Introduction
Recent self-supervised image representation learning approaches [He et al., 2020; Chen et al., 2020] have been reported to outperform supervised ones on a wide range of downstream tasks by (1) leveraging a large amount of unlabeled data available online for pre-training, and (2) designing a powerful algorithm to discriminate unlabeled samples with different semantic meanings. However, the situation for self-supervised video representation learning (SSVRL) is somewhat different: self-supervised methods still perform worse than supervised ones, since videos are extremely storage-intensive (scalability thus becomes a serious issue) and have a richer source of visual content (SSVRL thus becomes very difﬁcult). Therefore, designing storage-and-computation-efﬁcient as well as effective models for SSVRL remains a challenging problem that has not been well studied.
Existing state-of-the-art methods [Han et al., 2020b; Tao et al., 2020; Huo et al., 2021] mainly focus on designing effective algorithms without considering the storage and computation costs during large-scale video self-supervised training. Particularly, in order to leverage both appearance and temporal information in the video, they exploit the optical ﬂow as an extra view to complement the
RGB stream. However, this procedure is both storage- and computation-intensive because of storing the decoded frames and computing optical ﬂows, respectively (e.g., it costs more than 100GB for 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Processing time per-video (i.e., data pre-processing and inference) and the accuracies for different methods on UCF101. Each method is represented by a circle, whose size represents the storage size occupied by the test data. MVCGC achieves the highest accuracy under a signiﬁcantly reduced storage budget compared to its counterparts. More details can be found in Table 1. frame storage and a number of days for computing optical ﬂows [Han et al., 2020b] on the UCF101 dataset [Soomro et al., 2012]). This clearly hinders large-scale video self-supervised training.
Considering that videos are already stored in compressed formats to reduce storage requirements, a natural choice is to resort to on-the-ﬂy decoding, i.e., decoding frames from compressed videos during training/inference without introducing any extra storage. Moreover, since motion vectors encoded in compressed videos resemble low-resolution optical ﬂows in describing local motions, they can be used to avoid the costly optical ﬂow computation. Consequently, compressed videos inherently contain both static and motion information that is suitable for video representation learning. Recently, a pretext task based method called IMRNet [Yu et al., 2021] leverages the standard process in CoViAR [Wu et al., 2018] to decode compressed videos, but inevitably encounters the inefﬁciency and ineffectiveness problems: (1) The outdated CoViAR is not truly on-the-ﬂy, i.e., it re-encodes videos and stores them before decoding, which is storage and computation inefﬁcient. (2) Motion vectors are less discriminative and thus weaker than optical ﬂows, resulting in sub-optimal performance compared with state-of-the-art methods using optical ﬂows, as shown in Figure 1. As a result, exploiting the compressed videos efﬁciently, followed by learning discriminative representations, is still challenging.
Motivated by the above observations, we propose a Motion Vector based Cross Guidance Contrastive learning approach (MVCGC), which has several appealing beneﬁts: (1) It is able to decode RGB frames and motion vectors from compressed videos with various codecs. The storage and computation budgets are alleviated since videos can be decoded without re-encoding. (2) Our MVCGC can learn discriminative features from both RGB and motion vector streams. This is achieved by designing a cross guidance contrastive learning algorithm based on multi-instance InfoNCE loss: each sample pair is constructed by one RGB clip and one motion vector clip, and multiple positive samples can be mined by calculating the similarity in two views. Therefore, the two views can take supervision signals from each other to improve the representation quality of both streams, especially for the motion vector stream (whose learned representations are even comparable to optical ﬂow features).
Our main contributions are three-fold: (1) We propose an efﬁcient and effective framework called
MVCGC that can learn representations from compressed videos directly. To the best of our knowledge, we are the ﬁrst to exploit contrastive loss in compressed video self-supervised learning. (2) We design a novel contrastive learning algorithm to capture mutual information between RGB frames and motion vectors from compressed videos. Importantly, we even make the learned features of motion vectors as representative as those of optical ﬂows. (3) Extensive experiments by applying the learned video representations on two downstream tasks (i.e., action recognition and action retrieval) across different benchmarks demonstrate that our MVCGC achieves state-of-the-art performance while being more efﬁcient than its counterparts (see Figure 1). 2
2