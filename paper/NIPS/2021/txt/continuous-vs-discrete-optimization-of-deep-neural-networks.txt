Abstract
Existing analyses of optimization in deep learning are either continuous, focusing on (variants of) gradient flow, or discrete, directly treating (variants of) gradient descent.
Gradient flow is amenable to theoretical analysis, but is stylized and disregards computational efficiency. The extent to which it represents gradient descent is an open question in the theory of deep learning. The current paper studies this question.
Viewing gradient descent as an approximate numerical solution to the initial value problem of gradient flow, we find that the degree of approximation depends on the curvature around the gradient flow trajectory. We then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows us to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple deep neural networks, gradient descent with conventional step size is indeed close to gradient flow. We hypothesize that the theory of gradient flows will unravel mysteries behind deep learning.1 1

Introduction
The success of deep neural networks is fueled by the mysterious properties of gradient-based optimization, namely, the ability of (variants of) gradient descent to minimize non-convex training objectives while exhibiting tendency towards solutions that generalize well. Vast efforts are being directed at mathematically analyzing this phenomenon, with existing results typically falling into one of two categories: continuous or discrete. Continuous analyses usually focus on gradient flow (or variants thereof), which corresponds to gradient descent (or variants thereof) with infinitesimally small step size. Compared to their discrete (positive step size) counterparts, continuous settings are oftentimes far more amenable to theoretical analysis (e.g. they admit use of the theory of differential equations), but on the other hand are stylized, and disregard the critical aspect of computational efficiency (number of steps required for convergence). Works analyzing gradient flow over deep neural networks either accept the latter shortcomings (see for example [49, 4, 46]), or attempt to reproduce part of the results via completely separate analysis of gradient descent (cf. [30, 18, 5]). The extent to which gradient flow represents gradient descent is an open question in the theory of deep learning.
The current paper formally studies the foregoing question. Viewing gradient descent as a numerical method for approximately solving the initial value problem corresponding to gradient flow, we turn to the literature on numerical analysis, and invoke a fundamental theorem concerning the approximation error. The theorem implies that in general, the match between gradient descent and gradient flow is deter-mined by the curvature around gradient flow’s trajectory. In particular, the “more convex” the trajectory, i.e. the larger the (possibly negative) minimal eigenvalue of the Hessian is around the trajectory, the bet-1Due to lack of space, essential portions of this paper were deferred to supplementary material. We refer the reader to [21] for a self-contained version of the text. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
ter the match is guaranteed to be.2 We show that when applied to deep neural networks (fully connected as well as convolutional) with homogeneous activations (e.g. linear, rectified linear or leaky rectified lin-ear), gradient flow emanating from near-zero initialization (as commonly employed in practice) follows trajectories that are “roughly convex,” in the sense that the minimal eigenvalue of the Hessian along them is far greater than in arbitrary points in space, particularly towards convergence. This implies that over deep neural networks, gradient descent with moderately small step size may in fact be close to its contin-uous limit, i.e. to gradient flow. We exemplify an application of this finding by translating an analysis of gradient flow over deep linear neural networks into a convergence guarantee for gradient descent. The guarantee we obtain is, to our knowledge, the first to ensure that a conventional gradient-based algorithm optimizing a deep (three or more layer) neural network of fixed (data-independent3) size efficiently converges4 to global minimum almost surely under random (data-independent) near-zero initialization.
We corroborate our theoretical analysis through experiments with basic deep learning settings, which demonstrate that reducing the step size of gradient descent often leads to only slight changes in its trajectory. This confirms that, in basic settings, central aspects of deep neural network optimization may indeed be captured by gradient flow. Recent works (e.g. [8, 33, 53]) suggest that by appropriately modifying gradient flow it is possible to account for advanced settings as well, including ones with momentum, stochasticity and large step size. Encouraged by these developments, we hypothesize that the vast bodies of knowledge on continuous dynamical systems, and gradient flow in particular (see, e.g., [23, 3]), will pave way to unraveling mysteries behind deep learning. 1.1 Contributions
The main contributions of this work are: (i) we conduct the first formal study for the discrepancy between continuous and discrete optimization of deep neural networks; (ii) we demonstrate the use of generic mathematical machinery for translating a continuous non-convex convergence result into a discrete one; (iii) to our knowledge, the discrete result we obtain forms the first guarantee of random (data-independent) near-zero initialization almost surely leading a conventional gradient-based algorithm optimizing a deep (three or more layer) neural network of fixed (data-independent) size to efficiently con-verge to global minimum; (iv) the fundamental theorem (from numerical analysis) we employ is seldom used in machine learning contexts and may be of independent interest; and (v) we provide empirical evi-dence suggesting that gradient descent over simple deep neural networks is often close to gradient flow. 2 Preliminaries: Numerical Solution of Initial Value Problems
Let d ∈ N. Given a function g : [0,∞)×Rd → Rd (viewed as a time-dependent vector field) and a point
θs ∈ Rd, consider the initial value problem:
θ(0) = θs
, d dt θ(t) = g(t,θ(t)) for t ≥ 0 . (1)
The following result — an extension of the well known Picard-Lindelöf Theorem — establishes that local Lipschitz continuity of g(·) suffices for ensuring existence and uniqueness of a solution θ(·).
Theorem 1 (Existence-Uniqueness). Consider the initial value problem in Equation (1), and suppose g(·) is locally Lipschitz continuous. Then, there exists a solution θ : [0, te) → Rd, where either: (i) te = ∞; or (ii) te < ∞ and limt↗te ∥θ(t)∥2 = ∞. Moreover, the solution is unique in the sense that any other solution θ′ : [0,t′
Proof. The theorem is a direct consequence of the results in Section 1.5 of [25].5
It is typically the case that the solution to Equation (1) cannot be expressed in closed form, and a numerical approximation is sought after. Various numerical methods for approximately solving initial value problems have been developed over the years (see Chapter 12 in [55] for an introduction). The most basic one, Euler’s method, is parameterized by a step size η > 0, and when applied to Equation (1) follows the recursive scheme: e) → Rd must satisfy t′ e ≤ te and ∀t ∈ [0,t′ e) : θ′(t) = θ(t). 2In addition to the minimal eigenvalue of the Hessian, local smoothness and Lipschitz constants also affect the guaranteed match between gradient descent and gradient flow. However, the impact of these constants is exponentially weaker than that of the Hessian’s minimal eigenvalue. For details see Theorem 3. 3By data-independence we mean that no assumptions on training data are made beyond it being subject to standard whitening and normalization procedures. 4We regard convergence as efficient if its computational complexity is polynomial in training set size and dimensions, as well as the desired level of accuracy. 5A minor subtlety is that in [25] the vector field g(·) is defined over an open domain. To account for this requirement, simply extend g(·) to the domain (−∞,∞)×Rd by setting g(t,q) = g(0,q) for all t < 0, q ∈ Rd. 2
θk+1 = θk +ηg(tk,θk) for k = 0,1,2,... , (2) where tk := kη and the initial point θ0 is typically set to θs. The motivation behind Euler’s method is straightforward — a first order Taylor expansion of the exact solution θ(·) around time tk yields: θ(tk+1) = θ(tk + η) ≈ θ(tk) + η d dt θ(tk) = θ(tk) + ηg(tk,θ(tk)), therefore if θ(tk) is well approximated by θk, we may expect θk+1 to resemble θ(tk+1). The numerical solution produced by Euler’s method may be viewed as a continuous polygonal curve:
¯θ(0) = θ0
¯θ(t) = g(tk,θk) for t ∈ (tk,tk+1) , k = 0,1,2,... .
¯θ : [0,∞) → Rd (3)
,
, d dt
The quality of the numerical solution then boils down to the distance between this curve and the exact solution, i.e. between ¯θ(t) and θ(t) for t ≥ 0. Many efforts have been made to derive tight bounds for this distance. We provide below a modern result known as “Fundamental Theorem.”
Theorem 2 (Fundamental Theorem). Consider the initial value problem in Equation (1), and suppose g(·) is continuously differentiable. Let θ : [0,te) → Rd be the solution to this problem (see
Theorem 1), and let ¯θ : [0,∞) → Rd be a continuous polygonal curve (Equation (3)) born from Euler’s method (Equation (2)). For any t ∈ [0,te),q ∈ Rd, denote by J(t,q) ∈ Rd,d the Jacobian of g(·) with respect to its second argument at the point (t,q), and by λmax(t,q) the maximal eigenvalue 2 (J(t,q)+J(t,q)⊤).6 Let m : [0,te) → R be an integrable function satisfying: λmax(t,q) ≤ m(t) of 1 for all t ∈ [0,te) and q ∈ [θ(t),¯θ(t)], where [θ(t),¯θ(t)] stands for the line segment (in Rd) between θ(t) and ¯θ(t). Let δ : [0,te) → R≥0 be an integrable function that meets: ∥ d
¯θ(t+)−g(t,¯θ(t))∥2 ≤ δ(t) for dt all t ∈ [0,te), where d dt
¯θ(t+) represents the right derivative of ¯θ(·) at time t. Then, for all t ∈ [0,te):
∥θ(t)− ¯θ(t)∥2 ≤ eµ(t)(cid:16)
∥θ(0)− ¯θ(0)∥2 +∫ t 0e−µ(t′)δ(t′)dt′(cid:17)
, (4) where µ(t) := (cid:82) t 0 m(t′)dt′.
Proof. The theorem is simply a restatement of Theorem 10.6 in [27]. 3 Continuous vs. Discrete Optimization: Match Determined by Convexity
Let f : Rd → R, where d ∈ N, be a twice continuously differentiable function which we would like to minimize. Consider continuous optimization via gradient flow initialized at θs ∈ Rd: d dt θ(t) = −∇f (θ(t)) for t ≥ 0 . (5)
This is a special case of the initial value problem presented in Equation (1).7 By Theorem 1, it admits a unique solution θ : [0,te)→Rd, where either: (i) te =∞; or (ii) te <∞ and limt↗te ∥θ(t)∥2 =∞.
Numerically approximating this solution via Euler’s method (Equation (2)) yields a discrete optimization algorithm which is no other than gradient descent:
θ(0) = θs
,
θk+1 = θk −η∇f (θk) for k = 0,1,2,... , (6)
∥∇2f (q)∥spectral ≤ β˜t,ϵ and supq∈D˜t,ϵ where η > 0 is the chosen step size. We may thus invoke the Fundamental Theorem (Theorem 2) and obtain a bound on the distance between the trajectories of gradient flow and gradient descent.
Theorem 3. Consider the trajectory of gradient flow (solution to Equation (5)) θ : [0, te) → Rd, and let ˜t ∈ (0, te) and ϵ > 0. Define D˜t,ϵ := (cid:83) t∈[0,˜t ] Bϵ(θ(t)), where Bϵ(θ(t))⊂Rd stands for the (closed) Euclidean ball of radius ϵ centered at θ(t). Let β˜t,ϵ,γ˜t,ϵ > 0 be such that:
∥∇f (q)∥2 ≤ γ˜t,ϵ. Let m: [0,˜t ] → R be an integrable supq∈D˜t,ϵ (cid:0)∇2f (q)(cid:1) function satisfying: −λmin stands for the minimal eigenvalue of ∇2f (q). Then, if the step size η > 0 chosen for gradient descent (Equation (6)) satisfies: (cid:82) t 0 m(t′)dt′
∥θ0 −θ(0)∥2 (cid:82) t (cid:82) t t′ m(t′′)dt′′dt′ 0 e the first ⌊˜t/η⌋ iterates of gradient descent will ϵ-approximate the trajectory of gradient flow up to time ˜t, i.e. we will have ∥θk −θ(kη)∥2 ≤ ϵ for all k ∈ {1,2,...,⌊˜t/η⌋}.
Proof sketch (for complete proof see Subappendix J.2). The result
Fundamental Theorem (Theorem 2) with δ(·) fixed at β˜t,ϵγ˜t,ϵη. (cid:0)∇2f (q)(cid:1) ≤ m(t) for all t ∈ [0,˜t ] and q ∈ Bϵ(θ(t)), where λmin
ϵ−e
β˜t,ϵγ˜t,ϵ from applying the
η < inf follows t∈(0,˜t ] (7)
, 6This maximal eigenvalue is known as the logarithmic norm of J(t,q) (cf. Section I.10 in [27]). 7The vector field in this case is time-independent (given by g(t,q) = −∇f (q) for all t ∈ [0,∞),q ∈ Rd).
Initial value problems of this type are known as autonomous. 3
Theorem 3 gives a sufficient condition — upper bound on step size η (Equation (7)) — for gradient descent to follow gradient flow up to a given time ˜t. The bound is inversely proportional to smoothness and Lipschitz constants (β˜t,ϵ and γ˜t,ϵ respectively), and more importantly, depends exponentially on the integral of m(·) along the gradient flow trajectory, where m(·) corresponds to minus the minimal eigenvalue of the Hessian. The smaller the integral of m(·), i.e. the larger (less negative or more positive) the minimal eigenvalue of the Hessian around the trajectory is, the more relaxed the bound will be. That is, the “more convex” the objective function is around the gradient flow trajectory, the better the match between gradient flow and gradient descent is guaranteed to be.
Corollary 1 below coarsely applies Theorem 3 by fixing m(·) to minus the minimal eigenvalue of the Hessian across the entire space. If m(·) ≡ m (now a constant) is negative, i.e. the objective function f (·) is strongly convex, the upper bound on the step size η becomes constant, meaning it is independent of the time ˜t until which gradient descent is required to follow gradient flow. If m is equal to zero, i.e. f (·) is non-strongly convex, the upper bound on η mildly decreases with ˜t, namely it scales as 1/˜t. If on the other hand m is positive, meaning f (·) is non-convex, the bound on η shrinks to zero (becoming prohibitively restrictive) exponentially fast as ˜t grows. This suggests that as opposed to (strongly or non-strongly) convex objectives, over which gradient descent can easily be made to follow gradient flow, over non-convex objectives, in the worst case, gradient descent will immediately divert from gradient flow unless its step size is exponentially small. In Appendix B we present a simple example of such a worst case scenario. In this worst case, the minimal eigenvalue of the Hessian is bounded below and away from zero around the gradient flow trajectory. A question is then whether there are non-convex objectives in which the minimal eigenvalue of the Hessian around gradient flow trajectories is large enough for them to be followed by gradient descent. We will see that training losses of deep neural networks can meet this property.
Corollary 1. Assume that the objective function f (·) is non-negative and β-smooth with β > 0.8
Denote m := −inf q∈Rd λmin(∇2f (q)), where λmin(∇2f (q)) stands for the minimal eigenvalue of
∇2f (q). Consider the trajectory of gradient flow (solution to Equation (5)) θ : [0,te)→Rd,9 and let
˜t ∈ (0,te) and ϵ > 0. Then, if the step size η > 0 for gradient descent (Equation (6)) satisfies:



η < c(ϵ−∥θ0 −θ(0)∥2)|m| c(ϵ−∥θ0 −θ(0)∥2)(1/˜t )
,if m = 0 c(ϵ−∥θ0 −θ(0)∥2em˜t)(em˜t −1)−1m ,if m > 0
,if m < 0 (strong convexity) (non-strong convexity)
, (non-convexity) where c := (cid:0)(cid:112)2β3f (θ(0))+β2ϵ(cid:1)−1
Proof sketch (for complete proof see Subappendix J.3). The result follows from applying Theorem 3 with β˜t,ϵ = β, γ˜t,ϵ = (cid:112)2βf (θ(0))+βϵ and m(·) ≡ m.
, we will have ∥θk −θ(kη)∥2 ≤ ϵ for all k ∈ {1,2,...,⌊˜t/η⌋}. 4 Optimization of Deep Neural Networks is Roughly Convex
Section 3 has shown that the extent to which gradient descent matches gradient flow depends on “how convex” the objective function is around the gradient flow trajectory. More precisely, the larger (less negative or more positive) the minimal eigenvalue of the Hessian is around this trajectory, the longer gradient descent (with given step size) is guaranteed to follow it.2 In this section we establish that over training losses of deep neural networks (fully connected as well as convolutional) with homogeneous activations (e.g. linear, rectified linear or leaky rectified linear), when emanating from near-zero initialization (as commonly employed in practice), trajectories of gradient flow are “roughly convex,” in the sense that the minimal eigenvalue of the Hessian along them is far greater than in arbitrary points in space, particularly towards convergence. This finding suggests that when optimizing deep neural networks, gradient descent may closely resemble gradient flow. We demonstrate a formal application of the finding in Section 5, translating an analysis of gradient flow over deep linear neural networks into a guarantee of efficient convergence (to global minimum) for gradient descent, which applies almost surely with respect to a random near-zero initialization. 8Namely, ∥∇2f (q)∥spectral ≤ β for all q ∈ Rd. 9Lemma 3 in Appendix A shows that in the current context (β-smoothness of the objective function f (·)), it necessarily holds that te = ∞, i.e. the trajectory of gradient flow is defined over [0,∞). For simplicity, the statement of the corollary does not rely on this fact. 4
4.1 Fully Connected Architectures
Consider the mappings realized by a fully connected neural network with depth n ∈ N≥2, input dimension d0 ∈ N, hidden widths d1,d2,...,dn−1 ∈ N, and output dimension dn ∈ N: hθ : Rd0 → Rdn , hθ(x) = Wnσ(Wn−1σ(Wn−2···σ(W1x))···) , (8) where: Wj ∈ Rdj ,dj−1, j = 1,2,...,n, are learned weight matrices; θ ∈ Rd, with d := (cid:80)n j=1djdj−1, is their arrangement as a vector;10 and σ : R → R is a predetermined activation function that operates element-wise when applied to a vector.11 We assume that σ(·) is (positively) homogeneous, meaning
σ(cz) = cσ(z) for all c ≥ 0,z ∈ R. This allows for linear (σ(z) = z), as well as the commonly employed rectified linear (σ(z) = max{z,0}) and leaky rectified linear (σ(z) = max{z,¯αz} for some 0 < ¯α < 1) activations.
Let Y be a set of possible labels, and let S = ((xi,yi))|S| i=1, with xi ∈ Rd0,yi ∈ Y for i = 1,2,...,|S|, be a sequence of labeled inputs. Given a loss function ℓ : Rdn ×Y → R convex and twice continuously differ-entiable in its first argument (common choices include square, logistic and exponential losses), we learn the weights of the neural network by minimizing its training loss — average loss over elements of S: 1
|S|
Subsubsections 4.1.1 and 4.1.2 below show (for linear and non-linear activation functions, respectively) that although the minimal eigenvalue of ∇2f (θ) (Hessian of training loss) — denoted λmin(∇2f (θ)) — can in general be arbitrarily negative, along trajectories of gradient flow (which emanate from near-zero initialization) it is no less than moderately negative, approaching non-negativity towards convergence.
In light of Section 3, this suggests that over fully connected deep neural networks, gradient flow may lend itself to approximation by gradient descent — a prospect we confirm (for a case with linear activation) in Section 5. f : Rd → R , f (θ) =
ℓ(hθ(xi),yi) . (cid:88)|S| i=1 (9) 4.1.1 Linear Activation
Assume that the activation function of the fully connected neural network (Equation (8)) is linear, i.e. σ(z) = z, and define the end-to-end matrix:
Wn:1 := WnWn−1···W1 ∈ Rdn,d0 . (10)
The mappings realized by the network can then be written as hθ(x) = Wn:1x, and the training loss as f (θ) = ϕ(Wn:1), where
ϕ : Rdn,d0 → R , ϕ(W ) = 1
|S| (cid:88)|S| i=1
ℓ(W xi,yi) (11) is convex and twice continuously differentiable. Lemma 1 below expresses ∇2f (θ) in this case.
Lemma 1. For any θ ∈ Rd, regard ∇2f (θ) not only as a (symmetric) matrix in Rd,d, but also as a quadratic form ∇2f (θ)[·] that intakes a tuple (∆W1, ∆W2, ... , ∆Wn) ∈
Rd1,d0 × Rd2,d1 × ··· × Rdn,dn−1, arranges it as a vector ∆θ ∈ Rd (in correspondence with how weight matrices W1,W2,...,Wn are arranged to create θ), and returns ∆θ⊤∇2f (θ)∆θ ∈ R.
Similarly, for any W ∈ Rdn,d0, regard ∇2ϕ(W ) as a quadratic form ∇2ϕ(W )[·] that intakes a matrix in Rdn,d0 and returns a scalar (non-negative since ϕ(·) is convex). Then, ∇2f (θ) is given by:
∇2f (θ)[∆W1,∆W2,...,∆Wn] = ∇2ϕ(Wn:1) (cid:104)(cid:80)n j=1Wn:j+1(∆Wj)Wj−1:1 (cid:105) (12)
+2Tr (cid:16)
∇ϕ(Wn:1)⊤(cid:80) 1≤j<j′≤nWn:j′+1(∆Wj′)Wj′−1:j+1(∆Wj)Wj−1:1 (cid:17)
, where Wj′:j, for any j,j′ ∈ {1,2,...,n}, is defined as Wj′Wj′−1···Wj if j ≤ j′, and as an identity matrix (with size to be inferred by context) otherwise.
Proof. See Subappendix J.4.
The following proposition makes use of Lemma 1 to show that (under mild conditions) λmin(∇2f (θ)) can be arbitrarily negative, i.e. inf θ∈Rd λmin(∇2f (θ)) = −∞.
Proposition 1. Assume that the network is deep (n ≥ 3), and that the zero mapping is not a global minimizer of the training loss (meaning ∇ϕ(0) ̸= 0).12 Then inf θ∈Rd λmin(∇2f (θ)) = −∞. 10The exact order by which the entries of W1,W2,...,Wn are placed in θ is insignificant for our purposes — all that matters is that the same order be used throughout. 11Our analysis can easily be extended to account for different activation functions at different hidden layers.
We assume identical activation functions for simplicity of presentation. 12Both of these assumptions are necessary, in the sense that removing any of them (without imposing further assumptions) renders the proposition false — see Claim 1 in Appendix F. 5
2n
Proof. See Subappendix J.5.
Building on Lemma 1, Lemma 2 below provides a lower bound on λmin(∇2f (θ)).
Lemma 2. For any θ ∈ Rd:13
λmin(∇2f (θ)) ≥ −(n−1)(cid:112)min{d0,dn}∥∇ϕ(Wn:1)∥F robenius max
Proof. See Subappendix J.6.
Assuming the training loss is non-constant and the network is deep (n ≥ 3), the infimum (over θ ∈ Rd) of the lower bound in Equation (13) is minus infinity. In particular, if θ is not a global minimizer (∇ϕ(Wn:1) ̸= 0) and at least n − 2 of its weight matrices W1, W2, ... , Wn are non-zero, then by rescaling the latter it is possible to take the lower bound to minus infinity while keeping the end-to-end matrix Wn:1 (and thus the input-output mapping hθ(·) and the training loss value f (θ)) intact. However, gradient flow over fully connected neural networks (with homogeneous activations) initialized near zero is known to maintain balance between weight matrices — see [18] — and so along its trajectories the lower bound in Equation (13) takes a much tighter form. This is formalized in Proposition 2 below.
Proposition 2. If θ ∈ Rd resides on a trajectory of gradient flow (over f (·)) emanating from some point θs ∈ Rd, with ∥θs∥2 ≤ ϵ for some ϵ ∈ (cid:0)0, 1 (cid:3), then:
∥Wj∥spectral. (13)
J ⊆{1,2,...,n}
|J |=n−2 (cid:89) j∈J
λmin(∇2f (θ)) ≥ −(n−1)(cid:112)min{d0,dn}∥∇ϕ(Wn:1)∥F robenius∥Wn:1∥1−2/n spectral −cϵ1−2/n , (14)
. j=1 (cid:9)2(n−2) (cid:112)min{d0,dn}∥∇ϕ(Wn:1)∥F robeniusmax(cid:8)1,max{∥Wj∥spectral}n where c := 4n(n−1) (4n)2/n
Proof. See Subappendix J.7.
Assume the network is deep (n ≥ 3), and consider a trajectory of gradient flow (over f (·)) emanating from near-zero initialization. For every point on the trajectory, Proposition 2 may be applied with small ϵ, leading the lower bound in Equation (14) to depend primarily on the sizes (norms) of the end-to-end matrix Wn:1 and the gradient of the loss with respect to it, i.e. ∇ϕ(Wn:1) (see Equations (10) and (11)). In the course of optimization, Wn:1 is initially small, and (since the loss f (θ) = ϕ(Wn:1) is monotonically non-increasing) remains confined to sublevel sets of ϕ(·) (which is convex) thereafter.
∇ϕ(Wn:1) on the other hand tends to zero upon convergence to global minimum. We conclude that the lower bound on λmin(∇2f (θ)) in Equation (14) starts off slightly negative, and approaches non-negativity (if and) as the trajectory converges to global minimum. In light of Section 3, this implies that the gradient flow trajectory may lend itself to approximation by gradient descent. Indeed, the results of the current Subsubsection are used in Section 5 to establish proximity between gradient flow and gradient descent, thereby translating an analysis of gradient flow into a guarantee of efficient convergence (to global minimum) for gradient descent. 4.1.2 Non-Linear Activation
Due to lack of space, we defer our analysis for fully connected neural networks with non-linear activation to Appendix C. This analysis is similar in spirit to the one in Subsubsection 4.1.1 treating linear activation. In particular, it makes use of the fact that gradient flow initialized near zero maintains balance between weight matrices — cf. [18]. A key difference brought forth by non-linear activation is that the training loss f (·) (Equation (9)) is no longer differentiable. We circumvent this challenge by excluding from the analysis points of non-differentiability, which form a negligible (closed and zero measure) set. 4.2 Convolutional Architectures
We account for convolutional neural networks by allowing for weight sharing and sparsity patterns to be imposed on the layers of the fully connected model analyzed in Subsection 4.1. Namely, we consider the exact same mappings as in Equation (8), but now, rather than being learned directly, the matrices Wj ∈ Rdj ,dj−1, j = 1,2,...,n, are determined by learned weight vectors wj ∈ Rd′ j , with j ∈ N, j = 1,2,...,n, such that each entry of Wj is either fixed at zero or connected to a predetermined d′ coordinate of wj (with no repetition of coordinates within the same row). The weight setting θ ∈ Rd is then simply a concatenation of the weight vectors w1,w2,...,wn, and its dimension is accordingly d = (cid:80)n j. Our analysis for this model (which includes convolutional neural networks as a special case) is essentially the same as that presented for fully connected neural networks with non-linear activation (Subsubsection 4.1.2). In particular, we use the fact that even with weight sharing and sparsity patterns imposed on the layers of a fully connected neural network (with homogeneous activation), when initialized near zero, gradient flow over the network maintains balance between weights of different layers — cf. [18]. For the complete analysis see Appendix D. j=1d′ 13Note that by convention, an empty product (i.e. a product over the elements of the empty set) is equal to one. 6
5 Continuous Proof of Discrete Convergence for Deep Linear Neural Networks
Section 3 invoked the Fundamental Theorem for numerical solution of initial value problems (The-orem 2) to show that, in general, the extent to which gradient descent provably matches gradient flow is determined by how large (less negative or more positive) the minimal eigenvalue of the Hessian is around the gradient flow trajectory.2 Section 4 established that for training losses of deep neural networks, along trajectories of gradient flow emanating from near-zero initialization (as commonly em-ployed in practice), the minimal eigenvalue of the Hessian is far greater than in arbitrary points in space, particularly towards convergence. In this section we combine the two findings, translating an analysis of gradient flow over deep linear neural networks into a convergence guarantee for gradient descent. The guarantee we obtain is, to our knowledge, the first to ensure that a conventional gradient-based algorithm optimizing a deep (three or more layer) neural network of fixed (data-independent3) size efficiently converges4 to global minimum almost surely under random (data-independent) near-zero initialization.
Deep linear neural networks — fully connected neural networks with linear activation (see Subsec-tion 4.1) — are perhaps the most common subject of theoretical study in the context of optimization in deep learning. Though trivial from an expressiveness point of view (realize only linear input-output map-pings), they induce highly non-convex training losses, giving rise to highly non-trivial phenomena under gradient-based optimization. In recent years, various results concerning gradient flow over deep linear neural networks have been proven, most notably for the case of balanced initialization (see for exam-ple [49, 4, 34, 6, 46]). Under the notations of Subsection 4.1 (in particular with W1,W2,...,Wn standing for network weight matrices), balanced initialization means that when optimization commences:
W ⊤ j+1Wj+1 = WjW ⊤ j for j = 1,2,...,n−1 . (15)
The condition holds approximately with any near-zero initialization, and exactly when the following procedure (adaptation of Procedure 1 in [5]) is employed.
Procedure 1 (random balanced initialization). With a distribution P over dn-by-d0 matrices of rank at most min{d0,d1,...,dn}, initialize Wj ∈ Rdj ,dj−1 , j = 1,2,...,n, via following steps: (i) sample A ∼ P; (ii) take singular value decomposition A = U ΣV ⊤, where U ∈ Rdn,min{d0,dn} and V ∈ Rd0,min{d0,dn} have orthonormal columns, and Σ ∈ Rmin{d0,dn},min{d0,dn} is diagonal and holds the singular values of A; and (iii) set Wn ≃ U Σ1/n,Wn−1 ≃ Σ1/n,Wn−2 ≃ Σ1/n,...,W2 ≃ Σ1/n,W1 ≃ Σ1/nV ⊤, where
“≃” stands for equality up to zero-valued padding.
Compared to gradient flow, little is known about gradient descent when it comes to optimization of deep (three or more layer) linear neural networks. Indeed, there are relatively few results along this line (cf. [9, 30, 5]), and these are typically highly specific, built upon technical proofs that are difficult to generalize. Being able to obtain results via translation of gradient flow analyses is thus of prime interest.
We focus in this section on deep14 linear neural networks trained for scalar regression per least-squares criterion. In the context of Subsection 4.1, this means that the activation function σ(·) is linear (σ(z) = z), the output dimension dn is one, and the loss function ℓ(·) is the square loss (i.e. Y = R and ℓ(ˆy,y) = 1 2 (ˆy − y)2). We assume that training inputs are whitened, i.e. have been transformed i ∈ Rd0,d0 is equal to such that their empirical (uncentered) covariance matrix Λxx := 1
|S| identity. A standard calculation (see Appendix G) shows that in this case the function ϕ(·) defined i ∈ R1,d0 by Equation (11) becomes ϕ(W ) = 1 is the empirical (uncentered) cross-covariance matrix between training labels and inputs, and c ∈ R is a constant (independent of W ). We may thus write the training loss f (·) (Equation (9)) as:
F robenius +c, where Λyx := 1
|S| 2 ∥W −Λyx∥2 i=1xix⊤ i=1yix⊤ (cid:80)|S| (cid:80)|S| 1 2 f (θ) =
∥Wn:1 −Λyx∥2 1 2 where Wn:1 ∈ R1,d0 is the network’s end-to-end matrix (Equation (10)). We disregard the degenerate case where Λyx = 0, i.e. where the zero mapping attains the global minimum, and assume that training labels are normalized (jointly scaled) such that Λyx has unit length (∥Λyx∥F robenius = 1).
F robenius +minq∈Rd f (q) ,
∥Wn:1 −Λyx∥2
F robenius +c = (16)
Proposition 3 below analyzes gradient flow over the training loss in Equation (16). Relying on a known characterization for the dynamics of the end-to-end matrix (cf. [4]), it establishes convergence to global minimum. Moreover, harnessing the results of Section 4, it derives a lower bound on (the integral of) the minimal eigenvalue of the Hessian around the gradient flow trajectory. 14Our results apply to shallow (two layer) networks as well. We highlight the deep (three or more layer) setting as it is far less understood (cf. [5]), and arguably more central to deep learning. 7
yxWn:1,s)(cid:14)(cid:0)∥Λyx∥F robenius∥Wn:1,s∥F robenius
Proposition 3. Consider minimization of the training loss f (·) in Equation (16) via gradient flow (Equation (5)) starting from initial point θs ∈ Rd that meets the balancedness condition (Equation (15)). Denote by Wn:1,s the initial value of the end-to-end matrix (Equation (10)), and suppose that ∥Wn:1,s∥F robenius ∈ (0,0.2] (initialization is small but non-zero). Assume that Wn:1,s is (cid:1) ̸= −1. Then, not antiparallel to Λyx, i.e. ν := Tr(Λ⊤ the trajectory of gradient flow is defined over infinite time, and with θ : [0,∞) → Rd representing this trajectory, for any ¯ϵ > 0, the following time ¯t satisfies f (θ(¯t))−minq∈Rdf (q) ≤ ¯ϵ: 1−ν 1+ν 3 2 · (17)
∥Wn:1,s∥F robenius
Moreover, under the notations of Theorem 3, for any t > 0 and ϵ ∈ (cid:0)0, 1 (cid:3) with corresponding Dt,ϵ (ϵ-neighborhood of gradient flow trajectory up to time t), we have the smoothness and Lipschitz
√ constants βt,ϵ = 16n and γt,ϵ = 6 n respectively, and the following (upper) bound on the integral of (minus) the minimal eigenvalue of the Hessian: (cid:9)(cid:1)n
∥Wn:1,s∥F robeniusmin{1,2¯ϵ} 2n(cid:0)max(cid:8)1, 15nmax(cid:8)1, (cid:9)(cid:1)5(n−1)/2 1−ν 1+ν
¯t = (cid:9)(cid:1)n ln 2n (cid:19) (cid:18) (cid:9)
. m(t′)dt′ ≤ 15n3(cid:0)max(cid:8)1, 3 2 · 1−ν 1+ν
∥Wn:1,s∥F robenius tϵ
+ln (cid:18) n2(cid:0)e2max(cid:8)1, 1−ν 1+ν
∥Wn:1,s∥2
F robenius (cid:19)
, (18) (cid:90) t 0 where the function m : [0,t] → R is non-negative.
Proof. See Subappendix J.8.
Plugging the gradient flow results of Proposition 3 into the generic Theorem 3 translates them to the following convergence guarantee for gradient descent.
Theorem 4. Assume the same conditions as in Proposition 3, but with minimization via gradient descent (Equation (6)) instead of gradient flow.15 Then, with θ0,θ1,θ2,... representing the iterates of gradient descent, Wn:1,0 standing for the end-to-end matrix (Equation (10)) of the initial point θ0, and
ν:=Tr(Λ⊤ yxWn:1,0)(cid:14)(cid:0)∥Λyx∥F robenius∥Wn:1,0∥F robenius 15nmax(cid:8)1, (cid:18) (cid:32) (cid:1), for any ˜ϵ > 0, if the step size η meets: (cid:33) (cid:32) (cid:9) (cid:19)(cid:33)−2 1−ν 1+ν
η ≤ ∥Wn:1,0∥5 n17/2e7n+6(cid:0)max(cid:8)1,
F robeniusmin{1,˜ϵ} 1−ν 1+ν (cid:9)(cid:1)(11n−5)/2 ln
∥Wn:1,0∥F robeniusmin{1,˜ϵ}
∈ ˜Ω
∥Wn:1,0∥5 n17/2(cid:0)poly(cid:0) 1−ν 1+ν
F robenius˜ϵ (cid:1)(cid:1)n
, (19) it holds that f (θk)−minq∈Rd f (q) ≤ ˜ϵ, where: 15nmax(cid:8)1, (cid:22)2n(cid:0)max(cid:8)1, (cid:9)(cid:1)n 3 (cid:18) 2 ·
∥Wn:1,0∥F robeniusη ln 1−ν 1+ν k = (cid:9) 1−ν 1+ν
∥Wn:1,0∥F robeniusmin{1,˜ϵ} (cid:19) (cid:23)
∈ ˜O ln(cid:0) 1 (cid:18) n(cid:0)poly(cid:0) 1−ν
˜ϵ 1+ν
∥Wn:1,0∥F robeniusη (cid:1)(cid:1)n (cid:1) (cid:19)
. (20)
+1
Proof. See Subappendix J.9.
Remark 1. Theorem 3 — our generic tool for translating analyses between gradient flow and gradient descent — allows for the two to be initialized differently. Accordingly, the convergence guarantee of Theorem 4 may be extended to account for initialization which is not perfectly balanced, i.e. which satisfies Equation (15) only approximately. For details see Appendix H.
Remark 2. The convergence guarantee of Theorem 4 requires a number of iterates that scales exponentially with network depth (n). [51] has proven that under mild conditions, for a deep linear neural network whose input, hidden and output dimensions are all equal to one (i.e., in our notations, d0 = d1 = ··· = dn = 1), such exponential dependence on depth is unavoidable. We defer to future work the question of whether this also holds in the context of Theorem 4.
Combining Theorem 4 with random balanced initialization (Procedure 1) yields what is, to our knowledge, the first guarantee of random (data-independent) near-zero initialization almost surely leading a conventional gradient-based algorithm optimizing a deep (three or more layer) neural network of fixed (data-independent) size to efficiently converge to global minimum.
Corollary 2. Consider minimization of the training loss f (·) in Equation (16) via gradient descent (Equation (6)) emanating from a random balanced initialization (Procedure 1) whose underlying (cid:2)∥A∥F robenius ≤ 0.2(cid:3) = 1. Assume d0 (network distribution P is continuous and satisfies PrA∼P input dimension) is greater than one, and let Wn:1,0 and ν be as defined in Theorem 4. Then, almost surely with respect to (i.e. with probability one over) initialization, for any ˜ϵ > 0, if the step size η meets
Equation (19), the value of f (·) after k iterates will be within ˜ϵ from global minimum, where k is given by Equation (20).
Proof. See Subappendix J.10. 15The conditions on θs in Proposition 3 are now satisfied by the initialization of gradient descent, i.e. by θ0. 8
Figure 1: Over deep fully connected neural networks, trajectories of gradient descent with conventional step size barely change when step size is reduced, suggesting they are close to the continuous limit, i.e. to trajectories of gradient flow. Presented results were obtained on fully connected neural networks as analyzed in Subsection 4.1, trained to classify MNIST handwritten digits (28-by-28 grayscale images, each labeled as an integer between 0 and 9 — cf. [35]). Networks had depth n=3, input dimension d0=784 (corresponding to 28·28=784 pixels), hidden widths d1=d2=50 and output dimension d3=10 (corresponding to ten possible labels). Training was based on gradient descent applied to cross-entropy loss with no regularization, starting from a near-zero point drawn from Xavier distribution (cf. [24]). Separately on each network, we compared runs differing only in the step size η. Specifically, with η0=0.001 (standard choice of step size) and r ranging over {2,5,10,20}, we compared, in terms of training loss value and location in weight space, every iteration of a run using η=η0 to every r’th iteration of a run in which η=η0/r. Left pair of plots reports results obtained on a network with linear activation (σ(z) = z), while right pair corresponds to a network with rectified linear activation (σ(z) = max{z,0}). In each pair, left plot displays training loss values, and right one shows (Euclidean) distances in weight space, namely, distance between initialization and run with η=η0, alongside distances between run with η=η0 and runs having
η=η0/r for different values of r. Horizontal axes represent time in units of η=η0 iterations (meaning each time unit corresponds to r iterations of a run with η=η0/r). Notice that the drift between runs with different step sizes is minor compared to the distance traveled. For further implementation details, and results of similar experiments on convolutional neural networks, see Appendix I. 6 Experiments
In this section we corroborate our theory by presenting experiments suggesting that over simple deep neural networks, gradient descent with conventional step size is indeed close to the continuous limit, i.e. to gradient flow. Our experimental protocol is simple — on several deep neural networks classifying MNIST handwritten digits ([35]), we compare runs of gradient descent differing only in the step size η. Specifically, separately on each evaluated network, with η0 = 0.001 (standard choice of step size) and r ranging over {2,5,10,20}, we compare, in terms of training loss value and location in weight space, every iteration of a run using η=η0 to every r’th iteration of a run in which η=η0/r.
Figure 1 reports the results obtained on fully connected neural networks (as analyzed in Subsection 4.1), with both linear and non-linear activation. As can be seen, reducing the step size η leads to only slight changes, suggesting that the trajectory of gradient descent with η=η0 is already close to the continuous limit. Similar results obtained on convolutional neural networks (see Subsection 4.2 for corresponding analysis) are reported by Figure 3 in Subappendix I.1.
Our experimental findings suggest that in practice, proximity between gradient descent and gradient flow may take place even when the step size of gradient descent is larger than permitted by current theory. Indeed, the theoretical machinery developed in this paper brings forth upper bounds on step size that guarantee proximity, and while such upper bounds can be asymptotically tight under worst case conditions (see Appendix B), they are by no means tight in every given scenario, and therefore larger step sizes may also admit proximity. For illustration, a step size of η0, which in our experiments was seemingly sufficient for ensuring proximity, is many orders of magnitude greater than the upper bound on step size required by Theorem 4 (Equation (19)). 7