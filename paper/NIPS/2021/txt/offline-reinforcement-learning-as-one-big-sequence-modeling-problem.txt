Abstract
Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models, leveraging the Markov property to factorize prob-lems in time. However, we can also view RL as a generic sequence modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards. Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well in other domains, such as natural-language processing, can also provide effective solutions to the RL problem. To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence modeling problem simpliﬁes a range of design decisions, allowing us to dispense with many of the components common in ofﬂine RL algorithms. We demonstrate the ﬂexibility of this approach across long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and ofﬂine RL. Further, we show that this approach can be combined with existing model-free algorithms to yield a state-of-the-art planner in sparse-reward, long-horizon tasks. 1

Introduction
The standard treatment of reinforcement learning relies on decomposing a long-horizon problem into smaller, more local subproblems. In model-free algorithms, this takes the form of the principle of optimality (Bellman, 1957), a recursion that leads naturally to the class of dynamic programming methods like Q-learning. In model-based algorithms, this decomposition takes the form of single-step predictive models, which reduce the problem of predicting high-dimensional, policy-dependent state trajectories to that of estimating a comparatively simpler, policy-agnostic transition distribution.
However, we can also view reinforcement learning as analogous to a sequence generation problem, with the goal being to produce a sequence of actions that, when enacted in an environment, will yield a sequence of high rewards. In this paper, we consider the logical extreme of this analogy: does the toolbox of contemporary sequence modeling itself provide a viable reinforcement learning algorithm? We investigate this question by treating trajectories as unstructured sequences of states, actions, and rewards. We model the distribution of these trajectories using a Transformer architecture (Vaswani et al., 2017), the current tool of choice for capturing long-horizon dependencies. In place of the trajectory optimizers common in model-based control, we use beam search (Reddy, 1977), a heuristic decoding scheme ubiquitous in natural language processing, as a planning algorithm.
Posing reinforcement learning, and more broadly data-driven control, as a sequence modeling problem handles many of the considerations that typically require distinct solutions: actor-critic algorithms require separate actors and critics, model-based algorithms require predictive dynamics models, and ofﬂine RL methods often require estimation of the behavior policy (Fujimoto et al., 2019). These
Code is available at trajectory-transformer.github.io 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1 (Architecture) The Trajectory Transformer trains on sequences of (autoregressively dis-cretized) states, actions, and rewards. Planning with the Trajectory Transformer mirrors the sampling procedure used to generate sequences from a language model. components estimate different densities or distributions, such as that over actions in the case of actors and behavior policies, or that over states in the case of dynamics models. Even value functions can be viewed as performing inference in a graphical model with auxiliary optimality variables, amounting to estimation of the distribution over future rewards (Levine, 2018). All of these problems can be uniﬁed under a single sequence model, which treats states, actions, and rewards as simply a stream of data. The advantage of this perspective is that high-capacity sequence model architectures can be brought to bear on the problem, resulting in a more streamlined approach that could beneﬁt from the same scalability underlying large-scale unsupervised learning results (Brown et al., 2020).
We refer to our model as a Trajectory Transformer (Figure 1) and evaluate it in the ofﬂine regime so as to be able to make use of large amounts of prior interaction data. The Trajectory Transformer is a substantially more reliable long-horizon predictor than conventional dynamics models, even in
Markovian environments for which the standard model parameterization is in principle sufﬁcient.
When decoded with a modiﬁed beam search procedure that biases trajectory samples according to their cumulative reward, the Trajectory Transformer attains results on ofﬂine RL benchmarks that are competitive with the best prior methods designed speciﬁcally for that setting. Additionally, we describe how variations of the same decoding procedure yield a model-based imitation learning method, a goal-reaching method, and, when combined with dynamic programming, a state-of-the-art planner for sparse-reward, long-horizon tasks. Our results suggest that the algorithms and architectural motifs that have been widely applicable in unsupervised learning carry similar beneﬁts in RL. 2