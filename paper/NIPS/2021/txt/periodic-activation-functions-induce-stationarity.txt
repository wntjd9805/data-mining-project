Abstract
Neural network models are known to reinforce hidden data biases, making them unreliable and difﬁcult to interpret. We seek to build models that ‘know what they do not know’ by introducing inductive biases in the function space. We show that periodic activation functions in Bayesian neural networks establish a connection between the prior on the network weights and translation-invariant, stationary Gaussian process priors. Furthermore, we show that this link goes beyond sinusoidal (Fourier) activations by also covering triangular wave and periodic ReLU activation functions. In a series of experiments, we show that periodic activation functions obtain comparable performance for in-domain data and capture sensitivity to perturbed inputs in deep neural networks for out-of-domain detection. 1

Introduction
Deep feedforward neural networks [46, 16] are an integral part of contemporary artiﬁcial intelligence and machine learning systems for visual and auditory perception, medicine, and general data analysis and decision making. However, when these methods have been adopted into real-world use, concerns related to robustness (with respect to data that has not been seen during training), fairness (hidden biases in data being reinforced by the model), and interpretability (why the model acts as it does) have taken a central role. The knowledge gathered by contemporary neural networks has even been characterised as never truly reliable [31]. These issues relate to the sensitivity of the trained model to perturbed inputs being fed through it—or the lack thereof.
This motivates Bayesian deep learning, where the interests are two-fold: encoding prior knowledge into models and performing probabilistic inference under the speciﬁed model. We focus on the former.
Probabilistic approaches to specifying assumptions about the function space of deep neural networks have gained increasing attention in the machine learning community, comprising, among others, work analysing their posterior distribution [57, 1], discussing pathologies arising in uncertainty quantiﬁcation [9], and calls for better Bayesian priors (e.g., [42, 50, 37, 10]).
In this paper, we focus on stationary models, which act as a proxy for capturing sensitivity. Stationarity indicates translation-invariance, meaning that the joint probability distribution does not change when the inputs are shifted. This seemingly naive assumption has strong consequences in the sense that it induces conservative behaviour across the input domain, both in-distribution and outside the observed data. The resulting model is mean-reverting outside the training data (reversion to the prior), directly leading to higher uncertainty for out-of-distribution (OOD) samples (see Fig. 1 for examples). These features (together with some direct computational beneﬁts) have made stationary models/priors the standard approach in kernel methods [4, 20], spatial statistics [5], and Gaussian process (GP) models
[44], where the kernel is often chosen to induce stationarity.
Neural networks are parametric models, which typically fall into the class of non-stationary models.
Non-stationarity increases ﬂexibility and is often a sought-after property—especially if the interest is solely in optimizing for accuracy on in-domain test data. In fact, all standard neural network 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Non-stationarity
Local stationarity
Global/full stationarity (this paper) h t d i w
-e t i n
ﬁ n
I h t d i w
-e t i n i
F
ArcCos-1 kernel [3] /
ReLU activation [36]
Matérn- 3 2 -NN kernel/activation [34]
RBF-NN kernel/activation [59]
Exponential (Cauchy prior)
Matérn- 3 2 (t-distribution prior)
RBF (Gaussian prior)
Figure 1: Posterior predictive densities of single hidden layer Bayesian neural networks (BNNs) with 30 hidden units and their inﬁnite-width corresponding GPs on the banana classiﬁcation task.
Different activation functions induce different prior assumptions. Estimates obtained through HMC sampling [14] for 10k iterations. activation functions (ReLU, sigmoid, etc.) induce non-stationarity (see [3, 59]). However, non-stationary models can easily result in over-conﬁdence outside the observed data [18], [55] or spurious relationships between input variables (as illustrated in Fig. 1, where darker shades show higher conﬁdence).
Stationarity in neural networks has been tackled before under the restriction of local stationarity, i.e., translation-invariance is only local, induced by a Gaussian envelope (as realized by [59] for the
RBF kernel/activation). Recently, Meronen et al. [34] expanded this approach and derived activation functions corresponding to the widely used Matérn class of kernels [32, 44]. We go one step further and derive activation functions that induce global stationarity. To do so, we leverage theory from harmonic analysis [58] of periodic functions, which helps expand the effective support over the entire input domain. We also realize direct links to previous works leveraging harmonic functions in neural networks [13, 2, 61, 47, 56], and Fourier features in other model families [43, 51].
The contributions of this paper are: (i) We show that periodic activation functions establish a direct correspondence between the prior on the network weights and the spectral density of the covariance function of the limiting stationary Gaussian process (GP) of single hidden layer Bayesian neural networks (BNNs). (ii) We leverage this correspondence and show that placing a Student-t prior on the weights of the hidden layer corresponds to a prior on the function space with Matérn covariance. (iii) Finally, we show in a range of experiments that periodic activation functions obtain comparable performance for in-domain data, do not result in overconﬁdent predictions, and enable robust out-of-domain detection. 1.1