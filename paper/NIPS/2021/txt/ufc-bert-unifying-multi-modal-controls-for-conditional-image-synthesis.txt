Abstract
Conditional image synthesis aims to create an image according to some multi-modal guidance in the forms of textual descriptions, reference images, and image blocks to preserve, as well as their combinations. In this paper, instead of investigating these control signals separately, we propose a new two-stage architecture, UFC-BERT, to unify any number of multi-modal controls. In UFC-BERT, both the diverse control signals and the synthesized image are uniformly represented as a sequence of discrete tokens to be processed by Transformer. Different from existing two-stage autoregressive approaches such as DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the second stage to enhance the holistic consistency of the synthesized image, to support preserving speciﬁed image blocks, and to improve the synthesis speed. Further, we design a progressive algorithm that iteratively improves the non-autoregressively generated image, with the help of two estimators developed for evaluating the compliance with the controls and evaluating the ﬁdelity of the synthesized image, respectively. Extensive experiments on a newly collected large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal CelebA-HQ verify that UFC-BERT can synthesize high-ﬁdelity images that comply with ﬂexible multi-modal controls. 1

Introduction
Conditional image synthesis aims to create an image according to the given control signals. With the increasing demand for ﬂexible conditional image synthesis, various kinds of control signals have been introduced into this ﬁeld, which can be divided into three main modalities: (i) textual controls (TC), including the class labels [1] and natural language descriptions [62, 54]; (ii) visual controls (VC), such as a spatially-aligned sketch map for reference [17, 60] or another image for style transfer [15, 27]; (iii) preservation controls (PC), which require the synthesized image to preserve some given image blocks, e.g., image outpainting and inpainting [63, 69].
However, control signals of various modalities possess different characteristics. Existing works [62, 26, 61] hence typically design separate methods customized for each control modality. Moreover, most of these approaches only utilize one type of control signal and cannot simultaneously combine multiple types of controls in a concise and versatile model. This begs the question: can we integrate any number of multi-modal control signals into a uniﬁed framework for ﬂexible conditional image synthesis? There are two inevitable challenges in this setting: (i) how to unify the multi-modal controls and represent them in a uniﬁed form, especial when employing multiple control signals from different modalities concurrently; (ii) how to guarantee the fulﬁllment of the multi-modal controls while ensuring the ﬁdelity of the synthesized image. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The three main modalities of control signals for conditional image synthesis: Textual
Controls (TC), Visual Controls (VC), and Preservation Controls (PC).
Recently, two-stage image synthesis [42, 48, 3, 13, 47] has made great progress. The ﬁrst stage learns a convolutional autoencoder with quantized latent representations for converting an image into a sequence of discrete tokens, e.g., for compressing a 256×256 image into a sequence of 32 × 32 tokens where each token correlates mainly with an 8 × 8 block of the image. Converting a sequence of tokens back into an image is also supported. The second stage then typically adopts an autoregressive model, e.g., PixelCNN [41] or a unidirectional Transformer decoder [55], to capture the distribution over sequences of tokens. Particularly, the Transformer-based methods [3, 13, 47] exploit the global expressivity of Transformer to capture long-range relationships between local constituents.
In this paper, we make two key observations about the two-stage framework. First, the two-stage framework has the advantage that it can potentially unify the multi-modal control signals and the generated image into a single sequence of discrete tokens. However, existing works [42, 48, 13, 47] largely neglect this advantage of the two-stage framework over the traditional one-stage approaches such as those based mainly on the generative adversarial networks (GAN) [18]. Second, the autoregressive (AR) approach to sequence generation, adopted by the existing two-stage methods such as DALL-E [47] and VQGAN [13], brings undesirable shortcomings: (i) the token-by-token synthesis procedure leads to slow generation speed, especially for the heavyweight Transformer [3, 13, 47]; (ii) each generated token can only catch sight of the previously generated tokens and cannot incorporate bidirectional contexts, which may affect the holistic consistency of image synthesis; (iii) the ﬁxed left-to-right order of autoregressive decoding cannot respond to the preservation control signals unless the image blocks to be preserved are at the beginning of the sequence. Notably, different from AR generation, non-autoregressive (NAR) sequence generation with bidirectional
Transformer, i.e., BERT [9], can naturally avoid the three shortcomings.
Based on the aforementioned observations, we propose UFC-BERT, a novel BERT-based two-stage framework to UniFy any number of multi-modal Controls for conditional image synthesis. Concretely, the textual, visual, and preservation control signals, as well as the generated image, are uniformly represented as a sequence of discrete tokens, as shown in Figure 2. The textual control consists of word tokens for class labels or natural language descriptions. The visual control(s) and the generated image are both represented as discrete tokens due to the ﬁrst stage, where each token corresponds to a block within the reference image(s) or the generated image. Zero, one, or more reference images are supported. To preserve a given image block within the generated image, we encode the given image block into discrete tokens and ﬁx corresponding parts of the generated sequence to the tokens.
We train UFC-BERT via the masked sequence modeling task, which predicts a masked subset of the target image’s tokens conditioned on both the multi-modal control signals and the generation target’s unmasked tokens. During inference, we adopt Mask-Predict, a NAR generation algorithm [16, 21, 7], which predicts all target tokens at the ﬁrst iteration and then iteratively re-mask and re-predict a subset of tokens with low conﬁdence scores. To further improve upon the NAR generation algorithm, we exploit the discriminative capability of the BERT architecture [11, 70] and add two estimators (see Figure 2), where one estimator estimates the relevance between the generated image and the control signals, and the other one estimates the image’s ﬁdelity. The two estimators help improve the quality of the synthesized image, because at each iteration we can generate multiple samples and keep only the highly-scored one before starting the next iteration. The two estimators also help save 2
the number of iterations needed, since the algorithm can dynamically terminate if running for more iterations no longer improves the scores.
The extensive experiments on M2C-Fashion, a newly collected clothing dataset with tens of millions of image-text pairs, as well as on Multi-Modal CelebA-HQ [28, 61], a public facial dataset, demonstrate
UFC-BERT can synthesize high-quality images that comply with various multi-modal controls. 2