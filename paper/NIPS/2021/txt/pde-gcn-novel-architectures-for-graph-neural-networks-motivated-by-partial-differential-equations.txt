Abstract
Graph neural networks are increasingly becoming the go-to approach in various
ﬁelds such as computer vision, computational biology and chemistry, where data are naturally explained by graphs. However, unlike traditional convolutional neural networks, deep graph networks do not necessarily yield better performance than shallow graph networks. This behavior usually stems from the over-smoothing phenomenon. In this work, we propose a family of architectures to control this behavior by design. Our networks are motivated by numerical methods for solving
Partial Differential Equations (PDEs) on manifolds, and as such, their behavior can be explained by similar analysis. Moreover, as we demonstrate using an extensive set of experiments, our PDE-motivated networks can generalize and be effective for various types of problems from different ﬁelds. Our architectures obtain better or on par with the current state-of-the-art results for problems that are typically approached using different architectures. 1

Introduction
In recent years, Graph Convolutional Networks (GCNs) [1, 2, 3] have drawn the attention of re-searchers and practitioners in a variety of domains and applications, ranging from computer vision and graphics [4, 5, 6, 7, 8] to computational biology [9, 10, 11], recommendation systems [12] and social network analysis [13, 14]. However, GCNs still suffer from two main problems. First, they are usually shallow as opposed to the concept of deep convolutional neural networks (CNNs) [15, 16] due to the over-smoothing phenomenon [17, 18, 19], where the node feature vectors become almost identical, such that they are indistinguishable, which yields non-optimal performance. Furthermore,
GCNs are typically customized to a speciﬁc domain and application. That is, as we demonstrate in
Sec. 4.1, a successful point-cloud classiﬁcation network [5] can perform poorly on a citation graph node-classiﬁcation problem [19], and vice-versa. Furthermore, because many GCNs lack theoretical guarantees, it is difﬁcult to reason about their success on one problem and lack on others. These observations motivate us to develop a profound understanding of graph networks and their dynamics.
To this end, we suggest a novel, universal approach to the design of GCN architectures. Our inspiration stems from the similarities and equivalence between Partial Differential Equations (PDEs) and deep networks explored in [20, 21, 22]. Furthermore, as GCNs can be thought of as a generalization of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
CNNs, and a standard convolution can be represented as a combination of differential operators on a structured grid [22], we adopt this interpretation to explore versions of GCNs as PDEs on graphs or manifolds. We therefore call our network architectures PDE-GCN, and demonstrate that our approach is general with respect to the given task. That is, our architectures behave similarly for different problems, and their performance is on par or better than other domain-speciﬁc GCNs. Furthermore, our family of architectures are backed by theoretical guarantees that allow us to explain the behavior of the GCNs in some of the results that we present. To be more speciﬁc, our contribution is as follows:
• We introduce and implement general graph convolution operators, based on graph gradient and divergence. This abstraction of the spatial operation on the graph leads to a more general and
ﬂexible approach for architecture design.
• We propose treating a variety of graph related problems as discretized PDEs, and formulate the dy-namics that match different problems such as node-classiﬁcation and dense shape-correspondence.
This is in direct effort to propose a family of networks that can solve multiple problems, instead of
GCNs which are tailored for a speciﬁc application.
• Our method allows constructing a deep GCN without over-smoothing, with theoretical guarantees.
• We validate our method by conducting numerous experiments on multiple datasets and applications, achieving signiﬁcantly better or similar accuracy compared to state-of-the-art models. 2