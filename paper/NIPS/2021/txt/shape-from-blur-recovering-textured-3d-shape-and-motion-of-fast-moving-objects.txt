Abstract
We address the novel task of jointly reconstructing the 3D shape, texture, and motion of an object from a single motion-blurred image. While previous approaches address the deblurring problem only in the 2D image domain, our proposed rigorous modeling of all object properties in the 3D domain enables the correct description of arbitrary object motion. This leads to signiﬁcantly better image decomposition and sharper deblurring results. We model the observed appearance of a motion-blurred object as a combination of the background and a 3D object with constant translation and rotation. Our method minimizes a loss on reconstructing the input image via differentiable rendering with suitable regularizers. This enables estimating the textured 3D mesh of the blurred object with high ﬁdelity. Our method substantially outperforms competing approaches on several benchmarks for fast moving objects deblurring. Qualitative results show that the reconstructed 3D mesh generates high-quality temporal super-resolution and novel views of the deblurred object. 1

Introduction
Motion blur is a common cause of degraded image quality. It can originate from camera motion, fast object motion, long exposure times due to low light settings, or a combination of these effects. In this paper, we focus on deblurring of fast moving objects (FMOs), which move over a distance larger than their size for the duration of the camera exposure of a single image. Since FMOs appear as a mixture of motion-blurred object texture and the background, our ﬁrst goal is to decompose the combined appearance of the background and the one of the FMO in form of a matting mask [1]. Our main goal is to recover the shape, motion, and sharp texture of the FMO in order to explain its appearance in the input image in the best possible way (Fig. 1). The accurate reconstruction and motion estimation of
FMOs enables to generate videos with temporal super-resolution, as well as FMO motion analysis and prediction. This is useful for a variety of applications like sports analysis (e.g. soccer, tennis, baseball), meteorites detection in astrophysics, detecting fast moving obstacles in front of a vehicle for driver alert or autonomous driving, and general video quality enhancement or compression. Our work could eventually make FMO tracking accessible to everyone using regular cameras rather than expensive high-speed cameras, or push the capabilities of current high-speed cameras to a new level, e.g. by capturing gun shots.
The majority of deblurring methods aim for the generic task of removing any kind of blur in images [2, 3, 4] or videos [5, 6, 7, 8, 9]. However, they perform poorly on deblurring of FMOs as 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
e i b o r e a l l a b y e l l o v y e k n e p t o o b l l o r e p a t
Inputs
Temporal super-resolution and sub-frame decomposition Novel views Geo. GT
Figure 1: Temporal super-resolution and deblurring with 3D reconstruction of fast moving objects, given a single image. Solely from the inputs on the left (clean background B, blurry image
I), the proposed Shape-from-Blur (SfB) method reconstructs a high-quality textured 3D shape and its motion, by minimizing the input image reconstruction loss with suitable regularizers. Our method enables to generate temporal super-resolution from a single image, as well as synthesizing novel views of the reconstructed shape. The ground truth (GT) on the right shows the corresponding high-speed camera footage (rows 1-4) or a picture of the object without motion blur (rows 5-6). shown in comparisons to methods specialized for them [10, 11]. To the best of our knowledge, all generic and all FMO-specialized methods try to tackle the deblurring task only in the 2D image domain. This limits their ability to describe object rotations around any axis that does not correspond to the camera viewing direction, or any object translation along the viewing direction.
Our key idea is to model the object shape, motion, and texture in the 3D domain to better explain the input image as the projection and temporal aggregation of a continuously moving object. We recover motion as the initial 6-DoF pose of the object and the 3D translation and 3D rotation that it undergoes during the exposure time of the input image. Moreover, the proposed method is the ﬁrst to estimate the 3D shape of motion-blurred objects. Although we are solving a more general and more challenging problem than previous methods, we achieve substantially better deblurring results due to the rigorous joint modeling of object shape, motion, and texture in 3D.
In summary, we make the following contributions: (1) We propose a novel fast moving object deblurring method that for the ﬁrst time jointly estimates from a single input image the 3D shape, texture, and motion of an object (initial 6-DoF pose, 3D translation and 3D rotation). (2) We describe an effective pipeline that uses pure test-time optimization for the recovery of the object 3D shape, motion, and sharp appearance. To constrain the optimization, we propose several suitable regularizers. 2