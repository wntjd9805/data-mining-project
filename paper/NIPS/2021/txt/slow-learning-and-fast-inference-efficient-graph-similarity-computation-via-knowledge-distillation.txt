Abstract
Graph Similarity Computation (GSC) is essential to wide-ranging graph appli-cations such as retrieval, plagiarism/anomaly detection, etc. The exact computation of graph similarity, e.g., Graph Edit Distance (GED), is an NP-hard problem that cannot be exactly solved within an adequate time given large graphs. Thanks to the strong representation power of graph neural network (GNN), a variety of
GNN-based inexact methods emerged. To capture the subtle difference across graphs, the key success is designing the dense interaction with features fusion at the early stage, which, however, is a trade-off between speed and accuracy. For Slow
Learning of graph similarity, this paper proposes a novel early-fusion approach by designing a co-attention-based feature fusion network on multilevel GNN features.
To further improve the speed without much accuracy drop, we introduce an efﬁcient
GSC solution by distilling the knowledge from the slow early-fusion model to the student one for Fast Inference. Such a student model also enables the ofﬂine collection of individual graph embeddings, speeding up the inference time in orders. To address the instability through knowledge transfer, we decompose the dynamic joint embedding into the static pseudo individual ones for precise teacher-student alignment. The experimental analysis on the real-world datasets demonstrates the superiority of our approach over the state-of-the-art methods on both accuracy and efﬁciency. Particularly, we speed up the prior art by more than 10x on the benchmark AIDS data. 1

Introduction
Measuring the similarity across graphs, i.e., Graph Similarity Computation (GSC), is one of the core problems of graph data mining, centered around by multiple downstream tasks such as graph retrieval [1, 2], plagiarism/anomaly detection [22, 41], graph clustering [39], etc. As shown in Fig. 1, the graph similarity can be deﬁned as distances between graphs, such as Graph Edit Distance (GED).
The conventional solutions towards GSC are the exact computation of these graph distances, which, however, is an NP-hard problem. Therefore, such exact solutions are less favorable when handling large-scale graphs due to the expensive computation cost. Computational time, especially run time in inference stage, is particularly important in industrial scenarios. As a motivating example, in graph-structured molecules or chemical compounds query for in-silico drug screening, fast identifying similar compounds in a large database is a key process [25].
Leveraging the strong representational power of graph neural network (GNN) [21, 13, 43, 42], the
GNN-based approximate GSC solutions have gained increasing popularity. To adapt GNNs to the
GSC task, the target similarity score (e.g., GED) is normalized into the range of (0, 1]. In this way, the
∗Corresponding Author: qin.ca@northeastern.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of graph edit distance (GED), which is deﬁned as the number of edit operations in the optimal path to transform the source graph to the target graph.
GSC can be regarded as a single-value regression problem that outputs a similarity score given two graphs as inputs. A standard design can be summarized as a twin of GNNs bridged by a co-attention with a Multi-layer Perceptron (MLP) stacked as the regression head. Such approaches can be trained in a fully supervised way using the Mean Square Error (MSE) loss computed over the ground truth similarity score. Many GNN-based GSC methods [1, 2, 22] followed such strategy, which, however, suffers from the fusion issue.
The paper presents a novel solution to both effectively and efﬁciently address the task of approximate
GSC. Compared to the commonly used graph convolutional network as the backbone [1, 2], this paper adopts a more robust network, i.e., Graph Isomorphism Network (GIN) [43]. Cross-graph fusion is essential to the model. The multi-scale features within different GIN layers are fused with a new design. We have adopted an attention layer stacked over the concatenated cross-graph features for smooth feature fusion. To this end, similar features will be assigned with more weights to contribute to the desired task. Moreover, to make the model easier to deploy, we take an MLP for feature learning which is simple but effective to achieve cutting-edge performance.
Intuitively, speed and accuracy can be considered as a trade-off. GSC naturally requires dense con-nections/interactions between the two input graphs, which will consequently cause increasing compu-tations as the cost. This paper focuses on the efﬁ-ciency of inference speed which can be addressed by either model compression or a faster data load-ing pipeline. Especially in industrial scenarios, the raw graph data are usually pre-processed as the em-beddings off-line that can be easily applied to the real-time downstream tasks, e.g., molecular graph retrieval. However, as shown in Fig. 2, most of the co-attention-based GSC solutions employ feature fusion in the early stage, which only outputs the joint embedding of pairing graphs. Inspired by [26], we propose a lightweight model that removes all the early feature fusion modules in the encoder for efﬁcient GSC. In this way, as shown in Fig. 2, the individual embedding of each graph can be collected by a Siamese GNN. Such pairing graph embeddings will be fused with an attention layer to predict the ﬁnal similarity score.
Figure 2: Illustration of knowledge distillation to achieve a fast model (right side) given a early-fusion-based slow model (left side).
To overcome the accuracy drop of such a small network, we take a novel paradigm of Knowledge
Distillation (KD) speciﬁcally designed for our task. As shown in Fig. 3, we propose an early-feature fusion network regarded as the teacher model, and the student model is a siamese network without co-attention. It is found that the direct distillation of joint embeddings fails to work where the KD loss disturbs largely during training. To solve this, we generate the pseudo individual embeddings of the teacher model and use them for KD by minimizing their relational distances [29]. To ensure pseudo individual embeddings fully cover the information of raw graphs, we further apply an MSE loss on the reconstructed joint embeddings concatenated from pseudo individual ones. We have veriﬁed that there is only a marginal accuracy drop compared with the original joint embeddings, which justiﬁes the claim above. To sum up, our contributions can be summarized in three folds:
• We introduce a new early-feature fusion model to achieve the competitive accuracy by designing a strong co-attention network and taking the GIN as the backbone.
• For efﬁcient inference and off-line embedding collection, we propose a novel Knowledge
Distillation method for GSC where the joint embeddings are decomposed to distill. 2
• Extensive experiments on the popular GED benchmarks demonstrate the superiority of our model over the state-of-the-art GSC methods on both accuracy and efﬁciency. Compared with the co-attention models, there is a more than 10 times faster in inference speed compared with the best competitor on AIDS dataset. † 2