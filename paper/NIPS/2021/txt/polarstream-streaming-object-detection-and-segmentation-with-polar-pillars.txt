Abstract
Recent works recognized lidars as an inherently streaming data source and showed that the end-to-end latency of lidar perception models can be reduced signiﬁcantly by operating on wedge-shaped point cloud sectors rather then the full point cloud.
However, due to use of cartesian coordinate systems these methods represent the sectors as rectangular regions, wasting memory and compute. In this work we propose using a polar coordinate system and make two key improvements on this design. First, we increase the spatial context by using multi-scale padding from neighboring sectors: preceding sector from the current scan and/or the following sector from the past scan. Second, we improve the core polar convolutional architecture by introducing feature undistortion and range stratiﬁed convolutions.
Experimental results on the nuScenes dataset show signiﬁcant improvements over other streaming based methods. We also achieve comparable results to existing non-streaming methods but with lower latencies. 1

Introduction
The ability to accurately perceive objects in dense urban environments still remains a challenging problem for self-driving cars. While such self-driving cars typically deploy a wide variety of sensors lidars play a key role due to the accurate range information provided. Driven in part by the availability of benchmark datasets [12, 3, 27], the last decade has seen tremendous progress in lidar based 3D object detection [39, 16, 32, 21, 10]. However, these methods all ignore the fact that most lidar sensors scan the scene sequentially as the lidar rotates around the z-axis. They instead wait for the rotational scan to complete (colloquially known as full sweep) before processing data, thereby introducing a large data capture latency (usually 50 to 100 ms).
First, Han et al. [13] and then STROBE [11] recognized this problem and proposed solutions which processed lidar sectors (shown in Fig. 1) as soon as they arrived. They showed that a streaming based architecture can achieve signiﬁcantly reduced latency over the traditional non-streaming baselines.
Both of these methods encode the point clouds as an image in bird’s-eye view (BEV) using cuboid-shaped voxels. In doing so, they ignore the natural polar representation formed by the lidar sectors.
Using cuboid-shaped voxels restricts them to performing convolutions on the minimal rectangular region enclosing the point cloud sector which wastes both computation and memory. As shown in
Fig. 1, a large portion of the enclosed rectangular region remains empty.
∗work done while interning at Motional 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Left: An illustration of streaming lidar point clouds on bird’s eye view. Lidar point clouds arrive as wedge-shape sectors (shown in gray masks) as the scanner rotates. Previous methods, Han et al.[13] and STROBE[11], represent the sectors using rectangular regions, wasting half of memory and computation for empty regions. Ours represents the sectors as wedge-shape regions using a polar grid. Right: Comparison of different streaming methods wrt. Panoptic Quality vs End-to-End
Lantency as we slice the full sweep into n = 1, 2, 4, 8, 16, 32 sectors using the NuScenes[3] val split.
The end-to-end latency includes 50/n ms for LiDAR scan and the total runtime of the algorithms.
Another challenge associated with streaming perception models is the limited view of the scene observed by each sector. Objects close to the ego-vehicle can often be fragmented across multiple sectors as shown by the car highlighted in green in Fig.1. Han et al.[13] proposes to increase the context available to the model by maintaining a recurrent memory across consecutive sectors.
STROBE [11] also aggregates representations from the previous sectors by maintaining full-sweep feature maps across multiple scales. However, both these solutions add extra computation.
In this work, we propose to encode individual point cloud sectors using polar pillars. Polar pillars naturally address the inefﬁciency of existing streaming approaches by representing the point cloud sectors as more compact wedge-shaped regions as shown in Fig. 1. Further, we propose a simple minimal-latency approach to enhance the context available to the model by simply padding the representation of the neighboring sectors across multiple strides of the backbone. Using polar pillars allows us to pad features from the preceding sector of the current scan and/or the following sector from the previous scan, no matter how many sectors the full sweep is divided into.
The polar BEV representation has recently started gaining attention in the lidar perception literature primarily because it balances the points across grid cells. In fact, polar grid outperforms the cartesian grid on the lidar segmentation task [36, 37]. However, the detection peformance on a polar grid still lags the cartesian grid [1, 5, 23]. This is because of the distortion the objects undergo when this representation is ultimately unfolded to a rectangular representation to enable the use of convolutional layers. The object represented by the green box in Fig. 2 shows an example of this distortion. Further, the distortion increases with range as the pillars progressively become larger. This makes a polar representation not compatible with the translation-invariance property of convolution.
In this work, we propose several techniques to address the distortion problem described above.
We ﬁrst propose a Feature Undistortion module which transforms the polar representation into a canonical Cartesian representation (as shown in Fig. 2) for classiﬁcation branch. Next, we propose using the Range Stratiﬁed Convolution&Normalization layers on the regression branches of the detection head. These layers apply different convolution kernels and normalization based on range (Fig.2) to cater to the changing pillar sizes in a polar grid. Our proposed model closes the gap on 3D object detection models using cartesian representations without adding any signiﬁcant latency.
Finally, we train multitasking streaming models that do simultaneous 3D object detection, lidar segmentation and panoptic segmentation, for the ﬁrst time in literature. Results on the nuScenes dataset show that our proposed model PolarStream outperforms all streaming methods in both panoptic quality and speed. PolarStream also stays competitive with the top-performing lidar perception methods on the nuScenes leaderboard while being at least twice as fast as the rest. We do several ablation studies and extensive analysis to show the effectiveness of PolarStream.
In summary, our contributions are: 2
• An efﬁcient streaming based lidar perception models using a polar grid.
• Multi-scale context padding: an efﬁcient approach to enhance the context of streaming lidar perception models
• Several improvements to the core problem of applying convolutions on a polar grid: Feature
Undistortion, Range Stratiﬁed Convolution&Normalization all add minimal latency to our model. 2