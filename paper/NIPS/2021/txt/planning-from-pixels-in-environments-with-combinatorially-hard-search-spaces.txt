Abstract
The ability to form complex plans based on raw visual input is a litmus test for current capabilities of artiﬁcial intelligence, as it requires a seamless combination of visual processing and abstract algorithmic execution, two traditionally separate areas of computer science. A recent surge of interest in this ﬁeld brought advances that yield good performance in tasks ranging from arcade games to continuous control; these methods however do not come without signiﬁcant issues, such as limited generalization capabilities and difﬁculties when dealing with combina-torially hard planning instances. Our contribution is two-fold: (i) we present a method that learns to represent its environment as a latent graph and leverages state reidentiﬁcation to reduce the complexity of ﬁnding a good policy from exponential to linear (ii) we introduce a set of lightweight environments with an underlying discrete combinatorial structure in which planning is challenging even for humans.
Moreover, we show that our methods achieves strong empirical generalization to variations in the environment, even across highly disadvantaged regimes, such as
“one-shot” planning, or in an ofﬂine RL paradigm which only provides low-quality trajectories.
Figure 1: Planning from Pixels with Graph Search. Our method leverages learned latent dynamics to efﬁciently build and search a graph representation of the environment. Resulting policies show unrivaled performance across a distribution of hard combinatorial tasks. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
Decision problems with an underlying combinatorial structure pose a signiﬁcant challenge for a learning agent, as they require both the ability to infer the true low-dimensional state of the environment and the application of abstract reasoning to master it. A traditional approach for common logic games, given that a simulator or a model of the game are available, consists in applying a graph search algorithm to the state diagram, effectively simulating several trajectories to ﬁnd the optimal one. As long as the state space of the game grows at a polynomial rate with respect to the planning horizon, the solver is able to efﬁciently ﬁnd the optimal solution to the problem. Of course, when this is not the case, heuristics can be introduced at the expense of optimality of solutions.
Learned world models [17, 18] can learn to map complex observations to a lower-dimensional latent space and retrieve an approximate simulator of an environment. However, while the continuous structure of the latent space is suitable for training reinforcement learning agents [12, 19] or applying heuristic search algorithms [38], it also prevents a straightforward application of simpler graph search techniques that rely on identifying and marking visited states.
Our work follows naturally from the following insight: a simple graph search might be sufﬁcient for solving visually complex environments, as long as a world model is trained to realize a suitable structure in the latent space. Moreover, the complexity of the search can be reduced from exponential to linear by reidentifying visited latent states.
The method we propose is located at the intersection between classical planning, representation learning and model-based reinforcement learning. It relies on a novel low-dimensional world model trained through a combination of opposing losses without reconstructing observations. We show how learned latent representations allow a dynamics model to be trained to high accuracy, and how the dynamics model can then be used to reconstruct a latent graph representing environment states as vertices and transitions as edges. The resulting latent space structure enables powerful graph search algorithms to be deployed for planning with minimal modiﬁcations, solving challenging combinatorial environments from pixels. We name our method PPGS as it Plans from Pixels through
Graph Search.
We design PPGS to be capable of generalizing to unseen variations of the environment, or equivalently across a distribution of levels [13]. This is in contrast with traditional benchmarks [7], which require the agent to be trained and tested on the same ﬁxed environment.
We can describe the main contributions of this paper as follows: ﬁrst, we introduce a suite of environments that highlights a weakness of modern reinforcement learning approaches, second, we introduce a simple but principled world model architecture that can accurately learn the latent dynamics of a complex system from high dimensional observations; third, we show how a planning module can simultaneously estimate the latent graph for previously unseen environments and deploy a breadth ﬁrst search in the latent space to retrieve a competitive policy; fourth, we show how combining our insights leads to unrivaled performance and generalization on a challenging class of environments. 2 Method
For the purpose of this paper, each environment can be modeled as a family of fully-observable deterministic goal-conditioned Markov Decision Processes with discrete actions, that is the 6-tuples
{(S, A, T, G, R, γ)i}1...n where Si is the state set, Ai is the action set, Ti is a transition function
Ti : Si × Ai → Si, Gi is the goal set and Ri is a reward function Ri : Si × Gi → R and γi is the discount factor. We remark that each environment can also be modeled as a BlockMDP [14] in which the context space X corresponds to the state set Si we introduced.
In particular, we deal with families of procedurally generated environments. We refer to each of the n elements of a family as a level and omit the index i when dealing with a generic level. We assume that state spaces and action spaces share the same dimensionality across all levels, that is |Si| = |Sj| and |Ai| = |Aj| for all 0 ≤ i, j ≤ n.
In our work the reward simpliﬁes to an indicator function for goal achievement R(s, g) = 1s=g with
G ⊆ S. Given a goal distribution p(g), the objective is that of ﬁnding a goal-conditioned policy πg that maximizes the return 2
Figure 2: Architecture of the world model. A convolutional encoder extracts latent state representa-tions from observations, while a forward model and an inverse model reconstruct latent dynamics by predicting state transitions and actions that cause them. The notation is introduced in Sec. 2.1 (cid:34)
Jπ = E g∼p(g)
E
τ ∼p(τ |πg) (cid:35)
γtR(st, g) (cid:88) t (1) t=1 sampled from the policy. where τ ∼ p(τ |πg) is a trajectory (st, at)T
Our environments of interest should challenge both perceptive and reasoning capabilities of an agent.
In principle, they should be solvable through extensive search in hard combinatorial spaces. In order to master them, an agent should therefore be able to (i) identify pairs of bisimilar states [43], (ii) keep track of and reidentify states it has visited in the past and (iii) produce highly accurate predictions for non-trivial time horizons. These factors contribute to making such environments very challenging for existing methods. Our method is designed in light of these necessities; it has two integral parts, the world model and the planner, which we now introduce. 2.1 World Model
The world model relies solely on three jointly trained function approximators: an encoder, a forward model and an inverse model. Their overall orchestration is depicted in Fig. 2 and described in the following. 2.1.1 Encoder
Mapping highly redundant observations from an environment to a low-dimensional state space Z has several beneﬁts [17, 18]. Ideally, the projection should extract the compressed “true state” of the environment and ignore irrelevant visual cues, discarding all information that is useless for planning.
For this purpose, our method relies on an encoder hθ, that is a neural function approximator mapping each observed state s ∈ S and a low-dimensional representation z ∈ Z (embedding). While there are many suitable choices for the structure of the latent space Z, we choose to map observations to points on an d-dimensional hypersphere taking inspiration from Liu et al. [29]. 2.1.2 Forward Model
In order to plan ahead in the environment, it is crucial for an agent to estimate the transition function T .
In fact, if a mapping to a low-dimensional latent space Z is available, learning directly the projected transition function TZ : Z × A → Z can be largely beneﬁcial [17, 18]. The deterministic latent transition function TZ can be learned by a neural function approximator fφ so that if T (st, at) = st+1, then fφ(hθ(st), at) := fφ(zt, at) = hθ(st+1). We refer to this component as forward model.
Intuitively, it can be trained to retrieve the representation of the state of the MDP at time t + 1 given the representation of the state and the action taken at the previous time step t.
Due to the Markov property of the environment, an initial state embedding zt and the action sequence (at, . . . , at+k) are sufﬁcient to to predict the latent state at time t + k, as long as zt successfully captures all relevant information from the observed state st. The amount of information to be 3
embedded in zt and to be retained in autoregressive predictions is, however, in most cases, prohibitive.
Take for example the case of a simple maze: zt would have to encode not only the position of the agent, but, as the predictive horizon increases, most of the structure of the maze.
Invariant Structure Recovery To allow the encoder to only focus on local information, we adopt an hybrid forward model which can recover the invariant structures in the environment from previous observations. The function that the forward model seeks to approximate can then include an additional input: fφ(zt, at, sc) = zt+1, where sc ∈ S is a generic observation from the same environment and level. Through this context input the forward model can retrieve information that is constant across time steps (e.g. the location of walls in static mazes). In practice, we can use randomly sampled observation from the same level during training and use the latest observation during evaluation.
This choice allows for more accurate and structure-aware predictions, as we show in the ablations in
Suppl. A.
Given a trajectory (st, at)T t=1, the forward model can be trained to minimize some distance measure between state embeddings (zt+1)1...T −1 = (hθ(st+1))1...T −1 and one-step predictions (fφ(hθ(st), at, sc))1...T −1. In practice, we choose to minimize a Monte Carlo estimate of the ex-pected Euclidean distance over a ﬁnite time horizon, a set of trajectories and a set of levels. When training on a distribution of levels p(l), we extract K trajectories of length H from each level with a uniform random policy π and we minimize
LFW = E l∼p(l) (cid:20) 1
H − 1
H−1 (cid:88) h=1
E ah∼π (cid:104) (cid:107)fφ(zl h, ah, sl c) − zl h+1(cid:107)2 2 (cid:105)(cid:21) (2) where the superscript indicates the level from which the embeddings are extracted. 2.1.3
Inverse Model and Collapse Prevention
Unfortunately, the loss landscape of Equation 2 presents a trivial minimum in case the encoder collapses all embeddings to a single point in the latent space. As embeddings of any pair of states could not be distinguished in this case, this is not a desirable solution. We remark that this is a known problem in metric learning and image retrieval [8], for which solutions ranging from siamese networks [9] to using a triplet loss [22] have been proposed.
The context of latent world models offers a natural solution that isn’t available in the general embedding problem, which consists in additionally training a probabilistic inverse model pω(at | zt, zt+1) such that if TZ(zt, at) = zt+1, then pω(at | zt, zt+1) > pω(ak | zt, zt+1)∀ak (cid:54)= at ∈ A.
The inverse model, parameterized by ω, can be trained to predict the action at that causes the latent transition between two embeddings zt, zt+1 by minimizing multi-class cross entropy.
H−1 (cid:88) (cid:20) (cid:104)
− log pω(ah | zl h, zl h+1) (cid:105)(cid:21)
. (3)
LCE = E l∼p(l) 1
H − 1
E ah∼π h=1
Intuitively, LCE increases as embeddings collapse, since it becomes harder for the inverse model to recover the actions responsible for latent transitions. For this reason, it mitigates unwanted local minima. Moreover, it is empirically observed to enforce a regular structure in the latent space that eases the training procedure, as argued in Sec. A of the Appendix. We note that this loss plays a similar role to the reconstruction loss in Hafner et al. [18]. However, LCE does not force the encoder network to embed information that helps with reconstructing irrelevant parts of the observation, unlike training methods relying on image reconstruction [11, 17–20].
While LCE is sufﬁcient for preventing collapse of the latent space, a discrete structure needs to be recovered in order to deploy graph search in the latent space. In particular, it is still necessary to deﬁne a criterion to reidentify nodes during the search procedure, or to establish whether two embeddings (directly encoded from observations or imagined) represent the same true low-dimensional state.
A straightforward way to enforce this is by introducing a margin ε, representing a desirable minimum distance between embeddings of non-bisimilar states [43]. A third and ﬁnal loss term can then be introduced to encourage margins in the latent space:
Lmargin = E l∼p(l) (cid:20) 1
H − 1
H−1 (cid:88) h=1 (cid:16) max 0, 1 − 4 (cid:107)zl h(cid:107)2 2 h+1 − zl
ε2 (cid:17)(cid:21)
. (4)
Figure 3: Overview of latent-space planning. One-shot planning is possible by (i) embedding the current observation and goal to the latent space and (ii) iteratively growing a latent graph until a vertex is reidentiﬁed with the goal.
We then propose to reidentify two embeddings as representing the same true state if their Euclidean distance is less than ε 2 .
Adopting a latent margin effectively constrains the number of margin-separated states that can be represented on an hyperspherical latent space. However, this quantity is lower-bounded by the kissing number [41], that is the number of non-overlapping unit-spheres that can be tightly packed around one d dimensional sphere. The kissing number grows exponentially with the dimensionality d. Thus, the capacity of our d-dimensional unit sphere latent space (d = 16 in our case with margin ε = 0.1) is not overly restricted.
The world model can be trained jointly and end-to-end by simply minimizing a combination of the three loss functions:
L = αLFW + βLCE + Lmargin. (5)
To summarize, the three components are respectively encouraging accurate dynamics predictions, regularizing latent representations and enforcing a discrete structure for state reidentiﬁcation. 2.2 Planning Regimes
A deterministic environment can be represented as a directed graph G whose vertices V represent states s ∈ S and whose edges E encode state transitions. An edge from a vertex representing a state s ∈ S to a vertex representing a state s(cid:48) ∈ S is present if and only if T (s, a) = s(cid:48) for some action a ∈ A, where T is the state transition function of the environment. This edge can then be labelled by action a. Our planning module relies on reconstructing the latent graph, which is a projection of graph G to the latent state Z.
In this section we describe how a latent graph can be build from the predictions of the world model and efﬁciently searched to recover a plan, as illustrated in Fig. 3. This method can be used as a one-shot planner, which only needs access to a visual goal and the initial observation from a level. When iterated and augmented with online error correction, this procedure results in a powerful ap-proach, which we refer to as full planner, or simply as
PPGS.
One-shot Planner Breadth First Search (BFS) is a graph search algorithm that relies on a LIFO queue and on marking visited states to ﬁnd an optimal path O(V + E) steps. Its simplicity makes it an ideal candidate for solving combinatorial games by exploring their latent graph. If the number of reachable states in the environment grows polynomially, the size of the graph to search will increase at a modest rate and the method can be applied efﬁciently. 5
Figure 4: Number of leaf vertices when planning in ProcgenMaze, averaged over 100 levels, with 90% conﬁdence inter-vals.
We propose to execute a BFS-like algorithm on the latent graph, which is recovered by autoregressively simulating all transitions from visited states. As depicted in Fig. 3, at each step, the new set of leaves L is retrieved by feeding the leaves from the previous iteration through the forward model fφ. The efﬁciency of the search process can be improved as shown in Fig. 4, by exploiting the margin ε enforced by equation 4 to reidentify states and identify loops in the latent graph. We now provide a simpliﬁed description of the planning method in Algorithm 1, while details can be found in
Suppl. C.2.
Algorithm 1 Simpliﬁed one-shot PPGS
Input: Initial observed state s1, visual goal g, model parameters θ, φ 1: z1, zg = hθ(s1), hθ(g) 2: L, V = {z1} 3: for TM AX steps do 4: 5: 6: 7: 8: 9: 10: end for
L = {fφ(z, a, s1) : ∃z ∈ L, a ∈ A} if z∗ ∈ L can be reidentiﬁed with zg then return action sequence from z1 to z∗ end if
L = L \ V
V = V ∪ L (cid:46) project to latent space Z (cid:46) sets of leaves and visited vertices (cid:46) grow graph (cid:46) reidentify and discard visited vertices (details in Suppl. C.2) (cid:46) update visited vertices
Full Planner The one-shot variant of PPGS largely relies on highly accurate autoregressive predictions, which a learned model cannot usually guarantee. We mitigate this issue by adopting a model predictive control-like approach [15]. PPGS recovers an initial guess on the best policy (ai)1,...,n simply by applying one-shot PPGS as described in the previous paragraph and in Algorithm 2. It then applies the policy step by step and projects new observations to the latent space. When new observations do not match with the latent trajectory, the policy is recomputed by applying one-shot PPGS from the latest observation. This happens when the autoregressive prediction of the current embedding (conditioned on the action sequence since the last planning iteration) can not be reidentiﬁed with the embedding of the current observation. Moreover, the algorithm stores all observed latent transitions in a lookup table and, when replanning, it only trusts the forward model on previously unseen observation/action pairs. A detailed description can be found in Suppl. C.2. 3 Environments
In order to benchmark both perception and abstract reasoning, we empirically show the feasibility of our method on three challenging procedurally generated environments. These include the Maze environment from the procgen suite [13], as well as DigitJump and IceSlider, two combinatorially hard environments which stress the reasoning capabilities of a learning agent, or even of an human player. In the context of our work, the term “combinatorial hardness” is used loosely. We refer to an environment as "combinatorially hard" if only very few of the exponentially many trajectories actually lead to the goal, while deviating from them often results in failure (e.g. DigitJump or IceSlider).
Hence, some “intelligent” search algorithm is required. In this way, the process of retrieving a successful policy resembles that of a graph-traversing algorithm. The last two environments are made available in a public repository [1], where they can also be tested interactively. More details on their implementation are included in Suppl. D.
ProcGenMaze The ProcgenMaze environment consists of a family of procedurally generated 2D mazes. The agent starts in the bottom left corner of the grid and needs to reach a position marked by a piece of cheese. For each level, an unique shortest solution exists, and its length is usually distributed roughly between 1 and 40 steps. This environment presents signiﬁcant intra-level variability, with different sizes, textures, and maze structures. While retrieving the optimal solution in this environment is already a non-trivial task, its dynamics are uniform and actions only cause local changes in the observations. Moreover, ProcgenMaze is a forgiving environment in which errors can always be recovered from. In the real world, many operations are irreversible, for instance, cutting/breaking objects, gluing parts, mixing liquids, etc. Environments containing remote controls, for example, show non-local effects. We use these insights to choose the additional environments. 6
ProcgenMaze
DigitJump
IceSlider
Figure 5: Environments. Initial observations and one-shot PPGS’s solution (arrows) of a random level of each of the three environments. ProcgenMaze is from [13]. DigitJump and IceSlider are proposed by us and can be accessed at [1].
IceSlider
IceSlider is in principle similar to ProcgenMaze, since it also consists of procedurally generated mazes. However, each action propels the agent in a direction until an obstacle (a rock or the borders of the environments) is met. We generate solvable but unforgiving levels that feature irreversible transitions, that, once taken, prevent the agent from ever reaching the goal.
DigitJump DigitJump features a distribution of randomly generated levels which consist of a 2D 8x8 grid of handwritten digits from 1 to 6. The agent needs to go from the top left corner to the bottom right corner. The 4 directional actions are available, but each of them causes the agent to move in that directions by the number of steps expressed by the digit on the starting cell. Therefore, a single action can easily transport the player across the board. This makes navigating the environment very challenging, despite the reduced cardinality of the state space. Moreover, the game presents many cells in which the agent can get irreversibly stuck. 4