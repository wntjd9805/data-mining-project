Abstract
It was estimated that the world produced 59ZB (5.9 × 1013GB) of data in 2020, resulting in the enormous costs of both data storage and transmission. Fortunately, recent advances in deep generative models have spearheaded a new class of so-called "neural compression" algorithms, which signiﬁcantly outperform traditional codecs in terms of compression ratio. Unfortunately, the application of neural compression garners little commercial interest due to its limited bandwidth; there-fore, developing highly efﬁcient frameworks is of critical practical importance. In this paper, we discuss lossless compression using normalizing ﬂows which have demonstrated a great capacity for achieving high compression ratios. As such, we introduce iFlow, a new method for achieving efﬁcient lossless compression. We
ﬁrst propose Modular Scale Transform (MST) and a novel family of numerically invertible ﬂow transformations based on MST. Then we introduce the Uniform
Base Conversion System (UBCS), a fast uniform-distribution codec incorporated into iFlow, enabling efﬁcient compression. iFlow achieves state-of-the-art compres-sion ratios and is 5× quicker than other high-performance schemes. Furthermore, the techniques presented in this paper can be used to accelerate coding time for a broad class of ﬂow-based algorithms. 1

Introduction
The volume of data, measured in terms of IP trafﬁc, is currently witnessing an exponential year-on-year growth [13]. Consequently, the cost of transmitting and storing data is rapidly becoming prohibitive for service providers, such as cloud and streaming platforms. These challenges increas-ingly necessitate the need for the development of high-performance lossless compression codecs.
One promising solution to this problem has been the development of a new class of so-called “neural compression” algorithms [30, 38, 18, 4, 17, 37, 26, 31, 6, 41]. These methods typically posit a deep probabilistic model of the data distribution, which, in combination with entropy coders, can be used to compress data with the minimal codelength bounded by the negative log-likelihood [29]. However, despite reliably improved compression performance compared to traditional codecs [14, 33, 9, 34], meaningful commercial applications have been limited by impractically slow coding speed.
In this paper, we focus on developing approaches with deep probabilistic models based on normalizing
ﬂows [11, 25, 32, 27, 7, 28, 23]. A normalizing ﬂow admits a learnable bijective mapping between input data and a latent variable representation. In this paradigm, inputs can be compressed by
ﬁrst transforming data to latent variables, with the resulting output encoded by a prior distribution.
Compared to other classes of generative models, normalizing ﬂows typically perform best in both tasks of probability density estimation (as compared with variational autoencoders [24, 15, 8]) and inference speed (as compared with autoregressive factorizations [35, 39, 22]). This suggests that compression with ﬂows can jointly achieve high compression ratios along with fast coding times. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
IDF(++), iVPF encoding
𝐳
Quantize
Flow layer
Quantize
Flow layer
𝑧 = Φ ⋅ 𝑥 + Θ (Φ = 1 or ∏Φ = 1)
⌊Θ⌉
Φ ⋅ 𝑥
𝑧
Direct/MAT
𝑥
Direct: 𝑧 ← 𝑥 Φ = 1
MAT: 𝑧 ← Φ ⋅ 𝑥 ∏Φ = 1
LBB
De 𝑁 0, 𝜎𝑓′ 2 encoding
𝑓 𝑥
𝑧
𝐳
𝑥
𝑓−1 𝑧
En 𝑁 0, 𝜎2 de/encoding iFlow (Proposed)
𝑓  𝑥𝑙 encoding
𝑧
𝑓  𝑥ℎ
𝐳
MST
𝑥 𝑥𝑙  𝑥ℎ
MST: 𝑧 ← 𝑎 ⋅ 𝑥 −  𝑥𝑙 + 𝑓  𝑥𝑙 ,
𝑎 =
𝑓  𝑥ℎ −𝑓  𝑥𝑙  𝑥ℎ−  𝑥𝑙
𝑧 = 𝑓 𝑥
Flow layer
𝑧 = 𝑓 𝑥 iFlow layer
MST de/encoding
Flow layer iFlow layer
MST decoding
Dequant decoding
Dequant decoding
Dequant
𝐱
𝐱
𝐱
Compression ratio
Compression bandwidth
Figure 1: Illustration of IDF(++), iVPF, LBB and the proposed iFlow method. Flow layers with darker color denote higher expressive power. The top-right of each model illustration shows the key procedure of exact bijections between ¯x and ¯z. The compression ratio and bandwidth are listed below.
Unfortunately, lossless compression requires discrete data for entropy coding, and the continuous bijections of normalizing ﬂows would not guarantee discrete latent variables. As such, transformations would require discretization, resulting in a loss of information [41, 3]. To resolve this issue, Integer
Discrete Flows (IDF) [18, 4] (Fig. 1, left) proposed an invertible mapping between discrete data and latent variables. Similarly, iVPF [41] achieves a discrete-space bijection using volume-preserving
ﬂows. However, the above models must introduce constraints on ﬂow layers to ensure a discrete-space bijection, which limits the expressivity of the transformation. Local Bits-Back Coding (LBB)
[17] (Fig. 1, middle) was the ﬁrst approach to succeed in lossless coding with the ﬂexible family of continuous ﬂows. It resolves the information loss by coding numerical errors with the rANS coder [12]. However, the coder is extraordinarily slow, making the method impractical.
In this paper, we introduce iFlow: a numerically invertible ﬂow-based neural compression codec that bridges the gap between high-ﬁdelity density estimators and practical coding speed. To achieve this we introduce two novelties: Modular Scale Transform (MST) and Uniform Base Conversion Systems (UBCS). MST presents a ﬂexible family of bijections in discrete space, deriving an invertible class of
ﬂows suitable for lossless compression. UBCS, built on the uniform distribution, permits a highly efﬁcient entropy coding compatible with MST. The main ideas are illustrated in Fig. 1(right). We test our approach across a range of image datasets against both traditional and neural compression techniques. Experimentally we demonstrate that our method achieves state-of-the-art compression ratios, with coding time 5× quicker than that of the next-best scheme, LBB. 2 Numerically Invertible Flows and Lossless Compression
Let f : X → Z be our normalizing ﬂow, which we build as a composition of layers such that f = fL ◦ ... ◦ f2 ◦ f1. Deﬁning d-dimensional y0 = x ∈ X and yL = z ∈ Z, the latents can be computed as yl = fl(yl−1), with pX (x) calculated according to log pX (x) = log pZ(z) +
L (cid:88) l=1 log | det Jfl (yl−1)|, (1) where Jfl (x) is the Jocabian matrix of the transformation fl at yl−1. In this paper, we use log to denote base 2 logarithms. Training the ﬂow should minimize the negative log-likelihood − log pX (x).
The inverse ﬂow is deﬁned as f −1 = f −1
L , such that f −1(z) = x. There are many types of invertible bijections, fl. Popular choices include element-wise, autoregressive (e.g. coupling layers) [41] and 1 × 1 convolution transformations [25].
◦ ... ◦ f −1 1
To perform lossless compression with normalizing ﬂows, the input data should be transformed to latent variables, which are then encoded with a prior distribution. As data and encoded bits should be in binary format, x and z must be discrete, with a bijection between discrete inputs and outputs 2
established. However, as discussed, this operation is usually intractable with popular classes of transformations used in continuous ﬂows due to the numerical errors induced by discretization.
In this section, we introduce a novel algorithm derived from continuous ﬂows that allows us to achieve an exact bijiective mapping between discrete x and discrete z. We present the general idea of a ﬂow layer’s numerical invertibility in discrete space in Sec. 2.1. We then introduce our ﬂow layers in Sec. 2.2 and 2.3, with their application to compression discussed in Sec. 2.4 and 2.5. 2.1
Invertibility of Flows in Discrete Space
To discretize the data, we follow [41] and adopt k-precision quantization to assign ﬂoating points to discretization bins such that
¯x = (cid:98)2k · x(cid:99) 2k
, (2) where (cid:98)·(cid:99) is the ﬂoor function. The associated quantization error is bounded by |¯x − x| < 2−k. Any d-dimensional x can be quantized into bins with volume δ = 2−kd. The probability mass of ¯x can then be approximated by P (¯x) = p(¯x)δ, such that the theoretical codelength is − log(p(¯x)δ).
Denote our proposed numerically invertible ﬂow (iFlow) according to ¯f = ¯fL ◦ ... ◦ ¯f1. The input and output should be subject to k-precision quantization, where each layer should be appropriately invertible such that ¯yl−1 ≡ ¯f −1 ( ¯fl(¯yl−1)). Denoting ¯y0 = x and ¯z = ¯yL, it is further expected that
¯z = ¯f (x) and x ≡ ¯f −1(¯z). We will show in the following subsections that ¯f can be derived from any continuous ﬂow f in such a way that the error between ¯f (x) and f (x) is negligible. l 2.2 Numerically Invertible Element-Wise Flows
We begin by describing our approach for element-wise ﬂows z = f (x), where f is a monotonic element-wise transformation. For notation simplicity, we assume that the ﬂow contains just one layer. In most cases, f does not present a unique inverse in discrete space, such that f −1(¯z) (cid:54)= x.
In what follows, we introduce an approach to derive an invertible, discrete-space operation from any element-wise ﬂow transformation. For simplicity, we denote the inputs and output pairs of a continuous ﬂow as x and z = f (x); and we denote the k-precision quantized input-output pairs of iFlow as ¯x and ¯z = ¯f (¯x). 2.2.1 Scale Flow
Algorithm 1 Modular Scale Transform (MST): Numerically Invertible Scale Flow f (x) = R/S · x.
Forward MST: ¯z = ¯f (¯x). 1: ˆx ← 2k · ¯x; 2: Decode rd using U (0, R); ˆy ← R · ˆx + rd; 3: ˆz ← (cid:98)ˆy/S(cid:99), re ← ˆy mod S; 4: Encode re using U (0, S); 5: return ¯z ← ˆz/2k.
Inverse MST: ¯x = ¯f −1(¯z). 1: ˆz ← 2k · ¯z; 2: Decode re using U (0, S); ˆy ← S · ˆz + re; 3: ˆx ← (cid:98)ˆy/R(cid:99), rd ← ˆy mod R; 4: Encode rd using U (0, R); 5: return ¯x ← ˆx/2k.
We begin with the scale transformation deﬁned as z = f (x) = a · x, (a > 0), from which we are able to build more complex ﬂow layers. The input can be converted to an integer with ˆx = 2k · ¯x, and the latent variable ¯z can be recovered from integer ˆz by ¯z = ˆz/2k. Inspired by MAT in iVPF [41], we
S , where R, S ∈ N. Denote ˆy = R · ˆx + rd where
ﬁrst approximate a with a fractional such that a ≈ R rd is sampled uniformly from {0, 1, ..., R − 1}, we can obtain ˆz with ˆz = (cid:98)ˆy/S(cid:99), and the remainder re = ˆy mod R can be encoded to eliminate the error. It is clear that (ˆx, rd) ↔ ˆy ↔ (ˆz, re) are bijections.
Let U (0, A) be the uniform distribution such that P (s) = 1
A , s ∈ {0, 1, ..., A − 1}. The numerically invertible scale ﬂow ¯f is displayed in Alg. 1. As the modular operation is essential to Alg. 1, we name it the Modular Scale Transform (MST). Setting S and R = round(S · a) to be large, we observe that the following propositions hold.
Proposition 1. [41] |¯z − f (¯x)| < O(S−1, 2−k).
Proposition 2. The codelength of MST is Lf (¯x, ¯z) = log S − log R ≈ − log |a| = − log |f (cid:48)(¯x)|. 3
Proposition 1 establishes that the error is small if S and k are large. Proposition 2 demonstrates that the codelength is almost exactly the log-Jocabian of f . The above properties and the correctness of the algorithm are discussed in the Appendix. Further note that, with the exception of the usual encoding and decoding processes, MST’s operations can be parallelized, resulting in minimal additional overhead. In contrast, MAT’s operation in iVPF [41] only deals with volume-preserving afﬁne transform and is performed sequentially along dimensions, limiting its usage and efﬁciency. 2.2.2 General Element-wise Flow
Algorithm 2 Numerically Invertible Element-wise Flows
Forward: ¯z = ¯f (¯x). 1: Get ¯xl, ¯xh, ¯zl, ¯zh given ¯x, so that ¯x ∈ [¯xl, ¯xh); 2: Set large S; get R with Eq. (4); ¯x(cid:48) ← ¯x − ¯xl; 3: Get ¯z(cid:48) with forward MST in Alg. 1, given input ¯x(cid:48)
Inverse: ¯x = ¯f −1(¯z). 1: Get ¯xl, ¯xh, ¯zl, ¯zh given ¯z, so that ¯z ∈ [¯zl, ¯zh); 2: Set large S; get R with Eq. (4); ¯z(cid:48) ← ¯z − ¯zl; 3: Get ¯x(cid:48) with inverse MST in Alg. 1, given input ¯z(cid:48) and coefﬁcients R, S; 4: return ¯z = ¯z(cid:48) + ¯zl. (cid:46) ¯z ∈ [¯zl, ¯zh). and coefﬁcients R, S; 4: return ¯x = ¯x(cid:48) + ¯xl. (cid:46) ¯x ∈ [¯xl, ¯xh).
For simplicity, we assume the non-linear f is monotonically increasing. For a monotonically decreasing f , we simply invert the sign: −f .
MST in Alg. 1 works with linear transformations, and is incompatible with non-linear functions.
Motivated by linear interpolation, we can approximate the non-linear ﬂow with a piecewise linear function. In general, consider input ¯x to be within an interval ¯x ∈ [¯xl, ¯xh)1 and ¯zl = f (¯xl), ¯zh = f (¯xh). The linear interpolation of f is then finp(x) =
¯zh − ¯zl
¯xh − ¯xl (¯x − ¯xl) + ¯zl. (3)
It follows that ¯z can be derived from MST with input ¯x− ¯xl followed by adding ¯zl. Furthermore, ¯z and
¯x can be recovered with the corresponding inverse transformation. To preserve monotonicity, ¯z must be within interval ¯z ∈ [¯zl, ¯zh). Denoting the invertible linear ﬂow derived from finp as ¯finp, it must hold that ¯finp(¯xl) ≥ ¯zl, ¯finp(¯xh −2−k) ≤ ¯zh −2−k. As we use MST in Alg. 1, we observe that when is approximated with R/S, the minimum possible value of ¯finp(¯xl) is ¯zl (when rd = 0 in Line
¯zh−¯zl
¯xh−¯xl 2) and the maximum possible value of ¯finp(¯xh−2−k) is (cid:98)(R·2k(¯xh−¯xl−2−k)+R−1)/(S·2k)(cid:99)+¯zl (when rd = R − 1 in Line 2). Given large S, the largest possible value of R should be
R = (cid:4) (2k · (¯zh − ¯zl) − 1) · S + 1 2k · (¯xh − ¯xl) (cid:5). (4)
Another difﬁculty is in determining the correct interpolation interval [¯xl, ¯xh). One simple solution is to split the input domain into uniform intervals with length 2−h such that ¯xl = (cid:98)(2h · ¯x)/2h(cid:99), ¯xh =
¯xl + 2−h. However, for the inverse computation given ¯z, it is not easy to recover the interpolation interval as f −1(¯z) may not be within [¯xl, ¯xh). Instead, as ¯z ∈ [¯zh, ¯zl), the interval [¯xl, ¯xh) can be obtained via a binary search in which ¯xl is obtained with ¯z ∈ [f (¯xl), f (¯xh)). Another approach is to split the co-domain into uniform intervals such that ¯zl = (cid:98)(2h · ¯z)/2h(cid:99), ¯zh = ¯zl + 2−h, with
¯xl = f −1(¯zl), ¯xh = f −1(¯zh). In this case, determining [¯zl, ¯zh) during the inverse computation is simple. While for the forward pass, [¯zl, ¯zh) should be determined with a binary search such that ¯x ∈ [f −1(¯zl), f −1(¯zh)). In practice, we have simpler tricks to determine the correct interval
[¯xl, ¯xh), [¯zl, ¯xh) for both the forward and inverse computation, which can be found in the Appendix.
The general idea of the non-linear ﬂow adaptations are summarized in Alg. 2. Note that Alg. 2 can be used in any element-wise ﬂow including linear ﬂow. In Alg. 2, Proposition 2 holds such that Lf (¯x, ¯z) = − log(R/S) ≈ − log ¯zh−¯zl
≈ − log |f (cid:48)(¯x)|. It is possible to arrive at a similar
¯xh−¯xl conclusion in Proposition 1 where the corresponding error is |¯z − f (¯x)| < O(S−1, 2−k, 2−2h). 1All intervals must partition the domain of f . 4
2.3 Practical Numerically Invertible Flow Layers
Whilst one can use many types of complex transformations to build richly expressive ﬂow-based models, to ensure the existence and uniqueness of the inverse computation, these layers are generally constructed with element-wise transformations. In this subsection, we demonstrate the invertibility of discretized analogs to some of the most widely used ﬂow layers using the operators as described in
Sec. 2.2. Such ﬂows include autoregressive ﬂows [20] (including coupling ﬂows [10, 11, 16]) and 1 × 1 convolutional ﬂows [25]. 2.3.1 Autoregressive and Coupling Flows
Algorithm 3 Numerically Invertible Autoregressive Flow
Forward: ¯z = ¯f (¯x). 1: for i = m, ..., 1 do 2: 3: end for 4: return ¯z = [¯z1, ..., ¯zm](cid:62).
¯zi ← ¯fi(¯xi, ¯x<i) with Alg. 2 (forward);
Inverse: ¯x = ¯f −1(¯z). 1: for i = 1, ..., m do
¯xi ← ¯f −1 2: 3: end for 4: return ¯z = [¯z1, ..., ¯zm](cid:62). i (¯zi, ¯x<i) with Alg. 2 (inverse);
Supposing that the inputs and outputs, x and z, are split into m parts x = [x1, ..., xm](cid:62), z =
[z1, ..., zm](cid:62), the autoregressive ﬂow z = f (x) can be represented as zi = fi(xi; x<i), where fi(·; x<i) is the element-wise ﬂow (discussed in Sec. 2.2), conditioned on x<i. Now let ¯fi(·; x<i) denote the invertible element-wise ﬂow transformation as discussed in Alg. 1-2. Alg. 3 then illustrates the details of an invertible autoregressive ﬂow ¯f .
Propositions 1 and 2 hold in our discretized autoregressive transformation. In fact, the log-determinant of Jacobian is given by log | det Jf (x)| = (cid:80)m i=1 log | det Jfi(xi; x<i)|, and the expected codelength is simply Lf (¯x, ¯z) = (cid:80)m i=1 log | det Jfi (¯xi; ¯x<i)| = − log | det Jf (¯x)|.
When m = 2 and f1(x1) = x1, the autoregressive ﬂow is reduced to a coupling ﬂow, which is widely used in ﬂow-based models [10, 11, 16]. It is therefore trivially clear that the coupling ﬂow is additionally compatible with Alg. 3. i=1 Lf (¯xi, ¯zi) ≈ − (cid:80)m 2.3.2 1 × 1 Convolutional Flow 1 × 1 convolutional layers can be viewed as a matrix multiplication along a channel dimension [25].
Let ¯x, ¯z ∈ Rc be inputs and outputs along channels, and W ∈ Rc×c the weights of our network. The objective is to obtain ¯z = ¯f (¯x) where f (x) = Wx.
We use the ideas of iVPF [41] to achieve a numerically invertible 1 × 1 convolutional transformation.
In particular, we begin by performing an LU decomposition such that W = PLΛU.
It then follows that the 1 × 1 convolution is performed with successive matrix multiplications with U, Λ, L and P. In iVPF, the authors extensively discussed matrix multiplications with factors U, L and
P [41]. Meanwhile, one can view the matrix multiplication with Λ as a scale transform, such that f (x) = λ (cid:12) x, where (cid:12) is an element-wise multiplication and λ are the diagonal elements of
Λ. MST in Alg. 1 can then be applied. In such a case, it is clear that Proposition 1 and 2 hold for a 1 × 1 convolutional ﬂow. For Proposition 2, it is observed that Lf (¯x, ¯z) ≈ −sum(log λ) =
− log | det Λ| = − log | det W| = − log | det Jf (¯x)|. 2.4 Building Numerically Invertible Flows
Our ﬂow model is constructed as a composition of layers f = fL ◦ ... ◦ f2 ◦ f1, where each layer is a transformation of the type discussed in Sec. 2.2 and 2.3. Let us represent the resulting ﬂow as ¯f = ¯fL ◦ ... ◦ ¯f1, where ¯fl is a discretized transformation derived from the corresponding continuous f . It is clear that the quantized input ¯x(= ¯y0) and latent ¯z(= ¯yL) establish a bijection with successive transformations between discrete inputs and outputs. For the forward pass, ¯z is computed with ¯yl = ¯f (¯yl−1), l = 1, 2, ..., L; for the inverse pass, ¯x is recovered with the inverse
ﬂow ¯f −1 = ¯f −1
L such that ¯yl−1 = f −1
◦ ... ◦ ¯f −1 (¯yl). 1 l 5
For our resultant ﬂow model, we can draw similar conclusions as in Propositions 1 and 2. Firstly, the error of ¯z = ¯f (¯x) and z = f (¯x) is small, bounded by |¯z − z| < O(LS−1, L2−k, L2−2h). Secondly, the codelength is approximately Lf (¯x, ¯z) = (cid:80)L l=1 Lfl (¯yl−1, ¯yl) ≈ − (cid:80)L l=1 log | det Jfl(¯yl−1)|. 2.5 Lossless Compression with Flows via Bits-back Dequantization
Armed with our ﬂow model ¯f , performing lossless compression is straight-forward. For the encoding process, the latent is generated according to ¯z = ¯f (¯x). We then encode ¯z with probability pZ(¯z)δ.
For the decoding process, ¯z is decoded with pZ(¯z)δ, and ¯x is recovered with ¯f −1(¯z). The expected codelength is approximately − log(pX (¯x)δ) such that
L(¯x) ≈ − log(pZ(¯z)δ) −
L (cid:88) l=1 log | det Jfl (¯yl−1)| ≈ − log(pX (¯x)δ). (5)
However, we note that if k is large, − log δ = kd and the codelengths will also be large, resulting in a waste of bits. For what follows, we adopt the bits-back trick in LBB [17] to reduce the codelength. In particular, consider coding with input data x◦ ∈ Zd, where a k-precision noise vector ¯u ∈ [0, 1)d is decoded with q(¯u|x◦)δ and added to input data such that ¯x = x◦ + ¯u. In this way, ¯x is then encoded with our ﬂow ¯f . For the decoding process, x◦ is recovered by applying the inverse transformation.
We name this coding process Bits-back Dequantization, which is summarized in Alg. 4.
Algorithm 4 Lossless Compression with iFlow.
Encode x◦. 1: Decode ¯u using q(¯u|x◦)δ; 2: ¯z ← ¯f (x◦ + ¯u); 3: Encode ¯z using pZ (¯z)δ.
Decode. 1: Decode ¯z using pZ (¯z)δ; 2: ¯x ← ¯f −1(¯z), x◦ ← (cid:98)¯x(cid:99); 3: Encode ¯u = ¯x − x◦ using q(¯u|x◦)δ; 4: return x◦.
In practice, q(u|x◦) is constructed with a ﬂow model such that u = g((cid:15); x◦) where (cid:15) ∼ p((cid:15)).
¯u is decoded by ﬁrst decoding ¯(cid:15) with p(¯(cid:15))δ, and then applying ¯u = ¯g(¯(cid:15); x◦). Thus decoding ¯u involves − log(q(¯u|x◦)δ) bits. Overall, the expected codelength is exactly the dequantization lower bound [19] such that
L(x◦) ≈ (cid:88)
¯u q(¯u|x◦)δ[log(q(¯u|x◦)δ) − log(p(x◦ + ¯u)δ)]
≈ Eq(¯u|x◦)[log q(¯u|x◦) − log p(x◦ + ¯u)]. (6) 2.6 Extensions
With novel modiﬁcations, ﬂow models can be applied to the generation of various data types, obtaining superior performance [7, 28, 23]. These models can be used for improved lossless compression. In general, given input data x◦, these models generate intermediate data v with q(v|x◦), and the density of v is modelled with ﬂow model z = f (v). For generation, when v is generated with the inverse
ﬂow, x is generated with p(x◦|v). It is clear that p(x) can be estimated with variational lower bound such that log p(x◦) ≥ Eq(v|x◦)[log P (x◦|v) + log p(v) − log q(v|x◦)]. For lossless compression, x◦ can be coded with bits-back coding, which is similar with Alg. 4. In the encoding process, we ﬁrst decode ¯v with q(¯v|x◦)δ and then encode x◦ with P (x◦|¯v) (similar with Line 1 in Alg. 4-Encode).
We then obtain the prior ¯z = ¯f (¯v) (Line 2), before ﬁnally encoding ¯z with pZ(¯z)δ (Line 3). In the decoding process, ¯z is ﬁrstly decoded with pZ(¯z)δ (similar to Line 1 in Alg. 4-Decode), and then recovered ¯v and decoded x◦ with P (x◦|¯v) (Line 2). Finally, we encode using ¯v with q(¯v|x◦) [38].
The expected codelength is approximately
L(x◦) ≈ Eq(¯v|x◦)[log q(¯v|x◦) − log P (x◦|¯v) − log p(¯v)]. (7)
We introduce a selection of recent, state-of-the-art ﬂow-based models modiﬁed according to the above. Each model corresponds to a certain coding algorithm.
VFlow [7]. VFlow expands the input data dimension with variational data augmentation to resolve the bottleneck problem in the ﬂow model. In VFlow, v = [x◦ + u, r](u ∈ [0, 1)d), where u ∼ 6
qu(u|x◦), r ∼ qr(r|x◦ +u), is modelled with ﬂows gu, gr such that u = gu((cid:15)u; x◦), r = gr((cid:15)r; x◦ + u) ((cid:15)u, (cid:15)r are priors). Then we have q(v|x◦) = qu(u|x◦)qr(r|x◦ + u), P (x◦|v) = 1 (as v → (x◦ + u) → x◦). Thus for the encoding process, ¯(cid:15)u, ¯(cid:15)r are decoded. To construct ¯v we have
¯u = ¯gu(¯(cid:15)u; x◦), ¯r = ¯gr(¯(cid:15)r; x◦ + ¯u); and then ¯v is encoded with iFlow. For the decoding process, ¯v is decoded with the inverse iFlow, and then x◦, ¯u, ¯r is recovered with ¯v. Here ¯u, ¯r is encoded and x◦ is the decoded output. As VFlow achieves better generation results compared with general ﬂows, one would expect a better compression ratio with VFlow.
Categorical Normalizing Flow [28]. Categorical Normalizing Flows (CNF) succeed in mod-elling categorical data such as text, graphs, etc. Given categorical data x◦ = [x1, ..., xn], xi ∈
{1, 2, ..., C}n, v = [v1, ..., vn] is represented with word embeddings such that q(vi|xi) = qe(vi|µ(xi), σ(xi)), in which qe could be a Gaussian or logistic distribution. Then P (xi|vi) = i=1 q(vi|xi), P (x◦|v) = c=1 ˜p(c)q(vi|c) with ˜p being the prior over categories. Thus q(v|x◦) = (cid:81)n i=1 P (xi|vi), and x◦ can be coded with Alg. 8 given q(v|x◦), P (x◦|v) and the iFlow.
˜p(xi)q(vi|xi) (cid:80)C (cid:81)n 3 Uniform Base Conversion Systems
Algorithm 5 Uniform Base Conversion Systems
ENCODE s using U (0, R)(R < 2K ).
Input: symbol s, state c, bit-stream bs.
Output: new state c and bit-stream bs. 1: c ← c · R + s; 2: if c ≥ 2M +K then 3: bs.push_back(c mod 2K ); bits to bit-stream. c ← (cid:98) c 2K (cid:99); 4: 5: end if 6: return c, bs.
DECODE with U (0, R)(R < 2K ).
Input: state c, bit-stream bs.
Output: decoded s, new state c and bit-stream bs. 1: if c < 2M · R then 2: c ← 2K · c+bs.pop_back();
K bits from bit-stream and pop them. (cid:46) get last (cid:46) push K 3: end if 4: s ← c mod R; 5: c ← (cid:98)c/R(cid:99); 6: return s, c, bs.
The previous section demonstrates that coding with a uniform distribution is central to our algorithm.
Note that the distribution varies in each coding process, thus dynamic entropy coder is expected.
Compared to the Gaussian distribution used in LBB [17], a uniform distribution is simpler, yielding improved coding speed. As follows, we introduce our Uniform Base Conversion Systems (UBCS), which is easy to implement and the coding bandwidth is much greater than that of rANS [12].
UBCS is implemented based on a number-base conversion. The code state c is represented as an integer. For coding some symbol s ∈ {0, 1, ..., R − 1} with a uniform distribution U (0, R), the new state c(cid:48) is obtained by converting an R-base digit s to an integer such that c(cid:48) = E(c, s) = c · R + s. (8)
For decoding with U (0, R), given state c(cid:48), the symbol s and state c are recovered by converting the integer to an R-base digit such that s = c(cid:48) mod R, c = D(c(cid:48), s) = (cid:98) c(cid:48)
R (cid:99). (9)
We note, however, that c will become large when more symbols are encoded, and computing Eq. (8-9) with large c will be inefﬁcient. Similar to rANS, we deﬁne a “normalized interval” which bounds the state c such that c ∈ [2M , 2K+M ) (K, M are some integers values) after coding each symbol. For the encoding process, if c ≥ 2K+M , the lower K bits are written to disk, and the remaining bits are reserved such that c ← (cid:98) c 2K (cid:99). For the decoding process, if c < 2M · R, the stored K bits should be read and appended to the state before decoding. In this way, the decoded state is contained within the interval [2M , 2K+M ). The initial state can be set such that c = 2M . We illustrate this idea in Alg. 5.
The correctness of Alg. 5 is illustrated in the following theorem. P1 demonstrates that the symbols can be correctly decoded with a UBCS coder, with coding performed in a ﬁrst-in-last-out (FILO) fashion. P2 shows that the codelength closely approximates the entropy of a uniform distribution, subject to a large M and K. The proof of the theorem is in the Appendix. 7
Table 2: Coding performance of iFlow, LBB and iVPF on CIFAR10 dataset. We use batch size 64.
ﬂow arch.
Flow++ iVPF compression technique
LBB [17] iFlow (Ours) iVPF [41] iFlow (Ours) nll bpd aux. bits 3.116 3.195 3.118 3.118 3.201 3.196 39.86 34.28 6.00 7.00 encoding time (ms) coding inference decoding time (ms) coding inference 16.2±0.3 5.5±0.1 116±1.0 21.0±0.5 11.4±0.2 7.1±0.2 32.4±0.2 5.2±0.1 112±1.5 37.7±0.5 13.5±0.3 9.7±0.2
Theorem 3. Consider coding symbols s1, ...sn with si ∼ U (0, Ri), (i = 1, 2, ..., n, Ri < 2K) using
Alg. 5, and then decode s(cid:48) 1 sequentially. Suppose (1) the initial state and bit-stream are c0 = 2M , bs0 = empty respectively; (2) After coding si, the state is ci and the bit-stream is bsi; i and the bit-stream is bs(cid:48) (3) After decoding s(cid:48) i = si+1, c(cid:48) i = bsi for all i = 0, ..., n − 1. i+1, the state is c(cid:48) i = ci, bs(cid:48) n−1, ..., s(cid:48) i. We have
P1: s(cid:48) n, s(cid:48)
P2: Denote by the codelength li = (cid:100)log ci(cid:101) + len(bsi) where len is the total number of bits in (cid:2) (cid:80)n the bit-stream. Then ln − l0 < i=1 Ri + 1 + (ln 2 · 2M )−1(cid:3). 1 1−1/(ln 2·2M ·K)
In practice, we set K = 32 and M = 4. Compared to rANS [12] and Arithmetric Coding (AC) [40], UBCS is of greater efﬁciency as it necessitates fewer operations (more discussions are shown in the Appendix). UBCS can achieve greater computational efﬁciency via instantiating multiple
UBCS coders in parallel with multi-threading. Table 1 demonstrates that UBCS achieves coding bandwidths in excess of giga-symbol/s – speed signiﬁcantly greater than rANS.
Table 1: Coding bandwidth (M sym-bol/s) of UBCS and rANS coder on dif-ferent threads(thrd). We use the imple-mentations in [17] for evaluating rANS.
# thrd rANS
UBCS 1 16 1 16 5.1±0.3 21.6±1.1 0.8±0.02 7.4±0.5 380±5 2075±353 66.2±1.7 552±50
Encoder
Decoder 4 Experiments
In this section, we perform a number of experiments to establish the effectiveness of iFlow. We will investigate: (1) how closely the codelength matches the theoretical bound; (2) the efﬁciency of iFlow as compared with the LBB [17] baseline; (3) the compression performance of iFlow on a series low and high-resolution images. 4.1 Flow Architectures and Datasets
We adopt two types of ﬂow architectures for evaluation: Flow++ [16] and iVPF [41]. Flow++ is a state-of-the-art model using complex non-linear coupling layers and variational dequantizations [19]. iVPF is derived from a volume-preserving ﬂow in which numerically invertible discrete-space operations are introduced. The models are re-implemented or directly taken from the corresponding authors. Unless speciﬁed, we use h = 12, k = 28 and set large S – around 216, which we analyse further in the Appendix. To reduce the auxiliary bits in the bits-back coding scheme, we partition the d-dimensional data into b splits and perform MST (in Alg. 1 and 2) for each split sequentially. In this case, the auxiliary bits can be reduced to 1/b in MST. We use b = 4 in this experiment.
Following the lossless compression community [16, 4, 18, 37, 41], we perform evaluation using toy datasets CIFAR10, ImageNet32 and ImageNet64. Results for alternate methods are obtained via re-implementation or taken directly from the corresponding papers, where available. We further test the generalization capabilities of iFlow in which all toy datasets are compressed with a model trained on ImageNet32. For benchmarking our performance on high-resolution images, we evaluate iFlow using CLIC.mobile, CLIC.pro2 and DIV2k [1]. For this purpose, we adopt our ImageNet32/64 model for evaluation, and process an image in terms of 32 × 32 or 64 × 64 patches, respectively. The experiment is conducted with PyTorch framework with one Tesla P100 GPU. 2https://www.compression.cc/challenge/ 8
Table 3: Compression performance in bpd on benchmarking datasets. † denotes the generation performance in which the models are trained on ImageNet32 and tested on other datasets. ‡ denotes compression of high-resolution datasets with our ImageNet64-trained model.
ImageNet32
ImageNet64 CIFAR10 CLIC.mobile
CLIC.pro
DIV2K
PNG [5]
FLIF [36]
JPEG-XL [2]
L3C [30]
RC [31]
Bit-Swap [26]
IDF [18]
IDF++ [4] iVPF [41]
LBB [17] iFlow (Ours)
HiLLoC [37]†
IDF [18]† iVPF† [41] iFlow (Ours)† 6.39 4.52 6.39 4.76
-4.50 4.18 4.12 4.03 3.88 3.88 4.20 4.18 4.03 3.88 5.71 4.19 5.74 4.42
--3.90 3.81 3.75 3.70 3.70 3.90 3.94 3.79 3.65 5.87 4.19 5.89
--3.82 3.34 3.26 3.20 3.12 3.12 3.56 3.60 3.49 3.36 3.90 2.49 2.36 2.64 2.54
------4.00 2.78 2.63 2.94 2.93
------3.09 2.91 2.79 3.09 3.08
--------2.47/2.39‡ 2.26/2.26‡
--2.63/2.54‡ 2.45/2.44‡
--2.77/2.68‡ 2.60/2.57‡ 4.2 Compression Performance
For our experiments, we use the evaluation protocols of codelength and compression bandwidth. The codelength is deﬁned in terms of the average bits per dimension (bpd). For no compression, the bpd is assumed to be 8. The compression bandwidth evaluates coding efﬁciency, which we deﬁne in terms of symbols compressed per unit time.
Table 2 demonstrates the compression results on CIFAR10. Note that we only report the encoding time (results on decoding time are similar, and are available in the Appendix). For iVPF, we use settings almost identical to the original paper [41] such that k = 14.
Firstly, we observe that, when using both the Flow++ and iVPF architectures, iFlow achieves a bpd very close to theoretically minimal codelength. When using Flow++, iFlow achieves identical performance as that of LBB. For the iVPF architecture, iFlow outperforms the underlying iVPF as it avoids the need to store 16 bits for each data sample.
Secondly, the coding latency highlights the main advantage of iFlow: we achieve encoding 5× faster than that of LBB and over 1.5× that of iVPF. In fact, the use of UBCS only represents 20% of the total coding time for all symbols (4.8ms in Flow++ and 1.6ms in iVPF). In contrast, the rANS coder of LBB commands over 85% of the total coding latency, which is the principal cause of LBB’s impracticality. Indeed, Table 1 demonstrates that our UBCS coder achieves a speed-up in excess of 50× that of rANS (which results in a coder latency of 4.8ms in iFlow vs. 99.8ms in LBB).
Lastly, compared with LBB, iFlow necessitates fewer auxiliary bits. In fact, LBB requires crica 2k + log σ bits per dimension (for δ = 2−k and small σ in [17]). Meanwhile, iFlow requires approximately k + 1 b log S is usually small with large b. b log S, and 1 4.3 Comparison with the State-of-the-Art
To further demonstrate the effectiveness of iFlow, we compare the compression performance on benchmarking datasets against a variety of neural compression techniques. These include, L3C [30],
Bit-swap [26], HilLoc [37], and ﬂow-based models IDF [18], IDF++ [4], iVPF [41], LBB [17]. We additionally include a number of conventional methods, such as PNG [5], FLIF [36] and JPEG-XL [2].
Experiments on Low Resolution Images. Compression results on our described selection of datasets are available in left three columns of Table 3. Here we observe that iFlow obtains improved compression performance over all approaches with the exception of LBB on low-resolution images, for which we achieve identical results. 9
Generalization. The last four rows in Table 3 demonstrate the literature-standard test of general-ization, in which ImageNet32 trained model are used for testing. From these results, it is clear that iFlow achieves the best generalization performance in this test. It is worth noting that we obtain an improved performance on ImageNet64 when using our ImageNet32-trained model.
Experiments on High Resolution Images. Finally, we test iFlow across a number of high-resolution image datasets. Here images are processed into non-overlapping 32 × 32 and 64 × 64 patches for our
ImageNet32 and ImageNet64-trained models. The right three columns in Table 3 display the results, which is observed that iFlow outperforms all compression methods across all available benchmarks.
Note that as we crop patches for compression, the compression bandwidth is the same as that in small images like CIFAR10, i.e., 5 times faster than LBB and 30% speedup compared with iVPF. 5