Abstract
In recent years there has been increasing interest in leveraging denoisers for solving general inverse problems. Two leading frameworks are regularization-by-denoising (RED) and plug-and-play priors (PnP) which incorporate explicit likelihood func-tions with priors induced by denoising algorithms. RED and PnP have shown state-of-the-art performance in diverse imaging tasks when powerful denoisers are used, such as convolutional neural networks (CNNs). However, the study of their convergence remains an active line of research. Recent works derive the convergence of RED and PnP methods by treating CNN denoisers as approxima-tions for maximum a posteriori (MAP) or minimum mean square error (MMSE) estimators. Yet, state-of-the-art denoisers cannot be interpreted as either MAP or MMSE estimators, since they typically do not exhibit symmetric Jacobians.
Furthermore, obtaining stable inverse algorithms often requires controlling the
Lipschitz constant of CNN denoisers during training. Precisely enforcing this constraint is impractical, hence, convergence cannot be completely guaranteed.
In this work, we introduce image denoisers derived as the gradients of smooth scalar-valued deep neural networks, acting as potentials. This ensures two things: (1) the proposed denoisers display symmetric Jacobians, allowing for MAP and
MMSE estimators interpretation; (2) the denoisers may be integrated into RED and PnP schemes with backtracking step size, removing the need for enforcing their Lipschitz constant. To show the latter, we develop a simple inversion method that utilizes the proposed denoisers. We theoretically establish its convergence to stationary points of an underlying objective function consisting of the learned po-tentials. We numerically validate our method through various imaging experiments, showing improved results compared to standard RED and PnP methods, and with additional provable stability. 1

Introduction
General Recovery Inverse problems aim to recover an unknown signal from its observed corrupted or compressed version. This fundamental task appears in a variety of ﬁelds such as machine learning, signal processing, computer vision and medical imaging. When a model for the degradation process is available, one may derive a data ﬁdelity term which relates the measurements to the unknown signal through a forward operator. However, this term alone typically leads to ill-posed optimization problems with unstable solutions, requiring the incorporation of additional knowledge of the desired 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
signal into the inversion process. This has given rise to diverse regularization methods which have been investigated extensively over the last few decades.
RED and PnP A very basic inverse problem is denoising, in which a clean signal is to be recovered from its measurements contaminated with additive noise. During the last period of time there has been an overwhelming advancement in the ability to perform denoising, creating techniques that approach optimal performance [11, 29, 30]. This striking progress in denoising has been the motivation for the plug-and-play priors (PnP) [48, 10, 41] methodology, which introduced the paradigm of using denoisers in order to solve general inverse problems. PnP incorporates implicit regularization into the inverse solutions by replacing proximal operators in common optimization algorithms with powerful denoisers. This simple modiﬁcation has led to remarkable performance in diverse inverse tasks
[41, 50, 38, 22, 56, 3, 51], as well as to the development of an arsenal of PnP-based techniques which includes proximal-type methods but also goes beyond them [33, 7, 18, 15, 5, 55, 14, 17].
However, the lack of a clearly formulated regularization term corresponding to the denoiser poses a serious challenge in ensuring the stability of PnP-based methods, and typically requires careful parameter-tuning [49].
To ﬁll this void, the regularization by denoising (RED) [39] framework presents an explicit Laplacian-based regularization that relies on a denoiser. Under certain conditions on the denoiser [39, 37], the RED functional is convex and its gradient is given by the denoising residual, allowing for easy incorporation in standard optimization procedures. RED has demonstrated impressive recovery performance while providing global convergence guarantees. However, various common denoisers do not satisfy the conditions required by RED [37]. In such cases, there exists no objective function which justiﬁes the RED method [37] and the corresponding convergence analysis does not hold.
Thus RED, as well as PnP, shows state-of-the-art empirical results but without provable stability. A comprehensive overview of PnP and RED is given in [3].