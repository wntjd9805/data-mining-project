Abstract
The paradigm of differentiable programming has signiﬁcantly enhanced the scope of machine learning via the judicious use of gradient-based optimization. However, standard differentiable programming methods (such as autodiff) typically require the machine learning models to be differentiable, limiting their applicability. Our goal in this paper is to use a new, principled approach to extend gradient-based optimization to functions well modeled by splines, which encompass a large family of piecewise polynomial models. We derive the form of the (weak) Jacobian of such functions and show that it exhibits a block-sparse structure that can be computed implicitly and efﬁciently. Overall, we show that leveraging this redesigned Jacobian in the form of a differentiable “layer” in predictive models leads to improved performance in diverse applications such as image segmentation, 3D point cloud reconstruction, and ﬁnite element analysis. We also open-source the code at https://github.com/idealab-isu/DSA. 1

Introduction
Motivation: Differentiable programming has been a paradigm shift in algorithm design. The main idea is to leverage gradient-based optimization to optimize the parameters of the algorithm, allowing for end-to-end trainable systems (such as deep neural networks) to exploit structure in data and achieve better performance. This approach has found use in a large variety of applications such as scientiﬁc computing [Innes, 2020; Innes et al., 2019; Schafer et al., 2020], image processing [Li et al., 2018a], physics engines [Degrave et al., 2017], computational simulations [Alnæs et al., 2015], and graphics [Li et al., 2018b; Chen et al., 2019]. One way to leverage differentiable programming modules is to encode additional structural priors as “layers” in a larger machine learning model.
Inherent structural constraints such as monotonicity, or piecewise constancy, are particularly prevalent in applications such as physics simulations, graphics rendering, and network engineering. In such applications, it may be beneﬁcial to build models that obey such priors by design.
Challenges: For differentiable programming to work, all layers within the model must admit simple gradient calculations; however, this poses a major limitation in many settings. For example, consider computer graphics applications for rendering 3D objects [Kindlmann et al., 2003; Gross et al., 1995;
Loop and Blinn, 2006]. A common primitive in such cases is a spline (or a piecewise polynomial) function which either exactly or approximately interpolates between a discrete set of points to produce a continuous shape or surface. Similar spline (or other piecewise polynomial) approximations arise in
⇤† Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
partial differential equation (PDE) solvers [Hughes et al., 2005], network ﬂow problems [Balakrishnan and Graves, 1989], and other applications.
For such problems, we would like to compute gradients “through” operations involving spline approximation. However, algorithms for spline approximation often involve discontinuous (or even discrete) co-domains and may introduce undeﬁned (or even zero) gradients. Generally, embedding such functions as layers in a differentiable program, and running automatic differentiation on this program, requires special care. A popular solution is to relax these non-differentiable, discrete components into continuous approximations for which gradients exist. This has led to recent advances in differentiable sorting [Blondel et al., 2020; Cuturi et al., 2019], dynamic programming [Mensch and Blondel, 2018], and optimization [Djolonga and Krause, 2017; Agrawal et al., 2019; Deng et al., 2020].
Our contributions: We propose a principled approach for differentiable programming for spline functions without the use of continuous relaxation2. For the forward pass, we leverage fast algorithms for computing the optimal projection of any given input onto the space of piecewise polynomial functions. For the backward pass, we leverage a fundamental locality property in splines that every piece (or basis function) in the output approximation only interacts with a few other elements.
Using this, we derive a weak form of the Jacobian for the spline operation and show that it exhibits a particular block-structured form. While we focus on spline approximation in this paper, our approach can be generalized to any algorithmic module with piecewise smooth outputs. Our speciﬁc contributions are as follows: 1. We propose the use of spline function approximations as “layers” in differentiable programs. 2. We derive efﬁcient (nearly-linear time) methods for computing forward and backward passes for various spline approximation problems, showing that the (weak) Jacobian in each case can be represented using a block sparse matrix that can be efﬁciently used for backpropagation. 3. We show applications of our approach in three stylized applications: image segmentation, 3D point cloud reconstruction, and ﬁnite element analysis for the solution of partial differential equations.