Abstract
Sample efﬁciency and risk-awareness are central to the development of practical reinforcement learning (RL) for complex decision-making. The former can be addressed by transfer learning and the latter by optimizing some utility function of the return. However, the problem of transferring skills in a risk-aware manner is not well-understood. In this paper, we address the problem of risk-aware policy transfer between tasks in a common domain that differ only in their reward functions, in which risk is measured by the variance of reward streams. Our approach begins by extending the idea of generalized policy improvement to maximize entropic utilities, thus extending policy improvement via dynamic programming to sets of policies and levels of risk-aversion. Next, we extend the idea of successor features (SF), a value function representation that decouples the environment dynamics from the rewards, to capture the variance of returns. Our resulting risk-aware successor features (RaSF) integrate seamlessly within the RL framework, inherit the superior task generalization ability of SFs, and incorporate risk-awareness into the decision-making. Experiments on a discrete navigation domain and control of a simulated robotic arm demonstrate the ability of RaSFs to outperform alternative methods including SFs, when taking the risk of the learned policies into account. 1

Introduction
Reinforcement learning (RL) is a general framework for solving sequential decision-making problems, in which an agent interacts with an environment and receives continuous feedback in the form of rewards. However, many classical algorithms in RL do not explicitly address the need for safety, making them unreliable and difﬁcult to deploy in some real-world applications [10]. One reason for this is the relative sample inefﬁciency of model-free RL algorithms, which often require millions of costly or dangerous interactions with the environment or fail to converge altogether [44, 46]. Transfer learning addresses these problems by incorporating prior knowledge or skills [23, 41]. Despite this, using the expected return as a measure of optimality could still lead to undesirable behavior such as excessive risk-taking, since low-probability catastrophic outcomes with negative reward and high variance could be underrepresented [29]. For this reason, risk-awareness is becoming an important aspect in the design of practical RL [14]. Thus, an ultimate goal of developing reliable systems should be to ensure that they are both sample efﬁcient and risk-aware.
∗Afﬁliate to Vector Institute, Toronto, Canada. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
RL [7, 11, 20, 27, 28, 30, 36, 39, 45]
Transfer [15, 17, 19, 25, 26, 38, 43]
Successor Features [2–4, 8]
RaSF (Ours)
Transfers Skills (cid:55) (cid:51) (cid:51) (cid:51)
Exploits Task Structure Risk-Sensitive (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51)
Table 1: Comparison of RaSF with relevant work in transfer learning and risk-aware RL.
We take a step in this direction by studying the problem of risk-aware policy transfer between tasks with different goals. A powerful way to tackle this problem in the risk-neutral setting is the
GPI/GPE framework, of which successor features (SF) are a notable example [2]. Here, generalized policy improvement (GPI) provides a theoretical framework for transferring policies with monotone improvement guarantees, while generalized policy evaluation (GPE) facilitates the efﬁcient evaluation of policies on novel tasks and is a key component in satisfying the assumptions of GPI in practice.
Together, GPI/GPE provide strong transfer beneﬁts in novel task instances even before any direct interaction with them has taken place, a phenomenon we call task generalization. The key to the superb generalization of GPI/GPE lies in their ability to directly exploit the structure of the task space, taking advantage of subtle differences and commonalities between task goals to transfer skills seamlessly in a composable manner. This property could be an effective way of tackling problems in ofﬂine RL [24], such as the transfer of skills learned in a simulator to a real-world environment.
However in many cases, such as helicopter ﬂight control [16], making one wrong decision could lead to catastrophic outcomes. Hence, being risk-aware could offer one way to avoid worst-case outcomes when transferring skills in real-world settings.
Contributions. We contribute a novel successor feature framework for transferring policies with the goal of maximizing the entropic utility of return in MDPs (Section 2.2). Intuitively, the entropic utility encourages agents to follow policies with predictable and controllable returns characterized by low variance, thus providing a natural way to incorporate risk-awareness. Furthermore, while our theoretical framework could be extended to other classes of utility functions, the entropic utility has many favorable mathematical properties [13, 22] that we exploit directly in this work to achieve optimal transfer (Lemma 1, Theorem 1 and 2). We also derive a form of risk-aware GPE based on the mean-variance approximation, in which the sufﬁcient statistics of the return distribution can be computed directly (Section 3.3) or by leveraging recent developments in distributional RL [6].
Our resulting approach, which we call Risk-Aware Successor Features (RaSF), is able to exploit the task structure to achieve task generalization with respect to novel goal instances as well as levels of risk aversion, where emphasis is placed on avoiding high volatility of returns. Our approach is also complementary to other advances in successor features, including feature learning [3], universal approximation [8], exploration [21], and non-stationary reward preferences [4]. Empirical evaluations on discrete navigation and continuous robot control domains (Section 4) demonstrate the ability of
RaSFs to better manage the trade-off between return and risk and avoid catastrophic outcomes, while providing excellent generalization on novel tasks in the same domain.