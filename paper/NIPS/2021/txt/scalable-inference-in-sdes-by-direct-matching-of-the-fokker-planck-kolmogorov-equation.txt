Abstract
Simulation-based techniques such as variants of stochastic Runge–Kutta are the de facto approach for inference with stochastic differential equations (SDEs) in machine learning. These methods are general-purpose and used with parametric and non-parametric models, and neural SDEs. Stochastic Runge–Kutta relies on the use of sampling schemes that can be inefﬁcient in high dimensions. We address this issue by revisiting the classical SDE literature and derive direct approximations to the (typically intractable) Fokker–Planck–Kolmogorov equation by matching moments. We show how this workﬂow is fast, scales to high-dimensional latent spaces, and is applicable to scarce-data applications, where a non-parametric SDE with a driving Gaussian process velocity ﬁeld speciﬁes the model. 1

Introduction
Differential equations are the standard method of modelling change over time. In deterministic systems the dynamics specifying how the system evolves, are typically written in the form of an ordinary differential equation (ODE).
The dynamics act as prior knowledge and often stem from
ﬁrst-principles in application areas such as physics, control engineering, chemistry, or compartmental models in epi-demiology and pharmacokinetics. Recently, learning ODE dynamics with modern automatic differentiation packages in machine learning has awakened an interest in black-box learning of continuous-time dynamics (e.g., [6, 37]) and enabled their more general use across time-series modelling applications. 2. Gaussian approximation 3. Probability density p(z, t) as solution to the
Fokker–Planck–
Kolmogorov PDE 1. Euler–Maruyama sample trajectories
GP-SDE (SDE model speciﬁcation by a GP prior)
Figure 1: Views into solutions to SDEs.
A stochastic differential equation (SDE, [30, 40]) can be seen as a generalization of ODEs to stochastic dynamical settings, where the driving forces ﬂuctuate or are uncertain. Stochastic dynamics appear naturally in applications where small (and typically unobserved) forces interact with the process, such as tracking applications, molecule motion, gene modelling, or stock markets. In machine learning, SDE models have received wide-spread attention due to their robustness and appealing properties for uncertainty quantiﬁcation.
The concept of a ‘solution’ to an SDE is broader than that of an ODE. As the process is stochastic, the full solution entails a probability distribution, p(z, t), depending on time t and covering the space z (see, e.g., [36]). For Itô type SDEs, the evolution of the probability mass can be described in terms of the Fokker–Planck–Kolmogorov (FPK) partial differential equation (backward Kolmogorov equation). This equation is typically intractable, and instead the de facto approach for inference in
SDEs in machine learning is sampling. The most common approaches in this space are based on 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Stochastic Runge–Kutta schemes (such as the Euler–Maruyama scheme) which are derived from the
Itô–Taylor series. These schemes sample realization trajectories of the SDE by driving the dynamics with numerical simulation of Brownian motion. However, these schemes suffer from drawbacks both related to (ordinary) Runge–Kutta methods—such as step-size and sensitivity to stiffness—as well as problems associated with any sampling schemes, such as a high number of samples required for an accurate representation of the underlying distribution.
Despite these problems, few contemporary SDE approaches in machine learning explore SDE solutions beyond stochastic Runge–Kutta (or even the Euler–Maruyama scheme). Our aim is to try to broaden this view, and in Fig. 1 we sketch an example where we show three alternative solution perspectives to a Gaussian process prior SDE model (GP-SDE): the FPK probability density ﬁeld,
Euler–Maruyama samples, and a Gaussian assumed density approximation. We argue that a Gaussian approximation in latent space SDEs is reasonable, as Gaussian approximations are typically employed anyway in observation models, and allow for speeding-up learning by an order of magnitude.
The contributions of this paper are as follows. (i) We go through the workﬂow connecting ‘random
ODE’ models with Itô SDEs driven by a Gaussian process prior over the velocity ﬁeld, which allows for convenient speciﬁcation of prior knowledge on the vector ﬁeld and induces an implicit prior over the SDE trajectories; (ii) We revisit the classical SDE literature and derive direct approximations to the (typically intractable) Fokker–Planck–Kolmogorov equation in an assumed density Gaussian form that avoids sampling-based inference in the latent space, which makes inference fast and does not require sampling a high number of trajectories; (iii) We show how this workﬂow is fast, applicable to scarce-data applications, and how it also extends to previously presented latent SDE models. 1.1