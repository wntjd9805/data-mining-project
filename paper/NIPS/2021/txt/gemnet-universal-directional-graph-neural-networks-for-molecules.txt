Abstract
Effectively predicting molecular interactions has the potential to accelerate molec-ular dynamics by multiple orders of magnitude and thus revolutionize chemical simulations. Graph neural networks (GNNs) have recently shown great successes for this task, overtaking classical methods based on ﬁxed molecular kernels. How-ever, they still appear very limited from a theoretical perspective, since regular
GNNs cannot distinguish certain types of graphs. In this work we close this gap between theory and practice. We show that GNNs with directed edge embeddings and two-hop message passing are indeed universal approximators for predictions that are invariant to translation, and equivariant to permutation and rotation. We then leverage these insights and multiple structural improvements to propose the geometric message passing neural network (GemNet). We demonstrate the beneﬁts of the proposed changes in multiple ablation studies. GemNet outperforms pre-vious models on the COLL, MD17, and OC20 datasets by 34 %, 41 %, and 20 %, respectively, and performs especially well on the most challenging molecules. Our implementation is available online. 1 1

Introduction
Graph neural networks (GNNs) have shown great promise for predicting the energy and other quantum mechanical properties of molecules. They can predict these properties orders of magnitudes faster than methods from quantum chemistry – at comparable accuracy. GNNs can thus enable the accurate simulation of systems that are orders of magnitude larger. However, they still exhibit severe theoretical and practical limitations. Regular GNNs are only as powerful as the 1-Weisfeiler Lehman test of isomorphism and thus cannot distinguish between certain molecules [45, 60]. Moreover, they require a large number of training samples to achieve good accuracy.
In this work we ﬁrst resolve the questionable expressiveness of GNNs by proving sufﬁcient conditions for universality in the case of invariance to translations and rotations and equivariance to permutations; and then extending this result to rotationally equivariant predictions. Simply using the full geometric information (e.g. all pairwise atomic distances) in a layer does not ensure universal approximation.
For example, if our model uses a rotationally invariant layer we lose the relative information between components. Such a model thus cannot distinguish between features that are rotated differently. This issue is commonly known as the “Picasso problem”: An image model with rotationally invariant layers cannot detect whether a person’s eyes are rotated correctly. Instead, we need a model that preserves relative rotational information and is only invariant to global rotations. To prove universality in the rotationally invariant case we extend a recent universality result based on point cloud models that use representations of the rotation group SO(3) [18]. We prove that spherical representations are actually sufﬁcient; full SO(3) representations are not necessary. We then generalize this to rotationally equivariant predictions by leveraging a recent result on extending invariant to equivariant 1https://www.daml.in.tum.de/gemnet 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
predictions [57]. We then discretize spherical representations by selecting points on the sphere based on the directions to neighboring atoms. We can connect this model to GNNs by interpreting these directions as directed edge embeddings. For example, the embedding direction of atom a would be deﬁned by atom c, resulting in the edge embedding eca. Updating the spherical representation of atom a based on atom b then corresponds to two-hop message passing between the edges eca and edb via eba, with atoms c and d deﬁning the embedding directions. This message passing formalism naturally allows us to obtain the molecule’s full geometrical information (distances, angles, and dihedral angles), and the direct correspondence proves the model’s universality.
We call this edge-based two-hop message passing scheme geometric message passing, and propose multiple structural enhancements to improve the practical performance of this formalism. Based on these changes we develop the highly accurate and sample-efﬁcient geometric message passing neural network (GemNet). We furthermore show that stabilizing the variance of GemNet’s activations with predetermined scaling factors yields signiﬁcant improvements over regular normalization layers.
We investigate the proposed improvements in a range of ablation studies, and show that each of them signiﬁcantly reduces the model error. These changes introduce little to no computational overhead over two-hop message passing. Altogether, GemNet outperforms previous models for force predictions on COLL by 34 %, on MD17 by 41 %, and on OC20 by 20 % on average. We observe the largest improvements for the most challenging molecules, which exhibit dynamic, non-planar geometries. In summary, our contributions are:
• Showing the universality of spherical representations and two-hop message passing with directed edge embeddings for rotationally equivariant predictions.
• Geometric message passing: Symmetric message passing enhanced by geometric information.
• Incorporating all proposed improvements in the Geometric Message Passing Neural Network (GemNet), which signiﬁcantly outperforms previous methods for molecular dynamics prediction. 2