Abstract
For the problem of maximizing a monotone, submodular function with respect to a cardinality constraint k on a ground set of size n, we provide an algorithm that achieves the state-of-the-art in both its empirical performance and its theoretical properties, in terms of adaptive complexity, query complexity, and approximation ratio; that is, it obtains, with high probability, query complexity of O (n) in expec-tation, adaptivity of O (log(n)), and approximation ratio of nearly 1 − 1/e. The main algorithm is assembled from two components which may be of independent interest. The ﬁrst component of our algorithm, LINEARSEQ, is useful as a prepro-cessing algorithm to improve the query complexity of many algorithms. Moreover, a variant of LINEARSEQ is shown to have adaptive complexity of O(log(n/k)) which is smaller than that of any previous algorithm in the literature. The second component is a parallelizable thresholding procedure THRESHOLDSEQ for adding elements with gain above a constant threshold. Finally, we demonstrate that our main algorithm empirically outperforms, in terms of runtime, adaptive rounds, total queries, and objective values, the previous state-of-the-art algorithm FAST in a comprehensive evaluation with six submodular objective functions. 1

Introduction
The cardinality-constrained optimization of a monotone, submodular function f : 2N → R+, deﬁned on subsets of a ground set N of size n, is a general problem formulation that is ubiquitous in wide-ranging applications, e.g. video or image summarization [30], network monitoring [26], information gathering [23], and MAP Inference for Determinantal Point Processes [20], among many others. The function f : 2N → R+ is submodular iff for all S ⊆ T ⊆ N , x (cid:54)∈ T , ∆ (x | T ) ≤ ∆ (x | S)1; and the function f is monotone if f (S) ≤ f (T ) for all S ⊆ T . In this paper, we study the following submodular maximization problem (SM) maximizef (S), subject to |S| ≤ k, (SM) where f is a monotone, submodular function; SM is an NP-hard problem. There has been extensive effort into the design of approximation algorithms for SM over the course of more than 45 years, e.g.
[32, 12, 10, 21, 24]. For SM, the optimal ratio has been shown to be 1 − 1/e ≈ 0.63 [32]. 1∆ (x | S) denotes the marginal gain of x to S: f (S ∪ {x}) − f (S). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Theoretical comparison to algorithms that achieve nearly optimal adaptivity, ratio, and query complexity: the algorithm of Ene and Nguyen [14], RANDOMIZED-PARALLEL-GREEDY (RPG) of Chekuri and Quanrud [11], BINARY-SEARCH-MAXIMIZATION (BSM) and SUBSAMPLE-MAXIMIZATION (SM) of Fahrbach et al. [17], FAST of Breuer et al. [9]. The symbol † indicates the result holds with constant probability or in expectation; the symbol ‡ indicates the result does not hold on all instances of SM; while no symbol indicates the result holds with probability greater than 1 − O(1/n). Observe that our algorithm LS+PGB dominates in at least one category when compared head-to-head with any one of the previous algorithms.
Reference
Ene and Nguyen [14]
Chekuri and Quanrud [11] (RPG)
Fahrbach et al. [17] (BSM)
Fahrbach et al. [17] (SM)
Breuer et al. [9] (FAST)
LS+PGB [Theorem 3]
Ratio
Adaptivity
ε2 log(n)(cid:1)
O (cid:0) 1 1 − 1/e − ε
ε2 log(n)(cid:1) †
O (cid:0) 1 1 − 1/e − ε
ε2 log (n)(cid:1) 1 − 1/e − ε † O (cid:0) 1
ε2 log (n)(cid:1) 1 − 1/e − ε † O (cid:0) 1
ε2 log(n) log2 (cid:16) log(k) (cid:16) 1 1 − 1/e − ε ‡ O
ε2 log (n/ε)(cid:1) 1 − 1/e − ε
O (cid:0) 1
ε
Queries
O (npoly(log n, 1/ε))
ε4 log(n)(cid:1) †
O (cid:0) n
ε3 log log k(cid:1) †
O (cid:0) n
ε3 log(1/ε)(cid:1) †
O (cid:0) n (cid:16) log(k) (cid:16) n
ε2 log
O
ε (cid:1) †
O (cid:0) n
ε2 (cid:17)(cid:17) (cid:17)(cid:17)
As instance sizes have grown very large, there has been much effort into the design of efﬁcient, parallelizable algorithms for SM. Since queries to the objective function can be very expensive, the overall efﬁciency of an algorithm for SM is typically measured by the query complexity, or number of calls made to the objective function f [2, 9]. The degree of parallelizability can be measured by the adaptive complexity of an algorithm, which is the minimum number of rounds into which the queries to f may be organized, such that within each round, the queries are independent and hence may be arbitrariliy parallelized. Observe that the lower the adaptive complexity, the more parallelizable an algorithm is. To obtain a constant approximation factor, a lower bound of Ω(n) has been shown on the query complexity [24] and a lower bound of Ω(log(n)/ log log(n)) has been shown on the adaptive complexity [3].
Several algorithms have been developed recently that are nearly optimal in terms of query and adaptive complexities [14, 11, 17, 5]; that is, these algorithms achieve O (log n) adaptivity and
O (npolylog(n)) query complexity (see Table 1). However, these algorithms use sampling techniques that result in very large constant factors that make these algorithms impractical. This fact is discussed in detail in Breuer et al. [9]; as an illustration, to obtain ratio 1 − 1/e − 0.1 with 95% conﬁdence, all of these algorithms require more than 106 queries of sets of size k/ log(n) in every adaptive round
[9]; moreover, even if these algorithms are run as heuristics using a single sample, other inefﬁciencies preclude these algorithms of running even on moderately sized instances [9]. For this reason, the
FAST algorithm of Breuer et al. [9] has been recently proposed, which uses an entirely different sampling technique called adaptive sequencing. Adaptive sequencing was originally introduced in
Balkanski et al. [6], but the original version has quadratic query complexity in the size of the ground set and hence is still impractical on large instances. To speed it up, the FAST algorithm sacriﬁces theoretical guarantees to yield an algorithm that parallelizes well and is faster than all previous algorithms for SM in an extensive experimental evaluation. The theoretical sacriﬁces of FAST include: the adaptivity of FAST is Ω(log(n) log2(log n)), which is higher than the state-of-the-art, and more signiﬁcantly, the algorithm obtains no approximation ratio for k < 8502; since many applications require small choices for k, this limits the practical utility of FAST. A natural question is thus: is it possible to design an algorithm that is both practical and theoretically optimal in terms of adaptivity, ratio, and total queries? 1.1 Contributions
In this paper, we provide three main contributions. The ﬁrst contribution is the algorithm LINEARSEQ (LS, Section 2) that achieves with probability 1 − 1/n a constant factor (4 + O(ε))−1 in expected linear query complexity and with O(log n) adaptive rounds (Theorem 1). Although the ratio of
≈ 0.25 is smaller than the optimal 1 − 1/e ≈ 0.63, this algorithm can be used to improve the query 2The approximation ratio 1 − 1/e − 4ε of FAST holds with probability 1 − δ for k ≥ θ(ε, δ, k) = 2 log(2δ−1 log( 1
ε log(k)))/ε2(1 − 5ε). 2
Table 2: Empirical comparison of our algorithm with FAST, with each algorithm using 75 CPU threads on a broad range of k values and six applications; details provided in Section 4. Parameter settings are favorable to FAST, which is run without theoretical guarantees while LS+PGB enforces ratio ≈ 0.53 with high probability. For each application, we report the arithmetic mean of each metric over all instances. Observe that LS+PGB outperforms FAST in runtime on all applications.
Runtime (s)
Objective Value
Queries
Application
FAST
TrafﬁcMonitor
InﬂuenceMax
TwitterSumm
RevenueMax
MaxCover (BA)
ImageSumm 3.7 × 10−1 4.4 × 100 1.6 × 101 3.9 × 102 3.7 × 102 1.6 × 101
LS+PGB 2.1 × 10-1 2.3 × 100 3.5 × 100 5.4 × 101 7.6 × 101 8.1 × 10-1
FAST 4.7 × 108 1.1 × 103 3.8 × 105 1.4 × 104 6.0 × 104 9.1 × 103
LS+PGB FAST 5.0 × 108 1.1 × 103 3.8 × 105 1.4 × 104 6.3 × 104 9.1 × 103 3.5 × 103 7.7 × 104 1.5 × 105 7.6 × 104 5.8 × 105 1.3 × 105
LS+PGB 2.4 × 103 4.0 × 104 6.2 × 104 2.7 × 104 1.8 × 105 4.8 × 104 complexity of many extant algorithms, as we decribe in the related work section below. Interestingly,
LINEARSEQ can be modiﬁed to have adaptivity O (log(n/k)) at a small cost to its ratio as discussed in Appendix F. This version of LINEARSEQ is a constant-factor algorithm for SM with smaller adaptivity than any previous algorithm in the literature, especially for values of k that are large relative to n.
Our second contribution is an improved parallelizable thresholding procedure THRESHOLDSEQ (TS,
Section 3) for a commonly recurring task in submodular optimization: namely, add all elements that have a gain of a speciﬁed threshold τ to the solution. This subproblem arises not only in SM, but also e.g. in submodular cover [17] and non-monotone submodular maximization [4, 18, 15, 25]. Our
TS accomplishes this task with probability 1 − 1/n in O(log n) adaptive rounds and expected O(n) query complexity (Theorem 2), while previous procedures for this task only add elements with an expected gain of τ and use expensive sampling techniques [17]; have Ω(log2 n) adaptivity [22]; or have Ω(kn) query complexity [6].
Finally, we present in Section 3 the parallelized greedy algorithm PARALLELGREEDYBOOST (PGB), which is used in conjunction with LINEARSEQ and THRESHOLDSEQ to yield the ﬁnal algorithm
LS+PGB, which answers the above question afﬁrmatively: LS+PGB obtains nearly the optimal 1 − 1/e ratio with probability 1 − 2/n in O(log n) adaptive rounds and O(n) queries in expectation; moreover, LS+PGB is faster than FAST in an extensive empirical evaluation (see Table 2). In addition, LS+PGB improves theoretically on the previous algorithms in query complexity while obtaining nearly optimal adaptivity (see Table 1). 1.2 Additional