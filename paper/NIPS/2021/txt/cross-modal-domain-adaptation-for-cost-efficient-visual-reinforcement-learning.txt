Abstract
In visual-input sim-to-real scenarios, to overcome the reality gap between images rendered in simulators and those from the real world, domain adaptation, i.e., learning an aligned representation space between simulators and the real world, then training and deploying policies in the aligned representation, is a promising direction. Previous methods focus on same-modal domain adaptation. However, those methods require building and running simulators that render high-quality images, which can be difﬁcult and costly. In this paper, we consider a more cost-efﬁcient setting of visual-input sim-to-real where only low-dimensional states are simulated. We ﬁrst point out that the objective of learning mapping functions in previous methods that align the representation spaces is ill-posed, prone to yield an incorrect mapping. When the mapping crosses modalities, previous methods are easier to fail. Our algorithm, Cross-mOdal Domain Adaptation with Sequential structure (CODAS), mitigates the ill-posedness by utilizing the sequential nature of the data sampling process in RL tasks. Experiments on MuJoCo and Hand
Manipulation Suite tasks show that the agents deployed with our method achieve similar performance as it has in the source domain, while those deployed with previous methods designed for same-modal domain adaptation suffer a larger performance gap. 1

Introduction
Reinforcement learning (RL) for vision-based robotic control tasks has achieved remarkable success in recent years [1, 2]. However, current RL algorithms necessitate a substantial number of interactions with the environment, which are costly both in time and money on real robots. An appealing alternative is to train policies in simulators, then transfer these policies to real-world systems [3]. Due to inevitable differences of representation between simulators and the real world, which is also known as the “reality gap” [4], applying policies trained in one domain directly to another almost surely fail, especially in visual-input tasks, which is due to the poor generalization of RL policies [5]. Domain adaptation is a promising direction to handle the gap by mapping representation from two domains to an aligned representation and then training and deploying policies in the aligned representation.
Many recent works, which learn a mapping function to align the data distributions of the two domains, have adopted unsupervised visual domain adaptation [6, 7, 8]. We point out that, as illustrated in
Fig. 1(a), the objective is ill-posed for learning a correct mapping function. These adaptation methods exploit structural constraints [9] in two domains of the same modality (e.g., learned on simulated images and deployed on real images). These methods implicitly alleviate the intrinsic ill-posedness of distribution matching.
∗Equal Contribution †Corresponding Author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Mapping only considering state-distribution matching (b) Mapping with sequential structure
Figure 1: Illustration of the training objective of learning a mapping function in unsupervised domain adaptation.
Shaded regions denote data distributions, where the darker the color, the higher the probability. For each
ﬁgure, the left region is the target domain and the right is the source domain. In Fig. 1(a), both st and s(cid:48) t are
“realistic” instances, but only st is correct. Since they are of similar probabilities, mapping an instance ot in the target domain to somewhere of a similar probability in the source domain is “reasonable” if we only consider distribution matching. In RL, the policy may output unreliable actions when taking these incorrectly mapped states as inputs. In Fig. 1(b), a sequential structure can help rule out the wrong mapping via trajectory contexts.
However, such a kind of same-modal domain adaptation requires the simulator to render images when training the model, which introduces unwanted costs and difﬁculties that are ignored in previous works. First, building a rendering engine is a laborious task. Second, using RL methods to train a policy with an image-based simulator is usually harder [10] and slower (can be up to 20× slower [11]) than with a state-based simulator. An ideal solution to these problems is to train policies with states in the simulator and adapt the learned policies to real-world images. Current domain adaptation methods generally fail in this setting since the structural constraints based on the modality consistency are no longer available, which makes the representation alignment task harder.
To learn such a cross-modal mapping, we propose Cross-mOdal Domain Adaptation with Sequential structure (CODAS) that learns a mapping function from images in the target domain to states in the source domain, as illustrated in Fig. 1(b). With the help of the learned mapping function, policies trained on states in the source domain can be deployed in the target domain of images directly. Speciﬁcally, based on the sequential nature of RL problems, we formulate the cross-domain adaptation problem as a variational inference problem and decompose it into a series of solvable optimization objectives. We also design a special residual model structure in the recurrent neural network (RNN) to enforce additional inductive bias and stabilize the training process.
We evaluate our method on six MuJoCo [12] tasks and four Robot Hand Manipulation tasks [13], where we treat states as the source domain, and rendered images as the target domain. Results show that the learned mapping function can help transfer the policy to the target domain with only a small performance degradation in most of the tasks while existing methods [14, 15] suffer from a larger performance gap. 2