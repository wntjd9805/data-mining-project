Abstract
Recently, deep multi-agent reinforcement learning (MARL) has shown the promise to solve complex cooperative tasks. Its success is partly because of parameter sharing among agents. However, such sharing may lead agents to behave similarly and limit their coordination capacity. In this paper, we aim to introduce diversity in both optimization and representation of shared multi-agent reinforcement learning.
Speciﬁcally, we propose an information-theoretical regularization to maximize the mutual information between agents’ identities and their trajectories, encouraging extensive exploration and diverse individualized behaviors.
In representation, we incorporate agent-speciﬁc modules in the shared neural network architecture, which are regularized by L1-norm to promote learning sharing among agents while keeping necessary diversity. Empirical results show that our method achieves state-of-the-art performance on Google Research Football and super hard StarCraft
II micromanagement tasks†. 1

Introduction
Cooperative multi-agent reinforcement learning (MARL) has drawn increasing interest in recent years, which provides a promise for solving many real-world challenging problems, such as sensor networks [1], trafﬁc management [2], and coordination of robot swarms [3]. However, learning effective policies for such complex multi-agent systems remains challenging. One central problem is that the joint action-observation space grows exponentially with the number of agents, which imposes high demand on the scalability of learning algorithms.
To address this scalability challenge, policy decentralization with shared parameters (PDSP) is widely used, where agents share their neural network weights. Parameter sharing signiﬁcantly improves learning efﬁciency because it dramatically reduces the total number of policy parameters, while experiences and gradients of one agent can be used to train others. Enjoying these advantages, many advanced deep MARL approaches adopt the PDSP paradigm, including value-based methods [4–8], policy gradients [9–13] and communication learning algorithms [14, 15]. These approaches achieve state-of-the-art performance on tasks such as StarCraft II micromanagement [16].
While parameter sharing has been proven to accelerate training [17], its drawbacks are also apparent in complex tasks. These tasks typically require substantial exploration and diversiﬁed strategies among agents. When parameters are shared, agents tend to acquire homogeneous behaviors because they typically adopt similar actions under similar observations, preventing efﬁcient exploration and the emergence of sophisticated cooperative policies. This tendency becomes particularly problematic
*Equal advising
†Videos are available at https://sites.google.com/view/celebrate-diversity-shared with codes. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
for many challenging multi-agent coordination tasks, hindering deep MARL from broader applica-tions. For example, the unsatisfactory performance of state-of-the-art MARL algorithms on Google
Research Football (Fig. 1, and [18]) highlights an urgent demand for diverse behaviors.
Notably, sacriﬁcing the merits of parameter shar-ing for diversity is also unfavorable. Like hu-mans, sharing necessary experience or under-standing of tasks can broadly accelerate coop-eration learning. Without parameter sharing, agents search in a much larger parameter space, which may be wasteful because they do not need to behave differently all the time. Therefore, the question is how to adaptively trade-off diversity and sharing. In this paper, we solve this dilemma by proposing several structural and learning nov-elties.
Figure 1: Shared parameters induce behaviors (left) and can hardly learn successful policies on the challenging Google Research Football task.
Our method learns sophisticated cooperative strate-gies by trading off diversity and sharing (right).
To encourage diversity, we propose a novel information-theoretical objective to maximize the mutual information between agents’ identi-ties and trajectories. This objective enables each agent to distinguish themselves from others and thus involves the contribution of all agents. Accordingly, we derive an intrinsic reward for motivating diversity and optimize it with the global environmental reward by learning the total Q-function as a combination of individual Q-functions. Structurally, we further decompose individual Q-functions as the sum of shared and non-shared local Q-functions for sharing experiences while maintaining representation diversity. We hope agents can use and expand shared knowledge whenever possible.
Thus we introduce L1 regularization on each non-shared Q-function, encouraging agents to share and be diverse when necessary on several critical actions. Combining these novelties achieves a dynamic balance between diversity and homogeneity, efﬁciently catalyzing adaptive and sophisticated cooperation.
We benchmark our approach on Google Research Football (GRF) [18], and StarCraft II micro-management tasks (SMAC) [16]. The extraordinary performance of our approach on challenging benchmarking tasks shows that our approach achieve signiﬁcantly higher coordination capacity than baselines while using diversity as a catalyst for more robust and talent policies. To our best knowledge, our approach achieves state-of-the-art performance on SMAC super hard maps and challenging GRF multi-agent tasks like academy_3_vs_1_with_keeper, academy_counterattack_hard, and a full-ﬁeld scenario 3_vs_1_with_keeper (full field). 2