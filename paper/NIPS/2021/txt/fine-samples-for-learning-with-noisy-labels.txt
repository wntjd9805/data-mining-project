Abstract
Modern deep neural networks (DNNs) become weak when the datasets contain noisy (incorrect) class labels. Robust techniques in the presence of noisy labels can be categorized into two types: developing noise-robust functions or using noise-cleansing methods by detecting the noisy data. Recently, noise-cleansing methods have been considered as the most competitive noisy-label learning algorithms. De-spite their success, their noisy label detectors are often based on heuristics more than a theory, requiring a robust classiﬁer to predict the noisy data with loss val-ues. In this paper, we propose a novel detector for ﬁltering label noise. Unlike most existing methods, we focus on each data point’s latent representation dy-namics and measure the alignment between the latent distribution and each repre-sentation using the eigen decomposition of the data gram matrix. Our framework, coined as ﬁltering noisy instances via their eigenvectors (FINE), provides a robust detector using derivative-free simple methods with theoretical guarantees. Under our framework, we propose three applications of the FINE: sample-selection ap-proach, semi-supervised learning (SSL) approach, and collaboration with noise-robust loss functions. Experimental results show that the proposed methods con-sistently outperform corresponding baselines for all three applications on various benchmark datasets 1. 1

Introduction
Deep neural networks (DNNs) have achieved remarkable success in numerous tasks as the amount of accessible data has dramatically increased [21, 15]. On the other hand, accumulated datasets are typically labeled by a human, a labor-intensive job or through web crawling [48] so that they may be easily corrupted (label noise) in real-world situations. Recent studies have shown that deep neu-ral networks have the capacity to memorize essentially any labeling of the data [49]. Even a small amount of such noisy data can hinder the generalization of DNNs owing to their strong memorization of noisy labels [49, 29]. Hence, it becomes crucial to train DNNs that are robust to corrupted labels.
As label noise problems may appear anywhere, such robustness increases reliability in many appli-cations such as the e-commerce market [9], medical ﬁelds [45], on-device AI [46], and autonomous driving systems [11].
To improve the robustness against noisy data, the methods for learning with noisy labels (LNL) have been evolving in two main directions [18]: (1) designing noise-robust objective functions or regular-∗Equal contribution 1Code available at https://github.com/Kthyeon/FINE_official 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Noise-Cleansing-based Approach (b) FINE
Figure 1: Illustration of (a) basic concept of this work and (b) proposed detection framework, FINE.
Noise-cleansing learning generally separates clean data from the original dataset by using prediction outputs. We propose a novel derivative-free detector based on an unsupervised clustering algorithm on the high-order topological space. FINE measures the alignment of pre-logits (i.e., penultimate layer representation vectors) toward the class-representative vector that is extracted through the eigen decomposition of the gram matrix of data representations. izations and (2) detecting and cleansing the noisy data. In general, the former noise-robust direction uses explicit regularization techniques [6, 52, 50] or robust loss functions [38, 13, 40, 51], but their performance is far from state-of-the-art [49, 26] on datasets with severe noise rates. Recently, re-searchers have designed noise-cleansing algorithms focused on segregating the clean data (i.e., sam-ples with uncorrupted labels) from the corrupted data [19, 14, 47, 18, 32, 42]. One of the popular criteria for the segregation process is the loss value between the prediction of the noisy classiﬁer and its noisy label, where it is generally assumed that the noisy data have a large loss [19, 14, 47, 18] or the magnitude of the gradient during training [51, 40]. However, these methods may still be bi-ased by the corrupted linear classiﬁer towards label noise because their criterion (e.g., loss values or weight gradient) uses the posterior information of such a linear classiﬁer [24]. Maennel et al. [31] analytically showed that the principal components of the weights of a neural network align with the randomly labeled data; this phenomenon can yield more negative effects on the classiﬁer as the number of randomly labeled classes increases. Recently, Wu et al. [42] used an inherent geo-metric structure induced by nearest neighbors (NN) in latent space and ﬁltered out isolated data in such topology, and its quality was sensitive to its hyperparameters regarding NN clustering in the presence of severe noise rates.
To mitigate such issues for label noise detectors, we provide a novel yet simple detector frame-work, ﬁltering noisy labels via their eigenvectors (FINE) with theoretical guarantees to provide a high-quality splitting of clean and corrupted examples (without the need to estimate noise rates).
Instead of using the neural network’s linear classiﬁer, FINE utilizes the principal components of latent representations made by eigen decomposition which is one of the most widely used unsuper-vised learning algorithms and separates clean data and noisy data by these components (Figure 1a).
To motivate our approach, as Figure 1b shows, we ﬁnd that the clean data (blue points) are mainly aligned on the principal component (black dotted line), whereas the noisy data (orange points) are not; thus, the dataset is well clustered with the alignment of representations toward the principal component by ﬁtting them into Gaussian mixture models (GMM). We apply our framework to var-ious LNL methods: the sample selection approach, a semi-supervised learning (SSL) approach, and collaboration with noise-robust loss functions. The key contributions of this work are summarized as follows:
• We propose a novel framework, termed FINE (ﬁltering noisy labels via their eigenvectors), for detecting clean instances from noisy datasets. FINE makes robust decision boundary for the high-order topological information of data in latent space by using eigen decomposition of their gram matrix.
• We provide provable evidence that FINE allows a meaningful decision boundary made by eigenvectors in latent space. We support our theoretical analysis with various experimental results regarding the characteristics of the principal components extracted by our FINE detector.
• We develop a simple sample-selection method by replacing the existing detector method with FINE. We empirically validate that a sample-selection learning with FINE provides consistently superior detection quality and higher test accuracy than other existing alterna-tive methods such as the Co-teaching family [14, 47], TopoFilter [42], and CRUST [32]. 2
• We experimentally show that our detection framework can be applied in various ways to existing LNL methods and validate that ours consistently improves the generalization in the presence of noisy data: sample-selection approach [14, 47], SSL approach [25], and collaboration with noise-robust loss functions [51, 40, 29].
Organization. The remainder of this paper is organized as follows. In Section 2, we discuss the recent literature on LNL solutions and meaningful detectors. In Section 3, we address our motivation for creating a noisy label detector with theoretical insights and provide our main method, ﬁltering the noisy labels via their eigenvectors (FINE). In Section 4, we present the experimental results.
Finally, Section 5 concludes the paper. 2