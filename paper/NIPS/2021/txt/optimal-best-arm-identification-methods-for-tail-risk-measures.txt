Abstract
Conditional value-at-risk (CVaR) and value-at-risk (VaR) are popular tail-risk measures in ﬁnance and insurance industries as well as in highly reliable, safety-critical uncertain environments where often the underlying probability distributions are heavy-tailed. We use the multi-armed bandit best-arm identiﬁcation framework and consider the problem of identifying the arm from amongst ﬁnitely many that has the smallest CVaR, VaR, or weighted sum of CVaR and mean. The latter captures the risk-return trade-off common in ﬁnance. Our main contribution is an optimal δ-correct algorithm that acts on general arms, including heavy-tailed distributions, and matches the lower bound on the expected number of samples needed, asymptotically (as δ approaches 0). The algorithm requires solving a non-convex optimization problem in the space of probability measures, that requires delicate analysis. En-route, we develop new non-asymptotic, anytime-valid, empirical-likelihood-based concentration inequalities for tail-risk measures. 1

Introduction
Tail risk is a common term used to quantify losses occurring due to rare events, and has been an important topic in ﬁnance, insurance and other safety critical uncertain environments. [44] ﬁrst formalized the problem of identifying optimal investment in ﬁnancial assets as a multi-criteria optimization problem of maximizing the average return, while minimizing the risk (measured via variance). Since then, several other risk measures have been considered. Lately, risk-measures based on tails of the distribution, like the conditional value-at-risk (CVaR) and value-at-risk (VaR), have gained popularity in ﬁnancial regulations and risk management (see, [48, 47]), where the underlying probability distributions are mostly heavy tailed (i.e. having inﬁnite moment generating function for all θ > 0). Informally, for a probability measure η, VaR at level π ∈ (0, 1) is the πth quantile for η, i.e., the outcome below which there is exactly π mass. CVaR at level π is the conditional expectation of η, conditioned on values beyond the VaR at level π. See Section 2 for precise deﬁnitions, and
[50, 46] for applications of these risk measures in ﬁnance and optimization. As opposed to VaR,
CVaR is a coherent risk-measure, and is a preferable metric (see, [5] for precise deﬁnition and properties of coherence). Outside ﬁnance, these tail-risk measures are being used to control risk in operations management, for example, in inventory management [4], supply chain management [51], etc. Recently, coherent risk measures, especially CVaR, have also been used in connection with fairness in machine learning [58].
The importance of these risk measures in the sequential decision making set-up has well been acknowledged (see, [49, 42]). Typically in the stochastic multi-armed bandit (MAB) literature, the quality of an arm is measured using its mean. Tight asymptotic and ﬁnite time guarantees exist for different MAB problems with performance measured by the mean (see, [27, 36, 16, 13, 1, 7, 53]).
Also, see [12] for a survey of the variants of stochastic MAB problems. However, maximizing the average reward is not always the primary desirable objective. In clinical trials, for example, the treatment that is good on average might result in adverse outcomes for some patients. In ﬁnance, one 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is typically interested in balancing the mean return with the risk of extreme losses. Risk sensitivity has been well studied in the online learning setting, where in each round, the player sees reward from every arm (see, [24, 56]). However, there is very limited work which incorporates these risk-measures into the MAB framework.
In this paper, we provide a systematic approach for identifying the distribution (or arm) from a given
ﬁnite set of distributions (or arms) with minimum tail-risk (as measured by CVaR or VaR, or by a conic combination of mean and CVaR, which we will henceforth refer to as the “mean-CVaR” objective). Adopting the best-arm identiﬁcation (BAI) framework of the stochastic MAB problem, we consider algorithms that generate samples from the given arms, and are δ-correct, i.e., identify the correct answer (arm with minimum VaR, CVaR or mean-CVaR) with probability at least 1 − δ, for some pre-speciﬁed conﬁdence level δ. While ensuring δ-correctness, the aim is to minimize the number of samples needed by the algorithm before its termination. This is the typical ﬁxed-conﬁdence setting of the BAI MAB problem (see, [37, 2]). Variants of this problem have been widely studied in the literature, where the best-arm is the one with maximum mean (see, [43, 25, 6, 14, 26, 33, 27, 34]).
A relaxation of the pure exploration setting described above is the ((cid:15), δ)-PAC setting, where the aim is to output an (cid:15)-optimal arm (for an appropriate notion of (cid:15)-optimality), with probability at least 1 − δ, while minimizing the number of samples generated. [59, 18, 32] consider the pure exploration problem of identifying the arm with minimum risk in the ((cid:15), δ)-PAC setting. While [59] consider both VaR and CVaR as measures of risk, [18, 32] focus on the VaR-problem. Recently, [39] and
[35] have studied the BAI MAB problem with CVaR and mean-CVaR objectives, respectively, in the closely related “ﬁxed-budget” framework, in which the total number of samples the algorithm is allowed to take is ﬁxed, and the aim of the algorithm is to minimize the error-probability. 1.1 Outline of the approach and main assumption
As a warm-up, we ﬁrst solve our minimum tail risk identiﬁcation problems in the simple commonplace setting of arm-distributions belonging to a canonical single parameter exponential family (SPEF) of distributions. Each distribution in this family is uniquely identiﬁed with its parameter. We show that both CVaR and VaR are monotonic functions of this parameter, as is the mean. Hence, ﬁnding the best-(CVaR/VaR/mean-CVaR) arm reduces to ﬁnding the arm with the minimum mean.
Since risk-sensitive objectives are particularly important when there is a non-trivial probability of occurrence of extreme outcomes, it is important to consider arm-distributions beyond canonical SPEF, for which the above-mentioned equivalence breaks. We solve the VaR problem for arbitrary arm distributions.
In contrast the CVaR problem is unlearnable in full generality: on the class of all arm distributions, any δ-correct algorithm requires an inﬁnite number of samples in expectation to identify the best arm amongst any ﬁnite collection of arms (Remark 3.1). To avoid this, we impose a mild and standard raw (1 + (cid:15))-moment restriction on the arm-distributions. Let P((cid:60)) denote the collection of all the probability distributions on the reals (cid:60), and let B and (cid:15) be positive constants. For risk measure CVaR and for the mean-CVaR objective, we restrict the class of allowed arm distributions to
L = (cid:110)
η ∈ P((cid:60)) : Eη (cid:16)
|X|1+(cid:15)(cid:17) (cid:111)
.
≤ B
We discuss the choice of parameters in Section 1.3 below. For each tail measure, we prove information-theoretic lower bounds on the sample complexity of any δ-correct algorithm, and use these to develop a δ-correct algorithm whose sample complexity exactly matches the lower bound as δ → 0, for CVaR, mean-CVaR, and VaR problems. The mean-CVaR problem is conceptually and technically similar to the CVaR problem. Hence, for simplicity of presentation, we primarily focus on the CVaR setting in the main text and give details of the mean-CVaR setting in Appendix I. We also spell out the somewhat analogous analysis for the VaR setting towards the end (Section 4.2), with details deferred
Appendix to H. 1.2 Technical contributions
As is well known in the BAI MAB literature, the lower bound problem takes the underlying arm distributions as inputs and solves for optimal weights that determine the proportion of samples that should ideally be allocated to each arm. The proposed algorithm uses a plugin strategy that at each 2
sequential stage, modulo mild forced exploration, uses the generated empirical distributions as a proxy for the true distributions and arrives at weights that guide the sequential sampling strategy.
In order to highlight the technical challenges arising in our non-parametric case, we will need to introduce two functionals next that are central to our lower bounds, algorithms, conﬁdence intervals etc.
Information distance for CVaR problem: Given η1, η2 in P((cid:60)), let KL(η1, η2) denote the KL-divergence between them, i.e., KL(η1, η2) := (cid:82) log dη1 (y)dη1(y). Furthermore, for the probability dη2 measure η let cπ(η) denote its CVaR at the given conﬁdence level π ∈ (0, 1) (see Section 2 for the inf : P((cid:60))×(cid:60) −→ (cid:60)+, exact deﬁnition). Then, given η ∈ P((cid:60)) and x ∈ (cid:60), we deﬁne functionals KLU inf : P((cid:60)) × (cid:60) −→ (cid:60)+, where (cid:60)+ denotes the non-negative reals, as and KLL
KLU inf (η, x) := min
κ∈L: cπ(κ)≥x
KL(η, κ) and KLL inf (η, x) := min
κ∈L: cπ(κ)≤x
KL(η, κ). (1)
See [2, 30, 15] for related quantities. These projection functionals appear in the lower bound (Section 3), and are central to our plugin algorithm.
Unlike their analogues in the mean case, KLU inf in (1) are not symmetric, and need to be studied separately. In particular, KLU inf is not. This is because cπ(·) is a concave function, whence, the CVaR constraint in the KLL inf problem in (1) renders the feasible region non-convex (see Section 2). CVaR can be expressed as the optimal value of a minimization problem. This helped in re-expressing KLL inf as minimization over 2 variables,
ﬁxing one of which resulted in convex optimization over the other (see Section 3). inf is a convex optimization problem, while KLL inf and KLL
For proving δ-correctness, we develop a new concentration inequality for weighted sums of these functionals (Proposition 4.2). Dual representations of these suggest natural candidates for super-martingales, whose mixtures help us in proving the concentration result. Similar inequalities were developed in [38, 20, 54] in different settings. See [40, Chapter 20] for an overview of the method of mixtures. We also propose KLU inf -based tight anytime-valid conﬁdence intervals for CVaR for heavy-tailed distributions, and show that classical conﬁdence intervals derived using popular truncation-based estimators can be recovered using our method, with only a minor overhead (see
Section 4.3). inf - and KLL
Since distributions in L are not characterized by parameters, we work in the space of probability measures instead of in the Euclidean space. A key and non-trivial requirement for the proof of asymptotic optimality of the algorithm is the joint continuity of KLL inf in a well-chosen metric, which should generate a topology that is sufﬁciently ﬁne to ensure this continuity, but coarse to ensure fast convergence of the empirical distributions to the true-arm distributions. We endow
P((cid:60)) with the topology of weak convergence, or equivalently, with the Lévy metric (see Section 2 for deﬁnitions). Another nuance in our analysis is that the empirical distributions may not lie in L.
This is handled by projecting these on to L under a suitable metric. inf and KLU
Our proposed algorithm is a plugin strategy that involves solving the lower bound problem using the empirical distributions as a proxy for the actual arm distributions. This can be computationally demanding especially as the underlying samples in the empirical distribution become large. To ease the numerical burden we propose modiﬁcations that require solving the lower bound only order log(n) many times till stage n of the algorithm (where n samples are generated). This modiﬁcation substantially reduces the computation burden. We show that it is optimal up to a constant (Appendix
K).
VaR problem: Our algorithm for CVaR, with KLL inf replaced by the corresponding functionals with the VaR constraints instead, is asymptotically optimal for this problem in complete generality (Section 4.2). Here, KLU inf have closed form representations. However, they are no longer jointly-continuous in the Lévy metric, which introduces new technical challenges in the analysis of the algorithm. inf and KLU inf and KLL 1.3 Regarding the choice of (cid:15) and B in our assumption
Firstly, BAI problems are important in simulation where the best model may need to be identiﬁed amongst many intricate models in terms of a performance measure such as CVaR or VaR, using minimal computational effort (see, [31]). Input distributions in simulation are known and may often 3
involve heavy tails. In some cases, by the use of Lyapunov-function-based techniques, bounds on moments of output random variables, B, can be determined. (see, e.g., [28] and references therein).
Secondly, consider rewards (returns) from a number of hedge funds. Each time some amount of money is invested into a fund, a random return may be revealed from that fund but not from others.
To assume that these returns come from a class of parametric distributions or have known bounded support can be a substantially inaccurate simpliﬁcation. Typically, from historical analysis, it is known that the distribution of securities have a particular tail index, say, (1 + (cid:15)). For stock returns, extensive research suggests that (1 + (cid:15)) ∈ [2, 5]. For daily exchange rates and income and wealth distributions we may have (1 + (cid:15)) ∈ (1, 2]. Extreme value theory, under reasonable dependence structure amongst underlying securities, shows that a portfolio (a weighted sum) will also have the same tail index of (1 + (cid:15)) (see, [19]). So the key approximation needed is in arriving at B. It is easy to arrive at distributions η and κ whose (1 + (cid:15))th moments are arbitrarily far while the KL distance between them is arbitrarily close to zero. This makes it difﬁcult to infer B from a given sample of data without further restriction on the two distributions. One may take a pragmatic view and approximate B by estimating the (1 + (cid:15))th moment from observed samples and padding it up with a reasonably large factor. A further set of distributional assumptions would be needed to justify the above procedure to arrive at B. Again, verifying those assumptions will entail similar problems. In practice, one may live with the above approximation even though in rare settings it may be inaccurate and lead to sub-optimal allocations in our algorithm. One accepts this risk as one often accepts the assumption that the distributions of the random samples from each arm are time stationary or are independent, even though these may only be approximately correct. 2