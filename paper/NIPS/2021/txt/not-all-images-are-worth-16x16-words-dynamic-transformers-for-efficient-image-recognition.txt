Abstract
Vision Transformers (ViT) have achieved remarkable success in large-scale image recognition. They split every 2D image into a ﬁxed number of patches, each of which is treated as a token. Generally, representing an image with more to-kens would lead to higher prediction accuracy, while it also results in drastically increased computational cost. To achieve a decent trade-off between accuracy and speed, the number of tokens is empirically set to 16x16 or 14x14. In this paper, we argue that every image has its own characteristics, and ideally the to-ken number should be conditioned on each individual input. In fact, we have observed that there exist a considerable number of “easy” images which can be accurately predicted with a mere number of 4x4 tokens, while only a small frac-tion of “hard” ones need a ﬁner representation. Inspired by this phenomenon, we propose a Dynamic Transformer to automatically conﬁgure a proper num-ber of tokens for each input image. This is achieved by cascading multiple
Transformers with increasing numbers of tokens, which are sequentially acti-vated in an adaptive fashion at test time, i.e., the inference is terminated once a sufﬁciently conﬁdent prediction is produced. We further design efﬁcient fea-ture reuse and relationship reuse mechanisms across different components of the
Dynamic Transformer to reduce redundant computations. Extensive empirical results on ImageNet, CIFAR-10, and CIFAR-100 demonstrate that our method signiﬁcantly outperforms the competitive baselines in terms of both theoretical computational efﬁciency and practical inference speed. Code and pre-trained mod-els (based on PyTorch and MindSpore) are available at https://github.com/ blackfeather-wang/Dynamic-Vision-Transformer and https://github. com/blackfeather-wang/Dynamic-Vision-Transformer-MindSpore. 1

Introduction
Transformers, the dominant self-attention-based models in natural language processing (NLP) [10, 40, 3], have been successfully adapted to image recognition problems [11, 55, 38, 17] recently. In particular, vision Transformers achieve state-of-the-art performance on the large scale ImageNet benchmark [9], while exhibit excellent scalability with the further growing dataset size (e.g., on JFT-300M [11]). These models split each image into a ﬁxed number of patches and embed them into 1D tokens as inputs. Typically, representing the data using more tokens contributes to higher prediction accuracy, but leads to intensive computational cost, which grows quadratically with respect to the
∗Equal contribution.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
token number in self-attention blocks. For a proper trade-off between efﬁciency and effectiveness, existing works empirically adopt 14x14 or 16x16 tokens [11, 55].
Table 1: Accuracy and computa-tional cost of T2T-ViT-12 with dif-ferent token numbers on ImageNet.
In this paper, we argue that it may not be optimal to treat all samples with the same number of tokens. In fact, there exist considerable variations among different images (e.g., contents, scales of objects, backgrounds, etc.). Therefore, the number of representative tokens should ideally be conﬁgured speciﬁcally for each input. This issue is critical for the computational efﬁciency of the models. For example, we train a T2T-ViT-12
[55] with varying token numbers, and report the corresponding accuracy and FLOPs in Table 1.
One can observe that adopting the ofﬁcially recommended 14x14 tokens only correctly recognizes 15.9% (76.7% v.s. 60.8%) more test samples compared to that of using 4x4 tokens, while increases
∼ the computational cost by 8.5x (1.78G v.s. 0.21G). In other words, computational resources are wasted on applying the unnecessary 14x14 tokens to many “easy” images for which 4x4 tokens are sufﬁcient. 76.7% 70.3% 60.8% 1.78G 0.47G 0.21G
Accuracy
FLOPs
# of Tokens 14x14 4x4 7x7
Motivated by this observation, we propose a novel Dy-namic Vision Transformer (DVT) framework, aiming to automatically conﬁgure a decent token number conditioned on each image for high computational efﬁciency. In spe-ciﬁc, a cascade of Transformers are trained using increas-ing number of tokens. At test time, these models are se-quentially activated starting with less tokens. Once a pre-diction with sufﬁcient conﬁdence has been produced, the inference procedure will be terminated immediately. As a consequence, the computation is unevenly allocated among
“easy” and “hard” samples by adjusting the token number, yielding a considerable improvement in efﬁciency. Impor-tantly, we further develop feature-wise and relationship-wise reuse mechanisms to reduce redundant computations.
The former allows the downstream models to be trained on the basis of previously extracted deep features, while the later enables leveraging existing upstream self-attention relationships to learn more accurate attention maps. Illus-trative examples of our method are given in Figure 1.
Figure 1: Examples for DVT.
Notably, DVT is designed as a general framework. Most of the state-of-the-art image recognition
Transformers, such as ViT [11], DeiT [38], and T2T-ViT [55], can be straightforwardly deployed as its backbones for higher efﬁciency. Our method is also appealing in its ﬂexibility. The computational cost of DVT is able to be adjusted online by simply adapting the early-termination criterion. This characteristic makes DVT suitable for the cases where the available computational resources ﬂuctuate dynamically or a minimal power consumption is required to achieve a given performance. Both situations are ubiquitous in real-world applications (e.g., searching engines and mobile apps).
The performance of DVT is evaluated on ImageNet [9] and CIFAR [26] with T2T-ViT [55] and DeiT
[38]. Experimental results show that DVT signiﬁcantly improves the efﬁciency of the backbones. For examples, DVT reduces the computational cost of T2T-ViT by 1.6-3.6x without sacriﬁcing accuracy.
The real inference speed on a NVIDIA 2080Ti GPU is consistent with our theoretical results. 2