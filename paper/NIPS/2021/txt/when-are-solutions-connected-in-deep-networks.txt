Abstract
The question of how and why the phenomenon of mode connectivity occurs in training deep neural networks has gained remarkable attention in the research community. From a theoretical perspective, two possible explanations have been proposed: (i) the loss function has connected sublevel sets, and (ii) the solutions found by stochastic gradient descent are dropout stable. While these explanations provide insights into the phenomenon, their assumptions are not always satisﬁed in practice. In particular, the ﬁrst approach requires the network to have one layer with order of N neurons (N being the number of training samples), while the second one requires the loss to be almost invariant after removing half of the neurons at each layer (up to some rescaling of the remaining ones). In this work, we improve both conditions by exploiting the quality of the features at every intermediate layer together with a milder over-parameterization requirement. More speciﬁcally, we show that: (i) under generic assumptions on the features of intermediate layers,
N neurons, and (ii) it sufﬁces that the last two hidden layers have order of if subsets of features at each layer are linearly separable, then almost no over-parameterization is needed to show the connectivity. Our experiments conﬁrm that the proposed condition ensures the connectivity of solutions found by stochastic gradient descent, even in settings where the previous requirements do not hold.
√ 1

Introduction
The aim of this work is to provide further theoretical insights into the mode connectivity phenomenon which has recently attracted some considerable attention from the research community (i.e. training a neural network via stochastic gradient descent (SGD) from different random initializations often leads to solutions that could be connected by a path of low loss) [2, 9, 11, 12, 15]. Existing theoretical insights from optimization landscape analysis suggest that, for over-parameterized networks, the loss function has connected sublevel sets, and hence the solutions found by SGD are also connected
[32, 37, 43]. However, in order for such a result to hold, the network needs to have one wide layer with at least N + 1 neurons (N being the number of training samples) [33]. This bound is also known to be tight for 2-layer networks. Here, we would like to understand the above phenomenon for deep and mildly over-parameterized networks (i.e. we aim to prove weaker guarantees under milder conditions). The idea is that, even when the sublevel sets are disconnected, SGD may only concentrate in a (large) region with connected minima, see e.g. Figure 1. This leads to the question that whether one can characterize the properties of the solutions found by SGD which capture the mode connectivity phenomenon, for mildly over-parameterized networks. Along this line, a recent work [24] proposed the so-called “dropout stability” condition, that is, removing half the neurons from every hidden layer leads to almost no change in the value of the loss. As shown in the prior 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A function with disconnected sublevel sets and two connected sets of minima. works (also in our experiments in Section 6), this condition holds for very wide networks or when dropout is used as a regularizer during training, but it does not hold under other practical settings.
The aim of this work is to provide a new characterization of SGD solutions which capture their connectivity property, and to improve the previous assumptions (width Ω(N ), and dropout stability) for networks of practical sizes, learned without the support of dropout training. As a remark, although our work is motivated by the original phenomenon concerning mainly the solutions found by SGD, all the theoretical results are still valid for other pairs of solutions in parameter space.
Main contributions. Our main Theorem 4.1 provides an upper bound of the loss along a path connecting two given solutions in parameter space. This bound is characterized by the loss of L optimized and sparse subnetworks, L being the network depth. These L subnetworks connect subsets of at least half the neurons at each layer to the output (see Figure 2), and their weights can be optimized independently of the weights of the original network, which leads to a signiﬁcant reduction of the loss. This is the key difference to the previously proposed dropout stability property [24]. Next, we specialize our theorem to different settings to demonstrate the improvement over prior works:
• Corollary 4.2 shows that our condition is provably milder than dropout stability [24]. In fact, dropout stability implies that the loss does not change much when we remove at least half of the neurons at each layer and rescale the remaining ones. Here, we allow to optimize again all the weights of the subnetworks connecting the subsets of features to the output.
• As the weights of the subnetworks can be optimized, we establish a formal connection between mode connectivity and memorization capacity of neural nets. In particular, Corol-N ) neurons, then under some lary 4.3 proves that, if the last two hidden layers have Ω( additional mild conditions on the features of intermediate layers, any two given solutions can be connected by a path of low loss. This improves the width condition of prior works
[32, 33, 43] from Ω(N ) to Ω(
N ) at the cost of ensuring a weaker guarantee (i.e. the connectivity of sublevel sets proved in prior works implies the connectivity of any two pairs of solutions, but in this paper we only show the connectivity for certain pairs).
√
√
• Corollary 4.4 proves that, for a classiﬁcation task, if subsets of features at each layer are linearly separable, then no over-parameterization is required to prove the connectivity.
The message of this paper is that, in order to ensure the connectivity, a smooth trade-off between feature quality and over-parameterization sufﬁces. The two extremes of this trade-off are captured by Corollary 4.3 and 4.4 mentioned above. More generally, at each layer l, we pick a subset Il containing at least half the neurons. If the features indexed by Il are good enough, in the sense that there exists a small subnetwork ξl achieving small loss when trained on these features as inputs, then all layers k > l in the original network do not need to be larger than twice the widths of the subnet ξl.
Hence, the better the features at lower layers, the less over-parameterized the higher layers need to be.
Finally, our numerical results show that, if the learning task is sufﬁciently simple (e.g. MNIST) and the network is sufﬁciently over-parameterized, then dropout stability holds and the SGD solutions can be connected as in [24]. However, when the learning task is more difﬁcult (e.g. CIFAR-10), or for networks of moderate sizes, dropout stability does not hold and the path of [24] exhibits high loss.
In this challenging setting, we are still able to ﬁnd low-loss paths connecting the SGD solutions.
Proof techniques. We remark that, while previous work [32, 33, 43] focused on the connectivity of all the sublevel sets (i.e. a property of the global landscape), here we are only interested in the connectivity of a particular subset of solutions, especially those that are discovered by SGD. This leads to several differences in the proofs. First, previous analysis relies on the fact that the ﬁrst hidden layer has at least N + 1 neurons [33], and thus the training inputs can essentially be mapped to a 2
(a) Original net θ(cid:48) (b) Subnet ξ0 (c) Subnet ξ1 (d) Subnet ξ2
Figure 2: An illustration of the sub-networks appearing in our main Theorem 4.1. The blue neurons correspond to the subset Il in the theorem, and its cardinality is at least half the layer width. Each sub-network ξl receives as inputs the blue neurons from layer l, and pass it through the red ones at the higher layers, all the way to the output of the network. All the weights of the sub-networks (in green) can be re-optimized to minimize the upper bound in the theorem. This is shown via the inﬁmum taken over ξl on the RHS of Equation (4). feature space where they become linearly independent. This property allows the network to realize arbitrary outputs at each layer, which leads to the connectivity of sublevel sets. This property is no longer true for the networks considered here, where the layer widths are allowed to be sublinear in
N . Second, the analysis of sublevel sets often do not take into account the bias of SGD, whereas here we focus more on understanding the properties of these solutions which lead to their connectivity.
More closely related is the work by [24], with which we provide a detailed discussion in Section 4. 2 Further