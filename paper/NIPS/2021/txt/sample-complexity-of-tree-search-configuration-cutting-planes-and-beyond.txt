Abstract
Cutting-plane methods have enabled remarkable successes in integer program-ming over the last few decades. State-of-the-art solvers integrate a myriad of cutting-plane techniques to speed up the underlying tree-search algorithm used to
ﬁnd optimal solutions. In this paper we provide sample complexity bounds for cut-selection in branch-and-cut (B&C). Given a training set of integer programs sampled from an application-speciﬁc input distribution and a family of cut selection policies, these guarantees bound the number of samples sufﬁcient to ensure that using any policy in the family, the size of the tree B&C builds on average over the training set is close to the expected size of the tree B&C builds. We ﬁrst bound the sample complexity of learning cutting planes from the canonical family of
Chvátal-Gomory cuts. Our bounds handle any number of waves of any number of cuts and are ﬁne tuned to the magnitudes of the constraint coefﬁcients. Next, we prove sample complexity bounds for more sophisticated cut selection policies that use a combination of scoring rules to choose from a family of cuts. Finally, beyond the realm of cutting planes for integer programming, we develop a general abstraction of tree search that captures key components such as node selection and variable selection. For this abstraction, we bound the sample complexity of learning a good policy for building the search tree. 1

Introduction
Integer programming is one of the most broadly-applicable tools in computer science, used to formulate problems from operations research (such as routing, scheduling, and pricing), machine learning (such as adversarially-robust learning, MAP estimation, and clustering), and beyond. Branch-and-cut (B&C) is the most widely-used algorithm for solving integer programs (IPs). B&C is highly conﬁgurable, and with a deft conﬁguration, it can be used to solve computationally challenging problems. Finding a good conﬁguration, however, is a notoriously difﬁcult problem.
We study machine learning approaches to conﬁguring policies for selecting cutting planes, which have an enormous impact on B&C’s performance. At a high level, B&C works by recursively partitioning the IP’s feasible region, searching for the locally optimal solution within each set of the partition, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
until it can verify that it has found the globally optimal solution. An IP’s feasible region is deﬁned by a set of linear inequalities Ax ≤ b and integer constraints x ∈ Zn, where n is the number of variables. By dropping the integrality constraints, we obtain the linear programming (LP) relaxation of the IP, which can be solved efﬁciently. A cutting plane is a carefully-chosen linear inequality
αT x ≤ β which reﬁnes the LP relaxation’s feasible region without separating any integral point.
Intuitively, a well-chosen cutting plane will remove a large portion of the LP relaxation’s feasible region, speeding up the time it takes B&C to ﬁnd the optimal solution to the original IP. Cutting plane selection is a crucial task, yet it is challenging because many cutting planes and cut-selection policies have tunable parameters, and the best conﬁguration depends intimately on the application domain.
We provide the ﬁrst provable guarantees for learning high-performing cutting planes and cut-selection policies, tailored to the application at hand. We model the application domain via an unknown, application-speciﬁc distribution over IPs, as is standard in the literature on using machine learning for integer programming [e.g., 21, 23, 31, 36, 43]. For example, this could be a distribution over the routing IPs that a shipping company must solve day after day. The learning algorithm’s input is a training set sampled from this distribution. The goal is to use this training set to learn cutting planes and cut-selection policies with strong future performance on problems from the same application but which are not already in the training set—or more formally, strong expected performance. 1.1 Summary of main contributions and overview of techniques
As our ﬁrst main contribution, we provide sample complexity bounds of the following form: ﬁxing a family of cutting planes, we bound the number of samples sufﬁcient to ensure that for any sequence of cutting planes from the family, the average size of the B&C tree is close to the expected size of the B&C tree. We measure performance in terms of the size of the search tree B&C builds. Our guarantees apply to the parameterized family of Chvátal-Gomory (CG) cuts [10, 17], one of the most widely-used families of cutting planes.
The overriding challenge is that to provide guarantees, we must analyze how the tree size changes as a function of the cut parameters. This is a sensitive function—slightly shifting the parameters can cause the tree size to shift from constant to exponential in the number of variables. Our key technical insight is that as the parameters vary, the entries of the cut (i.e., the vector α and offset β of the cut αT x ≤ β) are multivariate polynomials of bounded degree. The number of terms deﬁning the polynomials is exponential in the number of parameters, but we show that the polynomials can be embedded in a space with dimension sublinear in the number of parameters. This insight allows us to better understand tree size as a function of the parameters. We then leverage results by Balcan et al.
[8] that show how to use structure exhibited by dual functions (measuring an algorithm’s performance, such as its tree size, as a function of its parameters) to derive sample complexity bounds.
Our second main contribution is a sample complexity bound for learning cut-selection policies, which allow B&C to adaptively select cuts as it solves the input IP. These cut-selection policies assign a number of real-valued scores to a set of cutting planes and then apply the cut that has the maximum weighted sum of scores. Tree size is a volatile function of these weights, though we prove that it is piecewise constant, as illustrated in Figure 1, which allows us to prove our sample complexity bound.
Finally, as our third main contribution, we provide guarantees for tuning weighted combinations of scoring rules for other aspects of tree search beyond cut selection, including node and variable selection. We prove that there is a set of hyperplanes splitting the parameter space into regions such that if tree search uses any conﬁguration from a single region, it will take the same sequence of actions. This structure allows us to prove our sample complexity bound. This is the ﬁrst paper to provide guarantees for tree search conﬁguration that apply simultaneously to multiple different aspects of the algorithm—prior research was speciﬁc to variable selection [5].
Sample complexity bounds are important because if the parameterized class of cuts or cut-selection policies that we optimize over is highly complex and the training set is too small, the learned cut or cut-selection policy might have great average empirical performance over the training set but terrible future performance. In other words, the parameter conﬁguration procedure may overﬁt to the training set. The sample complexity bounds we provide are uniform-convergence: we prove that given enough samples, uniformly across all parameter settings, the difference between average and empirical performance is small. In other words, these bounds hold for any procedure one might use to optimize over the training set: manual or automated, optimal or suboptimal. No matter what 2
Figure 1: Two examples of tree size as a function of a SCIP cut-selection parameter µ (the directed cutoff distance weight, deﬁned in Section 2) on IPs generated from the Combinatorial Auctions
Test Suite [30] (the “regions” generator with 600 bids and 600 goods). SCIP [16] is the leading open-source IP solver. parameter setting the conﬁguration procedure comes up with, the user can be guaranteed that so long as that parameter setting has good average empirical performance over the training set, it will also have strong future performance. 1.2