Abstract
What is the minimum necessary information required by a neural net D(·) from an image x to accurately predict its class? Extracting such information in the input space from x can allocate the areas D(·) mainly attending to and shed novel insights to the detection and defense of adversarial attacks. In this paper, we propose “class-disentanglement” that trains a variational autoencoder
G(·) to extract this class-dependent information as x − G(x) via a trade-off between reconstructing x by G(x) and classifying x by D(x − G(x)), where the former competes with the latter in decomposing x so the latter retains only necessary information for classiﬁcation in x − G(x). We apply it to both clean images and their adversarial images and discover that the perturbations generated by adversarial attacks mainly lie in the class-dependent part x − G(x). The decomposition results also provide novel interpretations to classiﬁcation and attack models. Inspired by these observations, we propose to conduct adversarial detection and adversarial defense respectively on x − G(x) and G(x), which consistently outperform the results on the original x. In experiments, this simple approach substantially improves the detection and defense against different types of adversarial attacks. Code is available: https://github.com/kai-wen-yang/CD-VAE. 1

Introduction
Deep learning provides an end-to-end solution to challenging tasks with data of complicated structures and have achieved breakthrough across different domains in recent years. However, what essential information the neural nets extract from the input and mainly rely on in producing predictions for the targeted tasks still remains mysterious. Moreover, the trained models are known to be fragile and sensitive to adversarial perturbations on the input, but how to explain this failure is still an open problem. Given entangled raw features as input, the inference of neural nets tend to remove task-redundant information and compress task-essential information to abstract concepts. On the one hand, identifying the task-essential information in the input can allocate the areas that the neural nets mainly attend to and explain the model prediction. On the other hand, this information can also be the main target of adversarial attacks, while the removed task-redundant information may suffer less distortion under attacks and still preserve clues to correct the prediction. Hence, disentanglement of these two types of information in the input (termed “task-disentanglement”) may shed critical insights to address aforementioned problems.
Disentangled representations [3] have been extensively studied in unsupervised learning when training a generative model, where each dimension of the representation controls a single generative factor of data variations and changing it does not affect other factors. Probably the most studied disentanglement model is β-variational autoencoder (VAE) [20], whose objective aims to reconstruct 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the input x from the latent representation z via p(x|z) while keeping p(z|x) close to the standard
Gaussian p(z) = N (0, I), which encourages the disentanglement of latent factors in z. However,
VAE usually produces unrealistic and blurry images comparing to other generative models such as
GAN [15]. And encouraging the disentanglement can further weaken the reconstruction performance.
In addition, the learned latent factors in z are often exceedingly abstract and their correspondence to speciﬁc tasks can be obscure. Hence, it is challenging to derive task-disentanglement of input from the disentangled latent representation.
Information bottleneck (IB) [42, 43, 1] can potentially provide an information-theoretic perspective of the task-disentanglement problem. IB models the information ﬂow from input x to task output y through some latent representation z. In IB, the ideal z only preserves the minimum necessary information to predict y but discards all redundant information in x that cannot further contribute to predicting y. This leads to an optimization principle that maximizes the mutual information
I(z; y) and meanwhile keeps I(x; z) small. From IB’s perspective, for supervised learning such as classiﬁcation, maximizing I(z; y) corresponds to minimizing the classiﬁcation error of using z to predict the class y. In unsupervised learning models like VAE, maximizing I(z; y) corresponds to minimizing the reconstruction error (or maximizing the data likelihood).
In both cases, an additional constraint is adopted to limit the amount of information transmitted from x to z, forming
In β-VAE, the an “information bottleneck” that ﬁlters out information redundant to the task. constraint keeps the posterior p(z|x) sufﬁciently close to the standard Gaussian. For classiﬁcation, previous researches [45, 50] argue that the hierarchical structure of neural nets encourages more information compression as going to deeper layers and the deep learning scheme (SGD + cross entropy loss) inherently solves an IB problem. However, IB has been mainly studied for latent bottleneck representations z [37, 2] but underexplored to disentangle the task-relevant information in the input space. Speciﬁcally, it is non-trivial to design an IB constraint in the input space.
Figure 1: Architecture and Training/Inference of class-disentangled variational auto-encoder (CD-VAE).
In this paper, we focus on image classiﬁcation tasks and study “class-disentanglement” as a special case of “task-disentanglement”, i.e., how to decompose an input image x into x = G(x)+(x−G(x)), where G(x) contains all the information redundant or irrelevant to the class y, while x − G(x) captures the essential information to predict the class y. To this end, we develop a simple architecture composed of a variational autoencoder (VAE) G(·) generating G(x) from x and a classiﬁcation network D(·) that can be applied to both x − G(x) and x. A class-disentanglement model is achieved by jointly training G(·) to reconstruct x and D(·) to classify x − G(x), where each task forms an IB constraint for the other in the objective. Therefore, x − G(x) only competes the essential information for classiﬁcation with G(x) and G(x) preserves all the class-redundant information for better reconstruction.
Unlike disentanglement of latent factors and IB deﬁned in a latent space [5, 10, 24], class-disentanglement performs disentanglement of class-essential and class-redundant information in the more intricate but interpretable input space. Hence, it can be applied to analysing the behaviors of neural nets and intrinsic properties of complicated data, shedding novel insights that cannot be revealed by latent factors. For example, x − G(x) can highlight the areas in an image x that neural nets mainly attend to and help explain why correct or incorrect predictions are made. In this paper, 2
we mainly explore its applications on analyzing adversarial examples. In particular, we ﬁnd that the adversarial perturbations generated by mainstream attack algorithms primarily lie in x − G(x), while
G(x) still preserves uncontaminated information useful for classiﬁcation. Therefore, we propose to detect adversarial attacks on x − G(x) and defend the attacks by make predictions based on G(x). In extensive experiments on both CIFAR and ImageNet dataset, these simple strategies signiﬁcantly im-prove previous approaches directly applied on x and additionally provide novel interpretations to the results. We also present an empirical study of the trade-off between reconstruction and classiﬁcation when changing their weights in the objective. 2