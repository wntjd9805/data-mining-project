Abstract
Perhaps the single most important use case for differential privacy is to privately answer numerical queries, which is usually achieved by adding noise to the answer vector. The central question is, therefore, to understand which noise distribution optimizes the privacy-accuracy trade-off, especially when the dimension of the answer vector is high. Accordingly, an extensive literature has been dedicated to the question and the upper and lower bounds have been successfully matched up to constant factors [BUV18, SU17]. In this paper, we take a novel approach to address this important optimality question. We ﬁrst demonstrate an intriguing central limit theorem phenomenon in the high-dimensional regime. More precisely, we prove that a mechanism is approximately Gaussian Differentially Private [DRS21] if the added noise satisﬁes certain conditions. In particular, densities proportional to e−(cid:107)x(cid:107)α p is the standard (cid:96)p-norm, satisﬁes the conditions. Taking this perspective, we make use of the Cramer–Rao inequality and show a “uncer-tainty principle”-style result: the product of privacy parameter and the (cid:96)2-loss of the mechanism is lower bounded by the dimension. Furthermore, the Gaussian mechanism achieves the constant-sharp optimal privacy-accuracy trade-off among all such noises. Our ﬁndings are corroborated by numerical experiments. p , where x (cid:107) (cid:107) 1

Introduction
Introduced in [DMNS06], to date differential privacy (DP) is perhaps the most popular privacy deﬁnition. One of the most important applications of differential privacy is to answer numeric queries.
Given a function f of interest, which is also termed a query, our goal is to evaluate this (potentially vector-valued) query f on the sensitive data. To preserve privacy, a DP mechanism M working on a dataset D, in its simplest form, is deﬁned as
M (D) = f (D) + tX. (1)
Above, X denotes the noise term and t is a scalar, which together are selected depending on the properties of the query f and the desired privacy level. Among these, perhaps the most popular examples are the Laplace mechanism and the Gaussian mechanism where the noise X follows the
Laplace distribution and the Gaussian distribution, respectively.
∗the Institute for Data, Econometrics, Algorithms, and Learning 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Aside from privacy considerations, the most important criterion of an algorithm is arguably the estimation accuracy in the face of choosing, for example, between the Laplace mechanism or its Gaussian counterpart for a given problem. To be concrete, consider a real-valued query f with sensitivity 1—that is, ∆f = supD,D(cid:48)
= 1, where the supremum is over all neighboring datasets D and D(cid:48). Assuming (ε, 0)-DP for the mechanism M , we are interested in minimizing its (cid:96)2 loss deﬁned as f (D)
| f (D(cid:48))
−
| err(M ) := E(M (D) f (D))2 = E(tX)2 = t2EX 2.
−
This question is commonly2 addressed by setting X to a standard Laplace random variable and t = ε−1 [DMNS06]. This gives err(M ) = 2ε−2. Moving forward, we relax the privacy constraint from (ε, 0)-DP to (ε, δ)-DP for some small δ. The canonical way, which was born together with the notion of (ε, δ)-DP, is to add Gaussian noise [DKM+06]. A well-known result demonstrates that
Gaussian mechanism with X being the standard normal and t = 1 2 log(1.25δ−1) is (ε, δ)-DP (see,
ε e.g., [DR14]). The (cid:96)2-loss is err(M ) = t2 = 2ε−2
A quick comparison between the two errors reveals a surprising message. The latter error 2ε−2 log(1.25δ−1) is larger than the former 2ε−2. In fact, the extra factor log(1.25δ−1) is al-ready greater than 10 when δ = 10−5. At least on the surface, this observation contradicts the fact that (ε, δ)-DP is a relaxation of (ε, 0)-DP. Put differently, moving from Laplace to Gaussian, both privacy and accuracy get worse. Nevertheless, this contradiction suggests that we need a better alter-native to the Gaussian mechanism instead of giving up the notion of (ε, δ)-DP. Indeed, the truncated
Laplace mechanism has been proposed as a better alternative to achieve (ε, δ)-DP [GDGK20], which outperforms the Laplace mechanism in terms of estimation accuracy 3. (cid:112) log(1.25δ−1).
·
·
Motivated by these facts concerning the Laplace, Gaussian, and truncated Laplace mechanisms, one cannot help asking: (Q1) Why was the truncated Laplace mechanism not considered in the ﬁrst place? Are there any insights behind the design of such mechanisms? (Q2) More importantly, are these insights inherent for answering one-dimensional queries, or can we extend them to high-dimensional setting?
In this paper, we tackle these fundamental questions, beginning with explaining (Q1) in Section 2 from the decision-theoretic perspective of DP [WZ10, KOV17, DRS21]. However, our main focus is (Q2).
In addressing this question, we uncover a seemingly surprising phenomenon — it is impossible to utilize the (ε, δ) privacy budget in high-dimensional problems the same way as the truncated Laplace mechanism utilizes it in the one-dimensional problem. More speciﬁcally, we show a central limit behavior of the noise-addition mechanism in high dimensions, which, roughly speaking, says that for general noise distributions, the corresponding mechanisms all behave like a Gaussian mechanism. The formal language of “a mechanism behaves like the Gaussian mechanism” has been set up in [DRS21], where a notion called Gaussian Differential Privacy (GDP) was proposed. Roughly speaking, a mechanism is µ-GDP if it offers as much privacy as adding N (0, µ−2) noise to a sensitivity-1 query.
As in the (ε, δ)-DP case, the smaller µ is, the stronger privacy is offered.
To state our ﬁrst main contribution, let f be an n-dimensional query and assume that its (cid:96)2-sensitivity is 1. Consider the noise addition mechanism M (D) = f (D) + tX where X has a log-concave e−ϕ(x) on Rn. Let density n Fisher information matrix
∝
I and 2 be its operator norm.
X (cid:107)
Theorem 1.1 (Central Limit Theorem (Informal version of Theorem 3.1)). Under certain conditions on ϕ, for t = µ−1 2, the corresponding noise addition mechanism M deﬁned in Eq.(1) (cid:107) is asymptotically µ-GDP as the dimension n except for an o(1) fraction of directions of f (D)
ϕ(X)T ] be the n
X := E[ f (D(cid:48)).
→ ∞
ϕ(X) (cid:107)I (cid:107)I (cid:112)
∇
∇
×
X
· p (p, α (cid:62) 1) satisfy these technical conditions.
α
In particular, the norm power functions ϕ(x) = x (cid:107) (cid:107)
Note that this class already contains correlated noise, so the results in [DRS21] do not apply here.
Numerical results in Figure 1 shows that the convergence occurs for a dimension as small as 30.
− 2If f is integer-valued, then the doubly geometric distribution is a better choice and yields an (cid:96)2-loss of 2 < 2ε−2. In the so-called high privacy regime, i.e. ε → 0, the two (cid:96)2-losses have the same order in 1 2 sinh−2 ε the sense that their ratio goes to 1. 3One may blame the sub-optimality of the choice of t, but the problem remains even if the smallest possible t from [BW18] is applied. 2
Figure 1: Fast convergence to GDP as claimed in Theorem 1.1. Blue solid curves indicate the true privacy (i.e. ROC functions, see Section 2 for details) of the noise addition mechanism considered in Theorem 1.1. Red dashed curves are GDP limit predicted by our CLT. In all three panels the dimension n = 30. Numerical details can be found in the appendix. f (D(cid:48))”. Following the original deﬁnition,
We then elaborate on the condition “o(1) fraction of f (D)
DP or GDP is a condition that needs to hold for arbitrary neighboring datasets D and D(cid:48). This worst case perspective is exactly what prevents us to observe the central limit behavior. For example, consider a certain pair of datasets with f (D) = (0, 0, . . . , 0) and f (D(cid:48)) = (1, 0, . . . , 0), then privacy is completely determined by the ﬁrst marginal distribution of X, and the dimension n plays no role f (D(cid:48))” rules out the essentially low-dimension cases and reveals here. The “o(1) fraction of f (D)
− the truly high-dimensional behavior.
−
In summary, Theorem 1.1 suggests that when the dimension is high, a large class of noise addition mechanisms behave like the Gaussian mechanism, and hence are doomed to a poor use of the given (ε, δ) privacy budget, in the same fashion as we have seen in the one-dimensional example.
However, admitting the central limit phenomenon, our second theorem turns the table and charac-terizes the optimal privacy-accuracy trade-off and justiﬁes the Gaussian mechanism. To see this, recall that the noise addition mechanism deﬁned in Equation (1) is determined by the pair (t, X).
Both privacy and accuracy are jointly determined by t and X. Adopting the central limit theorem 1.1, it is convenient to take an equivalent parametrization, which is (µ, X), where µ is the desired (asymptotic) GDP parameter. Given X, the two parametrizations are related by t = µ−1 2. (cid:107)
Using parameters (µ, X), the corresponding mechanism Mµ,X is given by (cid:107)I (cid:112)
X
·
Mµ,X (D) = f (D) + µ−1 (cid:112)
·
X 2 (cid:107)
· (cid:107)I
X
By Theorem 1.1, it is asymptotically µ-GDP. The following theorem states in an “uncertainty principle” fashion that the privacy parameter and the error cannot be small at the same time.
Theorem 1.2. As long as the Fisher information of X is deﬁned, we have
The equality holds if X is n-dimensional standard Gaussian. err(Mµ,X ) (cid:62) n.
µ2
·
Combining Theorems 1.1 and 1.2, among all the noise that satisﬁes the conditions of Theorem 1.1,
Gaussian yields the constant-sharp optimal privacy-accuracy trade-off. As far as we know, this is the
ﬁrst result characterizing optimality with the sharp constant when the dimension is high.
The privacy conclusion of Theorem 1.1 does not work for every pair of neighboring datasets, so it is worth noting that we do NOT intend to suggest this as a valid privacy guarantee. Instead, we present it as an interesting phenomenon that has been largely overlooked in the literature. Furthermore, this central limit theorem admits an elegant characterization of privacy-accuracy trade-off that is sharp in constant. From a theoretical point of view, the proof of Theorem 1.1, as we shall see in later sections, involves non-linear functionals of high dimensional distributions. This type of results are, to the best of our knowledge, quite underexplored compared to linear functionals, so our results may serve as an additional motivation to study this type of questions. 3