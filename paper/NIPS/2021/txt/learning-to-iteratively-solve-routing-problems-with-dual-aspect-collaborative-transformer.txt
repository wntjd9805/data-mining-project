Abstract 
Recently, Transformer has become a prevailing deep architecture for solving vehicle  routing problems (VRPs).  However, it is less effective in learning improvement  models for VRP because its positional encoding (PE) method is not suitable in  representing VRP solutions. This paper presents a novel Dual-Aspect Collaborative 
Transformer (DACT) to learn embeddings for the node and positional features  separately, instead of fusing them together as done in existing ones, so as to avoid  potential noises and incompatible correlations. Moreover, the positional features  are embedded through a novel cyclic positional encoding (CPE) method to allow 
Transformer to effectively capture the circularity and symmetry of VRP solutions  (i.e., cyclic sequences). We train DACT using Proximal Policy Optimization and  design a curriculum learning strategy for better sample efﬁciency. We apply DACT  to solve the traveling salesman problem (TSP) and capacitated vehicle routing  problem (CVRP). Results show that our DACT outperforms existing Transformer  based improvement models, and exhibits much better generalization performance  across different problem sizes on synthetic and benchmark instances, respectively.  1

Introduction 
Vehicle Routing problems (VRPs), such as the Traveling Salesman Problem (TSP) and the Capacitated 
Vehicle Routing Problem (CVRP) which consider ﬁnding the optimal route for a single or ﬂeet of  vehicles to serve a set of customers, have ubiquitous real-world applications [1, 2]. Despite being  intensively studied in the Operations Research (OR) community, VRPs still remain challenging due  to their NP-hard nature [3]. Recent studies on learning neural heuristics are gathering attention as  promising extensions to traditional hand-crafted ones (e.g., [4–14]), where reinforcement learning  (RL) [15] is usually exploited to train a deep neural network as an efﬁcient solver without hand-crafted  rules. A salient motivation is that deep neural networks may learn better heuristics by identifying  useful patterns in an end-to-end and data-driven fashion. 
Solutions to VRPs, i.e., routes, are sequences of nodes (customer and depot locations).  Naturally,  deep models for Natural Language Processing (NLP), which deal with sequence data as well, are ideal 
∗Zhiguang Cao and Wen Song are the corresponding authors.  35th Conference on Neural Information Processing Systems (NeurIPS 2021). 
Figure 1: Transformer frameworks for VRPs. (a) Wu et al. [11] (the original one); (b) DACT (ours).  (a)  (b)  choices for encoding VRP solutions. Given its remarkable performance in NLP tasks, Transformer 
[16] is standing at the forefront in the learning based methods for VRPs (e.g., [5, 7, 8, 11–13, 17]). The  original Transformer encodes a sentence, i.e., a sequence of words, into a uniﬁed set of embeddings  by injecting word positional information into its word embeddings through positional encoding (PE). 
When it comes to VRPs, while is not required in construction models, positional information is  critical for deep models that learn improvement heuristics since the input are solutions to be improved. 
Although some success has been achieved, learning improvement heuristics for VRPs based on the  original Transformer encoder is yet lacking from our perspective. Firstly, directly applying addition  operation on PE vectors and the embeddings in absolute PE method (i.e., Figure 1(a)) could limit the  representation of the model [18], as the mixed correlations2  existing in the self-attention can bring  unreasonable noises and random biases to the encoder (details in Appendix A). Secondly, existing 
PE methods tend to fuse the node and positional information into one uniﬁed representation. NLP  tasks such as translation may beneﬁt from this owing to the deterministic and instructive nature of  the positional information. However, such design may not be optimal for routing tasks because the  positional information therein can be non-deterministic and sometimes even random. This may cause  disharmony or disturbance in the encoder and may thus deteriorate the performance. Finally, most 
VRPs seek the shortest loop of the nodes, making their solutions to be cyclic sequences. However,  existing PE methods are only designated to encode linear sequences3, which may fail to identify such  circular input. As will be shown in our experiments, this could severely damage the generalization  performance, since the cyclic feature of VRP solutions is not correctly reﬂected by the encoder. 
In this paper, we address the above issues and contribute to the line of using RL to learn neural  improvement heuristics for VRPs. We introduce the Dual-Aspect Collaborative Transformer (DACT),  where we revisit the solution representations and propose to learn separated groups of embeddings  for the node and positional features of a VRP solution as shown in Figure 1(b). Our DACT follows  the encoder-decoder structure. In the encoder, each set of embeddings encodes the solution mainly  from its own aspect, and at the same time exploits a cross-aspect referential attention mechanism for  better perceiving the consistence and differentiation with respect to the other aspect. The decoder  then collects action distribution proposals from the two aspects and synthesizes them to output the 
ﬁnal one.  Meanwhile, we design a novel cyclic positional encoding (CPE) method to capture the  circularity and symmetry of VRP solutions, which allows Transformer to encode cyclic inputs, and  also boost the generalization performance for solving VRPs. As the last contribution, we design a  simple yet effective curriculum learning strategy to improve the sample efﬁciency. This further leads  to faster and more stable convergence of RL training. Extensive experiments show that our DACT  can outperform existing Transformer based improvement models with fewer parameters, and also  generalizes well across different sizes of synthetic and benchmark instances, respectively.  2The term correlation refers to the dot product between Query and Key in the self-attention module. The  term mixed correlation refers to the case where Query and Key are projected from different types of embeddings.  3The relative PE method seems to help, however, it is found to be even worse than the absolute PE method  for VRPs in Wu et al. [11], partly due to the disharmony issue caused by learning the uniﬁed representation.  2 
2