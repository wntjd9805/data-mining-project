Abstract
The Sliced-Wasserstein distance (SW) is being increasingly used in machine learn-ing applications as an alternative to the Wasserstein distance and offers signiﬁcant computational and statistical beneﬁts. Since it is deﬁned as an expectation over random projections, SW is commonly approximated by Monte Carlo. We adopt a new perspective to approximate SW by making use of the concentration of measure phenomenon: under mild assumptions, one-dimensional projections of a high-dimensional random vector are approximately Gaussian. Based on this observation, we develop a simple deterministic approximation for SW. Our method does not require sampling a number of random projections, and is therefore both accurate and easy to use compared to the usual Monte Carlo approximation. We derive nonasymptotical guarantees for our approach, and show that the approximation error goes to zero as the dimension increases, under a weak dependence condition on the data distribution. We validate our theoretical ﬁndings on synthetic datasets, and illustrate the proposed approximation on a generative modeling problem. 1

Introduction
Recent years have witnessed the emergence of numerical methods inspired by optimal transport (OT) to solve machine learning problems. In particular, Wasserstein distances are a core ingredient of OT and deﬁne metrics between probability measures. Despite their nice theoretical properties, they are in general computationally expensive in large-scale settings. Several workarounds that scale better to large problems have been developed, and include the Sliced-Wasserstein distance (SW, [1, 2]).
The SW metric is a computationally cheaper alternative to Wasserstein as it exploits the analytical form of the Wasserstein distance between univariate distributions. More precisely, consider two random variables X and Y in Rd with respective distributions µ and ν, and denote by θ(cid:63) (cid:93) ν the univariate distributions of the projections of X, Y along θ ∈ Rd. SW then compares µ and ν by computing E[W(θ(cid:63) (cid:93) ν)], where the expectation E is taken with respect to θ uniformly distributed on the unit sphere, and W is the Wasserstein distance. (cid:93) µ, θ(cid:63) (cid:93) µ, θ(cid:63)
In practice, the expectation is typically estimated by Monte Carlo: one uniformly draws L projection (cid:93) ν(cid:1). Since the Wasserstein directions {θl}L l=1, and approximates SW with L−1 (cid:80)L i=1 W(cid:0)(θl)(cid:63) (cid:93) µ, (θl)(cid:63)
∗Corresponding author: kimia.nadjahi@telecom-paris.fr 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
distance between univariate distributions can easily be computed in closed-form, this scheme leads to signiﬁcant computational beneﬁts as compared to the Wasserstein distance, provided that L is not chosen too large. SW has then been successfully applied in several practical tasks, such as classiﬁcation [3, 4], Bayesian inference [5], the computation of barycenters of measures [1, 6], and implicit generative modeling [7–12]. Besides, SW has been shown to offer nice theoretical properties as well. Indeed, it satisﬁes the metric axioms [13], the estimators obtained by minimizing SW are asymptotically consistent [14], the convergence in SW is equivalent to the convergence in Wasserstein
[14, 15], and even though the sample complexity of Wasserstein grows exponentially with the data dimension [16–18], the sample complexity of SW does not depend on the dimension [19]. However, the latter study also demonstrated with a theoretical error bound, that the quality of the Monte Carlo estimate of SW depends on the number of projections and the variance of the one-dimensional
Wasserstein distances [19, Theorem 6]. In other words, to ensure that the induced approximation error is reasonably small, one might need to choose a large value for L, which inevitably increases the computational complexity of SW. Alternative approaches have been proposed to overcome this issue, and mainly consist in picking more “informative” projection directions: e.g., SW based on orthogonal projections [10, 20], maximum SW [21], generalized SW distances [22] and distributional
SW distances [23].
In this paper, we adopt a different perspective and leverage concentration results on random projec-tions to approximate SW: previous work showed that, under relatively mild conditions, the typical distribution of low-dimensional projections of high-dimensional random variables is close to some
Gaussian law [24, 25]. Recently, this phenomenon has been illustrated with a bound in terms of the
Wasserstein distance [26]: let {Xi}d i=1 be a sequence of real random variables with distribution µd, such that X1, . . . , Xd are independent with ﬁnite fourth-order moments; then, E[W(θ(cid:63) (cid:93) µd, Nµd )2] goes to zero as d increases, where Nµd is a univariate Gaussian distribution whose variance depends on µd and the expectation is taken with respect to a Gaussian variable θ. This result has very recently been used to bound the “maximum-sliced distance” between any probability measure and its Gaussian approximation [27]. In our work, we use it to design a novel technique that estimates SW with a simple deterministic formula. As opposed to Monte Carlo, our method does not depend on a ﬁnite set of random projections, therefore it eliminates the need of tuning the hyperparameter L and can lead to a signiﬁcant computational time reduction. Besides, our proposal is quite different from the aforementioned variants of SW which consist in selecting “informative” projection directions: these alternatives are deﬁned as optimization problems whose resolution is challenging (e.g., [23, Section 3.2]) and are then computed by ﬁnding an approximate solution. This incurs an additional computa-tional cost and estimation error, while our method directly approximates SW (thus, does not deﬁne an alternative distance) via simple deterministic operations, does not rely on any hyperparameters, and comes with theoretical guarantees on its induced error.
The important steps to formulate our approximate SW are summarized as follows. We ﬁrst deﬁne an alternative SW whose projection directions are drawn from the same Gaussian distribution as in [26], instead of uniformly on the unit sphere, and establish its relation with the original SW. By combining this property with [26, Theorem 1], we bound the absolute difference between SW applied to any two probability measures µd, νd on Rd, and the Wasserstein distance between the univariate
Gaussians Nµd , Nνd . Then, we explain why the mean parameters of µd and νd should be zero for the approximation error to decrease as d grows. Nevertheless, we show that it is not a limiting factor, by exploiting the following decomposition of SW: SW between µd, νd can be equivalently written as the sum of the difference between their means and the SW between the centered versions of µd, νd.
Our approach then consists in estimating SW between the centered versions with the Wasserstein term between Gaussian approximations to meet the zero-means condition, and recover SW between the original measures via the aforementioned property. Since the Wasserstein distance between
Gaussian distributions admits a closed-form solution, our approximate SW is very easy to compute, and faster than the Monte Carlo estimate obtained with a large number of projections. We derive nonasymptotical guarantees on the error induced by our approach. Speciﬁcally, we deﬁne a weak dependence condition under which the error is shown to go to zero with increasing d. Our theoretical results are then validated with experiments conducted on synthetic data. Finally, we leverage our theoretical insights to design a novel adversarial framework for a typical generative modeling problem in machine learning, and illustrate its advantages in terms of accuracy and computational time, over 2
generative models based on the Monte Carlo estimate of SW. Our empirical results can be reproduced with our open source code2. 2