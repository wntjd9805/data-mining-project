Abstract
We present a High-Resolution Transformer (HRFormer) that learns high-resolution representations for dense prediction tasks, in contrast to the original Vision Trans-former that produces low-resolution representations and has high memory and computational cost. We take advantage of the multi-resolution parallel design introduced in high-resolution convolutional networks (HRNet [46]), along with local-window self-attention that performs self-attention over small non-overlapping image windows [21], for improving the memory and computation efﬁciency. In addition, we introduce a convolution into the FFN to exchange information across the disconnected image windows. We demonstrate the effectiveness of the High-Resolution Transformer on both human pose estimation and semantic segmentation tasks, e.g., HRFormer outperforms Swin transformer [27] by 1.3 AP on COCO pose estimation with 50% fewer parameters and 30% fewer FLOPs. Code is available at: https://github.com/HRNet/HRFormer. 1

Introduction
Vision Transformer (ViT) [13] shows promising performance on ImageNet classiﬁcation tasks.
Many follow-up works boost the classiﬁcation accuracy through knowledge distillation [42], adopting deeper architecture [43], directly introducing convolution operations [16, 48], redesigning input image tokens [54], and etc. Besides, some studies attempt to extend the transformer to address broader vision tasks such as object detection [4], semantic segmentation [63, 37], pose estimation [51, 23], video understanding [61, 2, 30], and so on. This work focuses on the transformer for dense prediction tasks, including pose estimation and semantic segmentation.
Vision Transformer splits an image into a sequence of image patches of size 16 × 16, and extracts the feature representation of each image patch. Thus, the output representations of Vision Transformer lose the ﬁne-grained spatial details that are essential for accurate dense predictions. The Vision
Transformer only outputs a single-scale feature representation, and thus lacks the capability to handle multi-scale variation. To mitigate the loss of feature granularity and model the multi-scale variation, we present High-Resolution Transformer (HRFormer) that contains richer spatial information and constructs multi-resolution representations for dense predictions.
The High-Resolution Transformer is built by following the multi-resolution parallel design that is adopted in HRNet [46]. First, HRFormer adopts convolution in both the stem and the ﬁrst stage as several concurrent studies [11, 50] also suggest that convolution performs better in the early stages. Second, HRFormer maintains a high-resolution stream through the entire process with parallel medium- and low-resolution streams helping boost high-resolution representations. With feature maps of different resolutions, thus HRFormer is capable to model the multi-scale variation. Third,
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustrating the HRFormer block. The HRFormer block is composed of (a) local-window self-attentionm and (b) feed-forward network (FFN) with depth-wise convolution. The local-window self-attention scheme is inspired by the interlaced sparse self-attention [56, 21].
HRFormer mixes the short-range and long-range attention via exchanging multi-resolution feature information with the multi-scale fusion module.
At each resolution, the local-window self-attention mechanism is adopted to reduce the memory and computation complexity. We partition the representation maps into a set of non-overlapping small image windows and perform self-attention in each image window separately. This reduces the memory and computation complexity from quadratic to linear with respect to spatial size. We further introduce a 3 × 3 depth-wise convolution into the feed-forward network (FFN) that follows the local-window self-attention, to exchange information between the image windows which are disconnected in the local-window self-attention process. This helps to expand the receptive ﬁeld and is essential for dense prediction tasks. Figure 1 shows the details of an HRFormer block.
We conduct experiments on image classiﬁcation, pose estimation, and semantic segmentation tasks, and achieve competitive performance on various benchmarks. For example, HRFormer-B gains
+1.0% top-1 accuracy on ImageNet classiﬁcation over DeiT-B [42] with 40% fewer parameters and 20% fewer FLOPs. HRFormer-B gains 0.9% AP over HRNet-W48 [41] on COCO val set with with 32% fewer parameters and 19% fewer FLOPs. HRFormer-B + OCR gains +1.2% and
+2.0% mIoU over HRNet-W48 + OCR [55] with 25% fewer parameters and slightly more FLOPs on
PASCAL-Context test and COCO-Stuff test, respectively. 2