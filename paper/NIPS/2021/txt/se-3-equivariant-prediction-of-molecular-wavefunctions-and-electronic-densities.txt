Abstract
Machine learning has enabled the prediction of quantum chemical properties with high accuracy and efﬁciency, allowing to bypass computationally costly ab initio calculations. Instead of training on a ﬁxed set of properties, more recent approaches attempt to learn the electronic wavefunction (or density) as a central quantity of atomistic systems, from which all other observables can be derived. This is com-plicated by the fact that wavefunctions transform non-trivially under molecular rotations, which makes them a challenging prediction target. To solve this issue, we introduce general SE(3)-equivariant operations and building blocks for construct-ing deep learning architectures for geometric point cloud data and apply them to reconstruct wavefunctions of atomistic systems with unprecedented accuracy. Our model achieves speedups of over three orders of magnitude compared to ab initio methods and reduces prediction errors by up to two orders of magnitude compared to the previous state-of-the-art. This accuracy makes it possible to derive properties such as energies and forces directly from the wavefunction in an end-to-end manner.
We demonstrate the potential of our approach in a transfer learning application, where a model trained on low accuracy reference wavefunctions implicitly learns to correct for electronic many-body interactions from observables computed at a higher level of theory. Such machine-learned wavefunction surrogates pave the way towards novel semi-empirical methods, offering resolution at an electronic
∗These authors contributed equally.
†Work done at TU Berlin prior to joining Google Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
level while drastically decreasing computational cost. Additionally, the predicted wavefunctions can serve as initial guess in conventional ab initio methods, decreas-ing the number of iterations required to arrive at a converged solution, thus leading to signiﬁcant speedups without any loss of accuracy or robustness. While we focus on physics applications in this contribution, the proposed equivariant framework for deep learning on point clouds is promising also beyond, say, in computer vision or graphics. 1

Introduction
Machine learning (ML) methods are becoming increasingly popular in quantum chemistry as a means to circumvent expensive ab initio calculations, and led to advances in a broad range of applications, including the construction of potential energy surfaces [1–8], prediction of electron densities and density functionals [9–14], and development of models capable of predicting a range of physical observables across chemical space [15–29]. Typically, such models are trained on reference data for a predetermined set of quantum chemical properties and need to be retrained if other properties are required. However, if a model is capable of predicting the wavefunction, expectation values for any observable can be derived from it. Unfortunately, such an approach is complicated by the fact that wavefunctions are typically expressed in terms of rotationally equivariant basis functions, introducing non-trivial transformations under molecular rotations, which are difﬁcult to learn from data. To solve this issue, we propose several SE(3)-equivariant operations for deep learning architectures for geometric point cloud data, which capture the effects of translations and rotations without needing to learn them explicitly. We assemble these building blocks to construct PhiSNet, a novel deep learning (DL) architecture for predicting wavefunctions and electronic densities, which is signiﬁcantly more accurate than non-equivariant models. For the ﬁrst time, sufﬁcient accuracy is reached to predict properties like energies and forces directly from the wavefunction and in end-to-end manner.
This makes it possible to learn wavefunctions that lead to modiﬁed properties, which is interesting from an inverse design perspective; or the development of novel machine-learned semi-empirical methods, for example by learning a correction to the wavefunction that mimics the effects of electron correlation. Such hybrid methods maintain the accuracy and generality of high level electronic structure calculations while drastically reducing their computational cost. In addition, the predicted wavefunctions can serve as initial guess to speed up conventional ab initio methods.
Beyond physics, other applications of our proposed equivariant DL architecture to e.g. computer vision or graphics are conceivable – whenever accurate invariant analyses of high dimensional point clouds are of importance.
In summary, this work provides the following contributions:
• We describe general SE(3)-equivariant operations and building blocks for constructing DL architectures for geometric point cloud data.
• We propose PhiSNet, a neural network for predicting wavefunctions and electronic densities from equivariant atomic representations, ensuring physically correct transformation under translations and rotations.
• We apply PhiSNet to predict wavefunctions and electronic densities of several molecules and show that our model reduces prediction errors of electronic structure properties by a factor of up to two orders of magnitude compared to the previous state-of-the-art and achieves speedups of over three orders of magnitude compared to ab initio solutions.
• We showcase a novel transfer-learning application, where a model trained on low accuracy wavefunctions is adapted to predict properties computed at a higher level of theory by learning a correction that implicitly captures the effects of many-body electron correlation.
• We demonstrate that the predicted wavefunctions can serve as initial guess in conventional quantum chemistry methods, leading to signiﬁcant speedups without sacriﬁcing the accuracy or robustness of ab initio solutions.
In principle, our method could also be used to construct orbital features as inputs for methods like
OrbNet [30], which otherwise rely on semi-empirical or ab initio methods. 2
Figure 1: A: Illustration of an aspirine molecule and its highest occupied molecular orbital (HOMO) in three different orientations, showing how the wavefunction (left) and Hamiltonian matrix (right) change with respect to rotations. B: Overview of the proposed PhiSNet architecture. The atomic representation network creates atom-wise equivariant features, which are used to produce self-interaction and pair-interaction features (Fig. S2), from which the Hamiltonian matrix is constructed block-by-block (Fig. S5). C: Visualisation of electronic densities (squared wavefunction) of various molecules predicted with our approach. D: Illustration of a transfer learning application, where a model pretrained on Hartree-Fock (HF) Hamiltonians is ﬁne-tuned to match energies and forces derived from highly accurate coupled cluster (CC) calculations. The model achieves this by learning a correction to the Hamiltonian matrix, which mimics the effects of many-body electron correlation.
The effective “CC-level” Hamiltonian can be thought of as a HF-level Hamiltonian plus a correction term. The
HOMO is shown to visualize subtle changes to the wavefunction (the correction is ampliﬁed in magnitude by a factor of 103 for better visibility). 2