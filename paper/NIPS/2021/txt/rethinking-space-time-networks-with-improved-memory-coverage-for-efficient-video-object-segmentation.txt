Abstract
This paper presents a simple yet effective approach to modeling space-time cor-respondences in the context of video object segmentation. Unlike most existing approaches, we establish correspondences directly between frames without re-encoding the mask features for every object, leading to a highly efﬁcient and robust framework. With the correspondences, every node in the current query frame is inferred by aggregating features from the past in an associative fashion. We cast the aggregation process as a voting problem and ﬁnd that the existing inner-product afﬁnity leads to poor use of memory with a small (ﬁxed) subset of memory nodes dominating the votes, regardless of the query. In light of this phenomenon, we propose using the negative squared Euclidean distance instead to compute the afﬁnities. We validate that every memory node now has a chance to contribute, and experimentally show that such diversiﬁed voting is beneﬁcial to both memory efﬁciency and inference accuracy. The synergy of correspondence networks and diversiﬁed voting works exceedingly well, achieves new state-of-the-art results on both DAVIS and YouTubeVOS datasets while running signiﬁcantly faster at 20+
FPS for multiple objects without bells and whistles. 1

Introduction
Video object segmentation (VOS) aims to identify and segment target instances in a video sequence.
This work focuses on the semi-supervised setting where the ﬁrst-frame segmentation is given and the algorithm needs to infer the segmentation for the remaining frames. This task is an extension of video object tracking [1, 2], requiring detailed object masks instead of simple bounding boxes.
A high-performing algorithm should be able to delineate an object from the background or other distractors (e.g., similar instances) under partial or complete occlusion, appearance changes, and object deformation [3].
Most current methods either ﬁt a model using the initial segmentation [4, 5, 6, 7, 8, 9] or leverage temporal propagation [10, 11, 12, 13, 14, 15, 16], particularly with spatio-temporal matching [17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]. Space-Time Memory networks [18] are especially popular recently due to its high performance and simplicity – many variants [22, 16, 23, 21, 24, 28, 29, 30], including competitions’ winners [31, 32], have been developed to improve the speed, reduce memory usage, or to regularize the memory readout process of STM.
In this work, we aim to subtract from STM to arrive at a minimalistic form of matching networks, dubbed Space-Time Correspondence Network (STCN) 1. Speciﬁcally, we start from the basic premise
†This work was done in The Hong Kong University of Science and Technology. 1Training/inference code and pretrained models: https://github.com/hkchengrex/STCN 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
that correspondences are target-agnostic. Instead of building a speciﬁc memory bank and therefore afﬁnity for every object in the video as in STM, we build a single afﬁnity matrix using only RGB relations. For querying, each target object passes through the same afﬁnity matrix for feature transfer.
This is not only more efﬁcient but also more robust – the model is forced to learn all object relations beyond just the labeled ones. With the learned afﬁnity, the algorithm can propagate features from the
ﬁrst frame to the rest of the video sequence, with intermediate features stored as memory.
While STCN already reaches state-of-the-art performance and speed in this simple form, we further probe into the inner workings of the construction of afﬁnities. Traditionally, afﬁnities are constructed from dot products followed by a softmax as in attention mechanisms [18, 33]. This however implicitly encoded “conﬁdence” (magnitude) with high-conﬁdence points dominating the afﬁnities all the time, regardless of query features. Some memory nodes will therefore be always suppressed, and the (large) memory bank will be underutilized, reducing effective diversity and robustness. We ﬁnd this to be harmful, and propose using the negative squared Euclidean distance as a similarity measure with an efﬁcient implementation instead. Though simple, this small change ensures that every memory node has a chance to contribute signiﬁcantly (given the right query), leading to better performance, higher robustness, and more efﬁcient use of memory.
Our contribution is three-fold:
• We propose STCN with direct image-to-image correspondence that is simpler, more efﬁcient, and more effective than STM.
• We examine the afﬁnity in detail, and propose using L2 similarity in place of dot product for a better memory coverage, where every memory node contributes instead of just a few.
• The synergy of the above two results in a simple and strong method, which suppresses previous state-of-the-art performance without additional complications while running fast at 20+ FPS. 2