Abstract
The backpropagation networks are notably susceptible to catastrophic forgetting, where networks tend to forget previously learned skills upon learning new ones.
To address such the ’sensitivity-stability’ dilemma, most previous efforts have been contributed to minimizing the empirical risk with different parameter regularization terms and episodic memory, but rarely exploring the usages of the weight loss landscape. In this paper, we investigate the relationship between the weight loss landscape and sensitivity-stability in the continual learning scenario, based on which, we propose a novel method, Flattening Sharpness for Dynamic Gradient
Projection Memory (FS-DGPM). In particular, we introduce a soft weight to represent the importance of each basis representing past tasks in GPM, which can be adaptively learned during the learning process, so that less important bases can be dynamically released to improve the sensitivity of new skill learning. We further introduce Flattening Sharpness (FS) to reduce the generalization gap by explicitly regulating the ﬂatness of the weight loss landscape of all seen tasks. As demonstrated empirically, our proposed method consistently outperforms baselines with the superior ability to learn new skills while alleviating forgetting effectively.2. 1

Introduction
Humans have the ability to continually learn new knowledge without forgetting their previously learned ones through mediating a rich set of neurocognitive mechanisms [41, 15, 39]. This ability, often known as continual learning or lifelong learning [29], is crucial for computational systems, such as deep neural networks (DNNs), which are required to sequentially learn and deal with multiple tasks when implemented in the dynamically changing environment. Continual learning remains a long-standing challenge for DNNs since these networks are typically trained with stationary training batches by stochastic gradient descent methods [19], which generally leads to an abrupt performance decrease on previously learned tasks as new tasks are learned. To address such catastrophic forgetting, we can brutally retrain an oracle network on the entire dataset containing all tasks to capture dynamic changes in the data distribution, but this methodology is obviously too inefﬁcient to hinder the learning of novel data in real time.
During the last few years, lots of research efforts have been devoted to improving the stability of
DNNs on old tasks while keeping sensitive to new information. The ﬁrst intuitive idea is to introduce
∗Corresponding author: Guangyong Chen. <gy.chen@siat.ac.cn> 2The code is available at: https://github.com/danruod/FS-DGPM 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
an independent branch for each new task while freezing the old task parameters to preserve the old knowledge [33, 45, 43, 25, 35, 22]. However, in this way, the network will inevitably become redundant as the task number continually increases. As presented in the neurocognitive works [41, 15], the reactivation of neuronal activity patterns, representing old memories, plays an important role in the continual learning of humans [39]. Thus, forgetness can be effectively mitigated by training a single network for new tasks by considering diverse information stored in the memory, including the original training samples of old tasks [31, 5, 13], the gradients induced from old tasks [9] and the feature subspace representing old tasks [34]. However, their continual learning performance is still limited because DNNs can easily overﬁt the limited information stored in the small-size memory.
The overﬁtting problem of DNNs is often attributed to the complex loss landscape containing multiple local optima, and the sharpness of the loss landscape has been widely used to characterize the generalization gap in standard training scenarios from both theoretical and empirical perspectives
[27, 21, 42, 10, 6]. While this characterization has inspired new approaches for model training with better generalization, practical algorithms that especially seek out ﬂatter minima to effectively address the ’sensitivity-stability’ dilemma for continual learning have thus far been elusive. In this paper, our
ﬁrst contribution is to characterize the weight loss landscape for the continual learning scenario and identify that a ﬂatter loss landscape with lower loss value often leads to better continual learning performance, as shown in Figure 1 and Figure 3.
Further, based on our characterization of the weight loss landscape, we ﬁnd that the recently proposed
Gradient Projection Memory (GPM) method [34] maintains the lowest loss value on old tasks among the previously proposed methods by taking gradient steps orthogonal to the subspace representing old tasks. However, its loss landscape on newly learned tasks is the sharpest due to the lack of sufﬁcient subspace left for new task learning. To improve the network’s sensitivity, our second contribution is to predict the importance of bases spanning the subspace for old tasks, so that less important bases can be dynamically released. In particular, we introduce a soft weight to indicate the bases importance, which can be dynamically adjusted by combining the Flattening Sharpness (FS) to minimize the loss value and loss sharpness simultaneously. Intuitively, a basis will be regarded as important for preserving old knowledge if the gradients induced by new tasks and old ones are aligned in the opposite direction on that basis. As demonstrated through extensive experiments, our proposed method can consistently outperform the state-of-the-art methods [17, 30, 24, 31, 5, 13, 34] by a notable margin across a range of widely used benchmark datasets. 2