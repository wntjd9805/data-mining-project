Abstract
We consider the problem of optimizing combinatorial spaces (e.g., sequences, trees, and graphs) using expensive black-box function evaluations. For example, optimizing molecules for drug design using physical lab experiments. Bayesian op-timization (BO) is an efﬁcient framework for solving such problems by intelligently selecting the inputs with high utility guided by a learned surrogate model. A recent
BO approach for combinatorial spaces is through a reduction to BO over continu-ous spaces by learning a latent representation of structures using deep generative models (DGMs). The selected input from the continuous space is decoded into a discrete structure for performing function evaluation. However, the surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box function.
To overcome this drawback, this paper proposes a principled approach referred as
LADDER. The key idea is to deﬁne a novel structure-coupled kernel that explicitly integrates the structural information from decoded structures with the learned latent space representation for better surrogate modeling. Our experiments on real-world benchmarks show that LADDER signiﬁcantly improves over the BO over latent space method, and performs better or similar to state-of-the-art methods. 1

Introduction
A huge range of science and engineering applications involve optimizing combinatorial spaces (e.g., sequences, trees, graphs) using expensive black-box function evaluations [18, 70, 17]. For example, in drug design application, each candidate structure is a molecule and evaluation involves performing an expensive physical lab experiment. Bayesian optimization (BO) [59, 20] is an effective framework for optimizing expensive black-box functions and has shown great success in practice [63, 74, 21, 6, 5].
The key idea is to learn a cheap-to-evaluate surrogate statistical model, e.g., Gaussian process (GP), from past function evaluations and employ it to select inputs with high-utility for evaluation. BO for combinatorial spaces is a relatively less-studied problem with many challenges in the small-data setting (number of function evaluations is small).
A recent approach to solve some of these challenges is through a reduction to BO over continuous spaces, which we refer as BO over latent space [26, 68]. This method relies on two ideas. First, we learn a latent space representation from a database of unsupervised structures (i.e., function evaluations are not available) using a encoder-decoder style deep generative model (DGM) [37, 33].
Second, we build a surrogate model over this latent space to perform BO. Each selected input from the latent space is decoded into a structured object for performing function evaluation. BO over latent space has shown good success when the number of function evaluations is large [26, 68, 19, 34].
However, we conjecture that this approach may not be effective in the small-data settings for the following reasons. 1) The surrogate model over the latent space only uses the information learned by the DGM, which may not have the desired inductive bias to approximate the target black-box 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
function. 2) The learned surrogate model may not generalize well beyond the training instances from the latent space. Indeed, our experiments on real-world problems demonstrate the ineffectiveness of this Naïve surrogate model over the latent space.
In this paper, we propose a novel approach referred as LADDER to overcome the above drawbacks for the small-data setting. We name it LADDER because it acts as a ladder in connecting the rich structural information of structures in the combinatorial space with their corresponding latent space representations. Intuitively, it improve surrogate modeling by combining the best of both representations. We provide a principled Gaussian process based method to integrate the latent space representation with the structural information from decoded structures using a novel structure-coupled kernel. The key insight is to extend a kernel over the latent space (with its hyper-parameters estimated on the evaluated points) to non-evaluated points in the space by utilizing the features of structured kernels deﬁned over combinatorial spaces (e.g., string kernels and graph kernels). LADDER has multiple advantages. First, we can leverage a large body of research on kernels over structured data.
Second, allows the use of deep generative models (DGMs) to learn latent space as a plug-and-play technology. This means advances in DGMs will directly improve the effectiveness of LADDER. Our experiments on real-world benchmarks show that LADDER performs better or similar to state-of-the-art methods, and signiﬁcantly better than the Naïve latent space BO approach in our problem setting. We also empirically demonstrate that superiority of LADDER’s performance is due to better surrogate model resulting from the proposed method to combine representations.
Contributions. The key contribution of this paper is the development and evaluation of the LADDER approach to perform BO over combinatorial spaces in the small-data setting. Speciﬁc list includes:
• Identifying the key reasons for the ineffectiveness of the Naïve latent space BO method in the small-data setting and providing empirical evidence on real-world problems.
• Development of a principled Gaussian process approach for improved surrogate modeling by combining the structural information from decoded structures with the learned latent space representation using a novel structure-coupled kernel.
• Experiments on real-world benchmarks to show the efﬁcacy of LADDER over prior methods in our problem setting. The code and data are available on the GitHub repository link https://github.com/aryandeshwal/LADDER. 2 Problem Setup and