Abstract
In real-world decision making tasks, it is critical for data-driven reinforcement learning methods to be both stable and sample efficient. On-policy methods typi-cally generate reliable policy improvement throughout training, while off-policy methods make more efficient use of data through sample reuse. In this work, we combine the theoretically supported stability benefits of on-policy algorithms with the sample efficiency of off-policy algorithms. We develop policy improvement guarantees that are suitable for the off-policy setting, and connect these bounds to the clipping mechanism used in Proximal Policy Optimization. This motivates an off-policy version of the popular algorithm that we call Generalized Proxi-mal Policy Optimization with Sample Reuse. We demonstrate both theoretically and empirically that our algorithm delivers improved performance by effectively balancing the competing goals of stability and sample efficiency. 1

Introduction
In recent years, model-free deep reinforcement learning has been used to successfully solve complex simulated control tasks [4]. Unfortunately, real-world adoption of these techniques remains limited.
High-stakes real-world decision making settings demand methods that deliver stable, reliable perfor-mance throughout training. In addition, real-world data collection can be difficult and expensive, so learning must make efficient use of limited data. The combination of these requirements is not an easy task, as stability and sample efficiency often represent competing interests. Existing model-free deep reinforcement learning algorithms often focus on one of these goals, and as a result sacrifice performance with respect to the other.
On-policy reinforcement learning methods such as Proximal Policy Optimization (PPO) [19] deliver stable performance throughout training due to their connection to theoretical policy improvement guarantees. These methods are motivated by a lower bound on the expected performance loss at every update, which can be approximated using samples generated by the current policy. The theoretically supported stability of these methods is very attractive, but the need for on-policy data and the high-variance nature of reinforcement learning often requires significant data to be collected between every update, resulting in high sample complexity and slow learning. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Off-policy algorithms address the issue of high sample complexity by storing samples in a replay buffer, which allows data to be reused to calculate multiple policy updates. The ability to reuse samples improves learning speed, but also causes the distribution of data to shift away from the distribution generated by the current policy. This distribution shift invalidates the standard performance guarantees used in on-policy methods, and can lead to instability in the training process. Popular off-policy algorithms often require various implementation tricks and extensive hyperparameter tuning to control the instability caused by off-policy data.
By combining the attractive features of on-policy and off-policy methods in a principled way, we can balance the competing goals of stability and sample efficiency required in real-world decision making. We consider the popular on-policy algorithm PPO as our starting point due to its theoretically supported stable performance, and develop an off-policy variant with principled sample reuse that we call Generalized Proximal Policy Optimization with Sample Reuse (GePPO). Our algorithm is based on the following main contributions: 1. We extend existing policy improvement guarantees to the off-policy setting, resulting in a lower bound that can be approximated using data from all recent policies. 2. We develop connections between the clipping mechanism used in PPO and the penalty term in our policy improvement lower bound, which motivates a generalized clipping mechanism for off-policy data. 3. We propose an adaptive learning rate method based on the same penalty term that more closely connects theory and practice.
We provide theoretical evidence that our algorithm effectively balances the goals of stability and sample efficiency, and we demonstrate the strong performance of our approach through experiments on high-dimensional continuous control tasks in OpenAI Gymâ€™s MuJoCo environments [3, 21]. 2