Abstract
We present SegFormer, a simple, efﬁcient yet powerful semantic segmentation framework which uniﬁes Transformers with lightweight multilayer perceptron (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed
MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efﬁcient segmentation on Transformers. We scale our approach up to obtain a series of models from
SegFormer-B0 to SegFormer-B5, reaching signiﬁcantly better performance and efﬁciency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5× smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on
Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C.
Code is available at: github.com/NVlabs/SegFormer. 1

Introduction
Semantic segmentation is a fundamental task in computer vision and enables many downstream applications. It is related to image classiﬁcation since it produces per-pixel category prediction instead of image-level prediction. This relation-ship is pointed out and systematically studied in a seminal work [1], where the authors used fully convolutional networks (FCNs) for semantic seg-mentation tasks. Since then, FCN has inspired many follow-up works and has become a predom-inant design choice for dense prediction.
Since there is a strong relation between classi-ﬁcation and semantic segmentation, many state-of-the-art semantic segmentation frameworks are variants of popular architectures for image classi-ﬁcation on ImageNet. Therefore, designing back-bone architectures has remained an active area
∗Corresponding authors: Zhiding Yu and Ping Luo
Figure 1: Performance vs. model efﬁciency on ADE20K. SegFormer achieves a new state-of-the-art 51.0% mIoU while being signiﬁcantly more efﬁcient than previous methods. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
in semantic segmentation. Indeed, starting from early methods using VGGs [1, 2], to the latest methods with signiﬁcantly deeper and more powerful backbones [3], the evolution of backbones has dramatically pushed the performance boundary of semantic segmentation. Besides backbone architectures, another line of work formulates semantic segmentation as a structured prediction problem, and focuses on designing modules and operators, which can effectively capture contextual information. A representative example in this area is dilated convolution [4, 5], which increases the receptive ﬁeld by “inﬂating” the kernel with holes.
Witnessing the great success in natural language processing (NLP), there has been a recent surge of interest to introduce Transformers to vision tasks. Dosovitskiy et al. [6] proposed vision Transformer (ViT) for image classiﬁcation. Following the Transformer design in NLP, the authors split an image into multiple linearly embedded patches and feed them into a standard Transformer with positional embeddings (PE), leading to an impressive performance on ImageNet. In semantic segmentation,
Zheng et al. [7] proposed SETR to demonstrate the feasibility of using Transformers in this task.
SETR adopts ViT as a backbone and incorporates several CNN decoders to enlarge feature resolution.
Despite the good performance, ViT has two important limitations: 1) ViT outputs single-scale low-resolution features instead of multi-scale ones, and 2) it has very high computational cost on large images. To address these limitations, Wang et al. [8] proposed a pyramid vision Transformer (PVT), a natural extension of ViT with pyramid structures for dense prediction. PVT shows considerable improvements over the ResNet counterpart on object detection and semantic segmentation. However, together with other emerging methods such as Swin Transformer [9] and Twins [10], these methods mainly consider the design of the Transformer encoder, neglecting the contribution of the decoder for further improvements.
This paper introduces SegFormer, a cutting-edge Transformer framework for semantic segmentation that jointly considers efﬁciency, accuracy, and robustness. In contrast to previous methods, our framework redesigns both the encoder and the decoder. The key novelties of our approach are:
• A novel positional-encoding-free and hierarchical Transformer encoder.
• A lightweight All-MLP decoder design that yields a powerful representation without complex and computationally demanding modules.
• As shown in Figure 1, SegFormer sets new a state-of-the-art in terms of efﬁciency, accuracy and robustness in three publicly available semantic segmentation datasets.
First, the proposed encoder avoids interpolating positional codes when performing inference on images with resolutions different from the training one. As a result, our encoder can easily adapt to arbitrary test resolutions without impacting the performance. In addition, the hierarchical part enables the encoder to generate both high-resolution ﬁne features and low-resolution coarse features, this is in contrast to ViT that can only produce single low-resolution feature maps with ﬁxed resolutions.
Second, we propose a lightweight MLP decoder where the key idea is to take advantage of the
Transformer-induced features where the attentions of lower layers tend to stay local, whereas the ones of the highest layers are highly non-local. By aggregating the information from different layers, the MLP decoder combines both local and global attention. As a result, we obtain a simple and straightforward decoder that renders powerful representations.
We demonstrate the advantages of SegFormer in terms of model size, run-time, and accuracy on three publicly available datasets: ADE20K, Cityscapes, and COCO-Stuff. On Citysapces, our lightweight model, SegFormer-B0, without accelerated implementations such as TensorRT, yields 71.9% mIoU at 48 FPS, which, compared to ICNet [11], represents a relative improvement of 60% and 4.2% in latency and performance, respectively. Our largest model, SegFormer-B5, yields 84.0% mIoU, which represents a relative 1.8% mIoU improvement while being 5 × faster than SETR [7]. On ADE20K, this model sets a new state-of-the-art of 51.8% mIoU while being 4 × smaller than SETR. Moreover, our approach is signiﬁcantly more robust to common corruptions and perturbations than existing methods, therefore being suitable for safety-critical applications. Code will be publicly available. 2
Figure 2: The proposed SegFormer framework consists of two main modules: A hierarchical Transformer encoder to extract coarse and ﬁne features; and a lightweight All-MLP decoder to directly fuse these multi-level features and predict the semantic segmentation mask. “FFN” indicates feed-forward network. 2