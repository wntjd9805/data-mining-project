Abstract
Piano transcription systems are typically optimized to estimate pitch activity at each frame of audio. They are often followed by carefully designed heuristics and post-processing algorithms to estimate note events from the frame-level predic-tions. Recent methods have also framed piano transcription as a multi-task learn-ing problem, where the activation of different stages of a note event are estimated independently. These practices are not well aligned with the desired outcome of the task, which is the speciﬁcation of note intervals as holistic events, rather than the aggregation of disjoint observations. In this work, we propose a novel formu-lation of piano transcription, which is optimized to directly predict note events.
Our method is based on Semi-Markov Conditional Random Fields (semi-CRF), which produce scores for intervals rather than individual frames. When formu-lating piano transcription in this way, we eliminate the need to rely on disjoint frame-level estimates for different stages of a note event. We conduct experiments on the MAESTRO dataset and demonstrate that the proposed model surpasses the current state-of-the-art for piano transcription. Our results suggest that the semi-CRF output layer, while still quadratic in complexity, is a simple, fast and well-performing solution for event-based prediction, and may lead to similar success in other areas which currently rely on frame-level estimates. 1

Introduction
The task of Automatic Music Transcription (AMT) aims to transcribe a music recording into some form of music notation [Benetos et al., 2018]. Examples of notation include MIDI event sequences, e.g, Hawthorne et al. [2018], Kong et al. [2020], Kim and Bello [2019], Kwon et al. [2020], and staff notation, e.g., Nakamura et al. [2018], Román et al., 2019]. In this work, we address the problem of transcribing piano music into a MIDI event sequence. MIDI transcription involves constructing a sequence of events, each speciﬁed by its onset and offset positions, with the constraint that two events of the same event type (e.g., certain pitches and pedals) cannot overlap. In addition to onsets and offsets, the velocity (i.e., a value that represents the intensity of a key strike, which informs the loudness) associated with each event is often estimated.
In recent years, neural network based approaches have reached the state of the art for the problem of piano transcription, e.g., Hawthorne et al. [2018], Kong et al. [2020], Kwon et al. [2020]. They operate at the frame-level and make predictions for different stages of a note event, i.e., the onset, offset, and pitch activation, separately. In order to extract note-level predictions, they use manually designed procedures to combine the disjoint frame-level predictions. These include thresholding,
∗Code is available at https://github.com/Yujia-Yan/Skipping-The-Frame-Level 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
peak picking [Hawthorne et al., 2018, Kong et al., 2020] or low-order hidden Markov models [Kwon et al., 2020]. This two-stage approach requires manually crafted procedures and a manually designed state structure modeling the temporal evolution of notes.
In this work, we propose a direct approach to note-level transcription. Instead of scoring and aggre-gating note activations across individual time frames, our approach directly scores a time interval’s likelihood of covering the entire process of a certain event (e.g., a note, pedal usage). This greatly simpliﬁes the formulation of piano transcription. Speciﬁcally, the proposed method takes the log-mel spectrogram of an audio segment (e.g., 10s) as input, and uses a contextual model to produce contextual embeddings for each frame. It then uses a score model to score each possible time interval within the segment to assess whether it covers the entire span of a musical event (e.g., notes, ped-als) as speciﬁed by a specialized zeroth-order semi-Markov conditional random ﬁelds (semi-CRF)
[Sarawagi and Cohen, 2004]. These scores are then decoded using Viterbi algorithm into a set of non-overlapping intervals for each event type, i.e., note-level transcription. Finally, other attributes of these intervals (i.e., velocity and reﬁned boundary positions) are also estimated.
Previous works using semi-CRF were concerned with shorter sequence lengths and smaller interval lengths [Kong et al., 2016, Liu et al., 2016, Kemos et al., 2019, Lu et al., 2016], and usually only attempted to estimate a single channel (track) of non-overlapping events. In contrast, our problem deals with sequences of 400-1600 frames (approximately 10s-40s) and requires a separate semi-CRF for around 90 different labelling channels (88 pitches + 1-3 pedals), each corresponding to a speciﬁc event type. Another challenge is the wide range of event duration that can be encountered in piano music, which further increases the computational expenses of the problem. These chal-lenges prevented us from using existing general purpose semi-CRF methods due to computational expenses. Instead, we introduce a zeroth-order Semi-CRF formulation that is speciﬁcally adapted to the problem and show that it can be implemented efﬁciently (see Table 3) for the problem size of interest.
Our experiments on the MAESTRO dataset show that the proposed system achieves Note w/ Offset
F1 of 88.72% and Note w/ Offset & Velocity F1 of 87.75%, exceeding the current state of the art while being smaller and faster. We believe that this simple, fast and well-performing approach is also extensible to other similar tasks with intervals as the prediction target, such as polyphonic sound event detection or speaker diarization. 2