Abstract
Recently, a series of algorithms have been explored for GAN compression, which aims to reduce tremendous computational overhead and memory usages when deploying GANs on resource-constrained edge devices. However, most of the existing GAN compression work only focuses on how to compress the generator, while fails to take the discriminator into account. In this work, we revisit the role of discriminator in GAN compression and design a novel generator-discriminator co-operative compression scheme for GAN compression, termed GCC. Within GCC, a selective activation discriminator automatically selects and activates convolutional channels according to a local capacity constraint and a global coordination con-straint, which help maintain the Nash equilibrium with the lightweight generator during the adversarial training and avoid mode collapse. The original generator and discriminator are also optimized from scratch, to play as a teacher model to progressively reﬁne the pruned generator and the selective activation discriminator.
A novel online collaborative distillation scheme is designed to take full advantage of the intermediate feature of the teacher generator and discriminator to further boost the performance of the lightweight generator. Extensive experiments on vari-ous GAN-based generation tasks demonstrate the effectiveness and generalization of GCC. Among them, GCC contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks. Our code and models are available at: https://github.com/SJLeo/GCC 1

Introduction
Generative Adversarial Networks (GANs) have been widely popularized in diverse image synthesis tasks, such as image generation [16, 50], image translation [64, 30] and super resolution [35, 59].
However, the ultra-high computational and memory overhead of GANs hinder its deployment on resource-constrained edge devices. To alleviate this issue, a series of traditional model compression algorithms, i.e., network pruning [18, 36], weight quantization [39, 26], low-rank decomposition [10, 19], knowledge distillation [22, 52], and efﬁcient architecture design [29, 55] have been applied to reduce calculational consumptions of GANs.
Previous work [56, 12, 61] attempted to directly employ network pruning methods to compress the generator but obtain unimpressive results, as shown in Figure 1(a). The similar phenomenon also occurred in SAGAN shown in Appendix A. A potential reason is that these methods failed to take into account the generator and the discriminator must follow the Nash equilibrium state to avoid the mode
∗This work was done when Shaojie Li was a intern at Bytedance Inc.
†Corresponding Author: rrji@xmu.edu.cn 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Visualization of mode collapse (b) Original generator + original discriminator (c) 1/4 channels width generator + original discriminator
Figure 1: Illustration of model collapse phenomenon. The experiment is conducted on Pix2Pix [30] based on the Cityscapes [8] dataset. (a) shows the inﬂuence of mode collapse in generating images.
Each image generated by the 1/4 channels width generator appears the forgery trace, which is marked by the red box. (b) and (c) show loss curves of the original generator and the 1/4 channels width generator with the original discriminator, respectively. (a) SAGAN (b) Pix2Pix
Figure 2: The performance of generators with different compression ratios on various channel number discriminators. (a) is the result of SAGAN [63] on CelebA [44] dataset. (b) is the result of Pix2Pix on the Cityscapes dataset. "1/2 channels" means that we reduce the network width of the generator network to 1/2 of the original size. GCC uses selective activation discriminator without distillation. collapse in the adversarial learning. In other words, these methods simply compress the generator while maintaining the original capability of the discriminator, which resulted in breaking the balance in adversarial learning. As shown in Figure 1(b) and 1(c), when compressing the generator, the loss of the discriminator gradually tends to zero, such a situation indicates that the capacity of the discriminator signiﬁcantly surpasses that of the lightweight generator. Furthermore, the capacity imbalance between the generator and discriminator leads to the mode collapse problem [47].
In order to retain the Nash equilibrium between the lightweight generator and discriminator, we must revisit the role of the discriminator in the procedure of GAN compression. A straightforward method is to reduce the capacity of the discriminator to re-maintain the Nash equilibrium between it and the lightweight generator. Therefore, we train a lightweight generator against the discriminators with different channel numbers. The results are shown in Figure 2, we can obtain the following observations: i) The channel numbers in the discriminator signiﬁcantly inﬂuence the performance of the lightweight generator; ii) The discriminator with the same channel compression ratio as the generator may not get the best result. iii) The optimal value of channel numbers for discriminators is task-speciﬁc. In short, it is arduous to choose an appropriate channel number for the discriminator in different tasks.
To solve the above issues, we design a generator-discriminator cooperative compression scheme, term
GCC in this paper. GCC consists of four sub-networks, i.e., the original uncompressed generator and discriminator, a pruned generator, and a selective activation discriminator. GCC selectively activates 2
partial convolutional channels of the selective activation discriminator, which guarantees the whole optimization process to maintains in the Nash equilibrium stage. In addition, the compression of the generator will damage its generating ability, and even affect the diversity of generated images, resulting in the occurrence of mode collapse. GCC employs an online collaborative distillation that combines the intermediate features of both the original uncompressed generator and discriminator and then distills them to the pruned generator. The introduction of online collaborative distillation boosts the lightweight generator performance for effective compression. The contributions of GCC can be summarized as follows:
• To the best of our knowledge, we offer the ﬁrst attempt to revisit the role of discriminator in
GAN compression. We propose a novel selective activation discriminator to automatically select and activate the convolutional channels according to a local capacity constraint and a global coordination constraint. The dynamic equilibrium relationship between the original models provides a guide to choose the activated channels in the discriminator.
• A novel online collaborative distillation is designed to simultaneously employ intermediate features of teacher generator and discriminator to guide the optimization process of the lightweight generator step by step. These auxiliary intermediate features provide more complementary information into the lightweight generator for generating high-ﬁdelity images.
• Extensive experiments on various GAN-based generation tasks (i.e., image generation, image-to-image translation, and super-resolution) demonstrate the effectiveness and gener-alization of GCC. GCC contributes to reducing 80% computational costs while maintains comparable performance in image translation. 2