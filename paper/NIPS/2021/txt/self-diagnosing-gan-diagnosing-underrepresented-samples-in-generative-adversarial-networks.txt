Abstract
Despite remarkable performance in producing realistic samples, Generative Ad-versarial Networks (GANs) often produce low-quality samples near low-density regions of the data manifold, e.g., samples of minor groups. Many techniques have been developed to improve the quality of generated samples, either by post-processing generated samples or by pre-processing the empirical data distribution, but at the cost of reduced diversity. To promote diversity in sample generation without degrading the overall quality, we propose a simple yet effective method to diagnose and emphasize underrepresented samples during training of a GAN. The main idea is to use the statistics of the discrepancy between the data distribution and the model distribution at each data instance. Based on the observation that the underrepresented samples have a high average discrepancy or high variability in discrepancy, we propose a method to emphasize those samples during training of a
GAN. Our experimental results demonstrate that the proposed method improves
GAN performance on various datasets, and it is especially effective in improving the quality and diversity of sample generation for minor groups. 1

Introduction
Generative Adversarial Networks (GANs) have achieved remarkable performance in producing realistic samples for complex generation tasks, including image/video synthesis [5, 22], style trans-fer [45, 14], and data augmentation [29]. However, GANs often fail to cover sparse regions of data manifold [16, 9], leading to the underrepresentation of minor groups in the dataset [43]. In particular,
GANs generate samples of minor groups with low ﬁdelity or even fail to generate such samples, exhibiting the mode collapse [43].
Many of previous techniques have focused on improving the overall sample quality of GANs, either by pre-processing the training dataset or by post-processing generated samples. The pre-processing aims to remove instances that cannot be well-represented by GANs even before the training starts and gains ﬁdelity on the focused samples [9]. A similar idea has been used to truncate the latent space by resampling or moving samples that fall outside of some acceptable range during training [16, 6].
∗Equal contribution.
†This work was done as a student at KAIST.
‡Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Post-processing, on the other hand, is a technique that can be applied after the training to remove low-quality generated samples by rejection sampling [3, 37]. All these approaches are effective in increasing the overall ﬁdelity of samples from GANs, but reducing the diversity as a trade-off, and may exacerbate biases against the minor groups in sample generation.
In this work, we aim to improve diversity in sample generation without degrading the overall quality, with a special focus on coverage and quality improvement for minor groups. Toward this, we design methods to detect and emphasize underrepresented samples in training of GANs. Due to the lack of explicit labels available, detecting minor-subgroup samples is especially challenging for unsupervised learning. Therefore, we ﬁrst develop two new metrics, which can be easily calculated from a discriminator output of GANs, to detect underrepresented samples. The main idea is to measure the statistics (mean and variance) of the estimated discrepancy between the data distribution and model distribution at each data instance over multiple epochs of the training. The mean discrepancy indicates how close the data distribution is to the model distribution at each data over the training, while the variance in discrepancy measures how such discrepancy ﬂuctuates across the training. We provide theoretical and empirical evidence that the mean discrepancy can effectively detect underrepresented samples, especially near collapsed modes, while the variance in discrepancy can detect minor data instances, which GANs suffer from modeling.
Based on these observations, we propose a novel method to emphasize underrepresented samples during the training of GANs by score-based weighted sampling, where the score is deﬁned as a weighted sum of the two metrics we devised. We validate our method with thorough experiments over controlled and real datasets and demonstrate the efﬁcacy of the proposed sampling method in improving not only the overall quality (both ﬁdelity and diversity combined) of sample generation but also the coverage and quality for semantic features of minor subgroups. Our contributions can be summarized as follows.
• We propose two new metrics, which can be simply computed from the discriminator, to diagnose GAN training and to detect underrepresented samples. By theoretical analysis and controlled experiments, we demonstrate that the proposed metrics are effective in detecting underrepresented minor samples.
• We propose an algorithm that can effectively emphasize underrepresented data by score-based weighted sampling during the training of GANs. Our experiments on controlled and real datasets show that our method improves diverse performance metrics on several GAN variants and enhances the coverage and quality of minor group generation.
Our code is publicly available at https://github.com/grayhong/self-diagnosing-gan. 2