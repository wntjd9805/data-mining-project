Abstract
Stabilizing an unknown control system is one of the most fundamental problems in control systems engineering. In this paper, we provide a simple, model-free algorithm for stabilizing fully observed dynamical systems. While model-free methods have become increasingly popular in practice due to their simplicity and
ﬂexibility, stabilization via direct policy search has received surprisingly little at-tention. Our algorithm proceeds by solving a series of discounted LQR problems, where the discount factor is gradually increased. We prove that this method efﬁ-ciently recovers a stabilizing controller for linear systems, and for smooth, nonlin-ear systems within a neighborhood of their equilibria. Our approach overcomes a signiﬁcant limitation of prior work, namely the need for a pre-given stabiliz-ing control policy. We empirically evaluate the effectiveness of our approach on common control benchmarks. 1

Introduction
Stabilizing an unknown control system is one of the most fundamental problems in control systems engineering. A wide variety of tasks - from maintaining a dynamical system around a desired equi-librium point, to tracking a reference signal (e.g a pilot’s input to a plane) - can be recast in terms of stability. More generally, synthesizing an initial stabilizing controller is often a necessary ﬁrst step towards solving more complex tasks, such as adaptive or robust control design [Sontag, 1999, 2009].
In this work, we consider the problem of ﬁnding a stabilizing controller for an unknown dynamical system via direct policy search methods. We introduce a simple procedure based off policy gradients which provably stabilizes a dynamical system around an equilibrium point. Our algorithm only requires access to a simulator which can return rollouts of the system under different control policies, and can efﬁciently stabilize both linear and smooth, nonlinear systems.
Relative to model-based approaches, model-free procedures, such as policy gradients, have two key advantages: they are conceptually simple to implement, and they are easily adaptable; that is, the same method can be applied in a wide variety of domains without much regard to the intricacies of the underlying dynamics. Due to their simplicity and ﬂexibility, direct policy search methods have become increasingly popular amongst practitioners, especially in settings with complex, nonlinear dynamics which may be challenging to model. In particular, they have served as the main workhorse for recent breakthroughs in reinforcement learning and control [Silver et al., 2016, Mnih et al., 2015,
Andrychowicz et al., 2020].
Despite their popularity amongst practitioners, model-free approaches for continuous control have only recently started to receive attention from the theory community [Fazel et al., 2018, Kakade et al., 2020, Tu and Recht, 2019]. While these analyses have begun to map out the computational and statistical tradeoffs that emerge in choosing between model-based and model-free approaches,
∗Correspondence to jcperdomo@berkeley.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
they all share a common assumption: that the unknown dynamical system in question is stable, or that an initial stabilizing controller is known. As such, they do not address the perhaps more basic question, how do we arrive at a stabilizing controller in the ﬁrst place? 1.1 Contributions
We establish a reduction from stabilizing an unknown dynamical system to solving a series of dis-counted, inﬁnite-horizon LQR problems via policy gradients, for which no knowledge of an initial stable controller is needed. Our approach, which we call discount annealing, gradually increases the discount factor and yields a control policy which is near optimal for the undiscounted LQR objec-tive. To the best of our knowledge, our algorithm is the ﬁrst model-free procedure shown to provably stabilize unknown dynamical systems, thereby solving an open problem from Fazel et al. [2018].
We begin by studying linear, time-invariant dynamical systems with full state observation and as-sume access to inexact cost and gradient evaluations of the discounted, inﬁnite-horizon LQR cost of a state-feedback controller K. Previous analyses (e.g., [Fazel et al., 2018]) establish how such eval-uations can be implemented with access to (ﬁnitely many, ﬁnite horizon) trajectories sampled from a simulator. We show that our method recovers the controller K(cid:63) which is the optimal solution of the undiscounted LQR problem in a bounded number of iterations, up to optimization and simulator error. The stability of the resulting K(cid:63) is guaranteed by known stability margin results for LQR. In short, we prove the following guarantee:
Theorem 1 (informal). For linear systems, discount annealing returns a stabilizing state-feedback controller which is also near-optimal for the LQR problem.
It uses at most polynomially many,
ε-inexact gradient and cost evaluations, where the tolerance ε also depends polynomially on the relevant problem parameters.
Since both the number of queries and error tolerance are polynomial, discount annealing can be efﬁciently implemented using at most polynomially many samples from a simulator.
Furthermore, our results extend to smooth, nonlinear dynamical systems. Given access to a sim-ulator that can return damped system rollouts, we show that our algorithm ﬁnds a controller that attains near-optimal LQR cost for the Jacobian linearization of the nonlinear dynamics at the equi-librium. We then show that this controller stabilizes the nonlinear system within a neighborhood of its equilibrium.
Theorem 2 (informal). Discount annealing returns a state-feedback controller which is exponen-tially stabilizing for smooth, nonlinear systems within a neighborhood of their equilibrium, using again only polynomially many samples drawn from a simulator.
In each case, the algorithm returns a near optimal solution K to the relevant LQR problem (or local approximation thereof). Hence, the stability properties of K are, in theory, no better than those of the optimal LQR controller K(cid:63). Importantly, the latter may have worse stability guarantees than the optimal solution of a corresponding robust control objective (e.g. H∞ synthesis). Nevertheless, we focus on the LQR subroutine in the interest of simplicity, clarity, and in order to leverage prior analyses of model-free methods for LQR. Extending our procedure to robust-control objectives is an exciting direction for future work.
Lastly, while our theoretical analysis only guarantees that the resulting controller will be stabilizing within a small neighborhood of the equilibrium, our simulations on nonlinear systems, such as the nonlinear cartpole, illustrate that discount annealing produces controllers that are competitive with established robust control procedures, such as H∞ synthesis, without requiring any knowledge of the underlying dynamics. 1.2