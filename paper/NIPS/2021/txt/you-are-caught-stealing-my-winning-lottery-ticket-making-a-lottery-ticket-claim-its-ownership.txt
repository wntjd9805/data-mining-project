Abstract
Despite tremendous success in many application scenarios, the training and in-ference costs of using deep learning are also rapidly increasing over time. The lottery ticket hypothesis (LTH) emerges as a promising framework to leverage a special sparse subnetwork (i.e., winning ticket) instead of a full model for both training and inference, that can lower both costs without sacriﬁcing the perfor-mance. The main resource bottleneck of LTH is however the extraordinary cost to
ﬁnd the sparse mask of the winning ticket. That makes the found winning ticket become a valuable asset to the owners, highlighting the necessity of protecting its copyright. Our setting adds a new dimension to the recently soaring interest in protecting against the intellectual property (IP) infringement of deep models and verifying their ownerships, since they take owners’ massive/unique resources to develop or train. While existing methods explored encrypted weights or predic-tions, we investigate a unique way to leverage sparse topological information to perform lottery veriﬁcation, by developing several graph-based signatures that can be embedded as credentials. By further combining trigger set-based methods, our proposal can work in both white-box and black-box veriﬁcation scenarios. Through extensive experiments, we demonstrate the effectiveness of lottery veriﬁcation in diverse models (ResNet-20, ResNet-18, ResNet-50) on CIFAR-10 and CIFAR-100.
Speciﬁcally, our veriﬁcation is shown to be robust to removal attacks such as model ﬁne-tuning and pruning, as well as several ambiguity attacks. Our codes are available at https://github.com/VITA-Group/NO-stealing-LTH. 1

Introduction
Deep neural networks (DNNs) have dramatically raised the state-of-the-art performance in various
ﬁelds. However, the over-parameterization of DNNs becomes a non-negligible problem. The amount of parameters now is often on the billion scale, which signiﬁcantly increases the inference cost when using these models. An emerging ﬁeld of lottery ticket hypothesis (LTH) explores a new scheme for pruning the model without sacriﬁcing performance. The core idea is to identify the sparsity pattern ahead of training (or in its early stage) and train a sparse network from scratch. It has been hypothesized [1] that DNNs contain sparse networks named winning tickets that can be trained to match the test accuracy of the full model. These winning tickets hence have comparable or even better inference performance while potentially reducing the computational footprints.
However, ﬁnding winning tickets is a non-trivial task: it involves the training-prune-retraining cycle for several times [1], which is burdensome and computation-consuming. Although other works [2–4] have shown that sparsity might emerge at the initialization or at the early stage of training, the iterative magnitude pruning (IMP) still outperforms these alternatives by clear margins [5]. Yet, the powerful
IMP method requires multiple rounds of train-prune-train process on the original training set, which
*Equal Contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is even much more expensive than training a dense network. That makes a found winning ticket a valuable asset to the owners, highlighting the necessity of protecting the winning tickets’ copyright.
Previous works [6–9] have shown that deep networks are vulnerable to intellectual property (IP) infringement.
For example, one can use transfer learning to adapt a trained model onto a new task or use model compres-sion techniques to create a new sparse model based on the target model. Fortunately, in recent years the own-ership veriﬁcation problem has been addressed with a number of solutions proposed. The key idea is to embed veriﬁable information, or called signatures, into models’ weights [6, 10, 11] or predictions [7] without visibly affect-ing the original performance. By extracting the embedded information from models, one can verify the ownership of models and hence protect their IPs. For the methods that embed information in weights, additional weights reg-ularizers are often used to enforce certain patterns, such as signs. As for the prediction methods, a special training set, which is often called a trigger set, is used as additional training data. The model trained upon both the original data and the trigger set can generate desired prediction labels for the privately-held trigger set, while preserving the performance on the original training set. However, those general methods did not take any structural property(e.g., sparsity) into account, leaving chance for improving their gains in the winning ticket mask protection.
Figure 1: Illustration of embedding signa-tures into the original sparse mask. These visualizations are projected from 4D tensors.
Dark entries are pruned elements. Note that the actual sparsity of the subnetwork is un-changed after encoding credentials.
On protecting the IP of winning tickets, we investigate a novel way to leverage sparse structural information for ownership veriﬁcation (Fig. 1). This structural information embedded in winning tickets is a good "credential" for ownership veriﬁcation since the winning ticket at extreme sparsity is naturally robust to ﬁne-tuning and (further) pruning attacks. The winning ticket at extreme sparsity cannot be pruned further; otherwise, the inference performance will drop (hence losing its "value").
Meanwhile, ﬁne-tuning the winning tickets can only tune the weights, but the sparsity pattern will not be changed. However, there remain some key questions to answer: How to formulate the ownership veriﬁcation process under the context of the lottery ticket hypothesis? What kind of structural information should be used? How to inject user-speciﬁc information into the structure of winning tickets? We present answers to these questions in this paper. We summarize our ﬁndings as follows:
• We formulate the lottery veriﬁcation problem and deﬁne two different protection scenarios. We show that even without speciﬁc protection, the extremely sparse winning ticket can partly claim its ownership because of the critical role of its sparse structure in the ﬁnal inference performance.
• We further propose a new mask embedding method that is capable of embedding ownership
“signatures" in the subnetwork’s sparse structural connectivity (Fig. 1), without much affecting its performance. The signature is robust, e.g., it can be extracted and decoded even after pruning or
ﬁne-tuning attacks. Combined further with the trigger set-based method, our mask embedding method can work under both white-box and black-box veriﬁcation frameworks.
• We investigate several veriﬁcation schemes, i.e., separate masks, embedding signatures, and embedding signatures with the trigger set. We show that these schemes are robust to the common removal and ambiguity attacks, as well as a new type of “add-on" attacks. Extensive experiment results demonstrate their competence on protecting the winning tickets. For example, on ResNet-20, our veriﬁcation framework can defend ﬁne-tuning attacks intrinsically, as well as pruning attacks and as add-on attack under all levels of pruning ratios. 2