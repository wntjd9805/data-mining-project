Abstract
The “cold posterior effect” (CPE) in Bayesian deep learning describes the uncom-forting observation that the predictive performance of Bayesian neural networks can be signiﬁcantly improved if the Bayes posterior is artiﬁcially sharpened using a temperature parameter T < 1. The CPE is problematic in theory and practice and since the effect was identiﬁed many researchers have proposed hypotheses to explain the phenomenon. However, despite this intensive research effort the effect remains poorly understood. In this work we provide novel and nuanced evidence relevant to existing explanations for the cold posterior effect, disentan-gling three hypotheses: 1. The dataset curation hypothesis of Aitchison (2020): we show empirically that the CPE does not arise in a real curated data set but can be produced in a controlled experiment with varying curation strength. 2. The data augmentation hypothesis of Izmailov et al. (2021) and Fortuin et al. (2021): we show empirically that data augmentation is sufﬁcient but not necessary for the CPE to be present. 3. The bad prior hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the relative importance of the prior and the likelihood, strongly linking the CPE to the prior. Our results demonstrate how the
CPE can arise in isolation from synthetic curation, data augmentation, and bad priors. Cold posteriors observed “in the wild” are therefore unlikely to arise from a single simple cause; as a result, we do not expect a simple “ﬁx” for cold posteriors. 1

Introduction
Deep neural networks have achieved great success in predictive accuracy for supervised learning tasks.
Unfortunately, however, they still fall short in giving useful estimates of their predictive uncertainty, i.e. meaningful conﬁdence values for how certain the model is about its predictions (Ovadia et al., 2019). Quantifying uncertainty is especially crucial in real-world settings, which often involve data distributions that are shifted from the one seen during training (Quionero-Candela et al., 2009).
Bayesian deep learning combines deep learning with Bayesian probability theory. Bayesian neural networks (BNNs) learn a distribution over model parameters or equivalently sample an ensemble of likely models given the data, promising better generalization performance and principled uncertainty quantiﬁcation (robust predictions) (Neal, 1995; MacKay, 1992; Dayan et al., 1995). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In Bayesian deep learning we either learn a distribution q(θ) over models compatible with the data, i.e. q(θ) (cid:39) p(θ|D), or we sample an ensemble of models θ1, . . . , θK ∼ p(θ|D) from the posterior over likely models p(θ|D) ∝ p(D|θ) p(θ), (1) where in the i.i.d. setting p(D|θ) = (cid:81)n learn to the observations D = {(xi, yi)}n
BNN predictions involve model averaging: rather than betting everything on a single point estimate of the parameters, we predict on a new instance x by averaging over all likely models compatible with the data, i=1 p(yi|xi, θ) is the likelihood, relating the model we want to i=1, and p(θ) is a proper prior, e.g. a Gaussian density. (cid:90) p(y|x, D) = p(y|x, θ) p(θ|D) dθ (cid:39)
K (cid:88) k=1 p(y|x, θk) , where θk ∼ p(θ|D) . (2)
Equation 2 is also known as the posterior predictive or Bayesian model average. Note that, in practice, solving the integral exactly is impossible. However, we can approximate it via Monte Carlo sampling using an ensemble of models θk ∼ p(θ|D), see Section C in the Appendix for further details.
The two main inference paradigms to learn a distribution over model parameters q(θ) (cid:39) p(θ|D) respectively to sample from the posterior θ1, . . . , θK ∼ p(θ|D) are Variational Bayes (VB) (Hinton and Van Camp, 1993; MacKay et al., 1995; Barber and Bishop, 1998; Blundell et al., 2015) and
Markov Chain Monte Carlo (MCMC) (Neal, 1995; Welling and Teh, 2011; Chen et al., 2014; Ma et al., 2015). We will focus on MCMC methods as they are simple to implement and can be scaled to large models and datasets when used with stochastic minibatch gradients (SG-MCMC) (Welling and
Teh, 2011; Chen et al., 2014; Li et al., 2016).
In recent years, the Bayesian deep learning community has developed increasingly accurate and efﬁcient approximate inference procedures for deep BNNs (cf. references in the paragraph above).
Despite this algorithmic progress, however, important questions surrounding BNNs remain unan-swered to this day. A recent and particularly prominent one concerns the “cold posterior effect” (CPE), which describes the observation that the predictive performance of BNNs can be signiﬁcantly improved if the Bayes posterior is artiﬁcially sharpened p(θ|D)1/T using a temperature parameter
T < 1 (Wenzel et al., 2020). Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristics in practice, see Section 2.3 in (Wenzel et al., 2020).
The CPE is problematic in theory and practice, and since the effect was identiﬁed many researchers have proposed hypotheses to explain the phenomenon. There has been an ongoing debate questioning the roles of isotropic Gaussian priors (Wenzel et al., 2020; Zeno et al., 2020; Fortuin et al., 2021), the likelihood model (Aitchison, 2020), inaccurate inference (Izmailov et al., 2021; Wenzel et al., 2020), and data augmentation (Izmailov et al., 2021; Fortuin et al., 2021). However, despite this intensive research effort the effect remains poorly understood.
Contributions: We provide novel and nuanced evidence relevant to existing explanations for the cold posterior effect, disentangling the roles of curation, data augmentation and the prior:
• The dataset curation hypothesis of Aitchison (2020): we show empirically that the CPE does not arise in a real curated data set but can be produced in a controlled experiment with varying curation strength.
• The data augmentation hypothesis of Izmailov et al. (2021) and Fortuin et al. (2021): we show empirically that data augmentation is sufﬁcient but not necessary for the CPE to be present.
• The bad prior hypothesis of Wenzel et al. (2020): we use a simple experiment evaluating the relative importance of the prior and the likelihood, strongly linking the CPE to the prior.
Our results demonstrate how the CPE can arise in isolation from synthetic curation, data augmentation, and bad priors. In fact, we are able to reproduce the cold posterior effect with each of the three factors alone. Cold posteriors observed “in the wild” are therefore unlikely to arise from a single cause; as a result, we do not expect a simple “ﬁx” for cold posteriors. 2
2 Cold Posteriors: