Abstract
Invariance under symmetry is an important problem in machine learning. Our paper looks speciﬁcally at equivariant neural networks where transformations of inputs yield homomorphic transformations of outputs. Here, steerable CNNs have emerged as the standard solution. An inherent problem of steerable representations is that general nonlinear layers break equivariance, thus restricting architectural choices. Our paper applies harmonic distortion analysis to illuminate the effect of nonlinearities on Fourier representations of SO(2). We develop a novel FFT-based algorithm for computing representations of non-linearly transformed activations while maintaining band-limitation. It yields exact equivariance for polynomial (approximations of) nonlinearities, as well as approximate solutions with tunable accuracy for general functions. We apply the approach to build a fully E(3)-equivariant network for sampled 3D surface data. In experiments with 2D and 3D data, we obtain results that compare favorably to the state-of-the-art in terms of accuracy while permitting continuous symmetry and exact equivariance. 1

Introduction
Modeling of symmetry in data, i.e., the invariance of properties under classes of transformations, is a cornerstone of machine learning: Invariance of statistical properties over samples is the basis of any form of generalization, and the prior knowledge of additional symmetries can be leveraged for performance gains. Aside from data efﬁciency prospects, some applications require exact symmetry.
For example, in computational physics, symmetry of potentials and force ﬁelds is directly linked to conservation laws, and is therefore important for the stability of simulations.
In deep neural networks, (discrete) translational symmetry over space and/or time is exploited in many architectures and is the deﬁning feature of convolutional neural networks (CNNs) and their successors. In most applications, we are typically interested in invariance (e.g., classiﬁcation remains unchanged) or co-variance (e.g., predicted geometry is transformed along with the input). Formally, this goal is captured under the more general umbrella of equivariance [6]:
Let f : X → Y be a function (e.g., a network layer) that maps between vector spaces X, Y (e.g., feature maps in a CNN). Let G be a group and let (in slight abuse of notation) g ◦ v denote the application of the action of group element g on a vector v. f is called equivariant, iff:
∀g ∈ G : f (g ◦ v) = h(g) ◦ f (v), (1) where h : G (cid:55)→ G(cid:48) is a group homomorphism mapping into a suitable group G(cid:48). Informally speaking, the effect of a transformation on the input should have an effect on the output that has (at least) the same algebraic structure. Invariance (h ≡ 1G(cid:48)) and covariance (h = idG→G(cid:48)) are special cases, along with contra-variance and any other isomorphisms of subgroups of G. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
G-CNNs: To make current CNN architectures (consisting of linear layers and nonlinearities) equiv-ariant, the standard (and also most general, see [40]) approach are group convolutional networks (G-CNNs) [6], which conceptually boil down to just applying all transformations g ∈ G to ﬁlters, correlating the result with the data, and storing the results. Typically, G will be a continuous, compact
Lie group such as SO(d) (our paper focuses on SO(2)). To avoid inﬁnite costs, results are band-limited, i.e., stored as coefﬁcients of a truncated Fourier basis on G. Simultaneously, the Fourier coefﬁcients provide a linear representation of a subgroup of G and thus exact equivariance in the sense of Eq. 1.
Using such a basis to directly construct sets of ﬁlters yields steerable ﬁlter banks [41], were each
ﬁlter outputs a whole vector of coefﬁcients that represent functions on G.
Nonlinearities: Unfortunately, band-limiting interferes in non-trivial ways with network architecture, as applying a standard nonlinearity such as ReLU, tanh, or even simple nonlinear polynomials to the Fourier coefﬁcients directly will break equivariance. Multiple solutions to this problem have been proposed [39]: Multiplicative nonlinearities [40; 22] as in tensor networks keep equivariance but behave differently from traditional nonlinearities and therefore cannot be used as a drop-in in classical CNN architectures. Complex nonlinearities such as C–ReLU that only act nonlinearly on the magnitude of Fourier coefﬁcients [43; 42] also keep perfect equivariance but are less expressive as they do not permit nonlinear operations on the phase information. A recent study by Weiler and
Cesa [39] shows that simple discretized rotations [6], which do not require architectural adaptations but provide only approximate equivariance, yield the best practical results in image classiﬁcation tasks.
SO(2)-equivariance: The goal of our paper is to clarify the effect of nonlinearities on Fourier-domain representations. We restrict ourselves to the case of SO(2), which permits the application of standard harmonic distortion analysis [26]. Our goal is to maintain a band-limited representation of a function on SO(2) (corresponding to a ﬁxed angular resolution) and efﬁciently compute the band-limited
Fourier-representation after application of a nonlinearity. We obtain an exact algorithm for polynomial nonlinearities with a computational overhead of O(D log D) for degree D. For general nonlinearities, we observe quick convergence in numerical experiments.
SE(3)-equivariant surface networks: While the limitation to SO(2) might appear restrictive, it is still important for many applications processing image and geometric data: Adding translational invariance (SE(2)-equivariance) is easy, and we also apply our representation to surface data with normal information, extending ideas of Wiersma et al. [42] to a fully E(3)-equivariant network.
We evaluate our networks on example benchmarks for 2D image and 3D object recognition. We obtain invariant results and equivariant intermediate representations up to numerical precision for polynomial nonlinearities (double checking the formal guarantees) and low-error approximations with general nonlinearities such as tanh, ReLU, ELU [5] at reasonable overhead (typically less than 20%, depending on approximation quality). Classiﬁcation accuracy on MNIST-rot and ModelNet-40 [44] is roughly similar to other recent literature for ReLU and slightly reduced for low-degree polynomial approximations [16] of ReLU.
Our main contributions are (i) a simple analytical model for the effect of nonlinearities on Fourier representations in SO(2)-equivariant networks and (ii) an efﬁcient algorithm for applying nonlineari-ties. It is provably exact for polynomials and empirically yields good approximations for common non-polynomial functions. This permits, for the ﬁrst time, a the usage of standard CNN architectures with common nonlinearities without compromising equivariance. 2