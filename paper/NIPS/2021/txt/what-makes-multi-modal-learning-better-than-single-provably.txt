Abstract
The world provides us with data of multiple modalities. Intuitively, models fusing data from different modalities outperform their uni-modal counterparts, since more information is aggregated. Recently, joining the success of deep learning, there is an influential line of work on deep multi-modal learning, which has remarkable empirical results on various applications. However, theoretical justifications in this field are notably lacking.
Can multi-modal learning provably perform better than uni-modal?
In this paper, we answer this question under a most popular multi-modal fusion framework, which firstly encodes features from different modalities into a common latent space and seamlessly maps the latent representations into the task space. We prove that learning with multiple modalities achieves a smaller population risk than only using its subset of modalities. The main intuition is that the former has a more accurate estimate of the latent space representation. To the best of our knowledge, this is the first theoretical treatment to capture important qualitative phenomena observed in real multi-modal applications from the generalization perspective.
Combining with experiment results, we show that multi-modal learning does possess an appealing formal guarantee. 1

Introduction
Our perception of the world is based on different modalities, e.g. sight, sound, movement, touch, and even smell [36]. Inspired from the success of deep learning [26, 21], deep multi-modal research is also activated, which covers fields like audio-visual learning [8, 43], RGB-D semantic segmentation [35, 20] and Visual Question Answering [17, 2].
While deep multi-modal learning shows excellent power in practice, theoretical understanding of deep multi-modal learning is limited. Some recent works have been done towards this direction [39, 48].
However, these works made strict assumptions on the probability distributions across different modalities, which may not hold in real-life applications [30]. Notably, they do not take generalization performance of multi-modal learning into consideration. Toward this end, the following fundamental problem remains largely open:
∗equal contribution
†Correspondence to: longbohuang@tsinghua.edu.cn 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: M vs N modalities latent space representation, where the latter is a subset of the former. zM, zN and z⋆ are images on the latent space Z corresponding to the representation mappings ˆgM,
ˆgN and g⋆. Mi denotes modality i.
Can multi-modal learning provably performs better than uni-modal?
In this paper, we provably answer this question from two perspectives:
• (When) Under what conditions multi-modal performs better than uni-modal?
• (Why) What results in the performance gains ?
The framework we study is abstracted from the multi-modal fusion approaches, which is one of the most researched topics of multi-modal learning [3]. Specifically, we first encode the complex data from heterogeneous sources into a common latent space Z. The true latent representation is g⋆ in a function class G, and the task mapping h⋆ is contained in a function class H defined on the latent space. Our model corresponds to the recent progress of deep multi-modal learning on various applications, such as scene classification [9] and action recognition [24, 43].
Under this composite framework, we provide the first theoretical analysis to shed light on what makes multi-modal outperform uni-modal from the generalization perspective. We identify the relationship between the population risk and the distance between a learned latent representation ˆg and the g⋆, under the metric we will define later. Informally, closer to the true representation leads to less population loss, which indicates that a better latent representation guarantees the end-to-end multi-modal learning performance. Instead of simply considering the comparison of multi vs uni modalities, we consider a general case, M vs N modalities, which are distinct subsets of all modalities. We focus on the condition that the latter is a subset of the former. Our second result is a bound for the closeness between ˆg and the g⋆, from which we provably show that the latent representation ˆgM learning from the M modalities is closer to the true g⋆ than ˆgN learning from N modalities. As shown in Figure 1, ˆgM has a more sufficient latent space exploration than ˆgN . Moreover, in a specific linear regression model, we directly verify that using multiple modalities rather than its subset learns a better latent representation.
The main contributions of this paper are summarized as follows:
• We formalize the multi-modal learning problem into a theoretical framework. Firstly, we show that the performance of multi-modal learning in terms of population risk can be bounded by the latent representation quality, a novel metric we propose to measure the distance from a learned latent representation to the true representation, which reveals that the ability of learning the whole task coincides with the ability of learning the latent representation when we have sufficient training samples.
• We derive an upper bound for the latent representation quality of training over a subset of modalities. This directly implies a principle to guide us in modality selection, i.e., when the number of sample size is large and multiple modalities can efficiently optimize the empirical risk, using multi-modal to build a recognition or detection system can have a better performance.
• Restricted to linear latent and task mapping, we provide rigorous theoretical analysis that latent representation quality degrades when the subset of multiple modalities is applied. 2
Experiments are also carried out to empirically validate the theoretical observation that ˆgN is inferior to ˆgM.
The rest of the paper is organized as follows. In the next section, we review the related literature. The formulation of multi-modal learning problem is described in Section 3. Main results are presented in Section 4. In Section 5, we show simulation results to support our theoretical claims. Additional discussions about the inefficiency of multi-modal learning are presented in Section 6. Finally, conclusions are drawn in Section 7. 2