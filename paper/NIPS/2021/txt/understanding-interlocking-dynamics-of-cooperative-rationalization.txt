Abstract
Selective rationalization explains the prediction of complex neural networks by
ﬁnding a small subset of the input that is sufﬁcient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm — model interlocking. Interlocking arises when the predictor overﬁts to the features selected by the generator thus reinforcing the generator’s selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator’s selection policy. We propose a new rationalization framework, called
A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors.
While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can signiﬁcantly alleviate the interlock problem and ﬁnd explanations that better align with human judgments.2 1

Introduction
Selective rationalization [8, 10, 11, 13, 14, 17, 27, 29, 46] explains the prediction of complex neural networks by ﬁnding a small subset of the input – rationale – that sufﬁces on its own to yield the same outcome as to the original data. To generate high-quality rationales, existing methods often train a cascaded system that consists of two components, i.e., a rationale generator and a predictor. The generator selects a subset of the input explicitly (a.k.a., binarized selection), which is then fed to the predictor. The predictor then predicts the output based only on the subset of features selected by the generator. The rationale generator and the predictor are trained jointly to optimize the prediction performance. Compared to many other interpretable methods [5, 23, 45, 38] that rely on attention mechanism as a proxy of models’ explanation, selective rationalization offers a unique advantage: certiﬁcation of exclusion, i.e., any unselected input is guaranteed to have no contribution to prediction.
⇤Authors contributed equally to this paper. Work was done when SC was at MIT-IBM Watson AI Lab. 2We release our code at https://github.com/Gorov/Understanding_Interlocking. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, binarized selective rationalization schemes are notoriously hard to train [8, 46]. To overcome training obstacles, previous works have considered using smoothed gradient estimations (e.g. gradient straight-through [9] or Gumbel softmax [21]), introducing additional components to control the complement of the selection [10, 46], adopting different updating dynamics between the generator and the predictor [11], using rectiﬁed continuous random variables to handle the constrained optimization in training [8], etc. In practice, these solutions are still insufﬁcient. They either still require careful tuning or are at a cost of reduced predictive accuracy.
In this paper, we reveal a major training problem of selective rationalization that has been largely overlooked — model interlocking. Intuitively, this problem arises because the predictor only sees what the generator selects during training, and tends to overﬁt to the selection of the generator. As a result, even if the generator selects a sub-optimal rationale, the predictor can still produce a lower prediction loss when given this sub-optimal rationale than when given the optimal rationale that it has never seen. As a result, the generator’s selection of the sub-optimal rationale will be reinforced.
In the end, both the rationale generator and the predictor will be trapped in a sub-optimal equilibrium, which hurts both model’s predictive accuracy and the quality of generated rationales.
By investigating the training objective of selective rationalization theoretically, we found that the fundamental cause of the problem of interlocking is that the rationalization objective we aim to minimize is undesirably concave with respect to the rationale generator’s policy, which leads to many sub-optimal corner solutions. On the other hand, although the attention-based models (i.e., via soft selection) produce much less faithful explanations and do not have the nice property of certiﬁcation of exclusion, their optimization objective has a better convexity property with respect to the attention weights under certain assumptions, and thus would not suffer from the interlocking problem.
Motivated by these observations, we propose a new rationalization framework, called A2R (attention-to-rationale), which combines the advantages of both the attention model (convexity) and binarized rationalization (faithfulness) into one. Speciﬁcally, our model consists of a generator, and two predictors. One predictor, called attention-based predictor, operates on the soft-attention, and the other predictor, called binarized predictor, operates on the binarized rationales. The attention as used by the attention-based predictor is tied to the rationale selection probability as used by the binarized predictor. During training, the generator aims to improve both predictors’ performance while minimizing their prediction gap. As we will show theoretically, the proposed rationalization scheme can overcome the concavity of the original setup, and thus can avoid being trapped in sub-optimal rationales. In addition, during inference time, we only keep the binarized predictor to ensure the faithfulness of the generated explanations. We conduct experiments on two synthetic benchmarks and two real datasets. The results demonstrate that our model can signiﬁcantly alleviate the problem of interlocking and ﬁnd explanations that better align with human judgments. 2