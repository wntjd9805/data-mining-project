Abstract
Reinforcement learning (RL) for continuous control typically employs distributions whose support covers the entire action space. In this work, we investigate the colloquially known phenomenon that trained agents often prefer actions at the boundaries of that space. We draw theoretical connections to the emergence of bang-bang behavior in optimal control, and provide extensive empirical evaluation across a variety of recent RL algorithms. We replace the normal Gaussian by a Bernoulli distribution that solely considers the extremes along each action dimension - a bang-bang controller. Surprisingly, this achieves state-of-the-art performance on several continuous control benchmarks - in contrast to robotic hardware, where energy and maintenance cost affect controller choices. Since exploration, learning, and the ﬁnal solution are entangled in RL, we provide additional imitation learning experiments to reduce the impact of exploration on our analysis. Finally, we show that our observations generalize to environments that aim to model real-world challenges and evaluate factors to mitigate the emergence of bang-bang solutions. Our ﬁndings emphasise challenges for benchmarking continuous control algorithms, particularly in light of potential real-world applications.3 1

Introduction
Real-world robotics tasks commonly manifest as control problems over continuous action spaces.
When learning to act in such settings, control policies are typically represented as continuous probability distributions that cover all feasible control inputs - often Gaussians. The underlying assumption is that this enables more reﬁned decisions compared to crude policy choices such as discretized controllers, which limit the search space but induce abrupt changes. While switching controls can be undesirable in practice as they may challenge stability and accelerate system wear-down, they are theoretically feasible and even arise as optimal strategies in some settings. It is therefore important to investigate our underlying assumption in designing policies for learning agents, and analyze how deviations from expected behavior can be explained.
Practitioners have empirically observed that even under Gaussian policies extremal switching, or bang-bang control [31], may naturally emerge (e.g. in [21, 41, 51]). Thus, recent work focused on developing methods for preventing this behavior [7] or improving training when bang-bang control is the optimal policy structure [28]. However, understanding the performance, extent, and reasons for emergence of Bang-Bang policies in RL is largely an open research question. Its answer is important for designing future benchmarks and informing empirical and theoretical RL research directions. 1Correspondence to tseyde@mit.edu. 2Equal advising. 3Please ﬁnd videos and additional details at https://sites.google.com/view/bang-bang-rl 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
In this paper we address these questions in two ways. First, we provide a theoretical intuition for bang-bang behavior in reinforcement learning. This is based on drawing connections to minimum-time problems where bang-bang control is often provably optimal. Second, we perform a set of experiments which optimize controllers via on-policy and off-policy learning as well as model-free and model-based state-of-the-art RL methods. Therein we compare the original algorithms with a slight modiﬁcation where the Gaussian policy head is replaced with the Bernoulli distribution resulting in a bang-bang controller.
In addition to theoretical justiﬁcations, our empirical results conﬁrm emergence of bang-bang policies in standard continuous control benchmarks. Across the board, our experiments also demonstrate high performance of explicitly enforced bang-bang policies: the modiﬁed policy head can even outperform the original method. In connection to optimal control theory, we demonstrate how action costs empirically lead to sub-optimal performance with bang-bang controllers. However, we also show the negative impact of action penalties on exploration with Gaussian policies. Due to the necessity of exploration in RL to ﬁnd the optimal solution, this can result in a complex trade-off.
We also provide a discrete action space version of Maxmimum A Posteriori Policy Optimization (MPO) [1] that avoids relaxations or high variance gradient estimators, which many algorithms rely on when replacing Gaussians with discrete distributions. This is particularly useful as exploration, learning process and ﬁnal performance are highly entangled in RL, and inaccurate or biased gradients can strongly affect learning. We furthermore aim to mitigate this entanglement by including results for distilling behaviour of a trained Gaussian agent into both a continuous and a discrete agent to compare their performance and provide further evidence for emergent bang-bang behavior.
In summary, our work contains the following key contributions: 1. We show competitive performance of bang-bang control on standard continuous control benchmarks by adapting several recent RL algorithms to leverage Bernoulli policies. 2. We draw theoretical connections to optimal control, motivating the emergence of bang-bang behavior in certain problem formulations even under continuous policy parameterizations. 3. We discuss the introduction of action penalties as a common method to reduce the emergence of bang-bang behaviour, and highlight resulting trade-offs with respect to exploration. 2