Abstract
Bandit problems with linear or concave reward have been extensively studied, but relatively few works have studied bandits with non-concave reward. This work considers a large family of bandit problems where the unknown underlying reward function is non-concave, including the low-rank generalized linear bandit problems and two-layer neural network with polynomial activation bandit problem. For the low-rank generalized linear bandit problem, we provide a minimax-optimal algorithm in the dimension, refuting both conjectures in [55, 43]. Our algorithms are based on a unified zeroth-order optimization paradigm that applies in great generality and attains optimal rates in several structured polynomial settings (in the dimension). We further demonstrate the applicability of our algorithms in RL in the generative model setting, resulting in improved sample complexity over prior approaches. Finally, we show that the standard optimistic algorithms (e.g., UCB) are sub-optimal by dimension factors. In the neural net setting (with polynomial activation functions) with noiseless reward, we provide a bandit algorithm with sample complexity equal to the intrinsic algebraic dimension. Again, we show that optimistic approaches have worse sample complexity, polynomial in the extrinsic dimension (which could be exponentially worse in the polynomial degree). 1

Introduction
Bandits [51] are a class of online decision-making problems where an agent interacts with the environment, only receives a scalar reward, and aims to maximize the reward. In many real-world applications, bandit and RL problems are characterized by large or continuous action space. To encode the reward information associated with the action, function approximation for the reward function is typically used, such as linear bandits [20]. Stochastic linear bandits assume the mean reward to be the inner product between the unknown model parameter and the feature vector associated with the action. This setting has been extensively studied, and algorithms with optimal regret are known [20, 51, 2, 13].
However, linear bandits suffer from limited representation power unless the feature dimension is prohibitively large. A comprehensive empirical study [60] found that real-world problems required non-linear models and thus non-concave rewards to attain good performance on a testbed of bandit problems. To take a step beyond the linear setting, it becomes more challenging to design optimal algorithms. Unlike linear bandits, more sophisticated algorithms beyond optimism are necessary. For instance, a natural first step is to look at quadratic [44] and higher-order polynomial [35] reward. In
∗Alphabetical order. Correspondence to: Qi Lei, qilei@princeton.edu, Jason D. Lee, jasonlee@ princeton.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the context of phase retrieval, which is a special case for the quadratic bandit, people have derived algorithms that achieve minimax risks in the statistical learning setting [9, 52, 15]. However, the straightforward adaptation of these algorithms results in sub-optimal dimension dependency.
In the bandit domain, existing analysis on the nonlinear setting includes eluder dimension [62], subspace elimination [55, 43], etc. Their results also suffer from a larger dimension dependency than the best known lower bound in many settings. (See Table 1 and Section 3 for a detailed discussion of these results.) Therefore in this paper, we are interested in investigating the following question:
What is the optimal regret for non-concave bandit problems, including structured polynomials (low-rank etc.)? Can we design algorithms with optimal dimension dependency?
Contributions: for various non-linear bandit problems.
In this paper, we answer the questions and close the gap (in problem dimension) 1. First, we design stochastic zeroth-order gradient-like2 ascent algorithms to attain minimax re-gret for a large class of structured polynomials. The class of structured polynomials contains bilinear and low-rank linear bandits and symmetric and asymmetric higher-order homoge-neous polynomial bandits with action dimension d. Though the reward is non-concave, we combine techniques from two bodies of work, non-convex optimization and numerical linear algebra, to design robust gradient-based algorithms that converge to global maxima. Our algorithms are also computationally efficient, practical, and easily implementable.
In all cases, our algorithms attain the optimal dependence on dimension d, which was not previously attainable using existing optimism techniques. As a byproduct, our algorithm refutes3 the conjecture from [43] on the bilinear bandit, and the conjecture from [55] on low-rank linear bandit by giving an algorithm that attains the optimal dimension dependence. 2. We demonstrate that our techniques for non-concave bandits extend to RL in the generative setting, improving upon existing optimism techniques. 3. When the reward is a general polynomial without noise, we prove that solving polynomial equations achieves regret equal to the intrinsic algebraic dimension of the underlying polynomial class, which is often linear in d for interesting cases. In general, this complexity cannot be further improved. 4. Furthermore, we provide a lower bound showing that all UCB algorithms have a sample complexity of Ω(dp), where p is the degree of the polynomial. The dimension of all homogeneous polynomials of degree p in dimension d is dp, showing that UCB is oblivious to the polynomial class and highly sub-optimal even in the noiseless setting. 1.1