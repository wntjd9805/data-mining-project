Abstract
Mitigating the dependence on spurious correlations present in the training dataset is a quickly emerging and important topic of deep learning. Recent approaches include priors on the feature attribution of a deep neural network (DNN) into the training process to reduce the dependence on unwanted features. However, until now one needed to trade off high-quality attributions, satisfying desirable axioms, against the time required to compute them. This in turn either led to long training times or ineffective attribution priors. In this work, we break this trade-off by considering a special class of efﬁciently axiomatically attributable
DNNs for which an axiomatic feature attribution can be computed with only a single forward/backward pass. We formally prove that nonnegatively homogeneous
DNNs, here termed
-DNNs, are efﬁciently axiomatically attributable and show that they can be effortlessly constructed from a wide range of regular DNNs by simply removing the bias term of each layer. Various experiments demonstrate the advantages of
-DNNs, beating state-of-the-art generic attribution methods on regular DNNs for training with attribution priors.
X
X 1

Introduction
Many traditional machine learning (ML) approaches, such as linear models or decision trees, are inherently explainable [4]. Therefore, an ML practitioner can comprehend why a method yields a particular prediction and correct the method if the explanation for the result is ﬂawed. The prevailing
ML architectures in use today [12, 25], namely deep neural networks (DNNs), unfortunately, do not come with this inherent explainability. This can cause models to depend on dataset biases and spurious correlations in the training data. For real-world applications, e.g., credit score or insurance risk assignment, this can be highly problematic and potentially lead to models discriminating against certain demographic groups [3, 20]. To mitigate the dependence on spurious correlations in DNNs, attribution priors have been recently proposed [7, 22, 23]. By enforcing priors on the feature attribution of a DNN at training time, they allow actively controlling its behavior. As it turns out, attribution priors are a very ﬂexible tool, allowing even complex model interventions such as making an object recognition model focus on shape [22] or less sensitive to high-frequency noise [7]. However, their use brings new challenges over regular training. First, computing the feature attribution of a
DNN is a nontrivial task. It is critical to use an attribution method that faithfully reﬂects the true behavior of the deep network and ideally satisﬁes the axioms proposed by Sundararajan et al. [34].
Otherwise, spurious correlations may go undetected and the attribution prior would be ineffective.
Second, since the feature attribution is used in each training step, it needs to be efﬁciently computable.
Existing work incurs a trade-off between high-quality attributions for which formal axioms hold [34] and the time required to compute them [7]. Prior work on attribution priors thus had to choose whether to rely on high-quality feature attributions or allow for efﬁcient training. In this work, we obviate this trade-off.
Code and additional resources at https://visinf.github.io/fast-axiomatic-attribution/. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Speciﬁcally, we make the following contributions: (i) We propose to consider a special class of
DNNs, termed efﬁciently axiomatically attributable DNNs, for which we can compute a closed-form axiomatic feature attribution that satisﬁes the axioms of Sundararajan et al. [34], requiring only one gradient evaluation. As a result, we can compute axiomatic high-quality attributions with only one forward/backward pass, and hence, require only a fraction of the computing power that would be needed for regular DNNs, which involve a costly numerical approximation of an integral [7, 34].
This signiﬁcant improvement in efﬁciency makes our considered class of DNNs particularly well suited for scenarios where the attribution is used in the training process, such as for training with attribution priors. (ii) We formally prove that nonnegatively homogeneous DNNs (termed
-DNNs) are efﬁciently axiomatically attributable DNNs and establish a new theoretical connection between
Gradient [27] and Integrated Gradients [34] for nonnegatively homogeneous DNNs of different
Input degrees. (iii) We show how
-DNNs can be instantiated from a wide range of regular DNNs by simply removing the bias term of each layer. While this may seem like a signiﬁcant restriction, we show that the impact on the predictive accuracy in two different application domains is surprisingly
-DNNs, showing that they minor. In a variety of experiments, we demonstrate the advantages of (iv) admit accurate axiomatic feature attributions at a fraction of the computational cost and (v) beat state-of-the-art generic attribution methods for training regular networks with an attribution prior.
×
X
X
X 2