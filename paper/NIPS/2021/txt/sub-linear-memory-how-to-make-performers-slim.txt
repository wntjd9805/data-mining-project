Abstract
Transformer architectures have become very popular yet the original implemen-tation requires O(L2) in serial time and memory as functions of input length L.
Recent works proposed various linear self-attention mechanisms, scaling only as O(L) for serial computation. We conduct a thorough complexity analysis of
Performers, a class which includes most recent linear Transformer mechanisms.
We note a remarkable computational ﬂexibility: the gradient computation can be performed with no approximations using sublinear memory as a function of L (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only O(1) memory, and still requires O(L) time. Due to complete backward-compatibility, this discovered time-memory tradeoff can be used for ﬁne-tuning on low-memory devices in a decentralized fashion without any server computations. 1

Introduction
The Transformer architecture [38] has changed the landscape of deep learning for sequential data.
A computational advantage of Transformers over conventional methods such as recurrent neural networks (RNNs) [17, 9] is parallelization over the sequence dimension, meaning that the training speed can be increased by simply using more compute resources. However, this parallel-friendly structure of self-attention comes at a cost of quadratic Θ(L2) time and memory complexity, where L is the length of the Transformer’s input sequence.
A recent line of work aimed to address this restriction, using either structured sparsity [8], truncated back-propagation [12], clustering [20, 31] or linear attention methods [18, 10, 11, 33, 23]. For a detailed overview of efﬁcient Transformers, see [37]. We refer to the family of linear attention architectures as Performers (also known as Linear Transformers), following [11], since their generic kernel formulation covers all the aforementioned linear attention methods. Performers reduce time and memory complexity to linear O(L) and can provably approximate conventional quadratic Trans-formers [11], demonstrating strong performance in a systematic comparison of efﬁcient Transformers
[36].
This recent trend of feeding longer sequences into Transformers, coupled with the use of deeper mod-els, introduces new challenges for researchers and practitioners. Whereas conventional Transformer setups beneﬁt from large-batch optimization [42], long sequence modelling necessitates smaller batch sizes in order to ﬁt the model into memory. For example, recently proposed efﬁcient Transformers 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) rth layer and its decomposition into T(r−1), Γ(r−1), U(r−1). (b) Illustration of Al-gorithm 1 when r = n = 2. Red color indicates objects stored in memory. I-II) forward passes for n = 1, 2 respectively, only the loss value and U (n) are stored. III) backward pass start, for-ward computation through the slice n = 2 to build symbolic Φ(2) and update U (2) → U (1). IV) back-propagation through Φ(2) to ﬁnd ∇θ(2)L and G(1). V,VI) the same backward iteration for n = 1. operating on long sequences used moderately small batch sizes of 1-8 instances [20, 18, 11]. Aiming to use larger batch sizes, practitioners introduced various tricks – e.g. [28] introduced gradient accumulation (included in the popular Fairseq library, [27]), which splits the batch into smaller chunks which are evaluated sequentially, then the resulting batch gradient is accumulated.
Gradient accumulation allows to decrease memory usage at the cost of longer time, but it can only be applied when the batch size is bigger than 1. In this paper, we are discussing a situation when even a batch size of 1 is prohibitive, while longer processing times are affordable e.g. when ﬁne-tuning a pretrained Transformer on low-memory devices (e.g. smartphones, embedded devices or microcontrollers) on the client-generated data without additional server computations. Heuristics, such as chunking the input into independent subsegments or truncated back-propagation [12], limit gradient propagation across the whole input, and, consequently, impair long-context learning.
We propose a solution based on the analysis of Performers. We discover a remarkable property: even for batch size of 1, a user can decrease memory consumption at the cost of smaller parallel bandwidth of the model. Notably, no approximations are introduced, so the obtained gradient is correct and backward-compatible. Our proposed long-sequence training algorithm can be used for training or ﬁne-tuning on a low-memory device, thus contributing towards decentralized and democratized deep learning. The algorithm has the following advantages: 1. The integer parameter C, 1 ≤ C ≤ L, controls a tradeoff between the memory, scaling as O(C) in addition to a negligible input sequence storage, and parallel running time, scaling as
O((L/C) log C). When C = 1, the algorithm consumes as much memory as if a single token were fed into Performer, plus a small fully characterized addition. 2. For any C, the algorithm requires as many ﬂoating point operations (FLOPs) as two standard forward and one backward pass plus a small addition.
We evaluate the proposed tradeoff empirically, and conﬁrm backward-compatibility for the synthetic
Copying Task and language modelling on Penn Treebank [25] and Enwik8 [24] datasets.1 1Code: https://github.com/google-research/google-research/tree/master/performer/ models/slim_performer. 2
2