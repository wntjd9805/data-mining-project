Abstract
Deep learning has exhibited superior performance for various tasks, especially for high-dimensional datasets, such as images. To understand this property, we investigate the approximation and estimation ability of deep learning on anisotropic Besov spaces. The anisotropic Besov space is characterized by direction-dependent smoothness and includes several function classes that have been investigated thus far. We demonstrate that the approximation error and es-timation error of deep learning only depend on the average value of the smooth-ness parameters in all directions. Consequently, the curse of dimensionality can be avoided if the smoothness of the target function is highly anisotropic. Un-like existing studies, our analysis does not require a low-dimensional structure of the input data. We also investigate the minimax optimality of deep learning and compare its performance with that of the kernel method (more generally, linear estimators). The results show that deep learning has better dependence on the in-put dimensionality if the target function possesses anisotropic smoothness, and it achieves an adaptive rate for functions with spatially inhomogeneous smoothness. 1

Introduction
Based on the recent literature pertaining to machine learning, deep learning has exhibited superior performance in several tasks such as image recognition (Krizhevsky et al., 2012), natural language processing (Devlin et al., 2018), and image synthesis (Radford et al., 2015). In particular, its superi-ority is remarkable for complicated and high-dimensional data like images. This is mainly due to its high ﬂexibility and superior feature-extraction ability for effectively extracting the intrinsic structure of data. Its theoretical analysis also has been extensively developed considering several aspects such as expressive ability, optimization, and generalization error.
Amongst representation ability analysis of deep neural networks such as universal approximation ability (Cybenko, 1989; Hornik, 1991; Sonoda & Murata, 2017), approximation theory of deep neural networks on typical function classes such as H¨older, Sobolev, and Besov spaces have been extensively studied. In particular, analyses of deep neural networks with the ReLU activation (Nair
& Hinton, 2010; Glorot et al., 2011) have been recently developed. Schmidt-Hieber (2020) showed that the deep learning with ReLU activations can achieve the minimax optimal estimation accuracy to estimate composite functions in H¨older spaces by using the approximation theory of Yarotsky (2017). Suzuki (2019) generalized this analysis to those on the Besov space and the mixed smooth 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Relationship between existing research and our work. β indicates the smoothness of the target function, d is the dimensionality of input x, D is the dimensionality of a low-dimensional structure on which the data are distributed, and (cid:101)β is the average smoothness of an anisotropic Besov space (Eq. (1)).
Function class
Author
Estimation error
H¨older
Besov
Suzuki (2019)
Schmidt-Hieber (2020)
˜O(n− 2β 2β+d ) smooth mixed
Besov
Suzuki (2019)
H¨older on a low-dimensional set
Nakada & Imaizumi (2020);
Schmidt-Hieber (2019); Chen et al. (2019) anisotropic
Besov
This work
˜O(n− 2β 2β+d ) (cid:16) n− 2β 2β+1 ×
˜O
˜O(n− 2β 2β+D ) 2(d−1)(u+β) 1+2β (cid:17) log(n)
˜O(n
− 2 (cid:101)β 2 (cid:101)β+1 )
Besov space by utilizing the techniques developed in approximation theories (Temlyakov, 1993;
DeVore, 1998). It was shown that deep learning can achieve an adaptive approximation error rate that is faster than that of (non-adaptive) linear approximation methods (DeVore & Popov, 1988;
DeVore et al., 1993; D˜ung, 2011), and it outperforms any linear estimators (including kernel ridge regression) in terms of the minimax optimal rate.
From these analyses, one can see that the approximation errors and estimation errors are strongly inﬂuenced by two factors, i.e., the smoothness of the target function and the dimensionality of the input (see Table 1). In particular, they suffer from the curse of dimensionality, which is unavoid-able. However, these analyses are about the worst case errors and do not exploit speciﬁc intrinsic properties of the true distributions. For example, practically encountered data usually possess low intrinsic dimensionality, i.e., data are distributed on a low dimensional sub-manifold of the input space (Tenenbaum et al., 2000; Belkin & Niyogi, 2003). Recently, Nakada & Imaizumi (2020);
Schmidt-Hieber (2019); Chen et al. (2019); Chen et al. (2019) have shown that deep ReLU network has adaptivity to the intrinsic dimensionality of data and can avoid curse of dimensionality if the intrinsic dimensionality is small. However, one drawback is that they assumed exact low dimen-sionality of the input data. This could be a strong assumption because practically observed data are always noisy, and injecting noise immediately destroys the low-dimensional structure. There-fore, we consider another direction in this paper. In terms of curse of dimensionality, Suzuki (2019) showed that deep learning can alleviate the curse of dimensionality to estimate functions in a so called mixed smooth Besov space (m-Besov). However, m-Besov space assumes strong smoothness toward all directions uniformly and does not include the ordinary Besov space as a special case.
Moreover, the convergence rate includes heavy poly-log term which is not negligible (see Table 1).
In practice, one of the typically expected properties of a true function on high-dimensional data is that it is invariant against perturbations of an input in some speciﬁc directions (Figure 1). For ex-ample, in image-recognition tasks, the target function must be invariant against the spatial shift of an input image, which is utilized by data-augmentation techniques (Simard et al., 2003; Krizhevsky et al., 2012). In this paper, we investigate the approximation and estimation abilities of deep learn-ing on anisotropic Besov spaces (Nikol’skii, 1975; Vybiral, 2006; Triebel, 2011) (also called dom-inated mixed-smooth Besov spaces). An anisotropic Besov space is a set of functions that have
“direction-dependent” smoothness, whereas ordinary function spaces such as H¨older, Sobolev, and
Besov spaces assume isotropic smoothness that is uniform in all directions. We consider a com-position of functions included in an anisotropic Besov space, including several existing settings as special cases; it includes analyses of the H¨older space Schmidt-Hieber (2020) and Besov space
Suzuki (2019), as well as the low-dimensional sub-manifold setting (Nakada & Imaizumi, 2020;
Schmidt-Hieber, 2019; Chen et al., 2019; Chen et al., 2019)1. By considering such a space, we can show that deep learning can alleviate curse of dimensionality if the smoothness in each direc-tion is highly anisotropic. Interestingly, any linear estimator (including kernel ridge regression) has worse dependence on the dimensionality than deep learning. Our contributions can be summarized as follows:
• We consider a situation in which the target function is included in a class of anisotropic Besov spaces and show that deep learning can avoid the curse of dimensionality even if the input data 1We would like to remark that the analysis of Nakada & Imaizumi (2020) does not require smoothness of the embedded manifold that is not covered in this paper. 2
do not lie on a low-dimensional manifold. Moreover, deep learning can achieve the optimal adaptive approximation error rate and minimax optimal estimation error rate.
• We compare deep learning with general linear estimators (including kernel methods) and show that deep learning has better dependence on the input dimensionality than linear estimators. 2 Problem setting and the model
In this section, we describe the problem setting considered in this work. We consider the following nonparametric regression model: yi = f o(xi) + ξi (i = 1, . . . , n), where xi is generated from a probability distribution PX on [0, 1]d, ξi ∼ N (0, σ2), and the i=1 are independently identically distributed. f o is the true function that data Dn = (xi, yi)n we want to estimate. We are interested in the mean squared estimation error of an estimator (cid:98)f :
EDn [(cid:107) (cid:98)f − f o(cid:107)2
L2(PX )], where EDn [·] indicates the expectation with respect to the training data Dn.
We consider a least-squares estimator in the deep neural network model as (cid:98)f (see Eq. (5)) and discuss its optimality. More speciﬁcally, we investigate how the “intrinsic dimensionality” of data affects the estimation accuracy of deep learning. For this purpose, we consider an anisotropic Besov space as a model of the target function. 2.1 Anisotropic Besov space
In this section, we introduce the anisotropic Besov which was investigated as the model of the true function in this paper. Throughout this paper, we set the domain of the input to Ω = [0, 1]d. For a function f : Ω → R, let (cid:107)f (cid:107)p := (cid:107)f (cid:107)Lp(Ω) := ((cid:82)
Ω |f |pdx)1/p for 0 < p < ∞. For p = ∞, we deﬁne (cid:107)f (cid:107)∞ := (cid:107)f (cid:107)L∞(Ω) := supx∈Ω |f (x)|. For β ∈ Rd
For a function f : Rd → R, we deﬁne the rth difference of f in the direction h ∈ Rd as (f )(x + h) − ∆r−1
++, let |β| = (cid:80)d (f )(x), ∆0 j=1 |βj|2. h(f )(x) := ∆r−1 h(f )(x) := f (x),
∆r h for x ∈ Ω with x + rh ∈ Ω, otherwise, let ∆r
Deﬁnition 1. For a function f ∈ Lp(Ω) where p ∈ (0, ∞], the r-th modulus of smoothness of f is deﬁned by wr,p(f, t) = suph∈Rd:|hi|≤ti (cid:107)∆r h(f )(cid:107)p, for t = (t1, . . . , td), ti > 0. h h(f )(x) = 0.
With this modulus of smoothness, we deﬁne the anisotropic Besov space Bβ (β1, . . . , βd)(cid:62) ∈ Rd
Deﬁnition 2 (Anisotropic Besov space (Bβ r := maxi(cid:98)βi(cid:99) + 1, let the seminorm | · |Bα
++ as follows. p,q(Ω))). For 0 < p, q ≤ ∞, β = (β1, . . . , βd)(cid:62) ∈ Rd p,q(Ω) for β =
++, be p,q
|f |Bβ p,q
:=


 (cid:18) ∞ (cid:80) k=0
[2kwr,p(f, (2−k/β1, . . . , 2−k/βd ))]q supk≥0 2kwr,p(f, (2−k/β1 , . . . , 2−k/βd )) (cid:19)1/q
The norm of the anisotropic Besov space Bβ
Bβ p,q(Ω) = {f ∈ Lp(Ω) | (cid:107)f (cid:107)Bβ
< ∞}. p,q p,q(Ω) is deﬁned by (cid:107)f (cid:107)Bβ p,q (q < ∞), (q = ∞).
:= (cid:107)f (cid:107)p + |f |Bβ p,q
, and
Roughly speaking β represents the smoothness in each direction. If βi is large, then a function in
Bβ p,q is smooth to the ith coordinate direction, otherwise, it is non-smooth to that direction. p is also an important quantity that controls the spatial inhomogeneity of the smoothness. If β1 = β2 = · · · =
βd, then the deﬁnition is equivalent to the usual Besov space (DeVore & Popov, 1988; DeVore et al., 1993). Suzuki (2019) analyzed curse of dimensionality of deep learning through a so-called mixed smooth Besov (m-Besov) space which imposes a stronger condition toward all directions uniformly. 2We let N := {1, 2, 3, . . . }, Z+ := {0, 1, 2, 3, . . . }, Zd
+ := {(z1, . . . , zd) | zi ∈ Z+}, R+ := {x ≥ 0 | x ∈ R}, and R++ := {x > 0 | x ∈ R}. We let [N ] := {1, . . . , N } for N ∈ N. 3
Particularly, it imposes stronger smoothness toward non-coordinate axis directions. Moreover, m-Besov space does not include the vanilla Besov space as a special case and thus cannot capture the situation that we consider in this paper.
Throughout this paper, for given β = (β1, . . . , βd)(cid:62) ∈ Rd
++, we write β := mini βi (smallest smoothness) and β := maxi βi (largest smoothness). The approximation error of a function in anisotropic Besov spaces is characterized by the harmonic mean of (βj)d j=1, which corresponds to the average smoothness, and thus we deﬁne (cid:101)β := (cid:16)(cid:80)d j=1 1/βj (cid:17)−1
. (1)
The Besov space is closely related to other function spaces such as H¨older space. Let ∂αf (x) = (x).
∂|α|f
∂α1 x1...∂αd xd
Deﬁnition 3 (H¨older space (Cβ(Ω))). For a smoothness paraemter β ∈ R++ with β (cid:54)∈ N, con-sider an m times differentiable function f : Rd → R where m = (cid:98)β(cid:99) (the largest integer (cid:13) (cid:13)∂αf (cid:107)∞ + less than β), and let the norm of the H¨older space Cβ(Ω) be (cid:107)f (cid:107)Cβ := max|α|≤m
. Then, (β-)H¨older space Cβ(Ω) is deﬁned as Cβ(Ω) = {f | max|α|=m supx,y∈Ω (cid:107)f (cid:107)Cβ < ∞}.
|∂αf (x)−∂αf (y)| (cid:107)x−y(cid:107)β−m
Let C0(Ω) be the set of continuous functions equipped with L∞-norm: C0(Ω) := {f : Ω → R | f is continuous and (cid:107)f (cid:107)∞ < ∞}. These function spaces are closely related to each other.
Proposition 1 (Triebel (2011)). There exist the following relations between the spaces: 1. For β = (β0, . . . , β0)(cid:62) ∈ Rd with β0 (cid:54)∈ N, it holds that Cβ0(Ω) = Bβ 2. For 0 < p1, p2, q ≤ ∞, p1 ≤ p2 and β ∈ Rd
∞,∞(Ω).
++ with (cid:101)β > (1/p1 − 1/p2)+ 3, it holds that4
Bβ p1,q(Ω) (cid:44)→ Bγβ p2,q(Ω) for γ = 1 − (1/p1 − 1/p2)+/ (cid:101)β. 3. For 0 < p, q1, q2 ≤ ∞, q1 < q2, and β ∈ Rd with properties 1 and 2, if (cid:101)β > 1/p, it holds that Bβ
++, it holds that Bβ (cid:44)→ Bβ
. In particular, p,q(Ω) (cid:44)→ Cγβ(Ω) where γ = 1 − 1/( (cid:101)βp). p,q1 p,q2 4. For 0 < p, q ≤ ∞ and β ∈ Rd
++, if (cid:101)β > 1/p, then Bβ p,q(Ω) (cid:44)→ C0(Ω).
This result is basically proven by Triebel (2011). For completeness, we provide its derivation in
Appendix D. If the average smoothness (cid:101)β is sufﬁciently large ( (cid:101)β > 1/p), then the functions in
Bβ p,q are continuous; however, if it is small ( (cid:101)β < 1/p), then they are no longer continuous. Small p indicates spatially inhomogeneous smoothness; thus, spikes and jumps appear (see Donoho &
Johnstone (1998) for this perspective, from the viewpoint of wavelet analysis). 2.2 Model of the true function
As a model of the true function f o, we consider two types of models: Afﬁen composition model and deep composition model. For a Banach space H, we let U (H) be the unit ball of H. (a) Afﬁne composition model: The ﬁrst model we introduced is a very naive model which is just a composition of an afﬁne transformation and a function in the anisotropic Besov space:
Haﬀ := {h(Ax + b) | h ∈ U (Bβ p,q([0, 1]
˜d)), A ∈ R ˜d×d, b ∈ Rb s.t. Ax + b ∈ [0, 1]
˜d (∀x ∈ Ω)}, where we assume ˜d ≤ d. Here, we assumed that the afﬁne transformation has an appropriate scaling such that Ax + b is included in the domain of h for all x ∈ Ω. This is a quite naive model but provides an instructive example to understand how the estimation error of deep learning behaves under the anisotropic setting. (b) Deep composition model: The deep composition model generalizes the afﬁne composition model to a composition of nonlinear functions. Let m1 = d, mL+1 = 1, m(cid:96) be the dimension of the 3Here, we let (x)+ := max{x, 0}. 4The symbol (cid:44)→ means continuous embedding. 4
the target
Figure 1: Near low dimensional data distribution with anisotropic func-smoothness of tion. The target function has less smoothness (s1, s2) toward the ﬁrst two coordinates on the manifold while it is almost constant toward the third coordinate (large s3).
++ be the smoothness parameter in the (cid:96)th layer. The deep composition (cid:96)th layer, and let β((cid:96)) ∈ Rm(cid:96) model is deﬁned as
Hdeep := {hH ◦ · · · ◦ h1(x) | h(cid:96) : [0, 1]m(cid:96)→ [0, 1]m(cid:96)+1, h(cid:96),k ∈ U (Bβ((cid:96)) p,q ([0, 1]m(cid:96))) (∀k ∈ [m(cid:96)+1])}.
Here, the interval [0, 1] can be replaced by another compact interval, such as [a(cid:96), b(cid:96)], but this dif-≤ 1 can also be ference can be absorbed by changing a scaling factor. The assumption (cid:107)h(cid:96),k(cid:107) relaxed, but we do not pursue that direction due to presentation simplicity. This model includes the afﬁne composition model as a special case. However, it requires a stronger assumption to properly evaluate the estimation error on this model.
Bβ((cid:96)) p,q
Examples The model we have introduced includes some instructive examples as listed below: (a) Linear projection Schmidt-Hieber (2020) analyzed estimation of the following model by deep learning: f o(x) = g(w(cid:62)x) where g ∈ Cβ([0, 1]) and w ∈ Rd. In this example, the function f o varies along only one direction, w. Apparently, this is an example of the afﬁne composition model. (b) Distribution on low dimensional smooth manifold Assume that the input x is distributed on a low-dimensional smooth manifold embedded in Ω, and the smoothness of the true function f o is anisotropic along a coordinate direction on the manifold. We suppose that the low dimen-sional manifold is ˜d-dimensional and ˜d (cid:28) d. In this situation, the true function can be written as f o(x) = h(φ(x)) where φ : Rd → R ˜d is a map that returns the coordinate of x on the manifold and h is an element in an anisotropic Besov space on R ˜d. This situation appears if data is distributed on a low-dimensional sub-manifold of Ω and the target function is invariant against noise injection to some direction on the manifold at each input point x (Figure 1 illustrates this situation). One typical example of this situation is a function invariant with data augmentation (Simard et al., 2003;
Krizhevsky et al., 2012). Even if the noise injection destroys low dimensionality of the data distribu-tion (i.e., ˜d = d), an anisotropic smoothness of the target function eases the curse of dimensionality as analyzed below, which is quite different from existing works (Yang & Dunson, 2016; Bickel &
Li, 2007; Nakada & Imaizumi, 2020; Schmidt-Hieber, 2019; Chen et al., 2019; Chen et al., 2019).