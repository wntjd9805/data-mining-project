Abstract
The visual world around us can be described as a structured set of objects and their associated relations. An image of a room may be conjured given only the descrip-tion of the underlying objects and their associated relations. While there has been signiﬁcant work on designing deep neural networks which may compose individual objects together, less work has been done on composing the individual relations between objects. A principal difﬁculty is that while the placement of objects is mutually independent, their relations are entangled and dependent on each other.
To circumvent this issue, existing works primarily compose relations by utilizing a holistic encoder, in the form of text or graphs. In this work, we instead propose to represent each relation as an unnormalized density (an energy-based model), enabling us to compose separate relations in a factorized manner. We show that such a factorized decomposition allows the model to both generate and edit scenes that have multiple sets of relations more faithfully. We further show that decompo-sition enables our model to effectively understand the underlying relational scene structure. Project page at: https://composevisualrelations.github.io/ 1

Introduction
The ability to reason about the component objects and their relations in a scene is key for a wide variety of robotics and AI tasks, such as multistep manipulation planning [11], concept learning [25], navigation [43], and dynamics prediction [3]. While a large body of work has explored inferring and understanding the underlying objects in a scene, robustly understanding the component relations in a scene remains a challenging task. In this work, we explore how to robustly understand relational scene description (Figure 1).
Naively, one approach towards understanding relational scene descriptions is to utilize existing multi-modal language and vision models. Such an approach has recently achieved great success in
DALL-E [36] and CLIP [35], both of which show compelling results on encoding object properties with language. However, when these approaches are instead utilized to encode relations between objects, their performance rapidly deteriorates, as shown in [36] and which we further illustrate in
Figure 7. We posit that the lack of compositionality in the language encoder prevents it from capturing all the underlying relations in an image.
To remedy this issue, we propose instead to factorize the scene description with respect to each individual relation. Separate models are utilized to encode each individual relation, which are then subsequently composed together to represent a relational scene description. The most straightforward
⇤indicates equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Image Generation
A small red metal cylinder below a small green rubber cube
A small red metal cylinder to the right of a large blue rubber cube
A small red metal cylinder above a small brown metal cylinder
Input relational scene description
StyleGAN2
Ours
Image Editing 
A small green metal cube below the large yellow rubber cube 
A large blue metal cylinder  above the large red rubber sphere
Input image
Input relational scene description
StyleGAN2
Ours
Figure 1: Our model can generate and edit images with multiple composed relations. Top: Image genera-tion results based on relational scene descriptions. Bottom: Image editing results based on relational scene descriptions. approach is to specify distinct regions of an image in which each relation can be located, as well as a composed relation description corresponding to the combination of all these regions.
Such an approach has signiﬁcant drawbacks. In practice, the location of one pair of objects in a relation description may be heavily inﬂuenced by the location of objects speciﬁed by another relation description. Specifying a priori the exact location of a relation will thus severely hamper the number of possible scenes that can be realized with a given set of relations. Is it possible to factorize relational descriptions of a scene and generate images that incorporate each given relation description simultaneously?
In this work, we propose to represent and factorize individual relations as unnormalized densities using Energy-Based Models. A relational scene description is represented as the product of the individual probability distributions across relations, with each individual relation specifying a separate probability distribution over images. Such a composition enables interactions between multiple relations to be modeled.
We show that this resultant framework enables us to reliably capture and generate images with multiple composed relational descriptions. It further enables us to edit images to have a desired set of relations. Finally, by measuring the relative densities assigned to different relational descriptions, we are able to infer the objects and their relations in a scene for downstream tasks, such as image-to-text retrieval and classiﬁcation.
There are three main contributions of our work. First, we present a framework to factorize and compose separate object relations. We show that the proposed framework is able to generate and edit images with multiple composed relations and signiﬁcantly outperforms baseline approaches.
Secondly, we show that our approach is able to infer the underlying relational scene descriptions and is robust enough in understanding semantically equivalent relational scene descriptions. Finally, we show that our approach can generalize to a previously unseen relation description, even if the underlying objects and descriptions are from a separate dataset not seen during training. We believe that such generalization is crucial for a general artiﬁcial intelligence system to adapt to the inﬁnite number of variations of the world around it. 2