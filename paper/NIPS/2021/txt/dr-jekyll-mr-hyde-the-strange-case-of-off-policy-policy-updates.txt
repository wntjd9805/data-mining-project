Abstract
The policy gradient theorem states that the policy should only be updated in states that are visited by the current policy, which leads to insufﬁcient planning in the off-policy states, and thus to convergence to suboptimal policies. We tackle this planning issue by extending the policy gradient theory to policy updates with re-spect to any state density. Under these generalized policy updates, we show con-vergence to optimality under a necessary and sufﬁcient condition on the updates’ state densities, and thereby solve the aforementioned planning issue. We also prove asymptotic convergence rates that signiﬁcantly improve those in the policy gradient literature. To implement the principles prescribed by our theory, we pro-pose an agent, Dr Jekyll & Mr Hyde (J&H), with a double personality: Dr Jekyll purely exploits while Mr Hyde purely explores. J&H’s independent policies al-low to record two separate replay buffers: one on-policy (Dr Jekyll’s) and one off-policy (Mr Hyde’s), and therefore to update J&H’s models with a mixture of on-policy and off-policy updates. More than an algorithm, J&H deﬁnes principles for actor-critic algorithms to satisfy the requirements we identify in our analysis.
We extensively test on ﬁnite MDPs where J&H demonstrates a superior ability to recover from converging to a suboptimal policy without impairing its speed of convergence. We also implement a deep version of the algorithm and test it on a simple problem where it shows promising results. 1

Introduction
Policy Gradient algorithms in Reinforcement Learning (RL) have enjoyed great success both theo-retically [50, 43, 16, 36] and empirically [23, 38, 37, 27]. Their principle consists in optimizing an objective function (the expected discounted return) through gradient steps, both being formally
J speciﬁed below [43]: (cid:34) (cid:35)
.
= E
∞(cid:88)
γtRt (π)
J (π) =
θ
∇
J t=0 dπ,γ(s) (cid:88) s
∈S (cid:12) (cid:12)
S0 ∼ (cid:12) (cid:12) (cid:88) a
∈A p0, At
π(
St), St+1 ∼
·|
∼ p(
St, At), Rt
·|
∼ r(
St, At)
·| qπ(s, a)
θπ(a s)
| and
θt+1 ←−−−−proj θt + ηt
θtJ
∇ (πt),
∇ (1) (2)
.
= (cid:80)∞t=0 γtP(St = s
|
.
= πθt are implicitly parametrized by θ for conciseness, and using standard Markov Decision Process notations (recalled in App. A), where ηt is the learning
.
= πθ and rate and dπ,γ(s)
←−−−−proj denotes the projection onto
πt the parameter space. We observe that the update established by the policy gradient theorem is proportional to the state density of the current policy. This is problematic, as it implies that no value improvement can be induced in off-policy states (i.e. states that are rarely visited). As a consequence,
π) the discounted state density, the policies π
∗Equal contribution. Correspondence to: rolaroch@microsoft.com. All code available at http://aka.ms/jnh. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
planning is inefﬁcient in those states to the point of potentially compromising convergence to the optimal policy.
To address the planning issue in off-policy states, we generalize the policy gradient theory by con-sidering the more general policy update:
U (θ, d) (cid:88)
.
= d(s) (cid:88) s
∈S a
∈A qπ(s, a)
∇ s)
θπ(a
| and
θt+1 ←−−−−proj θt + ηtU (θt, dt), (3) where dt designates the state distribution on which the policy is updated. It does not have to match the distribution induced by π and updates may therefore be off-policy. We note that U (θ, d) is not a
. gradient in the general case, hence the use of the term update. We prove in the direct: πθ
= θ and exp(θ) parametrizations that following the exact policy updates induces a sequence softmax: πθ ˙
.
∼
= qπt that is monotonously increasing, upper bounded, and thereby converges of value functions qt
” on the sequence (dt) is neces-to a q t ηtdt(s) = sary and sufﬁcient for q to be optimal. Our result generalizes previous theorems to broader updates and milder assumptions [2, 22]. Finally, we signiﬁcantly improve the existing asymptotic conver-gence rates. We show that (qt) converges to the optimal value i) exactly in ﬁnite time for the direct 2(cid:1) parametrization (previously in for the softmax parametrization, to be compared with 6(cid:1)[2]), and ii) in (cid:0)t− 2(1
. We then show that the condition “ (cid:0)t−
O
γ)−
|S||A| 6(cid:1) [22].
|S||A|
, (cid:80) (cid:0)t− s
∀
γ)−
γ)−
∈ S
∞ 1/2 (1 (1
O
−
−
∞
∞ 1 1
O 2
|S|
|A|
−
Building on our theoretical results, we design a novel algorithm: Dr Jekyll and Mr Hyde (J&H). Its principle consists in training two independent policies: a pure exploitation one (Dr Jekyll) and a pure exploration one (Mr Hyde), and give control to either one for full trajectories. J&H’s independent policies allow to record two separate replay buffers: one on-policy (Dr Jekyll’s) and one off-policy (Mr Hyde’s), and therefore to update J&H’s models with any desired mixture of on-policy and off-policy updates. Beyond an algorithm, J&H introduces conditions based on our analysis that actor-critic algorithms should follow to properly plan off-policy. Furthermore, the separation of exploration and exploitation allows to stabilise the training of the exploitation policy while ensuring a full coverage of the state-action space through deep exploration [30].
We empirically validate our theoretical analysis and algorithmic innovation in both planning and RL settings. In the planning setting where we assume that qt is exactly known, we analyze the impact of the off-policiness of dt and conclude that, while it improves over classic policy gradient, the theoret-ical sufﬁcient condition does not allow to converge in a reasonable amount of time in very hard plan-ning tasks: we thereby recommend to enforce the stronger condition “
Ω(t)”.
In the reinforcement learning domain, where qt must be estimated from the collected transitions, the off-policiness of J&H’s policy updates allows by design to satisfy the recommendation as long as Mr Hyde is able to cover the whole state space. This leads J&H to signiﬁcantly outperform all competing actor-critic algorithms in the hard planning tasks and to be competitive with gradient up-dates in simple planning problems. As a proof of concept, we also test J&H in a deep reinforcement learning setting and show that it outperforms various baselines on a simple environment. t ηtdt(s)
, (cid:80) s
∀
∈ S
∈
The paper is organized as follows. Section 2 develops the policy update theory. Section 3 describes
J&H and positions it with respect to the literature. Section 4 presents the experiments and their re-sults. Finally, Section 5 concludes the paper. The interested reader may refer to the appendix for the proofs (App. B), the domains (App. C), and the full experiment reports (App. D, E, and F). Finally, the supplementary material contains the code for the experiments: algorithms and environments. 2 Theoretical analysis
First, we recall some background: policy gradient methods depend on a parametrization θ
∈
R|S|×|A| of the policy. Like [2, 22], we will focus on the classic direct and softmax parametrizations:
• direct: π(a s)
|
.
= θs,a, for which us,a = dt(s)qπ(s, a) and the projection is on the simplex.
• softmax: π(a
.
= s)
| exp(θs,a) a(cid:48) exp(θs,a(cid:48)) (cid:80)
, for which us,a = dt(s)π(a s)(qπ(s, a)
| vπ(s)).
− 2
For both parametrizations, [2] proves that following the policy gradient tually leads to the optimal policy in ﬁnite MDPs under the following assumptions and condition2: (π) from Eq. (2) even-∇
J
θ
A1. The model (p0, p, r) is known.
A2. The initial state-distribution covers the full state space:
C3. The learning rate is constant: ηt = (1
− 2γ
, p0(s) > 0.
∈ S
γ)3 (direct) and ηt = (1
− 8 s
∀
γ)3 (softmax).
|A|
Those are stringent requirements. A1 implies that the values qπ and state density dπ,γ of the policy are known exactly. A2 is generally not satisﬁed in standard RL domains. Finally, verifying C3 makes learning slow to the point that it becomes impractical. 2.1 Convergence properties
In this section, we study, under A1, the convergence properties of the sequence of value functions (qt) induced by the update rule deﬁned in Eq. (3). We note that U (θ, d) is not a gradient in general.
Our ﬁrst theoretical result is the monotonicity of the value function sequence (qt).
Theorem 1 (Monotonicity under the direct and softmax parametrization). Under A1, the se-quence of value functions qt
.
= vπt are monotonously increasing.
.
= qπt and vt
By the monotonous convergence theorem, Thm. 1 directly implies the convergence of (qt):
Corollary 1 (Convergence under the direct and softmax parametrization). Under A1, the se-quence of value functions qt uniformly converges: q
.
= limt qt.
∞
→∞
In order to go further and prove convergence to a global optimal value, we need to enforce an additional condition. Indeed, the update U relies multiplicatively on dt which could be equal (or tend very fast) to 0 in some states, compromising the policy’s ability to reach the optimal value.
This argument is not only theoretical, it has been observed by many that policy gradient can get stuck in suboptimal policies, even with entropy regularization. We designed our chain domain to exhibit such behaviour (see Section 4.1).
Next, we show that (qt) converges to the optimal value function under some necessary and sufﬁcient condition.
Theorem 2 (Optimality under the direct and softmax parametrization). Under A1, the following condition:
C4. Each state s is updated with weights that sum to inﬁnity over time: (cid:80)∞t=0 ηtdt(s) =
,
∞ is necessary and sufﬁcient to guarantee that the sequence of value functions (qt) converges to opti-mality: q
.
= maxπ
= q(cid:63)
Π qπ.
∞
∈
∞ (s, a) (s, a)
.
= q
Proof sketch. In both direct and softmax parametrizations, we assume there exists a state-action vt, that is: pair (s, a) advantageous with respect to the state value limits q
∞ adv (s) > 0. We then prove that in this state s, the policy improvement yielded by the update is lower bounded by a linear function of the update weight ηtdt(s). Both parametrizations require different proof techniques and are dealt with in different theorems. Sum-ming over t, we notice that this lower bounding sum diverges to inﬁnity, which contradicts Corollary 1. We therefore infer that there cannot exist such a state-action pair, which allows us to conclude that no policy improvement is possible, and thus that the values are optimal: q
.
= limt and v
= maxπ
→∞
−
∞
∞
∞ v
∞
Π qπ.
∈
For the necessity of C4, we show that the parameter update is upper bounded in both parametriza-tions by a term linear in the action gap. By choosing the reward function adversarially, we may set it sufﬁciently small so that the sum of all the gradient steps is insufﬁcient to reach optimality.
Thm. 2 is impactful along ﬁve dimensions.
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates: As Thm. 2 is applicable to any distribution sequence, it
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
Practice of on-policy undiscounted updates:
. allows us in particular to consider the practice of using on-policy undiscounted updates: dt
= dπt,1. 2An assumption A# is a requirement on the environment or the application setting, while a condition C# is a requirement that may be enforced by a dedicated algorithm in any environment or setting. 3
Thus, it resolves a longstanding gap between the policy gradient theory and the actor-critic algorithm implementations [12, 7, 51, 17, 1, 19, 41]. This mismatch and its lack of theoretical grounding were identiﬁed in [45]. Later, [26] proved that the practitioners’ undiscounted updates are not the gradient of any function and may be strongly biased under state aliasing. Recently, [53] studied the practical advantage of the undiscounted updates from a representation learning perspective. C4 encompasses both standard policy gradient and the undiscounted update rule: convergence to optimality of both is guaranteed as long as C4 is veriﬁed. Our analysis thus shows that the convergence properties of policy gradient and undiscounted updates require the same set of assumptions and conditions.
Conversely, it is possible to prove convergence to sub-optimal policies of either when C4 is violated.
In other words, Thm. 2 allows to reduce the study of speciﬁc algorithms to whether they verify C4.
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates: It also justiﬁes the use of an experience replay for the
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates:
Experience replay and off-policy updates: actor, a trick also widely used in the literature to distributed the training over several agents [24, 34, 49, 14, 35]. Furthermore, while widely overlooked in the literature, we prove that off-policy updates have an even higher impact, since they are necessary to guarantee the convergence to optimality3.
We leverage this discovery to introduce new design principles and a novel algorithm in Section 3.
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory: Next, Thm. 2 generalizes previous optimality convergence results along two
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory:
Policy gradient theory: axes. The initial state distribution p0 is not required to cover the state space anymore (aka A2). A2 is unrealistic in most applications and cannot be enforced by an algorithm. Relaxing it is an important open problem [2], we disprove it in the full class of MDP. However, it might be possible to ﬁnd a class of MDPs larger than the one verifying A2 where policy gradient also converges. Additionally, the condition on our learning rates is also much more ﬂexible than C3.
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization: In contrast, C4 can be controlled by a dedicated algorithm. This
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization:
Density vs. policy regularization: theoretical result promotes the principle of density-based regularization [21, 32], at the expense of policy-based regularization that cannot guarantee that C4 is satisﬁed and sometimes fails to plan over the whole state space (our chain domain experiments in Section 4 illustrate it well).
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem: Finally, by combining Thm. 2, which analyzes pol-Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem:
Towards a generalized policy iteration theorem: icy improvement, with a thorough theoretical analysis of the policy evaluation step (discussed in
Section 3.1), we see a path towards results describing conditions on both components of generalized policy iteration that guarantee convergence to optimality [4]. Furthermore, Thm. 2 gives sufﬁcient conditions on the policy improvement step. The generalized policy iteration theorem was conjec-tured in [42] but never proved. 2.2 Convergence rates
Next, we give convergence rates for both softmax and direct parametrizations:
Theorem 3 (Asymptotic convergence rates under the direct parametrization). With direct parametrization, under A1 and C4, the sequence of value functions qt converges to optimality in
ﬁnite time: t0, such that
∃ t
∀
≥ t0, qt = q(cid:63). (4)
C4 is required for optimality and convergence rate contrarily to the softmax parametrization. We 6(cid:1)[2]. We wish to emphasize (cid:0)t− signiﬁcantly improve over previous bounds in that Theorem 3 does not say anything about how large t0 is, the rate is purely asymptotic.
Theorem 4 (Asymptotic convergence rates under the softmax parametrization). With softmax parametrization, under A1, C4, and A8:
|S||A|
γ)− 1/2 (1
O
−
A8. The optimal policy is unique: s, q(cid:63)(s, a1) = q(cid:63)(s, a2) = v(cid:63)(s) implies a1 = a2,
∀ the sequence of value functions qt converges asymptotically as follows: t0, such that
∃ t
∀
≥ t0, v(cid:63)(s) vt(s)
−
≤ (1
γ) mins
∈
− v (v 8 (cid:62) − supp(dπ(cid:63) ,γ ) δ(s) (cid:80)t
|A|
)
⊥ 1 t(cid:48)=t0 ηt(cid:48)dt(cid:48)(s)
−
, (5) 3Other papers proving convergence to optimality of policy gradient rely on A2, which implies C4. 4
where δ(s) = v(cid:63)(s) s, v (resp. v distribution of the optimal policy. maxa
−
/
⊥ (cid:62) q(cid:63)(s, a) is the gap with the best suboptimal action in state
) is the maximal (resp. minimal) value, and supp(dπ(cid:63),γ) denotes the support of the
π(cid:63)(s)
∈A
}
{
If (dt) satisﬁes the additional mild condition that the support of d(cid:63) is covered on average:
C5. s
∀
∈ supp(d(cid:63)), limt 1 t
→∞ (cid:80)t t(cid:48)=0 ηt(cid:48)dt(cid:48)(s)
.
= e
⊥ (s) > 0, then we obtain the following convergence rate in (t− 1):
O t0, such that
∃ t
∀
≥ t0, v(cid:63)(s) vt(s)
−
≤ (t
− t0)(1 (v 8
|A|
γ) mins v
)
⊥ (cid:62) − supp(dπ(cid:63) ,γ ) δ(s)e
∈
. (6) (s)
⊥
−
η
⊥
⊥
≥
> 0 (s) = η
⊥
|S| t, it is veriﬁed by a uniform
C5 is implied by A2 and C3. Alternately, assuming ηt
∀ distribution in which case e
, or by an on-policy distribution dπt that converges to the dis-dπ(cid:63) (s). Also, note that C4 is required tribution of some optimal policy dπ(cid:63) in which case e
. Thm. 4 establishes asymptotic for optimality, but not for convergence rates with respect to q 6(cid:1) [22]. The gap (cid:0)t−
∞ 2 1 rates in
|S| is partially explained by dropping C3 and allowing any learning rate (see App. A.6 for a thorough comparison). It is possible, see Thm. 7 of App. B, to show that under A2, a condition on the learning rate, and a bounding condition on dt (
), we have: t0 ≤ O 2(cid:1), to be compared with
> 0 such that 1 mins δ(s) (s) = η s, t, d
|S||A| dt(s) (cid:0)t−
γ)−
γ)− (cid:62) ≥ (cid:62) ≥ d (cid:62)d
⊥ 2(1
|A| (1
γ)7
O
O
≥
−
− (cid:16) (cid:17)
∀
∃ d d d (1 1
.
⊥
⊥
⊥
⊥
|S|
− 2.3