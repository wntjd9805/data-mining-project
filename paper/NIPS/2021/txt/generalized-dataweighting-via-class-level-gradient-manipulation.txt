Abstract
Label noise and class imbalance are two major issues coexisting in real-world datasets. To alleviate the two issues, state-of-the-art methods reweight each instance by leveraging a small amount of clean and unbiased data. Yet, these methods overlook class-level information within each instance, which can be further utilized to improve performance. To this end, in this paper, we propose
Generalized Data Weighting (GDW) to simultaneously mitigate label noise and class imbalance by manipulating gradients at the class level. To be speciﬁc, GDW unrolls the loss gradient to class-level gradients by the chain rule and reweights
In this way, GDW achieves remarkable the ﬂow of each gradient separately. performance improvement on both issues. Aside from the performance gain,
GDW efﬁciently obtains class-level weights without introducing any extra com-putational cost compared with instance weighting methods. Speciﬁcally, GDW performs a gradient descent step on class-level weights, which only relies on intermediate gradients. Extensive experiments in various settings verify the ef-fectiveness of GDW. For example, GDW outperforms state-of-the-art methods by 2.56% under the 60% uniform noise setting in CIFAR10. Our code is available at https://github.com/GGchen1997/GDW-NIPS2021. 1

Introduction
Real-world classiﬁcation datasets often suffer from two issues, i.e., label noise [1] and class im-balance [2]. On the one hand, label noise often results from the limitation of data generation, e.g., sensor errors [3] and mislabeling from crowdsourcing workers [4]. Label noise misleads the training process of DNNs and degrades the model performance in various aspects [5, 6, 7]. On the other hand, imbalanced datasets are either naturally long-tailed [8, 9] or biased from the real-world distribution due to imperfect data collection [10, 11]. Training with imbalanced datasets usually results in poor classiﬁcation performance on weakly represented classes [12, 13, 14]. Even worse, these two issues often coexist in real-world datasets [15].
To prevent the model from memorizing noisy information, many important works have been proposed, including label smoothing [16], noise adaptation [17], importance weighting [18], GLC [19], and
Co-teach [20]. Meanwhile, [12, 13, 14, 21] propose effective methods to tackle class imbalance.
However, these methods inevitably introduce hyper-parameters (e.g., the weighting factor in [13] and the focusing parameter in [21]), compounding real-world deployment.
Inspired by recent advances in meta-learning, some works [22, 23, 24, 25] propose to solve both issues by leveraging a clean and unbiased meta set. These methods treat instance weights as hyper-∗Equal contribution; Names listed in alphabetical order. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
parameters and dynamically update these weights to circumvent hyper-parameter tuning. Speciﬁcally,
MWNet [23] adopts an MLP with the instance loss as input and the instance weight as output. Due to the MLP, MWNet has better scalability on large datasets compared with INSW [24] which assigns each instance with a learnable weight. Although these methods can handle label noise and class imbal-ance to some extent, they cannot fully utilize class-level information within each instance, resulting in the potential loss of useful information. For example, in a three-class classiﬁcation task, every instance has three logits. As shown in Figure 1, every logit corresponds to a class-level gradient ﬂow which stems from the loss function and back-propagates. These gradient ﬂows represent three kinds of in-formation: "not cat", "dog", and "not bird". Instance weighting methods [23, 22] alleviate label noise by downweighting all the gradient ﬂows of the instance, which discards three kinds of information simultaneously. Yet, downweighting the "not bird" gradient ﬂow is a waste of information. Similarly, in class imbalance scenarios, different gradient ﬂows represent different class-level information.
Therefore, it is necessary to reweight instances at the class level for better information usage.
To this end, we propose Generalized Data
Weighting (GDW) to tackle label noise and class imbalance by class-level gradient manipulation.
Firstly, we introduce class-level weights to rep-resent the importance of different gradient ﬂows and manipulate the gradient ﬂows with these class-level weights. Secondly, we impose a zero-mean constraint on class-level weights for stable training. Thirdly, to efﬁciently obtain class-level weights, we develop a two-stage weight gener-ation scheme embedded in bi-level optimization.
As a side note, the instance weighting meth-ods [22, 23, 24, 25] can be considered special cases of GDW when class-level weights within any instance are the same. In this way, GDW achieves impressive performance improvement in various settings.
Figure 1: Motivation for class-level weighting. For a noisy instance (e.g. cat mislabeled as "dog"), all gradient ﬂows are downweighted by instance weighting. Although the gradient ﬂows for "dog" and "not cat" contain harmful information, the gra-dient ﬂow for "not bird" is still valuable for train-ing, which should not be downweighted.
To sum up, our contribution is two-fold: 1. For better information utilization, we propose GDW, a generalized data weighting method, which better handles label noise and class imbalance. To the best of our knowledge, we are the ﬁrst to propose single-label class-level weighting on gradient ﬂows. 2. To obtain class-level weights efﬁciently, we design a two-stage scheme embedded in a bi-level optimization framework, which does not introduce any extra computational cost. To be speciﬁc, during the back-propagation we store intermediate gradients, with which we update class-level weights via a gradient descent step. 2