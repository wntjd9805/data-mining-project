Abstract
√ n log n
We study the following learning problem with dependent data: Observing a tra-jectory of length n from a stationary Markov chain with k states, the goal is to predict the next state. For 3 ≤ k ≤ O( n), using techniques from universal compression, the optimal prediction risk in Kullback-Leibler divergence is shown to be Θ( k2 k2 ), in contrast to the optimal rate of Θ( log log n
) for k = 2 previ-ously shown in [FOPS16]. These rates, slower than the parametric rate of O( k2 n ), can be attributed to the memory in the data, as the spectral gap of the Markov chain can be arbitrarily small. To quantify the memory effect, we study irreducible reversible chains with a prescribed spectral gap. In addition to characterizing the optimal prediction risk for two states, we show that, as long as the spectral gap is not excessively small, the prediction risk in the Markov model is O( k2 n ), which coincides with that of an iid model with the same number of parameters. n 1

Introduction
Learning distributions from samples is a central question in statistics and machine learning. While significant progress has been achieved in property testing and estimation based on independent and identically distributed (iid) data, for many applications, most notably natural language processing, two new challenges arise: (a) Modeling data as independent observations fails to capture their temporal dependency; (b) Distributions are commonly supported on a large domain whose cardinality is comparable to or even exceeds the sample size. Continuing the progress made in [FOPS16, HOP18], in this paper we study the following prediction problem with dependent data modeled as Markov chains.
Suppose X1, X2, . . . is a stationary first-order Markov chain on state space [k] ≜ {1, . . . , k} with unknown statistics. Observing a trajectory X n ≜ (X1, . . . , Xn), the goal is to predict the next state Xn+1 by estimating its distribution conditioned on the present data. We use the Kullback-Leibler (KL) divergence as the loss function: For distributions P = [p1, . . . , pk] , Q = [q1, . . . , qk],
D(P ∥Q) = (cid:80)k if pi = 0 whenever qi = 0 and D(P ∥Q) = ∞ otherwise. The minimax i=1 pi log pi qi 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
prediction risk is given by
Riskk,n ≜ inf (cid:99)M sup
π,M
E[D(M (·|Xn)∥ (cid:99)M (·|Xn))] = inf (cid:99)M sup
π,M k (cid:88) i=1
E[D(M (·|i)∥ (cid:99)M (·|i))1{Xn=i}] (1) where the supremum is taken over all stationary distributions π and transition matrices M (row-stochastic) such that πM = π, the infimum is taken over all estimators (cid:99)M = (cid:99)M (X1, . . . , Xn) that are proper Markov kernels (i.e. rows sum to 1), and M (·|i) denotes the ith row of M . Our main objective is to characterize this minimax risk within universal constant factors as a function of n and k.
The prediction problem (1) is distinct from the parameter estimation problem such as estimating the transition matrix [Bar51, AG57, Bil61, WK19] or its properties [CS00, KV16, HJL+18, HKL+19] in that the quantity to be estimated (conditional distribution of the next state) depends on the sample path itself. This is precisely what renders the prediction problem closely relevant to natural applications such as autocomplete and text generation. In addition, this formulation allows more flexibility with far less assumptions compared to the estimation framework. For example, if certain state has very small probability under the stationary distribution, consistent estimation of the transition matrix with respect to usual loss function, e.g. squared risk, may not be possible, whereas the prediction problem is unencumbered by such rare states.
In the special case of iid data, the prediction problem reduces to estimating the distribution in KL divergence. In this setting the optimal risk is well understood, which is known to be k−1 2n (1 + o(1)) when k is fixed and n → ∞ [BFSS02] and Θ( k n ) for k = O(n) [Pan04, KOPS15].1 Typical in parametric models, this rate k n is commonly referred to the “parametric rate”, which leads to a sample complexity that scales proportionally to the number of parameters and inverse proportionally to the desired accuracy.
In the setting of Markov chains, however, the prediction problem is much less understood especially for large state space. Recently the seminal work [FOPS16] showed the surprising result that for stationary Markov chains on two states, the optimal prediction risk satisfies
Risk2,n = Θ (cid:18) log log n n (cid:19)
, (2) which has a nonparametric rate even when the problem has only two parameters. The follow-up work [HOP18] studied general k-state chains and showed a lower bound of Ω( k log log n
) for uniform (not necessarily stationary) initial distribution; however, the upper bound O( k2 log log n
) in [HOP18] relies on implicit assumptions on mixing time such as spectral gap conditions: the proof of the upper bound for prediction (Lemma 7 in the supplement) and for estimation (Lemma 17 of the supplement) is based on Berstein-type concentration results of the empirical transition counts, which depend on spectral gap. The following theorem resolves the optimal risk for k-state Markov chains: n n
Theorem 1 (Optimal rates without spectral gap). There exists a universal constant C > 0 such that for all 3 ≤ k ≤ n/C,
√ k2
Cn log (cid:17) (cid:16) n k2
≤ Riskk,n ≤
Ck2 n log (cid:17)
. (cid:16) n k2 (3)
Furthermore, the lower bound continues to hold even if the Markov chain is restricted to be irreducible and reversible.
Remark 1. The optimal prediction risk of O( k2 n log n k2 ) can be achieved by an average version of the add-one estimator (i.e. Laplace’s rule of succession). Given a trajectory xn = (x1, . . . , xn) of length n, denote the transition counts (with the convention Ni ≡ Nij ≡ 0 if n = 0, 1)
Ni = n−1 (cid:88)
ℓ=1 1{xℓ=i}, Nij = n−1 (cid:88)
ℓ=1 1{xℓ=i,xℓ+1=j}. (4) 1Here and below ≍, ≲, ≳ or Θ(·), O(·), Ω(·) denote equality and inequalities up to universal multiplicative constants. 2
The add-one estimator for the transition probability M (j|i) is given by (cid:99)M +1 xn (j|i) ≜ Nij + 1
Ni + k
, (5) which is an additively smoothed version of the empirical frequency. Finally, the optimal rate in (3) can be achieved by the following estimator (cid:99)M defined as an average of add-one estimators over different sample sizes: (cid:99)Mxn (xn+1|xn) ≜ 1 n n (cid:88) t=1 (cid:99)M +1 xn n−t+1 (xn+1|xn). (6)
In other words, we apply the add-one estimator to the most recent t observations (Xn−t+1, . . . , Xn) to predict the next Xn+1, then average over t = 1, . . . , n. Such Cesàro-mean-type estimators have been introduced before in the density estimation literature (see, e.g., [YB99]). It remains open whether the usual add-one estimator (namely, the last term in (6) which uses all the data) or any add-c estimator for constant c achieves the optimal rate. In contrast, for two-state chains the optimal risk (2) is attained by a hybrid strategy [FOPS16], applying add-c estimator for c = 1 log n for trajectories with at most one transition and c = 1 otherwise. Also note that the estimator in (6) can be computed in
O(nk) time. To derive this first note that given any j ∈ [k] calculating (cid:99)M +1 (j|xn−1) takes O(n) xn−1 1 (j|xn−1) we need O(1) time to calculate (cid:99)M +1 time and given any M +1 xn−1 over all j we get the algorithmic complexity upper bound. (j|xn−1). Summing xn−1 n−t+1 n−t+2
Theorem 1 shows that the departure from the parametric rate of k2 n , first discovered in [FOPS16,
HOP18] for binary chains, is even more pronounced for larger state space. As will become clear in the proof, there is some fundamental difference between two-state and three-state chains, resulting in
Risk3,n = Θ( log n
). It is instructive to compare the sample complexity for prediction in the iid and Markov model. Denote by d the number of parameters, which is k − 1 for the iid case and k(k − 1) for Markov chains. Define the sample complexity n∗(d, ϵ) as the smallest sample size n in order to achieve a prescribed prediction risk ϵ. For ϵ = O(1), we have n ) ≫ Risk2,n = Θ( log log n n n∗(d, ϵ) ≍


 d
ϵ d iid
ϵ log log 1
ϵ log 1
ϵ d
ϵ Markov with 2 states
Markov with k ≥ 3 states. (7)
At a high level, the nonparametric rates in the Markov model can be attributed to the memory in the data. On the one hand, Theorem 1 as well as (2) affirm that one can obtain meaningful prediction without imposing any mixing conditions;2 such decoupling between learning and mixing has also been observed in other problems such as learning linear dynamics [SMT+18, DMM+19]. On the other hand, the dependency in the data does lead to a strictly higher sample complexity than that of the iid case; in fact, the lower bound in Theorem 1 is proved by constructing chains with spectral gap as small as O( 1 n ) (see Section 3). Thus, it is conceivable that with sufficiently favorable mixing conditions, the prediction risk improves over that of the worst case and, at some point, reaches the parametric rate. To make this precise, we focus on Markov chains with a prescribed spectral gap.
It is well-known that for an irreducible and reversible chain, the transition matrix M has k real eigenvalues satisfying 1 = λ1 ≥ λ2 ≥ . . . λk ≥ −1. The absolute spectral gap of M , defined as
γ∗ ≜ 1 − max {|λi| : i ̸= 1} , quantifies the memory of the Markov chain. For example, the mixing time is determined by 1/γ∗ (relaxation time) up to logarithmic factors. As extreme cases, the chain which does not move (M is identity) and which is iid (M is rank-one) have spectral gap equal to 0 and 1, respectively. We refer the reader to [LP17] for more background. Note that the definition of absolute spectral gap requires irreducibility and reversibility, thus we restrict ourselves to this class of Markov chains (it is possible to use more general notions such as pseudo spectral gap to quantify the memory of the (8) 2To see this, it is helpful to consider the extreme case where the chain does not move at all or is periodic, in which case predicting the next state is in fact easy. 3
process, which is beyond the scope of the current paper). Given γ0 ∈ (0, 1), define Mk(γ0) as the set of transition matrices corresponding to irreducible and reversible chains whose absolute spectral gap exceeds γ0. Restricting (1) to this subcollection and noticing the stationary distribution here is uniquely determined by M , we define the corresponding minimax risk:
Riskk,n(γ0) ≜ inf (cid:99)M sup
M ∈Mk(γ0)
E (cid:104)
D(M (·|Xn)∥ (cid:99)M (·|Xn)) (cid:105) (9)
Extending the result (2) of [FOPS16], the following theorem characterizes the optimal prediction risk for two-state chains with prescribed spectral gaps (the case γ0 = 0 correspond to the minimax rate in
[FOPS16] over all binary Markov chains):
Theorem 2 (Spectral gap dependent rates for binary chain). For any γ0 ∈ (0, 1)
Risk2,n(γ0) ≍ 1 n (cid:26) (cid:18) (cid:26) max 1, log log min n, (cid:27)(cid:19)(cid:27)
. 1
γ0
Theorem 2 shows that for binary chains, parametric rate O( 1 n ) is achievable if and only if the spectral gap is nonvanishing. While this holds for bounded state space (see Corollary 4 below), for large state space, it turns out that much weaker conditions on the absolute spectral gap suffice to guarantee the parametric rate O( k2 n ), achieved by the add-one estimator applied to the entire trajectory. In other words, as long as the spectral gap is not excessively small, the prediction risk in the Markov model behaves in the same way as that of an iid model with equal number of parameters. Similar conclusion has been established previously for the sample complexity of estimating the entropy rate of Markov chains in [HJL+18, Theorem 1].
Theorem 3. The add-one estimator in (5) achieves the following risk bound. (i) For any k ≥ 2, Riskk,n(γ0) ≲ k2 n provided that γ0 ≳ ( log k k )1/4. (ii) In addition, for k ≳ (log n)6, Riskk,n(γ0) ≲ k2
Corollary 4. For any fixed k ≥ 2, Riskk,n(γ0) = O( 1 n provided that γ0 ≳ (log(n+k))2 n ) if and only if γ0 = Ω(1). k
. 1.1 Proof techniques
The proof of Theorem 1 deviates from existing approaches based on concentration inequalities for
Markov chains. For instance, the standard program for analyzing the add-one estimator (5) involves proving concentration of the empirical counts on their population version, namely, Ni ≈ nπi and
Nij ≈ nπiM (j|i), and bounding the risk in the atypical case by concentration inequalities, such as the Chernoff-type bounds in [Lez98, Pau15], which have been widely used in recent work on statistical inference with Markov chains [KV16, HJL+18, HOP18, HKL+19, WK19]. However, these concentration inequalities inevitably depends on the spectral gap of the Markov chain, leading to results which deteriorate as the spectral gap becomes smaller. For two-state chains, results free of the spectral gap are obtained in [FOPS16] using explicit joint distribution of the transition counts; this refined analysis, however, is difficult to extend to larger state space as the probability mass function of (Nij) is given by Whittle’s formula [Whi55] which takes an unwieldy determinantal form.
Eschewing concentration-based arguments, the crux of our proof of Theorem 1, for both the upper and lower bound, revolves around the following quantity known as redundancy:
Redk,n ≜ inf
QXn sup
PXn
D(PX n ∥QX n) = inf
QXn sup
PXn (cid:88) xn
PX n (xn) log
PX n (xn)
QX n (xn)
. (10)
Here the supremum is taken over all joint distributions of stationary Markov chains X n on k states, and the infimum is over all joint distributions QX n . A central quantity which measures the minimax regret in universal compression, the redundancy (10) corresponds to minimax cumulative risk (namely, the total prediction risk when the sample size ranges from 1 to n), while (1) is the individual minimax risk at sample size n – see Section 2 for a detailed discussion. We prove the following reduction between prediction risk and redundancy: 1 n
Redsym k−1,n − log k n
≲ Riskk,n ≤ 1 n − 1
Redk,n (11) 4
where Redsym denotes the redundancy for symmetric Markov chains. The upper bound is standard: thanks to the convexity of the loss function and stationarity of the Markov chain, the risk of the Cesàro-mean estimator (6) can be upper bounded using the cumulative risk and, in turn, the redundancy. The proof of the lower bound is more involved. Given a (k − 1)-state chain, we embed it into a larger state space by introducing a new state, such that with constant probability, the chain starts from and gets stuck at this state for a period time that is approximately uniform in [n], then enters the original chain. Effectively, this scenario is equivalent to a prediction problem on k − 1 states with a random (approximately uniform) sample size, whose prediction risk can then be related to the cumulative risk and redundancy. This intuition can be made precise by considering a Bayesian setting, in which the (k − 1)-state chain is randomized according to the least favorable prior for (10), and representing the
Bayes risk as conditional mutual information and applying the chain rule.
Given the above reduction in (11), it suffices to show both redundancies therein are on the order of k2 n log n k2 . The redundancy is upper bounded by pointwise redundancy, which replaces the average in (10) by the maximum over all trajectories. Following [DMPW81, CS04], we consider an explicit probability assignment defined by add-one smoothing and using combinatorial arguments to bound the pointwise redundancy, shown optimal by information-theoretic arguments.
The optimal spectral gap-dependent rate in Theorem 2 relies on the key observation in [FOPS16] that, for binary chains, the dominating contribution to the prediction risk comes from trajectories with a single transition, for which we may apply an add-c estimator with c depending appropriately on the spectral gap. The lower bound is shown using a Bayesian argument similar to that of [HOP18,
Theorem 1]. The proof of Theorem 3 relies on more delicate concentration arguments as the spectral gap is allowed to be vanishingly small. Notably, for small k, direct application of existing Bernstein inequalities for Markov chains in [Lez98, Pau15] falls short of establishing the parametric rate of
O( k2 n ) (see Remark 4 in Section 7.2 for details); instead, we use a fourth moment bound which turns out to be well suited for analyzing concentration of empirical counts conditional on the terminal state.
For large k, we further improve the spectral gap condition using a simulation argument for Markov chains using independent samples [Bil61, HJL+18]. A key step is a new concentration inequality for n,k), where (cid:98)P +1
D(P ∥ (cid:98)P +1 n,k is the add-one estimator based on n iid observations of P supported on [k]:
√ (cid:32) (cid:33)
P
D(P ∥ (cid:98)P +1 n,k) ≥ c · k n
+ polylog(n) · k n
≤ 1 poly(n)
, (12)
√ for some absolute constant c > 0. Note that an application of the classical concentration inequality of
McDiarmid would result in the second term being polylog(n)/ n, and (12) crucially improves this k/n. Such an improvement has been recently observed by [MJT+20, Agr20, GR20] to polylog(n) · in studying the similar quantity D( (cid:98)Pn∥P ) for the (unsmoothed) empirical distribution (cid:98)Pn; however, these results, based on either the method of types or an explicit upper bound of the moment generating function, are not directly applicable to (12) in which the true distribution P appears as the first argument in the KL divergence.
√ 1.2