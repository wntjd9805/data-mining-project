Abstract
Deep reinforcement learning has generated superhuman AI in competitive games such as Go and StarCraft. Can similar learning techniques create a superior AI teammate for human-machine collaborative games? Will humans prefer AI team-mates that improve objective team performance or those that improve subjective metrics of trust? In this study, we perform a single-blind evaluation of teams of humans and AI agents in the cooperative card game Hanabi, with both rule-based and learning-based agents. In addition to the game score, used as an objective metric of the human-AI team performance, we also quantify subjective measures of the human’s perceived performance, teamwork, interpretability, trust, and overall preference of AI teammate. We ﬁnd that humans have a clear preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art learning-based AI teammate (Other-Play) across nearly all subjective metrics, and generally view the learning-based agent negatively, despite no statistical difference in the game score. This result has implications for future AI design and reinforcement learning benchmarking, highlighting the need to incorporate subjective metrics of human-AI teaming rather than a singular focus on objective task performance. 4 1

Introduction
Advances in artiﬁcial intelligence (AI) have resulted in agents that perform at superhuman levels within domains previously thought to be solely the purview of human intelligence. Such performance has been demonstrated through application of reinforcement learning (RL) in environments such as board games [6, 39], arcade games [31], real-time strategy games [44], multiplayer online battle arenas [4], and simulated aerial dogﬁghts [11].
In nearly all cases, these demonstrations of AI superiority are in purely adversarial, one- or two-player games. However, in order to achieve real-world applicability and adoption, AI must be able to demonstrate teaming intelligence, particularly with human teammates [23]. Due to the focus on adversarial games, teaming intelligence has been understudied in RL research. AI teammates must also exhibit behavior that engenders an appropriate level of certain human reactions, such as trust, mental workload, and risk perception [26, 37]. Failure to do so risks the same kind of misuse, disuse, and abuse that Parasuraman and Riley illustrated with traditional automation systems [37]. These issues are distinct from much of current multi-agent AI work, as AI that is able to team effectively with other AI agents has failed to work effectively with humans [9].
∗MIT Lincoln Laboratory, {hochit.siu,jdpena,yutai.zhou,chestnut,ross.allen}@ll.mit.edu
†MIT Department of Electrical Engineering and Computer Science, edenna@mit.edu
‡U.S. Air Force Artiﬁcial Intelligence Accelerator, {victor.lopez.10,kyle.palko.1}@us.af.mil 4DISTRIBUTION STATEMENT A. Approved for public release. Distribution is unlimited. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
The objective of this paper is to evaluate human-AI teaming in the cooperative, imperfect-information game of Hanabi. We consider not only the objective performance of a human-AI team, but also the subjective human reactions and preferences when working with different kinds of AI teammates.
Based on the success of applying deep reinforcement learning to create superhuman AI in adversarial games, we hypothesize that similar RL techniques can render collaborative AI that outperform and are preferred over rule-based agents in human-AI Hanabi teams. Our results show that this hypothesis is not supported given the current state of the art of collaborative RL agents. Human participants show a clear preference toward rule-based AI even though the learning-based AI perform no worse and are speciﬁcally optimized for teaming with previously unknown partners (e.g. humans) [21]. To the best of our knowledge, this is the ﬁrst comparative study of objective performance of rule-based and learning-based Hanabi AI in human-teaming experiments, as well as the ﬁrst quantiﬁed study of subjective human preferences toward such AI. 2