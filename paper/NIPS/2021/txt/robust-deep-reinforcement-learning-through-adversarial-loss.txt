Abstract
Recent studies have shown that deep reinforcement learning agents are vulnerable to small adversarial perturbations on the agent’s inputs, which raises concerns about deploying such agents in the real world. To address this issue, we propose
RADIAL-RL, a principled framework to train reinforcement learning agents with improved robustness against lp-norm bounded adversarial attacks. Our frame-work is compatible with popular deep reinforcement learning algorithms and we demonstrate its performance with deep Q-learning, A3C and PPO. We experi-ment on three deep RL benchmarks (Atari, MuJoCo and ProcGen) to show the effectiveness of our robust training algorithm. Our RADIAL-RL agents consis-tently outperform prior methods when tested against attacks of varying strength and are more computationally efﬁcient to train. In addition, we propose a new evaluation method called Greedy Worst-Case Reward (GWC) to measure attack agnostic robustness of deep RL agents. We show that GWC can be evaluated efﬁciently and is a good estimate of the reward under the worst possible se-quence of adversarial attacks. All code used for our experiments is available at https://github.com/tuomaso/radial_rl_v2. 1

Introduction
Deep learning has achieved enormous success on a variety of challenging domains, ranging from computer vision [1], natural language processing [2] to reinforcement learning (RL) [3, 4]. Neverthe-less, the existence of adversarial examples [5] indicates that deep neural networks (DNNs) are not as robust and trustworthy as we would expect, as small and often imperceptible perturbations can result in misclassiﬁcations of state-of-the-art DNNs. Unfortunately, adversarial attacks have also been shown possible in deep reinforcement learning, where adversarial perturbations in the observation space and/or action space can cause arbitrarily bad performance of deep RL agents [6, 7, 8]. As deep
RL agents are deployed in many safety critical applications such as self-driving cars and robotics, it is of crucial importance to develop robust training algorithms (a.k.a. defense algorithms) such that the resulting trained agents are robust against adversarial (and non-adversarial) perturbation.
Many heuristic defenses have been proposed to improve robustness of DNNs against adversarial attacks for image classiﬁcation tasks, but they often fail against stronger adversarial attack algorithms.
For example, [9] showed that 13 such defense methods (recently published at prestigious conferences) can all be broken by more advanced attacks. One emerging alternative to heuristic defenses is
∗correspondence to: toikarinen@ucsd.edu, lweng@ucsd.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Screenshots of our RADIAL framework promoting robustness of deep RL agents while the standard deep RL agents without robust training are vulnerable to adversarial attacks. defense algorithms 2 based on robustness veriﬁcation or certiﬁcation bounds [10, 11, 12, 13]. These algorithms produce robustness certiﬁcates such that for any perturbations within the speciﬁed (cid:96)p-norm distance (cid:15), the trained DNN will produce consistent classiﬁcation results on given data points.
Representative works along this line include [11] and [14], where the learned models can have much higher certiﬁed accuracies by including the robustness veriﬁcation bounds in the loss function with proper training schedule, even if the veriﬁer produces loose robustness certiﬁcates [14] for models without robust training (a.k.a. nominal models). Here, the certiﬁed accuracy is calculated as the percentage of the test images that are guaranteed to be classiﬁed correctly under a given perturbation magnitude (cid:15).
However, most of the defense algorithms are developed for classiﬁcation tasks. Few defense algo-rithms have been designed for deep RL agents perhaps due to the additional challenges in RL that are not present in classiﬁcation tasks, including credit assignment and lack of a stationary training set. To bridge this gap, in this paper we present the RADIAL(Robust ADversarIAl Loss)-RL framework to train robust deep RL agents. We show that RADIAL can improve the robustness of deep RL agents by using carefully designed adversarial loss functions based on robustness veriﬁcation bounds. Our contributions are listed below:
• We propose a novel robust deep RL framework, RADIAL-RL, which can be applied to different types of deep RL algorithms. We demonstrate RADIAL on three popular RL algorithms, DQN [3], A3C [15] and PPO [16].
• We demonstrate the superior performance of RADIAL agents on both Atari games and continuous control tasks in MuJoCo: our agents are 2 − 10× more computationally efﬁcient to train than [17] and can resist up to 5× stronger adversarial perturbations better than existing works [18, 17].
• We also evaluate the effects of robust training on the ability of agents to generalize to new levels using the ProcGen benchmark, and show that our training also increases robustness on unseen levels, reaching high rewards even against (cid:15) = 5/255 PGD-attacks.
• We propose a new evaluation method, Greedy Worst-Case Reward (GWC), for efﬁciently (in linear time) evaluating RL agent performance under attack of strongest adversaries (i.e. worst-case perturbation) on discrete action environments. 2We don’t use the naming convention in this ﬁeld to call this type of defense as certiﬁed defense because we think it is misleading as such defense methodology cannot provide any certiﬁcates on unseen data. 2
2