Abstract
Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefﬁcient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowl-edge of training other networks. We introduce a large-scale dataset of diverse com-putational graphs of neural architectures – DEEPNETS-1M– and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efﬁcient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis. 1

Introduction
Consider the problem of training deep neural networks on large annotated datasets, such as
ImageNet [1]. This problem can be formalized as ﬁnding optimal parameters for a given neural network a, parameterized by w, w.r.t. a loss function L on the dataset D = {(xi, yi)}N i=1 of inputs xi and targets yi: arg min w (cid:88)N i=1
L(f (xi; a, w), yi), (1) where f (xi; a, w) represents a forward pass. Equation 1 is usually minimized by iterative optimization algorithms – e.g. SGD [2] and Adam [3] – that converge to performant parameters wp of the architecture a. Despite the progress in improving the training speed and convergence [4–7], obtaining wp remains a bottleneck in large-scale machine learning pipelines. For example, training a ResNet-50 [8] on ImageNet can take many GPU hours [9]. With the ever growing size of net-works [10] and necessity of training the networks repeatedly (e.g. for hyperparameter or architecture search), the classical process of obtaining wp is becoming computationally unsustainable [11–13].
A new parameter prediction task. When optimizing the parameters for a new architecture a, typical optimizers disregard past experience gained by optimizing other nets. However, leveraging this past experience can be the key to reduce the reliance on iterative optimization and, hence the high computational demands. To progress in that direction, we propose a new task where iterative optimization is replaced with a single forward pass of a hypernetwork [14] HD. To tackle the task, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
∗Part of the work was done while interning at Facebook AI Research.
HD is expected to leverage the knowledge of how to optimize other networks F. Formally, the task is to predict the parameters of an unseen architecture a /∈ F using HD, parameterized by θp:
ˆwp = HD(a; θp). The task is constrained to a dataset D, so ˆwp are the predicted parameters for which the test set performance of f (x; a, ˆwp) is similar to the one of f (x; a, wp). For example, we consider CIFAR-10 [15] and ImageNet image classiﬁcation datasets D, where the test set performance is classiﬁcation accuracy on test images.
Approaching our task. A straightforward approach to expose HD to the knowledge of how to optimize other networks is to train it on a large training set of {(ai, wp,i)} pairs, however, that is prohibitive2. Instead, we follow the bi-level optimization paradigm common in meta-learning [16–18], but rather than iterating over M tasks, we iterate over M training architectures F = {ai}M i=1: arg min
θ (cid:88)N (cid:88)M j=1 i=1 (cid:16) (cid:16)
L f xj; ai, HD(ai; θ) (cid:17) (cid:17)
.
, yj (2)
By optimizing Equation 2, the hypernetwork HD gradually gains knowledge of how to predict performant parameters for training architectures. It can then leverage this knowledge at test time – when predicting parameters for unseen architectures. To approach the problem in Equation 2, we need to design the network space F and HD. For F, we rely on the previous design spaces for neural architectures [19] that we extend in two ways: the ability to sample distinct architectures and an expanded design space that includes diverse architectures, such as ResNets and Visual
Transformers [20]. Such architectures can be fully described in the form of computational graphs (Fig. 1). So, to design the hypernetwork HD, we rely on recent advances in machine learning on graph-structured data [21–24]. In particular, we build on the Graph HyperNetworks method (GHNs) [24] that also optimizes Equation 2. However, GHNs do not aim to predict large-scale performant parameters as we do in this work, which motivates us to improve on their approach.
By designing our diverse space F and improving on GHNs, we boost the accuracy achieved by the predicted parameters on unseen architectures to 77% (top-1) and 48% (top-5) on CIFAR-10 [15] and
ImageNet [1], respectively. Surprisingly, our GHN shows good out-of-distribution generalization and predicts good parameters for architectures that are much larger and deeper compared to the ones seen in training. For example, we can predict all 24 million parameters of ResNet-50 in less than a second either on a GPU or CPU achieving ∼60% on CIFAR-10 without any gradient updates (Fig 1, (b)).
Overall, our framework and results pave the road toward a new and signiﬁcantly more efﬁcient paradigm for training networks. Our contributions are as follows: (a) we introduce the novel task of predicting performant parameters for diverse feedforward neural networks with a single hypernetwork forward pass; (b) we introduce DEEPNETS-1M – a standardized benchmark with in-distribution and out-of-distribution architectures to track progress on the task (§ 3); (c) we deﬁne several baselines and propose a GHN model (§ 4) that performs surprisingly well on CIFAR-10 and ImageNet (§ 5.1); (d) we show that our model learns a strong representation of neural network architectures (§ 5.2), and our model is useful for initializing neural networks (§ 5.3). Our DEEPNETS-1M dataset, trained
GHNs and code is available at https://github.com/facebookresearch/ppuda.
Example of evaluating on an unseen architecture a /∈ F (ResNet-50) (a) (b)
Figure 1: (a) Overview of our GHN model (§ 4) trained by backpropagation through the predicted parameters ( ˆwp) on a given image dataset and our DEEPNETS-1M dataset of architectures. Colored captions show our key improvements to vanilla GHNs (§ 2.2). The red one is used only during training GHNs, while the blue ones are used both at training and testing time. The computational graph of a1 is visualized as described in Table 1. (b) Comparing classiﬁcation accuracies when all the parameters of a ResNet-50 are predicted by GHNs versus when its parameters are trained with
SGD (see full results in § 5). 2Training a single network ai can take several GPU days and thousands of trained networks may be required. 2
2