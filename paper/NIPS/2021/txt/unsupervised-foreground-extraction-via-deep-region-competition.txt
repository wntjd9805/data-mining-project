Abstract
We present Deep Region Competition (DRC), an algorithm designed to extract foreground objects from images in a fully unsupervised manner. Foreground ex-traction can be viewed as a special case of generic image segmentation that focuses on identifying and disentangling objects from the background. In this work, we rethink the foreground extraction by reconciling energy-based prior with genera-tive image modeling in the form of Mixture of Experts (MoE), where we further introduce the learned pixel re-assignment as the essential inductive bias to cap-ture the regularities of background regions. With this modeling, the foreground-background partition can be naturally found through Expectation-Maximization (EM). We show that the proposed method effectively exploits the interaction be-tween the mixture components during the partitioning process, which closely con-nects to region competition [1], a seminal approach for generic image segmenta-tion. Experiments demonstrate that DRC exhibits more competitive performances on complex real-world data and challenging multi-object scenes compared with prior methods. Moreover, we show empirically that DRC can potentially general-ize to novel foreground objects even from categories unseen during training.1 1

Introduction
Foreground extraction, being a special case of generic image segmentation, aims for a binary par-tition of the given image with specific semantic meaning, i.e., a foreground that typically contains identifiable objects and the possibly less structural remaining regions as the background. There is a rich literature on explicitly modeling and representing a given image as foreground and background (or more general visual regions), such that a generic inference algorithm can produce plausible seg-mentations ideally for any images without or with little supervision [1–8]. However, such methods essentially rely on low-level visual features (e.g., edges, color, and texture), and some further re-quire human intervention at initialization [4, 5], which largely limits their practical performance on modern datasets of complex natural images with rich semantic meanings [9, 10]. These datasets typically come with fine-grained semantic annotations, exploited by supervised methods that learn representation and inference algorithm as one monolithic network [11–16]. Despite the success of densely supervised learning, the unsupervised counterpart is still favored due to its resemblance to how humans perceive the world [17, 18]. 1Code and data available at https://github.com/yuPeiyu98/DRC 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Attempting to combine unsupervised or weakly supervised learning with modern neural networks, three lines of work surge recently for foreground extraction: (1) deep networks as feature extractors for canonical segmentation algorithms, (2) GAN-based foreground-background disentanglement, and (3) compositional latent variable models with slot-based object modeling. Despite great suc-cesses of these methods, the challenge of unsupervised foreground extraction remains largely open.
Specifically, the first line of work trains designated deep feature extractors for canonical segmen-tation algorithms or metric networks as learned partitioning criteria [19–21]. These methods (e.g.,
W-Net [19]) define foreground objects’ properties using learned features or criteria and are thus gen-erally bottle-necked by the selected post-processing segmentation algorithm [22, 23]. As a branch of pioneering work that moves beyond these limitations, Yang et al. [24, 25] have recently proposed a general contextual information separation principle and an efficient adversarial learning method that is generally applicable to unsupervised segmentation, separation and detection. GAN-based models [26–31] capture the foreground objectness with oversimplified assumptions or require addi-tional supervision to achieve foreground-background disentanglement. For example, the segmenta-tion model in ReDO [28] is trained by redrawing detected objects, which potentially limits its ap-plication to datasets with diverse object shapes. OneGAN [31] and its predecessors [29, 30], though producing impressive results on foreground extraction, require a set of background images with-out foreground objects as additional inputs. Lastly, compositional latent variable models [32–40] include the background as a “virtual object” and induce the independence of object representations using an identical generator for all object slots. Although these methods exhibit strong performance on synthetic multi-object datasets with simple backgrounds and foreground shapes, they may fail on complex real-world data or even synthetic datasets with more challenging backgrounds [37, 38].
In addition, few unsupervised learning methods have provided explicit identification of foreground objects and background regions. While they can generate valid segmentation masks, most of these methods do not specify which output corresponds to the foreground objects. These deficiencies ne-cessitate rethinking the problem of unsupervised foreground extraction. We propose to confront the challenges in formulating (1) a generic inductive bias for modeling foreground and background re-gions that can be baked into neural generators, and (2) an effective inference algorithm based on a principled criterion for foreground-background partition.
Inspired by Region Competition [1], a seminal approach that combines optimization-based infer-ence [41–43] and probabilistic visual modeling [44, 45] by minimizing a generalized Bayes cri-terion [46], we propose to solve the foreground extraction problem by reconciling energy-based prior [47] with generative image modeling in the form of Mixture of Experts (MoE) [48, 49]. To generically describe background regions, we further introduce the learned pixel re-assignment as the essential inductive bias to capture their regularities. Fueled by our modeling, we propose to find the foreground-background partition through Expectation-Maximization (EM). Our algorithm ef-fectively exploits the interaction between the mixture components during the partitioning process, echoing the intuition described in Region Competition [1]. We therefore coin our method Deep
Region Competition (DRC). We summarize our contributions as follows: 1. We provide probabilistic foreground-background modeling by reconciling energy-based prior with generative image modeling in the form of MoE. With this modeling, the foreground-background partition can be naturally produced through EM. We further introduce an inductive bias, pixel re-assignment, to facilitate foreground-background disentanglement. 2. In experiments, we demonstrate that DRC exhibits more competitive performances on complex real-world data and challenging multi-object scenes compared with prior methods. Furthermore, we empirically show that using learned pixel re-assignment as the inductive bias helps to provide explicit identification for foreground and background regions. 3. We find that DRC can potentially generalize to novel foreground objects even from categories unseen during training, which may provide some inspiration for the study of out-of-distribution (OOD) generalization in more general unsupervised disentanglement. 2