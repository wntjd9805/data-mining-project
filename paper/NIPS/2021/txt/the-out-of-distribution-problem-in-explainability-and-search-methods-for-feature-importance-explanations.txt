Abstract
Feature importance (FI) estimates are a popular form of explanation, and they are commonly created and evaluated by computing the change in model conﬁdence caused by removing certain input features at test time. For example, in the standard
Sufﬁciency metric, only the top-k most important tokens are kept. In this paper, we study several under-explored dimensions of FI explanations, providing conceptual and empirical improvements for this form of explanation. First, we advance a new argument for why it can be problematic to remove features from an input when creating or evaluating explanations: the fact that these counterfactual inputs are out-of-distribution (OOD) to models implies that the resulting explanations are socially misaligned. The crux of the problem is that the model prior and random weight initialization inﬂuence the explanations (and explanation metrics) in unintended ways. To resolve this issue, we propose a simple alteration to the model training process, which results in more socially aligned explanations and metrics. Second, we compare among ﬁve approaches for removing features from model inputs. We ﬁnd that some methods produce more OOD counterfactuals than others, and we make recommendations for selecting a feature-replacement function.
Finally, we introduce four search-based methods for identifying FI explanations and compare them to strong baselines, including LIME, Anchors, and Integrated
Gradients. Through experiments with six diverse text classiﬁcation datasets, we
ﬁnd that the only method that consistently outperforms random search is a Parallel
Local Search (PLS) that we introduce. Improvements over the second best method are as large as 5.4 points for Sufﬁciency and 17 points for Comprehensiveness.1 1

Introduction
Estimating feature importance (FI) is a common approach to explaining how learned models make predictions for individual data points [51, 46, 34, 57, 36, 11]. FI methods assign a scalar to each feature of an input representing its “importance” to the model’s output, where a feature may be an individual component of an input (such as a pixel or a word) or some combination of components.
Alongside these methods, many approaches have been proposed for evaluating FI estimates (also known as attributions) [41, 1, 13, 22, 20, 67]. Many of these approaches use test-time input ablations, where features marked as important are removed from the input, with the expectation that the model’s conﬁdence in its original prediction will decline if the selected features were truly important. 1All supporting code for experiments in this paper is publicly available at https://github.com/ peterbhase/ExplanationSearch. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
For instance, according to the Sufﬁciency metric [13], the best FI explanation is the set of features which, if kept, would result in the highest model conﬁdence in its original prediction. Typically the top-k features would be selected according to their FI estimates, for some speciﬁed sparsity level k.
Hence, the ﬁnal explanation e is a k-sparse binary vector in {0, 1}d, where d is the dimensionality of the chosen feature space. For an explanation e and a model f that outputs a distribution over classes p(y|x) = f (x), Sufﬁciency can be given as:
Suff(f, x, e) = f (x)ˆy − f (Replace(x, e))ˆy where ˆy = arg maxy f (x)y is the predicted class for x and the Replace function replaces features in x with some uninformative feature at locations corresponding to 0s in the explanation e.
The Replace function plays a key role in the deﬁnition of such metrics because it deﬁnes the counterfactual input that we are comparing the original input with. Though FI explanations are often presented without mention of counterfactuals, all explanations make use of counterfactual situations
[38], and FI explanations are no exception. The only way we can understand what makes some features “important” to a particular model prediction is by reference to a counterfactual input which has its important features replaced with a user-speciﬁed (uninformative) feature.
In this paper, we study several under-explored dimensions of the problem of ﬁnding good explanations according to test-time ablation metrics including Sufﬁciency and a related metric, Comprehensiveness, with a focus on natural language processing tasks. We describe three primary contributions below.
First, we argue that standard FI explanations are heavily inﬂuenced by the out-of-distribution (OOD) nature of counterfactual model inputs, which results in socially misaligned explanations. We use this term, ﬁrst introduced by Jacovi and Goldberg [24], to describe a situation where an explanation communicates a different kind of information than the kind that people expect it to communicate.
Here, we do not expect the model prior or random weight initialization to inﬂuence FI estimates. This is a problem insofar as FI explanations are not telling us what we think they are telling us. We propose a training algorithm to resolve the social misalignment, which is to expose models to counterfactual inputs during training, so that counterfactuals are not out-of-distribution at test time.
Second, we systematically compare Replace functions, since this function plays an important role in evaluating explanations. To do so, we remove tokens from inputs using several Replace functions, then measure how OOD these ablated inputs are to the model. We compare methods that remove tokens entirely from sequences of text [41, 13], replace token embeddings with the zero embedding or a special token [34, 57, 1, 65, 57], marginalize predictions over possible counterfactuals [68, 30, 63], and edit the input attention mask rather than the input text. Following our argument regarding the
OOD problem (Sec. 4), we recommend the use of some Replace functions over others.
Third, we provide several novel search-based methods for identifying FI explanations. While ﬁnding the optimal solution to arg maxe Suff(f, x, e) is a natural example of binary optimization, a problem for which search algorithms are a common solution [43, 49, 3], we are aware of only a few prior works that search for good explanations [19, 47, 15]. We introduce our novel search algorithms for
ﬁnding good explanations by making use of general search principles [43]. Based on experiments with two Transformer models and six text classiﬁcation datasets (including FEVER, SNLI, and others), we summarize our core ﬁndings as follows: 1. We propose to train models on explanation counterfactuals and ﬁnd that this leads to greater model robustness against counterfactuals and yields drastic differences in explanation metrics. 2. We ﬁnd that some Replace functions are better than others at reducing counterfactual OOD-ness, although ultimately our solution to the OOD problem is much more effective. 3. We introduce four novel search-based methods for identifying explanations. Out of all the methods we consider (including popular existing methods), the only one that consistently outperforms random search is the Parallel Local Search (PLS) that we introduce, often by large margins of up to 20.8 points. Importantly, we control for the compute budget used by each method. 2