Abstract
While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to
ﬁnd areas that are predicted by the semantic representation of the stimulus words.
However, most studies of syntax have focused only on identifying areas correlated with syntactic processing load. One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal the correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax.
First, we ﬁnd that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our code and data will be available at https://github.com/anikethjr/brain_syntactic_representations. 1

Introduction
Neuroscientists have long been interested in how the brain processes syntax. To date, there is no consensus on which brain regions are involved in processing it. Classically, only a small number of regions in the left hemisphere were thought to be involved in language processing. More recently, the language system was proposed to involve a set of brain regions spanning the left and right hemisphere
[1]. Similarly, some ﬁndings show that syntax is constrained to speciﬁc brain regions [2, 3], while other ﬁndings show syntax is distributed throughout the language system [4–6].
The biological basis of syntax was ﬁrst explored through studies of the impact of brain lesions on language comprehension or production [7] and later through non-invasive neuroimaging experiments that record brain activity while subjects perform language tasks, using methods such as functional
Magnetic Resonance Imaging (fMRI) or electroencephalography (EEG). These experiments usually isolate syntactic processing by contrasting the activity between a difﬁcult syntactic condition and an easier one and by identifying regions that increase in activity with syntactic effort [3]. An example of these conditions is reading a sentence with an object-relative clause (e.g. “The rat that the cat chased was tired"), which is more taxing than reading a sentence with a subject-relative clause (e.g. “The cat that chased the rat was tired"). In the past decade, this approach was extended to study syntactic processing in naturalistic settings such as when reading or listening to a story [8–11].
Because such complex material is not organized into conditions, neuroscientists have instead devised 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
complexity metrics capturing the word-by-word evolving syntactic demands required to understand the material. Using these metrics, neuroscientists were able to identify regions with activity correlated with syntactic processing load, and suggest the involvement of these regions in syntactic processing.
While many have studied syntactic processing as captured through complexity measures, very few have studied the syntactic representations themselves. As an analogy, consider the neurobiology of language semantics. A large part of the literature has focused on characterizing the semantic processing load related to integrating or predicting incoming words (see [12] for a review). Concur-rently, another part has focused on studying where the meaning of words itself is represented, either through contrast-based studies (see [13] for a review) or through encoding model approaches (e.g.
[14–19]). It can be argued however, that in syntax, most research focuses on the ﬁrst approach (using complexity) and not the second (using representations).
There are two main reasons why the study of syntactic representations using fMRI is difﬁcult: 1. The ﬁrst reason is computational. To identify the brain correlates of syntactic representations, one has to embed the syntactic representation (often a tree) into a vector that can be then used to predict the time series of brain activity as a function of syntactic structure. This vector representation should change as the words are processed incrementally. The construction of such a vector space is akin to the problem of building a graph embedding, and it is not trivial. The goal is to have different sentences or segments of sentences with similar structure (irrespective of their meanings) map to nearby points in the vector space. 2. The second reason is that the fMRI signal is noisy, and it is not clear that the neural basis of the representation of syntactic structure lends itself to being studied using fMRI. For instance, it could be that the neural substrate for syntax is intermingled with that of other language components like semantics and is hard to disentangle, or it could be that neurons inside the same voxel perform different syntactic computations, making it hard to differentiate the signal corresponding to these different computations. More generally, it could also be that syntactic computations are organized in a way that the low Signal-to-Noise Ratio (SNR) of fMRI makes it difﬁcult to study them.
To help address the ﬁrst difﬁculty, we propose syntactic structure embeddings that encode the syntactic information inherent in natural text that subjects read in the scanner. These structure embeddings are proposed as an additional tool in the arsenal of neurolinguists and can serve to ask the question of whether the fMRI signal can reveal syntactic representations. We use our syntactic structure embeddings in a voxelwise encoding model framework [14, 16, 17, 20]. We ﬁnd that our embeddings – along with other simpler syntactic structure embeddings built using conventional syntactic features such as part-of-speech (POS) tags and dependency role (DEP) tags – are able to explain an additional portion of the variance in the fMRI data of subjects reading text, even after controlling for complexity metrics that capture processing load, suggesting that fMRI can indeed reveal syntactic representations.
Furthermore, the regions that are well-predicted by syntactic representations are distributed across the language network.
We also attempt to minimize the amount of semantic information present in our embeddings. After showing that they do not encode a signiﬁcant amount of semantics, we use our syntactic embeddings to address the aforementioned issue of whether regions that are predicted by syntactic features are selective for syntax, meaning they are only responsive to syntax and not to other language properties such as semantics. To answer this question, we use a contextual word embedding space [21] that integrates semantics and syntax. Consistent with prior literature, regions that are predicted by syntax are much better predicted by the contextual embeddings and do not appear to be selective for syntax. 2