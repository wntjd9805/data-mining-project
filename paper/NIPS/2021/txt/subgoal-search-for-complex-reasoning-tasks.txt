Abstract
Humans excel in solving complex reasoning tasks through a mental process of moving from one idea to a related one. Inspired by this, we propose Subgoal Search (kSubS) method. Its key component is a learned subgoal generator that produces a diversity of subgoals that are both achievable and closer to the solution. Using subgoals reduces the search space and induces a high-level search graph suitable for efﬁcient planning. In this paper, we implement kSubS using a transformer-based subgoal module coupled with the classical best-ﬁrst search framework. We show that a simple approach of generating k-th step ahead subgoals is surprisingly efﬁcient on three challenging domains: two popular puzzle games, Sokoban and the
Rubik’s Cube, and an inequality proving benchmark INT. kSubS achieves strong results including state-of-the-art on INT within a modest computational budget. 1

Introduction
Reasoning is often regarded as a deﬁning property of advanced intelligence [40, 18]. When confronted with a complicated task, humans’ thinking process often moves from one idea to a related idea, and the progress is made through milestones, or subgoals, rather than through atomic actions that are necessary to transition between subgoals [15]. During this process, thinking about one subgoal can lead to a possibly diverse set of subsequent subgoals that are conceptually reachable and make a promising step towards the problem’s solution. This intuitive introspection is backed by neuroscience evidence [20], and in this work, we present an algorithm that mimics this process. Our approach couples a deep learning generative subgoal modeling with classical search algorithms to allow for successful planning with subgoals. We showcase the efﬁciency of our method on the following complex reasoning tasks: two popular puzzle games Sokoban and the Rubik’s Cube, and an inequality theorem proving benchmark INT [55], achieving the state-of-the-art results in INT and competitive results for the remaining two.
The deep learning revolution has brought spectacular advancements in pattern recognition techniques and models. Given the hard nature of reasoning problems, these are natural candidates to provide
∗equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
search heuristics [4]. Indeed, such a blend can produce impressive results [44, 45, 37, 1]. These approaches seek solutions using elementary actions. Others, e.g. [30, 34, 24], utilize variational subgoals generators to deal with long-horizon visual tasks. We show that these ideas can be pushed further to provide algorithms capable of dealing with combinatorial complexity.
We present Subgoal Search (kSubS) method and give its practical implementations: MCTS-kSubS and BF-kSubS. kSubS consists of the following four components: planner, subgoal generator, a low-level policy, and a value function. The planner is used to search over the graph induced by the subgoal generator and is guided by the value function. The role of the low-level policy is to prune the search tree as well as to transition between subgoals. In this paper, we assume that the generator predicts subgoals that are k step ahead (towards the solution) from the current state, and to emphasize this we henceforth add k to the method’s abbreviation. MCTS-kSubS and BF-kSubS differ in the choice of the search engine: the former uses Monte-Carlo Tree Search (MCTS), while the latter is backed by Best-First Search (BestFS). We provide two sets of implementations for the generator, the low-level policy, and the value functions. The ﬁrst one uses transformer architecture [49] for each component, while the second utilizes a convolutional network for the generator and the value function, and the classical breadth-ﬁrst search for the low-level policy. This lets us showcase the versatility and effectiveness of the approach.
The subgoal generator lies at the very heart of Subgoal Search, being an implementation of reasoning with high-level ideas. To be useful in a broad spectrum of contexts, the generator should be imple-mented as a learnable (generative) model. As a result, it is expected to be imperfect and (sometimes) generate incorrect predictions, which may turn the search procedure invalid. Can we thus make planning with learned subgoals work? In this paper, we answer this question afﬁrmatively: we show that the autoregressive framework of transformer-based neural network architecture [49] leads to superior results in challenging domains.
We train the transformer with the objective to predict the k-th step ahead. The main advantages of this subgoal objective are simplicity and empirical efﬁciency. We used expert data to generate labels for supervised training. When ofﬂine datasets are available, which is the case for the environments considered in this paper2, such an approach allows for stable and efﬁcient optimization with high-quality gradients. Consequently, this method is often taken when dealing with complex domains (see e.g. [42, 52]) or when only an ofﬂine expert is available3. Furthermore, we found evidence of out-of-distribution generalization.
Finally, we formulate the following hypothesis aiming to shed some light on why kSubS is successful: we speculate that subgoal generation may alleviate errors in the value function estimation. Planning methods based on learning, including kSubS, typically use imperfect value function-based information to guide the search. While traditional low-level search methods are susceptible to local noise, subgoal generation allows for evaluations of the value functions at temporally distant subgoals, which improves the signal-to-noise ratio and allows a “leap over” the noise.
To sum up, our contributions are: 1. We propose Subgoal Search method with two implementations: MCTS-kSubS, BF-kSubS.
We demonstrate that our approach requires a relatively little search or, equivalently, is able to handle bigger problems. We also observe evidence of out-of-distribution generalization. 2. We provide evidence that a transformer-based autoregressive model learned with a simple supervised objective to predict states k-th step ahead is an effective tool to generate valid and diverse subgoals. 3. We show in our experiments that using subgoal planning help to might mitigate the negative inﬂuence of value function errors on planning.
We provide the code of our method and experiment settings at https://github.com/ subgoal-search/subgoal-search, and a dedicated website https://sites.google.com/ view/subgoal-search. 2The dataset for INT or Sokoban can be easily generated or are publicly available. For the Rubik’s Cube, we use random data or simple heuristic (random data are often sufﬁcient for robotic tasks and navigation.) 3For example, the INT engine can easily generate multiple proves of random statements, but cannot prove a given theorem. 2
2