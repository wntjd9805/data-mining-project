Abstract
We study the problem of Safe Policy Improvement (SPI) under constraints in the offline Reinforcement Learning (RL) setting. We consider the scenario where: (i) we have a dataset collected under a known baseline policy, (ii) multiple reward signals are received from the environment inducing as many objectives to optimize.
We present an SPI formulation for this RL setting that takes into account the prefer-ences of the algorithm’s user for handling the trade-offs for different reward signals while ensuring that the new policy performs at least as well as the baseline policy along each individual objective. We build on traditional SPI algorithms and propose a novel method based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB,
Laroche et al., 2019) that provides high probability guarantees on the performance of the agent in the true environment. We show the effectiveness of our method on a synthetic grid-world safety task as well as in a real-world critical care context to learn a policy for the administration of IV fluids and vasopressors to treat sepsis. 1

Introduction
Reinforcement Learning (RL) as a paradigm for sequential decision-making (Sutton, 1988) has shown tremendous success in a variety of simulated domains (Mnih et al., 2015; Silver et al., 2017; OpenAI, 2018). However, there are still quite a few challenges between the traditional RL research and real-world tasks. Most of these challenges stem from assumptions that are rarely satisfied in practice (Dulac-Arnold et al., 2019), or the inability of the algorithm’s user to specify the desired behavior of the agent without being a domain expert (Thomas et al., 2019). We focus on the real-world application point of view and posit the following requirements:
• Multiple reward functions: Traditional RL methods assume a single scalar reward is present in the environment. However, most real-world tasks, have multiple (possibly conflicting) objectives or constraints that need to be taken into consideration together, such as the signals related to the safety (physical well-being of the agent or the environment), budget utilization (energy or maintenance costs), etc.
• Stakeholder control of the trade-off: The ML practitioners should have the ability to control the different trade-offs the agent is making and choose the one they consider best for the task at hand.
• Offline setting: In many real-world domains (e.g., healthcare, finance or autonomous vehicles), there is an abundance of data, collected under a sub-optimal policy, but training the agent directly via interactions with the environment is expensive and risky. We assume that we only have access to a dataset of past trajectories that can be used for training (Lange et al., 2012). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
• Preventing unintended behavior: We want the agent to be robust to both extrapolation errors from offline RL and misaligned objectives that are poor proxy of the user’s intentions and algorithm’s actual performance (Ng et al., 1999; Amodei et al., 2016). We consider the case where the user can specify undesirable behavior in the context of the performance observed in the batch.
• Practical guarantees: We want guarantees about the undesirable behavior that might be caused by the agent in the real-world. We care about the results that can be obtained using the finite amount of samples we have in the batch, and aim to provide some measure of confidence in deploying the agents in the environment.
To achieve this set of properties, we adopt the Seldonian framework (Thomas et al., 2019), which is a general algorithm design framework that allows high-confidence guarantees for constraint satisfaction in a multi-objective setting. Based on the above specifications, we seek to answer the question: if we are given a batch of data collected under some (suboptimal) behavioral policy and some user preference, can we build a policy improvement algorithm that returns a policy with practical high-confidence guarantees on the performance of the policy w.r.t. the behavioral policy?
We acknowledge that there are other important challenges in RL, such as partial observability, safe exploration, non-stationary environments and function approximation in high-dimensional spaces, that also stand in the way of making RL a more applicable paradigm. These challenges are beyond the scope of this work, which should rather be thought of as taking a step towards this broader goal.
In Section 2, we present our contribution positioned with respect to other related work. In Section 3, we formalize the setting and then extend traditional SPI algorithms to this setting. We then show it is possible to extend the previous work on Safe Policy Iteration (SPI), particularly Safe Policy
Iteration with Baseline Bootstrapping (SPIBB, Laroche et al., 2019), for the design of agents that satisfy the above requirements. We show that the resulting algorithm is theoretically-grounded and provides practical high-probability guarantees. We extensively test our approach on a synthetic safety-gridworld task in Section 4 and show that the proposed algorithm achieves better data efficiency than the existing approaches. Finally, we show its benefits on a critical-care task in Section 5. The accompanying codebase is available at https://github.com/hercky/mo-spibb-codebase. 2