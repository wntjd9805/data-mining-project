Abstract
Consider the following instance of the Ofﬂine Meta Reinforcement Learning (OMRL) problem: given the complete training logs of N conventional RL agents, trained on N different tasks, design a meta-agent that can quickly maximize reward in a new, unseen task from the same task distribution. In particular, while each conventional RL agent explored and exploited its own different task, the meta-agent must identify regularities in the data that lead to effective exploration/exploitation in the unseen task. Here, we take a Bayesian RL (BRL) view, and seek to learn a Bayes-optimal policy from the ofﬂine data. Building on the recent VariBAD
BRL approach, we develop an off-policy BRL method that learns to plan an exploration strategy based on an adaptive neural belief estimate. However, learning to infer such a belief from ofﬂine data brings a new identiﬁability issue we term
MDP ambiguity. We characterize the problem, and suggest resolutions via data collection and modiﬁcation procedures. Finally, we evaluate our framework on a diverse set of domains, including difﬁcult sparse reward tasks, and demonstrate learning of effective exploration behavior that is qualitatively different from the exploration used by any RL agent in the data. Our code is available online at https://github.com/Rondorf/BOReL. 1

Introduction
A central question in reinforcement learning (RL) is how to learn quickly (i.e., with few samples) in a new environment. Meta-RL addresses this issue by training an agent on a large set of training environments [5, 9].
Intuitively, the meta-RL agent can learn regularities in the environments, which allow quick learning in any environment that shares a similar structure. Indeed, recent work demonstrated this by training memory-based controllers that ‘identify’ the domain [5, 28, 18], or by learning a parameter initialization that leads to good performance within a few gradient steps [9].
Another formulation of quick RL is Bayesian RL [BRL, 11]. In BRL, the environment parameters are treated as unobserved variables, with a known prior distribution. Consequentially, the standard problem of maximizing expected returns (taken with respect to the posterior distribution) explicitly accounts for the environment uncertainty, and its solution is a Bayes-optimal policy, wherein actions optimally balance exploration and exploitation. Recently, Zintgraf et al. [36] showed that meta-RL is in fact an instance of BRL, where the meta-RL environment distribution is simply the BRL prior.
Furthermore, a Bayes-optimal policy can be trained using standard RL methods, by adding to the state the belief over the environment parameters. The VariBAD algorithm [36] implements this idea using a variational autoencoder (VAE) for belief estimation and deep neural network policies. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Ofﬂine meta-RL on the Semi-Circle domain: the task is to navigate to a goal position that can be anywhere on the semi-circle. The reward is sparse (light-blue), and the ofﬂine data (left) contains training logs of conventional RL agents trained to ﬁnd individual goals. The meta-RL agent (right) needs to ﬁnd a policy that quickly ﬁnds the unknown goal, here, by searching across the semi-circle in the ﬁrst episode, and directly reaching it the second – a completely different strategy from the dominant behaviors in the data.
Most meta-RL studies, including VariBAD, have focused on the online setting, where, during training, the meta-RL policy is continually updated using data collected from running it in the training environments. In domains where data collection is expensive, such as robotics and healthcare to name a few, online training is a limiting factor. For standard RL, ofﬂine (a.k.a. batch) RL mitigates this problem by learning from data collected beforehand by an arbitrary policy [7, 21]. In this work we investigate the ofﬂine approach to meta-RL (OMRL).
Any ofﬂine RL approach is heavily inﬂuenced by the data collection policy. To ground our inves-tigation, we focus on the following practical setting:1 we assume that data is collected by running standard RL agents on a set of environments from the environment distribution. While the data was not speciﬁcally collected for the meta-RL task, we hypothesize that regularities between the training domains can still be learned, to provide faster learning in new environments. Figure 1 illustrates our problem: in this navigation task, each RL agent in the data learned to ﬁnd its own goal, and converged to a behavior that quickly navigates toward it. The meta-RL agent, on the other hand, needs to learn a completely different behavior that effectively searches for the unknown goal position.
Our method for solving OMRL is an off-policy variant of the VariBAD algorithm, based on replacing the on-policy policy gradient optimization in VariBAD with an off-policy Q-learning based approach.
This, however, requires some care, as Q-learning applies to states of fully observed systems. We show that the VariBAD approach of augmenting states with the belief in the data applies to the off-policy setting as well, leading to an effective and practical algorithm. The ofﬂine setting, however, brings about another challenge – when the agent visits different parts of the state space in different environments, learning to identify the correct environment and obtain an accurate belief estimate becomes challenging, a problem we term MDP ambiguity. We formalize this problem, and discuss how it manifests in common scenarios such as sparse rewards or sparse differences in transitions.
Based on our formalization, we propose a general data collection strategy that can mitigate the problem. Furthermore, when ambiguity is only due to reward differences, we show that a simple reward relabelling trick sufﬁces, without changing data collection. We collectively term our data collection/relabelling and off-policy algorithm as Bayesian Ofﬂine Reinforcement Learning (BOReL).
In our experiments, we show that BOReL learns effective exploration policies from ofﬂine data on both discrete and continuous control problems. We demonstrate signiﬁcantly better exploration than meta-RL methods based on Thompson sampling such as PEARL [28], even when these methods are allowed to train online. Furthermore, we explore the issue of MDP ambiguity in practice, and demonstrate that, when applicable, our proposed solutions successfully mitigate it.
An important implication of our study is that without suitably collected data, MDP ambiguity can make learning an effective ofﬂine BRL solution impossible. This stands in contrast to recent studies in ofﬂine (non-Bayesian) RL [30, 10, 19], where the focus is on designing learning algorithms for arbitrarily collected data, typically by avoiding actions that were not explored enough. Outside ofﬂine
RL, however, it is common to develop data collection methods based on formal limitations of the problem. For example, the use of randomized controlled trials in medical treatments is well motivated by the theory of causal inference [27]. The data collection methods we propose here are simple and practical to implement, and as we demonstrate, effectively handle MDP ambiguity. 1The theory and algorithms we develop, however, are not limited to any particular data collection protocol. 2
Our main contributions are therefore as follows: to our knowledge, this is the ﬁrst study of meta learning exploration in the ofﬂine setting; we provide the necessary theory to extend VariBAD to off-policy RL; we formulate MDP ambiguity, which characterizes which problems are solvable under the ofﬂine BRL setting, and based on this formulation we propose several principled data collection strategies; we show non-trivial empirical results that demonstrate signiﬁcantly better exploration than meta-RL methods based on Thompson sampling; ﬁnally, and of independent interest, our off-policy algorithm signiﬁcantly improves the sample efﬁciency of conventional VariBAD in the online setting. 2