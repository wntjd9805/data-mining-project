Abstract
In this paper, we explore how we can build upon the data and models of Internet images and use them to adapt to robot vision without requiring any extra labels. We present a framework called Self-supervised Embodied Active Learning (SEAL). It utilizes perception models trained on internet images to learn an active exploration policy. The observations gathered by this exploration policy are labelled using 3D consistency and used to improve the perception model. We build and utilize 3D semantic maps to learn both action and perception in a completely self-supervised manner. The semantic map is used to compute an intrinsic motivation reward for training the exploration policy and for labelling the agent observations using spatio-temporal 3D consistency and label propagation. We demonstrate that the
SEAL framework can be used to close the action-perception loop: it improves object detection and instance segmentation performance of a pretrained perception model by just moving around in training environments and the improved perception model can be used to improve Object Goal Navigation.

Introduction 1
Even though computer vision started out as a ﬁeld to aid embodied agents (robots) [21], in current times it has evolved into Internet computer vision, where the focus is on training models for and from Internet data. Fueled by the successes of supervised learning, today’s models can successfully classify, detect and segment out objects in Internet images reasonably well [19]. This raises a natural question: would we need to restart from scratch and gather similarly large-scale labeled datasets to get computer vision to work for embodied agents, or can we somehow bootstrap off the progress made in Internet computer vision?
Internet data comprises of sparse and unrelated snapshots of the world, carefully chosen by humans.
On the one hand, this simpliﬁes the problem as models only need to reason about a small and well-chosen subset of possible views of the world. On the other, this makes learning hard. The dog in image_0032 in the ImageNet dataset, is different from the dog in image_0033, and there is no way to understand how the dog in image_0032 will look like from another view. In contrast, an active agent, embodied in a 3D environment, experiences views of a 3D consistent world. Spatio-temporal continuity in views of the underlying 3D world can allow better inference at test time, and label efﬁcient learning at train time. These effects are further ampliﬁed, as the agent can actively choose the views it experiences to maximize learning. Thus, in contrast to Internet computer vision, embodiment opens up the possibility of deriving supervision from spatio-temporal continuity and interaction.
Such self-supervision for physical tasks (e.g. collisions, graspability) can be done through the use of appropriate sensors [17, 36]. How to do so for semantic tasks, such as segmenting and detecting objects is less clear, and is the focus of our work in this paper. We build off models from Internet computer vision and show how their performance can be improved through self-supervised interaction.
⇤Correspondence: dchaplot@fb.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Self-supervised Embodied Active Learning. Our framework called Self-supervised Embodied
Active Learning (SEAL) consists of two phases, Action, where we learn an active exploration policy, and
Perception, where we train the Perception Model on data gathered using the exploration policy and labels obtained using spatio-temporal label propagation. Both action and perception are learnt in a completely self-supervised manner without requiring access to the ground-truth semantic annotations or map information.
We show that a) this can be done purely via interaction, without needing any additional supervision, b) improved models exhibit better performance in test environments as is, c) models can be improved further through small amounts of self-supervised interaction in the test environment. We also show that improved perception can, in turn, improve the performance at interactive tasks.
We propose a framework called Self-supervised Embodied Active Learning (SEAL) as shown in
Figure 1. It consists of two phases, one for learning Action, and another for learning Perception.
During the Action phase, the agent learns a self-supervised exploration policy to gather observations of objects with at least one highly conﬁdent viewpoint. We propose an intrinsic motivation reward called Gainful Curiosity to train this policy. In the Perception phase, the learned exploration policy is used to gather a single episode of observations in an environment. We propose a method called 3DLabelProp to obtain labels for these observations in a self-supervised fashion. Both the action and perception phases involve constructing a 3D Semantic Map. The agent observations in an episode are used to construct an episodic 3D semantic map. The map is used to compute the Gainful Curiosity reward during the Action phase. And during the perception phase, the semantic labels in the 3D map are projected on the agent’s original observations using 3DLabelProp to generate the supervision for training the embodied perception model.
Our experiments demonstrate that the SEAL framework can be used to close the action-perception loop. Perceptual models allow the agent to act in the world and collect data that improve the perception models. Improved perception models can, in turn, improve the agent’s policy for interacting with the world. We ﬁrst use SEAL framework to improve object detection and instance segmentation performance of a pretrained perception model (Mask RCNN [19]) from 34.82/32.54 AP50 scores to 40.02/36.23 AP50 scores by just moving around in training environments, without having access to any additional human annotations. By allowing the agent to explore the test environment for a single episode, we can further improve the performance to 41.23/37.28 AP50 scores. Next, we also show that this improved perception model can be used to improve the performance of an embodied agent at
Object Goal Navigation from 54.4% to 62.7% success rate. 2