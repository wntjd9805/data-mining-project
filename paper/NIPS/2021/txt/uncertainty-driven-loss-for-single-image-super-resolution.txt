Abstract
In low-level vision such as single image super-resolution (SISR), traditional MSE or
L1 loss function treats every pixel equally with the assumption that the importance of all pixels is the same. However, it has been long recognized that texture and edge areas carry more important visual information than smooth areas in photographic images. How to achieve such spatial adaptation in a principled manner has been an open problem in both traditional model-based and modern learning-based approaches toward SISR. In this paper, we propose a new adaptive weighted loss for SISR to train deep networks focusing on challenging situations such as textured and edge pixels with high uncertainty. Speciﬁcally, we introduce variance estimation characterizing the uncertainty on a pixel-by-pixel basis into
SISR solutions so the targeted pixels in a high-resolution image (mean) and their corresponding uncertainty (variance) can be learned simultaneously. Moreover, uncertainty estimation allows us to leverage conventional wisdom such as sparsity prior for regularizing SISR solutions. Ultimately, pixels with large certainty (e.g., texture and edge pixels) will be prioritized for SISR according to their importance to visual quality. For the ﬁrst time, we demonstrate that such uncertainty-driven loss can achieve better results than MSE or L1 loss for a wide range of network architectures. Experimental results on three popular SISR networks show that our proposed uncertainty-driven loss has achieved better PSNR performance than traditional loss functions without any increased computation during testing. The code is available at https://see.xidian.edu.cn/faculty/wsdong/Projects/UDL-SR.htm 1

Introduction
Single image super-resolution (SISR) aims at reconstructing high-resolution (HR) images from their corresponding degraded low-resolution (LR) images. Since the publication of super-resolution with convolutional neural network (SRCNN) [1], there has been a ﬂurry of works on deep learning-based approaches toward SISR - e.g., EDSR [2], DPDNN [3], RCAN [4], SAN [5], and MoG-DUN [6].
The unifying theme along this line of research appears to be that deeper, bigger, and more complex networks can achieve improved SISR performance by facilitating the reconstruction of high-frequency details such as textures and edges in photographic images. Such improvement has been achieved by novel network architectures (e.g., skip connections [2]), new attention mechanism (e.g., residue channel attention [4]), and closed-loop supervision [7]. Surprisingly, most of these existing methods have adopted MSE or L1 loss to optimize the parameters of networks.
The commonly used practice, such as MSE or L1 loss, treats every pixel equally regardless of whether the pixel is in texture/edge regions or smooth areas. The optimality of such non-adaptive loss function
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Img_001 from Set14 (b) HR image x (c) SR image f (y) (d) Difference map |x − f (y)|
Figure 1: Illustration of the difference (d) between HR image (b) and SR image (c) reconstructed by
EDSR network [2] on dataset Set14 [8]. The image reconstructed by EDSR network is shown in (c) and (d) shows the absolute difference between the HR image and SR image. Best viewed in color. has been questioned in the literature of SISR calling for the proposition of perceptual loss function (e.g., [9]). From a Bayesian perspective, the assumption underlying the MSE or L1 loss is that each pixel obeys the independent and identically distribution with the same variance. Taking L1 loss as an example, the likelihood of all pixels in an image can be formulated as p(x | y, W ) =
M (cid:89) l=1 c exp(−
||x(l) − f (W )(y(l))||1
σ
), (1) where x and y denote the pair of HR and LR image, f (W )(·) denotes an arbitrary SISR network parameterized by W , and c, σ denote spatially invariant constants. However, such assumption of stationarity or spatial invariance of image prior model is invalid for photographic images in the real world. For instance, if one compares the ground-truth (HR image) and the SR image reconstructed by EDSR [2] as shown in Fig. 1 (c), it can be observed that texture areas (e.g., hair of baboon) are not restored as good as smooth areas (e.g., nose of baboon). Fig. 1 (d) depicts the absolute difference between the HR image and reconstructed SR image, from which we can observe spatial variation of the difference map. Such observation implies that the uncertainty of texture and edge areas as characterized by the variance is much larger than that in smooth areas. How to address such uncertainty-driven loss for SISR sets up the stage for this paper.
In this paper, we propose a new adaptive weighted loss (uncertainty-driven loss) for SISR by assigning texture and edge areas with higher weights during the training process. Unlike previous work of perceptual loss [9] focusing on characterizing content and style consistency, we target at explicitly estimating the variance ﬁeld underlying the unknown HR image in the ﬁrst step, which can be exploited as an auxiliary signal for guiding the SISR solution in the second step. A direct consequence of our two-step learning approach is that it delivers not only higher visual quality but also improved objective performance such as PSNR and SSIM. Moreover, uncertainty estimation perspective allows us to easily incorporate existing models such as Jefferey’s prior [10, 11] into the proposed SISR solution. It follows that the network training boils down to two sequential steps in which the variance map is estimated from the ﬁrst step and serves as the attention signal for the second step. The main technical contributions are summarized as follows.
• Uncertainty modeling and estimation. We propose to cast SISR into a Bayesian estimation framework under which SR image (mean) and uncertainty (variance) are derived simulta-neously. Unlike previous works in which pixels with large uncertainty are attenuated for high-level vision tasks, we advocate to prioritize them for low-level vision tasks such as
SISR.
• Uncertainty-driven loss (UDL). The estimation of variance map facilitates the training of SISR network by dividing it into two steps. In the ﬁrst step, an estimating sparsity uncertainty (ESU) loss function was derived from the classical Jeffrey’s prior to estimate the variance map. In the second step, the estimated variance map serves as the guidance signal leading to adaptive weighted loss named uncertainty-driven loss LUDL.
• Universality of UDL. The proposed uncertainty loss can easily be employed in any existing
SISR network to improve performance and do not increase any additional computation cost during testing.
• Experimental results on three different baseline networks show that our proposed uncertainty-driven loss has achieved better PSNR performance than traditional MSE or L1 loss. 2
2