Abstract
Policy optimization is a widely-used method in reinforcement learning. Due to its local-search nature, however, theoretical guarantees on global optimality often rely on extra assumptions on the Markov Decision Processes (MDPs) that bypass the challenge of global exploration. To eliminate the need of such assumptions, in this work, we develop a general solution that adds dilated bonuses to the policy update to facilitate global exploration. To showcase the power and generality of this technique, we apply it to several episodic MDP settings with adversarial losses and bandit feedback, improving and generalizing the state-of-the-art. Speciﬁcally, in the tabular case, we obtain (cid:101)O(
T ) regret where T is the number of episodes, improving the (cid:101)O(T 2/3) regret bound by [27]. When the number of states is inﬁnite, under the assumption that the state-action values are linear in some low-dimensional features, we obtain (cid:101)O(T 2/3) regret with the help of a simulator, matching the result of [24] while importantly removing the need of an exploratory policy that their algorithm requires. To our knowledge, this is the ﬁrst algorithm with sublinear regret for linear function approximation with adversarial losses, bandit feedback, and no exploratory assumptions. Finally, we also discuss how to further improve the regret or remove the need of a simulator using dilated bonuses, when an exploratory policy is available.1
√ 1

Introduction
Policy optimization methods are among the most widely-used methods in reinforcement learning.
Its empirical success has been demonstrated in various domains such as computer games [26] and robotics [21]. However, due to its local-search nature, global optimality guarantees of policy optimization often rely on unrealistic assumptions to ensure global exploration (see e.g., [1, 3, 24, 30]), making it theoretically less appealing compared to other methods.
Motivated by this issue, a line of recent works [7, 27, 2, 35] equip policy optimization with global exploration by adding exploration bonuses to the update, and prove favorable guarantees even without making extra exploratory assumptions. Moreover, they all demonstrate some robustness aspect of policy optimization (such as being able to handle adversarial losses or a certain degree of model mis-speciﬁcation). Despite these important progresses, however, many limitations still exist, including worse regret rates comparing to the best value-based or model-based approaches [27, 2, 35], or requiring full-information feedback on the entire loss function (as opposed to the more realistic bandit feedback) [7].
∗Equal contribution. 1In an improved version of this paper, we show that under the linear MDP assumption, an exploratory policy is not even needed. See https://arxiv.org/abs/2107.08346. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To address these issues, in this work, we propose a new type of exploration bonuses called dilated bonuses, which satisﬁes a certain dilated Bellman equation and provably leads to improved exploration compared to existing works (Section 3). We apply this general idea to advance the state-of-the-art of policy optimization for learning ﬁnite-horizon episodic MDPs with adversarial losses and bandit feedback. More speciﬁcally, our main results are:
√
• First, in the tabular setting, addressing the main open question left in [27], we improve their (cid:101)O(T 2/3)
T ) regret. This shows that policy optimization, which performs local regret to the optimal (cid:101)O( optimization, is as capable as other occupancy-measure-based global optimization algorithms [15, 20] in terms of global exploration. Moreover, our algorithm is computationally more efﬁcient than those global methods since they require solving some convex optimization in each episode. (Section 4)
• Second, to further deal with large-scale problems, we consider a linear function approximation setting where the state-action values are linear in some known low-dimensional features and also a simulator is available, the same setting considered by [24]. We obtain the same (cid:101)O(T 2/3) regret while importantly removing the need of an exploratory policy that their algorithm requires.
Unlike the tabular setting (where we improve existing regret rates of policy optimization), note that researchers have not been able to show any sublinear regret for policy optimization without exploratory assumptions for this problem, which shows the critical role of our proposed dilated bonuses. In fact, there are simply no existing algorithms with sublinear regret at all for this setting, be it policy-optimization-type or not. This shows the advantage of policy optimization over other approaches, when combined with our dilated bonuses. (Section 5)
• Finally, while the main focus of our work is to show how dilated bonuses are able to provide global exploration, we also discuss their roles in improving the regret rate to (cid:101)O(
T ) in the linear setting above or removing the need of a simulator for the special case of linear MDPs (with (cid:101)O(T 6/7) regret), when an exploratory policy is available. (Section 6)
√