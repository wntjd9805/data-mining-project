Abstract
Most existing neural architecture search (NAS) algorithms are dedicated to and evaluated by the downstream tasks, e.g., image classiﬁcation in computer vision.
However, extensive experiments have shown that, prominent neural architectures, such as ResNet in computer vision and LSTM in natural language processing, are generally good at extracting patterns from the input data and perform well on different downstream tasks. In this paper, we attempt to answer two fundamental questions related to NAS. (1) Is it necessary to use the performance of speciﬁc downstream tasks to evaluate and search for good neural architectures? (2) Can we perform NAS effectively and efﬁciently while being agnostic to the downstream tasks? To answer these questions, we propose a novel and generic NAS framework, termed Generic NAS (GenNAS). GenNAS does not use task-speciﬁc labels but instead adopts regression on a set of manually designed synthetic signal bases for architecture evaluation. Such a self-supervised regression task can effectively evaluate the intrinsic power of an architecture to capture and transform the input signal patterns, and allow more sufﬁcient usage of training samples. Extensive experiments across 13 CNN search spaces and one NLP space demonstrate the remarkable efﬁciency of GenNAS using regression, in terms of both evaluating the neural architectures (quantiﬁed by the ranking correlation Spearman’s ρ be-tween the approximated performances and the downstream task performances) and the convergence speed for training (within a few seconds). For example, on
NAS-Bench-101, GenNAS achieves 0.85 ρ while the existing efﬁcient methods only achieve 0.38. We then propose an automatic task search to optimize the combination of synthetic signals using limited downstream-task-speciﬁc labels, further improving the performance of GenNAS. We also thoroughly evaluate Gen-NAS’s generality and end-to-end NAS performance on all search spaces, which outperforms almost all existing works with signiﬁcant speedup. For example, on
NASBench-201, GenNAS can ﬁnd near-optimal architectures within 0.3 GPU hour.
Our code has been made available at: https://github.com/leeyeehoo/GenNAS 1

Introduction
Most existing neural architecture search (NAS) approaches aim to ﬁnd top-performing architectures on a speciﬁc downstream task, such as image classiﬁcation [1, 2, 3, 4, 5], semantic segmentation [6, 7, 8], neural machine translation [9, 10, 11] or more complex tasks like hardware-software co-design [12, 13, 14, 15, 16]. They either directly search on the target task using the target dataset (e.g., classiﬁcation on CIFAR-10 [2, 17] ), or search on a proxy dataset and then transfer to the target one (e.g. CIFAR-10 to ImageNet) [18, 3]. However, extensive experiments show that prominent neural architectures are generally good at extracting patterns from the input data and perform well to different downstream tasks. For example, ResNet [19] being a prevailing architecture in computer vision, shows outstanding performance across various datasets and tasks [20, 21, 22], because of its 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Classiﬁcation (b) Regression
Figure 1: For classiﬁcation, only samples near the decision boundary determine the classiﬁcation accuracy. For regression, all samples equally contribute to the regression accuracy. Therefore, regression is better at leveraging all training samples than classiﬁcation to achieve faster convergence. advantageous architecture, the residual blocks. This observation motivates us to ask the ﬁrst question:
Is there a generic way to search for and evaluate neural architectures without using the speciﬁc knowledge of downstream tasks?
Meanwhile, we observe that most existing NAS approaches directly use the ﬁnal classiﬁcation performance as the metric for architecture evaluation and search, which has several major issues. First, the classiﬁcation accuracy is dominated by the samples along the classiﬁcation boundary, while other samples have clearer classiﬁcation outcomes compared to the boundary ones (as illustrated in Fig. 1a).
Such phenomena can be observed in the limited number of effective support vectors in SVM [23], which also applies to neural networks because of the theory of neural tangent kernel [24]. Therefore, discriminating performance of classiﬁers needs many more samples than necessary (the indeed effective ones), causing a big waste. Second, a classiﬁer tends to discard a lot of valuable information, such as ﬁner-grained features and spatial information, by transforming input representations into categorical labels. This observation motivates us to ask the second question: Is there a more effective way that can make more sufﬁcient use of input samples and better capture valuable information?
To answer the two fundamental questions for NAS, in this work, we propose a Generic Neural
Architecture Search method, termed GenNAS. GenNAS adopts a regression-based proxy task using downstream-task-agnostic synthetic signals for network training and evaluation. It can efﬁciently (with near-zero training cost) and accurately approximate the neural architecture performance.
Insights. First, as opposed to classiﬁcation, regression can efﬁciently make fully use of all the input samples, which equally contribute to the regression accuracy (Fig. 1b). Second, regression on properly-designed synthetic signals is essentially evaluating the intrinsic representation power of neural architectures, which is to capture and distinguish fundamental data patterns that are agnostic to downstream tasks. Third, such representation power is heavily reﬂected in the intermediate data of a network (as we will show in the experiments), which are regrettably discarded by classiﬁcation.
Approach. First, we propose a regression proxy task as the supervising task to train, evaluate, and search for neural architectures (Fig. 2). Then, the searched architectures will be used for the target downstream tasks. To the best of our knowledge, we are the ﬁrst to propose self-supervised regression proxy task instead of classiﬁcation for NAS. Second, we propose to use unlabeled synthetic data (e.g., sine and random signals) as the groundtruth (Fig. 3) to measure neural architectures’ intrinsic capability of capturing fundamental data patterns. Third, to further boost NAS performance, we propose a weakly-supervised automatic proxy task search with only a handful of groundtruth architecture performance (e.g. 20 architectures), to determine the best proxy task, i.e., the combination of synthetic signal bases, targeting a speciﬁc downstream task, search space, and/or dataset (Fig. 4).
GenNAS Evaluation. The efﬁciency and effectiveness of NAS are dominated by neural architecture evaluation, which directs the search algorithm towards top-performing network architectures. To quantify how accurate the evaluation is, one widely used indicator is the network performance Ranking 2
Correlation [25] between the prediction and groundtruth ranking, deﬁned as Spearman’s Rho (ρ) or
Kendall’s Tau (τ ). The ideal ranking correlation is 1 when the approximated and groundtruth rankings are exactly the same; achieving large ρ or τ can signiﬁcantly improve NAS quality [26, 27, 28].
Therefore, in the experiments (Sec. 4), we evaluate GenNAS using the ranking correlation factors it achieves, and then show its end-to-end NAS performance in ﬁnding the best architectures. Extensive experiments are done on 13 CNN search spaces and one NLP space [29]. Trained by the regression proxy task using only a single batch of unlabeled data within a few seconds, GenNAS signiﬁcantly outperforms all existing NAS approaches on almost all the search spaces and datasets. For example,
GenNAS achieves 0.87 ρ on NASBench-101 [30], while Zero-Cost NAS [31], an efﬁcient proxy
NAS approach, only achieves 0.38. On end-to-end NAS, GenNAS generally outperforms others with large speedup. This implies that the insights behind GenNAS are plausible and that our proposed regression-based task-agnostic approach is generalizable across tasks, search spaces, and datasets.
Contributions. We summarize our contributions as follows:
• To the best of our knowledge, GenNAS is the ﬁrst NAS approach using regression as the self-supervised proxy task instead of classiﬁcation for neural architecture evaluation and search. It is agnostic to the speciﬁc downstream tasks and can signiﬁcantly improve training and evaluation efﬁciency by fully utilizing only a handful of unlabeled data.
• GenNAS uses synthetic signal bases as the groundtruth to measure the intrinsic capability of networks that captures fundamental signal patterns. Using such unlabeled synthetic data in regression, GenNAS can ﬁnd the generic task-agnostic top-performing networks and can apply to any new search spaces with zero effort.
• An automated proxy task search to further improve GenNAS performance.
• Thorough experiments show that GenNAS outperforms existing NAS approaches by large margins in terms of ranking correlation with near-zero training cost, across 13 CNN and one NLP space without proxy task search. GenNAS also achieves state-of-the-art performance for end-to-end NAS with orders of magnitude of speedup over conventional methods.
• With proxy task search being optional, GenNAS is ﬁne-tuning-free, highly efﬁcient, and can be easily implemented on a single customer-level GPU. 2