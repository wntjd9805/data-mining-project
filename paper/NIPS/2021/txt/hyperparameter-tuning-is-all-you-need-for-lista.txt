Abstract
Learned Iterative Shrinkage-Thresholding Algorithm (LISTA) introduces the con-cept of unrolling an iterative algorithm and training it like a neural network. It has had great success on sparse recovery. In this paper, we show that adding momentum to intermediate variables in the LISTA network achieves a better convergence rate and, in particular, the network with instance-optimal parameters is superlinearly convergent. Moreover, our new theoretical results lead to a practical approach of automatically and adaptively calculating the parameters of a LISTA network layer based on its previous layers. Perhaps most surprisingly, such an adaptive-parameter procedure reduces the training of LISTA to tuning only three hyperparameters from data: a new record set in the context of the recent advances on trimming down
LISTA complexity. We call this new ultra-light weight network HyperLISTA. Com-pared to state-of-the-art LISTA models, HyperLISTA achieves almost the same performance on seen data distributions and performs better when tested on unseen distributions (speciﬁcally, those with different sparsity levels and nonzero magni-tudes). Code is available: https://github.com/VITA-Group/HyperLISTA. 1

Introduction
In this paper, we study the sparse linear inverse problem, where we strive to recover an unknown sparse vector x∗ ∈ Rn from its noisy linear measurement b generated from b = Ax∗ + ε, (1) where b ∈ Rm is the measurement that we observe, A ∈ Rm×n is the dictionary, x∗ ∈ Rn is the unknown ground truth that we aim to recover, and ε ∈ Rm is additive Gaussian white noise. For simplicity, each column of A, is normalized to have unit (cid:96)2 norm. Typically, we have much fewer rows than columns in the dictionary A, i.e., m (cid:28) n. Therefore, Equation (1) is an under-determined system. The sparse inverse problem, also known as sparse coding, plays essential roles in a wide range of applications including feature selection, signal reconstruction and pattern recognition.
Sparse linear inverse problems are well studied in the literature of optimization. For example, it can be formulated into LASSO [29] and solved by many optimization algorithms [9, 3]. These solutions explicitly consider and incorporate the sparsity prior and usually exploit iterative routines.
Deep-learning-based approaches are proposed for empirically solving inverse problems recently [25], which produce black-box models that are trained with data in an end-to-end way, while somehow ignoring the sparsity prior. Comparing the two streams, the former type, i.e., the classic optimization methods, takes hundreds to thousands of iterations to generate accurate solutions, while black-box deep learning models, if properly trained, can achieve similar accuracy in tens of layers. However, classic methods come with data-agnostic convergence (rate) guarantees under suitable conditions, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
∗The ﬁrst two authors made equal contributions.
whereas deep learning methods only empirically work for instances similar to the training data, lacking theoretical guarantees and interpretability. More discussions are found in [6].
Unrolling is an uprising approach that bridges the two streams [22, 21]. By converting each iteration of a classic iterative algorithm into one layer of a neural network, one can unroll and truncate a classic optimization method into a feed-forward neural network, with a ﬁnite number of layers. Relevant parameters (e.g., the dictionary, step sizes and thresholds) in the original algorithm are transformed into learnable weights, that are trained on data. [15] pioneered the unrolling scheme for solving sparse coding and achieved great empirical success. By unrolling the Iterative Shrinkage-Thresholding
Algorithm (ISTA), the authors empirically demonstrated up to two magnitudes of acceleration of the learned ISTA (LISTA) compared to the original ISTA. The similar unrolling idea was later extended to numerous optimization problems and algorithms [28, 30, 26, 13, 36, 2, 16, 8, 5]. 1.1