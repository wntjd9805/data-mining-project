Abstract
Despite remarkable success in a variety of applications, it is well-known that deep learning can fail catastrophically when presented with out-of-distribution data.
Toward addressing this challenge, we consider the domain generalization problem, wherein predictors are trained using data drawn from a family of related training domains and then evaluated on a distinct and unseen test domain. We show that under a natural model of data generation and a concomitant invariance condition, the domain generalization problem is equivalent to an inﬁnite-dimensional con-strained statistical learning problem; this problem forms the basis of our approach, which we call Model-Based Domain Generalization. Due to the inherent chal-lenges in solving constrained optimization problems in deep learning, we exploit nonconvex duality theory to develop unconstrained relaxations of this statistical problem with tight bounds on the duality gap. Based on this theoretical motivation, we propose a novel domain generalization algorithm with convergence guarantees.
In our experiments, we report improvements of up to 30% over state-of-the-art domain generalization baselines on several benchmarks including ColoredMNIST,
Camelyon17-WILDS, FMoW-WILDS, and PACS. Our code is publicly available at the following link: https://github.com/arobey1/mbdg. 1

Introduction
Despite well-documented success in numerous applications [1–4], the complex prediction rules learned by modern machine learning methods can fail catastrophically when presented with out-of-distribution (OOD) data [5–9]. Indeed, rapidly growing bodies of work conclusively show that state-of-the-art methods are vulnerable to distributional shifts arising from spurious correlations
[10–12], adversarial attacks [13–17], sub-populations [18–21], and naturally-occurring variation
[22–24]. This failure mode is particularly pernicious in safety-critical applications, wherein the shifts that arise in ﬁelds such as medical imaging [25–28], autonomous driving [29–31], and robotics
[32–34] are known to lead to unsafe behavior. And while some progress has been made toward addressing these vulnerabilities, the inability of modern machine learning methods to generalize to
OOD data is one of the most signiﬁcant barriers to deployment in safety-critical applications [35, 36].
In the last decade, the domain generalization community has emerged in an effort to improve the
OOD performance of machine learning methods [37–40]. In this ﬁeld, predictors are trained on data drawn from a family of related training domains and then evaluated on a distinct and unseen test domain. Although a variety of approaches have been proposed for this setting [41, 42], it was recently shown that that no existing domain generalization algorithm can signiﬁcantly outperform empirical risk minimization (ERM) [43] over the training domains when ERM is properly tuned and equipped with state-of-the-art architectures [44, 45] and data augmentation techniques [46]. Therefore, due to the prevalence of OOD data in safety critical applications, it is of the utmost importance that new algorithms be proposed which can improve the OOD performance of machine learning methods.
In this paper, we introduce a new framework for domain generalization which we call Model-Based
Domain Generalization (MBDG). The key idea in our framework is to ﬁrst learn transformations 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
that map data between domains and then to subsequently enforce invariance to these transformations.
Under a general model of covariate shift and a novel notion of invariance to learned transformations, we use this framework to rigorously re-formulate the domain generalization problem as a semi-inﬁnite constrained optimization problem. We then use this re-formulation to prove that a tight approximation of the domain generalization problem can be obtained by solving the empirical, parameterized dual for this semi-inﬁnite problem. Finally, motivated by these theoretical insights, we propose a new algorithm for domain generalization; extensive experimental evidence shows that our algorithm advances the state-of-the-art on a range of benchmarks by up to thirty percentage points.
Contributions. Our contributions can be summarized as follows:
• We propose a new framework for domain generalization in which invariance is enforced to underlying transformations of data which capture inter-domain variation.
• Under a general model of covariate shift, we rigorously prove the equivalence of the domain generalization problem to a novel semi-inﬁnite constrained statistical learning problem.
• We derive data-dependent duality gap bounds for the empirical parameterized dual of this semi-inﬁnite problem, proving that tight approximations of the domain generalization problem can be obtained by solving this dual problem under the covariate shift assumption.
• We introduce a primal-dual style algorithm for domain generalization in which invariance is enforced over unsupervised generative models trained on data from the training domains.
• We empirically show that our algorithm signiﬁcantly outperforms state-of-the-art baselines on several standard benchmarks, including ColoredMNIST, Camelyon17-WILDS, and PACS. 2