Abstract
Recent studies show that advanced priors play a major role in deep generative models. Exemplar VAE, as a variant of VAE with an exemplar-based prior, has achieved impressive results. However, due to the nature of model design, an exemplar-based model usually requires vast amounts of data to participate in train-ing, which leads to huge computational complexity. To address this issue, we propose Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE), a new variant of
VAE with a prior based on Bayesian pseudocoreset. The proposed prior is condi-tioned on a small-scale pseudocoreset rather than the whole dataset for reducing the computational cost and avoiding overﬁtting. Simultaneously, we obtain the optimal pseudocoreset via a stochastic optimization algorithm during VAE training aiming to minimize the Kullback-Leibler divergence between the prior based on the pseudocoreset and that based on the whole dataset. Experimental results show that ByPE-VAE can achieve competitive improvements over the state-of-the-art
VAEs in the tasks of density estimation, representation learning, and generative data augmentation. Particularly, on a basic VAE architecture, ByPE-VAE is up to 3 times faster than Exemplar VAE while almost holding the performance. Code is available at https://github.com/Aiqz/ByPE-VAE. 1

Introduction
Deep generative models that learn implicit data distribution from the enormous amount of data have received widespread attention to generating highly realistic new samples in machine learning.
In particular, due to the utilization of the reparameterization trick and variational inference for optimization, Variational Autoencoders (VAEs) [1, 2] stand out and have demonstrated signiﬁcant successes for dimension reduction [3], learning representations [4], and generating data [5]. In addition, various variants of VAE have been proposed conditioned on advanced variational posterior
[6, 7, 8] or powerful decoders [9, 10].
It is worth noting that the prior in the typical VAE is a simple standard normal distribution that is convenient to compute while ignores the nature of the data itself. Moreover, a large number of experiments have empirically demonstrated that simplistic priors could produce the phenomena of over-regularization and posterior collapse, and ﬁnally cause poor performance [5, 11]. Hence, many researchers have worked to develop more complex priors to enhance the capacity of the variational posterior. In this line, Tomczak et al. [12] introduces a more ﬂexible prior named VampPrior, which is a mixture of variational posteriors based on pseudo-inputs to alleviate the problems like overﬁtting
∗Corresponding Author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and high computational cost. However, the way in which the pseudo-inputs are obtained is not interpretable. Recently, Norouzi et al. [13] develops an Exemplar VAE with a non-parametric prior based on an exemplar-based method, achieving excellent performance. To ensure the performance and the generation diversity, the exemplar set needs to be large enough, and usually the entire training data is utilized. Obviously, this leads to huge computational complexity. Even though Exemplar VAE further employs approximate nearest neighbor search to speed up the training process, the number of nearest neighbors should be as large as possible to ensure performance. In a nutshell, Exemplar VAE is computationally expensive.
To address such issues, we develop a new prior for VAE that is inspired by the paradigm of coresets
[14]. The coreset is a powerful tool aiming to ﬁnd a small weighted subset for efﬁciently approximat-ing the entire original dataset. Therefore, rather than using the large-scale training data directly, we seek to design a prior conditioned on a coreset, which greatly reduces the computational complexity and prevents overﬁtting. In practice, to better incorporate this idea with the framework of VAE, we further employ a speciﬁc form of coresets, namely Bayesian pseudocoresets [15], which is known as a small weighted subset of the pseudodata points, resulting in a Bayesian pseudocoreset based prior. With this prior, we gain a new variant of VAE called Bayesian Pseudocoresets Exemplar VAE (ByPE-VAE). To sample from the ByPE-VAE, we ﬁrst take a pseudodata point from the pseudocoreset according to its weight, and then transform it into a latent representation using the learned prior. Then a decoder is used to transform the latent representation into a new sample.
A crucial part of ByPE-VAE is to obtain the optimal pseudocoreset. We formulate this process as a variational inference problem where the pseudodata points and corresponding weights are the parameters of variational posterior approximation. More precisely, to seek the optimal pseudocoreset, we minimize the Kullback-Leibler (KL) divergence between the prior based on the pseudocoreset and that based on the entire dataset. This processing ensures that the learned prior is actually an approximation of the prior conditioned on whole training data. Thus, it is fundamentally different from general pseudodata based priors, like VampPrior. For optimization, we adopt a two-step alternative search strategy to learn two types of parameters of ByPE-VAE, which refer to the parameters in the VAE framework and the pseudodata points and corresponding weights in the pseudocoreset. In particular, we iteratively optimize one of the two types of parameters while keeping the other one
ﬁxed until convergence.
Finally, we compare ByPE-VAE with several state-of-the-art VAEs in a number of tasks, including density estimation, representation learning and generative data augmentation. Experimental results demonstrate the effectiveness of ByPE-VAE on Dynamic MNIST, Fashion MNIST, CIFAR10, and
CelebA. Additionally, to validate the efﬁciency of our model, we measure the running time on a basic
VAE architecture. Compared to the Exemplar VAE, ByPE-VAE is up to 3 times speed-up without losing performance on Dynamic MNIST, Fashion MNIST, and CIFAR10. 2 Preliminaries
Before presenting the proposed model, we ﬁrst introduce some preliminaries, namely Exemplar VAE and Bayesian pseudocoresets. Throughout this paper, vectors are denoted by bold lowercase letters, whose subscripts indicate their order, and matrices are denoted by upper-case letters. 2.1 Exemplar VAE
Exemplar VAE is regarded as a variant of VAE that integrates the exemplar-based method into
VAE for the sake of seeking impressive image generation. Speciﬁcally, it ﬁrst draws a random exemplar xn using uniform distribution from the training data X = {xn}N n=1 (where N denotes the sample amount), then transforms an exemplar xn into latent variable z using an example-based prior rφ(z | xn), ﬁnally generates observable data x by a decoder pφ(x | z). The parametric transition distribution Tφ,θ(x | xn) of exemplar-based generative models can be expressed as
Tφ,θ(x | xn) = (cid:90) z rφ(z | xn)pθ(x | z)dz, (1) where φ and θ denote corresponding parameters. Assuming that x is independent to xn conditioned on the latent variable z and marginalizing over the latent variable z, the objective O(θ, φ; x, X) of 2
Exemplar VAE can be formulated as
N (cid:88) 1
N log p(x; X, θ, φ) = log n=1
= O(θ, φ; x, X),
Tφ,θ(x|xn) ≥ Eqφ(z|x) log pθ(x|z) − Eqφ(z|x) log qφ(z|x) n=1 rφ(z|xn)/N (2) where O(θ, φ; x, X) is known as the evidence lower bound (ELBO). From the Eq. (2), it can be derived that the difference between Exemplar VAE and typical VAE is the deﬁnition of the prior p(z) in the second term. The prior of Exemplar VAE is deﬁned as a mixture form, that is p(z | X) = (cid:80)N n=1 rφ(z | xn)/N . The variational posterior qφ(z | x) and the exemplar-based prior rφ(z | xn) are assumed to be Gaussian distributions whose parameters are fulﬁlled by neural networks. Note that, the computational cost of this training process is related to the number of exemplars which usually set to be the entire training data. This indicates that Exemplar VAE is computationally expensive since the amount of training data is generally huge. (cid:80)N 2.2 Bayesian Pseudocoresets
Bayesian pseudocoresets is a method of coreset construction based on variational inference, which constructs a weighted set of synthetic “pseudodata” instead of the original dataset during inference.
First, the goal of this method is to approximate expectations under the posterior π(ψ) with the parameter ψ, which is formulated as
π(ψ) = 1
Z (cid:32) N (cid:88) exp (cid:33) f (xn, ψ)
π0(ψ), (3) n=1 where f (·, ψ) denotes a potential function, and usually is a log-likelihood function. π0(ψ) is the prior, and Z is the normalization constant. Instead of directly inferring the posterior π(ψ), the Bayesian pseudocoreset employs a weighted set of pseudodata points to approximate the true posterior π(ψ), which is given by
πU,w(ψ) = 1
˜ZU,w (cid:32) M (cid:88) exp (cid:33) wmf (um, ψ)
π0(ψ), (4) m=1 m=1 represents M pseudodata points um ∈ Rd, w = {wm}M where U = {um}M m=1 denotes non-negative weights, and ˜ZU,w is the corresponding normalization constant. Finally, this model obtains the optimal pseudodata points and their weights by minimizing the KL divergence, as follows,
This formulation can reduce the computational cost by decreasing data redundancy.
U (cid:63), w(cid:63) = argmin
U,w
DKL (πU,w(cid:107)π) . (5) 3 Bayesian Pseudocoresets Exemplar VAE
To generate a new observation x, the Exemplar VAE requires a large collection of exemplars from
X = {xn}N n=1 to guide the whole process, as shown in Eq. (2). One can see that the greater the number of exemplars set, the richer the prior information can be obtained. In practice, to ensure performance, the number of exemplars is relatively large which is generally set to the size of the entire training data. This leads to huge computational costs in the training process of the Exemplar
VAE. To overcome such a issue, inspired by the paradigm of Bayesian pseudocoresets, we adopt M pseudodata points U = {um}M m=1 to denote exemplars, importantly M (cid:28) N . That is, the original exemplars X are approximated by a small weighted set of pseudodata points known as a Bayesian pseudocoreset. The framework can be expressed as m=1 with corresponding weights w = {wm}M log p(x | X, θ) = log
N (cid:88) n=1 1
N
Tθ(x | xn) ≈ log
M (cid:88) m=1 wm
N
Tθ(x | um) = log p(x | U, w, θ), (6) where wm ≥ 0 (m = 1, · · · , M ) and (cid:107)w(cid:107)1 = N . Further, we integrate this approximated framework with VAE by introducing a latent variable z. Then the parametric function is given by
Tφ,θ(x | um) = (cid:90) z rφ(z | um)pθ(x | z)dz, (7) 3
where rφ(z | um) with parameter φ denotes a pseudodata based prior for generating z from a pseu-dodata point um. pθ(x | z) with parameter θ represents the decoder for generating the observation x from z. Similarly, we assume that an observation x is independent from a pseudodata point um conditional on z to simplify the formulation and optimization.
In general, we desire to maximize the marginal log-likelihood log p(x) for learning, however, this is intractable since we have no ability to integrate the complex posterior distributions out. Now, we focus on maximizing the evidence lower bound (ELBO) derived by Jensen’s inequality, as follows, log p(x; U, w, θ, φ) = log
M (cid:88) m=1 wm
N
Tφ,θ(x | um) = log
M (cid:88) m=1 wm
N (cid:90) z rφ(z | um)pθ(x | z)dz (8)
≥ Eqφ(z|x) log pθ(x | z) − Eqφ(z|x) log qφ(z | x) m=1 wmrφ(z | um)/N (cid:80)M
≡ O(θ, φ, U, w; x), (9) where qφ(z | x) represents the approximate posterior distribution. And O(θ, φ, U, w; x) is deﬁned as the objective function of the ByPE-VAE to optimize parameters θ and φ. The speciﬁc derivation can be found in Supp.A. As we can see from Eq. (9), the difference between the ByPE-VAE and other variants of VAE is the formulation of the prior p(z) in the second term. In detail, the prior of the ByPE-VAE is a weighted mixture model prior, in which each component is conditioned on a pseudodata point and the corresponding weight, i.e., p(z|U, w) = (cid:80)M m=1 wmrφ(z|um)/N . In contrast, the prior of the Exemplar VAE is denoted as p(z|X) = (cid:80)N (9), the ByPE-VAE includes two encoder networks, namely qφ(z | x) and
As shown in Eq. rφ(z | um). And the distributions of qφ(z | x) and rφ(z | um) are both designed as Gaussian distributions. According to the analysis in [16], the optimal prior is the form of aggregate posterior.
Inspired by this report, we make the prior be coupled with the variational posterior, as follows, n=1 rφ(z|xn)/N . qφ(z | x) = N (z | µφ(x), Λφ(x)), rφ(z | um) = N (z | µφ(um), σ2I). (10) (11)
We employ the same parametric mean function µφ for two encoder networks for better incorporation.
However, the covariance functions of the two encoder networks are different. Speciﬁcally, the variational posterior uses a diagonal covariance matrix function Λφ, while each component of the mixture model prior uses an isotropic Gaussian with a scalar parameter σ2. Note that the scalar parameter σ2 is shared by each component of the mixture model prior for effective computation.
Then, we can express the log of the weighted pseudocoreset based prior log pφ(z | U, w) as log pφ(z | U, w) = −dz log(
√ 2πσ) − log N + log
M (cid:88) m=1 wm exp
− (cid:107)z − µφ (um)(cid:107)2 2σ2
, (12) where dz denotes the dimension of z. Based on the formulation of Eq. (12), we can further obtain the objective function of the ByPE-VAE, as follows,
O(θ, φ, U, w; X) = Eqφ(z|x) log (cid:34) pθ(x | z) qφ(z | x)
+ log
M (cid:88) m=1 wm 2πσ)dz
√ ( exp
− (cid:107)z − µφ (um)(cid:107)2 2σ2 (cid:35)
, (cid:34)
Eqφ(z|xi) log
∝
N (cid:88) i=1 pθ(xi | z) qφ(z | xi)
+ log
M (cid:88) m=1 wm 2πσ)dz
√ ( exp
− (cid:107)z − µφ (um)(cid:107)2 2σ2 (13) (cid:35)
, (14) where the constant − log N is omitted for convenience. For Eqφ(z|xi), we employ the reparametriza-tion trick to generate samples. Note that, for the standard VAE with a Gaussian prior, the process of generating new observations involves only the decoder network after training. However, to generate a new observation from ByPE-VAE, we not only require the decoder network, but also the learned
Bayesian pseudocoreset and a pseudodata point based prior rφ. We summarize the generative process of the ByPE-VAE in Algorithm 1. 4
: Pseudocoreset {U, w}, Decoder pθ, Prior rφ.
Algorithm 1: The Generative Process of ByPE-VAE
Input
Output :A generated observation x.
Step.1 Sample um ∼ Multi(· | U, w
Step.2 Sample z ∼ rφ(· | um) using the pseudodata point based prior rφ for obtaining the latent
N ) for obtaining a pseudodata point um from the learned pseudocoreset. representation z.
Step.3 Sample x ∼ pθ(· | z) using the decoder pθ for generating a new observation x.
However, more importantly, the whole process above holds only if the pseudocoreset is an approxima-tion of all exemplars or training data. To ensure this, we ﬁrst re-represent frameworks of pφ(z | X) and pφ(z | U, w) from the Bayesian perspective. Concretely, pφ(z | X) and pφ(z | U, w) are also viewed as posteriors conditioned on the likelihood function pθ(· | z) and a certain prior p0(z), as follows, pφ(z | X) = 1
Z exp (cid:32) N (cid:88) n=1 (cid:33) log pθ(xn | z) p0(z), pφ(z | U, w) = 1
˜ZU,w exp (cid:32) M (cid:88) m=1 (cid:33) wm log pθ(um | z) p0(z), (15) (16) where Z and ˜ZU,w are their respective normalization constants, pθ(· | z) here speciﬁcally refers to the decoder. Then, we develop this problem into a variational inference problem, where the pseudodata points and the corresponding weights are the parameters of the variational posterior approximation followed [15]. Speciﬁcally, we construct the pseudocoreset by minimizing the KL divergence in terms of the pseudodata points and the weights, as follows,
U (cid:63), w(cid:63) = argmin
U,w
DKL (pφ(z | U, w)(cid:107)pφ(z | X)) . (17)
Further, the gradients of DKL in Eq. (17) with respect to the pseudodata point um and the weights w are given by
∇um DKL = −wm CovU,w
∇wDKL = − CovU,w (cid:2)∇U log pθ(um | z), log pθ(X | z)T 1N − log pθ(U | z)T w(cid:3) , (cid:2)log pθ(U | z), log pθ(X | z)T 1N − log pθ(U | z)T w(cid:3) , (18) (19) where CovU,w denotes the covariance operator for the pφ(z | U, w), and 1N ∈ RN represents the vector of all 1 entries. In practice, we adopt a black-box stochastic algorithm to obtain the optimal pseudodata points and weights. Details of these derivations are provided in Supp.B and C.
Note that ByPE-VAE involves two types of parameters, namely the parameters θ and φ in the VAE framework and the parameters U and w in the pseudocoreset. For optimization, we use a two-step alternative optimization strategy. In detail, (i) update θ and φ with ﬁxed pseudocoreset {U, w}, and (ii) update U and w with ﬁxed θ and φ. Steps (i) and (ii) are iteratively implemented until convergence. The detailed optimization algorithm is shown in Algorithm 2. One can see that the computation does not scale with N , but rather with the number of pseudocoreset points M , which greatly reduces the computational complexity and also prevents overﬁtting. Also note that, rather than be updated every epoch, the pseudocoreset {U, w} is updated by every k epochs. And k is set to 10 in the experiments. 4