Abstract
Tackling overestimation in Q-learning is an important problem that has been extensively studied in single-agent reinforcement learning, but has received com-paratively little attention in the multi-agent setting. In this work, we empirically demonstrate that QMIX, a popular Q-learning algorithm for cooperative multi-agent reinforcement learning (MARL), suffers from a more severe overestimation in practice than previously acknowledged, and is not mitigated by existing ap-proaches. We rectify this with a novel regularization-based update scheme that penalizes large joint action-values that deviate from a baseline and demonstrate its effectiveness in stabilizing learning. Furthermore, we propose to employ a softmax operator, which we efﬁciently approximate in a novel way in the multi-agent setting, to further reduce the potential overestimation bias. Our approach,
Regularized Softmax (RES) Deep Multi-Agent Q-Learning, is general and can be applied to any Q-learning based MARL algorithm. We demonstrate that, when applied to QMIX, RES avoids severe overestimation and signiﬁcantly improves performance, yielding state-of-the-art results in a variety of cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks. 1

Introduction
In recent years, multi-agent reinforcement learning (MARL) has achieved signiﬁcant progress [4, 20] under the popular centralized training with decentralized execution (CTDE) paradigm [23, 15, 7]. In
CTDE, the agents must learn decentralized policies so that at execution time they can act based on only local observations, but the training itself is centralized, with access to global information. A critical challenge in this setting is how to represent and learn the joint action-value function [28].
In learning the value function, overestimation is an important challenge that stems from the max operator [34] typically used in the bootstrapping target. Speciﬁcally, the max operator in Q-learning
[39] approximates the maximum expected value with the maximum estimated value. This can lead to overestimation as E [maxi Xi] ≥ maxi E [Xi] due to noise [34, 11], where Xi is a random variable representing the Q-value of action i given a state. This overestimation error can accumulate during learning, lead to sub-optimal policy updates and behaviors, and hurt the performance of both value-based [37, 2, 32, 16] and actor-critic algorithms [8], and has been widely studied in the single-agent domain. However, overestimation can be even more severe in the multi-agent setting. For example, suppose there are n agents, each agent has K actions, and the Q-value for each action given a state is 2 while E [maxi Xi] independently drawn from a uniform distribution U (0, 1). Then, maxi E [Xi] is 1 is Kn
Kn+1 , which quickly approaches 1 (the maximum value of Xi) as the size of the joint action space
∗Work done while at University of Oxford. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
increases exponentially with the number of agents. Nonetheless, this problem has received much less attention in MARL.
QMIX [29] is a popular CTDE deep multi-agent Q-learning algorithm for cooperative MARL. It combines the agent-wise utility functions Qa into the joint action-value function Qtot, via a monotonic mixing network to ensure consistent value factorization. Due to its superior performance, there have been many recent efforts to improve QMIX’s representation capability [31, 41, 27, 38]. However, the overestimation problem of the joint-action Q-function Qtot can be exacerbated in QMIX due not only to overestimation in agents’ Qa but also the non-linear monotonic mixing network. Despite this, the role of overestimation in limiting its performance has been largely overlooked.
In this paper, we show empirically that over-estimation in deep multi-agent Q-learning is more severe than previously acknowledged and can lead to divergent learning behavior in prac-tice. In particular, consider double estimators, which can successfully reduce overestimation bias in the single-agent domain [37, 8]. Al-though QMIX applies Double DQN [37] to es-timate the value function (as mentioned in Ap-pendix D.3 in [28] and open-source PyMARL
[30] implementations), we ﬁnd that it is ineffec-tive in deep multi-agent Q-learning. As shown in Figure 1 (same experimental setup as in Sec-tion 5), value estimates of the joint-action Q-function can increase without bound in tasks from the multi-agent particle framework [18], yielding catastrophic performance degradation. Our experiments show that surprisingly, even applying Clipped Double Q-learning (a key technique from a state-of-the-art TD3 [8] algorithm) to the multi-agent setting does not resolve the severe overestimation bias in the joint-action Q-function. Therefore, alleviating overestimation in MARL is a particularly important and challenging problem.
Figure 1: Normalized performance (left) and value estimations in log scale (right) of QMIX in the multi-agent particle environments. Value estima-tions can grow unbounded (right) and lead to catas-trophic performance degradation (left).
To tackle this issue, we propose a novel update scheme that penalizes large joint-action Q-values.
Our key idea is to introduce a regularizer in the Bellman loss. A direct penalty on the magnitude of joint-action Q-values can result in a large estimation bias and hurt performance. Instead, to better trade off learning efﬁciency and stability, we introduce a baseline into the penalty, thereby constraining the joint-action Q-values to not deviate too much from this baseline. Speciﬁcally, we use the discounted return as the baseline. By regularizing towards a baseline, we stabilize learning and effectively avoid the unbounded growth in our value estimates.
However, regularization is not enough to fully avoid overestimation bias in the joint-action Q-function due to the max operator in the target’s value estimate [34]. To this end, we propose to employ a softmax operator, which has been shown to efﬁciently improve value estimates in the single-agent setting [32, 26, 25]. Unfortunately, a direct application of the softmax operator is often too computationally expensive in the multi-agent case, due to the exponentially-sized joint action space. We therefore propose a novel method that provides an efﬁcient and reliable approximation to the softmax operator based on a joint action subspace, where the gap between our approximation and its direct computation converges to 0 at an exponential rate with respect to its inverse temperature parameter. The computational complexity of our approximation scales only linearly in the number of agents, as opposed to exponentially for the original softmax operator. We show that our softmax operator can further improve the value estimates in our experiments. We refer to our method as
RES (Regularized Softmax) deep multi-agent Q-learning, which utilizes the discounted return-based regularization and our approximate softmax operator.
To validate RES, we ﬁrst prove that it can reduce the overestimation bias of QMIX. Next, we conduct extensive experiments in the multi-agent particle tasks [18], and show that RES simultaneously enables stable learning, avoids severe overestimation when applied to QMIX, and achieves state-of-the-art performance. RES is not tied to QMIX and can signiﬁcantly improve the performance and stability of other deep multi-agent Q-learning algorithms, e.g., Weighted-QMIX [27] and QPLEX
[38], demonstrating its versatility. Finally, to demonstrate its ability to scale to more complex scenarios, we evaluate it on a set of challenging StarCraft II micromanagement tasks [30]. Results show that RES-QMIX provides a consistent improvement over QMIX in all scenarios tested. 2
2