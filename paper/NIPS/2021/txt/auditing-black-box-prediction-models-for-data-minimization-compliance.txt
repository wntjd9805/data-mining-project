Abstract
In this paper, we focus on auditing black-box prediction models for compliance with the GDPR’s data minimization principle. This principle restricts prediction models to use the minimal information that is necessary for performing the task at hand.
Given the challenge of the black-box setting, our key idea is to check if each of the prediction model’s input features is individually necessary by assigning it some constant value (i.e., applying a simple imputation) across all prediction instances, and measuring the extent to which the model outcomes would change. We introduce a metric for data minimization that is based on model instability under simple imputations. We extend the applicability of this metric from a ﬁnite sample model to a distributional setting by introducing a probabilistic data minimization guarantee, which we derive using a Bayesian approach. Furthermore, we address the auditing problem under a constraint on the number of queries to the prediction system. We formulate the problem of allocating a budget of system queries to feasible simple imputations (for investigating model instability) as a multi-armed bandit framework with probabilistic success metrics. We deﬁne two bandit problems for providing a probabilistic data minimization guarantee at a given conﬁdence level: a decision problem given a data minimization level, and a measurement problem given a ﬁxed query budget. We design efﬁcient algorithms for these auditing problems using novel exploration strategies that expand classical bandit strategies. Our experiments with real-world prediction systems show that our auditing algorithms signiﬁcantly outperform simpler benchmarks in both measurement and decision problems. 1

Introduction
Concerns about the widespread use of data-driven prediction models and their growing reliance on personal data of individuals, have led to a number of data protection laws and regulations in recent years [17, 19, 24]. Prominent amongst such regulations is the GDPR (article 5.1.c) [19] that proposes the data minimization principle to control the extent to which personal data can be acquired and used by prediction models. Speciﬁcally, the data minimization principle states that: “Personal data shall be adequate, relevant and limited to what is necessary for the purposes for which they are processed.”
Despite considerable public debate about the GDPR and the data minimization principle, to date, only a few works [2, 18] have attempted to operationalize (i.e., formally interpret) the legal principle for prediction models. These works have focused on ﬁnding operationalizations of data minimization that tie the purpose of data processing to some performance metric such as prediction accuracy (e.g., in recommender systems [2], or in classiﬁcation [18]). Speciﬁcally, they study whether a prediction model can be redesigned to achieve similar prediction performance, while using fewer input features.
In the process, these works assume a transparent setting, where the full knowledge of the prediction algorithm, training procedure, and desired performance metrics is available to the party investigating the feasibility of reducing data inputs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
When putting data minimization into practice, an important case to consider is an auditing scenario where an outsider (auditor) wants to test a given prediction model for compliance with the data minimization principle at deployment time. As prediction algorithms are often important business assets, a more realistic auditing scenario is one in a black-box setting in which the auditor interacts with the model without access to the model internals.
In this paper, we propose an operational deﬁnition of the data minimization principle that allows auditing black-box prediction models for compliance at deployment time. In particular, we assume an auditor who does not have access to how the prediction model works, or to the training data, or to information about the purpose of the prediction model. All the auditor can do is using data points to query a given black-box prediction model with a ﬁxed set of input features, and observe the outcomes.
We believe our black-box model setting covers many real-world scenarios, as it only requires the designers of prediction models to allow auditors to query their models with prediction instances.
Our key insight is that the auditor can test whether an input feature is needed by the prediction model, by imputing (i.e., guessing) its value and checking the extent to which the outcomes change (i.e., are unstable) for different prediction instances. Intuitively, if the actual value of an input feature is not needed (i.e., can be replaced with a constant) to arrive at similar (stable) outcomes for most prediction instances, then the use of the feature violates the data minimization principle. Note that our instability-based operatlonalization does not require knowledge of the purpose of the prediction model and is independent of performance metrics. Such an operational deﬁnition is particularly important given that it is common for companies who provide personalized services to justify data collection simply as “for improving service” [9].
We show how simple imputations, where the actual values of individual features are replaced with a constant value, can be leveraged as a strategy for limiting data inputs to a prediction system at deployment time. We deﬁne a data minimization guarantee that is based on a metric of model instability under different feasible simple imputations. While this guarantee induces a procedure for auditing data minimization assuming a ﬁnite sample model, we extend the applicability of our auditing framework in two ways. First, we propose a probabilistic audit that allows the auditor to provide a data minimization guarantee at some conﬁdence level with respect to an underlying data distribution. We achieve this goal by introducing the notion of a probabilistic data minimization guarantee and adopting a Bayesian approach. Second, we address the auditing problem under a constraint on the number of queries to the prediction system and we design auditing algorithms that use a budget of system queries strategically in order to provide a data minimization guarantee.
We cast the problem of allocating a query budget to feasible simple imputations into a multi-armed bandit framework, and we formulate two bandit problems that correspond to different auditing tasks given a ﬁxed conﬁdence level: a decision problem given a data minimization level, and a measurement problem given a ﬁxed query budget. Furthermore, we design efﬁcient algorithms for the above problems using exploration strategies for selecting arms. We propose four heuristic exploration strategies: two strategies inspired by Thompson sampling, and two that are custom for our setting.
Finally, we study the effectiveness of our auditing algorithms using different real-world prediction systems. We build prediction models by applying standard model evaluation and feature selection methods, and use the resulting models to perform black-box audits. Our experiments show that algorithms that exploit the proposed bandit framework signiﬁcantly outperform simpler benchmarks. 2