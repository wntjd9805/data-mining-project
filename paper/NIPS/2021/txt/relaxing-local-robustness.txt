Abstract
Certiﬁable local robustness, which rigorously precludes small-norm adversarial examples, has received signiﬁcant attention as a means of addressing security concerns in deep learning. However, for some classiﬁcation problems, local robustness is not a natural objective, even in the presence of adversaries; for example, if an image contains two classes of subjects, the correct label for the image may be considered arbitrary between the two, and thus enforcing strict separation between them is unnecessary. In this work, we introduce two relaxed safety properties for classiﬁers that address this observation: (1) relaxed top-k robustness, which serves as the analogue of top-k accuracy; and (2) afﬁnity robustness, which speciﬁes which sets of labels must be separated by a robustness margin, and which can be (cid:15)-close in (cid:96)p space. We show how to construct models that can be efﬁciently certiﬁed against each relaxed robustness property, and trained with very little overhead relative to standard gradient descent. Finally, we demonstrate experimentally that these relaxed variants of robustness are well-suited to several signiﬁcant classiﬁcation problems, leading to lower rejection rates and higher certiﬁed accuracies than can be obtained when certifying “standard” local robustness1. 1

Introduction
The discovery of adversarial examples [9, 24, 28] has led to security concerns in deep learning. A growing body of work has sought to address this problem by providing provable guarantees that a model’s predictions are robust to small-norm perturbations [3, 5, 8, 13, 16, 21, 22, 25, 29, 31, 32, 33].
This objective is typically captured by ensuring that a model satisﬁes point-wise local robustness; i.e., given a point, x, the model’s predictions must remain invariant over the (cid:15)-ball centered at x. Local robustness is accompanied by a metric, veriﬁed-robust accuracy (VRA), which corresponds to the fraction of points that are both correctly classiﬁed and locally robust.
In some contexts, however, it is not always clear that VRA is the most desirable objective or the most nat-ural metric for measuring a model’s success against adversaries. For example, in some contexts, not all adversarial examples are equally bad—this may reﬂect simply that a mistake is understandable, even if it was caused by an adversary (e.g., if a model mistakenly predicts an image of a leopard to be a jaguar); or that the correct label may be arbitrary in certain cases (e.g., if an image contains two classes of subjects).
For similar sorts of reasons, some computer vision tasks often use top-k accuracy as a benchmark metric that relaxes standard accuracy, allowing a prediction to be considered correct so long as the correct label appears among the model’s k highest logit outputs. In many applications, top-k accuracy is considered a more suitable metric/objective than standard top-1 accuracy [1]; meanwhile, studied robustness properties are typically only deﬁned with respect to a single predicted class. Thus, as part of this work, we introduce an analogous relaxation of local robustness to top-k accuracy, which we call relaxed top-K (RTK) robustness (Section 3, Deﬁnition 3). Moreover, we demonstrate how 1our code is publicly available at https://github.com/klasleino/gloro 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
a neural network can be instrumented in order to naturally incorporate certiﬁable RTK robustness into its learning objective, making runtime certiﬁcation of this property essentially free (Section 5).
In addition to applying to only top-1 predictions, standard robustness also focuses speciﬁcally on the problem of undirected adversarial examples. More concretely, adversarial examples are obtained broadly via two categories of attacks: (1) evasion (undirected) attacks, and (2) targeted (directed) attacks. Local robustness provides guarantees against the former, while the latter, which requires only a weaker guarantee, has remained largely unexplored by work on certiﬁable robustness. In this work we introduce afﬁnity robustness (Section 4, Deﬁnition 4), which captures resistance to speciﬁed sets of directed adversarial attacks. We show that certiﬁable afﬁnity robustness can also be achieved in a manner similar to RTK robustness (Section 5).
In recent work, Leino et al. [22] introduced a concept of (cid:15)-global-robustness, which can be thought of as requiring regions of the input space that are labeled as different classes to be separated by a margin of (cid:15), with any interstitial space labeled as ⊥, signifying rejection. By relaxing the notion of robustness, we essentially allow certain classes to be grouped together without any intervening rejected space.
This gives rise to a spatially-arranged hierarchy of classes that is conveyed through the robustness guarantee (see Figure 2 in Section 3 for an example). Interestingly, we ﬁnd that models trained with the objective of RTK robustness often form a logical hierarchy even without supervision. Additionally, afﬁnity robustness provides a way to supervise the hierarchy that forms, giving ﬁner control over the spatial relationships between the various labels in the model’s decision surface.
In summary, the primary contributions in this work are as follows:
• We introduce RTK robustness, a notion of model robustness compatible with top-k accuracy, and afﬁnity robustness, a notion of model robustness that captures resistance to particular sets of directed adversarial attacks.
• We show how to construct models that can be efﬁciently certiﬁed against these two properties.
• We show that applying these relaxed robustness variants to suitable domains leads to certiﬁable models with lower rejection rates and higher certiﬁed accuracy.
• We show that these relaxed robustness variants lead to interesting properties regarding how a network’s prediction space is laid out, and that this layout of classes can be supervised to impart a priori hierarchies. 2