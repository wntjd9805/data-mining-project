Abstract
Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overﬁtting by using common data augmentation schemes. We demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Furthermore, we com-pare various data augmentations techniques and observe that spatial composition techniques work best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against (cid:96)∞ and (cid:96)2 norm-bounded perturbations of size (cid:15) = 8/255 and (cid:15) = 128/255, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against (cid:96)∞ norm-bounded perturbations of size (cid:15) = 8/255, our model reaches 60.07% robust accuracy without using any external data. We also achieve a signiﬁcant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TINYIMAGENET. 1

Introduction
Despite their success, neural networks are not intrinsically robust. In particular, it has been shown that the addition of imperceptible deviations to the input, called adversarial perturbations, can cause neural networks to make incorrect predictions with high conﬁdence [5, 6, 18, 32, 49]. Starting with Szegedy et al. [49], there has been a lot of work on understanding and generating adversarial perturbations
[2, 6], and on building defenses that are robust to such perturbations [18, 29, 34, 41]. Unfortunately, many of the defenses proposed in the literature target failure cases found through speciﬁc adversaries, and as such they are easily broken by different adversaries [3, 53]. Among successful defenses are robust optimization techniques like the one by Madry et al. [34] that learns robust models by ﬁnding worst-case adversarial perturbations at each training step before adding them to the training data.
In fact, adversarial training as proposed by Madry et al. is so effective [20] that it is the de facto standard for training adversarially robust neural networks. Indeed, since Madry et al. [34], various modiﬁcations to their original implementation have been proposed [20, 27, 40, 44, 57, 63].
Notably, Carmon et al. [7], Hendrycks et al. [25], Najaﬁ et al. [36], Uesato et al. [54], Zhai et al. [60] showed that using additional data improves adversarial robustness, while Gowal et al. [20], Rice et al. [44], Wu et al. [56] found that data augmentation techniques did not boost robustness. This dichotomy motivates this paper. In particular, we explore whether it is possible to ﬁx the training procedure such that data augmentation becomes useful in the setting without additional data. By making the observation that model weight averaging (WA) [28] helps robust generalization to a wider extent when robust overﬁtting is minimized, we propose to combine model weight averaging with data augmentation techniques. Overall, we make the following contributions:
• We demonstrate that, when combined with model weight averaging, data augmentation techniques such as Cutout [15], CutMix [58] and MixUp [62] can improve robustness. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Robust accuracy of various mod-els submitted to RobustBench [11] against
AUTOATTACK [10] on CIFAR-10 with (cid:96)∞ perturbations of size 8/255 displayed in publication order. Our method builds on Gowal et al. [20] (shown above with 57.20%) and explores how augmented data can be used to improve robust accuracy by
+2.87% without using any additional exter-nal data.
• To the contrary of Gowal et al. [20], Rice et al. [44], Wu et al. [56] which all tried data augmenta-tion techniques without success, we are able to use any of these three aforementioned techniques to obtain new state-of-the-art robust accuracies (see Figure 1). We ﬁnd CutMix to be the most effective method by reaching a robust accuracy of 60.07% on CIFAR-10 against (cid:96)∞ perturbations of size (cid:15) = 8/255 (an improvement of +2.93% upon the state-of-the-art).
• We conduct thorough experiments to show that our approach generalizes across architectures, datasets and threat models. We also investigate the trade-off between robust overﬁtting and underﬁtting to explain why MixUp performs worse than spatial composition techniques.
• Finally, we provide empirical evidence that weight averaging exploits data augmentation by ensembling model snapshots which have the same total accuracy but differ at the individual prediction level. 2