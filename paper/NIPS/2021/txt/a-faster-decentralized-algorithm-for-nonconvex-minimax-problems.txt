Abstract
In this paper, we study the nonconvex-strongly-concave minimax optimization problem on decentralized setting. The minimax problems are attracting increasing attentions because of their popular practical applications such as policy evaluation and adversarial training. As training data become larger, distributed training has been broadly adopted in machine learning tasks. Recent research works show that the decentralized distributed data-parallel training techniques are specially promising, because they can achieve the efﬁcient communications and avoid the bottleneck problem on the central node or the latency of low bandwidth network.
However, the decentralized minimax problems were seldom studied in literature and the existing methods suffer from very high gradient complexity. To address this challenge, we propose a new faster decentralized algorithm, named as DM-HSGD, for nonconvex minimax problems by using the variance reduced technique of hybrid stochastic gradient descent. We prove that our DM-HSGD algorithm achieves stochastic ﬁrst-order oracle (SFO) complexity of O(κ3(cid:15)−3) for decen-tralized stochastic nonconvex-strongly-concave problem to search an (cid:15)-stationary point, which improves the exiting best theoretical results. Moreover, we also prove that our algorithm achieves linear speedup with respect to the number of workers.
Our experiments on decentralized settings show the superior performance of our new algorithm. 1

Introduction
Minimax optimization has enormous applications in machine learning tasks such as Generative
Adversarial Net (GAN) [8], adversarial training [26] and multi-agent reinforcement learning [43].
Speciﬁcally, in minimax optimization, variable x aims to minimize a payoff loss function f (x, y) :
Rd1 × Rd2 → R while variable y tries to maximize the loss, which can be formulated as min x∈X max y∈Y f (x, y), (1) where X ⊆ Rd1 and Y ⊆ Rd2. In the past a few decades, there are plenty of works to study minimax optimization problem in a variety of research ﬁelds and many methods have been developed. The most intuitive solution is Gradient Descent Ascent (GDA) algorithm [6, 29] with equal stepsize
ηx = ηy. Asymptotic and nonasymptotic convergence analysis has been provided when f is convex in x and concave in y. Recently, many deterministic and stochastic gradient algorithms for nonconvex-strongly-concave and nonconvex-concave problems were proposed. Some algorithms improve the performance of vanilla GDA method by adopting different stepsize on x and y, such as [10, 19], where the stepsize of y is typically larger than the stepsize of x. Some algorithms update x and y at different frequency, such as [14, 25, 32]. These kind of algorithms usually involve a nested loop structure that updates y more frequently than x to make f (x, y) close to function Φ(x), which is deﬁned by
Φ(x) = max y∈Y f (x, y). (2) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
As more large-scale machine learning problems are arising, distributed training becomes a popular and crucial framework because of its ability and efﬁciency to deal with large data. It is desired to generalize minimax optimization to distributed training to solve large-scale minimax problems. In distributed optimization, the original centralized optimization suffers from a bottleneck communication problem, i.e. the communication trafﬁc on the busiest central node, especially when the network is large
[18, 51]. To tackle this communication issue, decentralized optimization was proposed and has emerged as a promising technique. It is a kind of distributed machine learning training paradigm that does not rely on the centralized network topology. Different worker nodes collaboratively utilize their own local data to implement large-scale training tasks and at each iteration they only have to communicate with their neighbors. Decentralized algorithms have been shown to enhance the communication efﬁciency by avoiding the communication overhead problem. Decentralized methods are also advantageous when the network suffers from communication restriction or has low bandwidth between some nodes and the central node. Besides, it is also an essential method in some situations where data are geographically distributed and centralized data processing is not available or there are concerns to preserve data privacy [48].
Recently many works were proposed to improve the performance of decentralized training. D-PSGD
[18] theoretically justiﬁes the potential advantage of decentralized algorithm. D2 [38] improves the convergence rate to outperform D-PSGD by eliminating the inﬂuence of data variance among different workers. D-SPIDER-SFO [33] incorporates D2 and SPIDER [7, 44], which is a kind of variance reduction technique [15], to further reduce the gradient complexity. DQSFW [45] studies decentralized constrained problem with Frank-Wolfe method. GT-HSGD [46] extends hybrid stochastic gradient descent to decentralized setting, which is a variance-reduced approach that does not compute mega batch periodically. However, the decentralized minimax optimization is still very limited and existing methods suffer from very high gradient complexity [21, 41]. Thus, we are motivated to design an accelerated decentralized algorithm for minimax problems.
In this paper, thus, we propose a faster Decentralized Minimax Hybrid Stochastic Gradient De-scent (DM-HSGD) algorithm to solve the following decentralized stochastic minimax optimization problem: min x∈Rd1 max y∈Y f (x, y) = 1 n n (cid:88) i=1 fi(x, y), fi(x, y) := E
ξ(i)∼DiFi(x, y; ξ(i)) (3) where n is the number of worker nodes, Y is a convex set. Here the local component objective function Fi(x, y; ξ(i)) is L-smooth, nonconvex in x, and strongly-concave in y. Di is the data distribution on the i-th node. In this paper, the data distribution can be non-identical. Random variable ξ(i) is an index sampled from the local data. We summarize our contributions as follows: (1) In this paper, we propose a new accelerated decentralized stochastic ﬁrst-order algorithm, named as DM-HSGD, to solve the decentralized nonconvex-strongly-concave minimax optimization problems. Our algorithm is the ﬁrst stochastic gradient algorithm to solve general decentralized minimax problem on non-identical distributed data with theoretical guarantees. Besides, our algorithm does not require large batch size or nested loop which makes it more practical and efﬁcient to implement. (2) We provide a completed proof to guarantee the convergence of our algorithm to solve decen-tralized stochastic minimax optimization. Under nonconvex-strongly-concave condition, our algorithm obtains SFO complexity of O(κ3(cid:15)−3) to search an (cid:15)-stationary point of function
Φ(x) = maxy∈Y f (x, y). This result is faster than the complexity of previous decentralized minimax algorithms [21, 41]. Moreover, we also prove that our method achieves linear speedup as the number of workers n increases, which veriﬁes its ability to solve large-scale problems.
The rest of this paper will be organized as follows. In Section 2, we will introduce related works. In
Section 3, we will introduce our new DM-HSGD algorithm. In Section 4, we will show the main theorems of convergence and complexity analysis. In Section 5, we will discuss our experimental results, and Section 6 will conclude the paper. 2
Table 1: Comparison of Related Algorithms for Minimax Optimization
Name
SGDA
SGDmax
SREDA
Acc-MDA
DPOSG
GT/DA
DM-HSGD
SFO
O(κ3(cid:15)−4)
O(κ3(cid:15)−4 log( 1
O(κ3(cid:15)−3)
O(κ3(cid:15)−3)
O((cid:15)−12)
O(N (cid:15)−2 log( 1
O(κ3(cid:15)−3) (cid:15) )) (cid:15) ))
Decentralized
×
×
×
× (iid) (non-iid) (non-iid)
√
√
√
Stochastic
√
√
√
√
√
×
√
Implementation Reference single-loop double-loop double-loop single-loop single-loop double-loop single-loop
[19]
[19]
[25]
[11]
[21]
[41]
Ours 2