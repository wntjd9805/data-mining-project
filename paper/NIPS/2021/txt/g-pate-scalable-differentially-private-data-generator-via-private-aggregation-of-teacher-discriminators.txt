Abstract
Recent advances in machine learning have largely beneﬁted from the massive accessible training data. However, large-scale data sharing has raised great privacy concerns. In this work, we propose a novel privacy-preserving data Generative model based on the PATE framework (G-PATE), aiming to train a scalable differ-entially private data generator which preserves high generated data utility. Our approach leverages generative adversarial nets to generate data, combined with private aggregation among different discriminators to ensure strong privacy guaran-tees. Compared to existing approaches, G-PATE signiﬁcantly improves the use of privacy budgets. In particular, we train a student data generator with an ensemble of teacher discriminators and propose a novel private gradient aggregation mechanism to ensure differential privacy on all information that ﬂows from teacher discrimi-nators to the student generator. In addition, with random projection and gradient discretization, the proposed gradient aggregation mechanism is able to effectively deal with high-dimensional gradient vectors. Theoretically, we prove that G-PATE ensures differential privacy for the data generator. Empirically, we demonstrate the superiority of G-PATE over prior work through extensive experiments. We show that G-PATE is the ﬁrst work being able to generate high-dimensional image data with high data utility under limited privacy budgets (" 1). Our code is available at https://github.com/AI-secure/G-PATE.
 1

Introduction
Machine learning has been applied to a wide range of applications such as face recognition [30, 39, 21, 22], autonomous driving [26], and medical diagnoses [8, 20]. However, most learning methods rely on the availability of large-scale training datasets containing sensitive information such as personal photos or medical records. Therefore, such sensitive datasets are often hard to be shared due to privacy concerns [40]. To handle this challenge, data providers sometimes release synthetic datasets produced by generative models learned on the original data. Though recent studies show that generative models such as generative adversarial networks (GAN) [14] can generate synthetic records that are indistinguishable from the original data distribution, there is no theoretical guarantee on the privacy protection. While privacy deﬁnitions such as differential privacy [9] and Rényi differential privacy
[27] provide rigorous privacy guarantee, applying them to synthetic data generation is nontrivial.
Recently, two approaches have been proposed to combine differential privacy with synthetic data gen-eration: DP-GAN [35] and PATE-GAN [37]. DP-GAN modiﬁes GAN by training the discriminator using differentially private stochastic gradient descent. Though it achieves privacy guarantee due to
⇤Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
the post processing property [10] of differential privacy, DP-GAN incurs signiﬁcant utility loss on the synthetic data, especially when the privacy budget is low. In contrast, PATE-GAN trains differentially private GAN using Private Aggregation of Teacher Ensembles (PATE) [28]. Speciﬁcally, it trains a set of teacher discriminators and a student discriminator. To ensure differential privacy, the student discriminator is only trained on records that are produced by the generator and labeled by the teacher discriminators. The key limitation of this approach is that it relies on the assumption that the generator would be able to generate the entire real records space to bootstrap the training process. If most of the synthetic records are labeled as fake by the teacher discriminators, the student discriminator would be trained on a biased dataset and fail to learn the true data distribution. Consequently, this trained generator would not be able to produce high-quality synthetic data. This problem does not exist for traditional GAN, where the discriminator is always able to provide useful information to the generator since they can access the real data records rather than the synthetic data only.
The main contribution of this paper is a new approach named G-PATE for training a differentially private data generator by combining the generative model with PATE mechanism. Our approach is based on the key observation that: It is not necessary to ensure differential privacy for the discrimina-tor in order to train a differentially private generator. As long as we ensure differential privacy on the information ﬂow from the discriminator to the generator, it is sufﬁcient to guarantee the privacy property for the generator. To achieve this, we propose a private gradient aggregation mechanism to ensure differential privacy on all the information that ﬂows from the teacher discriminators to the student generator. The aggregation mechanism applies random projection and gradient discretization to reduce privacy budget consumed by each aggregation step and to increase model scalability. Com-pared to PATE-GAN, our approach has three advantages. First, it improves the use of privacy budget by only applying it to the part of the model that actually needs to be released for data generation.
Second, our discriminator can be trained on original data records since it does not need to satisfy differential privacy. Finally, G-PATE preserves better utility on high-dimensional data given its more efﬁcient gradient aggregation mechanism.
Theoretically, we show that our algorithm ensures differential privacy for the generator. Empirically, we conduct extensive experiments on the Kaggle credit dataset and image datasets. To the best of our knowledge, this is the ﬁrst work that is able to scale to high-dimensional face image dataset such as CelebA while still preserving high data utility. The results show that our method signiﬁcantly outperforms all baselines including DP-GAN and PATE-GAN. 2