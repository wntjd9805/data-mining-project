Abstract
Our main contribution in this work is an empirical ﬁnding that random General
Value Functions (GVFs), i.e., deep action-conditional predictions—random both in what feature of observations they predict as well as in the sequence of actions the predictions are conditioned upon—form good auxiliary tasks for reinforcement learning (RL) problems. In particular, we show that random deep action-conditional predictions when used as auxiliary tasks yield state representations that produce control performance competitive with state-of-the-art hand-crafted auxiliary tasks like value prediction, pixel control, and CURL in both Atari and DeepMind Lab tasks. In another set of experiments we stop the gradients from the RL part of the network to the state representation learning part of the network and show, perhaps surprisingly, that the auxiliary tasks alone are sufﬁcient to learn state representa-tions good enough to outperform an end-to-end trained actor-critic baseline. We opensourced our code at https://github.com/Hwhitetooth/random_gvfs. 1

Introduction
Providing auxiliary tasks to Deep Reinforcement Learning (Deep RL) agents has become an important class of methods for driving the learning of state representations that accelerate learning on a main task. Existing auxiliary tasks have the property that their semantics are ﬁxed and carefully designed by the agent designer. Some notable examples include pixel control, reward prediction, termination prediction, and multi-horizon value prediction (these are reviewed in more detail below). Unlike the prior approaches that require careful design of auxiliary task semantics, we explore here a different approach in which a set of random action-conditional prediction tasks are generated through a rich space of general value functions (GVFs) deﬁned by a language of predictions of random features of observations conditioned on a random sequence of actions.
Our main, and perhaps surprising, contribution in this work is an empirical ﬁnding that auxiliary tasks of learning random GVFs—again, random in both predicted features and actions the predictions are conditioned upon— yield state representations that produce control performance that is competitive with state-of-the-art auxiliary tasks with hand-crafted semantics. We demonstrate this competitiveness in Atari games and DeepMind Lab tasks, comparing to multi-horizon value prediction [8], pixel control [13], and CURL [15] as our baseline auxiliary tasks. Note that while we present a reasonable approach to generating the semantics of the random GVFs we employ in our experiments, the speciﬁcs of our approach is not by itself a contribution (and thus not evaluated against other approaches to producing semantics for random GVFs), and alternative reasonable approaches for generating random
GVFs could do as well. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Additionally, through empirical analyses on illustrative domains we show the beneﬁts of exploiting the richness of GVFs—their temporal depth and action-conditionality. We also provide direct evidence that using random GVFs learns useful representations for the main task through stop-gradient experiments in which the state representations are trained solely via the random-GVF auxiliary tasks without using the usual RL learning with rewards to inﬂuence representation learning. We show that, again, surprisingly, these stop-gradient agents outperform the end-to-end-trained actor-critic baseline. 2