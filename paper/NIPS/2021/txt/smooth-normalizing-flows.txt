Abstract
Normalizing ﬂows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art ﬂows accurately approximate distributions and energies, applications in physics additionally require smooth energies to com-pute forces and higher-order derivatives. Furthermore, such densities are often deﬁned on non-trivial topologies. A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.
Mixture transformations employ root-ﬁnding methods to invert them in practice, which has so far prevented bi-directional ﬂow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem. We demonstrate two advantages of such smooth ﬂows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations. 1

Introduction
Generative learning using normalizing ﬂows (NF) [50, 42, 39] has become a widely applicable tool in the physical sciences which has e.g. been used for sampling lattice models [36, 37, 31, 4, 1], approximating the equilibrium density of molecular systems [38, 29, 56, 58] or estimating free energy differences [55, 10].
Such models approximate a target density µ via diffeomorphic maps f (·; θ) : Ω ⊂ Rd → Ω by transforming samples z ∼ p0(z) of a base density into samples x = f (z; θ) such that they follow the push-forward density x ∼ pf (x; θ) := p0 (cid:0)f −1(x; θ)(cid:1) (cid:12) (cid:12)det ∂xf −1(x; θ)(cid:12) (cid:12) . (1)
Flows can be trained on data by maximizing the likelihood or via minimizing of the reverse KL divergence DKL [pf (·; θ)(cid:107)µ] if µ is known up to a normalizing constant.
While NFs are usually introduced as smooth diffeomorphisms, most applications like density estima-tion or sampling only require C 1-smooth transformations. Higher-order smoothness of ﬂows has not been discussed so far and can become a challenge as soon as multi-modal transformations on other topologies than Rd are discussed.
∗J.K and A.K. contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
C k-smoothness of NFs with k > 1 is especially important for applications in physics. Physical models usually come in the form of differential equations, where the derivatives bear a physical meaning that is often crucial to ﬁt, evaluate, or interpret the model. Thus, the construction of expressive, smooth ﬂow architectures will likely open up new avenues of research in this domain.
Boltzmann Generators A recent application of NFs to a physical problem are Boltzmann Genera-tors (BG) [38], which we see as the main application area for the methods introduced in this paper.
They are generative models trained to sample conformations of molecules in equilibrium, which follow a Boltzmann-type distribution µ(x) ∝ exp(−u(x)). Here u is the dimensionless potential energy deﬁned by the molecular system and the thermodynamic state (e.g. Canonical ensemble at a certain temperature) it is simulated in. BGs can be trained by a combination of MLE on possibly biased trajectory data and simultaneous minimization of the reverse KL divergence to the target density µ(x). This bi-directional training scheme can achieve sampling of low-energy equilibrium structures. After training, BGs can be used e.g. for importance sampling or for providing efﬁcient proposals when being used in MCMC applications [45].
In the context of BGs, the negative gradient of the log push-forward density with respect to x corre-sponds to the atomistic forces. Access to well-behaved forces is pivotal in most classical molecular computations: they simplify optimization of molecular models, drive molecular dynamics (MD) simulations, enable computations of macroscopic observables, and facilitate multiscale modeling.
In this work, we will primarily focus on two important implications of smooth ﬂow forces. First, we show that they enable the training of NFs via force matching (FM), i.e. minimizing the force mean-squared error with respect to reference data. In combining FM with density estimation, NFs are pushed to match the distributions and their derivatives, which implicitly leads to favorable regularization. Second, we apply ﬂow forces to drive dynamics simulations. Such simulations are required in many applications to not only sample the target distribution but also compute dynamical observables such as transition rates or diffusivities.
Respecting Topological Constraints Many physical models operate on nontrivial topologies, i.e. the d-dimensional hyper-torus Td, which can become an obstacle in constructing smooth ﬂows.
An important example are BGs for peptides and small proteins which require an internal coordinate (IC) transformation to achieve low-energy sampling of structures [38]. This non-trainable layer transforms Euclidean coordinates x ∈ Rn×3 into a representation given by distances d ∈ [a1, b1] ×
. . . × [an−1, bn−1], angles α ∈ [0, π]n−2 and dihedral torsion angles τ ∈ Tn−3. As molecular energies are often invariant to translation and rotation, IC transformations are useful as they are bijective with the equivalence class of all-atom coordinates that are identical up to global rotations and translations. The learning problem can then be reduced to modeling the joint distribution µ(d, α, τ ) which is supported on the nontrivial topological space XIC := I2n−3 × Tn−3, where I = [0, 1] denotes the closed unit interval.
Recent work [38, 56] suggested to model the density within an open set Ω ⊂ XIC, leverage C∞-smooth normalizing ﬂows deﬁned on Rd and then prevent signiﬁcant mass around singular points using regularizing losses. Such an approach however can lead to bias and ill-behaved training and requires a-priori knowledge of the support of the densities. Later work [9] approached the problem using C 1-smooth splines ﬂows which leads to accurate samples, however results in broken forces.
To overcome these limitations while still beneﬁting from the merits of prior work, such as bi-directional training and fast forward/inverse transformations we formulate the following desiderata for ﬂow transformations on-top of IC layers: (A) They must have support on Id and Td. (B) They should be C∞-smooth. (C) They must allow bi-directional training. (D) Forward and inverse direction must be efﬁcient to evaluate.
Satisfying (D) can be achieved using coupling layers [11, 12]. This reduces the problem to ﬁnding element-wise conditional transformations satisfying (A-C).
Contributions
In this work we propose the following novelties:
• We present a new C∞-smooth transformation with compact support which can simulta-neously be used for expressive transformations on Id as well as the hypertorus Td. This satisﬁes (A) and (B). 2
• We present novel algorithm which allows optimizing non-analytic inverse directions of ﬂows that can only be evaluated via black-box root ﬁnding methods. This satisﬁes (C).
• We show that training of smooth ﬂows through combinations of force matching, density estimation, and energy-based training can achieve nearly perfect equilibrium densities.
• We show that forces of such smooth ﬂows can be used in dynamical simulations. 2