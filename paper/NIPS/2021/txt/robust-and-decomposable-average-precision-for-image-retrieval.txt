Abstract
In image retrieval, standard evaluation metrics rely on score ranking, e.g. av-erage precision (AP). In this paper, we introduce a method for robust and de-composable average precision (ROADMAP) addressing two major challenges for end-to-end training of deep neural networks with AP: non-differentiability and non-decomposability. Firstly, we propose a new differentiable approximation of the rank function, which provides an upper bound of the AP loss and ensures robust training. Secondly, we design a simple yet effective loss function to reduce the de-composability gap between the AP in the whole training set and its averaged batch approximation, for which we provide theoretical guarantees. Extensive experiments conducted on three image retrieval datasets show that ROADMAP outperforms several recent AP approximation methods and highlight the importance of our two contributions. Finally, using ROADMAP for training deep models yields very good performances, outperforming state-of-the-art results on the three datasets.
Code and instructions to reproduce our results will be made publicly available at https://github.com/elias-ramzi/ROADMAP. 1

Introduction
The task of ‘query by example’ is a major prediction problem, which consists in learning a similarity function able to properly rank all the instances in a retrieval set according to their relevance to the query, such that relevant items have the largest similarity. In computer vision, it drives several major applications, e.g. content-based image retrieval, face recognition or person re-identiﬁcation.
Such tasks are usually evaluated with rank-based metrics, e.g. Recall@k, Normalized Discounted
Cumulative Gain (NDCG), and Average Precision (AP). AP is also the de facto metric used in several vision tasks implying a large imbalance between positive and negative samples, e.g. object detection.
In this paper, we address the problem of direct AP training with stochastic gradient-based optimization, e.g. using deep neural networks, which poses two major challenges.
Firstly, the AP loss LAP = 1 − AP is not differentiable and is thus not directly amenable to gradient-based optimization. There has been a rich literature for providing smooth and upper bound surrogate 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) LSupAP ≥ LAP and ∇LSupAP > 0 in this example, in contrast to SmoothAP [2]. This ensures robust training and comes from a new approximation of the rank function. (b) LAP non-decomposability: LAP = 0 in all batches
Bi despite LAP (cid:54)= 0 over the whole (cid:83) i Bi. Lcalibr. controls the absolute scores between batches, such that LROADMAP (cid:54)= 0 in each batch.
Figure 1: Our robust and decomposable Average Precision training (ROADMAP) includes (a) a smooth loss LSupAP upper-bounding LAP, and (b) a calibration loss Lcalibr. supporting decomposability. losses for LAP [43, 21, 22, 6, 25]. More recently, smooth differentiable rank approximations have been proposed [35, 14, 15, 3, 27, 8, 2], but generally lose the important LAP upper bound property.
The second important issue of AP optimization relates to its non-decomposability: LB
AP averaged over batches underestimates LAP on the whole training dataset, which we refer as the decomposability gap. In image retrieval, the attempts to circumvent the problem involve ad hoc methods based on batch sampling strategies [10, 32, 20, 32, 30], or storing all training representations/scores [39, 3, 27, 25], leading to complex models with a large computation and memory overhead.
In this paper, we introduce a method for RObust And DecoMposable Average Precision (ROADMAP), which explicitly addresses the aforementioned challenges of AP optimization.
Our ﬁrst contribution is to propose a new surrogate loss LSupAP for LAP. In particular, we introduce a smooth approximation of the rank function, with a different behaviour for positive and negative examples. By this design, LSupAP provides an upper bound of LAP, and always back-propagates gradients when the correct ranking is not satisﬁed. These two features illustrated in the the toy example on Figure 1a are not fulﬁlled by binning approaches [3, 27] or by SmoothAP [2].
As a second contribution, we propose to improve the non-decomposability in AP training. To this end, we introduce a simple yet effective training objective Lcalibr., which calibrates the scores among different batches by controlling the absolute value of positive and negative samples. We provide a theoretical analysis showing that Lcalibr. decreases the decomposability gap. Figure 1 illustrates how Lcalibr. can be leveraged to improve the overall ranking.
We provide a thorough experimental validation including three standard image retrieval datasets and show that ROADMAP outperforms state-of-the-art methods. We also report the large and consistent gain compared to rank/AP approximation baselines, and we highlight in the ablation studies the importance of our two contributions. Finally, ROADMAP does not entail any memory or computation overhead and remains competitive even with small batches. 2