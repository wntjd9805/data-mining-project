Abstract
Semantic segmentation requires per-pixel prediction for a given image. Typically, the output resolution of a segmentation network is severely reduced due to the downsampling operations in the CNN backbone. Most previous methods employ upsampling decoders to recover the spatial resolution. Various decoders were designed in the literature. Here, we propose a novel decoder, termed dynamic neural representational decoder (NRD), which is simple yet signiﬁcantly more efﬁcient. As each location on the encoder’s output corresponds to a local patch of the semantic labels, in this work, we represent these local patches of labels with compact neural networks. This neural representation enables our decoder to leverage the smoothness prior in the semantic label space, and thus makes our decoder more efﬁcient. Furthermore, these neural representations are dynamically generated and conditioned on the outputs of the encoder networks. The desired semantic labels can be efﬁciently decoded from the neural representations, resulting in high-resolution semantic segmentation predictions. We empirically show that our proposed decoder outperforms the decoder in DeeplabV3+ with only ∼ 30% computational complexity, and achieves competitive performance with the methods using dilated encoders with only ∼ 15% computational costs. Experiments on the
Cityscapes, ADE20K, and PASCAL Context datasets demonstrate the effectiveness and efﬁciency of our proposed method. 1

Introduction
Semantic segmentation is a fundamental task in computer vision, which requires pixel-level classiﬁ-cation on an input image. Fully convolutional networks (FCNs) are the de facto standard approaches to this task, which often consist of an encoder and a decoder. We focus on improving the decoder in this work and assume the encoder to be any backbone networks such as ResNet [HZRS16]. Due to the down-sampling layers (e.g., stridden convolutions or pooling) used in these networks, the encoder’s outputs are often of much lower resolutions than the input image. Thus, a decoder is used to spatially upsample the output. The decoder can simply be a bilinear upsampling, which directly upscales the low-resolution outputs of encoders to desired resolutions, or it can be a sophisticated network with a stack of convolutions and multi-level features. Note that another approach to tackle the issue of low-resolution outputs is the use of dilation convolution as in DeepLab [CPK+17], which balances the need for large receptive ﬁelds and maintaining a higher-resolution feature map. The computational cost is signiﬁcantly heavier introduced by dilation convolutions.
Popular decoders for semantic segmentation include the one in DeepLabV3+ [CPSA17], which fuses the low-level feature maps with 1/4 resolution of the input image, and ReﬁneNet [LMSR17] which gradually combines multi-level feature maps. A potential drawback of these decodes may be that they
∗BZ, YL, ZT contributed equally, and are listed alphabetically. CS is the corresponding author (e-mail: chunhua@me.com). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The overall concept of our neural rep-resentations. The top row is some examples of the semantic label patches. In the neural representa-tions, each patch is represented with a neural net-work gθ(·), as shown in the bottom of this ﬁgure.
The semantic label patch can be recovered by for-warding the coordinate maps (denoted by x and y in the ﬁgure) and the guidance maps (i.e., m in the
ﬁgure) through the network. As stated in our text, using neural representations for these label patches can implicitly take advantage of the smoothness prior in the semantic label patch. do not explicitly exploit the label dependency, thus being less efﬁcient in recovering the pixel-wise prediction accurately.
Let us consider an 8 × 8 local patch on a binary semantic label space, denoted by P ∈ {0, 1}64. If we do not consider any structural correlations in the patch, there would be 264 possibilities for this local patch. However, it is clear to see, for any natural images, the vast majority of the possibilities never exist in the real label map and only a tiny fraction of them are really possible (see Fig. 1).
Considering the redundancy in the labels, most existing decoders that do not explicitly take this into account would be sub-optimal. This motivates us to design a much more effective decoder by exploiting the prior.
A simple approach is dimensionality reduction techniques. As shown in [THSY19], the authors ﬁrst apply principal component analysis (PCA) to the label patches and compress them into low-dimension compact vectors. Next, the network is required to predict these low-dimension vectors, which are eventually restored into the semantic labels by inverting the PCA process. Their method achieves some success. However, the simplicity and linearity assumption of PCA also limit its performance.
The semantic label masks for natural images are not random and follow some distributions, as shown in Fig. 1. Therefore, a good mask representation/decoder must exploit this prior. For computational efﬁciency, we also want the decoder to be in a compact form. Thus, we require the prior to be effectively learnable from data. Recently, many works [SMB+20, MON+19, PNM+20] exploit neural networks to represent 3D shapes. The work of [Mit97] found that neural networks enjoy the inductive bias of smooth interpolation between data points, which means that for two points of the same label, the neural networks tend to assign the same label to the points between them as well.
As a result, we can conclude that the above idea of representing 3D shapes with neural networks can implicitly leverage the local smoothness prior. Therefore, inspired by these works, we can also represent the local patches of semantic labels with neural networks.
To be speciﬁc, as shown in Fig. 1, we represent each local label patch by a compact neural network gθi with a few convolution layers interleaved with non-linearities. The semantic labels of a local patch can be obtained by forwarding the corresponding network with (x, y)-coordinate maps and a guidance map m (explained later) as inputs. Furthermore, the parameters θ of these neural networks, which represent the local label patches, can be dynamically generated with the encoder network in FCNs, and each location on the encoder’s output feature maps is responsible for generating the parameters of the neural network representing the speciﬁc local label patch surrounding it. The dynamic network makes it possible to incorporate the neural representations into the conventional encoder-decoder architectures and enables a compact design of the decoder, resulting in an end-to-end trainable framework. This avoids the separable learning process as done in [THSY19].
Thus, our method is termed dynamic neural representation decoder (NRD) for semantic segmentation.
We summarize our main contributions as follows.
• We propose a novel decoder that is effective and compact for semantic segmentation, to recover the spatial resolutions. For the ﬁrst time, we represent the local label patches using neural networks and make use of dynamic convolutions to parametrize these neural networks. 2
(a) Accuracy vs. Computational cost (b) Boundary comparison
Figure 2: (a) Accuracy vs. computational cost on the validation set of Cityscapes. Our proposed NRD can achieve a better trade-off. (b) Comparison between our proposed NRD and the decoder in DeepLabV3+
[CZP+18]. We can see that NRD is capable of generating improved boundaries.
• Different from previous methods, which often neglect the redundancy in the semantic label space, our proposed decoder NRD can better take advantage of the redundancy, and thus it is able to achieve on par or improved accuracy with signiﬁcantly reduced computational cost. As shown in Fig. 2(a), we achieve a better trade-off between computational cost and accuracy compared to previous methods.
• Compared with the decoder used in classic encoder-decoder model DeeplabV3+ [CZP+18], we achieve an improvement of 0.9% mIoU on the Cityscapes dataset with less than 30% computational cost. Moreover, on the trimaps, where only the pixels near the object boundaries are evaluated, a 1.8% improvement can be obtained. This suggests that NRD can substantially improve the quality of the object boundaries.
Moreover, NRD is even more signiﬁcant than some methods that use dilated encoders, which usually require 4× more computational cost than ours with similar accuracy. For example,
NRD achieves 46.09% mIoU on the competitive ADE20K dataset, which is comparable to that of DeepLabV3+ with a dilated encoder (46.35%) but with only 30% computational cost. We also benchmark our method on the Pascal Context dataset and show excellent performance with much less computational cost. 1.1