Abstract
Knowledge graph embedding models learn the representations of entities and re-lations in the knowledge graphs for predicting missing links (relations) between entities. Their effectiveness are deeply affected by the ability of modeling and inferring different relation patterns such as symmetry, asymmetry, inversion, com-position and transitivity. Although existing models are already able to model many of these relations patterns, transitivity, a very common relation pattern, is still not been fully supported. In this paper, we ﬁrst theoretically show that the transitive relations can be modeled with projections. We then propose the Rot-Pro model which combines the projection and relational rotation together. We prove that
Rot-Pro can infer all the above relation patterns. Experimental results show that the proposed Rot-Pro model effectively learns the transitivity pattern and achieves the state-of-the-art results on the link prediction task in the datasets containing transitive relations. 1

Introduction
Knowledge graph embedding (KGE) aims to learn low-dimensional dense vectors to express the entities and relations in the knowledge graphs (KG). It is widely used in recommendation system, question answering, dialogue systems [5, 17, 7]. The general intuition of KGE is to model and infer relations between entities in knowledge graphs, which has complex patterns such as symmetry, asymmetry, inversion, composition, and transitivity as shown in Table 1.
Many studies dedicate to ﬁnd a method, which is able to model various relation patterns [3, 2, 21, 20].
TransE models relations as translations, aims to model the inversion and composition patterns;
DisMult can model symmetric relations by capturing interactions between head and tail entities and relations. One representative model proposed recently is RotatE [20], which is proved to be able to model symmetry, asymmetry, inversion and composition patterns by modeling relation as a rotation in the complex plane. However, none of them can model all the ﬁve relation patterns, especially the transitivity pattern.
This paper focus on modeling the transitivity pattern. We theoretically show that the transitive relations can be modeled with idempotent transformations, i.e. projections [25]. Any projection matrix is similar to a diagonal matrix with elements in the diagonal being 0 or 1. We design the projection by constraining the similarity matrix to be a rotation matrix, which has less parameters to learn.
In order to model not only transitivity but also other relation patterns shown in Table 1, we propose the Rot-Pro model which combines the projection and relational rotation together. We theoretically prove that Rot-Pro can infer the symmetry, asymmetry, inversion, composition, and transitivity
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Common relation patterns.
Relation pattern Deﬁnition
Symmetry
Asymmetry
Inversion
Composition
Transitivity if (h, r, t), then (t, r, h) if (h, r, t), then¬(t, r, h) if r = p−1 and (h, r, t), then (t, p, h) if (r = r1 ◦ · · · ◦ rn) ∧ (h, r1, u1) ∧ (u1, r2, u2)
. . . ∧ (un−1, rn, t), then (h, r, t) if (a, r, b) and (b, r, c) , then(a, r, c)
Table 2: The supported relation patterns of several models [20].
Symmetry Asymmetry
Inversion Composition Transitivity
TransE
DistMult
ComplEx
RotatE
Rot-Pro (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) (cid:37) (cid:37) (cid:33) (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) (cid:33) patterns. Experimental results show that the Rot-Pro model can effectively learn the transitivity pattern. The Rot-Pro model achieves the state-of-the-art results on the link prediction task in the
Countries dataset containing transitive relations and outperforms other models in the YAGO3-10 and
FB15k-237 dataset. 2