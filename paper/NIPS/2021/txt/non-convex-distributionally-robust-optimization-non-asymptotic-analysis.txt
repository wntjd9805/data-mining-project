Abstract
Distributionally robust optimization (DRO) is a widely-used approach to learn models that are robust against distribution shift. Compared with the standard optimization setting, the objective function in DRO is more difﬁcult to optimize, and most of the existing theoretical results make strong assumptions on the loss function.
In this work we bridge the gap by studying DRO algorithms for general smooth non-convex losses. By carefully exploiting the speciﬁc form of the DRO objective, we are able to provide non-asymptotic convergence guarantees even though the objective function is possibly non-convex, non-smooth and has unbounded gradient noise.
In particular, we prove that a special algorithm called the mini-batch normalized gradient descent with momentum, can ﬁnd an (cid:15)-ﬁrst-order stationary point within O((cid:15)−4) gradient complexity. We also discuss the conditional value-at-risk (CVaR) setting, where we propose a penalized DRO objective based on a smoothed version of the CVaR that allows us to obtain a similar convergence guarantee. We ﬁnally verify our theoretical results in a number of tasks and ﬁnd that the proposed algorithm can consistently achieve prominent acceleration. 1

Introduction
For a classical machine learning problem, the goal is typically to train a model over a training set that achieves good performance on a test set, where both the training set and the test set are drawn from the same distribution P . While such an assumption is reasonable and simple for theoretical analysis, it is often not the case in real applications. For example, this setting may be improper when there is a gap between training and test distribution (e.g. in domain adaptation tasks) [Zhang et al., 2021], when there is severe class imbalance in the training set [Sagawa et al., 2020], when fairness in minority groups is an important consideration [Hashimoto et al., 2018], or when the deployed model is exposed to adversarial attacks [Sinha et al., 2018].
Distributionally robust optimization (DRO), as a popular approach to deal with the above situations, has attracted great interest for the machine learning research communities in recent years. In contrast to classic machine learning problems, for DRO it is desired that the trained model still has good performance under distribution shift. Speciﬁcally, DRO proposes to minimize the worst-case loss over a set of probability distributions Q around P . This can be formulated as the following constrained optimization problem [Rahimian and Mehrotra, 2019, Shapiro, 2017]: minimizex∈X Ψ(x) := sup
Eξ∼Q [(cid:96)(x; ξ)]
Q∈U (P ) (1)
∗Equal Contribution, alphabetical order.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where x ∈ X is the parameter to be optimized, ξ is a sample randomly drawn from distribution Q, and (cid:96)(x; ξ) is the loss function so that Eξ∼Q [(cid:96)(x; ξ)] is the expected loss over distribution Q. The
DRO objective Ψ(x) is therefore the worst-case loss when the distribution P is shifted to Q. The set
U(P ) is called the uncertainty set and typically deﬁned as
U(P ) := {Q : d(Q, P ) ≤ (cid:15)} (2) where d measures the distance between two probability distributions, and the positive number (cid:15) corresponds to the magnitude of the uncertainty set.
Instead of imposing a hard constrained uncertainty set, sometimes it is more preferred to use a soft penalty term, resulting in the penalized DRO problem [Sinha et al., 2018]: minimizex∈X Ψ(x) := sup
Q
{Eξ∼Q [(cid:96)(x; ξ)] − λd(Q, P )} (3) where λ > 0 is the regularization coefﬁcient.
There are many possible choices of d. A detailed discussion of different distance measures and their properties can be found in Rahimian and Mehrotra [2019]. In this paper we consider a general class of distances d called the ψ-divergence, which is a popular choice in DRO literature [Namkoong and
Duchi, 2016, Shapiro, 2017]. Speciﬁcally, for a non-negative convex function ψ such that ψ(1) = 0 and two probability distributions P, Q such that Q is absolutely continuous w.r.t. P , the ψ-divergence between Q and P is deﬁned as dψ(Q, P ) := (cid:90)
ψ (cid:19) (cid:18) dQ dP dP. which satisﬁes dψ(Q, P ) ≥ 0 and dψ(Q, P ) = 0 if Q = P a.s.
The main focus of this paper is to study efﬁcient ﬁrst-order optimization algorithms for DRO problem (3) for non-convex losses (cid:96)(x, ξ). While non-convex models (especially deep neural networks) have been extensively used in DRO setting (e.g. Sagawa et al. [2020]), theoretical analysis about the convergence speed is still lacking. Most previous works (e.g. Levy et al. [2020]) assume the loss (cid:96)(·, ξ) is convex, and in this case (3) is equivalent to a convex optimization problem (see Section 2 for details). Recently some works provide convergence rates of algorithms for non-convex losses in certain special cases, e.g. the divergence measure ψ is chosen as the conditional-value-at-risk (CVaR) and the loss function has some nice structural properties [Soma and Yoshida, 2020, Kalogerias, 2020]. Gürbüzbalaban et al. [2020] considered a more general setting but only proved an asymptotic convergence result for non-convex DRO.
Compared with these works, we provide the ﬁrst non-asymptotic analysis of optimization algorithms for DRO with general smooth non-convex losses (cid:96)(x, ξ) and general ψ-divergence. In this setting, there are two major difﬁculties we must encounter: (i) the DRO objective Ψ(x) is non-convex and can become arbitrarily non-smooth, causing standard techniques in smooth non-convex optimization fail to provide a good convergence guarantee; (ii) the noise of the stochastic gradient of Ψ(x) can be arbitrarily large and unbounded even if we assume the gradient of the inner loss (cid:96)(x, ξ) has bounded variance. To tackle these challenges, we propose to optimize the DRO objective using mini-batch normalized SGD with momentum, and we are able to prove an O((cid:15)−4) complexity of this algorithm.
The core technique here is to exploit the speciﬁc structure of Ψ(x), which shows that (i) the DRO objective satisﬁes a generalized smoothness condition [Zhang et al., 2020a,b] and (ii) the variance of the stochastic gradient can be bounded by the true gradient. This motivates us to adopt the special algorithm that combines gradient normalization and momentum techniques into SGD, by which both non-smoothness and unbounded noise can be tackled, ﬁnally resulting in an O((cid:15)−4) complexity similar to standard smooth non-convex optimization.
The above analysis applies to a broad class of divergence functions ψ. We further discuss special cases when ψ has additional properties. In particular, to handle the CVaR case (a non-differentiable loss), we propose a divergence function which is a smoothed variant of CVaR and is further Lipschitz.
In this case we show that a convergence guarantee can be established using vanilla SGD, and an similar complexity bound holds.
We highlight that the algorithm and analysis in this paper are not limited to DRO setting, and are described in the context of a general class of optimization problem. Our analysis clearly demonstrates the effectiveness of gradient normalization and momentum techniques in optimizing ill-conditioned objective functions. We believe our result can shed light on why some popular optimizers, in particular
Adam [Kingma and Ba, 2015], often exhibit superior performance in real applications. 2
Contributions. We summarize our main results and contributions below. Let ψ∗ be the conjugate function of ψ (see Deﬁnition 2.3). For non-convex optimization problems, since obtaining the global minima is NP-hard in general, this paper adopts the commonly used (relaxed) criteria: to ﬁnd an (cid:15)-approximate ﬁrst-order stationary point of the function Ψ (see Deﬁnition 2.5). We measure the complexity of optimization algorithms by the number of computations of the stochastic gradient
∇(cid:96)(x, ξ) to reach an (cid:15)-stationary point.
• Assuming that ψ∗ is smooth and the loss (cid:96) is Lipschitz and smooth (possibly non-convex or unbounded), we show in Section 3.2 that the mini-batch normalized momentum algorithm (cf. Algorithm 1) has a complexity of O((cid:15)−4).
• Assuming that ψ∗ is further Lipschitz, in Section 3.4 we prove that vinilla SGD sufﬁces to achieve the O((cid:15)−4) complexity. As a special case, we propose a new divergence which is a smoothed approximation of CVaR.
• We conduct experiments to verify our theoretical results. We observe that our proposed methods signiﬁcantly accelerate the optimization process, and also demonstrates superior test performance. 1.1