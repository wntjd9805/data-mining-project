Abstract
The adversarial training is a popular tool to remedy the vulnerability of deep learn-ing models against adversarial attacks, and there is rich theoretical literature on the training loss of adversarial training algorithms. In contrast, this paper studies the algorithmic stability of a generic adversarial training algorithm, which can fur-ther help to establish an upper bound for generalization error. By ﬁguring out the stability upper bound and lower bound, we argue that the non-differentiability issue of adversarial training causes worse algorithmic stability than their natu-ral counterparts. To tackle this problem, we consider a noise injection method.
While the non-differentiability problem seriously affects the stability of adversar-ial training, injecting noise enables the training trajectory to avoid the occurrence of non-differentiability with dominating probability, hence enhancing the stability performance of adversarial training. Our analysis also studies the relation between the algorithm stability and numerical approximation error of adversarial attacks. 1

Introduction
Successful machine learning algorithms require not only a good empirical performance but also generalizing well to unseen data. For the robustness towards unseen data, empirical experiments show that deep learning models can be fragile and vulnerable against adversarial input (Biggio et al., 2013; Szegedy et al., 2014). To set an example, in image recognition problems, a deep neural network will predict a wrong label when the testing image is slightly altered, while the change is barely recognizable by human eyes (Papernot et al., 2016a).
Related research efforts in adversarial learning include designing adversarial attacks in various ap-plications (Papernot et al., 2016a,b; Moosavi-Dezfooli et al., 2016), detecting attacked samples (Tao et al., 2018; Ma and Liu, 2019), and modiﬁcations on the training process to obtain adversarially robust models, i.e., adversarial training (Shaham et al., 2015; Madry et al., 2017; Jalal et al., 2017).
However, although adversarial training improves the adversarial robustness during testing, its gener-alization performance is still poor. While Yin et al. (2018) presented that the adversarial Rademacher complexity is never smaller than its natural counterpart, Schmidt et al. (2018); Zhai et al. (2019) ar-gued that a better adversarial generalization requires more labeled/unlabeled data.
In the literature of natural DNN optimization via iterative gradient moves, the empirical loss at each iteration can be characterized by convergence rate analysis, yet generalization properties are not well understood. To characterize the generalization error, one popular way is to study the algorithmic sta-bility. Algorithmic stability is ﬁrst considered by Kearns and Ron (1999); Bousquet and Elisseeff (2001, 2002). Later, Hardt et al. (2016) explored the connection between algorithmic stability and generalization performance of gradient-type optimization. Some follow-up research studies the sta-bility for different classes of algorithms, or relax the deﬁnition of stability to generalize its usage, see Ramezani-Kebrya et al. (2018); Charles and Papailiopoulos (2018); Kuzborskij and Lampert (2018); Zhou et al. (2018); Lei and Ying (2020); Ho et al. (2020); Madden et al. (2020). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In general, there are two ways to utilize algorithmic stability. On the one hand, as showed by Hardt et al. (2016), the algorithmic stability provides an upper bound for the generalization error; hence it will be useful when establishing the convergence of generalization error. On the other hand, the algorithmic stability itself is also a measure that evaluates the performance of an algorithm.
Our work extends algorithmic stability analysis to adversarial training. Our contributions are:
•
•
•
•
Through ﬁguring out the stability upper bound and lower bound, we argue that adversarial training leads to poor algorithmic stability even the clean loss is smooth. To solve this problem, we propose to inject noise into the adversarial training process. Although some existing works proposed the usage of noise injection, we highlight that such a method is more meaningful in adversarial training than its natural counterpart. Theoretical justiﬁca-tion of the noise injection method is provided for a wide range of data-generating models in several tasks, including both linear regression and logistic classiﬁcation.
Noticing that, in practice, adversarial attacks are mostly approximated via numerical meth-ods, e.g., fast gradient method (FGM) or projected gradient method (PGD), our theory investigates the role of accuracy of attack approximation for the stability of adversarial training algorithms. In short, a more accurate attack leads to better stability upper bound.
L2,
L1
The effectiveness of noise-injected adversarial training is further generalized to the attack. Compared with training algorithm is generally less stable.
L1
Beyond the theoretical analysis under simple models, we provide a theory in two-layer
ReLU network with lazy training (training the hidden layer) and observe the effectiveness of the noise injection method. We also obtain empirical evidence that for deep neural networks model, proper forms of noise injection and more accurate attack calculation (e.g.,
PGD-k over FGM) improve the generalization error. 2