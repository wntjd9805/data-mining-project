Abstract
We study the problem of constructing coresets for clustering problems with time series data. This problem has gained importance across many ﬁelds including biology, medicine, and economics due to the proliferation of sensors for real-time measurement and rapid drop in storage costs. In particular, we consider the setting where the time series data on N entities is generated from a Gaussian mixture model with autocorrelations over k clusters in Rd. Our main contribution is an algorithm to construct coresets for the maximum likelihood objective for this mixture model. Our algorithm is efﬁcient, and, under a mild assumption on the covariance matrices of the Gaussians, the size of the coreset is independent of the number of entities N and the number of observations for each entity, and depends only polynomially on k, d and 1/ε, where ε is the error parameter. We empirically assess the performance of our coresets with synthetic data. 1

Introduction
A multivariate time series dataset, represented as X = (cid:8)Xi = (xi1, . . . , xi,Ti) ⊂ RTi×d | i ∈ [N ](cid:9), where N is the number of entities, Ti is the number of time periods corresponding to entity i and d is the number of features, tracks features of a cross-section of entities longitudinally over time. Such data is also referred to as panel data [8] and has seen rapid growth due to proliferation of sensors, IOT and wearables that facilitate real time measurement of various features associated with entities and the rapid drop in storage costs. Speciﬁc examples of time series data include biomedical measurements (e.g., blood pressure and electrocardiogram), epidemiology and diffusion through social networks, weather, consumer search, content browsing and purchase behaviors, mobility through cell phone and GPS locations, stock prices and exchange rates in ﬁnance [2].
Computational problems of interest with time series data include pattern discovery [49], regres-sion/prediction [39], forecasting [9], and clustering [47, 31, 2] which arises in applications such as anomaly detection [46]. Though clustering is a central and well-studied problem in unsupervised learning, most standard treatments of clustering tend to be on static data with one observation per entity [35, 11]. Time series clustering introduces a number of additional challenges and is an active area of research, with many types of clustering methods proposed [15]; direct methods on raw data, indirect methods based on features generated from the raw data, and model based clustering where the data are assumed to be generated from a model. For surveys of time series clustering, see [47, 2].
We focus on model-based time series clustering using a likelihood framework that naturally extends clustering with static data [11] to time series data [47], where each multivariate time series is generated by one of k different parametrically speciﬁed models. For a survey of the importance of this sub-literature and its various real-world applications, see [30]. A prevalent approach is to assume a ﬁnite mixture of data generating models, with Gaussian mixture models being a common speciﬁcation
[50, 53]. Roughly, the problem is to partition X probabilistically into k clusters, grouping those series 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(cid:80) i∈[N ] ln (cid:80) generated by the same time series model into one cluster. Generically, the model-based k-clustering l∈[k] αl · pi(X |θ(l)), where αl is the problem can be formulated as: arg maxα,θ(1),...,θ(k) probability of a time series belonging to cluster l and pi(X |θ(l)) is the likelihood of the data of entity i, given that the data is generated from model l (with parameters θ(l)). Temporal relationships in time series are commonly modeled as autocorrelations (AR)/moving averages (MA) [20, 61] that account for correlations across observations, and hidden Markov models (HMM) where the underlying data generating process is allowed to switch periodically [21, 62]. This paper focuses on the case when the data generation model for each segment l ∈ [k] is a multivariate Gaussian with autocorrelations. More formally, the generative model for each cluster is from a Gaussian mixture: xit := µ(l) + eit, where eit = N (0, Σ(l)) + Λ(l)ei,t−1, where N (0, Σ(l)) captures the mixture of Gaussian distributions from which entity level observations are drawn, and Λ captures the correlation between two successive observations through an AR(1) process [32, 44]. Overall, this model can be represented with cluster level model parameters θ(l) = {µ(l), Σ(l), Λ(l)} and cluster probability αl.
Time series datasets are also much larger than static datasets [47]. For instance, as noted in [42], ECG (electrocardiogram) data requires 1 GB/hour, a typical weblog requires 5 GB/week. Such large sizes lead to high storage costs and also make it necessary to work with a subset of the data to conduct the clustering analysis to be computationally practical. Further, long time series data on entities also entails signiﬁcant data security and privacy risks around the entity as it requires longer histories of entity behaviors to be stored and maintained [25, 41]. Thus, there is signiﬁcant value for techniques that can produce approximately similar clustering results with smaller samples as with complete data.
Coresets have emerged as an effective tool to both speed up and reduce data storage by taking into account the objective and carefully sampling from the full dataset in a way that any algorithm run on the sampled set returns an approximate solution for the original objective with guaranteed bounds on approximation error [34]. Coresets have been developed for both unsupervised clustering (e.g., k-means, Gaussian mixture models) and supervised machine learning (e.g., regression) methods for static data; for surveys on coresets for static data, see [7, 22]. A natural question is whether coresets can be also useful for addressing the time series clustering problem. Recently, there is evidence that small coresets can be efﬁciently constructed for time series data, albeit for regressions with time series (panel) data [37]. However, as we explain in Section 2, there are challenges in extending coresets for static data clustering and time series (panel) data regressions for time series clustering.
Our contributions. We study coresets for a general class of time series clustering in which all entities are drawn from Gaussian mixtures with autocorrelations (as described above; see Problem 3.1). We
ﬁrst present a deﬁnition of coresets for the log-likelihood k-clustering objective for this mixture model. One issue is that this objective cannot be decomposed into a summation of entity-time objectives due to the interaction of the ln term from log-likelihood and the exp term from Gaussians (Problem 3.1). To estimate this objective using a coreset, we introduce an analogous clustering objective on a subset of data (Deﬁnition 3.3). Our main result is an algorithm to construct coresets for this objective for the aforementioned mixture model (Theorem 4.3). Our algorithm is efﬁcient (linear on both N and (cid:80) i∈[N ] Ti) and, assuming that the condition number of the covariance matrices of the
Gaussians (Σ(l)) are bounded and the eigenvalues of the autocorrelation matrices (Λ(l)) are bounded away from 1 (Assumption 4.1), the size of our coreset is independent of the number of entities N and the number of observations Tis; it only polynomially depends on k, d and 1/ε, where ε is the error parameter (Theorem 4.3).
Our coreset construction (Algorithm 1) leverages the Feldman-Langberg (FL) framework [27]. Due to multiple observations for each entity and associated autocorrelations, the objective is non-convex and quite complicated in contrast to the static setting where the objective only contains one observation drawn from Gaussian mixtures for each entity. Thus, bounding the “sensitivity” (Lemma 4.6) of each entity, which is a key step in coreset construction using the FL-framework, becomes challenging.
We handle this technical difﬁculty by 1) upper-bounding the maximum effect of covariances and autocorrelations by using the observation that the gap between the clustering objectives with and without Σ and Λ is always constant, and 2) reducing the Gaussian mixture time series clustering problem to a certain k-means clustering problem whose total sensitivity is upper bounded.
Empirically, we assess the performance of our coreset on synthetic data for a three cluster problem (Section 5), and compare the performance with two benchmarks: uniform sampling and a static coreset benchmark (LFKF [48]) in which we do not consider autocorrelations and regard time series 2
data as static. We ﬁnd that our coreset performs better relative to uniform sampling and LFKF on both data storage and computation speed for a range of accuracy guarantees: To achieve a similar ﬁt with the full data, our coreset needs fewer entity-time observations (<40%); the computation time for a given level of accuracy reduces by a 3x-22x factor when compared to uniform sampling and LFKF.
Moreover, our coreset speeds up the computation time relative to the full data by 14x-171x. We note that the performance advantage of our coreset is greater when there is more time series data relative to entities (Ti (cid:29) N ). 2