Abstract
An overarching goal in machine learning is to build a generalizable model with a small number of samples. To this end, overparameterization has been the subject of immense interest to explain the generalization ability of deep nets even when the size of the dataset is smaller than that of the model. While prior literature focuses on the classical supervised setting, this paper aims to demystify overparameterization for meta-learning. Here we have a sequence of linear-regression tasks and we ask: (1) Given earlier tasks, what is the optimal linear representation of features for a new downstream task? and (2) How many samples do we need to build this representation? This work shows that surprisingly, overparameterization arises as a natural answer to these fundamental meta-learning questions. Speciﬁcally, for (1), we ﬁrst show that learning the optimal representation coincides with the problem of designing a task-aware regularization to promote inductive bias. This inductive bias explains how the downstream task actually beneﬁts from overparameterization, in contrast to prior works on few-shot learning. For (2), we develop a theory to explain how feature covariance can implicitly help reduce the sample complexity well below the degrees of freedom and lead to small estimation error. We then integrate these ﬁndings to obtain an overall performance guarantee for our meta-learning algorithm. Numerical experiments on real and synthetic data verify our insights on overparameterized meta-learning. 1

Introduction
In a multitude of machine learning (ML) tasks with limited data, it is crucial to build accurate models in a sample-efﬁcient way. Constructing a simple yet informative representation of features is a critical component of learning a model that generalizes well to an unseen test set. The ﬁeld of meta-learning dates back to [8, 4] and addresses this challenge by transferring insights across distinct but related tasks. Usually, the meta-learner ﬁrst (1) learns a feature-representation from previously seen tasks and then (2) uses this representation to succeed at an unseen task. The ﬁrst phase is called representation learning and the second is called few-shot learning. Such information transfer between tasks is the backbone of modern transfer and multitask learning and ﬁnds ubiquitous applications in image classiﬁcation [14], machine translation [6] and reinforcement learning [17].
Recent literature in ML theory has posited that overparameterization can be beneﬁcial to gener-alization in traditional single-task setups for both regression [28, 38, 3, 32, 29] and classiﬁcation
[31, 30] problems. Empirical literature in deep learning suggests that overparameterization is of interest for both phases of meta-learning as well. Deep networks are stellar representation learners despite containing many more parameters than the sample size. Additionally, overparameterization 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is observed to be beneﬁcial in the few-shot phase for transfer-learning in Figure 1(a). A ResNet-50 network pretrained on Imagenet was utilized to obtain a representation of R features for classiﬁca-tion on CIFAR-10. All layers except the ﬁnal (softmax) layer are frozen and are treated as a ﬁxed feature-map. We then train the ﬁnal layer of the network for the downstream task which yields a linear classiﬁer on pretrained features. The ﬁgure plots the effect of increasing R on the test error on CIFAR-10, for different choices of training size n2. For each choice of n2, increasing R beyond n2 is seen to reduce the test-error. These ﬁndings are corroborated by [17] (MAML) and [37], who successfully use a transfer learning method that adapts a pre-trained model, with 112980 parameters, to downstream tasks with only 1-5 new training samples.
Figure 1: Illustration of the beneﬁt of overparameterization in the few-shot phase. (a) Double-descent in transfer learning: dashed lines indicate the location where the number of features R exceed the number of training points; i.e., the transition from under to over-parameterization. The experimental details are contained in the supplement. (b) Illustration of the beneﬁt of using Weighted minL2-interpolation in Deﬁnition 3 (blue). See Remark 1 for details and discussion.
In Figure 1(b), we consider a sequence of linear regression tasks and plot the few-shot error of our proposed projection and eigen-weighting based meta-learning algorithm for a ﬁxed few-shot training size, but varying dimensionality of features. The resulting curve looks similar to Figure 1(a) and suggests that the observations regarding overparameterization for meta-learning in neural networks can, to a good extent, be captured by linear models, thus motivating their detailed study. This aligns with trends in recent literature: while deep nets are nonlinear, recent advances show that linearized problems such as kernel regression (e.g., via neural tangent kernel [20, 16, 23, 34, 12]) provide a good proxy to understand some of the theoretical properties of practical overparameterized deep nets.
However, existing analysis of subspace-based meta-learning algorithms for both the representation learning and few-shot phases of linear models have typically focused on the classical underparame-terized regime. These works (see Paragraphs 2-3 of Sec. 1.2) consider the case where representation learning involves projection onto a lower-dimensional subspace. On the other hand, recent works on double descent shows that an overparameterized interpolator beats PCA-based method. to build upon these results to develop a theoretical understanding of overparameterized meta-learning1. 1.1 Our contributions
This paper studies meta-learning when each task is a linear regression problem, similar in spirit to
[36, 22]. In the representation learning phase, the learner is provided with training data from T distinct tasks, with n1 training samples per task: using this data, it selects a matrix Λ ∈ Rd×R with arbitrary
R to obtain a linear representation of features via the map x → Λ(cid:62)x. In the few-shot learning phase, the learner faces a new task with n2 training samples and aims to use the representation Λ(cid:62)x to aid prediction performance.
We highlight that obtaining the representation consists of two steps: ﬁrst the learner projects x onto R basis directions, and then performs eigen-weighting of each of these directions, as shown in
Figure 2(b). The overarching goal of this paper is to propose a scheme to use the knowledge gained from earlier tasks to choose Λ that minimizes few-shot risk. This goal enables us to engage with important questions regarding overparameterization: 1The code for this paper is in https://github.com/sunyue93/Rep-Learning. 2
Q1: What should the size R and the representation Λ be to minimize risk at the few-shot phase?
Q2: Can we learn the Rd dimensional representation Λ with N (cid:28) Rd samples?
The answers to the questions above will shed light on whether overparameterization is beneﬁcial in few-shot learning and representation learning respectively. Towards this goal, we make several contributions to the ﬁnite-sample understanding of linear meta-learning, under assumptions discussed in Section 2. Our results are obtained for a general data/task model with arbitrary task covariance
Σβ and feature covariance ΣF which allows for a rich set of observations.
Optimal representation for few-shot learning. As a stepping stone towards the goal of characteriz-ing few-shot risk for different Λ, in Section 3 we ﬁrst consider learning with known covariances ΣT and ΣF respectively (Algorithm 1). Compared to projection-only representations in previous works (see Paragraphs 2-3 of Sec. 1.2), our scheme applies eigen-weighting matrix Λ∗ to incentivize the optimizer to place higher weight on promising eigen-directions. This eigen-weighting procedure has been shown in the single-task case to be extremely crucial to avail the beneﬁt of overparameterization
[5, 29, 32]: it captures an inductive bias that promotes certain features and demotes others. We show that the importance of eigen-weighting extends to the multi-task case as well.
Canonical task covariance. Our analysis in Section 3 also reveals that, the optimal subspace and rep-F ΣT Σ1/2 resentation matrix are closed-form functions of the canonical task covariance ˜ΣT = Σ1/2
F , which captures the feature saliency by summarizing the feature and task distributions.
ΣF
ΣT
˜ΣT n1
T
N n2
Λ
Feature covariance
Task covariance
Canonical task covariance
Samples per each earlier task
Number of earlier tasks
Total sample size T × n1
Samples for new task
Eigen-weighting matrix
Representation learning. In practice, task and fea-ture covariances (and hence the canonical covariance) are rarely known apriori. However, we can estimate the principal subspace of the canonical task covari-ance ˜ΣT (which has a degree of freedom (DoF) of
Ω(Rd)) from data. In Section 4 we ﬁrst present em-pirical evidence that feature covariance ΣF is “pos-itively correlated” with ˜ΣT . Then we propose an efﬁcient algorithm based on Method-of-Moments (MoM), and show that the sample complexity of rep-resentation learning is well below O(Rd) due to the inductive bias. Our sample complexity bound de-pends on interpretable quantities such as effective ranks ΣF , ˜ΣT and improves over prior art (e.g., [22, 36]), even though the prior works were special-ized to low-rank ˜ΣT and identity ΣF (see Table 2).
End to end meta-learning guarantee. In Section 5, we consider the generalization of Section 3, where we have only estimates of the covariances instead of perfect knowledge. This leads to an overall meta-learning guarantee in terms of Λ∗, N and n2 and uncovers a bias-variance tradeoff: As N decreases, it becomes more preferable to use a smaller R (more bias, less variance) due to inaccurate estimate of the weak eigen-directions of ˜ΣT . In other words, we ﬁnd that overparameterization is only beneﬁcial for few-shot learning if the quality of representation learning is sufﬁciently good. This explains why, in practice, increasing the representation dimension may not help reduce few-shot risk beyond a certain point (see Fig. 5).
Table 1: Main notation 1.2