Abstract
Deep neural networks are powerful machines for visual pattern recognition, but reasoning tasks that are easy for humans may still be difﬁcult for neural models.
Humans possess the ability to extrapolate reasoning strategies learned on simple problems to solve harder examples, often by thinking for longer. For example, a person who has learned to solve small mazes can easily extend the very same search techniques to solve much larger mazes by spending more time. In computers, this behavior is often achieved through the use of algorithms, which scale to arbitrarily hard problem instances at the cost of more computation. In contrast, the sequential computing budget of feed-forward neural networks is limited by their depth, and networks trained on simple problems have no way of extending their reasoning to accommodate harder problems. In this work, we show that recurrent networks trained to solve simple problems with few recurrent steps can indeed solve much more complex problems simply by performing additional recurrences during inference. We demonstrate this algorithmic behavior of recurrent networks on preﬁx sum computation, mazes, and chess. In all three domains, networks trained on simple problem instances are able to extend their reasoning abilities at test time simply by “thinking for longer.” 1

Introduction
In computational theories of mind, an analytical problem is tackled by embedding it in “working memory” and then iteratively applying transformations to the representation until the problem is solved [Baddeley, 2012, Baddeley and Hitch, 1974]. Iterative processes underlie the human ability to solve sequential reasoning problems, such as complex question answering, proof writing, and even object classiﬁcation [Liao and Poggio, 2016, Kar et al., 2019]. They also enable humans to extrapolate their knowledge to solve problems of potentially unbounded complexity, including harder problems than they have seen before, by thinking for longer. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This work examines whether recurrent neural networks trained on easy problems can extrapolate their knowledge to solve hard problems. We ﬁnd that recurrent networks can indeed generalize to harder problems simply by increasing their test time iteration budget (i.e., thinking for longer than they did at train time). Moreover, we ﬁnd that the performance of recurrent models improves as they recur for more iterations, even without adding parameters or re-training in the new, more challenging problem domain. This ability is speciﬁc to recurrent networks, as standard feed-forward networks rely on layer-speciﬁc behaviors that cannot be repeated to extend their reasoning power.
The behavior we observe in recurrent networks falls outside the classical notions of generalization in which models are trained and tested on the same distribution. Because we train and test on problems of different sizes/difﬁculties, our training and test distributions are disjoint, and systems must extrapolate to solve problems from the test distribution. Outside the ﬁeld of machine learning, computers achieve a functionally similar extrapolation ability through the use of algorithms, which encode the process required to solve a class of problems, and can therefore scale to problems of arbitrary size, albeit with longer runtime.
By training networks to solve problems iteratively, we hope to ﬁnd models that encode a scalable method for solving problems rather than memorizing a mapping between input features and outputs.
In short, the goal is to create recurrent architectures that are capable of learning an algorithm.
Our focus is on three reasoning problems that are classically solved using hand-crafted algorithms: computing preﬁx sums, solving mazes, and playing chess. Sequential reasoning tasks like these are ideal for our study because one can directly quantify the difﬁculty of a problem instance. In the case of mazes, for example, we can easily swap to a more challenging domain by increasing the size of the search space.
For each class of problems, recurrent networks are trained on a set of “easy” problems using a ﬁxed number of iterations of the recurrent module on the forward pass. After training is complete, we assess whether our models exhibit logical extrapolation behaviors by testing them on “hard” problems, with varying numbers of additional iterations. Remarkably, models trained on easy examples exhibit little extrapolative behavior until their iteration budget is increased — generalizing to harder problems requires thinking deeper. Moreover, we ﬁnd recurrent models tested with a sufﬁcient number of extra iterations outperform the inﬂexible feed-forward models of comparable depth, often by a wide margin. Finally, we visualize the iterative behavior of the recurrent module to gain insights into the problem solving process they discover. 1.1