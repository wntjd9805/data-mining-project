Abstract
Sparse variational Gaussian process (SVGP) methods are a common choice for non-conjugate Gaussian process inference because of their computational beneﬁts.
In this paper, we improve their computational efﬁciency by using a dual parame-terization where each data example is assigned dual parameters, similarly to site parameters used in expectation propagation. Our dual parameterization speeds-up inference using natural gradient descent, and provides a tighter evidence lower bound for hyperparameter learning. The approach has the same memory cost as the current SVGP methods, but it is faster and more accurate. 1

Introduction
Gaussian processes (GPs, [31]) have become ubiquitous models in the probabilistic machine learning toolbox, but their application is challenging due to two issues: poor O(n3) scaling in the number of data points, n, and challenging approximate inference in non-conjugate (non-Gaussian) models. In recent years, variational inference has become the go-to solution to overcome these problems, where sparse variational GP methods [35] tackle both the non-conjugacy and high computation cost. The computation is reduced to O(nm2) by using a small number of m ≪ n inducing-input locations. For large problems, the SVGP framework is a popular choice as it enables fast stochastic training, and reduces the cost to O(m3 + nbm2) per step, where nb is the batch size [10, 11].
It is a common practice in SVGP to utilize the standard mean-covariance parameterization which requires O(m2) memory. Inference is carried out by optimizing an objective L(q) that uses a Gaussian distribution q parameterized by parameters ξ = (m, L) where m is the mean and L is the Cholesky factor of the covariance matrix. We will refer to the SVGP methods using such parameterization as the q-SVGP methods. The advantage of this formulation is that, for log-concave likelihoods, the objective is convex and gradient-based optimization works well [2]. The optimization can further be improved by using natural gradient descent (NGD) which is shown to be less sensitive to learning rates [33]. The NGD algorithm with q-SVGP parameterization is currently the state-of-the-art and available in the existing software implementations such as GPﬂow [26] and GPyTorch [9].
An alternate parameterization to the q-SVGP parameterization is the one where every likelihood is assigned two sets of parameters which require O(n) memory. Existence of such parameterizations was initially shown by Csató and Opper [8] for general GP models, and later on extended to variational
∗Both authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
2
. 8 6
− 4
. 8 6
− 6
. 8 6
−
)
θ
,
) d l o
θ (
∗
ξ (
ξ
L
,
O
B
L
E d n u o b r e t h g i
T t-SVGP (ours) q-SVGP t-SVGP (ours) q-SVGP q-SVGP (whitened)
ELBO and gradient match 5 1 0 1 5
) e d u t i n g a m ( 2
θ 5 4 3 9 8 7 6 5 4 2 3 2 0.8 0.9
θ1 (length-scale) 1 1 0 0.2 0.4 0.6 0.8 1
θ
Figure 1: The dual parameterization gives a tighter bound compared to standard SVGP (whitened/ unwhitened) as shown on the left for a GP classiﬁcation tasks and varying kernel magnitude θ. The tighter bound helps take longer steps, speeding up convergence of hyperparameters, as shown on the right for the banana classiﬁcation task with coordinate ascent w.r.t. θ and variational parameters ξ. objectives for GPs [28, 29], and also to latent Gaussian models by using Lagrangian duality [20, 16].
Due to this later connection, we refer to this parameterization as the dual parameterization where the parameters are the Lagrange multipliers, ensuring that the marginal mean and variance of each latent function is consistent to the marginals obtained by the full GP [16]. Expectation propagation (EP,
[27]) too naturally employ such parameterizations, but by using site parameters. Although unrelated to duality, such methods are popular for GP inference, and due to this connection, we will refer to the methods using dual parameterization as t-SVGP (the letter ‘t’ refers to the sites). To the best of our knowledge, the dual parameterization for SVGP has only been used to speed up computation and inference for the speciﬁc case of Markovian GPs [4, 38].
The main contribution of this work is to introduce the dual parameterization for SVGP and show that it speeds up both the learning and inference. For inference, we show that the dual parameters are automatically obtained through a different formulation of NGD, written in terms of the expectation parameters [17, 15, 18]. The formulation is fast since it avoids the use of sluggish automatic differentiation to compute the natural gradients. We also match the typical O(m2) memory complexity in other SVGP methods by introducing a tied parametrization. For learning, we show that the dual parameterization results in a tighter lower bound to the marginal likelihood, which speed-up the hyperparameter optimization (see Fig. 1). We provide extensive evaluation on benchmark data sets, which conﬁrms our ﬁndings. Our work attempts to revive the dual parameterization, which was popular in the early 2000s, but was somehow forgotten and not used in the recent SVGP algorithms. 2