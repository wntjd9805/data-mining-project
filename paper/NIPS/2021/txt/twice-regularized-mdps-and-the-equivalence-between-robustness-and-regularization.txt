Abstract
Robust Markov decision processes (MDPs) aim to handle changing or partially known system dynamics. To solve them, one typically resorts to robust optimization methods. However, this signiﬁcantly increases computational complexity and limits scalability in both learning and planning. On the other hand, regularized MDPs show more stability in policy learning without impairing time complexity. Yet, they generally do not encompass uncertainty in the model dynamics. In this work, we aim to learn robust MDPs using regularization. We ﬁrst show that regularized
MDPs are a particular instance of robust MDPs with uncertain reward. We thus establish that policy iteration on reward-robust MDPs can have the same time complexity as on regularized MDPs. We further extend this relationship to MDPs with uncertain transitions: this leads to a regularization term with an additional dependence on the value function. We ﬁnally generalize regularized MDPs to twice regularized MDPs (R2 MDPs), i.e., MDPs with both value and policy regularization.
The corresponding Bellman operators enable developing policy iteration schemes with convergence and robustness guarantees. It also reduces planning and learning in robust MDPs to regularized MDPs. 1

Introduction
MDPs provide a practical framework for solving sequential decision problems under uncertainty
[30]. However, the chosen strategy can be very sensitive to sampling errors or inaccurate model estimates. This can lead to complete failure in common situations where the model parameters vary adversarially or are simply unknown [22]. Robust MDPs aim to mitigate such sensitivity by assuming that the transition and/or reward function (P, r) varies arbitrarily inside a given uncertainty set U [17, 27]. In this setting, an optimal solution maximizes a performance measure under the worst-case parameters. It can be thought of as a dynamic zero-sum game with an agent choosing the best action while Nature imposes it the most adversarial model. As such, solving robust MDPs involves max-min problems, which can be computationally challenging and limits scalability.
In recent years, several methods have been developed to alleviate the computational concerns raised by robust reinforcement learning (RL). Apart from [23, 24] that consider speciﬁc types of coupled uncertainty sets, all rely on a rectangularity assumption without which the problem can be NP-hard
[2, 40]. This assumption is key to deriving tractable solvers of robust MDPs such as robust value iteration [2, 11] or more general robust modiﬁed policy iteration (MPI) [18]. Yet, reducing time complexity in robust Bellman updates remains challenging and is still researched today [15, 11].
At the same time, the empirical success of regularization in policy search methods has motivated a wide range of algorithms with diverse motivations such as improved exploration [12, 20] or stability [34, 13]. Geist et al. [10] proposed a uniﬁed view from which many existing algorithms can be derived. Their regularized MDP formalism opens the path to error propagation analysis
∗Contact author: estherderman@campus.technion.ac.il 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
in approximate MPI [33] and leads to the same bounds as for standard MDPs. Nevertheless, as we further show in Sec. 3, policy regularization accounts for reward uncertainty only: it does not encompass uncertainty in the model dynamics. Despite a vast literature on how regularized policy search works and convergence rates analysis [36, 5], little attention has been given to understanding why it can generate strategies that are robust to external perturbations [13].
To our knowledge, the only works that relate robustness to regularization in RL are [6, 16, 9]. Derman
& Mannor [6] employ a distributionally robust optimization approach to regularize an empirical value function. Unfortunately, computing this empirical value necessitates several policy evaluation procedures, which is quickly unpractical. Husain et al. [16] provide a dual relationship with robust
MDPs under uncertain reward. Their duality result applies to general regularization methods and gives a robust interpretation of soft-actor-critic [13]. Although these two works justify the use of regularization for ensuring robustness, they do not enclose any algorithmic novelty. Similarly,
Eysenbach & Levine [9] speciﬁcally focus on maximum entropy methods and relate them to either reward or transition robustness. We shall further detail on these most related studies in Sec. 7.
The robustness-regularization duality is well established in statistical learning theory [41, 35, 19], as opposed to RL theory. In fact, standard setups such as classiﬁcation or regression may be considered as single-stage decision-making problems, i.e., one-step MDPs, a particular case of RL setting.
Extending this robustness-regularization duality to RL would yield cheaper learning methods with robustness guarantees. As such, we introduce a regularization function ΩU that depends on the uncertainty set U and is deﬁned over both policy and value spaces, thus inducing a twice regularized
Bellman operator (see Sec. 5). We show that this regularizer yields an equivalence of the form vπ,U = vπ,ΩU , where vπ,U is the robust value function for policy π and vπ,ΩU the regularized one.
This equivalence is derived through the objective function each value optimizes. More concretely, we formulate the robust value function vπ,U as an optimal solution of the robust optimization problem: (RO) (cid:104)v, µ0(cid:105) s. t. v ≤ inf
T π (P,r)v, max v∈RS (P,r)∈U (P,r) is the evaluation Bellman operator [30]. Then, we show that vπ,U is also an optimal where T π solution of the convex (non-robust) optimization problem: max v∈RS (cid:104)v, µ0(cid:105) s. t. v ≤ T π (P0,r0)v − ΩU (π, v), (CO) where (P0, r0) is the nominal model. This establishes equivalence between the two optimization problems. Moreover, the inequality constraint of (CO) enables to derive a twice regularized (R2)
Bellman operator deﬁned according to ΩU , a policy and value regularizer. For ball-constrained uncertainty sets, ΩU has an explicit form and under mild conditions, the corresponding R2 Bellman operators are contracting. The equivalence between the two problems (RO) and (CO) together with the contraction properties of R2 Bellman operators enable to circumvent robust optimization problems at each Bellman update. As such, it alleviates robust planning and learning algorithms by reducing them to regularized ones, which are known to be as complex as classical methods.
To summarize, we make the following contributions: (i) We show that regularized MDPs are a speciﬁc instance of robust MDPs with uncertain reward. Besides formalizing a general connection between the two settings, our result enables to explicit the uncertainty sets induced by standard regularizers. (ii) We extend this duality to MDPs with uncertain transition and provide the ﬁrst regularizer that recovers robust MDPs with s-rectangular balls and arbitrary norm. (iii) We introduce twice regularized MDPs (R2 MDPs) that apply both policy and value regularization to retrieve robust
MDPs. We establish contraction of the corresponding Bellman operators. This leads us to proposing an R2 MPI algorithm with similar time complexity as vanilla MPI, thus opening new perspectives towards practical and scalable robust RL.
Notations. We designate the extended reals by R := {−∞, ∞}. Given a ﬁnite set Z, the class of real-valued functions (resp. probability distributions) over Z is denoted by RZ (resp. ∆Z ), while the constant function equal to 1 over Z is denoted by 1Z . Similarly, for any set X , ∆X
Z denotes the class of functions deﬁned over X and valued in ∆Z . The inner product of two functions a, b ∈ RZ z∈Z a(z) b(z), which induces the (cid:96)2-norm (cid:107)a(cid:107) := (cid:112)(cid:104)a, a(cid:105). The (cid:96)2-norm is deﬁned as (cid:104)a, b(cid:105) := (cid:80) coincides with its dual norm, i.e., (cid:107)a(cid:107) = max(cid:107)b(cid:107)≤1(cid:104)a, b(cid:105) =: (cid:107)a(cid:107)∗. Let a function f : RZ → R.
The Legendre-Fenchel transform (or convex conjugate) of f is f ∗(y) := maxa∈RZ {(cid:104)a, y(cid:105) − f (a)}.
Given a set Z ⊆ RZ , the characteristic function δZ : RZ → R is δZ(a) = 0 if a ∈ Z; +∞ otherwise.
The Legendre-Fenchel transform of δZ is the support function σZ(y) = maxa∈Z(cid:104)a, y(cid:105) [3, Ex. 1.6.1]. 2
2 Preliminaries
This section describes the background material that we use throughout our work. Firstly, we recall useful properties in convex analysis. Secondly, we address classical discounted MDPs and their linear program (LP) formulation. Thirdly, we brieﬂy detail on regularized MDPs and the associated operators and lastly, we focus on the robust MDP setting.
Convex Analysis. Let Ω : ∆Z → R be a strongly convex function. Throughout this study, the function Ω plays the role of a policy and/or value regularization function. Its Legendre-Fenchel transform Ω∗ satisﬁes several smoothness properties, hence its alternative name "smoothed max operator" [25]. Our work makes use of the following result [14, 25].
Proposition 2.1. Given Ω : ∆Z → R strongly convex, the following properties hold: (i) ∇Ω∗ is Lipschitz and satisﬁes ∇Ω∗(y) = arg maxa∈∆Z (cid:104)a, y(cid:105) − Ω(a), ∀ y ∈ RZ . (ii) For any c ∈ R, y ∈ RZ , Ω∗(y +c1Z ) = Ω∗(y) + c. (iii) The Legendre-Fenchel transform Ω∗ is non-decreasing.
Discounted MDPs and LP formulation. Consider an inﬁnite horizon MDP (S, A, µ0, γ, P, r) with S and A ﬁnite state and action spaces respectively, 0 < µ0 ∈ ∆S an initial state distribution and γ ∈ (0, 1) a discount factor. Denoting X := S × A, P ∈ ∆X
S is a transition kernel mapping each state-action pair to a probability distribution over S and r ∈ RX is a reward function. A policy
π ∈ ∆S
A maps any state s ∈ S to an action distribution πs ∈ ∆A, and we evaluate its performance through the following measure:
ρ(π) := E (cid:34) ∞ (cid:88) t=0
γtr(st, at) (cid:12) (cid:12) (cid:12) (cid:12) (cid:35)
µ0, π, P
= (cid:104)vπ (P,r), µ0(cid:105). (1)
Here, the expectation is conditioned on the process distribution determined by µ0, π and P , and for all s ∈ S, vπ t=0 γtr(st, at)|s0 = s, π, P ] is the value function at state s. Maximizing (1) deﬁnes the standard RL objective, which can be solved thanks to the Bellman operators: (P,r)(s) = E[(cid:80)∞ (P,r)v := rπ + γP πv ∀v ∈ RS , π ∈ ∆S
T π
A,
T(P,r)v := max
π∈∆S
A (P,r)v ∀v ∈ RS ,
T π
G(P,r)(v) := {π ∈ ∆S
A : T π (P,r)v = T(P,r)v} ∀v ∈ RS , (P,r) and v∗ where rπ := [(cid:104)πs, r(s, ·)(cid:105)]s∈S and P π = [P π(s(cid:48)|s)]s(cid:48),s∈S with P π(s(cid:48)|s) := (cid:104)πs, P (s(cid:48)|s, ·)(cid:105). Both
T π (P,r) and T(P,r) are γ-contractions with respect to (w.r.t.) the supremum norm, so each admits a unique ﬁxed point vπ (P,r), respectively. The set of greedy policies w.r.t. value v deﬁnes (P,r)) is optimal [30]. For all v ∈ RS , the associated function
G(P,r)(v), and any policy π ∈ G(P,r)(v∗ q ∈ RX is given by q(s, a) = r(s, a) + γ(cid:104)P (·|s, a), v(cid:105) ∀(s, a) ∈ X . In particular, the ﬁxed point (P,r)(s, ·)(cid:105) where qπ (P,r) satisﬁes vπ vπ
The problem in (1) can also be formulated as an LP [30]. Given a policy π ∈ ∆S performance ρ(π) by the following v-LP [30, 26]: (P,r) is its associated q-function.
A, we characterize its (P,r) = (cid:104)πs, qπ (cid:104)v, µ0(cid:105) subject to (s.t.) v ≥ rπ + γP πv. min v∈RS (Pπ)
This primal objective provides a policy view on the problem. Alternatively, one may take a state visitation perspective by studying the dual objective instead: max
µ∈RS (cid:104)rπ, µ(cid:105) s. t. µ ≥ 0 and (IdRS − γP π
∗ )µ = µ0, (Dπ)
∗ is the adjoint policy transition operator2: [P π
¯s∈S P π(s|¯s)µ(¯s) ∀µ ∈ RS , where P π and IdS is the identity function in RS . Let I(s(cid:48)|s, a) := δs(cid:48)=s ∀(s, a, s(cid:48)) ∈ X × S the trivial transi-tion matrix and deﬁne its adjoint transition operator as I∗µ(s) := (cid:80) (¯s,¯a)∈X I(s|¯s, ¯a)µ(¯s, ¯a) ∀s ∈
∗ µ](s) := (cid:80) 2It is the adjoint operator of P π in the sense that (cid:104)P πv, v(cid:48)(cid:105) = (cid:104)v, P π
∗ v(cid:48)(cid:105) ∀v, v(cid:48) ∈ RS . 3
S. The correspondence between occupancy measures and policies lies in the one-to-one mapping
µ (cid:55)→ µ(·,·)
I∗µ(·) =: πµ and its inverse π (cid:55)→ µπ given by
µπ(s, a) :=
∞ (cid:88) t=0
γtP (cid:18) (cid:12) (cid:12) st = s, at = a (cid:12) (cid:12) (cid:19)
µ0, π, P
∀(s, a) ∈ X .
As such, one can interchangeably work with the primal LP (Pπ) or the dual (Dπ).
Regularized MDPs. A regularized MDP is a tuple (S, A, µ0, γ, P, r, Ω) with (S, A, µ0, γ, P, r) an inﬁnite horizon MDP as above, and Ω := (Ωs)s∈S a ﬁnite set of functions such that for all s ∈ S, Ωs : ∆A → R is strongly convex. Each function Ωs plays the role of a policy regularizer
Ωs(πs). With a slight abuse of notation, we shall denote by Ω(π) := (Ωs(πs))s∈S the family of state-dependent regularizers.3 The regularized Bellman evaluation operator is given by
[T π,Ω (P,r)v](s) := T π (P,r)v(s) − Ωs(πs) ∀v ∈ RS , s ∈ S, (P,r)v := maxπ∈∆S (P,r) (respectively T ∗,Ω (P,r)) is denoted by vπ,Ω
T π,Ω and the regularized Bellman optimality operator by T ∗,Ω (P,r)v ∀v ∈ RS [10].
The unique ﬁxed point of T π,Ω (P,r) (resp. v∗,Ω (P,r)) and deﬁnes the regularized value function (resp. regularized optimal value function). Although the regularized MDP formalism stems from the aforementioned Bellman operators in [10], it turns out that regularized
MDPs are MDPs with modiﬁed reward. Indeed, for any policy π ∈ ∆S
A, the regularized value function is vπ,Ω (P,r) = (IS − γP π)−1(rπ − Ω(π)), which corresponds to a non-regularized value with expected reward ˜rπ := rπ − Ω(π). Note that the modiﬁed reward ˜rπ(s) is no longer linear in πs because of the strong convexity of Ωs. Also, this modiﬁcation does not apply to the reward function r but only to its expectation rπ, as we cannot regularize the original reward without making it policy-independent.
A
Robust MDPs.
In general, the MDP model is not explicitly known but rather estimated from sampled trajectories. As this may result in over-sensitive outcome [22], robust MDPs reduce such performance variation. Formally, a robust MDP (S, A, µ0, γ, U) is an MDP with uncertain model
S and reward r ∈ R ⊆ RX [17, 40]. belonging to U := P × R, i.e., uncertain transition P ∈ P ⊆ ∆X
The uncertainty set U typically controls the conﬁdence level of a model estimate, which in turn determines the agent’s level of robustness. It is given to the agent, who seeks to maximize performance under the worst-case model (P, r) ∈ U. Although untractable in general, this problem can be solved in polynomial time for rectangular uncertainty sets, i.e., when U = ×s∈S Us = ×s∈S (Ps × Rs)
A and state s ∈ S, the robust value function at s is vπ,U (s) :=
[40, 23]. For any policy π ∈ ∆S min(P,r)∈U vπ vπ,U (s). Each of them is the unique ﬁxed point of the respective robust Bellman operators: (P,r)(s) and the robust optimal value function v∗,U (s) := maxπ∈∆A
S
[T π,U v](s) := min (P,r)∈U (P,r)v(s) ∀v ∈ RS , s ∈ S, π ∈ ∆A
T π
S ,
[T ∗,U v](s) := max
π∈∆S
A
[T π,U v](s) ∀v ∈ RS , s ∈ S, which are γ-contractions. For all v ∈ RS , the associated robust q-function is given by q(s, a) = min(P,r)∈U {r(s, a) + γ(cid:104)P (·|s, a), v(cid:105)} ∀(s, a) ∈ X , so that vπ,U = (cid:104)πs, qπ,U (s, ·)(cid:105) where qπ,U is the robust q-function associated to vπ,U . 3 Reward-robust MDPs
This section focuses on reward-robust MDPs, i.e., robust MDPs with uncertain reward but known transition model. We ﬁrst show that regularized MDPs represent a particular instance of reward-robust
MDPs, as both solve the same optimization problem. This equivalence provides a theoretical motivation for the heuristic success of policy regularization. Then, we explicit the uncertainty set 3In the formalism of Geist et al. [10], Ωs is initially constant over S. However, later in the paper [10, Sec. 5], it changes according to policy iterates. Here, we alternatively deﬁne a family Ω of state-dependent regularizers, which accounts for state-dependent uncertainty sets (see Sec. 5 below). 4
underlying some standard regularization functions, thus suggesting an interpretable explanation of their empirical robustness.
We ﬁrst show the following proposition 3.1, which applies to general robust MDPs and random policies. It slightly extends [17], as Lemma 3.2 there focuses on uncertain-transition MDPs and deterministic policies. For completeness, we provide a proof of Prop. 3.1 in Appx. A.1.
Proposition 3.1. For any policy π ∈ ∆S the robust optimization problem:
A, the robust value function vπ,U is the optimal solution of max v∈RS (cid:104)v, µ0(cid:105) s. t. v ≤ T π (P,r)v for all (P, r) ∈ U. (PU )
In the robust optimization problem (PU ), the inequality constraint must hold over the whole uncertainty set U. As such, a function v ∈ RS is said to be robust feasible for (PU ) if v ≤ T π (P,r)v for all (P, r) ∈ U or equivalently, if max(P,r)∈U {v(s) − T π (P,r)v(s)} ≤ 0 for all s ∈ S . Therefore, checking robust feasibility requires to solve a maximization problem. For properly structured uncertainty sets, a closed form solution can be derived, as we shall see in the sequel. As standard in the robust RL literature [32, 15, 1], the remaining of this work focuses on uncertainty sets centered around a known nominal model. Formally, given P0 (resp. r0) a nominal transition kernel (resp. reward function), we consider uncertainty sets of the form (P0 + P) × (r0 + R). The size of P × R quantiﬁes our level of uncertainty or alternatively, the desired degree of robustness.
Reward-robust and regularized MDPs: an equivalence. We now focus on reward-robust MDPs, i.e., robust MDPs with U = {P0} × (r0 + R). Thm. 3.1 establishes that reward-robust MDPs are in fact regularized MDPs whose regularizer is given by a support function. Its proof can be found in Appx. A.2. This result brings two take-home messages: (i) policy regularization is equivalent to reward uncertainty; (ii) policy iteration on reward-robust MDPs has the same convergence rate as regularized MDPs, which in turn is the same as standard MDPs [10].
Theorem 3.1 (Reward-robust MDP). Assume that U = {P0} × (r0 + R). Then, for any policy
A, the robust value function vπ,U is the optimal solution of the convex optimization problem:
π ∈ ∆S max v∈RS (cid:104)v, µ0(cid:105) s. t. v(s) ≤ T π (P0,r0)v(s) − σRs (−πs) for all s ∈ S .
Thm. 3.1 clearly highlights a convex regularizer Ωs(πs) := σRs(−πs) ∀s ∈ S. We thus recover a regularized MDP by setting [T π,Ωv](s) = T π (P0,r0)v(s) − σRs(−πs) ∀s ∈ S. In particular, when
Rs is a ball of radius αr s, the support function (or regularizer) can be written in closed form as
Ωs(πs) := αr
Corollary 3.1. Let π ∈ ∆S reward uncertainty set at s is Rs := {rs ∈ RA : (cid:107)rs(cid:107) ≤ αr is the optimal solution of the convex optimization problem:
A and U = {P0} × (r0 + R). Further assume that for all s ∈ S, the s}. Then, the robust value function vπ,U s(cid:107)πs(cid:107), which is strongly convex. We formalize this below (see proof in Appx. A.3). max v∈RS (cid:104)v, µ0(cid:105) s. t. v(s) ≤ T π (P0,r0)v(s) − αr s(cid:107)πs(cid:107) for all s ∈ S .
While regularization induces reward-robustness, Thm. 3.1 and Cor. 3.1 suggest that, on the other hand, speciﬁc reward-robust MDPs recover well-known policy regularization methods. In the following section, we explicit the reward-uncertainty sets underlying some of these regularizers.
Related Algorithms. Consider a reward uncertainty set of the form R := ×(s,a)∈X Rs,a. This deﬁnes an (s, a)-rectangular R (a particular type of s-rectangular R) whose rectangles Rs,a are independently deﬁned for each state-action pair. For the regularizers below, we derive appropriate
Rs,a-s that recover the same regularized value function. Detailed proofs are in Appx. A.4. There, we also include a summary table that reviews the properties of some RL regularizers, as well as our R2 function which we shall introduce later in Sec. 5. Note that the reward uncertainty sets here depend on the policy. This is due to the fact that standard regularizers are deﬁned over the policy space and not at each state-action pair. It similarly explains why the reward modiﬁcation induced by regularization does not apply to the original reward function, as already mentioned in Sec. 2. 5
Negative Shannon entropy. Let RNS port function enables to write: s,a(π) := [ln (1/πs(a)) , +∞)
∀(s, a) ∈ X . The associated sup-σRNS s (π)(−πs) = max r(s,·):r(s,a(cid:48))∈RNS s,a(cid:48) (π),a(cid:48)∈A (cid:88) a∈A
−r(s, a)πs(a) = (cid:88) a∈A
πs(a) ln(πs(a)), s,a(π) ∀(s, a) ∈ X . It amounts to translating the interval RNS where the last equality comes from maximizing −r(s, a) over [ln (1/πs(a)) , +∞) for each a ∈ A.
We thus recover the negative Shannon entropy Ω(πs) = (cid:80)
Kullback-Leibler divergence. Given an action distribution 0 < d ∈ ∆A, let RKL
RNS writing the support function yields Ω(πs) = (cid:80) divergence [34].
Negative Tsallis entropy. Letting RT negative Tsallis entropy Ω(πs) = 1 s,a(π) := ln (d(a)) + s,a by the given constant. Similarly a∈A πs(a) ln (πs(a)/d(a)), which is exactly the KL s,a(π) := [(1−πs(a))/2, +∞) a∈A πs(a) ln(πs(a)) [13].
∀(s, a) ∈ X , we recover the 2 ((cid:107)πs(cid:107)2 − 1) [20].
Policy-gradient for reward-robust MDPs. The equivalence between reward-robust and regular-ized MDPs leads us to wonder whether we can employ policy-gradient [37] on reward-robust MDPs using regularization. The following result establishes that a policy-gradient theorem can indeed be established for reward-robust MDPs (see proof in Appx. A.5).
Proposition 3.2. Assume that U = {P0} × (r0 + R) with Rs = {rs ∈ RA : (cid:107)rs(cid:107) ≤ αr gradient of the reward-robust objective JU (π) := (cid:104)vπ,U , µ0(cid:105) is given by s}. Then, the
∇JU (π) = E(s,a)∼µπ (cid:20)
∇ ln πs(a) (cid:18) qπ,U (s, a) − αr s (cid:19)(cid:21)
,
πs(a) (cid:107)πs(cid:107) where µπ is the occupancy measure under the nominal model P0 and policy π.
Although Prop. 3.2 is an application of [10, Appx. D.3] for a speciﬁc regularized MDP, its reward-robust formulation is novel and suggests another simpliﬁcation of robust methods. Indeed, previous works that exploit policy-gradient on robust MDPs involve the occupancy measure of the worst-case model [21], whereas our result sticks to the nominal. In practice, Prop. 3.2 enables to learn a robust policy by sampling transitions from the nominal model instead of all uncertain models.
This has a twofold advantage: (i) it avoids an additional computation of the minimum as done in
[29, 21, 7], where the authors sample next-state transitions and rewards based on all parameters from the uncertainty set, then update a policy based on the worst outcome; (ii) it releases from restricting to ﬁnite uncertainty sets. In fact, our regularizer accounts for robustness regardless of the sampling procedure, whereas the parallel simulations of [29, 21, 7] require the uncertainty set to be ﬁnite.
Technical difﬁculties are yet to be addressed for generalizing our result to transition-uncertain MDPs, because of the interdependence between the regularizer and the value function (see Secs. 4-5). We detail more on this issue in Appx. A.5. 4 General robust MDPs
Now that we have established policy regularization as a reward-robust problem, we would like to study the opposite question: can any robust MDP with both uncertain reward and transition be solved using regularization instead of robust optimization? If so, is the regularization function easy to determine? In this section, we answer positively to both questions for properly deﬁned robust MDPs.
This greatly facilitates robust RL, as it avoids the increased complexity of robust planning algorithms while still reaching robust performance.
The following theorem establishes that similarly to reward-robust MDPs, robust MDPs can be formulated through regularization (see proof in Appx. B.1). Although the regularizer is also a support function in that case, it depends on both the policy and the value objective, which may further explain the difﬁculty of dealing with robust MDPs.
Theorem 4.1 (General robust MDP). Assume that U = (P0 + P) × (r0 + R). Then, for any policy
A, the robust value function vπ,U is the optimal solution of the convex optimization problem:
π ∈ ∆S (2) (P0,r0)v(s) − σRs (−πs) − σPs(−γv · πs) for all s ∈ S, (cid:104)v, µ0(cid:105) s. t. v(s) ≤ T π max v∈RS where [v · πs](s(cid:48), a) := v(s(cid:48))πs(a) ∀(s(cid:48), a) ∈ X . 6
The upper-bound in the inequality constraint (2) is of the same spirit as the regularized Bellman operator: the ﬁrst term is a standard, non-regularized Bellman operator on the nominal model (P0, r0) to which we subtract a policy and value-dependent function playing the role of regularization. This function reminds that of [6, Thm. 3.1] also coming from conjugacy. This is the only similarity between both regularizers: in [6], the Legendre-Fenchel transform is applied on a different type of function and results in a regularization term that has no closed form but can only be bounded from above. Moreover, the setup considered there is different since it studies distributionally robust MDPs.
As such, it involves general convex optimization, whereas we focus on the robust formulation of an LP.
The support function further simpliﬁes when the uncertainty set is a ball, as shown below. Yet, the dependence of the regularizer on the value function prevents us from readily applying the tool-set of regularized MDPs. We shall study the properties of this new regularization function in Sec. 5.
Corollary 4.1. Assume that U = (P0 + P) × (r0 + R) with Ps := {Ps ∈ RX : (cid:107)Ps(cid:107) ≤ αP
Rs := {rs ∈ RA : (cid:107)rs(cid:107) ≤ αr solution of the convex optimization problem: s } and s} for all s ∈ S. Then, the robust value function vπ,U is the optimal max v∈RS (cid:104)v, µ0(cid:105) s. t. v(s) ≤ T π (P0,r0)v(s) − αr s(cid:107)πs(cid:107) − αP s γ(cid:107)v(cid:107)(cid:107)πs(cid:107) for all s ∈ S . (3)
One can actually take two different norms for the reward and the transition uncertainty sets. Similarly,
Cor. 4.1 can be rewritten with an arbitrary norm, which would reveal a dual norm (cid:107)·(cid:107)∗ instead of (cid:107)·(cid:107) in Eq. (3) (see proof in Appx. B.2). Here, we restrict our statement to the (cid:96)2-norm for notation convenience only, the dual norm of (cid:96)2 being (cid:96)2 itself. Thus, our regularization function recovers a robust value function independently of the chosen norm, which extends previous results from [15, 11].
Indeed, Ho et al. [15] lighten complexity of robust planning for the (cid:96)1-norm only, while Grand-Clément & Kroer [11] focus on KL and (cid:96)2 ball-constrained uncertainty sets. Both works rely on the speciﬁc structure induced by the divergence they consider to derive more efﬁcient robust Bellman updates. Differently, our method circumvents these updates using a generic, problem-independent regularization function while still encompassing s-rectangular uncertainty sets as in [15, 11]. 5 R2 MDPs
In Sec. 4, we showed that for general robust MDPs, the optimization constraint involves a regular-ization term that depends on the value function itself. This adds a difﬁculty to the reward-robust case where the regularization only depends on the policy. Yet, we provided an explicit regularizer for general robust MDPs that are ball-constrained. In this section, we introduce R2 MDPs, an extension of regularized MDPs that combines policy and value regularization. The core idea is to further regularize the Bellman operators with a value-dependent term that recovers the support functions we derived from the robust optimization problems of Secs. 3-4.
Deﬁnition 5.1 (R2 Bellman operators). For all v ∈ RS , deﬁne Ωv,R (cid:107)πs(cid:107)(αr 2 : ∆A → R as Ωv,R s γ(cid:107)v(cid:107)). The R2 Bellman evaluation and optimality operators are deﬁned as 2 (πs) := s + αP 2 2
[T π,R
[T ∗,R v](s) := T π (P0,r0)v(s) − Ωv,R 2 (πs) ∀s ∈ S, v](s) := max
π∈∆S
A 2
[T π,R v](s) = Ω∗ v,R 2 (qs) ∀s ∈ S .
For any function v ∈ RS , the associated unique greedy policy is deﬁned as
πs = arg max
πs∈∆A 2
T π,R v(s) = ∇Ω∗ v,R 2 (qs),
∀s ∈ S, that is, in vector form, π = ∇Ω∗ v,R 2 (q) =: GΩ
R2 (v) ⇐⇒ T π,R 2 v = T ∗,R 2 v.
The R2 Bellman evaluation operator is not linear because of the functional norm appearing in the regularization function. Yet, under the following assumption, it is contracting and we can apply
Banach’s ﬁxed point theorem to deﬁne the R2 value function.
Assumption 5.1 (Bounded radius). For all s ∈ S, there exists (cid:15)s > 0 such that


αP s ≤ min


 1 − γ − (cid:15)s
γ(cid:112)|S|
; min
+ ,(cid:107)uA(cid:107)=1
+,(cid:107)vS (cid:107)=1 uA∈RA vS ∈RS u(cid:62)
AP0(·|s, ·)vS
.


 7
Asm. 5.1 requires to upper bound the ball radius of transition uncertainty sets. The ﬁrst term in the minimum is needed for establishing contraction of R2 Bellman operators (item (iii) in Prop. 5.1), while the second one is used for ensuring monotonicity (item (i) in Prop. 5.1). We remark that the former depends on the original discount factor γ: radius αP s must be smaller as γ tends to 1 but can arbitrarily grow as γ decreases to 0, without altering contraction. Indeed, larger γ implies longer time horizon and higher stochasticity, which explains why we need tighter level of uncertainty then.
Otherwise, value and policy regularization seem unable to handle the mixed effects of parameter and stochastic uncertainties. The additional dependence on the state-space size comes from the q replaces (cid:112)|S| (cid:96)2-norm chosen for the ball constraints. In fact, for any (cid:96)p-norm of dual (cid:96)q, |S| in the denominator, so it becomes independent of |S| as (p, q) tends to (1, ∞) (see Appx. C.1).
Although we recognize a generalized Rayleigh quotient-type problem in the second minimum [28], its interpretation in our context remains unclear. Asm. 5.1 enables the R2 Bellman operators to admit a unique ﬁxed point, among other nice properties. We formalize this below (see proof in Appx. C.1).
Proposition 5.1. Suppose that Asm. 5.1 holds. Then, we have the following properties: (i) Monotonicity: For all v1, v2 ∈ RS such that v1 ≤ v2, we have T π,R
T ∗,R (ii) Sub-distributivity: For all v1 ∈ RS , c ∈ R, we have T π,R
T ∗,R (iii) Contraction: (cid:107)T π,R
∈ RS , v2(cid:107)∞ ≤ (1 − (cid:15)∗)(cid:107)v1 − v2(cid:107)∞. v2(cid:107)∞ ≤ (1 − (cid:15)∗)(cid:107)v1 − v2(cid:107)∞ and (cid:107)T ∗,R
:= mins∈S (cid:15)s > 0.
Then, 2 v1 − T ∗,R v1 + γc1S , ∀c ∈ R. (v1 + c1S ) ≤ T π,R (v1 + c1S ) ≤ T ∗,R v1 + γc1S and for all v1, v2 v1 ≤ T π,R v1 ≤ T ∗,R v1 − T π,R
Let (cid:15)∗ v2 and v2. 1 2 2 2 2 2 2 2 2 2 2 2
We should note that the contracting coefﬁcient in Prop. 5.1 is different from the original discount factor γ, since here we have 1 − (cid:15)∗. Yet, as Asm. 5.1 suggests it, an intrinsic dependence between γ and (cid:15)∗ makes the R2 Bellman updates similar to the standard ones: when γ tends to 0, the value of (cid:15)∗ required for Asm. 5.1 to hold increases, which makes the contracting coefﬁcient 1 − (cid:15)∗ tend to 0 as well, i.e., the two contracting coefﬁcients behave similarly. The contraction of both R2 Bellman operators ﬁnally leads us to introduce the R2 value functions.
Deﬁnition 5.2 (R2 value functions). (i) The R2 value function vπ,R point of the R2 Bellman evaluation operator: vπ,R vπ,R qπ,R unique ﬁxed point of the R2 Bellman optimal operator: v∗,R is q∗,R 2 is deﬁned as the unique ﬁxed 2 . The associated q-function is 2 is deﬁned as the 2 . The associated q-function (cid:105). (ii) The R2 optimal value function v∗,R
= T ∗,R (s, a) = r0(s, a) + γ(cid:104)P0(·|s, a), vπ,R (s, a) = r0(s, a) + γ(cid:104)P0(·|s, a), v∗,R
= T π,R v∗,R (cid:105). 2 2 2 2 2 2 2 2
The monotonicity of the R2 Bellman operators plays a key role in reaching an optimal R2 policy, as we show in the following. A proof can be found in Appx. C.2.
Theorem 5.1 (R2 optimal policy). The greedy policy π∗,R
R2 policy, i.e., for all π ∈ ∆S
≥ vπ,R
Remark 5.1. An optimal R2 policy may be stochastic. This is due to the fact that our R2 MDP framework builds upon the general s-rectangularity assumption. Robust MDPs with s-rectangular un-certainty sets similarly yield an optimal robust policy that is stochastic [40, Table 1]. Nonetheless, the
R2 MDP formulation recovers a deterministic optimal policy in the more speciﬁc (s, a)-rectangular case, which is in accordance with the robust MDP setting (see proof in Appx. C.3). 4
) is the unique optimal
A, vπ∗,R
R2 (v∗,R
= v∗,R
= GΩ 2 . 2 2 2 2
All of the results above ensure convergence of MPI in R2 MDPs.
We call that method R2 MPI and provide its pseudo-code in Alg. 1.
The convergence proof follows the same lines as in [30]. More-over, the contracting property of the R2 Bellman operator ensures the same convergence rate as in standard and robust MDPs, i.e., a geometric convergence rate. On the other hand, R2 MPI reduces the computational complexity of robust MPI by avoiding to solve a max-min problem at each iteration, as this can take polynomial time for general convex programs. Advantageously, the only optimization involved in R2 MPI lies in the greedy step: it amounts to projecting onto the simplex,
Algorithm 1: R2 MPI
Result: πk+1, vk+1
Initialize vk ∈ RS ; while not converged do
πk+1 ← GΩ
R2 (vk); vk+1 ← (T πk+1,R
)mvk; end 2 4The stochasticity of an optimal entropy-regularized policy as in the examples of Sec. 3 is not contradicting.
Indeed, even though the corresponding uncertainty set is (s, a)-rectangular there, it is policy-dependent. 8
which can efﬁciently be performed in linear time [8]. Still, such projection is not even necessary in the (s, a)-rectangular case: as mentioned in Rmk. 5.1, it then sufﬁces to choose a greedy action in order to eventually achieve an optimal R2 value function. 6 Numerical Experiments
We aim to compare the computing time of R2 MPI with that of MPI [30] and robust MPI [18]. The code is available at https://github.com/EstherDerman/r2mdp. To do so, we run experiments on an Intel(R) Core(TM) i7-1068NG7 CPU @ 2.30GHz machine, which we test on a 5 × 5 grid-world domain. In that environment, the agent starts from a random position and seeks to reach a goal state in order to maximize reward. Thus, the reward function is zero in all states but two: one provides a reward of 1 while the other gives 10. An episode ends when either one of those two states is attained.
The partial evaluation of each policy iterate is a building block of MPI. As a sanity check, we evaluate the uniform policy through both R2 and robust policy evaluation (PE) sub-processes, to ensure that the two value outputs coincide. For simplicity, we focus on an (s, a)-rectangular uncertainty set and take the same ball radius α (resp. β) at each state-action pair for the reward function (resp. transition function). Parameter values and other implementation details are deferred to Appx. D. We obtain the same value for R2 PE and robust PE, which numerically conﬁrms Thm. 4.1. On the other hand, both are strictly smaller than their non-robust, non-regularized counterpart, but as expected, they converge to the standard value function when all ball radii tend to 0 (see Appx. D). More importantly, R2 PE converges in 0.02 seconds, whereas robust PE takes 54.8 seconds to converge, i.e., 2740 times longer.
This complexity gap comes from the minimization problems being solved at each iteration of robust
PE, something that R2 PE avoids thanks to regularization. R2 PE still takes 2.5 times longer than its standard, non-regularized counterpart, because of the additional computation of regularization terms.
Table 1 shows the time spent by each algorithm until convergence.
Table 1: Computing time (in sec.) of planning algorithms using vanilla, R2 and robust approaches.
Each cell displays the mean ± standard deviation obtained from 5 running seeds.
PE
MPI (m = 1)
MPI (m = 4)
Vanilla 0.008 ± 0. 0.01 ± 0. 0.01 ± 0.
R2 0.02 ± 0. 0.03 ± 0. 0.03 ± 0.
Robust 54.8 ± 1.2 118.6 ± 1.3 98.1 ± 4.1
We then study the overall MPI process for each approach. We know that in vanilla MPI, the greedy step is achieved by simply searching over deterministic policies [30]. Since we focus our experiments on an (s, a)-rectangular uncertainty set, the same applies to robust MPI [40] and to R2 MPI, as already mentioned in Rmk. 5.1. We can see in Table 1 that the increased complexity of robust MPI is even more prominent than its PE thread, as robust MPI takes 3953 (resp. 3270) times longer than R2 MPI when m = 1 (resp. m = 4). Robust MPI with m = 4 is a bit more advantageous than m = 1, as it needs less iterations (31 versus 67), i.e., less optimization solvers to converge.
Interestingly, for both m ∈ {1, 4}, progressing from PE to MPI did not cost much more computing time to either the vanilla or the R2 version: both take less than one second to run. 7