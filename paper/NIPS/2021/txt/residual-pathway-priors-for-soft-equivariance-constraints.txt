Abstract
Models such as convolutional neural networks restrict the hypothesis space to a set of functions satisfying equivariance constraints, and improve generalization in problems by capturing relevant symmetries. However, symmetries are often only partially respected, preventing models with restriction biases from ﬁtting the data.
We introduce Residual Pathway Priors (RPPs) as a method for converting hard ar-chitectural constraints into soft priors, guiding models towards structured solutions while retaining the ability to capture additional complexity. RPPs are resilient to approximate or misspeciﬁed symmetries, and are as effective as fully constrained models even when symmetries are exact. We show that RPPs provide compelling performance on both model-free and model-based reinforcement learning problems, where contact forces and directional rewards violate the assumptions of equivariant networks. Finally, we demonstrate that RPPs have broad applicability, including dynamical systems, regression, and classiﬁcation. 1

Introduction
Central to the expanding application of deep learning to structured data like images, text, audio, sets, graphs, point clouds, and dynamical systems, has been a search for ﬁnding the appropriate set of inductive biases to match the model to the data. These inductive biases, such as recurrence [43], local connectivity [29], equivariance [10], or differential equations [8], reduce the set of explored hypotheses and improve generalization. Equivariance in particular has had a large impact as it allows ruling out a large class of meaningless shortcut features in many distinct domains, such as the ordering of the nodes in graphs and sets or the coordinate system chosen for an image.
A disadvantage of hard coding these restrictions is that this prior knowledge may not match reality.
A scene may have long range non-local interactions, rotation equivariance may be violated by a preferred camera angle, or a dynamical system may occasionally have discontinuous transitions.
In particular, symmetries are delicate. A small perturbation like adding wind breaks the rotational symmetry of a pendulum, and bumpy or tilted terrain could break the translation symmetry for locomotion. In these cases we would like to incorporate our prior knowledge in a way that admits our own ignorance, and allows for the possibility that the world is more complex than we imagined.
We aim to develop an approach that is more general, and can be applied when symmetries are exact, approximate, or non-existent.
The Bayesian framework provides a mechanism for expressing such knowledge through priors. In much of the past work on Bayesian neural networks, the relationship between the prior distribution and the functions preferred by the prior is not transparent. While it is easy to specify different variances for different channels, or to use heavy tailed distributions, it is hard know how high level properties meaningfully translate into these low level attributes. Ultimately priors should represent our prior beliefs, and those beliefs we have are about high level concepts like the locality, independence, and symmetries of the data.
∗Equal Contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Priors over Equivariant Solutions (b) Structure of RPP Models
Figure 1: Left: RPPs encode an Occam’s razor approach to modeling. Highly ﬂexible models like
MLPs lack the inductive biases to assign high evidence to key datasets, while models with strict equivariance constraints are not ﬂexible enough to support problems with only approximate symmetry.
Right: The structure of RPPs. Expanding the layers into a sum of the constrained and unconstrained solutions while setting the prior to favor the constrained solution, leads to the more ﬂexible layer explaining only the residual of what is already explained by the constrained layer.
To address the need for more interpretable priors we introduce Residual Pathway Priors (RPPs), a method for converting hard architectural constraints into soft priors. Practically, RPPs allow us to tackle problems in which perfect symmetry has been violated, but approximate symmetry is still present, as is the case for most real world physical systems.
RPPs have a prior bias towards equivariant solutions, but are not constrained to equivariance. The choice of RPPs can be viewed from an Occam’s razor perspective, shown in Figure 1a, in which we seek to use models that have both the correct inductive biases and level of ﬂexibility [41]. As we ﬁnd with problems in which symmetries exist, highly ﬂexible models with weak inductive biases like
MLPs fail to concentrate prior mass around solutions that exhibit any symmetry. On the other hand when symmetries are only approximate, the strong biases of constrained models like Equivariant
Multi-Layer Perceptrons (EMLP) [14] fail to provide support for the observations. As a middle ground between these two extremes, RPPs combine the inductive biases of constrained models with the ﬂexibility of MLPs to deﬁne a model class which excels when data show approximate symmetries.
In the following sections we introduce our method and show results across a variety of domains. We list our contributions and the accompanying sections below: 1. We propose Residual Pathway Priors as a mechanism to imbue models with soft inductive biases, without constraining ﬂexibility. 2. While our approach is general, we use RPPs to show how to turn hard architectural con-straints into soft equivariance priors (Section 4). 3. We demonstrate that RPPs are robust to varying degrees of symmetry (Section 5). RPPs perform well under exact, approximate, or misspeciﬁed symmetries. 4. Using RPP on the approximate symmetries in the complex state spaces of the Mujoco locomotion tasks, we improve the performance of model free RL agents (Section 6).
We provide a PyTorch implementation of residual pathway priors at https://github.com/mfinzi/residual-pathway-priors. 2