Abstract
Imitation learning enables high-ﬁdelity, vision-based learning of policies within rich, photorealistic environments. However, such techniques often rely on tradi-tional discrete-time neural models and face difﬁculties in generalizing to domain shifts by failing to account for the causal relationships between the agent and the environment. In this paper, we propose a theoretical and experimental framework for learning causal representations using continuous-time neural networks, speciﬁ-cally over their discrete-time counterparts. We evaluate our method in the context of visual-control learning of drones over a series of complex tasks, ranging from short- and long-term navigation, to chasing static and dynamic objects through photorealistic environments. Our results demonstrate that causal continuous-time deep models can perform robust navigation tasks, where advanced recurrent models fail. These models learn complex causal control representations directly from raw visual inputs and scale to solve a variety of tasks using imitation learning. 1

Introduction
Unlike machine learning sys-tems, natural learning systems excel at generalizing learned skills beyond the original data distribution (Hasani et al., 2020,
Hassabis et al., 2017, Sarma et al., 2018). This is due to the mechanisms they deploy during their learning process, such as active use of closed-loop inter-ventions, accounting for distri-bution shifts, and the temporal structures (de Haan et al., 2019,
Schölkopf, 2019). These factors are largely disregarded or are en-gineered away in the develop-ment of modern ML systems.
To take the ﬁrst steps towards re-solving these issues, we can in-vestigate the spectrum of causal modeling (Peters et al., 2017). At one end of the spectrum, there are physical models of agents
Figure 1: Causal navigation from raw visual inputs. Given a sequence of raw RGB inputs (left) a drone is trained to navigate towards the red-cube target. We visualize the saliency maps (right) for each model. Neural circuit policies (Lechner et al., 2020a) (a speciﬁc representation of CT-RNNs) can learn causal relation-ships (i.e., attend to the red-cube) directly from data while other models fail to do so. ODE-RNNs (Rubanova et al., 2019b), LSTM (Hochreiter and Schmidhuber, 1997) and CT- Gated Recurrent
Units (Mozer et al., 2017). Saliency maps are computed by the visual backprop algorithm (Bojarski et al., 2016).
∗Equal Contributions. 1CSAIL MIT, 2IST Austria, Correspondence to: rhasani@mit.edu Code and data are available at: https://github.com/mit-drl/deepdrone 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and environments described by differential equations. These physical models allow us to inves-tigate interventions, predict future events using past information, and can describe the statistical dependencies in the system.
On the other end of the spectrum, statistical models allow us to construct dependencies and make predic-tions given independent and identically distributed (i.i.d.) (see Fig. 2). While physical models provide complete descriptions, it is intractable to deﬁne the differential equation systems that effectively model high-dimensional and complex sensory data. For long, causal modeling frameworks aims at bridging this gap to extract statistical dependencies while con-structing causal graphs (Pearl, 2009, Spirtes et al., 2000) or structural causal models(Schölkopf, 2019) to intervene and explain relationships.
Figure 2: Taxonomy of models in the spec-trum of causality. Figure data is taken from (Peters et al., 2017).
In this paper, we aim to use continuous-time (CT) neural networks (Chen et al., 2018b, Funahashi and
Nakamura, 1993, Hasani et al., 2021b) equipped with causal structures in order to get closer to the properties of physical models. In particular, we look into different representations of CT networks to see under what conditions and formulation they can form a causal model. We discover that the class of liquid time-constant networks (LTCs) (Hasani et al., 2021b) which are expressive continuous-time models constructed by bilinear approximation of neural ordinary differential equations (Chen et al., 2018b), satisfy the properties of a causal model.
The uniqueness of their solution and their ability to capture both internal and external interventions make their forward- and backward- mode causal. Therefore, they can impose inductive biases on their architectures to learn causal representations.
We analyze how certain continuous-time (CT) neural networks are dynamical causal models. Fur-thermore, to justify the theoretical results, we empirically validate that these properties scale to high-dimensional visual input data in complex learning environments. We propose a series of control and navigation tasks for an end-to-end autonomous ﬂight across various levels of complexity and temporal reasoning. We observe that traditional deep-learning models are capable of solving this task on ofﬂine, passive datasets but fail when deployed in closed-loop, active testing settings (de Haan et al., 2019, Wen et al., 2020). On the other hand, we ﬁnd that only LTC-based models are able to complete the tasks in closed-loop interaction with the environments.
Speciﬁcally, consider a visual navigation task wherein a drone agent should learn to ﬂy from point A to a target ﬁxated at a point B, given only a sequence of raw visual inputs. The mapping between incoming pixels to the temporal and physical structure of the navigation is causal if we can explain and intervene at each step to observe how navigation decisions are decided based on the input pixels.
We observe that in a neighborhood environment, agents based on LTCs (such as Neural Circuit
Policies (Lechner et al., 2020a)) learn to stably (Lechner et al., 2020b) attend to the target at point
B throughout the task’s time horizon (Fig. 1). Therefore, causes of navigation to point B at a next time step is the agent’s learned attention proﬁle on the input images. These causal mappings are not present in other deep models (Fig. 1). Here, we show how LTCs’ ability to capture causal structures directly from visual inputs results in improved robustness as well as interpretable decision making.
Summary of Contributions. i) We show theoretical evidence for the capability of CT neural net-works in learning causal structures; ii) we perform extensive experiments supporting the effectiveness of causal CT models in visual drone navigation tasks with varying memory-horizons; and iii) we conduct robustness analysis of CT models in closed loop-testing within real-world scenarios. 2