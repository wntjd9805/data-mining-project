Abstract
Dropout regularization methods prune a neural network’s pre-determined backbone structure to avoid overﬁtting. However, a deep model still tends to be poorly calibrated with high conﬁdence on incorrect predictions. We propose a uniﬁed
Bayesian model selection method to jointly infer the most plausible network depth warranted by data, and perform dropout regularization simultaneously. In particular, to infer network depth we deﬁne a beta process over the number of hidden layers which allows it to go to inﬁnity. Layer-wise activation probabilities induced by the beta process modulate neuron activation via binary vectors of a conjugate
Bernoulli process. Experiments across domains show that by adapting network depth and dropout regularization to data, our method achieves superior performance comparing to state-of-the-art methods with well-calibrated uncertainty estimates. In continual learning, our method enables neural networks to dynamically evolve their depths to accommodate incrementally available data beyond their initial structures, and alleviate catastrophic forgetting. 1

Introduction
Both dropout regularization and network depth are critical to the success of deep neural networks (DNNs) [1, 2, 3, 4]. Finely tuned or selected structures empower DNNs with proper model capacity that can not only efﬁciently capture statistical regularities in data but also avoid being deceived by random noise into “discovering” non-existent relationships.
Dropout as an effective regularization method sparsiﬁes a neural network’s structure by randomly deleting neurons along with their connections in training to prevent it from overﬁtting [5, 6]. Recent studies extend the method from a Bayesian perspective to allow automatic tuning of the dropout probability in large models [4, 7, 8, 9, 10, 11]. Although dropout and its variants can effectively govern model capacity, without uncertainty calibration deep models tend to be overly conﬁdent with their predictions [3, 12]. Probabilistic inference approaches for network structure selection propose to bypass or down-weight certain hidden layers to reduce training cost and build smaller models that perform just as well as larger ones [3, 13, 14, 15]. These methods lead to more efﬁcient or better-calibrated models only by reducing the depth of a pre-determined network structure. They cannot scale the network structure up to accommodate incrementally available data beyond the upper limit of its capacity.
We thus propose a novel Bayesian model selection framework to jointly infer network depth and perform dropout regularization simultaneously. In particular, we model the depth of a network structure as a stochastic process by deﬁning the beta process [16, 17] over the number of hidden layers to enable it to go to inﬁnity in theory, as in Figure 1(a) (left). The beta process induces layer-wise activation probabilities, which allows its conjugate Bernoulli process to generate a binary
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b)
Figure 1: (a) Demonstrations of our proposed framework (left), a dropout variant (middle), and typical structure selection methods (right). Our framework enables depth goes to inﬁnity by modeling the number of hidden layers with a beta process. The dropout variant deﬁnes an Indian buffet process per layer to infer width, but depth is ﬁxed [11, 20]. Structure selection methods can only reduce depth of a pre-determined network structure [2, 13, 14, 15]. (b) On top, random draws from two beta processes with α > β on the left and β > α on the right over hidden layer function space
H = {hl |l → ∞}. An atom location is δhl indexing a hidden layer function hl, and the height denotes its activation probability πl. For both cases, the conjugate Bernoulli processes at bottom are obtained by random ﬁlled dots zml = 1 with probability πl and empty dots zml = 0 with probability 1 − πl. So each column is a binary vector zl corresponding to layer l. vector per hidden layer to prune neurons for regularization, as in Figure 1(b). The probabilistic inference framework automatically balances network depth and dropout regularization by computing a marginal likelihood over the hidden layers and their neuron activations, and provides well-calibrated uncertainty estimates for predictions. The exact computation of the marginal likelihood is intractable due to the nonlinear nature of neural networks with a potentially inﬁnite number of hidden layers.
We thus employ the structured stochastic variational inference [18, 19] with a continuous relaxation on the Bernoulli variables to efﬁciently approximate the integral with a continuum of lower bounds.
The relaxation allows to maintain differentiability and mitigate overﬁtting via model averaging of the sampled hidden layers and neuron activations. Beneﬁting from theoretical analysis, we readily integrate the joint inference with the parameter learning process through an alternating approach to efﬁciently update the parameters, and perform hidden-layer sampling and dropout regularization.
We analyze the behavior of our joint inference framework over multilayer perceptrons (MLPs) and convolutional neural networks (CNNs), and evaluate their performance across domains. The experiments show that our method leads to a compact neural network by balancing its depth and dropout regularization with uncertainty calibration, and achieves superior performance comparing to state-of-the-art dropout and structure selection methods. We also demonstrate our method on a continual learning task. By enabling both network depth and neuron activations to dynamically evolve to accommodate incrementally available data, we can alleviate catastrophic forgetting. 2