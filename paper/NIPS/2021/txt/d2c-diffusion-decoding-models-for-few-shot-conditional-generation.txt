Abstract
Conditional generative models of high-dimensional images have many applications, but supervision signals from conditions to images can be expensive to acquire.
This paper describes Diffusion-Decoding models with Contrastive representations (D2C), a paradigm for training unconditional variational autoencoders (VAEs) for few-shot conditional image generation. D2C uses a learned diffusion-based prior over the latent representations to improve generation and contrastive self-supervised learning to improve representation quality. D2C can adapt to novel generation tasks conditioned on labels or manipulation constraints, by learning from as few as 100 labeled examples. On conditional generation from new labels, D2C achieves superior performance over state-of-the-art VAEs and diffusion models.
On conditional image manipulation, D2C generations are two orders of magnitude faster to produce over StyleGAN2 ones and are preferred by 50% − 60% of the human evaluators in a double-blind study. We release our code at https:
//github.com/jiamings/d2c. 1

Introduction
Generative models trained on large amounts of unlabeled data have achieved great success in various domains including images [8, 50, 75, 42], text [56, 2], audio [26, 71, 93, 62], and graphs [36, 67].
However, downstream applications of generative models are often based on various conditioning signals, such as labels [61], text descriptions [60], reward values [101], or similarity with existing data [45]. While it is possible to directly train conditional models, this often requires large amounts of paired data [57, 74] that are costly to acquire. Hence, it would be desirable to learn conditional generative models using large amounts of unlabeled data and as little paired data as possible.
Contrastive self-supervised learning (SSL) methods can greatly reduce the need for labeled data in discriminative tasks by learning effective representations from unlabeled data [95, 37, 35], and have also been shown to improve few-shot learning [39]. It is therefore natural to ask if they can also be used to improve few-shot generation. Latent variable generative models (LVGM) are a natural candidate for this, since they already involve a low-dimensional, structured latent representation of the data they generate. However, generative adversarial networks (GANs, [34, 50]) and diffusion
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
models [42, 84], lack explicit tractable functions to map inputs to representations, making it difﬁcult to optimize latent variables with SSL. Variational autoencoders (VAEs, [52, 77]), on the other hand, can naturally adopt SSL through their encoder model, but they typically have worse sample quality.
Figure 1: Few-shot conditional generation with the unconditional D2C model (left). With a recogni-tion model over the latent space (middle), D2C can generate samples for novel conditions, such as image manipulation (right). These conditions can be deﬁned with very few labels.
In this paper, we propose Diffusion-Decoding models with Contrastive representations (D2C), a special VAE that is suitable for conditional few-shot generation. D2C uses contrastive self-supervised learning methods to obtain a latent space that inherits the transferrability and few-shot capabilities of self-supervised representations. Unlike other VAEs, D2C learns a diffusion model over the latent representations. This latent diffusion model ensures that D2C uses the same latent distribution for both training and generation. We provide a formal argument to explain why this approach leads to better sample quality than existing hierarchical VAEs. We further discuss how to apply D2C to few-shot conditional generation where the conditions are deﬁned through labeled examples or manipulation constraints. Our approach combines a discriminative model providing conditioning signal and generative diffusion model over the latent space, and is computationally more efﬁcient than methods that act directly over the image space (Figure 1).
We evaluate and compare D2C with several state-of-the-art generative models over 6 datasets. On unconditional generation, D2C outperforms state-of-the-art VAEs and is competitive with diffusion models under similar computational budgets. On conditional generation with 100 labeled examples,
D2C signiﬁcantly outperforms state-of-the-art VAE [91] and diffusion models [84]. D2C can also learn to perform certain image manipulation tasks from as few as 100 labeled examples. Notably, for manipulating images, D2C is two orders of magnitude faster than StyleGAN2 [106] and preferred by 50% − 60% of human evaluations, which to our best knowledge is the ﬁrst for any VAE model. 2