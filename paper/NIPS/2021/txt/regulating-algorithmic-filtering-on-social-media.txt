Abstract
By ﬁltering the content that users see, social media platforms have the ability to inﬂuence users’ perceptions and decisions, from their dining choices to their voting preferences. This inﬂuence has drawn scrutiny, with many calling for regulations on
ﬁltering algorithms, but designing and enforcing regulations remains challenging.
In this work, we examine three questions. First, given a regulation, how would one design an audit to enforce it? Second, does the audit impose a performance cost on the platform? Third, how does the audit affect the content that the platform is incentivized to ﬁlter? In response to these questions, we propose a method such that, given a regulation, an auditor can test whether that regulation is met with only black-box access to the ﬁltering algorithm. We then turn to the platform’s perspective. The platform’s goal is to maximize an objective function while meeting regulation. We ﬁnd that there are conditions under which the regulation does not place a high performance cost on the platform and, notably, that content diversity can play a key role in aligning the interests of the platform and regulators. 1

Introduction
In recent years, there have been increasing calls to regulate how social media platforms algorithmically
ﬁlter the content that appears on a user’s feed. For example, one may ask that the advertisements a user sees not be based (explicitly or implicitly) on their sexual orientation [65] or that content related to public health (e.g., COVID-19) does not reﬂect a user’s political afﬁliation [35].
However, translating a regulatory guideline into an auditing procedure has proven difﬁcult. Develop-ing such a method is the focus of this work. As our main contribution, we provide a general procedure such that, given a regulation, an auditor can test whether the platform complies with the regulation.
Providing a general procedure is important because, without it, auditing is destined to be reactive: auditors must design ways to enforce each regulation as they arise, and there is an inevitable delay between design and enforcement in order to test the proposed solutions.
On the other hand, designing a general procedure is challenging because audits can have unintended side effects on the social media ecosystem and its many stakeholders. As a result, we consider two additional questions in this work: 1. How does the proposed auditing procedure affect the platform’s bottom line? We consider whether the procedure imposes a high performance cost on the platform. 2. How does the auditing procedure affect the user’s content? We consider what type of content the platform is incentivized to show the user when the platform complies with the regulation.
Our main contributions are summarized as follows.
Auditing procedure. As our main contribution, we provide a procedure such that, given a regulation, an auditor can test whether the platform complies with the regulation (Section 3). We restrict our attention to regulations that can be written in counterfactual form, including the two examples given at the top of this Introduction. Namely, the procedure applies to any regulation that can be written as: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
”. How
“the ﬁltering algorithm to quantify “similarity” under x and x0 is crucial, and our second main contribution is to provide a precise notion of “similarity” in the context of algorithmic ﬁltering (Section 2). should behave similarly under inputs x and x0 for all (x, x0) 2S
F
Operationally, the auditing procedure has several desirable properties. First, it needs only black-box changes. Second, it does access to the algorithm and therefore holds even if the ﬁltering algorithm not require access to the platform’s users or their personal data. Third, its parameters are interpretable and easy to tune. Finally, the procedure is modular, which allows for many possible conﬁgurations.
F
Provable guarantees. To audit a counterfactual regulation, we begin by observing that algorithmic
ﬁltering is powerful (and often harmful) because information inﬂuences decisions: the content that a user sees can affect how they vote, whether they choose to receive a vaccine, what restaurants they frequent, and more. Therefore, if one seeks to enforce a counterfactual regulation, the notion of
“similarity” that is enforced should be with respect to the outcome of interest: the users’ decisions.
F
Suppose that the auditor seeks to audit content of type T (e.g., the advertisements or the entire feed). generates content Z of type T when given inputs x, and Z 0 when
Suppose the ﬁltering algorithm given x0. In Theorem 1, we prove that, if passes the audit, the decision-making of any user if shown Z and their decision-making if shown Z 0 instead are (asymptotically) indistinguishable. This guarantee—which we call decision robustness and formalize in Section 2—is powerful because it holds for any user and their decision-making even though the audit does not have access to the users’ personal data (e.g., gender) or knowledge about the user’s decision-making tendencies (e.g., how easily a user is inﬂuenced by restaurant advertisements). Providing such a strong guarantee without access to users is made possible by two well-known concepts from decision and learning theory: the hypothesis test and minimum-variance unbiased estimator. Combining these tools is one of our main technical insights and discussed in Section 4.
F
Cost of regulation. In Section 5.1, we study how the audit affects a platform’s ability to maximize an objective function R—which we refer to as reward—and ﬁnd that being audited does not necessarily place a high performance cost on the platform. Studying the cost of regulation is important because there are concerns that regulations can hurt innovation or proﬁts, and our ﬁndings surface conditions under which a performance-regulation trade-off does not exist. Because we leave R unspeciﬁed, the analysis in Section 5.1 is applicable to any R. As examples, R could measure time spent on the platform, the number of clicks on posts, or a combination of these factors.
Content diversity. In Section 5.2, we turn our attention to how an audit would affect the users’ content and discover an unexpected connection to content diversity. We ﬁnd that, under regulation, social media platforms are incentivized to add doses of content diversity. Speciﬁcally, when faced with a regulation, it is in the platform’s interest to ensure that the content of type T that it shows users is sufﬁciently diverse along the dimension by which x and x0 differ for all (x, x0)
. Because content diversity is not a part of the regulatory test by design, this result is unexpected and suggests that content diversity plays a key role in aligning the interests of regulators and platforms. 2S
All proofs are given in the Appendix as well as a toy example and further discussion of the audit. 2 Problem statement
Consider a system with two agents: a social media platform and an auditor.
:
F
X!Z such that Z = 1. The platform selects the content that is shown to its users using a ﬁltering algorithm (x) is the feed produced by
. Here, a
F feed is a collection of content that is shown to a user, and x captures any inputs that the platform uses to ﬁlter, such as the user’s interaction history, social network, available content sources, and so on. Each feed Z = consists of m pieces of content, where z1, . . . , zm}
{ zi 2 can be written as a generative model such that
; ✓(x)), where
F
Rr is unknown.1 Note that if one is only concerned with a certain type T of
✓(x) content (e.g., the auditor only wishes to monitor advertisements), then one can take Z to be the content of type T that is ﬁltered by generates the content in Z by drawing m samples from a distribution pz( rather than the entire feed.
[m]. We assume that given inputs x
Rn for all i 2X
⇥
F
F 2 2 2
·
F 1This representation is without loss of generality. For example, any deterministic mapping from x to Z can be achieved by letting r = nm, ✓(x) = (z>1 , . . . , z>m), and sequentially generating zi from the entries of ✓(x). 2
2. The auditor is given a regulatory guideline that they wish to enforce. The auditor’s goal is in compliance with the given is to check whether the platform’s ﬁltering algorithm
. In other words, the regulation. We assume that the auditor has black-box access to auditor can run
. Note that
} the inputs x and x0 need not correspond to real users and could represent hypothetical users. and observe its outputs on a set of inputs xj}
{
F
Zj = (xj)
F
F
F
{
F 2S
In this work, we restrict our analysis to counterfactual regulations. Speciﬁcally, the auditor is given should behave similarly on content of type T a regulation in the form: “The ﬁltering algorithm under inputs x and x0 for all (x, x0)
.” In the remainder of this work, we omit references to type
T and refer directly to content Z. We give three examples of counterfactual regulations.
Example 1. A regulation that prohibits targeted advertisements from being based on a user’s indicated sexual orientation [65] can be written as “the advertisements shown by should be similar when given two users who are identical except for their sexual orientations”.2
Example 2. A regulation that requires articles containing medical advice on COVID-19 be robust to and whether the user is left- or right-leaning can be framed as “the articles that are selected by provide medical advice on COVID-19 should be similar for left- and right-leaning users”.
Example 3. A regulation prescribing that a platform not sway voting preferences beyond serving as a social network can be framed as “content about political candidates that are injected by should be similar to the content a user would see from its social network without algorithmic ﬁltering.”
F
F
F
The goal of this work is to enforce a regulation of the form “ x0 for all (x, x0)
”. The question remains: What is an appropriate notion of “similarity”? should behave similarly under x and
F 2S 2.1 Decision robustness
We begin by observing that algorithmic ﬁltering is powerful (and often harmful) because information inﬂuences decisions: the content that a user sees can affect how they vote, whether they get vaccinated, what restaurants they frequent, what items they purchase, and more. Stated differently, if algorithmic
ﬁltering did not inﬂuence users’ decisions, then there would be no desire to regulate it.
Therefore, if one seeks to enforce a counterfactual regulation, the notion of “similarity” that is enforced should be with respect to the outcome of interest: the users’ decisions. However, an auditor does not and should not have access to the users or their decisions (e.g., whether they get vaccinated).
As such, the problem that the auditor faces can be stated as follows. (x) in one world and
Consider a (hypothetical) user. Suppose that there are two identical worlds except that the user is (x0) in the other. Because the worlds are otherwise identical, both shown users subsequently face an identical set of queries (e.g., where to eat, whether to get vaccinated, 0 denote the (hypothetical) decisions that the ﬁrst and second users make what to wear). Let
. Then, the auditor enforces similarity by ensuring decision robustness as follows: given queries and
Q
D
D
F
F
Q is decision-robust to (x, x0) if and only if, for any user and any
, one cannot
F determine with high conﬁdence that x
= x0 from the decisions
D
Q and 0.
D (x0)
Decision robustness guarantees that the decision-making behavior of any user under is indistinguishable with respect to (x, x0). However, ensuring decision robustness is challenging because the auditor does not have access to users or their decisions. Our objective is to provide an auditing procedure that guarantees decision robustness given only and black-box access to (x) and
F
F
.
S
F 2.2 Formalizing the auditor’s goal
Recall that
, one cannot determine with high conﬁdence that x and any user and any section, we show that decision robustness can be expressed as a binary hypothesis test. is decision-robust to (x, x0)—and therefore complies with the regulation—when, for 0. In this
= x0 from
Q
D
D
F
Formally, consider a pair of inputs (x, x0) from 0” is equivalent to using and 2S and
. To “determine whether x and set of queries 0 to decide between the following hypotheses:
Q
D
D
D
H0 : ✓(x) = ✓(x0)
D
H1 : ✓(x)
= ✓(x0)
= x0 (1) 2 This example is simpliﬁed to illustrate a simple counterfactual regulation. To protect against more nuanced effects, such as proxy variables, one could modify both sexual orientation and its proxies. Producing counterfactual inputs is out of the scope of this work. We direct interested readers to other texts [58, 41, 52, 43]. 3 6 6 6 6
= ✓(x0) from
H0, H1}
D
D
To see this equivalence, observe that we can write the Markov chain x words, decisions that ✓(x)
! 0, then one also cannot determine that x
. In other depend on inputs x only through the parameters ✓(x). If one cannot determine
✓(x)
!D and
!
D
Z 2{ denote the true (unknown) hypothesis.3 Let ˆH
Let H that is chosen, where the outcome ˆH = H1 is equivalent to determining that ✓(x)
We say that a test ˆH is (1
✏ 2
H0 is (1 test to satisfy P( ˆH = H0| achievable while retaining the property of (1

✏)-conﬁdent, a trivial test that always chooses
✏)-conﬁdent but has 100 percent error when H = H1. Therefore, one would also like the
[0, 1] may be
= ✓(x0) if ˆH = H1 and P( ˆH = H1|
 
✏)-conﬁdent that ✓(x)
 
[0, 1].4 While one would like the test to be (1 2
✏)-conﬁdence. denote the hypothesis
↵ for some small ↵
[0, 1], but not all ↵
H = H1)
H = H0)
= ✓(x0). 2{

  2
= x0.
H0, H1}
 
To this end, we turn to the uniformly most powerful unbiased (UMPU) test [21, 47], which one can think of as follows: if the UMPU test cannot conﬁdently determine that ✓(x)
= ✓(x0), then no other reasonable test can. Formally, suppose that one would like to ﬁnd a test that maximizes the true positive rate (TPR) while ensuring the false positive rate (FPR) is at most ✏ such that the test solves: max ˆH P( ˆH = H1|
H = H1) s.t.
P( ˆH = H1|
H = H0)
✏,
 (2)
  2
H = H1)
If a test ˆH solves (2) for all ✓(x), ✓(x0)
⇥, then it is the uniformly most powerful (UMP) test. The UMPU test is the UMP test among all unbiased tests, where a test ˆH is unbiased if
P( ˆH = H1|
P( ˆH = H1|
[0, 1].5
H = H1). Intuitively, ˆH ⇤✏ is
Let ˆH ⇤✏ denote the UMPU test, if it exists, and ↵⇤(✏) = P( ˆH ⇤✏ = H0| is not decision-robust (i.e., it maximizes the TPR) while the test that is best at detecting when
F of not being decision-robust (i.e., its FPR is at most ✏) making sure that it rarely falsely accuses among all reliable (i.e., unbiased) tests. Given the UMPU test, decision-robustness can be formalized as follows. For ✏, ↵
[0, 1] and when the UMPU test exists,
H = H0) for some  
 
 
F 2 2 is (✏, ↵)-decision-robust to (x, x0) for any Q, P( ˆH ⇤✏ = H0|
H = H1)
↵.
F ()
The goal of the auditing procedure. Therefore, determining whether a platform’s ﬁltering algorithm complies with a counterfactual regulation comes down to determining whether, for all Q and 0. However, this task is not affects users’ decisions 0). In this work, we show that and black-box
F (x, x0)
, the UMPU test cannot conﬁdently reject H0 given
D straightforward because the auditor’s goal is to provide a guarantee on how without access to the users or their decisions (i.e., without Q, it is possible to guarantee approximate asymptotic decision-robustness given only access to using insights from statistical learning and decision theory. and
, or 2S

D
D
D
F
S
F 3 Auditing procedure
In this section, we present a procedure such that, given a regulation on algorithmic ﬁltering that is expressed in counterfactual form, an auditor can test whether the platform’s ﬁltering algorithm is in compliance with the regulation. In Section 4, we show that, if is approximately asymptotically decision-robust. In Section 5, we study the cost of regulation and ﬁnd that there are conditions under which the audit does not place a performance cost on the platform. passes the audit, then
F
F 3.1 Notation and deﬁnitions
Before proceeding, we require some notation and deﬁnitions. Recall that m samples from pz( mapping
Deﬁnition 1. An estimator generates Z by drawing
; ✓(x)), where ✓(x) is unknown. In statistical inference [46], an estimator is a (Z) is an estimate of the parameters ✓ that generated Z.
·
⇥ such that
:
⇥ is unbiased if and only if Epz(
Z!
;✓)[
F
L
: (Z)] = ✓ for all ✓
L
⇥. 2
·
L
Z! 3Although the auditor has access to S and knows whether x
= x0, decision-robustness requires that one
L cannot determine this fact from and
D 4P is taken with respect to pz(
; ✓(x)) and pz(
· 5Intuitively, an unbiased test ˆH ensures that the probability that ˆH chooses x
; ✓(x0)).
D
· 0. Therefore, the hypothesis test treats x and x0 as unknown.
= x0 is always higher when
H1 : x
= x is true than when H0 : x = x0 is true. 4 6 6 6 6 6 6 6 6
Algorithm 1: Modular version of auditing procedure
Input: Regulation parameter ✏
[0, 1]; model family ⇥
⇢
Result: ˆH✏ = H0 if the test is passed; ˆH✏ = H1, otherwise.
; a pair of counterfactual inputs (x, x0) algorithm
X!Z
F 2
:
. 2S
Rr; black-box access to the ﬁltering
+( (x));  L
F
+( (x0));  L
F
˜✓0)>I(˜✓)(˜✓
 
 
Return ˆH✏ = H1; 1 ˜✓ 2 ˜✓0 3 if (˜✓ 4 5 end 6 Return ˆH✏ = H0;
˜✓0)
  2 m  2 r(1
✏) then
 
L

  (Z)
Epz(
;✓)[(
;✓)[(
✓)2]
+(Z)
⇥ is an
Deﬁnition 2. When it exists, the minimum-variance unbiased estimator (MVUE) estimator that is unbiased and has the lowest variance among all unbiased estimators, i.e., satisﬁes
Epz(
Z! r denote the chi-squared distribution with r degrees of freedom and  2 r. Lastly, let I(✓)

·
Let  2 r(q) be deﬁned such that r denote the Fisher information matrix at ✓.
P(v
An exact deﬁnition of I(✓) is given in the Appendix. Intuitively, I(✓) captures how well an estimator (µ,  2), where  2 is can learn ✓ from Z. As a simple example, suppose zi are drawn i.i.d. from known, r = 1, and ✓ = µ. It takes more samples to accurately estimate µ when the variance  2 is large, and, as expected, the Fisher information scales with 1/ 2.
L
⇥ and all ✓
✓)2] for all unbiased r(q)) = q where v
Z!
Rr
 2
 2
⇥.
N
⇠
 
L
L 2 2
⇥
:
·
+ : 3.2 The audit
F whether behave similarly under x and x0 for all (x, x0)
In this section, we present the auditing procedure. Recall that a counterfactual regulation requires that
. Algorithm 1 provides a test for determining 2S
. To test other pairs in
, simply repeat Algorithm 1 and modify x and x0 accordingly. If ˆH✏ = H1 for any pair, then the complies with the given regulation for a pair of inputs (x, x0)
S platform does not pass the audit. Below, we list and explain several characteristics of the audit. 2S
F
Modularity. Algorithm 1 is designed to be modular. Modularity allows the auditor to construct the audit as they wish. For example, the auditor may wish to add more pairs to or to repeat the audit behave similarly under x and x0 for every month. Alternatively, the auditor may require not that
[0, 1] of them.6 To do so, the auditor can run Algorithm 1 all (x, x0) over
, then the platform does not pass the audit. Modularity also allows the auditor to see the pairs (x, x0) for which
  and, if the number of times it returns H1 exceeds ↵ but for at least (1 fails the test. 2S
|S|
↵)
F 2
S
S
Tunable parameter. One beneﬁt of the procedure is that the tunable parameter ✏ has an intuitive meaning. We see in Section 4.1 that ✏ is a maximum FPR. Capping the FPR ensures that the auditor is not distracted by red herrings and prevents the auditor from investing the resources needed to investigate (or bring a case against) the platform unless they are at least (1
✏)-conﬁdent that the platform violates the regulation. Decreasing ✏
[0, 1] reduces the number of false positives while increasing ✏ makes the regulation more strict (at the risk of receiving more false positives).
  2
F
Other advantages. In addition to the beneﬁts regarding modularity and ✏ discussed above, this procedure has two additional advantages. First, the procedure does not require access to users or their
, which means that an auditor does not personal data. Second, it requires only black-box access to need to know the inner-workings of
) and, perhaps more importantly, the procedure works even when (there is often resistance to giving auditors full access to
F
+ denotes the MVUE. We will see
When the MVUE does not exist, use the MLE. Recall that that there is a theoretical justiﬁcation for using the MVUE (Proposition 2). However, there are cases in which the MVUE does not exist but the maximum likelihood estimator (MLE) does. The MLE is a good substitute for the MVUE because the MVUE and MLE are often asymptotically equivalent [59].
When this asymptotic equivalence holds, the main theoretical guarantee—namely, Theorem 1—holds when
+ is taken to be the MLE instead of the MVUE. changes internally.
F
F
F
L
L 6Here, ↵
[0, 1] would correspond to the maximum allowable false negative rate (FNR). 2 5
Symmetry. Algorithm 1 is not symmetric with respect to ✓ and ✓0 (or, equivalently, x and x0). This can be useful if the auditor would like to have a baseline input x and run Algorithm 1 over different x0. If the auditor would like symmetry, they may wish to run Algorithm 1 twice, swapping the order of x and x0, or to alter the Fisher information matrix in Line 3 to be I((˜✓ + ˜✓0)/2), if it exists.
Choice of ⇥. Recall that ⇥ captures the set of possible generative models. In choosing the model family ⇥, the auditor may ﬁnd that a simple ⇥ is more tractable and interpretable while a complex ⇥ is more general. As explained in Section 4.2, ⇥ can also be viewed as the set of possible cognitive models that users employ when making decisions. Therefore, the auditor may wish to choose ⇥ to be just rich enough to mirror the complexity of common cognitive models. 4 Explaining the procedure and its theoretical guarantees
Recall from Section 2 that a ﬁltering algorithm
F
F decision-robust. In Section 4.1, we show that, if the platform passes the audit in Algorithm 1, then
F is guaranteed to be approximately asymptotically decision-robust. In Section 4.2, we provide insights on the role of the MVUE. All proofs are given in the Appendix. complies with a counterfactual regulation if is 4.1 Guarantee on the audit’s effectiveness
Theorem 1. Consider (1). Let ✓⇤ = (✓(x) + ✓(x0))/2. Suppose that zi and z0i are drawn i.i.d. from p( is a regular exponential family that meets the regularity conditions stated in Appendix B. If ˆH is deﬁned as:
; ✓(x0)), respectively, for all i
; ✓(x)) and p(
[m] and pz(
{
; ✓) : ✓
⇥
=
P 2 2
}
·
·
·
 L
✏ as m 2
+(Z 0))
 2 m
 
P( ˆH = H0|
ˆH = H1 () (
L
+(Z)
+(Z 0))>I(✓⇤)(
+(Z)
L
 L r(1
✏),
  (3)

! 1
H = H0)
. If r = 1, then limm then P( ˆH = H1|
Understanding the result. Recall that the goal of an auditor is to determine whether the platform’s
ﬁltering algorithm is decision-robust. Theorem 1 conﬁrms that the audit in Algorithm 1 enforces approximate asymptotic decision robustness. To see this connection, observe that the test in (3) is identical to the test in Algorithm 1
+(Z) is replaced by ✓⇤—which implies that the test in (3) is asymptotically with one substitution— equivalent to the audit. Therefore, Theorem 1 establishes that, if Algorithm 1 returns ˆH✏ = H1, then the auditor is (1 is compliant with a given regulation by determining whether is not decision-robust as m
H = H1) = ↵⇤(✏).
✏)-conﬁdent that
!1
F
F
L
.7
 
F
! 1
Intuitively, if the platform passes the audit in Algorithm 1, then the decision-making of any user that (x) is guaranteed to be ✏-indistinguishable from their decision-making should they have is shown
F (x0) instead. Importantly, this guarantee is provided without access to users or their been shown decisions with the help of insights from statistical learning theory (see proof of Theorem 1).
F
A few remarks. First, ✏ is a false positive rate (FPR). Decreasing ✏ increases the conﬁdence that the auditor would like to have should it pursue action against the platform (see Section 3). Second, recall from Section 3 that Algorithm 1 is modular and that, in most cases, one may wish to repeat it several times with different inputs. In that case, the result of Theorem 1 holds for each individual run. Lastly,
Theorem 1 provides an asymptotic guarantee on the audit. In the next section, we show that, by using the MVUE, Algorithm 1 ensures a notion of decision robustness even for ﬁnite m. 4.2
Insight on the MVUE
Recall that the auditor’s goal is to provide a guarantee with respect to users’ decisions, but the auditor does not have access to the users or their decisions. In this section, we provide intuition for how the auditor enforces decision robustness without this information, and we explain how the use of the
MVUE in Algorithm 1 allows the auditor to enforce a notion of decision robustness for ﬁnite m. One can think of the MVUE as providing an “upper bound” on how much content Z can inﬂuence a user’s decisions. This result is useful because it allows the auditor to reason about users’ decisions without access to users or their decisions, both which can be expensive or unethical to obtain. 7We say that, if passes the audit, it is approximately decision-robust because ˆH✏ is not the UMPU test, as deﬁned in Section 2. Obtaining a UMPU test is generally difﬁcult for r > 1, but the test ˆH✏ is not far from the
UMPU test, as demonstrated by the fact that it is the UMPU test when r = 1.
F 6
·
; ˆ✓) denote the belief of a (hypothetical) user who is shown content Z, where ˆ✓
User model. A user’s decision-making process proceeds in three steps: the user observes information, updates their internal belief based on this information, then uses their belief to make decisions. Let
⇥.8 Let the pz( inﬂuence that Z has on a user’s belief be denoted by 2 (Z).9
; ˆ✓) is a Gaussian 1, 1]
Example 4. As a highly simpliﬁed example, suppose ⇥= [ distribution with mean ˆ✓1 and variance ˆ✓2, where ˆ✓1 = 1 implies that the user does not believe that vaccines are effective, ˆ✓1 = 1 implies the opposite, and ˆ✓2 scales with the user’s uncertainty 0.8, 0.1) in their belief. If a user is easily inﬂuenced, then they might develop the belief after being shown content Z with anti-vaccine content. Alternatively, the user could be conﬁdently pro-vaccine and very stubborn so that no matter what content they see,
L
) = (1, 0).
⇥ such that ˆ✓ =
L
) and pz( (Z) = (
 
 
Z!
[0, 1
 
⇥
L (
:
·
L
· 2Q
Suppose the user is given a query Q
, for which the user decides between two options: A0 and
A1, e.g., whether or not to get vaccinated. (Note that any decision between a ﬁnite number of options can be written as a series of binary decisions.) Internally, the user places a value on each choice such that, if the user were given ✓, the user would choose A0 if v0(✓)
However, ✓ is unknown. Instead, the user has a belief ˆ✓ that they use to infer whether v0(✓) v1(✓).
For example, a user does not know the ground truth values of being vaccinated versus unvaccinated, which may depend on many factors, such as the user’s underlying medical conditions. In the absence of these values, the user forms a belief about the value of receiving a vaccine based on information that they ingest. The user’s decision-making process is therefore a hypothesis test between: v1(✓) and A1, otherwise.
 
 
G0 : v0(✓) v1(✓)
 
G1 : v1(✓) > v0(✓). (4)
+ in the audit by demonstrating that the MVUE
L
The following result motivates the use of the MVUE enforces a ﬁnite-sample version of decision robustness.
Proposition 2. Consider (4). Let G denote the true (unknown) hypothesis. Suppose (x) = that v0, v1 :⇥ such that, for
[m]. Then, if the UMP z1, . . . , zm}
{ (or UMPU) test with a maximum FPR of ⇢ exists, it is given by the following decision rule: reject G0 (choose A1) when the minimum-variance unbiased estimate ˜✓ = v0(˜✓) >⌘ ⇢ where P(v1(˜✓)
G0, G1}
R are afﬁne mappings and there exists u : Rn
; v1(✓(x)) 2
+(Z) satisﬁes v1(˜✓)
G = G0) = ⇢; otherwise, accept G0 (choose A0).
!U v0(✓(x))) for all i
, one can write u(zi) 2{ pu(
!
 
 
⇠
F
L
· v0(˜✓) >⌘ ⇢|
 
L
L
. However,
Interpretation and implications. The auditor is interested in how users react to their content, which is captured by may difﬁcult or even unethical to obtain. For example, an auditor may wish to infer how advertisements affects a user’s behavior, but doing so may require access to the user’s personal data. Proposition 2 says that, under the stated conditions, if one wishes to
+ because, among all study the impact of content on users’ decisions, one can focus on the MVUE possible users, the one whose decisions are most inﬂuenced by their content is the hypothetical user
+. One can think of this hypothetical user as the “most gullible user”. Because Line 3 given by the
+(Z 0) are sufﬁciently close, the audit enforces approximate
+(Z) and in Algorithm 1 requires
+(Z 0)—and therefore
+(Z) and decision robustness by ensuring that the counterfactual beliefs the counterfactual decisions 0—of the most gullible user are indistinguishable. and
L
L
L
L
L
L
D
D
L
Understanding the MVUE. For intuition on why the MVUE is the “most gullible user”, recall that (Z) the MVUE is the unbiased estimator with the lowest variance. Suppose that a user’s estimate
+(Z). By deﬁnition, this estimate is biased or has higher variance. When biased, the differs from user’s estimate is consistently pulled by some factor other than Z. For example, a user who remains pro-vaccine no matter what content they see has a biased estimator. When the user’s estimate has higher variance than the MVUE, it is an indication that the user places less conﬁdence than the
MVUE in what they glean from Z. For example, the user could be skeptical of what they see on social media or scrolling very quickly and only reading headlines. In this way, the MVUE corresponds to the user who “hangs on every word”—whose decisions are most affected by their content Z.
L 8The use of distributions to represent beliefs is common in the cognitive sciences [22]. Note that, although representing beliefs as distributions is borrowed from Bayesian inference [10], our representation does not require that people are Bayesian (update their beliefs according to Bayes’ rule), which is contested [31]. 9A user’s belief may be impacted by information other than Z (e.g., the user’s previous belief, news that they receive from friends, or content that they view on other platforms). This can be modeled by letting captures off-platform information. Because it does not change our results (see
⇥ for notational simplicity.
L the Appendix), we use
⇥ where J
:
Z⇥J !
: 2J
Z!
L 7
5 Cost of regulation and the role of content diversity
In this section, we turn our attention to how the auditing procedure affects (a) the platform’s ability to maximize an objective function R and (b) the type of content the platform is incentivized to ﬁlter when compliant with a regulation. In Section 5.1, we ﬁnd that there are conditions under which the audit does not place a performance cost on the platform and, intuitively, this occurs when the platform has enough degrees of freedom with which to ﬁlter. We show in Section 5.2 that one of the ways the platform can increase R while complying with the regulation is to incorporate sufﬁcient content diversity. Because diversity does not appear in the audit by design, this result suggests that content diversity can align the interests of regulators and platforms. All proofs are given in the Appendix. 5.1 Cost of regulation
Suppose that the platform’s goal is to maximize an objective function R :
R—which we call reward—while passing the audit. For example, R could be a measure of user engagement, user satisfaction, content novelty, or a combination of these and other factors. We leave R unspeciﬁed, which means that our analysis holds for any choice of R, unless otherwise stated.10
Z⇥X !
Z
Recall that a counterfactual regulation deﬁned by ( to a subset
Z
). If there is no regulation, then the smaller the feasible set goal to maximize R given inputs x while complying with the regulation can be expressed as: denotes the set of all possible feeds (or collections of type-T content). Complying with is equivalent to restricting the platform’s choice of content
. Intuitively, the stricter the regulation,
. As such, the platform’s
S
, which we call the feasible set under
⇢Z
) =
Z
Z
Z
S
S
S
S
) ( (
Z 2 arg max (
W
S
)
R(W, x). 2Z
Platforms are often interested in whether a regulation imposes a performance cost. To make this notion precise, we deﬁne the cost of regulation as follows.
Deﬁnition 3. The cost of regulation for inputs x is: C = maxW
R(W, x)
) R(W, x). maxW ( 2Z
  2Z
S
A low cost of regulation implies that the platform can meet regulation without sacriﬁcing much reward, while a high cost of regulation implies that there is a strong performance-regulation trade-off. (
Z
Z to are not contained in
). Then,
Performance-regulation trade-off. Suppose that the feasible set shrinks from the maximum achievable reward is affected in one of two ways. If the feasible set shrinks such
), then the maximum reward that all reward-maximizing solutions in decreases, and the cost of regulation increases. Alternatively, if the feasible set shrinks but at least
), then the maximum reward stays the one reward-maximizing solution in
) preserves at least one same, and the cost of regulation does not increase. Therefore, as long as (near) optimal solution, the cost of regulation is low. The following result formalizes this notion. For this result, we overload the notation R such that R(
Theorem 3. Suppose there exists ⌦
✓1,i = ✓2,i for all i / 2 where ¯✓⌦,i = 0 for all i / 2
Then, if m <
, there exists a set
|
⇥,  > 0 and v
⌦ and a constant > 0 such that v>I(✓ + ¯✓⌦)v <  and ✓ + ¯✓⌦ 2 such that the cost of regulation for x under Algorithm 1 is 0.
< r such that R(✓1, x) = R(✓2, x) if
Rr, there exist a vector ¯✓⌦
⇥.
+(Z), x) denotes R(Z, x).
⌦. Suppose that, for any ✓ is contained in
[r] where 1 <
⇢
Z
Z
Z
Z
Z
⌦
L 2 2
S
S
S
S ( ( (
| 1
Z
Interpretation of the result. Imposing a regulation restricts the platform’s feasible set, which may place a performance cost on the platform. However, a high cost of regulation is not inevitable. Indeed, if the feasible set contains at least one (near) optimal solution, then the cost of regulation is low.
Theorem 3 provides a set of conditions under which the cost of regulation is low. Intuitively, the result states that, when R is independent of at least one element in the parameter vector ✓ and that element has sufﬁcient leverage over the Fisher information, then as long as the amount of content in Z is ﬁnite (i.e., m < is expressive enough, then the platform can 1 always construct a Z from
Z that passes the audit without sacriﬁcing reward.
) and the available content
One may ask whether the conditions in Theorem 3 are feasible. To illustrate that the conditions are achievable, consider the following highly simpliﬁed example.
Z 10There are settings in which R is time-varying, e.g., when a platform’s sources of revenue change with time.
Making R time-varying does not change our analysis or ﬁndings. Therefore, we leave R static for simplicity. 8
R
⇥ 0, and
Example 5. Suppose ✓ = (µ,  2), ⇥= R
. In other
}
  words, we consider the family of 1-D Gaussian distributions. If R is a function of the mean µ but and let ¯✓⌦ = (0, 1). not the variance  2, then Theorem 3 applies. To see this, observe that ⌦=
Recalling that I((µ,  2)) = diag(   4/2), the entries of I(✓) can be made arbitrarily small by increasing ✓2 =  2. By Line 3 in Algorithm 1, the smaller the entries of I(✓), the easier it is for the platform to pass the audit. Therefore, if Z ⇤ is high-reward but not in the feasible set, the platform can still achieve R(Z ⇤, x) by increasing the content’s variance to obtain a new collection of content
Z that is in the feasible set. As long as Z and Z ⇤ share µ, R(Z, x) = R(Z ⇤, x). (µ,  2) : (µ,  2) 2
}
{ 2,   
{N
⇥
=
P 2
Theorem 3 provides conditions under which there is no cost of regulation. These conditions can be relaxed if we are interested in scenarios for which the cost of regulation is low but not zero. We build this intuition in the next section, studying one of the ways a platform can achieve high reward while remaining compliant with regulation. 5.2 Content diversity
In this section, we show that one way that the platform can increases its reward while complying with the regulation is to ensure that the ﬁltered content is sufﬁciently diverse. These results suggest that content diversity may help to align the interests of regulators and platforms.
We ﬁrst formalize content diversity, then show how it relates to passing the proposed audit.
Deﬁnition 4. For v information matrices at
, content Z0 is more diverse than Z1 along v if the Fisher
+(Z1) satisfy: v>(I(
+(Z0))v > 0.
+(Z1))
I( 2
Rr and Z0, Z1 2Z
+(Z0) and
L
L
 
L
L
Interpretation. This deﬁnition says that the “smaller” the Fisher information, the higher the content diversity. The Fisher information matrix I(✓) can be viewed as a measure of how easy it is to learn ✓ from the content Z that is generated by pz(
; ✓). Consequently, when Z and Z 0 have low content
✓(x0)), an auditor can, without much effort, learn that ✓(x) is different from diversity along (✓(x)
= x0. Recall from Section 2.1 that being able to say with high conﬁdence
✓(x0) and therefore that x that x is not decision-robust and therefore does not comply with regulation. In passes the audit.11 Note that the Fisher this way, low content diversity reduces the likelihood that information matrix captures two notions of diversity—the diversity of topics in Z and the diversity of perspectives on each given topic in Z—simultaneously.
 
= x0 implies that
F
F
·
Connection between content diversity and the cost of regulation. Recall from Algorithm 1 that
F passes the audit when (˜✓
˜✓0) is below some threshold. The platform can therefore pass the audit by ensuring that I(˜✓) is sufﬁciently “small”. By Deﬁnition 4, whether the Fisher information is “small” is precisely an indication of content diversity.
˜✓0)>I(˜✓)(˜✓
 
 
In this way, increasing content diversity gives the platform more leeway when ﬁltering. By “shrinking”
˜✓0). Stated differently, if the Fisher information, the platform obtains more ﬂexibility in setting (˜✓ the Fisher information I(˜✓) is “large”, then the platform is more constrained because (˜✓
˜✓0) must be very small in order for the platform to pass the audit. Therefore, if the platform has a high-reward
Z that does not pass the audit (i.e., is not in the feasible set), then the platform can generally maintain a high reward while complying with the regulation by adding content diversity.12
 
 
Because content diversity is not part of the audit by design, this result is unexpected. It states that the audit naturally incentivizes the platform to include a sufﬁcient amount of content diversity with x0. Returning to Example 1, if regulators require that medical advice on COVID-19 respect to x be robust to whether a user is left- or right-leaning, then the differences between the medical advice shown to users across the political spectrum is captured by x x0. Adding content diversity along this dimension means that right-leaning users receive medical advice on COVID-19 not only from right-leaning news outlets, but also from left-leaning ones, and vice versa.
 
  11Content diversity can also be understood in the context of Section 4.2. Suppose that the content diversities of Z and Z 0 are very low. For example, suppose that Z contains only pro-vaccine content and Z 0 contains only anti-vaccine content. Then, the MVUE would learn a strong relationship between vaccines with positive outcomes from Z and vice versa for Z 0. If 0 would reﬂect these is less likely to be decision-robust when the content diversity is low. On the other strong beliefs. In this way, hand, if the content for all users contains both pro- and anti-vaccine content, then 0 are more similar. 12The platform does not increase content diversity indeﬁnitely because the platform must also ensure that contains a query about vaccines, and and
Q
D
D
D
F
D
˜✓0)—do not cause the platform to fail the audit. other terms in Line 3—speciﬁcally, (˜✓
  9 6 6
6