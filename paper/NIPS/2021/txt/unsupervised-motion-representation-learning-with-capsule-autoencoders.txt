Abstract
We propose the Motion Capsule Autoencoder (MCAE), which addresses a key transforma-challenge in the unsupervised learning of motion representations: tion invariance. MCAE models motion in a two-level hierarchy.
In the lower level, a spatio-temporal motion signal is divided into short, local, and semantic-agnostic snippets. In the higher level, the snippets are aggregated to form full-length semantic-aware segments. For both levels, we represent motion with a set of learned transformation invariant templates and the corresponding geometric transformations by using capsule autoencoders of a novel design. This leads to a robust and efﬁcient encoding of viewpoint changes. MCAE is evaluated on a novel Trajectory20 motion dataset and various real-world skeleton-based human action datasets. Notably, it achieves better results than baselines on Trajectory20 with considerably fewer parameters and state-of-the-art performance on the unsu-pervised skeleton-based action recognition task. 1

Introduction
Real-world movements contain a plethora of information beyond the literal sense of moving. For example, honeybees “dance” to communicate the location of a foraging site and human gait alone can reveal activities and identities [10]. Understanding these movements is vital for an artiﬁcial in-telligent agent to comprehend and interact with the ever-changing world. Studies on social behavior analysis [8, 9], action recognition [59, 64], and video summarizing [60] have also acknowledged the importance of movement.
A key step towards understanding movements is to analyze their patterns. However, learning motion pattern representations is non-trivial due to (1) the curse of dimensionality from input data, (2) difﬁ-culties in modeling long-term dependencies in motion sequences, (3) high intra-class variation as a result of subject or viewpoint change, and (4) insufﬁcient data annotation. The ﬁrst two challenges have been ameliorated by the advances in keypoint detection [57], spatial-temporal feature extrac-tors [38, 43, 50], and hierarchical temporal models [13, 49, 56]. The third and the fourth nonetheless remain hurdles and call for unsupervised transformation-invariant motion models.
Inspired by the viewpoint-invariant capsule-based representation for images [11, 17], we exploit capsule networks and introduce the Motion Capsule Autoencoder (MCAE), an unsupervised capsule framework that learns the transformation-invariant motion representation for keypoints. MCAE models motion signals in a two-level snippet-segment hierarchy. A snippet is a movement of a narrow time span, while a segment consists of multiple temporally-ordered snippets, representing a longer-time motion. In both the lower and the higher levels, the snippet capsules (SniCap) and the segment capsules (SegCap) maintain a set of templates as their identities—snippet templates and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
segment templates, respectively—and transform them to reconstruct the input motion signal. While the snippet templates are explicitly modeled as motion sequences, the SegCaps are built upon the
SniCaps and parameterize the segment templates in terms of the snippet templates, resulting in fewer parameters compared with single-layer modeling. The SniCaps and SegCaps learn transformation-invariant motion representation in their own time spans. The activations of the SegCaps serve as a high-level abstraction of the input motion signal.
The contributions of this work are as follows:
• We propose MCAE, an unsupervised capsule framework that learns a transformation-invariant, discriminative, and compact representation of motion signals. Two motion cap-sules are designed to generate representation at different abstraction levels. The lower-level representation captures the local short-time movements, which are then aggregated into higher-level representation that is discriminative for motion of wider time spans.
• We propose Trajectory20, a novel and challenging synthetic dataset with a wide class of motion patterns and controllable intra-class variations.
• Extensive experiments on both Trajectory20 and real-world skeleton human action datasets show the efﬁcacy of MCAE. In addition, we perform ablation studies to examine the effect of different regularizers and some key hyperparameters of the proposed MCAE. 2