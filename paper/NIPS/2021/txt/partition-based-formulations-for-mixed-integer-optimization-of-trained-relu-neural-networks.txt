Abstract
This paper introduces a class of mixed-integer formulations for trained ReLU neural networks. The approach balances model size and tightness by partitioning node inputs into a number of groups and forming the convex hull over the partitions via disjunctive programming. At one extreme, one partition per input recovers the convex hull of a node, i.e., the tightest possible formulation for each node. For fewer partitions, we develop smaller relaxations that approximate the convex hull, and show that they outperform existing formulations. Speciﬁcally, we propose strategies for partitioning variables based on theoretical motivations and validate these strategies using extensive computational experiments. Furthermore, the proposed scheme complements known algorithmic approaches, e.g., optimization-based bound tightening captures dependencies within a partition. 1

Introduction
Many applications use mixed-integer linear programming (MILP) to optimize over trained feed-forward ReLU neural networks (NNs) [5, 14, 18, 23, 32, 36]. A MILP encoding of a ReLU-NN enables network properties to be rigorously analyzed, e.g., maximizing a neural acquisition function
[33] or verifying robustness of an output (often classiﬁcation) within a restricted input domain [6].
MILP encodings of ReLU-NNS have also been used to determine robust perturbation bounds [9], compress NNs [28], count linear regions [27], and ﬁnd adversarial examples [17]. The so-called big-M formulation is the main approach for encoding NNs as MILPs in the above references. Optimizing the resulting MILPs remains challenging for large networks, even with state-of-the-art software.
Effectively solving a MILP hinges on the strength of its continuous relaxation [10]; weak relaxations can render MILPs computationally intractable. For NNs, Anderson et al. [3] showed that the big-M formulation is not tight and presented formulations for the convex hull (i.e., the tightest possible formulation) of individual nodes. However, these formulations require either an exponential (w.r.t. inputs) number of constraints or many additional/auxiliary variables. So, despite its weaker continuous relaxation, the big-M formulation can be computationally advantageous owing to its smaller size.
Given these challenges, we present a novel class of MILP formulations for ReLU-NNs. The formulations are hierarchical: their relaxations start at a big-M equivalent and converge to the convex hull. Intermediate formulations can closely approximate the convex hull with many fewer variables/constraints. The formulations are constructed by viewing each ReLU node as a two-part 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
disjunction. Kronqvist et al. [21] proposed hierarchical relaxations for general disjunctive programs.
This work develops a similar hierarchy to construct strong and efﬁcient MILP formulations speciﬁc to
ReLU-NNs. In particular, we partition the inputs of each node into groups and formulate the convex hull over the resulting groups. With fewer groups than inputs, this approach results in MILPs that are smaller than convex-hull formulations, yet have stronger relaxations than big-M.
Three optimization tasks evaluate the new formulations: optimal adversarial examples, robust veriﬁcation, and (cid:96)1-minimally distorted adversaries. Extensive computation, including with convex-hull-based cuts, shows that our formulations outperform the standard big-M approach with 25% more problems solved within a 1h time limit (average 2.2X speedups for solved problems).