Abstract
We present a new algorithm for the approximate near neighbor problem that com-bines classical ideas from group testing with locality-sensitive hashing (LSH). We reduce the near neighbor search problem to a group testing problem by desig-nating neighbors as “positives,” non-neighbors as “negatives,” and approximate membership queries as group tests. We instantiate this framework using distance-sensitive Bloom Filters to Identify Near-Neighbor Groups (FLINNG). We prove that FLINNG has sub-linear query time and show that our algorithm comes with a variety of practical advantages. For example, FLINNG can be constructed in a single pass through the data, consists entirely of efﬁcient integer operations, and does not require any distance computations. We conduct large-scale experiments on high-dimensional search tasks such as genome search, URL similarity search, and embedding search over the massive YFCC100M dataset. In our comparison with leading algorithms such as HNSW and FAISS, we ﬁnd that FLINNG can provide up to a 10x query speedup with substantially smaller indexing time and memory. 1

Introduction
Nearest neighbor search is a fundamental problem with many applications in machine learning systems. Informally, the task is as follows. Given a dataset D = {x1, x2, ...xN }, we wish to build a data structure that can be queried with any point q to obtain a small set of points xi ∈ D that have high similarity (low distance) to the query. This structure is called an index. Near neighbor indices form the backbone of production models in recommendation systems, social networks, genomics, computer vision and many other application domains.
Applications: In this paper, we focus on algorithms for approximate near neighbor search over high-dimensional large scale datasets. Such tasks frequently arise in genomics, web-scale data mining, machine learning, and other large-scale applications. Consider the Yahoo Flickr Creative Commons dataset (YFCC100M) which consists of 100 million media embeddings that are derived from the neuron activations for a convolutional neural network [21]. Each embedding is a 4096-dimensional vector. The dataset is about 1TB in size and presents a substantial challenge for even the most popular algorithms, which struggle with memory, index construction, and query time. Similar issues occur
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
in genomics, where the task is to identify genome sequences with a high Jaccard similarity to the query. Modern genomic datasets can contain millions of reads with billions of possible n-gram sequences [17]. Many algorithms work well when there are a few hundred dimensions but are inappropriate for such applications. Our experiments demonstrate that for the datasets of interest in this paper, popular indices like HNSW and FAISS can take days to build, require gigabytes of RAM and have a suboptimal precision-recall-query time tradeoff.
Since our goal is to perform approximate search, dimensionality reduction is a reasonable strategy.
However, dimensionality reduction is costly for ultra-high dimensional data. In genomics applications, n-gram sizes are typically very large (n > 18). Thus, the one-hot encoding of each sequence can require billions of dimensions (418 ≈ 68B), making it intractable to learn an embedding model. For embedding applications such as YFCC100M or product search, a large embedding dimension can lead to performance improvements [15]. Dimensionality reduction can incur a performance penalty, so we may wish to perform the near neighbor search over the original metric space.
Ideally, we would choose an algorithm that did not store data points in RAM, evaluate the distance function many times, employ iterative and non-streaming processes such as k-means, or construct complicated structures such as graphs, which are hard to parallelize and distribute. Recent algorithms such as FLASH [22] provide the ability to search based on aggregate LSH count statistics without computing distances, but these methods are heuristics that do not have theoretical guarantees. On the other hand, algorithms such as LSH, which have a well-established theoretical grounding, tend to perform poorly in practice because of their prohibitive hash table size and post ﬁltering stage, which needs many (N ρ in theory) distance computations. In this paper, we present an algorithm having all the practical advantages of a system like FLASH while also being more accurate, theoretically sound, and provably sub-linear. 1.1 Our Contribution
In this paper, we address the computational challenges of high-dimensional similarity search by presenting an index with fast construction time, low memory requirement, and zero query-time distance computations. Our approach is to transform a near neighbor search problem into a group testing problem by designing a test that outputs “positive” when a group of points contains a near neighbor to a query. That is, each test answers an approximate membership query over its group.
Given a query, our algorithm produces a B × R array of group test results that can be efﬁciently decoded to identify the nearest neighbors. This is more efﬁcient than statistical aggregation algorithms like FLASH because each test ﬁlters out entire groups of non-neighbors with a single test operation.
We develop a concrete example of such an algorithm by using Filters to Identify Near Neighbor
Groups (FLINNG). We use a standard non-adaptive group testing design with distance-sensitive
Bloom ﬁlters as tests. We prove that FLINNG solves the randomized nearest neighbor problem in O time, where γ is a query-dependent parameter that characterizes query stability. We also implement FLINNG in C++ and conduct experiments on real-world high-dimensional datasets from genomics, embedding search, and URL analysis, where FLINNG achieves up to a 10x query speedup over existing indices with faster construction time and lower memory.
δ ) log3(N )N 1 (cid:16) log2( 1 2 +γ(cid:17) 2