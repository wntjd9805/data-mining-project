Abstract
This paper discusses model estimation in ofﬂine model-based reinforcement learn-ing (MBRL), which is important for subsequent policy improvement using an estimated model. From the viewpoint of covariate shift, a natural idea is model estimation weighted by the ratio of the state-action distributions of ofﬂine data and real future data. However, estimating such a natural weight is one of the main challenges for off-policy evaluation, which is not easy to use. As an artiﬁcial alternative, this paper considers weighting with the state-action distribution ratio of ofﬂine data and simulated future data, which can be estimated relatively easily by standard density ratio estimation techniques for supervised learning. Based on the artiﬁcial weight, this paper deﬁnes a loss function for ofﬂine MBRL and presents an algorithm to optimize it. Weighting with the artiﬁcial weight is justiﬁed as evaluating an upper bound of the policy evaluation error. Numerical experiments demonstrate the effectiveness of weighting with the artiﬁcial weight. 1

Introduction
Reinforcement learning (RL) is a framework to learn a policy in an unknown environment [1].
Model-based RL (MBRL) is an approach that explicitly estimates a transition model and utilizes it to improve a policy. Compared to model-free RL, one major advantage of MBRL is the data efﬁciency
[2–4], which is important for applications where the data collection is expensive, such as robotics or healthcare. This paper focuses on ofﬂine MBRL, i.e. MBRL that learns a policy from previously collected datasets.
One of the main challenges in ofﬂine MBRL is distribution shift [5]. The distribution of ofﬂine data is different from the distribution of data obtained in the future when applying an improved policy.
This is a situation called covariate shift, where a model estimated using standard supervised learning techniques such as empirical risk minimization (ERM) may not generalize to test data. A major idea in MBRL is to estimate a model using such a standard method and to improve a policy using the estimated model in regions where the predictions are accurate [6–8]. A point to be improved here is model estimation without considering covariate shift. In supervised learning under covariate shift, importance-weighted ERM can obtain good predictive performance [9]. If a model in ofﬂine MBRL can be estimated in a similar way, then the estimated model will make more accurate predictions, resulting in increased policy improvement. Motivated by this, this paper tackles the issue of weighted model estimation considering covariate shift in ofﬂine MBRL.
From the viewpoint of covariate shift, a natural idea is weighting with the ratio of the state-action distributions of ofﬂine data and real future data, as the agent uses a model to predict real future data.
For convenience, this paper calls the ratio “the natural weight.” However, estimating the natural weight is one of the main challenges for off-policy evaluation [5, 10]. Compared to density ratio estimation for supervised learning [11], the difﬁculty of off-policy evaluation is the inaccessibility of samples from the distribution of real future data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
As an alternative, this paper considers weighting with the state-action distribution ratio of ofﬂine data and “simulated” future data. This paper calls the ratio “the artiﬁcial weight,” in contrast to the natural weight. Simulated future data can be generated in ofﬂine simulation, unlike real future data. Since samples from both distributions are available, the artiﬁcial weight can be relatively easily obtained by standard density ratio estimation techniques for supervised learning. Based on the artiﬁcial weight, this paper deﬁnes a loss function for ofﬂine MBRL in Section 4. This paper presents an algorithm to optimize it in Section 5.
The question here is the validity of weighting with the artiﬁcial weight, as it may not be natural for covariate shift. This paper justiﬁes it as evaluating an upper bound of the policy evaluation error in
Section 4. This paper demonstrates that it works in practice in numerical experiments in Section 6.
The above is the contribution of this paper. 2