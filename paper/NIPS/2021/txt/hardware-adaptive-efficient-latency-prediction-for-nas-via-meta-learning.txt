Abstract
For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-speciﬁc constraints (e.g., memory usage, latency and energy con-sumption) and enhance the model efﬁciency. Existing methods on hardware-aware
NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware speciﬁcations, and collecting samples from such a large number of de-vices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efﬁcient Latency Predictor (HELP), which formulates the device-speciﬁc latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model’s performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box func-tions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP. 1

Introduction
Neural Architecture Search (NAS) [39, 1, 25, 22, 21, 36, 5], which aims to search for the optimal architecture for a given task, has achieved a huge practical success by overcoming the sub-optimality of manual architecture designs. However, for NAS to be truly practical in real-world scenarios, it should be hardware-aware. That is, we need to search for the neural architectures that satisfy diverse device constraints (e.g., memory footprint, inference latency, and energy consumption). Due to the practical importance of the problem, many existing works propose to take into account the hardware efﬁciency constraints (mostly latency) in the search process [29, 3, 34, 32, 6, 2, 37, 4, 33].
However, one of the main challenges with such hardware-aware NAS is that collecting training samples (e.g., architecture-latency pairs on each target device) to build reliable prediction models for the efﬁciency metrics, is computationally costly or requires the knowledge of the hardware devices.
Existing hardware-aware NAS methods usually require a large number of samples (e.g., 5k) and train metric predictors for each device from scratch. Additionally, most works [29, 3, 34, 32, 6] are task-speciﬁc, and thus such collection of performance samples should be done from scratch, for a new task. Thus, the sample collection process is prohibitively costly when we consider real-world
∗These authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Concept. Conventional latency estimation methods require a large number of architecture-latency pairs to build a prediction model separately for each device, which is inefﬁcient. Contrarily, the proposed HELP uses a single meta-latency predictor that can fast adapt to any unknown device by collecting only a few latency measurements from it, by utilizing the meta-knowledge of the source device pool. deployment of a model to a large number of hardware devices, for any tasks. OFA [4] alleviate the search cost by utilizing a high-performance network trained on ImageNet as the supernet, which does not require training the models from scratch. Yet, they are still sub-optimal since building device-speciﬁc predictors for metric still requires a large number of samples to be collected, to build a layer-wise latency predictor for each device. This could take multiple hours depending on the task and the device, and becomes a bottleneck for latency-constrained NAS. BigNAS [38] considers FLOP as the efﬁciency metric, but this is a highly inaccurate proxy since latency of an architecture could differ based on its degree of parallelism and memory access cost, for the same FLOP.
To overcome such limitations, we propose a novel sample-efﬁcient latency predictor, namely
Hardware-adaptive Efﬁcient Latency Predictor (HELP), which supports the latency estimation for multiple devices with a single model, by allowing it to rapidly adapt to unseen devices with only a few latency measurements collected from each device. Speciﬁcally, we formulate the latency prediction problem as a few-shot regression task of estimating the latency given an architecture-device pair, and propose a novel hardware embedding that can embed any devices by utilizing the latencies of the reference architectures on each device as its embeddings. Then, we propose a meta-learning framework which combines amortized meta-learning with gradient-based meta-learning, to learn the latency predictor to generalize across multiple devices, utilizing the proposed hardware embeddings.
This allows the model to transfer knowledge learned from known devices to a new device, and thus to achieve high sample efﬁciency when estimating the latency of unseen devices.
HELP is highly versatile and general, as it is applicable to any hardware platforms and architecture search spaces, thanks to our device-agnostic hardware embeddings. Also, HELP can be coupled with any NAS frameworks to reduce its computational bottleneck in obtaining latency-constrained architectures. Especially, when coupled with rapid NAS methods such as MetaD2A [16], OFA [4] and HAT [33], HELP can perform the entire latency-constrained NAS process for a new device almost instantly. We validate the latency estimation performance of HELP on the NAS-Bench-201 space [9] with various devices from different hardware platforms, utilizing the latency dataset for an extensive device pool in HW-NAS-Bench dataset [18]. The results show that our meta-learned predictor successfully generalize to unseen target devices, largely outperforming baseline latency estimation methods using at least 90× less measurements. Then, we combine HELP with existing
NAS methods [4, 33] and show that our meta-latency predictor largely reduce their total search cost.
To summarize, the main contributions of this paper are as follows:
• We formulate the latency estimation of a neural architecture for a given device as a few-shot regression problem, which outputs the latency given an architecture-device pair as the input.
• To represent heterogeneous hardware devices, we propose a novel device-agnostic hardware embedding, which embeds a device by its latencies on reference architectures.
• We propose a novel latency predictor, HELP, which meta-learns a few-shot regression model to generalize across hardware devices, that can estimate the latency of an unseen device using only few measurements. HELP obtains signiﬁcantly higher latency estimation performance over baselines with minimal total time cost.
• We further combine HELP with existing NAS frameworks to show that it leads to ﬁnd latency-constrained architectures extremely fast, eliminating the latency-estimation bottleneck in the hardware-constrained NAS. 2
Figure 2: Overview. For the hardware-adaptive latency estimation of an unseen device for latency-constrained
NAS, we introduce a latency-based hardware embedding and a z modulator of the initial parameters. By formulating the sample-efﬁcient NAS problem as a few-shot regression problem under the meta-learning framework, our meta-learned predictor successfully exploits meta-knowledge θ from the source device pool, to achieve high sample efﬁciency on unseen devices. 2