Abstract
Privacy-protected microdata are often the desired output of a differentially pri-vate algorithm since microdata is familiar and convenient for downstream users.
However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (“sum query”) vs. accuracy for its component sub-populations (“point queries”). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata require-ment, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error O(1/(cid:15)2). With the microdata requirement, one must choose between allowing an additional log2(d) factor (d is the number of point queries) for some point queries or allowing an extra O(d2) factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem. 1

Introduction
Differential Privacy [16] is a mathematical theory of information leakage that allows organizations to publish noisy statistics about their datasets while protecting the conﬁdentiality of user information.
Its state-of-the-art guarantees have resulted in adoption by data collectors such as the U.S. Census
Bureau [31, 10, 23, 1], Google [19, 6], Apple [37], Microsoft [13], Uber [26], and Facebook [33].
In many cases, downstream users want the output of disclosure avoidance systems in the form of microdata (a set of records about individuals). For example, this has historically been the case for tabulations of Census Bureau data, and is currently a requirement for most 2020 Census of
Population and Housing tabulations[20]. However, an end-user study of demonstration data products released by an early prototype of the Census Bureau’s disclosure avoidance system showed signiﬁcant 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
anomalies in the privacy-protected microdata [34].1 They noted the following: the system ﬁrst produced differentially private noisy query answers, called measurements, and then synthesized privacy-protected microdata so that query answers computed from the privacy-protected microdata matched the noisy measurements as closely as possible (based on some objective function). However, after the privacy-protected microdata were created, they compared (1) the original measurement query noisy answers and (2) the values of the same queries computed from the privacy-protected microdata. They noted that in some cases, the query error from the privacy-protected microdata was
“much larger” than the measurement query error [34].
In this paper, we show that such anomalies are an inherent and unavoidable consequence of privacy-protected microdata (they affect all differentially private algorithms that must output microdata).
We further show that the additional errors caused by privacy-protected microdata also satisfy a new uncertainty principle that trades off error between accuracy on populations and accuracy on sub-populations. We next explain this principle.
First, our criterion is per-query expected squared error. That is, if Q is a collection of queries, D is the true data, and (cid:101)D is the privacy-protected microdata, we are interested in the left side of Equation 1 (below), where the expectation is taken over the randomness of the algorithm that ingests D and outputs privacy protected (cid:101)D. max q∈Q (cid:124)
E (cid:101)D[(q(D) − q( (cid:101)D))2] (cid:125) (cid:123)(cid:122)
Our focus: per-query error
≤ E (cid:101)D[max q∈Q (q(D) − q( (cid:101)D))2] (cid:125) (cid:124) (cid:123)(cid:122)
Most other papers: simultaneous/outlier error.
. (1)
This metric measures whether there exist “bad” queries that have systematically large errors on average. It is not to be confused with simultaneous/outlier noise error (right side of Equation 1) that is the focus of most theoretical papers on differential privacy, such as [7]. The reason is that simultaneous error cannot distinguish between systematic error in speciﬁc queries vs. outliers that result by chance when dealing with many random variables. On the other hand per query-error can make this distinction because it considers the average behavior of each query separately.
Next, consider a collection of d disjoint2 counting queries q1, . . . , qd and a special query q∗ that is equal to their sum (q∗(D) = (cid:80) i qi(D)). We call q1, . . . , qd the point queries and q∗ the sum query.
Examples include (1) q∗(D) = “# of Black or African Americans in the data living in California” and qi(D) = “# of Black or African Americans in the data living in county i in California” and (2) q∗(D) = “population of a given county” (which can be used in federal and state-level funding allocations) and qi(D) = “population in census block i in that county” (useful for redistricting).
Thus, for different use-cases, accuracies at these local and aggregate scales are important.
It is well-known that queries q1, . . . , qd, q∗ can be answered using (cid:15)-differential privacy by adding
Laplace(2/(cid:15)) noise to each query [18], thus guaranteeing that each query answer has expected squared error 8/(cid:15)2. However, in this paper, we show that it is not possible to guarantee this kind of error if one is required to produce differentially private microdata (cid:101)D and answer queries using it (i.e., computing q1( (cid:101)D), . . . , qd( (cid:101)D), q∗( (cid:101)D)). Speciﬁcally, suppose an (cid:15)-differentially private microdata-(cid:101)D[(q∗(D) − q∗( (cid:101)D))2] ≤ D2 and producing algorithm can guarantee that, for all datasets D, E maxi E (cid:101)D[(qi(D) − qi( (cid:101)D))2] ≤ C 2 for some constants C and D. Then one has to choose:
• If D2 ∈ O(1/(cid:15)2) then C 2 ∈ Ω( 1 (cid:15)2 log2(d)). That is, making the sum query accurate may force us to take a log2(d) penalty in the expected squared error some of the point queries, or
• If C 2 ∈ O(1/(cid:15)2) then D2 ∈ Ω( d2 (cid:15)2 ). That is, a low per-query error guarantee for point queries may increase expected squared error of the sum query by a factor of d2.
We present such lower bound results for pure differential privacy [16], approximate differential privacy [15], and concentrated differential privacy [8], with nearly matching upper bounds. 1Throughout this paper we use privacy-protected and privacy-preserving synonymously. The Census Bureau prefers “privacy-protected,” whereas the scientiﬁc literature has more often used “privacy-preserving.” Both terms mean that the conﬁdentiality of individual responses has been protected using differentially private algorithms. 2That is, adding/removing a record into the data can only affect the answer to one of the queries. 2
We note that this uncertainty principle affects some, but not all, possible datasets. That is, there are datasets for which the error penalties do not exist. Thus, the goal in practical privacy-protected microdata generation should be to minimize the occurrence of this uncertainty principle (since eliminating it entirely is impossible). To this end, we propose a benchmark suite of real and synthetic datasets that can be used by the wider community for further study of this problem. We also propose some algorithms, inspired by our lower and upper bound proofs, for mitigating the effects of this uncertainty principle. Limitations: empirically, these algorithms perform well on the benchmarks but we do not have theoretical proofs of performance. 2 Preliminaries
Let D denote a dataset, M a differentially private algorithm, and let (cid:101)D be a privacy-preserving dataset (e.g., M (D) = (cid:101)D). A counting query q is associated with a predicate ψ, and the query answer q(D) is the number of records in D that satisfy ψ. We let q1, . . . , qd represent a set of d counting queries whose corresponding predicates ψ1, . . . , ψd are disjoint (no record can satisfy more than one of the predicates). We also let q∗ denote their sum: q∗(D) = (cid:80)d i=1 qi(D). 2.1 Differential Privacy
Differential privacy is currently considered the gold standard in privacy protections. It relies on the concept of neighboring datasets, deﬁned as follows.
Deﬁnition 1 (Neighbors). Two datasets D1 and D2 are neighbors, denoted by D1 ∼ D2, if D1 can be obtained from D2 by adding or removing one record.
Using this concept of neighbors, differential privacy ensures that adding or removing one record from a dataset has little effect on the probabilistic outcomes of an algorithm:
Deﬁnition 2 (Differential Privacy [16]). Given privacy parameters (cid:15) > 0 and δ ≥ 0, a randomized algorithm M satisﬁes ((cid:15), δ)-DP if for all pairs of datasets D1, D2 that are neighbors of each other, and for all S ⊆ range(M ), the following equation holds:
P (M (D1) ∈ S) ≤ e(cid:15)P (M (D2) ∈ S) + δ, where the probability is only over the randomness in M (not the randomness in the data). When
δ = 0, we say that M satisﬁes pure differential privacy (also known as (cid:15)-differential privacy or (cid:15)-DP) and when δ > 0 we say that M satisﬁes approximate differential privacy.
Another important version of differential privacy, is ρ-zCDP (concentrated differential privacy):
Deﬁnition 3 (zCDP [8]). Given a privacy parameter ρ, a randomized algorithm M satisﬁes ρ-zCDP if for all pairs of datasets D1, D2 that are neighbors of each other and all numbers α > 1,
Dα(M (D1)||M (D2)) ≤ ρα where Dα(P ||Q) ≡ 1
α−1 log bility distributions P and Q. (cid:16)
Ex∼P (cid:104) P (x)α−1
Q(x)α−1 (cid:105)(cid:17) is the Renyi divergence of order α between proba-Although zCDP is difﬁcult to interpret, there are useful results that help provide intuition. First, any
M that satisﬁes (cid:15)-differential privacy also satisﬁes ρ-zCDP with ρ = (cid:15)2 2 [8]. In general a ρ-zCDP algorithm does not satisfy pure differential privacy but does satisfy ((cid:15), δ)-DP for inﬁnitely many pairs of (cid:15) and δ that lie along a curve (see [9] and [2] for conversions between ρ-zCDP and ((cid:15), δ)-DP). 2.2 Algorithm Design with Differential Privacy
A few basic principles underlie the construction of many algorithms for differential privacy. The
ﬁrst is sensitivity, which measures the maximum impact that one record can have on a set of queries (regardless of input data):
Deﬁnition 4 (Sensitivity [16]). The Lp global sensitivity of a set Q of queries, denoted by ∆p(Q), is deﬁned as sup
D1∼D2 (cid:16)(cid:80) q∈Q |q(D1) − q(D2)|p(cid:17)1/p
. 3
Global sensitivity can be used with the Laplace and Gaussian distributions to form basic mechanisms.
Let Lap(α) represent a draw from the Laplace distribution with density f (x) = 1 2α e−|x|/α and
N (0, σ2) represent the zero-mean Gaussian distribution with variance σ2. Each appearance of
Lap(α) or N (0, σ2) represents an independent sample from the corresponding distribution.
Theorem 1 (Laplace Mechanism [16]). Given a privacy parameter (cid:15) > 0, a set Q of queries, and an input dataset D, the mechanism M that returns the set of noisy answers {q(D)+Lap(∆1(Q)/(cid:15))}q∈Q satisﬁes (cid:15)-differential privacy.
Theorem 2 (Gaussian Mechanism [8]). Given a privacy parameter (cid:15) > 0, a set Q of queries, and an input dataset D, the mechanism M that returns the set of noisy answers {q(D) +
N (0, ∆2(Q)2/(2ρ))}q∈Q satisﬁes ρ-zCDP.
All of these privacy deﬁnitions are postprocessing invariant [18]. That is, let A be an arbitrary algorithm. Then A ◦ M (i.e., the algorithm that outputs A(M (D))) satisﬁes ((cid:15), δ)-DP (resp., ρ-zCDP) if M satisﬁes ((cid:15), δ)-DP (resp., ρ-zCDP); in other words, the privacy parameters do not degrade.
They also have useful sequential composition properties. Let M1, . . . , Mk be algorithms that satisfy pure differential privacy with corresponding parameters (cid:15)1, . . . , (cid:15)k (resp., zCDP with corresponding privacy parameters ρ1, . . . , ρk), then the algorithm M that releases all of their outputs (i.e., releases
M1(D), . . . , Mk(D)) satisﬁes (cid:80) i (cid:15)i−differential privacy [18] (resp., (cid:80) i ρi-zCDP [8]). 3 The Uncertainty Principle
The setting of d disjoint queries q1, . . . , qd and their sum q∗ are some of the most important types of query sets. As discussed earlier, population counts in small geographic regions such as census blocks (examples of qi) are important for redistricting while population counts in larger regions such as counties (examples of q∗) are used for federal and state funding formulas. Thus any tension between the qi and q∗ can have signiﬁcant impact on the entire U. S. population. While this is just one example of a query set, almost every table produced in previous censuses is a query set with disjoint queries and their sums [40]. Thus this is an important collection of queries to study. 3.1 Lower Bounds
We ﬁrst remove some restrictions on M . While its input is a dataset, its output can be a positively weighted dataset – a collection of records in which each record r has a nonnegative weight w. A query q with predicate ψ can be evaluated over a weighted dataset by summing the weights of the records that satisfy ψ. This simpliﬁes our proofs and slightly increases generality, since normal microdata is a special case of positively weighted data in which all weights are 1 (hence lower bounds for positively weighted data are also lower bounds for normal microdata). It also emphasizes the fact that these lower bounds arise speciﬁcally because negative query answers are disallowed. The lower bound is the following (see supplementary material for proofs).
Theorem 3. Let q1 . . . , qd be a set collection of disjoint queries and let q∗ be their sum. Let M be a randomized algorithm whose input is a dataset and whose output is a positively weighted dataset.
Suppose M guarantees that for each query qi and dataset D, E[(qi(D) − qi(M (D)))2] ≤ C 2 and
E[(q∗(D) − q∗(M (D)))2] ≤ D2 for some values C and D, where the expectation is only over the randomness in M .
• If M satisﬁes (cid:15)-differential privacy then for any k > 0, we have e2(cid:15)(2C+k) ≥ k(d−1) which implies (a) if D2 ≤ λ/(cid:15)2 for some constant λ, then C 2 ∈ Ω( 1
C ≤ λ/(cid:15)2 then D ∈ Ω(d2/(cid:15)2). 16C+8D+4k (cid:15)2 log2(d)), and (b) if
• If M satisﬁes ((cid:15), δ)-DP then for any k > 0, we have 1/4, which implies (a)
Ω (cid:0)min( 1 e4(cid:15)C+2k(cid:15) ≥ if D2 ≤ λ/(cid:15)2 then C 2 ∈
δ )(cid:1); (b) if C ≤ λ/(cid:15)2 then either (cid:15) ∈ O(δ) or D2 ∈ Ω(d2/(cid:15)2).
• If M satisﬁes ρ-zCDP, then the tradeoff function between C and D (which is more complex and omitted due to space constraints) implies: (a) if D2 ≤ λ/ρ for some λ, then C 2 ∈
Ω (log(d)/ρ), and (b) if C 2 ≤ λ/ρ, then for any γ ∈ (0, 1), we must have D2 ∈ Ω(d2γ/ρ). (cid:15) + 4C+2D+k k(d−1) for some constant λ, (cid:15)2 log2(d), 1 (cid:15)2 log2 (cid:15) (cid:16) δ (cid:17) 4
√
Balcer and Vadhan [3] recently showed a statistical price of privacy-preserving release of the top-k counts in a histogram. They proved an analogous O(log2(d/k)) penalty for point queries under (cid:15)-DP (and also results for approximate DP). Interestingly, although they did not consider tradeoffs with the sum query (since its value was assumed to be public in their work), the results in our Theorem 3 (for (cid:15)-DP and approximate DP, but not zCDP) can be proved using the result of their Theorem 7.2.
We also note that the tradeoff functions between C and D in Theorem 3 show a much stronger result than items (a) and (b) in Theorem 3. For example, they rule out the possibility that both C 2 and D2 can simultaneously be just slightly larger than O(1/(cid:15)2). To understand and interpret Theorem 3, let us compare to the Laplace and Gaussian mechanisms, which can produce negative query answers, hence are not equivalent to producing positively weighted datasets (hence not covered by Theorem 3).
It is easy to see that ∆1(q1, . . . , qd, q∗) = 2 and ∆2(q1, . . . , qd, q∗) = can add independent Lap(2/(cid:15)) noise to each query to satisfy (cid:15)-DP, and an algorithm M (cid:48) independent N (0, 1/ρ) noise to each query to satisfy ρ-zCDP. Thus M (cid:48) error of 8/(cid:15)2 for q∗ and each qi (i.e., C 2 = D2 = 8/(cid:15)2). Meanwhile M (cid:48) squared error (C 2 = D2 = 1/ρ). These expected error guarantees hold for all datasets D. 2. Hence, an algorithm M (cid:48) (cid:15)
ρ can add (cid:15) achieves expected squared
ρ achieves 1/ρ expected
Theorem 3 says that privacy-preserving algorithms M that are required to produce positively weighted datasets cannot guarantee the same low error – there are input datasets D for which the expected errors can be signiﬁcantly larger. In the case of M that satisfy (cid:15)-DP, if we want low error for the sum query (e.g., D2 = O(1/(cid:15)2), matching the Laplace mechanism), on some datasets we may need to pay a log2(d) penalty for some point queries (i.e., there will be speciﬁc point queries with consistently large error). On the other hand, if we want low error for the point queries (e.g., C 2 = O(1/(cid:15)2)) then on some datasets we will pay a d2 penalty on the sum query.
In the case of ρ-zCDP, the penalties are smaller. If we want to match the error of the Gaussian mechanism on the sum query, we may need to pay a penalty of log(d) on point queries; if we want
O(1/ρ) expected squared error on each point query, we may need to pay a penalty of nearly d2 on q∗.
For approximate DP, the weakest privacy deﬁnition here, the degradation factor can be roughly log2((cid:15)/δ) no matter how large d is.
Remark 1. The lower bounds in Theorem 3 imply that if privacy-preserving microdata is generated by obtaining noisy measurement query answers (e.g., with the Laplace or Gaussian mechanisms) and then postprocessing the noisy answers (e.g., [28, 24]), some of the measurement queries computed directly from the privacy-preserving microdata will have errors that are larger than their original noisy answers.
Remark 2. All is not lost, however, as the proofs are based on packing arguments that show that these errors are unavoidable for some difﬁcult datasets (but not all datasets are difﬁcult). An example of a difﬁcult dataset D∗ under pure differential privacy is one for which exactly one of the query answers q1(D∗), . . . , qd(D∗) equals log(d)/(cid:15) while the other d − 1 queries equal 0 (clearly, q∗(D∗) = log(d)/(cid:15)). As mentioned earlier, the Laplace mechanism [18], which does not produce microdata, can achieve 8/(cid:15)2 per query error although many of the noisy query answers will be negative. However, the proof of Theorem 3 implies that no algorithm that produces privacy-protected microdata (and hence nonnegative query answers) can do as well on such a dataset. In fact, for this speciﬁc difﬁcult dataset D∗, the large error described by Theorem 3 will either occur for q∗ or for that qi whose answer on D∗ is log(d)/(cid:15). On the other hand, an easy dataset is one for which q1(D), . . . , qd(D) are all large, since almost no effort is needed in ensuring that the privacy-protected query answers are nonnegative. 3.2 Upper Bounds
These lower bounds are nearly tight, as shown by the upper bounds in Theorem 4. The proofs construct postprocessing algorithms that ﬁrst obtain noisy answers a1, . . . , ad, a∗ to the queries q1, . . . , qd, q∗.
A postprocessing step converts the ai and a∗ into consistent noisy answers a(cid:48)
∗ (i.e., they are nonnegative and (cid:80)
∗). Weighted datasets are constructed from the latter quantities. To get weighted datasets with higher accuracy on point queries, the postprocessing ignores a∗ and sets i = max{0, ai}. To obtain synthetic data with higher accuracy on the sum query, a(cid:48) a(cid:48)
∗ is set to a∗ and 1, . . . , a(cid:48) i = a(cid:48) d, a(cid:48) i a(cid:48) 5
i are obtained by minimizing squared distance to the ai subject to the a(cid:48) the a(cid:48) adding up to a∗. The full proofs are in the supplementary material.
Theorem 4 (Upper bound for pure DP and zCDP). Let q1, . . . , qd be a set of disjoint queries and let q∗ be their sum. Given privacy parameters (cid:15) > 0 and ρ > 0, there exist algorithms M(cid:15), Mρ, M (cid:48) (cid:15), M (cid:48)
ρ,
M (cid:48) (cid:15),δ that output a positively weighted dataset and have the following properties: i being nonnegative and 1. M(cid:15) satisﬁes (cid:15)-DP, and for all D and i, E (cid:2)(qi(M(cid:15)(D)) − qi(D))2(cid:3) ≤ 2/(cid:15)2 and
E (cid:2)(q∗(M(cid:15)(D)) − q∗(D))2(cid:3) ≤ 2d2/(cid:15)2. 2. Mρ satisﬁes ρ-zCDP, and for all D and i, E (cid:2)(qi(Mρ(D)) − qi(D))2(cid:3) ≤ 1/(2ρ) and
E (cid:2)(q∗(Mρ(D)) − q∗(D))2(cid:3) ≤ d2/(2ρ). 3. M (cid:48) (cid:15) satisﬁes (cid:15)-DP, and for all D and i, E (cid:2)(qi(M (cid:48) (cid:15)(D)) − q∗(D))2(cid:3) ∈ O(1/(cid:15)2)
E (cid:2)(q∗(M (cid:48) (cid:15)(D)) − qi(D))2(cid:3) ∈ O(log2(d)/(cid:15)2) and 4. M (cid:48)
ρ satisﬁes ρ-zCDP, and for all D and i, E (cid:2)(qi(M (cid:48)
ρ(D)) − qi(D))2(cid:3) ∈ O(log(d)/ρ) and
E (cid:2)(q∗(M (cid:48)
ρ(D)) − q∗(D))2(cid:3) ∈ O(1/ρ) 5. M (cid:48) (cid:15),δ satisﬁes ((cid:15), δ)-DP and for all D and i, E (cid:15),δ(D)) − q∗(D))2(cid:105) (q∗(M (cid:48) (cid:104)
O(log2(1/δ)/(cid:15)2 + 1) and E
M (cid:48) (cid:15) satisfy (cid:15), δ-DP. (cid:104) (qi(M (cid:48) (cid:15),δ(D)) − qi(D))2(cid:105)
∈ O(1/(cid:15)2). Also note M(cid:15) and
∈
Note that Theorem 4 matches the lower bounds in Theorem 3 except for a slight difference for zCDP, where Item 2 of Theorem 4 has a d2 while the lower bound in Theorem 3 has in its place a d2γ for any γ arbitrarily close to 1. 4 Algorithms
For tabular data, typically end-users are interested in multiple marginals of the data. Examples include the gender by age marginals at the national, state, and county levels (for constructing age pyramids); the marginal on race at the national, state, county, tract, and block levels both for demographic research and for enforcement of voting rights; total populations in each state, county, etc. (for various funding formulas). Thus these query sets have many different point query/sum query collections embedded in them. Examples include: female population in a county (sum query) and number of females of each age in the county (point queries); or total Asian population (sum query) and Asian population in each county (point queries). Thus algorithms designed to minimize the appearance of the uncertainty principle should not be designed for a single collection of sum/point queries; instead, they should support many counting queries.
To describe algorithms, it is helpful to view the dataset D as a vector x, where each element i corresponds to a possible record ri. Then x[i] is the number of times ri appears in D. The goal is to produce a privacy-protected version (cid:101)x whose entries are nonnegative real numbers, which can be converted to a positively weighted dataset (cid:101)D ((cid:101)x[i] is the weight of record ri in (cid:101)D). In this setting, a counting query q is just a vector of 1s and 0s with the same dimensionality as x, and the query answer is computed as the dot product q · x.
The algorithms we present here (2 baselines and 2 proposed algorithms) are all based on the idea of
ﬁrst computing noisy query answers and then postprocessing them to obtain (cid:101)x. This setup allows an organization to release both (cid:101)x and the noisy answers (for more statistically-oriented end-users).
Thus, given a set Q of counting queries, for each q ∈ Q, the data collector computes a noisy answer aq by adding noise with distribution Fq to the true answer and then must postprocess them to create microdata.3 We assume the data collector chooses the noise distributions to achieve their desired privacy deﬁnition (e.g., (cid:15)-DP, ρ-zCDP). 3Although a data collector could add noise to a different set of queries and use them to infer the answers to q ∈ Q [43, 29, 42], it is the subsequent postprocessing step that would be more important in mitigating the uncertainty principle. 6
Baseline: NNLS Postprocessing. The ﬁrst baseline we consider is the commonly used nonnegative least squares (NNLS), in which (cid:101)x is produced as the solution to the following optimization problem: (aq − q · (cid:101)x)2 variance(Fq) s.t. (cid:101)x[i] ≥ 0 for all i (cid:101)x ← arg min (cid:101)x (cid:88) q∈Q
Baseline: Max Fitting Postprocessing. The next baseline is an adaptation of a bilevel optimization approach [32] that was originally used for optimization problems whose parameters are sensitive.
The idea here is to ﬁnd the positively weighted datasets whose query answers minimize the L∞ distance to the noisy query answers, breaking ties using least squares error: dist ← min (cid:101)x max q∈Q
|aq − q · (cid:101)x| std(Fq) s.t. (cid:101)x[i] ≥ 0 for all i (cid:101)x ← arg min (cid:101)x (cid:88) q∈Q (aq − q · (cid:101)x)2 variance(Fq) s.t. max q∈Q
|aq − q · (cid:101)x| std(Fq)
≤ dist and (cid:101)x[i] ≥ 0 for all i
Sequential Fitting Postprocessing. Since it is provably not always possible to output microdata that
ﬁts the noisy answers well, we propose an approach that prioritize queries. Thus the query set Q is partitioned by the user into query sets Q1, . . . , Qk. We use the above NNLS approach to ﬁt a vector (cid:101)x1 to the noisy answers of queries in Q1 (highest priority). We then ﬁt (cid:101)x2 to the noisy answers for queries in Q2 (next highest priority) subject to the constraints that (cid:101)x2 matches (cid:101)x1 on queries in Q1.
Then we ﬁt (cid:101)x3 using noisy answers to queries in Q3 while forcing (cid:101)x3 to match (cid:101)x2 on queries in Q1 and Q2, and so on and return the ﬁnal (cid:101)xk at the end. The pseudocode is shown in Algorithm 1. This algorithm is the one that matches the upper bounds in Theorem 4 (referred to as M (cid:48) (cid:15) when the noisy answers aq use Laplace noise, and M (cid:48)
ρ for Gaussian noise).
Algorithm 1: Sequential Fitting (Postprocessing) (cid:80) 1 Input: Query set Q, noisy answers aq for q ∈ Q and noise distributions Fq for q ∈ Q. 2 Input: Q1, . . . , Qk: partition of Q based on query priority. (aq−q·(cid:101)x)2 3 (cid:101)x1 ← arg min variance(Fq) s.t. (cid:101)x[i] ≥ 0 for all i (cid:101)x 4 Fit ← Q1 5 for (cid:96) = 2, . . . , k do (cid:101)x(cid:96) ← arg min (cid:101)x
Fit ← Fit ∪Q(cid:96) (aq−q·(cid:101)x)2 variance(Fq) s.t. (cid:101)x[i] ≥ 0 for all i and q · (cid:101)x = q · (cid:101)x(cid:96)−1 for all q ∈ Fit q∈Q1 q∈Q(cid:96) (cid:80) 6 7 8 Return: (cid:101)xk
Remark. The constrained optimizations in max ﬁtting and sequential ﬁtting are difﬁcult for quadratic program optimizers, often resulting in numerical errors, slow convergence, and infeasibility errors (due to occasional insufﬁcient solution quality in earlier stages of the multistage optimization). They require signiﬁcant engineering effort, tuning of slack parameters (slightly relaxing equality and inequality constraints) and optimizer-speciﬁc parameters. So, an ideal solution would also avoid constraints other than nonnegativity for point queries. This is a rationale for our next method.
ReWeighted Fitting Postprocessing. This method (shown in Algorithm 2) avoids constraints as much as possible in an eventual NNLS solve (Line 14) but is limited to query sets of the form
Q = (cid:83)k i=1 Qi, where the queries inside each Qi are disjoint and have the same noise distribution.
One example is when Q is a collection of marginal queries (e.g., Q1 = marginal on age, Q2 = marginal on age by race, Q3 = marginal on gender by race), which are arguably the most important types of queries. Within each Qi, the algorithm tries to ﬁnd a cutoff value so that queries with noisy answers above it are likely to have true value that is non-zero (Lines 5-6). The idea is that if n† is the number of queries below the threshold, and if they truly had value 0, then their largest noisy value (i.e., the max of n† 0-mean Laplace or Gaussian random variables) should not be near the cutoff with high probability (controlled by the conﬁdence parameter γ). The “low” queries are the ones with noisy answers below the cutoff. The algorithm uses the existing noisy answers to estimate the sum of these “low” queries (Lines 11-12) and adds that “low query sum” (Line 13) to the nonnegative 7
least squares optimization while downweighting the individual low queries (Line 9, the downweight depends on the extreme value distribution of the max of n† 0-mean Laplace or Gaussian random variables, Line 7). To avoid double counting, both places where a “low” query is used (individually and as part of a sum) have their weights cut in half. Note the algorithm only uses existing noisy answers and has no access to the true data.
Algorithm 2: ReWeighted Fitting (Postprocessing) 1 Input: Query set Q = (cid:83)k i=1 Qi; Within a Qi, the queries are disjoint. Fi is the noise distribution of each query in Qi. Given noisy answers aq for q ∈ Q that satisfy the chosen privacy deﬁnition. 2 Input: Conﬁdence parameter γ close to 1 (e.g., 0.99, the setting used in experiments) 3 S ← ∅ for i = 1, . . . , k do 4 a(1), a(2), . . . are the given noisy answers (to queries in Qi) arranged in sorted order j∗ ← smallest j s.t. P (max(j fresh random variable with distributionFi) ≥ a(j))≤ 1 − γ cutof f ← a(j∗). downweight ← median of distribution of max of j random variables sampled from Fi
For each query q ∈ Qi whose noisy answer aq is ≥ cutof f , add (q, aq, 1/var(Fi)) to S.
For each query q ∈ Qi whose aq is < cutof f , add (q, aq, n† i ← number of queries selected in Line 9 (i.e., their noisy answers were < cutof f ) q† ← sum of queries selected in Line 9 a† ← sum of their existing noisy answers
Add (q†, a†, 2∗var(Fi)∗downweight2 ) to S.
) to S 1 1 5 6 7 8 9 10 11 12 13 i var(Fi) 2∗n† (q(cid:48),a(cid:48),w(cid:48))∈S w(cid:48)(q(cid:48)((cid:101)x) − a(cid:48))2 s.t., (cid:101)x[i] ≥ 0 for all i. (cid:80) 14 (cid:101)x ← arg min (cid:101)x 15 Return: (cid:101)x 5 Experiments
To make our code fully open source, we wrote it in Julia [5] and after trying several open-source optimizers, we settled on COSMO [21]. We created a collection of benchmark datasets that were small enough to permit running the postprocessing algorithms thousands of times on each dataset (to estimate expected errors) but large enough to demonstrate the uncertainty principle. The full benchmark of 15 real datasets and 16 synthetic datasets is described in the supplementary material.4
Here we present results for an interesting subset. The only synthetic dataset discussed here, called
Level00-2d, is a 10 × 10 histogram where one element is large (i.e., 10,000) and the others are 0. The other 15 datasets we discuss here were taken from the 2016 ACS Public-Use Microdata Sample [39].
Each represents a 9 × 24 “race by Hispanic origin” histogram from Public-Use Microdata Areas that were considered outliers in their states in terms of racial composition.
For these datasets, we applied the Laplace mechanism with (cid:15) = 0.5 to answer the sum query, both 1-way marginal queries, and identity queries (for each cell, how many people are in it). This is also the priority order used by Sequential Fitting. Error results for the marginals, other privacy parameters and zCDP results can be found in the supplementary material. We ran the Laplace mechanism using different postprocessing strategies (described in Section 4) 1,000 times for each dataset to estimate expected squared error of each query. We added an ordinary least squares (OLS) optimization for comparison purposes (OLS is NNLS without nonnegativity constraints). OLS is free from the uncertainty principle because it does not produce positively weighted microdata. Thus, to minimize the effect of the uncertainty principle, the other postprocessing methods should try to achieve errors that are not much worse than OLS. We note that the multi-stage optimization in Max and Sequential
ﬁtting are generally very difﬁcult for optimization software, so we only kept those runs in which the optimizer succeeded (thus results for Max and Sequential Fitting are slightly optimistically biased).
In Table 1, we show the squared error of these postprocessing methods for the sum query. The NNLS and MaxFitting baselines perform poorly for this query, with errors typically 4-5x those of the OLS method (which is close to the variance of the original noisy answer to the sum query). Meanwhile 4See https://github.com/uscensusbureau/CostOfMicrodataNeurIPS2021 for the code and data. 8
Dataset Nickname
D01
D02
D03
D04
D05
D06
D07
D08
D09
D10
D11
D12
D13
D14
D15
D16
Dataset
Level00-2d
PUMA0101301
PUMA0800803
PUMA1304600
PUMA1703529
PUMA1703531
PUMA1901700
PUMA2401004
PUMA2602702
PUMA2801100
PUMA2901901
PUMA3200405
PUMA3603710
PUMA3604010
PUMA5101301
PUMA5151255
Seq 149.2 106.7 120.3 120.8 134.9 111.4 119.1 109.6 146.0 117.8 126.5 122.9 85.7 129.8 139.2 139.1
Table 1: Squared Error for Sum Query (overall (cid:15) = 0.5))
OLS NNLS MaxFit 533.9 461.9 101.3 500.3 547.2 107.2 571.7 446.1 107.2 426.3 408.1 107.2 426.3 435.3 107.2 677.4 584.0 107.2 443.6 395.1 107.2 329.0 369.3 107.2 472.0 467.8 107.2 558.2 543.7 107.2 464.4 485.2 107.2 301.0 329.1 107.2 293.3 300.3 107.2 386.5 399.9 107.2 369.5 396.1 107.2 280.3 330.7 107.2
ReWeight 108.5 112.5 107.2 109.8 110.9 108.1 110.4 107.5 109.2 110.8 110.8 108.4 108.8 111.3 107.2 107.8
OLS
Total Max 124.0 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 142.9 10516.5 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2 23906.2
NNLS
Total Max 147.4 344.2 135.6 809.0 107.5 1179.8 111.9 1313.0 105.3 1243.8 94.9 562.2 115.9 1516.1 130.0 1954.4 100.0 977.2 97.5 686.9 100.4 944.4 119.6 2189.2 119.2 2884.1 105.9 1432.5 108.3 1474.7 130.3 2239.7
MaxFit
Total Max 173.1 443.7 144.6 910.7 125.4 1235.3 142.3 1385.5 96.7 1257.2 72.1 599.0 129.7 1665.9 147.8 1971.8 109.4 956.4 79.0 705.7 103.2 919.2 134.7 2191.5 149.1 3088.6 120.6 1442.3 101.8 1498.6 124.3 2274.1
Seq
Total Max 283.6 437.3 179.7 782.9 189.8 1171.7 126.3 1049.1 114.3 1019.1 112.9 429.9 156.9 1312.1 311.3 1983.4 121.7 843.4 92.7 534.2 131.6 809.4 142.3 1918.5 140.7 2484.2 122.7 1262.1 203.4 1394.5 178.5 2079.0
ReWeight
Total Max 78.4 159.2 209.8 731.3 141.9 1123.8 136.0 1264.4 160.5 1285.7 78.8 409.8 205.0 1617.1 168.9 1760.1 156.2 930.1 78.7 516.0 138.2 888.2 259.1 2336.1 166.1 2870.4 194.0 1448.6 153.2 1392.9 172.8 2123.0
Data
D01
D02
D03
D04
D05
D06
D07
D08
D09
D10
D11
D12
D13
D14
D15
D16
Table 2: Squared Errors Id Query (overall (cid:15) = 0.5).
Sequential and ReWeighted ﬁtting perform much better. Standard errors were roughly 2-6% of the reported metrics (omitted for space, but shown in the supplementary materials).
For Table 2 we examine the expected errors of each cell query (i.e., qi is the number of people in cell i). We ﬁnd the cell with the largest expected error and report it (the “Max” column). We also ﬁnd the total squared error of the cell queries and report them in the “Total” column. Again, the standard errors are roughly 2-6% of the reported metrics, except that they are sometimes higher for Max and
Sequential ﬁtting since averages were only computing on the subset of runs for which the optimizer did not fail.
Generally, NNLS performed slightly better in terms of the maximum expected error compared to
ReWeight, although their total errors are comparable and ReWeight signiﬁcantly outperforms NNLS on the sum query.
Overall, these experiments and our supplementary material show that both ReWeight and Sequential
ﬁtting (though not perfect) avoid incidents where there are extremely high errors (unlike NNLS and
Max Fitting for sum queries), and this is important in practice. ReWeight and Sequential ﬁtting have similar performance. ReWeight is faster while Sequential needs signiﬁcant tuning of optimizers in order to succeed. However, one advantage of Sequential is its algorithmic transparency – it can 9
directly prioritize queries for the tradeoffs caused by the uncertainty principle (in our experiments, the sum query had highest priority for Sequential Fitting). 6