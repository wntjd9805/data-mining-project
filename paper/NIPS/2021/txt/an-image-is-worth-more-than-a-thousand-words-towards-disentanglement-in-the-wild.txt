Abstract
Unsupervised disentanglement has been shown to be theoretically impossible without inductive biases on the models and the data. As an alternative approach, recent methods rely on limited supervision to disentangle the factors of variation and allow their identiﬁability. While annotating the true generative factors is only required for a limited number of observations, we argue that it is infeasible to enumerate all the factors of variation that describe a real-world image distribution.
To this end, we propose a method for disentangling a set of factors which are only partially labeled, as well as separating the complementary set of residual factors that are never explicitly speciﬁed. Our success in this challenging setting, demonstrated on synthetic benchmarks, gives rise to leveraging off-the-shelf image descriptors to partially annotate a subset of attributes in real image domains (e.g. of human faces) with minimal manual effort. Speciﬁcally, we use a recent language-image embedding model (CLIP) to annotate a set of attributes of interest in a zero-shot manner and demonstrate state-of-the-art disentangled image manipulation results. 1

Introduction
High-dimensional data (e.g. images) is commonly assumed to be generated from a low-dimensional latent variable representing the true factors of variation [3, 29]. Learning to disentangle and identify these hidden factors given a set of observations is a cornerstone problem in machine learning, which has recently attracted much research interest [17, 22, 5, 30]. Recent progress in disentanglement has contributed to various downstream tasks as controllable image generation [44], image manipulation
[14, 15, 42] and domain adaptation [33]. Furthermore, disentangled representations pave the way for better interpretability [18], abstract reasoning [40] and fairness [10].
A seminal study [29] proved that unsupervised disentanglement is fundamentally impossible without any form of inductive bias. While several different priors have been explored in recent works [24, 45], the prominent approach is to introduce a limited amount of supervision at training time, i.e. assuming that a few samples are labeled with the true factors of variation [30]. There are two major limitations of such semi-supervised methods; (i) Manual annotation can be painstaking even if it is only required for part of the images (e.g. 100 to 1000 samples). (ii) For real-world data, there is no complete set of semantic and interpretable attributes that describes an image precisely. For example, one might ask:
“Can an image of a human face be uniquely described with natural language?”. The answer is clearly negative, as a set of attributes (e.g. age, gender, hair color) is far from uniquely deﬁning a face.
Therefore, in this work we explore how to disentangle a few partially-labeled factors (named as attributes of interest) in the presence of additional completely unlabeled attributes. We then show that we can obtain labels for these attributes of interest with minimal human effort by specifying their optional values as adjectives in natural language (e.g. "blond hair" or "wearing glasses"). Speciﬁcally, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
we use CLIP [35], a recent language-image embedding model with which we annotate the training set images. As this model is already pretrained on a wide range of image domains, it provides rich labels for various visual concepts without any further manual effort in a zero-shot manner.
Nonetheless, leveraging general-purpose models as CLIP imposes a new challenge: among the attributes of interest, only part of the images are assigned to an accurate label. For this challenging disentanglement setting, we propose ZeroDIM, a novel method for Zero-shot Disentangled Image
Manipulation. Our method disentangles a set of attributes which are only partially labeled, while also separating a complementary set of residual attributes that are never explicitly speciﬁed.
We show that current semi-supervised methods as Locatello et al. [30] perform poorly in the presence of residual attributes, while disentanglement methods that assume full supervision on the attributes of interest [14, 15] struggle when only partial labels are provided. First, we simulate the considered setting in a controlled environment with synthetic data, and present better disentanglement of both the attributes of interest and the residual attributes. Then, we show that our method can be effectively trained with partial labels obtained by CLIP to manipulate real-world images in high-resolution.
Our contributions are summarized as follows: (i) Introducing a novel disentanglement method for the setting where a subset of the attributes are partially annotated, and the rest are completely unlabeled. (ii) Replacing manual human annotation with partial labels obtained by a pretrained language-image embedding model (CLIP). (iii) State-of-the-art results on synthetic disentanglement benchmarks and real-world image manipulation tasks. 2