Abstract
Efﬁcient exploration in deep cooperative multi-agent reinforcement learning (MARL) still remains challenging in complex coordination problems. In this paper, we introduce a novel Episodic Multi-agent reinforcement learning with
Curiosity-driven exploration, called EMC. We leverage an insight of popular factor-ized MARL algorithms that the “induced" individual Q-values, i.e., the individual utility functions used for local execution, are the embeddings of local action-observation histories, and can capture the interaction between agents due to reward backpropagation during centralized training. Therefore, we use prediction errors of individual Q-values as intrinsic rewards for coordinated exploration and uti-lize episodic memory to exploit explored informative experience to boost policy training. As the dynamics of an agent’s individual Q-value function captures the novelty of states and the inﬂuence from other agents, our intrinsic reward can induce coordinated exploration to new or promising states. We illustrate the advantages of our method by didactic examples, and demonstrate its signiﬁcant outperformance over state-of-the-art MARL baselines on challenging tasks in the
StarCraft II micromanagement benchmark. 1

Introduction
Cooperative multi-agent reinforcement learning (MARL) has great promise to solve many real-world multi-agent problems, such as autonomous cars [1] and robots [2]. These complex applications post two major challenges for cooperative MARL: scalability, i.e., the joint-action space exponentially grows as the number of agents increases, and partial observability, which requires agents to make decentralized decisions based on their local action-observation histories due to communication constraints. Luckily, a popular MARL paradigm, called centralized training with decentralized execution (CTDE) [3], is adopted to deal with these challenges. With this paradigm, agents’ policies are trained with access to global information in a centralized way and executed only based on local
∗Equal contribution.
†Work performed while visiting Tsinghua Univeristy. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
histories in a decentralized way. Based on the paradigm of CTDE, many deep MARL methods have been proposed, including VDN [4], QMIX [5], QTRAN [6], and QPLEX [7].
A core idea of these approaches is to use value factorization, which uses neural networks to represent the joint state-action value as a function of individual utility functions, which can be referred to individial Q-values for terminological simplicity. For example, VDN learns a centralized but factorizable joint value function Qtot represented as the summation of individual value function Qi.
During execution, the decentralized policies can be easily derived for each agent i by greedily selecting actions with respect to its local value function Qi. By utilizing this factorization structure, an implicit multi-agent credit assignment is realized because Qi is represented as a latent embedding and is learned by neural network backpropagation from the total temporal-difference error on the single global reward signal, rather than on a local reward signal speciﬁc to agent i. This value factorization technique enables value-based MARL approaches, such as QMIX and QPLEX, to achieve state-of-the-art performance in challenging tasks such as the StarCraft unit micromanagement [8].
Despite the current success, since only using simple (cid:15)-greedy exploration strategy, these deep MARL approaches are found ineffective to solve complex coordination tasks that require coordinated and efﬁcient exploration [7]. Exploration has been extensively studied in single-agent reinforcement learning and many advanced methods have been proposed, including pseudo-counts [9, 10], curiosity
[11, 12], and information gain [13]. However, these methods cannot be adopted into MARL directly, due to the exponentially growing state space and partial observability, leaving multi-agent exploration challenging. Recently, only a few works have tried to address this problem. For instance, EDTI [14] uses inﬂuence-based methods to quantify the value of agents’ interactions and coordinate exploration towards high-value interactions. This approach empirically shows promising results but, because of the need to explicitly estimate the inﬂuence among agents, it is not scalable when the number of agents increases. Another method, called MAVEN [15], introduces a hierarchical control method with a shared latent variable encouraging committed, temporally extended exploration. However, since the latent variable still needs to explore in the space of joint behaviours [15], it is not efﬁcient in complex tasks with large state spaces.
In this paper, we propose a novel multi-agent curiosity-driven exploration method. Curiosity is a type of intrinsic motivation for exploration, which usually uses prediction errors on different spaces (e.g., future observations [12], actions [11], or learnable representation [16]) as a reward signal. Re-cently, curiosity-driven methods have achieved signiﬁcant success in single-agent reinforcement learn-ing [12, 17, 18]. However, curiosity-driven methods face a critical challenge in MARL: in which space should we deﬁne curiosity? The straightforward method is to measure curiosity on the global observa-tion [12] or joint histories in a centralized way. However, it is inefﬁcient to ﬁnd structured interaction between agents, which seems too sparse compared with the exponentially growing state space when the number of agents increases. In contrast, if curiosity is deﬁned as the novelty of local observation histories during the decentralized execution, although scalable, it still fails to guide agents to coordi-nate due to partial observability. Therefore, we ﬁnd a middle point of centralized curiosity and decen-tralized curiosity, i.e., utilizing the value factorization of the state-of-the-art multi-agent Q-learning approaches and deﬁning the prediction errors of individual Q-value functions as intrinsic rewards.
The signiﬁcance of this intrinsic reward is two-fold: 1) it provides a novelty measure of joint observation histories with scalability because in-dividual Q-values are latent embeddings (i.e., an effective state abstraction [19]) of observation histories in factorized multi-agent Q-learning (e.g., VDN or QPLEX); and 2) as shown in
Figure 1, it captures the inﬂuence from other agents due to the implicit credit assignment from global reward signal during centralized train-ing [20], and biases exploration into promising states where strong interdependence may lie between agents. Therefore, with this novel intrinsic reward, our curiosity-driven method enables efﬁcient, diverse, and coordinated exploration for deep multi-agent Q-learning with value factorization.
Figure 1: CTDE Framework
Besides efﬁcient exploration, another challenge for deep MARL approaches is how to make the best use of experiences collected by the exploration strategy. Prioritized experience replay based on TD errors shows effectiveness in single-agent deep reinforcement learning. However, it does 2
not carry this promise in factorized multi-agent Q-learning, since the projection error induced by value factorization is also fused into the TD error and severally degrades the effectiveness of the
TD error as a measure of the usefulness of experiences. To efﬁciently use promising exploratory experience trajectories, we augment factorized multi-agent reinforcement learning with episodic memory [21, 22]. This memory stores and regularly updates the best returns for explored states. We use the results in the episodic memory to regularize the TD loss, which allows fast latching onto past successful experience trajectories collected by curiosity-driven exploration and greatly improves learning efﬁciency. Therefore, we call our method Episodic Multi-agent reinforcement learning with
Curiosity-driven exploration, called EMC.
We evaluate EMC in didactic examples, and a broad set of StarCraft II micromanagement benchmark tasks [8]. The didactic examples along with detailed visualization illustrate that our proposed intrinsic reward can guide agents’ policies to novel or promising states, thus enabling effectively coordinated exploration. Empirical results on more complicated StarCraft II tasks show that EMC signiﬁcantly outperforms other multi-agent state-of-the-art baselines. 2