Abstract
Most of the medical observational studies estimate the causal treatment effects using electronic health records (EHR), where a patient’s covariates and outcomes are both observed longitudinally. However, previous methods focus only on ad-justing for the covariates while neglecting the temporal structure in the outcomes.
To bridge the gap, this paper develops a new method, SyncTwin, that learns a patient-speciﬁc time-constant representation from the pre-treatment observations.
SyncTwin issues counterfactual prediction of a target patient by constructing a synthetic twin that closely matches the target in representation. The reliability of the estimated treatment effect can be assessed by comparing the observed and synthetic pre-treatment outcomes. The medical experts can interpret the estimate by examining the most important contributing individuals to the synthetic twin.
In the real-data experiment, SyncTwin successfully reproduced the ﬁndings of a randomized controlled clinical trial using observational data, which demonstrates its usability in the complex real-world EHR. 1

Introduction
Estimating the causal individual treatment effect (ITE) using observational data has become increas-ingly common in the medical literature due to the popularization of electronic health records (EHR).
The EHR is a longitudinal collection of records: it contains repeated measurements of a patient’s health condition over irregular time intervals. Although the treatments may also vary over time, many studies consider a point treatment, where the treatment allocation is performed at some observed time and stays ﬁxed during the study [12, 44, 34, 5, 51]. Point treatment is widely applicable to problems involving one-off treatments (e.g. surgical operations) or treatments that do not change frequently (e.g. long term medication for chronic disease). We will refer to the setting above as Longitudinal and Irregularly sampled data with Point treatment setting, or LIP. The LIP setting will be the focus of this work (Figure 1 A).
The LIP setting is different from the conventional settings in causal inference. This is because we observe the pre-treatment outcome yt over time leading to the treatment allocation. These pre-treatment outcomes may unveil the inherent temporal structure in the outcome time series (e.g. trend and seasonality), leading to better ITE estimation over time. In contrast, the conventional settings 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A: Illustration of the LIP setting. Yellow dots: the potential outcomes. B: Illustration of SyncTwin (shaded area: the time points after the treatment allocation). 1. Temporal covariates are encoded as representation vectors. 2. The synthetic twin of a treated target individual is constructed as the weighted average of the few contributors from the control group. 3. The difference between the observed outcome and the synthetic twin outcome estimates ITE. Similar procedures can be carried out for control or new individuals. only consider the outcomes after treatment initiation [54, 52, 36, 43, 6]. This is true even for works that deal with dynamic treatment allocations [40, 35, 11].
A naive way to incorporate pre-treatment outcomes into the standard ITE methods is to treat them as additional covariates. This approach is viable but inadequate: the pre-treatment outcomes are arguably much more closely linked to the outcomes after treatment than the covariates — they hence deserve special considerations, e.g. modifying the architecture or loss function to reﬂect their importance.
Without such modiﬁcation, the ITE methods will not incorporate our prior belief on the importance of the pre-treatment outcomes, which may lead to worse performance.
To bridge this gap, we propose SyncTwin, a novel ITE estimation method tailored for the LIP setting.
We assume that the temporal outcomes are generated by individualized latent factors and time-varying latent trends. This assumption is similar to the “factor model” assumption commonly used in
Econometrics [1] and allows us to achieve the right balance between the parametric assumption and modeling ﬂexibility. Figure 1 B illustrates the schematics of SyncTwin using an example with two treatment options (treated and control). SyncTwin ﬁrst uncovers the individualized latent factors using representation learning. For a target individual, SyncTwin selects and weights a few contributors based on their latent factors and a sparsity constraint. It proceeds to construct a synthetic twin whose temporal outcomes are the weighted average of the contributors. Finally, the ITE is estimated as the difference between the target individual and the synthetic twin’s outcomes.
Unlike conventional methods, SyncTwin does not use the learned latent factors to directly predict the outcomes; instead it uses these variables to ﬁnd the contributors and their importance weights so as to construct the synthetic twin. This approach brings two bonus features; both are important to medical applications. (1) We can calculate an individualized estimation error bound based on the actual and the synthetic outcomes before treatment. In practice, the clinician can accept the recommended treatment when the error bound is below a threshold and resort to expert knowledge otherwise. (2)
We can identify the most important contributors to an estimate as the ones who receive the highest weights. The clinician can further examine these contributors (e.g. whether they are indeed similar to the target individual) to validate and interpret the estimate.
Contributions. (1) We develop SyncTwin to leverage the temporal structure in the outcomes for better ITE estimation. (2) We provide theoretical justiﬁcations for each step involved in SyncTwin and prove an individualized error bound that can be used to identify untrustworthy estimates. (3)
In addition to extensive simulations, we conduct an observational study using real EHR data and successfully reproduced the ﬁndings of a randomized controlled clinical trial with SyncTwin. 2 Problem setting
We consider an observational study with N individuals indexed by i ∈ [N ] = {1, . . . , N }. Let ai ∈ {0, 1} be the treatment indicator with ai = 1 if i received the treatment at some time and ai = 0 2
otherwise1. We realign the time steps such that all treatments were initiated at time t = 0. Let
I1 = {i ∈ [N ] | ai = 1} and I0 = {i ∈ [N ] | ai = 0} be the set of the treated and the control individuals respectively. Denote N0 = |I0| and N1 = |I1| as the sizes of the groups.
Let Xi = [xis]s∈[Si] be the temporal covariates consisting of a sequence of observations xis ∈ RD.
Let Mi = [mis]s∈[Si] be the sequence of binary masking vectors mis ∈ {0, 1}D, where [mis]d = 1 indicates the dth element of xis is measured and [mis]d = 0 otherwise. The entire sequence Xi contains Si ∈ N observations taken before treatment at times Ti = [tis]s∈[Si], tis < 0. The maximum sequence length S = maxi(Si).
The outcome yit ∈ R is observed at time t ∈ T − ∪ T +. Let T − = {−M, . . . , −1} and T + =
{0, . . . , H − 1} be the observation times before and after treatment allocation. We arrange the outcomes after treatment into a H-dimensional vector denoted as yi = [yit]t∈T + ∈ RH . Similarly i = [yit]t∈T − ∈ RM . deﬁne pre-treatment outcome vector y−
Using the potential outcome framework [41], let yit(ai) ∈ R denote the potential outcome at time t in a world where i received the treatment as indicated by ai. Let yi(1) = [yit(1)]t∈T + ∈ RH , and i (1) = [yit(1)]t∈T − ∈ RM . Similarly, let yi(0) = [yit(0)]t∈T + and y− y− i (0) = [yit(0)]t∈T − . The individual treatment effect (ITE) is deﬁned as τi = yi(1) − yi(0) ∈ RH . 3
ITE estimation via SyncTwin
SyncTwin estimates the ITE by predicting both potential outcomes yi(1) and yi(0) and take the difference. Note that for an individual in the dataset i ∈ [N ], it is only necessary to predict the counterfactual outcome yi(1 − ai) because the factual outcome yi(ai) = yi is observed (under the consistency assumption to be discussed later). Without loss of generality, we will use estimating yi(0) as an example in the following sections. One can estimate yi(1) using the same method. 3.1 Assumptions
SyncTwin relies on three assumptions. (1) Stable Unit Treatment Value Assumption [41]: yit(ai) = yit, ∀i ∈ [N ], t ∈ T − ∪ T +. (2) No anticipation, also known as causal systems [4]: yit = yit(1) = yit(0), ∀t ∈ T −, i ∈ [N ]. (3) Data generating process (DGP). The DGP assumption involves two parts. First, the causal directed acyclic graph (DAG) is speciﬁed in Figure 2 [37], which involves a latent factor ci ∈ RK. Secondly, we assume the potential outcomes have a parametric form: yit(0) = q(cid:62) (1) where qt ∈ RK, K < min(M, H) is a weight vector and ξit is the white noise. Equation 1 is commonly referred to as a latent factor model with ci as the individualized latent factor and qt as the time-varying latent trend [10]. Without loss of generality, we require the weight vectors ||qt|| = 1,
∀t ∈ T − ∪ T + [50]. t ci + ξit,
∀t ∈ T − ∪ T +,
Discussion. SyncTwin differs from the nonparametric ITE estimation methods [27] because it assumes the paramet-ric form in Equation 1. In essence, it means that we can fully separate the time effect from the individual effect.
We would like to highlight two aspects of this assumption.
First, the latent factor ci does not change over time. It provides a constant link between the pre-treatment out-comes and the post-treatment outcomes. Secondly, the time-varying latent trend qt is common to all individuals.
Together, they ensure that any linear combination of dif-ferent individuals’ outcomes will automatically preserve qt, i.e. (cid:80) (cid:80) i biξit, for any weights bi ∈ R, ∀i ∈ [N ]. For this reason, SyncTwin bypasses qt and directly ﬁnds the weights bi’s to issue counterfactual predictions. As we will show later, doing so allows us to derive various theoretical results to inform the design of SyncTwin . It also leads to a checking procedure to validate estimation quality and improvements in interpretability.
Figure 2: The DAG of the assumed data generating model. i biyit(0) = q(cid:62) t i bici + (cid:80) 1SyncTwin can model more than two treatment options, but we focus on binary treatments for illustration. 3
The DAG in Figure 2 is different from the one commonly used in the static setting [41]. We assume the latent factor ci captures the physiological factors that impact both the outcome and the covariates.
Here, the covariates xis is a “confounder” in a general sense because it opens up a backdoor path from treatment to outcome, i.e. ai → xis → ci → yis [37]. Based on the backdoor criterion [37], adjusting for the covariates xis is sufﬁcient to identify the treatment effect from observational data.
We compare our assumptions with those used in the related works in Appendix A.2. In particular, we show that our assumption is weaker than the one used by Synthetic Control, a widely-applied method in Econometrics [2]. In Appendix A.5, we further demonstrate the plausibility of our assumptions in real-world scenarios. In the simulation study (Section 5.1) we show experimentally that SyncTwin performs well even when the data is not generated from the assumed DGP exactly but instead from a set of differential equations. Finally, and perhaps most importantly, we show that based on our assumptions, SyncTwin is able to reproduce the ﬁndings of a large-scale randomized controlled trial using observational data (Section 5.2). 3.2 Learning to represent temporal covariates
The latent factor ci plays an important role as it affects the covariates xis and the outcomes yit.
SyncTwin uses deep neural networks to learn the representation ˜ci as a proxy for ci.
SyncTwin is agnostic to the exact choice of architecture as long as the network translates the irregularly sampled temporal covariates into a ﬁxed-sized representation vector. For this reason, we use the well-proven sequence-to-sequence architecture (Seq2Seq) [47] with a standard attentive encoder
[9] and a LSTM decoder [26]. The learned representation ˜ci = fe(Xi, Mi, Ti; θe), where fe is the encoder with trainable weights θe. The reconstructed covariates ˆXi = fd(˜ci, Ti; θd), where fd is the decoder with trainable weights θd. To ensure the learned representation ˜ci is a linear predictor of the potential outcome yi(0) (Equation 1), we introduce a trainable parameter ˜Q ∈ RH×K and deﬁne
˜yi(0) := ˜Q · ˜ci. Note that using a nonlinear function to map from ˜ci to ˜yi(0) will be inconsistent with the DGP and will not uncover the latent factor ci as desired.
We train the networks end-to-end by optimizing the loss function Ltr = λrLr + λpLs, where λr and
λp are hyper-parameters2 balancing the supervised loss Ls and the reconstruction loss Lr:
Ls(D0) = (cid:88) i∈D0
||˜yi(0) − yi(0)||2; Lr(D0, D1) = (cid:88) i∈D0∪D1
||( ˜Xi − Xi) (cid:12) Mi||2, (2) where D0 ⊆ I0 and D1 ⊆ I1 are training data, mis is the masking vector, (cid:12) represents element-wise product and || · || is the L2 norm. In Proposition 1, we show that minimizing the supervised loss
Ls will reduce the error bound on the learned representations, making them closer to the true latent factor ci. The proof and the motivations for Lr are shown in A.1.1.
Proposition 1 (Error bound on the learned representations). Under the assumptions in Section 3.1, the total error on the learned representations for the control is bounded by: (cid:88) j∈I0 (cid:107)cj − ˜cj(cid:107) ≤ βLs + (cid:88) j∈I0 (cid:107)ξj(cid:107), (3) where Ls is the supervised loss, β is a constant depending on qt and ˜Q, and ξj is the white noise (Equation 1 and 2). 3.3 Synthesizing the twin
At this point, one may be temped to predict the counterfactual outcome yi(0) of a treated individual i ∈ I1 with the output of the neural network ˜yi(0). However, ˜yi(0) is issued by a black-box neural network, which is not easily interpretable. As a remedy, SyncTwin explicitly constructs a synthetic twin who matches the target in representations. As we will show, this approach is able to control estimation bias and is more interpretable.
The synthetic twin of a target individual is deﬁned by a set of weights bi := [bij]j∈I0 ∈ RN0, each associated with a contributor in the control group j ∈ I0 (or treatment group when predicting ˆyi(1)). 2The hyperparamter sensitivity is studied in A.13. λr and λp do not signiﬁcantly impact the performance. 4
SyncTwin solves the following optimization problem to ﬁnd the weights: bi = arg min
˜bi (cid:107)˜ci − (cid:88) j∈I0
˜bij˜cj(cid:107)2 s.t. ˜bij ≥ 0, ∀j ∈ I0 and
˜bij = 1, (cid:88) j∈I0 (4) where ˜cj is the representation learned by the encoder. Denote the loss function in Equation 4 as
Lm. We will call Lm the matching loss because it indicates how well the synthetic twin matches the target individual in representations. Let dc i denote the optimal value of Lm and let ˆci be the synthetic representation given by the solution bi: dc i := (cid:107)˜ci − (cid:88) j∈I0 bij˜cj(cid:107)2;
ˆci := (cid:88) j∈I0 bij˜cj (5)
The optimization procedure is detailed in Appendix A.8. SyncTwin predicts the potential outcome yit(0) for a target individual i using the same weights bi: ∀t ∈ T − ∪ T +,
ˆyit(0) = (cid:88) j∈I0 bijyjt(0) = (cid:88) j∈I0 bijyjt, (6) j b∗ j b∗ where the last equality follows from the consistency assumption (Section 3.1). Denote ˆyi(0) =
[ˆyit(0)]t∈T + as the predicted potential outcomes after treatment initiation.
The estimator in Equation 6 follows directly from the DGP assumption (Equation 1). To see this, remember that yit(0) is linear in the latent factor ci. Hence, if we ﬁnd a set of weights b∗ ij to match the latent factor, i.e. ci = (cid:80) ijcj, the same set of weights will also match the outcome yit(0) ≈ (cid:80) ijyjt(0) up to random noise. In practice, since we do not observe ci, we have to perform matching on the learned representation ˜ci as in Equation 4.
Therefore we can see that the quality of the estimator ˆyit(0) depends on two things: (1) whether the learned representation ˜c is close to the true latent factor c and (2) whether good matching weights bi can be found on the learned representation. SyncTwin attempts to fulﬁll the ﬁrst condition by representation learning in Section 3.2. The second condition is facilitated by solving the optimization problem in Equation 4. Proposition 2 formalizes this intuition and shows that when a perfectly-matching twin is found, the estimation error only depends on the quality of the learned representations (proved in A.1.1).
Proposition 2 (Bias bound on counterfactual prediction). Suppose that dc (Equation 1 and 5), the absolute value of the counterfactual prediction bias for i is bounded by: (cid:16)
|E[ˆyi(0) − yi(0)]| ≤ |T +| i = 0 for some i ∈ I1 (cid:107)ci − ˜ci(cid:107) + (cid:107)cj − ˜cj(cid:107) (cid:88) (cid:17)
. j∈I0
Notes on Interpretability. In addition to reducing bias by learning and matching representations,
SyncTwin is also more interpretable. In particular, the weights bi can be interpreted as the “contribu-tion” or “importance” of a contributor j to the target i due to the optimization constraints. We can create a shortlist of the most important contributors based on bi. A domain expert can understand the rationale behind the estimate by examining the shortlist. For instance, they can check whether the important contributors share similar disease progression patterns as the target individual.
This procedure corresponds to the notion of data point interpretability or example-based interpretabil-ity [45], where one explains the prediction by presenting the most relevant data points to the users.
Note that the learned representations ˜ci are internal to the algorithm; we do not intend to show these representations to the user or to make interpretations on them. 3.4 Calculating individualized error bound with pre-treatment outcomes
Since Equation 6 applies to all time points before and after treatment, let ˆy− the predicted pre-treatment potential outcome vector. We deﬁne dy pre-treatment period: i (0) = [ˆyit(0)]t∈T − be i as the estimation error in the dy i = (cid:107)ˆy− where || · ||1 is the vector (cid:96)1-norm. The second equality allows dy
It holds because of the no anticipation assumption (Section 3.1). Achieving a small error dy (7) i to be evaluated for all individuals. i implies i (0)(cid:107)1 = (cid:107)ˆy− i (0) − y− i (0) − y− i (cid:107)1, 5
Table 1: Problem settings considered in the literature. “Static”: observed (or allocated) only once;
“Regular”: observed (or allocated) over time at a regular frequency; “Irregular”: observed over time irregularly; “-”: not observed or modeled. * can be extended to Irregular. † can be extended to
Regular. LIP: Longitudinal, Irregular, Point treatment.
Setting
Example
Pre-treatment y−
X
Treatment a
Static
DT
SC
LIP (This work) This work
[43]
[11]
[2]
Static*
-Regular*
-Regular
Regular
Irregular Regular
Static
Regular
Static
Static
Post-treatment Nonlinear f : y = f (X) (cid:88) (cid:88)
× (cid:88) y
Static†
Regular
Regular
Regular that the synthetic twin ˆci matches well with the true latent factor ci due to the linearity between y− i (0) and ci (Equation 1). Since ci is assumed to be constant over time, the pre-treatment error can be used to assess the post-treatment error. We formalize this intuition in Proposition 3 and use dy i to control the error in the post-treatment period (proved in A.1.1).
Proposition 3 (Error control under no hidden confounders). Given any target error threshold δ > 0, deﬁne the acceptance group of treated individuals as
Aδ = (cid:8)i ∈ I1|dy i ≤ δ|T −|/|T +|(cid:9) .
Under the assumptions in Section 3.1, the post-treatment estimation error |E[ˆyi(0)] − E[yi(0)]| ≤ δ,
∀i ∈ Aδ.
Proposition 3 shows that we can control the estimation error to be below a certain threshold δ by rejecting the estimate if its error dy i during the pre-treatment period is larger than δ|T −|/|T +|.
Alternatively, we can rank the estimation trustworthiness for the individuals based on dy i . This is helpful when the user is willing to accept a percentage of estimations which are deemed most trustworthy. 3.5 Training, validation and inference
We perform model training, validation and inference (testing) on three disjoint datasets. In summary, we train the encoder and decoder on the training data using the loss functions described in Section 3.2. The validation data is then used to validate and tune the hyper-parameters of the encoder and decoder. Then, we solve the optimization problem in Section 3.3 to ﬁnd the weights bi for the target individuals in the testing data. Finally, we compute the potential outcome estimator (Section 3.3) and the individualized error bound (Section 3.4). The pseudocode is described in A.7. 4