Abstract
Capturing accurate uncertainty quantiﬁcation of the predictions from deep neural networks is important in many real-world decision-making applications. A reliable predictor is expected to be accurate when it is conﬁdent about its predictions and indicate high uncertainty when it is likely to be inaccurate. However, modern neural networks have been found to be poorly calibrated, primarily in the direction of overconﬁdence. In recent years, there is a surge of research on model calibration by leveraging implicit or explicit regularization techniques during training, which achieve well calibration performance by avoiding overconﬁdent outputs. In our study, we empirically found that despite the predictions obtained from these regular-ized models are better calibrated, they suffer from not being as calibratable, namely, it is harder to further calibrate these predictions with post-hoc calibration methods like temperature scaling and histogram binning. We conduct a series of empirical studies showing that overconﬁdence may not hurt ﬁnal calibration performance if post-hoc calibration is allowed, rather, the penalty of conﬁdent outputs will compress the room of potential improvement in post-hoc calibration phase. Our experimental ﬁndings point out a new direction to improve calibration of DNNs by considering main training and post-hoc calibration as a uniﬁed framework. 1

Introduction
Modern over-parameterized deep neural networks (DNNs) have been shown to be very powerful modeling tools for many prediction tasks involving complex input patterns [37]. In addition to obtaining accurate predictions, it is also important to capture accurate quantiﬁcation of prediction uncertainty from deep neural networks in many real-world decision-making applications. A reliable predictive model should be accurate when it is conﬁdent about its predictions and indicate high uncertainty when it is likely to be inaccurate. However, modern DNNs trained with cross-entropy (CE) loss, despite being highly accurate, have been recently found to predict poorly calibrated probabilities, unlike traditional models trained with the same objective [4]. The overconﬁdent predictions of DNNs could cause undesired consequences in safety-critical applications such as medical diagnosis and autonomous driving. Bayesian DNNs, which indirectly infer prediction uncertainty through weight uncertainties, have innate abilities to represent the model uncertainty
[2, 16]. But training and inferring those bayesian models are computationally more expensive and conceptually more complicated than non-bayesian models, and their performance depends on the form of approximation made due to computational constraints. Therefore, the study on uncertainty calibration of deterministic DNNs is important for both development practice and the perspective of understanding DNNs.
Post-hoc calibration addresses the miscalibration problem by equipping a given neural network with an additional parameterized calibration component, which can be tuned with a hold-out validation
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dataset. Guo et al. [4] experimented with several classical calibration ﬁxes and found that simple post-hoc methods like Temperature Scaling (TS) [25] and Histogram Binning (HB) [33] are signif-icantly effective for DNNs. The authors of [10] and [27] proposed to learn linear and non-linear transformation functions to rescale the original output logits respectively. Gupta et al. [5] proposed to obtain a calibration function by approximating the empirical cumulative distribution of output probabilities via splines. Kumar et al. [11] proposed to integrate TS with HB to achieve more stable calibration performance. Patel et al. [24] proposed a mutual information maximization-based binning strategy to solve the severe sample-inefﬁciency issue in HB.
Recently, there is another line of research which presents a possibility of improving the calibration quality of deterministic DNNs via regularization during training. Guo et al. [4] found that training
DNNs with strong weight decay, which used to be the predominant regularization mechanism for training neural networks, has a positive impact on calibration. Müller et al.
[19] showed that training models using the standard CE loss with label smoothing [28], instead of one-hot labels, has a very favourable effect on model calibration. Mukhoti et al. [18] proposed to improve uncertainty calibration by replacing the conventionally used CE loss with the focal loss proposed in
[14] when training DNNs. It is important to note that CE loss with label smoothing and focal loss can be considered as standard CE with an additional maximum-entropy regularizer, which means minimizing these losses is equivalent to minimizing CE loss and maximizing the entropy of the predicted distribution simultaneously [18, 17]. Following these studies, a recent work [7] explored several explicit regularization techniques for improving the predictive uncertainty calibration directly.
In this paper, we conduct an empirical study showing that despite the predictions obtained from the regularized models are well calibrated, they suffer from worse calibratable, namely, it is harder to further improve the calibrate performance with post-hoc calibration methods like temperature scaling and histogram binning. We found that the regularization works by simply aligning the average conﬁdence of the whole dataset to the accuracy with some speciﬁc regularization strengths, and cannot achieve ﬁne-grained calibration. The comparison results show that when post-hoc calibration methods are allowed, the standard CE loss yields better calibration performance than those regularization methods. The extended experiments demonstrate that regularization will make DNNs lose the important information about the hardness of samples, which results in compressing the room of potential improvement by post-hoc calibration. Based on the experimental ﬁndings, we raise a natural question: can we design new loss functions in the opposite direction of these regularization methods to further improve the calibration performance? To this end, we propose inverse focal loss, and empirically found that it can learn more calibratable models in some cases compared with the CE loss, though it causes severer overconﬁdence problem without post-hoc calibration. Most importantly, our ﬁndings show that overconﬁdence of DNNs is not the nightmare in uncertainty qualiﬁcation and point out a new direction to improve the calibration of DNNs by considering main training and post-hoc calibration as a uniﬁed framework. 2 Preliminaries (cid:80)K
Let Y = {1, ..., K} denote the label space and X = Rd denote the feature space. Given a sample (x, y) ∈ X × Y sampled from an unknown distribution, a learned neural network classiﬁer f θ :
X → ∆K can produce a probability distribution for x on K classes, where ∆K denotes the K − 1 dimensional unit simplex. Here we assume f θ as a composition of a non-probabilistic K-way classiﬁer gθ and a softmax function σ, i.e. f θ = gθ ◦ σ. For a query instance x, f θ gives its exp(gθ i (x)) probability of assigning it to label i as i (x) denotes the i-th element of the k=1 exp(gθ logit vector produced by gθ. Then, ˆy := arg maxi f θ i (x) can be returned as the predicted label and
ˆp := maxi f θ
Expected Calibration Error (ECE) For a well-calibrated model, ˆp is expected to represent the true probability of correctness. Formally, a perfectly calibrated model satisﬁes P(ˆy = y|ˆp = p) = p for any p ∈ [0, 1]. In practice, ECE [20] is a commonly used calibration metric from ﬁnite samples. It works by ﬁrstly grouping all samples (let n denote the number of samples) into M equally interval bins {Bm}M m=1 with respect to their conﬁdence scores, then calculating the expected difference between the accuracy and average conﬁdence: ECE = (cid:80)M
Temperature Scaling By scaling the logits produced by gθ with a temperature T , the sharpness i (x) can be treated as the associated conﬁdence score.
|Bm| n |acc(Bm) − avgConf(Bm)|. k(x)) , where gθ m=1 2
of output probabilities can be changed. Formally, after adding TS, the new prediction conﬁdence can be expressed as: ˆp = maxi k(x)/T ) . The temperature softens the output probability with T > 1 and sharpens the probability with T < 1. As T → 0, the output probability collapses to one-hot vector. As T → ∞, the output probability approaches to a uniform distribution. After training of the model, T can be tuned on a hold-out validation set by optimization methods. exp(gθ k=1 exp(gθ i (x)/T ) (cid:80)K n=1 according to a set of intervals {In}N +1
Histogram Binning is a non-parametric calibration approach. Given an uncalibrated model, all the prediction conﬁdences of validation samples can be divided into mutually exclusive N bins
{Bn}N n=1 which partitions [0, 1]. Each bin is assigned a conﬁdence score η, which can be simply set to the corresponding accuracy of samples in each bin. If the uncalibrated conﬁdence ˆp of a query instance falls into bin Bn, then the calibrated conﬁdence is ηn. The bins can be chosen by two simple schemes: equal size binning (uniformly partitioning the probability interval in [0, 1]) and equal mass binning (uniformly distributing samples over bins).
Note that although the HB scheme is simple to implement and was demonstrated to achieve good calibration results in some datasets, it makes the predictor only produce very sparse conﬁdence distribution, and compromises the many legitimately conﬁdent predictions. 3 Regularization in Neural Networks for Calibration
In recent years, there is a surge of research on model calibration by leveraging implicit or explicit regularization techniques during training of DNNs, which makes better calibrated predictions by avoiding the overconﬁdent outputs. In this section, we ﬁrstly review three representative regularization methods and then empirically show their improvements on ECE compared with the baseline.
Label Smoothing is widely used as a means to reduce overﬁtting of DNNs. The mechanism of LS is simple: when training with CE loss, the one-hot label vector y is replaced with soft label vector (cid:101)y, whose elements can be formally denoted as (cid:101)yi = (1 − (cid:15))yi + (cid:15)/K, ∀i ∈ {1, ..., K}, where (cid:15) > 0 is a strength coefﬁcient. Müller et al. [19] demonstrated that label smoothing implicitly calibrates
DNNs by preventing the networks from becoming overconﬁdent. Let Lce denote the CE loss, then the following equation holds:
Lce((cid:101)y, f θ) = (1 − (cid:15))Lce(y, f θ) + (cid:15)Lce(u, f θ)
This can be simply proved. Therefore, minimizing CE loss between smoothed labels and the model outputs is equivalent to adding a conﬁdence penalty term, i.e., a weighted CE loss between the uniform distribution u and the model outputs, to the original CE loss. (1)
Lp Norm in the Function Space is one of the explicit regularization methods for calibration investi-gated by the recent work [7]. For a real number p ≥ 1, the Lp Norm of a vector z with dimension n can be expressed as: (cid:107)z(cid:107)p = ((cid:80)n i=1 |zi|p)1/p. By adding Lp Norm of logits gθ with a weighting coefﬁcient α into ﬁnal objective function, i.e. LLp (y, f θ) = Lce(y, f θ) + α (cid:13) (cid:13)p, the function complexity of neural networks can be directly penalized during training. (cid:13)gθ(cid:13)
Focal Loss is originally proposed to address the class imbalance problem in object detection. By reshaping the standard CE loss through weighting loss components of all samples according to how well the model ﬁts them, focal loss focuses on ﬁtting hard samples and prevents the easy samples from overwhelming the training procedure. Formally, for classiﬁcation tasks where the target distribution is one-hot encoding, it is deﬁned as: Lf = −(1 − f θ y , where γ is a predeﬁned coefﬁcient.
Mukhoti et al. [18] found that the models learned by focal loss produce output probabilities which are already very well calibrated. Interestingly, they also showed that focal loss is an upper bound of the regularized KL-divergence, which can be expressed formally as follows: y )γ log f θ
Lf ≥ KL(y||f θ) − γH(f θ) where H(p) denotes the entropy of distribution p. This upper bound property shows that replacing the CE loss with focal loss has the effect of adding a maximum-entropy regularizer. (2) 3.1 Empirical Comparison
We conduct a comparison study of the above regularization methods on four commonly used datasets.
We train ResNet-32 [6] models on SVHN [21], CIFAR-10/100 [9] and train a 8-layer 1D-CNN 3
Table 1: Comparison results (mean±std) of ECE (%) with M = 15 and predictive accuracy (%) over 5 random runs. The values with underline in ﬁrst row represent the chosen coefﬁcients of each regularization method on four datasets according to the ECE on test data.
SVHN
CIFAR-10
CIFAR-100 20 Newsgroups
Cross-Entropy 3.03±0.16 95.00±0.27 6.43±0.22 90.46±0.23 19.53±0.36 64.64±0.43 20.82±0.93 72.85±0.89
ECE
Accuracy
ECE
Accuracy
ECE
Accuracy
ECE
Accuracy
Label Smoothing 0.01/0.05/0.09/0.09 1.84±0.19 95.21±0.23 2.72±0.32 90.09±0.41 2.27±0.48 63.73±0.67 5.85±0.64 72.81±0.26
L1 Norm 0.01/0.05/0.01/0.01 1.85±0.04 95.29±0.13 2.93±0.39 90.06±0.59 8.07±0.44 63.07±0.29 13.31±0.56 73.61±0.80
Focal Loss 1/3/5/5 1.01±0.21 94.77±0.19 3.00±0.26 87.84±0.17 2.34±0.35 60.36±0.44 3.82±0.51 59.17±1.81 model on 20Newsgroups [13], using the standard CE loss and the above regularized losses respec-tively, with state-of-the-art learning policy settings (see implementation details in Appendix). For
Norm regularization, we use L1 Norm, which has been shown effective for calibration despite its simple form [7]. Table 1 shows the comparison of these methods. Note that for each of the above three regularization methods, there is a coefﬁcient, i.e., (cid:15), α and γ, controlling the strength of regularization. We conduct experiments using these methods with the following coefﬁcient set-tings: {0.01, 0.03, 0.05, 0.07, 0.09} for label smoothing, {0.001, 0.005, 0.01, 0.05, 0.1} for L1 Norm and {1, 3, 5, 7, 9} for focal loss. And we choose the best coefﬁcient for each method and dataset, according to their ECE directly on test data.
From the results of Table 1, it is obvious that the regularization methods signiﬁcantly decrease the ECE on all datasets, compared with the standard CE loss. The prediction accuracy results are also reported. When the strength coefﬁcients of L1 Norm and focal loss are large, their predictive performances are harmed. Especially, L1 Norm fails on CIFAR-100 and 20Newsgroups when
α ≥ 0.05, thus α is chosen from {0.001,0.005,0.01} on these two datasets. 4 Does Regularization Really Help Calibration?
As shown by the above empirical results, the regularization methods do help the calibration of DNNs during training, especially alleviate the overconﬁdence issue caused by the standard CE loss. In this section, we empirically investigate their calibration performance when integrating them with post-hoc calibration. After training, we use the post-hoc methods TS and HB to further calibrate the output probabilities. For TS, we simply search the best temperature in the temperature pool {0.01,0.02...,10} on the validation set (see data splits in Appendix). For HB, we use equal size binning scheme on the top-1 prediction of all classes with bin number set as 15. The experimental details used in this section are the same with those in Section 3.
Comparison Results Table 2 shows the comparison results of ECE with the help of TS and HB.
We can see that: (1) The standard CE loss achieves the best calibration performance on most cases. (2) The searched temperatures of models trained with the CE loss are signiﬁcantly higher than those of other losses, which indicates that CE loss causes higher predictive conﬁdences. These results demonstrate that despite the regularized models can produce better calibrated predictions, it is harder to further improve them with post-hoc calibration methods after main training. In other words, the penalty of conﬁdent predictions will compress the room of potential improvement by post-hoc methods. We also conduct experiments on CIFAR-10 and CIFAR-100 using a deeper model
ResNet-110 and similar comparison results are obtained (see Appendix Table A).
Coefﬁcient Sensitivity Figure 1(a), 1(b) and 1(c) illustrate the ECEs of these three regularization methods with varied coefﬁcient strengths. As we can see, since the complexity of the used datasets are different, the best coefﬁcients of these methods markedly vary across the datasets, and a small change on these coefﬁcients may cause large ECE increase. This means that we need to carefully choose the coefﬁcient of each method when employing them on new datasets, to achieve good calibration. We can also observe that for SVHN, on which the accuracy is highest among four datasets, the regularization methods obtain lowest ECE with small coefﬁcients. For CIFAR-100 and 20Newsgroups, on which the 4
Table 2: Comparison results (mean±std) of ECE (%) with M = 15 over 5 random runs. The coefﬁcients of the regularization methods on each dataset are same with those in Table 1. (cid:78)/(cid:78) and (cid:72)/(cid:72) indicate that the average ECE of regularization methods are higher and lower than standard CE, where (cid:78) and (cid:72) are based on two-sample t-test at 0.05 signiﬁcance level.
Cross-Entropy 0.72±0.26(cid:72) 1.82 (cid:72) 0.68±0.22(cid:72) 0.95±0.19(cid:72) 2.51 (cid:72) 0.74±0.15(cid:72) 1.35±0.19(cid:72) 2.19 (cid:72) 1.27±0.27(cid:72) 3.11±0.33(cid:72) 4.18 (cid:72) 2.52±0.47(cid:72)
Label Smoothing 0.01/0.05/0.09/0.09 1.35±0.11(cid:78) 1.11(cid:72) 0.70±0.21(cid:78) 2.54±0.11(cid:78) 0.96(cid:72) 0.94±0.21(cid:78) 1.37±0.27(cid:78) 1.04(cid:72) 2.01±0.22(cid:78) 5.22±0.60(cid:78) 1.06(cid:72) 2.67±0.82(cid:78)
L1 Norm 0.01/0.05/0.01/0.01 1.22±0.08(cid:78) 1.12(cid:72) 0.73±0.20(cid:78) 2.71±0.36(cid:78) 0.95(cid:72) 1.16±0.54(cid:78) 3.92±0.21(cid:78) 1.24(cid:72) 1.56±0.44(cid:78) 2.71±0.25(cid:72) 1.48(cid:72) 2.61±0.95(cid:78)
Focal Loss 1/3/5/5 0.80±0.22(cid:78) 1.10 0.96±0.14(cid:78) 1.39±0.28(cid:78) 0.76 1.65±0.31(cid:78) 2.14±0.42(cid:78) 0.97 1.83±0.30(cid:78) 3.77±0.41(cid:78) 0.89 3.16±0.97(cid:78) with TS
Temperature with HB with TS
Temperature with HB with TS
Temperature with HB with TS
Temperature with HB
SVHN
CIFAR-10
CIFAR-100 20 Newsgroups (a) LS with varied (cid:15) (b) L1 with varied α (c) FL with varied γ (d) Best coefﬁcients
Figure 1: (a-c): ECE (%) with M = 15 of regularization methods with controlled regularization strength. (d): Best coefﬁcients of regularization methods with respect to ECE with controlled training data size. accuracy is relatively lower, the regularization methods need larger coefﬁcients for better calibration.
Based on this observation, we conduct another experiment for investigating the correlation between the regularization coefﬁcients and accuracy. We learn networks on CIFAR-10 by controlling training data size, which leads to varied predictive accuracies, and choose the best coefﬁcient for each case. Here, (cid:15), α and γ are chosen from {0.01, 0.02, ..., 0.25}, {0.01, 0.02, ..., 0.1} and {1, 3, 5, 7, 9} respectively. Figure 2(d) shows that with the increase of training data size, which results in increase of predictive accuracy, the best coefﬁcients of the regularization methods keep decreasing.
Reliability Diagram We use reliability diagram to visually represent the gap between predictive conﬁdence and accuracy of each method. Due to the space limitation, here we only present the diagrams of CIFAR-10, and the rest ﬁgures are presented in Appendix. We can see that these visual results are similar with the comparison results of ECE reported in Table 2. Although the gap between conﬁdence and accuracy is large when using the standard CE loss, it can be signiﬁcantly diminished after using TS. However, the improvements of TS for the regularization methods are not obvious.
Most importantly, no matter whether TS is used or not, the regularization methods suffer from overconﬁdence on samples which have high predictive uncertainty, especially on label smoothing and
L1 Norm, which contradicts the traditional view. Combining with the observation in Figure 2(d), it is indicated that the regularization methods work by simply aligning the average predictive conﬁdence of the whole dataset to the accuracy with some speciﬁc regularization strengths, and does not produce
ﬁne-grained calibration with respect to the difference of samples.
Impact of Validation Size We also wonder how does the validation data size impact the post-hoc calibration. Figure 3 shows the ECE results with controlled validation data size. We can see that quite low ECE can be obtained with only a small size of validation data when using TS, which offers high efﬁciency for practice development. Relatively, HB needs more validation samples to obtain better calibration performance. Nevertheless, the standard CE loss stably achieves better calibration across varied validation data size with both TS and HB. 5
O verco n ﬁ dence
O verco n ﬁ dence
O verco n ﬁ dence (a) CE (b) LS, (cid:15) = 0.05 (c) L1 Norm, α = 0.05 (d) FL, γ = 3
Figure 2: Reliability diagrams of each methods (after TS calibration) on CIFAR-10. The results are chosen from one of the 5 random runs of Table 1. Darker color of bars indicates that more samples are assigned with the corresponding conﬁdence intervals. (a) SVHN, TS (b) CIFAR-10, TS (c) CIFAR-100, TS (d) 20Newsgroups, TS (e) SVHN, HB (f) CIFAR-10, HB (g) CIFAR-100, HB (h) 20Newsgroups, HB
Figure 3: ECE (%) (after post-hoc calibration) with of regularization methods with controlled validation data size. 5 From Calibrated to Calibratable: A Closer Look
The results reported in above section show the degradation of regularization methods when integrating them with post-hoc calibration methods, which indicates that though regularization helps DNNs obtain well-calibrated predictions, it makes these predictions worse calibratable. In this Section, we further investigate this phenomenon by a series of illustrative experiments. We ﬁrstly attempt to empirically understand the reason of the calibration degradation from the view of information loss.
Then, we propose an inverse form of focal loss to give a closer look at the correlation between the loss functions used in training and the calibration performance. The implementation details used in this section are also the same with those in Section 3. 5.1
Information Loss of Regularized Models
ECE among Epochs We start by investigating the ECEs of temperature-scaled outputs over epochs during model training. To avoid the impact of the bias of validation data, we directly search the best temperature on test data in each training epoch. We denote the corresponding ECE with this searched temperature as optimal ECE, which is the lower bound of ECE with temperatures searched on validation data. Figure 4 shows the curves of optimal ECE during epochs using label smoothing with different smoothing coefﬁcients. We can observe that the optimal ECE rises after some learning epochs: On SVHN and CIFAR-10, it starts to signiﬁcantly rise around the 10th epoch, and on
CIFAR-100 and 20Newsgroups, it tends to rise after 100 and 50 epochs, where the learning rate drops by a factor of 10. Another observation is that larger smoothing strength (cid:15) results in worse calibration performance and more remarkable (also earlier) ECE rising. According to the memorization effect
[35], DNNs usually learn easy samples at the early stage of training and tend to ﬁt the hard ones later. 6
(a) SVHN (b) CIFAR-10 (c) CIFAR-100 (d) 20Newsgroups
Figure 4: Curves of optimal ECE (%) during learning epochs using label smoothing with different coefﬁcients. Dark colors show the mean results of 5 random runs and light colors show the ranges between minimal and maximum results of 5 runs. (a) CE (b) LS, (cid:15) = 0.05 (c) L1, α = 0.05 (d) FL, γ = 3 (e) CE (f) LS, (cid:15) = 0.09 (g) L1, α = 0.01 (h) FL, γ = 5
Figure 5: Histograms of maximum logits produced from models trained with different methods.
Different colors of bars represent distributions of samples with different learned epochs. The ﬁrst row and second row are results of CIFAR-10 and CIFAR-100 respectively.
Therefore, a simple conjecture for the ECE rising is that after some learning epochs, DNNs start to
ﬁt hard samples, at the same time the regularizer would penalize the conﬁdences of easy samples, which makes the predictive conﬁdences of those easy and hard samples difﬁcult to be distinguished.
Similar phenomenon is also observed when using L1 Norm (see Appendix Figure A(a-d)) except 20Newsgroups dataset, on which large norm coefﬁcient will hurt the calibration. For focal loss (see
Appendix Figure A(e-h)), the optimal ECEs trained with large regularization strengths keep high without the remarkable rising.
Histogram of Logits We use histograms to visualize what the patterns of model outputs learned with different methods look like. Before that, we deﬁne learned epoch 2 of an individual training sample as the epoch, since which the sample can be correctly classiﬁed till the ﬁnal learning epoch.
As we mentioned above, DNNs usually learn hard samples after easy ones, hence the learned epoch of a sample can be used to indicate its corresponding hardness degree to be learned. Based on this, we want to investigate if the samples with different hardness degrees can be distinguished by model itself after training. To this end, we record the learned epochs of all training samples of CIFAR-10 and CIFAR-100 during training and statistic the distributions of their maximum logit outputs (i.e. maxi gθ i (x)). As shown in Figure 5, the logits of models trained with the standard CE loss cover much larger ranges, and the regularization methods compress the distributions too tight without distinction between samples with different learned epochs, especially in label smoothing and L1 Norm. This visual observation further conﬁrms that the regularization of DNNs works by only penalizing the conﬁdence of the whole dataset to a low level with a speciﬁc regularization strength. This will result in loss of the important information about the hardness of samples as an undesirable side effects, and compress the room of potential improvement by post-hoc calibration. On the contrary, models trained 2This concept is inspired by the related work [30], in which the authors qualify a sample as unforgettable if it is learned at some epoch and experience no forgetting events during training. 7
(a) Loss functions (b) Predictive accuracy (c) ECE without post-hoc calibration (d) Best temperatures (e) ECE after TS (f) ECE after HB
Figure 6: (a): Visual representation of focal loss, CE loss and inverse focal loss. (b): Predictive accuracies (%) of different methods. (c): ECEs (%) with M = 15 of different methods without post-hoc calibration. (d): Searched temperatures on validation data. (e-f): ECEs (%) with M = 15 of different methods with the help of post-hoc calibration. with the standard CE loss manage to preserve this information to a certain extent during training, hence achieve better results after post-hoc calibration. 5.2
Is Cross-Entropy the Best for Calibration?
Based on our experimental ﬁndings, one natural question is that can we design some loss functions in the opposite direction of these regularization methods to further improve the calibration? For label smoothing and Lp Norm, we can simply set the regularization coefﬁcients of these methods as negative values. However, we empirically found this will cause extremely low predictive accuracies even failures of training using only very small weighting coefﬁcients. Fortunately, we can design an inverse version of focal loss without prediction degradation by mimicking the original focal loss3.
Recall the form of focal loss, we see that it works by assigning larger weights to the samples with smaller conﬁdences. This makes the optimizer pay more attention to those hard samples when updating model parameters. Actually, this weighting scheme also implicitly exists in the standard CE loss, and this can be expressed by the gradients of CE loss function w.r.t. model parameters θ:
∂Lce(y, f θ(x))
∂θ
= − 1 f θ y (x)
∇θf θ y (x) (3) where the factor term 1 y (x) indicates that samples with smaller conﬁdences are weighted larger in f θ gradient calculation. Opposite to the principle of focal loss, we propose inverse focal loss as follows:
L ¯f = −(1 + f θ y )¯γ log f θ y (4)
By a simple modiﬁcation on the weighting term of original focal loss, the inverse focal loss assigns larger weights to the samples with larger output conﬁdences. Similar with original focal loss, the choice of coefﬁcient ¯γ has a huge impact on the property of this loss. In Figure 6(a), we plot the curves of inverse focal loss with varied ¯γ and also plot the standard CE loss and focal loss for comparison. We can see that different from the original focal loss, the curves of inverse focal loss are steeper when conﬁdence is large, and larger ¯γ gives steeper curves.
We conduct another experiment to evaluate the inverse focal loss, and also investigate what will happen when we increase its coefﬁcient ¯γ. Figure 6(c) shows the ECE results without post-hoc calibration. The ECEs of inverse focal loss are larger than CE and focal loss in most cases. This is consistent to our expectation since inverse focal loss aggravates the overconﬁdence issue of DNNs by weighting larger on the easy samples. Figure 6(e) and 6(f) show the ECE results with the help of 3As the reviewer suggested, there already exists an "inverse focal loss" in the literature, which was introduced in a totally different context and using a different mathematical expression, but similar motivation [15]. 8
post-hoc calibration. When using HB, the ECEs of inverse focal loss are worse than CE on SVHN and CIFAR-10, while better than CE on CIFAR-100. Generally speaking, there is no clear trend when we increase ¯γ. More interesting results appear when using TS: (1) On CIFAR-10 and CIFAR-100, the ECE results of inverse focal loss are better than that of the standard CE loss; and (2) there is a descend-then-ascend trend from focal loss with γ = 3 to inverse focal loss with ¯γ = 3. From these observations, we may say that the best loss function for calibration is varied across different tasks according to the characteristics of datasets. On SVHN, which is a relatively easy dataset, standard CE loss yields pretty good results; on CIFAR-10 and CIFAR-100, which is more complex and difﬁcult, the best results are obtained using inverse focal loss; on 20Newsgroups, which has fewest training samples among four datasets, the best result is obtained when using focal loss with
γ = 1. The searched temperatures when using TS are presented in Figure 6(d). The increasing of best temperatures indicates that the overconﬁdence problem is severer when using inverse focal loss with larger ¯γ. The predictive accuracies are presented in Figure 6(a). As is shown that inverse focal loss yields highly competitive results compared with the standard CE loss on SVHN, CIFAR-10 and
CIFAR-100. On 20Newsgroups, when using large ¯γ, the predictive performance of inverse focal loss is worse than the CE loss. 6