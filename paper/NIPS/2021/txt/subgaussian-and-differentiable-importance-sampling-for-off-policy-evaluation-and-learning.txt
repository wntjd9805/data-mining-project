Abstract
Importance Sampling (IS) is a widely used building block for a large variety of off-policy estimation and learning algorithms. However, empirical and theoretical studies have progressively shown that vanilla IS leads to poor estimations whenever the behavioral and target policies are too dissimilar. In this paper, we analyze the theoretical properties of the IS estimator by deriving a novel anticoncentration bound that formalizes the intuition behind its undesired behavior. Then, we pro-pose a new class of IS transformations, based on the notion of power mean. To the best of our knowledge, the resulting estimator is the ﬁrst to achieve, under certain conditions, two key properties: (i) it displays a subgaussian concentration rate; (ii) it preserves the differentiability in the target distribution. Finally, we provide numerical simulations on both synthetic examples and contextual bandits, in comparison with off-policy evaluation and learning baselines. 1

Introduction
The availability of historically collected data is a common scenario in many real-world decision-making problems, including medical treatments [e.g., 17, 67], recommendation systems [e.g., 33, 16], personalized advertising [e.g., 3, 60], ﬁnance [e.g., 43], and industrial robot control [e.g., 27, 26].
Historical data can be leveraged to address two classes of problems. First, given data collected with a behavioral policy, we want to estimate the performance of a different target policy. This problem is known as off-policy evaluation [Off-PE, 21]. Second, we want to employ the available data to improve the performance of a baseline policy. This latter problem is named off-policy learning [Off-PL 14]. Off-policy methods are studied by both the reinforcement learning [RL, 58] and contextual multi-armed bandit [CMAB, 30] communities. Given its intrinsic simplicity compared to RL, off-policy methods are nowadays well understood in the CMAB framework [e.g., 44, 1, 14, 64]. Among them, the doubly robust estimator [DR, 14] is one of the most promising off-policy methods for
CMABs. DR combines a direct method (DM), in which the reward is estimated from historical data via regression, with an importance sampling [IS, 46] control variate.
IS plays a crucial role in the off-policy methods and counterfactual reasoning. However, IS tends to exhibit problematic behavior for general distributions. This is formalized by its heavy-tailed properties [40], which prevent the application of exponential concentration bounds [4]. To cope with this issue, typically, corrections are performed on the importance weight including truncation [23] and self-normalization [SN, 46], among the most popular. Signiﬁcant results have recently been derived for both techniques [47, 29, 39]. Nevertheless, we believe that the widespread use of IS calls for a better theoretical understanding of its properties and for the design of principled weight corrections. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Deﬁning the desirable properties of an off-policy estimator is a non-trivial task. Some works employed the mean squared error (MSE) as an index of the estimator quality [34, 64]. However, controlling the MSE, while effectively capturing the bias-variance trade-off, does not provide any guarantee on the concentration properties of the estimator, which might still display a heavy-tailed behavior [37].
For this reason, we believe that a more suitable approach is to require that the estimator deviations concentrate at a subgaussian rate [12]. Subgaussianity implicitly controls the tail behavior and leads to tight exponential concentration inequalities. Unlike MSE, the probabilistic requirements are non-asymptotic (ﬁnite-sample), from which guarantees on the MSE can be derived. While subgaussianity can be considered a satisfactory requirement for Off-PE, additional properties are advisable when switching to Off-PL. In particular, the differentiability w.r.t. the target policy parameters is desirable whenever Off-PL is carried out via gradient optimization. For instance, weight truncation, as presented in [47], allows achieving subgaussianity but leads to a non-differentiable objective. Consequently, the optimization phase requires additional care, which sometimes leads to computationally heavy discretizations [47]. On the contrary, the SN estimator is differentiable in the target policy, but fails to achieve subgaussian concentration for general distributions.
In this paper, we take a step towards a better understanding of IS. After having introduced the necessary background (Section 2), we derive an anticoncentration bound for the mean estimation with vanilla IS. We show that polynomial concentration (Chebychev’s inequality) is tight in this setting (Section 3). This result formalizes the intuition behind the undesired behavior of these estimators for general distributions. Hence, we propose a class of importance weight corrections, based on the notion of power mean (Section 4). The rationale behind these corrections is to “smoothly shrink” the weights towards the mean, with different intensities. In this way, we mitigate the heavy-tailed behavior and, in the meantime, we exert control over the induced bias. Then, we derive bounds on the bias and variance that allow obtaining an exponential concentration inequality and, under certain conditions, subgaussian concentration (Section 5). Furthermore, the smooth transformation allows preserving the differentiability in the target policy, unlike some existing transformations, like weight truncation.
To the best of our knowledge, this is the ﬁrst IS correction that preserves the differentiablity and is proved to be subgaussian. This correction, however, requires knowledge of a distributional divergence between the target and behavioral policies, which may be unknown or difﬁcult to compute. To this end, we introduce an approach to empirically estimate the correction parameter, preserving the desirable concentration properties (Section 6). After providing a comparative review of the literature (Section 7), we present an experimental study comparing our approach with traditional and modern off-policy baselines on synthetic domains and in the CMAB framework (Section 8). The proofs of the results presented in the main paper can be found in Appendix A. A preliminary version of this work was presented at the “Workshop on Reinforcement Learning Theory” of ICML 2021 [42].1 2 Preliminaries
˘
˘
`` a logp1{δq{n
We start introducing the background about probability, importance sampling and contextual bandits.
Probability We denote with PpYq the set of probability measures over a pY, F q. Let P P PpYq, f : Y Ñ R be a function, and µn be an estimator for the mean µ “ Ey„P rf pyqs obtained with n i.i.d.
Y gpn, δq. For β ą 0, we say that samples. Suppose that with probability 1 ´ δ it holds that |µn ´ µ| ď
˘
µn admits: (i) polynomial concentration if gpn, δq “ O
; (ii) exponential concentration if
; (iii) subgaussian concentration if (ii) holds with β “ 1 [37]. These cases gpδq “ O correspond to Chebyshev’s, Bernstein’s, and Höeffding’s inequalities respectively [4].
Importance Sampling Let P, Q P PpYq admitting p and q as density functions, if P ! Q, i.e.,
P is absolutely continuous w.r.t. Q, for any α P p1, 2s, we introduce the integral: IαpP }Qq “
ş ppyqαqpyq1´αdy. If P “ Q a.s. (almost surely) then IαpP }Qq “ 1. IαpP }Qq allows deﬁning
Y several divergences, like Rényi [51]: pα ´ 1q´1 log IαpP }Qq. Let f : Y Ñ R be a function, (vanilla) importance sampling [IS, 46] allows estimating the expectation of f under the target distribution P , i.e., µ “ Ey„P rf pyqs, using i.i.d. samples tyiuiPrns collected with the behavioral distribution Q: 1{pnδqβ
`
β pµn “ 1 n
ÿ iPrns
ωpyiqf pyiq, where
ωpyq “ ppyq qpyq
, @y P Y. 1https://lyang36.github.io/icml2021_rltheory/camera_ready/7.pdf. 2
ş
ş
It is well-known that pµn is unbiased, i.e., Eyi„Qrpµns “ µ [46]. If f is bounded, the variance of the estimator can be upper-bounded as Varyi„Qrpµns ď 1 n }f }8I2pP }Qq [40]. More in general, the integral IαpP }Qq represents the α-moment of the importance weight ωpyq under Q.
Contextual Bandits A contextual multi-armed bandit [CMAB, 30] is represented by the tuple
C “ pX , A, ρ, pq, where X is the set of contexts, A is the ﬁnite set of actions (or arms), ρ P PpX q is the context distribution, and p : X ˆ A Ñ PpRq is the reward distribution. The agent’s behavior is encoded by a policy π : X Ñ PpAq. At each round t P N, the agent observes a context xt „ ρ, plays an action at „ πp¨|xtq, gets the reward rt „ pp¨|xt, atq and the system moves on to the next round.
The value of a policy π is given by vpπq “
R ppr|x, aqrdrdx. We denote with aP
A
R ppr|x, aqrdr the reward function. A policy π˚ is optimal if it maximizes the value rpx, aq “ function, i.e., π˚ P arg maxπPΠ vpπq, where Π “ tπ : X Ñ PpAqu is the set of all policies.
Let D “ tpxt, at, rtqutPrns be a dataset of samples collected in a CMAB with a behavioral policy
πb P Π. The off-policy evaluation [Off-PE, 21] problem consists in estimating the value function vpπeq of a target policy πe P Π using the samples in D. The off-policy learning [Off-PL, 14] problem consists in estimating an optimal policy π˚ P Π using the samples in D. The simplest approach to address the Off-PE/Off-PL problem is to learn from D an estimate prpx, aq of the reward function rpx, aq via regression. This approach is known as direct method (DM) and its properties heavily depend on the quality of the estimate pr. Another approach is to simply apply IS to reweight the samples of D, leading to the inverse propensity scoring [IS, 17] estimator. The two approaches are combined in the doubly-robust [DR, 14] estimator, in which the DM estimate is corrected with an IS control variate to reduce the variance using the estimated reward pr (see also Table 12 in Appendix D).
πpa|xq
ρpxq
ř
X
ş 3 Anticoncentration of Vanilla Importance Sampling
In this section, we analyze the intrinsic limitations of the vanilla IS. It is well-known that under the assumption that for some α P p1, 2s the divergence IαpP }Qq is ﬁnite and f is bounded, the vanilla IS estimator pµn admits polynomial concentration, i.e., with probability at least 1 ´ δ:2
|pµn ´ µ| ď }f }8
ˆ
˙ 1 22´αIαpP }Qq
δnα´1
α
. (1)
We now show that the concentration in Equation (1) is tight, by deriving an anticoncentration bound for |pµn ´ µ|; then, we discuss its implications and compare it with previous works.
Theorem 3.1. There exist two distributions P, Q P PpYq with P ! Q and a bounded measurable func-( tion f : Y Ñ R such that for every α P p1, 2s and δ P p0, e´1q if n ě δe max
, with probability at least δ it holds that: (cid:32) 1, pIαpP }Qq ´ 1q 1
α´1
|pµn ´ µ| ě }f }8
ˆ
˙ 1
ˆ
IαpP }Qq ´ 1
δnα´1
α 1 ´ eδ n
˙ n´1
α
.
First of all, we note the polynomial dependence on the conﬁdence level δ. The bound is vacuous when
IαpP }Qq “ 1, i.e., when P “ Q a.s., since in an on-policy setting and, being the function f bounded, subgaussian concentration bounds (like Höeffding’s inequality) hold. In particular, for α “ 2, n and
I2pP }Qq sufﬁciently large, the bound has order O
, matching Chebyshev’s and the existing concentration inequalities for vanilla importance sampling [40, 41].
I2pP }Qq{pδnq
`a
˘
Our result is of independent interest and applies for general distributions. Previous works considered the MAB [34] and CMAB [64] settings proving minimax lower bounds in mean squared error (MSE)
Ey„Qrppµn ´ µq2s. These results differ from ours in several respects. First, we focus on a speciﬁc estimator, the vanilla one, while those result are minimax. Second, they provide lower bounds to the
MSE, while we focus on the deviations in probability.3 From our probabilistic result, it is immediate to derive an MSE guarantee (Corollary A.1 of Appendix A.1). Finally, they assume that the second moment of the importance weight I2pP }Qq is ﬁnite, whereas our result allows considering scenarios in which only moments of order α ă 2 are ﬁnite. 2The original result [41, Theorem 2] was limited to α “ 2 and based on Cantelli’s inequality which approaches
Chebyshev’s when δ Ñ 0. See Theorem A.1 in Appendix A.1, for a proof of Equation (1). 3As noted in [36], when the estimator is not well-concentrated around its mean (e.g., in presence of heavy tails), the MSE is not adequate to capture the error and high-probability bounds are more advisable. 3
s
ωs,λpyq
´8 (minimum) mintωpyq, 1u
´1 (harmonic) 0 (geometric) 1 (arithmetic)
ωpyq 1 ´ λ ` λωpyq
ωpyq1´λ p1 ´ λqωpyq ` λ
Table 1: Choices of s for the pλ, sq-corrected importance weight of Deﬁnition 4.1.
Figure 1: Examples of importance weight corrections of Deﬁnition 4.1 for ﬁxed λ (left) and ﬁxed s (right). 4 Power-Mean Correction of Importance Sampling
In this section, motivated by the negative result of Theorem 3.1, we look for a weight correction able to achieve subgaussian concentration. Speciﬁcally, we introduce a class of corrections based on the notion of power mean [6] and we study its properties. Let us start with the following deﬁnition.
Deﬁnition 4.1. Let P, Q P PpYq be two probability distributions such that P ! Q, for every s P r´8, 8s and λ P r0, 1s, let ωpyq “ ppyq{qpyq, the pλ, sq-corrected importance weight is deﬁned as:
´
¯ 1
ωλ,spyq “ p1 ´ λqωpyqs ` λ s , @y P Y.
The correction can be seen as the weighted power mean with exponent s between the vanilla importance weight ωpyq and 1 with weights 1 ´ λ and λ respectively.4 We immediately notice that, regardless of the value of s, for λ “ 0, we reduce to the vanilla importance weight ω0,spyq “ ωpyq and for λ “ 1, we have identically ω1,spyq “ 1. Furthermore, the correction is unbiased when P “ Q a.s. regardless s and λ. Thus, the correction “smoothly interpolates” between the vanilla weight
ωpyq and its mean under Q, i.e., 1. s and λ govern the “intensity” of the correction in a continuous way. Differently from the truncation [23], this transformation leads to a differentiable weight. Some speciﬁc choices of s and λ are reported in Table 1 and in Figure 1. The following result provides a preliminary characterization of the correction, independent of the properties of the two distributions.
Lemma 4.1. Let P, Q P PpYq be two probability distributions with P ! Q, then for every λ P r0, 1s and y P Y it holds that: (i) if s ď s1 then ωλ,spyq ď ωλ,s1pyq; (ii) if s ă 0 then ωλ,spyq ď λ 1 (iii) if s ă 1 then Ey„Qrωλ,spyqs ď 1, otherwise if s ą 1 then Ey„Qrωλ,spyqs ě 1. s , otherwise if s ą 0 then ωλ,spyq ě λ 1 s ;
From point (ii) we observe that the corrected weight is bounded from below when s ą 0 and bounded from above when s ă 0. It is well-known that the inconvenient behavior of IS derives from the heavy-tailed properties [40]. Thus, the arithmetic correction (s “ 1) performs just a convex combination between the vanilla weight and 1, having no effect on the tail properties. Any correction with s ą 1 increases the weight value, making the tail even heavier. So, if we are looking for subgaussianity, we should restrict our attention to s ă 0, which leads to lighter tails or even bounded weights. 5 Subgaussian and Differentiable Importance Sampling
In this section, we focus on the harmonic correction (s “ ´1), which leads to a weight of the form:5
ωλ,´1pyq “ ωpyq 1´λ`λωpyq . We start analyzing the bias and variance of this class of estimators. Then, we provide an exponential concentration inequality that, under certain circumstances, results to be subgaussian. Finally, we show that the resulting estimator is differentiable in the target distribution.
To lighten the notation we neglect the ´1 subscript, abbreviating pµλ “ pµλ,´1.
Bias and Variance We now derive bounds for the bias and the variance induced by the pλ, ´1q-corrected importance weight. 4For s P t´8, 0, 8u the power mean is deﬁned as a limit. 5The choice of s “ ´1 is mainly for analytical convenience and, as we shall see, it already allows enforcing the desired properties. We leave investigating the other values of s for future work. 4
Lemma 5.1. Let P, Q P PpYq be two probability distributions with P ! Q. For every λ P r0, 1s, the bias and variance of the pλ, ´1q-corrected importance weight can be bounded for every α P p1, 2s as:
ˇ
ˇ
ˇ E y„Q
ˇ
ˇ rpµn,λs ´ µ
ˇ ď }f }8λα´1IαpP }Qq,
Var yi„Q rpµn,λs ď
}f }2 8 nλ2´α
IαpP }Qq.
As expected, the bias is zero for λ “ 0; it increases with λ and with the divergence term IαpP }Qq.
Indeed, we already observed that the bias is null when P “ Q a.s.. In particular, for α “ 2, the bound becomes }f }8λI2pP }Qq. Instead, the variance bound decreases in λ and increases with the divergence IαpP }Qq. For α “ 2, we obtain the bound 1 n }f }2 8I2pP }Qq. Note that when P “ Q a.s., we recover 1 8, which is the Popoviciu’s inequality for the variance [50]. Thus, our weight correction allows controlling bias and variance even when I2pP }Qq “ 8, i.e., when the vanilla IS estimator might have inﬁnite variance. Indeed, our transformed estimator has ﬁnite variance provided that there exists α P p1, 2q so that IαpP }Qq ă 8. Tighter (but less intelligible) bounds on bias and variance are reported in Appendix A.3. n }f }2
Concentration Inequality We are now ready to derive the core theoretical result. We prove an exponential concentration inequality for the pλ, ´1q-corrected IS estimator and we show that, if
I2pP }Qq is ﬁnite, we are able to achieve subgaussian concentration.6
Theorem 5.1. Let P, Q P PpYq be two probability distributions such that P ! Q. For every α P p1, 2s and δ P p0, 1q, if we select λ “ λ˚
α then, with probability at least 1 ´ δ it holds that: pµn,λ˚
α
´ µ ď }f }8p2 `
˜
? 3q 2IαpP }Qq 1
α´1 log 1
δ 3pα ´ 1q2n
¸ 1´ 1
α
, with λ˚
α “
ˆ 2 log 1
δ 3pα ´ 1q2IαpP }Qqn
˙ 1
α
.
We immediately notice that the dependence on the conﬁdence level δ is the one typical of exponential concentration for every α P p1, 2s. In particular, we observe that the bound is subgaussian when
α “ 2, requiring that I2pP }Qq ă 8. Recalling that I2pP }Qq governs the variance of the estimator, this result is in line with the general theory of estimators for which the existence of the variance is an unavoidable requirement to achieve subgaussian concentration [12]. Speciﬁcally, for α “ 2 the optimal value of the parameter is λ˚ p2 logp1{δqq{p3I2pP }Qqnq, leading to the bound: a 2 “ d pµn,λ˚ 2
´ µ ď }f }8p2 `
? 3q 2I2pP }Qq log 1
δ 3n
. (2)
A tighter bound, based on a different choice of the correction parameter λ˚˚ pendix A.3 and it is omitted here for clarity of presentation.
α is derived in Ap-Differentiability As we have already observed, our weight correction, differently from truncation, is smooth and, thus, differentiable in the target policy. We now focus on the properties of the gradient of the pλ, ´1q-corrected estimator and, to this purpose, we constrain the target distribution to belong to a parametric space differentiable distributions PΘ “ tPθ P PpYq : θ P Θu, where Θ Ď Rd. The gradient of the corrected weight ωλ w.r.t. the target policy parameters θ is given by:
∇θωλpyq “ ∇θ pθpyq qpyq
“ p1 ´ λqωpyq p1 ´ λ ` λωpyqq2 ∇θ log pθpyq, @y P Y.
In particular, it can be proved that }∇θωλpyq}8 ď 1 4λ }∇θ log pθpyq}8 (Proposition A.1 of Ap-pendix A.3). Thus, if the score ∇θ log pθ is bounded, the gradient will be bounded whenever λ ą 0.
This property is advisable for gradient optimization and it is not guaranteed, for example, for vanilla
IS (λ “ 0). Thus, we can also interpret λ as a regularization parameter for the gradient magnitude. 6 Data-driven Tuning of λ
The computation of the parameter λ˚ 2 requires the knowledge of the divergence I2pP }Qq. Even when P and Q are known, computing the I2pP }Qq can be challenging, especially for continuous 6We introduce our concentration inequalities as a one-sided bounds just for simplicity but they actually hold in both directions. Indeed, by replacing function f with function ´f , we obtain the reversed one-sided bound. 5
ř distributions, since it involves the evaluation of a complex integral.7 In principle, we could estimate the divergence I2pP }Qq from samples as the empirical second moment of the vanilla importance weights 1 iPrns ωpyiq2. However, although possibly well-performing in practice [40], this approach n would prevent any subgaussian concentration, as the behavior of the non-corrected ωpyq2 will be surely heavy-tailed whenever ωpyq is. A general-purpose approach to avoid the divergence estimation is the Lepski’s adaptation method [32], which only requires knowing an upper and a lower bound on
I2pP }Qq. Unfortunately, this method is known to be computationally intensive.
In this section, we follow a different path inspired by the recent work [66]. If a choice of the parameter
λ corrects the weight ωλ leading to an ideal estimator pµn,λ, for the mean µ, we may expect that the empirical second moment of the corrected weights ωλ will provide a reasonable estimation of
I2pP }Qq. Based on this observation, we propose to choose λ by solving the following equation:
ÿ
λ2 1
ωλn1{4pyiq2 n loooooooooomoooooooooon iPrns empirical second moment
“ 2 log 1
δ 3n
. (3)
ř a p2 logp1{δqq{p3I2pP }Qqnq “ λ˚
The intuition behind this approach can be stated as follows. If the empirical second moment is close pλ of Equation (3) approaches to the divergence, i.e., 1 iPrns ωλn1{4pyiq2 » I2pP }Qq, the solution n pλ » 2 . We formalize this reasoning in the optimal parameter, i.e., pλ P r0, 1s (Lemma A.4) and that when
Appendix A.4, proving that Equation (3) admits a unique root the number of samples n grows to inﬁnity, 2 (Lemma A.8). The following pλ instead of λ˚ result provides the concentration properties of the estimator pµn,λ when using 2 , under slightly more demanding requirements on the moments of the importance weights.
Theorem 6.1. Let P, Q P PpYq be two probability distributions such that P ! Q. Let pλ be the solution of Equation (3), then, if I3pP }Qq is ﬁnite, for sufﬁciently large n, for every δ P p0, 1q, with probability at least 1 ´ 2δ it holds that: pλ converges indeed to λ˚ d pµn,
λ ´ µ ď }f }8 p
? 5 ` 2 2 3 2I2pP }Qq log 1
δ 3n
.
Compared to Theorem 5.1, this result is weakened in two aspects. First, the inequality holds with a smaller probability 1 ´ 2δ since two estimation processes with the same samples are needed, i.e., the computation of
λ. Second, and most important, the result holds p for large n, whose minimum value is reported in the proof and depends on I3pP }Qq, which must be
ﬁnite. We think this is a not too strong requirement considering that even the variance of an empirical estimate of I2pP }Qq would depend on the fourth moment of the importance weight, i.e., I4pP }Qq.8 pλ and the corrected estimator pµn, 7