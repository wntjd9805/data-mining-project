Abstract
Kernel selection plays a central role in determining the performance of Gaussian
Process (GP) models, as the chosen kernel determines both the inductive biases and prior support of functions under the GP prior. This work addresses the chal-lenge of constructing custom kernel functions for high-dimensional GP regression models. Drawing inspiration from recent progress in deep learning, we introduce a novel approach named KITT: Kernel Identiﬁcation Through Transformers. KITT exploits a transformer-based architecture to generate kernel recommendations in under 0.1 seconds, which is several orders of magnitude faster than conventional kernel search algorithms. We train our model using synthetic data generated from priors over a vocabulary of known kernels. By exploiting the nature of the self-attention mechanism, KITT is able to process datasets with inputs of arbitrary di-mension. We demonstrate that kernels chosen by KITT yield strong performance over a diverse collection of regression benchmarks. 1

Introduction
In recent years deep parametric models have become a prominent class of model for supervised learning and have delivered impressive empirical performance over a wide range of tasks. An im-portant limitation, however, is that in their conventional form deep models do not provide prediction uncertainty. While their Bayesian counterparts try to achieve this, they require signiﬁcant modi-ﬁcations to the training procedure and are computationally expensive. Uncertainty quantiﬁcation in deep models is widely considered to be an open problem, the large array of research proposing alternative Bayesian neural networks underscores this [10, 12, 17].
On the other hand, kernel driven methods within the Bayesian framework like Gaussian processes (GPs) account for prediction uncertainty by design. While GPs provide a ﬂexible framework for inferring distributions over functions, the inductive biases are controlled by the kernel function1. A well chosen kernel will typically yield dramatically better performance than a poorly chosen one.
How should we learn expressive kernels for high-dimensional tasks? This has frequently been high-lighted as a central question for the continued relevance of GP methods [11]. This work uses rep-resentations generated by a deep neural network to identify suitably expressive kernels for high-dimensional GP regression tasks. Kernel recommendation is performed by a decoder with access to a large vocabulary of primitive kernels and products of primitive kernels. The decoder maps an
∗Work undertaken while at Secondmind 1also called covariance function or covariance kernel 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
encoded representation of a dataset to a kernel that can be used to model it. The representation is attained by encoding the dataset, treated as a sequence of (input, output) pairs D = {xi, yi}N i=1, utilising the permutation-equivariant nature of self-attention networks.
By training KITT with a sufﬁciently rich vocabulary of kernels, it can predict suitable kernels for a diverse array of real datasets. This work presents the following novel contributions:
• Inspired by the successes of image captioning networks, we develop a novel framework named KITT for amortised kernel search. KITT takes raw datasets for predictive modelling as input and proposes kernels composed from a large vocabulary of kernel functions.
• KITT’s architecture introduces two novel features: it is entirely agnostic to the length and dimensionality of the data we wish to perform inference on, and it offers double permuta-tion invariance (this ensures its outputs are invariant to permutations in either input dimen-sions or data points).
• We show that KITT can deliver kernel predictions in under 0.1 seconds.
• We introduce a novel variant of the linear kernel which forms a key component of KITT’s vocabulary.
• We demonstrate that the kernels identiﬁed by KITT offer strong performance against other baselines which deal with kernel engineering in the context of GPs. 2