Abstract
Recent work introduced deep kernel processes as an entirely kernel-based alter-native to NNs (Aitchison et al. 2020). Deep kernel processes ﬂexibly learn good top-layer representations by alternately sampling the kernel from a distribution over positive semi-deﬁnite matrices and performing nonlinear transformations. A particular deep kernel process, the deep Wishart process (DWP), is of particular interest because its prior can be made equivalent to deep Gaussian process (DGP) priors for kernels that can be expressed entirely in terms of Gram matrices. How-ever, inference in DWPs has not yet been possible due to the lack of sufﬁciently
ﬂexible distributions over positive semi-deﬁnite matrices. Here, we give a novel approach to obtaining ﬂexible distributions over positive semi-deﬁnite matrices by generalising the Bartlett decomposition of the Wishart probability density. We use this new distribution to develop an approximate posterior for the DWP that includes dependency across layers. We develop a doubly-stochastic inducing-point inference scheme for the DWP and show experimentally that inference in the DWP can improve performance over doing inference in a DGP with the equivalent prior. 1

Introduction
The successes of modern deep learning have highlighted that good performance on tasks such as image classiﬁcation (Krizhevsky et al., 2012) requires deep models with lower layers that have the
ﬂexibility to learn good representations. Up until very recently, this was only possible in feature-based methods such as neural networks (NNs). Kernel methods did not have this ﬂexibility because the kernel could be modiﬁed only using a few kernel hyperparameters. However, with the advent of deep kernel processes (DKPs; Aitchison et al., 2021), we now have deep kernel methods that offer neural-network like ﬂexibility in the kernel / top-layer representation. DKPs introduce this
ﬂexibility by taking the kernel from the previous layer, then sampling from a Wishart or inverse
Wishart centered on that kernel, followed by a nonlinear transformation. The sampling and nonlinear transformation steps are repeated multiple times to form a deep architecture. Remarkably, deep
Gaussian processes (DGPs; Damianou & Lawrence, 2013; Salimbeni & Deisenroth, 2017), standard
Bayesian NNs, inﬁnite-width Bayesian NNs (neural network Gaussian processes or NNGPs; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019; Garriga-Alonso et al., 2019) and inﬁnite
NNs with ﬁnite width bottlenecks (Aitchison, 2020) can be written as DKPs (Aitchison et al., 2021).
Indeed, for kernels that can be expressed in terms of operations on Gram matrices, Aitchison et al. (2021) showed that a particular DKP, the deep Wishart process (DWP) has a prior equivalent to a DGP’s prior. In a DGP, the random variables inferred in variational inference are the model’s intermediate features, with kernels computed as a function of these features at each layer. However, in a DWP, there are no features at all: the only random variables are the positive semi-deﬁnite kernel 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
matrices themselves, which are sampled directly from Wishart distributions: the DWP works entirely on the kernel matrices implied by the DGP’s features.
Aitchison et al. (2021) argued that DWPs should have considerable advantages over related feature-based models, because feature-based models have pervasive symmetries in the true posterior, which are difﬁcult to capture in standard variational approximate posteriors. For instance, in a neural network, it is possible to permute rows and columns of weight matrices, such that the activations at a given layer are permuted, but the network’s overall input-output function remains the same (MacKay, 1992; Sussmann, 1992; Bishop et al., 1995). These permutations result in network weights with exactly the same probability density under the true posterior, but with very different probability densities under standard variational approximate posteriors, which are generally unimodal. However, these issues do not arise with DWPs, because all permutations of the hidden units correspond to the same kernel (see Appendix D in Aitchison et al. (2021) for more details).
While Aitchison et al. (2021) showed the equivalence between DWPs and DGPs, they were not able to do inference in DWPs because they were not able to ﬁnd a sufﬁciently ﬂexible distribution over positive semi-deﬁnite matrices to form the basis of an approximate posterior. Instead, they were forced to work with a different DKP: the deep inverse Wishart processes (DIWPs), which was easier because the inverse Wishart itself forms a suitable approximate posterior. While the DIWP also avoids using features, it does not correspond directly to an already-established Bayesian model.
Therefore, the DWP is a more important model, as it allows us to directly compare feature-based and kernel-based inference. In this work, we show how to create a sufﬁciently ﬂexible approximate posterior for DWPs, thereby enabling us to compare directly to their equivalent DGPs. In particular, our contributions are:
• We develop a new family of ﬂexible distributions over positive semi-deﬁnite matrices by generalising the Bartlett decomposition (Sec. 3.1).
• We use this distribution to develop an effective approximate posterior for the deep Wishart process which incorporates dependency across layers (Sec. 3.2).
• We develop a doubly stochastic inducing-point inference scheme for the DWP. While the derivation mostly follows that for deep inverse Wishart processes (Aitchison et al., 2021), we need to give a novel scheme for sampling the test/training points conditioned on the inducing points, as this is very different in the DWP compared to the previous DIWP (Sec. 3.3).
• We empirically compare DGP and DWP inference with the exact same prior. This was not possible in Aitchison et al. (2021) as they only derived an inference scheme for deep inverse
Wishart processes, whose prior is not equivalent to a DGP prior.
We provide a reference implementation at https://github.com/LaurenceA/bayesfunc. 2