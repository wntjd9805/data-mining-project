Abstract
Normalization techniques have become a basic component in modern convolutional neural networks (ConvNets). In particular, many recent works demonstrate that promoting the orthogonality of the weights helps train deep models and improve robustness. For ConvNets, most existing methods are based on penalizing or normalizing weight matrices derived from concatenating or ﬂattening the convo-lutional kernels. These methods often destroy or ignore the benign convolutional structure of the kernels; therefore, they are often expensive or impractical for deep
ConvNets. In contrast, we introduce a simple and efﬁcient “Convolutional Normal-ization” (ConvNorm) method that can fully exploit the convolutional structure in the Fourier domain and serve as a simple plug-and-play module to be conveniently incorporated into any ConvNets. Our method is inspired by recent work on pre-conditioning methods for convolutional sparse coding and can effectively promote each layer’s channel-wise isometry. Furthermore, we show that our ConvNorm can reduce the layerwise spectral norm of the weight matrices and hence improve the
Lipschitzness of the network, leading to easier training and improved robustness for deep ConvNets. Applied to classiﬁcation under noise corruptions and generative adversarial network (GAN), we show that the ConvNorm improves the robustness of common ConvNets such as ResNet and the performance of GAN. We verify our
ﬁndings via numerical experiments on CIFAR and ImageNet. Our implementation is available online at https://github.com/shengliu66/ConvNorm. 1

Introduction
In the past decade, Convolutional Neural Networks (ConvNets) have achieved phenomenal success in many machine learning and computer vision applications [1–7]. Normalization is one of the most important components of modern network architectures [8]. Early normalization techniques, such as batch normalization (BatchNorm) [4], are cornerstones for effective training of models beyond a few layers. Since then, the values of normalization for optimization and learning is extensively studied, and many normalization techniques, such as layer normalization [9], instance normalization [10], and group normalization [11] are proposed. Many of such normalization techniques are based on estimating certain statistics of neuron inputs from training data. However, precise estimations of the statistics may not always be possible. For example, BatchNorm becomes ineffective when the batch size is small [12], or batch samples are statistically dependent [13].
∗The ﬁrst two authors contributed to this work equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Weight normalization [14] is a powerful alternative to BatchNorm that improves the conditioning of neural network training without the need to estimate statistics from neuron inputs. Weight normalization operates by either reparameterizing or regularizing the network weights so that all the weights have unit Euclidean norm. Since then, various forms of normalization for network weights are proposed and become critical for many tasks such as training Generative Adversarial Networks (GANs) [15] and obtaining robustness to input perturbations [16, 17]. One of the most popular forms of weight normalization is enforcing orthogonality, which has drawn attention from a diverse range of research topics. The idea is that weights in each layer should be orthogonal and energy-preserving.
Orthogonality is argued to play a central role for training ultra-deep models [18–22], optimizing recurrent models [23–26], improving generalization [27], obtaining robustness [28, 29], learning disentangled features [30, 31], improving the quality of GANs [32, 33], learning low-dimensional embedding [34], etc.
Exploiting convolution structures for normal-ization. Our work is motivated by the pivotal role of weight normalization in deep learning. In the context of ConvNets, the network weights are multi-dimensional (e.g., 4-dimensional for a 2D
ConvNet) convolutional kernels. A vast major-ity of existing literature [27, 28, 35–39] imposes orthogonal weight regularization for ConvNets by treating multi-dimensional convolutional ker-nels as 2D matrices (e.g., by ﬂattening certain di-mensions) and imposing orthogonality of the ma-trix. However, this choice ignores the translation-invariance properties of convolutional operators and, as shown in [22], does not guarantee en-ergy preservation. On the other hand, these meth-ods often involve dealing with matrix inversions that are computationally expensive for deep and highly overparameterized networks.
Figure 1: Comparison between BatchNorm and ConvNorm on activations of k = 1, . . . , C channels. BatchNorm subtracts and multiplies the activations of each channel by computed scalars: mean µ and variance σ2, before a per-channel afﬁne transform parameterized by learned param-eters β and γ; ConvNorm performs per-channel convolution with precomputed kernel v to nor-malize the spectrum of the weight matrix for the convolution layer, following with a channel-wise convolution with learned kernel r as the afﬁne transform.
In contrast, in this work we introduce a new nor-malization method dedicated to ConvNets, which explicitly exploits translation-invariance proper-ties of convolutional operators. Therefore, we term our method as Convolutional Normalization (ConvNorm). We normalize each output channel for each layer of ConvNets, similar to recent preconditioning methods for convolutional sparse coding [40]. The ConvNorm can be viewed as a reparameterization approach for the kernels, that actually it normalizes the weight of each channel to be tight frame.2 While extra mathematical hassles do exist in incorporating translation-invariance properties, and it turns out to be a blessing, rather than a curse, in terms of computation, as it allows us to carry out the inversion operation in our ConvNorm via fast Fourier transform (FFT) in the frequency domain, for which the computation complexity can be signiﬁcantly reduced.
In summary, for ConvNets our approach enjoys several clear advantages
Highlights of our method. over classical normalization methods [41–43], that we list below:
• Easy to implement. In contrast to weight regularization methods that often require hyperparameter tuning and heavy computation [41, 43], the ConvNorm has no parameter to tune and is efﬁcient to compute. Moreover, the ConvNorm can serve as a simple plug-and-play module that can be conveniently incorporated into training almost any ConvNets.
• Improving network robustness. Although the ConvNorm operates on each output channel separately, we show that it actually improves the overall layer-wise Lipschitzness of the ConvNets.
Therefore, as demonstrated by our experiments, it has superior robustness performance against noise corruptions and adversarial attacks.
• Improving network training. We numerically demonstrate that the ConvNorm accelerates training on standard image datasets such as CIFAR [44] and ImageNet [45]. Inspired by the work [40, 46], our high-level intuition is that the ConvNorm improves the optimization landscape that optimization algorithms converge faster to the desired solutions. 2Tight frame can be viewed as a generalization of orthogonality for overcomplete matrices, which is also energy preserving. 2