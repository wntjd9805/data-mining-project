Abstract
Knowledge distillation has shown great success in classiﬁcation, however, it is still challenging for detection. In a typical image for detection, representations from different locations may have different contributions to detection targets, making the distillation hard to balance. In this paper, we propose a conditional distillation framework to distill the desired knowledge, namely knowledge that is beneﬁcial in terms of both classiﬁcation and localization for every instance. The framework introduces a learnable conditional decoding module, which retrieves information given each target instance as query. Speciﬁcally, we encode the condition informa-tion as query and use the teacher’s representations as key. The attention between query and key is used to measure the contribution of different features, guided by a localization-recognition-sensitive auxiliary task. Extensive experiments demon-strate the efﬁcacy of our method: we observe impressive improvements under various settings. Notably, we boost RetinaNet with ResNet-50 backbone from 37.4 to 40.7 mAP (+3.3) under 1× schedule, that even surpasses the teacher (40.4 mAP) with ResNet-101 backbone under 3× schedule. Code has been released on https://github.com/megvii-research/ICD. 1

Introduction
Deep learning applications blossom in recent years with the breakthrough of Deep Neural Networks (DNNs) [17, 24, 21]. In pursuit of high performance, advanced DNNs usually stack tons of blocks with millions of parameters, which are computation and memory consuming. The heavy design hinders the deployment of many practical downstream applications like object detection in resource-limited devices. Plenty of techniques have been proposed to address this issue, like network pruning [15, 27, 18], quantization [22, 23, 35], mobile architecture design [38, 39] and knowledge distillation (KD)
[19, 37, 43]. Among them, KD is one of the most popular choices, since it can boost a target network without introducing extra inference-time burden or modiﬁcations.
KD is popularized by Hinton et al. [19], where knowledge of a strong pretrained teacher network is transferred to a small target student network in the classiﬁcation scenario. Many good works emerge following the classiﬁcation track [50, 37, 32]. However, most methods for classiﬁcation perform badly in the detection: only slight improvements are observed [28, 51]. This can be attributed to two reasons: (1) Other than category classiﬁcation, another challenging goal to localize the object is seldomly considered. (2) Multiple target objects are presented in an image for detection, where objects
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) KD [19]. (b) Conventional detection KD. (c) Ours: Instance-conditional KD.
Figure 1: Compare with different methods for knowledge distillation. (a) KD [19] for classiﬁcation transfers logits. (b) Recent methods for detection KD distill intermediate features, different region-based sampling methods are proposed. (c) Our method explicitly distill the desired knowledge. can distribute in different locations. Due to these reasons, the knowledge becomes rather ambiguous and imbalance in detection: representations from different positions like foreground or background, borders or centers, could have different contributions, which makes distillation challenging.
To handle the above challenge, two strategies are usually adopted by previous methods in detection.
First, the distillation is usually conducted among intermediate representations, which cover all necessary features for both classiﬁcation and localization. Second, different feature selection methods are proposed to overcome the imbalance issue. Existent works could be divided into three types according to the feature selection paradigm: proposal-based, rule-based and attention-based. In proposal-based methods [28, 11, 6], proposal regions predicted by the RPN [36] or detector are sampled for distillation. In rule-based methods [14, 45], regions selected by predesigned rules like foreground or label-assigned regions are sampled. Despite their improvements, limitations still exist due to the hand-crafted designs, e.g., many methods neglect the informative context regions or involve meticulous decisions. Recently, Zhang et al. [51] propose to use attention [43], a type of intermediate activation of the network, to guide the distillation. Although attention provides inherent hints for discriminative areas, the relation between activation and knowledge for detection is still unclear. To further improve KD quality, we hope to provide an explicit solution to connect the desired knowledge with feature selection.
Towards this goal, we present Instance-Conditional knowledge Distillation (ICD), which introduces a new KD framework based-on conditional knowledge retrieval. In ICD, we propose to use a decoding network to ﬁnd and distill knowledge associated with different instances, we deem such knowledge as instance-conditional knowledge. Fig. 1 shows the framework and compares it with former ones,
ICD learns to ﬁnd desired knowledge, which is much more ﬂexible than previous methods, and is more consistent with detection targets. In detail, we design a conditional decoding module to locate knowledge, the correlation between knowledge and each instance is measured by the instance-aware attention via the transformer decoder [5, 43]. In which human observed instances are projected to query and the correlation is measured by scaled-product attention between query and teacher’s representations. Following this formulation, the distillation is conducted over features decomposed by the decoder and weighted by the instance-aware attention. Last but not least, to optimize the decoding module, we also introduce an auxiliary task, which teaches the decoder to ﬁnd useful information for identiﬁcation and localization. The task deﬁnes the goal for knowledge retrieval, it facilitates the decoder instead of the student. Overall, our contribution is summarized in three-fold:
• We introduce a novel framework to locate useful knowledge in detection KD, we formulate the knowledge retrieval explicitly by a decoding network and optimize it via an auxiliary task.
• We adopt the conditional modeling paradigm to facilitate instance-wise knowledge transferring.
We encode human observed instances as query and decompose teacher’s representations to key and value to locate ﬁne-grained knowledge. To our knowledge, it is the ﬁrst trial to explore instance-oriented knowledge for detection.
• We perform comprehensive experiments on challenging benchmarks. Results demonstrate impres-sive improvements over various detectors with up to 4 AP gain in MS-COCO, including recent detectors for instance segmentation [41, 46, 16]. In some cases, students with 1× schedule are even able to outperform their teachers with larger backbones trained 3× longer. 2
2