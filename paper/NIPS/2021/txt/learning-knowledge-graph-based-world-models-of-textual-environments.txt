Abstract
World models improve a learning agent’s ability to efﬁciently operate in interactive and situated environments. This work focuses on the task of building world models of text-based game environments. Text-based games, or interactive narratives, are reinforcement learning environments in which agents perceive and interact with the world using textual natural language. These environments contain long, multi-step puzzles or quests woven through a world that is ﬁlled with hundreds of characters, locations, and objects. Our world model learns to simultaneously: (1) predict changes in the world caused by an agent’s actions when representing the world as a knowledge graph; and (2) generate the set of contextually relevant natural language actions required to operate in the world. We frame this task as a Set of Sequences generation problem by exploiting the inherent structure of knowledge graphs and actions and introduce both a transformer-based multi-task architecture and a loss function to train it. A zero-shot ablation study on never-before-seen textual worlds shows that our methodology signiﬁcantly outperforms existing textual world modeling techniques as well as the importance of each of our contributions. 1

Introduction
World models, often in the form of probabilistic generative models, are used in conjunction with model-based reinforcement learning to improve a learning agent’s ability to operate in various environments [33, 7]. They are inspired by human cognitive processes [15], with a key hypothesis being that the ability to predict how the world will change in response to one’s actions will help you better plan what actions to take [12]. Evidence towards this hypothesis comes in the form of studies showing that simulating trajectories using internal learned models of the world improves sample efﬁciency in learning to operate in an environment [12, 30].
Text-based games, in which players perceive and interact with the world entirely through textual natural language, are environments that provide new challenges for world model approaches. Text-based games are structured as long puzzles or quests that can only be solved by navigating and interacting with potentially hundreds of locations, characters, and objects. The puzzle-like structures to the games are exacerbated by two factors. First, these environments are partially observable, i.e. observations of the world are incomplete. Second, the agent faces a combinatorially-sized action space of the order of O(1014) possible actions at every step. For these reasons model-free reinforcement learning in text-based games is extremely data inefﬁcient.
Prior work on text-based game playing agents repeatedly demonstrated that providing agents with structured memory in the form of knowledge graphs (sets of (cid:104)s, r, o(cid:105) tuples such that s is a subject, r is a relation, and o is an object) is critical in enabling them to operate in and model these worlds [3, 21, 1, 2]—aiding in both the challenges mentioned. These works all rely on extracting information 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Two subsequent states in Zork1 consisting of: textual observations, world knowledge graphs, valid actions, and actions taken. about one’s surroundings while navigating novel environments, either through rules [3, 2], question-answering [5], or transformer-based extraction [1, 21]. This lifted representation helps agents remember aspects of the world that become unobservable as the agent navigates the environment.
However, we hypothesize that agents that rely on lifted representations of the world will beneﬁt from more than just memorization but the ability to predict how the graph state representation will change. For example, by inferring that a locked chest is likely to contain treasure before it is actually revealed provides an agent with a form of look-ahead that will potentially enable it to bias its actions towards opening such a chest. We introduce an approach to this knowledge representation problem that effectively simpliﬁes it by exploiting the inherent structure of knowledge graphs—framing it to be the task of inferring the difference in knowledge graphs between subsequent states given an action.
A consequence of the combinatorially-sized action space in text-based games is that that the set of contextually relevant actions—i.e. those that are most likely to affect change in the environment—are overwhelmed by the irrelevant actions. For example, it is not illegal to try to climb a tree when there are no trees present, and the game engine will just respond with feedback that nothing happens. An aspect of world modeling that has not been considered for other games is inferring which actions are valid in a particular context. We hypothesize that both the challenges mentioned are closely linked and present world models that multi-task learn to tackle both simultaneously—i.e. to answer the questions of “What actions can I perform?” and “How will the world change if I perform a particular action?”.
Our work has four core contributions. (1) We ﬁrst show how changes in the world can be represented in the form of differences between subsequent knowledge graph state representations. (2) We present the Worldformer, a novel multi-task transformer based architecture that learns to simultaneously generate both the set of graph differences and the set of contextually relevant actions. (3) We introduce a loss function and a training methodology that enable more effective training of the Worldformer by exploiting the fact that knowledge graphs and natural language valid actions can be represented permutation invariant Sets of Sequences—wherein the ordering of tokens within an item in the set matters but the set itself lacks ordering. (4) A zero-shot ablation study conducted on diverse set of never-before-seen text games shows the signiﬁcance of each of the prior three contributions in outperforming strong existing baselines. 2