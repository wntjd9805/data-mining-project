Abstract
We consider online optimization over Riemannian manifolds, where a learner attempts to minimize a sequence of time-varying loss functions deﬁned on Rieman-nian manifolds. Though many Euclidean online convex optimization algorithms have been proven useful in a wide range of areas, less attention has been paid to their Riemannian counterparts. In this paper, we study Riemannian online gradi-ent descent (R-OGD) on Hadamard manifolds for both geodesically convex and strongly geodesically convex loss functions, and Riemannian bandit algorithm (R-BAN) on Hadamard homogeneous manifolds for geodesically convex functions.
We establish upper bounds on the regrets of the problem with respect to time horizon, manifold curvature, and manifold dimension. We also ﬁnd a universal lower bound for the achievable regret by constructing an online convex optimiza-tion problem on Hadamard manifolds. All the obtained regret bounds match the corresponding results are provided in Euclidean spaces. Finally, some numerical experiments validate our theoretical results. 1

Introduction
The online optimization has been widely studied in the past decades in online routing, spam ﬁltering, and machine learning [4, 23, 8]. Without a prior knowledge of loss functions, an online convex optimization algorithm predicts solutions before the loss function is revealed.
In this paper, we consider the following Riemannian online convex optimization (R-OCO) problem, min xt∈K⊂M ft(xt), t = 1, 2, . . . , T, (1) where M is a complete Riemannian manifold equipped with a Riemannian metric g and K is a geodesically convex (g-convex) subset of M. Here, {ft}t=1,2,...,T is a sequence of unknown loss functions and every ft is a geodesically convex (g-convex) function with sufﬁcient smoothness.
The R-OCO problem (1) extends the online convex optimization in Euclidean spaces with potential applications in machine learning, such as online principal component analysis (PCA), dictionary learning, and neural networks [28, 19, 25].
∗Correspondence author (Y. Hong, +86-10-82541888 ) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The R-OCO problem (1) can be understood as a learning process of T rounds. At each round t = 1, 2, 3, . . . , T , an online learner chooses a strategy xt from the g-convex subset K. Later or simultaneously, the adversary (or nature) produces a g-convex loss function ft : K → R of which the learner has no prior knowledge. Finally, the learner receives the feedback and suffers the loss ft(xt).
Generally, there are two types of information feedback. One is the full information feedback, where the entire function ft is revealed to the learner; the other is the bandit feedback, where only the value ft(xt) is revealed. The goal of the R-OCO is to minimize the regret, deﬁned as
R(T ) =
T (cid:88) t=1 ft(xt) − min x∈K
T (cid:88) t=1 ft(x), which measures the difference between the cost by {xt}t=1,...,T and the best-ﬁxed point chosen in hindsight. An algorithm is called no-regret [32], if the regret of the algorithm goes sublinearly with the time horizon T .
For carrying out optimization on a manifold, some classical methods treat the manifold as a subset of an ambient Euclidean space and employ Euclidean constrained optimization techniques. For instance,
[30] presented an algorithm for the online PCA problem, where the variables were updated in an embedding Euclidean space and then projected onto a manifold. However, in practical applications, the dimension of an embedding Euclidean space can be too high (e.g., the Grassmann manifold [13]), and the projection can be expensive to compute (e.g., the manifold of symmetric positive deﬁnite (SPD) matrices [37]). An alternative approach termed Riemannian optimization makes use of intrinsic geometry of manifolds so that Riemannian optimization can optimize directly on the manifold as an unconstrained problem, and thus avoiding high dimension embedding and high computing cost for the projection. Furthermore, this viewpoint has shown beneﬁts from the g-convexity, by which a nonconvex optimization problem can be converted into a g-convex one [6]. Consequently, it is important to take a Riemannian approach in our problem (1).
Although there were many existing algorithms for ofﬂine manifold optimization problems [2, 31, 5], very few results were obtained about the Riemannian online optimization problem. [35] proposed an online algorithm for estimating hidden Markov chains on Hadamard homogeneous spaces and [9] analyzed Riemannian adaptive methods on products in the regret sense. More recently, [29] studied a zeroth-order online optimization problem on Hadamard manifolds with a sublinear assumption.
Contribution This paper aims to design no-regret algorithms for the R-OCO problem in both full information feedback and bandit feedback. The contribution of this paper is summarized as follows:
• We propose a Riemannian online gradient descent algorithm (R-OGD) for the R-OCO problem in the full information feedback, and then establish upper regret bounds of the
R-OGD algorithm for g-convex and strongly g-convex functions.
• We introduce a Riemannian bandit algorithm (R-BAN) for the R-OCO problem in the bandit feedback and then establish an upper regret bound for g-convex functions. Moreover, we develop a key technique to analyze the derivative of a local integration on homogeneous manifolds, which can be applied to estimate gradients in Riemannian optimization and beyond.
• We focus on the worst-case regret and present a universal lower regret bound of R-OCO algorithms with g-convex losses on Hadamard manifolds, which matches the upper bound achieved by the R-OGD algorithm for g-convex functions.
The established lower and upper bounds on the achievable bounds of R-OCO match their counterparts for Euclidean online convex optimization e.g., [38, 24, 20, 1]. We brieﬂy list our results in Table 1.