Abstract
Weakly supervised semantic segmentation produces pixel-level localization from class labels; however, a classiﬁer trained on such labels is likely to focus on a small discriminative region of the target object. We interpret this phenomenon using the information bottleneck principle: the ﬁnal layer of a deep neural network, activated by the sigmoid or softmax activation functions, causes an information bottleneck, and as a result, only a subset of the task-relevant information is passed on to the output. We ﬁrst support this argument through a simulated toy experiment and then propose a method to reduce the information bottleneck by removing the last activation function. In addition, we introduce a new pooling method that further encourages the transmission of information from non-discriminative regions to the classiﬁcation. Our experimental evaluations demonstrate that this simple modiﬁcation signiﬁcantly improves the quality of localization maps on both the
PASCAL VOC 2012 and MS COCO 2014 datasets, exhibiting a new state-of-the-art performance for weakly supervised semantic segmentation. The code is available at: https://github.com/jbeomlee93/RIB. 1

Introduction
Semantic segmentation is the task of recognizing objects in an image using pixel-level allocation of a semantic label. The development of deep neural networks (DNNs) has led to signiﬁcant advances in semantic segmentation [9, 24]. Training a DNN for semantic segmentation requires a dataset containing a large number of images annotated with pixel-level labels. However, preparing such a dataset requires considerable effort; for example, producing a pixel-level annotation for a single image in the Cityscapes dataset [12] takes more than 90 minutes. This high dependence on pixel-level labels can be alleviated by weakly supervised learning [62, 31, 34, 2].
The objective of weakly supervised semantic segmentation is to train a segmentation network with weak annotations, which provide less information about the location of a target object than pixel-level labels, but are cheaper to obtain. Weak supervision takes the form of scribbles [52], bounding boxes [50, 29, 34], or image-level class labels [31, 3, 2]. In this study, we focus on image-level class labels, because they are the cheapest and most popular option of weak supervision. Most methods that use class labels [31, 32, 55, 2, 7] generate pseudo ground truths for training a segmentation network using localization (attribution) maps obtained from a trained classiﬁer, such as a CAM [66] or a Grad-CAM [48]. However, these maps identify only small regions of a target object that are discriminative for the classiﬁcation [31, 2, 5] and do not identify the entire region occupied by the object, making the attribution maps unsuitable for training a semantic segmentation network. We interpret this phenomenon using the information bottleneck principle [54, 49, 46, 53].
∗Correspondence to: Sungroh Yoon <sryoon@snu.ac.kr>. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The information bottleneck theory analyzes the information ﬂow through sequential DNN layers: information regarding the input is compressed as much as possible as it passes through the layers of a
DNN, while preserving as much of the task-relevant information as possible. This is advantageous for obtaining optimal representations for classiﬁcation [15, 1] but is disadvantageous when apply-ing the attribution maps from the resulting classiﬁer to weakly supervised semantic segmentation.
The information bottleneck prevents the non-discriminative information of the target object from being considered in the classiﬁcation logit, and thus, the attribution maps focus on only the small discriminative regions of the target object.
We argue that the information bottleneck becomes prominent in the ﬁnal layer of the DNN due to the use of the double-sided saturating activation function therein (e.g., sigmoid, softmax). We propose a method to reduce this information bottleneck in the ﬁnal layer of the DNN by retraining the DNN without the last activation function. Additionally, we introduce a new pooling method that allows more information embedded in non-discriminative features, rather than discriminative features, to be processed in the last layer of a DNN. As a result, the attribution maps of the classiﬁer obtained by our method contain more information on the target object.
The main contributions of this study are summarized as follows. First, we highlight that the infor-mation bottleneck occurs mostly in the ﬁnal layer of the DNN, which causes the attribution maps obtained from a trained classiﬁer to restrict their focus to small discriminative regions of the target object. Second, we propose a method to reduce this information bottleneck by simply modifying the existing training scheme. Third, our method signiﬁcantly improves the quality of the localization maps obtained from a trained classiﬁer, exhibiting a new state-of-the-art performance on the PASCAL
VOC 2012 and MS COCO 2014 datasets for weakly supervised semantic segmentation. 2 Preliminaries 2.1
Information Bottleneck
Given two random variables X and Y , the mutual information I(X; Y ) quantiﬁes the mutual dependence between the two variables. Data processing inequality (DPI) [13] infers that any three variables X, Y , and Z that form a Markov Chain X → Y → Z satisfy I(X; Y ) ≥ I(X; Z). Each layer in a DNN processes the input only from the previous layer, which means that the DNN layers form a Markov chain. Therefore, the information ﬂow through these layers can be represented using
DPI. More speciﬁcally, when an L−layered DNN generates an output ˆY from a given input X through intermediate features Tl (1 ≤ l ≤ L), it forms a Markov Chain X → T1 → · · · → TL → ˆY , and the corresponding DPI chain can be expressed as follows:
I(X; T1) ≥ I(X; T2) ≥ · · · ≥ I(X; TL−1) ≥ I(X; TL) ≥ I(X; ˆY ). (1)
This implies that the information regarding the input X is compressed as it passes through the layers of the DNN.
Training a classiﬁcation network can be interpreted as extracting maximally compressed features of the input that preserve as much information as possible for classiﬁcation; such features are commonly referred to as minimum sufﬁcient features (i.e., discriminative information). The minimum sufﬁcient features (optimal representations T ∗) can be obtained by the information bottleneck trade-off between the mutual information of X and T (compression), and that of T and Y (classiﬁcation) [54, 15]. In other words, T ∗ = argminT I(X; T ) − βI(T ; Y ), where β ≥ 0 is a Lagrange multiplier.
Shwartz-Ziv et al. [49] observe a compression phase in the process of ﬁnding the optimal represen-tation T ∗: when observing I(X, Tl) for a ﬁxed l, I(X, Tl) steadily increases during the ﬁrst few epochs, but decreases in the later epochs. Saxe et al. [46] argue that the compression phase is mainly observed in DNNs equipped with double-sided saturating non-linearities (e.g., tanh and sigmoid), and is not observed in those equipped with single-sided saturating non-linearities (e.g., ReLU). This implies that DNNs with single-sided saturating non-linearities experience less information bottleneck than those with double-sided saturating non-linearities. This can also be understood in terms of gradient saturation in the double-sided saturating non-linearities: the gradient of those non-linearities with respect to an input above a certain value saturates close to zero [8]. Therefore, features above a certain value will have near-zero gradients during the back-propagation process and be restricted from additionally contributing to the classiﬁcation. 2
2.2 Class Activation Mapping
A class activation map (CAM) [66] identiﬁes regions of an image focused by a classiﬁer. The CAM is based on a convolutional neural network with global average pooling (GAP) before its ﬁnal classiﬁcation layer. This is realized by considering the class-speciﬁc contribution of each channel of the last feature map to the classiﬁcation score. Given a classiﬁer parameterized by θ = {θf , w} where f (·; θf ) is the feature extractor prior to GAP, and w is the weight of the ﬁnal classiﬁcation layer, a CAM of the class c is obtained from an image x as follows:
CAM(x; θ) = w(cid:124) max w c f (x; θf ) (cid:124) c f (x; θf )
, (2) where max(·) is the maximum value over the spatial locations for normalization. 2.3