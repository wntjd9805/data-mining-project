Abstract
Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works — how the network’s architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network’s robustness via the predictive power in its representations — the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also ﬁnd that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels. 1

Introduction
Supervised learning starts with collecting labeled data. Yet, high-quality labels are often expensive.
To reduce annotation cost, we collect labels from non-experts [1–4] or online queries [5–7], which are inevitably noisy. To learn from these noisy labels, previous works propose many techniques, including modeling the label noise [8–10], designing robust losses [11–14], adjusting loss before gradient updates [15–23], selecting trust-worthy samples [12, 22, 24–31], designing robust architectures [32– 39], applying robust regularization in training [40–45], using meta-learning to avoid over-ﬁtting [46, 47], and applying semi-supervised learning [28, 48–51] to learn better representations. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
While these methods improve some networks’ robustness to noisy labels, we observe that their effectiveness depends on how well the network’s architecture aligns with the target/noise functions, and they are less effective when encountering more realistic label noise that is class-dependent or instance-dependent. This motivates us to investigate an understudied topic: how the network’s architecture impacts its robustness to noisy labels.
We formally answer this question by analyzing how a network’s architecture aligns with the target function and the noise. To start, we measure the robustness of a network via the predictive power in its learned representations (Deﬁnition 1), as models with large test errors may still learn useful predictive hidden representations [52, 53]. Intuitively, the predictive power measures how well the representations can predict the target function. In practice, we measure it by training a linear model on top of the learned representations using a small set of clean labels and evaluate the linear model’s test performance [54].
We ﬁnd that a network having a more aligned architecture with the target function is more robust to noisy labels due to its more predictive representations, whereas a network having an architecture more aligned with the noise function is less robust. Intuitively, a good alignment between a network’s architecture and a function exists if the architecture can be decomposed into several modules such that each module can simulate one part of the function with a small sample complexity. The formal deﬁnition of alignment is in Section 2.3, adapted from [55].
Our proposed framework provides initial theoretical support for our ﬁndings on a simpliﬁed noisy setting (Theorem 2). Empirically, we validate our ﬁndings on synthetic graph algorithmic tasks by designing several variants of Graph Neural Networks (GNNs), whose theoretical properties and alignment with algorithmic functions have been well-studied [55–57]. Many noisy label training methods are applied to image classiﬁcation datasets, so we also validate our ﬁndings on image domains using different architectures.
Most of our analysis and experiments use standard neural network training. Interestingly, we ﬁnd similar results when using DivideMix [49], a SOTA method for learning with noisy labels: for networks less aligned with the target function, the SOTA method barely helps and sometimes even hurts test accuracy; whereas for more aligned networks, it helps greatly.
For well-aligned networks, the predictive power of their learned representation could further improve the test performance of SOTA methods, especially on class-dependent or instance-dependent label noise where current methods on noisy label training are less effective. Moreover, on Clothing1M [58], a large-scale dataset with real-world label noise, the predictive power of a well-aligned network’s learned representations could even outperform some sophisticated methods that use clean labels.
In summary, we investigate how an architecture’s alignments with different (target and noise) functions affect the network’s robustness to noisy labels, in which we discover that despite having large test errors, networks well-aligned with the target function can still be robust to noisy labels when evaluating their predictive power in learned representations. To formalize our ﬁnding, we provide a theoretical framework to illustrate the above connections. At the same time, we conduct empirical experiments on various datasets with various network architectures to validate this ﬁnding. Besides, this ﬁnding further leads to improvements over SOTA noisy-label-training methods on various datasets and under various kinds of noisy labels (Tables 5-10 in Appendix A). 1.1