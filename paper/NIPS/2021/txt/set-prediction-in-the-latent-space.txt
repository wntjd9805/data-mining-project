Abstract
Set prediction tasks require the matching between predicted set and ground truth set in order to propagate the gradient signal. Recent works have performed this matching in the original feature space thus requiring predeﬁned distance func-tions. We propose a method for learning the distance function by performing the matching in the latent space learned from encoding networks. This method enables the use of teacher forcing which was not possible previously since matching in the feature space must be computed after the entire output sequence is generated.
Nonetheless, a naive implementation of latent set prediction might not converge due to permutation instability. To address this problem, we provide sufﬁcient con-ditions for permutation stability which begets an algorithm to improve the overall model convergence. Experiments on several set prediction tasks, including image captioning and object detection, demonstrate the effectiveness of our method. Code is available at https://github.com/phizaz/latent-set-prediction. 1

Introduction
Set prediction is a task where a model predicts multiple elements whose ordering is not relevant for correctness. This task is central to many real-world problems such as object detection, image captioning, and multi-speaker speech recognition. Object detection requires predicting a set of bounding boxes without any speciﬁc ordering. Describing objects within an image is a kind of image captioning yet perfectly suitable for set prediction. Multi-speaker speech recognition is also well suited for set prediction since the order of transcripts is irrelevant. Though these tasks can naturally be modeled as set prediction, traditional deep learning is not inherently suitable for these tasks.
Multi-layer perceptrons and convolution networks with traditional loss functions impose a speciﬁc ordering on the prediction heads which hinders set prediction. A reasonable set prediction pipeline requires the model’s prediction heads to be more ﬂexible. Each head does not have a predeﬁned target, yet relies on its peers to determine what is best to predict to complete the target set. Recent works
[1, 2] emphasized using a Transformer model [3], which is permutation-invariant, coupled with a permutation-invariant loss function as the main ingredients. Any traditional loss function can be made permutation-invariant by solving for a minimum bijective matching between predicted set and ground truth set via the Hungarian algorithm under a certain distance metric. After the assignment, the loss function is calculated between the assigned pairs, and backpropagation is performed accordingly.
This scheme is known as Permutation Invariant Training (PIT). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
A distance metric used by the assignment must agree with the loss function in a way that the assignment is kept after an optimization step on the loss function. A distance metric that fails this criterion may switch pairings hindering the convergence. Hence, a distance metric is crucial to the convergence property of the set optimization scheme. One may argue to use the loss function itself as a distance metric. However, not all loss functions have meaningful scalar values. For example, the vanilla GAN’s loss [4] is not insightful in terms of progress or distance. Combining loss functions from different domains also complicates the matter because they are not easily comparable in their scalar forms. In object detection, both L1 error loss and cross entropy loss are used to learn bounding box prediction [2], but it is unclear how to deﬁne a proper distance metric from such a combination.
Either hand-tuned coefﬁcients or different surrogate distance metrics may be needed to form an effective distance metric. This begets the problem of selecting a proper distance metric for PIT. A set prediction scheme that does not require a hand-tuned distance function is appreciable.
Another hardship related to PIT is when applying set prediction on sequence domains that require teacher forcing to train. Auto-regressive with teacher forcing is often used for sequence prediction such as speech recognition [5, 6] and machine translation [7, 3]. However, teacher forcing requires a groundtruth assignment before it can begin prediction. PIT also relies on the teacher forcing prediction to do minimum assignment, resulting in a chicken and egg problem. If the set cardinality is small enough, it is possible to exhaustively teacher force with respect to all possible ground truths requiring O(N 2) forward passes through the model, and keep only those with the minimum assignment distances for optimization.
What if the Hungarian assignment is done in a latent space instead? Since the latent space is learned, the choice of any speciﬁc distance metric is alleviated – even a simple Euclidean distance is reasonable.
Since the latent space is prior to the sequence prediction, the prediction process knows exactly what its ground truth is which allows for efﬁcient, O(N ), teacher forcing. This paper presents latent set prediction (LSP) which enables the assignment in the latent space with Euclidean distance metric.
At the same time, it provides a convergence guarantee of the loss function by reducing the effect of permutation switches that can be problematic when performing matching in the latent space. Our contributions are as follows: 1. We propose a framework for deep set prediction that alleviates the need for hand-crafted distance metrics. 2. This framework is efﬁcient for the set of sequence predictions with teacher forcing requiring only O(N ) predictions, an improvement from the usual exhaustive O(N 2). 3. We provide a convergence proof of set prediction under this framework. 2