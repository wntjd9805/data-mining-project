Abstract
Holographic Reduced Representations (HRR) are a method for performing symbolic AI on top of real-valued vectors [1] by associating each vector with an abstract concept, and providing mathematical operations to manipulate vectors as if they were classic symbolic objects. This method has seen little use outside of older symbolic AI work and cognitive science. Our goal is to revisit this approach to understand if it is viable for enabling a hybrid neural-symbolic approach to learning as a differentiable component of a deep learning architecture. HRRs today are not effective in a differentiable solution due to numerical instability, a problem we solve by introducing a projection step that forces the vectors to exist in a well behaved point in space. In doing so we improve the concept retrieval efﬁcacy of HRRs by over 100
. Using multi-label classiﬁcation we demonstrate how to leverage the symbolic HRR properties to develop an output layer and loss function that is able to learn effectively, and allows us to investigate some of the pros and cons of an HRR neuro-symbolic learning approach. Our code can be found https://github.com/NeuromorphicComputationResearchProgram/ at
Learning-with-Holographic-Reduced-Representations
⇥ 1

Introduction
Symbolic and connectionist (or “neural”) based approaches to Artiﬁcial Intelligence (AI) and Machine
Learning (ML) have often been treated as two separate, independent methods of approaching AI/ML.
This does not need to be the case, and our paper proposes to study the viability of a hybrid approach to propagation based learning. In particular, we make use of the Holographic Reduced Representation (HRR) approach originally proposed by Plate [1]. Plate proposed using circular convolution as a
Rd, they can be
“binding” operator. Given two vectors in a d dimensional feature space, x, y
“bound” together using circular convolution, which we denote as s = x y. This gives us a new result s
Rd. The HRR approach also includes an inversion operation 2
⌦ that maps Rd
Rd.
†
! 2
With this binding and inverse operation, Plate showed that we can assign vectors to have a conceptual symbolic meaning and construct statements (or “sentences”) from these representations. For example, dog to represent “a red cat and a blue dog”. HRRs can also we can construct S = red cat query the statement representation S. To ask which animal was red, we compose S which gives us a numeric output approximately equal to the vector representing “cat”. cat + blue red †
⌦
⌦
⌦
⇡
The HRR framework requires a constant amount of memory, relies on well-optimized and scalable operations like the Fast Fourier Transform (FFT), provides symbolic manipulation, and uses only differentiable operations, that would make it seem ideal as a tool for exploring neuro-symbolic modeling. Unfortunately back-propagating through HRR operations does not learn in practice,
⇤Research work completed prior to joining Amazon. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
rendering it seemingly moot for such neuro-symbolic research. The goal of this work is to ﬁnd sufﬁcient conditions for successful back-propagation based learning with the HRR framework, and to develop evidence of its potential for designing future nero-symbolic architectures. Because the utility of HRRs for cognitive-science tasks are already well established, our application area will focus on a multi-label machine learning task to show how HRRs can provide potential value to research in areas it has not been previously associated with.
Backpropagating through HRRs is ineffective if done naively, and our goal is to show how to rectify this issue with a simple projection step and build viable loss functions out of HRR operations. We choose to do this with eXtreme Multi-Label (XML) classiﬁcation tasks as one that is impossible for
HRRs to tackle today. XML tasks have an input x, from which we have a large number L of binary
YL. Our approach will be to represent each of the L classes as a prediction problems
HRR concept vector, and construct an alternative to a fully connected output layer that uses the HRR framework for learning. Our experiments and ablations will focus on the impact of replacing a simple output layer with HRRs and other changes to the HRR approach to better understand its behaviors.
State-of-the-art XML performance is not a goal.
Y2, . . . ,
Y1,
Our contributions and the rest of our paper are organized as follows. In §2 we discuss prior approaches related to our own, and how our work differs from these methods. In §3 we give a brief overview of the HRR operator’s details given its niche recognition within the AI/ML community, and introduce a simple improvement that increases the binding stability and effectiveness by 100
. Then in §4 we show how we can leverage HRR’s symbolic nature to create a new output layer and loss function combination that provides several beneﬁts to deep extreme multi-label models. With this new method we demonstrate in §5 that compared to a simple fully-connected output layer we can reduce the output size by as much as 99.55%, resulting in a total reduction in model size by up to 42.09%, reduce training time by 41.86%, and obtain similar or improved relative accuracy by up to 30%. We also show that these results are often competitive with more advanced approaches for XML classiﬁcation, though do still suffer at the largest output spaces with 670k labels. Finally we conclude in §6.
⇥ 2