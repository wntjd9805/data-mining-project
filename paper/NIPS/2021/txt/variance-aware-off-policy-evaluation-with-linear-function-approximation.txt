Abstract
We study the off-policy evaluation (OPE) problem in reinforcement learning with linear function approximation, which aims to estimate the value function of a target policy based on the ofﬂine data collected by a behavior policy. We propose to incorporate the variance information of the value function to improve the sample efﬁciency of OPE. More speciﬁcally, for time-inhomogeneous episodic linear
Markov decision processes (MDPs), we propose an algorithm, VA-OPE, which uses the estimated variance of the value function to reweight the Bellman residual in Fitted Q-Iteration. We show that our algorithm achieves a tighter error bound than the best-known result. We also provide a ﬁne-grained characterization of the distribution shift between the behavior policy and the target policy. Extensive numerical experiments corroborate our theory. 1

Introduction
Reinforcement learning (RL) has been a hot spot in both theory and practice in the past decade. Many efﬁcient algorithms have been proposed and theoretically analyzed for ﬁnding the optimal policy adopted by an agent to maximize the long-term cumulative rewards. In contrast to online RL where the agent actively interacts with the environment, ofﬂine RL (a.k.a., batch RL) [25, 24] aims to extract information from past data and use this information to learn the optimal policy. There has been much empirical success of ofﬂine RL in various application domains [4, 6, 37, 40, 36].
Among various tasks of ofﬂine RL, an important task is called off-policy evaluation (OPE), which evaluates the performance of a target policy π given ofﬂine data generated by a behavior policy ¯π.
Most existing theoretical works on OPE are in the setting of tabular MDPs [34, 26, 11, 16, 45, 47–49], where the state space S and the action space A are both ﬁnite. However, real-world applications often have high-dimensional or even inﬁnite-dimensional state and action spaces, where function approximation is required for computational tractability and generalization. While provably efﬁcient online RL with linear function approximation has been widely studied recently [46, 17, 50, 15, 3, 54], little work has been done for analyzing OPE with linear function approximation, with one notable exception by Duan et al. [10]. More speciﬁcally, Duan et al. [10] analyzed a regression-based
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Fitted Q-Iteration method (FQI-OPE) that achieves an (cid:101)O(H 2(cid:112)(1 + d(π, ¯π))/N ) error for linear
MDPs [46, 17], where H is the planning horizon, N is the sample size, and d(π, ¯π) represents the distribution shift between the behavior policy and the target policy. They also proved a sample complexity lower bound for a subclass of linear MDPs, for which their algorithm is nearly minimax optimal. However, as we will show later, the H 2 dependence is not tight since they discard the useful variance information contained in the ofﬂine data. Consequently, their result is only optimal for a small class of MDPs of which the value functions have large variance. The H 2 dependence in the sample complexity also makes their algorithm less sample-efﬁcient for long-horizon problems, which is one of the major challenges in RL.
Extracting useful information from the data is particularly important for ofﬂine RL since the agent cannot sample additional data by interacting with the environment, as compared to online RL. In this paper, we propose a new algorithm that incorporates the variance information of the value functions to improve the sample efﬁciency of OPE. This allows us to achieve a deeper understanding and tighter error bounds of OPE with linear function approximation. In detail, we consider time-inhomogeneous linear MDPs [46, 17] where the transition probability and reward function are assumed to be linear functions of a known feature mapping and may vary from stage to stage.
The main contributions of this paper are summarized as follows:
• We develop VA-OPE (Variance-Aware Off-Policy Evaluation), an algorithm for OPE that effectively utilizes the variance information from the ofﬂine data. The core idea behind the proposed algorithm is to calibrate the Bellman residual in the regression by an estimator of the conditional variance of the value functions, such that data points of higher quality can receive larger important weights.
• We show that our algorithm achieves (cid:101)O((cid:80) h Λ−1
K) policy evaluation error, where vh is the expectation of the feature vectors under target policy and Λh is the uncentered covariance matrix under behavior policy weighted by the conditional variance of the value function. Our algorithm achieves a tighter error bound and milder dependence on H than FQI-OPE [10], and provides a tighter characterization of the distribution shift between the behavior policy and the target policy, which is also veriﬁed by extensive numerical experiments. h vh)1/2/ h(v(cid:62)
√
• Our analysis is based on a novel two-step proof technique. In the ﬁrst step, we use backward induction to establish worst-case uniform convergence2 results for the estimators of the value functions. In the second step, the convergence of OPE estimator is proved by tightening the uniform convergence result based on an average-case analysis. Our proof strategy provides a generic way for analyzing (weighted) ridge regression methods that are carried out in a backward and iterated fashion. The analyses in both steps might be of independent interest.
√
Notation We use lower case letters to denote scalars and use lower and upper case boldface letters to denote vectors and matrices respectively. For any vector x ∈ Rd and any positive semi-deﬁnite matrix Σ ∈ Rd×d, we denote by (cid:107)x(cid:107)2 the Euclidean norm and (cid:107)Σ(cid:107) the operator norm, and deﬁne x(cid:62)Σx. For any positive integer n, we denote by [n] the set {1, . . . , n}. For any ﬁnite set (cid:107)x(cid:107)Σ =
A, we denote by |A| the cardinality of A. For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn, and we write an = Ω(bn) if there exists an absolute constant C such that an ≥ Cbn. We use (cid:101)O(·) to further hide the logarithmic factors. 2 Preliminaries 2.1 Markov Decision Processes h=1, {Ph}H
We consider the time-inhomogeneous episodic Markov Decision Process (MDP), which is represented by a tuple M (S, A, H, {rh}H h=1). In speciﬁc, we denote the state space by S and the action space by A, and H > 0 is the horizon length of each episode. At each stage h ∈ [H], rh : S × A → [0, 1] is the reward function, and Ph(s(cid:48)|s, a) is the transition probability function which represents the probability for state s to transit to state s(cid:48) given action a. A policy π consists of
H mappings {πh}H h=1 from S to the simplex on A, such that for any (h, s) ∈ [H] × S, πh(·|s) is a probability distribution over A. Here a policy can be either deterministic (point mass) or stochastic. 2By uniform convergence we mean the convergence of the estimated value functions in (cid:96)∞-norm to their true values, which is different from the uniform convergence over all policies in Yin et al. [48]. 2
For any policy π, we deﬁne the associated action-value function Qπ at each stage h ∈ [H] as follows: h(s, a) and value function V π h (s)
Qπ h(s, a) = Eπ (cid:20) H (cid:88) i=h (cid:12) (cid:12) ri(si, ai) (cid:12) (cid:12) sh = s, ah = a (cid:21)
,
V π h (s) = (cid:90)
A
Qπ h(s, a)dπh(a|s), (2.1) where ai ∼ πi(·|si) and si+1 ∼ Pi(·|si, ai). For any function V : S → R, we introduce the following shorthand notation for the conditional expectation and variance of V :
[PhV ](s, a) = Es(cid:48)∼Ph(·|s,a)[V (s(cid:48))],
[VhV ](s, a) = [PhV 2](s, a) − ([PhV ](s, a))2. (2.2)
Time-inhomogeneous linear MDPs. We consider a special class of MDPs called linear MDPs [46, 17]. Note that most of the existing works on RL with linear function approximation rely on this assumption.
Assumption 2.1. M (S, A, H, {rh}H h=1) is called a linear MDP with a known feature mapping φ : S × A → Rd, if for any h ∈ [H], there exist γh and µh ∈ Rd, such that for any state-action pair (s, a) ∈ S × A, it holds that h=1, {Ph}H
Ph(· | s, a) = (cid:104)φ(s, a), µh(·)(cid:105), (2.3)
We assume that at any stage h, for any state-action pair (s, a) ∈ S × A, the reward received by the agent is given by r = rh(s, a) + (cid:15)h(s, a), where rh(s, a) ∈ [0, 1] is the expected reward and (cid:15)h(s, a) is the random noise. We assume that the noise is zero-mean and independent of anything else. rh(s, a) = (cid:104)φ(s, a), γh(cid:105).
Without loss of generality, we assume that (cid:107)γh(cid:107)2 ≤ 1 and (cid:107)φ(s, a)(cid:107)2 ≤ 1 for all (s, a) ∈ S × A.
We also assume that rh(s, a) + (cid:15)h(s, a) ≤ 1, |(cid:15)h(s, a)| ≤ 1 almost surely and thus Var((cid:15)h(s, a)) ≤ 1 d for all h ∈ [H] and (s, a) ∈ S × A. Moreover, we assume that maxh∈[H] for all bounded function f : S → R such that sups∈S |f (s)| ≤ 1.
S f (s)dµh(s)(cid:13) (cid:13) (cid:82) (cid:13) (cid:13)2 ≤
√
The above assumption on linear MDPs implies the following proposition for the action-value func-tions.
Proposition 2.2 (Proposition 2.3, [17]). For a linear MDP, for any policy π, there exist weights
{wπ h(s, a) = (cid:104)φ(s, a), wπ h(cid:105).
Moreover, we have (cid:107)wπ h, h ∈ [H]} such that for any (s, a, h) ∈ S × A × [H], we have Qπ d for all h ∈ [H].
√ h(cid:107)2 ≤ 2H
Following this proposition, we may further show that the value functions are also linear functions, but of different features. We deﬁne φπ
A φ(s, a)dπh(a|s) for all s ∈ [S] and h ∈ [H]. Then by (2.1) we have h(s) = (cid:82)
V π h (s) = (cid:90)
A 2.2 Off-policy Evaluation
φ(s, a)(cid:62)wπ hdπ(a|s) = (cid:104)φπ h(s), wπ h(cid:105).
The purpose of OPE is to evaluate a (known) target policy π given an ofﬂine dataset generated by a different (unknown) behavior policy ¯π. In this paper, our goal is to estimate the expectation of the value function induced by π over a ﬁxed initial distribution ξ1, i.e., 1 = Es∼ξ1[V π vπ
To faciliate the presentation, we further introduce some important notations. For all h ∈ [H], let νh be the occupancy measure over S × A at stage h induced by the transition P and the behavior policy
¯π, that is, for any E ⊆ S × A, 1 (s)].
νh(E) = E [(sh, ah) ∈ E | s1 ∼ ξ1, ai ∼ ¯π(·|si), si+1 ∼ Pi(·|si, ai), 1 ≤ i ≤ h] . (2.4)
For simplicity, we write Eh[f (s, a)] = E¯π,h[f (s, a)] = (cid:82)
S×A f (s, a)dνh(s, a) for any function f on
S × A. Similarly, we use Eπ,h[f (s, a)] to denote the expectation of f with respect to the occupancy measure at stage h induced by the transition P and the target policy π.
We deﬁne the following uncentered covariance matrix under behavior policy for all h ∈ [H]: (2.5)
Intuitively, these matrices measure the coverage of the ofﬂine data in the state-action space. It is known that the success of OPE necessitates a good coverage [10, 43]. Therefore here we make the same coverage assumption on the ofﬂine data.
Σh = E¯π,h (cid:2)φ(s, a)φ(s, a)(cid:62)(cid:3) . 3
Assumption 2.3 (Coverage). For all h ∈ [H], κh := λmin(Σh) > 0. Denote κ = minh∈[H] κh.
A key difference in our result is that, instead of depending on Σh directly, the error bound depends on the following weighted version of the covariance matrices deﬁned as (cid:2)σh(s, a)−2φ(s, a)φ(s, a)(cid:62)(cid:3) ,
Λh := E¯π,h (2.6) for all h ∈ [H], where each σh : S × A → R is deﬁned as
σh(s, a) := (cid:113) max{1, VhV π h+1(s, a)} + 1. (2.7)
Note that in the deﬁnition of σh(·, ·), taking the maximum and adding an extra 1 is purely for technical reason and is related to its estimator (cid:98)σh(·, ·), which we will introduce and explain later in Section 3.2.
In general, one can think of σ2 h+1(s, a). Therefore, compared with the raw covariance matrix Σh, Λh further incorporates the variance of the value functions under the target policy. This is the key to obtaining a tighter instance-dependent error bound.
Deﬁnition 2.4 (Variance-aware coverage). We deﬁne ιh := λmin(Λh) and ι = minh∈[H] ιh. h(s, a) ≈ VhV π
Since sup(s,a)∈S×A σh(s, a)2 is bounded from above, by (2.6) and Assumption 2.3, we immediately have ιh ≥ κh/[sup(s,a)∈S×A σh(s, a)2] > 0 for all h ∈ [H], and thus ι > 0. Even if Assump-tion 2.3 does not hold, we can always restrict to the subspace span{φ(sh, ah)}. For convenience of presentation, we make Assumption 2.3 in this paper.
Next, we introduce the assumption on the sampling process of the ofﬂine data.
Assumption 2.5 (Stage-sampling Data). We have two ofﬂine datasets D and ˇD where each dataset consists of data from H stages: D = {Dh}h∈[H] and ˇD = { ˇDh}h∈[H]. For the dataset D, we assume Dh1 is independent of Dh2 for h1 (cid:54)= h2. For each stage h, we have
Dh = {(sk,h, ak,h, rk,h, s(cid:48) the data point (sk,h, ak,h, rk,h, s(cid:48) k,h) is sampled identically and independently in the following way: (sk,h, ak,h) ∼ k,h ∼ Ph(·|sk,h, ak,h). The
νh(·, ·) where νh(·, ·) is the occupancy measure deﬁned in (2.4), and s(cid:48) same holds for ˇD, and we write ˇDh = {(ˇsk,h, ˇak,h, ˇrk,h, ˇs(cid:48) k,h (cid:54)= sk,h+1. k,h)}k∈[K], where we assume for each k ∈ [K], k,h)}k∈[K]. Note that here s(cid:48)
Assumptions 2.5 is standard in the ofﬂine RL literature [48, 10]. Note that in the assumption, there is a data splitting, i.e., one can view it as the whole dataset D ∪ ˇD being split into two halves. The datasets D and ˇD will then be used for two different purposes in Algorithm 1 as will be made clear in the next section. We would like to remark that the only purpose of the splitting is to avoid a lengthy analysis. There is no need to perform the data splitting in practice. Also, in our implementation and experiments, we do not split the data. 3 Algorithm
To ease the notation, we denote φk,h = φ(sk,h, ak,h), ˇφk,h = φ(ˇsk,h, ˇak,h), (cid:98)σk,h = (cid:98)σh(sk,h, ak,h) and rk,h = rh(sk,h, ak,h) + (cid:15)k,h for all (h, k) ∈ [H] × [K]. Recall that we use the check mark to denote the other half of the splitted dataset. How the splitted data is utilized will be clear in Section 3.2 when we introduce the proposed algorithm. 3.1 Regression-Based Value Function Estimation
By Proposition 2.2, it sufﬁces to estimate the vectors {wπ the Least-Square Value Iteration (LSVI) [17] which relies on the Bellman equation, Qπ rh(s, a) + [PhV π h+1](s, a), that holds for all h ∈ [H] and (s, a) ∈ S × A. By viewing V π as an unbiased estimate of [PhV π following ridge regression problem: h, h ∈ [H]}. A popular approach is to apply h(s, a) = h+1(s(cid:48) k,h) h+1](sk,h, ak,h), the idea of the LSVI-type method is to solve the (cid:98)wπ h := argmin w∈Rd
λ(cid:107)w(cid:107)2 2 +
K (cid:88) k=1 (cid:2)(cid:104)φk,h, w(cid:105) − rk,h − V π h+1(s(cid:48) k,h)(cid:3)2
, (3.1) 4
for some regularization parameter λ > 0. Since we do not know the exact values of V π replace it by an estimator (cid:98)V π manner, which enjoys a closed-form solution as follows h+1 in (3.1), we h+1, and then recursively solve the lease-square problem in a backward (cid:98)wπ h = (cid:34) K (cid:88) k=1
φk,hφ(cid:62) k,h + λId (cid:35)−1 K (cid:88) k=1
φk,h (cid:104) rk,h + (cid:98)V π h+1(s(cid:48) (cid:105) k,h)
.
This has been used in the LSVI-UCB algorithm proposed by Jin et al. [17] and the FQI-OPE algorithm studied by Duan et al. [10], for online learning and OPE of linear MDPs respectively. For this kind of algorithms, the key difﬁculty in the analysis lies in bounding the Bellman error: (cid:34) K (cid:88) k=1
φk,hφ(cid:62) k,h + λId (cid:35)−1 K (cid:88) k=1
φk,h (cid:0)[Ph (cid:98)V π h+1](sk,h, ak,h) − (cid:98)V π h+1(s(cid:48) k,h)(cid:1).
Jin et al. [17] applied a Hoeffding-type inequality to bound the Bellman error. Although Duan et al. [10] applied Freedman’s inequality in their analysis, their algorithm design overlooks the variance information in the data and consequently they can only adopt a crude upper bound on the conditional variance of the value function, i.e., VhV π h+1 ≤ (H − h)2, which simply comes from sups∈S V π h+1(s) ≤ H − h. Therefore, it prevents [10] from getting a tight instance-dependent error bound for OPE. This is further veriﬁed by our numerical experiments in Appendix A which show that the performance of FQI-OPE degrades for large H. This motivates us to utilize the variance information in the data for OPE. 3.2 The Proposed Algorithm
In particular, we present our main algorithm as displayed in Algorithm 1. Due to the greedy nature of the value functions, we adopt a backward estimation scheme.
Weighted ridge regression. For any h ∈ [H], let (cid:98)wπ the previous step, and correspondingly (cid:98)V π h+1(·) = (cid:104)φπ regression (3.1), we consider the following weighted ridge regression: h+1 be the estimate of wπ h+1(·), (cid:98)wπ h+1 computed at h+1(cid:105). Instead of the ordinary ridge (cid:98)wπ h := argmin w∈Rd
λ(cid:107)w(cid:107)2 2 +
K (cid:88) k=1 (cid:104) (cid:104)φk,h, w(cid:105) − rk,h − (cid:98)V π h+1(s(cid:48) k,h) (cid:105)2 (cid:14) (cid:98)σ2 k,h, (3.2) where (cid:98)σk,h = (cid:98)σh(sk,h, ak,h) for all (h, k) ∈ [H] × [K] with (cid:98)σh(·, ·) being a proper estimate of
σh(·, ·) deﬁned in (2.7). We then have the following closed-form solution (Line 9 and 7 of Alg. 1): (cid:98)wπ h = (cid:98)Λ−1 h (cid:16)
φk,h
K (cid:88) k=1 rk,h + (cid:98)V π h+1(s(cid:48) k,h) (cid:17) (cid:14) (cid:98)σ2 k,h, with (cid:98)Λh =
K (cid:88) k=1 (cid:98)σ−2 k,hφk,hφ(cid:62) k,h + λId. (3.3)
In the above estimator, we use the dataset D to estimate the value functions. Next, we apply an
LSVI-type method to estimate σh using the dataset ˇD.
Variance estimator. By (2.2), we can write
[VhV π h+1](s, a) =[Ph(V π h+1)2](s, a) − (cid:0)[PhV π h+1](s, a)(cid:1)2
. (3.4)
For the ﬁrst term in (3.4), by Assumption 2.1 we have
[Ph(V π h+1)2](s, a) = (cid:90)
S h+1(s(cid:48))2dPh(s(cid:48)|s, a) = φ(s, a)(cid:62)
V π (cid:90)
S h+1(s(cid:48))2 dµh(s(cid:48)),
V π which suggests that Ph(V π (cid:104)φ(s, a), (cid:98)βπ h+1)2 also has a linear representation. Thus we adopt a linear estimator h (cid:105) where (cid:98)βπ
K (cid:88) h (Line 4) is the solution to the following ridge regression problem: (cid:104)(cid:10) ˇφk,h, β(cid:11) − [ (cid:98)V π
ˇφk,h (cid:98)V π
+ λ(cid:107)β(cid:107)2 h+1]2(ˇs(cid:48) 2 = (cid:98)Σ−1 h h+1(ˇs(cid:48)
K (cid:88) k,h) (cid:105)2 k,h)2. (3.5) (cid:98)βπ h = argmin
β∈Rd k=1 k=1 5
Algorithm 1 Variance-Aware Off-Policy Evaluation (VA-OPE) 1: Input: target policy π = {πh}h∈[H], datasets D = {{(sk,h, ak,h, rk,h, s(cid:48) k,h)}h∈[H]}k∈[K] and k,h)}h∈[H]}k∈[K], initial distribution ξ1, (cid:98)wπ
H+1 = 0
ˇD = {{(ˇsk,h, ˇak,h, ˇrk,h, ˇs(cid:48) 2: for h = H, H − 1, . . . , 1 do 3: 4: 5: 6: 7: 8: 9: k=1 k=1 (cid:80)K
ˇφk,h (cid:80)K (cid:113)
ˇφ(cid:62)
ˇφk,h (cid:98)V π
ˇφk,h (cid:98)V π k=1 max{1, (cid:98)Vh (cid:98)V π k,h/(cid:98)σ2 k,h), (cid:98)wπ (cid:98)Σh ← (cid:80)K (cid:98)βh ← (cid:98)Σ−1 h (cid:98)θh ← (cid:98)Σ−1 h (cid:98)σh(·, ·) ← (cid:98)Λh ← (cid:80)K
Yk,h ← rk,h + (cid:104)φπ (cid:98)wπ (cid:98)Qπ k,h + λId k,h)2 h+1(ˇs(cid:48) h+1(ˇs(cid:48) k,h) h+1(·, ·)} + 1 k,h + λId h+1(cid:105) h ← (cid:98)Λ−1 k=1 φk,hYk,h/(cid:98)σ2 h(·, ·) ← (cid:104)φ(·, ·), (cid:98)wπ h (·) ← (cid:104)φπ (cid:98)V π h(cid:105), 1 ← (cid:82) k=1 φk,hφ(cid:62) h(s(cid:48) 1 (s) dξ1(s) (cid:80)K
S (cid:98)V π k,h h 10: 11: end for 12: Output: (cid:98)vπ h(·), (cid:98)wπ h(cid:105)
θ∈Rd with (cid:98)Σh = (cid:80)K k=1 k=1
ˇφk,h
Similarly, we estimate the second term in (3.4) by (cid:104)φ(s, a), (cid:98)θπ (cid:104)(cid:10) ˇφk,h, θ(cid:11) − (cid:98)V π (cid:98)θh = argmin
+ λ(cid:107)θ(cid:107)2 h+1(ˇs(cid:48)
K (cid:88) k,h) (cid:105)2 2 = (cid:98)Σ−1 h h (cid:105), where (cid:98)θπ h (Line 5) is given by
K (cid:88) k=1
ˇφk,h (cid:98)V π h+1(ˇs(cid:48) k,h), (3.6)
ˇφ(cid:62) k,h + λId. Combining (3.5) and (3.6), we estimate VhV π h+1 by (cid:105)2 (cid:104)
[(cid:98)Vh (cid:98)V π h+1](·, ·) = (cid:104)φ(·, ·), (cid:98)βπ h (cid:105)[0,(H−h+1)2] − where the subscript [0, (H − h + 1)2] denotes the clipping into the given range, and similar for the subscript [0, H − h + 1]. We do such clipping due to the fact that V π h+1 ∈ [0, H − h]. We add 1 to deal with the approximation error in (cid:98)V π
Based on (cid:98)Vh (cid:98)V π h+1, the ﬁnal variance estimator (cid:98)σh(·, ·) (Line 6) is deﬁned as h (cid:105)[0,H−h+1] (cid:104)φ(·, ·), (cid:98)θπ h+1. (3.7)
, (cid:113) (cid:98)σh(·, ·) = max{1, (cid:98)Vh (cid:98)V π h+1(·, ·)} + 1.
In order to deal with the situation where (cid:98)Vh (cid:98)V π between (cid:98)Vh (cid:98)V π which is an upper bound of the noise variance by Assumption 2.1. h+1 < 0 or is very close to 0, we take maximum h+1 and 1. Also, to account for the noise in the observed rewards, we add an extra 1
Final estimator. Recursively repeat the above procedure for h = H, H − 1, . . . , 1, and we obtain (cid:98)V1. Then the ﬁnal estimator for vπ 1 (Line 12) is deﬁned as (cid:98)vπ 1 (s) dξ1(s). 1 = (cid:82)
S (cid:98)V π
Intuition behind Λh. To illustrate the intuition behind the weighted covariance matrix Λh, here we provide some brief heuristics. Let {(sk,h, ak,h, s(cid:48) k,h)}k∈[K] be i.i.d. samples such that (sk,h, ak,h) ∼
ν for some distribution ν over S × A and s(cid:48) ek = φ(sk,h, ak,h) (cid:0)[PhV π k,h ∼ Ph(·|sk,h, ak,h). Deﬁne k,h)(cid:1) (cid:14) [VhV π h+1(s(cid:48) h+1](sk,h, ak,h) − V π h+1](sk,h, ak,h)2 for all k ∈ [K]. Note that ek’s are i.i.d zero-mean random vectors and a simple calculation yields
Cov(ek) = E (cid:2)[VhV π h+1](sk,h, ak,h)−2φ(sk,h, ak,h)φ(sk,h, ak,h)(cid:62)(cid:3) .
This coincides with (2.6). Suppose Cov(ek) (cid:31) 0, then by the central limit theorem, it holds that 1
√
K
K (cid:88) k=1 ek d−→ N (0, Cov(ek)).
Therefore, Cov(ek)−1, or equivalently Λ−1 h , can be seen as the Fisher information matrix asso-ciated with the weighted product of the Bellman error and the feature vectors. This is a tighter characterization of the convergence rate than bounding VhV π h+1 by its naive upper bound (H − h)2. 6
4 Theoretical Results
In this section, we introduce our main theoretical results and give an overview of the proof technique. 4.1 OPE Error Bound
Our main result is a reﬁned average-case OPE analysis that yields a tighter error bound in Theorem 4.1.
The proof is in Appendix D. To simplify the notation, we deﬁne:
Ch,2 =
H (cid:88) i=h
H − h + 1 2ιh
√
, Ch,3 = (H − h + 1)2 2
, Ch,4 = (cid:0)(cid:107)Λh(cid:107) · (cid:107)Λ−1 h (cid:107)(cid:1)1/2
.
Theorem 4.1. Set λ = 1. Under Assumptions 2.1, 2.3 and 2.5, if K satisﬁes
K ≥ C · C3 · d2 (cid:20) log (cid:18) dH 2K
κδ (cid:19)(cid:21)2
, (4.1) where C is some problem-independent universal constant and (cid:26)
C3 := max max h∈[H]
Ch,3 · C 2 8ι2 h h,2
,
H 4
κ2 ,
H 2
κ2 · max h∈[H]
Ch,3 2
· max h∈[H] (cid:27)
,
Ch,3
ιh then with probability at least 1 − δ, the output of Algorithm 1 satisﬁes 1 − (cid:98)vπ
|vπ 1 | ≤C · (cid:34) H (cid:88) h=1 (cid:107)vπ h(cid:107)Λ−1 h (cid:35) (cid:114)
· log(16H/δ)
K
+ C · C4 · log (cid:18) 16H
δ (cid:19)
· (cid:18) 1
K 3/4
+ (cid:19)
, 1
K where vπ h := Eπ,h[φ(sh, ah)] and C4 := (cid:80)H h=1 (cid:113)
Ch,4 · Ch,2 · (H−h+1)d 4ιh
· log (cid:0) dH 2K
κδ (cid:1) · (cid:107)vπ h(cid:107)Λ−1
. h h h
√
/
√
/ h(cid:107)Λ−1 h=1 (cid:107)vπ h=1(H − h + 1)(cid:107)vπ
Theorem 4.1 suggests that Algorithm 1 provably achieves a tighter instance-dependent error bound for OPE than that in [10]. In detail, the dominant term in our bound is (cid:101)O((cid:80)H
K), as compared to the (cid:101)O((cid:80)H
K) term in [10]. By (2.5) and (2.6), our h(cid:107)Σ−1 bound is at least as good as the latter since Σh (cid:22) [(H − h + 1)2 + 1]Λh. More importantly, it is instance-dependent and tight for the general class of linear MDPs: for those where VhV π h+1 is close to its crude upper bound (H − h + 1)2, our bound recovers the prior result. When VhV π h+1 is small, VA-OPE beneﬁts from incorporating the variance information and our bound gets tightened accordingly.
Remark 4.2. Note that we do not require VhV π h+1(s, a) to be uniformly small for all s, a, and h.
From the bound and (2.6), as long as the variances are smaller than (H − h + 1)2 on average of (s, a) ∈ S × A and in sum of h, the bound is improved. It is also worth noting that the lower bound proved in [10] only holds for a subclass of linear MDPs with VhV π h+1 = Ω((H − h + 1)2), and thus their minimax-optimality does not hold for general linear MDPs. For more detailed comparison we refer the reader to Appendix B.
Remark 4.3. Conceptually, the term (cid:107)vπ serves as a more precise characterization of the distribution shift between the behavior policy ¯π and the target policy π in a variance-aware manner.
This enables our algorithm to utilize the data more effectively. Compared with online RL where one can sample new data, OPE is more ‘data-hungry’: one cannot decide the overall quality of the data.
Thus it is especially beneﬁcial to put more focus on targeted values with less uncertainty. This is also the intuitive reason why our algorithm can achieve a tighter error bound. h(cid:107)Λ−1 h 4.2 Overview of the Proof Technique
Here we provide an overview of the proof for Theorem 4.1. Due to the parallel estimation of the the value functions and their variances, the analysis of VA-OPE is much more challenging compared with that of FQI-OPE. As a result, we need to develop a novel proof technique. First, we have the following error decomposition. 7
(4.2) (cid:35)
Lemma 4.4. For any h ∈ [H], let (cid:98)V π h+1 − (cid:98)V π (cid:34) h (s) − (cid:98)V π
V π
[Ph(V π h (s) = (cid:90)
A h be the output of Algorithm 1. Then it holds that h+1)](s, a)dπh(a|s) + λφπ h(s)(cid:62) (cid:98)Λ−1 h wπ h
+ φπ h(s)(cid:62) (cid:98)Λ−1 h
−λ h+1(s(cid:48)) − (cid:98)V π
V π h+1(s(cid:48)) (cid:90) (cid:16) (cid:17) dµh(s(cid:48)) +
K (cid:88) k=1
φk,h(cid:98)σ−2 k,h∆k,h
, k,h) − (cid:15)k,h. In particular, recall that (cid:98)vπ 1 = E[ (cid:98)V π 1 (s1) | where ∆k,h = [Ph (cid:98)V π s1 ∼ ξ1] and the OPE error can be decomposed as
S h+1(s(cid:48) h+1](sk,h, ak,h) − (cid:98)V π 1 − (cid:98)vπ vπ 1 = − λ
H (cid:88) (vπ h)(cid:62) (cid:98)Λ−1 h h=1 (cid:90) (cid:16)
S h+1(s) − (cid:98)V π
V π h+1(s) (cid:17)
µh(s)ds
H (cid:88)
+ (vπ h)(cid:62) (cid:98)Λ−1 h h=1
K (cid:88) k=1
φk,h(cid:98)σ−2 k,h∆k,h + λ
H (cid:88) h=1 (vπ h)(cid:62) (cid:98)Λ−1 h wπ h. (4.3)
The OPE error bound (Theorem 4.1) is proved by bounding the three terms separately in (4.3). This decomposition is different from [10] in that (cid:98)Σh is replaced by (cid:98)Λh. This prevents us from adopting a matrix embedding-type proof as used in the prior work.
The key is to show the convergence of (cid:98)Λh to its population counterpart. However, by deﬁnition of (cid:98)Λh, to establish such a result, it ﬁrst requires the convergence of (cid:98)V π h+1 in a uniform manner, i.e., a high probability bound for sups∈S | (cid:98)V π h (s)|. To show this, we leverage the decomposition in (4.2) and a backward induction technique, and prove a uniform convergence result which states that with high probability, for all h ∈ [H], Algorithm 1 can guarantee (cid:12) (cid:12) (cid:98)V π (cid:12) (cid:12) (cid:12) (cid:12) ≤ (cid:101)O h (s) h (s) − V π h (s) − V π h+1 to V π (cid:19)
. (cid:18) 1
√
K sup s∈S
This result is formalized as Theorem C.2 and proved in Appendix C. To the best of our knowledge,
Theorem C.2 is the ﬁrst to establish the uniform convergence of the estimation error for the value functions in ofﬂine RL with linear function approximation. We believe this result is of independent interest and may be broadly useful in OPE. 5 Numerical Experiments
In this section, we provide numerical experiments to evaluate our algorithm VA-OPE, and compare it with FQI-OPE.
We construct a linear MDP instance as follows. The MDP has |S| = 2 states and |A| = 100 actions, with the feature dimension d = 10. The behavior policy then chooses action a = 0 with probability p and a ∈ {1, · · · , 99} with probability 1 − p and uniformly over {1, · · · , 99}. The target policy π always chooses a = 0 no matter which state it is, making state 0 and 1 absorbing. The parameter p can be used to control the distribution shift between the behavior and target policies. Here p → 0 leads to small distribution shift, and p → 1 leads to large distribution shift. The initial distribution
ξ1 is uniform over |S|. For more details about the construction of the linear MDP and parameter conﬁguration, please refer to Appendix A.
We compare the performance of the two algorithms on the synthetic MDP described above under different choices of horizon length H. We plot the log-scaled OPE error versus
K in Figure 1. It is clear that VA-OPE is at least as good as FQI-OPE in all the cases. Speciﬁcally, for small H (Figure 1a), their performance is very comparable, which is as expected. As H increases, we can see from
Figure 1a, 1b and 1c that VA-OPE starts to dominate FQI-OPE, and the advantage is more signiﬁcant for larger H, as suggested by Theorem 4.1. Due to space limit, a comprehensive comparison under different parameter settings is deferred to Appendix A.
√ 6