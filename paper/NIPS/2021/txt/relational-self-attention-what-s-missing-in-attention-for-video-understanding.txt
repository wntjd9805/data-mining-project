Abstract
Convolution has been arguably the most important feature transform for modern neural networks, leading to the advance of deep learning. Recent emergence of
Transformer networks, which replace convolution layers with self-attention blocks, has revealed the limitation of stationary convolution kernels and opened the door to the era of dynamic feature transforms. The existing dynamic transforms, including self-attention, however, are all limited for video understanding where correspon-dence relations in space and time, i.e., motion information, are crucial for effective representation. In this work, we introduce a relational feature transform, dubbed the relational self-attention (RSA), that leverages rich structures of spatio-temporal relations in videos by dynamically generating relational kernels and aggregating re-lational contexts. Our experiments and ablation studies show that the RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on the standard motion-centric benchmarks for video action recognition, such as Something-Something-V1&V2, Diving48, and FineGym. 1

Introduction
Convolution [13, 25] is a feature transform that is ubiquitous in modern neural networks for visual recognition, and has driven the development of deep learning in the past decade. The stationarity of convolution kernels, however, may limit its expressivity and hinder adaptation to diverse com-positional possibilities of visual concepts [18]. Dynamic convolution [20] and self-attention [52] that construct kernels or attentions according to input contents have emerged as an alternative to static convolution in this context, being followed by further studies for dynamic feature trans-forms [6, 27, 33, 57]. The effectiveness of self-attention has been demonstrated by the success of
Transformer variants on different image understanding tasks such as image classiﬁcation [7, 51, 59], object detection [4], and semantic segmentation [43]. Recently, it has been further extended for video understanding, replacing spatio-temporal convolution [5, 48, 50] with spatio-temporal self-attention [1, 3, 35].
Despite their recent progress in the video domain, the existing dynamic feature transforms still leave much room for improvement in terms of learning relational patterns in space and time, i.e., motion information, which is known to be essential for video understanding [23, 53]. For example, spatio-temporal self-attention [55] often fails to learn motion representation without positional embeddings as demonstrated in [32], and even those with positional embeddings [1, 3, 35] turn out to be not effective on motion-centric action recognition benchmarks such as Something-Something [14].
In this work, we introduce a relational dynamic feature transform, dubbed relational self-attention (RSA), to address the limitation of existing methods. RSA leverages rich structures of spatio-temporal
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
relations in videos by dynamically generating relational kernels and aggregating relational contexts.
Combining these relational components with ordinary dynamic kernels and context features, our RSA learns a rich video representation that effectively captures both visual appearance and spatio-temporal motion dynamics.
The main contribution of this paper is three-fold:
• We re-interpret recent dynamic feature transforms in a uniﬁed way, and provide in-depth analysis on their capability of learning video representation.
• We introduce a new dynamic feature transform, i.e., RSA, which effectively captures both visual appearance and spatio-temporal motion dynamics for video understanding.
• Our RSA network substantially outperforms convolution and self-attention counterparts, achieving the state of the art on SS-V1&V2, Diving48, and FineGym. 2