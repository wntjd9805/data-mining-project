Abstract
Deep learning systems frequently fail at out-of-context (OOC) prediction, the prob-lem of making reliable predictions on uncommon or unusual inputs or subgroups of the training distribution. To this end, a number of benchmarks for measuring
OOC performance have been recently introduced. In this work, we introduce a framework unifying the literature on OOC performance measurement, and demon-strate how rich auxiliary information can be leveraged to identify candidate sets of OOC examples in existing datasets. We present NOOCH: a suite of naturally-occurring “challenge sets”, and show how varying notions of context can be used to probe speciﬁc OOC failure modes. Experimentally, we explore the tradeoffs between various learning approaches on these challenge sets and demonstrate how the choices made in designing OOC benchmarks can yield varying conclusions. 1

Introduction
People often ﬁnd context useful for prediction, both for improving accuracy and processing efﬁciency
[10]. However, deep learning systems frequently over-rely on context cues [18, 19, 38], which can lead to poor performance on out-of-context (OOC) examples, when contextual information is misleading. By OOC examples, we mean inputs which are uncommon or unusual with respect to the training distribution; these can be thought of as sampled from under-represented subgroups, or low (non-zero) density regions, of the training distribution. In safety-critical situations, this can be problematic; as such, it is important to have reliable methods for measuring how well a model can perform OOC. Furthermore, given the rise of larger models and datasets [31], there is a need for scalable approaches to OOC evaluation — even if manual evaluation of “corner cases” by domain experts may (always) be the gold standard.
A key pre-requisite task to evaluating OOC performance is identifying which examples should be considered OOC. This identiﬁcation task is a challenging one in and of itself: in a natural image,
“context” can be varied, complex and high-dimensional [6, 47, 59, 63]. Therefore, any evaluation method intending to measure a model’s OOC performance must (implicitly) select a speciﬁc notion of “OOC performance”. Indeed, since deep learning yields underspeciﬁed models [14], it is plausible that different choices may yield different measurements. Common approaches include generating semi-synthetic data to simulate the effect of a shift in some salient feature [32, 54, 63], or using some auxiliary information to guide choices about what a reasonable OOC set should be [28, 33].
In this work, we develop a conceptual framework for identifying sets of OOC examples in existing datasets. We show how our framework uniﬁes and generalizes prior literature on OOC performance measurement, and allows us to utilize more complex, structured annotated data to deﬁne various notions of “OOC performance”. We demonstrate this framework’s effectiveness through uncovering two OOC “challenge sets” [30] within an existing benchmark, each corresponding to differing notions 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
of context. We show how our framework enables scalable and targeted measurement of models’ OOC performance through clarifying the relationship between the concept of “OOC performance” and its implementation, allowing for clearer insight on current approaches as well as opportunities for improvement. Our contributions are as follows:
• We present NOOCH (Naturally-Occurring Out-of-context Challenge sets), a suite of “chal-lenge sets” for evaluating performance on naturally-arising OOC problems, available at https://github.com/dmadras/nooch;
• We develop a conceptual framework for automatically identifying OOC challenge sets from existing data by leveraging known underlying structure;
• We contrast two instantiations of this framework using two notions of “context”, deﬁning concepts of “hard positives” and “hard negatives” in the OOC setting; and
• We quantitatively analyze the performance of several methods from the robust learning literature on these challenge sets, exploring the tradeoffs inherent in different approaches to
OOC performance measurement; and qualitatively demonstrate how rich notions of context can yield rich investigation of OOC errors. 2 Measuring “Out-of-Context Performance”
Intuitively, a model which has good “OOC performance” should be able to maintain good performance under unusual or perturbed contextual conditions. We distinguish this from the out-of-distribution (OOD) problem [27], which is usually concerned with inputs from a different domain than the training set. Rather, the OOC prediction problem is more similar to subgroup robustness or distributional shift, where a model must perform well on uncommon input regions at training time. However, even after drawing this distinction, the notion is ill-deﬁned: “context” may refer to concepts as varied as object relationships [59], image backgrounds [63], experimental settings [47], or world models
[6]. Furthermore, even ﬁxing a notion of context, the criterion for what should make something
“out-of-context” (OOC) is still unclear. For instance, Peters et al. [47] focus on previously unobserved contexts (i.e. environments), whereas Xiao et al. [63] are concerned with unusual contexts given the class of interest (i.e. perturbations to image background).
Clearly, deﬁning a benchmark to measure a method’s OOC performance requires a number of design choices, which has enabled a recent proliferation of OOC benchmarks. We note in particular, one of the key choices is around the usage of auxiliary information. Across the literature on OOC performance measurement, there are a plethora of approaches to deﬁning OOC criteria using some type of auxiliary information C. For the purposes of algorithm designers, C may be assumed to be available at training, validation, and/or test time, or not at all — however, at the the time of benchmark design, it is available on a sufﬁciently large portion of the collected dataset to guide the designers’ choices about what a suitable OOC criterion should be. Examining the current literature on measuring
OOC performance, we identify the following as a unifying framework: 1. Identify some existing auxiliary information C, a variable which takes some value on many (or all) examples and speciﬁes some underlying structure in the data. 2. Select a notion of “OOC” (e.g. “images with misleading backgrounds are OOC”, “examples from unfamiliar time periods are OOC”) and deﬁne an “OOC criterion” by choosing a binary function φ of C. 3. Restrict the test set to those examples where φ = 1. Optionally, also restrict the training set to those examples where φ = 0.
We show in Table 1 how a range of prior literature leverages auxiliary information to deﬁne OOC criteria. We can think of C as providing benchmark designers with some type of “inductive bias” around what should be considered OOC for a given benchmark. The above framework implies that there is a diversity of OOC criteria which can be deﬁned over any dataset, and this class is as broad as the class of functions φ which can be deﬁned over the available auxiliary information. In the rest of the paper, we take advantage of the ﬂexibility of this framework to give two examples of such an approach. We show that by leveraging more complex annotated structure, we can create multiple OOC benchmarks from an existing dataset using multiple criteria for what should be considered “OOC”.
We trace out how the choices made in designing these criteria correspond to different notions of context, and demonstrate experimentally that these yield varying measurements of OOC performance. 2
Dataset
Waterbirds [54] iWildCam2020-Wilds[33]
FMoW-Wilds [33]
Imagenet-A [28]
Breeds [55]
Auxiliary Information C 1 if background is water (binary) camera trap ID (categorical) time stamp (ordinal) max NLL of ensemble (continuous) C
C subclass (categorical)
C
OOC function φ
= Y 1 . . . 245
} 2013
≥ log (0.15) target subset
C /
∈ {
Ctime
≥ −
∈
Table 1: Examples of OOC benchmarks from the literature under our framework. The right-most column lists the condition under which φ = 1. The OOC function φ in [28] has several additional
ﬁltering steps, some heavily manual.
Figure 1: Using the co-occurrence/extractibility (CE) criterion, examples of (0.05, 0.1)-hard positives (top row) and negatives (bottom row) for the classes (L to R): kite, sports_ball, surfboard. 3 Finding Naturally-Occurring OOC Problems
We now demonstrate concretely how rich auxiliary information can be used to study the way that context shifts arise naturally within an existing computer vision benchmark, and provide two criteria for OOC performance that can be computed from these annotations. Throughout, we consider the binary prediction task of determining object presence, a problem where relationships between various objects naturally provide helpful context — given an image X, is an object of class Y present or not?