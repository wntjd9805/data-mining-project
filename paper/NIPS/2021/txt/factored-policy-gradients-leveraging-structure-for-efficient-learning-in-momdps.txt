Abstract
Policy gradient methods can solve complex tasks but often fail when the dimension-ality of the action-space or objective multiplicity grow very large. This occurs, in part, because the variance on score-based gradient estimators scales quadratically.
In this paper, we address this problem through a factor baseline which exploits in-dependence structure encoded in a novel action-target inﬂuence network. Factored policy gradients (FPGs), which follow, provide a common framework for analysing key state-of-the-art algorithms, are shown to generalise traditional policy gradients, and yield a principled way of incorporating prior knowledge of a problem domain’s generative processes. We provide an analysis of the proposed estimator and iden-tify the conditions under which variance is reduced. The algorithmic aspects of
FPGs are discussed, including optimal policy factorisation, as characterised by minimum biclique coverings, and the implications for the bias-variance trade-off of incorrectly specifying the network. Finally, we demonstrate the performance advantages of our algorithm on large-scale bandit and trafﬁc intersection problems, providing a novel contribution to the latter in the form of a spatial approximation. 1

Introduction
Many sequential decision-making problems in the real-world have objectives that can be naturally decomposed into a set of conditionally independent targets. Control of water reservoirs, energy consumption optimisation, market making, cloud computing allocation, sewage ﬂow systems, and robotics are but a few examples [36]. While many optimisation methods have been proposed [25, 34]
— perhaps most prominently using Lagrangian scalarisation [46] — multi-agent learning has emerged as a promising new paradigm for sample-efﬁcient learning [6]. In this class of algorithms, the multi-objective learning problem is cast into a centralised, co-operative stochastic game in which co-ordination is achieved through global coupling terms in each agent’s objective/reward functions.
For example, a grocer who must manage their stock could be decomposed into a collection of sub-agents that each manage a single type of produce, but are subject to a global constraint on inventory.
This approach has been shown to be very effective in a number of domains [20, 50, 30, 24, 52], but presents both conceptual and technical issues.
The transformation of a multi-objective Markov decision process (MOMDP) [36] into a stochastic game is a non-trivial design challenge. In many cases there is no clear delineation between agents in the new system, nor an established way of performing the decomposition. What’s more, it’s unclear in many domains that a multi-agent perspective is appropriate, even as a technical trick. For example, the concurrent problems studied by Silver et al. [39] exhibit great levels of homogeneity, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
lending themselves to the use of a shared policy which conditions on contextual information. The key challenge that we address in this paper is precisely how to scale these single-agent methods — speciﬁcally, policy gradients — in a principled way. As we shall see, this study reveals that existing methods in both single- and multi-agent multi-objective optimisation can be formulated as special cases of a wider family of algorithms we entitle factored policy gradients. The contributions of this paper are summarised below: 1. We introduce inﬂuence networks as a framework for modelling probabilistic relationships between actions and objectives in an MOMDP, and show how they can be combined with policy factorisation via graph partitioning. 2. We propose a new control variate — the factor baseline — that exploits independence structures within a (factored) inﬂuence network, and show how this gives rise to a novel class of algorithms to which we ascribe the name factored policy gradients. 3. We show that FPGs generalise traditional policy gradient estimators and provide a com-mon framework for analysing state-of-the-art algorithms in the literature including action-dependent baselines and counterfactual policy gradients. 4. The variance properties of our family of algorithms are studied, and minimum factorisation is put forward as a principled way of applying FPGs, with theoretical results around the existence and uniqueness of the characterisation. 5. The ﬁnal contribution is to illustrate the effectiveness of our approach over traditional estimators on two high-dimensional benchmark domains. 1.1