Abstract
Novel view synthesis is a long-standing problem in machine learning and computer vision. Signiﬁcant progress has recently been made in developing neural scene representations and rendering techniques that synthesize photorealistic images from arbitrary views. These representations, however, are extremely slow to train and often also slow to render. Inspired by neural variants of image-based rendering, we develop a new neural rendering approach with the goal of quickly learning a high-quality representation which can also be rendered in real-time.
Our approach, MetaNLR++, accomplishes this by using a unique combination of a neural shape representation and 2D CNN-based image feature extraction, aggregation, and re-projection. To push representation convergence times down to minutes, we leverage meta learning to learn neural shape and image feature priors which accelerate training. The optimized shape and image features can then be extracted using traditional graphics techniques and rendered in real time. We show that MetaNLR++ achieves similar or better novel view synthesis results in a fraction of the time that competing methods require. 1

Introduction
Learning 3D scene representations from partial observations captured by a sparse set of 2D images is a fundamental problem in machine learning, computer vision, and computer graphics. Such a representation can be used to reason about the scene or to render novel views. Indeed, the latter application has recently received a lot of attention (e.g., [1]). For this problem setting, the key questions are: (1) How do we parameterize the scene, and (2) how do we infer the parameters from our observations efﬁciently? With our work, we offer new solutions to answer these questions.
Several classes of scene representation learning approaches have recently been proposed. One popular approach consists of coordinate-based neural networks combined with volume rendering, like NeRF [2]. Although these representations offer photorealistic quality for synthesized images, they are slow to train and render. Coordinate-based networks that implicitly model surfaces combined with sphere tracing–based rendering are another popular approach [3–6]. One beneﬁt of an implict surface is that, once trained, it can be extracted and rendered in real time [5]. However, training these representations is equally slow as training volumetric representations. Finally, approaches that use a proxy geometry with on-surface feature aggregation are also fast to render [7, 8], but the quality and runtime of these methods is limited by the traditional 3D computer vision algorithms that pre-compute the proxy shape, such as structure from motion (SfM) or multiview stereo (MVS).
Here, we develop a new framework for neural scene representation and rendering with the goal of enabling both fast training and rendering times. To optimize the training time of our framework, we do not learn a representation network for the view-dependent radiance, as other neural volume 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
or surface methods do, but directly aggregate the features extracted from the source views on the surface of our learned proxy shape. To cut down on pre-processing times required by SfM and
MVS, we optimize a coordinate-based network representing the proxy shape end-to-end with our
CNN-based feature encoder and decoder, and learned aggregation function. A key contribution of our work is to combine this unique surface-based neural rendering framework with meta learning, which enables us to learn efﬁcient initializations for all of the trainable parts of our framework and further minimize training time. Because our representation directly parameterizes an implicit surface, it can be extracted and rendered in real time. This representation thus incorporates both the fast training beneﬁts from generalizing over shape representations and image features, and the fast rendering capabilities of implicit surface-based methods. We demonstrate training of high-quality neural scene representations in minutes or tens of minutes, rather than hours or days, which can then be rendered at real-time framerates. 2