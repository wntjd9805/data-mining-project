Abstract
Current transfer learning algorithm designs mainly focus on the similarities between source and target tasks, while the impacts of the sample sizes of these tasks are often not sufficiently addressed. This paper proposes a mathematical framework for quantifying the transferability in multi-source transfer learning problems, with both the task similarities and the sample complexity of learning models taken into account. In particular, we consider the setup where the models learned from different tasks are linearly combined for learning the target task, and use the optimal combining coefficients to measure the transferability. Then, we demonstrate the analytical expression of this transferability measure, characterized by the sample sizes, model complexity, and the similarities between source and target tasks, which provides fundamental insights of the knowledge transferring mechanism and the guidance for algorithm designs. Furthermore, we apply our analyses for practical learning tasks, and establish a quantifiable transferability measure by exploiting a parameterized model. In addition, we develop an alternating iterative algorithm to implement our theoretical results for training deep neural networks in multi-source transfer learning tasks. Finally, experiments on image classification tasks show that our approach outperforms existing transfer learning algorithms in multi-source and few-shot scenarios. 1

Introduction
Transfer learning is nowadays an active research area in machine learning focusing on solving target learning tasks by the knowledge of learnable source tasks. The transferability between source and target tasks is the central topic in transfer learning for understanding the knowledge transferring mechanisms and the algorithm designs [1]. In general, the transferability can be affected by several factors, including: (i) the similarities between source tasks and the target task [2]; (ii) the sample sizes of the tasks; and (iii) the complexity or dimensionality of the machine learning model. Most of the existing transfer strategies are designed based on how similar the source and target tasks are [2, 3], without considering the impacts of the training sample sizes or the complexity of the models.
∗Work performed at Tsinghua-Berkeley Shenzhen Institute
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In theoretical analyses [4, 5, 6], sample sizes and model complexity are often included in deriving upper bounds for the transferability or the performance of transfer learning algorithms. However, it is pointed out that such bounds derived under general learning settings are often relatively loose under numerical simulations [7], and hence the algorithms designed by directly applying theoretical results can hardly achieve satisfactory performance in practical applications. Thus, the gap between theory and practice opposes the fundamental understandings of transfer learning algorithms.
In this paper, we propose a mathematical framework to investigate the transferability in multi-source transfer learning problem, and establish a quantifiable transferability measure for practical learning tasks. Specifically, for given source tasks, we learn the target task by a class of learning model which linearly combines the models learned from individual tasks by some designable coefficients.
In addition, the performance of this combined model is measured by the empirical risk of only the testing data of the target task, considered as the testing loss. Then, we adopt the optimal combining coefficients that achieve the minimum testing loss as the transferability measure, which illustrates the contribution of each model in learning the target task, and effectively quantifies the knowledge transferable among different tasks.
In our development, we establish an analytical solution of the transferability measure, which is jointly quantified by sample sizes, model complexity, and a similarity measure between source and target tasks. In particular, we demonstrate that the transferability of a particular source task is typically proportional to the number of samples and the measure of similarity to the target task, and is inversely proportional to the model complexity. This coincides with the intuition that when more training samples are available for a source task that is highly similar to the target task, more knowledge will be transferable from the source task to the target task. On the other hand, when the model is very complicated or high-dimensional, it is typically harder to train the model well, and less knowledge can be acquired and transferred. More importantly, our theoretical results can be applied for designing effective and efficient algorithms for real transfer learning problems, which are especially useful for multi-source transfer learning with a large number of source tasks that are generally difficult to deal with.
The contribution of this paper can be summarized as follows:
• We propose a mathematical framework for transfer learning analyses, and establish a transferability measure on discrete data, quantified by the number of samples, the complexity of the model, and the χ2-distance between source and target tasks.
• We extend the transferability analyses to the continuous data, and establish a similar transfer-ability measure that can be evaluated in practical tasks, by exploiting paramerized models.
• We apply our theoretical results to develop an iterative algorithm for training deep neural networks in general supervised transfer learning scenarios. Moreover, our algorithm can be practically applied for multi-source transfer learning.
• The experiments in real datasets validate our proposed algorithm, in which we show that our approach outperforms many existing transfer learning algorithms.
Due to the space limitations, the proofs of theorems and propositions are presented in the supplemental material. 2 Problem Formulation and Analysis
Let X and Y be the random variables denoting the data and label with domains X and Y, respectively, and let P denote the set of all distributions on X
Y. For the convenience of illustration, here we
× assume X to be discrete, and will extend our analyses to continuous cases later. Throughout our (α0, . . . , αk) : (cid:80)k analyses, we will use Ak ≜ to denote the i=0 αi = 1, αi ≥
{ k-dimensional simplex. 0, i = 0, . . . , k
} 2.1
Single-Source Transfer Learning
To begin, we consider the transfer learning setting with one source task and one target task, denoted as task 1 and 0, respectively. Specifically, for each task i = 0, 1, we assume that ni training 2
samples
XY (x, y) > 0, for all x, y, and the empirical distributions ˆP (i)
P (i)
ℓ=1 are i.i.d. generated from some underlying joint distribution P (i) ni
}
P with3
P of the samples are defined as
ℓ , y(i)
ℓ ) (x(i)
{
XY ∈
XY ∈
XY (x, y) ≜ 1
ˆP (i) ni ni(cid:88)
ℓ=1 1
{ x(i)
ℓ = x, y(i)
ℓ = y
,
} (1) denotes the indicator function [8]. Then, the empirical distributions ˆP (0) where 1
XY can be regarded as the models learned from the target task and the source task, respectively, when all the entries of the mass functions are required to determine.
XY and ˆP (1)
{·}
To develop the transferability measure, our proposed framework focuses on a convex combination of both learned models4:
Q(α0,α1)
XY (2)
A1 are parameters to be designed. Notice that these parameters characterize the where (α0, α1) knowledge transferred from the source task to target task, and the designing of these parameters will be affected by the sample sizes and the task similarities, which essentially leads to a transferability measure adjusted by the sample complexity.
XY (x, y) + α1 ˆP (1) (x, y) ≜ α0 ˆP (0) for all (x, y)
XY (x, y),
Y,
×
X
∈
∈
Then, the performance of the model Q(α0,α1) is evaluated by the testing loss, measured by its empirical risk on the testing data of target task. Conventionally, such empirical risk is often computed by the logarithm loss. However, the logarithm risk can be ill-defined5 in our setting. Therefore, we alternatively apply the referenced χ2-distance as the measure, defined as follows.
Definition 1. Given a reference distribution RXY , for any distribution PXY and QXY , the referenced
χ2-distance between them is defined as
XY
RXY (PXY , QXY ) ≜ (cid:88)
χ2 x∈X,y∈Y (PXY (x, y)
QXY (x, y))2
−
RXY (x, y)
.
Specifically, we denote χ2(PXY , QXY ) ≜ χ2
χ2-divergence.
PXY (PXY , QXY ), which corresponds to the Pearson
We choose the underlying target distribution P (0) averaged Pearson χ2-divergence
XY as the reference, and define the testing loss as the
L(α0,α1) test
≜ E (cid:104)
χ2 (cid:16)
XY , Q(α0,α1)
P (0)
XY (cid:17)(cid:105)
, (3) where the expectation is taken over all i.i.d. samples generated from the source and target distributions.
Moreover, we define the optimal coefficients (α∗ 0, α∗ 1) ≜ arg min (α0,α1)∈A1
L(α0,α1) test (4) as our transferability measure, which effectively quantifies the contributions of the source and target tasks in obtaining the optimal performance.
Then, we have the following characterization.
Theorem 2. The testing loss as defined in (3) is
L(α0,α1) test
= α2 1χ2 (cid:16)
XY , P (1)
P (0)
XY (cid:17)
+
α2 0 n0
V (0) +
α2 1 n1
V (1), (5) 3The assumption on positive entries is without loss of generality, since in practice such joint distributions are typically modeled by some positive parameterized families, e.g., the softmax function. 4We shall emphasize that such combination forms are naturally led by the transfer learning model proposed by
[4, Section 1] from optimizing a convex combination of Log-Loss, where we refer to Section A of supplementary material for a detailed discussion. 5Note that when some (x, y) pair is missing in training samples, we have Q(α0,α1) (x, y) = 0 while
XY
P (0)
XY (x, y) > 0, which would lead to an infinite logarithm risk. 3
and the transferability measures as defined in (4) are 1
V (0) n0
XY ) + 1 n0 where, for each i = 0, 1, V (i) is defined as
XY , P (1)
χ2(P (0) 1 =
α∗
V (0) + 1 n1
, and α∗ 0 = 1
α∗ 1,
−
V (1)
V (i) ≜ (cid:88) x∈X,y∈Y
P (i)
XY (x, y) (cid:16) 1 (cid:17)
P (i)
XY (x, y)
−
P (0)
XY (x, y)
. (6) (7)
X
Y
|
||
Y
| −
From (6), and the fact that V (0) = 1, the transferability is determined by three key factors: (i) the similarity between source and target tasks, measured by the χ2-divergence χ2(P (0)
XY ); (ii) the sample sizes n0 and n1 for source and target tasks; and (iii) the model complexity, characterized
X by the number of model parameter (
|
XY , P (1) 1) in V (0).6
Current transfer learning algorithm designs often focus on the similarities between source and target tasks, while the sample sizes and model complexity are often not sufficiently addressed. In Theorem 2, we show that the transferability is in fact proportional to the number of model parameters, and is inversely proportional to the number of samples in source tasks and the similarity between source and target tasks. Therefore, for a source task with a complex model or few training samples, even though it is similar to the target task, the knowledge transferable from this source task can still be very limited. Such insight was not well captured in many existing transfer learning algorithms, and our result essentially provides the optimal characterization of the task transferability adjusted by the sample complexity in transfer learning.
| −
||
The established transferability measure is also related to the optimal bias-variance trade-off [9] of this transfer learning problem. Indeed, note that the bias-variance trade-off in testing loss (5) is tuned by α0 and α1, as
L(α0,α1) test 1χ2 (cid:16)
= α2 (cid:124)
P (0)
XY , P (1) (cid:123)(cid:122) bias term
XY
+ (cid:17) (cid:125)
α2 0 n0 (cid:124)
V (0) +
V (1)
, (8)
α2 1 n1 (cid:123)(cid:122) variance term(s) (cid:125) where the bias term does not decay with the sample sizes n0, n1, while the variance terms vanish with sufficient samples. Then, the transferability measure corresponds to the coefficients α∗ 1 that achieve the optimal bias-variance trade-off, such that the testing loss is minimized. 0, α∗ 2.2 Multi-source Transfer Learning
Theorem 2 can be readily generalized to multi-source transfer learning problems. Specifically, suppose that there are k source tasks, referred to as task i, for i = 1, . . . , k, and a target task, referred
ℓ=1, and ˆP (i)
ℓ , y(i) to as task 0. Similarly, for each task i = 0, . . . , k, we use P (i) ni
XY to denote
XY ,
ℓ )
{
} the underlying distribution, ni i.i.d. samples generated from P (i)
XY , and the corresponding empirical distribution as defined in (1), respectively. (x(i)
Similar to (2), we consider the convex combination of the models learned from different tasks
Q(α)
XY
≜ k (cid:88) i=0
αi ˆP (i)
XY , α
Ak.
∈ (9)
Then, we define the testing loss L(α) (4)]: test and the corresponding transferability measure α∗, as [cf. (3),
L(α) test
≜ E (cid:104)
χ2 (cid:16)
XY , Q(α)
P (0)
XY (cid:17)(cid:105) and α∗ ≜ arg min
α∈Ak
L(α) test. (10)
Similar to Theorem 2, we have the following result for multi-source transfer learning. 6When evaluating V (1), this quantity is related to the source distribution. From this perspective, the model complexity reflects how hard the task is. 4
Figure 1: A pre-trained neural network for classification can be divided into (a) a feature extractor
Rd, and (b) a classifier with the weights g.
, fd(x)]T which generates feature f (x) = [f1(x),
· · ·
With f fixed, our framework optimizes the weights g in the topmost layer for each task, to obtain the corresponding parameterized representation.
∈
Theorem 3. For the model (9), the testing loss under the target task is (cid:32)
L(α) test = χ2
P (0)
XY , (cid:33)
αiP (i)
XY
+ k (cid:88) i=0 k (cid:88) i=0
α2 i ni
V (i), (11) where V (i)’s are as defined in (7), for all i.
From Theorem 3, the transferability measure α∗ as defined in (10) can be computed by solving a non-negative quadratic programming problem [10]. Similar to the discussions in Section 2.1, such transferability measure quantifies the knowledge transferable from different source tasks to the target task with the sample complexity being considered. 3 Parametric Models and Transfer Learning Algorithm 3.1 Transferability Measure with Pre-trained Neural Network
This section extends the analyses in the discrete data domain to continuous data in practical problems.
In such cases, the previously adopted learning model (1) has infinite parameters due to the infinite
, and thus can not be effectively represented. In order to apply the previous analyzing cardinality
| framework, we first propose a parameterized representation for modeling features of the continuous data by exploiting a pre-trained model.
X
|
As shown in Figure 1, a pre-trained network can be divided into two parts: (a) the previous layers for
, fd(x)]T from the data variable x, and (b) the extracting d-dimensional features f (x) = [f1(x),
, gd(y)]T indexed by label y. topmost layer for linear classification, with weights g(y) = [g1(y),
When the feature f (x) is given and fixed, the models learned from different tasks can be effectively
Y represented by a finite collection of parameters, i.e., g(1), . . . , g(
|
· · ·
).
· · ·
|
In particular, our framework considers the discriminative model in the factorization form
˜P (f ,g)
Y |X (y x) ≜ P (0)
|
Y (y) (cid:0)1 + f T(x)g(y)(cid:1) , (12) which is similar to the ones introduced in factorization machines [11] and natural language processing applications [12]. Then, for each task i = 0, . . . , k, we learn corresponding weights ˆgi, such that the learned model ˜P (f ,ˆgi)
Y |X fits the training samples7. The weight ˆgi can be formally defined as
ˆgi ≜ arg min g (cid:16)
χ2
RXY
XY , P (0)
ˆP (i)
X
˜P (f ,g)
Y |X (cid:17)
, (13) 7The approach of retraining (fine-tuning) the topmost layer is sometimes referred to as the retrain-head method [13], which has also been widely adopted in transfer learning applications. 5
XY and the joint distribution8 P (0) in fitting different tasks. where the fitness is measured as the referenced χ2-distance [cf. Definition 1] between the empirical
˜P (f ,g) distribution ˆP (i)
Y |X . For convenience, we adopt a unified reference RXY ≜ P (0)
˜P (f ,ˆgi)
Y |X plays the role in the continuous case corresponding to ˆP (i)
From (13), P (0)
X case. This allows us to apply previous analyses and focus on the discriminative model ˜P (f ,ˆgi)
Analogous to (9), we consider the convex combination of these discriminative models
XY in the discrete
Y |X ’s.
X P (0)
X
Y
Q(α)
Y |X
≜ k (cid:88) i=0
αi ˜P (f ,ˆgi)
Y |X = ˜P (f ,ˆg)
Y |X (14) with ˆg ≜ (cid:80)k
[cf. (10)] i=0 αi ˆgi. Then, we define the testing loss and corresponding transferability measure as
L(α) test
≜ E (cid:104)
χ2
RXY (cid:16)
XY , P (0)
P (0)
X Q(α)
Y |X (cid:17)(cid:105) and α∗ ≜ arg min
α∈Ak
L(α) test, (15) for which we have the following characterization.
Theorem 4. The testing loss (15) associated with the model (14) is
L(α) test = χ2
RXY (cid:32)
P (0)
X
˜P (f ,g0)
Y |X , k (cid:88) i=0 (cid:33)
αiP (0)
X
˜P (f ,gi)
Y |X
+ k (cid:88) i=0
α2 i ni
˜V (i) + χ2
RXY (cid:16)
XY , P (0)
P (0)
X
˜P (f ,g0)
Y |X (cid:17)
, (16) where gi ≜ arg ming χ2 characterized in the supplementary material [cf. (32)].
RXY (P (i)
XY , P (0)
X
˜P (f ,g)
Y |X ), and where ˜V (i) is a constant independent of α
˜P (f ,gi)
Moreover, note that from the definition of gi, the joint distribution P (0)
Y |X can be interpreted
X
Rd(cid:111)
˜P (f ,g) as a projection of P (i)
Y |X : g : Y
, with referenced
XY onto the distribution family
χ2-distance used as the distance measure. Therefore, the terms of (16) share similar interpretations as their counterparts in Theorem 2, with the distances measured in the projected space. Again, α∗ can be efficiently computed by solving a non-negative quadratic programming problem.
P (0)
X
→ (cid:110) 3.2 Multi-source Transfer Learning Algorithm
With our theoretic analyses in Theorem 4, we develop a knowledge transfer algorithm for multi-source transfer learning. Different from the previous analyses where f is fixed, our algorithm jointly optimizes the extracted feature f , the weights g, together with the combining coefficients α to obtain better performance.
To begin, for given f , g, and α, we introduce the loss function
L(α,f ,g) ≜ k (cid:88) i=0
αiχ2
RXY (cid:16)
XY , P (0)
ˆP (i)
X
˜P (f ,g)
Y |X (cid:17)
. (17)
The following result illustrates that, the ˆg can be computed via directly minimizing this loss, without evaluating each ˆgi individually.
Proposition 5. The ˆg as defined in (14) satisfies
ˆg = arg min g′
L(α,f ,g′). 8The joint distribution P (0)
X
Y |X is defined as (cid:2)P (0)
˜P (f ,g)
X
Note that when the discriminative model ˜P (f ,g) of the target distribution P (0)
XY .
Y |X is fixed, P (0)
X 6
˜P (f ,g)
Y |X (cid:3)(x, y) ≜ P (0)
X (x) ˜P (f ,g)
Y |X (y|x), for all (x, y).
Y |X corresponds to the optimal approximation
˜P (f ,g)
Table 1: Test accuracies (%) on the target task, with the network trained on samples from single source. All reported accuracies are averaged over 5 repeated experiments.
Source Task 1 2 3 4
Acc. on the target task 66.5 59.7 56.2 77.1
Table 2: Test accuracies (%) on the target task, compared with the combining coefficients α deter-mined by 20 rounds of random searches (RS).
Target Sample Size
Acc. with only target samples
Average acc. by 20 RS
Highest acc. by 20 RS
Acc. by Algorithm 1 6 70.9 67.8 74.4 78.9 20 74.4 73.9 78.0 81.2 100 81.5 75.4 80.8 83.7
Then, with training samples from different tasks, our algorithm alternates between two different kinds of optimizations: (i) the optimization of α for given (f , g) to minimize the testing loss L(α) test as defined in (16), via solving a non-negative quadratic programming problem; and (ii) the optimization of (f , g) for given α to minimize the loss L(α,f ,g) as defined in (17) via training the neural network.
We summarize the procedures as Algorithm 1.
Specifically, it can be shown that both the testing loss L(α) test and the loss L(α,f ,g) can be represented by some expectations of features f and g. In computing these losses, these expectations are approximated by corresponding empirical means, with details provided in the supplementary material.
Algorithm 1 Multi-Source Knowledge Transfer Algorithm (x(i) l
{
, y(i) ni l=1 (i = 0, l )
}
· · ·
, k) (f ∗, g∗) 1: Input: target and source data samples 2: Randomly initialize α∗ 3: repeat 4: 5: α∗
← 6: until α∗ converges 7: (f ∗, g∗) 8: return f ∗, g∗ arg minf ,g L(α∗,f ,g) arg minf ,g L(α∗,f ,g) arg minα∈Ak L(α)
←
← test
With the f ∗ and g∗ computed by the algorithm, for a newly observed target sample x, the predicted label ˆy is given by the MAP (maximum a posterior) decision rule
ˆy(x) = arg max y∈Y
˜P (f ∗, g∗)
Y |X (y
| x) = arg max y∈Y
Y (y) (cid:0)1 + f ∗T(x)g∗(y)(cid:1) .
P (0) (18) 4 Experiments
To validate the effectiveness of our algorithms in multi-source learning and few-shot transfer learning scenarios, we conduct a series of experiments on common datasets for image recognition, including
CIFAR-10 [14], Office-31 and Office-Caltech [15]. In all experiments, the g in the classifier is simply generated by an embedding layer. 4.1 Multi-source Transfer Learning
We conduct multi-source transfer learning experiments on CIFAR-10, which contains 50 000 training images and 10 000 testing images in 10 classes. To begin, we construct the source tasks and target task by dividing the original CIFAR-10 dataset into five disjoint subdatasets, each containing two classes of the original data, which corresponds to a binary classification task. Then, we choose one as our target task (task 0), and use the other four as source tasks for transferring knowledge, referred to as task 1, 2, 3, 4.
Moreover, for each source task, 2000 images are used for training, with 1000 images per binary class, and we set target sample size n0 to n0 = 6, 20, 100, respectively. Throughout this experiment, 7
Table 3: Test accuracies for target tasks under different transfer settings (source target) on Office-31
→
Method
A→D A→W D→W D→A W→A W→D
SDT [20]
DAMA [21]
FADA [22]
UDDA [23]
Ours 86.1 86.3 88.2 89.0 90.0 82.7 84.5 88.1 88.2 87.3 95.7 95.5 96.4 96.4 96.5 66.2 66.5 68.1 71.8 72.4 65.0 65.7 71.1 72.1 72.1 97.6 97.5 97.5 97.6 97.2
Table 4: Test accuracies for target tasks under different transfer settings on Office-Caltech
Method
A→C W→C D→C C→A
C→W C→D
GFK [15]
TLDA [24]
DTML [25]
CPNN [26]
Ours 68.4 76.1 72.0 78.5 80.3 68.4 71.0 71.6 73.5 72.9 64.5 65.4 67.1 68.0 72.2 83.8 84.2 86.0 86.3 88.4 78.7 85.2 85.0 86.2 85.9 74.6 78.9 79.6 80.1 83.5 the feature f is of dimensionality d = 10, generated by GoogLeNet [16], followed by two fully connected layers for further dimension reduction.
Unlike common transfer learning settings where the labels for the source and target tasks are closely related, here the binary labels for these 5 sub-datasets are in general irrelevant. Therefore, we first establish the correspondences between labels as follows. For each given source task, we first train the network on its training samples, while the test accuracy is evaluated on the test samples from the target task. Then, we flip the original binary label for this source, if the test accuracy is less than 50%.
The resulting test accuracies on the target set are summarized in Table 1.
In our implementation of Algorithm 1, we use the CVXPY [17, 18] package for solving the non-negative quadratic programming in line 5. In addition, the alternating iteration is stopped when the element-wise differences for α∗ computed in two successive iterations are at most 0.05.
Then, the test accuracies of our algorithm on the target set are shown in Table 2, where we have compared our performance with random search (RS) strategy. Specifically, in the RS strategy, we generate the coefficients α from the log-uniform distribution [19] in [0.001, 1], for 20 rounds. The results indicate that our approach outperforms the random search method. Also, the difference in Table 2 and Table 1 also shows the performance gain of multi-source transfer learning over single-source. 4.2 Few-shot Transfer Learning
To validate the effectiveness of our algorithm for few-shot learning tasks, we conduct experiments on
Caltech-31 and Office-Caltech datasets. These datasets provide typical transfer learning tasks with few available training samples, where the influence of sample complexity is shown. 4.2.1 Caltech-31
Caltech-31 dataset contains images of 31 categories, which come from 3 sub-datasets: Amazon (2817 images), Dslr (498 images), and Webcam (795 images). Then, different transfer settings among these sub-datasets are denoted by the “source
A, and W
D. We adopt the few-shot transfer learning setting in [20], illustrated as follows. Specifically, 3 target samples per category are used for training, and the training sample size (per category) for source task is set to 20 or 8, depending on whether the source task is Amazon or not. Moreover, we also adopt five train-test splits introduced in [20]. target”, as: A
A, W
W, D
W, D
D, A
→
→
→
→
→
→
→
In our experiment, the feature f is a 64-dimensional vector, extracted by a VGG-16 [27] network pre-trained on the ImageNet, succeeded by two fully connected layers for dimension reduction.
Table 3 summarizes test accuracies for target tasks under different transfer settings, where all reported accuracies are averaged over five train-test splits. The results indicate that our algorithm generally outperforms existing few-shot transfer learning methods. 8
Figure 2: The combining coefficient α1 of the task A of Dslr during iterations.
→
D and test accuracies under testing samples
In addition, we also investigate the convergence of the coefficient α∗ in Algorithm 1. As an example, for the A
D task, the changes of the coefficient α1 (the coefficient of the source loss) and the accuracy on the Dslr test dataset during iterations are shown in Figure 2. From the figure, the value of α1 converges under our stopping criterion, where the optimal testing accuracy is obtained.
→ 4.2.2 Office-Caltech
Office-Caltech dataset is composed of 10 common categories in Office-31 and Caltech-256, divided as four sub-datasets: Amazon (958 images), Caltech (1123 images), Webcam (295 images), and Dslr (157 images). We focus on the 6 transfer settings depending on C, i.e., A
A,
C
C, D
D, which have few common categories between source and target tasks.
W, and C
C, W
C, C
→
→
→
→
→
→
In addition, we follow the setting introduced in [15] for train-test split. The feature f is of dimen-sionality d = 10, based on the DeCAF feature [28, 29] with 2 fully connected layers for dimension reduction.
Table 4 shows the performance for our algorithm, in comparison with several semi-supervised and few-shot domain adaptation algorithms. It is worth mentioning that, though our approach does not use the unlabeled data samples in training, it provides competitive performance as the semi-supervised algorithm CPNN [26], and can be better on specific tasks. 5