Abstract
The representations learned by large-scale NLP models such as BERT have been widely used in various tasks. However, the increasing model size of the pre-trained models also brings efﬁciency challenges, including inference speed and model size when deploying models on mobile devices. Speciﬁcally, most operations in BERT consist of matrix multiplications. These matrices are not low-rank and thus canonical matrix decompositions do not lead to efﬁcient approximations. In this paper, we observe that the learned representation of each layer lies in a low-dimensional space. Based on this observation, we propose DRONE (data-aware low-rank compression), a provably optimal low-rank decomposition of weight matrices, which has a simple closed form solution that can be efﬁciently computed.
DRONE can be applied to both fully-connected and self-attention layers appearing in the BERT model. In addition to compressing standard models, our method can also be used on distilled BERT models to further improve the compression rate. Experimental results show that DRONE is able to improve both model size and inference speed with limited loss in accuracy. Speciﬁcally, DRONE alone achieves 1.92x speedup on the MRPC task with only 1.5% loss in accuracy, and when DRONE is combined with distillation, it further achieves over 12.3x speedup on various natural language inference tasks. 1

Introduction
The representations learned by large-scale Natural Language Processing (NLP) models such as BERT and its variations have been widely used in various tasks [8, 2, 25, 3, 24]. The successes of these large
NLP models rely on the usage of large corpus and big models. Indeed, researchers have reported better results with models that have more parameters [31] and number of layers [1]. The increasing model size of the pre-trained models inhibits public users from training a model from scratch, and it also brings forth efﬁciency challenges, including inference speed and model size when deploying models on mobile devices.
To deal with efﬁciency issues, most existing work resorts to adjusting the model structure or distilla-tion. For instance, [17] used locality-sensitive hashing to accelerate dot-product attention, [18] used repeating model parameters to reduce the size and [44] applied a pre-deﬁned attention pattern to save computation. A large body of prior work focused on variants of distillation has also been explored 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of the BERT-base computational model. |V | (at bottom of the Parameter Size column) denotes the number of tokens in the model. #Classes (at top of the Parmeter Size column) denotes the number of classes in the down-stream classiﬁcation task. Input encoding, Feed-forward 3 and Feed-forward 4 are computed only once and thus do not contribute much to overall time. The inference time (in milliseconds) listed here is based on the inference time measured on a CPU.
[27, 15, 35, 20, 34, 35, 41, 45, 4]. These methods require a speciﬁc design of model architecture, or a long training stage and thus it is less straightforward to combine these methods with each other.
In this paper, we explore a simpler acceleration method to speed up inference time which can be applied to most existing architectures. As shown in Figure 1, matrix multiplication (feed-forward layer) is a fundamental operation which appears many times in the Transformer [36], the backbone architecture of the BERT model. In fact, the underlying computation of both multi-head attention layers and feed-forward layers is matrix multiplication. Therefore, instead of resorting to the complex architecture redesign approaches, we aim to investigate whether low-rank matrix approximation, a classical and simple model compression approach, can be used to accelerate Transformers. Despite its successful application to CNNs [42, 33, 32], at ﬁrst glance, low-rank compression does not appear to work for BERT since the matrices in both feed-forward layers and attention layers are not low rank (see Figure 2). Therefore, even the optimal low-rank approximation (e.g., by SVD) will lead to very large reconstruction error. This is probably why low-rank approximation has not been successfully used in BERT compression.
In this paper, we propose a novel low-rank approximation algorithm to compress the weight matrices even though they are not low-rank. The main idea is to exploit the data distribution. In NLP applications, the latent features (features fed into each matrix mulitplication layer) usually indicate some information extracted from natural sentences, and they often lie in a subspace with a low intrinsic dimension [5, 32, 22]. Therefore, in most of the matrix-vector products, even though the weight matrices are not low-rank, the input vectors lie in a low-dimensional subspace, allowing dimension reduction with minimal degraded performance. We mathematically formulate this generalized low-rank approximation problem which includes the data distribution term and provide a closed-form solution for the optimal rank-k decomposition of the weight matrices. By leveraging the data distribution idea, we propose DRONE (data-aware low-rank compression). Our decomposition signiﬁcantly outperforms the SVD under the same rank constraint, and can successfully accelerate the BERT model without sacriﬁcing too much test performance. In addition to compressing standard models, DRONE can also be used on distilled BERT models to further improve the compression rate. For example, DRONE alone achieves 1.92x speedup on the MRPC task with only 1.5% loss in accuracy, and when combined with distillation, DRONE achieves over 12.3x speedup on various natural language inference tasks. 2