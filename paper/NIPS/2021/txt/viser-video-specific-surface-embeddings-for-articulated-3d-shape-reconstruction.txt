Abstract
We introduce ViSER, a method for recovering articulated 3D shapes and dense 3D trajectories from monocular videos. Previous work on high-quality reconstruction of dynamic 3D shapes typically relies on multiple synchronized cameras, strong category-speciﬁc priors, or 2D keypoint supervision. We show that none of these are required if one can reliably estimate long-range correspondences in a video, making use of only 2D object masks and two-frame optical ﬂow as inputs. ViSER infers correspondences by matching 2D pixels to a canonical, deformable 3D mesh via video-speciﬁc surface embeddings that capture the view-independent appearance features of each surface point. These embeddings behave as a continuous set of keypoint descriptors deﬁned over the mesh surface, which can be used to establish dense long-range correspondences across pixels. The surface embeddings are implemented as coordinate-based MLPs that are ﬁt to each video via self-supervised losses. Experimental results show that ViSER compares favorably against prior work on challenging videos of humans with loose clothing and unusual poses as well as animal videos from DAVIS and YTVOS. 1

Introduction
Reconstructing the world from a sequence of monocular frames is a long-standing task in computer vision. While there has been tremendous progress in reconstructing rigid scenes (via SfM and
SLAM [7, 39, 43], or recent techniques based on neural rendering [28]), reconstructing dynamic scenes with articulated objects remains elusive. For example, given a monocular video, it is still challenging to reconstruct an everyday scene of a moving person with loose clothing. In this work, we tackle the problem of estimating the deforming mesh of articulated objects given a segmented monocular video of that object. Our method avoids the use of any mesh templates or category-speciﬁc priors and generalizes to unknown deformable articulated objects in the wild.
Nonrigid shape recovery is highly under-constrained due to fundamental ambiguities between shape, appearance, and time-varying deformation. Current approaches for addressing these challenges fall into two camps: better data “likelihoods” or better “priors”. The ﬁrst camp extracts richer sensor data, via multi-camera studio setups [15] or depth sensors [30], but requires substantial efforts to work in the wild. The second camp makes use of category-level priors over object shapes [18, 20] and is particularly effective for human reconstruction. However, building such models requires considerable ofﬂine efforts in the form of registered 3D scans [26] or manual keypoint annotations [12], both of which are difﬁcult to scale to arbitrary object categories.
∗Work done at Google. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Given a long video (or multiple short videos), ViSER jointly learns articulated 3D shapes (represented as a mesh with vertices ¯V and faces F) and joint pixel-surface embeddings (including a surface embedding
FS and a pixel embedding FI) that establishes dense long-range pixel correspondences over time. As a result,
ViSER produces accurate shapes, long term trajectories and meaningful part segmentation.
In this work, we use a practical but less explored variant of the data-likelihood camp: we use multiple frames of a video rather than multiple cameras or depth sensors. This considerably complicates analysis for dynamic, non-rigid scenes. Nonrigid structure-from-motion (NRSfM) [4, 38] attempts to constrain the problem by relying on motion correspondences such as 2D point tracks. While 2D correspondences over short time scales (i.e., optical ﬂow) are relatively robust to extract, correspon-dences over long time scales are notoriously difﬁcult to estimate because of appearance variations arising from viewpoint changes, occlusion and fast motion. In practice, this limits the applicability of
NRSfM methods to controlled lab sequences.
We propose ViSER (Video-Speciﬁc Surface Embeddings for Reconstruction), which establishes long-range correspondence and reconstructs articulated 3D shapes from a monocular video. Fig. 1 shows a sample outdoor video and the corresponding ViSER results. The key insight behind ViSER is to force long-range video pixel correspondences to be consistent with an underlying canonical 3D mesh through the use of video-speciﬁc embeddings that capture the pixel appearance of each surface point. These embeddings behave as a continuous set of keypoint descriptors deﬁned over the surface mesh, learned with coordinate-based MLPs that are ﬁt to each video via self-supervised losses. ViSER simultaneously optimizes the image CNN, surface MLP, and 3D shape so as to ﬁt the observed video frames. It reconstructs state-of-the-art articulated 3D shape and 3D trajectories without using category-speciﬁc priors, making it easily scalable to diverse videos including humans with challenging clothing and poses as well as animals. Lastly, we demonstrate that ViSER recovers meaningful part segmentation and blend skinning weights from videos, which typically require considerable manual effort from 3D artists. 2