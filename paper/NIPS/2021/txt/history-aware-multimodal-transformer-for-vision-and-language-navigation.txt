Abstract
Vision-and-language navigation (VLN) aims to build autonomous visual agents that follow instructions and navigate in real scenes. To remember previously visited locations and actions taken, most approaches to VLN implement memory using recurrent states. Instead, we introduce a History Aware Multimodal Transformer (HAMT) to incorporate a long-horizon history into multimodal decision making.
HAMT efﬁciently encodes all the past panoramic observations via a hierarchical vision transformer (ViT), which ﬁrst encodes individual images with ViT, then models spatial relation between images in a panoramic observation and ﬁnally takes into account temporal relation between panoramas in the history. It, then, jointly combines text, history and current observation to predict the next action. We
ﬁrst train HAMT end-to-end using several proxy tasks including single step action prediction and spatial relation prediction, and then use reinforcement learning to further improve the navigation policy. HAMT achieves new state of the art on a broad range of VLN tasks, including VLN with ﬁne-grained instructions (R2R,
RxR), high-level instructions (R2R-Last, REVERIE), dialogs (CVDN) as well as long-horizon VLN (R4R, R2R-Back). We demonstrate HAMT to be particularly effective for navigation tasks with longer trajectories. 1

Introduction
Vision-and-language navigation (VLN) has recently received growing attention [1, 2, 3, 4, 5]. VLN requires an agent to understand natural language instructions, perceive the visual world, and perform navigation actions to arrive at a target location. A number of datasets have been proposed to support various VLN tasks such as indoor and outdoor navigation with ﬁne-grained instructions [2, 6, 7], language-driven remote object ﬁnding [8] and navigation in dialogs [9].
VLN agents are faced with several challenges. First, as opposed to static vision-text grounding [10], the agent continuously receives new visual observations and should align them with instructions.
Most of existing works adopt recurrent neural networks (RNNs) [6, 11, 12, 13, 14, 15, 16] to encode historical observations and actions within a ﬁxed-size state vector to predict the next action. Such condensed states might be sub-optimal for capturing essential information in extended trajectories [17].
For instance, “bring the spoon to me” requires the agent to remember its start location after navigating to the “spoon”, while early memories are prone to fade in the recurrent state. Few endeavors [18, 19] construct external map-like memories for received observations. Nevertheless, these approaches still rely on RNNs to track the navigation state. As the history plays an important role in environment understanding and instruction grounding, we propose to explicitly encode the history as a sequence of previous actions and observations instead of using recurrent states. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The architecture of History Aware Multimodal Tranformer (HAMT). HAMT jointly encodes textual instruction, full history of previous observations and actions, and current observation to predict the next action.
Another VLN challenge concerns the generalizations of agents to new environments that have not been observed during training [4]. One direction is to learn more generic text-image representations.
The PRESS model [20] improves language representation with a pretrained BERT encoder [21], and
PREVALENT [22] uses pairs of instruction and single-step observations to pretrain a multimodal transformer. Though achieved promising results, these works do not optimize visual representation for the target navigation task. Moreover, lack of history in training [22] makes it hard to learn cross-modal alignment and increases the risk of overﬁtting to training environments. Another direction towards better generalization is to overcome exposure bias [23] due to discrepancy between training and inference. Different methods have been adopted for VLN including DAgger [6, 24] and scheduled sampling [20, 25]. Reinforcement Learning (RL) [12, 26] is one of the most effective approach among them, but it is considered unstable to directly train large-scale transformers via RL [27].
To address the above challenges, we propose the History Aware Multimodal Transformer (HAMT), a fully transformer-based architecture for multimodal decision making in VLN tasks. As illustrated in Figure 1, HAMT consists of unimodal transformers for text, history and observation encoding, and a cross-modal transformer to capture long-range dependencies of the history sequence, current observation and instruction. Since our history contains a sequence of all previous observations, its encoding is computationally expensive. To resolve complexity issues, we propose a hierarchical vision transformer as shown in Figure 2, which progressively learns representations for a single view, spatial relationships among views within a panorama and, ﬁnally, the temporal dynamics across panoramas of the history. In order to learn better visual representations, we propose auxiliary proxy tasks for end-to-end training. Such tasks include single-step action prediction based on imitation learning, self-supervised spatial relationship reasoning, masked language and image predictions and instruction-trajectory matching. We empirically show that our training facilitates the subsequent ﬁne-tuning of our model with RL [28]. We carry out extensive experiments on various VLN tasks, including VLN with ﬁne-grained instructions (R2R [6] and RxR [7]), high-level instructions (REVERIE [8] and our proposed R2R-Last), dialogs [9] as well as long-horizon VLN (R4R [3] and our proposed R2R-Back which requires the agent to return back after arriving at the target location). HAMT outperforms state of the art on both seen and unseen environments in all the tasks.
We summarize our contributions as follows: (1) We introduce HAMT to efﬁciently model long-horizon history of observed panoramas and actions via hierarchical vision transformer; (2) We train
HAMT with auxiliary proxy tasks in an end-to-end fashion and use RL to improve the navigation policy; (3) We validate our method and outperform state of the art in a diverse range of VLN tasks, while demonstrating larger gains for long-horizon navigation. 2