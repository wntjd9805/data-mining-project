Abstract
Motivated by the recent successes of neural networks that have the ability to
ﬁt the data perfectly and generalize well, we study the noiseless model in the fundamental least-squares setup. We assume that an optimum predictor perfectly
ﬁts the inputs and outputs (cid:104)θ∗, φ(X)(cid:105) = Y , where φ(X) stands for a possibly inﬁnite dimensional non-linear feature map. To solve this problem, we consider the estimator given by the last iterate of stochastic gradient descent (SGD) with constant step-size. In this context, our contribution is twofold: (i) from a (stochastic) optimization perspective, we exhibit an archetypal problem where we can show explicitly the convergence of SGD ﬁnal iterate for a non-strongly convex problem with constant step-size whereas usual results use some form of average and (ii) from a statistical perspective, we give explicit non-asymptotic convergence rates in the over-parameterized setting and leverage a ﬁne-grained parameterization of the problem to exhibit polynomial rates that can be faster than O(1/T ). The link with reproducing kernel Hilbert spaces is established. 1

Introduction
As soon as large-scale statistics and optimization need to work together, stochastic gradient descent (SGD) is the core algorithm everybody tries to build upon [7]. Its versatility, practicability and adaptability make it the workhorse of almost every supervised machine learning problem. Yet, its outstanding efﬁciency remains mysterious, or at least surprising on certain aspects. Furthermore, the recent successes of deep neural networks (DNN) brought a new paradigm to the classical supervised learning setting with the ability to ﬁt the data perfectly and to generalize well [5]. Following this idea, the old statistical modelling where the model suffers from problem-dependent noise has to be revisited: there is a need of analyzing stochastic algorithms in this new light [18]. Whether we call it over-parameterization to put emphasis on the large number of neurons needed in DNNs, interpolation as in approximation theory or noiseless model to stress the absence of noise in this statistical model, all these terminologies refer to the same idea. This regime brings with it new insights that reﬂect better the current machine learning setup.
Hence, the main question: how would SGD proﬁt from this noiseless model? At ﬁrst glance, the story seems clear: the old problem of variance at optimum making the SGD iterates oscillate asymptotically now disappears. Thus, should also disappear techniques that prevent from this, namely, averaging and decaying step sizes: one should be able to study the convergence of the SGD ﬁnal iterate with constant step-size. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, the study of the last iterate of SGD has always caused some technical problems preventing from a clear theory in this case [26]. Indeed, the convergence of the ﬁnal iterate is much more difﬁcult to prove than the convergence of the average of the iterates [27, 13, 15]. This counter-intuitive difﬁculty can be explained by the fact that the interactions coming from the sampling noise of
SGD prevent the loss of the ﬁnal iterates from decreasing. Therefore standard Lyapounov strategies often failed in such a setting. Besides, even if averaged SGD has shown theoretically some good convergence properties, the ﬁnal iterate is commonly used in practice. Finally, averaging techniques always suffer from saturation coming from the slow forgetting of initial conditions.
To tackle these questions, we consider the simplest setting of the linear regression over features φ(X) in a Hilbert space H. In this context, the setup corresponds to the existence of linear relationship between the output and the input: Y = (cid:104)θ∗, φ(X)(cid:105). Note that this setting is rich as the features can be a non-linear transformation of the inputs, as it is commonly the case when they are deﬁned through a positive-deﬁnite kernel K(X, X (cid:48)) [25, 28]. Note also that our analysis will, unless stated explicitly, be conducted in a non-parametric and dimensionless fashion, enabling the features to come from a inﬁnite dimensional reproducing kernel Hilbert space. In this perspective, the problem we are considering is not strongly convex.
Main contributions. The aim of the present article is to answer at once the two following problems: (i) from a (stochastic) optimization perspective, the goal is to exhibit an archetypal problem where we can show explicitly the convergence of SGD ﬁnal iterate for a non-strongly convex problem with constant step-size and (ii) from a statistical/machine learning perspective, the aim is to push deeper the study of the over-parameterized setting for the non-parametric least-squares problem. Contrary to the noisy setting, where (almost) everything is known, the noiseless model suffers from a certain lack of understanding. More precisely, our contributions are the following:
• We show that the ﬁnal iterate of constant step-size SGD achieves a convergence rate of
O(ln(T )/T ) under minimal assumptions. With a slightly stricter hypothesis we improve this convergence rate to O(1/T ).
• Going further, we assume the usual non-parametric capacity and source conditions respectively on the spectrum of the covariance and the optimum, and derive bounds for this ﬁne-grained model.
In this setup, we show the SGD fast rates of O(1/T 1+α).
• We derive an explicit recursion on the eigenspaces of the covariance matrix that is of its own interest. Indeed, it is the cornerstone of our analysis and could be very useful in the future to understand important properties of SGD.
From a technical standpoint, going from the average to the last iterate is far from anodyne for constant step-size SGD even in the interpolation regime. Indeed, in a similar setting, D.Aldous considered it an open problem [1, Sec. 3.3, Open problem (i)]. Prior to our result, how to directly deal with the ﬂuctuations induced by the stochasticity of the gradients without any variance reduction was not known. Our work presents a direct Lyapunov technique that handles them without any explicit variance reduction methods like averaging or step-size decay. 1.1