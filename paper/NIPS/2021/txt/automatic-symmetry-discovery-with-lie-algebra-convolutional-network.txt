Abstract
Existing equivariant neural networks require prior knowledge of the symmetry group and discretization for continuous groups. We propose to work with Lie algebras (inﬁnitesimal generators) instead of Lie groups. Our model, the Lie algebra convolutional network (L-conv) can automatically discover symmetries and does not require discretization of the group. We show that L-conv can serve as a building block to construct any group equivariant feedforward architecture. Both CNNs and Graph Convolutional Networks can be expressed as L-conv with appropriate groups. We discover direct connections between L-conv and physics: (1) group invariant loss generalizes ﬁeld theory (2) Euler-Lagrange equation measures the robustness, and (3) equivariance leads to conservation laws and Noether current.
These connections open up new avenues for designing more general equivariant networks and applying them to important problems in physical sciences.1 1

Introduction
Incorporating symmetries into a deep learning architecture can reduce sample complexity, improve generalization, while signiﬁcantly decreasing the number of model parameters (Cohen et al., 2019b;
Cohen & Welling, 2016b; Ravanbakhsh et al., 2017; Ravanbakhsh, 2020; Wang et al., 2020). For instance, Convolutional Neural Networks (CNN) (LeCun et al., 1989, 1998) implement translation symmetry through weight sharing. General principles for constructing symmetry-aware group equivariant neural networks were introduced in Cohen & Welling (2016b), Kondor & Trivedi (2018), and Cohen et al. (2019b).
However, most work on equivariant networks requires knowing the symmetry group a priori. A different equivariant model needs to be re-designed for each symmetry group. In practice, we may not have a good inductive bias and such knowledge of the symmetries may not be available. Constructing and selecting the equivariant network with the appropriate symmetry group becomes quite tedious.
Furthermore, many existing works are limited to ﬁnite groups such as permutations Hartford et al. (2018); Ravanbakhsh et al. (2017); Zaheer et al. (2017), 90 degree rotations Cohen et al. (2018) or dihedral groups DN and E(2) Weiler & Cesa (2019). 1Code: github.com/nimadehmamy/L-conv-code 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
For a continuous group, existing approaches either discretize the group Weiler et al. (2018a,b);
Cohen & Welling (2016a), or use a truncated sum over irreducible representations (irreps) Weiler &
Cesa (2019); Weiler et al. (2018a) via spherical harmonics in Worrall et al. (2017) or more general
Clebsch-Gordon coefﬁcients Kondor et al. (2018); Bogatskiy et al. (2020). These approaches are prone to approximation error. Recently, Finzi et al. (2020) propose to approximates the integral over the Lie group by Monte Carlo sampling. This approach requires implementing the matrix exponential and obtaining a local neighborhood for each point. Both parametrizing Lie groups for sampling and
ﬁnding irreps are computationally expensive. Finzi et al. (2021) provide a general algorithm for constructing equivariant multi-layer perceptrons (MLP), but require explicit knowledge of the group to encode its irreps, and solving a set of constraints.
We provide a novel framework for designing equivariant neural networks. We leverage the fact that
Lie groups can be constructed from a set of inﬁnitesimal generators, called Lie algebras. A Lie algebra has a ﬁnite basis, assuming the group is ﬁnite-dimensional. Working with the Lie algebra basis allows us to encode an inﬁnite group without discretizing or summing over irreps. Additionally, all Lie algebras have the same general structure and hence can be implemented the same way. We propose Lie Algebra Convolutional Network (L-conv), a novel architecture that can automatically discover symmetries from data. Our main contributions can be summarized as follows:
• We propose the Lie algebra convolutional network (L-conv), a building block for construct-ing group equivariant neural networks.
• We prove that multi-layer L-conv can approximate group convolutional layers, including
CNNs, and ﬁnd graph convolutional networks to be a special case of L-conv.
• We can learn the Lie algebra basis in L-conv, enabling automatic symmetry discovery.
• L-conv also reveals interesting connections between physics and learning: equivariant loss generalizes important Lagrangians in ﬁeld theory; robustness and equivariance can be expressed as Euler-Lagrange equations and Noether currents.
Learning symmetries from data has been studied in limited settings for commutative Lie groups as in
Cohen & Welling (2014), 2D rotations and translations in Rao & Ruderman (1999), Sohl-Dickstein et al. (2010) or permutations (Anselmi et al., 2019). In the non-commutative case, GeoManCEr (Pfau et al., 2020) uses data points related by small transformations to learn non-abelian Lie groups, but it does not introduce an equivariant layer architecture. (Zhou et al., 2020) propose a general method for symmetry discovery. Yet, their weight-sharing scheme and the symmetry generators are very different from ours. Our approach use much fewer parameters and has a direct interpretation using Lie algebras (SI B.3). Benton et al. (2020) propose Augerino to learn a distribution over data augmentations. It also involves Lie algebras, but is restricted to a subgroup of 2D afﬁne transformations and requires matrix logarithm and sampling (SI B.3). In contrast, our approach is simpler and more general. Our approach uses composition of small transformations to achieve large transformations. In this sense bears some resemblance to symnets (Gens & Domingos, 2014), but the rest of the construction is different. 2