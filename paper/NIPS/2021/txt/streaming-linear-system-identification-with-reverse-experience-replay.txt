Abstract
We consider the problem of estimating a linear time-invariant (LTI) dynamical system from a single trajectory via streaming algorithms, which is encountered in several applications including reinforcement learning (RL) and time-series analysis.
While the LTI system estimation problem is well-studied in the offline setting, the practically important streaming/online setting has received little attention. Standard streaming methods like stochastic gradient descent (SGD) are unlikely to work since streaming points can be highly correlated. In this work, we propose a novel streaming algorithm, SGD with Reverse Experience Replay (SGD − RER), that is inspired by the experience replay (ER) technique popular in the RL literature.
SGD − RER divides data into small buffers and runs SGD backwards on the data stored in the individual buffers. We show that this algorithm exactly deconstructs the dependency structure and obtains information theoretically optimal guarantees for both parameter error and prediction error. Thus, we provide the first – to the best of our knowledge – optimal SGD-style algorithm for the classical problem of linear system identification with a first order oracle. Furthermore, SGD − RER can be applied to more general settings like sparse LTI identification with known sparsity pattern, and non-linear dynamical systems. Our work demonstrates that the knowledge of data dependency structure can aid us in designing statistically and computationally efficient algorithms which can “decorrelate” streaming samples. 1

Introduction
In this paper, we study the problem of learning linear-time invariant (LTI) systems, where the goal is to estimate the matrix A∗ ∈ Rd×d from the given samples (X0, . . . , XT ) that obey:
Xτ +1 = A∗Xτ + ητ , Xτ ∈ Rd, ητ i.i.d.∼ µ, (1) where µ is an unbiased noise distribution. The problem is central in control theory and reinforcement learning (RL) literature [1, 2]. It is also equivalent to estimating Vector Autoregressive (VAR) model popular in the time-series analysis literature [3], where it has been used in several applications like finding gene regulatory information network [4].
Despite a long line of classical literature for the problem, most of the existing results focus on the offline setting, where all the samples (X0, . . . , XT ) are available apriori. In this setting, ordinary 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
least squares (OLS) method that estimates A as, ˆA = arg minA
τ =0 ∥Xτ +1 − AXτ ∥2 is known to be nearly optimal [5, 6]. However, such offline solutions do not apply to the streaming setting – where A∗ needs to be estimated online – that has applications in several domains like RL, large-scale forecasting systems, recommendation systems [7, 8]. (cid:80)T −1
In this paper, we study the above mentioned problem of learning LTI systems via first order gradient oracle with streaming data. The goal is to design an estimator that provides accurate estimation while ensuring nearly optimal time complexity and space complexity that is nearly independent of T . Note that due to specific form arising in linear regression, the optimal solution to OLS can be estimated in online fashion using Sherman-Morrison-Woodbury formula. But such a solution is limited and does not apply to practically important settings like generalized non-linear dynamical system or when A∗ is high-dimensional and has special structure like low-rank or sparsity [9, 10].
So, in this work, we focus on designing Stochastic Gradient Descent (SGD) style methods that can work directly with first order gradient oracle, and hence is more widely applicable to the settings mentioned above. In fact, after the first appearance of this manuscript, the algorithm (SGD − RER) and the techniques introduced in this paper were used to obtain near-optimal guarantees for learning certain classes of non-linear dynamical systems [11] as well as in Q-learning tabular MDPs in RL
[12]. We note that prior to [11], even optimal offline algorithms were unknown for such non-linear systems.
SGD is a popular method for general streaming settings, and has been shown to be optimal for problems like streaming linear regression [13]. However, when the data has temporal dependencies, as in the estimation of linear dynamical systems, such a naive implementation of SGD may not perform well as observed in [14, 15]. In fact, for linear system identification, our experiments suggest that SGD suffers from a non-zero bias (Section 6). In order to address temporal dependencies in data, practitioners use a heuristic called experience replay, which maintains a buffer of points, and samples points randomly from the buffer. However, for linear system identification, experience replay does not seem to provide an accurate unbiased estimator for reasonable buffer sizes (see Section 6).
In this work, we propose reverse experience replay for linear system identification. Our method maintains a small buffer of points, but instead of random ordering, we replay the points in a reverse order. We show that this algorithm exactly unravels the temporal correlations to obtain a consistent estimator for A∗. Similar to the standard linear regression problem with i.i.d. samples, we can break the error in two parts: a) bias: that depends on the initial error ∥A0 − A∗∥, b) variance: the steady state error due to noise η. We show that our proposed method, under fairly standard assumptions and with a small buffer size, is able to decrease the bias at fast rate, while the variance error is nearly optimal (see Theorem 1), matching the information theoretic lower bounds [5, Theorem 2.3]. To the best of our knowledge, we provide first non-trivial analysis for a purely streaming
SGD-style algorithm with optimal computation complexity and nearly bounded space complexity that is dependent logarithmically on T . We note here that the idea of reverse experience replay was independently discovered in experimental reinforcement learning by [16] based on reverse replay observed in Hippocampal place cells [17] in Neurobiology. We also refer to [18] for more on this connection.
In addition to the transition matrix estimation error ∥A − A∗∥, we also provide analysis of prediction error, i.e., E[∥AX − A∗X∥2] (see Theorem 2). Here again, we bound the bias and the variance part of the error separately. We further derive new lower bounds for prediction error (see Theorem 4) and show that our algorithm is minimax optimal, under standard assumptions on the model. As mentioned earlier, our method work with general first order oracles, hence applies to more general problems like sparse LTI estimation with known sparsity structure and unlike online OLS methods, SGD − RER has nearly optimal time complexity. Finally, we also provide empirical validation of our method on simulated data, and demonstrate that the proposed method is indeed able to provide error rate similar to the OLS method while methods like SGD and standard experience replay, lead to biased estimates.