Abstract
Molecular representation learning is the first yet vital step in combining deep learn-ing and molecular science. To push the boundaries of molecular representation learning, we present PhysChem, a novel neural architecture that learns molecu-lar representations via fusing physical and chemical information of molecules.
PhysChem is composed of a physicist network (PhysNet) and a chemist network (ChemNet). PhysNet is a neural physical engine that learns molecular conforma-tions through simulating molecular dynamics with parameterized forces; ChemNet implements geometry-aware deep message-passing to learn chemical / biomedical properties of molecules. Two networks specialize in their own tasks and cooperate by providing expertise to each other. By fusing physical and chemical information,
PhysChem achieved state-of-the-art performances on MoleculeNet, a standard molecular machine learning benchmark. The effectiveness of PhysChem was further corroborated on cutting-edge datasets of SARS-CoV-2. 1

Introduction
The intersection between deep learning and molecular science has recently caught the eye of re-searchers in both areas. Remarkable progress was made in applications including molecular property prediction [36, 16], molecular graph generation [37, 12, 28], and virtual screening for drug discovery
[8, 31], yet learning representations for molecules remains the first yet vital step. Molecular repre-sentation learning, or learning molecular fingerprints, aims to encode input notations of molecules into numerical vectors, which later serve as features for downstream tasks. Earlier deep molecular representation methods generally used off-the-shelf network architectures including message-passing neural networks [9], graph attention networks [36] and Transformers [11, 24]. These methods took either line notations (e.g. SMILES3) or graph notations (i.e. structural formulas) of molecules as inputs, whereas physical and chemical essence of molecules was largely neglected. Notably, a trend of integrating 3D conformations (i.e. the 3D Cartesian coordinates of atoms) into molecular representations recently emerged [4, 15, 29], while most of these methods assume the availability of labeled conformations of target molecules.
In order to push the boundaries of molecular representation learning, we revisited molecules from both physical and chemical perspectives. Modern physicists generally regard molecules as particle systems that continuously move following the laws of (quantum) mechanics. The dominant conformations of molecules reflect the equilibriums of these micro mechanical systems, and are thus of wide interest.
∗Equal Contribution.
†Corresponding Author. 3SMILES (Simplified Molecular Input Line Entry Specification [34]) is a widely used protocol that specifies (non-unique) line notations for molecules, CCO for ethanol, for example. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Chemists, on the other hand, focus more on chemical bonds and functional groups of molecules, which denote the interactions of electrons and determine chemical / biomedical properties such as solubility and toxicity, etc. Nevertheless, physical and chemical information of molecules is not orthogonal. For example, torsions of bond lengths and angles greatly influence the dynamics of particle systems. Therefore, an ideal molecular representation is not only expected to capture both physical and chemical information, but also to appropriately fuse the two types of information.
Based on the observations above, we propose PhysChem, a novel neural architecture that captures and fuses physical and chemical information of molecules. PhysChem is composed of two specialist networks, namely a physicist network (PhysNet) and a chemist network (ChemNet), who understand molecules physically and chemically.4 PhysNet is a neural physical engine that learns dominant conformations of molecules via simulating molecular dynamics in a generalized space. In PhysNet, implicit positions and momenta of atoms are initialized by encoding input features. Forces between pairs of atoms are learned with neural networks, according to which the system moves following laws of classic mechanics. Final positions of atoms are supervised with labeled conformations under spatial-invariant losses. ChemNet utilizes a message-passing framework [9] to capture chemical characteristics of atoms and bonds. ChemNet generates messages from atom states and local geometries, and then updates the states of both atoms and bonds. Output molecular representations are merged from atomic states and supervised with labeled chemical / biomedical properties. Besides focusing on their own specialties, two networks also cooperate by sharing expertise: PhysNet consults the hidden representations of chemical bonds in ChemNet to generate torsion forces, whereas
ChemNet leverages the local geometries of the intermediate conformations in PhysNet.
Compared with existing methods, PhysChem adopts a more elaborated as well as interpretable architecture that incarnates physical and chemical understandings of molecules. Moreover, as PhysNet learns molecular conformations from scratch, PhysChem does not require labeled conformations of test molecules. This extends the applicability of PhysChem to situations where such labels are unavailable, for example, with neural-network-generated drug candidates. We evaluated PhysChem on several datasets in the MoleculeNet [35] benchmark, where PhysChem displayed state-of-the-art performances on both conformation learning and property prediction tasks. Results on cutting-edge datasets of SARS-CoV-2 further proved the effectiveness of PhysChem. 2