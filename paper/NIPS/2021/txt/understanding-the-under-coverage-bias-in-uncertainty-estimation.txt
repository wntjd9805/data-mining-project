Abstract
Estimating the data uncertainty in regression tasks is often done by learning a quantile function or a prediction interval of the true label conditioned on the input. It is frequently observed that quantile regression—a vanilla algorithm for learning quantiles with asymptotic guarantees—tends to under-cover than the desired coverage level in reality. While various ﬁxes have been proposed, a more fundamental understanding of why this under-coverage bias happens in the ﬁrst place remains elusive.
In this paper, we present a rigorous theoretical study on the coverage of uncertainty estimation algorithms in learning quantiles. We prove that quantile regression suffers from an inherent under-coverage bias, in a vanilla setting where we learn a realizable linear quantile function and there is more data than parameters. More quantitatively, for α > 0.5 and small d/n, the α-quantile learned by quantile regression roughly achieves coverage α − (α − 1/2) · d/n regardless of the noise distribution, where d is the input dimension and n is the number of training data. Our theory reveals that this under-coverage bias stems from a certain high-dimensional parameter estimation error that is not implied by existing theories on quantile regression. Experiments on simulated and real data verify our theory and further illustrate the effect of various factors such as sample size and model capacity on the under-coverage bias in more practical setups. 1

Introduction
This paper is concerned with the problem of uncertainty estimation in regression problems. Uncer-tainty estimation is an increasingly important task in modern machine learning applications—Models should not only make high-accuracy predictions, but also have a sense of how much the true label may deviate from the prediction. This capability is crucial for deploying machine learning in the real world, in particular in risk-sensitive domains such as medical AI [15, 29], self-driving cars [47], and so on. A common approach for uncertainty estimation in regression is to learn a quantile function or a prediction interval of the true label conditioned on the input, which provides useful distributional information about the label. Such learned quantiles are typically evaluated by their coverage, i.e., probability that it covers the true label on a new test example. For example, a learned 90% upper quantile function should be an actual upper bound of the true label at least 90% of the time.
Algorithms for learning quantiles date back to the classical quantile regression [35], which estimates the quantile function by solving an empirical risk minimization problem with a suitable loss function that depends on the desired quantile level α. Quantile regression is conceptually simple, and is 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
theoretically shown to achieve asymptotically correct coverage as the sample size goes to inﬁnity [34] or approximately correct coverage in ﬁnite samples under speciﬁc modeling assumptions [46, 60, 56].
However, it is observed that quantile regression often under-covers than the desired coverage level in practice [53]. Various alternative approaches for constructing quantiles and conﬁdence intervals are proposed in more recent work, for example by aggregating multiple predictions using Bayesian neural networks or ensembles [24, 37], or by building on the conformal prediction technique to construct prediction intervals with ﬁnite-sample coverage guarantees [68, 66, 39, 53]. However, despite these advances, a more fundamental understanding on why vanilla quantile regression exhibits this under-coverage bias is still lacking.
This paper revisits quantile regression and presents a ﬁrst precise theoretical study on its coverage, in a new regime where the number of samples n is proportional to the dimension d, and the ratio d/n is small (so that the problem is under-parametrized). Our main result shows that quantile regression exhibits an inherent under-cover bias under this regime, even in the well-speciﬁed setting of learning a linear quantile function when the true data distribution follows a Gaussian linear model. To the best of our knowledge, this is the ﬁrst rigorous theoretical justiﬁcation of the under-coverage bias. Our main contributions are summarized as follows.
• We prove that linear quantile regression exhibits an inherent under-coverage bias in the well-speciﬁed setting where the data is generated from a Gaussian linear model, and the number of samples n is proportional to the feature dimension d with a small d/n (Section 3). More quantitatively, quantile regression at nominal level α ∈ (0.5, 1) roughly achieves coverage
α − (α − 1/2)d/n regardless of the noise distribution. To the best of our knowledge, this is the
ﬁrst rigorous characterization of the under-coverage bias in quantile regression.
• Towards understanding the source of this under-coverage bias, we disentangle the effect of estimating the bias and estimating the linear coefﬁcient on the coverage of the learned linear quantile (Section 4). We show that the estimation error in the bias can have either an under-coverage or over-coverage effect, depending on the noise distribution. In contrast, the estimation error in the linear coefﬁcient always drives the learned quantile to under-cover, and we show this effect is present even on broader classes of data distributions beyond the Gaussian linear model.
• We perform experiments on simulated and real data to test our theory (Section 5). Our simulations show that the coverage of quantile regression in Gaussian linear models agrees well with our precise theoretical formula as well as the α − (α − 1/2)d/n approximation. On real data, we
ﬁnd quantile regression using high-capacity models (such as neural networks) exhibits severe under-coverage biases, while linear quantile regression can also have a mild but non-negligible amount of under-coverage, even after we remove the potential effect of model misspeciﬁcation.
• On the technical end, our analysis builds on recent understandings of empirical risk minimiza-tion problems in the high-dimensional proportional limit with a small d/n, and develops new techniques such as a novel concentration argument to deal with an additional learnable variable in learning linear models with biases, which we believe could be of further interest (Section 6). 1.1