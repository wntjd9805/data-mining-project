Abstract
In reinforcement learning, experience replay stores past samples for further reuse.
Prioritized sampling is a promising technique to better utilize these samples. Pre-vious criteria of prioritization include TD error, recentness and corrective feed-back, which are mostly heuristically designed. In this work, we start from the regret minimization objective, and obtain an optimal prioritization strategy for
Bellman update that can directly maximize the return of the policy. The theory suggests that data with higher hindsight TD error, better on-policiness and more accurate Q value should be assigned with higher weights during sampling. Thus most previous criteria only consider this strategy partially. We not only provide theoretical justiﬁcations for previous criteria, but also propose two new methods to compute the prioritization weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT exploits the temporal ordering of states.
Both methods outperform previous prioritized sampling algorithms in challenging
RL benchmarks, including MuJoCo, Atari and Meta-World. 1

Introduction
Reinforcement learning (RL) [1] has achieved great success in sequential decision making problems.
Off-policy RL algorithms [2, 3, 4, 5, 6] have the ability to learn from a more general data distribution than on-policy counterparts, and often enjoy better sample efﬁciency. This is critical when the data collection process is expensive or dangerous. Experience Replay [7] enables data reuse and has been widely used in off-policy reinforcement learning. Previous work [8] points out that emphasizing on important samples in the replay buffer can beneﬁt off-policy RL algorithms. Prioritized Experience
Replay (PER) [9] quantiﬁes such importance by the magnitude of temporal-difference (TD) error.
Based on PER, many sampling strategies [10, 11, 12] are proposed to perform prioritized sampling.
They are either based on TD error [9, 10, 12] or focused on the existence of corrective feedback [11].
However, these are all proxy objectives and different from the objective of RL, i.e., minimizing policy regret. They can be suboptimal in some cases due to this objective mismatch.
In this paper, we ﬁrst give examples to illustrate the objective mismatch in previous prioritization strategies. Experiments show that lower TD error or more accurate Q function can not guarantee better policy performance. To tackle this issue, we ﬁrst formulate an optimization problem that directly minimizes the regret of the current policy with respect to prioritization weights. We then make several approximations and solve this optimization problem. An optimal prioritization strategy is obtained and indicates that we should pay more attention to experiences with higher hindsight TD error, better on-policiness and more accurate Q value. To the best of our knowledge, this paper is
∗Equal contribution
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the ﬁrst to optimize the sampling distribution of replay buffer theoretically from the perspective of regret minimization.
We then provide tractable approximations to the theoretical results. The on-policiness can be esti-mated by training a classiﬁer to distinguish recent transitions, which are generally more on-policy, from early ones, which are generally more off-policy. The oracle Q value is inaccessible during training, so we can not calculate the accuracy of Q value directly. Inspired by DisCor
[11], we propose an algorithm named ReMERN which estimates the suboptimality of Q value with an error network updated by Approximate Dynamic Programming (ADP).
ReMERN outperforms previous methods in environments with high randomness, e.g. with stochas-tic target positions or noisy rewards. However, the training of an extra neural network can be slow and unstable. We propose another estimation of Q accuracy based on a temporal viewpoint. With
Bellman updates, the error in Q value accumulates from the next state to the previous one all across the trajectory. The terminal state has no bootstrapping target and low Bellman error. Therefore, states fewer steps away from the terminal state will have lower error in the updated Q value because of the more accurate Bellman target. This intuition is veriﬁed both empirically and theoretically.
We then propose Temporal Correctness Estimation (TCE) based on the distance of each state to a terminal state, and name the overall algorithm ReMERT.
Similar to PER, ReMERN and ReMERT can be a plug-in module to all off-policy RL algorithms with a replay buffer, including but not limited to DQN [5] and SAC [2]. Experiments show that
ReMERN and ReMERT substantially improve the performance of standard off-policy RL methods in various benchmarks. 2