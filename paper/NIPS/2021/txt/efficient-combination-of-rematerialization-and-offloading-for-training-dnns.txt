Abstract
Rematerialization and ofﬂoading are two well known strategies to save memory during the training phase of deep neural networks, allowing data scientists to consider larger models, batch sizes or higher resolution data. Rematerialization trades memory for computation time, whereas Ofﬂoading trades memory for data movements. As these two resources are independent, it is appealing to consider the simultaneous combination of both strategies to save even more memory. We precisely model the costs and constraints corresponding to Deep Learning frame-works such as PyTorch or Tensorﬂow, we propose optimal algorithms to ﬁnd a valid sequence of memory-constrained operations and ﬁnally, we evaluate the per-formance of proposed algorithms on realistic networks and computation platforms.
Our experiments show that the possibility to ofﬂoad can remove one third of the overhead of rematerialization, and that together they can reduce the memory used for activations by a factor 4 to 6, with an overhead below 20%. 1

Introduction
A general trend in machine learning, especially in NLP today, is to increase the size of models to improve accuracy. For example, NLP models like BERT [7] or GPT-3 [30] have billions of parameters and vision networks like EfﬁcientNet [35], AmoebaNet [34] or SENET [15] routinely have a hundred million parameters, which raises difﬁcult memory issues on architectures like GPUs. Moreover, it was observed [29] that a sufﬁciently large batch-size is required to obtain good and fast convergence.
In this paper we focus on the strategies for reducing the memory usage of activations that can be implemented when considering a single computation node, typically a single GPU. These techniques can of course be combined with the different possible parallelizations of the training phase, like data parallelism [39, 27, 11], model parallelism [25, 37, 26], ﬁlter or kernel [9] or image parallelisms [8].
The combination with parallel strategies is however out of the scope of the present paper and our goal here is to model, analyze the complexity and understand how to beneﬁt simultaneously from rematerialization and ofﬂoading.
Rematerialization [14, 5, 20, 3] is a technique that consists in deleting from memory some of the activations computed during the forward phase as soon as they are no longer needed for the forward phase. As these activations are required again during the backward phase, they are then recomputed from other activations kept in memory. The idea is therefore to trade additional computations for lower memory requirements. The objective, in the context of rematerialization, is to compute a sequence of valid operations (i.e. such that the inputs of any operation are actually in memory at the time of the operation) consisting of elementary forward, backward and delete operations, and which has a minimum execution time among all sequences that satisfy a given memory constraint.
Ofﬂoading [32, 2, 23, 4] is a technique that has the same objective, i.e. to produce a sequence of operations satisfying a given memory constraint. However, in the case of ofﬂoading, each computation operation is executed exactly once.The idea here is therefore to trade transfers on the PCI bus between 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
the CPU and the GPU for lower memory requirements. The objective, in the context of ofﬂoading, is therefore to compute a sequence of valid operations consisting of elementary forward, backward, ofﬂoad and prefetch operations, and which has a minimum execution time among all sequences that fulﬁll a given memory constraint.
While rematerialization and ofﬂoading share the common goal of limiting memory consumption, they rely on different techniques and achieve this result by consuming independent resources, using either more computation or more transfers between a GPU and a CPU. In this paper we solve the problem of ﬁnding the optimal sequence combining rematerialization and activation ofﬂoading, which has not been addressed in the literature to the best of our knowledge. We discuss related work in Section 2 and model and notations in Section 3. The algorithmic solution to combine optimally checkpointing and ofﬂoading is presented in Section 4 and experiments to assess the advantage of using both ofﬂoading and rematerialization, based on a large range of neural networks and architecture parameters, are presented in Section 5. 2