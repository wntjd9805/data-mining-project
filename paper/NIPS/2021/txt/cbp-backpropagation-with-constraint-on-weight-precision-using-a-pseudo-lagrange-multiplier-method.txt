Abstract
Backward propagation of errors (backpropagation) is a method to minimize objec-tive functions (e.g., loss functions) of deep neural networks by identifying optimal sets of weights and biases. Imposing constraints on weight precision is often required to alleviate prohibitive workloads on hardware. Despite the remarkable success of backpropagation, the algorithm itself is not capable of considering such constraints unless additional algorithms are applied simultaneously. To address this issue, we propose the constrained backpropagation (CBP) algorithm based on the pseudo-Lagrange multiplier method to obtain the optimal set of weights that satisfy a given set of constraints. The deﬁning characteristic of the proposed CBP algorithm is the utilization of a Lagrangian function (loss function plus constraint function) as its objective function. We considered various types of constraints
— binary, ternary, one-bit shift, and two-bit shift weight constraints. As a post-training method, CBP applied to AlexNet, ResNet-18, ResNet-50, and GoogLeNet on ImageNet, which were pre-trained using the conventional backpropagation. For most cases, the proposed algorithm outperforms the state-of-the-art methods on
ImageNet, e.g., 66.6%, 74.4%, and 64.0% top-1 accuracy for ResNet-18, ResNet-50, and GoogLeNet with binary weights, respectively. This highlights CBP as a learning algorithm to address diverse constraints with the minimal performance loss by employing appropriate constraint functions. The code for CBP is publicly available at https://github.com/dooseokjeong/CBP. 1

Introduction
Currently, deep learning-based methods are applied in a variety of tasks, including the classiﬁcation of static data, e.g., image recognition [1, 2]; classiﬁcation of dynamic data, e.g., speech recogni-tion [3–6]; function approximations, which require the output of precise predictions, e.g., electronic structure predictions [7] and nonlinear circuit predictions [8]. All of the aforementioned tasks require discriminative models. Additionally, generative models, including generative adversarial networks [9] and variants [10–13], comprise another type of deep neural network. Despite the diversity in applica-tion domain and model type used, almost all deep learning-based methods use backpropagation as a common learning algorithm.
Recent developments in deep learning have primarily focused on increasing the size and depth of deep neural networks (DNNs) to improve their learning capabilities as in the case of state-of-the-art DNNs like VGGNet [14] and ResNet [15]. Given that the memory capacity required by a
DNN is proportional to the number of parameters (weights and biases), memory usage for DNN becomes severe. Additionally, a signiﬁcant number of multiply-accumulate operations during the
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
training and inference stages impose prohibitive workload on hardware. Thus, efﬁcient hardware-resource consumption is critical to the optimal performance of deep learning. One way to address this requirement is the use of weights of limited precision, such as binary [16, 17] and ternary weights [18, 19]. To this end, particular constraints are applied to weights during training, and additional algorithms for weight quantization are used in conjunction with backpropagation. This is because such constraints are not considered during the minimization of the objective function (loss function) when backpropagation is executed.
We adopt the Lagrange multiplier method (LMM) to combine basic backpropagation with additional constraint algorithms and produce a single constrained backpropagation (CBP) algorithm. We refer to the adopted method as pseudo-LMM because the constraint functions cs (x) are nondifferentiable at xm (= arg minx cs (x)), rendering LMM inapplicable. Nevertheless, pseudo-LMM successfully attains the optimal point under particular conditions as for LMM. In the CBP algorithm, the optimal weights satisfying a given set of constraints are evaluated via a basic backpropagation algorithm.
It is implemented by simply replacing the conventional objective function (loss function) with a Lagrangian function L that comprises the loss and constraint functions as sub-functions that are subjected to simultaneous minimization. Therefore, this method is perfectly compatible with conventional deep learning frameworks. The primary contributions of this study are as follows.
• We introduce a novel and simple method to incorporate given constraints into backpropa-gation by using a Lagrangian function as the objective function. The proposed method is able to address any set of constraints on the weights insomuch as the constraint functions are mathematically well-deﬁned.
• We introduce pseudo-LMM with constraint functions cs (w) that are nondifferentiable at wm (= arg minw cs (w)) and analyze the kinetics of pseudo-LMM in the continuous time domain.
• We introduce optimal (sawtooth-like) constraint functions with gradually vanishing unconstrained-weight windows and provide a guiding principle for the stable co-optimization of weights and Lagrange multipliers in a quasi-static fashion.
• We evaluate the performance of CBP applied to AlexNet, ResNet-18, ResNet-50, and
GoogLeNet (pre-trained using backpropagation with full-precision weights) with four different constraints (binary, ternary, one-bit shift, and two-bit shift weight constraints) on
ImageNet as proof-of-concept examples. The results highlight the classiﬁcation accuracy outperforming the previous state-of-the-art results. 2