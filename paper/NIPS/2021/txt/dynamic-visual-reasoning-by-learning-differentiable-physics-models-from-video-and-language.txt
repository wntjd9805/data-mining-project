Abstract
In this work, we propose a uniﬁed framework, called Visual Reasoning with Differ-entiable Physics (VRDP) 1, that can jointly learn visual concepts and infer physics models of objects and their interactions from videos and language. This is achieved by seamlessly integrating three components: a visual perception module, a concept learner, and a differentiable physics engine. The visual perception module parses each video frame into object-centric trajectories and represents them as latent scene representations. The concept learner grounds visual concepts (e.g., color, shape, and material) from these object-centric representations based on the language, thus providing prior knowledge for the physics engine. The differentiable physics model, implemented as an impulse-based differentiable rigid-body simulator, per-forms differentiable physical simulation based on the grounded concepts to infer physical properties, such as mass, restitution, and velocity, by ﬁtting the simulated trajectories into the video observations. Consequently, these learned concepts and physical models can explain what we have seen and imagine what is about to happen in future and counterfactual scenarios. Integrating differentiable physics into the dynamic reasoning framework offers several appealing beneﬁts. More accurate dynamics prediction in learned physics models enables state-of-the-art performance on both synthetic and real-world benchmarks while still maintaining high transparency and interpretability; most notably, VRDP improves the accuracy of predictive and counterfactual questions by 4.5% and 11.5% compared to its best counterpart. VRDP is also highly data-efﬁcient: physical parameters can be optimized from very few videos, and even a single video can be sufﬁcient. Finally, with all physical parameters inferred, VRDP can quickly learn new concepts from few examples. 1

Introduction
Dynamic visual reasoning about objects, relations, and physics is essential for human intelligence.
Given a raw video, humans can easily use their common sense of intuitive physics to explain what has happened, predict what will happen next, and infer what would happen in counterfactual situations.
Such human-like physical scene understanding capabilities are also of great importance in practical applications such as industrial robot control [2, 53]. 1Project page: http://vrdp.csail.mit.edu/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Previous works have made great efforts to build artiﬁcial intelligence (AI) models with such physical reasoning capabilities. One popular strategy is to develop pure neural-network-based models [63, 19, 40]. These methods typically leverage end-to-end neural networks [32, 35] with powerful attention modules such as Transformer [69, 21] to extract attended features from both video frames and question words, based on which they answer questions directly. Despite their high question-answering accuracy on CLEVRER [79], a challenging dynamic visual question-answering benchmark, these black-box models neither learn concepts nor model objects’ dynamics. Therefore, they lack transparency, interpretability, and generalizability to new concepts and scenarios. Another common approach to dynamic visual reasoning is to build graph neural networks (GNNs) [47] to capture the dynamics of the scenes. These GNN models [54, 79, 16] treat objects in the video as nodes and perform object-and relation-centric updates to predict objects’ dynamics in future or counterfactual scenes. Such systems achieve decent performance with good interpretability on CLEVRER by combining the
GNN-based dynamics models with neural-symbolic execution [58, 80]. However, these dynamic models do not explicitly consider laws of physics or use concepts encoded in the question-answer pairs associated with the videos. As a result, they show limitations in counterfactual situations that require long-term dynamics prediction.
Although (graph-)neural-network-based approaches have achieved competitive performance on
CLEVRER, dynamic visual reasoning is still far from being solved perfectly. In particular, due to the lack of explicit physics models, existing models [79, 19, 16] typically struggle to reason about future and counterfactual events, especially when training data is limited. For this reason, one appealing alternative is to develop explicit physics-based methods to model and reason about dynamics, as highlighted in the recent development of differentiable physics engines [9, 17, 68, 18, 66] and their applications in robotics [9, 17, 68]. However, these physics engines typically take as input a full description of the scene (e.g., the number of objects and their shapes) which usually requires certain human priors, limiting their availability to applications with well-deﬁned inputs only.
In this work, we take an approach fundamentally different from either network-based methods or physics-based methods. Noting that deep learning based methods excel at parsing objects and learning concepts from videos and language, and physics laws are good at capturing object dynamics, we propose Visual Reasoning with Differentiable Physics (VRDP), a uniﬁed framework that combines a visual perception module, a concept learner, and a differentiable physics engine. VRDP jointly learns object trajectories, language concepts, and objects’ physics models to make accurate dynamic predictions. It starts with a perception module running an object detector [31] on individual frames to generate object proposals and connect them into trajectories based on a motion heuristic. Then, a concept learner learns object- and event-based concepts, such as ‘shape’, ‘moving’, and ‘collision’ as in DCL [16, 58]. Based on the obtained object trajectories and attributes, the differentiable physics engine estimates all dynamic and physical properties (e.g., velocity, angular velocity, restitution, mass, and the coefﬁcient of resistance) by comparing the simulated trajectories with the video observations.
With these explicit physical parameters, the physics engine reruns the simulation to reason about future motion and causal events, which a program executor then executes to get the answer. The three components of VRDP cooperate seamlessly: the concept learner grounds physical concepts needed by the physics engine like ‘shape’ onto the objects detected by the perception module; the differentiable physics engine estimates all physical parameters and simulates accurate object trajectories, which in turn help the concept learning process in the concept learner.
Compared with existing methods, VRDP has several advantages thanks to its carefully modularized design. First, it achieves the state-of-the-art performance on both synthetic videos (CLEVRER [79]) and real-world videos (Real-Billiard [63]) without sacriﬁcing transparency or interpretability, espe-cially in situations that require long-term dynamics prediction. Second, it has high data efﬁciency thanks to the differentiable physics engine and symbolic representation. Third, it shows strong generalization capabilities and can capture new concepts with only a few examples. 2