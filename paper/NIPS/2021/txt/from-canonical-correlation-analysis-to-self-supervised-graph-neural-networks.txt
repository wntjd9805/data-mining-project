Abstract
We introduce a conceptually simple yet effective model for self-supervised represen-tation learning with graph data. It follows the previous methods that generate two views of an input graph through data augmentation. However, unlike contrastive methods that focus on instance-level discrimination, we optimize an innovative feature-level objective inspired by classical Canonical Correlation Analysis. Com-pared with other works, our approach requires none of the parameterized mutual information estimator, additional projector, asymmetric structures, and most im-portantly, negative samples which can be costly. We show that the new objective essentially 1) aims at discarding augmentation-variant information by learning invariant representations, and 2) can prevent degenerated solutions by decorrelating features in different dimensions. Our theoretical analysis further provides an under-standing for the new objective which can be equivalently seen as an instantiation of the Information Bottleneck Principle under the self-supervised setting. Despite its simplicity, our method performs competitively on seven public graph datasets.
The code is available at: https://github.com/hengruizhang98/CCA-SSG. 1

Introduction
Self-supervised learning (SSL) has been a promising paradigm for learning useful representations without costly labels [7, 46, 5]. In general, it learns representations via a proxy objective between inputs and self-deﬁned signals, among which contrastive methods [46, 40, 16, 5, 12] have achieved impressive performance on learning image representations by maximizing the mutual information of two views (or augmentations) of the same input. Such methods can be interpreted as a discrimination of a joint distribution (positive pairs) from the product of two marginal ones (negative pairs) [50].
Inspired by the success of contrastive learning in vision [17, 46, 40, 5, 16, 12, 6], similar methods have been adapted to learning graph neural networks [48, 15, 33, 57, 58]. Although these models have achieved impressive performance, they require complex designs and architectures. For example,
DGI [48] and MVGRL [15] rely on a parameterized mutual information estimator to discriminate positive node-graph pairs from negative ones; GRACE [57] and GCA [58] harness an additional
MLP-projector to guarantee sufﬁcient capacity. Moreover, negative pairs sampled or constructed from data often play an indispensable role in providing effective contrastive signals and have a large impact on performance. Selecting proper negative samples is often nontrivial for graph-structured data, not to mention the extra storage cost for prohibitively large graphs. BGRL [39] is a recent endeavor on
⇤This work was done during the author’s internship at AWS Shanghai AI Lab.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Technical comparison of self-supervised node representation learning methods. We provide a conceptual comparison with more self-supervised methods in Appendix G. Target denotes the comparison pair, N/G/F denotes node/graph/feature respectively. MI-Estimator: parameterized mutual information estimator. Proj/Pred: additional (MLP) projector or predictor. Asymmetric: asymmetric architectures such as EMA and Stop-Gradient, or two separate encoders for two branches. Neg examples: requiring negative examples to prevent trivial solutions. Space denotes space requirement for storing all the pairs. Our method is simple without any listed component and memory-efﬁcient.
Methods
Target MI-Estimator
Proj/Pred Asymmetric Neg examples
Space l DGI [48] e v e l
-e c n a t s n
I
MVGRL [15]
GRACE [57]
GCA [58]
BGRL [39]
CCA-SSG (Ours)
N-G
N-G
N-N
N-N
N-N
F-F
!
!
------!
!
!
--!
--!
-!
!
!
!
--O(N )
O(N )
O(N 2)
O(N 2)
O(N )
O(D2) targeting a negative-sample-free approach for GNN learning through asymmetric architectures [12, 6].
However, it requires additional components, e.g., an exponential moving average (EMA) and Stop-Gradient, to empirically avoid degenerated solutions, leading to a more intricate architecture.
Deviating from the large body of previous works on contrastive learning, in this paper we take a new perspective to address SSL on graphs. We introduce Canonical Correlation Analysis inspired
Self-Supervised Learning on Graphs (CCA-SSG), a simple yet effective approach that opens the way to a new SSL objective and frees the model from intricate designs. It follows the common practice of prior arts, generating two views of an input graph through random augmentation and acquiring node representations through a shared GNN encoder. Differently, we propose to harness a non-contrastive and non-discriminative feature-level objective, which is inspired by the well-studied
Canonical Correlation Analysis (CCA) methods [18, 10, 11, 14, 2, 4]. More speciﬁcally, the new objective aims at maximizing the correlation between two augmented views of the same input and meanwhile decorrelating different (feature) dimensions of a single view’s representation. We show that the objective 1) essentially pursuits discarding augmentation-variant information and preserving augmentation-invariant information, and 2) can prevent dimensional collapse [19] (i.e., different dimensions capture the same information) in nature. Furthermore, our theoretical analysis sheds more lights that under mild assumptions, our model is an instantiation of Information Bottleneck
Principle [43, 44, 37] under SSL settings [53, 9, 45].
To sum up, as shown in Table 1, our new objective induces a simple and light model without reliance on negative pairs [48, 15, 57, 58], a parameterized mutual information estimator [48, 15], an additional projector or predictor [57, 58, 39] or asymmetric architectures [39, 15]. We provide a thorough evaluation for the model on seven node classiﬁcation benchmarks. The empirical results demonstrate that despite its simplicity, CCA-SSG can achieve very competitive performance in general and even superior test accuracy in ﬁve datasets. It is worth noting that our approach is agnostic to the input data format, which means that it can potentially be applied to other scenarios beyond graph-structured data (such as vision, language, etc.). We leave such a technical extension for future works.
Our contributions are as follows: 1) We introduce a non-contrastive and non-discriminative objective for self-supervised learning, which is inspired by Canonical Correlation Analysis methods. It does not rely on negative samples, and can naturally remove the complicated components. Based on it we propose CCA-SSG, a simple yet effective framework for learning node representations without supervision (see Section 3). 2) We theoretically prove that the proposed objective aims at keeping augmentation-invariant in-formation while discarding augmentation-variant one, and possesses an inherent relationship to an embodiment of Information Bottleneck Principle under self-supervised settings (see Section 4). 3) Experimental results show that without complex designs, our method outperforms state-of-the-art self-supervised methods MVGRL [15] and GCA [58] on 5 out of 7 benchmarks. We also provide thorough ablation studies on the effectiveness of the key components of CCA-SSG (see Section 5). 2
2