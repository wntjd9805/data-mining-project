Abstract
KL-regularized reinforcement learning from expert demonstrations has proved successful in improving the sample efﬁciency of deep reinforcement learning al-gorithms, allowing them to be applied to challenging physical real-world tasks.
However, we show that KL-regularized reinforcement learning with behavioral reference policies derived from expert demonstrations can suffer from patholog-ical training dynamics that can lead to slow, unstable, and suboptimal online learning. We show empirically that the pathology occurs for commonly chosen behavioral policy classes and demonstrate its impact on sample efﬁciency and online policy performance. Finally, we show that the pathology can be remedied by non-parametric behavioral reference policies and that this allows KL-regularized reinforcement learning to signiﬁcantly outperform state-of-the-art approaches on a variety of challenging locomotion and dexterous hand manipulation tasks. 1

Introduction
Reinforcement learning (RL) [15, 24, 46, 47] is a powerful paradigm for learning complex behaviors.
Unfortunately, many modern reinforcement learning algorithms require agents to carry out millions of interactions with their environment to learn desirable behaviors, making them of limited use for a wide range of practical applications that cannot be simulated [8, 28]. This limitation has motivated the study of algorithms that can incorporate pre-collected ofﬂine data into the training process either fully ofﬂine or with online exploration to improve sample efﬁciency, performance, and reliability [2, 6, 16, 23, 52, 53]. An important and well-motivated subset of these methods consists of approaches for efﬁciently incorporating expert demonstrations into the learning process [5, 11, 18, 42].
Reinforcement learning with Kullback-Leibler (KL) regularization is a particularly successful ap-proach for doing so [3, 27, 29, 31, 44, 51]. In KL-regularized reinforcement learning, the standard reinforcement learning objective is augmented by a Kullback-Leibler divergence term that penal-izes dissimilarity between the online policy and a behavioral reference policy derived from expert demonstrations. The resulting regularized objective pulls the agent’s online policy towards the behavioral reference policy while also allowing it to improve upon the behavioral reference policy by exploring and interacting with the environment. Recent advances that leverage explicit or implicit
KL-regularized objectives, such as BRAC [51], ABM [44], and AWAC [27], have shown that KL-regularized reinforcement learning from expert demonstrations is able to signiﬁcantly improve the sample efﬁciency of online training and reliably solve challenging environments previously unsolved by standard deep reinforcement learning algorithms.
∗Equal contribution. † Corresponding author: tim.rudner@cs.ox.ac.uk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Parametric
Non-Parametric
Figure 1: Predictive variances of non-parametric and parametric behavioral policies on a low dimen-sional representation (the ﬁrst two principal components) of a 39-dimensional dexterous hand manipula-tion state space (see “door-binary-v0” in Figure 5). Left: Parametric neural network Gaussian behavioral
ψ(s)). Right: Non-parametric Gaussian process posterior behavioral policy policy πψ(
πGP ( used to train the behavioral policies are shown in black. The GP predictive variance is well-calibrated: It is small near the expert trajectories and large in other parts of the state space. In contrast, the neural network predictive variance is poorly calibrated: It is relatively small on the expert trajectories, and collapses to near zero elsewhere. Note the signiﬁcant difference in scales. (µψ(s), σ2 (µ0(s), Σ0(s, s(cid:48))). Expert trajectories s) =
· | 0) =
D
N
GP
D s,
· |
Contributions. In this paper, we show that despite some empirical success, KL-regularized rein-forcement learning from expert demonstrations can suffer from previously unrecognized pathologies that lead to instability and sub-optimality in online learning. To summarize, our core contributions are as follows:
•
•
•
We illustrate empirically that commonly used classes of parametric behavioral policies experi-ence a collapse in predictive variance about states away from the expert demonstrations.
We demonstrate theoretically and empirically that KL-regularized reinforcement learning al-gorithms can suffer from pathological training dynamics in online learning when regularized against behavioral policies that exhibit such a collapse in predictive variance.
We show that the pathology can be remedied by non-parametric behavioral policies, whose predictive variances are well-calibrated and guaranteed not to collapse about previously unseen states, and that ﬁxing the pathology results in online policies that signiﬁcantly outperform state-of-the-art approaches on a range of challenging locomotion and dexterous hand manipulation tasks.
The left panel of Figure 1 shows an example of the collapse in predictive variance away from the expert trajectories in parametric behavioral policies. In contrast, the right panel of Figure 1 shows the predictive variance of a non-parametric behavioral policy, which—unlike in the case of the parametric policy—increases off the expert trajectories. By avoiding the pathology, we obtain a stable and reliable approach to sample-efﬁcient reinforcement learning, applicable to a wide range of reinforcement learning algorithms that leverage KL-regularized objectives.2 2