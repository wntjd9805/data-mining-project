Abstract
When compared to the image classiﬁcation models, black-box adversarial attacks against video classiﬁcation models have been largely understudied. This could be possible because, with video, the temporal dimension poses signiﬁcant additional challenges in gradient estimation. Query-efﬁcient black-box attacks rely on effec-tively estimated gradients towards maximizing the probability of misclassifying the target video. In this work, we demonstrate that such effective gradients can be searched for by parameterizing the temporal structure of the search space with geometric transformations. Speciﬁcally, we design a novel iterative algorithm Geo-metric TRAnsformed Perturbations (GEO-TRAP), for attacking video classiﬁcation models. GEO-TRAP employs standard geometric transformation operations to reduce the search space for effective gradients by searching for a small group of parameters that deﬁne these operations. This group of parameters describes the ge-ometric progression of gradients, resulting in a reduced and structured search space.
Our algorithm inherently leads to successful perturbations with surprisingly few queries. For example, adversarial examples generated from GEO-TRAP have better 73.55% fewer queries compared to the state-of-the-art attack success rates with method for video adversarial attacks on the widely used Jester dataset. Overall, our algorithm exposes vulnerabilities of diverse video classiﬁcation models and achieves new state-of-the-art results under black-box settings on two large datasets.
Code is available here: https://github.com/sli057/Geo-TRAP
∼ 1

Introduction
Adversarial attacks are designed to expose vulnerabilities of Deep Neural Networks (DNNs). With real-world applications of video classiﬁcation based on DNNs emerging [1–3], a key question that arises is “what type of adversarial inputs can mislead, and thus render video classiﬁcation networks vulnerable?” Designing such adversarial attacks not only helps expose security ﬂaws of DNNs, but can also potentially stimulate the design of more robust video classiﬁcation models.
Adversarial attacks against image classiﬁcation models have been studied in both white-box [4–8] and black-box [9–13] settings. In the white-box setting, an adversary has full access to the model under attack, including its parameters and training settings (hyper-parameters, training data, etc.) In the black-box setting, an adversary only has partial information about the victim model, such as the predicted labels of the model. In the case of video classiﬁcation models, adversarial attacks in both white-box and black-box settings have garnered some interest [14–22], although the body of work here is more limited than the case of image classiﬁcation models.
A common black-box attack paradigm is query-based, wherein the attacker can send queries to the victim model to collect the corresponding predicted labels, and thereby estimate the gradients needed
∗Equal contribution. Corresponding author: Shasha Li (sli057@ucr.edu) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison with state-of-the-art. GEO-TRAP, compared to current black-box attack methods for videos, doesn’t train a different network to craft perturbations, and parameterizes the temporal dimension of videos in searching for effective perturbation directions.
Methods
PATCHATTACK[19]
HEURISTICATTACK [20]
SPARSEATTACK [21]
MOTION-SAMPLER ATTACK [22]
GEO-TRAP (Ours)
WITHOUT training a
“perturbation” network (cid:55) (cid:88) (cid:55) (cid:88) (cid:88)
CONSIDER temporal dimension? (cid:55) (cid:55) (cid:55) (cid:88) (cid:88)
PARAMETERIZE temporal dimension? (cid:55) (cid:55) (cid:55) (cid:55) (cid:88) for curating the adversarial examples. Unlike static images, videos naturally include additional information from the temporal dimension. This high dimensionality (i.e., sequence of frames instead of one image) poses challenges to black-box adversarial attacks against video classiﬁcation models; in particular, signiﬁcantly more queries are typically needed for estimating the gradients for crafting adversarial samples [19–22]. [19] reduces the number of queries by adding perturbations on the patch level instead of at the pixel level; [20, 21] propose to add perturbations only on key pixels. [22] considers the intrinsic differences between images and videos (i.e., the temporal dimension), and proposes to use the optical-ﬂow of clean videos as the motion prior for adversarial video generation.
Similar to [22], we also explicitly consider the temporal dimension of video. However, rather than
ﬁxing the temporal search space using the motion prior of clean videos, we propose to parameterize the temporal structure of the space with geometric transformations. This results in a better structured and reduced search space, which allows us to generate successful attacks with much fewer queries in black-box settings than the state-of-the-art methods, including [22].
Contributions. In this paper, we propose a novel query-efﬁcient black-box attack algorithm against video classiﬁcation models. Due to the extra temporal dimension, generating video perturbations by searching for effective gradients remains a challenging task given the exceedingly large search space. These gradients are estimated by searching for ‘directions’ that maximize the probability of the victim model mis-classifying the crafted inputs. Our approach drastically reduces this large search space by deﬁning this space with a small set of parameters that describe the geometric progression of gradients in the temporal dimension, resulting in a reduced and temporally structured search space.
Conceptually, this parameterization of the temporal structure of the search space is performed using geometric transformations (e.g. afﬁne transformations). We refer to our algorithm as Geometrically
TRAnsformed Perturbations, or GEO-TRAP. Despite this surprisingly simple strategy, GEO-TRAP outperforms existing black-box video adversarial attack methods by signiﬁcant margins ( 1.8% 73.55% fewer queries for targeted attacks in comparison improvement in attack success rate with to the state-of-the-art [22] on the Jester dataset [23]).
∼
∼ 2