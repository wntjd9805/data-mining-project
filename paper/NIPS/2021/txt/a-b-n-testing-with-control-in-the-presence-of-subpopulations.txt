Abstract
Motivated by A/B/n testing applications, we consider a ﬁnite set of distributions (called arms), one of which is treated as a control. We assume that the population is stratiﬁed into homogeneous subpopulations. At every time step, a subpopulation is sampled and an arm is chosen: the resulting observation is an independent draw from the arm conditioned on the subpopulation. The quality of each arm is assessed through a weighted combination of its subpopulation means. We propose a strategy for sequentially choosing one arm per time step so as to discover as fast as possible which arms, if any, have higher weighted expectation than the control. This strategy is shown to be asymptotically optimal in the following sense: if τδ is the ﬁrst time when the strategy ensures that it is able to output the correct answer with
δ, then E[τδ] grows linearly with log(1/δ) at the exact probability at least 1 optimal rate. This rate is identiﬁed in the paper in three different settings: (1) when the experimenter does not observe the subpopulation information, (2) when the subpopulation of each sample is observed but not chosen, and (3) when the experimenter can select the subpopulation from which each response is sampled.
We illustrate the efﬁciency of the proposed strategy with numerical simulations on synthetic and real data collected from an A/B/n experiment.
− 1

Introduction
A/B/n testing is a website optimization procedure where multiple versions of the content (called
"arms" below) are compared, often in order to ﬁnd the one with the highest conversion rate. However, many e-commerce companies use A/B/n testing not only to deploy the best product implementation, but primarily to draw post-experiment inferences [11]. The decision-making involves, besides experiment results, factors such as the cost of scaling-up a solution, external data, or whether the implementation ﬁts in a broader theme. In this setting, each of the arms better than the default product (which we will refer to as the "control" arm) is a contender for being deployed and the interest is not only in the best arm. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
≥ 1 alternative implementations (variants), the simplest idea is to distribute
Given the control and K the trafﬁc uniformly among the arms; the arms that appear to be signiﬁcantly better than the control at the end of the experiment are considered for deployment. While well-established, this process can be inefﬁcient in terms of resources. Some alternatives are soon obviously worse (or better) than the control and would require fewer samples than the alternatives closer to the control. A second related shortcoming of the basic A/B/n testing approach is that setting the duration of the experiment –when done in advance– necessitates a very conservative approach by choosing a run-length that is sufﬁciently long to differentiate even the smallest possible changes.
To address these limitations, we consider in this work sequential testing policies that can both adjust the allocation of the samples and be stopped adaptively, in light of the data gathered during the experiment. In the terminology of multi-armed bandits, this corresponds to pure exploration problems (see, e.g., Chap. 33 of [15]). A pure exploration strategy will typically choose every minute (say), an allocation of trafﬁc that favors arms for which the uncertainty is the highest. The experiment is stopped as soon as the signiﬁcance is considered sufﬁcient for every arm. Approaches have been developed in [8, 12, 10] for the identiﬁcation of the single arm with the highest mean, a task called the
Best Arm Identiﬁcation (BAI) problem. In particular, [10] propose a strategy that is asymptotically optimal in the ﬁxed conﬁdence setting, meaning that, given a risk parameter δ, it ﬁnds the best arm with probability at least 1
δ, using an expected number of samples that is hardly improvable when δ is small. Later, [18] incorporated the special role of the control arm in BAI and proposed an algorithm that declares as winning arm the one with the highest mean only if it is signiﬁcantly better than the control. In this paper, we propose a solution to the problem of identifying all the arms that are better than the control, in a framework that generalizes the ﬁxed conﬁdence setting. In order to provide useful tools for practical A/B/n testing, we address two additional issues.
−
First, traditional stochastic bandit models are based on the assumption that the arm samples are i.i.d., whereas real world data streams usually show trends or some form of inhomogeneity. A particular case of interest for website optimization are the seasonal patterns caused by time-of-day or day-of-week variations. We henceforth include in our model observed covariates (e.g. the time of the day, but possibly also the country of origin, or controlled covariates like the order in which partners appear on the page, etc.) that stratify the observations into homogeneous subpopulations. We study different scenarios, depending on how much interaction is possible with these subpopulations. We provide a sample complexity analysis and an efﬁcient algorithm in each case. In particular, we will show that using the subpopulation information efﬁciently can provide signiﬁcant speedups of the decision-making. In the following, we will refer to the task of identifying the set of Arms that are
Better than the Control in the presence of Subpopulations as the ABC-S problem.
Second, the practice of A/B/n testing often differs from a pure sequential experiment in that the experimenter cannot always ﬁx a risk δ at the beginning and passively wait for the stopping time of the experiment without any time limitation. To address this issue, [11] proposed to deﬁne some notion of sequential "p-values" that can be monitored as the experiment progresses and used to terminate it.
This notion was further used in the BAI setting in [18]. In this contribution, we elaborate on this idea by sequentially updating a suggested solution to the ABC-S problem together with a risk assessment for this suggestion. We show that, for any stopping time, the probability that the suggested solution is incorrect is indeed lower than the risk assessment. When the stopping time is selected as in usual
ﬁxed-conﬁdence pure exploration, we recover the exact same guarantees but this view of the problem also provides useful results, for instance, if the experiment needs to be terminated prematurely.