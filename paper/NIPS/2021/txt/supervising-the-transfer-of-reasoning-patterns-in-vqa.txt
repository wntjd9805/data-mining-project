Abstract
Methods for Visual Question Anwering (VQA) are notorious for leveraging dataset biases rather than performing reasoning, hindering generalization. It has been recently shown that better reasoning patterns emerge in attention layers of a state-of-the-art VQA model when they are trained on perfect (oracle) visual inputs. This provides evidence that deep neural networks can learn to reason when training conditions are favorable enough. However, transferring this learned knowledge to deployable models is a challenge, as much of it is lost during the transfer. We propose a method for knowledge transfer based on a regularization term in our loss function, supervising the sequence of required reasoning operations. We provide a theoretical analysis based on PAC-learning, showing that such program prediction can lead to decreased sample complexity under mild hypotheses. We also demonstrate the effectiveness of this approach experimentally on the GQA dataset and show its complementarity to BERT-like self-supervised pre-training. 1

Introduction
Reasoning over images is the main goal of Visual Question Anwering (VQA), a task where a model is asked to answer questions over images. This problem is a test bed for the creation of agents capable of high-level reasoning, as it involves multi-modal and high-dimensional data as well as complex decision functions requiring latent representations and multiple hops. State-of-the-art models are notorious for leveraging dataset biases and shortcuts in learning rather than performing reasoning, leading to lack of generalization, as evidenced by extensive recent work on bias oriented benchmarks for vision-and-language reasoning [1, 20, 21, 31]. Even large-scale semi-supervised pre-training methods, which successfully managed to increase overall VQA performance, still struggle to address questions whose answers are rare given a context [20].
It has been recently shown that reasoning patterns emerge in attention layers of a SOTA VQA model when trained on perfect (oracle) visual inputs, which provides evidence that deep neural networks can learn to reason, when training conditions are favorable enough [21]. In particular, uncertainty and noise in visual inputs seems to be a major cause for shortcut learning in VQA. While this kind of methods provide strong empirical results and insights on the bottlenecks in problems involving learning to reason, they still suffer from signiﬁcant loss in reasoning capabilities during the transfer phase, when the model is required to adapt from perfectly clean visual input to the noisy input it will encounter after deployment. We conjecture, that reasoning on noisy data involves additional functional components not necessary in the clean case due to different types of domain shifts: (1) a presence shift, caused by imperfect object detectors, leading to missing visual objects necessary for 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: VQA takes visual input v and a question q and predicts a distribution over answers y. (a)
Classical discriminative training encodes the full reasoning function in the network parameters θ, while the network activations contain latent variables necessary for reasoning over multiple hops. (b)
Additional program supervision requires intermediate network activations to contain information on the reasoning process, simplifying learning the reasoning function g. Under the hypothesis of it’s decomposition into multiple reasoning modes, intermediate supervision favors separately learning the mode selector and each individual mode function. This intuition is analyzed theoretically in section 4. reasoning, or to multiple (duplicate) detections; (2) an appearance shift causing variations in object embeddings (descriptors) for the same class of objects due to different appearance.
In this paper, we propose a new method for transferring reasoning patterns from models learned on perfect visual input to models trained on noisy visual representations. Key to the success is a regularization term minimizing loss of the reasoning capabilities during transfer. In particular, we address this problem through program prediction as an additional auxiliary loss, i.e. supervision of the sequence of reasoning operations along with their textual and/or visual arguments. To maintain a strong link between the learned function and its objective during the knowledge transfer phase, when inputs are switched from clean oracle inputs to noisy input, the neural model is required to continue to predict complex reasoning programs from different types of inputs.
As a second justiﬁcation, we claim that program supervision in itself leads to a simpler learning problem, as the underlying reasoning function is decomposed into a set of tasks, each of which is easier to learn individually than the full joint decision function. We backup this claim through a theoretical analysis showing decreased sample complexity under mild hypotheses.
As a summary, we present the following contributions: (i) we propose a new program supervision module added on top of vision-language transformer models; (ii) we provide a theoretical analysis of the beneﬁt of supervising program prediction in VQA deriving bounds on sample complexity; (iii) we experimentally demonstrate the efﬁciency of program supervision and show that it increases
VQA performance on both in- and out-of-distribution sets, even when combined with BERT-like pre-training [30, 10], and that it improves the quality of oracle transfer initially proposed by [21]. 2