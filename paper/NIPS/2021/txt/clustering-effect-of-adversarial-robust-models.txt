Abstract
Adversarial robustness has received increasing attention along with the study of adversarial examples. So far, existing works show that robust models not only obtain robustness against various adversarial attacks but also boost the performance in some downstream tasks. However, the underlying mechanism of adversarial robustness is still not clear. In this paper, we interpret adversarial robustness from the perspective of linear components, and ﬁnd that there exist some statis-tical properties for comprehensively robust models. Speciﬁcally, robust models show obvious hierarchical clustering effect on their linearized sub-networks, when removing or replacing all non-linear components (e.g., batch normalization, max-imum pooling, or activation layers). Based on these observations, we propose a novel understanding of adversarial robustness and apply it on more tasks including domain adaption and robustness boosting. Experimental evaluations demonstrate the rationality and superiority of our proposed clustering strategy. Our code is available at https://github.com/bymavis/Adv_Weight_NeurIPS2021. 1

Introduction
Nowadays, deep neural networks (DNNs) have shown a strong learning capacity through a huge number of parameters and diverse structures [16, 31, 11]. Meanwhile, adversary has raised increasing security concerns on DNNs due to the observation of adversarial examples (i.e., the examples mislead a classiﬁer when crafted with human imperceptible but carefully designed perturbations) [13].
Adversarial robustness and defense techniques have thus become crucial for deep learning, yet many adversarial defense techniques are found with deﬁciencies such as obfuscated gradient [2]. Up to now, the widely accepted techniques to improve adversarial robustness are adversarial training variants
[23, 32, 38, 33, 35, 36], by training a DNN after data augmentation with the worst-case adversarial examples. Recent study has found that not only do robust models show moderate robustness under vast attacks, but also they can surprisingly boost the downstream tasks (e.g., domain adaption task with subpopulation shift) [29].
However, the underlying mechanism of adversarial robustness is still not clear. Existing studies have explored adversary on some speciﬁc components of DNNs, such as batch normalization [12], skip connection [34], or activation layers [14, 4], which yield some enlightening understanding. On the other hand, despite variation in DNNs’ structure, the components can be roughly divided into linear and non-linear ones. The non-linear components tend to be instance-wise. For example, adversarial
*Equal contribution.
†Correspondence to: Yisen Wang (yisen.wang@pku.edu.cn) and Shu-Tao Xia (xiast@sz.tsinghua.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021) .
CIFAR-10
Non-animal
Animal 0 1 8 9 2 3 4 5 6 7 (a) Class Hierarchy (b) VGG16-STD (c) VGG16-AT (d) VGG16-AT+C
Figure 1: Correlation matrices C of VGG-16 on CIFAR-10: b) ‘STD’ and c) ‘AT’ indicate standard and adversarial training, d) ‘AT+C’ indicates applying hierarchical clustering in adversarial training.
Robust models tend to show clustering effect aligned with a) the class hierarchy of CIFAR-10, which is enhanced in d) our clustering adversarially trained model. and benign examples tend to present different patterns on activation [4]. In contrast, for those linear components, once the optimization is done and model evaluation mode is activated, they are shared and ﬁxed independently from inputs. Nowadays, the intrinsic relationship of adversary with linearity has been studied. Some ascribes the adversary to linear accumulation of perturbations from inputs to
ﬁnal outputs, dubbed ‘accidental steganography’ [13]. Some show that linear backward propagation enhances adversarial transferability [34, 14]. Nevertheless, such mentioned linearity analyses are all on the original non-linear models, which hinders a deeper understanding on linearity directly. Further essential exploration is encouraged.
⇥
In this paper, we thus study adversarial robustness on the statistical regularity of linear components.
We provide a novel insight on adversarial robustness by showing the clustering effect well aligned with class hierarchy. As shown in Fig. 1(d), given a data set, superclasses are deﬁned by grouping semantically similar subclasses together to form a hierarchy, e.g., in CIFAR-10, ‘Cat’ and ‘Dog’ represent different subclasses/ﬁne labels, despite both being the same superclasses/coarse labels
‘Animal’. Based on this, we conduct backward propagation on linearized sub-networks of robust and non-robust DNNs (through adversarial/standard training respectively) to extract the corresponding
Doutput linear matrix W , which implicit linear expression. In this fashion, we could obtain a Dinput ⇥ is extracted from inputs to predictive outputs. Dinput represents for the dimension of one input data, channel. Doutput represents for the dimension of one output speciﬁcally deﬁned as width height
⇥ vector, which is also the number of classes. We further study the correlation of vectors in W by matrix
Doutput. The details of W and C are introduced in Sec. 3.2. Then we
C, whose shape is Doutput ⇥
ﬁnd that robust models tend to show hierarchical clustering effect in correlation matrix C, which is surprisingly consistent with class hierarchy. For example, in Fig. 1 on CIFAR-10, the ﬁne labels (0,1,8,9) belong to ‘Non-animal’, others belong to ‘Animal’. The matrix C of robust model shows block clustering: values are close to ‘+1’ in the two superclasses (0,1,8,9) and (2,3,4,5,6,7), and values are close to ‘-1’ across these two superclasses. Such clustering effect is enhanced by our clustering regularization penalty in Fig. 1(d). This phenomenon could be semantically explained as subclasses from the same superclass share more feature similarities. Such hierarchical clustering effect can give a novel and insightful explanation on the superiority of robust models in some observed downstream tasks, as they can extract more semantic and representative features. To further conﬁrm our ﬁndings, we enhance the hierarchical clustering effect in both adversarial robustness and downstream tasks, e.g., domain adaption proposed in [29, 28]. The improvements of experimental results demonstrate that the superiority of robust models is closely related to their hierarchical classiﬁcation capability.
Our contributions are summarized as follows:
• To the best of our knowledge, we are the ﬁrst to systematically analyze the statistical regularity of adversarially robust models (through adversarial training) compared to non-robust models (through standard training) on their linearized sub-networks.
• We present an intriguing phenomenon of hierarchical clustering effect in robust models, and provide a novel yet insightful understanding of adversarial robustness. The clustering effect aligned with class hierarchy demonstrates more semantic and representative feature extraction capacity of robust models, which beneﬁts a lot in various tasks.
• Based on the observations, we propose a plugged-in hierarchical clustering training strategy to generally enhance adversarial robustness and investigate some intriguing adversarial attack
ﬁndings. Besides adversarial-related study, we further explore some downstream tasks with 2
the understanding of hierarchical clustering, e.g., domain adaption with subpopulation shift.
Experimental results show that the clustering effect and hierarchical classiﬁcation learned by robust model beneﬁts the task as well. 2