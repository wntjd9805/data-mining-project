Abstract
Semi-supervised learning (SSL) is an effective means to leverage unlabeled data to improve a model’s performance. Typical SSL methods like FixMatch assume that labeled and unlabeled data share the same label space. However, in practice, unlabeled data can contain categories unseen in the labeled set, i.e., outliers, which can signiﬁcantly harm the performance of SSL algorithms. To address this problem, we propose a novel Open-set Semi-Supervised Learning (OSSL) approach called
OpenMatch. Learning representations of inliers while rejecting outliers is essential for the success of OSSL. To this end, OpenMatch uniﬁes FixMatch with novelty detection based on one-vs-all (OVA) classiﬁers. The OVA-classiﬁer outputs the conﬁdence score of a sample being an inlier, providing a threshold to detect outliers. Another key contribution is an open-set soft-consistency regularization loss, which enhances the smoothness of the OVA-classiﬁer with respect to input transformations and greatly improves outlier detection. OpenMatch achieves state-of-the-art performance on three datasets, and even outperforms a fully supervised model in detecting outliers unseen in unlabeled data on CIFAR10. The code is available at https://github.com/VisionLearningGroup/OP_Match.

Introduction 1
Semi-supervised learning (SSL) leverages unlabeled data to improve a model’s performance [27, 38, 2, 1, 35, 22, 37]. An SSL model can propagate the class information of a small set of labeled data to a large set of unlabeled data, which signiﬁcantly improves the recognition accuracy without any additional annotation cost. A common assumption of SSL is that the label spaces of labeled and unlabeled data are identical, but, in practice, the assumption is easily violated. Depending on how it was collected, the unlabeled data may contain novel categories unseen in the labeled training data, i.e., outliers. Since these outliers can signiﬁcantly harm the performance of SSL algorithms [14], detecting them is necessary to make SSL more practical. Ideally, a model should classify samples of known categories i.e., inliers, into correct classes while identifying samples of novel categories as outliers. This task is called Open-set Semi-supervised Learning (OSSL) [44]. While OSSL is a more realistic and practical scenario than standard SSL, it has not been as widely explored.
Existing strong SSL methods [35, 41] do not work well for OSSL. For example, FixMatch [35] generates pseudo-labels using the model’s predictions on weakly augmented unlabeled images and trains the model to match its predictions on strongly augmented images with the pseudo-labels. This method exploits the advantage of pseudo-labeling as well as regularizing a model with the consistency between differently augmented images. But, in OSSL, this risks assigning pseudo-labels of known categories to outliers, which degrades the recognition accuracy. A possible solution is to compute the
SSL objective only for unlabeled samples considered to be inliers, where conﬁdence thresholding is used to pick inliers. For instance, MTC [44] regards some proportion of samples as outliers by using
Otsu thresholding [25]. However, this is not robust to varying proportions of outliers as discussed in 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: An illustration of our proposed open-set soft-consistency loss used to enhance outlier detection. Two differently augmented inputs are fed into the network to obtain the predictions of the outlier detector. The detector consists of one-vs-all classiﬁers and is able to detect outliers in an unsupervised way. The consistency loss is computed in a soft manner, i.e., without sharpening logits.
[44]. DS3L [14] proposes meta-optimization that attempts to pick unlabeled data useful to improve generalization performance. But, this method does not have an objective to separate inliers from outliers.
Given the limitations of the existing methods, we aim to learn representations that separate outliers from inliers in a feature space and a threshold effective for detecting outliers. The challenge is learning such representations with a small number of labeled inliers and no supervision to ﬁnd outliers. Moreover, choosing an accurate threshold value is not a trivial problem.
We propose a new framework, OpenMatch, to address the above drawbacks of OSSL. First, we propose to utilize a One-Vs-All (OVA) network [32] that can learn a threshold to distinguish outliers from inliers. A separate OVA-classiﬁer is trained for each class, and a sample is labeled an outlier if all of the classiﬁers determine it to be one. Thus, this technique allows us to identify outliers in an unsupervised way. We call this an outlier detector. Second, we propose a novel open-set soft-consistency loss to learn more effective representations for detecting outliers. We ﬁrst transform an unlabeled input in two ways and obtain two logits from the outlier detector. Then, we minimize the distance between the two logits to encourage consistency (See Fig. 1). The main difference between
SSL and OSSL is that unlabeled outliers do not have any neighboring labeled samples, which makes it risky to perform hard-labeling such as pseudo-labeling. The outlier detector outputs a distance from inliers given an input, and enhancing the smoothness of this function allows us to improve its ability to ﬁnd outliers. Empirically, this objective provides signiﬁcant improvements in detecting outliers.
Finally, to correctly classify inliers, we propose to apply FixMatch [35] to unlabeled samples that are considered to be inliers by the outlier detector.
The resulting framework shows consistent gains over baselines on various datasets and settings of
OSSL. For example, OpenMatch achieves a 10.4% error rate on CIFAR10 with 300 labeled examples compared to the previous state-of-the-art of 20.3%. Surprisingly, OpenMatch demonstrates good performance in detecting outliers unseen in unlabeled training data. For instance, in the experiments on CIFAR10 with 100 labels per class, OpenMatch achieves a 3.4% higher AUROC in detecting outliers than a supervised model trained with all training samples. To summarize, our contributions are as follows:
• A soft open-set consistency regularization (SOCR), to improve outlier detection in OSSL.
• A new framework, OpenMatch, which combines a OVA-classiﬁer, SOCR, and FixMatch.
• A new state-of-the-art in both correctly classifying inliers and detecting outliers, even when the outliers are unseen in unlabeled training data. 2