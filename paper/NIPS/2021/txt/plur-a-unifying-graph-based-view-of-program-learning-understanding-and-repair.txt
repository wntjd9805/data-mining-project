Abstract
Machine learning for understanding and editing source code has recently attracted signiﬁcant interest, with many developments in new models, new code represen-tations, and new tasks. This proliferation can appear disparate and disconnected, making each approach seemingly unique and incompatible, thus obscuring the core machine learning challenges and contributions. In this work, we demonstrate that the landscape can be signiﬁcantly simpliﬁed by taking a general approach of mapping a graph to a sequence of tokens and pointers. Our main result is to show that 16 recently published tasks of different shapes can be cast in this form, based on which a single model architecture achieves near or above state-of-the-art results on nearly all tasks, outperforming custom models like code2seq and alternative generic models like Transformers. This uniﬁcation further enables multi-task learning and a series of cross-cutting experiments about the importance of different modeling choices for code understanding and repair tasks. The full frame-work, called PLUR, is easily extensible to more tasks, and will be open-sourced (https://github.com/google-research/plur). 1

Introduction
The advent of sequence-to-sequence [Sutskever et al., 2014] and, more recently, text-to-text [Raffel et al., 2019, Radford et al., 2019] abstractions has provided a simplifying and unifying view of much work in natural-language processing. By casting problems in this framework, one can apply a single model architecture to many different problems. This beneﬁts machine learning (ML) researchers by focusing attention on the core modeling problem. It also ampliﬁes ML advances, because a general formulation allows practitioners to frame new problems in terms of a standardized abstraction and then to apply state-of-the-art architectures easily, which also facilitates multi-task and transfer learning.
In this paper, we ask: What is the equivalent unifying abstraction for machine learning for source code (ML4Code)? While many ML4Code tasks have been cast in terms of the sequence-to-sequence abstraction, source code is inherently different from natural language [Hindle et al., 2016]. It is
∗Work done during internship at Google.
†Work done during visiting-faculty appointment at Google. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
syntactically more structured as it is designed to be machine-readable and unambiguously parsable.
As such, it lends itself to more sophisticated automated analysis than natural language (e.g., static analysis). Furthermore, code tokens follow a power-law distribution due to many rare identiﬁers, and exhibit high rate of repetition across ﬁles and projects [Allamanis and Sutton, 2013, Casalnuovo et al., 2019]. In addition, the software engineering community recognizes that signiﬁcant effort goes into software maintenance involving small changes, rather than writing code from scratch [Koskinen, 2003]. These considerations have led to a wealth of literature going beyond text-to-text models that make use of graphs, copy mechanisms, pointers, and other custom architectures [Maddison and
Tarlow, 2014, Bielik et al., 2016, Allamanis et al., 2016, Mukherjee et al., 2017, Yin and Neubig, 2017, Allamanis et al., 2018, Alon et al., 2018, Dinella et al., 2020, Yasunaga and Liang, 2020, Nye et al., 2020].
In particular, the GRAPH2TOCOPO formulation from Tarlow et al. [2020] pairs a graph encoder with a Transformer-style decoder augmented with pointers and a copy mechanism and has been shown to be effective for certain ML4Code tasks. Our proposal is thus to unify around GRAPH2TOCOPO as a common vernacular for ML4Code. This formulation strictly generalizes text-to-text, by additionally supporting graph structures over the input when available, allowing the generation of both pointers and tokens in the output—pointers to specify the input location to edit and tokens to specify the content of the edit—and it has a built-in copy mechanism to more easily enable the re-use of rare tokens from the input. This ﬂexibility enables us to take 16 tasks and models from the ML4Code literature and convert them to a uniﬁed form. Using the same hyperparameter sweep and GRAPH2TOCOPO models, we achieve at or above state-of-the-art results on nearly all tasks, including improving over the Hoppity model [Dinella et al., 2020] for program repair, the GREAT model [Hellendoorn et al., 2020] for variable misuse, and the code2seq model [Alon et al., 2018] for variable naming.
Having uniﬁed a number of previous tasks and achieved strong results, we can then answer a number of empirical questions: Can we remove the graph structure as input and just use a Transformer encoder? How important are pointers and copy mechanisms? We can also inspect the differences between our model and previous ones and observe the importance of different design choices. The framework also makes it easy to experiment with alternative graph representations of code, and we demonstrate the effect of ﬁve different choices for converting a set of abstract syntax tree (AST) paths into graphs in the code2seq domain. Finally, we demonstrate that due to the uniﬁed representation, the framework makes it easy to do multi-task learning across classiﬁcation and repair tasks, yielding improved performance compared to training the same model separately on each task.
In summary, the resulting framework, called PLUR for Program Learning, Understanding, and
Repair, provides a powerful and simplifying perspective from which to interpret and build upon many recent ML4Code advances. Open-source code for the full PLUR framework will be available under an Apache 2 license at https://github.com/google-research/plur. 2