Abstract
Modelling the behaviours of other agents is essential for understanding how agents interact and making effective decisions. Existing methods for agent modelling commonly assume knowledge of the local observations and chosen actions of the modelled agents during execution. To eliminate this assumption, we extract representations from the local information of the controlled agent using encoder-decoder architectures. Using the observations and actions of the modelled agents during training, our models learn to extract representations about the modelled agents conditioned only on the local observations of the controlled agent. The representations are used to augment the controlled agent’s decision policy which is trained via deep reinforcement learning; thus, during execution, the policy does not require access to other agents’ information. We provide a comprehensive evaluation and ablations studies in cooperative, competitive and mixed multi-agent environments, showing that our method achieves higher returns than baseline methods which do not use the learned representations. 1

Introduction
An important aspect of autonomous decision-making agents is the ability to reason about the un-known intentions and behaviours of other agents. Much research has been devoted to this agent modelling problem [Albrecht and Stone, 2018], with recent works focused on learning informative representations about another agent’s policy using deep learning architectures for agent modelling and reinforcement learning (RL) [He et al., 2016, Raileanu et al., 2018, Grover et al., 2018, Rabinowitz et al., 2018, Zintgraf et al., 2021].
A common assumption in existing methods is that the modelling agent has access to the local trajectory of the modelled agents during execution [Albrecht and Stone, 2018], which may include their local observations of the environment state and their past actions. While it is certainly desirable to be able to observe the agents’ local contexts in order to reason about their past and future decisions, in practice such an assumption may be too restrictive. Agents may only have a limited view of their surroundings, communication with other agents may be infeasible or unreliable [Stone et al., 2010], and knowledge of the perception system of other agents may be unavailable [Gmytrasiewicz and
Doshi, 2005]. In such cases, an agent must reason with only locally available information.
We consider the following question: Can effective agent modelling be achieved using only the locally available information of the controlled agent during execution? A strength of deep learning techniques is their ability to identify informative features in data. Here, we use deep learning techniques to extract informative features about the trajectory of the modelled agent from a stream of local observations for the purpose of agent modelling. Speciﬁcally, we consider a multi-agent setting in which we control one agent which must learn to interact with a set of other agents. We assume a set of possible policies for the non-learning agents and that these policies are ﬁxed. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We propose Local Information Agent Modelling (LIAM)1, which can be seen as the general idea of learning the relationship between the trajectory of the controlled agent and the trajectory of the modelled agent. In this work we propose one instantiation of this idea: an encoder-decoder agent modelling method that can extract a compact yet informative representation of the modelled agents given only the local information of the controlled agent (its local state observations, and past actions).
The model is trained to replicate the observations and actions of the modelled agents from the local information only. During training, the modelled agent’s observations and actions are utilised as reconstruction targets for the decoder; after training, only the encoding component is retained which generates representations using local observations of the controlled agent. The learned representation conditions the policy of the controlled agent in addition to its local observation, and the policy and model are optimised concurrently during the RL learning process.
We evaluate LIAM in three different multi-agent environments: double speaker-listener [Mordatch and
Abbeel, 2017, Lowe et al., 2017], level-based foraging (LBF) [Albrecht and Stone, 2017, Papoudakis et al., 2021], and a modiﬁed version of predator-prey proposed by [Böhmer et al., 2020]. Our results support the idea that effective agent modelling can be achieved using only local information during execution: the same RL algorithm generally achieved higher average returns when combined with representations generated by LIAM than without, and in some cases the average returns are comparable to those achieved by an ideal baseline which has access to the modelled agent’s trajectory during execution. We also provide detailed evaluations of the learned encoder and decoder of LIAM as well as comparison with different instantiations of LIAM. 2