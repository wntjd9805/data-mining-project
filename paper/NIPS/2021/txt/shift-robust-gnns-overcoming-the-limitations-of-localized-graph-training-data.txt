Abstract
There has been a recent surge of interest in designing Graph Neural Networks (GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed that the nodes labeled for use in training were selected uniformly at random (i.e. are an IID sample). However in many real world scenarios gathering labels for graph nodes is both expensive and inherently biased – so this assumption can not be met. GNNs can suffer poor generalization when this occurs, by overﬁtting to superﬂuous regularities present in the training data. In this work we present a method, Shift-Robust GNN (SR-GNN), designed to account for distributional differences between biased training data and a graph’s true inference distribution.
SR-GNN adapts GNN models to the presence of distributional shift between the nodes labeled for training and the rest of the dataset. We illustrate the effectiveness of SR-GNN in a variety of experiments with biased training datasets on common
GNN benchmark datasets for semi-supervised learning, where we see that SR-GNN outperforms other GNN baselines in accuracy, addressing at least ∼40% of the negative effects introduced by biased training data. On the largest dataset we consider, ogb-arxiv, we observe a 2% absolute improvement over the baseline and are able to mitigate 30% of the negative effects from training data bias 1. 1

Introduction
The goal of graph-based semi-supervised learning (SSL) is to use relationships between data (its graph inductive bias), along with a small set of labeled items, to predict the labels for the rest of a dataset. Unsurprisingly, varying exactly which nodes are labeled can have a profound effect on the generalization capability of a SSL classiﬁer. Any bias in the sampling process to select nodes for training can create distributional differences between the training set and the rest of the graph. During inference any portion of the graph can be used, so any uneven labeling for training data can cause training and test data to have different distributions. An SSL classiﬁer may then overﬁt to training data irregularities, thus hurting the performance at inference time.
Recently, GNNs have emerged as a way to combine graph structure with deep neural networks.
Surprisingly, most work on semi-supervised learning using GNNs for node classiﬁcation [15, 11, 1] have ignored this critical problem, and even the most recently proposed GNN benchmarks [12] assume that an independent and identically distributed (IID) sample is possible for training labels. 1Code and processed data are available at https://github.com/GentleZhu/Shift-Robust-GNNs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Online.
This problem of biased training labels can be quite pronounced when GNNs are applied for semi-supervised learning in practice. It commonly happens when the size of the dataset is so large that only a subset of it can afford to be labeled – the exact situation where graph-based SSL is supposed to have a value proposition! While the speciﬁc source of bias can vary, we have encountered it in many different settings. For example, sometimes ﬁxed heuristics are used to select a subset of data (which shares some characteristics) for labeling. Other times, human analysts individually choose data items for labeling, using complex domain knowledge. However, even this can be rooted in shared characteristics of data. In yet another scenario, a label source may have some latency, causing a temporal mismatch between the distribution of data at time of labeling and at the time of inference.
In all of these cases, the core problem is that the GNN overﬁts to spurious regularities as the subset of labeled data could not be created in an IID manner.
One particular area where this can apply is in the spam and abuse domain, a common area of application for GNNs [18, 10, 33]. However, the labels in these problems usually come from explicit human annotations, which are both sparse (as human labelling is expensive), and also frequently biased. Since spam and abuse problems typically have very imbalanced label distributions (e.g. in many problems there are relatively few abusers – typically less than 1:100), labeling nodes IID results in discovering very few abusive labels. In this case choosing the points to request labels for in an IID manner is simply not a feasible option if one wants to have a reasonable number of data items from the rare class.
In this paper, we seek to quantify and address the problem of localized training data in Graph Neural
Networks. We frame the problem as that of transfer learning – seeking to transfer the model’s performance from a small biased portion of the graph to the entire graph itself. Our proposed framework for addressing this problem, Shift-Robust GNN (SR-GNN), strives to adapt a biased sample of labeled nodes to more closely conform to the distributional characteristics present in an
IID sample of the graph. It can handle two kinds of bias that occur in both deeper GNNs and more recent linearized (shallow) versions of these models.
First we consider the case of addressing distributional shift for standard GNN models such as
GCNs [15], MPNNs [7], and many more [5]. These models create deep networks which iteratively convolve information over graph structure. SR-GNN addresses this variety of distributional shift via a regularization over the hidden layers of the network. Second, we consider a class of linearized models (APPNP [16], SimpleGCN [34], etc) which decouple GNNs into non-linear feature encoding and linear message passing. These models present an interesting challenge for debiasing, as the graph can introduce bias over the features after all learnable layers. In cases like this, SR-GNN can use an instance reweighting paradigm to ensure that the training examples are as representative as possible over the graph data.
We illustrate the effectiveness of our proposed method on both paradigms with an experimental framework that introduces bias to the train/test split in a GNN, which lets us simulate the ‘localized discovery’ pattern observed in real applications on fully-labeled academic datasets. With these experiments we show that that our method SR-GNN can recover at least 40% of the performance lost when training a GCN on the same biased input.
Speciﬁcally, our contributions are the following: 1. We provide the ﬁrst focused discussion on the distributional shift problem in GNNs. 2. We propose generalized framework, Shift-Robust GNN (SR-GNN), which can address shift in both shallow and deep GNNs. 3. We create an experimental framework which allows for creating biased train/test sets for graph learning datasets. 4. We run extensive experiments and analyze the results, proving that our methods can mitigate distributional shift. 2