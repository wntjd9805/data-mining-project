Abstract
Modularity is a compelling solution to continual learning (CL), the problem of modeling sequences of related tasks. Learning and then composing modules to solve different tasks provides an abstraction to address the principal challenges of
CL including catastrophic forgetting, backward and forward transfer across tasks, and sub-linear model growth. We introduce local module composition (LMC), an approach to modular CL where each module is provided a local structural component that estimates a module’s relevance to the input. Dynamic module com-position is performed layer-wise based on local relevance scores. We demonstrate that agnosticity to task identities (IDs) arises from (local) structural learning that is module-speciﬁc as opposed to the task- and/or model-speciﬁc as in previous works, making LMC applicable to more CL settings compared to previous works. In addi-tion, LMC also tracks statistics about the input distribution and adds new modules when outlier samples are detected. In the ﬁrst set of experiments, LMC performs favorably compared to existing methods on the recent Continual Transfer-learning
Benchmark without requiring task identities. In another study, we show that the locality of structural learning allows LMC to interpolate to related but unseen tasks (OOD), as well as to compose modular networks trained independently on different task sequences into a third modular network without any ﬁne-tuning. Finally, in search for limitations of LMC we study it on more challenging sequences of 30 and 100 tasks, demonstrating that local module selection becomes much more challenging in presence of a large number of candidate modules. In this setting best performing LMC spawns much fewer modules compared to an oracle based baseline, however it reaches a lower overall accuracy. The codebase is available under https://github.com/oleksost/LMC. 1

Introduction
The goal of continual learning (CL) is to learn efﬁciently from a non-stationary stream of tasks without (catastrophically) forgetting previous tasks [62]. CL is often modeled as a trade-off between knowledge retention (stability) and knowledge expansion (plasticity) [26, 64]. Parameter sharing can provide control over this trade-off. For example, learning a single model shared across tasks results in better knowledge transfer and faster learning at the expense of forgetting [46, 57]. Conversely, learning a separate model per task eliminates forgetting but minimizes transfer and data efﬁciency [2, 41].
Modular learning aims at balancing transfer and forgetting by learning a set of specialized modules that can be recomposed to solve (new) tasks while only updating a subset of relevant modules or adding new modules [6, 47, 27]. In principle, a modular learner capable of composing modules in meaningful structures can provide additional beneﬁts including (i) computational gains due to only executing modules that are relevant to a task [47, 4]; (ii) memory gains due to instantiating a sub-linear number of modules w.r.t. the number of tasks; (iii) systematic [8] and out-of-distribution (OOD) generalization [18] through knowledge recombination; and (iv) biological plausibility [91, 90, 96]. corresponding author: oleksiy.ostapenko@t-online.de 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Designing modular methods for CL comes with two main challenges. The ﬁrst is how and when to add new modules to ensure sufﬁcient plasticity to learn new tasks. Existing modular methods use greedy search variants, expanding the model when it improves validation performance [92, 63]. The second challenge is how to compose that is, retrieve task-speciﬁc structural knowledge given a new task (previously seen or not).
Existing methods rely on a task’s identiﬁer (ID) to retrieve task-speciﬁc structural knowledge, which comes either in the form of an optimal module layout [92] or as a model- and task-speciﬁc controller network that generates modular layouts [63]. Unfortunately, in many realistic CL scenarios task identities are unavailable at test time [23, 35, 14]. Lifting this limitation is challenging since standard mechanisms for task inference, for example, leveraging a task-inference model, could be subject to forgetting themselves.
To address both challenges, we equip each module with a local structural component that predicts a score indicating how relevant the module is for a given input. In-distribution inputs result in high scores, while out-of-distribution inputs result in low scores. In other words, modules self-determine their relevance given an input.
This local component is used for composing modules: for each datum, modules are combined at each layer according to their normalized scores without requiring a task’s ID (§3). The local component is also used for module expansion: a new module is instantiated if all the current modules ﬂag their input as being locally out-of-distribution (§3.1). Further, new shallow modules (i.e. closer to the input) are
ﬁrst trained in a projection phase to maximize the relatedness scores of subsequent, deeper, modules (§3.2). This process projects the output of new modules into the representation space expected by the subsequent modules and ensures the compatibility between low- and high-level modules.
In a set of studies, we explore the performance and versatility of our local structural approach, which we call Local Module Composer (LMC). First, we show that LMC reaches superior or comparable performance to existing modular and non-modular methods without requiring task IDs at test time using the Continual Transfer Learning (CTrL) benchmark, designed to evaluate transfer and forgetting in CL [92] (§4.1). Then, we demonstrate how LMC, relying on its projection phase, can solve out-of-distribution (OOD) tasks not seen during the continual training (§4.2). We also show it is possible to combine modules from independently trained models into a new model to solve tasks seen by each of the independent models without any ﬁnetuning (§4.3). Finally, an analysis of longer task sequences (30 and 100 tasks) reveals that LMC tends to spawn much fewer modules to reach good performance than the fully task-aware MNTDP [92] counterpart. However, LMC reaches slightly lower accuracy on longer sequences than MNTDP, which highlights the difﬁculty of automatic task-ID agnostic module selection in the presence of a large number of candidate modules. In
Appendix F we demonstrate the applicability of LMC in the meta-continual learning (meta-CL) setting, a task-agnostic setting by nature.
We highlight that by relying on a local (per-module) structural component, LMC offers a modular
CL approach that i) does not require task IDs during test in the standard task incremental settings; ii) balances parameter sharing to yield strong CL performances compared to baselines that require access to the task ID; iii) in our experiments instantiates a sub-linear number of modules; iv) permits recombination of modules at test time enabling OOD generalization as well as (v) the ability to combine independently trained models in a third model without ﬁne-tuning. Notably, the OOD generalization is only possible if the agent is task-agnostic in the module selection process, since
OOD tasks were not observed at training, the learner has to interpolate between the learned tasks, and a (categorial) task ID is of no use. 2