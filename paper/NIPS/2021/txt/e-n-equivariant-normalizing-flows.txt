Abstract
This paper introduces a generative model equivariant to Euclidean symmetries:
E(n) Equivariant Normalizing Flows (E-NFs). To construct E-NFs, we take the discriminative E(n) graph neural networks and integrate them as a differential equation to obtain an invertible equivariant function: a continuous-time normalizing
ﬂow. We demonstrate that E-NFs considerably outperform baselines and existing methods from the literature on particle systems such as DW4 and LJ13, and on molecules from QM9 in terms of log-likelihood. To the best of our knowledge, this is the ﬁrst ﬂow that jointly generates molecule features and positions in 3D. 1

Introduction
Leveraging the structure of the data has long been a core design principle for building neural networks. Convolutional layers for images famously do so by being translation equivariant and therefore incorporating the symmetries of a pixel grid. Analogously, for discriminatory machine learning tasks on 3D coordinate data, taking into account the symmetries of data has signiﬁcantly improved performance (Thomas et al., 2018; Anderson et al., 2019; Finzi et al., 2020; Fuchs et al., 2020; Klicpera et al., 2020). One might say that equivariance has been proven an effective tool to build inductive bias into the network about the concept of 3D coordinates. However, for generative tasks, e.g., sampling new molecular structures, the development of efﬁcient yet powerful rotation equivariant approaches—while having made great progress—is still in its infancy.
A recent method called E(n) Equivariant Graph Neural Networks (EGNNs) (Satorras et al., 2021) is both computationally cheap and effective in regression and classiﬁcation tasks for molecular data, while being equivariant to Euclidean symmetries. However, this model is only able to discriminate features on nodes, and cannot generate new molecular structures.
In this paper, we introduce E(n) Equivariant Normalizing Flows (E-NFs): A generative model for
E(n) Equivariant data such as molecules in 3D. To construct E-NFs we parametrize a continuous-time
ﬂow, where the ﬁrst-order derivative is modelled by an EGNN. We adapt EGNNs so that they are stable when utilized as a derivative. In addition, we use recent advances in the dequantization literature to lift the discrete features of nodes to a continuous space. We show that our proposed
ﬂow model signiﬁcantly outperforms its non-equivariant variants and previous equivariant generative methods (Köhler et al., 2020). Additionally, we apply our method to molecule generation and we show that our method is able to generate realistic molecules when trained on the QM9 dataset.
⇤Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of our method in the sampling direction. An equivariant invertible function g✓ has learned to map samples from a Gaussian distribution to molecules in 3D, described by x, h. 2