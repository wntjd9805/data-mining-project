Abstract
Recently many algorithms were devised for reinforcement learning (RL) with function approximation. While they have clear algorithmic distinctions, they also have many implementation differences that are algorithm-independent and some-times under-emphasized. Such mixing of algorithmic novelty and implementation craftsmanship makes rigorous analyses of the sources of performance improve-ments across algorithms difﬁcult. In this work, we focus on a series of off-policy inference-based actor-critic algorithms – MPO, AWR, and SAC – to decouple their algorithmic innovations and implementation decisions. We present uniﬁed derivations through a single control-as-inference objective, where we can catego-rize each algorithm as based on either Expectation-Maximization (EM) or direct
Kullback-Leibler (KL) divergence minimization and treat the rest of speciﬁcations as implementation details. We performed extensive ablation studies, and identiﬁed substantial performance drops whenever implementation details are mismatched for algorithmic choices. These results show which implementation or code details are co-adapted and co-evolved with algorithms, and which are transferable across algorithms: as examples, we identiﬁed that tanh Gaussian policy and network sizes are highly adapted to algorithmic types, while layer normalization and ELU are critical for MPO’s performances but also transfer to noticeable gains in SAC. We hope our work can inspire future work to further demystify sources of performance improvements across multiple algorithms and allow researchers to build on one another’s both algorithmic and implementational innovations.1 1

Introduction
Deep reinforcement learning (RL) has achieved huge empirical successes in both continuous [36, 18] and discrete [39, 26] problem settings with on-policy [52, 54] and off-policy [13, 22, 23] algorithms.
Especially in the continuous control domain, interpreting RL as probabilistic inference [60, 61] has yielded many kinds of algorithms with strong empirical performances [34, 50, 51, 12, 29, 21, 16, 24].
Recently, there has been a series of off-policy algorithms derived from this perspective for learning policies with function approximations [2, 22, 48]. Notably, Soft Actor Critic (SAC) [22, 23], based on a maximum entropy objective and soft Q-function, signiﬁcantly outperforms on-policy [52, 54] and off-policy [36, 18, 13] methods. Maximum a posteriori Policy Optimisation (MPO) [2], 1The implementation is available at https://github.com/frt03/inference-based-rl. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
inspired by REPS [50], employs a pseudo-likelihood objective, and achieves high sample-efﬁciency and fast convergence compared to the variety of policy gradient methods [36, 54, 7]. Similar to
MPO, Advantage Weighted Regression (AWR) [48], and its variant, Advantage Weighted Actor-Critic (AWAC) [42], also employ the pseudo-likelihood objective weighted by the exponential of advantages, and reports more stable performance than baselines both in online and ofﬂine [35] settings.
While these inference-based algorithms have similar derivations to each other, their empirical perfor-mances have large gaps among them when evaluated on the standard continuous control benchmarks, such as OpenAI Gym [6] or DeepMind Control Suite [58]. Critically, as each algorithm has unique low-level implementation or code design decisions – such as value estimation techniques, action distribution for the policy, and network architectures – aside from high-level algorithmic choices, it is difﬁcult to exactly identify the causes of these performance gaps as algorithmic or implementational.
In this paper, we ﬁrst derive MPO, AWR, and SAC from a single objective function, through either
Expectation-Maximization (EM) or KL minimization, mathematically clarifying the algorithmic connections among the recent state-of-the-art off-policy actor-critic algorithms. This uniﬁed deriva-tion allows us to precisely identify implementation techniques and code details for each algorithm, which are residual design choices in each method that are generally transferable to other algorithms.
To reveal the sources of the performance gaps, we experiment with carefully-selected ablations of these identiﬁed implementation techniques and code details, such as tanh-squashed Gaussian policy, clipped double Q-learning [13], and network architectures. Speciﬁcally, we keep the high-level algorithmic designs while normalizing the implementation designs, enabling proper algorithmic com-parisons. Our empirical results successfully distinguish between highly co-adapted design choices and no-co-adapted ones2. We identiﬁed that clipped double Q-learning and tanh-squashed policies, the sources of SoTA performance of SAC, are highly co-adapted, speciﬁc to KL-minimization-based method, SAC, and difﬁcult to transfer and beneﬁt in EM-based methods, MPO or AWR. In contrast, we discover that ELU [8] and layer normalization [4], the sources of SoTA performance of MPO, are transferable choices from MPO that also signiﬁcantly beneﬁt SAC. We hope our work can inspire more future works to precisely decouple algorithmic innovations from implementation or code details, which allows exact sources of performance gains to be identiﬁed and algorithmic researches to better build on one another. 2