Abstract
We study the problem of active learning with the added twist that the learner is assisted by a helpful teacher. We consider the following natural interaction protocol:
At each round, the learner proposes a query asking for the label of an instance xq, the teacher provides the requested label {xq, yq} along with explanatory informa-tion to guide the learning process. In this paper, we view this information in the form of an additional contrastive example ({xc, yc}) where xc is picked from a set constrained by xq (e.g., dissimilar instances with the same label). Our focus is to design a teaching algorithm that can provide an informative sequence of contrastive examples to the learner to speed up the learning process. We show that this leads to a challenging sequence optimization problem where the algorithm’s choices at a given round depend on the history of interactions. We investigate an efﬁcient teaching algorithm that adaptively picks these contrastive examples. We derive strong performance guarantees for our algorithm based on two problem-dependent parameters and further show that for speciﬁc types of active learners (e.g., a gen-eralized binary search learner), the proposed teaching algorithm exhibits strong approximation guarantees. Finally, we illustrate our bounds and demonstrate the effectiveness of our teaching framework via two numerical case studies. 1

Introduction
Active learning characterizes a natural learning paradigm where a learner actively engages in the learning process, and has demonstrated great success in both the educational domain [Prince, 2004] and the machine learning literature [Settles, 2012]. In the context of machine learning, active learning has traditionally been studied as the active instance labeling problem, where the goal of an active learner is to learn a concept by selectively querying the labels of a sequence of data points [Kosaraju et al., 1999; Dasgupta, 2004; Nowak, 2008; Balcan et al., 2006; Gonen et al., 2013; Hanneke and Yang, 2014; Yan and Zhang, 2017]. Recently, active learning has been investigated under richer interaction protocols, including learning from feature feedback [Poulis and Dasgupta, 2017; Dasgupta and
Sabato, 2020] or from demonstrations [Chernova and Thomaz, 2014; Silver et al., 2012]. Typically, these works assume that the rich feedback obtained from the teacher is adversarial [Dasgupta and
Sabato, 2020] or stochastic [Poulis and Dasgupta, 2017]. While these results have shown promising improvements in sample complexity by leveraging more expressive feedback, the assumptions of an adversarial or stochastic teacher could fall short in characterizing many real-world interactive learning systems (e.g. automated tutoring or multi-agent learning systems) where the teacher cooperates with the learner to achieve the learning objective [Zilles et al., 2011; Zhu et al., 2018].
Motivated by real-world cooperative learning and teaching scenarios [Chen et al., 2018a; Teso and
Kersting, 2019], we consider an optimistic yet natural interaction protocol between an active learner and a helpful teacher (see Figure 1 (c)): At each round, the learner proposes a query asking for the label of an instance xq; the teacher then provides the requested label {xq, yq} along with explanatory 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The difference between (a) active learning, (b) classic machine teaching and (c) our problem. information to guide the learning process. In this paper, we consider this information in the form of an additional contrastive example ({xc, yc}) where xc is picked from a set constrained by xq (e.g., dissimilar instances with the same label, or similar instances with opposite labels). In contrast to existing work in active learning with contrastive examples [Abe et al., 2006; Liang et al., 2020; Ma et al., 2021] or with comparison queries [Jamieson and Nowak, 2011; Kane et al., 2017] which focus on developing active learning algorithms, we assume that the learner’s querying and learning strategy is ﬁxed and known to the teacher, and focus on designing a teaching algorithm that can provide an informative sequence of contrastive examples to the learner to speed up the learning process.
Note that in the absence of the learner’s query, this problem reduces to the classical machine teaching problem [Goldman and Kearns, 1995]. When teaching a (passive) version space learner [Goldman and Kearns, 1995], classical machine teaching can be viewed as a set cover problem, and the greedy algorithm is known to compute a cover that is within a logarithmic factor of the minimum cover
[Chvatal, 1979]. However, as we show in section 3.3, an active learner actively trying to reduce the label cost could actually hurt the performance of a greedy teacher.
Given an active version space learner speciﬁed by an acquisition function, we show that the design of an optimal teaching algorithm induces a challenging sequence optimization problem, where the teacher’s available choices of constrastive examples at a given round depend on the history of interactions. We study the greedy teacher that adaptively picks contrastive examples in response to the learner’s queries. Speciﬁcally, we introduce several natural problem-dependent parameters to characterize (1) the constraints imposed by the active learner and (2) the structure of the coverage function resulting from the hypothesis space. Based on these characterizations, we explore the theoretical properties for the greedy teacher, and demonstrate the power and limitations of the interactive teaching framework. Our main technical contributions are summarized are:
I. We consider a novel interactive protocol for teaching an active learner, and derive a general performance guarantee for the greedy teacher, regardless of the choice of acquisition function by the active learner. Our result generalizes existing near-optimality guarantees for the greedy teacher under the classical teaching [Goldman and Kearns, 1995] to the more challenging sequence optimization problem (section 3).
II. We then show that, perhaps surprisingly, a greedy teacher may fail badly when interacting with an active learner compared to the optimal teacher, due to the constrained choices of contrastive examples imposed by the learner’s query (section 3). In fact, there exist pessimistic scenarios where an active learner alone is better off without the help of a greedy teacher (section 4).
III. When interacting with a generalized binary search (GBS) learner [Dasgupta, 2004; Nowak, 2008], we provide a strong performance guarantee for the greedy teacher, and show that it cannot perform arbitrarily badly (section 4). Our bound establishes a rigorous connection between the problem-dependent parameters introduced in section 3 to the classical k-neighborly condition and coherence parameter which are widely used in the active learning literature
[Nowak, 2008, 2011; Mussmann and Liang, 2018].
IV. We empirically illustrate our bounds via two experiments. These examples illustrate the intuitive sequential teaching process and demonstrate the effectiveness of our teaching framework. 2
1.1