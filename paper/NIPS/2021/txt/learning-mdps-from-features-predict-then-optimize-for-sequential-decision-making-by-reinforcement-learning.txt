Abstract
In the predict-then-optimize framework, the objective is to train a predictive model, mapping from environment features to parameters of an optimization problem, which maximizes decision quality when the optimization is subsequently solved.
Recent work on decision-focused learning shows that embedding the optimization problem in the training pipeline can improve decision quality and help generalize better to unseen tasks compared to relying on an intermediate loss function for evaluating prediction quality. We study the predict-then-optimize framework in the context of sequential decision problems (formulated as MDPs) that are solved via reinforcement learning. In particular, we are given environment features and a set of trajectories from training MDPs, which we use to train a predictive model that generalizes to unseen test MDPs without trajectories. Two signiﬁcant computa-tional challenges arise in applying decision-focused learning to MDPs: (i) large state and action spaces make it infeasible for existing techniques to differentiate through MDP problems, and (ii) the high-dimensional policy space, as parameter-ized by a neural network, makes differentiating through a policy expensive. We resolve the ﬁrst challenge by sampling provably unbiased derivatives to approxi-mate and differentiate through optimality conditions, and the second challenge by using a low-rank approximation to the high-dimensional sample-based derivatives.
We implement both Bellman–based and policy gradient–based decision-focused learning on three different MDP problems with missing parameters, and show that decision-focused learning performs better in generalization to unseen tasks. 1

Introduction
Predict-then-optimize [5, 9] is a framework for solving optimization problems with unknown parame-ters. Given such a problem, we ﬁrst train a predictive model to predict the missing parameters from problem features. Our objective is to maximize the resulting decision quality when the optimization problem is subsequently solved with the predicted parameters [25, 28]. Recent work on the decision-focused learning approach [7, 36] embeds the optimization problem [1, 3, 4] into the training pipeline and trains the predictive model end-to-end to optimize the ﬁnal decision quality. Compared with a
∗This work was done while the author was at Harvard University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
more traditional “two-stage” approach which maximizes the predictive accuracy of the model (rather than the ﬁnal decision quality), the decision-focused learning approach can achieve a higher solution quality and generalize better to unseen tasks.
This paper studies the predict-then-optimize framework in sequential decision problems, formulated as Markov decision processes (MDPs), with unknown parameters. In particular, at training time, we are given trajectories and environment features from “training MDPs.” Our goal is to learn a predictive model which maps from environment features to missing parameters based on these trajectories that generalizes to unseen test MDPs that have features, but not trajectories. The resulting “predicted” training and test MDPs are solved using deep reinforcement learning (RL) algorithms, yielding policies that are then evaluated by ofﬂine off-policy evaluation (OPE) as shown in Figure 1. This fully ofﬂine setting is motivated by real-world applications such as wildlife conservation and tuberculosis treatment where no simulator is available. However, such domains offer past ranger patrol trajectories and environmental features of individual locations from conservation parks for generalization to other unpatrolled areas. These settings differ from those considered in transfer-RL [21, 24, 29, 31] and meta-RL [8, 10, 33, 35, 40] because we generalize across different MDPs by explicitly predicting the mapping function from features to missing MDPs parameters, while transfer/meta RL achieve generalization by learning hidden representation of different MDPs implicitly with trajectories.
The main contribution of this paper is to extend the decision-focused learning approach to MDPs with unknown parameters, embedding the MDP problems in the predictive model training pipeline. To perform this embedding, we study two common types of optimality conditions in MDPs: a Bellman-based approach where mean-squared Bellman error is minimized, and a policy gradient-based approach where the expected cumulative reward is maximized. We convert these optimality conditions into their corresponding Karush–Kuhn–Tucker (KKT) conditions, where we can backpropagate through the embedding by differentiating through the KKT conditions. However, existing techniques from decision-focused learning and differentiating through KKT conditions do not directly apply as the size of the KKT conditions of sequential decision problems grow linearly in the number of states and actions, which are often combinatorial or continuous and thus become intractable.
We identify and resolve two computational challenges in applying decision-focused learning to
MDPs, that come up in both optimality conditions: (i) the large state and action spaces involved in the optimization reformulation make differentiating through the optimality conditions intractable and (ii) the high-dimensional policy space in MDPs, as parameterized by a neural network, makes differentiating through a policy expensive. To resolve the ﬁrst challenge, we propose to sample an estimate of the ﬁrst-order and second-order derivatives to approximate the optimality and KKT conditions. We prove such a sampling approach is unbiased for both types of optimality conditions.
Thus, we can differentiate through the approximate KKT conditions formed by sample-based deriva-tives. Nonetheless, the second challenge still applies—the sampled KKT conditions are expensive to differentiate through due to the dimensionality of the policy space when model-free deep RL methods are used. Therefore, we propose to use a low-rank approximation to further approximate the sample-based second-order derivatives. This low-rank approximation reduces both the computation cost and the memory usage of differentiating through KKT conditions.
We empirically test our decision-focused algorithms on three settings: a grid world with unknown rewards, and snare-ﬁnding and Tuberculosis treatment problems where transition probabilities are unknown. Decision-focused learning achieves better OPE performance in unseen test MDPs than two-stage approach, and our low-rank approximations signiﬁcantly scale-up decision-focused learning.