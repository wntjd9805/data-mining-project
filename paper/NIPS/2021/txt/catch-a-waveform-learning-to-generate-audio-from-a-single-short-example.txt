Abstract
Models for audio generation are typically trained on hours of recordings. Here, we illustrate that capturing the essence of an audio source is typically possible from as little as a few tens of seconds from a single training signal. Speciﬁcally, we present a GAN-based generative model that can be trained on one short audio signal from any domain (e.g. speech, music, etc.) and does not require pre-training or any other form of external supervision. Once trained, our model can generate random samples of arbitrary duration that maintain semantic similarity to the training waveform, yet exhibit new compositions of its audio primitives. This enables a long line of interesting applications, including generating new jazz improvisations or new a-cappella rap variants based on a single short example, producing coherent modiﬁcations to famous songs (e.g. adding a new verse to a Beatles song based solely on the original recording), ﬁlling-in of missing parts (inpainting), extending the bandwidth of a speech signal (super-resolution), and enhancing old recordings without access to any clean training example. We show that in most cases, no more than 20 seconds of training audio sufﬁce for our model to achieve state-of-the-art results. This is despite its complete lack of prior knowledge about the nature of audio signals in general. 1

Introduction
In recent years, deep models for audio generation have had an immense impact on a wide range of applications, including text-to-speech synthesis [12, 38, 15, 7], voice-to-voice translation [8, 53], music generation [33, 11], singing voice conversion [8, 53], timbre transfer [16, 40], bandwidth-extension [29, 6], and audio inpainting [37]. Existing generative models require large datasets of training signals from the domain of interest. However, there are practical scenarios in which such datasets are extremely hard to collect, or are even nonexistent. Examples include a speaker that has only recorded a few sentences, an artist that had the chance to record only a few songs, or a unique jazz improvisation appearing in one particular recording. A natural question to ask, then, is whether large amounts of training data are a necessity for training a generative model.
Here, we take this question to the extreme. We illustrate that capturing the essence of an audio source is possible from as little as a few tens of seconds from a single training recording. Speciﬁcally, we present a generative adversarial network (GAN) based model that can be trained on one short raw waveform and does not require pre-training or any other type of external supervision 1. Once the 1code is available at https://github.com/galgreshler/Catch-A-Waveform 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Single training example (20 sec.)
Random sample (60 sec.)
→
→
Figure 1: Catch-A-Waveform. We present a generative model that is able to capture the statistics of a single short audio recording (20 seconds in these examples). At inference, it can generate new diverse samples of arbitrary length, that exhibit new interesting compositions. The ﬁgure illustrates generation of new jazz improvisations and new freestyle rap variants. All examples can be listened to in our website. model is trained, it is capable of generating diverse new signals that are semantically similar to the training recording, but contain new compositions and structures. Our model can handle different types of audio signals, from instrumental music to speech. For example, after training on 20 seconds of a saxophone solo, our model is able to generate new similar improvisations. The same can be done with a-capella rap, or old famous speeches, as exempliﬁed in Fig.1. Our model can also generate samples conditioned on the low frequencies of some signal (be it the training signal or a similar one).
This constraints the global structure of the generated signals, allowing to generate e.g. new versions of a Beatles song (all audio samples mentioned in the paper can be found in our website).
It is important to note that a short snippet of an audio signal is insufﬁcient for learning language (for speech) or rules of harmony (for music). Therefore, our generated signals lack the linguistic semantics or long-range harmonic structure that can be potentially achieved with externally-trained models.
However, surprisingly, the coherence of our generated signals over short time scales, typically sufﬁces for confusing listeners to believe they are real, as we conﬁrm through extensive user studies.
Besides generating random samples, we illustrate the utility of our approach in the common tasks of bandwidth extension, inpainting and denoising (see Fig. 2). We show that in the latter two tasks, no training signal whatsoever is required beyond the input itself. This allows handling sources for which no training data exist, like old recordings of famous musicians. In fact, our evaluation suggests that for the tasks of bandwidth extension and inpainting (sections 4.3 and 4.4), limiting the training to a single short signal is actually beneﬁcial, and can lead to results that outperform models trained on hours of recordings.
Our work is inspired by generative models for visual data, which have been recently explored in the context of learning from a single image [50, 52] or a single short video [20]. Similarly to those works, we present a multi-scale GAN architecture that generates signals in their raw (time domain) representation. Audio signals, however, are very different from visual data; they are of high temporal resolution (usually at least 16,000 samples per second), they exhibit correlations at very long timescales, and they have diverse frequency contents. As we discuss, this necessitates dedicated architectures, losses, and adaptive selection of the multi-scale pyramid levels. 2