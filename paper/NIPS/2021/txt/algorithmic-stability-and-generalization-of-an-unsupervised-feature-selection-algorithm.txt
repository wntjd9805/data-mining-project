Abstract
Feature selection, as a vital dimension reduction technique, reduces data dimension by identifying an essential subset of input features, which can facilitate interpretable insights into learning and inference processes. Algorithmic stability is a key characteristic of an algorithm regarding its sensitivity to perturbations of input samples. In this paper, we propose an innovative unsupervised feature selection algorithm attaining this stability with provable guarantees. The architecture of our algorithm consists of a feature scorer and a feature selector. The scorer trains a neural network (NN) to globally score all the features, and the selector adopts a dependent sub-NN to locally evaluate the representation abilities for selecting features. Further, we present algorithmic stability analysis and show that our algorithm has a performance guarantee via a generalization error bound. Extensive experimental results on real-world datasets demonstrate superior generalization performance of our proposed algorithm to strong baseline methods. Also, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically conﬁrmed. 1

Introduction
High-dimensional data is challenging due to the curse of dimensionality [7]. Dimensionality reduction is an important technique for dealing with such data, comprising two typical approaches: feature extraction and feature selection. The former, including principal component analysis (PCA) [35] and autoencoder (AE) [41, 5], is widely used in various ﬁelds such as biology [3] and computer vision [49, 18, 38]. Nonetheless, new features produced by feature extraction form a new space and, in general, do not have a direct correspondence to original features, leading to difﬁculty in deriving interpretable insights for the domain problems, such as biomarker identiﬁcation and drug discovery.
Alternatively, feature selection identiﬁes essential features from the original feature space, providing critical interpretations and insights in many tasks [10, 45], e.g., gene functional enrichment analysis, biomarker detection, and high-throughput screening for drug discovery [22, 47, 27].
Stability is an important characteristic of an algorithm, which quantiﬁes the sensitivity of the output to the perturbation of its training samples [12, 19]. The current stability analyses of feature selection algorithms mainly focus on similarity-based and frequency-based stability measures for practical assessment, e.g., [16, 21, 34], rather than the algorithms themselves, leaving feature selection with proper algorithmic stability [8] an unmet need. Especially, existing unsupervised feature selection algorithms use the empirical error, or its proxy, to represent generalization error but provide no theoretical guarantee of such use, leaving their ability to generalize to new data unclear.
To address these issues, in this paper we propose a novel unsupervised feature selection algorithm with a proven algorithmic stability guarantee. Our approach trains a neural network (NN) to globally score all features and a dependent sub-NN to select the features with the highest scores to reconstruct the
∗Correspondence should be addressed to: qiang.cheng@uky.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
original data. Mathematically, we analyze our new algorithm and provide a generalization error upper bound, ensuring its algorithmic stability for guaranteed learning performance. Our contributions are summarized in the following:
• We propose an innovative approach for feature selection. It constructs a NN with a feature scorer to globally score all the features and a dependent sub-NN with a feature selector to locally evaluate the representation ability of highly scored features. Thus, our algorithm is capable of both globally exploring and locally excavating essential features.
• We establish performance guarantees for our algorithm, including a proven conver-λ1, λ1}(cid:1) + 1/n(cid:1) to ensure uniform stability and a rate n(cid:1) for generalization error. Here, n is the number of gence rate O (cid:0)1/ (cid:0)n min{
O (cid:0)1/ (cid:0)√
λ1, λ1}(cid:1) + 1/ n min{ training samples, and λ1 is a regularization parameter.
√
√
√
• We conﬁrm the effectiveness of our proposed algorithm with extensive experiments on 10 real datasets. It achieves more competitive performance for data reconstruction and down-stream classiﬁcation tasks than state-of-the-art methods. Notably, the features selected by our algorithm have performance comparable to the original features. Further, the properties revealed by our theoretical analysis and the stability of our algorithm-selected features are empirically veriﬁed.
The remainder of the paper is organized as follows. We ﬁrst discuss the related work, then present our proposed algorithm. Next, we show that our proposed algorithm is uniformly stable and has a guaranteed generalization bound. Finally, we conduct extensive experiments to validate our proposed new algorithm and the properties related to our proved generalization bound. 2