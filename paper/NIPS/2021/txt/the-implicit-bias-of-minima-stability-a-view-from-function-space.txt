Abstract
The loss terrains of over-parameterized neural networks have multiple global min-ima. However, it is well known that stochastic gradient descent (SGD) can stably converge only to minima that are sufﬁciently ﬂat w.r.t. SGD’s step size. In this paper we study the effect that this mechanism has on the function implemented by the trained model. First, we extend the existing knowledge on minima stability to non-differentiable minima, which are common in ReLU nets. We then use our stability results to study a single hidden layer univariate ReLU network. In this setting, we show that SGD is biased towards functions whose second derivative (w.r.t the input) has a bounded weighted L1 norm, and this is regardless of the initialization. In particular, we show that the function implemented by the net-work upon convergence gets smoother as the learning rate increases. The weight multiplying the second derivative is larger around the center of the support of the training distribution, and smaller towards its boundaries, suggesting that a trained model tends to be smoother at the center of the training distribution. 1

Introduction
Understanding the overwhelming success of deep learning requires unveiling the mechanisms that allow over-parametrized models to generalize well, a phenomenon that is in sharp contrast to classical wisdom. It has been suggested that one of the sources for this behavior is the implicit bias that training algorithms have towards certain solutions. Implicit biases of generic optimization methods had been known for decades [49] and have also been discussed in the context of neural network training already in [30]. However, their dramatic effect on modern deep learning has only recently started to unveil
[36, 57]. For example, in binary classiﬁcation tasks with linearly separable data, it has been shown that among all global minima, gradient descent (GD) converges to the maximum margin separator
[48]. Similarly, in over-parametrized linear regression, GD converges to the minimum norm solution when initialized at zero [57].
Implicit biases were recently studied in many different settings, including in linear convolutional networks [16], matrix and tensor factorization [14, 41], with weight normalization [54] and with various loss functions [15]. Some works have drawn analogies between these biases and traditional regularization schemes. For example, it has been conjectured that the implicit bias of GD can be expressed as some regularization term that is added to the training loss [14]. This turned out to be true for several settings, such as over-parametrized linear regression with the quadratic loss [57], matrix factorization under certain assumptions [26], and linear classiﬁcation on separable data using losses with an exponential tail [48]. But recent work has pointed out that this is not true in general, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Loss vs. iteration (b) Interpolating functions found by GD (c) Second derivative and the weight function
Figure 1: We train a univariate single hidden layer ReLU network using GD. In one experiment (black) we use a constant learning rate η. In a second experiment (green) we use the same step size and initialization, but when GD arrives near the minimum we increase the step size by a factor of 3.
This is equivalent to initializing near the minimum and training with two different step sizes. Panel (a) shows the losses during training. Before the step-size changes, the loss curves coincide. After the change, GD with the larger step-size experiences an abrupt increase in the training loss, and the optimization eventually ends in a different minimum, as evident from Panels (b) and (c). Panel (b) shows the resulting functions implemented by the net, where the cross marks are the training points.
The function obtained with the large step size is smoother, in line with our main result (1). Panel (c) depicts (a smoothed version of) |f (cid:48)(cid:48)(x)| for the two solutions, as well as the weight function g(x) in (1). The |f (cid:48)(cid:48)(x)| obtained with the larger step-size is smaller, especially where g(x) is high. by illustrating that the implicit bias of GD is often not equivalent to a loss term of the form of any function of the network’s weights [40, 44].
The current paradigm for analyzing implicit bias seeks to ensure convergence to global minima as a
ﬁrst step. This requires making sure that GD is well behaved, i.e. does not escape the minimum. A common way to guarantee this is by considering a sufﬁciently small learning rate, or even the limit of inﬁnitesimal step size, known as gradient ﬂow (GF). In this case, the minima to which GD converges tend to be close to the initialization [39]. Therefore, existing results are initialization-dependent and relevant only to small step-sizes. Unfortunately, these type of analyses fail to capture phenomena that are known to be directly related to the step size in practical training, e.g. [8, 17, 19, 24, 29, 45, 46, 53].
For instance, Fig. 1 demonstrates that when initializing GD near a minimum, different step sizes lead to convergence to completely different solutions. This behavior is not reﬂected by current studies.
Following this understanding, here we seek an initialization-independent analysis that explicitly takes into account the learning rate and reveals its effect on the learned model. To this end, instead of analyzing the trajectory of the parameters from initialization to convergence, we study the properties of the minima to which stochastic GD (SGD) can converge. Speciﬁcally, it is well known that SGD cannot stably converge to minima that are too sharp relative to its step size [8, 19, 45, 53]. This property has been studied in [53] for twice differential minima, but its implication on the learned function has not been discussed. Here we extend this analysis to non-differentiable minima, which are common in ReLU networks, and use this property to characterize the end-to-end functions that a neural network can implement upon convergence.
As in [23, 42, 51], we study an over-parameterized single-hidden-layer univariate ReLU network, and focus on the quadratic loss. Under this setting, the network can implement inﬁnitely many piecewise linear functions f that globally minimize the loss (i.e. interpolate the data). Our key result (see
Theorem 1) is that when using a step size of η, SGD can only converge to solutions satisfying (cid:90)
R
|f (cid:48)(cid:48)(x)|g(x)dx ≤ 1
η
− 1 2
, (1) with some particular weight function g (see Fig. 1). In other words, the larger the step size, the smoother the solutions that a network learns. The weight g is larger around the center of the support of the training distribution, and smaller towards its boundaries. This implies that the solutions found by SGD tend to be smoother around the center of the distribution. 2
2 Minima stability
SGD is routinely used to minimize objective functions that have multiple global minima. However, it is well known that not all minima are accessible to SGD [53]. Understanding to which minima
SGD can converge, requires analyzing its dynamics in the vicinity of minima. If once SGD arrives near a minimum, it converges to it, then we say that this is a stable minimum. If SGD repels from the minimum, then we say that this is an unstable minimum. Here we brieﬂy survey and extend the existing knowledge about stable minima for SGD.
Let (cid:96)j : Rd (cid:55)→ R be differentiable almost everywhere for all j ∈ [n]. Consider a loss function L and its stochastic counterpart ˆL, given by
L(θ) = 1 n n (cid:88) j=1 (cid:96)j(θ) and
ˆLt(θ) = 1
B (cid:88) j∈Bt (cid:96)j(θ), (2) where Bt is a batch of size B sampled at iteration t. We assume that the batches {Bt} are drawn uniformly from the training set, independently across iterations. SGD’s update rule is given by
θt+1 = θt − η∇ ˆLt(θt), where η is the step size. In the following we deﬁne the notion of stability for minima, and provide conditions for a minimum to be stable. (3) 2.1 Twice differentiable minima
We start by examining a twice differentiable minimum θ∗. Using a Taylor expansion about θ∗, we have
ˆLt(θ) ≈ ˆLt(θ∗) + (θ − θ∗)T ∇ ˆLt(θ∗) + 1 2 (θ − θ∗)T ∇2 ˆLt(θ∗)(θ − θ∗), (4) where ∇ ˆLt(θ∗) and ∇2 ˆLt(θ∗) are the gradient and Hessian of ˆLt at θ∗. Therefore, in the vicinity of the minimum, (3) is approximately given by
θt+1 ≈ θt − η (cid:16) (cid:17)
∇ ˆLt(θ∗) + ∇2 ˆLt(θ∗)(θt − θ∗)
. (5)
This approximation gets more accurate as θt gets closer to θ∗. We can thus use this linearized dynamics to learn about the stability of minima [53].
Deﬁnition 1 (Linear stability). Let θ∗ be a twice differentiable minimum of L. Consider the linearized stochastic dynamical system
θt+1 = θt − η (cid:16) (cid:17)
∇ ˆLt(θ∗) + ∇2 ˆLt(θ∗)(θt − θ∗)
. (6)
Then θ∗ is said to be ε linearly stable if lim sup t→∞
E[(cid:107)θt − θ∗(cid:107)] ≤ ε. for any θ0 in the ε-ball Bε(θ∗), we have
In other words, θ∗ is ε linearly stable if once we have arrived at a distance of ε from it (at t = 0 without loss of generality), we end up at a distance no greater than ε around it in expectation. Some of our results do not depend on ε, in which case we simply refer to “linear stability” (without the ε).
Wu et al. [53] provided a sufﬁcient condition for linear stability under the assumption that
∇ ˆLt(θ∗) = 0 for all t ≥ 1. For our analysis, we need a necessary condition. This is simple to obtain from the well known stability criterion for GD and the fact that GD’s trajectory corresponds to the expectation of SGD’s steps. Speciﬁcally, we have the following condition (which only requires that E[∇ ˆLt(θ∗)] = 0; see formal proof in Appendix II).
Lemma 1 (Necessary condition for stability). Consider SGD with step size η, where batches are drawn uniformly from the training set, independently across iterations. If θ∗ is an ε linearly stable minimum of L, then
λmax (cid:0)∇2L(θ∗)(cid:1) ≤ 2
η
. (7)
Note that this condition is also sufﬁcient when considering GD (i.e. full batch SGD). 3
2.2 Non-differentiable minima
We now generalize the deﬁnition of linear stability to non-differentiable minima. In deep learning, non-differentiability is typically caused by a mode switch in the system, e.g. switching of ReLU activations or max-pooling layers. However, if we ﬁx the mode (e.g. constant activation or pooling selection patterns per sample), then the loss becomes inﬁnitely differentiable everywhere. In these cases, we can therefore model SGD’s dynamics as a switching dynamical system.
Let {Sm} be a partition of Rd that represents the regions of the different modes, i.e.
∀i (cid:54)= j Si ∩ Sj = ∅, (8)
Additionally, let ψm : Rd (cid:55)→ R be an analytic function representing the loss for the mth mode. We assume the overall loss (and its stochastic version) can be written as1 m Sm = Rd. and (cid:83) where ˆψ(t) approximate the loss as
L(θ) = ψm(θ), (9) m is the stochastic counterpart of ψm at time t. Therefore, near a minimum θ∗ we can 1 2
ˆLt(θ) ≈ ˆLt(θ∗) + (θ − θ∗)T ˆg(t) (t)
θ (θ − θ∗), (θ − θ∗)T ˆH
θ ∈ Sm, m (θ)
θ + (10) if
ˆLt(θ) = ˆψ(t) where (deﬁning Int(A) as the interior of A)
∀θ ∈ Int(Sm)
ˆg(t)
θ (cid:44) ∇ ˆψ(t) m (θ∗), and
ˆH (t)
θ (cid:44) ∇2 ˆψ(t) m (θ∗). (11) (cid:16)
θt+1 ≈ θt − η
The update rule of SGD (3) can thus be approximated as
+ ˆH
ˆg(t) (12)
θt
Note that this approximation is relevant only when θt is close to θ∗. Let2 I = {m : θ∗ ∈ ¯Sm} be the indices of the sets around θ∗, and let A = (cid:83) m∈I Sm be their union. We assume there is a ﬁnite number of modes in A (this is always the case for ReLU networks with a ﬁnite number of parameters).
Consider an ε-neighborhood around the minimum such that Bε(θ∗) ⊆ A. We deﬁne linear stability in this neighborhood as follows.
Deﬁnition 2 (Generalized linear stability). Let θ∗ be a minimum point of L. Consider the switching stochastic dynamical system (cid:17) (θt − θ∗) (t)
θt
. (cid:17) (θt − θ∗) (13) (cid:16) (t)
θt and assume that θt ∈ A with probability one for all t > 0.
• θ∗ is said to be ε linearly stable if lim sup t→∞
θt+1 = θt − η
+ ˆH
ˆg(t)
θt
• θ∗ is said to be ε linearly strongly stable if sup t
E[(cid:107)θt − θ∗(cid:107)] ≤ ε for any θ0 ∈ Bε(θ∗).
E[(cid:107)θt − θ∗(cid:107)] ≤ ε for any θ0 ∈ Bε(θ∗).
In other words, we consider a situation where at some point in time (which we call t = 0 without loss of generality), the parameter vector is ε-close to θ∗. If this guarantees that from that moment on, θt is always ε-close to θ∗ in expectation, then θ∗ is called strongly stable. If this only guarantees that from some later point in time, θt is always ε-close to θ∗ in expectation, then θ∗ is called stable.
Our results are stated in terms of H m = ∇2ψm(θ∗) and ˆg(t) a necessary condition for linear stability (see proof in Appendix III).
Lemma 2 (Necessary condition for stability). Assume that SGD with step size η draws batches uniformly from the training set, independently across iterations. Let θ∗ be a minimum point of L, for which there is a ﬁnite number of modes in I. Suppose there exist q ∈ Sd−1 and {λm} such that (cid:107)H mq − λmq(cid:107) ≤ δ for all m ∈ I and denote m (θ∗). The next lemma gives m = ∇ ˆψ(t)
λlower = min m∈I
{λm}, (14) and3 γ = maxm∈I E[|qT ˆg(t) m |]. If 2
η then θ∗ is not an ε strongly stable minimum. Furthermore, if δ = 0 (i.e. q is a common eigenvector) then θ∗ is not an ε stable minimum.
λlower >
+ δ + (15)
γ
ε
, 1This is true since the modes corresponding to a batch are a subset of those of the full data. 2 ¯Sm denotes the closure of Sm. 3Note that {ˆg(t) m }∞ t=1 are i.i.d. and therefore E[|qT ˆg(t) m |] is time invariant. 4
Note that the lemma considers the case where the Hessians of all subsystems have an approximate common eigenvector q.
In this setting, a necessary condition for stability is that the smallest (approximate) eigenvalue associated with q is not too large w.r.t. 2/η. Interestingly, in the setting of
ReLU networks, this (approximate) common eigenvector assumption holds true (see Lemma 5). We thus make use of this property in our analysis of the solutions to which SGD can converge.
For completeness, we also present here a sufﬁcient condition for stability in the special case of GD where ∇ψm(θ∗) = 0 for all m ∈ I (see proof in Appendix IV). This setting is of interest in our case, since the global minima of overparameterized ReLU networks under the quadratic loss always satisfy this condition (see proof in Appendix VII for our setting).
Lemma 3 (Sufﬁcient conditions for stability). Consider full batch SGD (i.e. GD) with step size η.
Let θ∗ be a minimum point of L, for which ∇ψm(θ∗) = 0 for all m ∈ I. Denote
λupper max = max m∈I
λmax(H m). (16)
If then θ∗ is linearly strongly stable.
λupper max ≤ 2
η (17)
Here we see that if all the subsystems are stable, then the overall switching system is strongly stable. 3 The implicit bias of minima stability
We now use our results to study the implicit bias of minima stability in the context of training of univariate ReLU networks. Speciﬁcally, consider the set of functions that can be implemented by a one-hidden-layer neural network with k neurons, (cid:40) (cid:41)
F = f : R (cid:55)→ R f (x) = (cid:16) w(2) i σ i x + b(1) w(1) i (cid:17)
+ b(2)
, (18) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) k (cid:88) i=1 where σ(·) is the ReLU activation function. Each f ∈ F is a piece-wise linear function with at most k knots. We are interested in functions that minimize the quadratic loss,
L(f ) = 1 2n n (cid:88) j=1 (f (xj) − yj)2 , (19) where {(xj, yj)}n j=1 are n training samples such that4 xi (cid:54)= xj whenever i (cid:54)= j.
Deﬁnition 3. We say that f ∈ F is a solution if L(f ) = 0. In this case the function satisﬁes f (xj) = yj for all j ∈ [n].
When k ≥ n there are inﬁnitely many solutions. However, as discussed above, when training the network using SGD, not all of these global minima can be reached. Particularly, assume we use SGD to minimize the loss w.r.t. the parameter vector (cid:104)
θ = 1 , . . . , w(1) w(1) k , b(1) 1 , . . . , b(1) k , w(2) 1 , . . . , w(2) k , b(2)(cid:105)T
∈ R3k+1. (20)
What are the properties of the solutions (in function space) to which we can converge?
In Sec. 2 we saw that dynamic stability is associated with the Hessian of the loss at the minimum. Our goal is thus to link the Hessian to properties of the solution f in function space. A major challenge, however, is that each f ∈ F may have inﬁnitely many different implementations θ, and each such implementation may have a different Hessian. Since we are only interested in the functionality f and not in its particular implementation, we consider a solution to be accessible by SGD if there exists some implementation of that solution which is a stable minimum for SGD (see Deﬁnition 1 and 2).
Deﬁnition 4. We say that a solution f ∈ F is (strongly) stable for step-size η if there exists a minimum point θ∗ of the loss that corresponds to f , where θ∗ is linearly (strongly) stable for SGD with step-size η. 4In this work we focus on cases in which perfect ﬁt is possible. Note that when xi = xj, if yi (cid:54)= yj then perfect interpolation is impossible, and if yi = yj then the jth training sample is redundant. 5
Our ﬁrst result characterizes solutions in F that correspond to twice differentiable minima. For this type of minima, the knots of f do not coincide with any training point in the data set. In the following, f (cid:48)(cid:48)(x) should be interpreted in the weak sense (i.e. it is a sum of weighted Dirac delta functions).
Theorem 1 (Properties of twice differentiable stable solutions). Let f be a linearly stable solution for SGD with step-size η. Assume that the knots of f do not coincide with any training point. Then where with (cid:90) ∞
−∞
|f (cid:48)(cid:48)(x)| g(x)dx ≤ 1
η
− 1 2
, g(x) = (cid:26)min {g–(x), g+(x)} , 0, x ∈ [xmin, xmax], otherwise, g–(x) = P2 (X < x) E [x − X|X < x] g+(x) = P2 (X > x) E [X − x|X > x] (cid:113) 1 + (E[X|X < x])2, (cid:113) 1 + (E[X|X > x])2. (21) (22) (23)
Here X is drawn from the empirical distribution of the data (a sample chosen uniformly from {xj}).
This theorem shows that stable solutions of SGD correspond to functions whose second derivative has a bounded weighted norm. Importantly, the bound depends on the step size. As the learning rate increases, the set of stable solutions contains less and less non-smooth functions. Figure 2 depicts the weight g for various distributions of the training data. We can see that most of g’s mass is located at the center of the training data. Furthermore, g decays towards the extreme data points, and vanishes beyond them. This implies that stable solutions tend to be smoother for instances near the center of the data distribution, and less smooth for instances near the edges. Particularly, minima stability imposes no restrictions on the function’s smoothness outside the support of the data distribution.
A limitation of Theorem 1 is that when training with a very large step size, the set of stable solutions contains only very smooth interpolating functions. But such functions tend to have their knots coincide with data points [42], which contradicts the theorem’s assumption that the minimum is twice differentiable. To cope with such settings, we now present a result for non-differentiable minima.
This result requires assumptions on the maximal number of neurons that can coincide with each data point, and is thus stated in terms of the number of knots of the function f (see Sec. 4.2 for detail).
Theorem 2 (Properties of non-differentiable strongly stable solutions). Let f ∈ F be a piece-wise linear function with L ≤ k knots. Assume that f has an implementation θ∗ such that (cid:107)θ∗(cid:107)∞ ≤ ρ.
Deﬁne M = maxj∈[n] |xj| and C = max{1, ρ(1 + M )}. If f is a linearly strongly stable solution for SGD with step-size η, then (cid:90) ∞
−∞
|f (cid:48)(cid:48)(x)| g(x)dx ≤ 1
η
− 1 2
+ ∆, where g(x) is deﬁned in (22) and
∆ = 3 2
C 2 1 + k − L n
+ C (cid:115) 3 4 1 + k − L n (cid:18) 3C 2 1 + k − L n
+ (cid:19)
. 2
η (24) (25)
Note that in the small over-parametrization regime, ∆ is small. Indeed, k − L is the number of excess neurons, i.e. the number of neurons employed by the network beyond the minimum required to realize an L-knot function. Therefore, in the regime where the number of samples n and number of neurons k grow while (k − L)/n → 0, we have that ∆ → 0 provided that C is bounded. The latter happens, for example, when the training data are bounded and generated by a smooth function5. 4 Proof outlines 4.1 Twice differentiable minima
Our goal is to characterize the minima to which SGD with step size η can converge, by using Lemma 1.
We start by computing the Hessian matrix at a twice differentiable global minimum with zero error 5Savarese et al. [42] showed that ||θ∗||2 is bounded in this case, implying that ||θ∗||∞ ≤ ρ as well. 6
(a) Uniform distribution (b) Gaussian distribution (c) Laplace distribution
Figure 2: Graphs of g(x) for different distributions. For each distribution, the empirical graph of g (red) is based on n = 20 i.i.d. samples, where we normalized them to zero average and standard deviation one. The theoretical graph (blue) is computed by using the population distribution of X in g (via numerical integration). (so f (xj) = yj for all j ∈ [n]). The gradient of the loss (19) w.r.t. θ is given by
∇θL = 1 n n (cid:88) j=1 (f (xj) − yj) ∇θf (xj), (26) where ∇θf (x) is written explicitly in Appendix V. Thus, the Hessian is given by
∇2
θL =
= 1 n 1 n n (cid:88) j=1 n (cid:88) j=1 (∇θf (xj)) (∇θf (xj))T + 1 n n (cid:88) j=1 (f (xj) − yj) ∇2
θf (xj) (∇θf (xj)) (∇θf (xj))T , (27) where we used the fact that f (xj) = yj for all j ∈ [n]. Let us denote the tangent features matrix by
θL = ΦΦT /n,
Φ = [∇θf (x1), ∇θf (x2), . . . , ∇θf (xn)]. Then the Hessian can be expressed as ∇2 and its maximal eigenvalue can be written as
λmax(∇2
θL) = max v∈S3k vT (cid:0)∇2
θL(cid:1) v = max v∈S3k 1 n (cid:107)ΦT v(cid:107)2 = max u∈Sn−1 1 n (cid:107)Φu(cid:107)2 . (28)
Notice that this eigenvalue is implementation dependent. Namely, there are uncountably many sets of network parameters which correspond to the same end-to-end function f ∈ F, and different parameter sets can have different top Hessian eigenvalues. However, recall from Deﬁnition 4 that we only care whether there exists one implementation of f ∈ F whose top eigenvalue is small enough to allow convergence of SGD to that minimum. We would therefore like to analyze the implementation that minimizes λmax. Mathematically, we denote the set of all parameters corresponding to f as (cid:40)
Ω(f ) =
θ ∈ R3k+1 (cid:12) (cid:12) (cid:12) f (x) = (cid:12) (cid:12) k (cid:88) i=1 w(2) i σ (cid:16) i x + b(1) w(1) i (cid:17) (cid:41)
+ b(2)
. (29)
By using the right-hand side of (28) for λmax, we show the following (see proof in Appendix VI).
Lemma 4 (Top eigenvalue lower bound). Let f ∈ F be a twice-differentiable minimizer of the loss function, then min
θ∈Ω(f )
λmax(∇2
θL) ≥ 1 + 2 (cid:90) ∞
−∞
|f (cid:48)(cid:48)(x)| g(x)dx, (30) where g is deﬁned in (22).
Now we can prove Theorem 1 based on Lemma 1 and 4.
Proof of Theorem 1. Since f is a stable solution, there exists a linearly stable minimum point θ∗ of L such that θ∗ ∈ Ω(f ). Therefore, (cid:90) ∞ 1 + 2
|f (cid:48)(cid:48)(x)| g(x)dx ≤ min
θ∈Ω(f )
−∞
λmax(∇2
θL(θ)) ≤ λmax(∇2
θL(θ∗)) ≤ (31) 2
η
, 7
where the ﬁrst inequality follows from Lemma 4 and the last inequality follows from Lemma 1. From the leftmost and rightmost sides of (31), we get (cid:90) ∞
−∞
|f (cid:48)(cid:48)(x)| g(x)dx ≤ 1
η
− 1 2
, (32) which completes the proof. 4.2 Non-differentiable minima
A minimum θ∗ that is not twice differentiable corresponds to a network that interpolates the data, while at least one of its knots coincides with a data point. The neuron corresponding to each such knot, divides the parameter space into two regions: one where the neuron is active, and one where it is inactive. At the interior of each such region, the loss function is twice differentiable w.r.t. parameters.
If we denote by pj the number of neurons that toggle precisely on xj, then the total number of toggling neurons for θ∗ is p = (cid:80)n j=1 pj and the switching system in Deﬁnition 2 has 2p relevant6
“modes”. We denote by H the set of 2p Hessian matrices corresponding to these modes. Additionally, we denote the analytic function representing f (xj; θ) for the mth mode by φ(xj, θ, mj). For more details, see our switching system formulation in Appendix VII.
Our key observation is that when a neuron switches its mode, it hardly affects the Hessian matrix, let alone its top eigenvector. Therefore, if not too many neurons toggle, then the principal directions of all matrices in H tend to align (see proof in Appendix VIII).
Lemma 5. Assume that (cid:107)∇θφ(xj, θ∗, mj)(cid:107)∞ ≤ C for all j ∈ [n] and m ∈ I. Denote the maximal number of neurons that toggle on any single data point by pmax = max{pj}. Let H be some matrix in H having a top eigen-pair q ∈ S3k and λmax(H). Then for all other ˜H ∈ H,
√ (cid:18)(cid:113) (cid:113) (cid:13)
˜Hq − λmax(H)q (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) ≤ 3C
λmax (cid:0)H(cid:1) +
λmax (cid:0) ˜H(cid:1) (cid:19) (cid:114) pmax n
. (33)
Note that if we choose H to be the one with the largest top eigenvector, λupper from the lemma that for all Hm ∈ H, max (see (16)), then we get (cid:107)Hmq − λupper max q(cid:107) ≤
√ 3C (cid:16)(cid:112) max + (cid:112)λmax(Hm)
λupper (cid:17) (cid:114) pmax n
√
≤ 2 (cid:112) 3C
λupper max (cid:114) pmax n
. (34)
To use Lemma 2, we would like to ﬁnd an explicit expression for this bound. Under the assumption (cid:107)θ∗(cid:107)∞ ≤ ρ we have that (cid:107)∇θφ(xj, θ, mj)(cid:107)∞ ≤ max{1, ρ(1 + M )}, where M = maxj∈[n] |xj|.
Thus, C can be taken as max{1, ρ(1 + M )}. Note that ˆg(t) m = 0 for all m ∈ I and t > 0 (see
Appendix VII). Thus, using Lemma 2 with γ = 0, δ = 2 3C max for all m (such that λlower = λupper max ), we get that if θ∗ is strongly stable then (see Appendix IX) n and λm = λupper
λupper max (cid:112) pmax (cid:112)
√ max (θ∗) ≤
λupper 2
η
+ 3C 2 pmax n (cid:115) 3
+ C pmax n (cid:18) 3C 2 pmax n
+ (cid:19)
. 2
η (35)
If f ∈ F has L ≤ k knots, then pmax ≤ 1 + k − L. Substituting this upper-bound in (35), the necessary condition becomes max (θ∗) ≤
λupper 2
η
+ 3C 2 1 + k − L n (cid:115) 3
+ C 1 + k − L n (cid:18) 3C 2 1 + k − L n
+ (cid:19)
. 2
η (36)
Finally, we show in Appendix X that, similarly to the twice differentiable case of Lemma 4, here as well min
θ∈Ω(f )
λupper max (θ) ≥ 1 + 2 (cid:90) ∞
−∞
|f (cid:48)(cid:48)(x)| g(x)dx. (37)
Combining (37) and (36), shows that any strongly stable solution that meets the assumptions must satisfy (24). 6Modes that are near the minimum, i.e. that are in I. 8
(a) Examples of interpolating functions (b) Corresponding second derivative and weight function (c) Sharpness versus learning rate
Figure 3: GD training of a one-hidden-layer ReLU network using various step sizes. Panel (a) shows the training data and the solutions to which GD converged for three different step sizes. Panel (b) depicts the corresponding weight function g, and the second derivative of the solutions. Panel (c) visualizes the sharpness of the solutions versus the learning rate. From these plots we see that the GD solution gets smoother as the step size increases, particularly at the center of the training data. 5 Experiments
We now verify our theoretical predictions in experiments. We train a single-hidden-layer ReLU network using GD with varying step sizes, all initialized at the same point. Figure 3(a) shows the training data and solutions to which GD converged. Figure 3(b) depicts the corresponding weight function g and the (smoothed) absolute value of the second derivative of the solutions. When trained with a very small step size, GD converges to the black function, whose second derivative is quite large. However, even in this extreme case, the second derivative is small where the weight function g is high, which is in accordance with (21),(24). When trained with a medium step size, GD converges to the blue function, which is much smoother, particularly at the center of the training data. This trend continues as we increase the learning rate. Note that the green function, which is obtained with a large step size, is ﬂat at the center, but has relatively large second derivative outside the support of the training data. This demonstrates that while minima stability restrains the solution within the support of the training data, it imposes no restrictions on the function’s smoothness outside of the support.
Figure 3(c) visualizes the sharpness of the solution as a function of the learning rate. Speciﬁcally, the red line marks the edge of the stable region, 2/η. Recall that GD cannot converge to a minimum whose sharpness exceeds this value. The yellow dashed line depicts the sharpness of the minima to which GD converged in practice. Here the two lines coincide, indicating that GD converged on the edge of the stable region. This is known to be a common phenomenon [8]. In blue, we plot the sharpness of the ﬂattest implementation for each converged network (see Appendix XI for details).
Finally, the purple curve depicts our lower bound on the sharpness of the ﬂattest implementation, presented in Lemma 4. Note that our bound is relatively tight (purple curve close to the blue curve).
Moreover, at the large step size regime, it is also quite close to the actual sharpness upon convergence (yellow curve). Additionally, since our bound is the weighted L1 norm of the second derivative of f , 9
the decay of the purple line indicates that f indeed gets smoother as the step size increases, just like our theoretical results predict. Please refer to Appendix I for more details and additional experiments. 6