Abstract
Inverse reinforcement learning attempts to reconstruct the reward function in a Markov decision problem, using observations of agent actions. As already observed in Russell [1998] the problem is ill-posed, and the reward function is not identiﬁable, even under the presence of perfect information about optimal behavior. We provide a resolution to this non-identiﬁability for problems with entropy regularization. For a given environment, we fully characterize the reward functions leading to a given policy and demonstrate that, given demonstrations of actions for the same reward under two distinct discount factors, or under sufﬁciently different environments, the unobserved reward can be recovered up to a constant.
We also give general necessary and sufﬁcient conditions for reconstruction of time-homogeneous rewards on ﬁnite horizons, and for action-independent rewards, generalizing recent results of Kim et al. [2021] and Fu et al. [2018]. 1

Introduction
Inverse reinforcement learning aims to use observations of agents’ actions to determine their reward function. The problem has roots in the very early stages of optimal control theory; Kalman [1964] raised the question of whether, by observation of optimal policies, one can recover coefﬁcients of a quadratic cost function (see also Boyd et al. [1994]). This question naturally generalizes to the generic framework of Markov decision process and stochastic control.
In the 1970s, these questions were taken up within economics, as a way of determining utility functions from observations. For instance, Keeney and Raiffa [1976] set out to determine a proper ordering of all possible states which are deterministic functions of actions. In this setup, the problem is static and the outcome of an action is immediate. Later in Sargent [1978], a dynamic version of a utility assessment problem was studied, under the context of ﬁnding the proper wage through observing dynamic labor demand.
As exempliﬁed by Lucas’ critique1, in many applications it is not enough to ﬁnd some pattern of rewards corresponding to observed policies; instead we may need to identify the speciﬁc rewards agents face, as it is only with this information that we can make valid predictions for their actions in a changed environment. In other words, we do not simply wish to learn a reward which allows us to imitate agents in the current environment, but which allows us to predict their actions in other settings. 1The critique is best summarized by the quotation: “Given that the structure of an econometric model consists of optimal decision rules of economic agents, and that optimal decision rules vary systematically with changes in the structure of series relevant to the decision maker, it follows that any change in [regulatory] policy will systematically alter the structure of econometric models.” (Lucas [1976]) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we give a precise characterization of the range of rewards which yield a particular policy for an entropy regularized Markov decision problem. This separates the main task of estimation (of the optimal policy from observed actions) from the inverse problem (of inferring rewards from a given policy). We ﬁnd that even with perfect knowledge of the optimal policy, the corresponding rewards are not fully identiﬁable; nevertheless, the space of consistent rewards is parameterized by the value function of the control problem. In other words, the reward can be fully determined given the optimal policy and the value function, but the optimal policy gives us no direct information about the value function.
We further show that, given knowledge of the optimal policy under two different discount rates, or sufﬁciently different transition laws, we can uniquely identify the rewards (up to a constant shift).
We also give conditions under which action-independent rewards, or time-homogenous rewards over ﬁnite horizons, can be identiﬁed. This demonstrates the fundamental challenge of inverse reinforcement learning, which is to disentangle immediate rewards from future rewards (as captured through preferences over future states). 2