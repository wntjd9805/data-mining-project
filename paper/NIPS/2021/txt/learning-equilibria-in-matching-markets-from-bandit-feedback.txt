Abstract
Large-scale, two-sided matching platforms must ﬁnd market outcomes that align with user preferences while simultaneously learning these preferences from data.
But since preferences are inherently uncertain during learning, the classical notion of stability (Gale and Shapley, 1962; Shapley and Shubik, 1971) is unattainable in these settings. To bridge this gap, we develop a framework and algorithms for learning stable market outcomes under uncertainty. Our primary setting is matching with transferable utilities, where the platform both matches agents and sets mone-tary transfers between them. We design an incentive-aware learning objective that captures the distance of a market outcome from equilibrium. Using this objective, we analyze the complexity of learning as a function of preference structure, casting learning as a stochastic multi-armed bandit problem. Algorithmically, we show that “optimism in the face of uncertainty,” the principle underlying many bandit algorithms, applies to a primal-dual formulation of matching with transfers and leads to near-optimal regret bounds. Our work takes a ﬁrst step toward elucidating when and how stable matchings arise in large, data-driven marketplaces.1 1

Introduction
Data-driven marketplaces face the simultaneous challenges of learning agent preferences and aligning market outcomes with the incentives induced by these preferences. Consider, for instance, online platforms that match two sides of a market to each other (e.g., Lyft, TaskRabbit, and Airbnb). On these platforms, customers are matched to service providers and pay for the service they receive. If agents on either side are not offered desirable matches at fair prices, they would have an incentive to leave the platform and switch to a competing platform. Agent preferences, however, are often unknown to the platform and must be learned. When faced with uncertainty about agent preferences (and thus incentives), when can a marketplace efﬁciently explore and learn market outcomes that align with agent incentives?
We center our investigation around a model called matching with transferable utilities, proposed by Shapley and Shubik [32]. In this model, there is a two-sided market of customers and service providers. Each customer has a utility that they derive from being matched to a given provider and vice versa. The platform selects a matching between the two sides and assigns a monetary transfer between each pair of matched agents. Transfers are a salient feature of most real-world matching
†Equal contribution. 1A full version of this paper (referenced as [18]) is available at https://arxiv.org/abs/2108.08843. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
markets: riders pay drivers on Lyft, clients pay freelancers on TaskRabbit, and guests pay hosts on
Airbnb. An agent’s net utility is their value for being matched to their partner plus the value of their transfer (either of which can be negative in the cases of costs and payments). In matching markets, the notion of stability captures alignment of a market outcome with agent incentives. Informally, a market outcome is stable if no pair of agents would rather match with each other than abide by the market outcome, and stable matchings can be computed when preferences are fully known.
However, in the context of large-scale matching platforms, the assumption that preferences are known breaks down. Platforms usually cannot have users report their complete preference proﬁles. Moreover, users may not even be aware of what their own preferences are. For example, a freelancer may not exactly know what types of projects they prefer until actually trying out speciﬁc ones. In reality, a data-driven platform is more likely to learn information about preferences from repeated feedback over time. Two questions now emerge: In such marketplaces, how can stable matchings be learned?
And what underlying structural assumptions are necessary for efﬁcient learning to be possible?
Toward answering these questions, we propose and investigate a model for learning stable matchings from noisy feedback. We model the platform’s learning problem using stochastic multi-armed bandits, which lets us leverage the extensive body of techniques in the bandit literature to analyze the data efﬁciency of learning. Speciﬁcally, our main contributions are: (i) We develop an incentive-aware learning objective—Subset Instability—that captures the distance of a market outcome from equilibrium. (ii) Using Subset Instability as a measure of regret, we show that any “UCB-based” algorithm can be adapted to this incentive-aware setting. (iii) We instantiate this idea for two families of preference structures to design efﬁcient algorithms for incentive-aware learning, helping elucidate how preference structure affects the complexity of learning stable matchings.
Designing the learning objective. Since mistakes are inevitable while exploring and learning, achieving exact stability at every time step is an unattainable goal. To address this issue, we lean on approximation, focusing on learning market outcomes that are approximately stable. Thus, we need a metric that captures the distance of a market outcome from equilibrium.
We introduce a notion for approximate stability that we call Subset Instability. Speciﬁcally, we deﬁne the Subset Instability of a market outcome to be the maximum difference, over all subsets S of agents, between the total utility of the maximum weight matching on S and the total utility of S under the market outcome.We show Subset Instability can be interpreted as how much the platform would have to subsidize participants to keep them on the platform and make the resulting matching stable. We also show that Subset Instability is the maximum gain in utility that a coalition of agents could have derived from an alternate matching such that no agent in the coalition is worse off.
Subset Instability also satisﬁes the following properties, which make it suitable for learning: (i) Subset
Instability is equal to 0 if and only if the market outcome is (exactly) stable; (ii) Subset Instability is robust to small perturbations to the utility functions of individual agents, which is essential for learning with noisy feedback; (iii) Subset Instability upper bounds the utility difference of a market outcome from the socially optimal market outcome.
Designing algorithms for learning a stable matching. Using Subset Instability, we investigate the problem of learning a stable market outcome from noisy user feedback in a stochastic bandit model. In each round, the platform selects a market outcome (i.e., a matching along with transfers), with the goal of minimizing cumulative instability.
We develop a general approach for designing bandit algorithms within our framework. Our approach is based on a primal-dual formulation of matching with transfers [32] We ﬁnd that “optimism in the face of uncertainty,” the principle underlying many UCB-style bandit algorithms [3, 22], can be adapted to this primal-dual setting. The resulting algorithm is simple: maintain upper conﬁdence bounds on the agent utilities and compute, in each round, an optimal primal-dual pair in terms of these upper conﬁdence bounds. Following this approach, we show regret bounds for two sets of structural assumptions on agent preferences, which we now state.
√
Theorem 1.1 (Unstructured Preferences, Informal). There exists a UCB-style algorithm that incurs nT ) regret according to Subset Instability after T rounds, where N is the number of agents (cid:101)O(N on the platform and n is the number of agents that arrive in any round. (In fact, this bound is optimal up to logarithmic factors.) 2
Theorem 1.2 (Separable Linear Preferences, Informal). Suppose each agent a gets utility (cid:104)φ(a), ca(cid:48)(cid:105) from matching with a(cid:48), where φ(a) ∈ Rd is unknown and ca(cid:48) ∈ Rd is known. Then there exists a
UCB-style algorithm that incurs (cid:101)O(d nT ) regret according to Subset Instability after T rounds, for N the number of agents on the platform and n the maximum number of agents in any round.
√
√
N
These results highlight the role of preference structure on the complexity of learning a stable matching.
To see this concretely, consider the case where all agents show up each round, so n = N . In this case, our regret bound for unstructured preferences is superlinear in N ; and this dependence on N is necessary due to a lower bound (see Lemma 5.4). On the other hand, our regret bound is linear in N for separable linear preferences. This means that in large markets, a centralized platform can efﬁciently learn a stable matching with this preference structure assumption.
Extensions.
In the full version [18], we provide several extensions of our framework and results to additional classes of preference structures, alternative forms of regret bounds, and richer platform objectives, and other models of matching markets. In Section 6, we brieﬂy discuss two of these extensions—instance-dependent regret bounds and matching markets with non-transferable utilities. 1.1