Abstract
Stochastic sparse linear bandits offer a practical model for high-dimensional online decision-making problems and have a rich information-regret structure. In this work we explore the use of information-directed sampling (IDS), which naturally balances the information-regret trade-off. We develop a class of information-theoretic Bayesian regret bounds that nearly match existing lower bounds on a variety of problem instances, demonstrating the adaptivity of IDS. To efﬁciently implement sparse IDS, we propose an empirical Bayesian approach for sparse posterior sampling using a spike-and-slab Gaussian-Laplace prior. Numerical results demonstrate signiﬁcant regret reductions by sparse IDS relative to several baselines. 1

Introduction
Standard linear bandits associate each action with a feature vector and assume the mean reward is the inner product between the feature vector and an unknown parameter vector [Auer, 2002, Dani et al., 2008, Rusmevichientong and Tsitsiklis, 2010, Chu et al., 2011, Abbasi-Yadkori et al., 2011].
Sparse linear bandits generalize linear bandits by assuming the unknown parameter vector is sparse
[Abbasi-Yadkori et al., 2012, Carpentier and Munos, 2012, Hao et al., 2020b] and is of great practical signiﬁcance for modeling high-dimensional online decision-making problems [Bastani and Bayati, 2020]. sdn) regret lower bound for the data-rich
Lattimore and Szepesv´ari [2020, §24.3] established a Ω( regime, where n is the horizon, d is the feature dimension, s is the sparsity and data-rich regime refers to the horizon n ≥ dα for some α > 0. This means polynomial dependence on d is generally not avoidable without additional assumptions. However, this bound hides much of the rich structure of sparse linear bandits by a crude maximisation over all environments.
√
When the action set admits a well-conditioned exploration distribution, Hao et al. [2020b] discovered the information-regret trade-off phenomenon by establishing an Θ(poly(s)n2/3) minimax rate for the data-poor regime. An interpretation for this optimal rate is that the agent needs to acquire enough information for fast sparse learning by pulling informative actions that even have high regret.
Explore-then-commit algorithm can achieve this rate for the data-poor regime but is sub-optimal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
in the data-rich regime. Therefore, our goal is to develop an efﬁcient algorithm that can adapt to different information-regret structures for sparse linear bandits.
Contributions Our contribution is three-fold:
• We prove that optimism-based algorithms fail to optimally address the information-regret trade-off in sparse linear bandits, which results in a sub-optimal regret bound.
• We provide the ﬁrst analysis using information theory for sparse linear bandits and derive a class of nearly optimal Bayesian regret bounds for IDS that can adapt to information-regret structures.
• To approximate the information ratio, we develop an empirical Bayesian approach for sparse posterior sampling using spike-and-slab Gaussian-Laplace prior. Through several experiments, we justify the great empirical performance of sparse IDS with an efﬁcient implementation. 2 Preliminary
We ﬁrst introduce the basic setup of stochastic sparse linear bandits. The agent receives a compact action set A ⊆ Rd in the beginning where |A| = K. At each round t, the agent chooses an action
At ∈ A and receives a reward Yt = (cid:104)At, θ∗(cid:105) + ηt, where (ηt)n t=1 is a sequence of independent standard Gaussian random variables and θ∗ ∈ Rd is the true parameter unknown to the agent. We make the mild boundedness assumption that for all a ∈ A, (cid:107)a(cid:107)∞ ≤ 1. The notion of sparsity can be deﬁned through the parameter space Θ:
Θ =


θ ∈ Rd
 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) d (cid:88) j=1 1{θj (cid:54)= 0} ≤ s, (cid:107)θ(cid:107)2 ≤ 1



.
We assume s is known and it can be relaxed by putting a prior on it. We consider the Bayesian setting where θ∗ is a random variable taking values in Θ and denote ρ as the prior distribution.
E[(cid:104)a, θ∗(cid:105)|θ∗]. The agent chooses At based on the history
The optimal action is x∗ = argmaxa∈A
Ft = (A1, Y1, . . . , At−1, Yt−1). Let D(A) be the space of probability measures over A. A policy
π = (πt)t∈N is a sequence of deterministic functions where πt(Ft) speciﬁes a probability distribution over A. The information-theoretic Bayesian regret of a policy π [Russo and Van Roy, 2014] is deﬁned as
BR(n; π) = E (cid:104)x∗, θ∗(cid:105) − (cid:34) n (cid:88) n (cid:88) (cid:35)
Yt
, where the expectation is over the interaction sequence induced by the agent and environment and the prior distribution over θ∗. t=1 t=1
Notation Denote Id as the d × d identity matrix. Let [n] = {1, 2, . . . , n}. For a vector x and
√ x(cid:62)Ax be the weighted (cid:96)2-norm and σmin(A) be positive semideﬁnite matrix A, we let (cid:107)x(cid:107)A = the minimum eigenvalue of A. The relation x (cid:38) y means that x is greater or equal to y up to some universial constant and (cid:101)O(·) hides modest logarithmic factors and universal constant. The cardinality of a set A is denoted by |A|. Given a measure P and jointly distributed random variables X and Y we let PX denote the law of X and we let PX|Y be the conditional law of X given Y : PX|Y (·) = P(X ∈
·|Y ). The mutual information between X and Y is I(X; Y ) = E[DKL(PX|Y ||PX )] where DKL is the relative entropy. We write Pt(·) = P(·|Ft) as the posterior measure where P is the probability measure over θ and the history and Et(·) = E(·|Ft). Denote It(X; Y ) = Et[DKL(Pt,X|Y ||Pt,X )]. 3