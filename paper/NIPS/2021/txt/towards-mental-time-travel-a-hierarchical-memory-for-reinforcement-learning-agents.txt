Abstract
Reinforcement learning agents often forget details of the past, especially after de-lays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by ﬁrst performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore “mentally time-travel”— remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other mem-ory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efﬁciently in a new neighborhood, and rapidly learn-ing and retaining new object names. Agents with HCAM can extrapolate to task sequences an order of magnitude longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efﬁciency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments. 1

Introduction
Human learning and generalization relies on our detailed memory of the past [55, 52, 16, 30, 43]. If we watch a show, we can generally recall its scenes in some detail afterward. If we explore a new neighborhood, we can recall our paths through it in order to plan new routes. If we are exposed to a new noun, we can ﬁnd that object by name later. We experience relatively little interference from delays or intervening events. Indeed, human episodic memory has been compared to “mental time travel”
[55, 52, 37]—we are able to transport ourselves into a past event and re-live it in sequential detail, without attending to everything that has happened since. That is, our recall is both sparse (we attend to a small chunk of the past, or a few chunks) and detailed (we recover a large amount of information 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
from each chunk). This combination gives humans the ﬂexibility necessary to recover information from memory that is speciﬁcally relevant to the task at hand, even after long periods of time.
If we want Reinforcement Learning (RL) agents to meaningfully interact with complex environments over time, our agents will need to achieve this type of memory. They will need to remember event details. They will need to retain information they have acquired, despite unrelated intervening tasks.
They will need to rapidly learn many pieces of new knowledge without discarding previous ones.
They will need to reason over their memories to generalize beyond their training experience.
However, current models struggle to achieve rapid encoding combined with detailed recall, even over relatively short timescales. Meta-learning approaches [57] can slowly learn global task knowledge in order to rapidly learn new information, but they generally discard the details of that new information immediately to solve the next task. While LSTMs [20], Transformers [56], and variants like the
TransformerXL [9] can serve as RL agent memories in short tasks [44], we show that they are ineffective at detailed recall even after a few minutes. Transformer attention can be ineffective at long ranges even in supervised tasks [54], and this problem is likely exacerbated by the sparse learning signals in RL. By contrast, prior episodic memory architectures [21, 14] can maintain information over slightly longer timescales, but they struggle with tasks that require recalling a temporally-structured event, rather than simply recalling a single past state. That is, they are particularly poor at the type of event recall that is fundamental to human memory. We suggest that this is because prior methods lack 1) memory chunks longer than a single timestep and 2) sparsity of attention. This means that these models cannot effectively “time-travel” to an event, and relive that speciﬁc event in detail, without interference from all other events. This also limits their ability to reason over their memories to adapt to new tasks—for example, planning a new path by combining pieces of previous ones [49].
Here, we propose a new type of memory that begins to address these challenges, by leveraging sparsity, chunking, and hierarchical attention. We refer to our architecture as a Hierarchical Chunk
Attention Memory (HCAM). The fundamental insight of HCAM is to divide the past into distinct chunks before storing it in memory, and to recall hierarchically. To recall, HCAM ﬁrst uses coarse chunk summaries to identify relevant chunks, and then mentally travels back to attend to each relevant chunk in further detail. This approach combines beneﬁts of the sparse and relatively long-term recall ability of prior episodic memories [60, 49] with the short-term sequential and reasoning power of transformers [56, 9, 44] in order to achieve better recall and reasoning over memory than either prior approach. While we do not address the problem of optimally chunking the past here, instead employing arbitrary chunks of ﬁxed length, our results show that even ﬁxed chunking can substantially improve memory. (Relatedly, Ke et al. [25] have shown that sparse, top-k retrieval of memory chunks can improve credit assignment in LSTMs with attention.)
We show that HCAM allows RL agents to recall events over longer intervals and with greater sample efﬁciency than prior memories. Agents with HCAM can learn new words and then maintain them over distractor phases. They can extrapolate far beyond the training distribution, to maintain and recall memories after 5× more distractors than during training, or after multiple episodes when trained only on single ones. Agents with HCAM can reason over their memories to plan paths near-optimally as they explore a new neighborhood, comparable to an agent architecture designed speciﬁcally for that task. HCAM is robust to hyperparameters, and agents with HCAM consistently outperform agents with Gated TransformerXL memories [44] that are twice as wide or twice as deep. Furthermore,
HCAM is more robust to varying hyperparameters than other architectures (App. D.11). Thus HCAM provides a substantial improvement in the memory capabilities of RL agents. 1.1