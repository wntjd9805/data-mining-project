Abstract
Variational autoencoders (VAEs) are one of the powerful likelihood-based gen-erative models with applications in many domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs’ poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior deﬁned by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments conﬁrm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets. Our method is simple and can be applied to a wide variety of VAEs to improve the expressivity of their prior distribution. 1

Introduction
Variational autoencoders (VAEs) [39, 64] are one of the powerful likelihood-based generative models that have applications in image gener-ation [6, 35, 62], music synthesis [12], speech generation [54, 60], image captioning [2, 3, 11], semi-supervised learning [33, 40], and representa-tion learning [15, 79].
Although there has been tremendous progress in improving the expressiv-ity of the approximate posterior, several studies have observed that VAE priors fail to match the aggregate (approximate) posterior [30, 66]. This phenomenon is sometimes described as holes in the prior, referring to regions in the latent space that are not decoded to data-like samples. Such regions often have a high density under the prior but have a low density under the aggregate approximate posterior.
Figure 1: We propose an EBM prior using the product of a base prior p(z) and a reweighting factor r(z), designed to bring p(z) closer to the aggregate posterior q(z).
The prior hole problem is commonly tackled by increasing the ﬂexibility of the prior via hierarchical priors [42], autoregressive models [21], a mixture of encoders [72], normalizing ﬂows [8, 81], resampled priors [5], and energy-based models [57, 75–77]. Among them, energy-based models (EBMs) [13, 57] have shown promising results. However, they require running iterative MCMC during training which is computationally expensive when the energy function is represented by a neural network. Moreover, they scale poorly to hierarchical models where an EBM is deﬁned on each group of latent variables. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our key insight in this work is that a trainable prior is brought as close as possible to the aggregate posterior as a result of training a VAE. The mismatch between the prior and the aggregate posterior can be reduced by reweighting the prior to re-adjust its likelihood in the area of mismatch with the aggregate posterior. To represent this reweighting mechanism, we formulate the prior using an EBM that is deﬁned by the product of a reweighting factor and a base trainable prior as shown in Fig. 1. We represent the reweighting factor using neural networks and the base prior using Normal distributions.
Instead of computationally expensive MCMC sampling, notorious for being slow and often sensitive to the choice of parameters [13], we use noise contrastive estimation (NCE) [22] for training the EBM prior. We show that NCE trains the reweighting factor in our prior by learning a binary classiﬁer to distinguish samples from a target distribution (i.e., approximate posterior) vs. samples from a noise distribution (i.e., the base trainable prior). However, since NCE’s success depends on closeness of the noise distribution to the target distribution, we ﬁrst train the VAE with the base prior to bring it close to the aggregate posterior. And then, we train the EBM prior using NCE.
In this paper, we make the following contributions: i) We propose an EBM prior termed noise contrastive prior (NCP) which is trained by contrasting samples from the aggregate posterior to samples from a base prior. NCPs are simple and can be learned as a post-training mechanism to improve the expressivity of the prior. ii) We also show how NCPs are trained on hierarchical VAEs with many latent variable groups. We show that training hierarchical NCPs scales easily to many groups, as they are trained for each latent variable group in parallel. iii) Finally, we demonstrate that
NCPs improve the generative quality of several forms of VAEs by a large margin across datasets. 2