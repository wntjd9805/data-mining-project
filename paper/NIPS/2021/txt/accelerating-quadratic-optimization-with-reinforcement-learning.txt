Abstract
First-order methods for quadratic optimization such as OSQP are widely used for large-scale machine learning and embedded optimal control, where many related problems must be rapidly solved. These methods face two persistent challenges: manual hyperparameter tuning and convergence time to high-accuracy solutions.
To address these, we explore how Reinforcement Learning (RL) can learn a policy to tune parameters to accelerate convergence. In experiments with well-known QP benchmarks we find that our RL policy, RLQP, significantly outperforms state-of-the-art QP solvers by up to 3x. RLQP generalizes surprisingly well to previously unseen problems with varying dimension and structure from different applications, including the QPLIB, Netlib LP and Maros-Mészáros problems. Code, models, and videos are available at https://berkeleyautomation.github.io/rlqp/. 1

Introduction
Solving quadratic programs (QPs) efficiently is critical to applications in finance, robotic control and operations research. While state-of-the-art interior-point methods scale poorly with problem dimensions, first-order methods for solving QPs typically require thousands of iterations. Moreover, real-time control applications have tight latency constraints for solvers [33]. Therefore, it is important to develop efficient heuristics to solve QPs in fewer iterations.
The Alternating Direction Method of Multipliers (ADMM) [6, 15, 18] is an efficient first-order optimization algorithm, and is the basis for the widely used and state-of-the art Operator-Splitting QP (OSQP) solver [44]. ADMM performs a linear solve on a matrix based on the optimality conditions of the QP to generate a step direction, and then projects the step onto the constraint bounds.
While state-of-the-art, the ADMM algorithm has numerous hyperparameters that must be tuned with heuristics to regularize and control optimization. Most importantly, the step size parameter ρ has considerable impact on the convergence rate. However, is still unclear how to select ρ before attempting the QP solution. While some theoretical works compute the optimal ρ [17], they rely on solving semidefinite optimization problems which are much harder than solving the QP itself.
Alternatively, some heuristics introduce “feedback” by adapting ρ throughout optimization in order to balance primal and dual residuals [44, 6, 22].
We propose RLQP (see Fig. 1), an accelerated QP solver based on OSQP that uses reinforcement learning (RL) to adapt the internal parameters of the ADMM algorithm between iterations to minimize solve times. An RL algorithm learns a policy πθ : S → A, parameterized by θ (e.g., the weights of a neural network), that maps states in a set S to actions in set A such that the selected action maximizes the accumulated reward r. To train the policy for RLQP, we define S to be the internal state of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Agent policy π(·)
QP
P, q, A, l, u
Environment (QP solver) state s action adapt ρ
KKT matrix
ADMM iterations parameters θ state x, y, z reward r = −1 state s = f (x, y, z; A, l, u)
Scalar adapt-ρ policy:


ρ ← πscalar(s) hi
... hm





Vector adapt-ρ policy:
πρ(si)
...
πρ(sm)


ρ ← πvec(s) =



Figure 1: RLQP uses deep reinforcement learning (RL) to compute a policy that adapts the internal parameters of a first-order quadratic program (QP) solver to speed up the solver’s convergence rate.
In a standard RL formulation, a policy computes an action based on its observation of the state of the environment, and taking the action results in a change in state and a reward. In RLQP, the policy is parameterized by a neural network, the state is a function of the solver’s internal state parameterized by the QP, the action changes a parameter (ρ) of the solver, and the reward minimizes QP solve time.
RLQP proposes 2 formulations of RL and associated policy: scalar and vector.
QP solver (e.g., the constraint bounds, the primal and dual estimates), A to be the adaptation to the internal parameter (ρ) vector, and r to minimize the number of ADMM iterations taken.
RLQP’s policy can be trained either jointly across general classes of QPs or with respect to a specific class. The general version of RLQP is trained once on a broad class of QPs and can be used out-of-the-box on new problems. The specialized version of RLQP is trained on a specific class of problems that the solver will repeatedly encounter. While this requires additional setup and training time, it is useful when QPs will be repeatedly solved in application (e.g., in a 100 Hz control loop).
In experiments, we train RLQP on a set of randomized QPs, and compare convergence rates of RLQP to non-adaptive and heuristic adaptive policies. To compare generalization and specialization, we investigate RLQP’s performance in the settings where 1) the train and test sets of QPs come from the same class of problems, 2) the train set contains problems from a superset of classes contained in the test set, 3) the train set contains a subset, and 4) when the train and test sets are from distinct classes.
In the results section we show that RLQP outperforms OSQP by up to 3x.
The contributions of this paper are:
• Two RL formulations to train policies that provide coarse (scalar) and fine (vector) grain updates to the internal parameters of a QP solver for faster convergence times
• Policies trained jointly across QP problem classes or to specialize to specific classes
• Experimental results showing that RLQP reduces convergence times by up to 3x and generalizes to different problem classes and outperform existing methods 2