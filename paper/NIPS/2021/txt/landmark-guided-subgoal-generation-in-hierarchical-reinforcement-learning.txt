Abstract
Goal-conditioned hierarchical reinforcement learning (HRL) has shown promising results for solving complex and long-horizon RL tasks. However, the action space of high-level policy in the goal-conditioned HRL is often large, so it results in poor exploration, leading to inefﬁciency in training. In this paper, we present HIerarchi-cal reinforcement learning Guided by Landmarks (HIGL), a novel framework for training a high-level policy with a reduced action space guided by landmarks, i.e., promising states to explore. The key component of HIGL is twofold: (a) sampling landmarks that are informative for exploration and (b) encouraging the high-level policy to generate a subgoal towards a selected landmark. For (a), we consider two criteria: coverage of the entire visited state space (i.e., dispersion of states) and novelty of states (i.e., prediction error of a state). For (b), we select a landmark as the very ﬁrst landmark in the shortest path in a graph whose nodes are landmarks.
Our experiments demonstrate that our framework outperforms prior-arts across a variety of control tasks, thanks to efﬁcient exploration guided by landmarks.1 1

Introduction
Deep reinforcement learning (RL) has demonstrated wide success in a variety of sequential decision-making problems, i.e., board games [38, 42], video games [1, 26, 38], and robotic control tasks
[15, 30, 53]. However, solving complex and long-horizon tasks has still remained a major challenge in RL, where hierarchical reinforcement learning (HRL) provides a promising direction by enabling control at multiple time scales via a hierarchical structure. Among HRL frameworks, goal-conditioned
HRL has long been recognized as an effective paradigm [5, 20, 29, 37], showing signiﬁcant success in a variety of long and complex tasks, e.g., navigation with locomotion [21, 54]. The framework comprises a high-level policy and a low-level policy; the former breaks the original task into a series of subgoals, and the latter aims to reach those subgoals.
The effectiveness of goal-conditioned HRL depends on the acquisition of effective and semantically meaningful subgoals. To this end, several strategies have been proposed, e.g., learning the subgoal representation space [6, 11, 21, 29, 31, 32, 35, 44, 50] or utilizing domain-speciﬁc knowledge for pre-deﬁning subgoal space [28, 54]. However, the learned or pre-deﬁned subgoal space is often too large, which results in poor high-level exploration, leading to inefﬁcient training. To address this issue, Zhang et al. [54] recently proposed a reduction of the high-level action space into the k-step adjacency region around the current state. This approach, however, is limited in that it considers all the states within the k-step adjacency region equally as the candidate actions for the high-level policy, without considering the novelty of states, which is crucial for exploration. 1Code is available https://github.com/junsu-kim97/HIGL 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of HIerarchical reinforcement learning Guided by Landmarks (HIGL). (1) We collect trajectories using a high-level and a low-level policy. (2) Sample landmarks from visited states based on “coverage” and “novelty” criteria, respectively, and merge them. (3) Select a single landmark among the sampled landmarks in a graph constructed by landmarks, a goal, and a current state. (i.e., select the very ﬁrst landmark in the shortest path to the goal). (4) Train a high-level policy to generate a subgoal toward the selected landmark.
Contribution.
In this paper, we present HIerarchical reinforcement learning Guided by Landmarks (HIGL), a novel framework for training a high-level policy that generates a subgoal toward landmarks, i.e., promising states to explore. HIGL consists of the following key ingredients (see Figure 1):
• Landmark sampling: To effectively sample landmarks that represent promising states to explore, it is important to sample landmarks that cover a wide area of state space and contain novel states.
To this end, we propose two sampling schemes: (a) coverage-based sampling scheme that samples located as far away from each other as possible and (b) novelty-based sampling scheme that stores novel states encountered during training and utilizes them as landmarks. We ﬁnd that our method successfully samples diverse and novel landmarks.
• Landmark-guided subgoal generation: Among the sampled landmarks, we select the most urgent landmark by our landmark selection scheme with the shortest path planning algorithm.
Then we propose to shift the action space of a high-level policy toward a selected landmark.
Because HIGL constructs a high-level action space that is both (a) reachable from the current state and (b) shifted towards a promising landmark state, we ﬁnd that the proposed method can effectively guide the subgoal generation of a high-level policy.
We demonstrate the effectiveness of HIGL on various long-horizon continuous control tasks based on
MuJoCo simulator [48], which is widely used in the HRL literature [9, 16, 28, 29]. In our experiments,
HIGL signiﬁcantly outperforms the prior state-of-the-art method, i.e., HRAC [54], especially in complex environments with sparse reward signals. For example, HIGL achieves the success rate of 65.1% in Ant Maze (sparse) environment, where HRAC only achieves 17.6%. 2