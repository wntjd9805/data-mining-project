Abstract
Differential Privacy (DP) [14] is an important privacy-enhancing technology for private machine learning systems. It allows to measure and bound the risk associ-ated with an individual participation in a computation. However, it was recently observed that DP learning systems may exacerbate bias and unfairness for different groups of individuals [3, 27, 23]. This paper builds on these important observa-tions and sheds light on the causes of the disparate impacts arising in the problem of differentially private empirical risk minimization. It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation [11] and differentially private stochastic gradient descent [2]. The paper analyzes which data and model properties are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects. The proposed approach is evaluated on several datasets and settings. 1

Introduction
While learning systems have become instrumental for many decisions and policy operations involving individuals, the use of rich datasets combined with the adoption of black-box algorithms has sparked concerns about how these systems operate. Two key concerns regard how these systems handle discrimination and how much information they leak about the individuals whose data is used as input.
Differential Privacy (DP) [14] has become the paradigm of choice for protecting data privacy and its deployments are growing at a fast rate. DP is appealing as it bounds the risks of disclosing sensitive information of individuals participating in a computation. However, it was recently observed that DP systems may induce biased and unfair outcomes for different groups of individuals [3, 23, 27]. The resulting outcomes can have significant societal and economic impacts on the involved individuals: classification errors may penalize some groups over others in important determinations including criminal assessment, landing, and hiring [3] or can result in disparities regarding the allocation of critical funds, benefits, and therapeutics [23]. While these surprising observations have become apparent in several contexts, their causes are largely understudied and not fully understood.
This paper makes a step toward addressing this important knowledge gap. It builds on these key observations and sheds light on the causes of the disparate impacts arising in the problem of dif-ferentially private empirical risk minimization (ERM). It focuses on the accuracy disparity arising among groups of individuals in two well-studied DP learning methods: output perturbation [11] and differentially private stochastic gradient descent (DP-SGD) [2]. The paper analyzes which properties of the model and the data are responsible for the disproportionate impacts, why these aspects are affecting different groups disproportionately, and proposes guidelines to mitigate these effects.
In summary, the paper makes the following contributions: 1. It develops a notion of fairness under private training that relies on the concept of excessive risk. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
2. It analyzes this fairness notion in two DP learning methods: output perturbation and DP-SGD. 3. It isolates the relevant components related with noise addition and gradient clipping responsible for the disparate impacts. 4. It studies the behaviors and the causes for these components to affect different groups of individuals disproportionately during private training. 5. Based on these observations, it proposes a mitigation solution and evaluates its effectiveness on several standard datasets.
To the best of the authors knowledge, this work represents a first step toward a deeper understanding of the causes of the unfairness impacts in differentially private learning. 2