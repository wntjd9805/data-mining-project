Abstract
This paper considers two fundamental sequential decision-making problems: the problem of prediction with expert advice and the multi-armed bandit problem. We focus on stochastic regimes in which an adversary may corrupt losses, and we inves-tigate what level of robustness can be achieved against adversarial corruption. The main contribution of this paper is to show that optimal robustness can be expressed by a square-root dependency on the amount of corruption. More precisely, we show that two classes of algorithms, anytime Hedge with decreasing learning rate and algorithms with second-order regret bounds, achieve O( log N
∆ )-regret, where N, ∆, and C represent the number of experts, the gap parameter, and the corruption level, respectively. We further provide a matching lower bound, which means that this regret bound is tight up to a constant factor. For the multi-armed bandit problem, we also provide a nearly-tight lower bound up to a logarithmic factor. (cid:113) C log N
∆ + 1

Introduction
In this work, we consider two fundamental sequential decision-making problems, the problem of prediction with expert advice (expert problem) and the multi-armed bandit (MAB) problem. In both, a player chooses probability vectors pt over a given action set [N ] = {1, 2, . . . , N } in a sequential manner. More precisely, in each round t, a player chooses a probability vector pt ∈ [0, 1]N over the action set, and then an environment chooses a loss vector (cid:96)t ∈ [0, 1]N . The player chooses pt, and then observes (cid:96)t in the expert problem. In the MAB problem, the player picks action it ∈ [N ] following pt and then observes (cid:96)tit. The goal of the player is to minimize the (pseudo-) regret ¯RT deﬁned as
RT i∗ =
T (cid:88) t=1 (cid:96)(cid:62) t pt −
T (cid:88) t=1 (cid:96)ti∗ ,
¯RT i∗ = E [RT i∗ ] ,
¯RT = max i∗∈[N ]
¯RT i∗ . (1)
For such decision-making problems, two main types of environments have been studied: stochastic environments and adversarial environments. In stochastic environments, the loss vectors are assumed to follow an unknown distribution, i.i.d. for all rounds. It is known that the difﬁculty of the problems can be characterized by the suboptimality gap parameter ∆ > 0, which denotes the minimum gap between the expected losses for the optimal action and for suboptimal actions. Given the parameter
∆, mini-max optimal regret bounds can be expressed as Θ( log N
∆ ) in the expert problem [Degenne and
Perchet, 2016, Mourtada and Gaïffas, 2019] and Θ( N log T
∆ ) in the MAB problem [Auer et al., 2002a,
Lai and Robbins, 1985, Lai, 1987]. In contrast to the stochastic model, the adversarial model does not assume any generative models for loss vectors, but the loss at each round may behave adversarially depending on the choices of the player up until that round. The mini-max optimal regret bounds
T log N ) in the expert problem [Cesa-Bianchi and Lugosi, 2006, for the adversarial model are Θ(
√ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Regret bounds in stochastic regimes with adversarial corruption
Problem setting
Expert problem
O (cid:18) (cid:18)
Multi-armed bandit O
Upper bound (cid:16) log N
∆ + C
O log N
∆ + (cid:17)1[Amir et al., 2020] (cid:19) (cid:113) C log N
∆
[Theorems 3, 4]
N log T
∆ + (cid:19) (cid:113) CN log T
∆
[Zimmert and Seldin, 2021]
Lower bound (cid:18)
Ω log N
∆ + (cid:19) (cid:113) C log N
∆
[Theorem 5] (cid:17) (cid:113) CN
∆ (cid:16) N
Ω
∆ +
[Theorem 6]
Freund and Schapire, 1997] and Θ( et al., 2002b].
√
T N ) in the MAB problem [Audibert and Bubeck, 2009, Auer
As can be seen in the regret bounds, achievable performance differs greatly between the stochastic and adversarial regimes, which implies that the choice of models and algorithms will matter in many practical applications. One promising solution to this challenge is to develop best-of-both-worlds (BOBW) algorithms, which perform (nearly) optimally in both stochastic and adversarial regimes. For the expert problem, Gaillard et al. [2014] provide an algorithm with a BOBW property, and Mourtada and Gaïffas [2019] have shown that the well-known Hedge algorithm with decreasing learning rate (decreasing Hedge) enjoys a BOBW property as well. For the MAB problem, the Tsallis-INF algorithm by Zimmert and Seldin [2021] has a BOBW property, i.e., achieves O( N log T
∆ )-regret in the stochastic regime and O(
N T )-regret in the adversarial regime. One limitation of BOBW guarantees is, however, that they do not necessarily provide nontrivial regret bounds for a situation in which the stochastic and the adversarial regimes are mixed, i.e., an intermediate situation.
√
To overcome this BOBW-property limitation, our work focuses on an intermediate (and compre-hensive) regime between the stochastic and adversarial settings. More speciﬁcally, we consider the adversarial regime with a self-bounding constraint introduced by Zimmert and Seldin [2021]. As shown by them, this regime includes the stochastic regime with adversarial corruption [Lykouris et al., 2018, Amir et al., 2020] as a special case in which an adversary modiﬁes the i.i.d. losses to the extent that the total amount of changes does not exceed C, which is an unknown parame-ter referred to as the corruption level. For expert problems in the stochastic regime with adver-sarial corruption, Amir et al. [2020] have shown that the decreasing Hedge algorithm achieves
O( log N
∆ + C)-regret. For the MAB problem, Zimmert and Seldin [2021] have shown that Tsallis-INF achieves O( N log T
)-regret in adversarial regimes with self-bounding constraints. To be more precise, they have proved a regret upper bound of O((cid:80)
), where i∗ ∈ [N ] is the optimal action and ∆i represents the suboptimality gap for each action i ∈ [N ] \ {i∗}. Ito [2021b] has shown that similar regret bounds hold even when there are mul-tiple optimal actions, i.e., even if the number of actions i with ∆i = 0 is greater than 1.
In such cases, the terms of (cid:80)
. In addi-tion to this, Masoudian and Seldin [2021] have improved the analysis to obtain a reﬁned regret i(cid:54)=i∗ 1/∆i in regret bounds are replaced with (cid:80) (cid:115) (cid:113) CN log T
∆
∆ +
C (cid:80) log T
∆i log T
∆i i:∆i>0 1
∆i i(cid:54)=i∗ i(cid:54)=i∗ (cid:113)
+ (cid:32) (cid:16)(cid:80) (cid:17) 1
∆i i(cid:54)=i∗ log+ (cid:18) (cid:19) (N −1)T 1
∆i i(cid:54)=i∗ ((cid:80)
+
C
)2 (cid:16)(cid:80) i(cid:54)=i∗ (cid:18) (cid:17) 1
∆i log+ (N −1)T
C (cid:80) i(cid:54)=i∗ 1
∆i (cid:19)(cid:33)
, where bound of O log+(x) = max {1, log x}.
The contributions of this work are summarized in Table 1, alongside previously reported results.
As shown in Theorems 3 and 4, this paper provides an improved regret upper bound of O( log N
∆ + (cid:113) C log N
∆ ) for the expert problem in the adversarial regime with self-bounding constraints. This regret upper bound is tight up to a constant factor. In fact, we provide a matching lower bound in
Theorem 5. In addition to this, we show an Ω( N
∆ )-lower bound for MAB, which implies that (cid:113) CN
∆ + 1Note that Amir et al. [2020] adopt a different deﬁnition of regret than in this paper. Details and notes for comparison are discussed in Remark 1. 2
Tsallis-INF by Zimmert and Seldin [2021] achieves a nearly optimal regret bound up to an O(log T ) factor in the adversarial regime with self-bounding constraints.
The regret bounds in Theorems 3 and 4 are smaller than the regret bound shown by Amir et al.
[2020] for the stochastic regime with adversarial corruption, especially when C = Ω( log N
∆ ), and they can be applied to more general problem settings in the adversarial regime with self-bounding constraints. Note here that this study and their study consider slightly different deﬁnitions of regret.
More precisely, they deﬁne regret using losses without corruption, while this study uses losses after corruption to deﬁne regret. In practice, appropriate deﬁnitions would vary depending on individual situations. For example, if each expert’s prediction is itself corrupted, the latter deﬁnition is suitable.
In contrast, if only the observation of the player is corrupted, the former deﬁnition seems appropriate.
However, even after taking this difference in deﬁnitions into account, we can see that the regret bound in our work is, in a sense, stronger than theirs, as is discussed in Remark 1 of this paper. In particular, we would like to emphasize that the new bound of O( log N
∆ ) provides the ﬁrst theoretical evidence implying that the corresponding algorithms are more robust against adversarial corruption than the naive Follow-the-Leader algorithm. Note also that the regret bound by Amir et al. [2020] is tight as long as the former regret deﬁnition is used. (cid:113) C log N
∆ +
This work shows the tight regret upper bounds for two types of known algorithms. The ﬁrst, (Theorem 3), is the decreasing Hedge algorithm, which has been analyzed by Amir et al. [2020] and Mourtada and Gaïffas [2019] as well. The second (Theorem 4) represents algorithms with second-order regret bounds [Cesa-Bianchi et al., 2007, Gaillard et al., 2014, Hazan and Kale, 2010,
Steinhardt and Liang, 2014, Luo and Schapire, 2015]. It is worth mentioning that Gaillard et al.
[2014] have shown that a kind of second-order regret bounds implies O( log N
∆ )-regret in the stochastic regime. Theorem 4 in this work extends their analysis to a broader setting of the adversarial regime with self-bounding constraints. In the proof of Theorems 3 and 4, we follow a proof technique given by Zimmert and Seldin [2021] to exploit self-bounding constraints.
To show regret lower bounds in Theorems 5 and 6, we construct speciﬁc environments with corruption that provide insight into effective attacks which would make learning fail. Our approach to corruption is to modify the losses so that the optimality gaps decrease. This approach achieves a mini-max lower bound in the expert problem (Theorem 5) and a nearly-tight lower bound in MAB up to a logarithmic factor in T (Theorem 6). We conjecture that there is room for improvement in this lower bound for
MAB under assumptions regarding consistent policies [Lai and Robbins, 1985], and that the upper bound by Zimmert and Seldin [2021] is tight up to a constant factor. 2