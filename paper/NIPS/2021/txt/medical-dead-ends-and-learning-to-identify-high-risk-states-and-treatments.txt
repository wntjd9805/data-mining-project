Abstract
Machine learning has successfully framed many sequential decision making prob-lems as either supervised prediction, or optimal decision-making policy iden-tiﬁcation via reinforcement learning. In data-constrained ofﬂine settings, both approaches may fail as they assume fully optimal behavior or rely on exploring alternatives that may not exist. We introduce an inherently different approach that identiﬁes possible “dead-ends” of a state space. We focus on the condition of pa-tients in the intensive care unit, where a “medical dead-end” indicates that a patient will expire, regardless of all potential future treatment sequences. We postulate
“treatment security” as avoiding treatments with probability proportional to their chance of leading to dead-ends, present a formal proof, and frame discovery as an
RL problem. We then train three independent deep neural models for automated state construction, dead-end discovery and conﬁrmation. Our empirical results discover that dead-ends exist in real clinical data among septic patients, and further reveal gaps between secure treatments and those that were administered. 1

Introduction
Off-policy Reinforcement Learning (RL) was designed as the way to isolate behavioural policies, which generate experience, from the target policy, which aims for optimality. It also enables learning multiple target policies with different goals from the same data-stream or from previously recorded experience [1]. This algorithmic approach is of particular importance in safety-critical domains such as robotics [2], education [3] or healthcare [4] where data collection should be regulated as it is expensive or carries signiﬁcant risk. Despite signiﬁcant advances made possible by off-policy RL combined with deep neural networks [5–7], the performance of these algorithms degrade drastically in fully ofﬂine settings [8], without additional interactions with the environment [9, 10]. These challenges are deeply ampliﬁed when the dataset is limited and exploratory new data cannot be collected for ethical or safety purposes. This is because robust identiﬁcation of an optimal policy requires exhaustive trial and error of various courses of actions [11, 12]. In such fully ofﬂine cases, naively learned policies may signiﬁcantly overﬁt to data-collection artifacts [13–15]. Estimation errors due to limited data may further lead to mistimed or inappropriate decisions with adverse safety consequences [16].
Even if optimality is not attainable in such constrained cases, negative outcomes in data can be used to identify behaviors to avoid, thereby guarding against overoptimistic decisions in safety-critical domains that may be signiﬁcantly biased due to reduced data availability. In one such domain, healthcare, RL has been used to identify optimal treatment policies based on observed outcomes of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
past treatments [17]. These policies correspond to advising what treatments to administer, given a patient’s condition. Unfortunately, exploration of potential courses of treatment is not possible in most clinical settings due to legal and ethical implications; hence, RL estimates of optimal policies are largely unreliable in healthcare [18].
In this paper, we develop a novel RL-based method, Dead-end Discovery (DeD), to identify treatments to avoid as opposed to what treatment to select. Our paradigm shift avoids pitfalls that may arise from constraining policies to remain close to possibly suboptimal recorded behavior as is typical in current state of the art ofﬂine RL approaches [10, 19–21]. When the data lacks sufﬁcient amounts of exploratory behavior, these methods fail to attain a reliable policy. We instead use this data to constrain the scope of the policy, based on retrospective analysis of observed outcomes, a more tractable approach when data is limited. Our goal is to avoid future dead-ends or regions in the state space from which negative outcomes are inevitable (formally deﬁned in Section 3.2). DeD identiﬁes dead-ends via two complementary Markov Decision Processes (MDPs) with a speciﬁc reward design so that the underlying value functions will carry special meaning (Section 3.4). These value functions are independently estimated using Deep Q-Networks (DQN) [5] to infer the likelihood of a negative outcome occurring (D-Network) and the reachability of a positive outcome (R-Network). Altogether
DeD formally connects the notion of value functions to the dead-end problem, learned directly from ofﬂine data.
We validate DeD in a carefully constructed toy domain, and then evaluate real health records of septic patients in an intensive care unit (ICU) setting [22]. Sepsis treatment and onset is a common task in medical RL [23–26] because the condition is highly prevalent [27, 28], physiologically severe [29], costly [30] and poorly understood [31]. Notably, the treatment of sepsis itself may also contribute to a patient’s deterioration [32, 33], thus making treatment avoidance a particularly well-suited objective.
We ﬁnd that DeD conﬁrms the existence of dead-ends and demonstrate that 12% of treatments administered to terminally ill patients reduce their chances of survival, some occurring as early as 24 hours prior to death. The estimated value functions underlying DeD are able to capture signiﬁcant deterioration in patient health 4 to 8 hours ahead of observed clinical interventions, and that higher-risk treatments possibly account for this delay. Early identiﬁcation of suboptimal treatment options is of great importance since sepsis treatment has shown multiple interventions within tight time frames (10 to 180 minutes) after suspected onset decreases sepsis mortality [34].
While motivated by healthcare, we propose the use of DeD in safety-critical applications of RL in most data-constrained settings. We introduce a formal methodology that outlines how DeD can be implemented within an RL framework for use with real-world ofﬂine data. We construct and train DeD in a generic manner which can readily be used for other data-constrained sequential decision-making problems. In particular, we emphasize that DeD is well suited to analyze high-risk decisions in real-world domains. 2