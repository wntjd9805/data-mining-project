Abstract
Graph neural networks (GNN) have recently emerged as a vehicle for applying deep network architectures to graph and relational data. However, given the increasing size of industrial datasets, in many practical situations the message passing computations required for sharing information across GNN layers are no longer scalable. Although various sampling methods have been introduced to approximate full-graph training within a tractable budget, there remain unresolved complications such as high variances and limited theoretical guarantees. To address these issues, we build upon existing work and treat GNN neighbor sampling as a multi-armed bandit problem but with a newly-designed reward function that introduces some degree of bias designed to reduce variance and avoid unstable, possibly-unbounded pay outs. And unlike prior bandit-GNN use cases, the resulting policy leads to near-optimal regret while accounting for the GNN training dynamics introduced by SGD. From a practical standpoint, this translates into lower variance estimates and competitive or superior test accuracy across several benchmarks. 1

Introduction
Graph convolution networks (GCN) and Graph neural networks (GNN) in general [21, 17] have recently become a powerful tool for representation learning for graph structured data [6, 2, 33]. These neural networks iteratively update the representation of a node using a graph convolution operator or message passing operator which aggregate the embeddings of the neighbors of the node, followed by a non-linear transformation. After stacking multiple graph convolution layers, these models can learn node representations which can capture information from both immediate and distant neighbors.
GCNs and variants [32] have demonstrated the start-of-art performance in a diverse range of graph learning prolems [21, 17, 3, 30, 13, 15, 23]. However, they face signiﬁcant computational challenges given the increasing sizes of modern industrial datasets. The multilayers of graph convolutions is equivalent to recursively unfold the neighbor aggregation in a top-down manner which will lead to an exponentially growing neighborhood size with respect to the number of layers. If the graph is dense and scale-free, the computation of embeddings will involve a large portion of the graph even with a few layers, which is intractable for large-scale graph [21, 34].
Several sampling methods have been proposed to alleviate the exponentially growing neighborhood sizes, including node-wise sampling [17, 9, 24], layer-wise sampling [8, 37, 20] and subgraph sampling [10, 35, 19]. However, the optimal sampler with minimum variance is a function of the neighbors’ embeddings unknown apriori before the sampling and only partially observable for those sampled neighbors. Most previous methods approximate the optimal sampler with a static distribution which cannot reduce variance properly. And most of existing approaches [8, 37, 20, 10, 35, 19]
∗Corresponding author. Work done during the internship at Amazon Shanghai AI Lab. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
do not provide any asymptotic convergence guarantee on the sampling variance. We are therefore less likely to be conﬁdent of their behavior as GNN models are applied to larger and larger graphs.
Recently, Liu et al. [24] propose a novel formulation of neighbor sampling as a multi-armed bandit problem (MAB) and apply bandit algorithms to update sampler and reduce variance. Theoretically, they provide an asymptotic regret analysis on sampling variance. Empirically, this dynamic sampler named as BanditSampler is more ﬂexible to capture the underlying dynamics of embeddings and exhibits promising performance in a variety of datasets.
However, we will show in Section 2.3 that there are several critical issues related to the numerical stability and theoretical limitations of the BanditSampler [24]. First, the reward function designed is numerically unstable. Second, the bounded regret still can be regarded as a linear function of training horizon T . Third, their analysis relies on two strong implicit assumptions, and does not account for the unavoidable dependency between embedding-dependent rewards and GNN training dynamics.
In this paper, we build upon the bandit formulation for GNN sampling and propose a newly-designed reward function that trades bias with variance. In Section 3.1, we highlight that the proposed reward has the following crucial advantages: (i) It is numerically stable. (ii) It leads to a more meaningful notion of regret directly connected to sampling approximation error, the expected error between aggregation from sampling and that from full neighborhood. (iii) Its variation can be formulated by GNN training dynamics. Then in Section 3.2, we clarify how the induced regret is connected to sampling approximation error and emphasize that the bounded variation of rewards is essential to derive a meaningful sublinear regret, i.e., a per-iteration regret that decays to zero as T becomes large. In that sense, we are the ﬁrst to explicitly account for GNN training dynamic due to stochastic gradient descent (SGD) so as to establish a bounded variation of embedding-dependent rewards, which we present in Section 3.3.
√
Based on that, in Section 4, we prove our main result, namely, that the regret of the proposed algorithm ln T )2/3, which is near-optimal and manifest that the sampling approximation as the order of (T error of our algorithm asymptotically converges to that of the optimal oracle with the near-fastest rate. Hence we name our algorithm as Thanos from "Thanos Has A Near-Optimal Sampler". Finally, empirical results in Section 5 demonstrate the improvement of Thanos over BanditSampler and others in terms of variance reduction and generalization performance. 2