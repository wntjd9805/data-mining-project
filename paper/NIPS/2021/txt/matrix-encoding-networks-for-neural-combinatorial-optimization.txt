Abstract
Machine Learning (ML) can help solve combinatorial optimization (CO) prob-lems better. A popular approach is to use a neural net to compute on the parame-ters of a given CO problem and extract useful information that guides the search for good solutions. Many CO problems of practical importance can be speciﬁed in a matrix form of parameters quantifying the relationship between two groups of items. There is currently no neural net model, however, that takes in such matrix-style relationship data as an input. Consequently, these types of CO problems have been out of reach for ML engineers. In this paper, we introduce Matrix Encoding
Network (MatNet) and show how conveniently it takes in and processes parame-ters of such complex CO problems. Using an end-to-end model based on MatNet, we solve asymmetric traveling salesman (ATSP) and ﬂexible ﬂow shop (FFSP) problems as the earliest neural approach. In particular, for a class of FFSP we have tested MatNet on, we demonstrate a far superior empirical performance to any methods (neural or not) known to date. 1

Introduction
Many combinatorial optimization (CO) problems of industrial importance are NP-hard, leaving them intractable to solve optimally at a large scale. Fortunately, researchers in operations research (OR) have developed ways to tackle these NP-hard problems in practice, mixed integer programming (MIP) and meta-heuristics being two of the most general and popular approaches. With the rapid progress in deep learning techniques over the last several years, a new approach based on machine learning (ML) has emerged. ML has been applied in both ways successfully [1], as a helper for the traditional OR methods aforementioned or as an independent CO problem solver trained in an end-to-end fashion.
One way to leverage ML for solving CO problems is to employ a “front-end” neural net, which directly takes in the data specifying each problem. The neural net plays a critical role of analyzing all input data as a whole, from which it extracts useful global information. In an end-to-end ML-based approach, such global information may be encoded onto the representations of the entities making up the problem. Greedy selection strategies based on these representations are then no longer too near-sighted, allowing globally (near-) optimal solutions to be found quickly. Existing OR methods can also beneﬁt from incorporating the global information. Many hybrid approaches already exist that leverage information extracted by a neural net, both in ML-MIP forms [2, 3] and ML-heuritic forms [4, 5, 6].
The literature on neural combinatorial optimization contains many different types of the front-end models, conforming to the variety of data types upon which CO problems are deﬁned. Yet, there has been no research for a model that encodes matrix-type data. This puts a serious limitation on the range of CO problems that an ML engineer can engage. Take, for example, the traveling salesman problem (TSP), the most intensely studied topic by the research community of neural combinatorial 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
optimization. Once we lift the Euclidean distance restriction (the very ﬁrst step towards making the problem more realistic), the problem instantly becomes a formidable challenge that has never been tackled before because it requires a neural net that can process the distance matrix.1
The list of other classical CO problems based on data matrices includes job-shop/ﬂow-shop schedul-ing problems and linear/quadratic assignment problems, just to name a few. All of these examples are fundamental CO problems with critical industrial applications. To put it in general terms, imagine a
CO problem made up of two different classes of items, ˜A = {˜a1, . . . , ˜aM } and ˜B = {˜b1, . . . , ˜bN }, where M and N are the sizes of ˜A and ˜B respectively. We are interested in the type where features of each item are deﬁned by its relationships with those in the other class. The data matrix we want to encode, D ∈ RM ×N , would be given by the problem,2 where its (i, j)th element represents a quanti-tative relationship of some sort between a pair of items (˜ai, ˜bj). The sets ˜A and ˜B are unordered lists, meaning that the orderings of the rows and the columns of D are arbitrary. Such permutation invari-ance built into our data matrices is what sets them apart from other types of matrices representing stacked vector-lists or a 2D image.
In this paper, we propose Matrix Encoding Network (MatNet) that computes good representations for all items in ˜A and ˜B within which the matrix D containing the relationship information is en-coded. To demonstrate its performance, we have implemented MatNet as the front-end models for two end-to-end reinforcement learning (RL) algorithms that solve the asymmetric traveling sales-man (ATSP) and the ﬂexible ﬂow shop (FFSP) problems. These classical CO problems have not been solved using deep neural networks. From our experiments, we have conﬁrmed that MatNet achieves near-optimal solutions. Especially, for the speciﬁc FFSP instances we have investigated, our MatNet-based approach performs substantially better than the conventional methods used in operations research including mixed integer programming and meta-heuristics. 2