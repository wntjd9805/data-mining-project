Abstract
We address an inherent difﬁculty in welfare-theoretic fair machine learning (ML), by proposing an equivalently-axiomatically justiﬁed alternative setting, and study-ing the resulting computational and statistical learning questions. Welfare metrics quantify overall wellbeing across a population of groups, and welfare-based objec-tives and constraints have recently been proposed to incentivize fair ML methods to satisfy their diverse needs. However, many ML problems are cast as loss minimiza-tion tasks, rather than utility maximization, and thus require nontrivial modeling to construct utility functions. We deﬁne a complementary metric, termed malfare, measuring overall societal harm, with axiomatic justiﬁcation via the standard axioms of cardinal welfare, and cast fair ML as malfare minimization over the risk values (expected losses) of each group. Surprisingly, the axioms of cardinal welfare (malfare) dictate that this is not equivalent to simply deﬁning utility as negative loss and maximizing welfare. Building upon these concepts, we deﬁne fair-PAC learning, where a fair-PAC learner is an algorithm that learns an ε-δ malfare-optimal model with bounded sample complexity, for any data distribution and (axiomatically justiﬁed) malfare concept. Finally, we show conditions under which many standard PAC-learners may be converted to fair-PAC learners, which places fair-PAC learning on ﬁrm theoretical ground, as it yields statistical — and in some cases computational — efﬁciency guarantees for many well-studied ML models. Fair-PAC learning is also practically relevant, as it democratizes fair ML by providing concrete training algorithms with rigorous generalization guarantees. 1

Introduction
It is now well-understood that contemporary ML systems exhibit differential accuracy across gender, race, and other protected groups, for tasks like facial recognition [5–7], in medical settings [2, 17], and many others. This exacerbates existing inequality, as facial recognition in policing yields disproportionate false-arrest rates, and medical ML yields disproportionate health outcomes. Welfare-centric ML methods encode both accuracy and fairness in a welfare function deﬁned on a set of groups, and optimize or constrain welfare to learn fairly. This addresses differential accuracy and bias issues across groups by ensuring that (1) each group is seen and considered during training, and (2) an outcome is incentivized that is desirable overall, ideally according to some mutually-agreed-upon welfare function. Unfortunately, welfare metrics require a notion of (positive) utility, and we argue that this is not natural to many ML tasks, wherein we instead minimize some negatively connoted loss value. We thus deﬁne a complementary measure to welfare, which we term malfare, measuring societal harm (rather than wellbeing). In particular, malfare arises naturally when one applies the standard axioms of cardinal welfare to loss values rather than utility values. We then cast fair ML as a direct malfare minimization problem, and study its computational and statistical aspects. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Perhaps surprisingly, minimizing a malfare function is not equivalent to maximizing some welfare function while taking utility to be negative loss, except in the egalitarian and utilitarian edge cases.
This is because nearly every function satisfying the standard axioms of cardinal welfare requires nonnegative inputs, and it is in general impossible to contort a loss function into a utility function while satisfying this requirement. For example, while minimizing the 0-1 loss, which simply counts the number of mistakes a classiﬁer makes, is isomorphic to maximizing the 1-0 gain, which counts number of correct classiﬁcations, minimizing some malfare function deﬁned on 0-1 loss over groups is not in general equivalent to maximizing any welfare function deﬁned on 1-0 gain.
Building upon these concepts, we develop a generic notion of fair ML, termed fair-PAC (FPAC) learning, where the goal is to learn models for which ﬁnite training samples may guarantee (additively)
ε-optimal malfare, with probability at least 1 − δ, for any (axiomatically justiﬁed) malfare concept.
This deﬁnition extends Valiant’s [31] classic PAC-learning formalization, and we show that, with appropriate modiﬁcations, many (standard) PAC-learners may be converted to FPAC learners. In particular, we show via a constructive polynomial reduction that realizable FPAC-learning reduces to realizable PAC-learning. Furthermore, we show, non-constructively, that for learning problems where PAC-learnability implies uniform convergence, it is equivalent to FPAC-learnability. We also show that when training is possible via convex optimization under standard assumptions, then training
ε-δ malfare-optimal models, like training risk-optimal models, requires polynomial time. We brieﬂy summarize our contributions below. 1. We derive in section 2 the malfare concept, extending welfare to measure negatively-connoted attributes, and show that malfare-minimization naturally generalizes risk-minimization to produce fairness-sensitive ML objectives that consider multiple protected groups. 2. Section 3 extends PAC-learning to FPAC-learning, where we consider minimization of malfare rather than risk (expected loss) objectives. Both PAC and FPAC learning are parameterized by a learning task (model space and loss function), and we explore the resulting learnability hierarchy. 3. We show that for many loss functions, PAC and FPAC learning are statistically equivalent, and convexity conditions for computationally efﬁcient PAC-learning are also sufﬁcient for FPAC-learning. 1.1