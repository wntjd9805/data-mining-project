Abstract
Empirically it has been observed that the performance of deep neural networks steadily improves with increased model size, contradicting the classical view on overﬁtting and generalization. Recently, the double descent phenomenon has been proposed to reconcile this observation with theory, suggesting that the test error has a second descent when the model becomes sufﬁciently overparameterized, as the model size itself acts as an implicit regularizer. In this paper we add to the growing body of work in this space, providing a careful study of learning dynamics as a function of model size for the least squares scenario. We show an excess risk bound for the gradient descent solution of the least squares objective. The bound depends on the smallest non-zero eigenvalue of the sample covariance matrix of the input features, via a functional form that has the double descent behaviour. This gives a new perspective on the double descent curves reported in the literature, as our analysis of the excess risk allows to decouple the effect of optimization and generalization error. In particular, we ﬁnd that in the case of noiseless regression, double descent is explained solely by optimization-related quantities, which was missed in studies focusing on the Moore-Penrose pseudoinverse solution. We believe that our derivation provides an alternative view compared to existing works, shedding some light on a possible cause of this phenomenon, at least in the considered least squares setting. We empirically explore if our predictions hold for neural networks, in particular whether the spectrum of the sample covariance of features at intermediary hidden layers has a similar behaviour as the one predicted by our derivations in the least squares setting. 1

Introduction
Deep Neural Networks optimized by Gradient Descent (GD) methods have shown amazing versatility across a large range of domains. One of their most intriguing features is their ability to perform better with scale. Indeed, some of the most impressive results [see e.g. Brock et al., 2021, Brown et al., 2020, Senior et al., 2020, Schrittwieser et al., 2020, Silver et al., 2017, He et al., 2016 and references therein] have been obtained often by exploiting this fact, leading to neural network models that have at least as many parameters as the number of examples in the dataset they are trained on.
Empirically, the limitation on the model size seems to be mostly imposed by hardware or compute.
From a theoretical point of view, however, this property is quite surprising and counter-intuitive, as one would expect that in such extremely overparameterized regimes the learning would be prone to overﬁtting [Hastie et al., 2009, Shalev-Shwartz and Ben-David, 2014]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recently Belkin et al. [2019] proposed Double Descent (DD) phenomenon as an explanation. They argue that the classical view of overﬁtting does not apply in extremely overparameterized regimes, which were less studied prior to the emergence of the deep learning era. The classical view in the parametric learning models was based on error curves showing that the training error decreases monotonically when plotted against model size, while the corresponding test errors displayed a
U-shape curve, where the model size for the bottom of the U-shape was taken to be a “sweet spot”
[cf. Belkin et al., 2019, Fig. 1A] achieving an ideal trade-off between model size and generalization.
Here, larger model sizes than the aforementioned ideal were thought to constitute the ‘overﬁtting’ regime, since the gap between the test error and the training error increased.
Figure 1: Evaluation of a synthetic setting inspired by Belkin et al. [2020]. We consider a linear regression problem (n = 20, d ∈ [100]), where regression parameters are ﬁxed, and instances are sampled from a truncated (to [−1, 1]) normal density. GD is run with learning rate α = 0.05 and initialization drawn from N (0, 1 d I).
The ﬁrst row demonstrates behavior of the excess risk (see Eq. (1)), the second shows an estimate of the excess risk (on 104 held-out points), and the third shows an estimate of the optimization error.
The classical U-shape test error curve dwells in what is now called the under-parameterized regime, where the model size is smaller than the size of the dataset. Arguably, the restricted model sizes used in the past were tied to the available computing power. By contrast, it is common nowadays for model sizes to be larger than the amount of available data, which is called the over-parameterized regime.
The divide between these two regimes is marked by a point where model size matches dataset size, which can be called the interpolation threshold [cf. Belkin et al., 2019, Fig. 1B].
The work of Belkin et al. [2019] argues that as model size grows beyond the interpolation threshold, one will observe a second descent of the test error that asymptotes in the limit to smaller values than those in the underparameterized regime, which indicates better generalization rather than overﬁtting.
To some extent this was already known in the nonparametric learning where model complexity scales with the amount of data by design (such as in nearest neighbor rules and kernels), yet one can generalize well and even achieve statistical consistency [Györﬁ et al., 2002]. This has lead to a growing body of works trying to identify the mechanisms behind DD, to which the current manuscript belongs too. We refer the reader to Section 2, where the related literature is discussed. Similar to these works, our goal is also to understand the cause of DD. In this paper we explore a ﬁnite-time
GD solution to least squares problems that allows us to work with analytic expressions for all the quantities involved. Fig. 1 provides a summary of our ﬁndings. In particular, it shows the behaviour of the risk (or the expected loss of the algorithm) in a linear regression setting with random inputs 2
and noise-free labels, for which in Section 3 we prove a bound that has the form (cid:18) (1 − α(cid:98)λ+ min)2T + (cid:19) 1
√ n (cid:107)w(cid:63)(cid:107)2 . (cid:112)
Here labels are generated as Y = X (cid:62)w(cid:63) for some unknown ground truth model w(cid:63) ∈ Rd and
X (cid:62)M X = 1 for a positive matrix M ). The random inputs X ∈ Rd on a unit sphere ((cid:107)X(cid:107)M = factor α is a constant learning rate, and n is the number of examples in the training set. Note that the feature dimension d coincides with the number of parameters in this particular setting, hence d > n is the overparameterized regime. The quantity (cid:98)λ+ min is the smallest positive eigenvalue of the sample covariance matrix of the inputs, and it is of special importance: We observe that the shape of the bound is controlled by (cid:98)λ+ min via the exponential term, which increases as (cid:98)λ+ min decreases while it decreases as (cid:98)λ+ min grows, resulting in a peaked shape of the bound. In Fig. 1 we observe a peaking behavior not only in the risk but also in the quantity that we label ‘optimization error’ which is this special term of the risk bound that is purely related to optimization. The peaking behaviour of the risk—the Mean Squared Error (MSE) in case of the squared loss—was observed and studied in a number of settings [Belkin et al., 2019, Mei and Montanari, 2019, Derezinski et al., 2020] sometimes relating it to (cid:98)λ+ min, however, to the best of our knowledge the connection between the peaking behavior and optimization so far received less attention. In the absence of label noise, we conclude that DD manifests due to the optimization process. On the other hand, when label noise is present, in addition to the optimization effect, (cid:98)λ+ min also has an effect on the generalization error.
Our contributions: Our main theoretical contribution is presented in Section 3. In particular,
Section 3.1 focuses on the noise-free least squares problem, Section 3.2 adds noise to the problem, and
Section 3.3 deals with concentration of the sample-dependent (cid:98)λ+ min around its population counterpart.
The essential idea of the proof of our main result is outlined in Section 5. Sections 4 and 6 provide some discussion on the implications of our ﬁndings and an empirical exploration of the question whether simple neural networks have a similar behaviour than that of the least squares setting.
Notation: The linear algebra and analysis notations used in this work are deﬁned in Appendix A.
We brieﬂy mention here that we denote column vectors and matrices with small and capital bold letters, respectively, e.g. α = [α1, α2, . . . , αd](cid:62) ∈ Rd and A ∈ Rd1×d2 . Singular values of a rectangular matrix A ∈ Rn×d are denoted by smax(A) = s1(A) ≥ . . . ≥ sn∧d(A) = smin(A). The rank of A is r = max{k | sk(A) > 0}. The eigenvalues of a Positive Semi-Deﬁnite (PSD) matrix
M ∈ Rd×d are non-negative and are denoted λmax(M ) = λ1(M ) ≥ . . . ≥ λd(M ) = λmin(M ), while λ+ min(M ) denotes the smallest positive (non-zero) eigenvalue. 2