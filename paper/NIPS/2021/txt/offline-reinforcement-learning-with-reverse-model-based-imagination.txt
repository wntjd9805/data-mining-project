Abstract
In ofﬂine reinforcement learning (ofﬂine RL), one of the main challenges is to deal with the distributional shift between the learning policy and the given dataset. To address this problem, recent ofﬂine RL methods attempt to introduce conservatism bias to encourage learning in high-conﬁdence areas. Model-free approaches directly encode such bias into policy or value function learning using conservative regular-izations or special network structures, but their constrained policy search limits the generalization beyond the ofﬂine dataset. Model-based approaches learn forward dynamics models with conservatism quantiﬁcations and then generate imaginary trajectories to extend the ofﬂine datasets. However, due to limited samples in of-ﬂine datasets, conservatism quantiﬁcations often suffer from overgeneralization in out-of-support regions. The unreliable conservative measures will mislead forward model-based imaginations to undesired areas, leading to overaggressive behav-iors. To encourage more conservatism, we propose a novel model-based ofﬂine
RL framework, called Reverse Ofﬂine Model-based Imagination (ROMI). We learn a reverse dynamics model in conjunction with a novel reverse policy, which can generate rollouts leading to the target goal states within the ofﬂine dataset.
These reverse imaginations provide informed data augmentation for model-free policy learning and enable conservative generalization beyond the ofﬂine dataset.
ROMI can effectively combine with off-the-shelf model-free algorithms to enable model-based generalization with proper conservatism. Empirical results show that our method can generate more conservative behaviors and achieve state-of-the-art performance on ofﬂine RL benchmark tasks. 1

Introduction
Deep reinforcement learning (RL) has achieved tremendous successes in a range of domains [1–3] by utilizing a large number of interactions with the environment. However, in many real-world applications, collecting sufﬁcient exploratory interactions is usually impractical, because online data collection can be costly or even dangerous, such as in healthcare [4] and autonomous driving
[5]. To address this challenge, ofﬂine RL [6, 7] develops a new learning paradigm that trains RL agents only with pre-collected ofﬂine datasets and thus can abstract away from the cost of online exploration [8–17]. For such ofﬂine settings, recent studies demonstrate that directly applying
∗Equal contribution.
†Work done while Guangxiang was a Ph.D. student at Tsinghua University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) Basic idea of ROMI. (b) A concrete RL task to demonstrate the superiority of ROMI. the online RL algorithms can lead to poor performance [8, 9, 16]. This phenomenon is primarily attributed to distributional shift [7] between the learning policy and the behavior policy induced by the given dataset. With function approximation, the learning policy can overgeneralize the ofﬂine dataset and result in unexpected or dangerous behaviors. Thus developing techniques to handle distributional shift is becoming an active topic in the community of ofﬂine RL.
Recently, a variety of advanced ofﬂine RL algorithms have been proposed, which introduce con-servatism bias and constrain the policy search in high-conﬁdence regions induced by the ofﬂine dataset. Model-free ofﬂine RL methods [8–11] explicitly encode such bias into policy or value functions by using conservative regularizations or specially designed network structures. These methods often effectively address distributional shift issues, but their constrained policy search can limit the generalization beyond the ofﬂine dataset. In contrast, model-based ofﬂine RL [12–15] adopts more aggressive approaches. They ﬁrst learn a forward dynamics model from the ofﬂine dataset with conservatism quantiﬁcations and then generate imaginary trajectories on high conﬁdence regions to extend the ofﬂine dataset. Speciﬁcally, these methods may use a model-uncertainty quantiﬁ-cation [12, 13], representation learning of a robust model [14], or a conservative estimation of value functions [15] to ensure the conﬁdence of model-based rollouts. However, because samples in ofﬂine datasets are limited, conservatism quantiﬁcations often suffer from overgeneralization, especially in out-of-support regions. Thus, these unreliable measures can overestimate some unknown states and mislead forward model-based imaginations to undesired areas, leading to radicalism. In this paper, we will investigate a new direction in the context of model-based ofﬂine RL and propose reverse model imagination that enables effective conservative generalization.
We use Figure 1a to illustrate our basic ideas. When an ofﬂine dataset contains expert or nearly optimal trajectories, model-free ofﬂine RL methods [8–11] show promising performance. In other situations, such as in Figure 1a, when the optimal policy requires a composition of multiple trajectories (
) in the ofﬂine dataset, model-free ofﬂine RL usually fails because it may get stuck in isolated regions of the ofﬂine dataset (
). In contrast, model-based approaches [12–15] have advantages of connecting trajectories in the ofﬂine dataset by generating bridging rollouts. When using forward dynamics models, model-based methods can generate aggressive rollouts from the dataset to outside areas (
).
Such forward imaginations potentially discover a better policy outside the ofﬂine dataset, but may
) due to overgeneralization also lead to undesirable regions consisting of fake high-value states ( errors. Things will be different if we reverse the model imagination. Reverse imaginations (
) generate possible traces leading to target goal states (
) inside the ofﬂine dataset, which provides a conservative way of augmenting the ofﬂine dataset. We assume that the reverse dynamics model has similar accuracy to the forward model. In this way, reverse models not only maintain the generalization ability to interpolate between given trajectories and potential better policies but also avoid radical model imagination by enabling bidirectional search in conjunction with the existing forward real trajectories (
) in the ofﬂine dataset. The scenario illustrated in Figure 1a is common in RL tasks [18], where the agent may encounter new obstacles or unexpected dangers at any time (e.g., bumping into walls like in Figure 1b or falling). As many unexpected data are not recorded by an ofﬂine dataset, there are likely large generalization errors outside the support of the dataset. Thus, reverse models that can generate conservative imaginations are more appealing for real-world ofﬂine
RL tasks. 2
Based on the above observation, we present a novel model-based ofﬂine RL framework, called
Reverse Ofﬂine Model-based Imagination (ROMI). ROMI trains a reverse dynamics model and generates backward imaginary trajectories by a backtracking rollout policy. This rollout policy is learned by a conditional generative model to produce diverse reverse actions for each state and lead to the unknown data space with high probabilities. ROMI ensures start states of the reverse rollout trajectories (i.e., target goals of forward trajectories) are within the ofﬂine dataset, and thus naturally imposes conservative constraints on imaginations. With this conservative data augmentation, ROMI has the advantage of effectively combining with off-the-shelf model-free algorithms (e.g., BCQ [8] and CQL [11]) to further strengthen its generalization with proper conservatism. Taken together, the whole ROMI framework enables extensive interpolation of the dataset and potentially better performance (contributed by diverse policy) in a safe manner (contributed by reverse imagination).
To our best knowledge, ROMI is the ﬁrst ofﬂine RL approach that utilizes reverse model-based imaginations to induce conservatism bias with data augmentation. It provides a novel bidirectional learning paradigm for ofﬂine RL, which connects reverse imaginary trajectories with pre-collected forward trajectories in the ofﬂine dataset. Such a bidirectional learning paradigm shares similar motivations with humans’ bidirectional reasoning. Studies from Psychology [19] show that, during the decision-making process, humans not only consider the consequences of possible future actions from a forward view but also imagine possible traces leading to the ideal goal through backward inference.
We conduct extensive evaluations on the D4RL ofﬂine benchmark suite [18]. Empirical results show that ROMI signiﬁcantly outperforms state-of-the-art model-free and model-based baselines. Our method achieves the best or comparable performance on 16 out of 24 tasks among all algorithms.
Ablation studies verify that the reverse model imagination can effectively generate more conservative behaviors than forward model imagination. Videos of the experiments are available online3. 2 Preliminaries
We consider a Markov decision process (MDP) deﬁned by a tuple M = (S, A, T, r, µ0, γ), where
S and A denote the state space and the action space, respectively. T (s(cid:48)|s, a) : S × A × S → R denotes the transition distribution function, r(s, a) : S × A → R denotes the reward function,
µ0 : S → [0, 1] is the initial state distribution, and γ ∈ (0, 1) is the discount factor. Moreover, we denote the reverse transition distribution by Tr(s|s(cid:48), a) = T −1 : S × A × S → R. The goal of an RL agent is to optimize a policy π(a|s) : S × A → R that maximizes the expected cumulative reward, i.e., J (π) = Es0∼µ0,st+1∼T (·|st,π(st)) [(cid:80)∞ t=0 γtr(st, π(st))].
In the ofﬂine RL setting, the agent only has access to a static dataset Denv = {(s, a, r, s(cid:48))} and is not allowed to interact with the environment for additional online explorations. The data can be collected through multi-source logging policies and we denote the empirical distribution of behavior policy in a given dataset Denv collected by πD . Logging policies are not accessible in our setting.
Model-based RL methods aim at performing planning or policy searches based on a learned model of the environment. They usually learn a dynamics model (cid:98)T and a reward model (cid:98)r(s, a) from a collection of environmental data in a self-supervised manner. Most of the existing approaches use the forward model (cid:98)Tf (s(cid:48)|s, a) for dynamics learning, but we will show in Section 3 that the reverse dynamics model (cid:98)Tr(s|s(cid:48), a) can induce more conservatism and is critical for the ofﬂine RL tasks. 3 Reverse Ofﬂine Model-based Imagination
In the ofﬂine RL setting, the agent can only access a given dataset without additional online explo-ration. Model-based ofﬂine RL algorithms usually face three challenges in this setting. (i) The ofﬂine dataset has limited samples and generalization over the given data is required. (ii) There are a lot of uncertainties that are difﬁcult to estimate out of the support of the dataset. (iii) The model-based rollouts cannot receive feedback from the real environment. Thus it is important to augment diverse data and keep conservative generalization at the same time. In this section, we will introduce the
Reverse Ofﬂine Model-based Imagination (ROMI) framework to combine model-based imagina-3https://sites.google.com/view/romi-offlinerl/. 3
tion with model-free ofﬂine policy learning. This framework encourages diverse augmentation of model-based rollouts and enables conservative generalization of the generated imaginations.
). We gather an ofﬂine dataset consisting of three data blocks (
We use a concrete example to show the superiority of our framework in Figure 1b. The agent navigates in a u-maze to reach the goal (
).
This dataset only contains the trajectories that do not hit the wall, thus the agent will be unaware of the walls during the ofﬂine learning process. In this situation, the conventional forward model may generate a greedy and radical imagination (
) that takes a shortcut to go straight to the goal (whose value function may be overestimated) and hit the wall. On the contrary, ROMI adopts backward imaginations and generates traces that can lead to the target goal states in the support of the dataset
) in the ( dataset to form an optimal policy. In this example, we assume that the model error is mainly due to the prediction of the OOD states (outside of the support of the data), then we can expect reverse models to help combat overestimation since they prevent rollout trajectories that end in the OOD states. A more detailed illustrative example is deferred to Appendix A.
). Such imaginations can be further connected with the given forward trajectories (
Speciﬁcally, our framework consists of two main components: (i) a reverse model learned from the ofﬂine dataset, (ii) a diverse rollout policy to generate reverse actions that are close to the dataset.
ROMI pre-generates the model-based trajectories under the rollout policy based on the reverse model.
Then we use the mixture of the imaginary data and the original dataset to train a model-free ofﬂine agent. The whole algorithm is shown in Algorithm 1.
Training the reverse model. To backtrace the dynamics and reward function of the environment, we introduce a reverse model to estimate the reverse dynamics model (cid:98)Tr(s|s(cid:48), a) and reward model (cid:98)r(s, a) from the ofﬂine dataset simultaneously. For simplicity, we unify the dynamics and reward function into our reverse model p(s, r|s(cid:48), a), i.e., p(s, r|s(cid:48), a) = p(s|s(cid:48), a)p(r|s(cid:48), a, s) = Tr(s|s(cid:48), a)p(r|s, a), (1) where we assume that the reward function only depends on the current state and action. This uniﬁed reverse model represents the probability of the current state and immediate reward conditioned on the next state and current action. We parameterize it by φ and optimize it by minimizing the loss function
LM (φ) (which is equivalent to maximizing the log-likelihood):
LM (φ) =
E (s,a,r,s(cid:48))∼Denv
[− log (cid:98)pφ(s, r|s(cid:48), a)] , (2) where Denv is the ofﬂine dataset.
Training the reverse rollout policy. To encourage diversity for the reverse model-based imagination near the dataset, we train a generative model (cid:98)Gθ(a|s(cid:48)), which samples diverse reverse actions from the ofﬂine dataset using stochastic inference. Speciﬁcally, we use a conditional variational auto-encoder (CVAE) [20, 8] to represent the diverse rollout policy (cid:98)Gθ(a|s(cid:48)), which is parameterized by
θ and depends on the next state. The rollout policy (cid:98)Gθ(a|s(cid:48)) contains two modules: (i) an action encoder (cid:98)Eω(s(cid:48), a) that outputs a latent vector z under the gaussian distribution z ∼ (cid:98)Eω(s(cid:48), a), and (ii) an action decoder (cid:98)Dξ(s(cid:48), z) whose input is the latent vector z and reconstructs the given action (cid:101)a = (cid:98)Dξ(s(cid:48), z). The action encoder and decoder are parameterized by ω and ξ, respectively. For simplicity, denote the parameters of the rollout policy (cid:98)Gθ(a|s(cid:48)) by θ = {ω, ξ}. We defer the detailed discussion of CVAE to Appendix B.
We train the rollout policy (cid:98)Gθ(a|s(cid:48)) by maximizing the variational lower bound Lp(θ), (cid:20)(cid:16) (cid:17)2 (cid:16) (cid:17)(cid:21) a − (cid:98)Dξ(s(cid:48), z)
+ DKL (cid:98)Eω(s(cid:48), a)(cid:107)N (0, I)
, (3)
Lp(θ) =
E (s,a,r,s(cid:48))∼Denv,z∼ (cid:98)Eω(s(cid:48),a) where I is an identity matrix. To optimize such loss function, we adopt similar optimization techniques as those used in the context of image generalization and video prediction [20, 8].
After the rollout policy (cid:98)Gθ(a|s(cid:48)) is well trained, we can sample reverse actions (cid:98)a based on the policy.
We ﬁrst draw a latent vector from the multivariate normal distribution, (cid:98)z ∼ N (0, I), and then utilize the action decoder to sample actions conditioned on the next state, (cid:98)a = (cid:98)Dξ(s(cid:48), (cid:98)z). To explore more possibilities, the rollout policy (cid:98)Gθ(a|s(cid:48)) uses stochastic layers to generate a variety of reverse actions 4
for multiple times. In addition, if we need an extremely diverse policy in easy tasks, we can replace the generative model based policy with a uniformly random rollout policy.
Combination with model-free algorithms. Based on the learned reverse dynamics model and the reverse rollout policy, ROMI can generate reverse imaginations. We collect these rollouts to form a model-based buffer Dmodel and further compose the total dataset with the ofﬂine dataset,
Dtotal = Denv ∪ Dmodel. Since our rollout policy is agnostic to policy learning, such buffer can be obtained before the policy learning stage, i.e., ROMI can be combined with any model-free ofﬂine
RL algorithm (e.g., BCQ [8] or CQL [11]). Speciﬁcally, during the model-based imagination, we sample the target state st+1 from the dataset Denv, and generate reverse imaginary trajectory (cid:98)τ with the rollout horizon h by the reverse model (cid:98)pφ and rollout policy (cid:98)Gθ: (cid:110) (st−i, at−i, rt−i, st+1−i) (cid:12) (cid:12) at−i ∼ (cid:98)Gθ (·|st+1−i) and st−i, rt−i ∼ (cid:98)pφ (·|st+1−i, at−i) (cid:98)τ =
We gather trajectories (cid:98)τ to form the buffer Dmodel and further combine it with Denv to obtain Dtotal.
Then we will run the model-free ofﬂine policy learning algorithm on the total buffer to derive the
ﬁnal policy πout. Compared to existing model-based approaches [12–15], ROMI provides informed data augmentation to extend the ofﬂine dataset. It is agnostic to policy optimization and thus can be regarded as an effective and ﬂexible plug-in component to induce conservative model-based imaginations for ofﬂine RL. i=0
. (cid:111)h−1
Algorithm 1 ROMI: Reverse Ofﬂine Model-based Imagination 1: Require: Ofﬂine dataset Denv, rollout horizon h, the number of iterations Cφ, Cθ, T , learning rates αφ, αθ, model-free ofﬂine RL algorithm (i.e., BCQ or CQL) (cid:46) Learning a reverse dynamics model (cid:98)pφ (cid:46) Learning a diverse rollout policy (cid:98)Gθ
Compute LM using the dataset Denv
Update φ ← φ − αφ∇φLM 2: Randomly initialize reverse model parameters φ 3: for i = 0 . . . Cφ − 1 do 4: 5: 6: Randomly initialize rollout policy parameters θ 7: for i = 0 . . . Cθ − 1 do
Compute Lp using the dataset Denv 8:
Update θ ← θ − αθ∇θLp 9: 10: Initialize the replay buffer Dmodel ← ∅ 11: for i = 0 . . . T − 1 do 12: 13:
Sample target state st+1 from the dataset Denv
Generate reverse model rollout (cid:98)τ = {(st−i, at−i, rt−i, st+1−i)}h−1 samples from the dynamics model (cid:98)pφ and rollout policy (cid:98)Gθ (cid:46) Collecting the replay buffer Dmodel i=0 from st+1 by drawing
Add model rollouts to replay buffer, Dmodel ← Dmodel ∪ {(st−i, at−i, rt−i, st+1−i)}h−1 i=0 14: 15: Compose the ﬁnal dataset Dtotal ← Denv ∪ Dmodel 16: Combine model-free ofﬂine RL algorithms to derive the ﬁnal policy πout using the dataset Dtotal 17: Return: πout 4 Experiments
In this section, we conduct a bunch of experiments in the ofﬂine RL benchmark [18] to answer the following questions: (i) Does ROMI outperform the state-of-the-art ofﬂine RL baselines (see
Table 1 and 2)? (ii) Does ROMI achieve excellent performance because of the reverse model-based imagination (see Section 4.3)? (iii) Is CVAE-based rollout policy critical for ROMI (see Table 4)? (iv) Compared with the forward imagination, does ROMI trigger more conservative and effective behaviors (see Figure 3)? 4.1 Evaluation Environments
We evaluate ROMI on a wide range of domains in the D4RL benchmark [18], including the Maze2D domain, the Gym-MuJoCo tasks, and the AntMaze domain. Figure 2 shows the snapshots of nine environments used in our experiments. We defer the quantiﬁcation of ROMI’s model accuracy in 5
Figure 2: Experimental environments. these domains to Appendix C and empirical evaluations show that reverse models have comparable accuracy, if not worse, than forward models.
Maze2D. The maze2d domain requires a 2D agent to learn to navigate in the maze to reach a ﬁxed target goal and stay there. As shown in Figure 2, there are three maze layouts (i.e., umaze, medium, and large) and two dataset types (i.e., sparse and dense reward singal) in this domain. The dataset of each layout is generated by a planner moving between randomly sampled waypoints. From the detailed discussion of the D4RL benchmark [18], we found that the agents in the maze2d dataset are always moving on the clearing and will not stay in place. We will visualize the dataset of mazed2d-umaze in Section 4.5.
Gym-MuJoCo. The Gym-MuJoCo tasks consist of three different environments (i.e., walker2d, hopper, and halfcheetah), and four types of datasets (i.e., random, medium, medium-replay, and medium-expert). Random dataset contains experiences selected by a random policy. Medium dataset contains experiences from an early-stopped SAC policy. Medium-replay dataset records the samples in the replay buffer during the training of the "medium" SAC policy. Medium-expert dataset is mixed with suboptimal samples and samples generated from an expert policy.
AntMaze. The antmaze domain combines challenges of the previous two domains. The policy needs to learn to control the robot and navigate to the goal simultaneously. This domain also contains three different layouts (i.e., umaze, medium, and large) shown in Figure 2. D4RL benchmark [18] introduces three ﬂavors of datasets (i.e., ﬁxed, diverse, and play) in this setting, which commands the ant from different types of start locations to various types of goals. 4.2 Overall Results
In this subsection, the experimental results are presented in Table 1 and 2, which are evaluated in the D4RL benchmark tasks [18] illustrated in Figure 2. We compare ROMI with 11 state-of-the-art baselines: MF denotes the best performance from ofﬂine model-free algorithms, including BCQ [8],
BEAR [10], BRAC-v, BRAC-p [9], BAIL [17], and CQL [11]; MB denotes the best performance from ofﬂine model-based algorithms, including MOPO [13], MOReL [12], Repb-SDE [14], and
COMBO [15]; BC denotes the popular behavior cloning from the dataset in the imitation learning.
The implementation details of ROMI and these algorithms are deferred to Appendix D.2. Towards fair evaluation, all experimental results are illustrated with the averaged performance with ± standard deviation over three random seeds.
We evaluate ROMI in nine D4RL benchmark domains with 24 tasks. Our experiments show that
ROMI signiﬁcantly outperforms baselines and achieves the best or comparable performance on 16 out of 24 continuous control tasks. Table 1 and 2 only contain the best performance of MF and MB categories4. We defer the pairwise comparison of ROMI and each baseline to Appendix E, which 4In Table 2, each score of COMBO or MOReL is the better one between the score our reproduction and their reported score. 6
Table 1: Performance of ROMI and best performance of prior methods on the maze and antmaze domains, on the normalized return metric proposed by D4RL benchmark [18]. Scores roughly range from 0 to 100, where 0 corresponds to a random policy performance and 100 corresponds to an expert policy performance. med is short for medium.
Environment sparse-maze2d-umaze sparse-maze2d-med sparse-maze2d-large dense-maze2d-umaze dense-maze2d-med dense-maze2d-large
ﬁxed-antmaze-umaze play-antmaze-med play-antmaze-large diverse-antmaze-umaze diverse-antmaze-med diverse-antmaze-large
BC
-3.2
-0.5
-1.7
-6.9 2.7
-0.3 82.0 0.0 0.0 47.0 0.0 0.0
ROMI-BCQ 139.5 ± 3.6 82.4 ± 15.2 83.1 ± 22.1 98.3 ± 2.5 102.6 ± 32.4 124 ± 1.3 68.7 ± 2.7 35.3 ± 1.3 20.2 ± 14.8 61.2 ± 3.3 27.3 ± 3.9 41.2 ± 4.2
MF 65.7 ± 6.9BEAR 70.6 ± 34.3BRAC-v 81.0 ± 65.3BEAR 51.5 ± 8.2BRAC-p 41.7 ± 2.0BAIL 133.0 ± 25.5BEAR 75.3 ± 13.7BCQ 1.7 ± 1.0BAIL 2.2 ± 1.3BAIL 54.0 ± 15.0BAIL 61.5 ± 10.0CQL 1.0 ± 0.9BAIL
MB 76.4 ± 19.2COMBO 68.5 ± 83.6COMBO 14.1 ± 10.7COMBO 94.3 ± 13.6Repb-SDE 84.2 ± 9.5COMBO 36.8 ± 12.4MOPO 80.3 ± 18.5COMBO 0.0 0.0 57.3 ± 33.6COMBO 0.0 0.0
Table 2: Performance of ROMI and best performance of prior methods on Gym-MuJoCo tasks.
Environment
ROMI-CQL
BC random-walker2d random-hopper random-halfcheetah medium-walker2d medium-hopper medium-halfcheetah medium-replay-walker2d medium-replay-hopper medium-replay-halfcheetah medium-expert-walker2d medium-expert-hopper medium-expert-halfcheetah 0.0 0.9
-0.1 41.7 40.0 39.2 2.2 8.1 25.6 73.4 36.0 39.7 7.5 ± 20.0 30.2 ± 4.4 24.5 ± 0.7 84.3 ± 1.1 72.3 ± 17.5 49.1 ± 0.8 109.7 ± 9.8 98.1 ± 2.6 47.0 ± 0.7 109.7 ± 5.3 111.4 ± 5.6 86.8 ± 19.7
MF 11.1 ± 8.8 BEAR 31.4 ± 0.1 CQL 19.6 ± 1.2 CQL 83.8 ± 0.2 CQL 66.6 ± 4.1 CQL 49.0 ± 0.4 CQL 88.4 ± 1.1 CQL 97.0 ± 0.8 CQL 46.4 ± 0.3 CQL 109.5 ± 0.1 CQL 106.8 ± 2.9 CQL 90.8 ± 5.6 CQL
MB 7.0 COMBO 31.7 ± 0.1 Repb-SDE 38.8 COMBO 85.3 ± 2.2 Repb-SDE 95.4 MOReL 69.5 ± 0.0 MOPO 83.8 ± 7.6 Repb-SDE 93.6 MOReL 68.2 ± 3.2 MOPO 111.2 ± 0.2 Repb-SDE 111.1 COMBO 95.6 MOReL can demonstrate that ROMI in conjunction with off-the-shelf model-free methods (i.e., BCQ [8] and
CQL [11]) can outperform all ofﬂine RL baselines. Speciﬁcally, we denote the methods with ROMI as ROMI-BCQ, and ROMI-CQL, respectively. The sufﬁx -BCQ or -CQL indicates that ROMI adopts
BCQ [8] or CQL [11] as base learning algorithms for policy optimization.
Table 1 shows that ROMI-BCQ is the best performer on 10 out of 12 tasks in the maze2d and antmaze domains. In these tasks, model-free methods can achieve reasonable performance, while current model-based algorithms with forward imagination cannot perform well, especially in antmaze.
This may be because that messy walls (see Figure 2) are unknown from the given datasets, and the forward imagination may lead RL agent to bump into walls and lose the game. In contrast, reverse model-based imagination can avoid directing agents towards unconﬁdent area, which makes a safe way to imagine in the complex domains. Table 2 shows that ROMI-CQL achieves the best performer on six out of 12 tasks in gym domain. CQL is the best performer among all model-free methods, and ROMI-CQL can further outperform CQL on four challenging tasks. In this domain, current model-based methods perform pretty well. We argue that similar to forward imagination, reverse direction can also generalize beyond ofﬂine datasets for better performance in the relatively safe tasks, i.e., there is no obstacle around, as shown in Figure 2. 7
4.3 Ablation Study with Model-based Imagination
In this subsection, we conduct an ablation study to investigate whether ROMI works due to the reverse model-based imagination. Speciﬁcally, we replace the reverse imagination with the forward direction in ROMI, which is denoted as Forward rOMI (FOMI). In this subsection, we study the performance of ROMI and FOMI in maze2d and antmaze domains and defer the ablation study in gym domain to Appendix F. Towards fair comparison, we integrate FOMI with BCQ [8] in these settings, called FOMI-BCQ. Table 3 shows that ROMI-BCQ signiﬁcantly outperforms FOMI-BCQ and the base model-free method BCQ [8], which implies that reverse model-based imagination is critical for ROMI in the ofﬂine RL settings.
In the maze2d domain, compared with the base model-free algorithm BCQ, ROMI-BCQ outperforms all settings, while FOMI-BCQ achieves the superior performance in maze2d-medium but performs poorly in the umaze and large layouts. As illustrated in Figure 2, maze2d-medium enjoys less obstacles on the diagonal to the goal than maze2d-umaze and maze2d-large. In this case, the conservative reverse model-based imagination can enable safe generalization in all layouts. A detailed case study of maze2d-umaze will be provided in Section 4.5. Moreover, in the antmaze domain, ROMI-BCQ achieve the best performance in the medium and large layouts, while FOMI-BCQ and BCQ perform well in antmaze-umaze. From Figure 2, we ﬁnd that the medium and large layouts of antmaze have larger mazes with narrower passages, which may frustrate forward imagination and make reverse imagination more effective.
Table 3: Ablation study about ROMI with model-based imagination. Delta equals the improvement of ROMI-BCQ over BCQ on the normalized return metric.
Dataset type Environment
ROMI-BCQ (ours)
FOMI-BCQ
BCQ (base) Delta sparse sparse sparse dense dense dense
ﬁxed play play diverse diverse diverse maze2d-umaze maze2d-medium maze2d-large maze2d-umaze maze2d-medium maze2d-large antmaze-umaze antmaze-medium antmaze-large antmaze-umaze antmaze-medium antmaze-large 139.5 ± 3.6 82.4 ± 15.2 83.1 ± 22.1 98.3 ± 2.5 102.6 ± 32.4 124.0 ± 1.3 68.7 ± 2.7 35.3 ± 1.3 20.2 ± 14.8 61.2 ± 3.3 27.3 ± 3.9 41.2 ± 4.2 8.1 ± 15.5 93.6 ± 41.3
-2.5 ± 0.0 30.7 ± 0.9 64.7 ± 37.0
-0.7 ± 7.1 79.5 ± 2.5 26.2 ± 5.5 12.0 ± 3.3 66.8 ± 3.5 12.3 ± 2.1 17.8 ± 2.1 41.1 ± 7.6 9.7 ± 14.2 38.3 ± 10.4 37.0 ± 5.3 37.9 ± 4.5 79.8 ± 12.2 75.3 ± 13.7 0.0 0.0 49.3 ± 9.9 0.0 0.0 98.4 72.7 44.8 61.3 64.7 44.2
-6.6 35.3 20.2 11.9 27.3 41.2 4.4 Ablation Study with Different Rollout Policies
In this subsection, we conduct an ablation study to investigate the effect of ROMI’s different rollout policies. To compare with a CVAE-based policy, we propose a new rollout policy (i.e., reverse behav-ior cloning) for ROMI-BCQ, denoted by ROMI-RBC-BCQ. To realize the reverse behavior cloning method, we train a stochastic policy (cid:98)πϕ(a|s(cid:48)), which is parameterized by ϕ and can sample current action depended on the next state. During training (cid:98)πϕ(a|s(cid:48)), the objective Lrbc(ϕ) is formalized by (4)
Lrbc(ϕ) =
[− log (cid:98)πϕ(a|s(cid:48))] ,
E (s,a,r,s(cid:48))∼Denv where Denv is the given ofﬂine dataset, and minimizing the loss function Lrbc(ϕ) is equivalent to maximizing the log-likelihood of probability.
We illustrate the performance of ROMI with different rollout policies in Table 4, where ROMI-BCQ achieves the best performance and ROMI-RBC-BCQ also outperforms BCQ. As suggested by prior generative models [21, 20], in comparison to the deterministic layers (e.g., fully connected layers),
CVAE-based methods can generate more diverse and realistic structured samples using stochastic inference. We argue that, the rollout policy implemented by CVAE is critical for ROMI to rollout more diverse trajectories for proposed generalization, and BC-based implementation is also effective in reverse model-based imagination. 8
Table 4: Ablation study about ROMI with different rollout policies.
Dataset type Environment
ROMI-BCQ (ours) ROMI-RBC-BCQ BCQ (base)
ﬁxed play play diverse diverse diverse antmaze-umaze antmaze-medium antmaze-large antmaze-umaze antmaze-medium antmaze-large 68.7 ± 2.7 35.3 ± 1.3 20.2 ± 14.8 61.2 ± 3.3 27.3 ± 3.9 41.2 ± 4.2 62.2 ± 5.6 33.8 ± 6.2 13.3 ± 16.1 43.8 ± 13.3 20.8 ± 15.5 14.2 ± 9.8 75.3 ± 13.7 0.0 0.0 49.3 ± 9.9 0.0 0.0 4.5 A Case Study in maze2d-umaze
To dive deeper into how ROMI triggers more conservative and effective behaviors, we provide a detailed visual demonstration of one particular task in D4RL benchmark [18]: maze2d-umaze-sparse.
As mentioned in Section 4.1, experiences in maze2d domain are generated by a planner moving between randomly sampled waypoints on the clearing. Figure 3a shows the movement of the agent from randomly sampled trajectories. To earn high returns, the agent not only needs to learn how to direct to the goal, but also how to stay in the high reward region — the latter behavior is not in the dataset yet. Model-free ofﬂine RL algorithms constrain their policy "close" to the dataset, thus it is hard to learn such behaviors out of the support. To see this, Figure 3d shows the behavior of a trained BCQ policy during execution. After reaching the goal, the agent will still oscillate between the high-reward and low-reward regions. This motivates us to use model imagination to generalize beyond the dataset.
As shown in Table 1, ROMI solves this task but previous model-based methods have poor performance, sometimes even worse than model-free algorithms. To better understand this counter-intuitive phenomenon, we compare the rollout trajectories and the learned policy of ROMI-BCQ, FOMI-BCQ (mentioned in Section 4.3), and MOPO [13]. Figure 3(g-i) shows the imagined trajectories in the learning process of the three methods. Figure 3(b,c,e) shows the learned policy behavior at the execution phase. While all model-based imagination will leave the dataset for better generalization, forward model rollout naturally takes some risks as it directs the agent to unknown areas. Undesired forward model imagination will ruin the policy learning (e.g., FOMI-BCQ in Figure 3e and Table 3) or mislead the policy optimization to the suboptimal solution (e.g., MOPO in Figure 3c). Moreover, as shown in Figure 3f, the regularization penalty based on model uncertainty also failed, which is also pointed out in the literature [15]. On the other hand, reverse model imagination inherits the conservatism of the dataset, as it is always ended in the real experience points. Figure 3(b,g) shows that ROMI induces the conservative and optimal behaviors: ROMI will stay around the goal point, and will stick to the data points for higher expected returns. We aim to quantify the aggressiveness of each learned policy and thus deﬁne the following trajectory-based discrepancy to measure the distance between learning policy and dataset:
Deﬁnition 1 (Average Trajectory Discrepancy). Given a dataset D = {(s, a, r, s(cid:48))} and a trajectory
τ = (s0, a0, . . . , sH , aH ), the discrepancy between D and τ is deﬁned as:
D(D, τ ) = 1
H + 1
H (cid:88) t=0 min (s,a,r,s(cid:48))∈D (cid:107)st − s(cid:107)2. (5)
We report the average trajectory discrepancy for policies during execution in Figure 3j and defer results for other maze environments in Appendix G. The results implicate that ROMI is as conservative as model-free method BCQ, but forward model-based ofﬂine RL policy (i.e., ROMI and MOPO) tends to deviate from the dataset and will touch the undesirable walls. 5