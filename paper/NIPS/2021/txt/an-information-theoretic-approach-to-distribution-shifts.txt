Abstract
Safely deploying machine learning models to the real world is often a challenging process. Models trained with data obtained from a speciﬁc geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are ﬁt to a subset of the population might carry some selection bias into their decision process. In this work, we describe the problem of data shift from a novel information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization and fair classiﬁcation literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process. 1

Introduction
One of the most common assumptions for machine learning models is that the training and test data are independently and identically sampled (IID) from the same distribution. In practice, this assumption does not hold in many practical scenarios (Bengio et al., 2020). A machine learning model trained to recognize land usage from satellite images using pictures from the early 2000s may struggle to recognize the style of modern architectures (Christie et al., 2018), data collected on a limited set of hospitals may not be representative of the variation introduced by the use of different machines or procedures (Zech et al., 2018; Beede et al., 2020). Other kinds of distribution shifts are more subtle and difﬁcult to recognize despite having a noticeable impact on the model’s predictive performance. Examples include under-represented or over-represented population groups (Popejoy &
Fullerton, 2016; Buolamwini & Gebru, 2018) or biased annotations collected from crowd-sourcing services (Zhang et al., 2017; Xia et al., 2020).
Different approaches in literature address these issues by using some external source of knowledge such as domain or environment annotations (Wang & Deng, 2018), protected attributes (Mehrabi et al., 2019) or sub-population groups (Santurkar et al., 2020) to reduce bias and minimize the model error outside of the training distribution. Despite the progress in the ﬁeld of domain generalization literature, Gulrajani & Lopez-Paz (2020) has shown that the effectiveness of some of the most common algorithms heavily relies on the hyper-parameter tuning strategy, revealing limitations of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
examined models when compared to a more traditional empirical risk minimization strategy. One of the major issues behind this observed behavior is a lack of clarity and applicability for the underlying assumptions regarding the problem statement and the data-generating process (Zhao et al., 2019;
Rosenfeld et al., 2020; Mahajan et al., 2020).
With this work, we aim to present a new perspective on the problem to analyze and clarify the fundamental differences between some of the most common approaches by: 1. Introducing a novel information-theoretical framework to describe the problem of distribu-tion shift and connecting it to the test error and its components (section 2). 2. Analyzing four main families of objectives and describing some of their guarantees and assumptions (section 3). 3. Demonstrating that the effectiveness of different criteria is determined by the structure of the underlying data-generating process (section 4). 4. Showing that the results obtained by popular models designed according to the aforemen-tioned criteria can drastically differ from the theoretically expected performance (section 4).
The analysis of the bottleneck, independence, sufﬁciency, and separation criteria reveals that some of the most popular models can systematically fail to reduce the test error even for simple datasets. No unique objective is simultaneously optimal for every problem, but additional knowledge about the selection procedure and better approximations can help to mitigate the bias. 1.1 Problem Statement
Consider x and y as the features and targets respectively with joint density p(x, y) for the predictive problem of interest. Let t be a binary random variable representing which data is selected for training (t = 1) and which is not (t = 0). We will refer to p(x, y|t = 1) as the joint Training distribution and p(x, y|t = 0) as the Test distribution that are induced by the selection t. Throughout the paper, we will consider selections t that can be expressed as a function of the features x, targets y, independent noise (cid:15) and other variables e, which represent other factors that are affecting the data collection procedure (e.g. geographic location, time intervals, population groups).
Let q(y|x) represent a learnable model for the predictive distribution p(y|x). We use the Kullback-Leibler divergence to express the Train and Test error respectively1:
DKL(pt=1 y|x||qy|x) (cid:123)(cid:122) (cid:125) (cid:124)
Train error
DKL(pt=0 y|x||qy|x) (cid:123)(cid:122) (cid:125) (cid:124)
Test error
:= DKL(p(y|x, t = 1)||q(y|x)),
:= DKL(p(y|x, t = 0)||q(y|x)). (1) (2)
Although only the train distribution can be accessed at training time, we are interested in learning models q(y|x) that result in small test error. 1.2 Characterizing Distribution Shift
As a ﬁrst step, we characterize the difference between training and test distribution as a function of the selection. The effect that the selection has in the joint distribution p(x, y) can be quantiﬁed by considering the mutual information between the features-target pair xy and the selection variable t, which can be also expressed as a Kullback-Leibler divergence: I(xy; t) = DKL(p(x, y|t)|p(x, y)).
This measure of distribution shift intuitively represents how many bits the selection variable carries about the joint distribution of targets and features or, equivalently, how much the joint density p(x, y) has changed as a result of the selection.
Using the chain rule of mutual information, one can express distribution shift as the sum of two separate components:
I(xy; t) (cid:124) (cid:123)(cid:122) (cid:125)
Distribution shift
= I(x; t) (cid:124) (cid:123)(cid:122) (cid:125)
Covariate shift
,
+ I(y; t|x) (cid:124) (cid:123)(cid:122) (cid:125)
Concept shift (3) 1Further details regarding the convention used for conditional KL-divergence can be found in appendix A 2
y x1 x2 e (b) t (a)
Figure 1: (a) Example of distribution shift due to selection bias (chest pain complaints) for the prediction of cholesterol levels (y-axis) given blood pressure (x-axis). A maximum likelihood approach (green dashed line) results in a model that over-estimates y for any given x when compared to the best model in the class (red dashed line). (b) Example of a data generating process in which the features x are composed of two parts x1 and x2. When both feature components x1 and x2 are observed, concept shift can be strictly positive. Discarding information about x2 will reduce the effect of the selection bias, while removing x1 might increase it 0 = I(y; t|x1) ≤ I(y; t|x1x2) ≤ I(y; t|x2). which can be interpreted as the amount of covariate shift (Shimodaira, 2000) and concept shift (Widmer & Kubat, 1996; Moreno-Torres et al., 2012), respectively. These two quantities refer to the changes in the predictive distribution p(y|x) and the marginal features distribution p(x) respectively, which add up to represent the changes in the joint distribution.
Whenever the data selection is perfectly IID, the selection variable can be expressed as a function of some independent noise (t = f ((cid:15))) and the corresponding distribution shift I(xy; t) is zero. On the other hand, if the data collection procedure has been inﬂuenced by other factors, we do not have such a guarantee, even when the selection does not depend on features and targets directly (t = f (e, (cid:15))). 2
Information-theoretic Framework
Using the quantities described in the previous sections, we can show that the sum train and test error is lower-bounded by the amount concept shift:
Proposition 1. For any model q(y|x) and α := min{p(t = 0), p(t = 1)}:
DKL(pt=1 y|x||qy|x) + DKL(pt=0 y|x||qy|x) ≥ 1 1 − α
I(y; t|x), (4)
As a consequence, whenever the selection induces concept shift I(y; t|x) > 0, any sufﬁciently
ﬂexible model ˆq(y|x) trained using a maximum likelihood approach must incur in strictly positive test error. Intuitively, whenever the selection induces a change in the predictive distribution (p(y|x) (cid:54)= p(y|x, t = 1)) ﬁtting the model to the training distribution will incorporate the selection bias into the model prediction, necessarily resulting in errors when evaluated on the test distribution, as shown in the example reported in ﬁgure 1a.
The different approaches analyzed in this work are based in the introduction of a latent representation z, which allows for the deﬁnition of different regularization strategies. Using the latent representation as an intermediate variable, the model q(y|x) can be re-parametrized with an encoder q(z|x) and a latent classiﬁer2 q(y|z): q(y|x) = Ez∼q(z|x) [q(y|z = z)] . (5)
The effect of the introduction of a latent representation can be observed by expressing the training and test error as a function of the encoder and the classiﬁer3. 2The name classiﬁer will be used for convenience to express both classiﬁcation and regression problems. 3The expressions in equations 6 and 7 hold with equality for deterministic encoders as shown in appendix B. 3
Proposition 2. For any encoder q(z|x) and classiﬁer q(y|z):
DKL(pt=1 y|x||qy|x) ≤ It=1(x; y|z) (cid:125) (cid:123)(cid:122)
Train information loss (cid:124)
DKL(pt=0 y|x||qy|x) ≤ It=0(x; y|z) (cid:124) (cid:125) (cid:123)(cid:122)
Test information loss (cid:124)
+ DKL(pt=1 y|z ||qy|z) (cid:123)(cid:122) (cid:125)
Latent train error
+ DKL(pt=0 y|z ||qy|z) (cid:123)(cid:122) (cid:125)
Latent test error (cid:124)
. (6) (7)
The two terms It=1(x; y|z) and It=0(x; y|z) represent the amount of predictive information that is lost by encoding the features x into z on train and test distribution respectively, while DKL(pt=1 y|z ||qy|z) and DKL(pt=0 y|z ||qy|z) refer to the train and test error when using z instead of the original observations as the predictive features, which will be referred to as latent training error and latent test error respectively. Test information loss and latent test error capture two intrinsically different kinds of error. The former indicates the increase in the prediction uncertainty as a result of the encoding procedure, while the latter represents the discrepancy between the model q(y|z) and the latent test predictive distribution p(y|z, t = 0). 2.1 Latent test error
Minimizing the latent test error makes the latent predictor q(y|z) approach the test latent predictive distribution p(y|z, t = 0). We can show that the Jensen-Shannon divergence between the two distributions is upper-bounded by a monotonic function of the latent training error and the amount of concept shift in the latent space (latent concept shift I(y; t|z)):
Proposition 3. For any q(z|x), q(y|z) and any representation z that satisﬁes p(z = z|t = 0) > 0 and p(z = z|t = 1) > 0: (cid:32)(cid:114) 1 2α y|z=z||qy|z=z) y|z=z||qy|z=z).
≥ DJSD(pt=0
I(y; t|z = z) +
DKL(pt=1 (cid:114) 1 2 (cid:33)2 (8)
Whenever the Jensen-Shannon divergence between the test predictive distribution p(y|z, t = 0) and the classiﬁer q(y|z) is small, the latent test error (measured in terms of KL-divergence) must also be small at least for the regions that have positive probability according to both train p(x|t = 1) and test p(x|t = 0) data distributions. Since the train predictive distribution p(y|x, t = 1) is not deﬁned for x that have zero probability on the train distribution, we have no guarantees regarding the model predictions in those regions unless other inductive biases are considered.
The left hand side of the expression in proposition 3 can be minimized by addressing I(y; t|z) and
DKL(pt=1 y|z ||qy|z) with respect to the encoder and classiﬁer respectively. In other words, we can minimize the latent test error by simultaneously ﬁtting q(y|z) to the train predictive distribution p(y|z, t = 1) and minimizing the latent concept shift induced by the encoder q(z|x). When the latent concept shift and latent test error approach zero, the latent train predictive distribution p(y|z, t = 1) approaches the true (unselected) predictive distribution p(y|z), and so does the modeled classiﬁer q(y|z). In this ideal scenario, the only source of test error is the additional uncertainty that is due to the information lost in the encoding procedure. 2.2 Minimizing the information loss
Since losing information generally results in increased test error (equation 7), we will consider objectives that discard the minimal amount of information required to reduce the latent concept shift. This can be done by minimizing the KL-divergence between the training predictive distribution p(y|x; t = 1) and the latent classiﬁer model q(y|z): min q(z|x)
It=1(x; y|z) = min q(z|x),q(y|z)
DKL(pt=1 y|x||qy|z) − KL(pt=1 y|z ||qy|z)
≤ min q(z|x),q(y|z)
DKL(pt=1 y|x||qy|z). (9)
In addition to reducing the amount of information lost in the encoding procedure, minimizing the right hand side of the expression in equation 9 makes the model q(y|z) approach the latent predictive 4
distribution p(y|z, t = 1). Note that only the selected training distribution p(y, x|t = 1) is available at training time. In practice, minimizing the train information loss also generally results in decreased test information loss since the same features are usually informative for both train and test distributions. 2.3 A General Loss function
To summarize, the overall objective consists in ﬁnding the maximally informative representation that minimizes latent concept shift so that the same learned predictor q(y|z) can perform similarly on both train and test settings. This is achieved by (i) minimizing the amount of latent concept shift induced by the representation, (ii) maximizing the amount of predictive information in the representation, and (iii) matching q(y|z) and the training latent predictive distribution p(y|z, t = 1). The three requirements can be enforced by considering a loss function with the following form:
L(qz|x, qy|z; λ) = DKL(pt=1 y|x||qy|z) + λR(qz|x), (10) in which, the ﬁrst term addresses (ii) and (iii), while the second term represent a regularization term that acts on the encoder q(z|x) to minimize latent concept shift I(y; t|z), following (i). For a sufﬁciently ﬂexible family of latent predictors, requirement (iii) depends only q(y|z), while the hyper-parameter λ deﬁnes the trade-off between latent concept shift (i) and predictive information loss (ii). 3 Regularization Criteria
Since only selected data (t = 1) is accessible at training time, the latent concept shift can not be computed or minimized directly. Most of the approaches considered in this analysis make use of a regularization R(qz|x) that is based on the observation of an additional variable e which relates to the selection criteria. This variable is usually referred to as domain or environment4 in the domain adaptation and generalization literature, while the name protected attribute is used in the context of fair classiﬁcation. We will refer to this variable e as environmental factors in the following sections.
We analyze four families of criteria proposed in the representation learning (Tishby & Zaslavsky, 2015), domain generalization (Koyama & Yamaguchi, 2020) and fair classiﬁcation (Barocas et al., 2018) literature focusing on their underlying assumptions and theoretical guarantees. The different regularization strategies and models can be seen as speciﬁc instance of the loss function in equation 10.
An empirical comparison between instances of the different criteria can be found in section 4, proofs are reported in appendix B, while the relation between the reported criteria is further discussed in appendix B.9. 3.1
Information Bottleneck Criterion
Combining the results from propositions 1 and 2 we can infer that reducing the latent test error necessarily requires the representation to discard some train predictive information (It=1(x; y|z)>0).
This is because a lossless encoder ˆq(z|x) together with an optimal latent classiﬁer ˆq(y|z) would result in an overall model ˆq(y|x) that matches the train distribution p(y|x, t = 1), and, therefore, results in positive latent test error (proposition 1).
The Information Bottleneck criterion (Tishby & Zaslavsky, 2015) introduces a regulariza-tion term R(qz|x) = It=1(x; z) to control the amount of information in the representation, deﬁning a lossy compression scheme (Alemi et al., 2017) that can be regulated using the hyper-parameter λ.
Note that this criterion does not use any environmental information and blindly discards data-features depending on the regularization strength λ. As shown in the example reported in ﬁgure 1b, discarding information without any additional constraint can increase the amount of latent concept shift depending on the structure of the underlying data generating process. This is because discarding information is a necessary but not sufﬁcient condition to reduce the latent concept shift. 4In contrast with the domain adaptation and generalization literature, we will consider the more general case in which e is represented by a vector. 5
3.2
Independence Criterion
Whenever the data selection t depends only on some observed variable e (t = f (e; (cid:15))), the most intuitive approach to reduce the latent test error is to make the representation z independent of the environmental factors e. This can be done by minimizing mutual information between e and z:
R(qz|x) := It=1(e; z). This criterion, known as independence or statistical parity in the fair classiﬁ-cation literature (Dwork et al., 2011; Corbett-Davies et al., 2017), aims to remove any environmental information from z, resulting in a representation that satisﬁes p(z|t = 1) = p(z|e, t = 1).
Despite the usefulness of this criterion in the fairness and differential privacy literature, enforcing independence does not necessarily reduce the test error (Zhao et al., 2019; Johansson et al., 2019).
This is because a consistent marginal across different environments does not imply a consistent predictive distribution (p(y|z, e, t = 1) (cid:54)= p(y|z, t = 1)), and enforcing independence may even increase the latent concept shift and the test error (as shown in ﬁgure 3). 3.3 Sufﬁciency Criterion
Instead of enforcing a property on the marginal feature distribution, one can consider stable properties of the joint distribution of features z and labels y. The requirement of creating a representation that yields a stable classiﬁer for different values of the environmental factor e can be captured by the following regularization: R(qz|x) := It=1(y; e|z). Intuitively minimizing It=1(y; e|z) corresponds to minimizing the distance between the predictive distribution p(y|e, z, t = 1) for each one of the observed environmental conditions. We can show the following:
Proposition 4. Whenever the selection t can be expressed as a function of e and some independent noise (cid:15), the latent concept shift induced by a representation z of x is upper-bounded by I(y; e|z):
∃f : t = f (e, (cid:15)) =⇒ I(y; t|z) ≤ I(y; e|z). (11)
In other words, for a given selection t, it is possible to remove the effect of concept shift by enforcing the sufﬁciency constraint using the variable (or variables) e that are responsible for that selection.
The result from proposition 4 is applicable only if the two following conditions are met: (i) a sufﬁcient representation must exist (Koyama & Yamaguchi, 2020); (ii) enforcing sufﬁciency on the selected train distribution results in sufﬁciency for the overall joint distribution (It=1(y; e|z) = 0 =⇒
I(y; e|z) = 0) (Rosenfeld et al., 2020). Even if assumptions (i) and (ii) are often acceptable in practice, lack of environment variety at training time or direct dependencies between environmental factors and targets (such as in the y-CMNIST dataset in section 4.2) can compromise the effectiveness of the sufﬁciency criterion. An in depth discussion with a simple example is reported in appendix G. 3.4 Separation Criterion
The last family of objectives and design principles includes approaches that aim to capture the stability in the latent feature distribution when the target is observed across different environmental conditions p(z|y) = p(z|y, e) (Chouldechova, 2017; Li et al., 2018b). This requirement can be enforced by minimizing the dependency between environmental factors and the representation when the label is observed: R(qz|x) := It=1(e; z|y). The resulting separation criterion (since y separates z and e) can be used to identify stable properties of the joint distributions even when the selection t depends on both targets y and environmental factors e:
Proposition 5. If the selection t can be expressed as a function of e, y and some independent noise (cid:15), the latent concept shift of a representation z of x is upper-bounded by the sum of prior-shift I(y; t) and I(e; z|y):
∃f : t = f (e, y, (cid:15)) =⇒ I(y; t|z) ≤ I(y; t) + I(e; z|y). (12)
When selection t and targets y are marginally independent, proposition 5 guarantees that the latent concept shift of a representation that enforces separation is zero. Furthermore, whenever the marginal distribution p(y|t = 0) is known, it is possible to adjust the prediction of the latent classiﬁer on the test distribution5. 5Further details on the re-weighting procedure can be found in appendix B.8 6
CMNIST d-CMNIST y-CMNIST y d x e c y d e c y d x (a) e c x (b)
Figure 2: Graphical models (a) and error components (b) for the CMNIST, d-CMNIST and y-CMNIST data distributions. (a) Dashed lines are used to underline marginal independence between color c and environment e, while red arrows denote dependencies added to the original CMNIST distribution. (b)
Models trained with strong regularization (λ ≈ 107) for the different criteria are compared against the classiﬁers trained using only color, digit, picture, or prior information. The colors show the proportion of the test error (in nats) due to the predictive information loss (It=0(x; y|z), in orange) and the latent test error (DKL(pt=0 y|z ||qy|z), in blue) according to the decomposition in equation 7.
Similarly to the other criteria, one needs to assume that enforcing separation on train (It=1(e; z|y) = 0) sufﬁces to guarantee I(e; z|y) = 0. Although a representation that enforces separation always exists, the effectiveness of the separation criteria depends on the data-generating process, since the requirement could be exclusively satisﬁed by a constant representation. 4 Experiments
We evaluate the effectiveness of the criteria presented in section 3 and some of their most popular implementations on multiple versions of the CMNIST dataset (Arjovsky et al., 2019) produced by altering the data-generating process (ﬁgure 2a) to underline the shortcomings of the different methods.
The main advantage of using a synthetic dataset such as CMNIST lies in the possibility to directly optimize for the different criteria using differentiable discrete mutual information measurements.
This is possible since the joint occurrence of color and digit ˆx := [c, d] is a low-dimensional discrete sufﬁcient statistic of the pictures x, and, consequently, all the mutual information quantities of interest involving x ∈ [0, 1]28×28×2 can be computed using ˆx ∈ {0, 1} × {0, . . . , 9} instead. Further details regarding the direct optimization of the criteria are reported in appendix D.
In ﬁgure 3, the theoretical performance for each criterion is compared against the results obtained by training different models on the pictures x using neural network architectures to parametrize the encoder q(z|x) and classiﬁer q(y|z) for different regularization strength λ. For the comparison, we consider diverse popular models designed according to the criteria deﬁned in section 3:
• Variational Information Bottleneck (VIB) (Alemi et al., 2017): a variational tractable approximation of the Information Bottleneck criterion;
• Domain Adversarial Neural Network (DANN) (Ganin et al., 2016): an adversarial model based on a min-max game with discriminator d(e|z) that is optimized to predict environment information from the representation z to enforce the Independence criterion;
• Invariant Risk Minimization (IRM) (Arjovsky et al., 2019): A model designed following the Sufﬁciency criterion that aims to create a representation from which the same latent classiﬁer is simultaneously optimal on all environments.
• Conditional Domain Adversarial Neural Network (CDANN) (Li et al., 2018b): an adversarial approximation of the Separation criterion in which, analogously to DANN, a discriminator tries to predict the environment when the representation z and the true label y are given. 7
Figure 3: Comparison of training cross-entropy (x-axis) and test cross-entropy (y-axis) on the
CMNIST, d-CMNIST and y-CMNIST datasets for representations trained using different criteria directly (top row) and corresponding approximations/model (bottom row). Each trajectory describes the trade-off between the two errors for a wide range of regularization strength values λ. The white symbols in each plot represent the error of models that consider only color (p(y|c, t = 1)), digit (p(y|d, t = 1)), picture (p(y|x, t = 1)) or prior (p(y|t = 1)) information. The shaded area represents the standard deviation observed across three runs with different seeds.
• Variance-based Risk Extrapolation (VREx) (Krueger et al., 2020): a model designed follow-ing the Sufﬁciency and Independence criteria, based on the minimization of the training error variance across the different environments;
For a better comparison, all the models use the same encoder and classiﬁer neural network architec-tures. Each model has been trained by slowly increasing the regularization strength after an initial pre-training with small λ. Further details regarding the neural network architectures, objectives, optimization and speciﬁc hyper-parameters can be found in appendix E.6 4.1 Evaluation metric
The measure of test error, information loss and latent test error reported in ﬁgure 2b can be computed only for discrete variables. For neural network models the training and test error deﬁned in section 2 can be estimated up to a constant entropy by considering the expected negative log-likelihood (empirical cross-entropy):
DKL(pt=1 y|x||qy|x) ≈ − 1
N
N (cid:88) i=1 log q(y = yi|x = xi) − Ht=1(y|x) (cid:125) (cid:124) (cid:123)(cid:122) constant (13)
The samples xi, yi are obtained from p(x, y|t = 1) and p(x, y|t = 0) for train and test cross-entropy respectively. By computing the training and test cross-entropy values for different regularization strength λ, each model deﬁnes a trajectory from a maximum likelihood solution (λ = 0) to the results obtained when the corresponding independence constraint is enforced (large λ), as shown in ﬁgure 3.
Contrarily to accuracy measurements, the use of cross-entropy allows us to detect under-conﬁdent or over-conﬁdent predictive distributions. 6The implementation of the models in this work is available at https://github.com/mfederici/dsit 8
4.2 Datasets
The two variants of the CMNIST dataset considered in this work have been designed by minimally changing the original distribution to underline the strengths and weaknesses of the different criteria.
Across different experiments, each MNIST picture x of a digit d is associated with a color c that depends on a binary target y and an environment e. The d-CMNIST and y-CMNIST datasets are created by adding a dependency from the environment to digit d (d-CMNIST) and label y (y-CMNIST), which results in a correlation between targets and environment (I(e; y) > 0). Further details regarding the conditional distributions used to produce the different datasets can be found in appendix C.
The strong correlation between color and label across the different CMNIST versions can be seen as an artifact introduced by the selection t = f (e), and models that consider only digit information (white triangles in ﬁgure 3) outperform the ones that capture color information (white crosses) or both (white circle) in terms of test cross-entropy (y-axis).
CMNIST The CMNIST dataset was originally designed to underline the weaknesses of maximum-likelihood and Empirical Risk Minimization strategies (Arjovsky et al., 2019). The results displayed in the ﬁrst column of ﬁgure 3 conﬁrm that both the sufﬁciency and separation criteria manage to effectively reduce the test cross entropy error for sufﬁciently strong regularization λ, minimizing the latent test error while retaining more predictive information than a constant representation (ﬁrst column in ﬁgure 2b). On the CMNIST dataset, most of the models in analysis closely follow the trajectory estimated by optimizing the corresponding criterion directly. The trajectory deﬁned by the
VIB model differs from the one described by the Information Bottleneck criterion since, in absence of additional constraints, the nature of the information that is discarded (either color, digit or style) depends on the inductive bias introduced by the speciﬁc architecture. d-CMNIST The d-CMNIST dataset adds a dependency between environment e and digit d, increas-ing the frequency of speciﬁc digits for some environments. Digit information makes environment and label conditionally independent (y and e are d-separated in ﬁgure 2a). As claimed in Arjovsky et al. (2019), models based on the sufﬁciency criterion manage to reduce the latent test error (second column of ﬁgure 2b), while the separation criterion fails because of the direct dependency between environment and digits. The independence criterion does not improve the test performance since both color and digit correlate with the environment. Both DANN and CDANN architectures results in trajectories that are similar to the ones obtained optimizing for the corresponding criteria. Note that enforcing separation or independence does not improve upon the result that can be obtained by blindly discarding information using VIB. Despite the effectiveness of the sufﬁciency criterion, the model trained with the IRM objective lies far from the optimal solution. We believe that this is due to the relaxations and approximation introduced by the optimized objective as discussed in appendix F. y-CMNIST Adding a dependency between environment e and label y simulates the scenario in which some labels are more frequent in some environments. The arrow between d and y ﬂips when compared to the d-CMNIST dataset to represent stable p(d|y) across different environments. The corresponding y-CMNIST dataset includes a path from e to y that can not be blocked since no representation z can achieve sufﬁciency (I(e; y|z) = 0). Despite the optimality of the separation criterion (red line, third column of ﬁgure 3), the model trained with the CDANN objective struggles to minimize the test error due to instabilities of the adversarial training procedure, as discussed in appendix F . Once again, the trajectory described by the VREx objective is more favorable when compared to VIB, while the model trained using the IRM objective is far from optimality. Figure 2b conﬁrms that, on the y-CMNIST dataset, the separation criterion is the only one that minimizes the latent test error without discarding the entirety of the picture information. This underlines that the effectiveness of sufﬁciency and separation criteria strongly depends on the structure of the underlying graphical model. 5