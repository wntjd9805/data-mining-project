Abstract
We consider the problem of online classiﬁcation under a privacy constraint. In this setting a learner observes sequentially a stream of labelled examples (𝑥𝑡 , 𝑦𝑡 ), for 1 ≤ 𝑡 ≤ 𝑇, and returns at each iteration 𝑡 a hypothesis ℎ𝑡 which is used to predict the label of each new example 𝑥𝑡 . The learner’s performance is measured by her regret against a known hypothesis class H. We require that the algorithm satisﬁes the following privacy constraint: the sequence ℎ1, . . . , ℎ𝑇 of hypotheses output by the algorithm needs to be an ((cid:178), δ)-diﬀerentially private function of the whole input sequence (𝑥1, 𝑦1), . . . , (𝑥𝑇 , 𝑦𝑇 ). We provide the ﬁrst non-trivial regret bound for the realizable setting. Speciﬁcally, we show that if the class H has constant Littlestone dimension then, given an oblivious sequence of labelled examples, there is a private learner that makes in expectation at most 𝑂 (log 𝑇) mistakes – comparable to the optimal mistake bound in the non-private case, up to a logarithmic factor. Moreover, for general values of the Littlestone dimension 𝑑, the same mistake bound holds but with a doubly-exponential in 𝑑 factor. A recent line of work has demonstrated a strong connection between classes that are online learnable and those that are diﬀerentially-private learnable. Our results strengthen this connection and show that an online learning algorithm can in fact be directly privatized (in the realizable setting). We also discuss an adaptive setting and
√ provide a sublinear regret bound of 𝑂 (
𝑇). 1

Introduction
Privacy-preserving machine learning has attracted considerable attention in recent years, motivated by the fact that individuals’ data is often collected to train statistical models, and such models can leak sensitive data about those individuals [13, 32]. The notion of diﬀerential privacy has emerged as a central tool which can be used to formally reason about the privacy-accuracy tradeoﬀs one must make in the process of analyzing and learning from data. A considerable body of literature on diﬀerentially private machine learning has resulted, ranging from empirical works which train deep neural networks with a diﬀerentially private form of stochastic gradient descent [1], to a recent line of theoretical works which aim to characterize the optimal sample complexity of privately learning an arbitrary hypothesis class [3, 11, 20].
Nearly all of these prior works on diﬀerentially private learning, however, are limited to the statistical learning setting (also known as the oﬄine setting): this is the setting where the labeled data, (𝑥𝑡 , 𝑦𝑡 ), are assumed to be drawn i.i.d. from some unknown population distribution. This setting, while very well-understod and readily amenable to analysis, is unlikely to hold in practice. Indeed, the data (𝑥𝑡 , 𝑦𝑡 ) fed as input into the learning algorithm may shift over time (e.g., as a consequence of demographic changes in a population), or may be subject to more drastic changes which are adaptive to the algorithm’s prior predictions (e.g., drivers’ reactions to the recommendations of route-planning apps may aﬀect traﬃc patterns, which inﬂuence the input data to those apps). For this reason, it is desirable to develop provable algorithms which make fewer assumptions on the data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we do so by studying the setting of (private) online learning, in which the sequence of data (𝑥𝑡 , 𝑦𝑡 ) is allowed to be arbitrary, and we also discuss a certain notion of privacy in a setting where it is even allowed to adapt to the algorithm’s predictions in prior rounds. We additionally restrict our attention to the problem of classiﬁcation, namely where the labels 𝑦𝑡 ∈ {0, 1}; thus we introduce the problem of diﬀerentially private online classiﬁcation, and prove the following results (see Section 3 for the exact setup):
• In the realizable setting with an oblivious adversary, we introduce a private learning algo-rithm which, for hypothesis classes of Littlestone dimension 𝑑 (see Section 2.1) and time
˜𝑂 (2𝑂 (2𝑑) · log 𝑇), ignoring the dependence on horizon 𝑇, achieves a mistake bound of privacy parameters (Theorem 4.1).
• In the realizable setting with an adaptive adversary, we show that a slight modiﬁcation of the above algorithm achieves a mistake bound of ˜𝑂 (2𝑂 (2𝑑) ·
𝑇) (Theorem 4.2).
√
We remark that no algorithm (even without privacy, allowing randomization, and in the oblivious adversary setting) can achieve a mistake bound of smaller than Ω(𝑑) for classes of Littlestone dimension 𝑑 [30, 33]. Therefore, a class of inﬁnite Littlestone dimension cannot have any ﬁnite mistake bound, and the regret for any algorithm, for any time horizon 𝑇, is Ω(𝑇). Thus, our results listed above, which show a mistake-bound (which is also the regret in the realizable setting) of
˜𝑂 𝑑 (
𝑇) for classes of Littlestone dimension 𝑑, establish that in the realizable setting, ﬁniteness of the Littlestone dimension is necessary and suﬃcient for online learnability ([31]) with diﬀerential privacy.
√
Recently it was shown by Alon et al. [3] and Bun et al. [11] (later to be improved by Ghazi et al.
[20]) that ﬁniteness of the Littlestone dimension is necessary and suﬃcient for private learnability in the oﬄine setting, namely with i.i.d. data (and both in the realizable and agnostic settings). Since, as remarked above, the Littlestone dimension characterizes online learnability (even without privacy), this means that a binary hypothesis class is privately (oﬄine) learnable if and only if it is online learnable. Our result thus strengthens this connection, showing that the equivalence also includes private online learnability (in the realizable setting). 1.1