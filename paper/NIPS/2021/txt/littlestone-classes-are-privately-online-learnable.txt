Abstract
We consider the problem of online classiï¬cation under a privacy constraint. In this setting a learner observes sequentially a stream of labelled examples (ğ‘¥ğ‘¡ , ğ‘¦ğ‘¡ ), for 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡, and returns at each iteration ğ‘¡ a hypothesis â„ğ‘¡ which is used to predict the label of each new example ğ‘¥ğ‘¡ . The learnerâ€™s performance is measured by her regret against a known hypothesis class H. We require that the algorithm satisï¬es the following privacy constraint: the sequence â„1, . . . , â„ğ‘‡ of hypotheses output by the algorithm needs to be an ((cid:178), Î´)-diï¬€erentially private function of the whole input sequence (ğ‘¥1, ğ‘¦1), . . . , (ğ‘¥ğ‘‡ , ğ‘¦ğ‘‡ ). We provide the ï¬rst non-trivial regret bound for the realizable setting. Speciï¬cally, we show that if the class H has constant Littlestone dimension then, given an oblivious sequence of labelled examples, there is a private learner that makes in expectation at most ğ‘‚ (log ğ‘‡) mistakes â€“ comparable to the optimal mistake bound in the non-private case, up to a logarithmic factor. Moreover, for general values of the Littlestone dimension ğ‘‘, the same mistake bound holds but with a doubly-exponential in ğ‘‘ factor. A recent line of work has demonstrated a strong connection between classes that are online learnable and those that are diï¬€erentially-private learnable. Our results strengthen this connection and show that an online learning algorithm can in fact be directly privatized (in the realizable setting). We also discuss an adaptive setting and
âˆš provide a sublinear regret bound of ğ‘‚ (
ğ‘‡). 1

Introduction
Privacy-preserving machine learning has attracted considerable attention in recent years, motivated by the fact that individualsâ€™ data is often collected to train statistical models, and such models can leak sensitive data about those individuals [13, 32]. The notion of diï¬€erential privacy has emerged as a central tool which can be used to formally reason about the privacy-accuracy tradeoï¬€s one must make in the process of analyzing and learning from data. A considerable body of literature on diï¬€erentially private machine learning has resulted, ranging from empirical works which train deep neural networks with a diï¬€erentially private form of stochastic gradient descent [1], to a recent line of theoretical works which aim to characterize the optimal sample complexity of privately learning an arbitrary hypothesis class [3, 11, 20].
Nearly all of these prior works on diï¬€erentially private learning, however, are limited to the statistical learning setting (also known as the oï¬„ine setting): this is the setting where the labeled data, (ğ‘¥ğ‘¡ , ğ‘¦ğ‘¡ ), are assumed to be drawn i.i.d. from some unknown population distribution. This setting, while very well-understod and readily amenable to analysis, is unlikely to hold in practice. Indeed, the data (ğ‘¥ğ‘¡ , ğ‘¦ğ‘¡ ) fed as input into the learning algorithm may shift over time (e.g., as a consequence of demographic changes in a population), or may be subject to more drastic changes which are adaptive to the algorithmâ€™s prior predictions (e.g., driversâ€™ reactions to the recommendations of route-planning apps may aï¬€ect traï¬ƒc patterns, which inï¬‚uence the input data to those apps). For this reason, it is desirable to develop provable algorithms which make fewer assumptions on the data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we do so by studying the setting of (private) online learning, in which the sequence of data (ğ‘¥ğ‘¡ , ğ‘¦ğ‘¡ ) is allowed to be arbitrary, and we also discuss a certain notion of privacy in a setting where it is even allowed to adapt to the algorithmâ€™s predictions in prior rounds. We additionally restrict our attention to the problem of classiï¬cation, namely where the labels ğ‘¦ğ‘¡ âˆˆ {0, 1}; thus we introduce the problem of diï¬€erentially private online classiï¬cation, and prove the following results (see Section 3 for the exact setup):
â€¢ In the realizable setting with an oblivious adversary, we introduce a private learning algo-rithm which, for hypothesis classes of Littlestone dimension ğ‘‘ (see Section 2.1) and time
Ëœğ‘‚ (2ğ‘‚ (2ğ‘‘) Â· log ğ‘‡), ignoring the dependence on horizon ğ‘‡, achieves a mistake bound of privacy parameters (Theorem 4.1).
â€¢ In the realizable setting with an adaptive adversary, we show that a slight modiï¬cation of the above algorithm achieves a mistake bound of Ëœğ‘‚ (2ğ‘‚ (2ğ‘‘) Â·
ğ‘‡) (Theorem 4.2).
âˆš
We remark that no algorithm (even without privacy, allowing randomization, and in the oblivious adversary setting) can achieve a mistake bound of smaller than â„¦(ğ‘‘) for classes of Littlestone dimension ğ‘‘ [30, 33]. Therefore, a class of inï¬nite Littlestone dimension cannot have any ï¬nite mistake bound, and the regret for any algorithm, for any time horizon ğ‘‡, is â„¦(ğ‘‡). Thus, our results listed above, which show a mistake-bound (which is also the regret in the realizable setting) of
Ëœğ‘‚ ğ‘‘ (
ğ‘‡) for classes of Littlestone dimension ğ‘‘, establish that in the realizable setting, ï¬niteness of the Littlestone dimension is necessary and suï¬ƒcient for online learnability ([31]) with diï¬€erential privacy.
âˆš
Recently it was shown by Alon et al. [3] and Bun et al. [11] (later to be improved by Ghazi et al.
[20]) that ï¬niteness of the Littlestone dimension is necessary and suï¬ƒcient for private learnability in the oï¬„ine setting, namely with i.i.d. data (and both in the realizable and agnostic settings). Since, as remarked above, the Littlestone dimension characterizes online learnability (even without privacy), this means that a binary hypothesis class is privately (oï¬„ine) learnable if and only if it is online learnable. Our result thus strengthens this connection, showing that the equivalence also includes private online learnability (in the realizable setting). 1.1