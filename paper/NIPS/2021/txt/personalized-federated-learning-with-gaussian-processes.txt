Abstract
Federated learning aims to learn a global model that performs well on client devices with limited cross-client communication. Personalized federated learning (PFL) further extends this setup to handle data heterogeneity between clients by learning personalized models. A key challenge in this setting is to learn effectively across clients even though each client has unique data that is often limited in size. Here we present pFedGP, a solution to PFL that is based on Gaussian processes (GPs) with deep kernel learning. GPs are highly expressive models that work well in the low data regime due to their Bayesian nature. However, applying GPs to
PFL raises multiple challenges. Mainly, GPs performance depends heavily on access to a good kernel function, and learning a kernel requires a large training set. Therefore, we propose learning a shared kernel function across all clients, parameterized by a neural network, with a personal GP classiﬁer for each client. We further extend pFedGP to include inducing points using two novel methods, the ﬁrst helps to improve generalization in the low data regime and the second reduces the computational cost. We derive a PAC-Bayes generalization bound on novel clients and empirically show that it gives non-vacuous guarantees. Extensive experiments on standard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new setup of learning under input noise show that pFedGP achieves well-calibrated predictions while signiﬁcantly outperforming baseline methods, reaching up to 21% in accuracy gain. 1

Introduction
In recent years, there is a growing interest in applying learning in decentralized systems under the setup of federated learning (FL) [37, 51, 66]. In FL, a server node stores a global model and connects to multiple end-devices (“clients"), which have private data that cannot be shared. The goal is to learn the global model in a communication-efﬁcient manner. However, learning a single shared model across all clients may perform poorly when the data distribution varies signiﬁcantly across clients. Personalized Federated Learning (PFL) [67] addresses this challenge by jointly learning a personalized model for each client. While signiﬁcant progress had been made in recent years, leading approaches still struggle in realistic scenarios. First, when the amount of data per client is limited, even though this is one of the original motivations behind federated learning [4, 51, 72]. Second, when the input distribution shifts between clients, which is often the case, as clients use different devices and sensors. Last, when we require well-calibrated predictions, which is an important demand from medical and other safety-critical applications. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: pFedGP - learning a shared deep kernel function with client-speciﬁc GP models. Each client stores private data, possibly from a different distribution. The data is ﬁrst mapped to an embedding space with a shared neural network across all clients. Then, using common kernels a GP is applied to the data of the client for model learning and inference. We illustrate the per-client kernel matrix kθ(xi, xj). Bold cells indicate a stronger covariance.
Here, we show how Gaussian Processes (GPs) with deep kernel learning (DKL) [80] is an effective alternative for handling these challenges. GPs have good predictive performance in a wide range of dataset sizes [2, 81], they are robust to input noise [75], can adapt to shifts in the data distribution [48], and provide well-calibrated predictions [69]. While regression tasks are more natural for GPs, here we focus on classiﬁcation tasks for consistency with common benchmarks and learning procedures in the ﬁeld; however, our approach is also applicable to regression tasks.
Consider a naive approach that ﬁts a separate GP classiﬁer to each client based on its personal data.
Its performance heavily depends on the quality of the kernel, and standard kernels tend to work poorly in domains such as images. A popular solution to this problem is to use deep kernel learning (DKL)
[80], where a kernel is applied to features outputted by a neural network (NN). Unfortunately, GPs with DKL can strongly overﬁt, often even worse than standard NNs [56], and thus negate the main beneﬁt of using a GP. We solve this issue by jointly learning a shared kernel function across clients.
As the kernel captures similarities between inputs, a single kernel should work well across clients, while using a separate GP per client will give the required ﬂexibility for personalization.
We adapt a GP classiﬁer recently proposed in [2] which uses the Pólya-Gamma augmentation [57] in a tree-structure model to the federated setting. We term our method pFedGP. We extend pFedGP by tailoring two inducing points (IPs) methods [58, 70]. The ﬁrst helps generalization in the low data regime and, unlike common inducing point methods, does not reduce the computational costs. The second does focus on reducing the computational cost to make our approach scalable and work in low-resource clients. We also adjust previous PAC-Bayes generalization bounds for GPs [60, 64] to include the Pólya-Gamma augmentation scheme. These bounds are suitable for cases where the kernel is not learned, such as when new clients arrive after the shared NN was already learned.
Therefore, this paper makes the following contributions: (i) introduce pFedGP as a natural solution to
PFL; (ii) develop two IP methods to enhance GP classiﬁers that use the Pólya-Gamma augmentation scheme and integrate them with pFedGP; (iii) derive a PAC-Bayes generalization bound on novel clients and show empirically that it gives meaningful guarantees; (iv) achieve state-of-the-art results in a wide array of experiments, improving accuracy by up to 21% 1. 2