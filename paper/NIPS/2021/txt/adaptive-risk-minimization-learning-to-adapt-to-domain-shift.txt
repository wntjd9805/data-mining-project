Abstract
A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this as-sumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to changing temporal correlations, atypical end users, or other factors. In this work, we consider the problem setting of domain generalization, where the training data are structured into domains and there may be multiple test time shifts, corresponding to new domains or domain distributions. Most prior methods aim to learn a single robust model or invariant feature space that performs well on all domains. In contrast, we aim to learn models that adapt at test time to domain shift using unlabeled test points. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), in which models are directly optimized for effective adaptation to shift by learning to adapt on the training domains. Compared to prior methods for robustness, in-variance, and adaptation, ARM methods provide performance gains of 1-4% test accuracy on a number of image classiﬁcation problems exhibiting domain shift. 1

Introduction
The standard assumption in empirical risk minimization (ERM) is that the data distribution at test time will match the training distribution. When this assumption does not hold, i.e., when there is distribution shift, the performance of standard ERM methods can deteriorate signiﬁcantly [54, 38].
As an example which we study quantitatively in Section 5, consider a handwriting classiﬁcation model that, after training on data from past users, is deployed to new end users. Each new user represents a new test distribution that differs from the training distribution. Thus, each test set-ting involves dealing with shift. In Figure 1, we visualize a batch of 50 examples from a test user, and we highlight an ambiguous example which may be either a “2” (written with a loop) or an “a” (in the double-storey style) depend-ing on the user’s handwriting. Due to the biases in the training data, an ERM trained model incorrectly classiﬁes this example as “2”. However, we can see that the batch of images from this test user contains other examples of “2” (written without loops) and “a” (also double-storey) from this user. Can we somehow leverage this unlabeled data to better handle test shifts caused by new users?
⇤equal contribution
Figure 1: An example of ambiguous data points in handwriting classiﬁcation, eval-uated quantitatively in Section 5. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Any framework that aims to address this question must use additional assumptions beyond the ERM setting, and many such frameworks have been proposed [54]. One commonly used assumption within several frameworks, such as domain generalization [7, 23], is that the training data are provided in domains and distributions at test time will represent new domains. The example above neatly ﬁts this description if we equate users with domains – we would be assuming that the training data are organized by users and that the model will be tested separately on new users, and these are reasonable assumptions. Constructing training domains in practice is generally accomplished by using meta-data, which exists for many commonly used datasets. Thus, this domain assumption is applicable for a wide range of realistic distribution shift problems (see, e.g., Koh et al. [35]).
However, prior benchmarks for domain generalization and similar settings typically center around invariances – i.e., in these benchmarks, there is a consistent input-output relationship across all domains, and the goal is to learn this relationship while ignoring the spurious correlations within the domains (see, e.g., Gulrajani and Lopez-Paz [23]). Thus, prior methods aim for generalization to shifts by discovering this relationship, through techniques such as robust optimization and learning an invariant feature space [41, 3, 60]. These methods are appealing in that they make minimal assumptions about the information provided at test time – in particular, they do not require test labels, and the learned model can be immediately applied to predict on a single point. Nevertheless, these methods also have limitations, such as in dealing with problems where the input-output relationship varies across domains, e.g., the handwriting classiﬁcation example above.
In this paper, we instead focus on methods that aim to adapt at test time to domain shift. To do so, we study problems in which it is both feasible and helpful (and perhaps even necessary) to assume access to a batch or stream of inputs at test time. Leveraging this test assumption does not require labels for any test data and is feasible in many practical setups. For example, for handwriting classiﬁcation, we do not access only single handwritten characters from an end user, but rather collections of characters such as sentences or paragraphs. Unlabeled adaptation has been shown empirically to be useful for distribution shift problems [69, 63, 75], such as for dealing with image corruptions [25].
Taking inspiration from these ﬁndings, we propose and evaluate on a number of problems, detailed in
Section 5, for which adaptation is beneﬁcial in dealing with domain shift.
Our main contribution is to introduce the framework of adaptive risk minimization (ARM), which proposes the following objective: optimize the model such that it can maximally leverage the unlabeled adaptation phase to handle domain shift. To do so, we instantiate a set of methods that, given a set of training domains, meta-learns a model that is adaptable to these domains. These methods are straightforward extensions of existing meta-learning approaches, thereby demonstrating that tools from the meta-learning toolkit can be readily adapted to tackle domain shift. Our experiments in Section 5 test on several image classiﬁcation problems, derived from benchmarks for federated learning [9] and image classiﬁer robustness [25], in which training and test domains share structure that can be leveraged for improved performance. These testbeds are also a contribution of our work, as we believe these problems can supplement existing benchmarks which, almost exclusively, are designed with invariance in mind [3, 53, 23]. We also evaluate on the WILDS suite of distribution shift problems [35], which have been curated to faithfully represent important real world problems.
Empirically, we demonstrate that the proposed ARM methods, by leveraging meta-training and test time adaptation, are often able to outperform prior state-of-the-art methods by 1-4% test accuracy. 2