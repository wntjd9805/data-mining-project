Abstract
We propose a novel technique for faster deep neural network training which system-atically applies sample-based approximation to the constituent tensor operations, i.e., matrix multiplications and convolutions. We introduce new sampling tech-niques, study their theoretical properties, and prove that they provide the same convergence guarantees when applied to SGD training. We apply approximate tensor operations to single and multi-node training of MLP and CNN networks on
MNIST, CIFAR-10 and ImageNet datasets. We demonstrate up to 66% reduction in the amount of computations and communication, and up to 1.37x faster training time while maintaining negligible or no impact on the ﬁnal test accuracy. 1

Introduction
Approximation techniques for faster inference and training of deep neural networks (DNNs) have received considerable attention. Examples include quantization [1–5], low-rank and structured-sparse models [6–9], weight extrapolations [10], and partial/asynchronous gradient updates in the context of distributed training [11, 12]. Sampling-based approximations were used to accelerate inference
[13, 14], but using them in training [15–17] has not been systematically studied nor demonstrated end-to-end GPU performance beneﬁts in practice.
We propose a novel approach to accelerating DNN training by systematically approximating tensor operations via sampling. At a high level, the original matrix products and convolutions are replaced with their faster approximate versions. The approximation is applied separately to each tensor operation, keeping the network architecture and tensor dimensions intact, thereby facilitating the adoption of this technique in existing DNN training frameworks, potentially in combination with other approximation techniques. Furthermore, when combined with distributed training, our technique allows for seamless reduction in the communication bandwidth and increased performance gains.
We begin by reviewing the plethora of existing methods for approximating matrix multiplication. We compare several known algorithms [18–25], and ﬁnd column-row sampling (CRS) [20] to be the most suitable for approximating matrix multiplications in training. In order to compute the product of two matrices A(cid:62)B, the CRS algorithm samples the columns of A(cid:62) and the corresponding rows of B thus constructing smaller matrices which are then multiplied as usual. This method incurs low sampling overheads and lends itself to an efﬁcient implementation using existing dense matrix product routines.
CRS minimizes the approximation error for the Frobenius norm of the resulting matrix while keeping the approximation unbiased.
∗A Viterbi fellow 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Sampling-based approximations can be interpreted as a form of Dropout [26], and we discuss the similarities and differences between the two. While Dropout aims to prevent overﬁtting, we focus on approximations as means to accelerate training by reducing the amount of computation.
In this work we aim to answer two main questions. First, can neural networks be trained while using approximate tensor operations? Second, what are the relations between using exact or approximate operations during training?
We start by analyzing the simpler case of linear regression, where we can derive the effects of approximations in closed form. We deﬁne a new loss function that takes the sampling into account, and observe that the resulting gradients differ from the exact training due to the dependency between sampled features. To this end, we propose a new Bernoulli-CRS variant which achieves statistical independence of samples, study its properties, and show that in linear regression it is equivalent to dynamic L2 weight regularization of the original, non-approximate loss.
We then turn to the more general case of non-linear deep neural networks. We show that using sampling-based approximations in the backward pass provides the same convergence guarantees as the exact SGD for bounded weights. The convergence result holds for unbounded weights as well if approximation is applied only to the weight gradients and if the activation functions are bounded.
We also study a new TopK-CRS algorithm which deterministically selects the top-k column-row pairs with the highest norms. We show that this algorithm is equivalent to the minimal mean square error estimator (MMSE) in case column-row pairs are pairwise independent with zero mean.
Last, we generalize matrix product approximation to convolutions and analyze the approximation error to derive the optimal sampling policy. This allows us to apply approximations to training of convolutional neural networks (CNNs).
We implement our techniques in PyTorch [27] 2 and evaluate them on several DNN topologies, including MLP and CNN networks on MNIST [28], Wide ResNet 28-10 [29] on CIFAR-10 [30], and ResNet-50 and ResNet-152 [31] on ImageNet [32]. We demonstrate up to 66% reduction in the number of FLOPs and up to 1.33x faster training time with little or no degradation in model accuracy.
We develop another ﬂavor of TopK-CRS which samples according to the weight norms only. When sampling the same subset of weights for different workers in a data-parallel setting, our sampling technique enables reducing the amount of gradient communication between workers. Notably, our algorithm is compatible with the standard AllReduce approach used in distributed deep learning.
We implement an AllReduce scheme that takes advantage of the smaller gradient footprint and demonstrate 1.09x-1.37x speedup in multi-node training.
Our contributions are as follows:
• We derive general convergence guarantees for training with approximate tensor operations.
• We develop novel sampling algorithms and analyze their theoretical properties.
• We extend sampling-based algorithms to fast approximation of multi-channel convolutions.
• We show that our approach can reduce the computation, communication and total training time on several popular neural network architectures with little or no accuracy degradation. 2