Abstract
In this paper, we answer the question of when inserting label noise (less informative labels) can instead return us more accurate and fair models. We are primarily in-spired by three observations: 1) In contrast to reducing label noise rates, increasing the noise rates is easy to implement; 2) Increasing a certain class of instances’ label noise to balance the noise rates (increasing-to-balancing) results in an easier learning problem; 3) Increasing-to-balancing improves fairness guarantees against label bias. In this paper, we ﬁrst quantify the trade-offs introduced by increasing a certain group of instances’ label noise rate w.r.t. the loss of label informativeness and the lowered learning difﬁculties. We analytically demonstrate when such an increase is beneﬁcial, in terms of either improved generalization power or the fairness guarantees. Then we present a method to insert label noise properly for the task of learning with noisy labels, either without or with a fairness constraint.
The primary technical challenge we face is due to the fact that we would not know which data instances are suffering from higher noise, and we would not have the ground truth labels to verify any possible hypothesis. We propose a detection method that informs us which group of labels might suffer from higher noise without using ground truth labels. We formally establish the effectiveness of the proposed solution and demonstrate it with extensive experiments. 1

Introduction
The presence of training label noise is generally considered harmful. Typically the goal of learning with noisy labels is to improve the training by reducing the amount of noise in the training data
[4, 11]. This paper discusses the feasibility of the opposite strategy and shows cases when adding label noise, leading to a scenario with less informative labels, will result in more accurate and fair models. We are primarily motivated by three observations:
Observation I: Reducing label noise is hard, but increasing it is easy. While the literature has provided us with solutions to perform data cleaning, they often require a highly customized training process. Their lack of theoretical rigor has also posed challenges when performing evaluations.
On the other hand, as we will see later, increasing label noise is easy — one can always do so by randomly ﬂipping the current noisy labels to increase it further.
Observation II: Increasing a certain class of instances’ noise rate to balance the noise rates results in an easier learning problem. When label noise is class-dependent (label class Y = +1 v.s. Y = −1), popular noise-tolerant learning algorithms typically require the knowledge of noise rates [25]. However, the learner often needs to identify the unknown noise rates by some estimation procedure [26, 27, 36]. We articulate that the mis-speciﬁcation of noise rates will introduce additional learning errors, especially when the label noise is asymmetric (see Theorem 1). On the other 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
hand, although increasing the noise rate of the lower class one to balanced error rates reduces the informativeness of the training labels, it is regarded as an easier case to handle — when error rates are balanced, invoking loss correction procedures becomes unnecessary (see Lemma 2 & Theorem 3).
Observation III: Increasing-to-balancing improves fairness guarantees against label bias.
When the label noise rates are group-dependent (data from young group v.s. senior group), it has been reported in the literature that imposing fairness constraints directly on the noisy labels would instead reinforce the unfairness [30]. Fixing the fairness constraints again requires the knowledge of the label noise rates. We will show that increasing and balancing noise rates allows us to directly impose fairness guarantees on the noisy training data without knowing the noise rates (Theorem 6).
This paper ﬁrst quantiﬁes the trade-offs introduced by increasing a certain group of instances’ label noise rate with respect to the loss of informative labels and the gained beneﬁts for doing so.
We analytically demonstrate when such an increase proves beneﬁcial in terms of either improved generalization error or fairness guarantees. Then we present a method to leverage our idea of inserting label noise for the task of learning with noisy labels, either without or with a fairness constraint. The primary technical challenge we face is that we would not know which data instances are suffering from higher noise, and again we would not have the ground truth labels to verify any possible hypothesis. In response, we propose a detection method that informs us which group of labels might suffer from higher noise without using ground truth information. The core discovery is a couple of metrics that check the agreements of noisy labels among local neighbors. These two metrics can be easily estimated using noisy labels only and are shown to be sufﬁcient to inform us of the class of labels with a higher noise rate. With this knowledge, we propose an algorithm (NOISE+) to gradually insert noise into the lower noise class of labels. Our contributions summarize as follows:
• Our paper prototypes the idea of pre-processing noisy training labels for both the constrained and unconstrained learning problems. We show the possibility of improving the training accuracy and fairness guarantees by increasing a certain class of instances’ noise rates (increasing-to-balancing).
• To enable the deployment of our idea, we propose a detection algorithm to identify the class of instances with a higher label noise rate without using any ground truth label information.
• Our solution contributes to the learning with noisy labels literature by adding another tool that is robust to noise rate (noise transition matrix) estimation errors.
All omitted proofs can be found in the Appendix. The code for reproducing the experimental results is available at https://github.com/UCSC-REAL/CanLessBeMore. 1.1