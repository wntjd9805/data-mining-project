Abstract
Optical Processing Units (OPUs) – low-power photonic chips dedicated to large scale random projections – have been used in previous work to train deep neural networks using Direct Feedback Alignment (DFA), an effective alternative to backpropagation. Here, we demonstrate how to leverage the intrinsic noise of optical random projections to build a differentially private DFA mechanism, making
OPUs a solution of choice to provide a private-by-design training. We provide a theoretical analysis of our adaptive privacy mechanism, carefully measuring how the noise of optical random projections propagates in the process and gives rise to provable Differential Privacy. Finally, we conduct experiments demonstrating the ability of our learning procedure to achieve solid end-task performance. 1

Introduction
The widespread use of machine learning models has created concern about their release in the wild when trained on sensitive data such as health records or queries in data bases [7, 15]. Such concern has motivated a abundant line of research around privacy-preserving training of models. A popular technique to guarantee privacy is differential privacy (DP), that works by injecting noise in an deterministic algorithm, making the contribution of a single data-point hardly distinguishable from the added noise. Therefore it is impossible to infer information on individuals from the aggregate.
While there are alternative methods to ensure privacy, such as knowledge distillation (e.g. PATE
[25]), a simple and effective strategy is to use perturbed and quenched Stochastic Gradient Descent (SGD) [1]: the gradients are clipped before being aggregated and then perturbed by some additive noise, ﬁnally they are used to update the parameters. The DP property comes at a cost of decreased utility. These biased and perturbed gradients provide a noisy estimate of the update direction and decrease utility, i.e. end-task performance.
We revisit this strategy and develop a private-by-design learning algorithm, inspired by the implemen-tation of an alternative training algorithm, Direct Feedback Alignment [23], on Optical Processing
Units [20], photonic co-processors dedicated to large scale random projections. The analog nature of the photonic co-processor implies the presence of noise, and while this is usually minimized, in this case we propose to leverage it, and tame it to fulﬁll our needs, i.e to control the level of privacy of the learning process. The main sources of noise in Optical Processing Units can be modeled as additive
Poisson noise on the output signal, and approach a Gaussian distribution in the operating regime of the device. In particular, these sources can be modulated through temperature control, in order to attain a desired privacy level.
Finally, we test our algorithm using the photonic hardware demonstrating solid performance on the goal metrics. To summarize, our setting consists in OPUs performing the multiplication by a ﬁxed random matrix, with a different realization of additive noise for every random projection.
∗Equal contribution. Corresponding authors: ruben@lighton.ai and hj.medinaruiz@criteo.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1