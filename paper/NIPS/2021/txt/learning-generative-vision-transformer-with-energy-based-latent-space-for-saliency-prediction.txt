Abstract
Vision transformer networks have shown superiority in many computer vision tasks. In this paper, we take a step further by proposing a novel generative vision transformer with latent variables following an informative energy-based prior for salient object detection. Both the vision transformer network and the energy-based prior model are jointly trained via Markov chain Monte Carlo-based maximum likelihood estimation, in which the sampling from the intractable posterior and prior distributions of the latent variables are performed by Langevin dynamics. Further, with the generative vision transformer, we can easily obtain a pixel-wise uncertainty map from an image, which indicates the model conﬁdence in predicting saliency from the image. Different from the existing generative models which deﬁne the prior distribution of the latent variables as a simple isotropic Gaussian distribution, our model uses an energy-based informative prior which can be more expressive to capture the latent space of the data. We apply the proposed framework to both
RGB and RGB-D salient object detection tasks. Extensive experimental results show that our framework can achieve not only accurate saliency predictions but also meaningful uncertainty maps that are consistent with the human perception. 1

Introduction
In the ﬁeld of computer vision, salient object detection [64, 65, 16, 17, 5, 89] (SOD) or visual saliency prediction, which aims at highlighting objects more attentive than the surrounding areas in images, has achieved signiﬁcant performance improvement with the deep convolutional neural network revolution.
Given a set of training images along with their saliency annotations, the conventional SOD models seek to learn a deterministic one-to-one mapping from image domain to saliency domain.
Two main issues exist in the above conventional deep saliency prediction framework: (i) the convolu-tion operation based on sliding window makes the deep saliency prediction model less effective in modeling the global contrast of the image, which is essential for salient object detection [7]; (ii) the one-to-one deterministic mapping mechanism makes the current framework not only impossible to represent the pixel-wise uncertainty in predicting salient objects, but also hard to handle incomplete data in a weakly supervised scenario [89]. Besides, given an image, the saliency output of a human is subjective, therefore, a stochastic generative model is more natural than a deterministic model for representing saliency prediction. Although [85] introduces a conditional variational autoencoder (CVAE) [56] for RGB-D salient object detection, the potential posterior collapse problem [23] makes the stochastic predictions less effective in generating meaningful uncertainty estimation.
Motivated by the above two issues, we propose a novel framework, the generative vision transformer, for salient object detection, where a vision transformer structure [40] is used as a backbone and latent variables are introduced in designing our generative framework. On the one hand, transformers [60] have proven to be very effective in long-range dependency modeling, and are capable of modeling 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
various scopes of object context information with the multi-head self-attention module. With such an architecture, we can achieve global context modeling for effective salient object detection. On the other hand, the latent variables account for randomness and uncertainty in modeling the mapping from image domain to saliency domain, and also enable the model to produce stochastic saliency predictions for uncertainty estimation. Therefore, the proposed model is a latent variable transformer.
Nowadays, there are two types of generative models that have been widely used, namely the varia-tional autoencoder (VAE) [31] and the generative adversarial network (GAN) [20], which correspond to two different generative learning strategies to train latent variable models. To train a top-down latent variable generator, the VAE introduces an extra encoder to approximate the intractable pos-terior distribution of the latent variables, and trains the generator via a perturbation of maximum likelihood; while the GAN introduces a discriminator to distinguish between generated samples and real data, and trains the generator to fool the discriminator. [22, 69] present the third learning strategy, namely alternating back-propagation (ABP), to train the generator with latent variables being directly sampled from the true posterior distribution by using a gradient-based Markov chain Monte Carlo (MCMC) [38], e.g., Langevin dynamics [43, 66, 12]. All the three generative models deﬁne the prior distribution of the latent variables as a simple non-informative isotropic Gaussian distribution, which is less expressive in capturing meaningful latent representation of the data.
In this paper, we investigate generative modeling and learning of the vision transformer. We construct a generative model for salient object detection in the form of a top-down conditional latent variable model. Speciﬁcally, we propose a generative vision transformer by adding latent variables into the traditional deterministic vision transformer, and assume the latent variables follow an informative trainable energy-based prior distribution [47, 48]. Following [72], we parameterize the energy function of the energy-based model (EBM) by a deep net. Instead of using variational learning or adversarial learning, we jointly train the parameters of the EBM prior and the transformer network by maximum likelihood estimation (MLE). The MLE algorithm relies on MCMC sampling to evaluate the intractable prior and posterior distributions of the latent variables.
Experimental results on RGB and RGB-D salient object detections [64, 68, 85, 16] show that the generative framework equipped with the EBM prior and the transformer-based non-linear mapping is powerful in representing the conditional distribution of object saliency given an image, leading to more reasonable uncertainty estimation as shown in Figure 1, where stochastic saliency prediction is provided by a learned model and the visualization of the pixel-wise uncertainty is presented.
Figure 1: An illustration of the stochastic saliency prediction obtained by the proposed generative vision transformer with an EBM prior, as well as the corresponding pixel-wise uncertainty map.
We summarize our main contributions and novelties as follows: (i) we propose a novel top-down generative vision transformer network with an energy-based prior distribution deﬁned on latent space for salient object detection; (ii) we jointly train the vision transformer network and the energy-based prior model by an MCMC-based maximum likelihood estimation, without relying on any extra assisting network for adversarial learning or variational learning; (iii) we achieve new benchmark results for both RGB and RGB-D salient object detections, and obtain meaningful uncertainty maps that are highly consistent with human perception for saliency prediction. 2