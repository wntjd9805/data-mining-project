Abstract
Ensembles of neural networks achieve superior performance compared to stand-alone networks in terms of accuracy, uncertainty calibration and robustness to dataset shift. Deep ensembles, a state-of-the-art method for uncertainty estimation, only ensemble random initializations of a ﬁxed architecture. Instead, we propose two methods for automatically constructing ensembles with varying architectures, which implicitly trade-off individual architectures’ strengths against the ensemble’s diversity and exploit architectural variation as a source of diversity. On a variety of classiﬁcation tasks and modern architecture search spaces, we show that the resulting ensembles outperform deep ensembles not only in terms of accuracy but also uncertainty calibration and robustness to dataset shift. Our further analysis and ablation studies provide evidence of higher ensemble diversity due to architectural variation, resulting in ensembles that can outperform deep ensembles, even when having weaker average base learners. To foster reproducibility, our code is available: https://github.com/automl/nes 1

Introduction
Some applications of deep learning rely only on point estimate predictions made by a neural network.
However, many critical applications also require reliable predictive uncertainty estimates and robust-ness under the presence of dataset shift, that is, when the observed data distribution at deployment differs from the training data distribution. Examples include medical imaging [15] and self-driving cars [5]. Unfortunately, several studies have shown that neural networks are not always robust to dataset shift [46, 26], nor do they exhibit calibrated predictive uncertainty, resulting in incorrect predictions made with high conﬁdence [21].
Deep ensembles [33] achieve state-of-the-art results for predictive uncertainty calibration and robust-ness to dataset shift. Notably, they have been shown to outperform various approximate Bayesian neural networks [33, 46, 22]. Deep ensembles are constructed by training a ﬁxed architecture mul-tiple times with different random initializations. Due to the multi-modal loss landscape [18, 54], randomization by different initializations induces diversity among the base learners to yield a model with better uncertainty estimates than any of the individual base learners (i.e. ensemble members).
Our work focuses on automatically selecting varying base learner architectures in the ensemble, exploiting architectural variation as a beneﬁcial source of diversity missing in deep ensembles due to their ﬁxed architecture. Such architecture selection during ensemble construction allows a more “ensemble-aware” choice of architectures and is based on data rather than manual biases. As
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
discussed in Section 2, while ensembles with varying architectures has already been explored in the literature, variation in architectures is typically limited to just varying depth and/or width, in contrast to more complex variations, such as changes in the topology of the connections and operations used, as considered in our work. More generally, automatic ensemble construction is well-explored in AutoML [17, 36, 45, 43]. Our work builds on this by demonstrating that, in the context of uncertainty estimation, automatically constructed ensembles with varying architectures outperform deep ensembles that use state-of-the-art, or even optimal, architectures (Figure 1). Studied under controlled settings, we assess the ensembles by various measures, including predictive performance, uncertainty estimation and calibration, base learner performance and two ensemble diversity metrics, showing that architectural variation is beneﬁcial in ensembles.
Note that, a priori, it is not obvious how to ﬁnd a set of diverse architectures that work well as an en-semble. On the one hand, optimizing the base learn-ers’ architectures in isolation may yield multiple base learners with similar architectures (like a deep ensemble). On the other hand, selecting the archi-tectures randomly may yield numerous base learn-ers with poor architectures harming the ensemble.
Moreover, as in neural architecture search (NAS), we face the challenge of needing to traverse vast architectural search spaces. We address these chal-lenges in the problem of Neural Ensemble Search (NES), an extension of NAS that aims to ﬁnd a set of complementary architectures that together form a strong ensemble. In summary, our contributions are as follows: 1. We present two NES algorithms for automati-cally constructing ensembles with varying base learner architectures. As a ﬁrst step, we present
NES with random search (NES-RS), which is simple and easily parallelizable. We further pro-pose NES-RE inspired by regularized evolution
[48], which evolves a population of architectures yielding performant and robust ensembles. 2. This work is the ﬁrst to apply automatic ensem-ble construction over architectures to complex, state-of-the-art neural architecture search spaces. 3. In the context of uncertainty estimation and ro-Figure 1: A comparison of a deep ensem-ble with the best architecture (out of 15,625 possible architectures) and ensembles con-structed by our method NES on ImageNet-16-120 over NAS-Bench-201. Performance metrics (smaller is better): error, negative log-likelihood (NLL) and expected calibration error (ECE). We also measure average base learner
NLL and two metrics for ensemble diversity (see Section 3): oracle NLL and [1−predictive disagreement]; small means more diversity for both metrics. NES ensembles outperform the deep ensemble, despite the latter having a sig-niﬁcantly stronger average base learner. bustness to dataset shift, we demonstrate that ensembles constructed by NES improve upon state-of-the-art deep ensembles. We validate our ﬁndings over ﬁve datasets and two architecture search spaces. 2