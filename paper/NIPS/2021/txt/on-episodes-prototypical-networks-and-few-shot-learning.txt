Abstract
Episodic learning is a popular practice among researchers and practitioners inter-ested in few-shot learning. It consists of organising training in a series of learning problems (or episodes), each divided into a small training and validation subset to mimic the circumstances encountered during evaluation. But is this always neces-sary? In this paper, we investigate the usefulness of episodic learning in methods which use nonparametric approaches, such as nearest neighbours, at the level of the episode. For these methods, we not only show how the constraints imposed by episodic learning are not necessary, but that they in fact lead to a data-inefﬁcient way of exploiting training batches. We conduct a wide range of ablative experi-ments with Matching and Prototypical Networks, two of the most popular methods that use nonparametric approaches at the level of the episode. Their “non-episodic” counterparts are considerably simpler, have less hyperparameters, and improve their performance in multiple few-shot classiﬁcation datasets. 1

Introduction
The problem of few-shot learning (FSL) – classifying examples from previously unseen classes given only a handful of training data – has considerably grown in popularity within the machine learning community in the last few years. The reason is likely twofold. First, being able to perform well on FSL problems is important for several applications, from learning new symbols [23] to drug discovery [2]. Second, since the aim of researchers interested in meta-learning is to design systems that can quickly learn novel concepts by generalising from previously encountered learning tasks, FSL benchmarks are often adopted as a practical way to empirically validate meta-learning algorithms.
To the best of our knowledge, there is not a widely recognised deﬁnition of meta-learning. In a recent survey, Hospedales et al. [22] informally describe it as “the process of improving a learning algorithm over multiple learning episodes”. In practical terms, following the compelling rationale that “test and train conditions should match” [48, 13], several seminal meta-learning papers (e.g. [48, 32, 14]) have emphasised the importance of organising training into episodes, i.e. learning problems with a limited amount of “training” (the support set) and “test” examples (the query set) to mimic the test-time scenario presented by FSL benchmarks.
However, several recent works (e.g. [9, 49, 11, 44]) showed that simple baselines can outperform established FSL meta-learning methods by using embeddings pre-trained with the standard cross-entropy loss, thus casting a doubt on the importance of episodes in FSL. Inspired by these results, we aim at understanding the practical usefulness of episodic learning in popular FSL methods relying on metric-based nonparametric classiﬁers such as Matching and Prototypical Networks [48, 40]. We chose this family of methods because they do not perform any adaptation at test time. This allows us
∗Work done while research intern at FiveAI 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
NO EPISODES
EPISODES
PAIRS LOST
POSITIVES (cid:1)w (cid:0)m+n 2 wmn 2 (m2 + n2 − m − n) w
NEGATIVES (cid:1)(m + n)2 (cid:0)w 2 w(w − 1)mn 2 (w − 1)(m2 + n2) w
Figure 1: Difference in batch exploitation for metric-based methods between adopting or not adopting the concept of episodes during training, on an illustrative few-shot learning problem with 2 ways (classes), and 4 shots (examples) and 1 query per class.
Table 1: The extra number of gradients that, on the same batch, a non-episodic method can exploit with respect to its episodic counterpart grows quadratically as O(w2(m2 + n2)), where w is the number of ways, and n and m are the number of shots and queries per class. to test the efﬁcacy of episodic training without having to signiﬁcantly change the baseline algorithms, which could potentially introduce confounding factors.
In this work we perform a case study focussed on Matching Networks [48] and Prototypical Net-works [40], and we show that within this family of methods episodic learning a) is detrimental for performance, b) is analogous to randomly discarding examples from a batch and c) introduces a set of superﬂuous hyperparameters that require careful tuning. Without episodic learning, these methods are closely related to the classic Neighbourhood Component Analysis (NCA) [19, 35] on deep embeddings and achieve, without bells and whistles, an accuracy that is competitive with recent methods on multiple FSL benchmarks: miniImageNet, CIFAR-FS and tieredImageNet.
PyTorch code is available at https://github.com/fiveai/on-episodes-fsl. 2