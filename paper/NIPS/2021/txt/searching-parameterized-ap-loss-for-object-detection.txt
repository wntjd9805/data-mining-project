Abstract
Loss functions play an important role in training deep-network-based object de-tectors. The most widely used evaluation metric for object detection is Average
Precision (AP), which captures the performance of localization and classiﬁcation sub-tasks simultaneously. However, due to the non-differentiable nature of the AP metric, traditional object detectors adopt separate differentiable losses for the two sub-tasks. Such a mis-alignment issue may well lead to performance degradation.
To address this, existing works seek to design surrogate losses for the AP metric manually, which requires expertise and may still be sub-optimal. In this paper, we propose Parameterized AP Loss, where parameterized functions are introduced to substitute the non-differentiable components in the AP calculation. Different AP approximations are thus represented by a family of parameterized functions in a uni-ﬁed formula. Automatic parameter search algorithm is then employed to search for the optimal parameters. Extensive experiments on the COCO benchmark with three different object detectors (i.e., RetinaNet, Faster R-CNN, and Deformable DETR) demonstrate that the proposed Parameterized AP Loss consistently outperforms existing handcrafted losses. Code shall be released. 1

Introduction
The past decade has witnessed the signiﬁcant success of deep neural networks in object detection, in which loss functions play an indispensable role in training networks. To evaluate the object detection methods, the Average Precision (AP) metric is usually used, which captures the performance of localization and classiﬁcation simultaneously. However, as in most object detectors [26], the training of localization and classiﬁcation sub-tasks are driven by two separate losses (see Figure 1). For example, the L1/smooth-L1 [11] or GIoU [38] losses are usually employed for localization, while the cross-entropy or Focal [25] losses are usually used for classiﬁcation. Such a mis-alignment between network training and evaluation may well lead to performance degradation.
To mitigate this mis-alignment issue, a straight-forward solution is to approximate the AP metric in network training. Because the AP metric is non-differentiable, many works [3, 5, 13, 40, 35] have explored hand-crafted losses based on the mathematical formula of the AP metric. [3] replaces the non-differentiable parts in the AP metric with hand-crafted differentiable approximations. The loss gradient is obtained by taking derivation with respect to the hand-crafted function. However, as both [5] and our experiments show, such a hand-crafted AP approximated loss produces lower
∗Equal contribution. †This work is done when Chenxin Tao and Zizhang Li are interns at SenseTime
Research. ‡Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Comparison between existing approaches and our Parameterized AP Loss. performance than the commonly used cross-entropy and L1/smooth-L1 losses. Another line of works try to estimate the gradient for the AP metric directly [5, 13, 40, 35]. These works try to manually design the loss gradient. Therein, the AP approximated gradient only drives the training of the classiﬁcation branch, while the training of the localization branch is still supervised by traditional regression losses. In practice, these methods still do not address the mis-alignment issue well.
The common issue of existing approaches is they do not optimize over the numerous approximations of the non-differentiable discrete AP metric. The AP metric itself is a piecewise constant function, whose differentiable approximations have inﬁnite possibilities. The hand-crafted approximations / gradients may well be sub-optimal for driving network training. Instead of manually determining the surrogate loss form, we propose to approximate the non-differentiable parts with a family of continuous parameterized functions, which helps to represent the numerous AP approximations in a uniﬁed formula. Then, an efﬁcient parameter search procedure is employed to ﬁnd out the desired loss function, so as to optimize the trained object detector’s performance on the evaluation set with the AP metric. Because the parameterized AP approximations constitute a compact search space, the search process would be very effective.
To this end, we propose the Parameterized AP Loss, which is built on top of the AP metric to mitigate the mis-alignment between network training and evaluation. It utilizes parameterization to construct the search space so that the optimal parameters can be searched automatically. Speciﬁcally, we ﬁrst explicitly reformulate the AP metric for object detection as a function of classiﬁcation scores and box coordinates. Then we replace the non-differentiable components in the reformulated function with parameterized functions. Finally, to obtain the optimal loss function for network training, we search the parameters through a reinforcement-learning-based search process, which aims to maximize the
AP score on the evaluation set.
We evaluate the searched Parameterized AP Loss on various object detectors, including RetinaNet [25],
Faster R-CNN [37] and Deformable DETR [42]. Extensive experiments on the COCO benchmark [23] demonstrate that the proposed Parameterized AP Loss consistently outperforms existing delicately designed losses.
The main contributions of our work can be summarized as:
• By reformulating the AP metric and introducing differentiable parameterized substitutions,
Parameterized AP Loss can represent numerous AP approximations in a uniﬁed formula, which captures classiﬁcation and localization sub-tasks simultaneously in a single loss function.
• Instead of hand-crafting AP losses or gradient approximations, our approach automatically searches for the optimal parameter, optimizing for the trained object detector performance.
• Extensive experiments on different object detectors demonstrate that the searched Parameterized
AP Loss consistently outperforms the existing losses. 2