Abstract
Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efﬁcient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture ﬁne-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity.
Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classiﬁcation. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3× as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224 × 224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls. 1

Introduction
Transformer-based models [1] have achieved great success in the domains of natural language processing (NLP) [2, 3] and computer vision [4–6]. These models beneﬁt from the self-attention module, which can capture both adjacent and long-range correlations between tokens while efﬁciently scaling on modern hardware. However, the time and memory consumed by self-attention scale quadratically with the input length, making it very expensive to process long sequences. Many language and vision tasks beneﬁt from modeling long sequences. In NLP, document-level tasks require processing long articles [e.g., 7, 8], and the performance of language models often increases with sequence length [e.g., 9, 10]. In computer vision, many tasks involve high-resolution images, which are converted to long sequences of image patches before being processed with Transformer models [4, 6, 11]. As a result, it is crucial to design an efﬁcient attention mechanism for long sequence modeling that generalizes well across different domains.
∗Work done during an internship at NVIDIA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Various methods have been proposed to reduce the quadratic cost of full attention. However, an efﬁcient attention mechanism that generalizes well in both language and vision domains is less explored. One family of methods is to sparsify the attention matrix with predeﬁned patterns such as sliding windows [e.g., 12–15] and random sparse patterns [16]. These methods leverage strong inductive biases to improve both computational and model performance, but they limit the capacity of a self-attention layer because each speciﬁc token can only attend to a subset of tokens. Another family of methods leverages low-rank projections to form a low resolution representation of the input sequence, but the successful application of these methods has been limited to certain NLP tasks [e.g., 17–19]. Unlike sparse attention, this family of methods allows each token to attend to the entire input sequence. However, due to the loss of high-ﬁdelity token-wise information, their performance sometimes is not as good as full attention or sparse attention on tasks that require ﬁne-grained local information, including standard benchmarks in language [20] and vision [21].
Despite the rapid progress in efﬁcient Transformers, some proposed architectures can only be applied to bidirectional models [e.g., 15, 16, 18]. Transformer-based autoregressive models have achieved great successes in language modeling [22], image synthesis [23], and text-to-image synthesis [24], which also involve long texts or high-resolution images. It is desirable to design an efﬁcient trans-former that can be applied to both autoregressive and bidirectional models.
In this work, we unify a local window attention and a novel long-range attention into a single efﬁcient attention mechanism. We show that these two kinds of attention have complementary effects that together yield the state-of-the-art results on a range of tasks in language and vision, for both autoregressive and bidirectional models. Speciﬁcally, we make the following contributions:
• We propose Long-Short Transformer (Transformer-LS), an efﬁcient Transformer that integrates a dynamic projection based attention to model long-range correlations, and a local window attention to capture ﬁne-grained correlations. Transformer-LS can be applied to both autoregressive and bidirectional models with linear time and memory complexity.
• We compute a dynamic low-rank projection, which depends on the content of the input sequence.
In contrast to previous low-rank projection methods, our dynamic projection method is more
ﬂexible and robust to semantic-preserving positional variations (e.g., insertion, paraphrasing).
We demonstrate that it outperforms previous low-rank methods [17, 18] on Long Range Arena benchmark [20].
• We identify a scale mismatch problem between the embeddings from the long-range and short-term attentions, and design a simple but effective dual normalization strategy, termed DualLN, to account for the mismatch and enhance the effectiveness of the aggregation.
• We demonstrate that Long-Short Transformer, despite its low memory and runtime complexity, outperforms the state-of-the-art models on a set of tasks from Long Range Arena, and autore-gressive language modeling on enwik8 and text8. In addition, the proposed efﬁcient attention mechanism can be easily applied to the most recent vision transformer architectures [6, 11] and provides state-of-the-art results, while being more scalable to high-resolution images. We also investigate the robustness properties of the Transformer-LS on diverse ImageNet datasets. 2