Abstract
Image classiﬁcation models tend to make decisions based on peripheral attributes of data items that have strong correlation with a target variable (i.e., dataset bias).
These biased models suffer from the poor generalization capability when evalu-ated on unbiased datasets. Existing approaches for debiasing often identify and emphasize those samples with no such correlation (i.e., bias-conﬂicting) without deﬁning the bias type in advance. However, such bias-conﬂicting samples are signiﬁcantly scarce in biased datasets, limiting the debiasing capability of these approaches. This paper ﬁrst presents an empirical analysis revealing that training with “diverse” bias-conﬂicting samples beyond a given training set is crucial for debiasing as well as the generalization capability. Based on this observation, we propose a novel feature-level data augmentation technique in order to synthesize diverse bias-conﬂicting samples. To this end, our method learns the disentangled representation of (1) the intrinsic attributes (i.e., those inherently deﬁning a certain class) and (2) bias attributes (i.e., peripheral attributes causing the bias), from a large number of bias-aligned samples, the bias attributes of which have strong correlation with the target variable. Using the disentangled representation, we synthesize bias-conﬂicting samples that contain the diverse intrinsic attributes of bias-aligned samples by swapping their latent features. By utilizing these diversiﬁed bias-conﬂicting features during the training, our approach achieves superior classi-ﬁcation accuracy and debiasing results against the existing baselines on synthetic and real-world datasets. 1

Introduction
Despite the recent advancement of deep neural networks, they often rely overly on the correlation between peripheral attributes and labels, referred to as dataset bias [1], especially when such strong bias is found in a given dataset. A majority of samples in the biased dataset exhibit visual attributes that are not innate but frequently co-occur with target labels (i.e., bias attributes). For example, most of the bird images in the training dataset may contain the background as the blue sky, while the birds may still be found in different places. Thus, the model trained with such a biased dataset is likely to learn the bias attributes more than intrinsic attributes, the innate visual attributes that inherently deﬁne a certain class, e.g., the wings of birds. This causes the model to learn shortcuts for classiﬁcation [2], failing to generalize on the images with no such correlations (e.g., birds on grounds or grass) during the test phase. Throughout the paper, bias-aligned samples correspond to data items containing a strong correlation between bias attributes and labels (e.g., birds in the sky), while bias-conﬂicting samples indicate the other cases that are rarely found (e.g., birds on grounds).
To tackle such a task, previous studies often deﬁne a speciﬁc bias type (e.g., color and texture) in advance [3, 4, 5, 6, 7, 8, 9, 10], which enables them to design a debiasing network tailored for the predeﬁned bias type. For example, Bahng et al. [6] leverage BagNet [11], which has limited size
* indicates equal contribution. The order of ﬁrst authors was chosen by tossing a coin. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
of receptive ﬁelds, to focus on learning color and texture. However, deﬁning a bias type in advance 1) limits the capability of debiasing in other bias types and 2) requires expensive labor to manually identify the bias type. To handle such an issue, a recent approach [12] deﬁnes a bias based on an intuitive observation that the bias attributes are often easier to learn than the intrinsic attributes for neural networks. In this regard, they re-weight bias-conﬂicting samples while de-emphasizing the bias-aligned ones. However, we point out that the reason behind the limited generalization capability of existing debiasing approaches lies in the signiﬁcant scarcity of bias-conﬂicting samples compared to the bias-aligned ones in a given training set. In other words, it is challenging to learn the debiased representation from these scarce bias-conﬂicting samples because the models are prone to memorize (thus being overﬁtted to) these samples, failing to learn the intrinsic attributes. Therefore, we claim that a neural network can learn properly debiased representation when these data items are diversiﬁed during training.
We conduct a brief experiment to demonstrate the importance of diversity in debiasing. Diversity in our work indicates the different valid realization of intrinsic attributes in a certain class (e.g., thick, narrow, tilted, and scribbled digit shapes in MNIST [13]). Our observation is that training a model with diverse bias-conﬂicting samples beyond a given training set is crucial for learning debiased representation (Section 3.2). In this regard, synthesizing bias-conﬂicting samples is one of the straightforward approaches to increase the diversity of such samples. In fact, a large amount of bias-aligned samples in a given training set already contain diverse intrinsic attributes, which can work as informative sources for increasing the diversity. However, as bias and intrinsic attributes are highly entangled in their embedding space, it is difﬁcult to extract the intrinsic ones from these bias-aligned samples. Therefore, disentangling these correlations enables to synthesize diversiﬁed bias-conﬂicting samples that originate from bias-aligned samples.
In this paper, we propose a novel feature augmentation approach via disentangled representation for debiasing. We ﬁrst train two different encoders to embed images into the disentangled representation of their intrinsic and bias attributes. With the disentangled representation, we randomly swap the latent vectors extracted from different images, most of which are bias-aligned samples in our training set. These swapped features thus contain both bias and intrinsic attributes without the correlation between them, which, in turn, can work as augmented bias-conﬂicting samples in our training. These features include intrinsic features of bias-aligned ones, increasing the diversity of a given training set, especially for bias-conﬂicting data items. Furthermore, to enhance the quality of diversiﬁed features, we propose a scheduling strategy of feature augmentation which enables to utilize the representation disentangled to a certain degree. In summary, the main contributions of our work include:
• Through our preliminary experiment, we reveal that increasing the diversity of bias-conﬂicting samples is crucial for debiasing.
• Based on such an observation, we propose a novel feature augmentation method via disen-tangled representation for diversifying the bias-conﬂicting samples.
• We achieve the state-of-the-art performances in two synthetic datasets (i.e., Colored MNIST and Corrupted CIFAR-10) and one real-world dataset (i.e., Biased FFHQ) against existing baselines. 2