Abstract
Lightweight image super-resolution (SR) networks have obtained promising re-sults with moderate model size. Many SR methods have focused on designing lightweight architectures, which neglect to further reduce the redundancy of net-work parameters. On the other hand, model compression techniques, like neural architecture search and knowledge distillation, typically consume considerable memory and computation resources. In contrast, network pruning is a cheap and effective model compression technique. However, it is hard to be applied to SR net-works directly, because ﬁlter pruning for residual blocks is well-known tricky. To address the above issues, we propose aligned structured sparsity learning (ASSL), which introduces a weight normalization layer and applies L2 regularization to the scale parameters for sparsity. To align the pruned ﬁlter locations across different layers, we propose a sparsity structure alignment penalty term, which minimizes the norm of soft mask gram matrix. We apply aligned structured sparsity learn-ing strategy to train efﬁcient image SR network, named as ASSLN, with smaller model size and lower computation than state-of-the-art methods. We conduct ex-tensive comparisons with lightweight SR networks. Our ASSLN achieves superior performance gains over recent methods quantitatively and visually. 1

Introduction
Image super-resolution (SR) is a fundamental computer vision application, which aims to recover a high-resolution (HR) image from its low-resolution (LR) counterpart. In general, image SR is an ill-posed problem, because there exist many HR candidates for one LR input. To alleviate this problem, more and more researchers have been investigating plenty of deep convolutional neural networks (CNNs) [11, 31, 38] to achieve more accurate mapping from LR image to its HR target.
Deep CNN was ﬁrstly introduced for image SR in SRCNN [11] and has attracted continuous attention from both academic and industry communities with its promising SR performance. SRCNN only consists of three convolutional (Conv) layers, hindering its performance. Kim et al. achieved notable improvements over SRCNN by increasing the network depth in VDSR [30] with residual learning.
Deeper CNNs could be trained successfully with residual blocks [22]. By utilizing simpliﬁed residual blocks, Lim et al. [38] built a much deeper network EDSR. Zhang et al. [64] proposed a residual channel attention network (RCAN), which is one of the deepest SR networks. With increased network size (i.e., deeper and wider), very deep networks, like EDSR [38] and RCAN [64], have achieved remarkable SR performance. However, they also suffer from some drawbacks, such as heavy model parameters, number of operations, and inference time. Therefore, it is impractical to directly deploy them on resource-limited platforms without neural processing units or off-chip memory [36].
†Equal Contribution
*Corresponding author: Huan Wang (wang.huan@northeastern.edu) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
LapSRN [34] 813K/149.4G
HR (×4)
Params/FLOPs
MemNet [55] 677K/2,662.4G
CARN [1] 1,592K/90.9G
Figure 1: Visual results, parameter number, and FLOPs comparison for 4× SR on Urban100 [26] dataset (img_012 and img_020) among lightweight SR networks and a large one EDSR. When calculating FLOPs, we set output size as 3×1280×720. Our ASSL has the smallest number of parameters and FLOPs, while achieving comparable or even better results than others.
ASSLN (ours) 708K/40.6G
IMDN [27] 715K/40.9G
EDSR [38] 43M/2.9T
From this point of view, more and more works turn to design lightweight network architectures for efﬁcient image SR [1, 27]. Ahn et al. proposed cascading residual network (CARN) [1] by implementing a cascading mechanism upon a residual network. Hui et al. proposed information multi-distillation network (IMDN) [27]. Lee et al. introduced knowledge distillation (KD) [25] for image
SR [24, 36] with student and teacher networks. Besides, neural architecture search (NAS) [66, 15] was also utilized for lightweight SR models, like MoreMNAS [8] and FALSR [7]. However, there are still several downsides among these networks: (1) The knowledge distillation based methods usually introduce a large teacher network, which will consume more computation resources during distillation training. (2) The training in some of these NAS-based methods can also consume heavy computation resources. For example, 8 Tesla V100 GPUs are needed to train a single network for about three days in FALSR [7]. (3) Most lightweight SR methods neglect to consider the sparsity or redundancy in the Conv kernels, which can be optimized to be more efﬁcient. In short, more effective, resource-friendly, and general lightweight SR networks are still in need.
To further peel off the redundancy of Conv kernels, neural network pruning techniques [50, 53] are usually introduced to reduce the model complexity. Researchers mainly focus on ﬁlter pruning (a.k.a. structured pruning, e.g., [37]) rather than weight-element pruning (a.k.a. unstructured pruning, e.g., [18, 17]) for acceleration. Bridging ﬁlter pruning with image SR seems a plausible solution to strike a better performance-complexity trade-off. However, ﬁlter pruning methods in image classiﬁcation can hardly be transferred to SR networks directly. The main reason is that residual blocks have become a essential component in state-of-the-art SR networks to ease the training (e.g., the deep version of EDSR [38] has 80 residual blocks; RCAN [64] even has 200 residual blocks).
However, it is well-known that residual connections are hard to prune in structured pruning [37].
To tackle the above issues, we present aligned structured sparsity learning (ASSL) for efﬁcient image
SR (see Fig. 1). ASSL is essentially a regularization-based ﬁlter pruning method. We introduce a weight normalization layer [51] after each convolutional layer and apply sparsity-inducing L2 regularization to the scale parameters in the weight normalization. Besides, a central problem in pruning residual networks in image SR is to align the consequent sparsity structure across different layers (see Fig. 2 constrained Conv layers). In this regard, we propose a novel sparsity structure alignment regularization term to encourage the pruned ﬁlter locations across different layers to be the same. To the best of our knowledge, our ASSL is the ﬁrst attempt to leverage ﬁlter pruning for efﬁcient image SR. The main contributions of our work can be summarized as follows:
• We propose aligned structured sparsity learning (ASSL) for efﬁcient image super-resolution (SR). To the best of our knowledge, jointly optimizing image SR networks with structured sparsity constraint has received little research attention so far.
• Our ASSL offers a generic framework to structurally prune SR networks with extensive residual connections. To tackle the pruned ﬁlter location mismatch issue, a sparsity structure alignment penalty term is introduced to align the pruned ﬁlter indices across different layers.
• We employ ASSL to train an efﬁcient aligned structured sparsity learning network (ASSLN), with detailed pruning process visualization for analysis. Our ASSLN achieves superior gains over SOTA lightweight image SR methods quantitatively and visually. 2