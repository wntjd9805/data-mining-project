Abstract
How can we make use of information parallelism in online decision making prob-lems while efﬁciently balancing the exploration-exploitation trade-off? In this paper, we introduce a batch Thompson Sampling framework for two canonical online decision making problems, namely, stochastic multi-arm bandit and lin-ear contextual bandit with ﬁnitely many arms. Over a time horizon T , our batch
Thompson Sampling policy achieves the same (asymptotic) regret bound of a fully sequential one while carrying out only O(log T ) batch queries. To achieve this ex-ponential reduction, i.e., reducing the number of interactions from T to O(log T ), our batch policy dynamically determines the duration of each batch in order to bal-ance the exploration-exploitation trade-off. We also demonstrate experimentally that dynamic batch allocation dramatically outperforms natural baselines such as static batch allocations. 1

Introduction
Many problems in machine learning and artiﬁcial intelligence are sequential in nature and require making decisions over a long period of time and under uncertainty. Examples include A/B testing
[Graepel et al., 2010], hyper-parameter tuning [Kandasamy et al., 2018], adaptive experimental design [Berry and Fristedt, 1985], ad placement [Schwartz et al., 2017], clinical trials [Villar et al., 2015], and recommender systems [Kawale et al., 2015], to name a few. Bandit problems provide a simple yet expressive view of sequential decision making with uncertainty. In such problems, a repeated game between a learner and the environment is played where at each round the learner selects an action, so called an arm, and then the environment reveals the reward. The goal of the learner is to maximize the accumulated reward over a horizon T . The main challenge faced by the learner is that the environment is unknown, and thus the learner has to follow a policy that identiﬁes an efﬁcient trade-off between the exploration (i.e., trying new actions) and exploitation (i.e., choosing among the known actions). A common way to measure the performance of a policy is through regret, a game-theoretic notion, which is deﬁned as the difference between the reward accumulated by the policy and that of the best ﬁxed action in hindsight.
We say that a policy has no regret, if its regret growth-rate as a function of T is sub-linear. There has been a large body of work aiming to develop no-regret policies for a wide range of bandit problems (for a comprehensive overview, see [Lattimore and Szepesv´ari, 2020, Bubeck and Cesa-Bianchi, 2012, Slivkins, 2019]). However, almost all the existing policies are fully sequential in nature, meaning that once an action is executed the reward is immediately observed by the learner and can be incorporated to make the subsequent decisions. In practice however, it is often more preferable (and sometimes the only way) to explore many actions in parallel, so called a batch of actions, in order to gain more information about the environment in a timely fashion. For instance, in clinical trials, a phase of medical treatment is often carried out on a group of individuals and the results are gathered for the entire group at the end of the phase. Based on the collected information, the treatment for the subsequent phases are devised [Perchet et al., 2016]. Similarly, in a marketing campaign, the response to a line of products is not collected in a fully sequential manner, instead, a batch of products are mailed to a subset of costumers and their feedback is gathered collectively 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[Schwartz et al., 2017]. Note that developing a no-regret policy is impossible without any infor-mation exchange about the carried out actions and obtained rewards. Thus, the main challenge in developing a batch policy is to balance between how many actions to run in parallel (i.e., batch size) versus how frequently to share information (i.e., number of batches). At one end of the spectrum lie the fully sequential no-regret bandit policies where the batch size is 1, and the number of batches is
T . At the other end of the spectrum lie the fully parallel policies where the batch size is T and all the actions are completely determined a priory without any amount of information exchange (such policies clearly suffer a linear regret).
In this paper, we investigate the sweet spot between the batch size and the corresponding regret in the context of Thompson Sampling (TS). More precisely,
• For the stochastic N -armed bandit, we develop Batch Thomson Sampling (B-TS), a batch version of the vanilla Thomson Sampling policy, that achieves the problem-dependent asymptotic optimal regret with O(N log T ) batches. B-TS policy with the same number of batches also achieves the problem independent regret bound of O(
N T log T ) with Beta priors, and a slightly improved regret bound of O(
N T log N ) with Gaussian priors.
√
√
• For the stochastic N -armed bandit, we develop Batch Minimax Optimal Thompson Sam-pling (B-MOTS), a batch Thompson Sampling policy that achieves the optimal minimax problem-independent regret bound of O(
N T ) with O(N log T ) batches. We also present
B-MOTS-J, a variant of B-MOTS, designed for Gaussian rewards, which achieves both minimax and asymptotic optimality with O(N log(T )) batches.
√
• Finally, for the linear contextual bandit with N arms, we develop Batch Thompson Sam-pling for Contextual Bandits (B-TS-C) that achieves the problem-independent regret bound of ˜O(d3/2
T ) with O(N log(T )) batches.
√
The main idea that allows our batch policy to achieve near-optimal regret guarantees while reducing the number of sequential interactions with the environment from T to O(log T ) is a novel dynamic batch mechanism that determines the duration of each batch based on an ofﬂine estimation of the regret accumulated during that phase. We also observe empirically that batch Thompson Sampling methods with a ﬁxed batch size, but equal number of batches, incur higher regrets. 2