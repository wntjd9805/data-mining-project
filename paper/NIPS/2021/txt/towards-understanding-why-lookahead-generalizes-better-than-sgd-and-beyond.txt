Abstract
To train networks, lookahead algorithm [1] updates its fast weights k times via an inner-loop optimizer before updating its slow weights once by using the latest fast weights. Any optimizer, e.g. SGD, can serve as the inner-loop optimizer, and the derived lookahead generally enjoys remarkable test performance improvement over the vanilla optimizer. But theoretical understandings on the test performance improvement of lookahead remain absent yet. To solve this issue, we theoretically justify the advantages of lookahead in terms of the excess risk error which mea-sures the test performance. Speciﬁcally, we prove that lookahead using SGD as its inner-loop optimizer can better balance the optimization error and generalization error to achieve smaller excess risk error than vanilla SGD on (strongly) convex problems and nonconvex problems with Polyak-Łojasiewicz condition which has been observed/proved in neural networks. Moreover, we show the stagewise op-timization strategy [2] which decays learning rate several times during training can also beneﬁt lookahead in improving its optimization and generalization errors on strongly convex problems. Finally, we propose a stagewise locally-regularized lookahead (SLRLA) algorithm which sums up the vanilla objective and a local regularizer to minimize at each stage and provably enjoys optimization and gener-alization improvement over the conventional (stagewise) lookahead. Experimental results on CIFAR10/100 and ImageNet testify its advantages. Codes is available at https://github.com/sail-sg/SLRLA-optimizer. 1

Introduction
Deep neural networks (DNNs) have been successfully applied to various applications, such as image classiﬁcation [3–8], speech recognition [9–11], and classic games [12, 13]. Typically, their training models can be formally formulated as a ﬁnite-sum optimization problem: minθ FS (θ) (cid:44) 1 n (cid:96)(f (xi; θ), yi), (cid:88)n (1) i=1 where (x, y) is a sample pair from an unknown distribution D, the loss (cid:96)(f (x; θ), y) measures the discrepancy between the prediction f (x; θ) parameterized by θ and the target y, and n is the training sample number. Besides DNNs, the formulation (1) also encapsulates a large body of other problems, e.g., least square regression and logistic regression. Though many algorithms, e.g., variance-reduced algorithms [14–17] and adaptive gradient algorithms [18, 19], can solve problem (1), SGD [20, 21] is one of the most preferable algorithms because of its efﬁciency and good generalization [22].
In this work, we are particularly interested in the lookahead algorithm [1] for solving (1). Its core idea is to maintain two kinds of network parameters, i.e. “fast weights" v and “slow weights" θ, and update them in turn. Speciﬁcally, in the inner loop, it takes the slow weights θ as warm-start 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
initialization and updates the fast weights k times to obtain vk using any vanilla optimizer; for the outer loop, it updates the slow weights as θ+ = (1 − α)θ + αvk, where α ∈ (0, 1]. Any standard optimizer, e.g. SGD, Adam [18] and RAdam [23], can serve as the inner-loop optimizer, and the derived lookahead algorithm generally enjoys remarkable test performance improvement than the standard optimizer [1]. Because of its simplicity and strong compatibility, lookahead has been widely used [24–29]. However, the theoretical reasons for the superiority in test performance of lookahead are rarely investigated, though heavily desired. Moreover, in practice, to train faster and generalize better, one often uses the stagewise optimization strategy [2], namely running a large learning rate at the beginning and geometrically decaying it several times during the following training. But for lookahead, the theoretical beneﬁts of the stagewise optimization strategy still remain unclear.
Our Contributions. In this work, we provide a theoretical viewpoint to understand the test perfor-mance improvement of lookahead, and also show the beneﬁts of stagewise optimization strategy in lookahead. Moreover, we further propose a new stagewise locally-regularized lookahead algo-rithm which provably enjoys faster convergence speed, smaller generalization error, and better test performance than conventional (stagewise) lookahead. Our contributions are highlighted below. 1 n
Firstly, we prove that on convex problems, lookahead using SGD as its inner-loop optimizer has optimization error O(cid:0)
αηkT + η(cid:1), where T and k respectively denote the outer- and inner-loop iteration numbers, and η is the learning rate for the inner-loop optimizer, i.e. SGD. Then we prove that lookahead has generalization error bound O(cid:0) αηkT (cid:1) on convex problems with n training samples.
Since the excess risk error can well measure the test performance of an algorithm and is upper bounded by the sum of optimization and generalization errors, we can bound the excess risk error of (cid:1). When α = 1, lookahead degenerates to vanilla SGD and has lookahead with O(cid:0) excess risk error O(cid:0) 1 (cid:1). Since the optimum of α in lookahead to optimally balance the optimization and generalization errors is often not one, lookahead can enjoy a smaller excess risk error than vanilla SGD. Similarly, on strongly convex problems and a class of nonconvex problems that obey Polyak-Łojasiewicz (PŁ) condition, our results also show that lookahead can better trade-off optimization and generalization errors and achieve smaller excess risk error than vanilla SGD. For
PŁ condition, it is observed /proved for deep learning models in [30–34] and our empirical results.
These results well explain the better test performance of lookahead than SGD.
αηkT + η + αηkT
ηkT + η + ηkT n n 1
Secondly, we propose a Stagewise Locally-Regularized LookAhead (SLRLA) algorithm, and prove its advantages over lookahead and stagewise lookahead in terms of excess risk error. SLRLA ﬁrst divides the optimization into several stages. Then at each stage, it minimizes a locally-regularized function that contains the vanilla loss FS (θ) in (1) and a local regularizer β 2 (cid:107)θ − θq(cid:107)2 with the output
θq of the previous stage. A similar stagewise strategy is commonly used in practice, but does not has the local regularizer. Our results show two advantages of SLRLA over lookahead and stagewise lookahead. (i) On strongly convex problems and weakly quasi convex problems (a class of nonconvex problems including convex problems as a special case), SLRLA achieves faster convergence rates (i.e. smaller optimization error), and enjoys smaller generalization error than lookahead and its stagewise variant. (ii) On nonconvex problems under PŁ condition, when the problems are not heavily nonconvex (the smallest eigenvalue of ∇2FS (θ) is not very small), SLRLA achieves linear convergence rate, and greatly improves optimization and generalization errors in lookahead, while stagewise lookahead cannot enjoy the linear convergence rate and the improvement.
Finally, when β = 0, SLRLA degenerates to conventional stagewise lookahead, and our results show the advantages of stagewise lookahead over lookahead in terms of excess risk error on strongly convex problems, explaining the beneﬁts of the stagewise strategy in lookahead. 2