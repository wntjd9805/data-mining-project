Abstract
Fine-tuning is a common practice in deep learning, achieving excellent general-ization results on downstream tasks using relatively little training data. Although widely used in practice, it is lacking strong theoretical understanding. Here we analyze the sample complexity of this scheme for regression with linear teachers in several architectures. Intuitively, the success of ﬁne-tuning depends on the similarity between the source tasks and the target task, however measuring this similarity is non trivial. We show that generalization is related to a measure that considers the relation between the source task, target task and covariance structure of the target data. In the setting of linear regression, we show that under realistic settings a substantial sample complexity reduction is plausible when the above measure is low. For deep linear regression, we present a novel result regarding the inductive bias of gradient-based training when the network is initialized with pretrained weights. Using this result we show that the similarity measure for this setting is also affected by the depth of the network. We further present results on shallow ReLU models, and analyze the dependence of sample complexity on source and target tasks in this setting. 1

Introduction
In recent years ﬁne-tuning has emerged as an effective approach to learning tasks with relatively little labeled data. In this setting, a model is ﬁrst trained on a source task where much data is available (e.g., masked language modeling for BERT), and then it is further tuned using gradient descent methods on labeled data of a target task [1, 2, 3, 4]. Furthermore, it has been observed that ﬁne-tuning can outperform the strategy of ﬁxing the representation learned on the source task, mainly in natural language processing [1, 5]. Despite its empirical success, ﬁne-tuning is poorly understood from a theoretical perspective. One apparent conundrum is that ﬁne-tuned models can be much larger than the number of target training points, resulting in a heavily overparameterized model that is prone to overﬁtting and poor generalization. Thus, the answer must lie in the fact that ﬁne-tuning is performed with gradient descent and not an arbitrary algorithm that could potentially “ignore” the source task
[6]. Here we set out to formalize this problem and understand the factors that determine whether
ﬁne-tuning will succeed. We note that this question can be viewed as part of the general quest to understand the implicit bias of gradient based methods [6, 7, 8, 9, 10, 11, 12, 13], but in the particular context of ﬁne-tuning. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We begin by highlighting the obvious link between ﬁne-tuning and initialization. Namely, the only difference between “standard” training of a target task and ﬁne-tuning on it, is the initial value of the model weights before beginning the gradient updates. Our goal is to understand the interplay between the model parameters at initialization (namely the source task), the target distribution, and the accuracy of the ﬁne-tuned model. A natural hypothesis is that the distance between the pretrained and ﬁne-tuned model weights is what governs the success of ﬁne-tuning. Indeed, some argue that this is both the key to bound the generalization error of a model and the implicit regularization of gradient-based methods [14, 15, 16, 17]. However, this approach has been discouraged both by empirical testing of the generalization bounds inspired by it [18] and by theoretical works showing this cannot be the inductive bias in deep neural networks [19]. Our results further establish the hypothesis that the success of ﬁne-tuning is affected by other factors.
In this paper we focus on the case in which both source and target regression tasks are linear functions of the input. We start by considering one layer linear networks, and derive novel sample complexity results for ﬁne-tuning. We then proceed to the more complex case of deep linear networks, and prove a novel result characterizing the ﬁne-tuned model as a function of both the weights after pretraining and the depth of the network, and use it to derive corresponding generalization results.
Our results provide several surprising insights. First, we show that the covariance structure of the target data has a signiﬁcant effect on the success of ﬁne-tuning. In particular, sample complexity is affected by the degree of alignment between the source-target weight difference and the eigenvectors of the target covariance. Second, we ﬁnd a strong connection between the depth of the network and the results of the ﬁne-tuning process, since deeper networks will serve to cancel the effect of scale differences between source and target tasks. Our results are corroborated by empirical evaluations.
We conclude with results on ReLU networks, providing the ﬁrst sample complexity result for ﬁne-tuning. For the case of linear teachers, this asserts a simple connection between the source and target models and the test error of ﬁne-tuning.
Taken together, our results demonstrate that ﬁne-tuning is affected not only by some notion of distance between the source and target tasks, but also by the target covariance and the architecture of the model. These results can potentially lead to improved accuracy in this setting via appropriate design of the tasks used for pretraining and the choice of the model architecture. 2