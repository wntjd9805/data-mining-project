Abstract
Ofﬂine reinforcement learning (RL) tasks require the agent to learn from a pre-collected dataset with no further interactions with the environment. Despite the potential to surpass the behavioral policies, RL-based methods are generally im-practical due to the training instability and bootstrapping the extrapolation errors, which always require careful hyperparameter tuning via online evaluation. In contrast, ofﬂine imitation learning (IL) has no such issues since it learns the policy directly without estimating the value function by bootstrapping. However, IL is usually limited in the capability of the behavioral policy and tends to learn a mediocre behavior from the dataset collected by the mixture of policies. In this paper, we aim to take advantage of IL but mitigate such a drawback. Observing that behavior cloning is able to imitate neighboring policies with less data, we propose
Curriculum Ofﬂine Imitation Learning (COIL), which utilizes an experience pick-ing strategy for imitating from adaptive neighboring policies with a higher return, and improves the current policy along curriculum stages. On continuous control benchmarks, we compare COIL against both imitation-based and RL-based meth-ods, showing that it not only avoids just learning a mediocre behavior on mixed datasets but is also even competitive with state-of-the-art ofﬂine RL methods. 1

Introduction
Ofﬂine reinforcement learning (RL), or batch RL, aims to learn a well-behaved policy from arbitrary datasets without interacting with the environment. This setting is generally a more practical paradigm than online RL since it is expensive or dangerous to interact with the environment in most real-world applications. Typically, two main kinds of ofﬂine datasets are considered in previous ofﬂine RL works [5, 18]: one contains transitions sampled by a single behavioral policy; the other includes a buffer collected by a mixture of policies.
Two main approaches have been deeply investigated for Ofﬂine RL. First, RL-based methods, in particular, Q-learning and policy gradient-based algorithms [13, 7, 12], have the potential to outperform the behavioral policy. However, they always suffer from serious bootstrapping errors and training instability. This shortcoming makes such algorithms impractical to be utilized since too many hyperparameters need to be tuned to achieve a good performance, and it is hard to evaluate a suitable model in an ofﬂine manner, as revealed in [18]. In contrast, ofﬂine imitation learning [1, 17, 2], speciﬁcally, behavior cloning (BC), can always stably learn to perform as the behavioral policy, which may be helpful under single-behavior datasets. However, BC may fail in learning a good behavior under a diverse dataset containing a mixture of policies (both goods and bads).
Quantity-quality dilemma on mixed dataset. As a supervised learning technique, BC is not easy to fulﬁll a desired result, especially on a mixed dataset. Speciﬁcally, it requires both quantity and quality
∗Equal contribution. †Corresponding author. Codes are available at https://github.com/apexrl/COIL. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
of the demonstration data, which can hardly be satisﬁed in ofﬂine RL tasks. Directly mimicking the policy from a mixed dataset that contains bad-to-good demonstrations can be regarded as imitation learning from a mediocre behavior policy. To achieve the best performance of the dataset, a naive idea is to imitate the top trajectories ordered by its return. However, such a simple strategy will reach the quantity-quality dilemma on the mixed dataset. For example, Fig. 1a illustrates the ordered trajectories on the Walker-final-dataset, which contains the whole training experience sampled by an online training agent. Different BC agents are trained by the top 10%, 25%, 50%, and 100% trajectories, but none of them gets rid of a mediocre performance, as shown in Fig. 1b. Typically, on such dataset, less data owns higher quality but less quantity, and thus cause serious compounding error problems [21, 10, 6]; on the other hand, more data provides a larger quantity, yet its mean quality becomes worse. In this work, we aim to solve such a dilemma and exploit the most potential of IL to derive a stable and practical algorithm reaching the best performance of a given dataset.
Our intuition comes from the observation that under RL scenarios, the agent can imitate a neighboring policy with much fewer samples. This observation promotes a curriculum solution for the above challenge. Speciﬁcally, for mixed datasets, the agent can adaptively imitate the better neighboring policies step by step and ﬁnally reach the optimal behavior policy of the dataset.
Our work. We propose Curriculum
Ofﬂine Imitation Learning (COIL), a simple yet effective ofﬂine imitation learning framework for ofﬂine RL.
At each training iteration, COIL im-proves the current policy with the data sampled by neighboring policies. To achieve that, COIL utilizes an adap-tive experience picking strategy and a return ﬁlter to select proper trajec-tories from the ofﬂine dataset for the current level of the agent and thus pro-duces stages of the curriculum. No-tably, COIL stops with a close-to-data-optimal policy without ﬁnding the best model under online evaluation. This feature allows to deploy the algorithm in practi-cal problems. In experiments, we show the effectiveness of COIL on various kinds of ofﬂine datasets.
Fig. 1 offers a quick review of our results: depending solely on BC, COIL can learn from scratch to reach the best performance of the given dataset.
Figure 1: Examples of the quality-quantity dilemma for BC. (a) Tra-jectories of the Walker2d-ﬁnal dataset ordered by their accumulated return. (b) Performances of behavior cloning (BC) for learning the top 10%, 25%, 50%, and 100% trajectories of the dataset. (b) Returns of BC v.s. COIL. (a) Ordered trajectories.
Contributions. To summarize, the main technical contributions of this paper are as follows.
• We highlight how the discrepancy between the target policy and the initialized policy affects the number of samples required by BC (Section 3);
• Depending on BC, we propose a practical and effective ofﬂine RL algorithm with a practical neighboring experience picking strategy that ends with a good policy (Section 4);
• We present promising comparison results with comprehensive analysis for our algorithm, which is competitive to the state-of-the-art methods (Section 6). 2 Preliminaries
Notations. We consider a standard Markov Decision Process (MDP) as a tuple M = (cid:104)S, A, T , ρ0, r, γ(cid:105), where S is the state space, A represents the action space, T : S × A × S → [0, 1] is the state transition probability distribution, ρ0 : S → [0, 1] is the initial state distribution,
γ ∈ [0, 1] is the discounted factor, and r : S × A → R is the reward function. The goal of
RL is to ﬁnd a policy π(a|s) : S × A → [0, 1] that maximizes the expected cumulative dis-counted rewards (or called return) along a trajectory τ : R(τ ) = (cid:80)T t=0 γtrt. The dataset D consists of trajectories {τ N 1 } that are the collected by a mixture of bad-to-good policies, where
, ai 0, s(cid:48)i a trajectory τi = {(si
)}, and hi is the horizon hi of τi. For any dataset, we assume a behavior policy πb that collects such data and its empir-, where I is the ical estimation ˆπb can be induced from D as ˆπb(a|s) = indicator function. We further introduce a common used term, occupancy measure, which is
I[s(cid:48)=s,a(cid:48)=a]
I[s(cid:48)=s] 1), · · · , (si hi (s(cid:48) ,a(cid:48) )∈D (cid:80) 0), (si 1, s(cid:48)i 0 , ri 1 , ri
, s(cid:48)i hi
, ri hi 1, ai 0, ai s(cid:48) ∈D (cid:80) 2
t=0 γtP (st = s, at = a|π) = π(a|s) (cid:80)∞ deﬁned as the discounted occurrence probability of states or state-action pairs under policy π:
ρπ(s, a) = (cid:80)∞ t=0 γtP (st = s|π) = π(a|s)ρπ(s). With such a deﬁnition we can write down that Eπ[·] = (cid:80)
Deﬁnition 1. The partial order of different policies is deﬁned as the relative return quantity that a policy can achieve when deployed in the environment. Formally, given two policies π1 and π2: s,a ρπ(s, a)[·] = E(s,a)∼ρπ [·].
π1 (cid:22) π2 ⇔ R1 (cid:22) R2 (1)
Therefore, by deﬁnition, in a mixed dataset D that is collected by K different policies π1, · · · , πK, the optimal behavior policy π∗ can be determined such that for ∀i ∈ [1, K], πi (cid:22) π∗.
Curriculum learning. Curriculum learning design and construct a curriculum automatically as a sequence of tasks G1, . . . , GN to train on, such that the efﬁciency or performance on a target task Gt is improved. The expected loss on the jth task is denoted Lj. 3 Empirical Observations and Theoretic Analysis
In this section, we begin with empirical observations that motivate the core idea of our method, followed by the theoretical analysis to support our motivation. Generally, we aim to investigate how the asymptotic performance of BC is affected by the discrepancy between the demonstrated policy and the initialized imitating policy. Previous research shows that BC tends to fail with a small number of high-quality demonstrations but can learn well from large-quantity and high-quality data [10, 8].
On the contrary, we ﬁnd that the requirement of quantity can be highly relaxed as the similarities between the demonstrated policy and the initialized imitating policy increase. (a) Online training curve. (b) Final performance of BC.
Figure 2: (a) Online training curves of an SAC agent trained on the Hopper environment, where the crosses and dashed lines indicate the stage of selected policies. (b) Final performances achieved by imitating the demo policy using BC, initialized with different stages of policies. The curves depict the fact that close-to-demonstration policy can easily imitate the demonstrated policy with fewer samples. (c) Empirical estimation on the discrepancy between the initialized policy and the trained policy outside the support of the demonstrations. Initialized with a closer-to-demo policy always enjoys more minor discrepancy. (c) Empirical discrepancy. 3.1 BC with Different Initialization
To construct the motivating example, we choose Hopper as the testbed, and train an SAC agent until convergence to sample various counts of trajectories as the demonstration data. We then take the online-trained policy checkpoints at different training iterations as the initiated policy to train an IL agent. Particularly, the ﬁrst agent adopts the Random policy to imitate the demonstration by BC; the second uses the policy of 1/3 Return and the other two agents start with 1/3 Trained and 2/3
Trained policy separately, in terms of training iterations (see Fig. 2a).
The results are shown in Fig. 2b, where we illustrate the average return of each agent given the different number of demonstrations (the exact quantitative results can be found in Appendix E.1).
The results show that initialized with a Random policy, the agent can only learn to imitate the demonstrated policy well with a large number of samples2; in addition, both 1/3 Return and 1/3 Trained policies can achieve a sub-optimal performance with fewer samples, where the 1/3 2We note that the Random agent can only work well on large datasets with normalized state space; however the other agents learn well upon raw states. 3
Trained one is more efﬁcient. In comparison, the 2/3 Trained agent, which is the closest to the optimal policy, can stably achieve the best performance with all amounts of trajectories. 3.2 Theoretical Analysis
Beyond these observations, we are inquisitive to ﬁnd a theoretical explanation to support our claim.
Standing on the primary result of existing works, we obtain a performance bound of BC with a possible solution to handle the quantity-quality dilemma.
Theorem 1 (Performance bound of BC). Let Π be the set of all deterministic policy and |Π| = |A||S|.
Assume that there does not exist a policy π ∈ Π such that π(si) = ai, ∀i ∈ {1, · · · , |D|}. Let ˆπb be the empirical behavior policy as well as the corresponding state marginal occupancy is ρˆπb . Suppose
BC begins from initial policy π0, and deﬁne ρπ0 similarly. Then, for any δ > 0, with probability at least 1 − δ, the following inequality holds: where c(π0, πb, |D|) =
DTV(ρπ(s, a)(cid:107)ρπb (s, a)) ≤ c(π0, πb, |D|) 1 (cid:88) 2
|ρπ(s) − ρπ0 (s)| +
ρπb (s) + (cid:88) 1 2 s /∈D s /∈D (cid:88) s /∈D 1 2 (cid:124)
|ρπ0 (s) − ρπb (s)| (cid:123)(cid:122) initialization gap (cid:125)
+ (cid:88) s∈D 1 2 (cid:124)
|ρπ(s) − ρˆπb (s)| + 1
|D|
|D| (cid:88) i=1 (cid:123)(cid:122)
BC gap
π(si) (cid:54)= ai(cid:105) (cid:104)
I
+ (cid:20) log |S| + log(2/δ) 2|D| (cid:21) 1 2
+ (cid:20) log |Π| + log(2/δ) 2|D| (cid:21) 1 2 (cid:124) (cid:125) (cid:123)(cid:122) data gap (cid:125) (2)
The proof can be found in Appendix B.1. Theorem 1 shows the upper bound of the state-action distribution between the imitating policy π and the behavior policy πb, which consists of three important terms: the initialization gap, the BC gap and the data gap. Speciﬁcally, the BC gap arises from the empirical error and the difference between the imitating policy and the empirical behavior policy, which is corresponding to the training procedure. The data gap, however, depends on the number of samples and complexity of the state space, acting as an intrinsic gap due to the dataset and the environment. As for the initialization gap, it is in the form of distance between the state marginal distribution of the initial policy π0 and behavior policy πb out of the dataset. Notice that the second term in Eq. (2) relates to the distance between the state marginal of the initial policy π0 and the learned policy πb outside the data support, which is hard to measure theoretically due to the
Markov property of the environment dynamics. Therefore, we estimate the empirical discrepancy outside the dataset 1 s /∈D |ˆρπ(s) − ˆρπ0 (s)| for this term3. The results shown in Fig. 2c, as expected, 2 suggests that the second term in Eq. (2) in fact decreases as the initialized policy gets close to the demonstrated policy because of the poor generalization on unseen states, and the error can be further reduced with a larger dataset. (cid:80)
Such analysis brings a possible theoretical explanation to our empirical intuition Section 3.1. Gen-erally, given the same discrepancy c(π0, πb, |D|) = C, if the initialized policy narrows down the initialization gap as is close to the demonstrated policy, then the requirement for more samples to minimize the data gap can be relaxed. This may seem unreasonable in the learning theory in the traditional supervised learning domain. However, under the RL scenario, the performance of a policy depends on the accumulated reward along the rollout trajectories, which will lead to serious compounding error problems [21, 20]. Therefore, as the distance between the initialized policy and the demonstrated policy gets closer, the generalization errors of the learned policy can be reduced.
Brief conclusion. Both the experimental and the theoretical results indicate an interesting fact that the asymptotic performance of BC is highly related to the discrepancy between the initialized policy and the demonstrated policy. Speciﬁcally, a close-to-demonstration policy can easily imitate the demonstrated policy with fewer samples. On the contrary, when the distance between the initialized policy and the demonstrated policy is far, then successfully mimicking the policy will require much more samples. Such an observation motivates the intuition for proposing our Curriculum Ofﬂine
Imitation Learning (COIL) in the following literature. The key insight enabling COIL is adaptively imitating the close policies with a small number of samples and ﬁnally terminates with the optimal behavior policy of the dataset. 3For implementation details, see Appendix C.1. 4
(a) Online off-policy training. (b) Curriculum ofﬂine imitation learning.
Figure 3: Comparison between online off-policy training and curriculum ofﬂine imitation learning. 4 Curriculum Ofﬂine Imitation Learning 4.1 Online RL as Imitating Optimal Policies
Before starting to formulate our methodology, we ﬁrst introduce the formulation of the online RL training. Typically, if we treat an optimization step of the policy as a training stage, an RL online learning algorithm can be realized as on-policy / off-policy depending on whether the agent is trained using the data collected by policies in the previous training stage. Taking off-policy RL as an example: beginning with a randomized policy π0, at every training stage i, the agent uses its policy πi to interact with the environment to collect trajectory τ i and save it to replay buffer B. The agent then samples several state-action pairs from B and take an optimization step to get policy πi+1, towards obtaining the most accumulated rewards: maximize
π
Eτ ∼π[R(τ )] . (3)
Under the principle of maximum entropy, we can model the distribution of trajectories sampled by the optimal policy as a Boltzmann distribution [27, 4] as:
P ∗(τ ) ∝ exp (R(τ )) (4)
With such a model, trajectories with higher rewards are exponentially more preferred. And ﬁnding the optimal policy through RL is equivalent to imitating the optimal policy modeled by Eq. (4) [4, 3]: minimize
π
DKL(Pπ(τ )(cid:107)P ∗(τ )) , (5) where Pπ(τ ) = ρ(s0) (cid:80)T t=0 T (st+1|st, at)π(at|st) is the distribution of generating a trajectory τ according to policy π. Thus, πi+1 is updated follows the direction of minimizing the KL divergence:
πi+1 = πi − ∇πDKL(Pπ(τ )(cid:107)P ∗(τ )) (6) 4.2 Ofﬂine RL as Adaptive Imitation
Compared with online policy training, the ofﬂine agent can only have a pre-collected dataset for policy training. Such dataset could be generated by a single policy, or collected by kinds of policies.
In analogy to online RL, a similar solution on ofﬂine tasks can be imitating the optimal behavior policy from the dataset in an ofﬂine way. However, as we show before, ofﬂine IL methods, like
BC, are only capable of matching the performance of the behavior policy, but hard to reach a good performance on the mixed dataset due to the quantity-quality dilemma. 4.2.1 Leverage Behavior Cloning with Curriculum Learning
In Section 3 we have seen evidence that a possible solution to the quantity-quality dilemma could be adaptive imitation through the ofﬂine dataset. An overview of such an adaptive imitation learning diagram compared with online RL is shown in Fig. 3. Formally, with dataset D = {τ }N 1 , at every training stage i, the agent updates its policy πi by adaptively selecting τ ∼ ˜πi from D as the imitating target such that:
πi+1 = πi − ∇πDKL(P˜πi(τ )(cid:107)Pπ(τ )) (cid:2)DKL(˜πi(·|s)(cid:107)πi(·|s))(cid:3) ≤ (cid:15)
˜π − Ri s.t. E˜π
Ri (7)
π ≥ δ where (cid:15) and δ are small positive numbers that limit the difference between the demonstrated policy ˜πi and the learner π, and prevent π from learning poorly behaved policies. Correspondingly, each training 5
iteration i creates a curriculum automatically such that a task Gi is to imitating the closet demonstrated policy ˜πi with Lπ i = DKL(P˜πi(τ )(cid:107)Pπ(τ )), and the target task Gt is to imitate the optimal policy
˜π∗. Speciﬁcally, we aims to construct a ﬁnite sequence π0, ˜π1, π1, ˜π2, π2, · · · , ˜πN , πN such that
πi (cid:22) πi+1, where ˜πi is characterized by its trajectory, and is picked from D based on the current policy πi−1; πi is the imitation result taken ˜πi as the target policy.
In the following sections, we explain how our algorithm is designed to achieve Eq. (7) that leads the target policy ˜π to ﬁnally collapse into the optimal behavior policy ˜π∗, while solving the quality-quantity dilemma of BC to avoid getting a mediocre result. 4.2.2 Adaptive Experience Picking by Neighboring Policy Assessment
We ﬁrst provide a practical solution to evaluate whether E˜π [DKL(˜π(·|s)(cid:107)π(·|s))] ≤ (cid:15). In other words, we want to know whether a trajectory τi ∈ D is sampled by a neighboring policy. This can be regarded as ﬁnding a policy whose importance sampling ratio is near to 1, and thus a lot of density ratio estimation works can be referred to [16, 26]. However, such estimation requires extra costs on training neural networks, and the estimation is inaccurate with fewer data points. Therefore, in this paper, we design a simple yet efﬁcient neighboring policy assessment principle instead that brings the algorithm into practice.
We assume that each trajectory is sampled by a single policy. Beyond such a slight and practical as-sumption, let π be the current policy and trajectory τ˜π = {(s0, a0, s(cid:48) h, rh)} is collected by an unknown deterministic behavior policy ˜π with exploration noise such that
E(s,a)∈τ˜π [log ˜π(at|st)] ≥ log (1 − β), where β denote the portion of exploration.
In this way, we ﬁnd a practical solution that relaxes the KL-divergence constraint through an observation:
Observation 1. Under the assumption that each trajectory τ˜π in the dataset D is collected by an unknown deterministic behavior policy ˜π with an exploration ratio β. The requirement of the KL divergence constraint E˜π [DKL(˜π(·|s)(cid:107)π(·|s))] ≤ (cid:15) sufﬁces to ﬁnding a trajectory that at least 1 − β state-action pairs are sampled by the current policy π with a probability of more than (cid:15)c such that (cid:15)c ≥ 1/ exp (cid:15), i.e.: 0, r0), · · · , (sh, ah, s(cid:48)
E(s,a)∈τ˜π [I(π(a|s) ≥ (cid:15)c)] ≥ 1 − β , (8)
The corresponding deviation is shown in Appendix B.2. Therefore, to ﬁnd whether τ˜π is sampled by a neighboring policy, we calculate the probability of sampling at at state st by π in τ for every timestep τ˜π(π) = {π(a0|s0), · · · , π(ah|sh)}, where h is the horizon of the trajectory. In practice, instead of ﬁne-tuning (cid:15) and β, we heuristically set β = 0.05 as an intuitive ratio of exploration. As for (cid:15)c, we let the agent choose the value through ﬁnding N nearest policies that matches Eq. (8). 4.2.3 Return Filter
We now present how to ensure the second constraint R˜π − Rπ ≥ δ, which is designed to refrain the performance from getting worse by imitating to a poorer target than the current level of the imitating policy. In a practical ofﬂine scenario, it is impossible to get the accurate return of the current policy, but we can evaluate its performance based on the current curriculum. To this end, we adopt a return
ﬁltering mechanism that ﬁltrates the useless, poor-behaved trajectories.
In practice, we initialize the return ﬁlter V with 0, and update the value at each curriculum. Speciﬁ-cally, if we choose {τ }n 1 from D at iteration k, then V is updated by moving average:
Vk = (1 − α) · Vk−1 + α · min{R(τ )}n 1 (9) where {R(τ )}n determining the ﬁltering rate. Then, the dataset is updated as D = {τ ∈ D | R(τ ) ≥ V }. 1 is the accumulated reward set of trajectories {τ }n 1 , and α is the moving window 4.2.4 Overall Algorithm
Combining the adaptive experience picking strategy and the return ﬁlter, we ﬁnally get the simple and practical curriculum ofﬂine imitation learning (COIL) algorithm. To be speciﬁc, COIL holds an experience pool that contains the candidate trajectories to be selected. Every training time creates a stage of the curriculum where the agent selects appropriate trajectories as the imitation target from the pool and learns them via direct BC. After training, the used experience will be cleaned from the pool, and the return ﬁlter also ﬁltrates a set of trajectories. An attractive property of COIL is that it 6
has a terminating condition that stops the algorithm automatically with a good policy when there is no candidate trajectory to be selected. This makes it easier to be applied in real-world applications without further ﬁnding the best-learned policy checkpoints under online evaluation as the previous algorithms do. The step-by-step algorithm is shown in Algo. 1.
It is worth noting that COIL has only two critical hyperparameters, namely, the number of selected trajectories N and the moving window of the return ﬁlter α, both of which can be determined by the property of the dataset. Speciﬁcally, N is related to the average discrepancy between the sampling policies in the dataset; α is inﬂuenced by the changes of the return of the trajectories contained in the dataset. In the ablation study Section 6.3 and Appendix E.2, we demonstrate how we select different hyperparameters for different datasets. 5