Abstract
Adversarial examples for neural network image classiﬁers are known to be trans-ferable: examples optimized to be misclassiﬁed by a source classiﬁer are often misclassiﬁed as well by classiﬁers with different architectures. However, targeted adversarial examples—optimized to be classiﬁed as a chosen target class—tend to be less transferable between architectures. While prior research on constructing transferable targeted attacks has focused on improving the optimization procedure, in this work we examine the role of the source classiﬁer. Here, we show that train-ing the source classiﬁer to be “slightly robust”—that is, robust to small-magnitude adversarial examples—substantially improves the transferability of class-targeted and representation-targeted adversarial attacks, even between architectures as dif-ferent as convolutional neural networks and transformers. The results we present provide insight into the nature of adversarial examples as well as the mechanisms underlying so-called “robust” classiﬁers. 1

Introduction
Neural-network image classiﬁers are well-known to be susceptible to adversarial examples—images that are perturbed in a way that is largely imperceptable to humans but that cause the neural network to make misclassiﬁcations. Much research has gone into methods for constructing adverarial examples in order to understand the nature of neural-network vulnerabilities and to develop methods to make neural-network classiﬁers more robust to attacks [7, 11, 21, 45, 50, 55, 73].
Untargeted adversarial examples are designed to elicit an unspeciﬁed incorrect class, while targeted adversarial examples are designed to elicit a speciﬁc (incorrect) target class. A given adversarial perturbation is designed via optimization with respect to a given trained network; here we call this the source network. While untargeted adversarial examples are often transferable—examples designed to attack a source network also successfully attack other trained networks with different parameters and architectures [21, 73]—targeted adversarial examples tend to be less transferable [44].
Prior research on constructing transferable adversarial examples for image classiﬁers has focused primarily on improving optimization methods for generating successful image perturbations. In this paper, we take a different tack—focusing on the source neural network used in constructing adversarial examples. Speciﬁcally, we ﬁnd that “slightly-robust” convolutional neural networks (CNNs)—ones that have been trained to be robust to small adversarial perturbations—can be leveraged to substantially improve the transferability of targeted adversarial examples to different architectures. Surprisingly, we show that targeted adversarial examples constructed with respect to a slightly robust CNN transfer successfully not only to different CNN architectures but also to transformer architectures such as ViT
[16], LeViT [22], CCT [24], and even CLIP [57], which is trained on different objective than the source CNN. Such transfers are all “black box” attacks—while generating an adversarial example 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
requires knowledge of the source network’s architecture, no such knowledge is required for the black-box architectures that can also be attacked by the same example.
The vulnerability of networks to both targeted and untargeted attacks has huge signiﬁcance for security purposes, and thus understanding how to construct highly effective attacks can help motivate defenses. However, we believe that targeted attacks are especially important for understanding neural-network classiﬁers, as they provide a tool to compare the features of two models. When a targeted attack transfers from one network to another, it suggests that the two networks rely on similar information for classiﬁcation, and that they use the information in the same way. In our work, we show that each individual slightly-robust neural network transfers features effectively to all tested non-robust networks, suggesting the surprising result that slightly-robust networks rely on features that overlap with every non-robust network, even though it is not the case that any particular non-robust network has features that substantially overlap with all other non-robust networks.
In addition, we leverage the techniques of adversarial transferability to examine which features are learned by neural networks. In accordance with prior work [71], we ﬁnd that, on the spectrum from non-robust (standard) to highly robust classiﬁers, those that are only slightly robust exhibit the most transferable representation-targeted adversarial examples, suggesting that the features of slightly-robust networks overlap substantially with every tested desination network. This can explain why slightly robust networks give rise to more transferable adversarial attacks and have better weight initializations for downstream transfer-learning tasks [43, 62, 76, 80].
The main contributions of this paper are the following: 1. We demonstrate that adversarial examples generated with respect to slightly robust CNNs are more transferable than those generated with respect to standard (non-robust) networks, This transferability extends not only to other CNNs, but also to transformer architectures. 2. We ﬁnd that, as the robustness of the source network increases, there is also a substantial increase in transferability of targeted adversarial examples to adversarially-defended networks. 3. We examine the role of the adversarial loss function in generating transferable adversarial exam-ples. 4. We show, surprisingly, that non-robust neural networks do not exhibit substantial feature (rep-resentation) transferability, while slightly-robust neural networks do. This helps explain why slightly-robust neural networks enable superior transferability of targeted adversarial examples. 2