Abstract
While reinforcement learning algorithms provide automated acquisition of optimal policies, practical application of such methods requires a number of design deci-sions, such as manually designing reward functions that not only deﬁne the task, but also provide sufﬁcient shaping to accomplish it. In this paper, we view rein-forcement learning as inferring policies that achieve desired outcomes, rather than as a problem of maximizing rewards. To solve this inference problem, we establish a novel variational inference formulation that allows us to derive a well-shaped reward function which can be learned directly from environment interactions. From the corresponding variational objective, we also derive a new probabilistic Bellman backup operator and use it to develop an off-policy algorithm to solve goal-directed tasks. We empirically demonstrate that this method eliminates the need to hand-craft reward functions for a suite of diverse manipulation and locomotion tasks and leads to effective goal-directed behaviors. 1

Introduction
Reinforcement learning (RL) provides an appealing formalism for automated learning of behavioral skills, but requires considerable care and manual design to use in practice. One particularly delicate decision is the design of the reward function, which has a signiﬁcant impact on the resulting policy but is largely heuristic in practice, often lacks theoretical grounding, can make effective learning difﬁcult, and may lead to reward mis-speciﬁcation.
To avoid these shortcomings, we propose to circumvent the process of manually specifying a reward function altogether: Instead of framing the reinforcement learning problem as ﬁnding a policy that maximizes a heuristically-deﬁned reward function, we express it probabilistically, as inferring a state–action trajectory distribution conditioned on a desired future outcome. By building off of prior work on probabilistic perspectives on RL [10, 23, 35, 46, 47, 58] and goal-directed RL in particular [3, 6, 19, 48], we derive a tractable variational objective, an temporal-difference algorithm that provides a shaping-like effect for effective learning, as well as a reward function that captures the semantics of the underlying decision problem and facilitates effective learning.
We demonstrate that unlike prior works that proposed inference methods for ﬁnding policies that achieve desired outcomes [3, 11, 19, 48], the resulting algorithm, Outcome-Driven Actor–Critic (ODAC), is amenable to off-policy learning and applicable to complex, high-dimensional continuous control tasks over ﬁnite and inﬁnite horizons. The resulting variational algorithm can be interpreted as an automatic shaping method, where each iteration learns a reward function that automatically pro-vides dense rewards, as we visualize in Figure 1. In tabular settings, ODAC is guaranteed to converge
∗Equal contribution. † Corresponding authors: tim.rudner@cs.ox.ac.uk and vitchyr@berkeley.edu. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of the shaping effect of the reward function derived from the goal-directed variational inference objective. Left: A 2-dimensional grid world with a desired outcome marked by a star. Center-left
The corresponding sparse reward function provides little shaping. Center-right: The reward function derived from our variational inference formulation at initialization. Right: The derived reward function after training.
We see that the derived reward learns to provide a dense reward signal everywhere in the state space. to an optimal policy, and in non-tabular settings with linear Gaussian transition dynamics, the derived optimization objective is convex in the policy, facilitating easier learning. In high-dimensional and non-linear domains, our method can be combined with deep neural network function approximators to yield a deep reinforcement learning method that does not require manual speciﬁcation of rewards, and leads to good performance on a range of benchmark tasks.
Contributions. The core contributions of this paper are the probabilistic formulation of a general framework for inferring policies that lead to desired outcomes and the derivation of a variational objective from which we obtain a novel outcome-driven Bellman backup operator. We show that this
Bellman backup operator induces a shaping-like effect which ensures a clear and dense learning signal even in the early stages of training. Crucially, unlike heuristic approaches for incorporating shaping often used in standard RL, this “shaping” emerges automatically from variational inference. We demonstrate that the resulting variational objective is a lower bound on the log-marginal likelihood of achieving the outcome given an initial state and that it leads to an off-policy temporal-difference algorithm. We evaluate this algorithm—Outcome-Driven Variational Inference (ODAC)—on a range of reinforcement learning tasks without having to manually specify task-speciﬁc reward functions. In our experiments, we ﬁnd that our method results in signiﬁcantly faster learning across a variety of robot manipulation and locomotion tasks than alternative approaches. 2 Preliminaries
Standard reinforcement learning (RL) addresses reward maximization in a Markov decision pro-cess (MDP) deﬁned by the tuple (S, A, pS0, pd, r, γ) [43, 44], where S and A denote the state and action space, respectively, p0 denotes the initial state distribution, pd is a state transition dis-tribution, r is an immediate reward function, and γ is a discount factor. To sample trajectories, an initial state is sampled according to pS0 , and successive states are sampled from the state tran-sition distribution St+1 ∼ pd(· | st, at) and actions from a policy At ∼ π(· | st). We will write
T 0:t = {S0, A0, S1, . . . , St, At} to represent a ﬁnite-horizon and T 0 ˙= {At, St+1}∞ t=0 to repre-sent an inﬁnite-horizon stochastic state–action trajectory, and write τ0:t = {s0, a0, s1, . . . , st, at} and t=0 for the respective trajectory realizations. Given a reward function r : S ×A → R
τ0 ˙= {at, st+1}∞ and discount factor γ ∈ (0, 1), the objective in reinforcement learning is to ﬁnd a policy π that max-imizes the returns, deﬁned as Epπ [(cid:80)∞ t=0 γtr(st, at)] , where pπ denotes the distribution of states induced by a policy π.
Goal-Conditioned Reinforcement Learning.
In goal-conditioned reinforcement learning [22], which can be considered a special case of the broader class of stochastic boundary value problems [1, 14], the objective is for an agent to reach some pre-speciﬁed goal state, g ∈ S, so that the policy and reward function introduced above become dependent on the goal and are expressed as π(a | s, g) and r(s, a, g), respectively. Typically, such a reward function needs to be deﬁned manually, with a common choice being to use a sparse indicator reward r(s, g) = I{s = g}. However, this approach presents a number of challenges both in theory and in practice. From a theoretical perspective, the indicator reward will almost surely equal zero for environments with continuous goal spaces and non-trivial stochastic dynamics. From a practical perspective, such sparse rewards can be slow to learn from, as most transitions provide no reward supervision, while manually designing dense reward functions that provide a better learning signal is time-consuming and often based on heuristics. 2
In Section 3, we will present a framework that addresses these practical and theoretical considerations by casting goal-conditioned RL as probabilistic inference.
Q-Learning. Off-policy Q-learning algorithms [52] allow training policies from data collected under alternate decision rules by estimating the expected return Qπ for a given state–action pair:
Qπ(s, a) ˙= Epπ (cid:104) (cid:88)∞ t=0
γtr(st, at) (cid:12) (cid:105) (cid:12) (cid:12)S0 = s, A0 = a
.
Crucially, the expected return given a state–action pair can be expressed recursively as
Qπ(s, a) = r(s, a) + γ Epπ [Qπ(s1, a1) | S0 = s, A0 = a], (1) which makes it possible to estimate the expectation on the right-hand side from single-step transitions.
The resulting estimates can then be used to ﬁnd a policy that results in actions which maximize the expected return Qπ(s, a) for all available state–action pairs. 3 Outcome-Driven Reinforcement Learning
In this section, we derive a variational inference objective to infer an approximate posterior policy for achieving desired outcomes. Instead of using the heuristic goal-reaching rewards discussed in Section 2, we will derive a general framework for inferring actions that lead to desired outcomes by formulating a probabilistic objective, using the tools of variational inference. As we will show in the following sections, we use this formulation to translate the problem of inferring a policy that leads to a desired outcome into a tractable variational optimization problem, which we show corresponds to an RL problem with a well-shaped, dense reward signal from which the agent can learn more easily.
We start with a warm-up problem that demonstrates how to frame the task of achieving a desired outcome as an inference problem in a simpliﬁed setting. We then describe how to extend this approach to more general settings. Finally, we show that the resulting variational objective can be expressed as a recurrence relation, which allows us to derive an outcome-driven variational Bellman operator and prove an outcome-driven probabilistic policy iteration theorem. 3.1 Warm-up: Achieving a Desired Outcome at a Fixed Time Step
S1
S0
. . .
St(cid:63)
St(cid:63)−1
We ﬁrst consider a simpliﬁed problem, where the desired outcome is to reach a speciﬁc state g ∈ S at a speciﬁc time step t(cid:63) when starting from initial state s0.
We can think of the starting state s0 and the desired outcome g as boundary condi-tions, and the goal is to learn a stochastic policy that induces a trajectory from s0 to g. To derive a control law that solves this stochastic boundary value problem, we frame the problem probabilistically, as inferring a state–action trajectory posterior distribution conditioned on the desired outcome and the initial state. We will show that, by framing the learning problem this way, we obtain an algorithm for learning outcome-driven policies without needing to manually specify a reward function. We consider a model of the state–action trajectory up to and including the desired outcome g,
Figure 2: A Probabilistic graphical model of a state–action trajectory with observed random variables S0 = s0 and St(cid:63)= g.
At(cid:63)−1
A0
A1
. . . p ˜T 0:t,St(cid:63) ( ˜τ0:t, g | s0) ˙= pd(g | st, at)p(at | st) t−1 (cid:89) t(cid:48)=0 pd(st(cid:48)+1 | st(cid:48), at(cid:48))p(at(cid:48) | st(cid:48)), where t(cid:63) ˙= t+1, ˜T 0:t is the state–action trajectory up to an including t but excluding S0, p(at | st) is a conditional action prior, and pd(st+1 | st, at) is the environment’s state transition distribution. If the dynamics are simple (e.g., tabular or Gaussian), the posterior over actions can be computed in closed form [3], but we would like to be able to infer outcome-driven posterior policies in any environments, including those where exact inference may be intractable. To do so, we start by expressing posterior inference as the variational minimization problem min q ˜T 0:t|S0
∈ ˆQ
DKL(q ˜T 0:t|S0 (· | s0) (cid:107) p ˜T 0:t|S0,St(cid:63) (· | s0, g)), (2) 3
where DKL(· (cid:107) ·) is the KL divergence, and ˆQ denotes the variational family over which to optimize.
We consider a family of distributions parameterized by a policy π and deﬁned by q ˜T 0:t|S0 ( ˜τ0:t | s0) ˙= π(at | st) t−1 (cid:89) pd(st(cid:48)+1 | st(cid:48), at(cid:48))π(at(cid:48) | st(cid:48)), (3) t(cid:48)=0 where π ∈ Π, a family of policy distributions, and where (cid:81)t−1 t=0 pd(st+1 | st, at) is the true action-conditional state transition distribution up to but excluding the state transition at t(cid:63) − 1, since St(cid:63) = g is observed. Under this variational family, the inference problem in Equation (2) can be equivalently stated as the problem of maximizing the following objective with respect to the policy π:
Proposition 1. Given q ˜T 0:t|S0 outcome g, solving Equation (2) is equivalent to maximizing this objective with respect to π ∈ Π: ( ˜τ0:t | s0) from Equation (3), any state s0, termination time t(cid:63), and
¯F(π, s0, g) ˙= Eq ˜T 0:t|S0 (cid:20) ( ˜τ0:t | s0) log pd(g | st, at) −
DKL(π(· | st(cid:48)) || p(· | st(cid:48))) (cid:21)
. (4) t−1 (cid:88) t(cid:48)=0
Proof. See Appendix A.1.
A variational problem of this form—which corresponds to ﬁnding a posterior distribution over state–action trajectories—can equivalently be viewed as a reinforcement learning problem:
Corollary 1. The objective in Equation (4) corresponds to KL-regularized reinforcement learning with a time-varying reward function given by r(st(cid:48), at(cid:48), g, t(cid:48)) ˙= I{t(cid:48) = t} log pd(g | st(cid:48), at(cid:48)).
Corollary 1 illustrates how a reward function emerges automatically from a probabilistic framing of outcome-driven reinforcement learning problems where the sole speciﬁcation is which variable (St(cid:63) ) should attain which value (g). In particular, Corollary 1 suggests that we ought to learn the environment’s state-transition distribution, and view the log-likelihood of achieving the desired outcome given a state–action pair as a “reward” that can be used for off-policy learning as described in Section 2. Importantly—and unlike in model-based RL—such a transition model would not have to be accurate beyond single-step predictions, as it would not be used for planning (see Appendix B).
Instead, log pd(g | st, at) only needs to be well shaped, which we expect to happen for commonly used model classes. For example, when the dynamics are linear-Gaussian, using a conditional
Gaussian model [29] yields a reward function that is quadratic in St+1, making the objective convex and thus more amenable to optimization. 3.2 Outcome-Driven Reinforcement Learning as Variational Inference
Thus far, we assumed that the time at which the outcome is achieved is given. In many problem settings, we do not know (or care) when an outcome is achieved. In this section, we present a variational inference perspective on achieving desired outcomes in settings where no reward function and no termination time are given, but only a desired outcome is provided. As in the previous section, we derive a variational objective and show that a principled algorithm and reward function emerge automatically when framing the problem as variational inference.
To derive such an objective, we modify the probabilistic model used in the previous section to model that the time at which the outcome is achieved is not known. As before, we deﬁne an “outcome” as a point in the state space, but instead of deﬁning the event of “achieving a desired outcome” as a realization St(cid:63) = g for a known t(cid:63), we deﬁne it as a realization ST (cid:63) = g for an unknown termination time T (cid:63) at which the outcome is achieved. Speciﬁcally, we model the distribution over the trajectory and the unknown termination time with t−1 (cid:89) p ˜T 0:T ,ST (cid:63) ,T |S0 ( ˜τ0:t, g, t | s0) = pT (t)pd(g | st, at)p(at | st) pd(st(cid:48)+1 | st(cid:48), at(cid:48))p(at(cid:48) | st(cid:48)), (5) where pT (t) is the probability of reaching the outcome at t + 1. Since the trajectory length is itself a random variable, the joint distribution in Equation (5) is a transdimensional distribution deﬁned on (cid:85)∞ t=0{t} × S t+1 × At+1 [19].
Unlike in the warm-up, the problem of ﬁnding an outcome-driven policy that eventually achieves the desired outcome corresponds to ﬁnding the posterior distribution over state–action trajectories and t(cid:48)=0 4
the termination time T conditioned on the desired outcome ST (cid:63) and a starting state. Analogously to Section 3.1, we can express this inference problem variationally as min q ˜T 0:T ,T |S0
∈Q
DKL(q ˜T 0:T ,T |S0 (· | s0) (cid:107) p ˜T 0:T ,T |S0,ST (cid:63) (· | s0, g)), (6) where t denotes the time immediately before the outcome is achieved, Q denotes the variational family. In general, solving this variational problem in closed form is challenging, but by choosing ( ˜τ0:t | t, s0)qT (t), where qT is a distribution ( ˜τ0:t, t | s0) = q ˜T 0:T |T,S0 a variational family q ˜T 0:T ,T |S0 over T in some variational family QT parameterized by t (cid:89) qT (t) = q∆t+1(∆t+1 = 1) q∆t(cid:48) (∆t(cid:48) = 0), (7) t(cid:48)=1 with Bernoulli random variables ∆t denoting the event of “reaching g at time t given that g has not yet been reached by time t − 1,” we can equivalently express the variational problem in Equation (6) in a way that is tractable and amenable to off-policy optimization:
Theorem 1. Let qT (t) and q ˜T 0:T |T,S0 ( ˜τ0:t | t, s0) be as deﬁned before, and deﬁne
V π(st, g; qT ) ˙= Eπ(at | st) [Qπ(st, at, g; qT )] − DKL(π(· | st) (cid:107) p(· | st))
Qπ(st, at, g; qT ) ˙= r(st, at, g; q∆) + q∆t+1(∆t+1 = 0) Epd(st+1 | st,at) [V π(st+1, g; π, qT )] r(st, at, g; q∆) ˙= q∆t+1(∆t+1 = 1) log pd(g | st, at) − DKL(q∆t+1 (cid:107) p∆t+1). (8) (9) (10)
Then given any initial state s0 and outcome g,
DKL(q ˜T 0:T ,T |S0 (· | s0) (cid:107) p ˜T 0:T ,T |S0,ST ∗ (· | s0, g)) = −V π(s0, g; qT ) + log p(g | s0), where log p(g | s0) is independent of π and qT and hence maximizing V π(s0, g; π, qT ) is equivalent to minimizing Equation (6).
Proof. See Appendix A.2.
This theorem tells us that the maximizer of V π(st, g; qT ) is equal to the minimizer of Equation (6).
In other words, Theorem 1 presents a variational objective with dense reward functions deﬁned solely in terms of the desired outcome and the environment dynamics, which we can learn directly from environment interactions. It further makes precise that the variational objective, V π(s0, g; qT ), is a lower bound on the log-marginal likelihood, that is, log p(g | s0) ≥ V π(s0, g; qT ), where
V π(s0, g; qT ) = E (cid:34) ∞ (cid:88) (cid:32) t (cid:89) t=0 t(cid:48)=1 (cid:33) (cid:16) q∆t(cid:48) (∆t(cid:48) = 0) (cid:17) r(st, at, g; q∆) − DKL(π(· | st) (cid:107) p(· | st)) (cid:35)
, ( ˜τ0 | s0). with the expectation taken with respect to the inﬁnite-horizon trajectory distribution q ˜T 0|S0
Thanks to the recursive expression of the variational objective, we can ﬁnd the optimal variational over T as a function of the current policy and Q-function analytically:
Proposition 2. The optimal distribution q(cid:63)
T with respect to Equation (8) is q(cid:63)
∆t+1 (∆t+1 = 0; π, Qπ) = σ (cid:0)Λ(st, π, qT , Qπ) + σ−1 (cid:0)p∆t+1 (∆t+1 = 0)(cid:1)(cid:1), (11) where
Λ(st, π, qT , Qπ) ˙= Eπ(at+1 | st+1)pd(st+1 | st,at)π(at | st)[Qπ(st+1, at+1, g; qT ) − log pd(g | st, at)] and σ(·) is the sigmoid function, that is, σ(x) = 1
Proof. See Appendix A.3 e−x+1 and σ−1(x) = log x 1−x .
Alternatively, if instead of learning qT variationally, we ﬁx qT to the prior pT , we recover the more conventional ﬁxed-discount factor objective [13, 16, 39]:
Corollary 2. Let qT = pT , assume that pT is a Geometric distribution with parameter γ ∈ (0, 1).
Then the inference problem in Equation (6) of ﬁnding a goal-directed variational trajectory distribu-tion simpliﬁes to maximizing the following recursively deﬁned variational objective with respect to
π: where
¯V π(s0, g; γ) ˙= Eπ(a0 | s0) [Q(s0, a0, g; γ)] − DKL(π(· | s0) (cid:107) p(· | s0))),
¯Qπ(s0, a0, g; γ) ˙= (1 − γ) log pd(g | s0, a0) + γ Epd(s1|s0,a0) (cid:2)V(s1, g; γ)(cid:3). (12) (13)
In the next section, we derive a temporal-difference algorithm and discuss how we can learn the
Q-function in Theorem 1 using off-policy transitions. 5
4 Outcome-Driven Reinforcement Learning
In this section, we show that the variational objective in Theorem 1 is amenable to off-policy learning and that it can be estimated efﬁciently from single-step transitions. We then describe how to instantiate the resulting outcome-driven algorithm in large environments where function approximation is necessary. 4.1 Outcome-Driven Policy Iteration
To develop an outcome-directed off-policy algorithm, we deﬁne the following Bellman operator:
Deﬁnition 1. Given a function Q : S × A × S → R, deﬁne the operator T π as
T πQ(st, at, g; qT ) ˙= r(st, at, g; q∆) + q∆t+1(∆t+1 = 0) Epd(st+1 | st,at) where r(st, at, g; q∆) is from Theorem 1 and (cid:2)V(st+1, g; qT )(cid:3), (14)
V(st, g; qT ) ˙= Eπ(at | st) [Q(st, at, g; qT )] + DKL(π(· | st) (cid:107) p(· | st)). (15)
Unlike the standard Bellman operator, the above operator has a varying weight factor q∆t+1(∆t+1 = 0), with the optimal weight factor given by Equation (11). From Equation (11), we see that as the outcome likelihood pd(g | s, a) becomes large relative to the Q-function, the weight factor automatically adjusts the target to rely more on the rewards.
Below, we show that repeatedly applying the operator T π (policy evaluation) and optimizing π with respect to Qπ (policy improvement) converges to a policy that maximizes the objective in Theorem 1.
Theorem 2. Assume MDP is ergodic and |A| < ∞. 1. Outcome-Driven Policy Evaluation (ODPE): Given policy π and a function Q0 : S × A × S → R, deﬁne Qi+1 = T πQi. Then the sequence Qi converges to the lower bound in Theorem 1. 2. Outcome-Driven Policy Improvement (ODPI): The policy
π+ = arg max
π(cid:48)∈Π
{Eπ(cid:48)(at | st) [Qπ(st, at, g; qT )] − DKL(π(cid:48)(· | st) || p(· | st)} (16) and the variational distribution over T deﬁned in Equation (11) improve the variational objective, that is, F(π+, qT , s0) ≥ F(π, qT , s0) and F(π, q+
T , s0) ≥ F(π, qT , s0) for all s0, π, qT . 3. Alternating between ODPE and ODPI converges to a policy π(cid:63) and a variational distribution
T ) ≥ Qπ(s, a, g; qT ) for all (π, qT ) ∈ Π × QT and any (s, a, g; q(cid:63) over T , qT , such that Qπ(cid:63) (s, a) ∈ S × A.
Proof. See Appendix A.4.
This result tells us that alternating between applying the outcome-driven Bellman operator in Deﬁni-tion 1 and optimizing the bound in Theorem 1 using the resulting Q-function, which can equivalently be viewed as expectation maximization, will lead to a policy that induces an outcome-driven trajectory and solves the inference problem in Equation (6). As we discuss in Appendix A.4, this implies that
Variational Outcome-Driven Policy Iteration is theoretically at least as good as or better than standard policy iteration for KL-regularized objectives. 4.2 Outcome-Driven Actor–Critic (ODAC)
We now build on previous sections to develop a practical algorithm that handles large and continuous domains. In such domains, the expectation in the Bellman operator in Deﬁnition 1 is intractable, and so we approximate the policy πθ and Q-function Qφ with neural networks parameterized by parameters θ and φ, respectively. We train the Q-function to minimize
FQ(φ) = E (cid:20)(cid:16)
Qφ(s, a, g) − (r(s, a, g; q∆) + q∆t(∆t = 0) ˆV (s(cid:48), g)) (cid:17)2(cid:21)
, (17) where the expectation is taken with respect to (s, a, g, s(cid:48)) sampled from a replay buffer, D, of data collected by a policy. We approximate the ˆV -function using a target Q-function Q ¯φ: 6
Algorithm 1 ODAC: Outcome-Driven Actor–Critic 1: Initialize policy πθ, replay buffer R, Q-function Qφ, and dynamics model pψ. 2: for iteration i = 1, 2, ... do 3: 4: 5: 6: 7: end for
Collect on-policy samples to add to R by sampling g from environment and executing π.
Sample batch (s, a, s(cid:48), g) from R.
Compute approximate reward and optimal weights with Equation (20) and Equation (11).
Update Qφ with Equation (17), πθ with Equation (18), and pψ with Equation (19).
ˆV (s(cid:48), g) ≈ Q ¯φ(s(cid:48), a(cid:48), g) − log π(a(cid:48) | s(cid:48); g), where a(cid:48) are samples from the amortized variational policy πθ(· | s(cid:48); g). We further assume a uniform prior policy p(· | st) in all of our experiments.
The parameters ¯φ slowly track the parameters of φ at each time step via the standard update
¯φ ← τ ¯φ + (1 − τ )φ [27]. We then train the policy to maximize the approximate Q-function by performing gradient descent on
Fπ(θ) = − Es∼D,a∼πθ(· | s;g) [Qφ(s, a, g) − log πθ(a | s; g)] . (18)
We estimate ˆq∆t+1(∆t+1 = 0) with a Monte Carlo estimate of Equation (11) obtained via a single Monte Carlo sample (s, a, s(cid:48), a(cid:48), g) from the replay buffer.
In practice, a value of q∆t+1(∆t+1 = 0) = 1 can lead to numerical instabilities with bootstrapping, and so we also up-per bound the estimated q∆t+1(∆t+1 = 0) by the prior distribution p∆t+1(∆t+1 = 0).
To compute the rewards, we need to compute the likelihood of achieving the desired outcome. If the transition dynamics are unknown, we learn a dynamics model from environment interactions by training a neural network pψ that parameterizes the mean and scale of a factorized Laplace distribution. We train this model by maximizing the log-likelihood of the data collected by the policy,
Fp(ψ) = E(s,a,s(cid:48))∼D[log pψ(s(cid:48) | s, a)], (19) and use it to compute the rewards
ˆr(st, at, g; q∆) ˙= ˆq∆t+1(∆t+1 = 1) log pψ(g | st, at) − DKL(q∆t (cid:107) p∆t). (20)
The complete algorithm is presented in Algorithm 1 and consists of alternating between collecting data via policy π and minimizing Equations 17, 18, and 19 via gradient descent. This algorithm alternates between approximating the lower bound in Equation (8) by repeatedly applying the outcome-driven
Bellman operator to an approximate Q-function, and maximizing this lower bound by performing approximate policy optimization on Equation (18). 5