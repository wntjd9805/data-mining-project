Abstract
In many sequential decision-making problems (e.g., robotics control, game play-ing, sequential prediction), human or expert data is available containing useful information about the task. However, imitation learning (IL) from a small amount of expert data can be challenging in high-dimensional environments with com-plex dynamics. Behavioral cloning is a simple method that is widely used due to its simplicity of implementation and stable convergence but doesn’t utilize any information involving the environment’s dynamics. Many existing methods that exploit dynamics information are difﬁcult to train in practice due to an adversarial optimization process over reward and policy approximators or biased, high variance gradient estimators. We introduce a method for dynamics-aware IL which avoids adversarial training by learning a single Q-function, implicitly representing both reward and policy. On standard benchmarks, the implicitly learned rewards show a high positive correlation with the ground-truth rewards, illustrating our method can also be used for inverse reinforcement learning (IRL). Our method, Inverse soft-Q learning (IQ-Learn) obtains state-of-the-art results in ofﬂine and online imitation learning settings, signiﬁcantly outperforming existing methods both in the number of required environment interactions and scalability in high-dimensional spaces, often by more than 3x. 1

Introduction
Imitation of an expert has long been recognized as a powerful approach for sequential decision-making [29, 1], with applications as diverse as healthcare [39], autonomous driving [41], and playing complex strategic games [8]. In the imitation learning (IL) setting, we are given a set of expert trajectories, with the goal of learning a policy which induces behavior similar to the expert’s. The learner has no access to the reward, and no explicit knowledge of the dynamics.
The simple behavioural cloning [34] approach simply maximizes the probability of the expert’s actions under the learned policy, approaching the IL problem as a supervised learning problem.
While this can work well in simple environments and with large quantities of data, it ignores the sequential nature of the decision-making problem, and small errors can quickly compound when the learned policy departs from the states observed under the expert. A natural way of introducing the environment dynamics is by framing the IL problem as an Inverse RL (IRL) problem, aiming to learn a reward function under which the expert’s trajectory is optimal, and from which the learned imitation policy can be trained [1]. This framing has inspired several approaches which use rewards either explicitly or implicitly to incorporate dynamics while learning an imitation policy [17, 10, 33, 22].
However, these dynamics-aware methods are typically hard to put into practice due to unstable learning which can be sensitive to hyperparameter choice or minor implementation details [21].
In this work, we introduce a dynamics-aware imitation learning method which has stable, non-adversarial training, allowing us to achieve state-of-the-art performance on imitation learning bench-35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: A comparison of various algorithms for imitation learning. “Convergence Guarantees” refers to if a proof is given that the algorithm converges to the correct policy with sufﬁcient data.
We consider an algorithm “directly optimized” if it consists of an optimization algorithm (such as gradient descent) applied to the parameters of a single function
Method
Reference
Dynamics
Aware
Non-Adversarial
Training
Convergence
Guarantees
Non-restrictive
Reward
Direct
Optimization e n i l n
O i e n
ﬂ f
O
Max Margin IRL
Max Entropy IRL
GAIL/AIRL
ASAF
SQIL
Ours (Online)
Max Margin IRL
Max Likelihood IRL
Max Entropy IRL
ValueDICE
Behavioral Cloning
Regularized BC
EDM
Ours (Ofﬂine)
[29, 1]
[43]
[17, 10]
[4]
[33] –
[24, 20]
[18]
[16]
[22]
[34]
[30]
[19] – 3 3 3 3 3 3 3 3 3 3
⇥ 3 3 3 3 3
⇥ 3 3 3 3 3 3
⇥ 3 3 3 3 3 3 3 3
⇥ 3 3 3 3
⇥ 3 3
⇥ 3
⇥
⇥ 3
⇥
⇥ 3
⇥
⇥
⇥
⇥
⇥
⇥ 3 3
⇥
⇥
⇥ 3 3 3
⇥
⇥
⇥
⇥ 3 3 3 3 marks. Our key insight is that much of the difﬁculty with previous IL methods arises from the
IRL-motivated representation of the IL problem as a min-max problem over reward and policy [17, 1].
This introduces a requirement to separately model the reward and policy, and train these two functions jointly, often in an adversarial fashion. Drawing on connections between RL and energy-based models [13, 14], we propose learning a single model for the Q-value. The Q-value then implicitly deﬁnes both a reward and policy function. This turns a difﬁcult min-max problem over policy and reward functions into a simpler minimization problem over a single function, the Q-value. Since our problem has a one-to-one correspondence with the min-max problem studied in adversarial IL [17], we maintain the generality and guarantees of these previous approaches, resulting in a meaningful reward that may be used for inverse reinforcement learning. Furthermore, our method may be used to minimize a variety of statistical divergences between the expert and learned policy. We show that we recover several previously-described approaches as special cases of particular divergences, such as the regularized behavioural cloning of [30], and the conservative Q-learning of [23].
In our experiments, we ﬁnd that our method is performant even with very sparse data - surpassing prior methods using one expert demonstration in the completely ofﬂine setting - and can scale to complex image-based tasks like Atari reaching expert performance. Moreover, our learnt rewards are highly predictive of the original environment rewards.
Concretely, our contributions are as follows:
• We present a modiﬁed Q-learning update rule for imitation learning that can be implemented on top of soft-Q learning or soft actor-critic (SAC) algorithms in fewer than 15 lines of code.
• We introduce a simple framework to minimize a wide range of statistical distances: Integral
Probability Metrics (IPMs) and f-divergences, between the expert and learned distributions.
• We empirically show state-of-art results in a variety of imitation learning settings: online and ofﬂine IL. On the complex Atari suite, we outperform prior methods by 3-7x while requiring 3x less environment steps.
• We characterize our learnt rewards and show a high positive correlation with the ground-truth rewards, justifying the use of our method for Inverse Reinforcement Learning. 2