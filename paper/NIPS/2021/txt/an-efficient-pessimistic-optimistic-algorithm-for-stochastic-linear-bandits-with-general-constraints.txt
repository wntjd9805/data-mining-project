Abstract
τ
¯
?
´´
K0.75
¯
δ ` d
This paper considers stochastic linear bandits with general nonlinear constraints.
The objective is to maximize the expected cumulative reward over horizon T subject to a set of constraints in each round τ ď T . We propose a pessimistic-optimistic algorithm for this problem, which is efﬁcient in two aspects. First, the algorithm yields ˜O (pseudo) regret in round τ ď T, where K is the number of constraints, d is the dimension of the reward feature space, and
δ is a Slater’s constant; and zero constraint violation in any round τ ą τ 1, where
τ 1 is independent of horizon T. Second, the algorithm is computationally efﬁcient.
Our algorithm is based on the primal-dual approach in optimization and includes two components. The primal component is similar to unconstrained stochastic linear bandits (our algorithm uses the linear upper conﬁdence bound algorithm (LinUCB)). The computational complexity of the dual component depends on the number of constraints, but is independent of the sizes of the contextual space, the action space, and the feature space. Thus, the computational complexity of our algorithm is similar to LinUCB for unconstrained stochastic linear bandits. 1

Introduction
Stochastic linear bandits have a broad range of applications in practice, including online recommen-dations, job assignments in crowdsourcing, and clinical trials in healthcare. Most existing studies on stochastic linear bandits formulated them as unconstrained online optimization problems, limiting their application to problems with operational constraints such as safety, fairness, and budget con-straints. In this paper, we consider a stochastic linear bandit with general constraints. As in a standard stochastic linear bandit, at the beginning of each round t P rT s, the learner is given a context cptq that is randomly sampled from the context set C (a countable set), and takes an action Aptq P rJs. The learner then receives a reward Rpcptq, Aptqq “ rpcptq, Aptqq ` ηptq, where rpc, jq “ xθ˚, φpc, jqy,
φpc, jq P Rd is a d-dimensional feature vector for (context, action) pair pc, jq, θ˚ P Rd is an unknown underlying vector to be learned, and ηptq is a zero-mean random variable. For constrained stochastic linear bandits, we further assume when action Aptq is taken on context cptq, it incurs K different types of costs, denoted by W pkqpcptq, Aptqq. We assume W pkqpc, jq is a random variable with mean wpkqpc, jq that is unknown to the learner. This paper considers general cost functions and does not require wpkqpc, jq to have a linear form like rpc, jq.
Denote the action taken by policy π in round t by Aπptq. The learner’s objective is to learn a policy
π that maximizes the cumulative rewards over horizon T subject to anytime cumulative constraints: 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
«
Tÿ
ﬀ
Rpcptq, Aπptqq (1)
E
«
τÿ
ﬀ
W pkq pcptq, Aπptqq
ď 0, @ τ P rT s, k P rKs. (2)
The constraint (2) above may represent different operational constraints including safety, fairness, and budget constraints. t“1
E max
π t“1
Anytime cumulative constraints
In the literature, constraints in stochastic bandits have been formulated differently. There are two popular formulations. The ﬁrst one is a cumulative constraint over horizon T, including knapsack bandits [10, 9, 3, 4, 5, 17, 11] where the process terminates when the total budget has been consumed; fair bandits where the number of times an action can be taken must exceed a threshold at the end of the horizon [12]; and contextual bandits with a cumulative budget constraint [40, 14]. In these settings, the feasible action set in each round depends on the history. In general, the learner has more ﬂexibility in the earlier rounds, close to that in the unconstrained setting. Another formulation is anytime constraints, which either require the expected cost of the action taken in each round to be lower than a threshold [6, 29] or the expected cost of the policy in each round is lower than a threshold [32]. We call them anytime action constraints and anytime policy constraints, respectively.
Our constraint in the form of (2) is an anytime cumula-tive constraint, i.e., it imposes a cumulative constraint in every round. This anytime cumulative constraint is most similar to the anytime policy constraint in [32] because the average cost of a policy is close to its mean after the policy has been applied for many rounds and the process converges, so it can be viewed as a cumulative constraint on actions over many rounds (like ours). Furthermore, when our anytime cumulative constraint (2) is satisﬁed, it is guaranteed that the time-average cost is below a threshold in every round.
In summary, our anytime cumulative constraint is stricter than a cumulative constraint over ﬁxed horizon T but is less restrictive than anytime action constraint in [6, 29]. Figure 1 provides a conceptual description of the relationship between these different forms of constraints.
Main Contributions
Figure 1: A conceptual description of feasi-ble policy sets under different constraint for-mulations.
´´
This paper presents a pessimistic-optimistic algorithm based on the primal-dual approach in optimiza-tion for the problem deﬁned in (1)-(2). The algorithm is efﬁcient in two aspects. First, the algorithm yields ˜O regret in round τ ď T and achieves zero constraint violation in any round τ ą τ 1 for a constant τ 1 independent of horizon T . Second, the algorithm is computationally efﬁcient.
¯
δ ` d
K0.75
?
¯
τ
For computational efﬁciency, the design of our algorithm is based on the primal-dual approach in optimization. The computation of the primary component is similar to unconstrained stochastic linear bandits [15, 33, 25, 1, 13]. The dual component includes a set of Lagrangian multipliers that are updated in a simple manner to keep track of the levels of constraint violations so far in each round; the update depends on the number of constraints, but it is independent of the sizes of the contextual space, the action space, and the feature space. Thus, the overall computational complexity of our algorithm is similar to that of LinUCB in the unconstrained setting. This results in a much more efﬁcient calculation comparing to OPLB proposed in [32]. OPLB needs to construct a safe policy set in each round, hence, its computational complexity is prohibitively high as the authors acknowledged.
For constraint violation, our algorithm guarantees that for any τ ą τ 1, the constraint holds with probability one. In other words, after a constant number of rounds, the constraint is always satisﬁed.
This is in contrast to prior works [32, 6], where anytime constraints are proven to hold over horizon
T with probability 1 ´ χ for a constant χ. In other words, the anytime constraints may be violated with probability χ, and it is not clear how often they are violated when it happens. Furthermore, 2
beyond mean cost constraints considered in (2) and in [32, 6], we prove that a sample-path version of
? constraint (2) holds with probability 1 ´ O in round τ under our algorithm.
´ e´ δ 50K2.5
¯
τ
To summarize, our algorithm is computationally efﬁcient and provides strong guarantees on both regret and constraint violations. Additionally, our cost function is in a general form and does not need to be linear as those in [32, 6]. We discuss more related work in the following.