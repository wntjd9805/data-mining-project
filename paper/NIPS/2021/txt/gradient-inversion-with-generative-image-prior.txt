Abstract
Federated Learning (FL) is a distributed learning framework, in which the local data never leaves clients’ devices to preserve privacy, and the server trains models on the data via accessing only the gradients of those local data. Without further privacy mechanisms such as differential privacy, this leaves the system vulnerable against an attacker who inverts those gradients to reveal clients’ sensitive data.
However, a gradient is often insufﬁcient to reconstruct the user data without any prior knowledge. By exploiting a generative model pretrained on the data distribution, we demonstrate that data privacy can be easily breached. Further, when such prior knowledge is unavailable, we investigate the possibility of learning the prior from a sequence of gradients seen in the process of FL training. We experimentally show that the prior in a form of generative model is learnable from iterative interactions in FL. Our ﬁndings strongly suggest that additional mechanisms are necessary to prevent privacy leakage in FL. 1

Introduction
Federated learning (FL) is an emerging framework for distributed learning, where central server aggregates model updates, rather than user data, from end users [5, 17]. The main premise of federated learning is that this particular way of distributed learning can protect users’ data privacy as there is no explicit data shared by the end users with the central server.
However, a recent line of work [34, 31, 9, 29] demonstrates that one may recover the private user data used for training by observing the gradients. This process of recovering the training data from gradients, so-called gradient inversion, poses a huge threat to the federated learning community, as it may imply the fundamental ﬂaw of its main premise.
Even more worryingly, recent works suggest that such gradient inversion attacks can be made even stronger if certain side-information is available. For instance, Geiping et al. [9] show that if the attacker knows a prior that user data consists of natural images, then the gradient inversion attack can leverage such prior, achieving a more accurate recovery of the user data. Another instance is when batch norm statistics are available at the attacker in addition to gradients. This can actually happen if the end users share their local batch norm statistics as in [17]. Yin et al. [29] show that such batch normalization statistics can signiﬁcantly improve the strength of the gradient inversion attack, enabling precise recovery of high-resolution images.
In this paper, we systematically study how one can maximally utilize and even obtain the prior information when inverting gradients. We ﬁrst consider the case that the attacker has a generative model pretrained on the exact or approximate distribution of the user data as a prior. For this, we propose an efﬁcient gradient inversion algorithm that utilizes the generative model prior. In
∗ equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: An example showing the superiority of GIAS compared to existing method. Images of the authors are reconstructed from gradients by exploiting a generative model pretrained on human face images. particular, the algorithm consists of two steps, in which the ﬁrst step searches the latent space (of lower dimension) deﬁned by the generative model instead of the ambient input space (of higher dimension), and then the second step adapts the generative model to each input given the gradient.
Each step provides substantial improvement in the reconstruction. We name the algorithm as gradient inversion in alternative spaces (GIAS). Figure 1 represents reconstruction results with the proposed method and existing one.
We then consider a realistic scenario in which the user data distribution is not known in advance, and thus the attacker needs to learn it from gradients. For this scenario, we develop a meta-learning framework, called gradient inversion to meta-learn (GIML), which learns a generative model on user data from observing and inverting multiple gradients computed on the data, e.g. across different FL epochs or participating nodes. Our experimental results demonstrate that one can learn a generative model via GIML and reconstruct data by making use of the learned generative model.
This implies a great threat on privacy leakage in FL since our methods can be applied for any data type in most FL scenarios unless a specialized architecture prevents the gradient leakage explicitly, e.g., [18].
Our main contributions are as follows:
• We introduce GIAS that fully utilizes a pretrained generative model to invert gradient. In addition, we propose GIML which can train generative model from gradients only in FL.
• We demonstrate signiﬁcant privacy leakage occurring by GIAS with a pretrained generative model in various FL scenarios which are challenging to other existing methods, e.g., [9, 29].
• We experimentally show that GIML can learn a generative model on the user data from only gradients, which provides the same level of data recovery with a given pretrained model. To our best knowledge, GIML is the ﬁrst capable of learning explicit prior on a set of gradient inversion tasks.
• We note that a gradient inversion technique deﬁnes a standard on defence mechanism in
FL for privacy [28]. By substantiating that our proposed methods are able to break down defense mechanisms that were safe according to the previous standard, we give a strong warning to the FL community to use a higher standard deﬁned by our attack methods, and raise the necessity of a more conservative choice of defense mechanisms. 2
2