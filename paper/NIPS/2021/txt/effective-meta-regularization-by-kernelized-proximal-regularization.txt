Abstract
We study the problem of meta-learning, which has proved to be advantageous to accelerate learning new tasks with a few samples. The recent approaches based on deep kernels achieve the state-of-the-art performance. However, the regularizers in their base learners are not learnable. In this paper, we propose an algorithm called
MetaProx to learn a proximal regularizer for the base learner. We theoretically establish the convergence of MetaProx. Experimental results conﬁrm the advantage of the proposed algorithm. 1

Introduction
Humans, by leveraging prior knowledge and experience, can easily learn new tasks from a handful of examples. In contrast, deep networks are data-hungry, and a large number of training samples are required to avoid overﬁtting. To reduce the labor-intensive and time-consuming process of data labeling, meta-learning (or learning to learn) [3, 37] aims to exact meta-knowledge from seen tasks to accelerate learning on unseen tasks. Recently, meta-learning has been receiving increasing attention due to its diverse successful applications in few-shot learning [41, 12, 39, 35], hyperparameter optimization [14], neural architecture search [24, 44], and reinforcement learning [29].
Many meta-learning algorithms operate on two levels. A base learner learns task-speciﬁc models in the inner loop, and a meta-learner learns the meta-parameter in the outer loop. A popular class of algorithms is based on meta-initialization [12, 25, 11, 38], such as the well-known MAML [12]. It learns a model initialization such that a good model for an unseen task can be learned from limited samples by a few gradient updates. However, computing the meta-gradient requires back-propagating through the entire inner optimization path, which is infeasible for large models and/or there are many gradient steps. During testing, it is common for MAML’s base learner to perform many gradient steps to seek a more accurate solution [12]. However, for regression using a linear base learner and square loss, we will show that though the meta-learner can converge to the optimal meta-initialization, the base learner may overﬁt the training data at meta-testing.
Another class of meta-learning algorithms is based on meta-regularization [28, 43, 7, 8, 9], in which the base learner learns the task-speciﬁc model by minimizing the loss with a proximal regularizer (a biased regularizer from the meta-parameter). Denevi et al. [7] uses a linear model with efﬁcient closed-form solution for the base learner. However, extending to nonlinear base learners requires computing the meta-gradient using matrix inversion, which can be infeasible for deep networks [28].
To introduce nonlinearity to the base learner, a recent approach is to make use of the kernel trick.
For example, R2D2 [4] and MetaOptNet [22] use deep kernels [42] in meta-learning for few-shot
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
classiﬁcation. Speciﬁcally, the deep network is learned in the meta-learner, while a base kernel is used in the base learner. Though they achieve state-of-the-art performance, their base learners use a
Tikhonov regularizer rather than a learnable proximal regularizer as in meta-regularization methods.
As learning a meta-regularization has been shown to be effective in linear models for regression [7] and classiﬁcation [8], in this paper we propose a kernel-based algorithm to meta-learn a proximal regularizer for a nonlinear base learner. After kernel extension, the learnable function in the proximal regularizer is a function in the reproducing kernel Hilbert space (RKHS) induced by the base kernel.
The proposed algorithm is guaranteed to converge to a critical point of the meta-loss and its global convergence is also established. Experiments on various benchmark regression and classiﬁcation datasets demonstrate the superiority of the proposed algorithm over the state-of-the-arts.
Notations. Vectors are denoted by lowercase boldface, and matrices by uppercase boldface. For a vector x, (cid:107)x(cid:107) = (cid:112)(cid:80) i and diag(x) constructs a diagonal matrix with x on the diagonal. (cid:107) · (cid:107)H is the norm on the Hilbert space H. N (0, σ2) is the univariate normal distribution with mean zero and variance σ2. N (m, Σ) is the multivariate normal distribution with mean m and covariance matrix Σ. i x2 2