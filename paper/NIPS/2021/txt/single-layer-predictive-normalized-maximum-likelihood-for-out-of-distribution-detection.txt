Abstract
Detecting out-of-distribution (OOD) samples is vital for developing machine learn-ing based models for critical safety systems. Common approaches for OOD detection assume access to some OOD samples during training which may not be available in a real-life scenario. Instead, we utilize the predictive normalized maxi-mum likelihood (pNML) learner, in which no assumptions are made on the tested input. We derive an explicit expression of the pNML and its generalization error, denoted as the regret, for a single layer neural network (NN). We show that this learner generalizes well when (i) the test vector resides in a subspace spanned by the eigenvectors associated with the large eigenvalues of the empirical correlation matrix of the training data, or (ii) the test sample is far from the decision boundary.
Furthermore, we describe how to efﬁciently apply the derived pNML regret to any pretrained deep NN, by employing the explicit pNML for the last layer, followed by the softmax function. Applying the derived regret to deep NN requires neither addi-tional tunable parameters nor extra data. We extensively evaluate our approach on 74 OOD detection benchmarks using DenseNet-100, ResNet-34, and WideResNet-40 models trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30 showing a signiﬁcant improvement of up to 15.6% over recent leading methods. 1

Introduction
An important concern that limits the adoption of deep neural networks (DNN) in critical safety systems is how to assess our conﬁdence in their predictions, i.e, quantifying their generalization capability (Kaufman et al., 2019; Willers et al., 2020). Take, for instance, a machine learning model for medical diagnosis (Bibas et al., 2021). It may produce (wrong) diagnoses in the presence of test inputs that are different from the training set rather than ﬂagging them for human intervention (Singh et al., 2021). Detecting such unexpected inputs had been formulated as the out-of-distribution (OOD) detection task (Hendrycks and Gimpel, 2017), as ﬂagging test inputs that lie outside the training classes, i.e., are not in-distribution (IND).
Previous learning methods that designed to offer such generalization measures, include VC-dimension (Zhong et al., 2017; Vapnik and Chervonenkis, 2015) and norm based bounds (Neyshabur et al., 2018; Bartlett et al., 2017). As a whole, these methods characterized the generalization ability based on the properties of the parameters. However, they do not consider the test sample that is presented to the model (Jiang et al., 2020), which makes them useless for OOD detection. Other approaches build heuristics over the empirical risk minimization (ERM) learner, by post-processing 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the model output (Sastry and Oore, 2020) or modifying the training process (Papadopoulos et al., 2021; Vyas et al., 2018). Regardless of the approach, these methods choose the learner that minimizes the loss over the training set. This may lead to a large generalization error because the ERM estimate may be wrong on unexpected inputs; especially with large models such as DNN (Belkin et al., 2019).
To produce a useful generalization measure, we exploit the individual setting framework (Merhav and
Feder, 1998). In the individual setting, there is no assumption about how the training and the test data are generated, nor about their probabilistic relationship. Moreover, the relationship between the labels and data can be deterministic and may therefore be determined by an adversary. The generalization error in this setting is often referred to as the regret (Merhav and Feder, 1998). This regret is deﬁned by the log-loss difference between a learner and the genie: a learner that knows the speciﬁc test label, yet is constrained to use an explanation from a set of possible models. The individual setting is the most general framework and so the result holds for a wide range of scenarios. Speciﬁcally, the result holds to OOD detection where the distribution of the OOD inputs is unknown.
The pNML learner (Fogel and Feder, 2018) was proposed as the min-max solution of the regret, where the minimum is over the learner choice and the maximum is for any possible test label value.
Intuitively, the pNML assigns a probability for a potential outcome as follows: Add the test sample to the training set with an arbitrary label, ﬁnd the ERM solution of this new set, and take the probability it gives to the assumed label. Follow this procedure for every label and normalize to get a valid probability assignment. The pNML was developed before for linear regression (Bibas et al., 2019b) and was evaluated empirically for DNN (Fu and Levine, 2021; Bibas et al., 2019a).
We derive an analytical solution of the pNML learner and its generalization error (the regret) for a single layer NN. We analyze the derived regret and show it obtains low values when the test input either (i) lies in a subspace spanned by the eigenvectors associated with the large eigenvalues of the training data empirical correlation matrix or (ii) is located far from the decision boundary. Crucially, although our analysis focuses on a single layer NN, our results are applicable to the last layer of
DNNs without changing the network architecture or the training process: We treat the pretrained
DNN as a feature extractor with the last layer as a single layer NN classiﬁer. We can therefore show the usage of the pNML regret as a conﬁdence score for the OOD detection task.
To summarize, we make the following contributions. 1. Analytical derivation of the pNML regret. We derive an analytical expression of the pNML regret, which is associated with the generalization error, for a single layer NN. 2. Analyzing the pNML regret. We explore the pNML regret characteristics as a function of the test sample data, training data, and the corresponding ERM prediction. We provide a visualization on low dimensional data and demonstrate the situations in which the pNML regret is low and the prediction can be trusted. 3. DNN adaptation. We propose an adaptation of the derived pNML regret to any pretrained
DNN that uses the softmax function with neither additional parameters nor extra data.
Applying our pNML regret to a pretrained DNN does not require additional data, it is efﬁcient and can be easily implemented. The derived regret is theoretically justiﬁed for OOD detection since it is the individual setting solution for which we do not require any knowledge on the test input distribution.
Our evaluation includes 74 IND-OOD detection benchmarks using DenseNet-BC-100, ResNet-34, and WideResNet-40 trained with CIFAR-100, CIFAR-10, SVHN, and ImageNet-30. Our approach outperforms leading methods in nearly all 74 OOD detection benchmarks up to a remarkable +15.2% 2