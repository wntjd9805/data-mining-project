Abstract
Optimization in the latent space of variational autoencoders is a promising ap-proach to generate high-dimensional discrete objects that maximize an expensive black-box property (e.g., drug-likeness in molecular generation, function approxi-mation with arithmetic expressions). However, existing methods lack robustness as they may decide to explore areas of the latent space for which no data was available during training and where the decoder can be unreliable, leading to the generation of unrealistic or invalid objects. We propose to leverage the epistemic uncertainty of the decoder to guide the optimization process. This is not trivial though, as a naive estimation of uncertainty in the high-dimensional and structured settings we consider would result in high estimator variance. To solve this problem, we introduce an importance sampling-based estimator that provides more robust estimates of epistemic uncertainty. Our uncertainty-guided optimization approach does not require modiﬁcations of the model architecture nor the training process. It produces samples with a better trade-off between black-box objective and validity of the generated samples, sometimes improving both simultaneously. We illustrate these advantages across several experimental settings in digit generation, arithmetic expression approximation and molecule generation for drug design. 1

Introduction
We consider the task of optimizing an expensive black-box objective function taking inputs in a high-dimensional discrete space. This could be for example ﬁnding new molecules for drug design, or automatically generating a computer program that matches a desired output. Solving this task directly in the original space (e.g., with discrete local search methods such as genetic algorithms) may be challenging given the complex structure and high dimensionality of the data. Recently, Variational autoencoders (VAEs) [1, 2] have been successfully leveraged to model a wide range of discrete data modalities — from natural language [3], to arithmetic expressions [4], computer programs [5] or molecules [6]. By learning a lower-dimensional continuous representation of objects in their latent space, VAEs allow to transform the original discrete optimization problem into a simpler continuous optimization one in latent space. For example, this can be achieved via Bayesian Optimization in the latent space, or via gradient ascent with a jointly-trained neural network predicting the black box 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
property from the latent space representation [6, 7]. Initial methods in this area have suffered from the fact that the search in latent space may explore areas for which no data was available at train time, and therefore where the decoder network of the VAE will be unreliable [8]: seemingly good candidate points in latent space may be decoded into objects that are invalid, unrealistic or low quality.
Figure 1: Uncertainty-guided optimization in VAE latent space The goal of black-box optimiza-tion in latent space is to attain regions with high values of the back-box objective after decoding, while avoiding the regions that lead to invalid decodings (left). Standard Bayesian Optimization in latent space may query these suboptimal areas (e.g., regions on left hand side, center). High decoder uncertainty regions overlap with regions leading to invalid decodings (right), so that censoring high uncertainty points helps guiding the optimization towards the most promising latent points.
While several methods have been introduced to promote validity of decoded objects (§2.1), they either focus on modifying the generative model learning procedure or adapting the decoder architecture to satisfy the syntactic requirements of the data modality of interest. We propose instead to quantify and leverage the uncertainty of the decoder network to guide the optimization process in latent space (Fig. 1). This approach does not require any change to the model training nor architecture, and can easily be integrated within several optimization frameworks. It results in a better trade-off between the values of the black-box objective and the validity of the newly generated objects, sometimes improving both simultaneously.
To be effective, this method requires robust estimates of model uncertainty for high dimensional structured data. Existing methods for uncertainty estimation in this domain often rely on heuristics or make independence assumptions to make computations tractable (§2.2). We demonstrate that such assumptions are not appropriate in our setting, and propose new methods for uncertainty estimation in high dimensional structured data instead.
Our contributions are as follows:
• We introduce an algorithm to quantify the uncertainty of high-dimensional discrete data, and use it to estimate the uncertainty of the decoder (§3);
• We show how the uncertainty of the decoder can be incorporated across several optimization frameworks, including gradient ascent and Bayesian optimization (§4);
• We illustrate our approach in a digit generation setting — a simple setup to provide intuition for the method — then quantify its beneﬁts in the more complex tasks of arithmetic expres-sions approximation and molecule generation, covering a diverse set of decoder architectures across experiments (Convolutional, Recurrent and Graph Neural Networks) (§5). 2