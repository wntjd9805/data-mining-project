Abstract
While early research in neural architecture search (NAS) required extreme compu-tational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to run many types of multi-ﬁdelity techniques, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate bench-marks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the ﬁnal validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-ﬁdelity algorithms, showing that it leads to improvements over popular single-ﬁdelity algo-rithms which claimed to be state-of-the-art upon release. Our code and pretrained models are available at https://github.com/automl/nas-bench-x11. 1

Introduction
In the past few years, algorithms for neural architecture search (NAS) have been used to automatically
ﬁnd architectures that achieve state-of-the-art performance on various datasets [82, 53, 38, 13]. In 2019, there were calls for reproducible and fair comparisons within NAS research [34, 57, 75, 36] due to both the lack of a consistent training pipeline between papers and experiments with not enough trials to reach statistically signiﬁcant conclusions. These concerns spurred the release of tabular benchmarks, such as NAS-Bench-101 [76] and NAS-Bench-201 [11], created by fully training all architectures in search spaces of size 423 624 and 6 466, respectively. These benchmarks allow researchers to easily simulate NAS experiments, making it possible to run fair NAS comparisons and to run enough trials to reach statistical signiﬁcance at very little computational cost or carbon emissions [17]. Recently, to extend the beneﬁts of tabular NAS benchmarks to larger, more realistic
NAS search spaces which cannot be evaluated exhaustively, it was proposed to construct surrogate benchmarks [60]. The ﬁrst such surrogate benchmark is NAS-Bench-301 [60], which models the DARTS [38] search space of size 1018 architectures. It was created by fully training 60 000 architectures (both drawn randomly and chosen by top NAS methods) and then ﬁtting a surrogate model that can estimate the performance of all of the remaining architectures. Since 2019, dozens of papers have used these NAS benchmarks to develop new algorithms [67, 55, 73, 64, 59].
An unintended side-effect of the release of these benchmarks is that it became signiﬁcantly easier to devise single ﬁdelity NAS algorithms: when the NAS algorithm chooses to evaluate an architecture, the architecture is fully trained and only the validation accuracy at the ﬁnal epoch of training is outputted. This is because NAS-Bench-301 only contains the architectures’ accuracy at epoch 100,
∗Equal contribution. Correspondence to yanshen6@msu.edu, colin@abacus.ai, ysavani@cs.cmu.edu, fh@cs.uni-freiburg.de. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Each image shows the true learning curve vs. the learning curve predicted by our surrogate models, with and without predicted noise modeling. We also plot the 90% conﬁdence interval of the predicted noise distribution. We plot two architectures each for NAS-Bench-111, NAS-Bench-211,
NAS-Bench-311, and NAS-Bench-NLP11. and NAS-Bench-101 only contains the accuracies at epochs 4, 12, 36, and 108 (allowing single
ﬁdelity or very limited multi-ﬁdelity approaches). NAS-Bench-201 does allow queries on the entire learning curve (every epoch), but it is smaller in size (6 466) than NAS-Bench-101 (423 624) or
NAS-Bench-301 (1018). In a real world experiment, since training architectures to convergence is computationally intensive, researchers will often run multi-ﬁdelity algorithms: the NAS algorithm can train architectures to any desired epoch. Here, the algorithm can make use of speedup techniques such as learning curve extrapolation (LCE) [63, 8, 1, 28] and successive halving [35, 14, 32, 29].
Although multi-ﬁdelity techniques are often used in the hyperparameter optimization community [20, 21, 35, 14, 63], they have been under-utilized by the NAS community in the last few years.
In this work, we ﬁll in this gap by releasing NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, surrogate benchmarks with full learning curve information for train, validation, and test loss and accuracy for all architectures, signiﬁcantly extending NAS-Bench-101, NAS-Bench-301, and
NAS-Bench-NLP [30], respectively. With these benchmarks, researchers can easily incorporate multi-ﬁdelity techniques, such as early stopping and LCE into their NAS algorithms. Our technique for creating these benchmarks can be summarized as follows. We use a training dataset of architectures (drawn randomly and chosen by top NAS methods) with good coverage over the search space, along with full learning curves, to ﬁt a model that predicts the full learning curves of the remaining architectures. We employ three techniques to ﬁt the model: (1) dimensionality reduction of the learning curves, (2) prediction of the top singular value coefﬁcients, and (3) noise modeling. These techniques can be used in the future to create new NAS benchmarks as well. To ensure that our surrogate benchmarks are highly accurate, we report statistics such as Kendall Tau rank correlation and Kullback Leibler divergence between ground truth learning curves and predicted learning curves on separate test sets. See Figure 1 for examples of predicted learning curves on the test sets.
To demonstrate the power of using the full learning curve information, we present a framework for converting single-ﬁdelity NAS algorithms into multi-ﬁdelity algorithms using LCE. We apply our framework to popular single-ﬁdelity NAS algorithms, such as regularized evolution [53], local search [68], and BANANAS [67], all of which claimed state-of-the-art upon release, showing that they can be further improved across four search spaces. Finally, we also benchmark multi-ﬁdelity algorithms such as Hyperband [35] and BOHB [14] alongside single-ﬁdelity algorithms. Overall, our work bridges the gap between different areas of AutoML and will allow researchers to easily develop effective multi-ﬁdelity and LCE techniques in the future. To promote reproducibility, we release our code and we follow the NAS best practices checklist [36], providing the details in Appendix A.
Our contributions. We summarize our main contributions below. 2
Table 1: Overview of existing NAS benchmarks. We introduce NAS-Bench-111, -311, and -NLP11.
Benchmark
Size Queryable
Based on
NAS-Bench-101
NAS-Bench-201
NAS-Bench-301
NAS-Bench-NLP
NAS-Bench-ASR
NAS-Bench-111
NAS-Bench-311
NAS-Bench-NLP11 423k 6k 1018 1053 8k 423k 1018 1053 (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51)
Full train info (cid:55) (cid:51) (cid:55) (cid:55) (cid:51)
DARTS
NAS-Bench-101
DARTS
NAS-Bench-NLP (cid:51) (cid:51) (cid:51)
• We develop a technique to create surrogate NAS benchmarks that include the full training information for each architecture, including train, validation, and test loss and accuracy learning curves. This technique can be used to create future NAS benchmarks on any search space.
• We apply our technique to create NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, which allow researchers to easily develop multi-ﬁdelity NAS algorithms that achieve higher performance than single-ﬁdelity techniques.
• We present a framework for converting single-ﬁdelity NAS algorithms into multi-ﬁdelity NAS algorithms using learning curve extrapolation, and we show that our framework allows popular state-of-the-art NAS algorithms to achieve further improvements. 2