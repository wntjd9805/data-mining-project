Abstract
Gradient-based hyperparameter optimization has earned a widespread popularity in the context of few-shot meta-learning, but remains broadly impractical for tasks with long horizons (many gradient steps), due to memory scaling and gradient degradation issues. A common workaround is to learn hyperparameters online, but this introduces greediness which comes with a signiﬁcant performance drop. We propose forward-mode differentiation with sharing (FDS), a simple and efﬁcient algorithm which tackles memory scaling issues with forward-mode differentiation, and gradient degradation issues by sharing hyperparameters that are contiguous in time. We provide theoretical guarantees about the noise reduction properties of our algorithm, and demonstrate its efﬁciency empirically by differentiating through 104 gradient steps of unrolled optimization. We consider large hyperparameter
∼ search ranges on CIFAR-10 where we signiﬁcantly outperform greedy gradient-based alternatives, while achieving 20 speedups compared to the state-of-the-art black-box methods. Code is available at: https://github.com/polo5/FDS
× 1

Introduction
Deep neural networks have shown tremendous success on a wide range of applications, including classiﬁcation [1], generative models [2], natural language processing [3] and speech recognition [4].
This success is in part due to effective optimizers such as SGD with momentum or Adam [5], which require carefully tuned hyperparameters for each application. In recent years, a long list of heuristics to tune such hyperparameters has been compiled by the deep learning community, including things like: how to best decay the learning rate [6], how to scale hyperparameters with the budget available [7], and how to scale learning rate with batch size [8]. Unfortunately these heuristics are often dataset speciﬁc and architecture dependent [9]. They also don’t apply well to new optimizers [10], or new tools, like batch normalization which allows for larger learning rates [11].
With so many ways to choose hyperparameters, the deep learning community is at risk of adopting models based on how much effort went into tuning them, rather than their methodological insight.
The ﬁeld of hyperparameter optimization (HPO) aims to ﬁnd hyperparameters that provide a good generalization performance automatically. Unfortunately, existing tools are rather unpopular for deep learning, largely owing to their computational cost. The method developed here is a gradient-based HPO approach; that is, it calculates hypergradients, i.e. the gradient of some validation loss with respect to each hyperparameter. Gradient-based HPO should be more efﬁcient than black-box methods as the dimensionality of the hyperparameter space increases, since it is able to utilize gradient information rather than rely on trial and error. In practice however, learning hyperparameters with gradients has only been popular in few-shot learning tasks where the horizon is short, i.e. where the underlying task is solved with a few gradient steps. This is because long horizons cause hypergradient degradation, and incur a memory cost that makes reverse-mode differentiation prohibitive. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Greedy gradient-based methods alleviate both of these issues by calculating local hypergradients based on intermediate validation losses. Un-fortunately, this introduces some bias [15] and results in a signiﬁcant performance drop, which we are able to quantify in this work. We make use of forward-mode differentiation, which has been shown to offer a memory cost constant with horizon size. However, previous forward-mode methods don’t address gradient degrada-tion explicitly and are thus limited to the greedy setting [16, 17].
We introduce FDS (Foward-mode Differentia-tion with hyperparameter Sharing), which to the best of our knowledge demonstrates for the ﬁrst time that hyperparameters can be differentiated non-greedily over long horizons. Speciﬁcally, we make the following contributions: (1) we propose to share hyperparameters through time, both motivating it theoretically and with vari-ous experiments, (2) we combine the above in a forward-mode differentiation algorithm, and (3) we show that our method can signiﬁcantly out-perform various HPO algorithms, for instance when learning the hyperparameters of the SGD-momentum optimizer. 2