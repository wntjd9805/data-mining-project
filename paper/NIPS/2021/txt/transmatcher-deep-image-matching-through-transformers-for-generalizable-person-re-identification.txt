Abstract
Transformers have recently gained increasing attention in computer vision. How-ever, existing studies mostly use Transformers for feature representation learning, e.g. for image classiﬁcation and dense predictions, and the generalizability of
Transformers is unknown. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of im-ages. We ﬁnd that the Vision Transformer (ViT) and the vanilla Transformer with decoders are not adequate for image matching due to their lack of image-to-image attention. Thus, we further design two naive solutions, i.e. query-gallery concatena-tion in ViT, and query-gallery cross-attention in the vanilla Transformer. The latter improves the performance, but it is still limited. This implies that the attention mechanism in Transformers is primarily designed for global feature aggregation, which is not naturally suitable for image matching. Accordingly, we propose a new simpliﬁed decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Addition-ally, global max pooling and a multilayer perceptron (MLP) head are applied to decode the matching result. This way, the simpliﬁed decoder is computationally more efﬁcient, while at the same time more effective for image matching. The proposed method, called TransMatcher, achieves state-of-the-art performance in generalizable person re-identiﬁcation, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively, on several popular datasets. Code is available at https://github.com/ShengcaiLiao/QAConv. 1

Introduction
The Transformer [24] is a neural network based on attention mechanisms. It has shown great success in the ﬁeld of natural language processing. Recently, it has also shown promising performance for computer vision tasks, including image classiﬁcation [7, 14], object detection [2, 37, 14, 26], and image segmentation [14, 26], thus gaining increasing attention in this ﬁeld. However, existing studies mostly use Transformers for feature representation learning, e.g. for image classiﬁcation or dense predictions, and the generalizability of Transformers is unknown. At a glance, query-key similarities are computed by dot products in the attention mechanisms of Transformers. Therefore, these models could potentially be useful for image matching. In this work, we further investigate the possibility of applying Transformers for image matching and metric learning given pairs of images, with applications in generalizable person re-identiﬁcation.
Attention mechanisms are used to gather global information from different locations according to query-key similarities. The vanilla Transformer [24] is composed of an encoder that employs
∗Shengcai Liao is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The structure of the proposed TransMatcher for image matching. A standard Transformer encoder without positional encoding is used for feature encoding. Then, query and gallery encodings are matched by a dot product. Global max pooling (GMP) is applied to ﬁnd the optimal matching scores and locations, and an MLP head is appended to produce the ﬁnal matching scores. Note that the batch dimension is ignored in this ﬁgure for simplicity. self-attention, and a decoder that further incorporates a cross-attention module. The difference is that the query and key are the same in the self-attention, while they are different in the cross-attention.
The Vision Transformer (ViT) [7] applies a pure Transformer encoder for feature learning and image classiﬁcation. While the Transformer encoder facilitates feature interaction among different locations of the same image, it cannot address the image matching problem being studied in this paper, because it does not enable interaction between different images. In the decoder, however, the cross-attention module does have the ability for cross interaction between query and the encoded memory. For example, in the decoder of the detection Transformer (DETR) [2], learnable query embeddings are designed to decode useful information in the encoded image memory for object localization. However, the query embeddings are independent from the image inputs, and so there is still no interaction between pairs of input images. Motivated by this, how about using actual image queries instead of learnable query embeddings as input to decoders?
Person re-identiﬁcation is a typical image matching and metric learning problem. In a recent study called QAConv [10], it was shown that explicitly performing image matching between pairs of deep feature maps helps the generalization of the learned model. This inspires us to investigate the capability and generalizability of Transformers for image matching and metric learning between pairs of images. Since training through classiﬁcation is also a popular strategy for metric learning, we start from a direct application of ViT and the vanilla Transformer with a powerful ResNet [3] backbone for person re-identiﬁcation. However, this results in poor generalization to different datasets. Then, we consider formulating explicit interactions between query2 and gallery images in Transformers.
Two naive solutions are thus designed. The ﬁrst one uses a pure Transformer encoder, as in ViT, but concatenates the query and gallery features together as inputs, so as to enable the self-attention module to read both query and gallery content and apply the attention between them. The second design employs the vanilla Transformer, but replaces the learnable query embedding in the decoder by the ready-to-use query feature maps. This way, the query input acts as a real query from the actual retrieval inputs, rather than a learnable query which is more like a prior or a template. Accordingly, the cross-attention module in the decoder is able to gather information across query-key pairs, where the key comes from the encoded memory of gallery images.
While the ﬁrst solution does not lead to improvement, the second one is successful with notable performance gain. However, compared to the state of the art in generalizable person re-identiﬁcation, 2Query/gallery in person re-identiﬁcation and query/key or target/memory in Transformers have very similar concepts originated from information retrieval. We use the same word query here in different contexts. 2
the performance of the second variant is still not satisfactory. We further consider that the attention mechanism in Transformers might be primarily for global feature aggregation, which is not naturally suitable for image matching, though the two naive solutions already enable feature interactions between query and gallery images. Therefore, to improve the effectiveness of image matching, we propose a new simpliﬁed decoder, which drops the full attention implementation with the softmax weighting, keeping only the query-key similarity computation. Additionally, inspired from QAConv
[10], global max pooling (GMP) is applied, which acts as a hard attention to gather similarity values, instead of a soft attention to weight feature values. This is because, in image matching, we are more interested in matching scores than feature values. Finally, a multilayer perceptron (MLP) head maps the matching result to a similarity score for each query-gallery pair. This way, the simpliﬁed decoder is computationally more efﬁcient, while at the same time more effective for image matching.
We call the above design TransMatcher (see Fig. 1), which targets at efﬁcient image matching and metric learning in particular. The contributions of this paper are summarized as follows.
• We investigate the possibility and generalizability of applying Transformers for image match-ing and metric learning, including direct applications of ViT and the vanilla Transformer, and two solutions adapted speciﬁcally for matching images through attention. This furthers our understanding of the capability and limitation of Transformers for image matching.
• According to the above, a new simpliﬁed decoder is proposed for efﬁcient image matching, with a focus on similarity computation and mapping.
• With generalizable person re-identiﬁcation experiments, the proposed TransMatcher is shown to achieve state-of-the-art performance on several popular datasets, with up to 6.1% and 5.7% performance gains in Rank-1 and mAP, respectively. 2