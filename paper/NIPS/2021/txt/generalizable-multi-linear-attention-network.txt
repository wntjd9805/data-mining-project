Abstract
The majority of existing multimodal sequential learning methods focus on how to obtain powerful individual representations and neglect to effectively capture the multimodal joint representation. Bilinear attention network (BAN) is a com-monly used integration method, which leverages tensor operations to associate the features of different modalities. However, BAN has a poor compatibility for more modalities, since the computational complexity of the attention map increases exponentially with the number of modalities. Based on this concern, we propose a new method called generalizable multi-linear attention network (MAN), which can associate more modalities in acceptable complexity with hierarchical approxima-tion decomposition. Speciﬁcally, considering the fact that softmax attention kernels cannot be decomposed as linear operation directly, we adopt the addition random features mechanism to approximate the non-linear softmax functions with enough theoretical analysis. Furthermore, we also introduce the local sequential constraints, which can be combined with ARF conveniently, as positional information. We conduct extensive experiments on several datasets of corresponding tasks, the experimental results show that MAN could achieve competitive results compared with baseline methods, showcasing the effectiveness of our contributions. 1

Introduction
Multimodal learning draws increasing attention in recent years, which aims to process and understand the information from multiple modalities (i.e. vision, language, audio) with machine learning skills. Many endeavors have been devoted to the multimodal interaction and robust individual representation learning [29, 6, 39, 40]. Existing multimodal interaction methods could be categorized into Transformer-based [29, 6] and Non-Transformer-based methods [39, 40]. With deep stacked attention blocks [30] and suitable number of training samples, Transformer-based methods could achieve relatively good performances with relatively high computational complexity. After obtaining the individual representations of all the modalities, joint representation learning [5, 15, 14, 19, 36] is performed by leveraging simple matrix and vector operations. Bilinear functions have been widely utilized in this ﬁeld. Speciﬁcally, bilinear pooling [15] employs low-rank decomposition to approximate high-order tensor operations for input vectors. Bilinear attention network (BAN) [14] uses a bilinear attention distribution, on top of low-rank bilinear pooling. In general, BAN exploits bilinear interactions between two groups of input channels and bilinear pooling extracts the joint representations for each pair of channels. Besides, bilinear pooling could be extended to multi-linear pooling with acceptable computational complexity. However, BAN has the limitation to generalize to more modalities due to the complexity. For example, when we want to integrate the features with the same space Rd×T of m modalities, we would obtain a high-order tensor A ∈ RT m that captures multimodal interactions. It is observed that A scales exponentially with the number of modalities.
Motivated by the observations, in this paper, we propose a novel method called generalizable multi-linear attention network (MAN), which could be leveraged to integrate as many modalities as possible
∗Corresponding Author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
with linear complexity. Concretely, we employ multi-linear pooling to extract the joint representations for multiple feature channels of different modalities. With the previous assumption, we could obtain
T m joint representations for m modalities in total. Following the main idea of BAN, an interaction tensor A ∈ RT m represents the attention weights of the joint representations (in practise, we devise the combinational addition attention map). To reduce the computational complexity and space requirements, we devise a hierarchical approximation decomposition (HAD) mechanism for the high-order tensor A, leading to the efﬁcient approximation decomposition. Speciﬁcally, considering the fact that softmax attention kernels cannot be decomposed as linear operation directly, we devise the addition random features (ARF) mechanism to approximate the non-linear softmax function.
Furthermore, we facilitate the sparse attention distributions by leveraging local sequential constraints (LSC), which can be combined with ARF conveniently. We conduct extensive experiments on three tasks, the experimental results show that MAN could achieve competitive results compared with the state-of-the-art methods. To sum up, the contributions of our work are four-fold:
• We propose the multi-linear attention network (MAN), which is the extension of bilinear attention network (BAN), to learn and utilize multi-linear attention distributions on top of the multi-linear pooling operations. To reduce the computational complexity and space requirements, we devise a hierarchical approximation decomposition mechanism for the combinational addition interaction tensor, leading to the efﬁcient approximation decomposition.
• Most importantly, considering that softmax attention kernels cannot be decomposed as linear operation directly, we adopt the addition random features (ARF) approach to approximate the non-linear softmax functions with enough theoretical analysis.
• To process multimodal sequential features, we introduce the local sequential constraints (LSC), which can be combined with ARF conveniently.
• To be compatible with Transformer, the multi-head and residual properties are introduced into
MAN. We conduct extensive experiments on three different tasks, the experimental results show that MAN could achieve competitive results compared with the baseline methods. 2 Approach
In this section, we ﬁrst introduce the low-rank multi-linear pooling, which is the extension of bilinear pooling [15]. Then we illustrate the details of multi-linear attention network. 2.1 Multi-Linear Pooling
We ﬁrst review the low-rank bilinear pooling. [15] proposes to reduce the rank of bilinear weight matrix Wi ∈ Rd1×d2 with CP decomposition [7]. Wi could be rewrited as the multiplication of two i , where Ui ∈ Rd1×d and Vi ∈ Rd2×d, d denotes the value of rank with the smaller matrices UiVT constraint d ≤ min(d1, d2). We calculate the scalar output fi as follows: fi = xT Wiy ≈ xT UiVT i y = 1T (cid:0)UT i x (cid:12) VT i y(cid:1) (1) where 1 ∈ Rd consists of ones and (cid:12) denotes element-wise multiplication, note that we omit the bias term for convenience. When the output is a vector f ∈ Rc, the 1 is replaced by a pooling matrix: f = PT (cid:0)UT x (cid:12) VT y(cid:1) (2) where P ∈ Rd×c, U ∈ Rd1×d, and V ∈ Rd2×d. By leveraging the pooling matrix P, the number of parameters is reduced signiﬁcantly. Based on the bilinear pooling, we introduce the calculation process of multi-linear pooling. Suppose the number of modalities is m and the feature (channel) dimensions of them are {dj|j ∈ [1, m]}, Eq. 2 is extended to: f = PT ( m (cid:89) j=1
UT j vj) = PT (UT 1 v1 (cid:12) UT 2 v2 (cid:12) ... (cid:12) UT mvm) (3) 2
Figure 1: The core module of multi-linear attention network (only one head), where we provide detailed input and output of each step, making it easy to understand. Note that, for convinience, we give an example of three modalities. where vj ∈ Rdj and Uj ∈ Rdj ×d denote the feature vector of j-th modality and corresponding projection matrix. Theoretically, P and {Uj|j ∈ [1, m]} are the components of d-rank approximation of the high-order tensor. 2.2 Multi-Linear Attention Network
Since the features of most modalities are multi-channel (for example, video features have temporal dimension), we propose MAN to learn and utilize the multi-linear attention distributions on top of the multi-linear pooling operations. Suppose the feature matrices of m modalities are {Vj ∈ Rdj ×Tj |j ∈
[1, m]}, we could obtain (cid:81)m j=1 Tj groups of joint representations. To combine these information, we introduce the multi-linear attention map A ∈ R(cid:81)m j=1 Tj as follows: (cid:48) k = ((A ×1 (VT f 1 U (cid:48) 1)k) ×2 (VT 2 U (cid:48) 2)k) ×3 ... ×m (VT mU (cid:48) m)k (4) (cid:48) (cid:48) j)k ∈ RTj , the operator ×j denotes the j-th mode product between a j ∈ Rdj ×K, (VT j U where U tensor and a vector, and f k denotes the k-th element of intermediate representation. Note that Eq. 4 is a multi-linear model for m groups of input channels, where A is a multi-linear weight matrix. To make it easier to understand, Eq. 4 could be rewritten as follows: (cid:48) f (cid:48) k =
T1(cid:88)
...
Tm(cid:88) t1=1 tm=1
At1,t2,...,tm ( m (cid:89)
VT j,tj
U (cid:48) j,k) j=1 (5) (cid:48) j,k ∈ Rdj denotes the k-th column where Vj,tj ∈ Rdj denotes the tj-th channel of input Vj and U of Uj, At1,t2,...,tm denotes an element in tensor A with indices [t1, t2, ..., tm]. The intermediate result f
K]. Then the multi-linear joint representation k, ..., f where f ∈ Rc and P ∈ RK×c. For convenience, we deﬁne the multi-linear attention is f = PT f network as a function of m multi-channel inputs parameterized by a multi-linear attention map as follows:
∈ RK consists of K elements [f 1, ..., f (cid:48) (cid:48) (cid:48) (cid:48) (cid:48) f = MAN({Vj|j ∈ [1, m]}; A) (6)
Combinational Additive Attention Map. We argue that multi-linear attention map should have several properties: (1) The attention logits are calculated by leveraging the interaction information between different modalities. Each modality is in a symmetric (equal) position. (2) Following the main idea of BAN, the attention map can be calculated with Hadamard product and tensor operations similarly. However, since the absolute values of most multiplicative factors are between 0 and 1, directly employing softmax function may result in even attention distributions when the attention map is large. (3) As for the processing of multimodal sequential features, the time interval (relative position) of different modalities should be considered. In general, locally close moments are more 3
relevant. Following the idea of Transformer, most positional encoding methods can be integrated with the inner product of two vectors. Therefore, we utilize simply combinational addition operation (CAO) to replace the Hadamard product. The attention map A is deﬁned as follows:
A := softmax(A), At1,t2,...,tm = ( m (cid:88) m (cid:88) (VT q,tq
U (cid:48)(cid:48) q )(U (cid:48)(cid:48)T j Vj,tj )) (7) j=1 q=1,(cid:54)=j (cid:48) (cid:48)(cid:48) j ∈ Rdj ×K
, A ∈ R(cid:81)m j=1 Tj and the softmax function is applied element-wisely along all the where U (cid:48)(cid:48)T dimensions. In Eq. 7, the number of additive items (VT j Vj,tj ) is positively correlated q )(U with the number of modalities, resulting in the bigger difference between elements and sparser attention distributions. Besides, the combinational addition operation has satisfactory compatibility with local sequential constraints introduced in the subsequent section. q,tq
U (cid:48)(cid:48)
Local Sequential Constraints (LSC) for Multimodal Temporal Features. When the number of modalities increases, the attention tensor A becomes larger and the probability distributions are much denser. As mentioned in the previous section, the features of temporal proximity are more relevant.
So we devise an attenuation function related to the time steps of associated modalities. Suppose the number of time steps is T , we assign each time step a T s denotes the number of chunks and s adjusts the scale), which consists of e and −e. For example, the temporal vectors for the 1-st and 2-nd temporal chunks are [e, −e, −e, ..., −e] and [e, e, −e, ..., −e], the inner product result is ( T s − 1) × e2. Similarly, the inner product result between any t1 and t2 time steps are ( T s − |t2 − t1|) × e2, which ﬁts the property of attenuation function. In practice, we simply rewrite the Eq. 7, concatenating the temporal vectors to the features of different modalities: s -dimensional vector ( T
At1,t2,...,tm = ( m (cid:88) m (cid:88) (cid:104)
VT q,tq
U (cid:48)(cid:48) q , Etq (cid:105)(cid:104)
VT j,tj
U (cid:48)(cid:48) j , Etj (cid:105)T
) (8) j=1 q=1,(cid:54)=j
To balance the weights of modality association and local constraints, we adjust the value of e.
Hierarchical Approximation Decomposition. Considering the fact that the size of combinational additive interaction tensor A scales exponentially to the number of modalities. We devise a hierarchi-cal decomposition mechanism to cope with this problem. Without restrictions, A can be decomposed with CP algorithm:
A = m (cid:79) j=1
Bj, At1,t2,...,tm = 1T ( m (cid:89)
Bj,tj ) j=1 (9) where 1 ∈ RH , Bj ∈ RH×Tj and H denotes the rank value, ⊗ is the deﬁned outer product operation for multiple matrices, which is a bit different from traditional vectorial operation. The right equation is the detailed calculation for each element, where Bj,tj ∈ RH denotes the tj-th channel of Bj. We directly substitude the Eq. 8 into Eq. 5 as follows2: (cid:48) f
=
T1(cid:88)
...
Tm(cid:88) (1T ( t1=1 tm=1 m (cid:89) j=1 m (cid:89)
Bj,tj ))( j=1
VT j,tj
U (cid:48) j) = 1T ( m (cid:89) j=1
BjVT j U (cid:48) j) (10) where we obtain a simple conclusion. Bj ∈ RH×Tj and U projection weights for different dimensions, respectively. (cid:48) j ∈ Rdj ×K can be treated as two linear
Addition Random Features. Although Eq. 9 is quite simple and pretty, the high-order tensor A cannot be decomposed directly due to the non-linearity caused by softmax function. Thus, it is necessary to ﬁnd a substitute approximate solution. Inspired by the Performer [3], we propose the addition random features mechanism, which can be treated as an extension of vanilla random features mechanism for more than two modalities. The detailed process is shown as follows1: 2The detailed proof and complete algorithm process are shown in the appendix. 4
SM({vj|j ∈ [1, m]}) = E w∼N (0,I
K

 m (cid:89) j=1 (cid:48) ) (cid:18) exp w(cid:62)vj −
 (cid:19)
 (cid:107)vj(cid:107)2 2 (11) (cid:80)m k=1,(cid:54)=j vT where vj denotes the feature vector of j-th modality, SM({vj|j ∈ [1, m]}) is equal to exp((cid:80)m k vj), E and N (0, IK(cid:48) ) denote expectation and sampling distribution. The j=1 detailed derivations can be found in the supplementary materials. Eq. 10 is employed to calculate
At1,t2,...,tm directly. Bj in Eq. 8 can be obtained by project the features of j-th modality directly with ARF mechanism. In this way, the number of random features is equal to the rank value H of A.
Note that softmax function consists of exponential operation and sum normalization, we need the value of normalized denominator. In practice, we directly employ {Bj|j = [1, m]} to calculate it1: (cid:88)
A = (cid:88) m (cid:89) ( j=1
Bj · 1) (12) where (cid:80) denotes the sum of all elements in the tensor and 1 ∈ RTj . We could keep (cid:80) A and utilize the integration result f to divide it. Note that, during the calculation of ARF for softmax function, a reduction factor 1
H is introduced into both numerator and denominator, thus it directly cancel out.
We could treat the expectation operation in Eq. 10 as the summation operation. The core module of
MAN is shown in Fig. 1. (cid:48)
Multi-head Multi-linear Attention. Following the main idea of Transformer [30] that projecting the features into multiple representation spaces could capture more information, we also adopt the multi-head strategy in the multi-linear attention. In practice, we add multiple linear mappings to the (cid:48) input features before fusing them (i.e. multiple U j |j ∈ [1, m]} ). The results of all the heads are concatenated. The calculation follows the process described above. j for {VT j and U j, VT j U j U (cid:48)(cid:48) (cid:48)(cid:48) (cid:48)
Stacked Multi-linear Attention. The residual connection is proved to be effective in most structures
[8]. Therefore, such strategy can also be introduced into multi-linear attention network. Speciﬁcally,
Eq. 6 is modiﬁed as: n+1 = MAN(f l f l n, {Vj|j ∈ [1, m], j (cid:54)= l}; A) · 1T + f l n (13) g =K=c, g denotes the number of heads). n ∈ Rdl×Tl denotes the n-th output of l-th modality and f l where 1 ∈ RTl , f l g =...= dm g = d2 d1
Complexity. Without approximation decomposition, the computational complexity increases expo-nentially with respect to the number of modalities. Concretely, we need (cid:81)m j=1 Tj low-rank multimodal pooling operations, leading to the unfriendly complexity O((cid:81)m j=1 Tj). While the complexity scales linearly O((cid:80)m j=1 Tj) with the number of modalities by leveraging approximation decomposition. 0 = Vl (if 3 Experiments 3.1 Datasets
We evaluate MAN on three challenging tasks, multimodal sentiment analysis, multimodal speaker traits recognition, and multimodal video retrieval. In this section, we provide a brief introduction of the datasets (CMU-MOSI [41] for multimodal sentiment analysis, POM [25] for multimodal speaker traits recognition, MSR-VTT [33] and LSMDC [28] for multimodal video retrieval).
CMU-MOSI: The CMU-MOSI dataset is a collection of 93 opinion videos from YouTube movie reviews. Each video consists of multiple opinion segments (2199 segments in total) and each segment is annotated with the score in the range [−3, 3], where −3 and 3 indicate highly negative and positive.
There are 1284 segments in the training set, 229 in the validation set, and 686 in the test set.
POM: POM is a multimodal speaker traits recognition dataset made up of 903 movie review videos.
Each video is annotated for various personality and speaker traits, speciﬁcally: Conﬁdent (con), 5
Model \Metric
MV-LSTM [27]
TFN [38]
MARN [40]
MFN [39]
RAVEN [31]
LMF [19]
MAN (Non-Transformer)
MulT [29]
MAN (Transformer)
BA 73.9 73.9 77.1 77.4 78.0 76.4 79.4 83.0 82.7
F1 74.0 73.4 77.0 77.3 – 75.7 79.3 82.8 83.0
MAE 1.019 1.040 0.968 0.965 0.915 0.912 0.894 0.870 0.866
Corr 0.601 0.633 0.625 0.632 0.691 0.668 0.697 0.698 0.707
MA 33.2 32.1 34.7 34.1 – 32.8 37.8 40.0 40.5
Table 1: MAN achieves superior performances over baseline models for CMU-MOSI dataset. We report BA (binary accuracy), F1, Corr (Pearson Correlation Coefﬁcient), and MA (Multi-class accuracy, all higher is better), MAE (Mean-absolute Error, lower is better).
Model \ Trait
MV-LSTM [27]
TFN [38]
MARN [40]
MFN [39]
LMF [29]
MAN (Non-Transformer)
MulT [29]
MAN (Tranformer)
Model \ Trait
MV-LSTM [27]
TFN [38]
MARN [40]
MFN [39]
LMF [29]
MAN (Non-Transformer)
MulT [29]
MAN (Transformer)
Viv
Voi
Pas
Cre
Exp
Dom 28.6 31.0 33.0 35.5 35.9 38.3 34.5 38.9
Tru 28.1 31.5
-37.4 34.8 37.9 36.5 37.9
Rel 25.6 24.6 31.5 34.5 34.5 37.0 37.4 37.9
Tho 32.5 25.6
-36.9 35.9 38.7 36.9 39.9
Ner
Ent
Con
MA7 MA7 MA7 MA7 MA7 MA7 MA7 MA7 32.5 34.5 29.6 25.6 27.6 34.5 29.1 24.1
---29.1 36.0 41.9 37.9 34.5 37.8 39.6 36.5 35.9 40.5 43.6 38.9 37.5 37.9 38.9 39.4 34.5 40.4 40.9 44.8 37.9
Hum
Res
Per
Out
MA5
MA5 MA5 MA5 MA5 MA5 MA5 MA7 38.9 26.1 38.4 33.0 33.0 27.6 37.4 30.5 44.8 31.0
-36.9 47.3 34.0 46.8 38.4 45.8 34.9 44.8 35.5 49.8 37.2 47.4 39.6 43.3 33.5 43.3 41.4 51.2 38.3 49.1 41.9 50.7 35.5 52.2 53.2 53.2 55.2 54.2 55.2 52.2 38.9
-57.1 54.2 59.5 60.6 61.4 42.4 42.4 47.3 47.8 43.5 47.0 46.3 48.3 37.9 33.0
-47.3 42.7 46.5 49.3 44.8
Table 2: MAN achieves superior performances over baseline models in POM dataset (multimodal personality traits recognition). MA(5,7) denotes multi-class accuracy for (5,7) classes.
Passionate (Pas), Voice Pleasant (Voi), Dominant (Dom), Credible (Cre), Vivid (Viv), Expertise (Exp), Entertaining (Ent), Reserved (Res), Trusting (Tru), Relaxed (Rel), Outgoing (Out), Thorough (Tho), Nervous (Ner), Persuasive (Per) and Humorous (Hum). The training, validation, and test set distributions are approximately 600, 100, and 203, respectively.
MSR-VTT: MSR-VTT is composed of 10K YouTube videos, collected using 257 queries from a commercial video search engine. Each video is 10 to 30s long, and is paired with 20 natural sentences describing it, obtained from Amazon Mechanical Turk Workers. We report results on the train/test splits introduced in [34] that uses 9000 videos for training and 1000 for testing.
LSMDC: It contains 118081 short video clips (about 45s) extracted from 202 movies. Each clip is annotated with a caption, extracted from either the movie script or the audio description. The test set is composed of 1000 videos, from movies not presented in the training set. 6
Model \Metric w/o CAO w/o LSC w/o HAD
MAN (Non-Transformer) w/o CAO w/o LSC w/o HAD
MAN (Transformer)
BA 78.4 78.4 79.4 79.4 81.5 81.4 82.6 82.7
F1 78.3 78.3 79.3 79.3 81.6 81.3 82.8 83.0
MAE 0.924 0.928 0.901 0.894 0.880 0.883 0.872 0.866
Corr 0.668 0.673 0.693 0.697 0.692 0.698 0.707 0.707
MA 35.2 36.5 38.1 37.8 38.9 39.8 40.5 40.5
Table 3: Ablation study on CMU-MOSI dataset.
Metric \Model
FLOPs w/o CAO 8.31 × 106 w/o HAD 10.23 × 106 w/o LSC 3.07 × 105
MAN 3.87 × 105
Table 4: Computational complexity of different variants on CMU-MOSI, note that we only consider the FLOPs of one-layer integration module. 3.2 Experiments for Multimodal Sentiment Analysis and Speaker Traits Recognition
Data Preprocessing: Each dataset (CMU-MOSI, POM) consists of three modalities, including textual, visual, and audio modalities. For textual features, we employ the pre-trained 300-dimensional
Glove embeddings [24]. For visual features, we utilize Facet [11] to indicate 35 facial action units, which records facial muscle movement for representing the basic and advanced emotions. For audio features, we use COVAREP [4] acoustic analysis framework. To align the different modalities along the temporal dimension, we perform word alignment with P2FA [37], which aligns the modalities at the word granularity.
Experimental Details: Transformer-based and Non-Transformer-based methods both have their practical meanings of existence. For example, without enough labeled data, non-transformer-based methods may be an optimal solution due to less computational complexity. Henceforce, we combine
MAN with both Transformer-based and Non-Transformer-based backbones and make corresponding comparisons with existing methods. We refer to [29] and [19] for our implementation of backbones, respectively. Note that we do not remove the original structures of [29] and [19], but add MAN as an extra module. We fuse their results for the tasks. The hyperparameters of MAN include Adam learning rate 0.001, the structure of integration network (1 layer of integration block, with hidden sizes of 40, number of heads 10, number of random features 24). “Hidden size” denotes the common size of d1, d2,...,dm. We divide all the time steps into 4 chunks and apply local sequential constraints.
Furthermore, for the speciﬁc hyperparameters of backbones, we use similar values as original papers.
Results: Table 1 presents the overall comparison of MAN and existing methods on CMU-MOSI dataset. As for the Non-Transformer-based methods, we could observe that MV-LSTM, MFN,
MARN, RAVEN perform worse than MAN as they pay more attention to multimodal interaction and representation learning, and ignore the importance of subsequent multimodal integration. Besides,
MAN achieves the best performances on all the metrics among the existing multimodal integration methods TFN, LMF and gains a large margin. Since TFN, LMF neglect the ﬁne-grained temporal interaction which includes rich structured information for multimodal modeling. Particularly, the performance of MAN decreases the MAE from 0.912 to 0.894 compared to the best counterparts. As for the Transformer-based methods, MAN also performs better than MulT on all the metrics, which demonstrates that, to some extent, subsequent multimodal joint representation learning could be integrated with the early multimodal interaction, even though only multimodal interaction has been able to achieve competitive performances. In general, the best performances of MAN attribute to the advanced hierarchical approximation decomposition which imposes ﬁne-grained temporal correlation with acceptable computational complexity, as well as the addition random features mechanism that provides support for the approximation decomposition. Table 2 shows the experimental results of different methods on speaker traits recognition dataset POM, where we report the multi-class accuracy of all the traits. The similar observation could be found from the table, MAN achieves competitive performances compared with both Transformer-based and Non-Transformer-based methods on most of the traits. The only difference is that the metrics of Transformer-based and Non-Transformer-based 7
methods are close. Therefore, even the Transformer has great potential in multimodal learning, its ability may be limited by the number of training samples. We argue that Non-Transformer-based methods still have their meanings in the real-world applications.
Figure 2: The evaluation of different numbers of random features, where we report the values of Corr and MA on CMU-MOSI.
Ablation Study: We set some control experiments on CMU-MOSI to verify the effectiveness of
MAN and the results are shown in Table 3 and Table 4, where “w/o CAO” denotes the model without combinational addition operation in Eq. 7, we simply employ the continued Hadamard product and do not decompose the attention map, in this way, the local constraints also cannot be utilized. “w/o HAD” denotes the model without hierarchical approximation decomposition, but with other contributions. “w/o LSC” denotes the model with all the contributions except for local sequential constraints. We could observe that “w/o HAD” performs similarly to MAN with much more computational complexity, since “w/o HAD” needs to generate a high-order attention map with exponential complexity. Besides, MAN and “w/o HAD” perform much better than “w/o CAO”, since
“w/o CAO” generates denser attention distributions and cannot capture really important information.
Additionally, by comparing the metrics of “w/o LSC” and MAN, we argue that the local constraints have impacts on better capturing the correlation between multimodal sequential features. Since we utilize addition random features mechanism to approximate the softmax function, the number of random features should be considered. We examine the performances of MAN (Non-Transformer) on CMU-MOSI with different values of H. As shown in Fig. 2, when the value of H is big enough,
MAN achieves competitive performances of Corr and MA close to “w/o HAD”. 3.3 Experiments for Multimodal Video Retrieval
Data Preprocessing: The videos contain abundant multimodal information. Thus, we use multiple pre-trained models for extracting features. Concretely, we utilize following seven experts: Motion embeddings are extracted from S3D [32] trained on the kinetics dataset. Scene embeddings are extracted with DenseNet-161 [10] trained on the Places365 dataset [42]. OCR embeddings are extracted in three stages. First, the pixel link text detection model is used to detect the overlaid text.
Then, the detected boxes are passed through the text recognition model. Finally, each character sequence is encoded using a word2vec embedding. Audio embeddings are obtained with a VGGish model, trained on the YouTube-8m dataset. Speech features are extracted using the Google Cloud speech API, to extract word tokens from the audio stream, which are then encoded via pre-trained word2vec embeddings [22]. Face features are extracted by ResNet-50 [8] trained for face classiﬁ-cation on the VGGFace2 dataset. Appearance features are extracted from the ﬁnal global average pooling layer of SENet-154 [9] trained on ImageNet.
Experimental Details: We also combine MAN with Transformer-based and Non-Transformer-based backbones. For MAN (Non-Transformer), each word of the text query is encoded with pre-trained word2vec embeddings and then passed through the GPT model. For MAN (Transformer), we adopt the basic structure of [6] and train the word vectors from scratch. We refer to [18] and [6] for the implementation of backbones and keep the original structures, only adding MAN as an extra module. We further fuse their results. The hyperparameters of MAN include Adam learning rate 5 × 10−5, which we decay by a multiplicative factor 0.95 every 1000 optimization steps, the structure of integration network (1 layer of integration block, with hidden size 512, number of heads 8, number of random features 512). “Hidden size” denotes the common size of d1, d2,...,dm. We divide the time steps into 10 chunks and employ local sequential constraints. We also use the same hyperparameters of speciﬁc backbones as the original papers. 8
Text −→ Video
Video −→ Text
Model \Metric
JSFusion [34]
HT [21]
CE [18]
MAN (Non-Transformer)
MMT [6]
MAN (Transformer)
R@1↑ R@5↑ R@10↑ MdR↓ R@1↑ R@5↑ R@10↑ MdR↓ 10.2 14.9 20.9 21.4 24.6 24.1
--50.3 51.3 56.0 57.1
--64.0 65.4 67.8 68.5 31.2 40.2 48.8 51.0 54.0 55.7 43.2 52.8 62.4 63.5 67.1 68.1
--20.6 21.6 24.4 24.4
--5.3 5 4 4 13 9 6 5 4 4
Table 5: Retrieval performances on the MSR-VTT dataset.
Method \Metric
CT-SAN [35]
JSFusion [34]
CCA [16] (rep. by [20])
MEE [20]
CE [18]
MAN (Non-Transformer)
MMT [6]
MAN (Transformer)
Text −→ Video
Video −→ Text
R@1↑ R@5↑ R@10↑ MdR↓ R@1↑ R@5↑ R@10↑ MdR↓ 5.1 9.1 7.5 10.1 11.2 11.8 13.2 13.6 16.3 21.2 21.7 25.6 26.9 27.9 29.2 30.0 25.2 34.1 31.0 34.6 34.8 36.0 38.8 39.4 46 36 33 27 25.3 24 21 20
------12.1 12.5
------29.3 29.8
------37.9 38.7
------22.5 22
Table 6: Retrieval performances on the LSMDC dataset.
Results: We report the evaluation results of MAN and the competing text-video retrieval methods on
MSR-VTT (Table 5) and LSMDC (Table 6). In order to be fair, we still divide all the methods into two categories, Non-Transformer-based and Transformer-based methods. As for the former, MAN outperforms existing state-of-the-art methods in all the metrics on both MSR-VTT and LSMDC.
Beneﬁting from the ﬁne-grained temporal correlation of different modalities, the ﬁnal representation contains not only the global multimodal information, but also local temporal interaction information.
When we replace the backbone with Transformer, the metrics of MMT and MAN (Transformer) are very close, in other words, the effect of MAN is diminished. It may be because that the sample number of MSR-VTT and LSMDC is sufﬁcient to stimulate the potential of Transformer. 4