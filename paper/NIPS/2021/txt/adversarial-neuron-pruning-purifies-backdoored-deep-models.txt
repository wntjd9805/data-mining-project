Abstract
As deep neural networks (DNNs) are growing larger, their requirements for compu-tational resources become huge, which makes outsourcing training more popular.
Training in a third-party platform, however, may introduce potential risks that a malicious trainer will return backdoored DNNs, which behave normally on clean samples but output targeted misclassiﬁcations whenever a trigger appears at the test time. Without any knowledge of the trigger, it is difﬁcult to distinguish or recover benign DNNs from backdoored ones. In this paper, we ﬁrst identify an unexpected sensitivity of backdoored DNNs, that is, they are much easier to col-lapse and tend to predict the target label on clean samples when their neurons are adversarially perturbed. Based on these observations, we propose a novel model repairing method, termed Adversarial Neuron Pruning (ANP), which prunes some sensitive neurons to purify the injected backdoor. Experiments show, even with only an extremely small amount of clean data (e.g., 1%), ANP effectively removes the injected backdoor without causing obvious performance degradation. Our code is available at https://github.com/csdongxian/ANP_backdoor. 1

Introduction
In recent years, deep neural networks (DNNs) achieve satisfactory performance in many tasks, including computer vision [15], speech recognition [45], and gaming agents [36]. However, their success heavily relies on a large amount of computation and data, forcing researchers to outsource the training in “Machine Learning as a Service” (MLaaS) platforms or download pretrained models from the Internet, which brings potential risks of training-time attacks [17, 35, 2]. Among them, backdoor attack [14, 6, 42] is remarkably dangerous because it stealthily builds a strong relationship between a trigger pattern and a target label inside DNNs by poisoning a small proportion of the training data.
As a result, the returned model always behaves normally on the clean data but is controlled to make target misclassiﬁcation by presenting the trigger pattern such as a speciﬁc single-pixel [41] or a black-white checkerboard [14] at the test time.
Since DNNs are deployed in many real-world and safety-critical applications, it is urgent to defend against backdoor attacks. While there are many defense methods during training [38, 41, 8, 29, 20], this work focuses on a more realistic scenario in outsourcing training that repairs models after training. In particular, the defenders try to remove the injected backdoor without access to the model training process. Without knowledge of the trigger pattern, previous methods only achieve limited robustness [7, 24, 22]. Some works try to reconstruct the trigger [43, 5, 33, 50, 44], however, the
∗Work was done as an internship at Peking University when he was a student at Tsinghua University. Now, he is a Post-doc at the University of Tokyo.
†Corresponding author: Yisen Wang (yisen.wang@pku.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
trigger pattern in brand-new attacks becomes natural [26], invisible [49], and dynamic [30], leading to the reconstruction infeasible. Different from them, in this paper, we turn to explore whether we can successfully remove the injected backdoor even without knowing the trigger pattern.
Usually, the adversary perturbs inputs to cause misclassiﬁcation, e.g., attaching triggers for back-doored models or adding adversarial perturbations. Parallel to this, we are also able to cause misclassiﬁcation by perturbing neurons of DNNs [47]. For any neuron inside DNN, we could per-turb its weight and bias by multiplying a relatively small number, to change its output. Similar to adversarial perturbations, we can optimize the neuron perturbations to increase its classiﬁcation loss.
Surprisingly, we ﬁnd that, within the same perturbation budget, backdoored DNNs are much easier to collapse and prone to output the target label than normal DNNs even without the presence of the trigger. That is, the neurons that are sensitive to adversarial neuron perturbation are strongly related to the injected backdoor. Motivated by this, we propose a novel model repairing method, named
Adversarial Neuron Pruning (ANP), which prunes most sensitive neurons under the adversarial neuron perturbation. Since the number of neurons is much smaller than weight parameters, e.g., only 4810 neurons for ResNet-18 while 11M parameters for the same model, our method can work well only based on 1% of clean data. Our main contributions are summarized as follows:
• We ﬁnd that through adversarially perturbing neurons, backdoored DNNs can present backdoor behaviors even without the presence of the trigger patterns and are much easier to output misclassiﬁcation than normal DNNs.
• To defend backdoor attacks, we propose a simple yet effective model repairing method, Ad-versarial Neuron Pruning (ANP), which prunes the most sensitive neurons under adversarial neuron perturbations without ﬁne-tuning.
• Extensive experiments demonstrate that ANP consistently provides state-of-the-art defense performance against various backdoor attacks, even using an extremely small amount of clean data. 2