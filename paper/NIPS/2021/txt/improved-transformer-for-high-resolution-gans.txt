Abstract
Attention-based models, exempliﬁed by the Transformer, can effectively model long range dependency, but suffer from the quadratic complexity of self-attention operation, making them difﬁcult to be adopted for high-resolution image generation based on Generative Adversarial Networks (GANs). In this paper, we introduce two key ingredients to Transformer to address this challenge. First, in low-resolution stages of the generative process, standard global self-attention is replaced with the proposed multi-axis blocked self-attention which allows efﬁcient mixing of local and global attention. Second, in high-resolution stages, we drop self-attention while only keeping multi-layer perceptrons reminiscent of the implicit neural function.
To further improve the performance, we introduce an additional self-modulation component based on cross-attention. The resulting model, denoted as HiT, has a nearly linear computational complexity with respect to the image size and thus directly scales to synthesizing high deﬁnition images. We show in the experiments that the proposed HiT achieves state-of-the-art FID scores of 30.83 and 2.95 on unconditional ImageNet 128 × 128 and FFHQ 256 × 256, respectively, with a reasonable throughput. We believe the proposed HiT is an important milestone for generators in GANs which are completely free of convolutions. Our code is made publicly available at https://github.com/google-research/hit-gan. 1

Introduction
Attention-based models demonstrate notable learning capabilities for both encoder-based and decoder-based architectures [66, 69] due to their self-attention operations which can capture long-range depen-dencies in data. Recently, Vision Transformer [12], one of the most powerful attention-based models, has achieved a great success on encoder-based vision tasks, speciﬁcally image classiﬁcation [12, 60], segmentation [37, 64], and vision-language modeling [46]. However, applying the Transformer to image generation based on Generative Adversarial Networks (GANs) is still an open problem.
The main challenge of adopting the Transformer as a decoder/generator lies in two aspects. On one hand, the quadratic scaling problem brought by the self-attention operation becomes even worse when generating pixel-level details for high-resolution images. For example, synthesizing a high deﬁnition image with the resolution of 1024 × 1024 leads to a sequence containing around one million pixels in the ﬁnal stage, which is unaffordable for the standard self-attention mechanism. On the other hand, generating images from noise inputs poses a higher demand for spatial coherency in structure, color, and texture than discriminative tasks, and hence a more powerful yet efﬁcient self-attention mechanism is desired for decoding feature representations from inputs.
In view of these two key challenges, we propose a novel Transformer-based decoder/generator in
GANs for high-resolution image generation, denoted as HiT. HiT employs a hierarchical structure of Transformers and divides the generative process into low-resolution and high-resolution stages, focusing on feature decoding and pixel-level generating, respectively. Speciﬁcally, its low-resolution
∗This work was done while Long Zhao was a student researcher at the Google Brain team. Correspondence to: Long Zhao (lz311@rutgers.edu) and Han Zhang (zhanghan@google.com). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
stages follow the design of Nested Transformers [73] but enhanced by the proposed multi-axis blocked self-attention to better capture global information. Assuming that spatial features are well decoded after low-resolution stages, in high-resolution stages, we drop all self-attention operations in order to handle extremely long sequences for high deﬁnition image generation. The resulting high-resolution stages of HiT are built by multi-layer perceptrons (MLPs) which have a linear complexity with respect to the sequence length. Note that this design aligns with the recent ﬁndings [58, 59] that pure MLPs manage to learn favorable features for images, but it simply reduces to an implicit neural function [41, 42, 43] in the case of generative modeling. To further improve the performance, we present an additional cross-attention module that acts as a form of self-modulation [8]. In summary, this paper makes the following contributions:
• We propose HiT, a Transformer-based generator for high-ﬁdelity image generation. Standard self-attention operations are removed in the high-resolution stages of HiT, reducing them to an implicit neural function. The resulting architecture easily scales to high deﬁnition image synthe-sis (with the resolution of 1024 × 1024) and has a comparable throughput to StyleGAN2 [28].
• To tame the quadratic complexity and enhance the representation capability of self-attention operation in low-resolution stages, we present a new form of sparse self-attention operation, namely multi-axis blocked self-attention. It captures local and global dependencies within non-overlapping image blocks in parallel by attending to a single axis of the input tensor at a time, each of which uses a half of attention heads. The proposed multi-axis blocked self-attention is efﬁcient, simple to implement, and yields better performance than other popular self-attention operations [37, 62, 73] working on image blocks for generative tasks.
• In addition, we introduce a cross-attention module performing attention between the input and intermediate features. This module re-weights intermediate features of the model as a function of the input, playing a role as self-modulation [8], and provides important global information to high-resolution stages where self-attention operations are absent.
• The proposed HiT obtains competitive FID [16] scores of 30.83 and 2.95 on unconditional
ImageNet [49] 128 × 128 and FFHQ [28] 256 × 256, respectively, highly reducing the gap between ConvNet-based GANs and Transformer-based ones. We also show that HiT not only works for GANs but also can serve as a general decoder for other models such as VQ-VAE [61].
Moreover, we observe that the proposed HiT can obtain more performance improvement from regularization than its ConvNet-based counterparts. To the best of our knowledge, these are the best reported scores for an image generator that is completely free of convolutions, which is an important milestone towards adopting Transformers for high-resolution generative modeling. 2