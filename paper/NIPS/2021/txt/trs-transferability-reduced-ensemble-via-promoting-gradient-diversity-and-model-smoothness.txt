Abstract
Adversarial Transferability is an intriguing property – adversarial perturbation crafted against one model is also effective against another model, while these mod-els are from different model families or training processes. To better protect ML systems against adversarial attacks, several questions are raised: what are the sufﬁ-cient conditions for adversarial transferability and how to bound it? Is there a way to reduce the adversarial transferability in order to improve the robustness of an ensemble ML model? To answer these questions, in this work we ﬁrst theoretically analyze and outline sufﬁcient conditions for adversarial transferability between models; then propose a practical algorithm to reduce the transferability between base models within an ensemble to improve its robustness. Our theoretical analysis shows that only promoting the orthogonality between gradients of base models is not enough to ensure low transferability; in the meantime, the model smoothness is an important factor to control the transferability. We also provide the lower and upper bounds of adversarial transferability under certain conditions. Inspired by our theoretical analysis, we propose an effective Transferability Reduced Smooth (TRS) ensemble training strategy to train a robust ensemble with low transferability by enforcing both gradient orthogonality and model smoothness between base mod-els. We conduct extensive experiments on TRS and compare with 6 state-of-the-art ensemble baselines against 8 whitebox attacks on different datasets, demonstrating that the proposed TRS outperforms all baselines signiﬁcantly. 1

Introduction
Machine learning systems, especially those based on deep neural networks (DNNs), have been widely applied in numerous applications [27, 18, 46, 10]. However, recent studies show that DNNs are vulnerable to adversarial examples, which are able to mislead DNNs by adding small magnitude of perturbations to the original instances [47, 17, 54, 52]. Several attack strategies have been proposed so far to generate such adversarial examples in both digital and physical environments [36, 32, 51, 53, 15, 28]. Intriguingly, though most attacks require access to the target models (whitebox attacks), several studies show that adversarial examples generated against one model are able to transferably
∗The authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
attack another target model with high probability, giving rise to blackbox attacks [39, 41, 31, 30, 57].
This property of adversarial transferability poses great threat to DNNs.
Some work have been conducted to understand adversarial transferability [48, 33, 12]. However, a rigorous theoretical analysis or explanation for transferability is still lacking in the literature. In addition, although developing robust ensemble models to limit transferability shows great potential towards practical robust learning systems, only empirical observations have been made in this line of research [38, 23, 56]. Can we deepen our theoretical understanding on transferability? Can we take advantage of rigorous theoretical understanding to reduce the adversarial transferability and therefore generate robust ensemble ML models?
In this paper, we focus on these two questions. From the theoretical side, we are interested in the sufﬁcient conditions under which the adversarial transferabil-ity can be lower bounded and upper bounded. Our theoretical arguments provides the ﬁrst theoretical interpretation for the sufﬁcient conditions of trans-ferability. Intuitively, as illustrated in Figure 1, we show that the commonly used gradient orthogonality (low cosine similarity) between learning models [12] cannot directly imply low adversarial transferability; on the other hand, orthogonal and smoothed models would limit the transferability. In particular, we prove that the gradient similarity and model smoothness are the key factors that both contribute to the adversarial transferability, and smooth models with orthogonal gradients can guarantee low transferability.
Figure 1: An illustration of the relationship be-tween adversarial transferability, gradient orthog-onality, and model smoothness. (a) Gradient or-thogonality alone cannot minimize transferabil-ity as the decision boundaries between two clas-siﬁers can be arbitrarily close yet orthogonal al-most everywhere; (b) Gradient orthogonality with model smoothness provides a stronger guarantee on model diversity, as our theorems will show.
Under an empirical lens, inspired by our theoretical analysis, we propose a simple yet effective approach,
Transferability Reduced Smooth (TRS) ensemble to limit adversarial transferability between base models within an ensemble and therefore improve its robustness. In particular, we reduce the loss gradient similarity between models as well as enforce the smoothness of models to introduce global model orthogonality.
We conduct extensive experiments to evaluate TRS in terms of the model robustness against different strong white-box and blackbox attacks following the robustness evaluation procedures [5, 6, 49], as well as its ability to limit transferability across the base models. We compare the proposed
TRS with existing state-of-the-art baseline ensemble approaches such as ADP [38], GAL [23], and DVERGE [56] on MNIST, CIFAR-10, and CIFAR-100 datasets, and we show that (1) TRS achieves the state-of-the-art ensemble robustness, outperforming others by a large margin; (2) TRS achieves efﬁcient training; (3) TRS effectively reduces the transferability among base models within an ensemble which indicates its robustness against whitebox and blackbox attacks; (4) Both loss terms in TRS contribute to the ensemble robustness by constraining different sufﬁcient conditions of adversarial transferability.
Contributions. In this paper, we make the ﬁrst attempt towards theoretical understanding of adver-sarial transferability, and provide practical approach for developing robust ML ensembles. (1) We provide a general theoretical analysis framework for adversarial transferability. We prove the lower and upper bounds of adversarial transferability. Both bounds show that the gradient similarity and model smoothness are the key factors contributing to the adversarial transferability, and smooth models with orthogonal gradients can guarantee low transferability. (2) We propose a simple yet effective approach TRS to train a robust ensemble by jointly reducing the loss gradient similarity between base models and enforcing the model smoothness. The code is publicly available2. (3) We conduct extensive experiments to evaluate TRS in terms of model robustness under different attack settings, showing that TRS achieves the state-of-the-art ensemble robustness and outper-forms other baselines by a large margin. We also conduct ablation studies to further understand the contribution of different loss terms and verify our theoretical ﬁndings. 2https://github.com/AI-secure/Transferability-Reduced-Smooth-Ensemble 2