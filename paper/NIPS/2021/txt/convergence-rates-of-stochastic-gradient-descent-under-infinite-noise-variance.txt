Abstract
Recent studies have provided both empirical and theoretical evidence illustrating that heavy tails can emerge in stochastic gradient descent (SGD) in various scenar-ios. Such heavy tails potentially result in iterates with diverging variance, which hinders the use of conventional convergence analysis techniques that rely on the existence of the second-order moments. In this paper, we provide convergence guarantees for SGD under a state-dependent and heavy-tailed noise with a poten-tially inﬁnite variance, for a class of strongly convex objectives. In the case where the p-th moment of the noise exists for some p ∈ [1, 2), we ﬁrst identify a condition on the Hessian, coined ‘p-positive (semi-)deﬁniteness’, that leads to an interesting interpolation between the positive semi-deﬁnite cone (p = 2) and the cone of diagonally dominant matrices with non-negative diagonal entries (p = 1). Under this condition, we provide a convergence rate for the distance to the global optimum in Lp. Furthermore, we provide a generalized central limit theorem, which shows that the properly scaled Polyak-Ruppert averaging converges weakly to a multi-variate α-stable random vector. Our results indicate that even under heavy-tailed noise with inﬁnite variance, SGD can converge to the global optimum without necessitating any modiﬁcation neither to the loss function nor to the algorithm itself, as typically required in robust statistics. We demonstrate the implications of our results over misspeciﬁed models, in the presence of heavy-tailed data. 1

Introduction
We consider the unconstrained minimization problem minimize x∈Rn f (x), (1.1) using the stochastic gradient descent (SGD) algorithm. Initialized at x0 ∈ Rn, the SGD algorithm is given by the iterations, xt+1 = xt − γt+1 (cid:0)∇f (xt) + ξt+1(xt)(cid:1), t = 0, 1, 2, ... (1.2) where {γt}t∈N+ denotes the step-size sequence, and {ξt}t∈N+ is a martingale difference sequence adapted to a ﬁltration {Ft}t∈N, characterizing the noise in the gradient (the sequence {xt}t∈N is also adapted to the same ﬁltration, if we assume x0 is F0-measurable). Our focus is on the case where the noise is state dependent, and its variance is inﬁnite, i.e., E(cid:2)(cid:107)ξt(cid:107)2
∗Work partially conducted while afﬁliated with the Vector Institute. (cid:3) = ∞. 2 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Many problems in modern statistical learning can be written in the form (1.1), where f (x) typically corresponds to the population risk, that is, f (x) := Ez∼ν[(cid:96)(x, z)] for a given loss function (cid:96) and an unknown data distribution ν. In practice, one observes independent and identically distributed (i.i.d.) samples zi ∼ ν for i ∈ [n], and estimates the population gradient ∇f (x) with a noisy gradient at each iteration, which is based on an empirical average over a subset of the samples {zi}i∈[n]. Due to its simplicity, superior generalization performance, and well-understood theoretical guarantees, SGD has been the method of choice for minimization problems arising in statistical machine learning.
Starting from the pioneering works of Robbins and Monro [1951], Chung [1954], Sacks [1958],
Fabian [1968], Ruppert [1988], Shapiro [1989], Polyak and Juditsky [1992], theoretical properties of the SGD algorithm and its variants have been receiving a growing attention under different scenarios.
Recent works, for example Tripuraneni et al. [2018], Su and Zhu [2018], Duchi and Ruan [2021],
Toulis and Airoldi [2017], Fang et al. [2018], Anastasiou et al. [2019], Yu et al. [2020] established convergence rates for SGD in various settings. By building on the analysis of Polyak and Juditsky
[1992] to prove a central limit theorem (CLT) for the Polyak-Ruppert averaging, these works led to novel methodologies to compute conﬁdence intervals using SGD. However, a recurring assumption in this line of work is the ﬁnite noise variance, which may be violated frequently in modern frameworks.
Heavy-tailed behavior in statistical methodology may naturally arise from the underlying model, or through the iterative optimization algorithm used during model training. In robust statistics, one often encounters heavy-tailed noise behavior in data, which in conjunction with standard loss functions leads to inﬁnite noise variance in SGD. Very recently, heavy-tailed behavior is shown to emerge from the multiplicative noise in SGD, when the step-size is large and/or the batch-size is small [Hodgkinson and Mahoney, 2021, Gürbüzbalaban et al., 2021]. On the other hand, there is strong empirical evidence in modern machine learning that the gradient noise often exhibits a heavy-tailed behavior, which indicates an inﬁnite variance. For example, this is observed in fully connected and convolutional neural networks [ ¸Sim¸sekli et al., 2019, Gürbüzbalaban and Hu, 2021] as well as recurrent neural networks [Zhang et al., 2020]. Thus, understanding the behavior of SGD under inﬁnite noise variance becomes extremely important for at least two reasons. A computational complexity reason: modern machine learning and robust statistics frameworks lead to heavy-tailed behavior in SGD; thus, understanding the performance of this algorithm in terms of precise convergence rates as well as the required conditions on the step-size sequence as a function of the ‘heaviness’ of the tail become crucial in this setup. A statistical reason: many inference methods that rely on Polyak-Ruppert averaging utilize a CLT which holds under ﬁnite noise variance (see e.g. online bootstrap and variance estimation approaches [Fang et al., 2018, Su and Zhu, 2018, Chen et al., 2020]). Using the same methodology in the aforementioned modern frameworks (under heavy-tailed noise) will ultimately result in incorrect conﬁdence intervals, jeopardizing the statistical procedure.
Thus, establishing the limit distribution in this setting is of great importance.
In this work, we study the behavior of the SGD algorithm with diminishing step-sizes for a class of strongly convex problems when the noise variance is inﬁnite. We establish the convergence rates of the SGD iterates towards the global minimum, and identify a sufﬁcient condition on the Hessian of f , which interpolates between the positive semi-deﬁnite cone and the cone of diagonally dominant matrices (with non-negative diagonal entries). We further study the Polyak-Ruppert averaging of the SGD iterates, and show that the limit distribution is a multivariate α-stable distribution. We illustrate our theory on linear regression and generalized linear models, demonstrating how to verify the conditions of our theorems. Perhaps surprisingly, our results show that even under heavy-tailed noise with inﬁnite variance, SGD with diminishing step-sizes can converge to the global optimum without requiring any modiﬁcation neither to the loss function nor to the algorithm itself, as opposed to the conventional techniques used in robust statistics [Huber, 2004]. Finally, we argue that our work has potential implications in constructing conﬁdence intervals in the inﬁnite noise variance setting. 2 Preliminaries and Technical