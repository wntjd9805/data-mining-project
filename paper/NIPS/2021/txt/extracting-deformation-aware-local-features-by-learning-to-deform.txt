Abstract
Despite the advances in extracting local features achieved by handcrafted and learning-based descriptors, they are still limited by the lack of invariance to non-rigid transformations. In this paper, we present a new approach to compute features from still images that are robust to non-rigid deformations to circumvent the problem of matching deformable surfaces and objects. Our deformation-aware local descriptor, named DEAL, leverages a polar sampling and a spatial transformer warping to provide invariance to rotation, scale, and image deformations. We train the model architecture end-to-end by applying isometric non-rigid deformations to objects in a simulated environment as guidance to provide highly discriminative local features. The experiments show that our method outperforms state-of-the-art handcrafted, learning-based image, and RGB-D descriptors in different datasets with both real and realistic synthetic deformable objects in still images. The source code and trained model of the descriptor are publicly available at https:
//www.verlab.dcc.ufmg.br/descriptors/neurips2021. 1

Introduction
High-quality matching of images taken from cameras in different poses and conditions remains one of the key challenges in several tasks. Tracking points as a camera or an object of interest is moving is crucial for ﬁnding and reconstructing objects and scenes [29, 30, 31], camera localization and mapping [18, 36], registration [43, 37], and image retrieval [26, 39], to name a few tasks. Extracting invariant and compact representations to describe the vicinity of a pixel has been one of the key forces in enabling high-quality matching on these tasks. Despite the importance of invariance to varying illumination, viewpoint, and the distance to the object of interest, real-world scenes impose additional challenges. After all, we live in a non-rigid world populated by objects that may have different shapes over time due to deformations. Over the past few decades, many descriptors have been proposed [19, 3, 6, 35, 42, 47, 44, 40, 23, 13]. These approaches can be roughly grouped based on the type of input information, such as intensity and/or depth images, and can be further divided into handcrafted and learning-based methods. While local learning-based approaches, such as
Log-Polar [13], hold state of the art regarding afﬁne transformations and local illumination changes, recent handcrafted techniques achieve better performance in the presence of non-rigid transformations by using depth data [25] or by modeling the deformations from image intensity information, which inevitably leads to an increase of the computational cost to compute the description [38]. Since depth data is not available in many contexts and applications, and low processing time is highly
∗Department of Computer Science, Universidade Federal de Minas Gerais, Brazil.
VIBOT EMR CNRS 6000, ImViA, Université Bourgogne Franche-Comté, France.
Corresponding author’s e-mail: guipotje@dcc.ufmg.br 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
recommended for low-level processing such as the estimation of local features, devising a descriptor that correctly and reliably establishes invariant features from corresponding points is of central importance to high-quality matching of images in real-world scenarios.
In this paper, we present a new end-to-end method to extract local features from still images that are robust to non-rigid deformations. Conversely to recent approaches that exploit a single network branch to extract features from local patches or depth data, our approach is composed of an additional guidance branch component (spatial transformer non-rigid warper) that jointly learns context about the global deformation affecting the image.
Figure 1 depicts the rationale behind our method.
For learning the contextual deformation infor-mation, we provide several views of deformed objects by applying isometric non-rigid trans-formations to the objects in a simulated envi-ronment as guidance to extract highly discrim-inative local deformation-aware features. Un-like non-rigid aware deformations descriptors like GeoBit [25], our approach does not require depth data to produce descriptors robust to non-rigid deformations and it is notably faster than
DaLI [38] while holding the best performance.
Although many descriptors still disregard non-rigid transformations, in recent years, that as-sumption has been dispelled, partly due to the increasing popularity in the use of RGB-D sen-sors to capture properties that characterize 3D objects such as geodesic distances. On the ﬂip side, RGB-D sensors are still less prevalent than RGB cameras. In our nonrigid-world with
ﬂexible humans, animals, tissues, and leaves where high-resolution RGB cameras are ubiq-uitous, the ability to create discriminative and deformation-invariant descriptors using a single picture plays a central role in achieving high-quality matching in real-world scenarios. In this work, we demonstrate that our end-to-end architec-ture outperforms state-of-the-art handcrafted and learning-based image and RGB-D descriptors in matching scores on different benchmark datasets of real deformable objects, as well as with realistic application scenarios on content-based object retrieval and tracking.
Figure 1: Learning to deform and extract fea-tures. Our network learns meaningful deforma-tions during training that lead to improved match-ing performance, as shown in the experiments.
Notice that the descriptor’s signatures (Descriptor
Value plot) of corresponding keypoints, with and without a local deformation (red and blue curves), are highly correlated when extracted with the pro-posed model. 2