Abstract
The lottery ticket hypothesis (LTH) [20] states that learning on a properly pruned network (the winning ticket) improves test accuracy over the original unpruned net-work. Although LTH has been justiﬁed empirically in a broad range of deep neural network (DNN) involved applications like computer vision and natural language processing, the theoretical validation of the improved generalization of a winning ticket remains elusive. To the best of our knowledge, our work, for the ﬁrst time, characterizes the performance of training a pruned neural network by analyzing the geometric structure of the objective function and the sample complexity to achieve zero generalization error. We show that the convex region near a desirable model with guaranteed generalization enlarges as the neural network model is pruned, indicating the structural importance of a winning ticket. Moreover, when the algorithm for training a pruned neural network is speciﬁed as an (accelerated) stochastic gradient descent algorithm, we theoretically show that the number of samples required for achieving zero generalization error is proportional to the number of the non-pruned weights in the hidden layer. With a ﬁxed number of samples, training a pruned neural network enjoys a faster convergence rate to the desired model than training the original unpruned one, providing a formal justiﬁca-tion of the improved generalization of the winning ticket. Our theoretical results are acquired from learning a pruned neural network of one hidden layer, while experimental results are further provided to justify the implications in pruning multi-layer neural networks. 1

Introduction
Neural network pruning can reduce the computational cost of model training and inference signif-icantly and potentially lessen the chance of overﬁtting [33, 26, 15, 25, 28, 51, 58, 41]. The recent
Lottery Ticket Hypothesis (LTH) [20] claims that a randomly initialized dense neural network al-35th Conference on Neural Information Processing Systems (NeurIPS 2021).
ways contains a so-called “winning ticket,” which is a sub-network bundled with the corresponding initialization, such that when trained in isolation, this winning ticket can achieve at least the same testing accuracy as that of the original network by running at most the same amount of training time.
This so-called “improved generalization of winning tickets” is veriﬁed empirically in [20]. LTH has attracted a signiﬁcant amount of recent research interests [45, 70, 39]. Despite the empirical success
[19, 63, 55, 11], the theoretical justiﬁcation of winning tickets remains elusive except for a few recent works. [39] provides the ﬁrst theoretical evidence that within a randomly initialized neural network, there exists a good sub-network that can achieve the same test performance as the original network.
Meanwhile, recent work [42] trains neural network by adding the (cid:96)1 regularization term to obtain a relatively sparse neural network, which has a better performance numerically.
However, the theoretical foundation of network pruning is limited. The existing theoretical works usually focus on ﬁnding a sub-network that achieves a tolerable loss in either expressive power or training accuracy, compared with the original dense network [2, 71, 61, 43, 4, 3, 35, 5, 59]. To the best of our knowledge, there exists no theoretical support for the improved generalization achieved by winning tickets, i.e., pruned networks with faster convergence and better test accuracy.
Contributions: This paper provides the ﬁrst systematic analysis of learning pruned neural networks with a ﬁnite number of training samples in the oracle-learner setup, where the training data are generated by a unknown neural network, the oracle, and another network, the learner, is trained on the dataset. Our analytical results also provide a justiﬁcation of the LTH from the perspective of the sample complexity. In particular, we provide the ﬁrst theoretical justiﬁcation of the improved generalization of winning tickets. Speciﬁc contributions include: 1. Pruned neural network learning via accelerated gradient descent (AGD): We propose an
AGD algorithm with tensor initialization to learn the pruned model from training samples. Our algorithm converges to the oracle model linearly, which has guaranteed generalization. 2. First sample complexity analysis for pruned networks: We characterize the required number of samples for successful convergence, termed as the sample complexity. Our sample complexity bound depends linearly on the number of the non-pruned weights and is a signiﬁcant reduction from directly applying conventional complexity bounds in [69, 66, 67]. 3. Characterization of the benign optimization landscape of pruned networks: We show analyt-ically that the empirical risk function has an enlarged convex region for a pruned network, justifying the importance of a good sub-network (i.e., the winning ticket). 4. Characterization of the improved generalization of winning tickets: We show that gradient-descent methods converge faster to the oracle model when the neural network is properly pruned, or equivalently, learning on a pruned network returns a model closer to the oracle model with the same number of iterations, indicating the improved generalization of winning tickets.
Notations. Vectors are bold lowercase, matrices and tensors are bold uppercase. Scalars are in normal font, and sets are in calligraphy and blackboard bold font. I denote the identity matrix. N and R denote the sets of nature number and real number, respectively. (cid:107)z(cid:107) denotes the (cid:96)2-norm of a vector z, and (cid:107)Z(cid:107)2, (cid:107)Z(cid:107)F and (cid:107)Z(cid:107)∞ denote the spectral norm, Frobenius norm and the maximum value of matrix Z, respectively. [Z] stands for the set of {1, 2, · · · , Z} for any number Z ∈ N. In addition, f (r) = O(g(r)) ( or f (r) = Ω(g(r)) ) if f ≤ C · g ( or f ≥ C · g ) for some constant
C > 0 when r is large enough. f (r) = Θ(g(r)) if both f (r) = O(g(r)) and f (r) = Ω(g(r)) holds, where c · g ≤ f ≤ C · g for some constant 0 ≤ c ≤ C when r is large enough. 1.1