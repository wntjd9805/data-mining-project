Abstract
Consider the sequential optimization of a continuous, possibly non-convex, and expensive to evaluate objective function f . The problem can be cast as a Gaussian
Process (GP) bandit where f lives in a reproducing kernel Hilbert space (RKHS).
The state of the art analysis of several learning algorithms shows a signiﬁcant gap between the lower and upper bounds on the simple regret performance. When N is the number of exploration trials and γN is the maximal information gain, we prove an ˜O((cid:112)γN /N ) bound on the simple regret performance of a pure exploration algorithm that is signiﬁcantly tighter than the existing bounds. We show that this bound is order optimal up to logarithmic factors for the cases where a lower bound on regret is known. To establish these results, we prove novel and sharp conﬁdence intervals for GP models applicable to RKHS elements which may be of broader interest. 1

Introduction
Sequential optimization has evolved into one of the fastest developing areas of machine learning [1].
We consider sequential optimization of an unknown objective function from noisy and expensive to evaluate zeroth-order1 observations. That is a ubiquitous problem in academic research and industrial production. Examples of applications include exploration in reinforcement learning, recommendation systems, medical analysis tools and speech recognizers [4]. A notable application in the ﬁeld of machine learning is automated hyper-parameter tuning. Prevalent methods such as grid search can be prohibitively expensive [5, 6]. Sequential optimization methods, on the other hand, are shown to efﬁciently ﬁnd good hyper-parameters by an adaptive exploration of the hyper-parameter space [7].
Our sequential optimization setting is as follows. Consider an objective function f deﬁned over a domain X ⊂ Rd, where d ∈ N is the dimension of the input. A learning algorithm is allowed to perform an adaptive exploration to sequentially observe the potentially corrupted values of the objective function {f (xn) + (cid:15)n}N n=1, where (cid:15)n are random noises. At the end of N exploration trials, the learning algorithm returns a candidate maximizer ˆx∗
N ∈ X of f . Let x∗ ∈ argmaxx∈X f (x) be a true optimal solution. We may measure the performance of the learning algorithm in terms of simple regret; that is, the difference between the performance under the true optimal solution, f (x∗), and that under the learnt value, f (ˆx∗
N ).
Our formulation falls under the general framework of continuum armed bandit that signiﬁes receiving feedback only for the selected observation point xn at each time n [8, 9, 10, 11]. Bandit problems have been extensively studied under numerous settings and various performance measures including simple 1Zeroth-order feedback signiﬁes observations from f in contrast to ﬁrst-order feedback which refers to observations from gradient of f as e.g. in stochastic gradient descent [see, e.g., 2, 3]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
regret [see, e.g., 10, 12, 13], cumulative regret [see, e.g., 14, 15, 16], and best arm identiﬁcation [see, e.g., 17, 18]. The choice of performance measure strongly depends on the application. Simple regret is suitable for situations with a preliminary exploration phase (for instance hyper-parameter tuning) in which costs are not measured in terms of rewards but rather in terms of resources expended [10].
Due to inﬁnite cardinality of the domain, approaching f (x∗) is feasible only when appropriate regularity assumptions on f and noise are satisﬁed. Following a growing literature [19, 20, 21, 22], we focus on a variation of the problem where f is assumed to belong to a reproducing kernel Hilbert space (RKHS) that is a very general assumption. Almost all continuous functions can be approximated with the RKHS elements of practically relevant kernels such as Matérn family of kernels [19]. We consider two classes of noise: sub-Gaussian and light-tailed.
Our regularity assumption on f allows us to utilize Gaussian processes (GPs) which provide powerful
Bayesian (surrogate) models for f [23]. Sequential optimization based on GP models is often referred to as Bayesian optimization in the literature [4, 24, 25]. We build on prediction and uncertainty estimates provided by GP models to study an efﬁcient adaptive exploration algorithm referred to as
Maximum Variance Reduction (MVR). Under simple regret measure, MVR embodies the simple principle of exploring the points with the highest variance ﬁrst. Intuitively, the variance in the GP model is considered as a measure of uncertainty about the unknown objective function and the exploration steps are designed to maximally reduce the uncertainty. At the end of exploration trials,
MVR returns a candidate maximizer based on the prediction provided by the learnt GP model. With its simple structure, MVR is amenable to a tight analysis that signiﬁcantly improves the best known bounds on simple regret. To this end, we derive novel and sharp conﬁdence intervals for GP models applicable to RKHS elements. In addition, we provide numerical experiments on the simple regret performance of MVR comparing it to GP-UCB [19, 20], GP-PI [26] and GP-EI [26]. 1.1 Main Results
We ﬁrst derive novel conﬁdence intervals for GP models applicable to RKHS elements (Theorems 1 and 2). As part of our analysis, we formulate the posterior variance of a GP model as the sum of two terms: the maximum prediction error from noise-free observations, and the effect of noise (Proposition 1). This interpretation elicits new connections between GP regression and kernel ridge regression [27]. These results are of interest on their own.
We then build on the conﬁdence intervals for GP models to provide a tight analysis of the simple regret of the MVR algorithm (Theorem 3). In particular, we prove a high probability ˜O((cid:112) γN
N )2 simple regret, where γN is the maximal information gain (see § 2.4). In comparison to ˜O( γN√
)
N
γN ) improvement. It is noteworthy bounds on simple regret [see, e.g., 19, 20, 28], we show an O( that our bound guarantees convergence to the optimum value of f , while previous ˜O( γN√
) bounds do
N not, since although γN grows sublinearly with N , it can grow faster than
N .
√
√
We then specialize our results for the particular cases of practically relevant Matérn and Squared
Exponential (SE) kernels. We show that our regret bounds match the lower bounds and close the gap (cid:17) reported in [28, 29], who showed that an average simple regret of (cid:15) requires N = Ω exploration trials in the case of SE kernel. For the Matérn-ν kernel (where ν is the smoothness parameter, see § 2.1) they gave the analogous bound of N = Ω
. They also reported a signiﬁcant gap between these lower bounds and the upper bounds achieved by GP-UCB algorithm.
In Corollary 1, we show that our analysis of MVR closes this gap in the performance and establishes upper bounds matching the lower bounds up to logarithmic factors. (cid:16) 1 (cid:15)2 (log( 1 (cid:15) )2+ d ( 1 (cid:15) )) d (cid:17) (cid:16)
ν 2
In contrast to the existing results which mainly focus on Gaussian and sub-Gaussian distributions for noise, we extend our analysis to the more general class of light-tailed distributions, thus broadening the applicability of the results. This extension increases both the conﬁdence interval width and the simple regret by only a multiplicative logarithmic factor. These results apply to e.g. the privacy preserving setting where often a light-tailed noise is employed [30, 31, 32]. 2The notations O and ˜O are used to denote the mathematical order and the mathematical order up to logarithmic factors, respectively. 2
1.2 Literature Review
√
The celebrated work of Srinivas et al. [19] pioneered the analysis of GP bandits by proving an
˜O(γN
N ) upper bound on the cumulative regret of GP-UCB, an optimistic optimization algorithm sequentially selecting xn which maximize an upper conﬁdence bound index over the search space.
That implies an ˜O( γN√
) simple regret [28]. Their analysis relied on deriving conﬁdence intervals for
N
GP models applicable to RKHS elements. They also considered a fully Bayesian setting where f is assumed to be a sample from a GP and noise is assumed to be Gaussian. [20] built on feature space representation of GP models and self-normalized martingale inequalities, ﬁrst developed in [33] for linear bandits, to improve the conﬁdence intervals of [19] by a multiplicative log(N ) factor. That led to an improvement in the regret bounds by the same multiplicative log(N ) factor. A discussion on the comparison between these results and the conﬁdence intervals derived in this paper is provided in § 3.3. A technical comparison with some recent advances in regret bounds requires introducing new notations and is deferred to § 4.4.
The performance of Bayesian optimization algorithms has been extensively studied under numer-ous settings including contextual information [34], high dimensional spaces [35, 36], safety con-straints [37, 38], parallelization [39], meta-learning [40], multi-ﬁdelity evaluations [41], ordinal models [42], corruption tolerance [43, 29], and neural tangent kernels [44, 45]. [46] introduced an adaptive discretization of the search space improving the computational complexity of a GP-UCB based algorithm. Sparse approximation of GP posteriors are shown to preserve the regret orders while improving the computational complexity of Bayesian optimization algorithms [36, 47, 48]. Under the RKHS setting with noisy observations, GP-TS [20] and GP-EI [49, 50] are also shown to achieve the same regret guarantees as GP-UCB (up to logarithmic factors). All these works report ˜O( γN√
)
N regret bounds.
The regret bounds are also reported under other often simpler settings such as noise-free ob-servations [51, 52, (cid:15)n = 0, ∀n] or a Bayesian regret that is averaged over a known prior on f [39, 53, 54, 55, 56, 57, 58, 59], rather than for a ﬁxed and unknown f as in our setting.
Other lines of work on continuum armed bandit exist, relying on other regularity assumptions such as Lipschitz continuity [9, 11, 12, 60], convexity [61] and unimodality [62], to name a few. A d+1 notable example is [11] who showed that hierarchical algorithms based on tree search yield O(N d+2 ) cumulative regret. We do not compare with these results due to the inherent difference in the regularity assumptions. 1.3 Organization
In § 2, the problem formulation, the regularity assumptions, and the preliminaries on RKHS and
GP models are presented. The novel conﬁdence intervals for GP models are proven in § 3. MVR algorithm and its analysis are given in § 4. The experiments are presented in § 5. We conclude with a discussion in § 6. 2 Problem Formulation and Preliminaries
Consider an objective function f : X → R, where X ⊆ Rd is a convex and compact domain.
Consider an optimal point x∗ ∈ argmaxx∈X f (x). A learning algorithm A sequentially selects observation points {xn ∈ X }n∈N and observes the corresponding noise disturbed objective values
{yn = f (xn) + (cid:15)n}n∈N, where (cid:15)n is the observation noise. We use the notations Hn = {Xn, Yn},
Xn = [x1, x2, ..., xn](cid:62), Yn = [y1, y2, ..., yn](cid:62), xn ∈ X , yn ∈ R, for all n ≥ 1. In a simple regret setting, the learning algorithm determines a sequence of mappings {Sn}n≥1 where each mapping
Sn : Hn → X predicts a candidate maximizer ˆx∗ n. For algorithm A, the simple regret under a budget of N tries is deﬁned as
N = f (x∗) − f (ˆx∗ rA
N ). (1)
The budget N may be unknown a priori. Notationwise, we use Fn = [f (x1), f (x2), . . . , f (xn)](cid:62) and En = [(cid:15)1, (cid:15)2, . . . , (cid:15)n](cid:62) to denote the noise free part of the observations and the noise history, respectively, similar to Xn and Yn. 3
2.1 Gaussian Processes
The Bayesian optimization algorithms build on GP (surrogate) models. A GP is a random process
{ ˆf (x)}x∈X , where each of its ﬁnite subsets follow a multivariate Gaussian distribution. The distribu-tion of a GP is fully speciﬁed by its mean function µ(x) = E[ ˆf (x)] and a positive deﬁnite kernel (or (cid:105) (cid:104) ( ˆf (x) − µ(x))( ˆf (x(cid:48)) − µ(x(cid:48))) covariance function) k(x, x(cid:48)) = E
. Without loss of generality, it is typically assumed that, for prior GP distributions, µ(x) = 0, ∀x ∈ X .
Conditioning GPs on available observations provides us with powerful non-parametric Bayesian (surrogate) models over the space of functions. In particular, using the conjugate property, conditioned on Hn, the posterior of ˆf is a GP with mean function µn(x) = E[ ˆf (x)|Hn] and kernel function kn(x, x(cid:48)) = E[( ˆf (x) − µn(x))( ˆf (x(cid:48)) − µn(x(cid:48)))|Hn] speciﬁed as follows:
µn(x) = k(cid:62)(x, Xn) (cid:0)k(Xn, Xn) + λ2In (cid:1)−1
Yn, kn(x, x(cid:48)) = k(x, x(cid:48)) − k(cid:62)(x, Xn) (cid:0)k(Xn, Xn) + λ2In n(x) (cid:44) kn(x, x), (2) where with some abuse of notation k(x, Xn) = [k(x, x1), k(x, x2), . . . , k(x, xn)](cid:62), k(Xn, Xn) =
[k(xi, xj)]n i,j=1 is the covariance matrix, In is the identity matrix of dimension n, and λ > 0 is a real number. k(x(cid:48), Xn), σ2 (cid:1)−1
In practice, Matérn and squared exponential (SE) are the most commonly used kernels for Bayesian optimization [see, e.g., 4, 24], kMatérn(x, x(cid:48)) = 1
Γ(ν)2ν−1 (cid:32) √ 2νρ l (cid:33)ν (cid:32) √
Bν (cid:33)
, 2νρ l kSE(x, x(cid:48)) = exp (cid:18)
−
ρ2 2l2 (cid:19)
, where l > 0 is referred to as lengthscale, ρ = ||x − x(cid:48)||l2 is the Euclidean distance between x and x(cid:48),
ν > 0 is referred to as the smoothness parameter, Γ and Bν are, respectively, the Gamma function and the modiﬁed Bessel function of the second kind. Variation over parameter ν creates a rich family of kernels. The SE kernel can also be interpreted as a special case of Matérn family when ν → ∞. 2.2 RKHSs and Regularity Assumptions on f
Consider a positive deﬁnite kernel k : X × X → R with respect to a ﬁnite Borel measure (e.g., the Lebesgue measure) supported on X . A Hilbert space Hk of functions on X equipped with an inner product (cid:104)·, ·(cid:105)Hk is called an RKHS with reproducing kernel k if the following is satisﬁed.
For all x ∈ X , k(·, x) ∈ Hk, and for all x ∈ X and f ∈ Hk, (cid:104)f, k(·, x)(cid:105)Hk = f (x) (reproducing property). A constructive deﬁnition of RKHS requires introducing Mercer theorem, which provides an alternative representation of kernels as an inner product of inﬁnite dimensional feature maps [see, e.g., 27, Theorem 4.1], and is deferred to Appendix A. We have the following regularity assumption on the objective function f .
Assumption 1 The objective function f is assumed to live in the RKHS corresponding to a positive deﬁnite kernel k. In particular, ||f ||Hk ≤ B, for some B > 0, where (cid:107)f (cid:107)2
Hk
= (cid:104)f, f (cid:105)Hk .
For common kernels, such as Matérn family of kernels, members of Hk can uniformly approximate any continuous function on any compact subset of the domain X [19]. This is a very general class of functions; more general than, e.g., the class of convex functions. It has thus gained increasing interest in recent years. 2.3 Regularity Assumptions on Noise
We consider two different cases regarding the regularity assumption on noise. Let us ﬁrst revisit the deﬁnition of sub-Gaussian distributions.
Deﬁnition 1 A random variable X is called sub-Gaussian if its moment generating function M (h) (cid:44)
E[exp(hX)] is upper bounded by that of a Gaussian random variable.
The sub-Gaussian assumption implies that E[X] = 0. It also allows us to use Chernoff-Hoeffding concentration inequality [63] in our analysis. 4
We next recall the deﬁnition of light-tailed distributions.
Deﬁnition 2 A random variable X is called light-tailed if its moment-generating function exists, i.e., there exists h0 > 0 such that for all |h| ≤ h0, M (h) < ∞.
For a zero mean light-tailed random variable X, we have [64]
M (h) ≤ exp(ξ0h2/2), ∀|h| ≤ h0, ξ0 = sup{M (2)(h), |h| ≤ h0}, (3) where M (2)(.) denotes the second derivative of M (.) and h0 is the parameter speciﬁed in Deﬁnition 2.
We observe that the upper bound in (3) is the moment generating function of a zero mean Gaussian random variable with variance ξ0. Thus, light-tailed distributions are also called locally sub-Gaussian distributions [65].
We provide conﬁdence intervals for GP models and regret bounds for MVR under each of the following assumptions on the noise terms.
Assumption 2 (Sub-Gaussian Noise) The noise terms (cid:15)n are independent over n.
∀h ∈ R, ∀n ∈ N, E[eh(cid:15)n ] ≤ exp( h2R2
), for some R > 0. 2
In addition,
Assumption 3 (Light-Tailed Noise) The noise terms (cid:15)n are zero mean independent random vari-ables over n. In addition, ∀h ≤ h0, ∀n ∈ N, E[eh(cid:15)n] ≤ exp( h2ξ0 2 ), for some ξ0 > 0.
Bayesian optimization uses GP priors for the objective function f and assumes a Gaussian distribution for noise (for its conjugate property). It is noteworthy that the use of GP models is merely for the purpose of algorithm design and does not affect our regularity assumptions on f and noise. We use the notation ˆf to distinguish the GP model from the ﬁxed f . 2.4 Maximal Information Gain
The regret bounds derived in this work are given in terms of the maximal information gain, deﬁned as γN = supXN ⊆X I(YN ; ˆf ), where I(YN ; ˆf ) denotes the mutual information between Yn and
ˆf [see, e.g., 66, Chapter 2]. In the case of a GP model, the mutual information can be given as
λ2 k(Xn, Xn)(cid:1) , where the notation (log) det denotes the (logarithm of)
I(Yn; ˆf ) = 1 determinant of a square matrix. Note that the maximal information gain is kernel-speciﬁc and XN -independent. Upper bounds on γN are derived in [19, 21, 22] which are commonly used to provide explicit regret bounds. In the case of Matérn and SE kernels, γN = O and
γN = O (cid:0)(log(N ))d+1(cid:1), respectively [22]. 2 log det (cid:0)In + 1 2ν+d (log(N )) 2ν 2ν+d
N (cid:17) (cid:16) d 3 Conﬁdence Intervals for Gaussian Process Models
The analysis of bandit problems classically builds on conﬁdence intervals applicable to the values of the objective function [see, e.g., 67, 68]. The GP modelling allows us to create conﬁdence intervals for complex functions over continuous domains. In particular, we utilize the prediction (µn) and the uncertainty estimate (σn) provided by GP models in building the conﬁdence intervals which become an important building block of our analysis in the next section. To this end, we ﬁrst prove the following proposition which formulates the posterior variance of a GP model as the sum of two terms: the maximum prediction error for an RKHS element from noise free observations and the effect of noise. n be the posterior variance of the surrogate GP model as deﬁned in (2). Let
Proposition 1 Let σ2 n (x) = k(cid:62)(x, Xn) (cid:0)k(Xn, Xn) + λ2In
Z (cid:62)
σ2 n(x) = sup f :||f ||Hk ≤1 (cid:1)−1
. We have (f (x) − Z (cid:62) n (x)Fn)2 + λ2(cid:107)Zn(x)(cid:107)2 l2 .
Notice that the ﬁrst term f (x) − Z (cid:62) n (x)Fn captures the maximum prediction error from noise free observations Fn. The second term captures the effect of noise in the surrogate GP model (and is independent of Fn). A detailed proof for Proposition 1 is provided in Appendix B. 5
Proposition 1 elicits new connections between GP models and kernel ridge regression. While the equivalence of the posterior mean in GP models and the regressor in kernel ridge regression is well known, the interpretation of posterior variance of GP models as the maximum prediction error for an RKHS element is less studied [see 27, Section 3, for a detailed discussion on the connections between GP models and kernel ridge regression]. 3.1 Conﬁdence Intervals under Sub-Gaussian Noise
The following theorem provides a conﬁdence interval for GP models applicable to RKHS elements under the assumption that the noise terms are sub-Gaussian.
Theorem 1 Assume Assumptions 1 and 2 hold. Provided n noisy observations Hn = {Xn, Yn} from f , let µn and σn be as deﬁned in (2). Assume Xn are independent of En. For a ﬁxed x ∈ X , deﬁne the upper and lower conﬁdence bounds, respectively, n(x) (cid:44) µn(x) + (B + β(δ)) σn(x), and Lδ
U δ n(x) (cid:44) µn(x) − (B + β(δ)) σn(x), (4) (cid:113) with β(δ) = R
λ tions 1 and 2. We have 2 log( 1
δ ), where δ ∈ (0, 1), and B and R are the parameters speciﬁed in Assump-f (x) ≤ U δ n(x) w.p. at least 1 − δ, and f (x) ≥ Lδ n(x) w.p. at least 1 − δ.
We can write the difference in the objective function and the posterior mean as follows. f (x) − µn(x) = f (x) − Z (cid:62) n (x)Yn = f (x) − Z (cid:62) (cid:123)(cid:122) (cid:124)
Prediction error from noise free observations n (x)Fn (cid:125)
− Z (cid:62) (cid:124) n (x)En (cid:123)(cid:122) (cid:125)
The effect of noise
.
The ﬁrst term can be bounded directly following Proposition 1. The second term is bounded as a result of Proposition 1 and Chernoff-Hoeffding inequality. A detailed proof of Theorem 1 is provided in Appendix C. 3.2 Conﬁdence Intervals under Light-Tailed Noise
We now extend the conﬁdence intervals to the case of light-tailed noise. The main difference with sub-Gaussian noise is that Chernoff-Hoeffding inequality is no more applicable. We derive new bounds accounting for light-tailed noise in the analysis of Theorem 2.
Theorem 2 Assume Assumptions 1 and 3 hold. For a ﬁxed x ∈ X , deﬁne the upper and lower conﬁ-n(x) and Lδ dence bounds U δ n(x) similar to Theorem 1 with β(δ) = 1
δ ) 3,
λ where δ ∈ (0, 1), and B, h0 and ξ0 are speciﬁed in Assumptions 1 and 3. Assume Xn are independent of En. We have log( 1 2
ξ0 ∨ 2 log( 1
δ ) h2 0 (cid:114) (cid:16) (cid:17) f (x) ≤ U δ n(x) w.p. at least 1 − δ, and f (x) ≥ Lδ n(x) w.p. at least 1 − δ.
In comparison to Theorem 1, under the light-tailed assumption, the conﬁdence interval width increases
δ )) factor. A detailed proof of Theorem 2 is provided in Appendix C. with a multiplicative O( log( 1 (cid:113)
Remark 1 Theorems 1 and 2 rely on the assumption that Xn are independent of En. As we shall see in § 4, this assumption is satisﬁed when the conﬁdence intervals are applied to the analysis of MVR. 3.3 Comparison with the Existing Conﬁdence Intervals
The most relevant work to our Theorems 1 and 2 is [20, Theorem 2] which itself was an improvement over [19, Theorem 6]. [20] built on feature space representation of GP kernels and self-normalized martingale inequalities [33, 69] to establish a 1 − δ conﬁdence interval in the same form as in
δ )) 4
Theorem 1, under Assumptions 1 and 2, with conﬁdence interval width B +R 2(γn + 1 + log( 1 (cid:113) 3The notation ∨ is used to denote the maximum of two real numbers, ∀a, b ∈ R, (a ∨ b) (cid:44) max(a, b). 4Note that the effect of λ is absorbed in γn. 6
(instead of B + β(δ)). There is a stark contrast between this conﬁdence interval and the one given in
Theorem 1 in its dependence on γn, which has a relatively large and possibly polynomial in n value.
That contributes an extra O(
γN ) multiplicative factor to regret.
√
Neither of these two results (our Theorem 1 and [20, Theorem 2]) imply the other. Although our conﬁdence interval is much tighter, there are two important differences in the settings of these theorems. One difference is in the probabilistic dependencies between the observation points xn and the noise terms {(cid:15)j}j<n. While Theorem 1 assumes that Xn are independent of En, [20, Theorem 2] allows for the dependence of xn on the previous noise terms {(cid:15)j}j<n. This is a reﬂection of the difference in the analytical requirements of MVR and GP-UCB. The other difference is that [20,
Theorem 2] holds for all x ∈ X . While, Theorem 1 holds for a single x ∈ X . As we will see in § 4.2, a probability union bound can be used to obtain conﬁdence intervals applicable to all x in (a discretization of) X , which contributes only logarithmic terms to regret, in contrast to O(
γn).
Roughly speaking, we are trading off the extra O(
γn) term for restricting the conﬁdence interval to hold for a single x. It remains an open problem whether the same can be done when xn are allowed to depend on {(cid:15)j}j<n.
√
√ 4 Maximum Variance Reduction and Simple Regret
In this section, we ﬁrst formally present an exploration policy based on GP models referred to as
Maximum Variance Reduction (MVR). We then utilize the conﬁdence intervals for GP models derived in § 3 to prove bounds on the simple regret of MVR. 4.1 Maximum Variance Reduction Algorithm
MVR relies on the principle of maximally reducing the uncertainty, where the uncertainty is measured by the posterior variance of the GP model. After N exploration trials, MVR returns a candidate maximizer according to the prediction provided by the learnt GP model. A pseudo-code is given in
Algorithm 1.
Algorithm 1 Maximum Variance Reduction (MVR) 0(x) (cid:44) k(x, x). 1: Initialization: k, X , f , σ2 2: for n = 1, 2, . . . , N do xn = argmaxx∈X σ2 3:
Update σ2 4: 5: end for 6: Update µN (.) according to (2). 7: return ˆx∗ n(.) according to (2). n−1(x), where a tie is broken arbitrarily.
N = argmaxx∈X µN (x), where a tie is broken arbitrarily.
We note that MVR, similar to other standard GP bandit algorithms, requires optimizing an internal index created based on previous observations (here, σ2 n−1(.) and µN (.)). Examples of other typical indices are UCB and EI, which are often referred to as acquisition functions. The index itself may be multi modal in general. It is however standard to assume a perfect optimization of the index, since the cost of evaluating f is considered to dominate the cost of maximizing the index [19]. 4.2 Regret Analysis
For the analysis of MVR, we assume there exists a ﬁne discretization of the domain for RKHS elements, which is a standard assumption in the literature [see, e.g., 19, 20, 48].
Assumption 4 For each given n ∈ N and f ∈ Hk with (cid:107)f (cid:107)Hk ≤ B, there exists a discretization Dn of X such that f (x) − f ([x]n) ≤ 1√
||x(cid:48) − x||l2 is the closest point in n , where [x]n = argminx(cid:48)∈Dn
Dn to x, and |Dn| ≤ CBdnd/2, where C is a constant independent of n and B.
Assumption 4 is a mild assumption that holds for typical kernels such as SE and Matérn [19, 20]. The following theorem provides a high probability bound on the regret performance of MVR, when the noise terms satisfy either Assumption 2 or 3. 7
Theorem 3 Consider the GP bandit problem, with a ﬁxed N . Under Assumptions 1, 4, and (2 or 3), for δ ∈ (0, 1), with probability at least 1 − δ, MVR satisﬁes (cid:115) rMVR
N
≤
 2γN log(1 + 1
λ2 )N

2B + β( (cid:18)
) + β
δ 3
δ (cid:16) 3C
B +
√
N β(2δ/3N ) (cid:17)d
N d/2 (cid:19)


 + 2
√
N
, where under Assumption 2, β(δ) = R
λ 2 log( 1
δ ), and under Assumption 3, β(δ) = (cid:113) (cid:17) log( 1
δ ), and B, R, h0, ξ0 and C are the constants speciﬁed in Assump-(cid:114) (cid:16) 2
ξ0 ∨ 2 log( 1
δ ) 1 h2
λ 0 tions 1, 2, 3 and 4.
A detailed proof of the theorem is provided in Appendix D.
Remark 2 Under Assumptions 2 and 3, respectively, the regret bounds can be simpliﬁed as (cid:114) rMVR
N = O(
γN log(N d/δ)
N
), and rMVR
N = O (cid:18)(cid:114) γN
N log(N d/δ) (cid:19)
.
For instance, in the case of Matérn-ν kernel, under Assumption 2 and 3, respectively, rMVR
N = O (cid:18)
N
−ν 2ν+d (log(N ))
ν 2ν+d (cid:113) (cid:19) log(N d/δ)
, and rMVR
N = O (cid:16)
N
−ν 2ν+d (log(N ))
ν 2ν+d log(N d/δ) (cid:17)
, which always converge to zero as N grows.
Remark 3 In the analysis of Theorem 3, we apply Assumption 4 to µN as well as f . For this
N β(2δ/3N ) upper bound on (cid:107)µN (cid:107)Hk (see Lemma 4 purpose, we derive a high probability B + in Appendix D), which appears in the regret bound expression.
√
Remark 4 Theorem 3 holds for a ﬁxed N . The result however easily extends to an anytime regret bound, using a standard probability union bound over N . Speciﬁcally, if we replace δ in the bound
π2N 2 , the theorem holds, with probability 1−δ, for all N ∈ N. Note that (cid:80)∞ with δ0 = 6δ 6δ
π2N 2 = δ.
N =1 4.3 Optimal Order Simple Regret with SE and Matérn Kernels
To enable a direct comparison with the lower bounds on simple regret proven in [28, 29], in the following corollary, we state a dual form of Theorem 3 for the Matérn and SE kernels. Speciﬁcally we formalize the number of exploration trials required to achieve an average (cid:15) regret.
Corollary 1 Consider the GP bandit problem with an SE or a Matérn kernel. For (cid:15) ∈ (0, 1), deﬁne
N(cid:15) = min{N ∈ N : E[rMVR
] ≤ (cid:15), ∀n ≥ N }. Under Assumptions 1, 4, and (2 or 3), upper bounds on N(cid:15) are reported in Table 1. n
Table 1: The upper bounds on N(cid:15) deﬁned in Corollary 1 with SE or Matérn kernel.
Kernel
SE
Under Assumption 2
Under Assumption 3
Matérn-ν N(cid:15) = O
N(cid:15) = O (cid:0)( 1 (cid:16) (cid:15) )2 log( 1
ν (log( 1 (cid:15) )d+2(cid:1) (cid:15) )) 4ν+d 2ν (cid:17)
N(cid:15) = O (cid:0)( 1 (cid:16) (cid:15) )2+ d ( 1 (cid:15) )2 log( 1
ν (log( 1 (cid:15) )d+3(cid:1) (cid:15) )) 6ν+2d 2ν
N(cid:15) = O (cid:17) (cid:15) )2+ d ( 1
A proof is provided in Appendix E. [28, 29] showed that for the SE kernel, an average simple regret
. For the Matérn-ν kernel they gave the analogous bound of of (cid:15) requires N(cid:15) = Ω (cid:17) (cid:16) 1 (cid:15)2 (log( 1 (cid:15) )) d 2 (cid:16) (cid:15) )2+ d ( 1
ν (cid:17)
N(cid:15) = Ω
. They also reported signiﬁcant gaps between these lower bounds and the existing results [see, e.g., 28, Table I]. Comparing with Corollary 1, our bounds are tight in all cases up to log(1/(cid:15)) factors. 8
4.4 Comparing the Regret Bounds with Other