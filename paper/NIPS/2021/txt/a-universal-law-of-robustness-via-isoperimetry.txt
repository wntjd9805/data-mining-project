Abstract
Classically, data interpolation with a parametrized model class is possible as long as the number of parameters is larger than the number of equations to be satis-ﬁed. A puzzling phenomenon in deep learning is that models are trained with many more parameters than what this classical theory would suggest. We propose a theoretical explanation for this phenomenon. We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth interpolation requires d times more parameters than mere interpolation, where d is the ambi-ent data dimension. We prove this universal law of robustness for any smoothly parametrized function class with polynomial size weights, and any covariate dis-tribution verifying isoperimetry (or a mixture thereof). In the case of two-layer neural networks and Gaussian covariates, this law was conjectured in prior work by Bubeck, Li and Nagaraj. We also give an interpretation of our result as an improved generalization bound for model classes consisting of smooth functions. 1

Introduction
Solving n equations generically requires only n unknowns1. However, the revolutionary deep learn-ing methodology revolves around highly overparametrized models, with many more than n parame-ters to learn from n training data points. We propose an explanation for this enigmatic phenomenon, showing in great generality that ﬁnding a smooth function to ﬁt d-dimensional data requires at least nd parameters. In other words, overparametrization by a factor of d is necessary for smooth inter-polation, suggesting that perhaps the large size of the models used in deep learning is a necessity rather than a weakness of the framework. Another way to phrase the result is as a tradeoff between the size of a model (as measured by the number of parameters) and its “robustness” (as measured by its Lipschitz constant): either one has a small model (with n parameters) which must then be non-robust, or one has a robust model (constant Lipschitz) but then it must be very large (with nd parameters). Such a tradeoff was conjectured for the speciﬁc case of two-layer neural networks and Gaussian data in [BLN21]. Our result shows that in fact it is a universal phenomenon, which applies to essentially any parametrized function class (including in particular deep neural networks) as well as a much broader class of data distributions. As in [BLN21] we obtain an entire tradeoff curve between size and robustness: our universal law of robustness states that, for any function class smoothly parametrized by p parameters, and for any d-dimensional dataset satisfying mild regularity conditions, any function in this class that ﬁts the data below the noise level must have its (Euclidean)
Lipschitz constant larger than (cid:113) nd p .
Theorem 1 (Informal version of Theorem 3). Let F be a class of functions from Rd → R and let (xi, yi)n i=1 be i.i.d. input-output pairs in Rd × [−1, 1]. Assume that: 1. F admits a Lipschitz parametrization by p real parameters, each of size at most poly(n, d). 1As in, for instance, the inverse function theorem in analysis or B´ezout’s theorem in algebraic geometry.
See also [YSJ19, BELM20] for versions of this claim with neural networks. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(cid:18)(cid:113) (cid:19)
˜d/d 2. The distribution µ of the covariates xi satisﬁes isoperimetry (or is a mixture theoreof). 3. The expected conditional variance of the output (i.e., the “noise level”) is strictly positive, denoted σ2 := Eµ[V ar[y|x]] > 0.
Then, with high probability over the sampling of the data, one has simultaneously for all f ∈ F: 1 n n (cid:88) (f (xi) − yi)2 ≤ σ2 − (cid:15) ⇒ Lip(f ) ≥ (cid:101)Ω i=1 (cid:32) (cid:115) (cid:15) (cid:33)
. nd p
Remark 1.1. For the distributions µ we have in mind, for instance uniform on the unit sphere, there exists with high probability some O(1)-Lipschitz function f : Rd → R satisfying f (xi) = yi for all i. Indeed, with probability 1 − e−Ω(d) we have ||xi − xj|| ≥ 1 for all 1 ≤ i (cid:54)= j ≤ n so long as n ≤ poly(d). In this case we may apply the Kirszbraun extension theorem to ﬁnd a suitable f regardless of the labels yi. More explicitly we may ﬁx a smooth bump function g : R+ → R with g(0) = 1 and g(x) = 0 for x ≥ 1, and then interpolate using the sum of radial basis functions f (x) = n (cid:88) i=1 g(||x − xi||)yi.
In fact this construction requires only p = n(d + 1) parameters to specify the values (xi, yi)i∈[n] and thus determine the function f . Hence p = n(d + 1) parameters sufﬁce for robust interpolation, i.e. Theorem 3 is essentially best possible for L = O(1). A similar construction shows the same conclusion for any p ∈ [(cid:101)Ω(n), nd], essentially tracing the entire tradeoff curve. This is because one can ﬁrst project onto a ﬁxed subspace of dimension ˜d = p/n, and the projected inputs xi now have pairwise distances at least Ω with high probability. The analogous construction on the projected points now requires only p = ˜dn parameters and has Lipschitz constant L = (cid:18)(cid:113) (cid:19) d/ ˜d
O
= O (cid:16)(cid:113) nd p (cid:17)
. 1.1 Speculative implication for real data
To put Theorem 1 in context, we compare to the empirical results presented in [MMS+18]. In the latter work, they consider the MNIST dataset which consists of n = 6 × 104 images in dimension 282 = 784. They trained robustly different architectures, and reported in Figure 4 the size of the architecture versus the obtained robust test accuracy (third plot from the left). One can see a sharp transition from roughly 10% accuracy to roughly 90% accuracy at around 2 × 105 parameters (capacity scale 4 in their notation). Moreover the robust accuracy keeps climbing up with more parameters, to roughly 95% accuracy at roughly 3 × 106 parameters.
How can we compare these numbers to the law of robustness? There are a number of difﬁculties that we discuss below, and we emphasize that this discussion is highly speculative in nature, though we ﬁnd that, with a few leaps of faith, our universal law of robustness sheds light on the potential parameter regimes of interest for robust deep learning.
The ﬁrst difﬁculty is to evaluate the “correct” dimension of the problem. Certainly the number of pixels per image gives an upper bound, however one expects that the data lies on something like a lower dimensional sub-manifold. Optimistically, we hope that Theorem 1 will continue to apply for an appropriate effective dimension which may be rather smaller than the literal number of pixels.
This hope is partially justiﬁed by the fact that isoperimetry holds in many less-than-picturesque situations, some of which are stated in the next subsection.
The next difﬁculty is to estimate/interpret the noise value σ2. From a theoretical point of view, this noise assumption is necessary for otherwise there could exist a smooth classiﬁer with perfect accuracy in F, defeating the point of any lower bound on the size of F. We tentatively would like to think of σ2 as capturing the contribution of the “difﬁcult” part of the learning problem, that is σ2 could be thought of as the non-robust generalization error of reasonably good models, so a couple 2
of % of error in the case of MNIST. With that interpretation, one gets “below the noise level” in
MNIST with a training error of a couple of %. We believe that versions of the law of robustness might hold without noise; these would need to go beyond representational power and consider the dynamics of learning algorithms.
Finally another subtlety to interpret the empirical results of [MMS+18] is that there is a mismatch between what they measure and our quantities of interest. Namely the law of robustness talks about two things: the training error, and the worst-case robustness (i.e., the Lipschitz constant). On the other hand [MMS+18] measures the robust generalization error. Understanding the interplay between those three quantities is a fantastic open problem. Here we take the perspective that a small robust generalization error should imply a small training error and a small Lipschitz constant.
Another important mismatch is that we stated our universal law of robustness for Lipschitzness in (cid:96)2, while the experiments in [MMS+18] are for robustness in (cid:96)∞. We believe that a variant of the law of robustness remains true for (cid:96)∞, a belief again partially justiﬁed by how broad isoperimetry is (see next subsection).
With all the caveats described above, we can now look at the numbers as follows: in the [MMS+18] experiments, smooth models with accuracy below the noise level are attained with a number of parameters somewhere in the range 2 × 105 − 3 × 106 parameters (possibly even larger depending on the interpretation of the noise level), while the law of robustness would predict any such model must have at least nd parameters, and this latter quantity should be somewhere in the range 106 − 107 (corresponding to an effective dimension between 15 and 150). While far from perfect, the law of robustness prediction is far more accurate than the classical rule of thumb # parameters (cid:39) # equations (which here would predict a number of parameters of the order 104).
Perhaps more interestingly, one could apply a similar reasoning to the ImageNet dataset, which consists of 1.4 × 107 images of size roughly 2 × 105. Estimating that the effective dimension is a couple of order of magnitudes smaller than this size, the law of robustness predicts that to obtain good robust models on ImageNet one would need at least 1010 − 1011 parameters. This number is larger than the size of current neural networks trained robustly for this task, which sports between 108 − 109 parameters. Thus, we arrive at the tantalizing possibility that robust models for ImageNet do not exist yet simply because we are a couple orders of magnitude off in the current scale of neural networks trained for this task. 1.2