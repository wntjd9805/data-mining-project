Abstract
Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide efﬁcient querying mechanism over large and incomplete databases.
Current approaches employ spatial geometries such as boxes to learn query rep-resentations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities.
Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also deﬁne the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query rea-soning problem, we demonstrate that the proposed PERM signiﬁcantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM’s competence on a COVID-19 drug-repurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM’s query answering process through a low-dimensional visualization of the Gaussian representations. 1

Introduction
Knowledge Graphs (KGs) are structured heterogeneous graphs where information is organized as triplets of entity pair and the relation between them. This organization provides a ﬂuid schema with applications in several domains including e-commerce [1], web ontologies [2, 3], and medical research [4, 5]. Chain reasoning is a fundamental problem in KGs, which involves answering a chain of ﬁrst-order existential (FOE) queries (translation, intersection, and union) using the KGs’ relation paths. A myriad of queries can be answered using such logical formulation (some examples are given in Figure 1). Current approaches [6, 7, 8] in the ﬁeld rely on mapping the entities and relations onto a representational latent space such that the FOE queries can be reduced to mathematical operations in order to further retrieve the relevant answer entities.
Euclidean vectors [6, 9] provide a nice mechanism to encode the semantic position of the entities by leveraging their neighborhood relations. They utilize a ﬁxed threshold over the vector to query for answer entities (such as a k-nearest neighbor search). However, queries differ in their breadth. Certain queries would lead to a greater set of answers than others, e.g., query Canadians will result in a higher number of answers than query Canadian Turing Award winners. To capture this query behavior, spatial embeddings [7, 8, 10, 11] learn a border parameter that accounts for broadness of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Drug Repurposing (DRKG). (b) Open-domain (FB15K).
Figure 1: Sample FOE queries from different datasets that utilize existential quantiﬁcation ( intersection ( end-to-end objective function to retrieve relevant results for complex queries.
), 9
) operations. The simple operations need to be chained together in an
), and union (
\
[ queries by controlling the volume of space enclosed by the query representations. However, these spatial embeddings rely on more complex geometries such as boxes [7] which do not have a closed form solution to the union operation, e.g., the union of two boxes is not a box. Thus, further FOE operations cannot be applied to the union operation. Additionally, their strict borders lead to some ambiguity in the border case scenarios and a non-smooth distance function, e.g., a point on the border will have a much smaller distance if it is considered to be inside the box than if it is considered to be outside. This challenge also applies to other geometric enclosures such as hyperboloids [8].
Another line of work includes the use of structured geometric regions [12, 7] or density functions
[13, 14, 11, 15] instead of vector points for representation learning. While these approaches utilize the representations for modeling individual entities and relations between them, we aim to provide a closed form solution to logical queries over KGs using the Gaussian density function which enables chaining the queries together. Another crucial difference in our work is in handling a stream of queries. Previous approaches rely on Disjunctive Normal Form (DNF) transformation which requires the entire query input. In our model, every operation is closed in the Gaussian space and, thus, operations of a large query can be handled individually and aggregated together for the ﬁnal answers. (a) Union of box queries. (b) Gaussian mixture queries. (c) Distance of entities from query space. For comparability, distances are given relative to entity Bengio’s distance to European query.
Canadians. Entities in the darker areas have higher
Figure 2: Results of the query Europeans probability of being the answers than lighter areas. We can observe from (c) that the non-smooth borders of box geometry do not encompass the answer Hinton.
[
To alleviate the drawbacks of operations not being closed under unions and border ambiguities, we propose Probabilistic Entity Representation Model (PERM). PERM models entities as a mixture of
Gaussian densities. Gaussian densities have been previously used in natural language processing [14] and graphs [15] to enable more expressive parameterization of decision boundaries. In our case, we 2
utilize a mixture of multivariate Gaussian densities due to their intuitive closed form solution for translation, intersection, and union operations. In addition, they can also enable the use of a smooth distance function; Mahalanobis distance [16]. Figure 2 provides an example of such a case where the non-smooth boundaries of box query embeddings are not able to capture certain answers. We utilize the mean (µ) and co-variance (⌃) parameters of multivariate Gaussian densities to encode the semantic position and spatial query area of an entity, respectively. The closed form solution for the operations allows us to solve complex queries by chaining them in a pipeline. PERM does not need to rely on DNF transformations, since all the outputs are closed in the Gaussian space and complex queries can be consolidated in an end-to-end objective function, e.g., in Figure 2b,
Canadians is a Gaussian mixture and the single objective is to minimize the distance
Europeans between the mixture and entity Hinton, whereas in the case of boxes (shown in Figure 2a), we have two independent objectives to minimize the distance from each box in the union query. Summarizing, the contributions of our work is as follows:
[ 1. We develop Probabilistic Entity Representation Model (PERM), a method to reason over KGs using (mixture of) Gaussian densities. Gaussians are able to provide a closed form solution to intersection and union, and also a smooth distance function. This enables us to process a chain of complex logical queries in an end-to-end objective function. 2. PERM is able to outperform the current state-of-the-art baselines on logical query reasoning over standard benchmark datasets. Additionally, it is also able to provide better drug recommendations for COVID-19. 3. PERM is also interpretable since the Gaussian embeddings can be visualized after each query process to understand the complete query representation.
The rest of the paper is organized as follows: Section 2 presents the current work in the ﬁeld. In section 3, we present PERM and deﬁne its various operations. Section 4 provides the formulation for building the reasoning chains for complex queries. We provide the experimental setup and results in section 5. We conclude our paper in section 6 and present its broader impact in section 7. 2