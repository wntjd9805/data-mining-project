Abstract
This paper addresses the problem of Unbalanced Optimal Transport (UOT) in which the marginal conditions are relaxed (using weighted penalties in lieu of equality) and no additional regularization is enforced on the OT plan. In this context, we show that the corresponding optimization problem can be reformulated as a non-negative penalized linear regression problem. This reformulation allows us to propose novel algorithms inspired from inverse problems and nonnegative matrix factorization. In particular, we consider majorization-minimization which leads in our setting to efﬁcient multiplicative updates for a variety of penalties.
Furthermore, we derive for the ﬁrst time an efﬁcient algorithm to compute the regularization path of UOT with quadratic penalties. The proposed algorithm provides a continuity of piece-wise linear OT plans converging to the solution of balanced OT (corresponding to inﬁnite penalty weights). We perform several numerical experiments on simulated and real data illustrating the new algorithms, and provide a detailed discussion about more sophisticated optimization tools that can further be used to solve OT problems thanks to our reformulation. 1

Introduction
Optimal Transport (OT) theory provides powerful tools for comparing probability distributions and has been successfully employed in a wide range of machine learning applications such as supervised learning (Frogner et al., 2015), clustering (Ho et al., 2017), generative modelling (Arjovsky et al., 2017), domain adaptation (Courty et al., 2017), learning of structured data (Maretic et al., 2019;
Vayer et al., 2019) or natural language processing (Kusner et al., 2015), among many others. One reason for those recent successes is the introduction of entropy-regularized OT that can be solved with the efﬁcient Sinkhorn-Knopp matrix scaling algorithm (Cuturi, 2013). However, the classical
OT problem seeks the optimal cost to transport all the mass from a source distribution to a target one
∗First two authors have equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(Villani, 2009), greatly limiting its use in scenarii where the measures have different masses or when they contain noisy observations or outliers.
Unbalanced Optimal Transport (UOT) (Benamou, 2003) has been introduced to tackle this shortcom-ing, allowing some mass variation in the transportation problem. It is expressed as a relaxation of the
Kantorovich formulation (Kantorovich, 1942) by penalizing the divergence between the marginals of the transportation plan and the given distributions. Several divergences can be considered, such as the
Kullback-Leiber (KL) divergence (Frogner et al., 2015; Liero et al., 2018), the (cid:96)1 norm corresponding to the partial optimal transport problem (Caffarelli and McCann, 2010; Figalli, 2010), or the squared (cid:96)2 norm (Benamou, 2003). Regarding numerical solutions, Chizat et al. (2018) considered an entropic-regularized version of UOT leading to a class of scaling algorithms in the vein of the Sinkhorn-Knopp approach (Sinkhorn and Knopp, 1967). The introduction of this entropic regularization improves the scalability of OT, but involves a spreading of the mass and a loss of sparsity in the OT plan. When a sparse transport plan is sought, the convergence is slowed down, necessitating the use of acceleration strategies (Thibault et al., 2021). Regarding UOT with the (squared) (cid:96)2 norm, Blondel et al. (2018) showed that the resulting OT plan is sparse and proposed to use an efﬁcient L-BFGS-B algorithm (Byrd et al., 1995) to address this case. Note that the L-BFGS-B method can be used to solve UOT with differentiable divergences even without the entropic-regularization on the OT plan that induces the Sinkhorn-like iterations. Also note that, as for balanced OT, UOT can be solved more efﬁciently when the data has a speciﬁc structure, such as unidimensional distributions (Bonneel and Coeurjolly, 2019) or distributions supported on trees (Sato et al., 2020). Finally, recent work investigated UOT between Gaussians and provided closed form solutions for the regularized Janati et al. (2020) and unregularized (Janati, 2021, Eq. 2.72) versions of UOT associated with a KL divergence.
Contributions.
In this paper, we show after some preliminaries that UOT can be recast as a convex penalized linear regression problem with non-negativity constraints (Section 2.2). The main interest of this reformulation resides in the fact that non-negative linear regression has been extensively studied in inverse problems and machine learning, offering a large panel of tools for devising new numerical algorithms. Our reformulation involves a design/dictionary matrix that is structured and sparse. Leveraging this structure, we propose two new families of algorithms for solving the exact (i.e., without regularization of the plan) UOT problem in Section 3.
We ﬁrst derive in Section 3.1 a new Majorization-Minimization (MM) algorithm for solving UOT with Bregman divergences, and more speciﬁcally KL and (cid:96)2-penalized UOT. The MM approach results in multiplicative updates that have appealing features: i) they are easy to implement, ii) have low complexity per iteration and can be instantiated on GPU, iii) ensure monotonicity of the objective function and inherit existing convergence results. Our methodology is inspired by well-known algorithms in image restoration (Richardson, 1972; De Pierro, 1993) and non-negative matrix factorization (NMF) (Lee and Seung, 2001; Dhillon and Sra, 2005; Févotte and Idier, 2011).
Interestingly, the resulting multiplicative updates bear a similarity with the celebrated Sinkhorn scaling algorithm, with some key differences that are discussed.
Next, we derive in Section 3.2 an efﬁcient algorithm to compute the regularization path in (cid:96)2-penalized UOT. To do so, we build on our proposed reformulation and more precisely on the fact that (cid:96)2-penalized UOT can be reformulated as a weighted Lasso problem. We propose a new methodology inspired by LARS (Efron et al., 2004; Hastie et al., 2004), which, to the best of our knowledge, is the
ﬁrst regularization path algorithm for OT problems. It brings a novel understanding of the properties of the evolution of the support of OT plans, besides the practical interest of computing the complete regularization path when hyperparameter validation is necessary.
Our new families of algorithms (MM for general UOT, LARS for (cid:96)2-penalized UOT) are showcased in the numerical experiments of Section 4. Python implementation of the algorithms, provided in supplementary, will be released with MIT license on GitHub. The connection between UOT and linear regression that we reveal in the paper opens the door to further fruitful developments and in particular to more efﬁcient algorithms, thanks to the large literature dealing with non-negative penalized linear regression. We discuss those possible research directions in Section 5, before concluding the paper.
Notations. Vectors such as m are written with lower case and bold font, with coefﬁcients mi or
[m]i, according to context. The is written mA.
Matrices such as M are written with upper case and bold font, with coefﬁcients Mi,j. We introduce a vectorization operator deﬁned by m = vec(M ) = [M1,1, M1,2, . . . , Mn,m−1, Mn,m](cid:62), i.e., the
-dimensional sub-vector with indexes in set
|A|
A 2
concatenation of the rows of the matrix, following the Numpy/C memory convention. 1n is a vector 0 denotes entry-wise non-negativity. Finally, Dϕ is the Bregman divergence of n ones and M generated by the strictly convex and differentiable function ϕ, i.e., Dϕ(u, v) = (cid:80) i dϕ(ui, vi) = (cid:80)
≥ i[ϕ(ui)
−
ϕ(vi)
ϕ(cid:48)(vi)(ui vi)].
−
− 2 Reformulation of UOT as non-negative penalized linear regression 2.1