Abstract
In this work, we propose to leverage out-of-distribution samples, i.e., unla-beled samples coming from outside target classes, for improving few-shot learn-ing. Speciﬁcally, we exploit the easily available out-of-distribution samples (e.g., from base classes) to drive the classiﬁer to avoid irrelevant features by maximizing the distance from prototypes to out-of-distribution samples while minimizing that to in-distribution samples (i.e., support, query data). Our ap-proach is simple to implement, agnostic to feature extractors, lightweight with-out any additional cost for pre-training, and applicable to both inductive and transductive settings. Extensive experiments on various standard benchmarks demonstrate that the proposed method consistently improves the performance of pretrained networks with different architectures. Our code is available at https://github.com/VinAIResearch/poodle. 1

Introduction
Learning with limited supervision is a key challenge to translate the research efforts of deep neural networks to real-world applications where large-scale annotated datasets are prohibitively costly to acquire. This issue has motivated the recent topic of few-shot learning (FSL), which aims to build a system that can quickly learn new tasks from a small number of labeled data.
A popular group of methods in FSL focus on strengthening the backbone network by various techniques, from increasing model capacity [6, 11], self-supervised learning (SSL) [18, 51, 65], to knowledge distillation (KD) [53]. With these techniques, few-shot methods are expected to learn better representations that are more robust and generalized. However, even if the network can discover visual features and semantic cues, few-shot learners have to deal with a key challenge - the ambiguity: as we have only a small amount of support evidence, there are multiple plausible hypotheses at the inference stage. Existing works, therefore, rely on the developed inductive bias of the network (during the pretraining stage), such as shape bias [47, 14], to reduce the hypothesis space.
In this work, we view the classiﬁcation problem as conditional reasoning, i.e., “if X has P then X is
Q”. Human beings are good at learning such inferences, thus quickly grasping new concepts with minimal supervision. More importantly, humans learn new concepts in context - where we have already had prior knowledge about other entities. According to mental models in cognitive science, when assessing the validity of an inference, one would retrieve counter-examples, i.e., which do not
⇤First two authors contribute equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of advantages of counter-example data. The query image has features from both support classes (i.e., a child and a suit), which makes classiﬁcation ambiguous. The prediction result would depend on the inductive bias or prior knowledge of the network. By using an out-of-distribution sample, it becomes clear that the query result should favor Class 2. lead to the conclusion despite satisfying the premise [9, 13, 29, 30, 55]. Thus, if there exists at least one of such counter-examples, the inference is known to be erroneous.
Hence, we attempt to equip few-shot learning with the above ability so that it can eliminate incorrect hypotheses when learning novel tasks in a data-driven manner. Speciﬁcally, we leverage out-of-distribution data, i.e., samples belonging to classes separated from novel tasks 2, as counter-examples for preventing the learned prototypes from overﬁtting to their noisy features. To that end, when learning novel tasks, we adopt the large margin principle in metric learning [59] to encourage the learned prototypes to be close to support data while being distant from out-of-distribution samples.
Our approach is complementary to existing works in FSL and could be combined to advance the state of the art. Moreover, our method is agnostic to the backbone network; thus, it does not have the need of a training phase to adopt as in SSL and KD, while incurring just a little overhead at inference (for
ﬁne-tuning prototypes with approximately 200 gradient updating steps).
In summary, our contributions are as follows:
• We propose a novel yet simple approach to learn the inductive bias of deep neural networks for
FSL by leveraging out-of-distribution data. We empirically show that out-of-distribution data only require weak labels (i.e., in the form of whether a sample is in- or out-of-distribution) even in challenging problems such as cross-domain FSL.
• We introduce a new loss function to implement the above idea, which is applicable for both inductive and transductive inference. Our extensive experiments on different standard benchmarks show that the proposed approach consistently improves the performance of various network architectures.
• We validate the effectiveness of our method in various FSL settings, including cross-domain FSL. 2