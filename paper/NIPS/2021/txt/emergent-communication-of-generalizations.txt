Abstract
To build agents that can collaborate effectively with others, recent research has trained artiﬁcial agents to communicate with each other in Lewis-style referential games. However, this often leads to successful but uninterpretable communication.
We argue that this is due to the game objective: communicating about a single object in a shared visual context is prone to overﬁtting and does not encourage language useful beyond concrete reference. In contrast, human language conveys a rich variety of abstract ideas. To promote such skills, we propose games that require communicating generalizations over sets of objects representing abstract visual concepts, optionally with separate contexts for each agent. We ﬁnd that these games greatly improve systematicity and interpretability of the learned languages, according to several metrics in the literature. Finally, we propose a method for identifying logical operations embedded in the emergent languages by learning an approximate compositional reconstruction of the language. 1

Introduction
The communication systems that emerge when two agents are trained to cooperate offer a window on the evolution of human language, as well as a promising avenue for improving the collaboration abilities of artiﬁcial agents. Much recent work studies Lewis-style [24] signaling games (Figure 1a), where agents are trained to refer to a single object in a shared visual context. However, a general consensus of this work is that without careful environmental pressures, agents develop successful but uninterpretable communication schemes distinctly unlike human language [1, 5, 6, 16, 20].
We argue that the reference games typically used in these studies are ill-suited to drive linguistic systematicity for two reasons. One is perceptual: agents can exploit inscrutable patterns in single inputs, which leads to communication via spurious features [3]. The other reason is cognitive: human language can convey abstract ideas, such as kinds and causes, not only reference to speciﬁc objects.
Simple reference games are unlikely to drive emergence of such abstract language. In particular, generalizations over categories are a crucial part of language [36], helping us transfer knowledge that may be useful in the future. For example, we would like to teach our kin not just to avoid one speciﬁc lion, but to avoid all lions, including those that have not yet been seen. Some have even argued that language emerged precisely from this need to teach hard-won generalizations to others [19]. With this idea in mind, can we design an experimental setting that better catalyzes these abilities?
In this paper, we propose extensions of Lewis-style signaling games to sets. In the set reference (setref) game, a teacher must communicate to a student not just a single object, but rather a group of objects belonging to a concept (Figure 1b). In the concept game, each agent sees different examples of the concept (Figure 1c). Inspired by human teaching [9], our core insight is that requiring generalization to combinatorially large sets of (possibly unseen) objects encourages agents to learn and communicate rich abstractions across inputs (e.g. seagulls), instead of low-level features (e.g. color #FDA448).
These tasks are more difﬁcult than traditional reference games, and we will show with a variety of metrics that the learned languages are more systematic, compositional, and interpretable. Finally, the rich compositional space of concepts explored in these games allows us to probe for speciﬁc logical 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Communication games for the concept red triangle. Given a set of targets (red borders) and distractors, a teacher must send a message to help a student identify the targets. In (a) reference games, targets are identical; in (b) set reference (setref) games, there are multiple targets; and in (c) concept games, the agents see different inputs. operators in the emergent language. We propose a method for doing so, thereby demonstrating how the emergent languages reﬂect the compositional structure of their inputs. 2 Communication Games n } is a set of labels for the teachers’ inputs, deﬁned as yT
First imagine a generic communication game between a teacher T and student S. Let G = (c, X T , Y T , X S, Y S) be a communication game, where c : X (cid:55)→ {0, 1} is a latent concept to be communicated, X T = {xT n } is a set of n inputs presented to the teacher, and
Y T = {yT i a target if yT is a distractor and yT i = 0. X S and Y S are deﬁned similarly for the student. Given its targets and distractors (but not the latent concept c), the teacher must send a message m to a student that allows them to correctly identify their own targets, where m = (m1, . . . , mn) is a discrete sequence over a ﬁxed alphabet mi ∈ M. Now we can deﬁne variants of this communication game as follows: i ). We call xT i = c(xT is a member of the concept c; otherwise xT i 1 , . . . , yT i = 1, which indicates that xT i 1 , . . . , xT
Reference game.
X S, Y T = Y S) and there is a single (repeated) target: xT
In basic reference games, the teacher and student see the same examples (X T = i = xT j for all i, j where yT j = 1.1 i = yT
Set reference (setref) game. Now we extend our game to sets: the teacher and student see the same examples, but there are multiple target images encoding the concept (e.g. different red triangles).
Concept game. Finally, we propose the more abstract concept game, where the teacher and student see different examples (X T (cid:54)= X S, Y T (cid:54)= Y S) of the same concept. When X T and Y T contain a single positive and negative example, this is a reference game with separate inputs for each agent, a setup which has been shown to encourage linguistic systematicity in some settings [8, 21, 22]. 3 Models
Now we will formalize our models of the teacher and student. Given a communication game G, a teacher is deﬁned as a distribution over messages given inputs pT (m | X T , Y T ), and a student is a distribution over targets given a message: pS(Y S | X S, m) = (cid:81)
| xS i pS(yS i i , m).
Teacher. The teacher encodes all inputs with a convolutional neural network (CNN) f T
θ ; embed-dings for targets and distractors are averaged to form target and distractor prototype embeddings
[33],2 which then conditions a recurrent neural network (RNN) used to produce the message. Let
− denote the sets of targets and distractors in X T ; then deﬁne a prototype embedding
X T
+ and X T 1For the most consistent comparison across games, our reference game has multiple identical targets and student target decisions made independently for each input, instead of the single-target forced-choice setting.
Appendix E shows results with traditional games trained with cross entropy loss; conclusions are the same. 2Note that the teacher’s job is one of representation learning for sets [42] and thus one can use any set repre-sentation learning method beyond the prototypical networks explored here. As a more advanced implementation, we tried a variant of Set Transformers [23], but this did not give any tangible performance beneﬁt. 2
Figure 2: Example games, with targets (red border) and distractors for the ShapeWorld concept blue OR rectangle (left) and the Birds concept painted bunting (right). Concepts are represented as intensional logical formulas (top) or extensional sets of boolean input features (bottom), where each vector is the binary representation of an individual member of the concept (e.g. blue rectangle, or the labeled attributes of one particular bird). See Figure S1 in Appendix A for additional game examples. (cid:80)
+ = 1 xT
|X T
+ |
+; xT proj([xT xi∈X T
+ f T
θ (xi) (analogously for xT
−). Then pT (m | X T , Y T ) = pRNN-DECODE(m |
−])) where proj is a linear projection to the RNN hidden state.
Student. The student takes a message and makes predictions about the labels ˆyS i for each input xS i . Given the teacher message and an input image, deﬁne pS(yS i
σ(RNN-ENCODE(m) · f S
φ is a separate CNN for the student. i )), where f S
φ (xS independently i , m) =
| xS
We jointly train a teacher-student pair, including the vision modules and communication protocol, via stochastic gradient descent to maximize the student likelihood of selecting the correct target images.
Formally, the loss for a single game is
L(T, S, G) = − (cid:88) i log pS(yS i
| xS i , ˆm),
ˆm ∼ pT (m | X T , Y T ). (1)
To maintain backwards differentiability, we use the straight-through Gumbel-Softmax [14] trick with
τ = 1, simulating samples from the teacher distribution over tokens via softmax and discretizing in the forward pass only. For full model and training details and a link to code, see Appendix A. 4 Tasks
We examine the languages developed for our proposed communication games over two datasets: ﬁrst, an artiﬁcial shape dataset which allows us to evaluate communication over cleanly deﬁned logical concepts; second, a dataset of birds to test agents’ ability to learn concepts from realistic visual input.
ShapeWorld. We use the ShapeWorld visual reasoning dataset [18] (Figure 2, left). For reference games, target images are a single object, deﬁned by a conjunction of a shape and a color (e.g. red triangle, green square); of the 30 possible shapes, we hold out 20% for testing. For setref and concept games, concepts include the conjunctions tested in reference games, but also primitive concepts (e.g. blue shapes) and arbitrary disjunctions or conjunctions of (possibly negated) shapes and/or colors.
This produces 312 concepts, 20% of which are held out for testing. These more general concepts cannot be tested in reference games, since a single object is always identiﬁed by a shape and color.
This rules out disjunctive concepts like red OR blue that only make sense across multiple objects.
Similarly, since reference game targets must necessarily have both color and shape, we can never guarantee that a message for a reference game only carries the semantics blue and not, for example, blue circle, if the target is a blue circle. By looking at sets, setref and concept games allow us to more precisely control the semantics of the concepts in each game.
Each game consists of 10 targets depicting shapes satisfying the concept, and 10 distractors. We speciﬁcally sample “hard” targets and distractors to test understanding of conjunctions or disjunctions (see Appendix B for details). Finally, we specify an agent vocabulary of 14 tokens and maximum length 5, so that the communication channel has the same bandwidth as the true concept formulas;3 see Appendix C for experiments varying these parameters for both this dataset and the next one.
Birds. We next use the Caltech-UCSD Birds dataset [40] which contains 200 classes of birds with 40–60 images (Figure 2, right). As before, reference games involve a single target; setref and concept 3In the true concept formulas there are 5 shapes, 6 colors, and the 3 AND/OR/NOT operators, i.e. 14 tokens; and the longest concept formulas have the form NOT x AND NOT y, i.e. length 5. 3
game targets are members of a speciﬁc bird class. We use 100 classes at train and 50 at test, sampling 5 targets and 5 distractors per game. The dataset contains boolean attributes (e.g. beak, size) for individual birds and classes.4 Thus, we represent reference game concepts as the feature vector of the target bird, and setref/concept game concepts as the feature vector of the class. In our evaluation, we will measure how well the languages capture these features. As there is no reference language for this task, we set the vocabulary size to 20 and the message length to 8 (though again see Appendix C). 5 Evaluation
We ﬁrst measure communication success, as deﬁned by student accuracy on held-out games from seen and unseen concepts, with the unseen concepts testing a language’s ability to generalize compositionally. Ultimately, however, we are interested in the systematicity of the learned languages, which we evaluate via the following measures:
Information theoretic measures. We ﬁrst ignore the speciﬁc content of messages and concepts, and simply compute simple information theoretic quantities, by treating each distinct message and concept as a unique value and imagining probability distributions over these values. First, we measure the conditional entropy of teacher messages given concepts, H(M | C), averaged across seen and unseen games; lower entropy indicates that agents use more consistent language for a ﬁxed concept.
However, H(M | C) measures systematicity only in one direction; as a more symmetric measure, we also use the adjusted mutual information
AMI(M, C) = (I(M, C) − E(I(M, C))) / (max(H(M ), H(C)) − E(I(M, C))) . (2)
Here, E(I(M, C)) is evaluated with respect to a hypergeometric model of randomness, where M and C are assumed to be random permutations subject to the number of unique values in either set [39]. AMI ∈ [0, 1] represents the mutual information between M and C, adjusted for chance to maintain comparability across different distributions of messages and concepts. A higher score indicates overall higher alignment between messages and concepts.
Topographic ρ. To more precisely measure the lexical compositionality of a language, a measure often used in the literature is topographic ρ [4, 20, 22, 25], which quantiﬁes the agreement between two representational systems a la Representational Similarity Analysis [17]. We deﬁne a distance metric between game concepts dC(ci, cj) and another between agent messages dM (mi, mj), compute pairwise distances between concepts and between messages, and measure their alignment with
Spearman’s ρ. A high ρ indicates that a teacher sends lexically similar messages for similar concepts.
For our distance function on messages, we use the Edit (i.e. Levenshtein) distance with equal insert/delete/replace costs. For distances between game concepts, we deﬁne two distances based on intensional and extensional representations of concepts (Figure 2). First, we use the word-level Edit distance between string representations of logical formulas. Second, we use the Hausdorff distance dH , a distance function between sets of members of a concept. Let Z a = {za n} be the set of feature-based boolean representations of inputs belonging to concept a. For ShapeWorld, these are two-hot vectors denoting the color and shape of all objects belonging to a speciﬁc formula; for example, for the concept red, we have vectors for red triangle, red circle, and so on. For Birds, these are boolean vectors of the attributes of each individual bird of a species. Then the Hausdorff distance dH is the maximum distance from any point in one set to the closest point in the other: dH (Z a, Z b) = max(supi d(za j , Z a)), where d(a, B) = inf b∈B EditDistance(a, b). i , Z b), supj d(zb 1 , . . . , za 6 Results
Table 1 shows test accuracy, as measured by student classiﬁcation accuracy (partial credit given), on communication games over seen and unseen concepts for 5 models trained in each condition.
Reference game performance is high across both datasets, and agents are able to generalize well to unseen games. Accuracy on setref and concept games is lower, with lower performance on novel games in both datasets.5 Overall, communicating sets is a much harder task than speciﬁc reference. 4Feature vectors for individual birds in a class vary due to the visibility of features in the image; class vectors are averaged across all individual birds, then rounded to 1 or 0. 5To calibrate setref and concept performance, Appendix D tests listeners on ideal (human) languages. 4
Table 1: Student accuracy (seen and unseen concepts, where chance accuracy is 50%), conditional entropy of messages given concepts (lower is better), and adjusted mutual information score (higher is better), with (SD) across 5 runs.
Dataset
Game
Acc (Seen) Acc (Unseen) H(M | C) AMI(M, C)
ShapeWorld Ref
Birds
Setref
Concept
Ref
Setref
Concept 97 (0.4) 92 (2.2) 88 (3.4) 93 (0.3) 89 (0.2) 88 (0.1) 98 (0.3) 87 (1.6) 75 (3.0) 89 (0.1) 78 (0.2) 73 (0.3) 7.3 (0.2) 3.9 (0.6) 2.4 (0.2) 5.9 (0.2) 5.2 (0.1) 4.1 (0.2) 0.04 (0.00) 0.59 (0.08) 0.66 (0.07) 0.05 (0.00) 0.17 (0.02) 0.26 (0.02)
The ability to communicate accurately, even on unseen concepts, is not necessarily indicative of more systematic communication; generalization without compositional language is a common ﬁnding in the literature [1, 6, 16, 22]. Instead, we ﬁnd that the more difﬁcult games produce more systematic language. For ShapeWorld, concept game entropy over messages is lower than reference game entropy (2.4 vs. 7.3), and AMI is higher (0.66 vs. 0.04), with setref in the middle; this pattern also occurs in Birds. Furthermore, Figure 3 shows that topographic ρ between the languages and the (Edit and Hausdorff) concept distances is higher for concept and setref than ref, throughout training.
Figure 4 (more examples in Appendix F) shows messages generated by agents for concepts in both games, where we arbitrarily assign letters to agent “words” to assist with interpretation. For example, for concept game teachers conveying the concept red AND triangle, the innermost circle is largely red, indicating that the majority of messages sent for this concept begin with d; proceeding outward, we see blue regions indicating the token e. Thus, concept agents consistently use the 5-token sequence deeee to refer to red triangles (less commonly, edeee). In contrast, for different red triangles, the language of reference game agents is strikingly inconsistent, as indicated by the extreme diversity of colors, while setref language is somewhere in the middle. The entropies over the message distributions for these concepts correlate with the “busyness” of the plots, reinforcing the idea that the setref and concept languages are more consistent. 6.1 Set Size
The difference between reference and setref/concept games can be interpreted as a continuum, ranging from referring to a single object (reference) to referring to a potentially inﬁnite num-ber of objects. With a small number of objects, it may still be possible to communicate only low-level, non-generalizable features of the set, similar to the strategies adopted by our refer-ence game agents. In contrast, increasingly large numbers of objects should put further pressures on the semantics of the messages to not refer to individual inputs, but rather entire categories.
In Figure 5, we conﬁrm this hypothesis, show-ing how increasing the number of targets n (with equal numbers of distractors) increases language systematicity. n has a statistically signiﬁcant effect on topographic ρ for Shape-World setref (Spearman ρ = 0.39, p = 0.029) and concept (ρ = 0.75, p < 10−5) and Birds setref (ρ = 0.90, p < 10−7) and concept (ρ = 0.51, p = 0.024). When n = 1, the se-tref game is equivalent to a reference game with 1 target and 1 distractor, and the concept game is similar, but with agents given separate inputs.
Our results suggest that this decoupling, as pro-posed by Lazaridou et al. [21] and often used in the emergent communication literature, pro-motes systematicity in some cases (Birds) but not others (ShapeWorld). We additionally show that (1) sets are an alternative way of encouraging systematicity without needing this separation, and (2) even with this separation, larger set sizes further improve the systematicity of the resulting language.
Figure 5: Topographic ρ (concept Edit distance) with varying number of targets (average over seen/unseen splits). Each point and Ref line is an independent run. 5
Figure 3: Topographic ρ between language and Edit (top) or Hausdorff (bottom) concept distances for seen and unseen games across both datasets. Results from 5 runs plotted, with averages in bold.
Figure 4: Distribution of 300 messages and entropies for game concepts in ref, setref, and concept settings. Messages start at the center and proceed outwards, with each colored section corresponding to a unique token and its frequency of occurrence at that position in the message. Empty spaces indicate end of sentence. For convenience, ShapeWorld plots are partially labeled with tokens. 6.2 Generalizing across game types
To measure the generality of the agents’ strategies, we evaluate their ability to generalize zero-shot across game types. Table 2 shows accuracy and systematicity metrics for agents evaluated on games of a different type. Agents trained on the setref and concept games are able to generalize to reference games (yellow cells), producing systematic referring expressions, as they have already been biased towards generic language. Importantly, setref game agents can generalize to concept games with separate inputs (magenta cells), though to a lesser extent for Birds (75% vs 84%). This suggests that sets pressure agents to learn generalizable features despite the shared input. In contrast, we see little generalization ability from reference games to setref and concept games (orange cells), suggesting that agents are not conveying generalizable features, but rather spurious patterns in the input [3]. 7 Probing for compositionality
The richer set of concepts afforded by our setref and concept games allow for more detailed analyses of the emergent languages beyond the standard metrics presented above. Broadly, we are interested in whether the compositional structure of concepts is reﬂected in the language. For example, our agents produce messages for the primitive concepts red and triangle, as well as the conjunctive concept red AND triangle.6 Natural language is equipped with a composition operator, AND(m1, m2) = m1 AND m2, that operates solely on lexical forms and whose meaning is deﬁned as the conjunction of the meanings of its arguments. Does a similar operator exist in the emergent language (Figure 6a)? 6We cannot do this analysis for reference games, since we cannot test primitive concepts; recall Section 4. 6
Table 2: Accuracy/AMI/Topographic ρ (concept Edit distance) for agents trained on different game types (columns), then evaluated (zero-shot) on different game types (rows). Chance accuracy is 50%.
Gray shaded cells indicate standard test-time evaluation; other cell colors are explained in the text.
Note that for ShapeWorld reference agents, we evaluate only on setref and concept games that use the 30 conjunctive concepts tested in reference games (e.g. red triangle, blue square).
Train Ref
Train Setref
Train Concept
ShapeWorld
Eval Ref
Eval Setref
Eval Concept
Birds
Eval Ref
Eval Setref
Eval Concept 98 (0.4)/.04 (.00)/.00 (.00) 56 (0.1)/.02 (.00)/.00 (.00) 50 (0.0)/.02 (.00)/.00 (.00) 91 (4.3)/.43 (.09)/.42 (.11) 90 (1.3)/.59 (.08)/.12 (.03) 90 (4.6)/.59 (.08)/.12 (.03) 83 (5.6)/.60 (.04)/.63 (.08) 83 (6.7)/.66 (.07)/.09 (.01) 82 (3.0)/.66 (.07)/.09 (.01) 91 (0.8)/.05 (.00)/.04 (.01) 64 (1.7)/.03 (.01)/.15 (.01) 56 (0.9)/.03 (.01)/.15 (.01) 85 (1.1)/.14 (.02)/.16 (.02) 84 (1.1)/.17 (.02)/.37 (.04) 75 (0.9)/.17 (.02)/.37 (.04) 82 (0.9)/.20 (.02)/.15 (.01) 78 (0.7)/.26 (.02)/.40 (.03) 82 (0.6)/.26 (.02)/.40 (.03)
We propose to learn such an operator by training a model to compose messages in the emergent language to form their conjunctions. Like the English AND, this operator must be able to combine any of the concepts in the language, and must crucially generalize to novel combinations of features.
If, given new concepts, we can reconstruct a message that induces the right behavior in the student, this suggests our model has learned some analog of AND in the language. The reconstruction accuracy of this model can then be interpreted as a much more explicit measure of compositionality than the measures explored above: it reveals the degree to which the syntax of the emergent language operationalizes the speciﬁc logical operations present in the underlying space of concepts.
Our method is inspired by the Tree Reconstruction Error (TRE) metric proposed by Andreas [1], which learns a compositional approximation to a representation space, assuming the latent compo-sitional structure is known.7 However, there are several crucial differences in our method that are optimized for probing for linguistic structure. First, we learn a set of arbitrary composition operations, instead of imposing a predeﬁned operation (e.g. elementwise addition). Moreover, these learned composition operators are actually valid and interpretable linguistic transformations on messages, rather than operations (like addition) that work only on internal representations of sub-concepts. And
ﬁnally, we evaluate our operators on held-out concepts, examining how the reconstructed messages serve the ultimate communicative goal: inducing the correct generalization behavior in the student. 7.1 Learning an Approximate Compositional Reconstruction (ACRe)
Let us ﬁrst formalize the learning problem: after training our agents, we have a dataset of message and concept pairs T = {(mi, ci)} generated by a teacher for each game. Each concept is one of a set of logical forms L(C) deﬁned inductively over a set of primitive concepts C (e.g. red, triangle) and composition operations Ω as follows: 1. Every primitive concept is in L(C): C ⊆ L(C). 2. Every composition of concepts is a concept: let Ωn be the set of n-ary composition opera-tions. Then ∀n, (c1, c2, . . . , cn) ∈ L(C)n, ω ∈ Ωn, we have ω(c1, c2, . . . , cn) ∈ L(C).
T deﬁnes a probability distribution over messages given concepts: pT (m | c). Our aim is to learn an Approximate Compositional Reconstruction to these messages ˆpη(m | c), composed of
η-parameterized message distributions that factorize along the compositional structure of L(C):
ˆpη(m | c) = (cid:26)ˆpc
η(m)
E ˆmi∼ ˆpη(mi|ci) (cid:2)ˆpω
η (m | ˆm1, . . . , ˆmn)(cid:3) if c ∈ C, i.e. c is primitive if c = ω(c1, . . . , cn), (3) 7We cannot apply TRE directly to our setting. Andreas [1] applied TRE to a standard reference game, where targets are represented as conjunctions of shape and color features (e.g. blue square, red triangle). As we mention in Section 4, because reference games cannot test primitive concepts like red and triangle, Andreas [1] proposed to learn representations for primitives which can then be composed via some (predeﬁned) composition function. However, in our setting, it makes little sense to learn arbitrary representations for primitive concepts, when we actually have real messages for such concepts in the ﬁrst place, hence the method we propose. 7
Figure 6: Our ACRe procedure. (a) Does a lexical analog of AND exist in our emergent language? (b) We ﬁrst train primitive LMs to mimic the distribution of agent messages given a ﬁxed concept. (c)
We then train composition operations by sampling arguments from primitive LMs, then training a seq2seq model to mimic the agent message produced for a higher-order concept.
η(m) is a model of the distribution of messages for primitive concept c, and ˆpω where ˆpc
η (m | m1, . . . , mn) is a learned lexical analog to operation ω that takes in messages and outputs a distri-bution over composed messages. Given a concept c, we can sample a message from ˆpη(m | c) by either sampling directly from ˆpc
η (if c is primitive) or recursively sampling messages mi from the constituents of c, then sampling from the corresponding ˆpω
η .
We implement ˆpc
η as small 2-layer transformer-based language models (LMs) [38]. For each c, ˆpc
η is an unconditional LM (i.e. we have an LM for red, triangle, etc.). For n-ary operations ω,
ˆpω
η is a sequence-to-sequence (seq2seq) model decoding from the concatenated arguments: ˆpω
η (m | m1, . . . , mn) = pdecode (m | m1 [SEP] m2 . . . [SEP] mn). Note that this formulation imposes few constraints on the mechanism of the composition operator: for example, we are not enforcing that there exists a token denoting conjunction (AND), or that the arguments must be copied verbatim into the output. These constraints, generally true of human languages, could be explored in future work.
η and ˆpω
η
To train ACRe, let Tc = {(mi, ci) ∈ T | ci = c} be the set of message-concept pairs with concept c, and Tω = {(mi, ci) ∈ T | ci = ω(·)} be the set of messages where ci uses ω. Now, for each primitive c ∈ C, train the LM ˆpc
η on Tc to approximate pT (m | c). Then, freezing these models, for n = 1, . . . , N , ω ∈ Ωn, train the composition model ˆpω
η on Tω. Speciﬁcally, given a pair (m, ω(c1, . . . , cn)), ﬁrst sample messages for the sub-concepts ci from our frozen models:
ˆmi ∼ ˆpη(mi | ci).8 Then, train the composition model to maximize ˆpω
η (m | ˆm1, . . . , ˆmn). For example, given a message bca for the concept red AND triangle, we ﬁrst sample messages for red and triangle from ˆpred
, then train ˆpAND to decode bca from these messages (Figure 6).
Full training and model details are in Appendix G.
η and ˆptriangle
η
η 7.2 Evaluation and results
After training, each ˆpω
η is a learned lexical analog to the composition operation ω. To test whether our approximation has learned the semantics of the language, we hold out 20% of conjunctive and disjunctive concepts c(cid:48) i during training and sample messages for these concepts from the learned
ˆpη(m | c(cid:48) i) according to Equation 3. We evaluate how well these reconstructed messages encode the concepts via (1) lexical overlap with the true teacher messages (as measured by BLEU) and (2) student performance when given our language and a game for the corresponding concept.9
Results are in Table 3. The upper bound on performance is given by the true Teacher language, pT (m | c). We also use language sampled randomly from (1) teacher messages for any concept pT (m) (Random) and (2) teacher messages for the Closest concept as measured by Edit distance (breaking ties randomly). ACRe’s performance is a measure of the degree of compositionality in the language, upper bounded by the teacher language and lower bounded by the random baseline. The results reveal some degree of compositional structure: ACRe reconstructs the training data well and crucially outperforms baselines in predicting messages for unseen concepts. Of course, our ACRe 8As presented, this procedure is only possible if the composition models can be trained and frozen in a
η on concepts of the form
η . Learning from arbitrary concepts is speciﬁc order without circular dependencies. For example, we cannot naively train ˆpω
ω(ω(·)), since sampling from the inner ω requires an already-trained ˆpω possible by backpropagating through samples (e.g. via Gumbel-Softmax), which we leave for future work. 9In these experiments, we run ACRe on agents trained on the full set of 312 concepts in ShapeWorld, since we are not testing compositional generalization of the agents, but of ACRe. 8
Table 3: ACRe evaluation. (SD) across 5 runs. In the highlighted cells we conduct paired t-tests, comparing ACRe to Closest; * indicates signiﬁcance at p < 0.05.
Game
Setref
Concept
Language
BLEU-1
Teacher
ACRe
Closest
Random
Teacher
ACRe
Closest
Random 100 (0.0) 96 (2.1) 78 (4.6) 71 (6.6) 100 (0.0) 95 (3.4) 64 (3.7) 54 (3.0)
Train
BLEU-4 100 (0.0) 73 (5.0) 28 (6.2) 22 (4.3) 100 (0.0) 80 (7.6) 28 (6.2) 19 (2.8)
Student Acc
BLEU-1 91 (2.7) 81 (3.4) 48 (0.7) 50 (0.0) 88 (4.2) 82 (2.2) 48 (1.6) 50 (0.2) 100 (0.0) 91 (3.9)* 88 (3.8) 73 (7.4) 100 (0.0) 87 (6.9)* 76 (3.2) 56 (2.2)
Test
BLEU-4 100 (0.0) 52 (10.0)* 38 (9.0) 24 (5.5) 100 (0.0) 59 (12.2)* 38 (6.7) 20 (3.2)
Student Acc 86 (5.2) 65 (3.4)* 56 (0.7) 50 (0.3) 84 (4.4) 70 (4.0)* 56 (1.3) 50 (0.4)
Figure 7: Composition in the emergent languages. A concept game teacher’s messages for primitive concepts yellow, green, and triangle, conjunctions yellow AND triangle and green AND triangle (where green AND triangle is an unseen combination for ACRe), and predicted messages according to ACRe and other baselines. Color key same as Figure 4. model trails Teacher language accuracy by 16–20 points. This gap could stem from a failure of ACRe to ﬁnd the correct compositional generalization, or lack of compositionality in the language itself.
A qualitative analysis supports the interpretation that the compositional operations are not always interpretable. Figure 7 shows an example of message distributions for the primitive concepts yellow, green, and triangle, as well as distributions for the conjunctive concepts yellow AND triangle and green AND triangle, with predictions from ACRe and baseline models. The message for yellow AND triangle is intuitively composed out of tokens concatenated from both primitives similar to natural language [12]. However, the message green AND triangle uses tokens from green, but none from triangle, thereby violating the mutual exclusivity principle of language [28]. Regardless, in both cases our ACRe model is able to approximate such messages better than the other baselines. An exciting avenue for future work is encouraging models to develop operators more akin to human language, and evaluating their acquisition by searching among a more restricted class of ACRe models ˆpω
η . 8