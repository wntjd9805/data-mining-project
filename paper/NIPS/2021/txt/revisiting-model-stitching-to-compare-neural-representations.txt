Abstract
We revisit and extend model stitching (Lenc & Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models A and B, we consider a “stitched model” formed by connecting the bottom-layers of A to the top-layers of B, with a simple trainable layer between them. We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative veriﬁcations for intuitive statements such as “good networks learn similar representations”, by demonstrating that good networks of the same architecture, but trained in very different ways (e.g.: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that “more is better” by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be
“plugged in” to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call “stitching connectivity”, akin to mode-connectivity: typical minima reached by SGD can all be stitched to each other with minimal change in accuracy. 1

Introduction
The success of deep neural networks can, arguably, be attributed to the intermediate features or representations learnt by them [Rumelhart et al., 1985]. While neural networks are trained in an end-to-end fashion with no explicit constraints on their intermediate representations, there is a body of evidence that suggests that they learn rich a representation of the data along the way [Goh et al., 2021, Olah et al., 2017]. However, theoretically we understand very little about how to formally characterize these representations, let alone why representation learning occurs. For instance, there are various ad-hoc pretraining methods [Chen et al., 2020a,b], that are purported to perform well by learning good representations, but it is unclear which aspects of these methods (objective, training algorithm, architecture) are crucial for representation learning and if these methods learn qualitatively different representations at all.
Moreover, we have an incomplete understanding of the relations between different representations.
Are all “good representations” essentially the same, or is each representation “good” in its own unique way? That is, even when we train good end-to-end models (i.e., small test loss), the internals of these models could potentially be very different from one another. A priori, the training process could evolve in either one of the following extreme scenarios (see Figure 1): (1) In the “snowﬂakes” scenario, training with different initialization, architectures, and objectives (e.g., supervised vs self-supervised) will result in networks with very different internals, which are completely incompatible with one another. For example, even if we train two models with identical data, architecture, and task, but starting from two different initializations, we may end up at local 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
minima with very different properties (e.g., Liu et al. [2020]). If models are trained with different data (e.g., different samples), different architecture (e.g., different width), or different task (e.g., self-supervised vs supervised) then they could end up being even more different from one another. (2) In the “Anna Karenina” scenario1, all successful models end up learning roughly the same internal representations. For example, all models for vision tasks will have internal representation corresponding to curve detectors, and models that are better (for example, trained on more data, are bigger, or trained for more time) will have better curve detectors.
Figure 1: Two extreme “cartoons” for training dynamics of neural networks. In the “snowﬂakes” scenario, there are exponentially many well-performing neural networks with highly diverging internals. In the “Anna Karenina” scenario all well-performing networks end up learning similar representations, even if their initialization, architecture, data, and objectives differ. Image credits: Li et al. [2018], Olah et al. [2017, 2020], Komarechka [2021].
The “Anna Karenina” scenario implies the following predictions:
“All roads lead to Rome:” Successful models learned with different initializations, architectures, and tasks, should learn similar internal representations, and so if A and B are two such models, it should be possible to “plug in” the internals from A into B without a signiﬁcant loss in performance. See
Figure 2A-B.
“More is better:” Better models trained using more data, bigger size, or more compute, should learn better versions of the same internal representations. Hence if A is a more successful model than B, it should be possible to “plug in” A’s internals to B and obtain improved performance. See Figure 2C.
In this work, we revisit the empirical methodology of “model stitching” to test the above predictions.
Initially proposed by Lenc and Vedaldi [2015], model stitching is natural way of “plugging in” the bottom layers of one network into the top layers of another network, thus forming a stitched network (however care must be taken in the way it is performed, see Section 2). We show that model stitching has some unique advantages that make it more suitable for studying representations than representational similarity measures such as CKA [Kornblith et al., 2019] and SVCCA [Raghu et al., 2017]. Our work provides quantitative evidence for the intuition, shared by many practitioners, that the internals of neural networks often end up being very similar in a certain sense, even when they are trained under different settings. 1.1 Summary of Results
Model stitching as an experimental tool. We establish model stitching as a way of studying the representations of neural networks. A version of model stitching was proposed in Lenc and Vedaldi
[2015] to study the equivalence between representations. In this work, we argue that the idea behind model stitching is more powerful than has been appreciated: we analyze the beneﬁts of model-stitching over other methods to study representations, and we then use model-stitching to establish an number of intuitive properties, including new results on the properties of SGD. 1“All happy families are alike; each unhappy family is unhappy in its own way.” Leo Tolstoy, 1877. https://en.wikipedia.org/wiki/Anna_Karenina_principle 2
Figure 2: Summary of main results (A) Various models trained on CIFAR-10 identically except with different random initializations are “stitching connected”: can be stitched at all layers with minimal performance drop (see Section 4). Stitching with a random bottom network shown for reference. (B) Models of the same architecture and similar test error, but trained on ImageNet with end-to-end supervised learning versus self-supervised learning can be stitched with good performance (see Section 5). (C) Better representation obtained by training the network with more samples can be
"plugged-in" with stitching to improve performance (see Section 6). In all ﬁgures, stitching penalty is the difference in error between the stitched model and the base top model.
In this paper, we use model stitching in the following way. Suppose we have a neural network A (which we’ll think of as the “top model”) for some task with loss function L (e.g. the CIFAR-10 or
ImageNet test error). Let r : X → Rd be a candidate “representation” function, which can come from the ﬁrst (bottom-most) layers of some “bottom model” B . Our intuition is that r has better quality than the ﬁrst (cid:96) layers of network A if “swapping out” these layers with r will improve performance.
“Swapping out” is performed by introducing an additional trainable stitching layer to r with A (deﬁned more formally in Section 2). The stitching layers have very low capacity, and are only meant to
“align” representations, rather than improving the model.
Comparison to Representational Similarity Metrics. Much of the current work studying represen-tations focuses on similarity metrics such as CKA. However, we argue that model stitching can be a better suited tool to study representations in various scenarios, and can give qualitatively different conclusions about the behavior of neural representations compared to these metrics. We analyze the differences between model stitching and prior similarity metrics in Section 3.
Quantitative evidence for intuitions. Using model stitching, we are able to provide formal and quantitative evidence to the intuitions mentioned above. In particular we give evidence for the “all roads lead to rome” intuition by showing compatibility of networks with representation that are trained using (1) different initializations, (2) different subsets of the dataset, (3) different tasks (e.g., self-supervised or coarse labels). See Figure 2A-B for results. We also show that network with different random initialization enjoy a property which we call stitching connectivity, wherein almost all minima reachable via SGD can be “stitched” to each other with minimal loss of accuracy. We also give evidence for the “more is better” intuition by showing that we can improve the performance of a network A by plugging in a representation r that was trained with (1) more data, (2) larger width, or (3) more training epochs. See example with more samples in Figure 2C.
The results above are not surprising, in the sense that they conﬁrm intuitions that practitioners might already have. However model stitching allows us to obtain quantitative and formal measures of these in a way that is not achievable by prior representation measures.
Organization. We ﬁrst deﬁne model stitching formally in Section 2. We compare it with prior work on representational similarity measures in Section 3. Then, we formally deﬁne stitching connectivity and provide experimental evidence for it in Section 4. Finally, in Section 5 and 6, we provide quantitative evidence for the "all roads lead to Rome" and "more is better" intuitions respectively.