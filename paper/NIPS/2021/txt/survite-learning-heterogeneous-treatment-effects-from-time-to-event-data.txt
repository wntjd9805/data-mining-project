Abstract
We study the problem of inferring heterogeneous treatment effects from time-to-event data. While both the related problems of (i) estimating treatment effects for binary or continuous outcomes and (ii) predicting survival outcomes have been well studied in the recent machine learning literature, their combination – albeit of high practical relevance – has received considerably less attention. With the ultimate goal of reliably estimating the effects of treatments on instantaneous risk and survival probabilities, we focus on the problem of learning (discrete-time) treatment-speciﬁc conditional hazard functions. We ﬁnd that unique challenges arise in this context due to a variety of covariate shift issues that go beyond a mere combination of well-studied confounding and censoring biases. We theoretically analyse their effects by adapting recent generalization bounds from domain adaptation and treatment effect estimation to our setting and discuss implications for model design. We use the resulting insights to propose a novel deep learning method for treatment-speciﬁc hazard estimation based on balancing representations. We investigate performance across a range of experimental settings and empirically conﬁrm that our method outperforms baselines by addressing covariate shifts from various sources. 1

Introduction
The demand for methods evaluating the effect of treatments, policies and interventions on individuals is rising as interest moves from estimating population effects to understanding effect heterogeneity in ﬁelds ranging from economics to medicine. Motivated by this, the literature proposing machine learning (ML) methods for estimating the effects of treatments on continuous (or binary) end-points has grown rapidly, most prominently using tree-based methods [1, 2, 3, 4, 5], Gaussian processes
[6, 7], and, in particular, neural networks (NNs) [8, 9, 10, 11, 12, 13, 14, 15]. In comparison, the ML literature on heterogeneous treatment effect (HTE) estimation with time-to-event outcomes is rather sparse. This is despite the immense practical relevance of this problem – e.g. many clinical studies consider time-to-event outcomes; this could be the time to onset or progression of disease, the time to occurrence of an adverse event such as a stroke or heart attack, or the time until death of a patient.
In part, the scarcity of HTE methods may be due to time-to-event outcomes being inherently more challenging to model, which is attributable to two factors [16]: (i) time-to-event outcomes differ from standard regression targets as the main objects of interest are usually not only expected survival times but the dynamics of the underlying stochastic process, captured by hazard and survival functions, and (ii) the presence of censoring. This has led to the development of a rich literature on survival analysis particularly in (bio)statistics, see e.g. [16, 17]. Classically, the effects of treatments in clinical studies with time-to-event outcomes are assessed by examining the coefﬁcient of a treatment
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
indicator in a (semi-)parametric model, e.g. Cox proportional hazards model [18], which relies on the often unrealistic assumption that models are correctly speciﬁed. Instead, we therefore adopt the nonparametric viewpoint of van der Laan and colleagues [19, 20, 21, 22] who have developed tools to incorporate ML methods into the estimation of treatment-speciﬁc population average parameters.
Nonparametrically investigating treatment effect heterogeneity, however, has been studied in much less detail in the survival context. While a number of tree-based methods have been proposed recently [23, 24, 25, 26], NN-based methods lack extensions to the time-to-event setting despite their successful adoption for estimating the effects of treatments on other outcomes – the only exception being [27], who directly model event times under different treatments with generative models.
Instead of modeling event times directly like in [27], we consider adapting machine learning methods, with special focus on NNs, for estimation of (discrete-time) treatment-speciﬁc hazard functions. We do so because many target parameters of interest in studies with time-to-event outcomes are functions of the underlying temporal dynamics; that is, hazard functions can be used to directly compute (differences in) survival functions, (restricted) mean survival time, and hazard ratios. We begin by exploring and characterising the unique features of the survival treatment effect problem within the context of empirical risk minimization (ERM); to the best of our knowledge, such an investigation is lacking in previous work. In particular, we show that learning treatment-speciﬁc hazard functions is a challenging problem due to the potential presence of multiple sources of covariate shift: (i) non-randomized treatment assignment (confounding), (ii) informative censoring and (iii) a form of shift we term event-induced covariate shift, all of which can impact the quality of hazard function estimates. We then theoretically analyze the effects of said shifts on ERM, and use our insights to propose a new NN-based model for treatment effect estimation in the survival context.
Contributions (i) We identify and formalize key challenges of heterogeneous treatment effect estimation in time-to-event data within the framework of ERM. In particular, as discussed above, we show that when estimating treatment-speciﬁc hazard functions, multiple sources of covariate shift arise. (ii) We theoretically analyse their effects by adapting recent generalization bounds from domain adaptation and treatment effect estimation to our setting and discuss implications for model design. This analysis provides new insights that are of independent interest also in the context of hazard function estimation in the absence of treatments. (iii) Based on these insights, we propose a new model (SurvITE) relying on balanced representations that allows for estimation of treatment-speciﬁc target parameters (hazard and survival functions) in the survival context, as well as a sister model (SurvIHE), which can be used for individualized hazard estimation in standard survival settings (without treatments). We investigate performance across a range of experimental settings and empirically conﬁrm that SurvITE outperforms a range of natural baselines by addressing covariate shifts from various sources. 2 Problem Deﬁnition
In this section, we discuss the problem setup of heterogeneous treatment effect estimation from
In Appendix A, we present a self-time-to-event data, our target parameters and assumptions. contained introduction to and comparison with heterogeneous treatment effect estimation with standard (binary/continuous) outcomes.
Problem setup. Assume we observe a time-to-event dataset D = {(ai, xi, ˜τi, δi)}n i=1 comprising realizations of the tuple (A, X, ˜T , ∆) ∼ P for n patients. Here, X ∈ X and A ∈ {0, 1} are random variables for a covariate vector describing patient characteristics and an indicator whether a binary treatment was administered at baseline, respectively. Let T ∈ T and C ∈ T denote random variables for the time-to-event and the time-to-censoring; here, events are usually adverse, e.g. progression/onset of disease or even death, and censoring indicates loss of follow-up for a patient.
Then, the observed time-to-event outcomes of each patient are described by ˜T = min(T, C) and
∆ = 1(T ≤ C), which indicate the time elapsed until either an event or censoring occurs and whether the event was observed or not, respectively. Throughout, we treat survival time as discrete2 and the time horizon as ﬁnite with pre-deﬁned maximum tmax, so that the set of possible survival times is
T = {1, · · · , tmax}. 2Where necessary, discretization can be performed by transforming continuous-valued times into a set of contiguous time intervals, i.e., T = τ implies T ∈ [tτ , tτ + δt) where δt implies the temporal resolution. 2
We transform the short data structure outlined above to a so-called long data structure which can be used to directly estimate conditional hazard functions using standard machine learning methods
[20]. We deﬁne two counting processes NT (t) and NC(t) which track events and censoring, i.e.
NT (t) = 1( ˜T ≤ t, ∆ = 1) and NC(t) = 1( ˜T ≤ t, ∆ = 0) for t ∈ T ; both are zero until either an event or censoring occurs. By convention, we let NT (0) = NC(0) = 0. Further, let
Y (t) = 1(NT (t) = 1 ∩ NT (t−1) = 0) be the indicator for an event occuring exactly at time t; thus, for an individual with ˜T = τ and ∆ = 1, Y (t) = 0 for all t (cid:54)= τ , and Y (t) = 1 at the event time t = τ . The conditional hazard is the probability that an event occurs at time τ given that it does not occur before time τ , hence it can be deﬁned as [22]
λ(τ |a, x) = P( ˜T = τ, ∆ = 1| ˜T ≥ τ, A = a, X = x)
= P(Y (τ ) = 1|NT (τ −1) =C (τ −1) = 0, A = a, X = x) (1)
It is easy to see from (1) that given data in long format, λ(τ |a, x) can be estimated for any τ by solving a standard classiﬁcation problem with Y (τ ) as target variable, considering only the samples at-risk at time τ in each treatment arm (individuals for which neither event nor censoring has occurred until that time point; i.e. the set I(τ, a) def= {i ∈ [n] : NT (τ −1)i = NC(τ −1)i = 0 ∩ Ai = a}).
Finally, given the hazard, the associated survival function S(τ |a, x) = P(T > τ |A = a, X = x) (cid:0)1 − λ(t|a, x)(cid:1). The censoring hazard λC(t|a, x) and can then be computed as S(τ |a, x) = (cid:81) survival function SC(t|a, x) can be deﬁned analogously. t≤τ
Target parameters. While the main interest in the standard treatment effect estimation setup with continuous outcomes usually lies in estimating only the (difference between) conditional outcome means under different treatments, there is a broader range of target parameters of interest in the time-to-event context, including both treatment-speciﬁc target functions and contrasts that represent some form of heterogeneous treatment effect (HTE). We deﬁne the treatment-speciﬁc (conditional) hazard and survival functions as
λa(τ |x) = P(T = τ |T ≥ τ, do(A = a, C ≥ τ ), X = x) (cid:89)
Sa(τ |x) = P(T > τ |do(A = a, C ≥ τ ), X = x) = t≤τ (cid:0)1 − λa(t|x)(cid:1) (2) tk≤L
Here, do(·) denotes [28]’s do-operator which indicates an intervention; in our context, do(A = a, C ≥ τ ) ensures that every individual is assigned treatment a and is observed at (not censored before) the time-step of interest [20]. Below we discuss assumptions that are necessary to identify such interventional quantities from observational datasets in the presence of censoring.
Given λa(τ |x) and Sa(τ |x), possible HTEs of interest3 include the difference in treatment-speciﬁc survival times at time τ , i.e. HTEsurv(τ |x) = S1(τ |x) − S0(τ |x), the difference in restricted mean (cid:0)S1(tk|x) − S0(tk|x)(cid:1) · (tk − tk−1), survival time (RMST) up to time L, i.e. HTErmst(x) = (cid:80) and hazard ratios. In the following, we will focus on estimation of the treatment speciﬁc hazard functions {λa(t|x)}a∈{0,1},t∈T as this can be used to compute survival functions and causal contrasts.
Assumptions. (1. Identiﬁcation) To identify interventional quantities from observational data, it is necessary to make a number of untestable assumptions on the underlying data-generating process (DGP) [28] – this generally limits the ability to make causal claims to set-tings where sufﬁcient domain knowledge is available. Here, as [20, 21], we assume the data was generated from the fairly general di-rected acyclic graph (DAG) presented in Fig. 1. As there are no arrows originating in hidden nodes entering treatment or censoring nodes, this graph formalizes (1.a) The ‘No Hidden
Confounders’ Assumption in static treatment effect estimation and (1.b) The ‘Censoring At
Figure 1: The assumed underlying DAG. Covariates
X can be split into (possibly overlapping) subsets
X1, X2 and X3, determining treatment selection, informative censoring, and event times, respectively. 3Note: All parameters of interest to us are heterogeneous (also sometimes referred to as individualized), i.e. a function of the covariates X, while the majority of existing literature in (bio)statistics considers population average parameters that are functions of quantities such as P(T > τ |do(A = a)), which average over all X. 3
Random’ Assumption in survival analysis [20]. The latter is necessary here as estimating an effect of treatment on event time implicitly requires that censoring can be ‘switched off’ – i.e. intervened on.
This graph implicitly also formalizes (1.c) The Consistency Assumption, i.e. that observed outcomes are ‘potential’ outcomes under the observed intervention, as each node in a DAG is deﬁned as a function of its ancestors and exogenous noise [20]. Under these assumptions, λa(τ |x) = λ(τ |a, x). (2. Estimation) To enable nonparametric estimation of λa(τ |x) for some τ ∈ T , we additionally need to assume that the interventions of interest are observed with non-zero probability; within different literatures these assumptions are known under the label of ‘overlap’ or ‘positivity’ [9, 19].
In particular, for 0 < (cid:15)1, (cid:15)2, < 1 we need that (2.a) (cid:15)1 < P(A = a|X = x) < 1 − (cid:15)1, i.e. treatment assignment is non-deterministic, and that (2.b) P(NC(t) = 0|A = a, X = x) > (cid:15)2 for all t < τ , i.e. no individual will be deterministically censored before τ . Finally, because
λa(τ |x) is a probability deﬁned conditional on survival up to time τ , we need to assume that (2.c)
P(NT (τ −1) = 0|A = a, X = x) > (cid:15)3 > 0 for it to be well-deﬁned. We formally state and discuss all assumptions in more detail in Appendix C. 3 Challenges in Learning Treatment-Speciﬁc Hazard Functions using ERM
Preliminaries: ERM under Covariate Shift. Recall that in problems with covariate shift, the training distribution X, Y ∼ Q0(·) used for ERM and target distribution X, Y ∼ Q1(·) are mis-matched: One assumes that the marginals do not match, i.e. Q0(X) (cid:54)= Q1(X), while the conditionals remain the same, i.e. Q0(Y |X) = Q1(Y |X) [29]. If the hypothesis class H used in ERM does not contain the truth (or in the presence of heavy regularization), this can lead to suboptimal hypothesis choice as arg minh∈H EX,Y ∼Q1(·)[(cid:96)(Y, h(X))] (cid:54)= arg minh∈H EX,Y ∼Q0(·)[(cid:96)(Y, h(X))] in general. 3.1 Sources of Covariate Shift in Learning Treatment-Speciﬁc Hazard Functions
We now consider how to learn a treatment-speciﬁc hazard function λa(τ |x) from observational data using ERM. As detailed in Section 2, we exploit the long data format by realizing that λa(τ |x) can be estimated by solving a standard classiﬁcation problem with Y (τ ) as dependent variable and X as covariates, using only the samples at risk with treatment status a, i.e. I(τ, a), which corresponds to solving the empirical analogue of the problem
ˆλa(τ |x) ∈ arg min ha,τ ∈H
EX,Y (τ )∼Pa,τ (·)[(cid:96)(Y (τ ), ha,τ (X)] (3) where we use Pa,τ to refer to the observational (at-risk) distribution Pa,τ (X, Y (τ )) =
T (τ |X)Pa,τ (X) with Pa,τ (X) = P(X|NT (τ −1) = NC(τ −1) = 0, A = a) = P(X| ˜T ≥
λa
τ, A = a). If the loss function (cid:96) is chosen to be the log-loss, this corresponds to optimizing the likelihood of the hazard.
The observational (at-risk) covariate distribution Pa,τ (X), however, is not our target distribution: instead, to obtain reliable hazard estimates for the whole population, we wish to optimize the ﬁt over the population at baseline, i.e. the marginal distribution X ∼ P(X) which we will refer to as P0(X) below to emphasize it being the baseline at-risk distribution4. Here, differences between P0(X) and the population at-risk Pa,τ (X) can arise due to three distinct sources of covariate shift:
• (Shift 1) Confounding/treatment selection bias: if treatment is not assigned completely at random, then P(X|A = a) (cid:54)= P0(X) and the distribution of characteristics across the treatment arms differs already at baseline, thus Pa,τ (X) (cid:54)= P0(X) in general.
• (Shift 2) Censoring bias: regardless of the presence of confounding, if the censoring hazard is not independent of covariates, i.e. λC(τ |a, x) (cid:54)= λC(τ |a), then the population at-risk changes over time such that Pa,τ1(X) (cid:54)= Pa,τ2(X) (cid:54)= P0(X) in general. If, in addition, there are differences between the treatment-speciﬁc censoring hazards, then the at-risk distribution will also differ across treatment arms at any given time-point, i.e. Pa,τ (X) (cid:54)= P1−a,τ (X) for τ > 1 in general.
• (Shift 3) Event-induced shifts: Counterintuitively, even in the absence of both confounding and censoring, there will be covariate shift in the at-risk population if the event-hazard depends on covariates, i.e. if λ(τ |a, x) (cid:54)= λ(τ |a) then Pa,τ1 (X) (cid:54)= Pa,τ2(X) (cid:54)= P0(X) in general. Further, if there are heterogenous treatment effects, then Pa,τ (X) (cid:54)= P1−a,τ (X) for τ > 1 in general. 4With slight abuse of notation, we will use P0 and Pa,τ also to refer to densities of continuous x 4
What makes the survival treatment effect estimation problem unique? While Shift 1 arises also in the standard treatment effect estimation setting, Shift 2 and Shift 3 arise uniquely due to the nature of time-to-event data5. Thus, estimating treatment effects from time-to-event data is inherently more involved than estimating treatment effects in the standard static setup, as covariate shift at time horizon τ > 1 can arise even in a randomized control trial (RCT). Thus, in addition to the overall at-risk population changing over time, both treatment effect heterogeneity and treatment-dependent censoring can lead to differences in the composition of the population at-risk in each treatment arm.
Further, Shifts 1, 2 and 3 can also interact to create more extreme shifts; e.g. if treatment selection is based on the same covariates as the event process (i.e. X1 = X3 in Fig. 1) then event-induced shift can amplify the selection effect over time (refer to Appendix E for a synthetic example of this). 3.2 Possible Remedies and Theoretical Analysis pτ,a a,τ (x) =
P0(x)
Pa,τ (x) =
A natural solution to tackle bias in ERM caused by covariate shift is to use importance weighting
[30]; i.e. to reweight the empirical risk by the density ratio of target P0(X) and observed distribution
Pa,τ (X). If we wanted to obtain a hazard estimator for (τ, a), optimized towards the marginal population, optimal importance weights are given by w∗ ea(x)ra(x,τ ) with pτ,a =
P( ˜T ≥ τ, A = a), ea(x) = P(A = a|X = x) the propensity score, and ra(x, τ ) = P( ˜T ≥
τ |A = a, X = x) the probability to be at risk, i.e. the probability that neither event nor censoring occurred before time τ . These weights are well-deﬁned due to the overlap assumptions detailed in
Sec. 2; however, they are in general unknown as they depend on the unknown target parameters
λa(τ |x) through ra(x, τ ). Further, especially for large τ , these weights might be very extreme even if known, which can lead to highly unstable results [31] – making biased yet stabilized weighting schemes, e.g. truncation, a good alternative. Therefore, we only assume access to some (possibly imperfect) weights wa,τ (x) s.t. EX∼Pa,τ [wa,τ (x)] = 1, so that we can create a weighted distribution a,τ = wa,τ (x)Pa
Pw
Either instead of [8, 9] or in addition to weighting [10, 12, 14, 32], the literature on learning balanced representations for static treatment effect estimation has focused on ﬁnding a different remedy for distributional differences between treatment arms: creating representations Φ : X → R which have similar (weighted) distributions across arms as measured by an integral probability metric (IPM), motivated by generalization bounds. As we show below, we can exploit a similar feature in our context by ﬁnding a representation that minimizes the IPM term not between treatment arms, but between covariate distribution at baseline P0 and Pw a,τ . The proposition below bounds the target risk of a hazard estimator ˆλa(τ |x) = h(Φ(x)) relying on any representation. The proof, which relies on the concept of excess target information loss, proposed recently to analyze domain-adversarial training [33], and the standard IPM arguments made in e.g. [32], is stated in Appendix C.
Proposition 1. For ﬁxed a, τ and representation Φ : X → R, let PΦ a,τ and Pw,Φ a,τ denote the target, observational, and weighted observational distribution of the representation Φ. Deﬁne the pointwise losses
τ (x) can be recovered by using wa,τ (x) = 1.)
τ (x). (Note: Pa 0 , PΦ (cid:96)h,Q(x; a, τ ) def= EY (τ )|x,a∼Q[(cid:96)(Y (τ ), h(Φ(X)))|X = x, A = a] (cid:96)h,QΦ (φ; a, τ ) def= EY (τ )|φ,a∼QΦ [(cid:96)(Y (τ ), h(Φ))|Φ = φ, A = a] (4) of (hazard) hypothesis h ≡ ha,τ : R → [0, 1] w.r.t. distributions in covariate and representation space, respectively. Assume there exists a constant CΦ > 0 s.t. CΦ (φ, a, τ ) ∈ G for some family of functions G. Then we have that
−1(cid:96)h,Pw,Φ a,τ
EX∼P0 [(cid:96)h,P(X; a, τ )] (cid:124) (cid:125) (cid:123)(cid:122)
Target Risk
≤ EX∼Pa,τ [wa,τ (X)(cid:96)h,P(X; a, τ )] (cid:125) (cid:123)(cid:122)
Weighted observational risk (cid:124) 0 , Pw,Φ
+CΦ IPMG(PΦ a,τ ) (cid:125) (cid:123)(cid:122)
Distance in Φ-space (cid:124)
+ ηl
Φ(h) (cid:124) (cid:123)(cid:122) (cid:125)
Info loss (5) 5Interestingly, changes of the at-risk population over time arise also in standard survival problems (without treatments); yet in the context of prediction these do not matter: as the at-risk population at any time-step is also the population that will be encountered at test-time, this shift in population over time is not problematic, unless it is caused by censoring. If, however, our goal is estimation of the best target parameter (here: the hazard at a speciﬁc point in time τ ) over the whole population, this corresponds to a setting where the ideal evaluation is performed on a population different from the observed one – and hence requires careful consideration of the consequences of the covariate shifts discussed above. 5
where IPMG(P, Q) = supg∈G tion loss η(cid:96)
Φ(h) analogously to [33] as η(cid:96) (cid:96)h,QΦ (φ; a, τ ) − (cid:96)h,Q(x; a, τ ). For invertible Φ, η(cid:96) (cid:82) g(x)(P(x) − Q(x))dx(cid:12) (cid:12) (cid:12)
Φ(h) def= EX∼P[ξPΦ
Φ(h) = ξQΦ,Q(x) = 0. (cid:12) and we deﬁne the excess target informa-a,τ ,P(X)] with ξQΦ,Q(x) def= 0 ,P(X) − ξPw,Φ
Φ(h) = ξPΦ,P(x) = ξPw,Φ
Unlike the bounds provided in [9, 10, 32, 14, 27], this bound does not rely on representations to be invertible; we consider this feature important as none of the works listed actually enforced invertibility in their proposed algorithms. Given bound (5), it is easy to see why non-invertibilty can be useful: for any (possibly non-invertible) representation for which it holds that Y (τ )|=X|Φ(X), A, it also holds that η(cid:96) a,τ ,P(x) = 0 and the causally identifying restrictions continue to hold. A simple representation for which this property holds is a selection mechanism that chooses only the causal parents of Y (τ ) from within X; if X can be partitioned into variables affecting the instantaneous risk (X3 in Fig. 1), and variables affecting only treatment assignment (X1 \ X3) and/or censoring mechanism (X2 \ X3), then the IPM term can be reduced by a representation which drops the latter two sets of variables – or irrelevant variables correlated with any such variables – without affecting η(cid:96)
Φ(h). As a consequence, event-induced covariate shift can generally not be fully corrected for using non-invertible representations (unless the variables affecting event time are different at every time-step). Further, given perfect importance weights w∗, both η(cid:96)
Φ(h) and IPM term are zero.
Except for the dependence on η(cid:96)
Φ(h), this bound differs from the regression-based bound for survival treatment effects stated in [27] (which is identical to the original treatment effect bound in [9]) in that we have dependence on τ in the IPM term, which, among other things, explicitly captures the effect of censoring. Our bound motivates that, instead of ﬁnding representations that balance treatment-and control group at baseline (or at each time step) we should ﬁnd representations that balance PΦ a,τ towards the baseline distribution PΦ 0 for each time step, which motivates our method detailed below.
If, instead, we would apply the IPM-term to encourage only the arm-speciﬁc at-risk distributions at each time-step to be similar, this would correct only for shifts due to (i) confounding at baseline, (ii) treatment-induced differences in censoring and (iii) treatment-induced differences in events. It will, however, not allow handling the event- and censoring-induced shifts that occur regardless of treatment status. Note that this bound therefore also motivates the use of balanced representations for modeling time-to-event outcomes in the presence of informative censoring even in the standard prediction setting, which is a ﬁnding that could be of independent interest for the ML survival analysis literature. 3.3 From hazards to survival functions t≤τ
If the ultimate goal is to use the hazard function to estimate survival functions as ˆSa(τ |x) = (cid:0)1 − ˆλa(t|x)(cid:1), the best target population to consider during hazard estimation may not be the (cid:81) marginal distribution. Instead, the optimal target population may depend on the metric by which we wish to evaluate the resulting survival function. If we wanted to ﬁnd the survival function that maximises the complete data likelihood (corresponding to the hypothetical setting in which we intervened to set A = a and C ≥ τ ), the target population (at each time step t) would be
P(X|T ≥ t, do(A = a, C ≥ t)) – the population that preserves event-induced shift but removes selection- and censoring-induced shifts. If, instead, we focused on the MSE of estimating the survival function (as in our experiments), it becomes more difﬁcult to derive an exact target population for estimating the hazards. If we assume access to a perfect estimate of the survival function for the
ﬁrst τ −1 time steps (i.e. ˆS(τ −1|x) = S(τ −1|x)) and focus only on estimating the next hazard,
λa(τ |X), we can write
EX∼P0 [(Sa(τ |X)− ˆSa(τ |X))2] = EX∼P0[(Sa(τ −1|X)(1−λa(τ |X))− ˆSa(τ −1|X)(1−ˆλa(τ |X)))2]
= EX∼P0[Sa(τ −1|X)2(ˆλa(τ |X)−λa(τ |X))2] and notice that the MSE will implicitly down-weigh individuals with lower survival probability6.
Deﬁning an exact target population for the hazard when the goal is to also estimate the survival function well is thus not straightforward, making exact importance weighting difﬁcult. Additionally, unlike the marginal population, interventional populations which change over time, such as P(X|T ≥ t, do(A = a, C ≥ t)), are never observed in practice and hence cannot be used to perform balancing 6Due to the square in the term Sa(τ − 1|X)2, this will be even more extreme than exact up-weighting of the population P(X|T ≥ τ, do(A = a, C ≥ τ )). 6
Figure 2: SurvITE architecture. regularization of a representation using empirical IPM-terms. Therefore, we refrain from using importance weighting in our method (which is described in the next section), and resort to using the marginal population for balancing regularization of the representation throughout. Intuitively, as outlined in the previous section, we expect that doing so will not over-correct for event-induced shifts that are predictive of outcome (and should hence be preserved) as such “over-balancing" would reduce the predictive power of the representation, which would immediately be penalized by the presence of the expected loss component in the bound7. Additionally, we expect that using the marginal population for balancing could be useful also for estimating the survival function even in the absence of selection- and censoring-induced shifts, as it may help to remove the effect of variables that appear spuriously correlated with outcome over time. 4 SurvITE: Estimating HTEs from Time-to-Event Data
Based on the theoretical analysis above, we propose a novel deep learning approach to HTE estimation from observed time-to-event data, which we refer to as SurvITE (Individualized Treatment Effect estimator for Survival analysis).8 The network architecture is illustrated in Figure 2. Note that even in the absence of treatments we can use this architecture for estimation of hazards and survival functions by using only one treatment a = 0. As we show in the experiments, this version of our method –
SurvIHE (Individualized Hazard Estimator for Survival analysis) – is of independent interest in the standard survival setting, as it tackles Shifts 2 & 3. Below, we describe the empirical loss functions we use to ﬁnd representation Φ and hypotheses ha,τ .
Let Φ : X → R denote the representation (parameterized by θφ) and ha,τ : R → [0, 1] the hazard estimator for treatment a and time τ (parameterized by θha,τ ), each implemented as a fully-connected neural network. While the output heads are thus unique to each treatment-group time-step combination, we allow hazard estimators to share information by using one shared representation for all hazard functions. This allows for both borrowing of information across different a, τ and signiﬁcantly reduces the number of parameters of the network. Then, given the time-to-event data D, we use the following empirical loss functions for the observational risk and the IPM term:
Lrisk(θφ, θh) = 1 tmax tmax(cid:88) (cid:88) 1,t ai(cid:96)(cid:0)yi(t), h1,t(Φ(xi))(cid:1) + n−1 n−1 0,t (1−ai)(cid:96)(cid:0)yi(t), h0,t(Φ(xi))(cid:1),
Lipm(θφ) = (cid:88)
W ass(cid:0){Φ(xi)}n i=1, {Φ(xi)}i:˜τi≥t,ai=a (cid:1), a∈{0,1} t=1 where W ass(·, ·) is the ﬁnite-sample Wasserstein distance [34]; refer to Appendix D for further detail.
Note that Lipm(θφ), which penalizes the discrepancy between the baseline distribution and each at-risk distribution PΦ a,τ , simultaneously tackles all three sources of shifts. Further, na,t = |I(τ, a)| is the number of samples at-risk in each treatment arm; its presence ensures that each a, τ -combination contributes equally to the loss. Overall, we can ﬁnd Φ and ha,τ ’s that optimally trade off balance and 7In practice, we ensure this by weighting the contribution of the IPM term by a hyperparameter that is chosen to preserve predictive performance of the representation (see Appendix D). 8The source code for SurvITE is available in https://github.com/chl8856/survITE. 7 t=1 i:˜τi≥t tmax(cid:88)
predictive power as suggested by the generalization bound (5) by minimizing the following loss:
Ltarget(θφ, θh) = Lrisk(θφ, θh) + βLipm(θφ) where θh = {θha,τ }a∈{0,1},τ ∈T , and β > 0 is a hyper-parameter. The pseudo-code of SurvITE, the details of how to obtain W ass(·, ·) and how we set β can be found in Appendix D. (6) 5