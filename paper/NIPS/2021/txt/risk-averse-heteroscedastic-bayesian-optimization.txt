Abstract
Many black-box optimization tasks arising in high-stakes applications require risk-averse decisions. The standard Bayesian optimization (BO) paradigm, however, optimizes the expected value only. We generalize BO to trade mean and input-dependent variance of the objective, both of which we assume to be unknown a priori. In particular, we propose a novel risk-averse heteroscedastic Bayesian optimization algorithm (RAHBO) that aims to identify a solution with high return and low noise variance, while learning the noise distribution on the ﬂy. To this end, we model both expectation and variance as (unknown) RKHS functions, and propose a novel risk-aware acquisition function. We bound the regret for our approach and provide a robust rule to report the ﬁnal decision point for applications where only a single solution must be identiﬁed. We demonstrate the effectiveness of RAHBO on synthetic benchmark functions and hyperparameter tuning tasks. 1

Introduction
Black-box optimization tasks arise frequently in high-stakes applications such as drug and material discovery [17, 22, 28], genetics [16, 27], robotics [5, 12, 25], hyperparameter tuning of complex learning systems [13, 21, 34], to name a few. In many of these applications, there is often a trade-off between achieving high utility and minimizing risk. Moreover, uncertain and costly evaluations are an inherent part of black-box optimization tasks, and modern learning methods need to handle these aspects when balancing between the previous two objectives.
Bayesian optimization (BO) is a powerful framework for optimizing such costly black-box functions from noisy zeroth-order evaluations. Classical BO approaches are typically risk-neutral as they seek to optimize the expected function value only. In practice, however, two different solutions might attain similar expected function values, but one might produce signiﬁcantly noisier realizations. This is of major importance when it comes to actual deployment of the found solutions. For example, when selecting hyperparameters of a machine learning algorithm, we might prefer conﬁgurations that lead to slightly higher test errors but at the same time lead to smaller variance.
In this paper, we generalize BO to trade off mean and input-dependent noise variance when sequen-tially querying points and outputting ﬁnal solutions. We introduce a practical setting where both the black-box objective and input-dependent noise variance are unknown a priori, and the learner needs to estimate them on the ﬂy. We propose a novel optimistic risk-averse algorithm – RAHBO – that makes sequential decisions by simultaneously balancing between exploration (learning about uncertain actions), exploitation (choosing actions that lead to high gains) and risk (avoiding unreliable actions). We bound the cumulative regret of RAHBO as well as the number of samples required to output a single near-optimal risk-averse solution. In our experiments, we demonstrate the risk-averse performance of our algorithm and show that standard BO methods can severely fail in applications where reliability of the reported solutions is of utmost importance. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Unknown objective f (b) Unknown variance ρ2 (c) Histogram of variance
Figure 1: When there is a choice between identical optima with different noise level, standard BO tends to query points corresponding to higher noise. (a) Unknown objective with 3 global maxima marked as (A, B, C); (b) Heteroscedastic noise variance over the same domain: the noise level at (A, B, C) varies according to the sigmoid function; (c) Empirical variance distribution at all points acquired during BO procedure (over 9 experiments with different seeds). The three bumps correspond to the three global optima with different noise variance. RAHBO dominates in choosing the risk-averse optimum, consequently yielding lower risk-averse regret in Figure 5a.