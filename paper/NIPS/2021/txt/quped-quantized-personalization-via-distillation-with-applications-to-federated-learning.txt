Abstract
Traditionally, federated learning (FL) aims to train a single global model while collaboratively using multiple clients and a server. Two natural challenges that
FL algorithms face are heterogeneity in data across clients and collaboration of clients with diverse resources. In this work, we introduce a quantized and personalized FL algorithm QuPeD that facilitates collective (personalized model compression) training via knowledge distillation (KD) among clients who have access to heterogeneous data and resources. For personalization, we allow clients to learn compressed personalized models with different quantization parameters and model dimensions/structures. Towards this, ﬁrst we propose an algorithm for learning quantized models through a relaxed optimization problem, where quantization values are also optimized over. When each client participating in the (federated) learning process has different requirements for the compressed model (both in model dimension and precision), we formulate a compressed personalization framework by introducing knowledge distillation loss for local client objectives collaborating through a global model. We develop an alternating proximal gradient update for solving this compressed personalization problem, and analyze its convergence properties. Numerically, we validate that QuPeD outperforms competing personalized FL methods, FedAvg, and local training of clients in various heterogeneous settings. 1

Introduction
Federated Learning (FL) is a learning procedure where the aim is to utilize vast amount of data residing in numerous (in millions) edge devices (clients) to train machine learning models without collecting clients’ data [26]. Formally, if there are n clients and fi denotes the local loss function at client i, then traditional FL learns a single global model by minimizing (cid:16) f (w) := arg min w∈Rd 1 n n (cid:88) i=1 fi(w) (cid:17)
. (1)
It has been realized lately that a single model may not provide good performance to all the clients in settings where data is distributed heterogeneously. This leads to the need for personalized learning, where each client wants to learn its own model [7, 8]. Since a locally learned client model may not generalize well due to insufﬁcient data, in personalized FL process, clients maintain personalized models locally and utilize other clients’ data via a global model. Resource diversity among clients, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
which is inherent to FL as the participating edge devices may vary widely in terms of resources, is often overlooked in personalized FL literature. This resource diversity may necessitate clients to learn personalized models with different precision as well as different dimension/architecture. Systemati-cally studying both these resource heterogeneity together with data heterogeneity in personalized FL is the primary objective of this paper.
In this work, we propose a model compression framework1 for personalized FL via knowledge distillation (KD) [14] that addresses both data and resource heterogeneity in a uniﬁed manner. Our framework allows collaboration among clients with different resource requirements both in terms of precision as well as model dimension/structure, for learning personalized quantized models (PQMs).
Motivated by FL, where edge devices are resource constrained when actively used (e.g. when several applications are actively running on a battery powered smartphone) and available for training when not in use (e.g., while charging and on wi-ﬁ), we do training in full precision for learning compressed models to be deployed for inference time. For efﬁcient model compression, we learn the quantization parameters for each client by including quantization levels in the optimization problem itself. First, we investigate our approach in a centralized setup, by formulating a relaxed optimization problem and minimizing it through alternating proximal gradient steps, inspired by [3]. To extend this to FL for learning PQMs with different dimensions/architectures, we employ our centralized algorithm locally at clients and introduce KD loss for collaboration of personalized and global models. Although there exist empirical works where KD is used in personalized FL [20], we formalize it as an optimization problem, solve it using alternating proximal updates, and analyze its convergence.
Contributions. Our contributions can be summarized as follows:
• In the centralized case, we propose a novel relaxed optimization problem that enables optimization over quantization values (centers) as well as model parameters. We use alternating proximal updates to minimize the objective and analyze its convergence properties.
• More importantly, our work is the ﬁrst to formulate a personalized FL optimization problem where clients may have different model dimensions and precision requirements for their personalized models. Our proposed scheme combines alternating proximal updates with knowledge distillation.
• For optimizing a non-convex objective, in the centralized setup, we recover the standard conver-gence rate of O(1/T ) (despite optimizing over quantization centers), and for federated setting, we
√ recover the standard convergence rate of O(1/
T ) (despite learning PQMs with different preci-sions/dimensions). In the federated setting, our convergence bound has an error term that depends on multiplication of two terms averaged over clients: one characterizing client’s local model smoothness and the other data heterogeneity with respect to overall data distribution.2
• We perform image and text classiﬁcation experiments on multiple datasets in various resource and data heterogeneity settings, and compare performance of QuPeD against Per-FedAvg [8], pFedMe
[7], FedAvg [26], and local training of clients. We observe that QuPeD in full precision outperforms all these methods on all the datasets that we considered for our experiments. Further, our results show that even with quantization, QuPeD outperforms the other methods in full precision on multiple settings and datasets, demonstrating the effectiveness of our scheme for FL settings.
Our work should not be confused with works in distributed/federated learning, where models/gradients are compressed for communication efﬁciency [2, 17]. We also achieve communication efﬁciency through local iterations, but the main goal of our work is personalized quantization for inference.