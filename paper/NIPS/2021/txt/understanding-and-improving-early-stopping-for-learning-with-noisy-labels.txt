Abstract
The memorization effect of deep neural network (DNN) plays a pivotal role in many state-of-the-art label-noise learning methods. To exploit this property, the early stopping trick, which stops the optimization at the early stage of training, is usually adopted. Current methods generally decide the early stopping point by considering a DNN as a whole. However, a DNN can be considered as a composition of a series of layers, and we ﬁnd that the latter layers in a DNN are much more sensitive to label noise, while their former counterparts are quite robust.
Therefore, selecting a stopping point for the whole network may make different
DNN layers antagonistically affect each other, thus degrading the ﬁnal performance.
In this paper, we propose to separate a DNN into different parts and progressively train them to address this problem. Instead of the early stopping which trains a whole DNN all at once, we initially train former DNN layers by optimizing the
DNN with a relatively large number of epochs. During training, we progressively train the latter DNN layers by using a smaller number of epochs with the preceding layers ﬁxed to counteract the impact of noisy labels. We term the proposed method as progressive early stopping (PES). Despite its simplicity, compared with the traditional early stopping, PES can help to obtain more promising and stable results.
Furthermore, by combining PES with existing approaches on noisy label training, we achieve state-of-the-art performance on image classiﬁcation benchmarks. The code is made public at https://github.com/tmllab/PES. 1

Introduction
Deep networks have revolutionized a wide variety of tasks, such as image processing, speech recognition, and language modeling [7], However, this highly relies on the availability of large annotated data, which may not be feasible in practice. Instead, many large datasets with lower quality annotations are collected from online queries [5] or social-network tagging [18]. Such annotations inevitably contain mistakes or label noise. As deep networks have large model capacities, they can easily memorize and eventually overﬁt the noisy labels, leading to poor generalization performance [36]. Therefore, it is of great importance to develop a methodology that is robust to noisy annotations.
Existing methods on learning with noisy labels (LNL) can be mainly categorized into two groups: model-based and model-free algorithms. Methods in the ﬁrst category mainly model noisy labels with the noise transition matrix [24, 34, 33, 30]. With perfectly estimated noise transition matrix, models trained with corrected losses can approximate to the models trained with clean labels. However,
∗co-ﬁrst author
†Correspondence to Tongliang Liu (tongliang.liu@sydney.edu.au) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Symmetric 50% (b) Pairﬂip 45% (c) Instance 40%
Figure 1: We train a ResNet-18 model on CIFAR-10 with three types of noisy labels and evaluate the impact of noisy labels on the representations from the 9-th layer, the 17-th layer, and the ﬁnal layer.
The X-axis is the number of epochs for the ﬁrst block of the network. The curves present the mean of
ﬁve runs and the best performances are indicated with dotted vertical lines. current methods are usually fragile to estimate the noise transition matrix for heavy noisy data and are also hard to handle a large number of classes [9]. The second type explores the dynamic process of optimization policies, which relates to the memorization effect−deep neural networks tend to ﬁrst memorize and ﬁt majority (clean) patterns and then overﬁt minority (noisy) patterns [2]. Recently, based on this phenomenon, many methods [9, 26, 15, 16, 29] have been proposed and achieved promising performance.
To exploit the memorization effect, when the double descent phenomenon [3, 22, 11] cannot be guaranteed to occur, a core issue is to study when to stop the optimization of the network. While stopping the training for too few epochs can avoid overﬁtting to noisy labels, it can also make the network underﬁt to clean labels. Current methods [25, 23] usually adopt an early stopping strategy, which decides the stopping point by considering the network as a whole. However, since DNNs are usually optimized with stochastic gradient descent (SGD) with backpropagation, supervisory signals will gradually propagate through the whole network from latter layers (i.e., layers that are closer to output layers) to former layers (i.e., layers that are closer to input layers). Noting that the output layer is followed by the empirical risk in the optimization procedure. We hypothesize that noisy labels may have more severe impacts for the latter layers, which is different from current methods [9, 15] that usually stop the training of the whole network at once.
To empirically verify the above hypothesis, we analyze the impact of noisy labels on representations from different layers with different training epochs. To quantitatively measure the impact of noisy labels from intermediate layers, we ﬁrst train the whole network on noisy data with different training epochs and ﬁx the parameters for the selected layer and its previous layers. We then reinitialize and optimize the rest layers with clean data, and the ﬁnal classiﬁcation performance is adopted to evaluate the impact of noisy labels. For the ﬁnal layer, we directly report the overall classiﬁcation performance.
As illustrated in Figure 1, we can see that latter layers always achieve the best performance at relatively smaller epoch numbers and then exhibit stronger performance drops with additional training epochs, which veriﬁes the hypothesis that noisy data may have more severe impacts for latter layers. With this understanding, we can infer that the early stopping, which optimizes the network all at once, may fail to fully exploit the memorization effect and induce sub-optimal performance.
To address the above problem, we propose to optimize a DNN by considering it as a composition of several DNN parts and present a novel progressive early stopping (PES) method. Speciﬁcally, we initially train former DNN layers by optimizing them with a relatively large number of epochs. Then, to alleviate the impact of noisy labels for latter layers, we reinitialize and progressively train latter
DNN layers by using smaller numbers of epochs with preceding DNN layers ﬁxed. Since different layers are progressively trained with different early stopping epochs, we term the proposed method as progressive early stopping (PES). Despite its simplicity, compared with normal early stopping trick,
PES can help to better exploit the memorization effect and obtain more promising and stable results.
Moreover, since the model size and training epochs are gradually reduced during the optimization procedure, the training time of PES is only slightly greater than that of the normal early stopping.
Finally, by combining PES with existing approaches on noisy label training tasks, we establish new state-of-the-art (SOTA) results on CIFAR-10 and CIFAR-100 with synthetic noise. We also achieve competitive results on one dataset with real-world noise: Clothing-1M [32]. 2
The rest of the paper is organized as follows. In Section 2, we ﬁrst introduce the proposed progressive early stopping and then present the details of the proposed algorithm by combining our method with existing approaches on noisy label training tasks. Section 3 shows the experimental results of our proposed method.