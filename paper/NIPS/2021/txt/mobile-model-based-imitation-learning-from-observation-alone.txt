Abstract
This paper studies Imitation Learning from Observations alone (ILFO) where the learner is presented with expert demonstrations that consist only of states visited by an expert (without access to actions taken by the expert). We present a provably efﬁcient model-based framework MobILE to solve the ILFO problem. MobILE in-volves carefully trading off strategic exploration against imitation - this is achieved by integrating the idea of optimism in the face of uncertainty into the distribu-tion matching imitation learning (IL) framework. We provide a uniﬁed analysis for MobILE, and demonstrate that MobILE enjoys strong performance guaran-tees for classes of MDP dynamics that satisfy certain well studied notions of structural complexity. We also show that the ILFO problem is strictly harder than the standard IL problem by presenting an exponential sample complexity separation between IL and ILFO. We complement these theoretical results with experimental simulations on benchmark OpenAI Gym tasks that indicate the ef-ﬁcacy of MobILE. Code for implementing the MobILE framework is available at https://github.com/rahulkidambi/MobILE-NeurIPS2021. 1

Introduction
This paper considers Imitation Learning from Observation Alone (ILFO). In ILFO, the learner is presented with sequences of states encountered by the expert, without access to the actions taken by the expert, meaning approaches based on a reduction to supervised learning (e.g., Behavior cloning (BC) [49], DAgger [50]) are not applicable. ILFO is more general and has potential for applications where the learner and expert have different action spaces, applications like sim-to-real [56, 14] etc.
Recently, [59] reduced the ILFO problem to a sequence of one-step distribution matching problems that results in obtaining a non-stationary policy. This approach, however, is sample inefﬁcient for longer horizon tasks since the algorithm does not effectively reuse previously collected samples when solving the current sub-problem. Another line of work considers model-based methods to infer the expert’s actions with either an inverse dynamics [63] or a forward dynamics [16] model; these recovered actions are then fed into an IL approach like BC to output the ﬁnal policy. These works rely on stronger assumptions that are only satisﬁed for Markov Decision Processes (MDPs) with injective transition dynamics [68]; we return to this in the related works section.
∗Work initiated when RK was a post-doc at Cornell University; work done outside Amazon. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
We introduce MobILE—Model-based Imitation
Learning and Exploring, a model-based framework, to solve the ILFO problem. In contrast to existing model-based efforts, MobILE learns the forward tran-sition dynamics model—a quantity that is well de-ﬁned for any MDP. Importantly, MobILE combines strategic exploration with imitation by interleaving a model learning step with a bonus-based, optimistic distribution matching step – a perspective, to the best of our knowledge, that has not been considered in
Imitation Learning. MobILE has the ability to au-tomatically trade-off exploration and imitation. It simultaneously explores to collect data to reﬁne the model and imitates the expert wherever the learned model is accurate and certain. At a high level, our the-oretical results and experimental studies demonstrate that systematic exploration is beneﬁcial for solving
ILFO reliably and efﬁciently, and optimism is a both theoretically sound and practically effective approach for strategic exploration in ILFO (see Figure 1 for comparisons with other ILFO algorithms). This paper extends the realm of partial information problems (e.g. Reinforcement Learning and Bandits) where optimism has been shown to be crucial in obtaining strong performance, both in theory (e.g.,
E3 [30], UCB [3]) and practice (e.g., RND [10]). This paper proves that incorporating optimism into the min-max IL framework [69, 22, 59] is beneﬁcial for both the theoretical foundations and empirical performance of ILFO.
Figure 1: Expert performance normalized scores of ILFO algorithms averaged across 5 seeds in environments with discrete action spaces (Reacher-v2) and continuous action spaces (Hopper-v2 and Walker2d-v2).
Our Contributions: We present MobILE (Algorithm 1), a provably efﬁcient, model-based frame-work for ILFO that offers competitive results in benchmark gym tasks. MobILE can be instantiated with various implementation choices owing to its modular design. This paper’s contributions are: 1. The MobILE framework combines ideas of model-based learning, optimism for exploration, and adversarial imitation learning. MobILE achieves global optimality with near-optimal regret bounds for classes of MDP dynamics that satisfy certain well studied notions of complexity. The key idea of MobILE is to use optimism to trade-off imitation and exploration. 2. We show an exponential sample complexity gap between ILFO and classic IL where one has access to expert’s actions. This indicates that ILFO is fundamentally harder than IL. Our lower bound on ILFO also indicates that to achieve near optimal regret, one needs to perform systematic exploration rather than random or no exploration, both of which will incur sub-optimal regret. 3. We instantiate MobILE with a model ensemble of neural networks and a disagreement-based bonus.
We present experimental results on benchmark OpenAI Gym tasks, indicating MobILE compares favorably to or outperforms existing approaches. Ablation studies indicate that optimism indeed helps in signiﬁcantly improving the performance in practice. 1.1