Abstract
Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from ﬁnding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efﬁciency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Auto-former as a novel decomposition architecture with an Auto-Correlation mechanism.
We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representa-tion aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efﬁciency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering
ﬁve practical applications: energy, trafﬁc, economics, weather and disease. Code is available at this repository: https://github.com/thuml/Autoformer. 1

Introduction
Time series forecasting has been widely used in energy consumption, trafﬁc and economics planning, weather and disease propagation forecasting. In these real-world applications, one pressing demand is to extend the forecast time into the far future, which is quite meaningful for the long-term planning and early warning. Thus, in this paper, we study the long-term forecasting problem of time series, characterizing itself by the large length of predicted time series. Recent deep forecasting models
[41, 17, 20, 28, 23, 29, 19, 35] have achieved great progress, especially the Transformer-based models.
Beneﬁting from the self-attention mechanism, Transformers obtain great advantage in modeling long-term dependencies for sequential data, which enables more powerful big models [7, 11].
However, the forecasting task is extremely challenging under the long-term setting. First, it is unreliable to discover the temporal dependencies directly from the long-term time series because the dependencies can be obscured by entangled temporal patterns. Second, canonical Transformers with self-attention mechanisms are computationally prohibitive for long-term forecasting because of the quadratic complexity of sequence length. Previous Transformer-based forecasting models
[41, 17, 20] mainly focus on improving self-attention to a sparse version. While performance is signiﬁcantly improved, these models still utilize the point-wise representation aggregation. Thus, in the process of efﬁciency improvement, they will sacriﬁce the information utilization because of the sparse point-wise connections, resulting in a bottleneck for long-term forecasting of time series. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To reason about the intricate temporal patterns, we try to take the idea of decomposition, which is a standard method in time series analysis [1, 27]. It can be used to process the complex time series and extract more predictable components. However, under the forecasting context, it can only be used as the pre-processing of past series because the future is unknown [15]. This common usage limits the capabilities of decomposition and overlooks the potential future interactions among decomposed components. Thus, we attempt to go beyond pre-processing usage of decomposition and propose a generic architecture to empower the deep forecasting models with immanent capacity of progressive decomposition. Further, decomposition can ravel out the entangled temporal patterns and highlight the inherent properties of time series [15]. Beneﬁting from this, we try to take advantage of the series periodicity to renovate the point-wise connection in self-attention. We observe that the sub-series at the same phase position among periods often present similar temporal processes. Thus, we try to construct a series-level connection based on the process similarity derived by series periodicity.
Based on the above motivations, we propose an original Autoformer in place of the Transformers for long-term time series forecasting. Autoformer still follows residual and encoder-decoder structure but renovates Transformer into a decomposition forecasting architecture. By embedding our proposed decomposition blocks as the inner operators, Autoformer can progressively separate the long-term trend information from predicted hidden variables. This design allows our model to alternately decompose and reﬁne the intermediate results during the forecasting procedure. Inspired by the stochastic process theory [8, 24], Autoformer introduces an Auto-Correlation mechanism in place of self-attention, which discovers the sub-series similarity based on the series periodicity and aggre-gates similar sub-series from underlying periods. This series-wise mechanism achieves (L log L) complexity for length-L series and breaks the information utilization bottleneck by expanding the point-wise representation aggregation to sub-series level. Autoformer achieves the state-of-the-art accuracy on six benchmarks. The contributions are summarized as follows:
O
• To tackle the intricate temporal patterns of the long-term future, we present Autoformer as a decomposition architecture and design the inner decomposition block to empower the deep forecasting model with immanent progressive decomposition capacity.
• We propose an Auto-Correlation mechanism with dependencies discovery and information aggregation at the series level. Our mechanism is beyond previous self-attention family and can simultaneously beneﬁt the computation efﬁciency and information utilization.
• Autoformer achieves a 38% relative improvement under the long-term setting on six bench-marks, covering ﬁve real-world applications: energy, trafﬁc, economics, weather and disease. 2