Abstract
Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks. A range of defense methods have been proposed to train adversarially robust DNNs, among which adversarial training has demonstrated promising results. However, despite preliminary understandings developed for adversarial training, it is still not clear, from the architectural perspective, what conﬁgurations can lead to more robust DNNs. In this paper, we address this gap via a comprehensive investigation on the impact of network width and depth on the robustness of adversarially trained
DNNs. Speciﬁcally, we make the following key observations: 1) more parame-ters (higher model capacity) does not necessarily help adversarial robustness; 2) reducing capacity at the last stage (the last group of blocks) of the network can actually improve adversarial robustness; and 3) under the same parameter budget, there exists an optimal architectural conﬁguration for adversarial robustness. We also provide a theoretical analysis explaning why such network conﬁguration can help robustness. These architectural insights can help design adversarially robust
DNNs. Code is available at https://github.com/HanxunH/RobustWRN. 1

Introduction
Deep neural networks (DNNs) are becoming standard models for many real-world applications such as image classiﬁcation [1], object detection [2] and natural language processing [3]. However, a line of research has shown that DNNs are vulnerable to adversarial examples (attacks), which can be easily crafted by slightly perturbing the input instance to maximize the model’s prediction error [4–6].
This vulnerability of DNNs has become a major concern for their deployment in security-critical applications such as autonomous driving [7, 8] and medical diagnosis [9, 10].
A number of defense methods have been proposed to train adversarially robust DNNs [11–14], among which adversarial training has demonstrated the most promising results [15–17]. Adversarial training can be viewed as a type of data augmentation that trains DNNs on adversarial (instead of natural) examples [15–19]. Based on adversarial training, a set of works have been proposed to understand its learning and convergence behaviors, and the key factors for training adversarially robust DNNs. For example, it has been found that adversarial training encourages the model to learn more robust or compact features [20, 21], and it requires more data [22–25] or higher capacity models to gain more robustness [15, 26]. While these understandings have motivated several improved defense methods, it is still not clear, from an architectural perspective, what makes an adversarially robust DNN.
†Correspondence to: Xingjun Ma (danxjma@gmail.com) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this paper, we present the ﬁrst comprehensive investigation on the architectural ingredients of adversarially robust DNNs. Our investigation is based on adversarial training and WideResNet-34-10 (WRN-34-10) [27], one extensively tested architecture in the defense literature. Based on the base architectural conﬁguration of WRN-34-10, we apply a ﬁnely-controlled grid search to explore the impact of network width and depth conﬁgurations on the robustness of adversarial trained DNNs.
The standard WRN-34-10 consists of 3 stages with each stage being a group of 5 (i.e., depth) residual blocks and each residual block having 2 convolutional layers. We denote the three stages as Stage-1,
Stage-2 and Stage-3 following the direction from the input to the output. Each stage is conﬁgured by a depth (number of residual blocks) and a width (number of ﬁlters) factor. The hyper-parameters for width and depth of each stage control the scale of learnable parameters (capacity). In this paper, we explore different conﬁgurations of width and depth for each of the three stages. Based on our explorations, we make the following key observations:
• Simply increasing the number of parameters (model capacity) by upscaling width or depth does not necessarily lead to improved robustness. This contrasts with current beliefs that, under the same type of architecture, more parameters (higher model capacity) can improve adversarial robustness [15, 26, 28]. Adversarial training does require larger capacity models, but there exists a trade-off. We provide both theoretical and empirical evidences that wider/deeper models increase Lipschitzness (larger Lipschitz constant).
• For a larger model used in adversarial training, reducing capacity at the last stage (Stage-3) of WRNs can achieve a better trade-off between capacity and Lipschitzness, thus improving adversarial robustness. This can be achieved by reducing either depth or width, with width reduction being slightly more effective. This highlights that smaller DNNs can also have better robustness if the parameter reduction is applied at the right place (i.e., the last stage).
• Under the same type of architectures (i.e., WRNs) and parameter budget, there may exist an optimal architectural conﬁguration that can produce the most robust DNN. We show that the same conﬁguration rule can also be applied to improve the robustness of VGGs, DenseNets (DNs), as well as networks found by Differentiable Architecture Search (DARTS).
Furthermore, we provide a series of understandings for the above ﬁndings, which can not only provide useful insights for training more robust models with adversarial training, but also shed new light on the architectural ingredients of adversarially robust DNNs. 2