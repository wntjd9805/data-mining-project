Abstract
We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck as a regularizer for learning an adversarially robust deep neural network classiﬁer.
In addition to the usual cross-entropy loss, we add regularization terms for every intermediate layer to ensure that the latent representations retain useful informa-tion for output prediction while reducing redundant information. We show that the HSIC bottleneck enhances robustness to adversarial attacks both theoretically and experimentally. In particular, we prove that the HSIC bottleneck regularizer reduces the sensitivity of the classiﬁer to adversarial examples. Our experiments on multiple benchmark datasets and architectures demonstrate that incorporating an HSIC bottleneck regularizer attains competitive natural accuracy and improves adversarial robustness, both with and without adversarial examples during train-ing. Our code and adversarially robust models are publicly available.2 1

Introduction
Adversarial attacks [8, 17, 18, 3, 5] to deep neural networks (DNNs) have received considerable attention recently. Such attacks are intentionally crafted to change prediction outcomes, e.g, by adding visually imperceptible perturbations to the original, natural examples [25]. Adversarial ro-bustness, i.e., the ability of a trained model to maintain its predictive power under such attacks, is an important property for many safety-critical applications [4, 6, 26]. The most common approach to construct adversarially robust models is via adversarial training [34, 36, 30], i.e., training the model over adversarially constructed samples.
Alemi et al. [1] propose using the so-called Information Bottleneck (IB) [27, 28] to ehnance ad-versarial robustness. Proposed by Tishby and Zaslavsky [28], the information bottleneck expresses a tradeoff between (a) the mutual information of the input and latent layers vs. (b) the mutual in-formation between latent layers and the output. Alemi et al. show empirically that using IB as a learning objective for DNNs indeed leads to better adversarial robustness. Intuitively, the IB objec-tive increases the entropy between input and latent layers; in turn, this also increases the model’s robustness, as it makes latent layers less sensitive to input perturbations.
Nevertheless, mutual information is notoriously expensive to compute. The Hilbert-Schmidt inde-pendence criterion (HSIC) has been used as a tractable, efﬁcient substitute in a variety of machine
∗Equal contribution. 2https://github.com/neu-spiral/HBaR 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of HBaR for adversarial robustness. A neural network trained with HBaR gives a more constrained prediction w.r.t. perturbed inputs. Thus, it is less sensitive to adversarial examples. learning tasks [31, 32, 33]. Recently, Ma et al. [16] also exploited this relationship to propose an
HSIC bottleneck (HB), as a variant to the more classic (mutual-information based) information bot-tleneck, though not in the context of adversarial robustness.
We revisit the HSIC bottleneck, studying its adversarial robustness properties. In contrast to both
Alemi et al. [1] and Ma et al. [16], we use the HSIC bottleneck as a regularizer in addition to com-monly used losses for DNNs (e.g., cross-entropy). Our proposed approach, HSIC-Bottleneck-as-Regularizer (HBaR) can be used in conjunction with adversarial examples; even without adversarial training, it is able to improve a classiﬁer’s robustness. It also signiﬁcantly outperforms previous
IB-based methods for robustness, as well as the method proposed by Ma et al.
Overall, we make the following contributions: 1. We apply the HSIC bottleneck as a regularizer for the purpose of adversarial robustness. 2. We provide a theoretical motivation for the constituent terms of the HBaR penalty, proving that it indeed constrains the output perturbation produced by adversarial attacks. 3. We show that HBaR can be naturally combined with a broad array of state of the art adversarial training methods, consistently improving their robustness. 4. We empirically show that this phenomenon persists even for weaker methods. In particular,
HBaR can even enhance the adversarial robustness of plain SGD, without access to adversarial examples.
The remainder of this paper is structured as follows. We review related work in Sec. 2. In Sec. 3, we discuss the standard setting of adversarial robustness and HSIC. In Sec. 4, we provide a theoret-ical justiﬁcation that HBaR reduces the sensitivity of the classiﬁer to adversarial examples. Sec. 5 includes our experiments; we conclude in Sec. 6. 2