Abstract
We present a methodology for modifying the behavior of a classiﬁer by directly rewriting its prediction rules.1 Our approach requires virtually no additional data collection and can be applied to a variety of settings, including adapting a model to new environments, and modifying it to ignore spurious features. 1

Introduction
At the core of machine learning is the ability to automatically discover prediction rules from raw data. However, there is mounting evidence that not all of these rules are reliable [Torralba and
Efros, 2011, Beery et al., 2018, Shetty et al., 2019, Agarwal et al., 2020, Xiao et al., 2020, Bissoto et al., 2020, Geirhos et al., 2020]. In particular, some rules could be based on biases in the training data: e.g., learning to associate cows with grass since they are typically depicted on pastures [Beery et al., 2018]. While such prediction rules may be useful in some scenarios, they will be irrelevant or misleading in others. This raises the question:
How can we most effectively modify the way in which a given model makes its predictions?
The canonical approach for performing such post hoc modiﬁcations is to intervene at the data level.
For example, by gathering additional data that better reﬂects the real world (e.g., images of cows on the beach) and then using it to further train the model. Unfortunately, collecting such data can be challenging: how do we get cows to pose for us in a variety of environments? Furthermore, data collection is ultimately a very indirect way of specifying the intended model behavior. After all, even when data has been carefully curated to reﬂect a given real-world task, models still end up learning unintended prediction rules from it [Ponce et al., 2006, Torralba and Efros, 2011, Tsipras et al., 2020, Beyer et al., 2020].
Our contributions
The goal of our work is to develop a toolkit that enables users to directly modify the prediction rules learned by an (image) classiﬁer, as opposed to doing so implicitly via the data. Concretely:
Editing prediction rules. We build on the recent work of Bau et al. [2020a] to develop a method for modifying a classiﬁer’s prediction rules with essentially no additional data collection (Section 2). At a high level, our method enables the user to modify the weight of a layer so that the latent
∗Equal contribution. 1Our code is available at https://github.com/MadryLab/EditingClassifiers. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Editing prediction rules in pre-trained classiﬁers using a single exemplar. (a) We edit a VGG16 ImageNet classiﬁer to map the representation of the concept “snow” to that of “asphalt road”. (b) This edit corrects classiﬁcation errors on snowy scenes corresponding to various classes. (c) We edit a CLIP [Radford et al., 2021] model such that the text “iPod” maps to a blank area. (d)
This change makes the model robust to the typographic attacks from Goh et al. [2021]. representations of a speciﬁc concept (e.g., snow) map to the representations of another (e.g., road).
Crucially, this allows us to change the behavior of the classiﬁer on all occurrences of that concept, beyond the speciﬁc examples (and the corresponding classes) used in the editing process.
Real-world scenarios. We demonstrate our approach in two scenarios motivated by real-world applications (Section 3). First, we focus on adapting an ImageNet classiﬁer to a new environment: recognizing vehicles on snowy roads. Second, we consider the recent “typographic attack” of Goh et al. [2021] on a zero-shot CLIP [Radford et al., 2021] classiﬁer: attaching a piece of paper with
“iPod” written on it to various household items causes them to be incorrectly classiﬁed as “iPod.”
In both settings, we ﬁnd that our approach enables us to signiﬁcantly improve model performance, using only a single, synthetic example to perform the edit—cf. Figure 1.
Large-scale synthetic evaluation. To evaluate our method at scale, we develop an automated pipeline to generate a suite of varied test cases (Section 4). Our pipeline revolves around identi-fying speciﬁc concepts (e.g., “road" or “pasture”) in an existing dataset using pre-trained instance segmentation models and then modifying them using style transfer [Gatys et al., 2016] (e.g., to create “snowy road”). We ﬁnd that our editing methodology is able to consistently correct a signif-icant fraction of model failures induced by these transformations. In contrast, standard ﬁne-tuning approaches are unable to do so given the same data, often causing more errors than they are ﬁxing.
Probing model behavior with counterfactuals. Beyond model editing, our concept-transformation pipeline can also be viewed as a scalable way of generating image counterfactuals.
In Section 5, we demonstrate how such counterfactuals can be useful to gain insights into how a given model makes its predictions and pinpoint certain spurious correlations that it has picked up. 2 A toolkit for editing prediction rules
It has been widely observed that models pick up various context-speciﬁc correlations in the data— e.g., using the presence of “road” or a “wheel” to predict “car” (cf. Section 5). Such unreliable prediction rules (dependencies of predictions on speciﬁc input concepts) could hinder models when they encounter novel environments (e.g., snow-covered roads), and confusing or adversarial test conditions (e.g., cars with wooden wheels). Thus, a model designer might want to modify these rules before deploying their model.
The canonical approach to modify a classiﬁer post hoc is to collect additional data that captures the desired deployment scenario, and use it to retrain the model. However, even setting aside the 2
challenges of data collection, it is not obvious a priori how much of an effect such retraining (e.g., via ﬁne-tuning) will have. For instance, if we ﬁne-tune our model on “cars” with wooden wheels, will it now recognize “scooters” or “trucks” with such wheels?
The goal of this work is to instead develop a more direct way to modify a model’s behavior: rewriting its prediction rules in a targeted manner. For instance, in our previous example, we would ideally be able to modify the classiﬁer to correctly recognize all vehicles with wooden wheels by simply teaching it to treat any wooden wheel as it would a standard one. Our approach is able to do exactly this. However, before describing this approach (Section 2.2), we ﬁrst provide a brief overview of recent work by Bau et al. [2020a] which forms its basis. 2.1