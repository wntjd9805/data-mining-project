Abstract
In recent years, personalized federated learning (pFL) has attracted increasing attention for its potential in dealing with statistical heterogeneity among clients.
However, the state-of-the-art pFL methods rely on model parameters aggregation at the server side, which require all models to have the same structure and size, and thus limits the application for more heterogeneous scenarios. To deal with such model constraints, we exploit the potentials of heterogeneous model settings and propose a novel training framework to employ personalized models for different clients. Speciﬁcally, we formulate the aggregation procedure in original pFL into a personalized group knowledge transfer training algorithm, namely, KT-pFL, which enables each client to maintain a personalized soft prediction at the server side to guide the others’ local training. KT-pFL updates the personalized soft prediction of each client by a linear combination of all local soft predictions using a knowledge coefﬁcient matrix, which can adaptively reinforce the collaboration among clients who own similar data distribution. Furthermore, to quantify the contributions of each client to others’ personalized training, the knowledge coefﬁcient matrix is parameterized so that it can be trained simultaneously with the models. The knowledge coefﬁcient matrix and the model parameters are alternatively updated in each round following the gradient descent way. Extensive experiments on various datasets (EMNIST, Fashion_MNIST, CIFAR-10) are conducted under different settings (heterogeneous models and data distributions). It is demonstrated that the proposed framework is the ﬁrst federated learning paradigm that realizes personalized model training via parameterized group knowledge transfer while achieving signiﬁcant performance gain comparing with state-of-the-art algorithms. 1

Introduction
Federated Learning (FL) [1] has emerged as an efﬁcient paradigm to collaboratively train a shared machine learning model among multiple clients without directly accessing their private data. By periodically aggregating parameters from the clients for global model updating, it can converge to high accuracy and strong generalization. FL has shown its capability to protect user privacy while there remains a crucial challenge that signiﬁcantly degrades the learning performance, i.e., statistic heterogeneity in users’ local datasets. Given the Non-Independent and Identically Distributed (Non-IID) user data, the trained global model often cannot be generalized well over each client [2–5].
To deal with the above issues, employing personalized models appears to be an effective solution in
FL, i.e., personalized federated learning (pFL). Recent works regarding pFL include regularization-based methods [6–8] (i.e., pFedMe [6], L2SGD [7], FedAMP [8]), meta-learning-based Per-FedAvg
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
[9] and cluster-based IFCA [10, 11]. However, in order to aggregate the parameters from all clients, it is inevitable for them to have identical model structure and size. Such constraints would prevent status quo pFL methods from further application in practical scenarios, where clients are often willing to own unique models, i.e., with customized neural architectures to adapt to heterogeneous capacities in computation, communication and storage space, etc. Motivated by the paradigm of
Knowledge Distillation (KD) [12–16] that knowledge can be transferred from a neural network to another via exchanging soft predictions instead of using the whole model parameters, KD-based FL training methods have been studied [12, 14–19] to collaboratively train heterogeneous models in a privacy-preserving way. However, these works have neglected the further personalization requirement of FL clients, which can be well satisﬁed via the personalized knowledge transfer for heterogeneous
FL users.
In this paper, we seek to develop a novel training framework that can accommodate heterogeneous model structures for each client and achieve personalized knowledge transfer in each FL training round. To this end, we formulate the aggregation phase in FL to a personalized group knowledge transfer training algorithm dubbed KT-pFL, whose main idea is to allow each client to maintain a personalized soft prediction at the server that can be updated by a linear combination of all clients’ local soft predictions using a knowledge coefﬁcient matrix. The principle of doing so is to reinforce the collaboration between clients with similar data distributions. Furthermore, to quantify the contribution of each client to other’s personalized soft prediction, we parameterize the knowledge coefﬁcient matrix so that it can be trained simultaneously with the models following an alternating way in each iteration round.
We show that KT-pFL not only breaks down the barriers of homogeneous model restriction, which requires to transfer the entire parameters set in each round, whose data volume is much larger than that of the soft prediction, but also improves the training efﬁciency by using a parameterized update mechanism. Experimental results on different datasets and models show that our method can signiﬁcantly improve the training efﬁciency and reduce the communication overhead. Our contributions are:
• To the best of our knowledge, this paper is the ﬁrst to study the personalized knowledge transfer in FL. We propose a novel training framework, namely, KT-pFL, that maintains a personalized soft prediction for each client in the server to transfer knowledge among all clients.
• To encourage clients with similar data distribution to collaborate with each other during the training process, we propose the ‘knowledge coefﬁcient matrix’ to identify the contribution from one client to others’ local training. To show the efﬁciency of the parameterized method, we compared KT-pFL with two non-parameterized learning methods, i.e., TopK-pFL and Sim-pFL, which calculate the knowledge coefﬁcient matrix on the cosine similarity between different model parameters.
• We provide theoretical performance guarantee for KT-pFL and conduct extensive experiments over various deep learning models and datasets. The efﬁciency superiority of KT-pFL is demonstrated by comparing our proposed training framework with traditional pFL methods. 2