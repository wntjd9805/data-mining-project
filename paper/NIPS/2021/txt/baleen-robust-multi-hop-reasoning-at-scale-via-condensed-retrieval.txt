Abstract
Multi-hop reasoning (i.e., reasoning across two or more documents) is a key ingredient for NLP models that leverage large corpora to exhibit broad knowledge.
To retrieve evidence passages, multi-hop models must contend with a fast-growing search space across the hops, represent complex queries that combine multiple information needs, and resolve ambiguity about the best order in which to hop between training passages. We tackle these problems via Baleen, a system that improves the accuracy of multi-hop retrieval while learning robustly from weak training signals in the many-hop setting. To tame the search space, we propose condensed retrieval, a pipeline that summarizes the retrieved passages after each hop into a single compact context. To model complex queries, we introduce a focused late interaction retriever that allows different parts of the same query representation to match disparate relevant passages. Lastly, to infer the hopping dependencies among unordered training passages, we devise latent hop ordering, a weak-supervision strategy in which the trained retriever itself selects the sequence of hops. We evaluate Baleen on retrieval for two-hop question answering and many-hop claim veriﬁcation, establishing state-of-the-art performance. 1

Introduction
In open-domain reasoning, a model is tasked with retrieving evidence from a large corpus to answer questions, verify claims, or otherwise exhibit broad knowledge. In practice, such open-domain tasks often further require multi-hop reasoning, where the evidence must be extracted from two or more documents. To do this effectively, a model must learn to use partial evidence it retrieves to bridge its way to additional documents leading to an answer. In this vein, HotPotQA [31] contains complex questions answerable by retrieving two passages from Wikipedia, while HoVer [13] contains claims that can only be veriﬁed (or disproven) by combining facts from up to four Wikipedia passages.
Table 1 illustrates this using the claim “The MVP of [a] game Red Flaherty umpired was elected to the Baseball Hall of Fame” from the HoVer validation set. To verify this claim, a HoVer model must identify facts spread across three Wikipedia pages: Red Flaherty umpired in the World Series in 1955, 1958, 1965, and 1970. The MVP of the 1965 World Series was Sandy Koufax. Koufax was later elected to the Baseball Hall of Fame.
This three-hop claim illustrates three major challenges in multi-hop retrieval. First, multi-hop queries encompass multiple information needs; the claim above referenced facts from three disparate passages.
Second, retrieval errors in each hop propagate to subsequent hops. This can happen if the model directly retrieves information about the Baseball Hall of Fame, confuses Red Flaherty with, say,
Robert Flaherty, or singles out the MVP of, say, the 1958 World Series, which Flaherty also umpired.
Third, due to the dependency between hops, retrievers must learn an effective sequence of hops, where previously-retrieved clues lead to other relevant passages. These inter-passage dependencies 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: An example (multi-hop) claim Q0 to verify and an illustration of condensed retrieval queries
Qt after each hop t ≥ 1. Page titles are in bold. We shorten the Wikipedia sentences for presentation.
Q0
Q1
Q2
Q3
The MVP of [a] game Red Flaherty umpired was elected to the Baseball Hall of Fame.
The MVP of [a] game Red Flaherty umpired was elected to the Baseball Hall of Fame. Red Flaherty:
He umpired in World Series 1955, 1958, 1965, and 1970.
The MVP of [a] game Red Flaherty umpired was elected to the Baseball Hall of Fame. Red Flaherty:
He umpired in World Series 1955, 1958, 1965, and 1970. 1965 World Series: It is remembered for
MVP Sandy Koufax.
The MVP of [a] game Red Flaherty umpired was elected to the Baseball Hall of Fame. Red Flaherty:
He umpired in World Series 1955, 1958, 1965, and 1970. 1965 World Series: It is remembered for
MVP Sandy Koufax. Sandy Koufax: He was elected to the Baseball Hall of Fame. can be non-obvious for many-hop problems with three or more passages, and are often left unlabeled, as it can be expensive to annotate one (or every) sequence in which facts could be retrieved.
These challenges call for highly expressive query representations, robustness to retrieval errors, and scalability to many hops over massive document collections. Existing systems fall short on one or more of these criteria. For instance, many state-of-the-art systems rely on bag-of-words [23] or single-vector dot-product [29] retrievers, whose capacity to model an open-domain question is limited
[17], let alone complex multi-hop queries. Furthermore, existing systems embed trade-offs when it comes to “hopping”: they employ brittle greedy search, which limits recall per hop; or they employ beam search over an exponential space, which reduces scalability to many hops; or they assume explicit links that connect every passage with related entities, which ties them to link-structured corpora. Lastly, to order the hops, many systems use fragile supervision heuristics (e.g., ﬁnding passages whose page titles appear in other passages) tailored for particular datasets like HotPotQA.
We tackle these problems with Baleen,1 a scalable multi-hop reasoning system that improves accuracy and robustness. We introduce a condensed retrieval architecture, where the retrieved facts from each hop are summarized into a short context that becomes a part of the query for subsequent hops, if any (Table 1). Unlike beam search, condensed retrieval allows effective scaling to many hops, and we ﬁnd that it complements greedy search (i.e., taking the best passage per hop) in improving recall considerably. We then tackle the complexity of queries by proposing a focused late interaction passage retriever (FLIPR), a robust learned search model that allow different parts of the same query representation to match disparate relevant passages. FLIPR inherits the scalability of the vanilla late interaction paradigm of ColBERT [16] but uniquely allows the same query to exhibit tailored matching patterns against each target passage. Lastly, we devise latent hop ordering, a weak-supervision strategy that uses the retriever itself to select effective hop paths.
We ﬁrst test Baleen on the two-hop HotPotQA benchmark, ﬁnding evidence of saturation in retrieval: we achieve 96.3% answer recall in the top-20 retrieved passages, up from 89.4% for existing work.
We then test Baleen’s ability to scale accurately to more hops, reporting our main results using the recent many-hop HoVer task. We build a strong many-hop baseline model that combines recent results from the open-domain question answering [17] and multi-hop [23] literatures. This baseline combines ColBERT retrieval and standard greedy search with an ELECTRA [5] re-ranker, and generalizes typical text-based heuristics to order the hops for training. After verifying its strong results on HotPotQA, we show that it outperforms the ofﬁcial TF-IDF + BERT baseline of HoVer by over 30 points in retrieval accuracy. Against this strong baseline itself, Baleen improves passage retrieval accuracy by another 17 points, raising it to over 90% at k = 100 passages. Baleen also improves the evidence extraction F1 score dramatically, outperforming even the oracle retrieval +
BERT results by Jiang et al. [13]. Our ablations (§5.4) show that Baleen’s FLIPR retriever, condenser architecture, and latent supervision are essential to its strong performance. 1https://github.com/stanford-futuredata/Baleen
In marine biology, “baleen” refers to the ﬁlter-feeding system that baleen whales use to capture small organisms,
ﬁltering out seawater. So too does our system seek to capture relevant facts from a sea of documents. 2
2