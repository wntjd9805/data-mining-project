Abstract
Multimodal meta-learning is a recent problem that extends conventional few-shot meta-learning by generalizing its setup to diverse multimodal task distributions.
This setup makes a step towards mimicking how humans make use of a diverse set of prior skills to learn new skills. Previous work has achieved encouraging per-formance. In particular, in spite of the diversity of the multimodal tasks, previous work claims that a single meta-learner trained on a multimodal distribution can sometimes outperform multiple specialized meta-learners trained on individual uni-modal distributions. The improvement is attributed to knowledge transfer between different modes of task distributions. However, there is no deep investigation to verify and understand the knowledge transfer between multimodal tasks. Our work makes two contributions to multimodal meta-learning. First, we propose a method to quantify knowledge transfer between tasks of different modes at a micro-level.
Our quantitative, task-level analysis is inspired by the recent transference idea from multi-task learning. Second, inspired by hard parameter sharing in multi-task learning and a new interpretation of related work, we propose a new multimodal meta-learner that outperforms existing work by considerable margins. While the major focus is on multimodal meta-learning, our work also attempts to shed light on task interaction in conventional meta-learning. The code for this project is available at https://miladabd.github.io/KML. 1

Introduction
Multimodal meta-learning was recently proposed as an extension of conventional few-shot meta-learning. In [1], it is deﬁned as a meta-learning problem that involves classiﬁcation tasks from multiple different input and label domains. An example in their work is a 3-mode few-shot image classiﬁcation which includes tasks to classify characters (Omniglot) and natural objects of different characteristics (FC100, mini-ImageNet). The multimodal extension of meta-learning is proposed with two objectives. First, it generalizes the conventional meta-learning setup for more diverse task distributions. Second, it makes a step towards mimicking humans’ ability to acquire a new skill via prior knowledge of a set of diverse skills. For example, humans can quickly learn a novel snowboarding trick by exploiting not only fundamental snowboarding knowledge but also skiing and skateboarding experience [1].
Multimodal Model-Agnostic Meta-Learning (MMAML) [1] proposes a framework to better handle multimodal task distributions and achieves encouraging performance. As one of the most intriguing
ﬁndings, MMAML claims that a single meta-learner trained on a multimodal distribution can sometimes outperform multiple specialized meta-learners trained on individual unimodal distributions.
This was observed in spite of the diversity of the multimodal tasks. In [1], this observation is attributed to knowledge transfer across different modes of multimodal task distribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In our work, we delve into understanding knowledge transfer in multimodal meta-learning. While improved performance using a multimodal task distribution is reported in [1], there is no deep investigation to verify and understand how tasks from different modes beneﬁt from each other.
Towards understanding knowledge transfer in multimodal meta-learning at a micro-level, we propose a new quantiﬁcation method inspired by the idea of transference recently proposed in multi-task learning (MTL) [2]. Despite the large number of meta-learning algorithms proposed in the literature, we remark that little work has been done in analyzing meta-learning at the task sample level. In particular, to the best of our knowledge, there is no previous work on understanding knowledge transfer among tasks and quantitative analysis of task samples in the context of meta-learning.
Because of the lack of such study, the interaction between task samples remains rather opaque in meta-learning. Interestingly, despite the notion of a task in meta-learning and MTL, there has not been much intersections between these two branches of research, perhaps due to several fundamental differences between meta-learning and MTL1 (e.g, meta-learning optimizes the risk over a large number of future tasks sampled from an unknown distribution of tasks, while MTL optimizes the average risk over a ﬁnite number of known tasks; this will be further discussed). We note that because of these fundamental differences, we propose adaptations to develop our method to quantify knowledge transfer for multimodal tasks in meta-learning.
Another contribution of our work is a new multimodal meta-learner. Our idea is inspired by hard parameter sharing in MTL [4]. Furthermore, we discuss our own interpretation of the modulation mechanism in [1]. These lead us to propose a new method that achieves substantial improvement over the best results in multimodal meta-learning [1]. While our major focus in this work is on multimodal meta-learning, we have also performed experiments on conventional meta-learning for our proposed knowledge transfer quantiﬁcation. Our work makes an attempt to shed light on task interaction in conventional meta-learning.
Our main contributions are
• Focusing on multimodal meta-learning, we propose a method to understand and quantify knowledge transfer across different modes at a micro-level.
• We propose a new multimodal meta-learner that outperforms existing state-of-the-art meth-ods by substantial margins. 2