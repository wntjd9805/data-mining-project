Abstract
We study the problem of estimating a rank-1 signal in the presence of rotationally invariant noise—a class of perturbations more general than Gaussian noise. Prin-cipal Component Analysis (PCA) provides a natural estimator, and sharp results on its performance have been obtained in the high-dimensional regime. Recently, an Approximate Message Passing (AMP) algorithm has been proposed as an alter-native estimator with the potential to improve the accuracy of PCA. However, the existing analysis of AMP requires an initialization that is both correlated with the signal and independent of the noise, which is often unrealistic in practice. In this work, we combine the two methods, and propose to initialize AMP with PCA. Our main result is a rigorous asymptotic characterization of the performance of this estimator. Both the AMP algorithm and its analysis differ from those previously derived in the Gaussian setting: at every iteration, our AMP algorithm requires a speciﬁc term to account for PCA initialization, while in the Gaussian case, PCA initialization affects only the ﬁrst iteration of AMP. The proof is based on a two-phase artiﬁcial AMP that ﬁrst approximates the PCA estimator and then mimics the true AMP. Our numerical simulations show an excellent agreement between
AMP results and theoretical predictions, and suggest an interesting open direction on achieving Bayes-optimal performance. 1

Introduction
We consider the problem of estimating a rank-1 signal from a noisy data matrix. In the square symmetric case, the data matrix is modeled as
X =
α n u∗u∗T + W ∈ Rn×n, (1.1) where u∗ ∈ Rn is the unknown rank-1 signal, W ∈ Rn×n is a symmetric noise matrix, and α > 0 captures the signal-to-noise ratio (SNR). In the rectangular case, we observe the data matrix
X =
α m u∗v∗T + W ∈ Rm×n, (1.2) where u∗ ∈ Rm and v∗ ∈ Rn are the unknown signals, and W ∈ Rm×n is a rectangular noise matrix.
A natural estimator of the signal in the symmetric case is the principal eigenvector of X (singular vectors, in the rectangular case). The performance of this principal component analysis (PCA) estimator and, more generally, the behavior of eigenvalues and eigenvectors of models like (1.1)-(1.2) has been widely studied in statistics [27, 49] and random matrix theory [2, 3, 10, 11, 15, 22, 30].
If u∗, v∗ are unstructured (e.g., they are uniformly distributed on a sphere), then it is not generally possible to improve on the PCA estimator. However, in a broad range of applications, the unknown signals have some underlying structure, e.g., they may be sparse, their entries may belong to a certain 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
set, or they may be modelled using a prior distribution. Examples of structured matrix estimation problems include sparse PCA [17, 28, 61], non-negative PCA [32, 43], community detection under the stochastic block model [1, 16, 45], and group synchronization [50]. Since PCA is ill-equipped to capture the structure of the signal, we aim to improve on it using a family of iterative algorithms known as approximate message passing (AMP). AMP algorithms have two particularly attractive features: (i) they can be tailored to take advantage of prior information on the structure of the signal; and (ii) under suitable model assumptions, their performance in the high-dimensional limit is precisely characterized by a succinct deterministic recursion called state evolution [7, 13, 26].
AMP algorithms have been applied to a wide range of inference problems: estimation in linear models [8, 7, 19, 31, 39], generalized linear models [5, 37, 38, 42, 51, 53, 55], and low-rank matrix estimation with Gaussian noise [6, 17, 23, 29, 34, 44]. The survey [21] provides a uniﬁed description of AMP for these applications. Using the state evolution analysis, it has been proved that AMP achieves Bayes-optimal performance in some Gaussian models [17, 18, 44], and a bold conjecture from statistical physics posits that AMP is optimal among polynomial-time algorithms.
We study rank-1 matrix estimation in the setting where the noise matrix W is rotationally invariant.
This is a much milder assumption than W being Gaussian: it only imposes that the orthogonal matrices in the spectral decomposition of W are uniformly random, and allows for arbitrary eigen-values/singular values. Hence, W can capture a more complex correlation structure, which is typical in applications. For the models (1.1)-(1.2) with rotationally invariant noise, AMP algorithms were derived in [14, 48] and generalized in [20]. In particular, the AMP algorithm of [20] for the problem (1.1) produces estimates ut ∈ Rn as follows: ut = ut(f t−1), f t = Xut − bt,iui, t ≥ 2. (1.3) t (cid:88) i=1
The iteration is initialized with a pilot estimate u1. We can interpret (1.3) as a generalized power method. Recall that the power method approximates the principal eigenvector of X using the iterative updates ¯ut = X ¯ut−1/(cid:107)X ¯ut−1(cid:107). For each t, the function ut can be chosen to exploit any structural information known about the signal (e.g., sparsity). The “memory” coefﬁcients {bt,1, . . . , bt,t} have a speciﬁc form to ensure that the iterates (f t, ut+1) have desirable statistical properties captured by state evolution. A rigorous state evolution result for the iteration (1.3) is established in [20], but the algorithm and its analysis require an initialization u1 that is correlated with the unknown signal and independent of the noise W . In practice, one typically does not have access to such an initialization.
Main contribution.
In this paper, we propose an AMP algorithm initialized via the PCA estimator, namely, the principal eigenvector of X for the square case (1.1) and the left singular vector of X for the rectangular case (1.2). Our main technical contribution is a state evolution result for this AMP algorithm, which gives a rigorous characterization of its performance in the high-dimensional limit.
The challenge is that, as the PCA initialization depends on the noise matrix W , one cannot apply the state evolution machinery of [20]. To circumvent this issue, our key idea is to construct and analyze a two-phase artiﬁcial AMP algorithm. In the ﬁrst phase, the artiﬁcial AMP performs a power method approaching the PCA estimator; and in the second phase, it mimics the behavior of the true AMP.
We remark that the artiﬁcial AMP only serves as a proof technique. Thus, we can initialize it with a vector correlated with the signal u∗ and independent of the noise matrix W , which allows us to analyze it using the existing state evolution result.
Our analysis is tight in the sense that our AMP algorithm can be initialized with PCA whenever the PCA estimate has strictly positive correlation with the signal. This requires showing that, when
PCA is effective, the state evolution of the ﬁrst phase of the artiﬁcial AMP has a unique ﬁxed point.
To obtain such a result, we exploit free probability tools developed in [10, 11]. The agreement between the practical performance of AMP and the theoretical predictions of state evolution is demonstrated via numerical results for different spectral distributions of W . Our simulations also show that the performance of AMP—as well as its ability to improve upon the PCA initialization— crucially depends on the choice of the denoising functions ut in the algorithm. Thus, the design of a
Bayes-optimal AMP remains an exciting avenue for future research.