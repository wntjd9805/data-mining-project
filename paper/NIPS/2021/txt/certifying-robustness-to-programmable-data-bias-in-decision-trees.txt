Abstract
Datasets can be biased due to societal inequities, human biases, under-representation of minorities, etc. Our goal is to certify that models produced by a learning algorithm are pointwise-robust to potential dataset biases. This is a challenging problem: it entails learning models for a large, or even inﬁnite, num-ber of datasets, ensuring that they all produce the same prediction. We focus on decision-tree learning due to the interpretable nature of the models. Our approach allows programmatically specifying bias models across a variety of dimensions (e.g., missing data for minorities), composing types of bias, and targeting bias towards a speciﬁc group. To certify robustness, we use a novel symbolic technique to evaluate a decision-tree learner on a large, or inﬁnite, number of datasets, certi-fying that each and every dataset produces the same prediction for a speciﬁc test point. We evaluate our approach on datasets that are commonly used in the fairness literature, and demonstrate our approach’s viability on a range of bias models. 1

Introduction
The proliferation of machine-learning algorithms has raised alarming questions about fairness in automated decision-making [4]. In this paper, we focus our attention on bias in training data. Data can be biased due to societal inequities, human biases, under-representation of minorities, malicious data poisoning, etc. For instance, historical data can contain human biases, e.g., certain individuals’ loan requests get rejected, although (if discrimination were not present) they should have been approved, or women in certain departments are consistently given lower performance scores by managers.
Given biased training data, we are often unable to de-bias it because we do not know which samples are affected. This paper asks, can we certify (prove) that our predictions are robust under a given form and degree of bias in the training data? We aim to answer this question without having to show which data are biased (i.e., poisoned). Techniques for certifying poisoning robustness (i) focus on speciﬁc poisoning forms, e.g., label-ﬂipping [31], or (ii) perform certiﬁcation using defenses that create complex, uninterpretable classiﬁers, e.g., due to randomization or ensembling [23, 24, 31].
To address limitation (i), we present programmable bias deﬁnitions that model nuanced biases in practical domains. To address (ii), we target existing decision-tree learners—considered interpretable and desirable for sensitive decision-making [32]—and exactly certify their robustness, i.e., provide proofs that the bias in the data will not affect the outcome of the trained model on a given point.
We begin by presenting a language for programmatically deﬁning bias models. A bias model allows us to ﬂexibly specify what sort of bias we suspect to be in the data, e.g., up to n% of the women may have wrongly received a negative job evaluation. Our bias-model language is generic, allowing
∗ Author’s name in native alphabet: (cid:250)(cid:10) (cid:17)(cid:71)(cid:241) (cid:9)(cid:171)(cid:81)(cid:30)(cid:46)(cid:203)(cid:64) (cid:128)(cid:240) (cid:13) (cid:64) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A simple, hypothetical running example us to compose simpler bias models into more complex ones, e.g., up to n% of the women may have wrongly received a negative evaluation and up to m% of Black men’s records may have been completely missed. The choice of bias model depends on the provenance of the data and the task.
After specifying a bias model, our goal is to certify pointwise robustness to data bias: Given an input x, we want to ensure that no matter whether the training data is biased or not, the resulting model’s prediction for x remains the same. Certifying pointwise robustness is challenging. One can train a model for every perturbation (as per a bias model) of a dataset and make sure they all agree. But this is generally not feasible, because the set of possible perturbations can be large or inﬁnite. Recall the bias model where up to n% of women may have wrongly received a negative label. For a dataset with 1000 women and n = 1%, there are more than 1023 possible perturbed datasets.
To perform bias-robustness certiﬁcation on decision-tree learners, we employ abstract interpreta-tion [12] to symbolically run the decision-tree-learning algorithm on a large or inﬁnite set of datasets simultaneously, thus learning a set of possible decision trees, represented compactly. The crux of our approach is a technique that lifts operations of decision-tree learning to symbolically operate over a set of datasets deﬁned using our bias-model language. As a starting point, we build upon Drews et al.’s [16] demonstration of poisoning-robustness certiﬁcation for the simple bias model where an adversary may have added fake training data. Our approach completely reworks and extends their technique to target the bias-robustness problem and handle complex bias models, including ones that may result in an inﬁnite number of datasets.
Contributions. We make three contributions: (1) We formalize the bias-robustness-certiﬁcation problem and present a language to compositionally deﬁne bias models. (2) We present a symbolic technique that performs decision-tree learning on a set of datasets deﬁned by a bias model, allowing us to perform certiﬁcation. (3) We evaluate our approach on a number of bias models and datasets from the fairness literature. Our tool can certify pointwise robustness for a variety of bias models; we also show that some datasets have unequal robustness-certiﬁcation rates across demographics groups.
Running example. Consider the example in Fig. 1; our goal is to classify who should be hired based on a test score. A standard decision-tree-learning algorithm would choose the split (predicate) score (cid:54) 3, assuming we restrict tree depth to 1.2 As shown in Fig. 1 (middle), the classiﬁcation depends on the data split; e.g., on the right hand side, we see that a person with score > 3 is accepted, because the proportion (“probability”) of the data with positive labels and score > 3 is 4/5 (> 1/2).
Now suppose that our bias model says that up to one Black person in the dataset may have received a wrongful rejection. Our goal is to show that even if that is the case, the prediction of a new test sample x will not change. As described above, training decision trees for all possible modiﬁed datasets is generally intractable. Instead, we symbolically learn a set of possible decision trees compactly, as illustrated in Fig. 1 (right). In this case the learning algorithm always chooses score (cid:54) 3 (generally, our algorithm can capture all viable splits). However, the proportion of labels on either branch varies.
For example, on the right, if the highlighted sample is wrongly labeled, then the ratio changes from 0.8 to 1. To efﬁciently perform this calculation, we lift the learning algorithm’s operations to interval arithmetic and represent the probability as [0.8, 1]. Given a new test sample x = (cid:104)race=Black, score=7(cid:105), we follow the right branch and, since the interval is always larger than 0.5, we certify that 2Other predicates, e.g., score (cid:54) 4, will yield the same split. We choose a single split for illustrative purposes here. (The implementation considers all possible splits that yield distinct partitions, so it would consider score (cid:54) 3 and score (cid:54) 4 as a single entity.) 2
the algorithm is robust for x. In general, however, due to the use of abstraction, our approach may fail to ﬁnd tight intervals, and therefore be unable to certify robustness for all robust inputs. 2