Abstract
Simulated DAG models may exhibit properties that, perhaps inadvertently, render their structure identiﬁable and unexpectedly affect structure learning algorithms.
Here, we show that marginal variance tends to increase along the causal order for generically sampled additive noise models. We introduce varsortability as a measure of the agreement between the order of increasing marginal variance and the causal order. For commonly sampled graphs and model parameters, we show that the remarkable performance of some continuous structure learning algorithms can be explained by high varsortability and matched by a simple baseline method.
Yet, this performance may not transfer to real-world data where varsortability may be moderate or dependent on the choice of measurement scales. On standardized data, the same algorithms fail to identify the ground-truth DAG or its Markov equivalence class. While standardization removes the pattern in marginal variance, we show that data generating processes that incur high varsortability also leave a distinct covariance pattern that may be exploited even after standardization. Our
ﬁndings challenge the signiﬁcance of generic benchmarks with independently drawn parameters. The code is available at https://github.com/Scriddie/
Varsortability. 1

Introduction
Causal structure learning aims to infer a causal model from data. Academic disciplines anywhere from biology, medicine, ﬁnance, to machine learning are interested in causal models [Rothman et al., 2008, Imbens and Rubin, 2015, Sanford and Moosa, 2012, Schölkopf, 2019]. Causal models not only describe the observational joint distribution of variables but also formalize predictions under inter-ventions and counterfactuals [Spirtes et al., 2000, Pearl, 2009, Peters et al., 2017]. Directed acyclic graphs (DAGs) are common to represent causal structure: nodes represent variables and directed edges point from cause to effect representing the causal relationships. This graphical representation rests on assumptions which have been critically questioned, for example by Dawid [2010]. Inferring causal structure from observational data is difﬁcult: Often we can only identify the DAG up to its
Markov equivalence class (MEC) and ﬁnding high-scoring DAGs is NP-hard [Chickering, 1996,
Chickering et al., 2004]. Here, we focus on learning the DAG of linear additive noise models (ANM).
Data scale and marginal variance may carry information about the data generating process. This information can dominate benchmarking results, such as, for example, the outcome of the NeurIPS
Causality 4 Climate competition [Runge et al., 2020]. Here, the magnitude of regression coefﬁcients was informative about the existence of causal links such that ordinary regression-based methods on raw data outperformed causal discovery algorithms [Weichwald et al., 2020]. Multiple prior works 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
state the importance of data scale for structure learning either implicitly or explicitly. Structure identiﬁcation by ICA-LiNGAM [Shimizu et al., 2006], for example, is susceptible to rescaling of the variables. This motivated the development of DirectLiNGAM [Shimizu et al., 2011], a scale-invariant causal discovery algorithm for linear non-Gaussian models. The causal structure of ANMs is proven to be identiﬁable given the noise scale (cf. Section 2.2). Yet, such identiﬁability results require knowledge about the ground-truth data scale.
Simulated DAGs may be identiﬁable from marginal variances under generic parameter distributions.
An instructive example is the causal graph A → B with structural equations A = NA and B = wA + NB with w (cid:54)= 0 and independent zero-centered noise variables NA, NB. The mean squared error (MSE) of a model X → Y is given by MSE (X → Y ) = Var(X) + Var(Y |X). It holds that
MSE (A → B) < MSE (B → A) ⇐⇒ Var(A) < Var(B) ⇐⇒ (1−w2) Var(NA) < Var(NB) (see Appendix A). Deciding the directionality of the edge between A and B based on the MSE amounts to inferring an edge from the lower-variance variable to the higher-variance variable. For error variances Var(NA) ≤ Var(NB) and any non-zero edge weight w, the MSE-based inference is correct. This resembles known scale-based identiﬁability results based on equal or monotonically increasing error variances [Peters and Bühlmann, 2014, Park, 2020]. However, if the observations of A were multiplied by a sufﬁciently large constant, the MSE-based inference would wrongfully conclude that A ← B. This is problematic since simply choosing our units of measurement differently may change the scale and variance of A. Arguably, this is often the case for observations from real-world systems: There is no canonical choice as to whether we should pick meters or yards for distances, gram or kilogram for weights, or yuan or dollar as currency. A researcher cannot rely on obtaining the same results for different measurement scales or after re-scaling the data when applying any method that leverages the data scale (examples include Peters and Bühlmann [2014], Park [2020], or Zheng et al. [2018], who employ the least squares loss studied by Loh and Bühlmann [2014]).
Continuous causal structure learning algorithms optimize model ﬁt under a differentiable acyclicity constraint [Zheng et al., 2018]. This allows for the use of continuous optimization and avoids the explicit combinatorial traversal of possible causal structures. This idea has found numerous applications and extensions [Lachapelle et al., 2019, Lee et al., 2019, Ng et al., 2020, Yu et al., 2019, Brouillard et al., 2020, Pamﬁl et al., 2020, Wei et al., 2020, Zheng et al., 2020, Bhattacharya et al., 2021]; Vowels et al. [2021] provide a review. NOTEARS [Zheng et al., 2018] uses the MSE with reference to Loh and Bühlmann [2014], while GOLEM [Ng et al., 2020] assesses model ﬁt by the penalized likelihood assuming a jointly Gaussian model. On simulated data and across noise distributions, both methods recover graphs that are remarkably close to the ground-truth causal graph in structural intervention distance (SID) and structural hamming distance (SHD). We agree with the original authors that these empirical ﬁndings, especially under model misspeciﬁcation and given the non-convex loss landscape, may seem surprising at ﬁrst. Here, we investigate the performance under data standardization and explain how the causal order is (partially) identiﬁable from the raw data scale alone in common generically simulated benchmarking data.
Contribution. We show that causal structure drives the marginal variances of nodes in an ANM and can lead to (partial) identiﬁability. The pattern in marginal variances is dominant in ANM benchmark simulations with edge coefﬁcients drawn identically and independently. We introduce varsortability as a measure of the information the data scale carries about the causal structure. We argue that high varsortability affects the optimization procedures of continuous structure learning algorithms. Our experiments demonstrate that varsortability dominates the optimization and helps achieve state-of-the-art performance provided the ground-truth data scale. Data standardization or an unknown data scale remove this information and the same algorithms fail to recover the ground-truth DAG. Even methods using a score-equivalent likelihood criterion (GOLEM) recover neither ground-truth DAG nor its MEC on standardized data. To illustrate that recent benchmark results depend heavily on high varsortability, we provide a simple baseline method that exploits increasing marginal variances to achieve state-of-the-art results on these benchmarks. We thereby provide an explanation for the unexpected performance of recent continuous structure learning algorithms in identifying the true
DAG. Neither algorithm dominates on raw or standardized observations of the analyzed real-world data. We show how, even if data is standardized and even in non-linear ANMs, a causal discovery benchmark may be gamed due to covariance patterns. Consequently, recent benchmark results may not transfer to (real-world) settings where the correct data scale is unknown or where edge weights are not drawn independent and identically distributed (iid). We conclude that structure learning 2
benchmarks on ANMs with generically sampled parameters may be distorted due to unexpected and perhaps unintended regularity patterns in the data. 2