Abstract
Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions
Neural Networks (CNNs). Nonetheless, surprisingly, we ﬁnd these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks.
In this paper, we aim to provide the ﬁrst fair & in-depth comparisons between
Transformers and CNNs, focusing on robustness evaluations.
With our uniﬁed training setup, we ﬁrst challenge the previous belief that
Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we ﬁnd CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers’ training recipes.
While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling
Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely beneﬁted by the Transformer’s self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs. 1

Introduction
Convolutional Neural Networks (CNNs) have been the widely-used architecture for visual recognition in recent years [22, 38, 40, 16, 21]. It is commonly believed the key to such success is the usage of the convolutional operation, as it introduces several useful inductive biases (e.g., translation equivalence) to models for beneﬁting object recognition. Interestingly, recent works alternatively suggest that it is also possible to build successful recognition models without convolutions [34, 60, 3]. The most representative work in this direction is Vision Transformer (ViT) [12], which applies the pure self-attention-based architecture to sequences of images patches and attains competitive performance on the challenging ImageNet classiﬁcation task [35] compared to CNNs. Later works [26, 47] further expand Transformers with compelling performance on other visual benchmarks, including COCO detection and instance segmentation [23], ADE20K semantic segmentation [61].
The dominion of CNNs on visual recognition is further challenged by the recent ﬁndings that
Transformers appear to be much more robust than CNNs. For example, Shao et al. [37] observe that the usage of convolutions may introduce a negative effect on models’ adversarial robustness, while migrating to Transformer-like architectures (e.g., the Conv-Transformer hybrid model or the pure Transformer) can help secure models’ adversarial robustness. Similarly, Bhojanapalli et al. [4] report that, if pre-trained on sufﬁciently large datasets, Transformers exhibit considerably stronger robustness than CNNs on a spectrum of out-of-distribution tests (e.g., common image corruptions
[17], texture-shape cue conﬂicting stimuli [13]). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Though both [4] and [37] claim that Transformers are preferable to CNNs in terms of robustness, we ﬁnd that such conclusion cannot be strongly drawn based on their existing experiments. Firstly,
Transformers and CNNs are not compared at the same model scale, e.g., a small CNN, ResNet-50 (∼25 million parameters), by default is compared to a much larger Transformer, ViT-B (∼86 million parameters), for these robustness evaluations. Secondly, the training frameworks applied to
Transformers and CNNs are distinct from each other (e.g., training datasets, number of epochs, and augmentation strategies are all different), while little efforts are devoted on ablating the corresponding effects. In a nutshell, due to these inconsistent and unfair experiment settings, it remains an open question whether Transformers are truly more robust than CNNs.
To answer it, in this paper, we aim to provide the ﬁrst benchmark to fairly compare Transformers to CNNs in robustness evaluations. We particularly focus on the comparisons between Small
Data-efﬁcient image Transformer (DeiT-S) [43] and ResNet-50 [16], as they have similar model capacity (i.e., ∼22 million parameters vs. ∼25 million parameters) and achieve similar performance on
ImageNet (i.e., 76.8% top-1 accuracy vs. 76.9% top-1 accuracy1). Our evaluation suite accesses model robustness in two ways: 1) adversarial robustness, where the attackers can actively and aggressively manipulate inputs to approximate the worst-case scenario; 2) generalization on out-of-distribution samples, including common image corruptions (ImageNet-C [17]), texture-shape cue conﬂicting stimuli (Stylized-ImageNet [13]) and natural adversarial examples (ImageNet-A [19]).
With this uniﬁed training setup, we present a completely different picture from previous ones [37, 4].
Regarding adversarial robustness, we ﬁnd that Transformers actually are no more robust than CNNs— if CNNs are allowed to properly adopt Transformers’ training recipes, then these two types of models will attain similar robustness on defending against both perturbation-based adversarial attacks and patch-based adversarial attacks. While for generalization on out-of-distribution samples, we ﬁnd
Transformers can still substantially outperform CNNs even without the needs of pre-training on sufﬁciently large (external) datasets. Additionally, our ablations show that adopting Transformer’s self-attention-like architecture is the key for achieving strong robustness on these out-of-distribution samples, while tuning other training setups will only yield subtle effects here. We hope this work can serve as a useful benchmark for future explorations on robustness, using different network architectures, like CNNs, Transformers, and beyond [42, 24]. 2