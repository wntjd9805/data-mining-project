Abstract
We consider a sequential setting in which a single dataset of individuals is used to perform adaptively-chosen analyses, while ensuring that the differential privacy loss of each participant does not exceed a pre-speciﬁed privacy budget. The standard approach to this problem relies on bounding a worst-case estimate of the privacy loss over all individuals and all possible values of their data, for every single analysis. Yet, in many scenarios this approach is overly conservative, especially for “typical” data points which incur little privacy loss by participation in most of the analyses. In this work, we give a method for tighter privacy loss accounting based on the value of a personalized privacy loss estimate for each individual in each analysis. To implement the accounting method we design a ﬁlter for Rényi differential privacy. A ﬁlter is a tool that ensures that the privacy parameter of a composed sequence of algorithms with adaptively-chosen privacy parameters does not exceed a pre-speciﬁed budget. Our ﬁlter is simpler and tighter than the known
ﬁlter for ((cid:15), δ)-differential privacy by Rogers et al. [29]. We apply our results to the analysis of noisy gradient descent and show that personalized accounting can be practical, easy to implement, and can only make the privacy-utility tradeoff tighter. 1

Introduction
Understanding how privacy of an individual degrades as the number of analyses using their data grows is of paramount importance in privacy-preserving data analysis. This allows individuals to participate in multiple disjoint statistical analyses, all the while knowing that their privacy cannot be compromised by aggregating the resulting reports. Furthermore, this feature is crucial for privacy-preserving algorithm design—instead of having to reason about the privacy properties of a complex algorithm, it allows reasoning about the privacy of the subroutines that make up the ﬁnal algorithm.
For differential privacy (DP) [13], this accounting of privacy losses is typically done using composition theorems. Importantly, given that statistical analyses often rely on the outputs of previous analyses, and that algorithmic subroutines feed into one another, the composition theorems need to be adaptive, namely, allow the choice of which algorithm to run next to depend on the outputs of all previous computations. For example, in gradient descent, the computation of the gradient depends on the value of the current iterate, which itself is the output of the previous steps of the algorithm.
Given the central role of adaptive composition theorems in differential privacy, they have been investigated in numerous works (e.g. [16, 23, 11, 27, 26, 4, 29, 8, 30]). While they differ in some aspects, they also share one limitation. Namely, all of these theorems reason about the worst-case privacy loss for each constituent algorithm in the composition. Here, “worst-case” refers to the worst choice of individual in the dataset and worst choice of value for their data. This pessimistic accounting implies that every algorithm is summarized via a single privacy parameter, shared among all participants in the analysis.
In most scenarios, however, different individuals have different effects on each of the algorithms, as measured by differential privacy. More precisely, the output of an analysis may have little to no 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
dependence on the presence of some individuals. For example, if we wish to report the average income in a neighborhood, removing an individual whose income is close to the average has virtually no impact on the ﬁnal report after noise addition. Similarly, when training a machine learning model via gradient descent, the norm of the gradient given by a data point is often much smaller than the maximum norm (typically determined by a clipping operation). As a result, in many cases no single individual is likely to have the worst-case effect on all the steps of the analysis. This means that accounting based on existing composition theorems may be unnecessarily conservative.
In this work, we present a tighter analysis of privacy loss composition by computing the associated divergences at an individual level. In particular, to achieve a pre-speciﬁed privacy budget, we keep track of a personalized estimate of the privacy loss divergence for each individual in the analyzed dataset, and ensure that the respective estimate is maintained under the budget for all individuals throughout the composition. We do so by applying each analysis only to the points that are estimated to have sufﬁcient leftover privacy budget. 1.1 Overview of main results
It is feasible to measure the worst-case effect of a ﬁxed data point on a given analysis in terms of DP.
One can simply replace the supremum over all datasets in the standard deﬁnition of (removal) DP with the supremum over datasets that include that speciﬁc data point (see Deﬁnition 2.5). However, a meaningful application of adaptive composition with such a deﬁnition immediately runs into a technical challenge: standard composition theorems require that the privacy parameter of each step be ﬁxed in advance. For individual privacy, this approach requires using the worst-case value of the individual privacy loss over all the possible analyses at a given step. Individual privacy parameters are much more sensitive to the choice of analysis than worst-case parameters, and thus maximizing over all analyses is likely to negate the beneﬁts of using individual losses in the ﬁrst place.
Thus the main technical challenge in analyzing composition of individual privacy losses is that they are themselves random variables that depend on the outputs of all previous computations. If we denote by a1, . . . , at the output of the ﬁrst t adaptively composed algorithms A1, . . . , At, then the individual privacy loss of any point incurred by applying algorithm At+1 is a function of a1, . . . , at. Hence, to tackle the problem of composing individual privacy losses we need to understand composition with adaptively-chosen privacy parameters. We refer to this kind of composition as fully adaptive.
The setting of fully adaptive privacy composition is rather subtle and even deﬁning privacy in terms of the adaptively-chosen privacy parameters requires some care. This setting was ﬁrst studied by Rogers et al. [29], who introduced the notion of a privacy ﬁlter. Informally, a privacy ﬁlter is a stopping time rule that halts a computation based on the adaptive sequence of privacy parameters and ensures that a pre-speciﬁed privacy budget is not exceeded. Rogers et al. deﬁne a ﬁlter for approximate DP that asymptotically behaves like the advanced composition theorem [16], but is substantially more involved and loses a constant factor. Moreover, several of the tighter analyses of Gaussian noise addition require composition to be done in Rényi differential privacy (RDP) [1, 26].
Our main result can be seen as a privacy ﬁlter for RDP which justiﬁes stopping the analyses based on the sum of privacy parameters so far even under fully adaptive composition.
Theorem 1.1. Fix B ≥ 0, α ≥ 1. Assume At is (α, ρt)-RDP, where ρt is any function of a1, . . . , at−1.
If (cid:80)k t=1 ρt ≤ B holds almost surely, then the adaptive composition of A1, . . . , Ak is (α, B)-RDP.
When ρ1, . . . , ρk are ﬁxed, Theorem 1.1 recovers the usual composition result for RDP [26]. Our
RDP ﬁlter immediately implies a simple ﬁlter for approximate DP that is as tight as any version of the advanced composition theorem obtained via concentrated DP [4]. These Rényi-divergence-based analyses are known to improve on the classical rate [16] and, in particular, improve on the rate in [29].
We instantiate our general result for fully adaptive composition in the setting of individual privacy accounting. This allows us to deﬁne an individual privacy ﬁlter, which, given a ﬁxed privacy budget, adaptively drops points from the analysis once their personalized privacy loss estimate exceeds the budget. Therefore, instead of keeping track of a single running privacy loss estimate for all individuals, we track a less conservative, personalized estimate for each individual in the dataset.
Individual privacy ﬁltering allows for better, adaptive utilization of data points for a given budget. It can also naturally be applied to accounting in the local DP model, whereby each user stops responding once their local implementation of the ﬁlter indicates that their personal privacy budget is exhausted. 2
Individual privacy parameters are particularly easy to compute for linear queries, as well as their high-dimensional generalizations. We show that our technique gives an algorithm for answering a sequence of adaptively-chosen linear queries that are sparse across time, meaning that, for any user, the number of queries that are non-zero on that user’s data is small. Such queries arise, for example, when a platform counts the number of users that participate in certain activities (the type of activity being adaptive to the data collected in the previous days) and users generally participate in a small number of activities. Formally, a special case of our result implies the following theorem.
Theorem 1.2. There exists an algorithm A that, given a dataset S = (X1, . . . , Xn) ∈ X n, spar-sity parameter s and privacy level κ, for any adaptively-chosen sequence of queries q1, . . . , qk of arbitrary length k, where qi : X → {0, 1}, provides a sequence of answers a1, . . . , ak such that: (1) A is (α, ακ)-RDP for all α ≥ 1; (2) for all t and any δ ∈ (0, 1), the probability that qt(Xi)| > (cid:112)s log(1/δ)/κ is at most δ, where St = (Xi ∈ S : (cid:80)t
|at − (cid:80) j=1 qj(Xi) ≤ s).
Xi∈St
We note that the provided answers are guaranteed to be accurate only as long as the queries are truly sparse, meaning (cid:80)t j=1 qj(Xi) ≤ s for (almost) all i ∈ [n]. This follows because the queries are accurate on the set St, hence St needs to be similar to S for the queries to be accurate on S. The privacy guarantee, on the other hand, holds for any sequence of queries of any length k. We describe a more general version of this result in Section 4.2. A natural application of our general theorem is the setting of high-dimensional linear queries generated by gradient descent. We apply our theory to the analysis of private gradient descent [1], and show—both theoretically and empirically—that individual accounting can be easy to implement and can only make the resulting privacy-utility tradeoff tighter. Independently, without any individual accounting, in our empirical evaluations we also observe that private batch gradient descent, when tuned appropriately, outperforms private stochastic gradient descent in terms of the privacy-utility tradeoff. While we make this observation only on MNIST, we believe this phenomenon holds more generally and is worth further investigation. 1.2