Abstract
Inspired by BatchNorm, there has been an explosion of normalization layers in deep learning. Recent works have identiﬁed a multitude of beneﬁcial properties in BatchNorm to explain its success. However, given the pursuit of alternative normalization layers, these properties need to be generalized so that any given layer’s success/failure can be accurately predicted. In this work, we take a ﬁrst step towards this goal by extending known properties of BatchNorm in randomly initialized deep neural networks (DNNs) to several recently proposed normalization layers. Our primary ﬁndings follow: (i) similar to BatchNorm, activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric techniques require explicit remedies; (ii) use of GroupNorm can ensure an informative forward propagation, with different samples being assigned dissimi-lar activations, but increasing group size results in increasingly indistinguishable activations for different samples, explaining slow convergence speed in models with LayerNorm; and (iii) small group sizes result in large gradient norm in earlier layers, hence explaining training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis reveals a uniﬁed set of mechanisms that underpin the success of normalization methods in deep learning, providing us with a compass to systematically explore the vast design space of DNN normalization layers. 1

Introduction
Normalization techniques are often necessary to effectively train deep neural networks (DNNs) [1, 2, 3]. Arguably, the most popular of these is BatchNorm [1], whose success can be attributed to several beneﬁcial properties that allow it to stabilize a DNN’s training dynamics: for example, ability to propagate informative activation patterns in deeper layers [4, 5]; reduced dependence on initializa-tion [6, 7, 8]; faster convergence via removal of outlier eigenvalues [9, 10]; auto-tuning of learning rates [11], equivalent to modern adaptive optimizers [12]; and smoothing of loss landscape [13, 14].
However, depending on the application scenario, BatchNorm’s use can be of limited beneﬁt or even a hindrance: for example, BatchNorm struggles when training with small batch-sizes [3, 15]; in settings with train-test distribution shifts, BatchNorm can undermine a model’s accuracy [16, 17]; in meta-learning, it can lead to transductive inference [18]; and in adversarial training, it can hamper accuracy on both clean and adversarial examples by estimating incorrect statistics [19, 20].
To either address speciﬁc shortcomings or to replace BatchNorm in general, several recent works propose alternative normalization layers (interchangeably called normalizers in this paper). For example, Brock et al. [23] propose to match BatchNorm’s forward propagation behavior in Residual
Email: {eslubana, dickrp}@umich.edu, and hidenori.tanaka@ntt-research.com
*Work partially performed during an internship at Physics & Informatics Laboratories, NTT Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Non-Residual CNN with 10 layers (b) Non-Residual CNN with 20 layers (c) ResNet-56 (without SkipInit [6]) (d) ResNet-56 (with SkipInit [6])
Figure 1: Each normalization method has its own success and failure modes. We plot training curves (3 seeds) for different combinations of normalizer (see Table 1), network architecture, and batch-size at largest stable initial learning rate on CIFAR-100. Learning rate is scaled linearly with batch-size [21]. Layers for which loss reaches inﬁnity are not plotted. Test curves and several other settings are provided in the appendix. The plots show that all methods, including BatchNorm (BN), have their respective success and failure modes: e.g., LayerNorm (LN) [2] often converges slowly and Instance Normalization (IN) [22] can have unstable training with large depth or small batch-sizes. networks [24] by replacing it with Scaled Weight Standardization [25, 26]. Wu and He [3] design
GroupNorm, a batch-independent method that groups multiple channels in a layer to perform normalization. Liu et al. [27] use an evolutionary algorithm to search for both normalizers and activation layers. Given the right training conﬁguration, these works show their proposed normalizers often achieve similar test accuracy to BatchNorm and even outperform it on some benchmarks. This begs the question, are we ready to replace BatchNorm? To probe this question, we plot training curves for models deﬁned using different combinations of normalizer, network architecture, batch size, and learning rate on CIFAR-100. As shown in Figure 1, clear trends begin to emerge. For example, we see LayerNorm [2] often converges at a relatively slower speed; Weight Normalization [28] cannot be trained at all for ResNets (with and without SkipInit [6]); Instance Normalization [22] results in unstable training in deeper non-residual networks, especially with small batch-sizes. Overall, evaluating hundreds of models in different settings, we see evident success/failure modes exist for all normalization techniques, including BatchNorm.
As we noted before, prior works have established several properties to help explain such suc-cess/failure modes for the speciﬁc case of BatchNorm. However, given the pursuit of alternative normalizers in recent works, these properties need to be generalized so that one can accurately determine how normalization techniques beyond BatchNorm affect DNN training. In this work, we take a ﬁrst step towards this goal by extending known properties of BatchNorm at initialization to several alternative normalization techniques. As we show, these properties are highly predictive of a normalizer’s inﬂuence on DNN training and can help ascertain exactly when an alternative technique is capable of serving as a replacement for BatchNorm. Our contributions follow.
• Stable Forward Propagation: In Section 3, we show activations-based normalizers are provably able to prevent exploding variance of activations in ResNets, similar to BatchNorm [5, 6]. Paramet-ric normalizers like Weight Normalization [28] do not share this property; however, we explain why architectural modiﬁcations proposed in recent works [6, 7] can resolve this limitation.
• Informative Forward Propagation: In Section 4, we ﬁrst show the ability of a normalizer to generate dissimilar activations for different inputs is a strong predictor of optimization speed. We then extend a known result for BatchNorm to demonstrate the rank of representations in the deepest layer of a Group-normalized [3] model is at least Ω((cid:112)width/Group Size). This helps us illustrate how use of GroupNorm can prevent high similarity of activations for different inputs if the group size is small, i.e., the number of groups is large. This suggests Instance Normalization [22] (viz., 2
GroupNorm with group size equal to 1) is most likely and LayerNorm [2] (viz., GroupNorm with group size equal to layer width) is least likely to produce informative activations.
• Stable Backward Propagation: In Section 5, we show normalization techniques that rely on individual sample and/or channel statistics (e.g., Instance Normalization [22]) suffer from an exacerbated case of gradient explosion [29], often witnessing unstable backward propagation. We show this behavior is mitigated by grouping of channels in GroupNorm, thus demonstrating a speed–stability trade-off characterized by group size.