Abstract
Understanding 3D scenes from a single image is fundamental to a wide variety of tasks, such as for robotics, motion planning, or augmented reality. Existing works in 3D perception from a single RGB image tend to focus on geometric reconstruction only, or geometric reconstruction with semantic segmentation or instance segmentation. Inspired by 2D panoptic segmentation, we propose to unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation into the task of panoptic 3D scene reconstruction – from a single
RGB image, predicting the complete geometric reconstruction of the scene in the camera frustum of the image, along with semantic and instance segmentations. We thus propose a new approach for holistic 3D scene understanding from a single
RGB image which learns to lift and propagate 2D features from an input image to a 3D volumetric scene representation. We demonstrate that this holistic view of joint scene reconstruction, semantic, and instance segmentation is beneﬁcial over treating the tasks independently, thus outperforming alternative approaches. 1

Introduction 3D scene understanding from a single RGB image is fundamental to many downstream applications, such as for robotics, motion planning, or augmented reality. From a photograph of a scene, one can infer the underlying geometric structures and identify object and structural semantics, even from such a partial, single view observation. Such an understanding is also fundamental towards enabling higher-level interactions with environments, as estimating unseen geometry and semantics without having to observe all parts of an object or scene is paramount for tasks such as robotic interactions, augmented reality, or content creation.
Recently, signiﬁcant advances have been made towards complete geometric reconstruction from an RGB image input [10, 37], as well as reconstruction of object instances detected in the image
[15, 28, 32]; however, 3D semantic reconstruction and instance reconstruction have largely been considered independently. We thus aim to unify the tasks of geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation from a single image, and inspired by the 2D panoptic segmentation task [27], we call this task panoptic 3D scene reconstruction. Inspired by the uniﬁcation of the 2D semantic segmentation and instance segmentation tasks to panoptic segmentation in [27], we similarly adopt a joint semantic segmentation of “stuff” and “things,” where “stuff” elements do not have any distinct instances (e.g., structural elements such as walls and ﬂoors) and “things” denote semantic objects with instance ids deﬁning the distinct object instances. Thus, for panoptic 3D reconstruction, we aim to predict the surface geometry of the scene from an image, including any missing regions from occlusion, and for each point on the predicted surface geometry, it must be assigned a semantic label and an instance id. To evaluate a predicted panoptic reconstruction, we then evaluate both the segmentation quality and recognition quality of the 3D object instance geometry and geometry corresponding to structural semantic labels. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: We propose the task of Panoptic 3D Scene Reconstruction from a single RGB image. From an RGB image input, we simultaneously predict geometric reconstruction, semantic labels and object instances, and combine these to form a panoptic reconstruction output.
We believe that this task not only brings together geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation, but also introduces new algorithmic challenges, and hope that the panoptic 3D scene reconstruction task leads to additional insights in holistic 3D perception.
To address the panoptic 3D scene reconstruction task, we propose a new method to jointly reconstruct and segment the observed 3D scene from a RGB image. We ﬁrst extract features from the image with a 2D convolutional backbone which predicts both depth estimates and 2D instance segmentation. We use the depth estimates to guide the feature lifting to 3D by back-projecting the learned features into a 3D volume of the camera frustum, and develop an instance propagation approach to effectively leverage instance predictions in 2D as seeds for propagation to 3D geometric instances, providing effective object recognition. In contrast to prior approaches for 3D reconstruction from 2D images, we develop a propagation-based approach for object recognition, explicitly bringing 2D predictions to 3D for a reﬁnement rather than direct 3D prediction on a coarse resolution than the 2D signal. We train the object recognition jointly with a reconstruction loss on the predicted scene geometry as well as semantic “stuff” labels, resulting in a holistic 3D semantic scene estimate for an RGB image.
In summary, our contributions are:
• We introduce the task of panoptic 3D scene reconstruction, which aims for holistic 3D scene understanding by jointly predicting scene geometry, semantic labels, and instance ids.
• We propose a new approach for panoptic 3D scene reconstruction by learning to lift 2D features to 3D while propagating learned instance information to 3D object understanding for a robust joint prediction of scene geometry, semantics, and object recognition1.
• Our method signiﬁcantly outperforms alternative approaches which treat reconstruction and semantics separately and/or infer only a subset of the outputs. 2