Abstract
Existing work in language grounding typically study single environments. How do we build uniﬁed models that apply across multiple environments? We propose the multi-environment Symbolic Interactive Language Grounding benchmark (SILG), which uniﬁes a collection of diverse grounded language learning environments under a common interface. SILG consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM,
Messenger, NetHack), as well as symbolic counterparts of visual worlds that re-quire interpreting rich natural language with respect to complex scenes (ALFWorld,
Touchdown). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language speciﬁcation, and plan com-plexity. In addition, we propose the ﬁrst shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG.
Our shared architecture achieves comparable performance to environment-speciﬁc architectures. Moreover, we ﬁnd that many recent modelling advances do not result in signiﬁcant gains on environments other than the one they were designed for. This highlights the need for a multi-environment benchmark. Finally, the best models signiﬁcantly underperform humans on SILG, which suggests ample room for future work. We hope SILG enables the community to quickly identify new methodolo-gies for language grounding that generalize to a diverse set of environments and their associated challenges. 1

Introduction
An ideal language-conditioned agent should interpret language in diverse environments with vary-ing observation space, action space, language, and plan complexity. However, existing language-grounding literature typically focuses on single environments, and proposes methodological con-tributions speciﬁc to those environments [35, 51]. In order to determine which contributions are environment-speciﬁc and which apply across multiple environments, it is critical to develop universal models that can be easily evaluated in many different settings.
To facilitate this research, we present the multi-environment Symbolic Interactive Language Ground-ing Benchmark (SILG). We focus on symbolic environments with semantic symbols instead of raw visual observations for efﬁciency, interpretability, and emphasis on abstractions over perception.
SILG consists of diverse environments including grid-worlds RTFM [58], Messenger [22], and
NetHack [34], which require generalization to new dynamics (i.e. how entities behave), entity refer-ences, and partially observed worlds. SILG also contains symbolic counterparts of visual grounding
Corresponding author Victor Zhong vzhong@cs.washington.edu 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Agent
World obs, text obs, relative pos, reward
Action
RTFM
Messenger
NetHack
ALFWorld
Touchdown
SILG conversion
Dynamics, goal,  inventory
Enemy, goal,  messenge
In-game  message, player  stats
Goal, in-game  message
Navigation  instruction
Figure 1: Environments included in SILG. The world observations and text ﬁelds are shown for each environment. Detailed examples are in Appendix F. environments ALFRED [48] and Touchdown [9] , which require interpreting rich natural language in complex scenes. For the former, we use its textual variant ALFWorld [49]. For the latter, we create
SymTD by applying object segmentation to Touchdown panoramas. Despite signiﬁcant implementa-tion differences, we unify these environments under a common interface in SILG, so that one can easily develop and evaluate language grounded RL methods across all of these challenges.
SILG environments present a variety of unique grounding challenges in the richness of the observation space, action space, language speciﬁcation, and plan complexity. We quantify these challenges and additionally analyze the success rate and lengths of expert playthroughs. For visual grounding environments, we show symbolic variants (ALFWorld and SymTD) facilitate faster learning and result in policies that transfer to their visual counterparts. While a uniﬁed model may not outperform specialized models engineered for speciﬁc environments, it can be helpful to understand whether particular modelling innovations are environment speciﬁc or more general techniques. Furthermore, while the challenges in each environment are very different, we want to encourage the development of uniﬁed architectures and approaches that can scale across many language grounding tasks.
In addition to SILG, we propose the Symbolic Interactive Reader (SIR), the ﬁrst shared model archi-tecture for these environments. We combine SIR with several recent advances in language-conditioned
RL, including FiLM2 [58], egocentric local convolution [27], recurrent state-tracking [34], entity-centric attention [22], and large pretrained LMs [28]. On most environments, SIR achieves compara-ble performance to methods designed speciﬁcally for single environments. In addition, we ﬁnd that many recent advances do not result in signiﬁcant gains on environments other than the one they were designed for. This highlights the need for a multi-environment benchmark. Finally, the best models signiﬁcantly underperform humans on SILG (10-85% depending on environment), which suggests ample room for modelling improvements that generalize across environments.
In summary, we (1) combine ﬁve language-grounding environments under the same interface to evaluate language grounded RL methods across diverse grounding challenges, (2) present the ﬁrst shared model architecture for these environments, and (3) analyze recent modelling contributions across these environments. We hope SILG enables the community to quickly identify new models and learning algorithms that generalize to a diverse set of environments and their associated challenges.
The code for SILG is available at https://github.com/vzhong/silg. 2 SILG Environments
SILG contains ﬁve language-grounding environments including both grid-worlds (RTFM, Messenger,
SILGNetHack) and symbolic counterparts of 3D-visual worlds (ALFWorld, SymTD). While all in-volve agents situated in interactive worlds, each presents unique challenges in richness of observation space, action space, language speciﬁcation, and plan complexity. Table 1 quantiﬁes their theoretical complexity along these dimensions as well as empirical complexity using expert playthroughs.1 1For each environment, an expert plays as many episodes as necessary to learn about the game. We then record the playthroughs to compute the empirical win rate and trajectory length. More details in Appendix F. 2
Table 1: SILG statistics. “dynamics” are high level rules dictating behaviour of entities. “Ref hops” are number of intra-text references the agent must resolve to determine correct course of action. Messenger and SymTD text are human-written instead of procedurally generated. Distinctive properties are bold.
Action space
State space
RTFM 5 ﬁxed 6 6 grid 5 entities
⇥
Mean text len 31 words
Vocab size 262 words
Messenger
SILGNetHack
ALFWorld
SymTD 5 ﬁxed 23 ﬁxed 50+ choices 1-5 choices 10 grid 10
⇥ 14 entities 30 words 595 words 79 21 partial obs
⇥ 102 nodes 191 entities 29.6k complex panoramas 9 words 100 words 90 words 100 words 1237 words 4999 words
⇠
Generalization new dynamics new dynamics new layouts
Ref hops 6 hops
Human win % 100% 3 hops 100% 1 hop 78.1%
Human # steps 6.0 steps 2.2 steps 34.4 steps new instr new layouts 4 hops
⇠ 100% new instr 100% +layouts new instr 7 hops
⇠ 61.5% 7.8 steps new instr 9.6 steps +layouts 33.6 steps
Env FPS 240 1627 439 7
Key challenge multi-step reasoning adversarial generalization partial obs large action space 779 complex language
The goal of SILG is to provide a simple-to-use benchmark that allows researchers to quickly evaluate methods across all of these environments as well as their respective challenges. We thus combine these environments under a uniﬁed interface built on top of OpenAI Gym [7]. In each environment instance, the agent observes text inputs as well as world observations. For grid worlds such as
RTFM, Messenger, and SILGNetHack, the agent receives a 2-D bird’s-eye-view symbolic grid as observations. For visually inspired environments such as ALFWorld and SymTD, the agent receives a symbolic egocentric view of the present scene. Figure 2 shows how SILG environments are rendered to players via the play utility. In the rest of this section, we describe each SILG environment in detail.
Appendix B shows how to use SILG in Python. Appendix G shows licensing for SILG environments.
Selection criteria We select interactive environments that span the challenges presented in Table 1, are easily converted to symbolic representations, and avoid the use of additional simulators (e.g. Mat-terport3D [1]). While visual perception is clearly important for language grounding [19], we focus on the unique challenges of symbolic environments such as multi-hop reasoning and generalization to rich sets of procedurally generated dynamics. We leave the challenge of developing a visually rich multi-environment grounding benchmark to future work. Due to the lack of gold trajectories in many of the selected environments, we do not support imitation learning (IL) in this version of SILG.
RTFM RTFM [58] is a grid-world environment where an agent interprets text to acquire the correct items to ﬁght the correct monsters. A key challenge in RTFM is multi-modal multi-step reasoning (at least 6 steps) combining world observations with texts associated with multiple entities. Given a team to beat, the agent must identify which monster is on the team, then identify the item descriptor that would beat the monster descriptor. Finally, the agent must acquire the item with the correct descriptor and engage the correct monster to win. RTFM evaluation is on games with unseen rules, forcing agents to make novel reasoning steps to generalize successfully. At each step, the agent receives a symbolic grid containing names of entities present, as well as texts indicating the high level rules, the agent inventory, and the goal of the particular game instance. We include all 4 RTFM curriculum stages, but only show results for the ﬁrst stage in this preliminary study.
Messenger Messenger [22], is a grid environment where the agent must acquire a message and deliver it to the goal while avoiding an enemy after extracting entity-role assignments from a text manual. A key challenge in Messenger is the adversarial train-evaluation split without prior entity-text grounding. There is no overlap in entity-role assignments between training and evaluation, forcing agents to make compositional entity-role generalizations. At each step, the agent receives a symbolic grid containing symbol IDs of entities present, as well as texts indicating roles of each entity. The 3
Figure 2: The SILG play utility (shown here for SymTD) enables human playthrough as well as visualizing what input the model observes. Because all environments are symbolic, the playutility works in console (e.g. via ssh, tmux) without need for X-forwarding. entities are referred to in text by many names, which have no lexical overlap with their symbol ID.
That is, the text “dog” in the text for example is the non-textual symbol 2 in the observation and the association between entities and references must be learned via interaction. We include all 3
Messenger curriculum stages, but only show results for the ﬁrst stage in this preliminary study.
SILGNetHack NetHack is a a complex rogue-like game from the NetHack learning environ-ment [34]. In SILGNethack, we combine 3 tasks (Score, Gold, and Scout) and specify the task to complete for each episode via a text prompt. SILGNetHack is challenging due to its large state space and partial observability. The agent may descend multiple ﬂoors and sections of each ﬂoor may be obscured until exploration by the agent. Because of the different score distributions of each task, we mark a trajectory as successful if it exceeds a task-speciﬁc score threshold determined from human playthroughs. We evaluate agents on previously unseen map layouts that are procedurally generated with new seeds disjoint from the ones used during training. More information about the SILG multi-task SILGNetHack is in Appendix D. At each step, the agent receives a symbolic grid containing symbol IDs of entities present, as well texts denoting the goal, agent stats, and feedback from the environment after the agent’s last action. SILGNetHack vocabulary is technically inﬁnite because players can arbitrarily name things, however in our expert playthroughs of SILG SILGNetHack, we observe just over 100 unique words. Human experts win just under 80% of games with an average of 34 steps, which demonstrates the challenge of SILGNetHack. All failures can be attributed to hitting the step limit before acquiring the necessary win conditions.
In ALFWorld, an agent navigates and manipulates objects inside a
ALFWORLD (text ALFRED) 3D kitchen [49]. Its large text action space, with more than 50 valid actions (given by the game engine) for most scenes is a key challenge. Unlike its visual counterpart ALFRED [48] where the agent observes 3-D images of the kitchen, in ALFWorld the agent must rely on language descriptions of the kitchen. Goals are provided in human written language (e.g. put a clean sponge on the metal rack).
The language in ALFWORLD is not complex, but are 100 words on average due to a large number of items in a single scene. Following recent work [49], we evaluate on both unseen instructions (new instr) and unseen room layouts (new layouts). At each step, the agent receives the goal text and a list of items present in the room (e.g. “cup 1”, “bottle 2”). We concatenate the names of these items into a symbolic world observation grid , each entry containing the name of one item. The agent then selects from plausible commands given what is present in the scene.
In Touchdown, the agent navigates through Google Street
SILGTouchdown (SymTD, VisTD)
View panoramas according to long compositional instructions that tests spatial reasoning [9, 37, 38].
A key challenge is the rich human-written navigation instructions that describe photorealistic images.
Touchdown’s long human-written instructions contain many intra-text reference hops, which we 4
RL Policy
<latexit sha1_base64="C0rp3hus6McOKChN4H1Uz0BpM7c=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOgE3KBMwFkiXMTs4mY2Znl5lZIYSUVjYWitj6FKl8CDufwZdwcik08YeBj/8/hznn+DFnSjvOl5VaWV1b30hvZra2d3b3svsHNRUlkmKVRjySDZ8o5ExgVTPNsRFLJKHPse73byZ5/R6lYpG41YMYvZB0BQsYJdpYlVI7m3PyzlT2MrhzyF1/jCvfD8fjcjv72epENAlRaMqJUk3XibU3JFIzynGUaSUKY0L7pItNg4KEqLzhdNCRfWqcjh1E0jyh7an7u2NIQqUGoW8qQ6J7ajGbmP9lzUQHBW/IRJxoFHT2UZBwW0f2ZGu7wyRSzQcGCJXMzGrTHpGEanObjDmCu7jyMtTO8+5l/qLi5IoFmCkNR3ACZ+DCFRShBGWoAgWER3iGF+vOerJerbdZacqa9xzCH1nvP2StkRM=</latexit>H
FiLM2 network
RL baseline estimate
<latexit sha1_base64="C0rp3hus6McOKChN4H1Uz0BpM7c=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOgE3KBMwFkiXMTs4mY2Znl5lZIYSUVjYWitj6FKl8CDufwZdwcik08YeBj/8/hznn+DFnSjvOl5VaWV1b30hvZra2d3b3svsHNRUlkmKVRjySDZ8o5ExgVTPNsRFLJKHPse73byZ5/R6lYpG41YMYvZB0BQsYJdpYlVI7m3PyzlT2MrhzyF1/jCvfD8fjcjv72epENAlRaMqJUk3XibU3JFIzynGUaSUKY0L7pItNg4KEqLzhdNCRfWqcjh1E0jyh7an7u2NIQqUGoW8qQ6J7ajGbmP9lzUQHBW/IRJxoFHT2UZBwW0f2ZGu7wyRSzQcGCJXMzGrTHpGEanObjDmCu7jyMtTO8+5l/qLi5IoFmCkNR3ACZ+DCFRShBGWoAgWER3iGF+vOerJerbdZacqa9xzCH1nvP2StkRM=</latexit>H
Concat
Concat
<latexit sha1_base64="+sxOtlbjS9ymjgAkV74K4RaTZuo=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOgBaWCZgLJEuYnZwkY2Znl5lZISwprWwsFLH1KVL5EHY+gy/h5FJo4g8DH/9/DnPO8SPOlHacLyu1tLyyupZez2xsbm3vZHf3qiqMJcUKDXko6z5RyJnAimaaYz2SSAKfY83vX43z2j1KxUJxqwcRegHpCtZhlGhjla9b2ZyTdyayF8GdQe7yY1T+fjgclVrZz2Y7pHGAQlNOlGq4TqS9hEjNKMdhphkrjAjtky42DAoSoPKSyaBD+9g4bbsTSvOEtifu746EBEoNAt9UBkT31Hw2Nv/LGrHuFLyEiSjWKOj0o07MbR3a463tNpNINR8YIFQyM6tNe0QSqs1tMuYI7vzKi1A9zbvn+bOykysWYKo0HMARnIALF1CEGyhBBSggPMIzvFh31pP1ar1NS1PWrGcf/sh6/wFenZEP</latexit>D
Encode text
<latexit sha1_base64="rVYsicB5ptqjANHzlzp1hKz3wbs=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOiI1lAuYCyRJmJyfJmNnZZWZWCEtKKxsLRWx9ilQ+hJ3P4Es4uRSa+MPAx/+fw5xz/IgzpR3ny0otLa+srqXXMxubW9s72d29qgpjSbFCQx7Kuk8UciawopnmWI8kksDnWPP71+O8do9SsVDc6kGEXkC6gnUYJdpY5atWNufknYnsRXBnkLv8GJW/Hw5HpVb2s9kOaRyg0JQTpRquE2kvIVIzynGYacYKI0L7pIsNg4IEqLxkMujQPjZO2+6E0jyh7Yn7uyMhgVKDwDeVAdE9NZ+Nzf+yRqw7BS9hIoo1Cjr9qBNzW4f2eGu7zSRSzQcGCJXMzGrTHpGEanObjDmCO7/yIlRP8+55/qzs5IoFmCoNB3AEJ+DCBRThBkpQAQoIj/AML9ad9WS9Wm/T0pQ169mHP7LefwBaEZEM</latexit>
A
Joint text summary  conditioned on text ﬁeld i-1
<latexit sha1_base64="v2U0Jq1ddDlFaAT3n2Tqy7lWGUE=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOQBrLBMwFkiXMTs4mY2Znl5lZIYSUVjYWitj6FKl8CDufwZdwcik08YeBj/8/hznn+DFnSjvOl5VaWV1b30hvZra2d3b3svsHNRUlkmKVRjySDZ8o5ExgVTPNsRFLJKHPse73S5O8fo9SsUjc6kGMXki6ggWMEm2sSqmdzTl5Zyp7Gdw55K4/xpXvh+NxuZ39bHUimoQoNOVEqabrxNobEqkZ5TjKtBKFMaF90sWmQUFCVN5wOujIPjVOxw4iaZ7Q9tT93TEkoVKD0DeVIdE9tZhNzP+yZqKDgjdkIk40Cjr7KEi4rSN7srXdYRKp5gMDhEpmZrVpj0hCtblNxhzBXVx5GWrnefcyf1FxcsUCzJSGIziBM3DhCopwA2WoAgWER3iGF+vOerJerbdZacqa9xzCH1nvP10ZkQ4=</latexit>C
Summary of text ﬁeld i
Attention and weighted averaging
<latexit sha1_base64="wU9m1NAoa+XZFKpBp9DCfb1xZrc=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOgI1lAuaCyRJmJyfJmNnZZWZWCEtKKxsLRWx9ilQ+hJ3P4Es4uRSa+MPAx/+fw5xz/IgzpR3ny0otLa+srqXXMxubW9s72d29qgpjSbFCQx7Kuk8UciawopnmWI8kksDnWPP7V+O8do9SsVDc6EGEXkC6gnUYJdpY5dtWNufknYnsRXBnkLv8GJW/Hw5HpVb2s9kOaRyg0JQTpRquE2kvIVIzynGYacYKI0L7pIsNg4IEqLxkMujQPjZO2+6E0jyh7Yn7uyMhgVKDwDeVAdE9NZ+Nzf+yRqw7BS9hIoo1Cjr9qBNzW4f2eGu7zSRSzQcGCJXMzGrTHpGEanObjDmCO7/yIlRP8+55/qzs5IoFmCoNB3AEJ+DCBRThGkpQAQoIj/AML9ad9WS9Wm/T0pQ169mHP7LefwB/9ZEl</latexit>Z
<latexit sha1_base64="bGDh9QInq8KIY6ywnusFm1ck65c=">AAAB6HicbZC7SgNBFIbPxluMt6ilIItBsAq7IprOgI1lAm4SSEKYnZxNxszOLjOzQlhSWtlYKGLrU6TyIex8Bl/CyaXQ6A8DH/9/DnPO8WPOlHacTyuztLyyupZdz21sbm3v5Hf3aipKJEWPRjySDZ8o5Eygp5nm2IglktDnWPcHV5O8fodSsUjc6GGM7ZD0BAsYJdpYVa+TLzhFZyr7L7hzKFy+j6tf94fjSif/0epGNAlRaMqJUk3XiXU7JVIzynGUayUKY0IHpIdNg4KEqNrpdNCRfWycrh1E0jyh7an7syMloVLD0DeVIdF9tZhNzP+yZqKDUjtlIk40Cjr7KEi4rSN7srXdZRKp5kMDhEpmZrVpn0hCtblNzhzBXVz5L9ROi+558azqFMolmCkLB3AEJ+DCBZThGirgAQWEB3iCZ+vWerRerNdZacaa9+zDL1lv33hhkSA=</latexit>U
Position emb
World obs emb
<latexit sha1_base64="y8mxjEmAK28oweU0KuQ/CeEJ/GM=">AAAB6HicbZC7SgNBFIbPxluMt6ilIINBsAq7IprOgI1lArlBsoTZydlkzOyFmVkhhJRWNhaK2PoUqXwIO5/Bl3ByKTTxh4GP/z+HOed4seBK2/aXlVpZXVvfSG9mtrZ3dvey+wc1FSWSYZVFIpINjyoUPMSq5lpgI5ZIA09g3evfTPL6PUrFo7CiBzG6Ae2G3OeMamOVK+1szs7bU5FlcOaQu/4Yl78fjseldvaz1YlYEmComaBKNR071u6QSs2ZwFGmlSiMKevTLjYNhjRA5Q6ng47IqXE6xI+keaEmU/d3x5AGSg0Cz1QGVPfUYjYx/8uaifYL7pCHcaIxZLOP/EQQHZHJ1qTDJTItBgYok9zMSliPSsq0uU3GHMFZXHkZaud55zJ/UbZzxQLMlIYjOIEzcOAKinALJagCA4RHeIYX6856sl6tt1lpypr3HMIfWe8/dt2RHw==</latexit>T
Joint text
Encode text
…
Text ﬁeld i-1
<latexit sha1_base64="zu8svhJaNbaXIuIiadmGZpVN9kM=">AAAB6nicbVDLSgNBEOz1GVejUY9eBkPAU9gV0RwDgniSiOYByRJmJ7PJkJnZZWZWCEs+wYsHRbyKH+InePNvnDwOmljQUFR1090VJpxp43nfzsrq2vrGZm7L3d7J7+4V9g8aOk4VoXUS81i1QqwpZ5LWDTOcthJFsQg5bYbDy4nffKBKs1jem1FCA4H7kkWMYGOlu5su6xaKXtmbAi0Tf06K1fxnWrpyP2rdwlenF5NUUGkIx1q3fS8xQYaVYYTTsdtJNU0wGeI+bVsqsaA6yKanjlHJKj0UxcqWNGiq/p7IsNB6JELbKbAZ6EVvIv7ntVMTVYKMySQ1VJLZoijlyMRo8jfqMUWJ4SNLMFHM3orIACtMjE3HtSH4iy8vk8Zp2T8vn93aNCowQw6O4BhOwIcLqMI11KAOBPrwCM/w4nDnyXl13matK8585hD+wHn/AaXpkEg=</latexit>Ni
Encode text
<latexit sha1_base64="lSnQ7AOdJeGJ4+JLRRytX+VBIqk=">AAAB6nicbVDJSgNBEK1xjaPRqEcvjSHgKcyIaI4BQTzGJQskQ+jp9CRNenqGXoQw5BO8eFDEq/ghfoI3/8bOctDEBwWP96qoqhemnCnted/Oyura+sZmbsvd3snv7hX2DxoqMZLQOkl4IlshVpQzQeuaaU5bqaQ4DjlthsPLid98oFKxRNzrUUqDGPcFixjB2kp3t13WLRS9sjcFWib+nBSr+U9TunI/at3CV6eXEBNToQnHSrV9L9VBhqVmhNOx2zGKppgMcZ+2LRU4pirIpqeOUckqPRQl0pbQaKr+nshwrNQoDm1njPVALXoT8T+vbXRUCTImUqOpILNFkeFIJ2jyN+oxSYnmI0swkczeisgAS0y0Tce1IfiLLy+TxmnZPy+f3dg0KjBDDo7gGE7AhwuowjXUoA4E+vAIz/DicOfJeXXeZq0rznzmEP7Aef8BrAGQTA==</latexit>Ri
Text ﬁeld i
…
Figure 3: The Symbolic Interactive Reader (SIR) baseline. Inputs are green, intermediate results white, outputs red, and model components yellow. Details about the FiLM2 layer is in Appendix C. approximate as the number of sentences plus the number of sequential connectors such as “then”. We convert Touchdown to a symbolic environment by segmentating its panoramas into semantic grids. In each step, the agent observes the instruction text and a grid of discretized segmentation class IDs corresponding to the current panorama. It then chooses among a list of radial directions to proceed to the next panorama. The agent wins if it passes the goal location. We use the same train-test split as the original Touchdown environment, which features unseen navigation texts.
We show that our symbolic Touchdown (SymTD) facilitates faster learning compared to learning in its visual equivalent (VisTD). Human performance demonstrates some limitations of SymTD, with an expert win rate just over 60%. This may be due to the symbolic representations removing information referenced by the instructions such as color, or because the segmented features are visually disparate from real-world views [17]. We also include manual stop variants of SymTD and VisTD, which are functionally equivalent to the original Touchdown. Appendix E details these variants, SymTD/VisTD creation as well as discussions on human performance. Compared to prior work on Touchdown and
ALFWorld, we train using RL without supervised trajectories as opposed to imitation learning. 3 The Symbolic Interactive Reader Baseline Model
Figure 3 shows the SIR baseline for the SILG benchmark. To the best of our knowledge, this is the ﬁrst shared model architecture for RTFM, Messenger, NetHack, ALFWorld, and Touchdown. Consider an agent situated in an arbitrary SILG environment. At each time step t, the model receives from the environment the following inputs (precise inputs for each environments are shown in Appendix F).
• World observations X w
⇥
⇥ k where h and w are the height and width of the observa-tion and each element corresponds to the k-word symbol ID(s) of its content.
Rh 2
• Joint text T 2
• Text ﬁelds R
Rl of l tokens of the text to attend over.
Rn
⇥ 2 as agent inventory or environment feedback. m is the max token count of these texts. m where the ith row contains the ith of n environment text ﬁeld such
• Relative position Z
Rh relative to the player agent in the x and y directions. 2
⇥
⇥ w 2 cell-wise feature that denotes the position of each cell
As a policy learner, the model must output a distribution Y over the action space. We additionally output a baseline estimate of the value function to stabilize policy learning [18]. Let d and r denote embedding and bidirectional LSTM sizes. We ﬁrst sum embeddings for each cell in the world d. Next, we encode the
Rh observation to obtain world representation U = sum (emb (X)) ith text ﬁeld Ri and the joint text T using an bidirectional LSTMs [30]. 2
⇥
⇥ w
Ni = BiLSTMN (emb (Ri))
D = BiLSTMD (emb (T )) m r
⇥ 2
R l r
R
⇥ 2 (1) (2)
We then compute weighted average over text ﬁelds ˜Ci and attention ˜Ai over the joint text. 5
(3) (4)
˜Ci = weightavei (Ni) =
˜Ai = attend
D, ˜Ci
= j
X softmax (lineari (Ni))j Nij 2 r
R softmax
D ˜Ci
Dj 2 j r
R j
X
⌘ r again to support any number of text ﬁelds.
⇣
⌘
We compress ˜C
Rn
⇥ 2
C = weightaveC r and ˜A
˜C
⇣ 2 2
⇥
Rn r
R (5)
A = weightaveA r
R (6)
˜A
⌘ 2
⇣
⌘
We now have representations for world observations U , text ﬁelds C, and joint text conditioned on text
ﬁelds A. We apply successive FiLM2 layers to build multiple levels of codependent representations between texts and world observations to model multiple cross-modal reasoning steps [58]. To support arbitrary number of text ﬁelds, we modify the text input of the ith FiLM2 layer to be the concatenation of the text ﬁelds C, attention over joint text conditioned on text ﬁelds A, and attention over joint text conditioned on the visual summary of the last FiLM2 layer s(i 1).
⇣
 
V (i), s(i) = FiLM2
V (i
  1); Z
,
C, A, attend
D, s(i
  1) (7)
⇣h i h
⇣
⌘i⌘
We use the deﬁnition of FiLM2(visuals, texts) from Zhong et al. [58] and summarize its intuition and computation in Appendix C. We deﬁne V (1) and s(1) to be the initial world observation U and its spacial max-pooling. Finally, we use a multi-layer perceptron to build a ﬁxed-size codependent repre-sentation of the inputs based on the last FiLM2 layer’s output H = tanh
, which is used to compute the baseline estimate of the value function B = MLPB (H) and the policy
  
Y (H) expressed as a distribution over actions. While the core architecture of SIR is identical for all environments, a different policy module Y is necessary for different types of action spaces.
ﬂatten(V (last) linear4
 
 
Fixed sized action space (RTFM, Messenger, SILGNetHack) We simply apply a multilayer perceptron to the ﬁnal representation Y = MLPY (H).
Multiple-choice text action space (ALFWorld) Let Qj denote tokens for the jth choice (e.g. pick up the mug), which we encode a bidrectional LSTM Gj = BiLSTMG (emb (Qj)). We then attend over this text using the ﬁnal representation H to score for jth choice Yj = linear4 (attend (Gj, H)).
Multiple-choice navigation action space (SILGTouchdown) Let j denote the index of the world representation corresponding to a movement direction. For example, for a world observation width of 100, the index corresponding to advancing in the 30 degrees direction is 30 8. We encode the navigation choice by selecting its corresponding world observation representation, then scoring it via dot product with the ﬁnal output representation Yj = linear5 (Uj)| H. 100
⇤ 360 ⇡ 4 Experiments
Setup How well does a shared architecture do across all ﬁve SILG environments? To answer this, we train and evaluate SIR using Torchbeast [33], a distributed RL framework with importance weighted actor-learners based on IMPALA [18]. For each environment (separately), we train on training, do early stop on validation, and evaluate on test. NetHack does not distinguish between train and evaluation, hence we create our own splits by dividing the seed range (ﬁrst 1 million seeds for training, second for validation, and third for test). We run 5 random seeds for each environment. The hyperparameter and compute resources are respectively shown in Appendix H and I. SILG.
Results Figures 4 through 8 show learning curves for each environment. Table 2 shows the test performance for the baseline model and the best model variant. Despite sharing the same core model architecture, SIR achieves reasonable performance across all environments except Messenger, where it overﬁts due to lack of pretrained LM and entity-centric attention. Nevertheless, the best performing model signiﬁcantly trails human performance, indicating room for further improvement. 4.1 Analyses of recent grounded language RL modelling contributions
Next, we use SILG to evaluate recent modelling advances for language grounding across environments by adding them to the SIR baseline. These modelling enhancements were proposed for (and resulted in key gains on) the environments included in SILG. Namely, we analyze the effectiveness of recurrent state-tracking, entity-centric local convolution, entity-centric attention, and pretrained LMs. 6
Table 2: Success rate on test environments for SIR and its best variant. Standard deviation are in brackets. We early stop on validation and evaluate best checkpoint on test. For RTFM, Messenger, and SILGNetHack, we evaluate 100 episodes. For ALFWorld and Touchdown, we evaluate on initial states from each test episode. The variant with best performance across envs is +state. The
SOTA for RTFM, Messenger, and ALFWorld are respectively from Zhong et al. [58], Hanjie et al.
[22], and Shridhar et al. [49] (std was not reported in ALFWorld). MSOTA for ALFWorld relies on supervised trajectories and beam search, which SIR does not use. There are no previous results for multitask SILGNetHack and SymTD as they are introduced here. Though not comparable, the manual stop VisTD SOTA trained using imitation learning on supervised trajectories is 16.7% [56].
Model
RTFM
Messenger
SILGNetHack
ALFWorld
SymTD new inst new inst+layouts
Base
Best 88.8 (22.4)
+state 99.2 (0.7)
SOTA 83 (21)
Human 100 0 (0)
+all 31 (2.6) 85 (1.4) 100 23.8 (0.8)
+local conv 25.4 (3.3) 21.0 (1.5)
+state 23.6 (2.8) 16.0 (2.1)
+state 16.6 (2.9)
N/A 78.1 40M 100 37M 100 9.7 (1.3)
+state 14.9 (1.8)
N/A 61.5
Figure 4: RTFM performance. Left: train envs, right: validation envs.
Recurrent state tracking (state) As in Küttler et al. [34], we augment the SIR baseline with a state-tracking LSTM by replacing the ﬁnal H with H 0 = H + LSTM (H, St 1 is the previous LSTM state (summing LSTM output and H outperforms replacing H with LSTM output).
State-tracking consistently improves convergence and generalization, even when the correct next step is fully determined by current world observations (e.g. RTFM). This may be because it helps prevent local minima that cause repetitive actions. The exception to this is Messenger, where state-tracking does not help generalize to the evaluation distribution. 1), where St
 
 
Entity-centric local convolution (local conv) Hill et al. [27] proposed local convolution around the agent to obtain an egocentric view of world observations. While this helps generalize in SIL-GNetHack, it does not help signiﬁcantly in other environments. One reason is that this provides redundant information as positional embeddings, which is already included in the base model and is a cheaper alternative to adding an additional egocentric convnet.
Entity-centric attention (entity attn) Hanjie et al. [22] propose replacing entity representa-tions with attention over text speciﬁcation, such that the world observations are forcibly composed using text representations. We add this by replacing world representation U with entity attention over text ﬁelds R as described in Hanjie et al. [22]. This constraint causes underﬁtting of SIR on most environments. Since the entity representation is built entirely using the text, when there is incomplete entity information or it is difﬁcult to extract the relevant information from the manual text this can be a handicap. However, for Messenger, entity-centric attention prevents overﬁtting.
Pretrained language model (bert) A natural question in language-grounding is how to leverage large, pretrained LMs [28]. We use a simple method to incorporate BERT [16] by replacing all text encoding with the summation of the original bidirectional LSTM encoding and BERT encoding. Due to the memory requirement of large pretrained LMs, we cannot ﬁne-tune the LM during training, and thus keep the LM parameters ﬁxed. Pretrained LMs (bert and all) help generalization in Messenger but does not improve performance on other environment in our experiments. For tasks such as RTFM 7
Figure 5: Messenger performance. Left: train envs, right: validation envs.
Figure 6: SILGNetHack performance. Left: train envs, right: validation envs. and SILGNetHack, our use of a general-purpose LM may not be beneﬁcial for the highly speciﬁc language used in those tasks (i.e. fantasy world with word like shaman, goblin, mage etc). We stress that this is a preliminary investigation into the use of LMs on these environments, and we encourage future research on how to effectively use pretrained LMs across environments using SILG. 4.2 Analyses of SILG environments
Finally, we examine performance of SIR and variants to analyze challenges presented by SILG.
Generalization requirement of environments SILG’s evaluation environments require different types of generalization. RTFM requires generalizing to new environment dynamics by referring between world observations and multiple texts; because SIR adopts FiLM2 from Zhong et al. [58], it is able to achieve such generalization. Messenger requires compositional entity-role generalizations.
That is, if an entity (e.g. dog) has a certain role (e.g. message holder) in training, such an entity-role assignment never appears in validation or test. SIR quickly overﬁts to to entity-role assumptions (e.g. dog as the message) in training suggesting the need for additional work on achieving this type of generalization using a joint model architecture. Combining pretrained LM with other enhancements (+all) results in generalization improvement, however the convergence remains very slow. This suggests that generalizing to new dynamics across environments without obvious lexical cues from the text remains a difﬁcult challenge. SILGNetHack and ALFWorld require generalizing to new procedurally generated scenes, which SIR achieves. In the additional out-of-domain ALFWorld evaluation where the model must generalize to new layouts, state-tracking allows the model to generalize faster. Touchdown requires generalizing to new natural language instructions. Here, the baseline suffers from a large generalization gap. We hypothesize that more effective means of incorporating pretrained LMs is necessary to achieve this type of generalization.
In concat, we concatenate text ﬁelds into a single string, which
Necessity of separate text ﬁelds we encode using a bidirectional LSTM. In this case, both joint text D and text ﬁeld representations N are set to this encoding. This degrades performance especially in RTFM, which shows that multi-hop references is more easily learned when the text ﬁelds are separated and modeled via structured attention. Note that this model variant is not shown for Touchdown because it only has one text ﬁeld.
Learning from symbolic vs visual world observations Table 3 shows that policies learned in the symbolic environment transfer to the 3-D environment. Using oracle and Masked-RCNN [24], the
ALFWorld policy can be transferred by ﬁlling observation text templates using detected objects. Our result with oracle detector is in line with Shridhar et al. [49], though our performance is weaker 8
Figure 7: ALFWorld perfomance. Left: train envs, middle: new instruction validation envs, right: new instruction+new layouts validation envs. For efﬁciency we only evaluate on a subset (50 out of 140) of the validation environments for early stopping. We do train BERT variants here due to computational constraints. ALFWorld does not have entity IDs and no agent location, hence we do not show local convolution nor entity attention experiments.
Figure 8: SymTD performance. Left: train envs, right: validation envs. Touchdown does not have entities, hence we do not show experiments for entity attention. because we do not use annotated data nor DAgger [44]. As with prior results, transfer to visual worlds with new layouts remains very challenging [49]. Transfer using Masked-RCNN results in large drop in performance, nevertheless SILG allows perception, albeit an important challenge, to be factored out so that one can focus and quickly iterate on abstraction challenges. Table 3 also shows that models trained on SymTD outperform those trained on VisTD (where U is 10-dim PCA features from a ResNet [23] panorama encoding) despite being faster (383 for SymTD vs. 344 frames per second for VisTD). That is, by applying segmentation to obtain SymTD, we are able to obtain a better policy than training directly with visual features using VisTD. The results from both ALFWorld and
SymTD show that learning in faster symbolic environments such as SILG can transfer to their visual counterparts, and allows certain perception challenges to be factored out.
Future work We ﬁnd that some of the most challenging aspects of situated interactive language grounding include (1) grounding text references to entities without lexical overlap, (2) choosing from large textual action spaces, and (3) interpreting complex natural language descriptions. On the methodology front, further work is needed to investigate how to effectively use pretrained LMs for language grounding. Moreover, apart from recurrent state tracking, the other model enhancements do not yield signiﬁcant gains on environments other than the ones they were proposed for. These results highlight the need for modelling techniques that generalize across environments.
SIR suggests that with additional improvements, it may be possible to have a performant model with the same architecture (but trained independently) across environments. Future work may explore whether (1) a single model with the same parameters can accomplish all tasks, (2) a single model with pretraining can be quickly ﬁnetuned on each task, and (3) learning in one environment is transferable to another. We believe SILG is well-suited to help answer these questions. Furthermore, SILG is designed to be easily extensible, with opportunities to add additional environments in the future. 5