Abstract
Temporal information is essential to learning effective policies with Reinforcement
Learning (RL). However, current state-of-the-art RL algorithms either assume that such information is given as part of the state space or, when learning from pixels, use the simple heuristic of frame-stacking to implicitly capture temporal information present in the image observations. This heuristic is in contrast to the current paradigm in video classiﬁcation architectures, which utilize explicit encodings of temporal information through methods such as optical ﬂow and
Inspired by two-stream architectures to achieve state-of-the-art performance. leading video classiﬁcation architectures, we introduce the Flow of Latents for
Reinforcement Learning (Flare), a network architecture for RL that explicitly encodes temporal information through latent vector differences. We show that
Flare recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information. Flare is the most sample efﬁcient model-free pixel-based RL algorithm on the DeepMind Control suite when evaluated on the 500k and 1M step benchmarks across 5 challenging control tasks, and, when used with Rainbow DQN, outperforms the competitive baseline on Atari games at 100M time step benchmark across 8 challenging games. 1

Introduction
Reinforcement learning (RL) [41] holds the promise of enabling artiﬁcial agents to solve a diverse set of tasks in uncertain and unstructured environments. Recent developments in RL with deep neural networks have led to tremendous advances in autonomous decision making. Notable examples include classical board games [36, 37], video games [29, 6, 45], and continuous control [34, 28].
There has been a large body of research on extracting high quality features during the RL process, such as with auxiliary losses [20, 27, 35] or data augmentation [25, 26]. However, another important component in RL representation learning has been largely overlooked: a more effective architecture to incorporate temporal features. This becomes especially crucial in an unstructured real-world setup like the home when compact state representations such as calibrated sensory inputs are unavailable.
Motivated by this understanding, we explore architectural improvements to better utilize temporal features for the problem of efﬁcient and effective deep RL from pixels.
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Mean and median evaluation scores on DMControl[42] and Atari[3]. Flare is an architectural modiﬁcation that improves RAD and Rainbow, the base algorithms it integrates with.
Current approaches in deep RL for learning temporal features are largely heuristic in nature. A commonly employed approach is to stack the most recent frames [29] as inputs to a convolutional neural network (CNN). This can be interpreted as a form of early fusion [24], where information from the recent time window is combined at the pixel level for input to the CNN. In contrast, modern video recognition systems use alternate architectures that employ optical ﬂow and late fusion [38], where frames are processed individually with CNN layers before fusion and downstream processing.
Late fusion is typically beneﬁcial due to better performance, fewer parameters, and the ability to use multi-modal data [8, 21]. However, it is not straightforward how to directly extend such architectures to RL. Real-time computation of optical ﬂow for action selection can be computationally infeasible for many applications with fast control loops like robotics. Furthermore, optical ﬂow computation at training time can also be prohibitively expensive. In our experiments, we also observe that a naive late fusion architecture minus the optical ﬂow yields poor results in RL settings (see Section 6.3).
This observation is consistent with recent ﬁndings in related domains like visual navigation [46].
To overcome the above challenges, we develop Flow of Latents for Reinforcement Learning (Flare), a new architecture for deep RL from pixels (Figure 4). Flare can be interpreted as a structured late fusion architecture. It processes each frame individually to compute latent vectors, similar to a standard late fusion approach. Subsequently, temporal differences between the latent feature vectors are computed and fused along with the latent vectors by concatenation for downstream processing.
By incorporating this structure of temporal difference in latent feature space, we provide the learning agent with appropriate inductive bias.
We highlight the main empirical contributions from Flare in the following2: 1. Flare recovers optimal performance in state-based RL without explicit access to the state velocity, solely with positional state information. 2. Flare achieves state-of-the-art performance compared to model-free methods on several challenging pixel-based continuous control tasks within the DeepMind control benchmark suite [42], while being the most sample efﬁcient model-free pixel-based RL algorithm across these tasks, outperforming the prior model-free state-of-the-art RAD on the 500k and 1M environment step benchmarks respectively (Figure 1). A video demonstration of Flare achieving SOTA performance on Quadruped Walk is in the supplementary materials. 3. When augmented over Rainbow DQN, Flare outperforms the baseline on 5 out of 8 chal-lenging Atari games at 100M step benchmark. Notebly, Flare scores 1668 on Montezuma’s
Revenge, a signifcant gain over the baseline Rainbow DQN’s 900. 2