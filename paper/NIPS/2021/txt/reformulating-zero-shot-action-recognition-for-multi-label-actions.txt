Abstract
The goal of zero-shot action recognition (ZSAR) is to classify action classes which were not previously seen during training. Traditionally, this is achieved by training a network to map, or regress, visual inputs to a semantic space where a nearest neighbor classiﬁer is used to select the closest target class. We argue that this approach is sub-optimal due to the use of nearest neighbor on static semantic space and is ineffective when faced with multi-label videos - where two semantically distinct co-occurring action categories cannot be predicted with high conﬁdence.
To overcome these limitations, we propose a ZSAR framework which does not rely on nearest neighbor classiﬁcation, but rather consists of a pairwise scoring function. Given a video and a set of action classes, our method predicts a set of conﬁdence scores for each class independently. This allows for the prediction of several semantically distinct classes within one video input. Our evaluations show that our method not only achieves strong performance on three single-label action classiﬁcation datasets (UCF-101, HMDB, and RareAct), but also outperforms previous ZSAR approaches on a challenging multi-label dataset (AVA) and a real-world surprise activity detection dataset (MEVA). 1

Introduction
Current image and video classiﬁcation models require large labeled training datasets but they perform really well on previously seen classes. However, if novel classes are presented to these models, as is the case in many real-world applications, they tend to fail. The system has to be retrained with additional samples for this class to correctly predict these novel classes, which requires additional training time as well as computational resources. Therefore, it would be beneﬁcial if we can train a single system on a ﬁxed dataset which can be applied to new, previously unseen classes. To this end, zero-shot learning (ZSL) approaches [1] have been proposed which leverage semantic information, e.g. textual descriptions or class names, and relate them to new unseen class categories.
In this work, we focus on zero-shot action recognition (ZSAR), where the goal is to classify videos of unseen action categories. There has been a great progress focusing on ZSAR [2, 3, 4], however, most of these existing methods utilize a similar fundamental approach. These approaches project a video representation to a ﬁxed semantic space (e.g. a text embedding space generated from a pre-trained
Word2Vec [5] model) and perform classiﬁcation using a nearest neighbor operation. We argue that such a solution is sub-optimal because the class selection is performed on a static text-based semantic space. This formulation forces classes that are more similar in the semantic space to have closer classiﬁcation boundaries, even if they are visually dissimilar; conversely, actions that are visually similar, can appear further apart in the semantic space. For example, the action "Tennis Swing" is closer in semantic space to "Swing" (i.e. a child on a swing) than "Table Tennis Shot" even though it is more visually similar to the latter action. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: C onventional nearest neighbor ZSAR methods (top) cannot predict semantically dissimilar actions "Walking" and "Talking" without actions "Running" and "Jumping" also being predicted. Our proposed pairwise scoring approach (bottom), can predict two dissimilar classes by mapping the video and semantic embeddings to a joint embedding space which can be linearly separated.
In addition, this is detrimental in the multi-label case where multiple semantically distinct actions can occur within a sample. This is illustrated in Figure 1 (top). In conventional ZSAR approaches, the video network can only generate a single representation in the semantic space so the actions
"Walking" and "Talking" can not both be predicted without many incorrect action categories also being predicted. This issue could be solved by ﬁne-tuning the textual encoder which generates the semantic space (i.e. moving the representations of "Walking" and "Talking" closer together).
However, previous approaches train the video models by regressing to the action class’s semantic feature vector. If the vector is not ﬁxed (i.e. the text encoder is ﬁne-tuned), this would collapse to a trivial solution: it can map all classes to zero vectors which are easily regressed to by the video model.
Several constraints could prevent this collapse, however such solutions to the best of our knowledge, have not been proposed in the ZSAR literature.
We propose a novel solution to this problem where a model ﬁnds agreement between the features from a pair of inputs (i.e. the video and a semantic/textual feature vector) and outputs a match vs. no-match probability score. Our method consists of a pairwise scoring function which has two main beneﬁts. First, the model can reﬁne the input text features through additional parameterized layers, without collapsing to a trivial solution. Second, the method outputs individual action probability scores used for classiﬁcation instead of relying on nearest neighbor. Figure 1 (bottom) depicts this behaviour. The Pairwise Scoring ZSAR (PS-ZSAR) approach is able to map the merged visual and semantic representations into a joint space, on which a decision boundary can be created to successfully predict both "Walking" and "Talking" without predicting irrelevant actions.
PS-ZSAR is trained end-to-end using standard classiﬁcation losses. We evaluate our proposed network on ﬁve video datasets. On three single-label action recognition datasets - UCF-101, HMDB, and RareAct - it achieves strong performance when compared to previous approaches. We report, to the best of our knowledge, the ﬁrst results for zero-shot multi-label action recognition on the AVA dataset, which we believe will be a challenging baseline for future ZSAR methods. Lastly, we use
PS-ZSAR in the NIST ActEV challenge "Surprise Activity" task which demonstrates the method’s ability to scale to difﬁcult real-world scenarios. 2