Abstract
We study the reinforcement learning problem for discounted Markov Decision
Processes (MDPs) under the tabular setting. We propose a model-based algorithm named UCBVI-γ, which is based on the optimism in the face of uncertainty principle and the Bernstein-type bonus. We show that UCBVI-γ achieves an (cid:101)O(cid:0)√
SAT /(1 − γ)1.5(cid:1) regret, where S is the number of states, A is the number of actions, γ is the discount factor and T is the number of steps. In addition, we construct a class of hard MDPs and show that for any algorithm, the expected regret is at least (cid:101)Ω(cid:0)√
SAT /(1 − γ)1.5(cid:1). Our upper bound matches the minimax lower bound up to logarithmic factors, which suggests that UCBVI-γ is nearly minimax optimal for discounted MDPs. 1

Introduction
The goal of reinforcement learning (RL) is designing algorithms to learn the optimal policy through interactions with the unknown dynamic environment. Markov decision process (MDPs) plays a central role in reinforcement learning due to their ability to describe the time-independent state transition property. More speciﬁcally, the discounted MDP is one of the standard MDPs in reinforcement learning to describe sequential tasks without interruption or restart. For discounted MDPs, with a generative model [12], several algorithms with near-optimal sample complexity have been proposed.
More speciﬁcally, Azar et al. [3] proposed an Empirical QVI algorithm which achieves the optimal sample complexity to ﬁnd the optimal value function. Sidford et al. [22] proposed a sublinear randomized value iteration algorithm that achieves a near-optimal sample complexity to ﬁnd the optimal policy, and Sidford et al. [23] further improved it to reach the optimal sample complexity.
Since generative model is a powerful oracle that allows the algorithm to query the reward function and the next state for any state-action pair (s, a), it is natural to ask whether there exist online RL algorithms (without generative model) that achieve optimality.
To measure an online RL algorithm, a widely used notion is regret, which is deﬁned as the summation of sub-optimality gaps over time steps. The regret is ﬁrstly introduced for episodic and inﬁnite-horizon average-reward MDPs and later extended to discounted MDPs by [15, 30, 35, 35]. Liu and
Su [15] proposed a double Q-learning algorithm with the UCB exploration (Double Q-learning), 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
√
SAT /(1 − γ)2.5) regret, where S is the number of states, A is the number of which enjoys (cid:101)O( actions, γ is the discount factor and T is the number of steps. While Double Q-learning enjoys a
√
T -regret, it still does not match the lower bound proved in [15] in terms of the dependence standard on S, A and 1/(1 − γ). Recently, Zhou et al. [34] proposed a UCLK+ algorithm for discounted
T /(1 − γ)1.5(cid:1) regret, where d
MDPs under the linear mixture MDP assumption and achieved (cid:101)O(cid:0)d is the dimension of the feature mapping. However, directly applying their algorithm to our setting
T /(1 − γ)1.5(cid:1) regret1, which is even worse that of double Q-learning [15] would yield an (cid:101)O(cid:0)S2A in terms of the dependence on S, A.
√
√
In this paper, we aim to close this gap by designing a practical algorithm with a nearly optimal regret.
In particular, we propose a model-based algorithm named UCBVI-γ for discounted MDPs without using the generative model. At the core of our algorithm is to use a “reﬁned” Bernstein-type bonus and the law of total variance [3, 4], which together can provide tighter upper conﬁdence bound (UCB). Our contributions are summarized as follows:
• We propose a model-based algorithm UCBVI-γ to learn the optimal value function under the discounted MDP setting. We show that the regret of UCBVI-γ in ﬁrst T steps is upper
SAT /(1 − γ)1.5). Our regret bound strictly improves the best existing regret bounded by (cid:101)O( (cid:101)O(
SAT /(1 − γ)2.5)2 in [15] by a factor of (1 − γ)−1.
√
√
• We also prove a lower bound of the regret by constructing a class of hard-to-learn discounted
MDPs, which can be regarded as a chain of the hard MDPs considered in [15]. We show that for
SAT /(1 − γ)1.5) on the any algorithm, its regret in the ﬁrst T steps can not be lower than (cid:101)Ω( constructed MDP. This lower bound also strictly improves the lower bound Ω(
SAT /(1 − γ) +
√
√
√
AT /(1 − γ)1.5) proved by [15].
• The nearly matching upper and the lower bounds together suggest that the proposed UCBVI-γ algorithm is minimax-optimal up to logarithmic factors.
We compare the regret of UCBVI-γ with previous online algorithms for learning discounted MDPs in Table 1.
Notation For any positive integer n, we denote by [n] the set {1, . . . , n}. For any two numbers a and b, we denote by a ∨ b as the shorthand for max(a, b). For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn, and we write an = Ω(bn) if there exists an absolute constant C such that an ≥ Cbn. We use (cid:101)O(·) and (cid:101)Ω(·) to further hide the logarithmic factors. 2