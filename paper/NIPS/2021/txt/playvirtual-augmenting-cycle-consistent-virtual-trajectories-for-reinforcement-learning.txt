Abstract
Learning good feature representations is important for deep reinforcement learning (RL). However, with limited experience, RL often suffers from data inefficiency for training. For un-experienced or less-experienced trajectories (i.e., state-action sequences), the lack of data limits the use of them for better feature learning.
In this work, we propose a novel method, dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to enhance the data efficiency for RL feature representation learning. Specifically, PlayVirtual predicts future states in a la-tent space based on the current state and action by a dynamics model and then predicts the previous states by a backward dynamics model, which forms a tra-jectory cycle. Based on this, we augment the actions to generate a large amount of virtual state-action trajectories. Being free of groudtruth state supervision, we enforce a trajectory to meet the cycle consistency constraint, which can signifi-cantly enhance the data efficiency. We validate the effectiveness of our designs on the Atari and DeepMind Control Suite benchmarks. Our method achieves the state-of-the-art performance on both benchmarks. Our code is available at https://github.com/microsoft/Playvirtual. 1

Introduction
Deep reinforcement learning (RL) combines the powerful representation capacity of deep neural networks and the notable advantages of RL for solving sequential decision-making problems. It has made great progress in many complex control tasks such as video games [35, 46, 3], and robotic control [22, 54, 36]. Despite the success of deep RL, it faces the challenge of data/sample inefficiency when learning from high-dimensional observations such as image pixels from limited experience
[27, 61, 29]. Fitting a high-capability feature encoder using only scarce reward signals is data inefficient and prone to suboptimal convergence [49]. Humans can learn to play Atari games in several minutes, while RL agents need millions of interactions [44]. However, collecting experience in the real world is often expensive and time-consuming. One may need several months to collect interaction data for robotic arms training [22] or be troubled by collecting sufficient patient data to train a healthcare agent [52]. Therefore, from another perspective, making efficient use of limited experience for improving data efficiency becomes vital for RL.
Many methods improve data efficiency by introducing auxiliary tasks with useful self-supervision to learn compact and informative feature representations, which better serves policy learning. Previous works have demonstrated that good auxiliary supervision can significantly improve agent learning, like leveraging image reconstruction [49], the prediction of future states [41, 13, 30, 40], maximizing
Predictive Information [38, 1, 34, 42, 31], or promoting discrimination through contrastive learning
∗This work was done when Tao Yu was an intern at Microsoft Research Asia.
†Corresponding Author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) (b) (c)
Figure 1: Illustration of the main pipeline of our method. (a) A glance at the overall framework which consists of an encoder for learning the latent state representation zt, a policy learning head, and our auxiliary task module. The auxiliary task module consumes a real trajectory as shown in (b) and an augmented virtual trajectory as shown in (c), respectively. In (b), we train the dynamics model (DM) to be predictive of the future state based on the input state and action, with the supervision from the future state. To enhance data efficiency, as shown in (c), we augment the actions to generate a virtual trajectory formed by a forward and a backward trajectory. Particularly, the forward state-action trajectory is obtained based on the current state zt, the DM and a sequence of augmented/generated actions (e.g., for K steps). Similarly, based on the predicted future state ˆzt+K, a backward dynamics model (BDM), and that sequence of augmented actions, we obtain the backward state-action trajectory.
For the virtual trajectory, we add the consistency constraint on the current state zt and the predicted current state z′ for optimizing the feature representations.
[29, 59, 32, 25]. Although the above methods have been proposed to improve the data efficiency of RL, the limited experience still hinders the achievement of high performance. For instance, the current state-of-the-art method SPR [40] only achieves about 40% of human level on Atari [2] when using data from 100k interactions with the environment. Some methods improve data efficiency by applying modest image augmentation (i.e., transformations of the input images like random shifts and intensity) [28, 50]. Such perturbation on images improves the diversity of appearances of the input images. However, it cannot enrich the experienced trajectories (state-action sequences) in training and thus the deep networks are still deficient in experiencing/ingesting flexible/diverse trajectories.
In this work, to address the above problem, we propose a new method dubbed PlayVirtual, which augments cycle-consistent virtual trajectories to improve data efficiency. Particularly, we predict future states in the latent space by a dynamics model in the forward prediction (trained using real trajectories to predict the future state based on the current state and action) and then predict the previous states by a backward dynamics model, forming a loop. In this way, we can augment the actions to generate a large amount of virtual/fictitious state-action transitions for feature representa-tion training, with the self-supervised cycle consistency constraint (which is a necessary/desirable condition for a good feature encoder). Note that such design is free of groundtruth supervision. Our augmentation to generate abundant virtual trajectories can significantly enhance the data efficiency.
As illustrated in Figure 1, on top of a baseline RL framework, we introduce our PlayVirtual for augmenting state-action virtual trajectories with cycle consistency constraint. The dynamics model infers the future states recurrently based on the current state and a set of randomly sampled actions, and the backward dynamics model predicts the previous states according to the predicted future state and those sampled/augmented actions. We enforce the backwardly predicted state of the current time step to be similar to the original current state to meet the cycle consistency constraint.
We summarize the contributions of this work as follows: 2
• We pinpoint that augmenting the experience of RL in terms of trajectories is important for feature representation learning in RL (which is a sequential decision-making problem) to enhance data efficiency. To our best knowledge, we are the first to generate virtual trajectories (experience) for boosting feature learning.
• We propose a practical method PlayVirtual for augmenting virtual trajectories under self-supervised cycle consistency constraint, which significantly improves the data efficiency.
We demonstrate the effectiveness of our PlayVirtual on discrete control benchmark Atari [2], and continuous control benchmark DMControl Suite [43], where our PlayVirtual achieves the best performance on both benchmarks. 2