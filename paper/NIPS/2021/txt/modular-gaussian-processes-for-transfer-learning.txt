Abstract
We present a framework for transfer learning based on modular variational Gaussian processes (GP). We develop a module-based method that having a dictionary of well ﬁtted GPs, one could build ensemble GP models without revisiting any data. Each model is characterised by its hyperparameters, pseudo-inputs and their corresponding posterior densities. Our method avoids undesired data centralisation, reduces rising computational costs and allows the transfer of learned uncertainty metrics after training. We exploit the augmentation of high-dimensional integral operators based on the Kullback-Leibler divergence between stochastic processes to introduce an efﬁcient lower bound under all the sparse variational GPs, with different complexity and even likelihood distribution. The method is also valid for multi-output GPs, learning correlations a posteriori between independent modules.
Extensive results illustrate the usability of our framework in large-scale and multi-task experiments, also compared with the exact inference methods in the literature. 1

Introduction
Imagine a supervised learning problem, for instance regression, where N data points are processed for training a model. At a later time, new data are observed, this time corresponding to a binary classiﬁcation task, that we know are generated by the same phenomena, e.g. using a different sensor.
Having kept the observations from regression stored, a common approach would be to use them in combination with the classiﬁcation dataset to generate a new model.
However, this practice might be inconvenient because of i) the need of centralising data to train the model, ii) the rising data-dependent computational cost as the number of samples increases and iii) the obsolescence of ﬁtted models, whose future usability is not guaranteed for the new set of observations. Looking at the deployment in large-scale scenarios, by any organization or use case, where data are ever changing, this solution becomes prohibitive. The main challenge is to incorporate unseen tasks as former models are recurrently discarded.
Figure 1: GP modules (A, B, C) are used for training (D) without revisiting data.
Alternatively, we propose a framework based on modules of Gaussian processes (GP) (Rasmussen and Williams, 2006). Considering the current example, the regression model (or module) is kept intact. Once new data arrives, one ﬁts a meta-GP using the module, but without revisiting any sample.
If no data are observed, combining multiple modules is also allowed — see Figure 1. Under this framework, a new family of module-driven GP models emerges. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).