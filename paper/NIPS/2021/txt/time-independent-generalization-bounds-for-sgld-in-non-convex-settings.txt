Abstract
We establish generalization error bounds for stochastic gradient Langevin dy-namics (SGLD) with constant learning rate under the assumptions of dissipativ-ity and smoothness, a setting that has received increased attention in the sam-pling/optimization literature. Unlike existing bounds for SGLD in non-convex settings, ours are time-independent and decay to zero as the sample size increases.
Using the framework of uniform stability, we establish time-independent bounds by exploiting the Wasserstein contraction property of the Langevin diffusion, which also allows us to circumvent the need to bound gradients using Lipschitz-like assumptions. Our analysis also supports variants of SGLD that use different discretization methods, incorporate Euclidean projections, or use non-isotropic noise. 1

Introduction
Investigating the generalization error of a learning algorithm is a fundamental problem in machine learning that has motivated the development of a rich theory connecting notions of model complexity and sensitivity to the ability of a learning algorithm to generalize well to unseen data. Recently, there has been increased interest in studying the generalization capabilities of stochastic gradient descent (SGD) and its variants, e.g. [13, 1, 6, 17, 22, 19]. Despite them being some of the most important methods of modern machine learning and statistics, their learning capabilities have not been fully explored, particularly in non-convex settings.
One variant that has attracted a lot of attention is stochastic gradient Langevin dynamics (SGLD)
[29]. With the addition of independent Gaussian noise to each iteration, SGLD combines the SGD framework with the Langevin diffusion, a stochastic process that converges to the Gibbs distribution of a given objective function. By tuning the scale of the noise applied, SGLD has been shown to work well as both a sampling scheme and a statistical learning algorithm in a variety of settings, e.g. [29, 25, 4, 14]. Using its relationship with the Langevin equation, various tools from stochastic analysis have been adopted to develop a rich theory analyzing SGLD in settings where no comparable results seem to be available for SGD, e.g. [23, 32, 5, 31]. In particular, a recent work from Raginsky et al. [23] provides non-asymptotic excess risk bounds for SGLD for a broad class of non-convex learning problems and these bounds were derived using the exponential ergodicity of Langevin diffusions. Following this work, there has been ongoing progress in the development of bounds for both optimization error and mixing times, e.g. [32, 5, 31, 7, 4]. Many of these works are motivated by recent results showing that Langevin diffusions contract in particular Wasserstein distances [8, 9].
In parallel to this literature, there has been a growing body of work that considers generalization error bounds for SGLD. Using uniform stability, Mou et al. [17] showed that under the assumption of bounded gradient updates and bounded loss functions, SGLD attains generalization bounds that grow slower with time than those that are known for SGD in identical settings [13, 6]. By bounding 35th Conference on Neural Information Processing Systems (NeurIPS 2021).            
the generalization error using the mutual information between its input and output [30], Pensia et al.
[22] established similar results while replacing the assumption of bounded loss functions with the weaker assumption that the loss function applied to the data is subgaussian. This technique has also been used to develop data-dependent generalization bounds that depend on the gradient along the trajectory of the iterates [18, 12, 17, 19].
Although the assumptions made in the aforementioned bounds cover several settings of interest, we are not aware of any convergence analysis that considers Lipschitz, bounded, or subgaussian objective functions. In general, the Gibbs distribution, which is an object that is fundamental to the design and known convergence analyses for SGLD [29, 25, 23, 5, 32], cannot be deﬁned in the case that the objective is bounded over the Euclidean space, as the distribution would not integrate to one.
Similarly, we are not aware of any generalization bounds for SGLD that use the assumptions of dissipativity and smoothness that are consistently applied in the non-convex sampling/optimization literature, e.g. [23, 5, 32].
Another commonality in existing generalization bounds for SGLD is that they grow indeﬁnitely with time. Sampling from the Gibbs distribution, the limiting behavior of the Langevin diffusion, is known to be uniformly stable in multiple settings [30, 23] and it has also been shown that the continuous-time Langevin dynamics are uniformly stable when applied to bounded Lipschitz functions with weight decay [16]. However, convergence in generalization error for SGLD seems to have only been observed in preliminary empirical evidence that uses data-dependent bounds [18, 12]. Time-independent bounds have been obtained for SGD, for example, under the assumption of strong convexity in [13] and in non-convex settings, Lei and Ying [15] obtain bounds that decrease with time but do not decay to zero as the sample size increases. 1.1 Our contributions
In this paper, we obtain expected generalization error bounds for SGLD for learning problems under dissipativity and smoothness assumptions. We analyze both the discrete-time algorithm as well as a continuous-time version and in both cases, we obtain bounds that converge with time. Taking the supremum with respect to time yields time-independent bounds that, with the appropriate scaling of the learning rate, decay to zero as the sample size increases.
At ﬁrst, we focus on the special case of Lipschitz loss functions with weight decay, which is the primary example of a dissipative objective given in Raginsky et al. [23]. For the continuous-time algorithm, we obtain a bound with rate O(n−1) and for the discrete-time algorithm, we obtain a bound with the slower rate O(n−1 + η1/2), where η is the learning rate. Then we extend the result to the full dissipative case without Lipschitz requirements. In this setting, we obtain bounds with rate
O(n−1η−1/2) for the continuous-time algorithm and with rate O(n−1η−1/2 + η1/2) in the discrete-time case. We also discuss how our method allows for the consideration of different discretization techniques and how it can be used to obtain generalization error bounds for modiﬁcations of SGLD that incorporate Euclidean projections or non-isotropic noise. From our bounds, a scheme for choosing η follows: in the Lipschitz setting, we obtain dimension-free O(n−1) bounds by setting
η ∝ n−2d−1 where d is the model dimension, and in the dissipative setting we ﬁnd that a scaling of
η ∝ n−1 leads to O(n−1/2) bounds that, in general, scale exponentially in dimension.
To derive time-independent bounds, our proof technique depends fundamentally on the two sources of noise that occur in the algorithm: the random mini-batches and the injected Gaussian noise. Using the framework of uniform stability, in the sense deﬁned in [10], we derive generalization bounds by estimating how much SGLD diverges in Wasserstein distance when an element of the data set is changed. We exploit recent results that use reﬂection couplings to show that under dissipativity-type assumptions, Langevin diffusions contract in Wasserstein distance. Though this property has been adopted extensively in the sampling/optimization literature, we are not aware of any results for generalization bounds based on this property. Using the convexity of the Wasserstein distance, we combine this with the stability induced by using stochastic gradients to obtain bounds that are time-independent.
A peculiarity of our results that arises from the methodology we use is that some of our bounds diverge as η → 0. This contrasts with the usual approach of stability-based generalization bounds based on non-expansivity that often require the learning rate to decay sufﬁciently fast or the time-horizon to be sufﬁciently small to guarantee bounds that are non-vacuous [13, 17]. Furthermore, for our bounds to 2
Paper
Assumptions
EGE Bound
Raginsky et al. [23] D, S
B, L
Mou et al. [17]
L, SG, (cid:96)2
Mou et al. [17]
L, SG
Pensia et al. [22]
O(ηt + e−ηt/c + 1/n)
O((ηt)1/2/n)
O(log(t + 1)/n)1/2
O(ηt/n)1/2
Present work
L, S, (cid:96)2
D, S
O((ηt ∧ 1)(1/n + η1/2))
O((ηt ∧ 1)(η−1/2/n + η1/2))
Key
Bounded
B
Lipschitz
L
S
Smooth (cid:96)2 Weight decay
D
SG Subgaussian
Dissipative
Table 1: Comparison of expected generalization error bounds for SGLD in recent works. converge with time, we require the mini-batch size to be less than n. While our bounds support the case of full-batch gradient descent, we ﬁnd that our bounds grow indeﬁnitely with time. 2