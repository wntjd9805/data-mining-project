Abstract
We study the problem of active pure exploration with ﬁxed conﬁdence in generic stochastic bandit environments. The goal of the learner is to answer a query about the environment with a given level of certainty while minimizing her sampling budget. For this problem, instance-speciﬁc lower bounds on the expected sample complexity reveal the optimal proportions of arm draws an Oracle algorithm would apply. These proportions solve an optimization problem whose tractability strongly depends on the structural properties of the environment, but may be instrumental in the design of efﬁcient learning algorithms. We devise Frank-Wolfe-based Sampling (FWS), a simple algorithm whose sample complexity matches the lower bounds for a wide class of pure exploration problems. The algorithm is computationally efﬁcient as, to learn and track the optimal proportion of arm draws, it relies on a single iteration of Frank-Wolfe algorithm applied to the lower-bound optimization problem. We apply FWS to various pure exploration tasks, including best arm identiﬁcation in unstructured, thresholded, linear, and Lipschitz bandits. Despite its simplicity, FWS is competitive compared to state-of-art algorithms. 1

Introduction
Pure exploration in stochastic bandits [24] refers to the task of answering a given question about the reward distributions of the different arms, using as few arm pulls (or samples) as possible. The task may correspond to identifying the best arm [13], the top-m arms [37], all (cid:15)-good arms [27], a set of arms whose expected rewards exceed a given threshold [26], etc. To reduce the sample complexity of such a task, the learner needs to leverage as much as possible the information available about reward distributions, which typically comes as known structural properties of the set of their expected rewards.
Exploiting particular structures (e.g., unimodal, Lipschitz, convex, linear) has been thoroughly studied in the regret minimization setting (see [6], and references therein), but less in the pure exploration framework, where most efforts have focused on linear structures [35, 20, 39, 36, 10, 18, 9].
In this paper, we investigate a generic learning problem proposed in [8] and covering the aforemen-tioned pure exploration tasks with or without structure. Consider K arms whose reward distribu-tions (ν1, . . . , νK) come from a one-dimensional exponential family and are of unknown means
µ = (µ1, . . . , µK). The parameter µ is known to belong to Λ ⊂ RK, the set of possible instances.
For each µ ∈ Λ, we assume that there is a unique true answer i(cid:63)(µ) that belongs to the ﬁnite set I of possible answers1 (e.g., for the best arm identiﬁcation task, i(cid:63)(µ) = arg maxk µk). We consider pure 1Scenarios with several correct answers require a more involved analysis, see [7]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
exploration tasks in the ﬁxed conﬁdence setting where the learner wishes, for any possible µ ∈ Λ, to discover i(cid:63)(µ) with a certain level of conﬁdence 1 − δ, for some δ ∈ (0, 1). The learner’s strategy is deﬁned by (i) an adaptive sampling rule dictating the sequence of arm pulls, (ii) a stopping rule deﬁning τ , the round where, based on the data gathered so far, the learner decides to stop pulling arms, and (iii) a decision rule specifying her answer. The goal is to devise a δ-PAC (it outputs the right answer with probability at least 1 − δ for any µ ∈ Λ) strategy minimizing the expected sample complexity Eµ[τ ].
Using the same arguments as those used in [13] for classical MAB problems, we may derive a lower bound of the expected sample complexity satisﬁed by any δ-PAC strategy. This lower bound, whose proof can be found in Appendix B for completeness, is given by T (cid:63)(µ)kl(δ, 1 − δ), where the characteristic time T (cid:63)(µ) is deﬁned through the following optimization problem:
T (cid:63)(µ)−1 = sup
ω∈Σ inf
λ∈Alt(µ)
K (cid:88) k=1
ωkd(µk, λk), (1) where Σ is the (K − 1)-dimensional simplex, Alt(µ) is the set of confusing parameters λ ∈ Λ such that i(cid:63)(µ) (cid:54)= i(cid:63)(λ), kl(a, b) is the KL divergence between two Bernoulli distributions of means a and b, and d(µk, λk) denotes the KL divergence of arm-k reward distributions under parameters µ and λ
. A solution ω(cid:63)(µ) of (1) can be interpreted as an optimal allocation, in the sense that pulling each arm i a proportion of round equal to ω(cid:63) i (µ) (in expectation) constitutes an optimal sampling rule.
Most existing algorithms achieving an asymptotically (when δ goes to 0) minimal sample complexity leverage a Track-and-Stop (TaS) framework [13]. In each round t, they plug ˆµ(t) the estimated expected arm rewards in the lower bound optimization problem (1), and track the allocation w(cid:63)( ˆµ(t)).
As already noticed in [28], the main drawback of the Track-and-Stop framework is that it requires a recurrent access to an Oracle able to solve (1) (actually existing analyses usually assume that the
Oracle outputs the exact solution for any µ). (1) is a concave program but can become difﬁcult to solve depending the underlying structure Λ. Indeed, for complex structures, identifying the most k=1 ωkd(µk, λk) can be hard. confusing parameters leading to the objective function inf λ∈Alt(µ)
Contributions. 1) Instead of solving (1) in each round as in the TaS framework, we propose an online iterative method to approach the optimal allocation of arm pulls. Speciﬁcally, we devise
Frank-Wolfe-based Sampling (FWS), a computationally efﬁcient algorithm that just relies, in each round, on a single iteration Frank-Wolfe (FW) algorithm applied to (1) instantiated at ˆµ(t). 2) For a wide class of pure exploration problems with or without structure, we derive an upper bound of the expected sample complexity of FWS for any certainty level δ, and show that this bound matches the lower bound T (cid:63)(µ)kl(δ, 1 − δ) asymptotically as δ goes to 0. 3) We illustrate the performance of FWS on various pure exploration problems, including best arm identiﬁcation in unstructured, linear, and Lipschitz bandits. In all tested scenarios, and despite its simplicity, FWS matches the performance of the best existing algorithms. (cid:80)K
The use of the FW algorithm has been suggested in [13] in the case of best arm identiﬁcation problem in unstructured bandits. In this case, FW iterations take a very simple and intuitive form (see Example 1 introduced in §3). The corresponding sampling rule is referred to as Best Challenger in [13], and leads to algorithms with remarkably low sample complexity empirically – sometimes lower than that of TaS algorithms solving (1) in each round. So far however, as discussed in [28], the analysis of
FW-type sampling rules, and even their convergence, have eluded researchers. Towards the design of
FWS algorithm, we devise a simple variant of the FW algorithm that yields a sampling rule whose sample complexity can be analyzed. We conﬁrm the asymptotic optimality of as well as its empirical superiority, not only for the case of best arm identiﬁcation in unstructured bandits as predicted by
[13], but also for a wide class of pure exploration problems. We believe that our analysis also brings interesting solutions to the three important obstacles we needed to tackle to devise and analyze a
FW-type sampling rule: (i) the objective function in (1) is not smooth; (ii) its curvature becomes inﬁnite in general close to the boundary of Σ; and (iii) the estimate ˆµ(t) is evolving and might be far from µ. 2