Abstract
Generative adversarial networks (GANs) typically require ample data for train-ing in order to synthesize high-ﬁdelity images. Recent studies have shown that training GANs with limited data remains formidable due to discriminator overﬁt-ting, the underlying cause that impedes the generator’s convergence. This paper introduces a novel strategy called Adaptive Pseudo Augmentation (APA) to en-courage healthy competition between the generator and the discriminator. As an alternative method to existing approaches that rely on standard data augmentations or model regularization, APA alleviates overﬁtting by employing the generator itself to augment the real data distribution with generated images, which deceives the discriminator adaptively. Extensive experiments demonstrate the effectiveness of APA in improving synthesis quality in the low-data regime. We provide a theoretical analysis to examine the convergence and rationality of our new training strategy. APA is simple and effective. It can be added seamlessly to powerful contemporary GANs, such as StyleGAN2, with negligible computational cost.
Code: https://github.com/EndlessSora/DeceiveD. 1

Introduction
While state-of-the-art GANs like StyleGAN2 [20] are constantly pushing forward the ﬁdelity and resolution of synthesized images, they usually require a large amount of training data to fully unleash their power. Training GANs with insufﬁcient data tend to generate poor-quality images, as shown in
Figure 1. Collecting sufﬁcient data samples for these GANs is sometimes infeasible, especially in domains where data are sparse and privacy-sensitive. To ease the practical deployment of powerful
GANs, it is necessary to devise new strategies for training GANs with limited data while preserving the quality of synthesis.
Recent studies have shown that the overﬁtting of the discriminator is the critical reason that impedes effective GAN training on limited data [2, 18, 50, 42], rendering severe instability of training dynamics. Speciﬁcally, when the discriminator starts to overﬁt, the distributions of its outputs for real and generated samples gradually diverge from each other [18, 42], and its feedback to the generator becomes less informative. Consequently, the generator converges to an inferior point, compromising the quality of synthesized images. Recent solutions to this problem include the use of standard data augmentations, either conventional or differentiable, to real and generated images [18, 50, 52, 41] or applying an additional model regularization term [42]. Addressing the discriminator overﬁtting is still an open problem. We are interested in ﬁnding an alternative way to the aforementioned approaches.
In this paper, we present a simple yet effective way to regularize a discriminator without introducing any external augmentations or regularization terms. We call our method Adaptive Pseudo Augmen-tation (APA). In contrast to previous standard data augmentations [18, 50, 52, 41], we exploit the
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: StyleGAN2 [20] synthesized results (no truncation) deteriorate given the limited amount of training data (256 × 256), i.e., FFHQ [19] (a subset of 5, 000 images, ∼ 7% of full data), AFHQ-Cat [8] (5, 153 images, which is small by itself), and Danbooru2019 Portraits (Anime) [4] (a subset of 5, 000 images, ∼ 2% of full data). The proposed Adaptive Pseudo Augmentation (APA) effectively ameliorates the degraded performance of StyleGAN2 on limited data. generator in a GAN itself to provide the augmentation, a more natural way to regularize the overﬁtting of the discriminator. Compared to the model regularization, our approach is more adaptive to ﬁt different settings and training status without manual tuning. Speciﬁcally, APA takes the fake/pseudo samples synthesized by the generator and moderately feeds them into the limited real data. Such pseudo data are adaptively presented to the discriminator as “real” instances. The goal of this pseudo augmentation for the real data is not to enlarge the real dataset but to suppress the discriminator’s conﬁdence in distinguishing real and fake distributions. The deceit is introduced adaptively, which is moderated by a deception probability according to the degree of overﬁtting. To quantify overﬁtting, we study a series of plausible heuristics derived from the discriminator raw output logits.
The main contribution of this work is a novel adaptive pseudo augmentation method for training
GANs with limited data. This approach deceives the discriminator adaptively and mitigates the problem of discriminator overﬁtting. The proposed APA can be readily added to existing GAN training with negligible computational cost. We conduct extensive experiments to demonstrate the effectiveness of APA for state-of-the-art GAN training with limited data. The results are comparable or even better than other types of solutions [18, 42]. APA is also complementary to existing methods based on standard data augmentations for gaining a further performance boost. Besides, we theoretically connect APA with minimizing the JS divergence [23] between the smoothed data distribution and generated distribution, proving its convergence and rationality. We hope that our approach could extend the breadth and potential of solutions to GAN training with limited data. 2