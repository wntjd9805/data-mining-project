Abstract
We consider the problem of preference bandits in the contextual setting. At each round, the learner is presented with a context set of K items, chosen randomly from a potentially inﬁnite set of arms D ⊆ Rd. However, unlike classical contextual bandits, our framework only allows the learner to receive feedback in terms of item preferences: At each round, the learner is allowed to play a subset of size q (any q ∈ {2, . . . , K}) upon which only a (noisy) winner of the subset is revealed. Yet, same as the classical setup, the goal is still to compete against the best context arm at each round. The problem is relevant in various online decision-making scenarios, including recommender systems, information retrieval, tournament ranking–typically any application where it’s easier to elicit the items’ relative strength instead of their absolute scores. To the best of our knowledge, this work is the ﬁrst to consider preference-based stochastic contextual bandits for potentially inﬁnite decision spaces. We start with presenting two algorithms for the special case of pairwise preferences (q = 2): The ﬁrst algorithm is simple and easy to implement with an ˜O(d
T ) regret guarantee, while the second algorithm is shown to achieve the optimal ˜O( dT ) matching lower bound analysis. We then proceed to analyze the problem for any general q-subsetwise preferences (q ≥ 2), where surprisingly, our lower bound proves the fundamental performance limit to be Ω( dT ) yet again, independent of the subsetsize q. Following this, we propose a matching upper bound algorithm justifying the tightness of our results. This implies having access to subsetwise preferences does not help in faster information aggregation for our feedback model.
All the results are corroborated empirically against existing baselines. dT ) regret, as follows from our Ω(
√
√
√
√ 1

Introduction
Sequential decision-making problems with side information (in the form of features or attributes), have been popular in machine learning as contextual bandits [16, 13, 26]. A contextual bandit learner, at each round, observes a context before taking action based on it. The resulting payoff is typically assumed to depend on the context and the action taken according to an unknown map. The learner aims to play the best possible action for the current context at each time and thus minimize its regret with respect to an oracle that knows the payoff function.
In many learning settings, however, it is more common to be able to only relatively compare actions, in a decision step, instead of being able to gauge their absolute utilities. E.g., information retrieval, search engine optimization, recommender systems, crowdsourcing, drug testing, tournament ranking, social surveys etc. [18, 21]. A speciﬁc application example is: Consider the problem of recommending items from a catalog to users on a shopping website. Each time, the context is determined by the visiting user’s features together with all items’ features. When a subset of items is presented, the user clicks on one of them according to a relative preference model; only the items presented matter – this plausibly models comparative cognitive choices being made by humans. The aim is to converge to identify the overall best item in the catalog. Additionally, since our model is designed to leverage
∗Microsoft Research, New York, US; aasa@microsoft.com. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the structured preference feedback, we can handle very large decision spaces, unlike state of the art dueling bandits algorithms that only deals with pairwise preferences [47], [23] [43], [41] etc., or even some of the recent works on general subsetwise preference bandits (e.g. [30, 33, 38]). All of them need to maintain a K 2 matrix explicitly, and their regret scales as O(K). This becomes impractical for large (or inﬁnite) action spaces of size K.
In this work, we consider a natural structured contextual preference bandit setting, comprised of items with intrinsic (absolute) scores depending on their features in an unknown way, e.g., linear with unknown weights. In the most general setup, the learner plays (compares) a subset 2 ≤ q ≤ K items and gets to see a (noisy) (cid:96)-length rank-ordered feedback (where 1 ≤ (cid:96) ≤ q) of the top-ranked items of the selected subset with a probability distribution governed by the items’ scores. We are primarily interested in developing adaptive subset-selection algorithms for which guarantees can be given for a suitably deﬁned measure of regret.
To the best of our knowledge, we are the ﬁrst to give theoretical guarantees for the above problem of regret minimization in contextual preference bandits for potentially inﬁnitely large decision spaces and design provably optimal algorithms for the same. Some recent works [17, 38] have considered this problem in the subset selection setup but their algorithms do not guarantee any ﬁnite time regret bounds, neither validate their performance optimality theoretically. In [15], authors considered a version of adversarial contextual dueling bandit problem, which only takes into account the special case of pairwise feedback (q = 2). Though similar in names, their problem framework and consequently the analysis are very different from ours. In their setup, at each time the preference matrix is determined by a randomly chosen known context variable, and the algorithm is assumed to have access to a pool of ﬁnite set of policies. The goal of the learner is to compete with the ‘optimal policy in the pool’ for which they proposed an sparring-EXP4 like algorithm—their regret bound though scales as O(
T ) in T , it depends linearly on the size of arm space (K in their notation), and thus become vacuous for inﬁnitely large decision space. Moreover, their computational complexity also scales linearly with the size of the policy class, which tends to be prohibitively large in practice.
√ 1. We propose the problem of structured contextual preference bandits, where at each iteration, the learner is presented with a set of K-arms St (each represented by d-dimensional feature vectors), and the task of the learner is to select a subset of at most q-items (1 ≤ q ≤ K), with the objective being to identify the ‘best arm’ of St in every round. The novelty of the framework lies in the relative preference based feedback model, which only allows the learner to see a noisy draw of the top-ranked items of the selected subset, unlike the absolute reward feedback used for standard bandits setup. 2. We ﬁrst address the problem for the special case of dueling bandits, where q = 2, i.e. the learner only have access to pairwise preferences of the selected item pairs at each round. We propose two algorithms for the basic dueling bandit setting: Our ﬁrst algorithm, Maximum-Informative-Pair (Alg 1), is based on the idea of selecting the most uncertain pair (‘max-variance’) from the set of
‘promising candidates’, and we prove an O(d
T ) regret for the same (Thm. 3).
√ 3. Our second algorithm Stagewise-Adaptive-Duel (Alg. 3), is developed on the idea of tracking, in a phased fashion, the best arm of the context set, which ensures a sharper concentration rate of the pairwise scores. This results in ˜O( dT ) 2 regret guarantee (Thm. 5), improving upon the regret d factor. bound of our previous algorithm by a
√
√
√ 4. We also show that fundamental regret lower bound of Ω( dT ) for the contextual dueling bandit problem addressed here (Thm. 3). Thus theoretically our second algorithm (Alg. 3) is provably optimal, however our Alg. 1 often works better in practice as we show in the experiments (Sec. 6). 5. We then analyze the problem for more general subsetwise preference feedback, where at each round, the learner is allowed to play a subset of q-items (q ≥ 2), upon which the winner feedback of dT ) regret lower bound (Thm. 11) the played subset is revealed [10, 30, 33]. We ﬁrst prove an Ω( for the problem, which establishes that, interestingly, having access to subsetwise preferences does not really help in faster information aggregation (for the speciﬁc preference model considered here).
Subsequently, we also discuss an algorithm with near-optimal regret guarantee, whose regret bound is also independent of the subsetsize q up to logarithmic factors (Thm. 12).
√ 2The notation ˜O(·) hides logarithmic dependencies. 2
Finally, we corroborate our theoretical ﬁndings with empirical evaluations. Detailed