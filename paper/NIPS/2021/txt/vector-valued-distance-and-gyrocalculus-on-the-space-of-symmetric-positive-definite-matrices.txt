Abstract
We propose the use of the vector-valued distance to compute distances and extract geometric information from the manifold of symmetric positive deﬁnite matrices (SPD), and develop gyrovector calculus, constructing analogs of vector space op-erations in this curved space. We implement these operations and showcase their versatility in the tasks of knowledge graph completion, item recommendation, and question answering. In experiments, the SPD models outperform their equivalents in Euclidean and hyperbolic space. The vector-valued distance allows us to visual-ize embeddings, showing that the models learn to disentangle representations of positive samples from negative ones. 1

Introduction
Symmetric Positive Deﬁnite (SPD) matrices have been applied in many tasks in computer vision such as pedestrian detection [75, 79], action [36, 51, 60] or face recognition [41, 42], object [43, 90] and image set classiﬁcation [86], visual tracking [87], and medical imaging analysis [5, 65] among others. They have been used to capture statistical notions (Gaussian distributions [68], covariance
[78]), while respecting the Riemannian geometry of the underlying SPD manifold, which offers a convenient trade-off between structural richness and computational tractability [27]. Previous work has applied approximation methods that locally ﬂatten the manifold by projecting it to its tangent space [22, 83], or by embedding the manifold into higher dimensional Hilbert spaces [35, 90].
These methods face problems such as distortion of the geometrical structure of the manifold and other known concerns with regard to high-dimensional spaces [29]. To overcome these issues, several distances on SPD manifolds have been proposed, such as the Afﬁne Invariant metric [65], the
Stein metric [71], the Bures–Wasserstein metric [13] or the Log-Euclidean metric [5, 6], with their respective geometric properties. However, the representational power of SPD is not fully exploited in many cases [6, 65]. At the same time, it is hard to translate operations into their non-Euclidean domain given the lack of closed-form expressions. There has been a growing need to generalize basic operations, such as addition, rotation, reﬂection or scalar multiplication, to their Riemannian geometric counterparts to leverage this structure in the context of Geometric Deep Learning [19].
SPD manifolds have a rich geometry that contains both Euclidean and hyperbolic subspaces. Thus, embeddings into SPD manifolds are beneﬁcial, since they can accommodate hierarchical structure in data sets in the hyperbolic subspaces while at the same time represent Euclidean features. This makes them more versatile than using only hyperbolic or Euclidean spaces, and in fact, their different submanifold geometries can be used to identify and disentangle such substructures in graphs.
∗Correspondence to federico.lopez@h-its.org 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work, we introduce the vector-valued distance function to exploit the full representation power of SPD (§3.1). While in Euclidean or hyperbolic space the relative position between two points is completely captured by their distance (and this is the only invariant), in SPD this invariant is a vector, encoding much more information than a single scalar. This vector reﬂects the higher expressivity of
SPD due to its richer geometry encompassing Euclidean as well as hyperbolic spaces. We develop algorithms using the vector-valued distance and showcase two main advantages: its versatility to implement universal models, and its use in explaining and visualizing what the model has learned.
Furthermore, we bridge the gap between Euclidean and SPD geometry by developing gyrocalculus in
SPD (§4), which yields closed-form expressions of arithmetic operations, such as addition, scalar multiplication and matrix scaling. This provides means to translate previously implemented ideas in different metric spaces to their analog notions in SPD. These arithmetic operations are also useful to adapt neural network architectures to SPD manifolds.
We showcase this on knowledge graph completion, item recommendation, and question answering. In the experiments, the proposed SPD models outperform their equivalents in Euclidean and hyperbolic space (§6). These results reﬂect the superior expressivity of SPD, and show the versatility of the approach and ease of integration with downstream tasks.
The vector-valued distance allows us to develop a new tool for the analysis of the structural properties of the learned representations. With this tool, we visualize high-dimensional SPD embeddings, providing better explainability on what the models learn (§6.4). We show that the knowledge graph models are capable of disentangling and clustering positive triples from negative ones. 2