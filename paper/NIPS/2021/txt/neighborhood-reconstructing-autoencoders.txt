Abstract
Vanilla autoencoders often produce manifolds that overﬁt to noisy training data, or have the wrong local connectivity and geometry. Autoencoder regularization techniques, e.g., the denoising autoencoder, have had some success in reducing overﬁtting, whereas recent graph-based methods that exploit local connectivity information provided by neighborhood graphs have had some success in mitigating local connectivity errors. Neither of these two approaches satisfactorily reduce both overﬁtting and connectivity errors; moreover, graph-based methods typically involve considerable preprocessing and tuning. To simultaneously address the two issues of overﬁtting and local connectivity, we propose a new graph-based autoencoder, the Neighborhood Reconstructing Autoencoder (NRAE). Unlike existing graph-based methods that attempt to encode the training data to some prescribed latent space distribution – one consequence being that only the encoder is the object of the regularization – NRAE merges local connectivity information contained in the neighborhood graphs with local quadratic approximations of the decoder function to formulate a new neighborhood reconstruction loss. Com-pared to existing graph-based methods, our new loss function is simple and easy to implement, and the resulting algorithm is scalable and computationally efﬁcient; the only required preprocessing step is the construction of the neighborhood graph.
Extensive experiments with standard datasets demonstrate that, compared to exist-ing methods, NRAE improves both overﬁtting and local connectivity in the learned manifold, in some cases by signiﬁcant margins. Code for NRAE is available at https://github.com/Gabe-YHLee/NRAE-public. 1

Introduction
Autoencoders are widely used to identify, and to generate samples from, the underlying low-dimensional manifold structure of a given data distribution [14, 1]. It has been widely observed that vanilla autoencoders quite often produce manifolds that (i) are highly sensitive to noisy training data (see Figure 1(a)), or (ii) have the wrong local connectivity and geometry (see Figure 1(b)), signiﬁcantly impairing their performance. Regularization techniques have had some success in mitigating the former, e.g., the Denoising Autoencoder [21], which uses deliberately corrupted inputs to train the autoencoder, typically learns manifolds that are robust to noise, but not always with the correct local geometry.
Recently, autoencoder regularization methods that use neighborhood graphs have had some success in addressing the incorrect connectivity issue [16, 18, 10, 7]. Notwithstanding the additional computa-tional overhead of constructing local neighborhood graphs, the local geometric information obtained from these graphs can signiﬁcantly reduce errors in the local geometry of the learned manifold, and make learning more well-behaved and robust.
The underlying premise behind these methods is that since the local geometry and topology of the data is captured in the latent space distribution, which is determined entirely by the encoder, regularizing only the encoder should be sufﬁcient; little if any consideration needs to be given to the decoder. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Learned manifolds that (a) overﬁt the data or (b) have the wrong local geometry.
The ﬂaw with this premise is that although the encoder learns the correct latent space representation of the manifold data, the decoder is still susceptible to overﬁtting of the type shown in Figure 1(a).
These existing methods moreover rely on computation-intensive preprocessing steps like manifold learning [10], linear coefﬁcients computation [7], or computing topological features using persistent homology at each training iteration [16, 2], each of whose computational requirements can grow signiﬁcantly with problem dimension and scale.
The main contribution of this paper is a new graph-based autoencoder training method that addresses both the overﬁtting and connectivity issues illustrated in Figure 1(a)-(b). Like current methods, our method also employs local graphs that capture the local geometry of the data distribution. The key idea behind our method, which we call the Neighborhood Reconstructing Autoencoder (NRAE), is to employ a local quadratic (and in some cases linear) approximation of the decoder function to formulate a new neighborhood reconstruction loss in lieu of the point reconstruction loss typically used for autoencoder training. This idea leads to learning the correct geometry and reducing noise sensitivity, signiﬁcantly improving the robustness of autoencoder training.
To make things more explicit, let gφ : Rn → Rm be the encoder (parametrized by φ) and fθ : Rm →
Rn be the decoder (parametrized by θ). Whereas vanilla autoencoders are trained to minimize the sum of the point reconstruction errors (cid:80) i (cid:107)xi − fθ(gφ(xi))(cid:107)2, NRAE minimizes a reconstruction error of the form (cid:88) (cid:88) i x∈N (xi) (cid:13) (cid:13) 2 (cid:13)x − ˜fθ(gφ(x); gφ(xi)) (cid:13) (cid:13) (cid:13)
, (1) where N (xi) is the set of neighborhood points of xi (including xi) and ˜fθ(·; gφ(xi)) is a local quadratic (or linear) approximation of fθ about gφ(xi). The vanilla autoencoder is obtained by setting
N (xi) = {xi} for all i. The key idea here is to locally approximate the decoder only, and to exploit the local geometric information extracted from the decoded manifold represented by the image of fθ.
Like other neighborhood graph-based methods, NRAE also learns the correct local geometry of the decoded manifold. At the same time, the local quadratic (or linear) approximation of fθ considerably reduces any overﬁtting to noisy training data or sensitivity to outliers, while maintaining computational efﬁciency – rather than the entire Jacobian or Hessian of fθ, only the more easily computed Jacobian-vector and Hessian-vector products are needed for the approximation.
Compared to existing graph-based autoencoder regularization methods, NRAE is easy to implement, computationally efﬁcient, and scalable, requiring only a single prior construction of the graph without additional pre-processing steps. Experiments with both synthetic and image data (MNIST, Fashion-MNIST, KMNIST, Omniglot, SVHN, CIFAR10, CIFAR100, CelebA) conﬁrm that overall our method better learns the correct geometry of manifolds, showing improved generalization performance vis-á-vis existing graph-based and other autoencoder regularization methods. 2 Neighborhood Reconstructing Autoencoder
In this section, we ﬁrst provide a high-level mathematical description of the Neighborhood Recon-structing Autoencoder (NRAE), followed by algorithmic details and a discussion of the NRAE’s properties and behavior. Throughout we consider a deterministic autoencoder with an encoder function gφ : Rn → Rm and decoder function fθ : Rm → Rn (m ≤ n), with their composition denoted by Fθ,φ := fθ ◦ gφ. We use the notation D := {xi ∈ Rn}M i=1 to denote the set of observed data points. 2
2.1 Mathematical Description
In what follows we use the notation N (x) to denote the set of neighborhood points of x, with x included in N (x). We begin with the following deﬁnition:
Deﬁnition 1 Let ˜Fθ,φ(·; x) := ˜fθ(gφ(·); gφ(x)), where ˜fθ(·; z) is a local quadratic (or in some cases linear) approximation of fθ at z = (z1, z2, ...zm):
˜fθ(z(cid:48); z) := fθ(z) + m (cid:88) i=1
∂fθ
∂zi (z)dzi + m (cid:88) i,j=1 1 2
∂2fθ
∂zi∂zj (z)dzidzj, (2) where dz = z(cid:48) − z. ˜Fθ,φ(N (x); x) is said to be a neighborhood reconstruction of N (x).
If instead of a quadratic approximation we use the linear approximation of fθ, the image of ˜Fθ,φ(·; x) is the tangent space of the decoded manifold at Fθ,φ(x), and the neighborhood reconstruction of
N (x) is a subset of the tangent space; the neighborhood reconstruction in this case contains ﬁrst-order local geometric information about the decoded manifold.
The key idea behind Deﬁnition 1 is that we locally approximate the decoder, and not the encoder, to extract and exploit local geometric information on the decoded manifold, which is captured in the image of ˜Fθ,φ(·; x) (i.e., the local approximation of the decoded manifold). Figure 2 illustrates an example where the autoencoder reconstructs the points almost perfectly, but the neighborhood reconstruction of N (x), whose elements lie in the tangent space (here we use the linear approximation of fθ) is considerably different from N (x).
Figure 2: The training data points (blue), the decoded manifold (orange), the neighborhood of x denoted by N (x), and the neighborhood reconstruction (red). The black dotted lines represent the correspondences between x(cid:48) ∈ N (x) and ˜Fθ,φ(x(cid:48); x).
Given that the neighborhood reconstruction of N (x) reﬂects the local geometry of the decoded manifold, minimizing a loss function that measures the difference between N (x) and its image
˜Fθ,φ(N (x); x) is one means of training an autoencoder to preserve the local geometry of the original data distribution. With that goal in mind, we formulate a neighborhood reconstruction loss L as follows:
L(θ, φ; D) = (cid:88) 1
|D| 1
|N (x)| (cid:88)
K(x(cid:48), x) · (cid:107)x(cid:48) − ˜Fθ,φ(x(cid:48); x)(cid:107)2, (3) x∈D x(cid:48)∈N (x) where K(x(cid:48), x) is a positive symmetric kernel function that determines the weight for each x(cid:48) ∈ N (x).
Figure 3 illustrates how the neighborhood reconstruction loss can differentiate among the quality of the learned manifolds whose point reconstruction losses are all the same (close to zero): Case 3 has the smallest neighborhood reconstruction loss compared to Case 1 (wrong local geometry) and Case 2 (overﬁtting). NRAE converges to the vanilla AE – that is, the neighborhood reconstruction loss recovers the point reconstruction loss – if one of the following conditions is met: (i) N (x) = {x}, (ii) K(x(cid:48), x) = δ(x(cid:48), x), or (iii) fθ is linear.
It is reasonable to ask whether there are any advantages to using an approximation for both the encoder and decoder, i.e., to use a local quadratic (or linear) approximation for the composition map
Fθ,φ rather than just the decoder. As veriﬁed below in Section 4.3, using a quadratic approximation for both the encoder and decoder results in minimal to no performance improvement (the results for this Extended NRAE (E-NRAE) case are nearly identical to those obtained for NRAE), but the computational requirements increase substantially. As intuition suggests, applying a quadratic approximation for the decoder is sufﬁcient in evaluating the neighborhood reconstruction loss. 3
Figure 3: The orange curves represent the learned manifolds, the red points represent the neighborhood reconstruction, and the lengths of the black dotted lines represent the neighborhood reconstruction loss. 2.2 Algorithmic Details
Graph construction. The problem of inferring the geometric structure of a data distribution is typically posed as a graph construction problem [4]. We use one of the simplest graph construction methods, the k-NN graph with the Euclidean distance metric. The robustness of our algorithm with respect to the choice of k is tested in the Supplementary Material.
Kernel design. We choose the following simple kernel
K(x(cid:48), x) = λ + (1 − λ) δ(x(cid:48), x), (4) where 0 ≤ λ < 1 and δ(x(cid:48), x) = 1 if x(cid:48) = x and zero otherwise. This assigns the weight 1 for the center x ∈ N (x) and the weight λ for the remaining neighborhood points.
Batch sampling. To estimate the gradient of the proposed loss function, we use batch sampling for both summations over D and N (x). Given a batch B ⊂ D, we again sample a batch Bx from N (x).
We empirically ﬁnd that forcing each batch Bx to include x improves convergence. In this paper, we set Bx = {x, xn} where xn is uniformly sampled from N (x) − {x}. 3