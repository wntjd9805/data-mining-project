Abstract
We study the gradient ﬂow for a relaxed approximation to the Kullback-Leibler (KL) divergence between a moving source and a ﬁxed target distribution. This approximation, termed the KALE (KL Approximate Lower bound Estimator), solves a regularized version of the Fenchel dual problem deﬁning the KL over a restricted class of functions. When using a Reproducing Kernel Hilbert Space (RKHS) to deﬁne the function class, we show that the KALE continuously inter-polates between the KL and the Maximum Mean Discrepancy (MMD). Like the
MMD and other Integral Probability Metrics, the KALE remains well-deﬁned for mutually singular distributions. Nonetheless, the KALE inherits from the limiting
KL a greater sensitivity to mismatch in the support of the distributions, compared with the MMD. These two properties make the KALE gradient ﬂow particularly well suited when the target distribution is supported on a low-dimensional manifold.
Under an assumption of sufﬁcient smoothness of the trajectories, we show the global convergence of the KALE ﬂow. We propose a particle implementation of the ﬂow given initial samples from the source and the target distribution, which we use to empirically conﬁrm the KALE’s properties. 1

Introduction
We consider the problem of transporting probability mass from a source distribution P to a target distribution Q using a Wasserstein gradient ﬂow in probability space. When the density of the target is well-deﬁned and available, the Wasserstein gradient ﬂow of the Kullback-Leibler (KL) divergence provides a simple way to transport mass towards the target through the Fokker-Planck equation as established in the seminal work of [31]. Its time discretization yields a practical algorithm, the
Unadjusted Langevin Algorithm (ULA), which comes with strong convergence guarantees [22, 19]. A more recent gradient ﬂow approach, Stein Variational Gradient Descent (SVGD) [36], also leverages the analytic expression of the density and constructs a gradient ﬂow of the KL, albeit using a metric different from the Wasserstein metric.
The KL divergence is of particular interest due to its information theoretical interpretation [56] and its use in Bayesian Inference [13]. The KL deﬁnes a strong notion of convergence between probability distributions, and as such is often widely used for learning generative models, through Maximum
Likelihood Estimation [20]. Using the KL as a loss requires knowledge of the density of the target, however; moreover, this loss is well-deﬁned only when the distributions share the same support.
Consequently, we cannot use the KL in settings where the probability distributions are mutually singular, or when they are only accessible through samples. In particular, the Wasserstein gradient
ﬂow of the KL in these settings is ill-deﬁned.
⇤Work mostly completed at the Gatsby Unit. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recent works have considered the gradient ﬂow of Integral Probability Metrics (IPM) [46] instead of the KL, in settings where only samples (and not the density) of the target are known. This includes the
Maximum Mean Discrepancy (MMD) [4] and the Kernelized Sobolev Discrepancy (KSD) [45, 44].
One motivation for considering these particle ﬂows is their connection with the training of Generative
Adversarial Networks (GANs) [28] using IPMs such as the Wasserstein distance [7, 29, 27], the
MMD [24, 34, 33, 9, 10, 5] or the Sobolev discrepancy [43]. As discussed in [45, Section 3.3], these
ﬂows deﬁne update equations that are similar to those of a generator in a GAN. Thus, studying the convergence ﬂows can provide helpful insight into conditions for GAN convergence, and ultimately, improvements to GAN training algorithms. A second motivation lies in the connection between the training dynamics of inﬁnitely wide 2-layer neural networks and the Wasserstein gradient ﬂow of particular functionals [52]. Thus, analyzing the asymptotic behavior of such ﬂows [40, 58, 18] can ultimately provide convergence guarantees for the training dynamics of neural networks. Establishing such results remains challenging for some classes of IPMs, however, such as the MMD [4].
In this paper, we construct the gradient ﬂow of a relaxed approximation of the KL, termed the KALE (KL Approximate Lower bound Estimator). Unlike the KL, the KALE is well-deﬁned given any source and target, regardless of their relative absolute continuity. The KALE is obtained by solving a regularized version of the Fenchel dual problem deﬁning the KL, deﬁned over a restricted function class [48, 6], and can be estimated solely from samples from the data. The version of the KALE we consider in this work beneﬁts from two important features that are crucial for deﬁning and analyzing a relaxed gradient ﬂow of the KL. (1) We deﬁne the function class to be a Reproducing Kernel Hilbert
Space (RKHS). This makes the optimization problem deﬁning the KALE convex and allows for practical algorithms computing it. (2) We consider a regularized version of the problem deﬁning the
KALE, thus providing a simpler expression for the gradient ﬂow by virtue of the envelope theorem
[42]. In Section 2, we review the KALE, and show that it is a divergence that metrizes the weak convergence of probability measures, while interpolating between the KL and the MMD depending on the amount of regularization. We then construct in Section 3 the Wasserstein Gradient Flow of the KALE, and we show global convergence of the KALE ﬂow provided that the trajectories are sufﬁciently regular. In Section 4, we introduce the KALE particle descent algorithm as well as a practical way to implement it. In Section 5, we present the results obtained by running the KALE particle descent algorithm on a set of problems with different geometrical properties. We show empirically that the sensitivity to support mismatch of the KALE inherited from the KL leads to well-behaved trajectories compared to the MMD ﬂow, making the KALE ﬂow a desirable alternative when a KL ﬂow cannot be deﬁned.