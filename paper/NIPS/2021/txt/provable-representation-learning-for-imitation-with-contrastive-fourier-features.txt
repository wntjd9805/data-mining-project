Abstract
In imitation learning, it is common to learn a behavior policy to match an unknown target policy via max-likelihood training on a collected set of target demonstrations.
In this work, we consider using ofﬂine experience datasets – potentially far from the target distribution – to learn low-dimensional state representations that provably accelerate the sample-efﬁciency of downstream imitation learning. A central challenge in this setting is that the unknown target policy itself may not exhibit low-dimensional behavior, and so there is a potential for the representation learning objective to alias states in which the target policy acts differently. Circumventing this challenge, we derive a representation learning objective that provides an upper bound on the performance difference between the target policy and a low-dimensional policy trained with max-likelihood, and this bound is tight regardless of whether the target policy itself exhibits low-dimensional structure. Moving to the practicality of our method, we show that our objective can be implemented as contrastive learning, in which the transition dynamics are approximated by either an implicit energy-based model or, in some special cases, an implicit linear model with representations given by random Fourier features. Experiments on both tabular environments and high-dimensional Atari games provide quantitative evidence for the practical beneﬁts of our proposed objective.1 1

Introduction
In the ﬁeld of sequential decision making one aims to learn a behavior policy to act in an environment to optimize some criteria. The well-known ﬁeld of reinforcement learning (RL) corresponds to one aspect of sequential decision making, where the aim is to learn how to act in the environment to maximize cumulative returns via trial-and-error experience [38]. In this work, we focus on imitation learning, where the aim is to learn how to act in the environment to match the behavior of some unknown target policy [25]. This focus puts us closer to the supervised learning regime, and, indeed, a common approach to imitation learning – known as behavioral cloning (BC) – is to perform max-likelihood training on a collected a set of target demonstrations composed of state-action pairs sampled from the target policy [31, 34].
Since the learned behavior policy produces predictions (actions) conditioned on observations (states), the amount of demonstrations needed to accurately match the target policy typically scales with the state dimension, and this can limit the applicability of imitation learning to settings where collecting large amounts of demonstrations is expensive, , in health [18] and robotics [24] applications. The limited availability of target demonstrations stands in contrast to the recent proliferation of large ofﬂine datasets for sequential decision making [28, 19, 7, 21]. These datasets may exhibit behavior far from the target policy and so are not directly relevant to imitation learning via max likelihood 1Find experimental code at https://github.com/google-research/google-research/tree/ master/rl_repr. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
training. Nevertheless, the ofﬂine datasets provide information about the unknown environment, presenting samples of environment reward and transition dynamics. It is therefore natural to wonder, is it possible to use such ofﬂine datasets to improve the sample efﬁciency of imitation learning?
Recent empirical work suggests that this is possible [40, 10], by using the ofﬂine datasets to learn a low-dimensional state representation via unsupervised training objectives. While these empirical successes are clear, the theoretical foundation for these results is less obvious. The main challenge in providing theoretical guarantees for such techniques is that of aliasing. Namely, even if environment rewards or dynamics exhibit a low-dimensional structure, the target policy and its demonstrations may not. If the target policy acts differently in states which the representation learning objective maps to the same low-dimensional representation, the downstream behavioral cloning objective may end up learning a policy which “averages” between these different states in unpredictable ways.
In this work, we aim to bridge the gap between practical objectives and theoretical understanding. We derive an ofﬂine objective that learns low-dimensional representations of the environment dynamics and, if available, rewards. We show that minimizing this objective in conjunction with a downstream behavioral cloning objective corresponds to minimizing an upper bound on the performance difference between the learned low-dimensional BC policy and the unknown and possibly high-dimensional target policy. The form of our bound immediately makes clear that, as long as the learned policy is sufﬁciently expressive on top of the low-dimensional representations, the implicit “averaging” occurring in the BC objective due to any aliasing is irrelevant, and a learned policy can match the target regardless of whether the target policy itself is low-dimensional.
Extending our results to policies with limited expressivity, we consider the commonly used parameter-ization of setting the learned policy to be log-linear with respect to the representations (, a softmax of a linear transformation). In this setting, we show that it is enough to use the same ofﬂine representation learning objective, but with linearly parameterized dynamics and rewards, and this again leads to an upper bound showing that the downstream BC policy can match the target policy regardless of whether the target is low-dimensional or log-linear itself. We compare the form of our representation learning objective to “latent space model” approaches based on bisimulation principles, popular in the
RL literature [20, 41, 22, 13], and show that these objectives are, in contrast, very liable to aliasing issues even in simple scenarios, explaining their poor performance in recent empirical studies [40].
We continue to the practicality of our own objective, and show that it can be implemented as a contrastive learning objective that implicitly learns an energy based model, which, in many common cases, corresponds to a linear model with respect to representations given by random Fourier features [33]. We evaluate our objective in both tabular synthetic domains and high-dimensional Atari game environments [11]. We ﬁnd that our representation learning objective effectively leverages ofﬂine datasets to dramatically improve performance of behavioral cloning. 2