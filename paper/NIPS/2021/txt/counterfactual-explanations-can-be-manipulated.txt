Abstract
Counterfactual explanations are emerging as an attractive option for providing recourse to individuals adversely impacted by algorithmic decisions. As they are deployed in critical applications (e.g. law enforcement, ﬁnancial lending), it becomes important to ensure that we clearly understand the vulnerabilties of these methods and ﬁnd ways to address them. However, there is little understanding of the vulnerabilities and shortcomings of counterfactual explanations. In this work, we introduce the ﬁrst framework that describes the vulnerabilities of counterfactual explanations and shows how they can be manipulated. More speciﬁcally, we show counterfactual explanations may converge to drastically different counterfactuals under a small perturbation indicating they are not robust. Leveraging this insight, we introduce a novel objective to train seemingly fair models where counterfactual explanations ﬁnd much lower cost recourse under a slight perturbation. We describe how these models can unfairly provide low-cost recourse for speciﬁc subgroups in the data while appearing fair to auditors. We perform experiments on loan and violent crime prediction data sets where certain subgroups achieve up to 20x lower cost recourse under the perturbation. These results raise concerns regarding the dependability of current counterfactual explanation techniques, which we hope will inspire investigations in robust counterfactual explanations.1 1

Introduction
Machine learning models are being deployed to make consequential decisions on tasks ranging from loan approval to medical diagnosis. As a result, there are a growing number of methods that explain the decisions of these models to affected individuals and provide means for recourse [1]. For example, recourse offers a person denied a loan by a credit risk model a reason for why the model made the prediction and what can be done to change the decision. Beyond providing guidance to stakeholders in model decisions, algorithmic recourse is also used to detect discrimination in machine learning models [2–4]. For instance, we expect there to be minimal disparity in the cost of achieving recourse between both men and women who are denied loans. One commonly used method to generate recourse is that of counterfactual explanations [5]. Counterfactual explanations offer recourse by attempting to ﬁnd the minimal change an individual must make to receive a positive outcome [6–9].
Although counterfactual explanations are used by stakeholders in consequential decision-making settings, there is little work on systematically understanding and characterizing their limitations.
Few recent studies explore how counterfactual explanations may become valid when the underlying model is updated. For instance, a model provider might decide to update a model, rendering previously generated counterfactual explanations invalid [10, 11]. Others point out that counterfactual explanations, by ignoring the causal relationships between features, sometimes recommend changes that are not actionable [12]. Though these works shed light on certain shortcomings of counterfactual explanations, they do not consider whether current formulations provide stable and reliable results, whether they can be manipulated, and if fairness assessments based on counterfactuals can be trusted. 1Project Page: https://dylanslacks.website/cfe/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Training with BCE Objective (b) Training Adversarial Model
Figure 1: Model trained with BCE objective and adversarial model on a toy data set using
Wachter et al.’s Algorithm [6]. The surface shown is the loss in Wachter et al.’s Algorithm with respect to x, the line is the path of the counterfactual search, and we show results for a single point, x. For the model without the manipulation (subﬁgure 1a), the counterfactuals for x and x + δ converge to the same minima and are similiar cost recourse. For the adversarial model (subﬁgure 1b), the recourse found for x has higher cost than x + δ because the local minimum initialized at x is farther than the minimum starting at x + δ, demonstrating the problematic behavior of counterfactual explanations.
In this work, we introduce the ﬁrst formal framework that describes how counterfactual explanation techniques are not robust.2 More speciﬁcally, we demonstrate how the family of counterfactual explanations that rely on hill-climbing (which includes commonly used methods like Wachter et al.’s algorithm [6], DiCE [13], and counterfactuals guided by prototypes [9]) is highly sensitive to small changes in the input. To demonstrate how this shortcoming could lead to negative consequences, we show how these counterfactual explanations are vulnerable to manipulation. Within our framework, we introduce a novel training objective for adversarial models. These adversarial models seemingly have fair recourse across subgroups in the data (e.g., men and women) but have much lower cost recourse for the data under a slight perturbation, allowing a bad-actor to provide low-cost recourse for speciﬁc subgroups simply by adding the perturbation. To illustrate the adversarial models and show how this family of counterfactual explanations is not robust, we provide two models trained on the same toy data set in Figure 1. In the model trained with the standard BCE objective (left side of Fig 1), the counterfactuals found by Wachter et al.’s algorithm [6] for instance x and perturbed instance x + δ converge to same minima (denoted A(x) and A(x + δ)). However, for the adversarial model (right side of Fig 1), the counterfactual found for the perturbed instance x + δ is closer to the original instance x. This result indicates that the counterfactual found for the perturbed instance x + δ is easier to achieve than the counterfactual for x found by Wachter et al.’s algorithm! Intuitively, counterfactual explanations that hill-climb the gradient are susceptible to this issue because optimizing for the counterfactual at x versus x + δ can converge to different local minima.
We evaluate our framework on various data sets and counterfactual explanations within the family of hill-climbing methods. For Wachter et al.’s algorithm [6], a sparse variant of Wachter et al.’s,
DiCE [13], and counterfactuals guided by prototypes [9], we train models on data sets related to loan prediction and violent crime prediction with fair recourse across subgroups that return 2-20× lower cost recourse for speciﬁc subgroups with the perturbation δ, without any accuracy loss. Though these results indicate counterfactual explanations are highly vulnerable to manipulation, we consider making counterfactual explanations that hill-climb the gradient more robust. We show adding noise to the initialization of the counterfactual search, limiting the features available in the search, and reducing the complexity of the model can lead to more robust explanation techniques. 2