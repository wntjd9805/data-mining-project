Abstract
Without positional information, attention-based Transformer neural networks are permutation-invariant. Absolute or relative positional embeddings are the most popular ways to feed Transformer models with positional information. Absolute positional embeddings are simple to implement, but suffer from generalization issues when evaluating on sequences longer than seen at training time. Relative po-sitions are more robust to input length change, but are more complex to implement and yield inferior model throughput due to extra computational and memory costs.
In this paper, we propose an augmentation-based approach (CAPE) for absolute positional embeddings, which keeps the advantages of both absolute (simplicity and speed) and relative positional embeddings (better generalization). In addition, our empirical evaluation on state-of-the-art models in machine translation, im-age and speech recognition demonstrates that CAPE leads to better generalization performance as well as increased stability with respect to training hyper-parameters. 1

Introduction
Transformers have been shown to be highly effective on problems involving sequential modeling, such as machine translation (MT) [48] and natural language processing (NLP) [14, 41, 6]. Following its success on these tasks, the Transformer architecture raised immediate interest in other domains: automatic speech recognition (ASR) [15, 23], music generation [26], object detection [7], and ﬁnally image recognition [16, 47] and video understanding [5].
Two major components of the Transformer are the attention mechanism [2, 48] and the positional encoding [48, 44, 26, 11]. Without the latter, vanilla attention Transformers are invariant with respect to input tokens permutations (making “cat eats ﬁsh” and “ﬁsh eats cat” identical to the model). In the original Transformer publication, sinusoidal positional encoding was introduced [48]. Token positions were encoded in an absolute manner, which was sufﬁcient to achieve state-of-the-art performance in numerous tasks. However, performance issues were later observed when dealing with sequences of length not seen at training time [26, 11, 59, 33, 42, 18]. For most applications relative positions between tokens are more relevant than absolute ones. A number of approaches were thus proposed to encode relative positions in an explicit form [44, 11, 26, 42, 27], leading to improvements in modeling long sequences. However, all these approaches focus on modifying the attention mechanism and suffer from additional computational and memory costs [57]. Relative positional encoding is also
⇤Currently at Apple. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
notably hard to implement efﬁciently for multidimensional case, and recent advances in Transformer models for computer vision [7, 16, 47] still rely on learnable absolute positional encoding.
Instead of changing the Transformer attention mechanism, we propose to improve absolute sinusoidal positional encodings in two ways: a) instead of discrete positions, rely on continuous ones, which better match the continuous nature of image, sound or video data; b) preserve some information about relative token positions, via a speciﬁc augmentation approach for positional embeddings during training. We empirically evaluate our approach, dubbed continuous augmented positional embedding (CAPE), with recent state-of-the-art models in several application domains. We study generalization properties and introduce unique features unlocked by CAPE. The main contributions of this work are:
• new augmented continuous positional embedding (CAPE), which encodes some relative position information in a computationally efﬁcient way, and improves generalization per-formance compared to other positional embeddings across a variety of domains: machine translation, image and speech recognition;
• a single vision Transformer (UniViT) trained with CAPE on the mix of different resolutions: it outperforms each single-resolution baseline, generalizes better to unseen resolutions and can naturally process images of any size;
• new CAPE-based adaptive training scheme for ASR that eliminates need for padding. 2