Abstract
Operating in the real-world often requires agents to learn about a complex envi-ronment and apply this understanding to achieve a breadth of goals. This problem, known as goal-conditioned reinforcement learning (GCRL), becomes especially challenging for long-horizon goals. Current methods have tackled this problem by augmenting goal-conditioned policies with graph-based planning algorithms.
However, they struggle to scale to large, high-dimensional state spaces and assume access to exploration mechanisms for efﬁciently collecting training data. In this work, we introduce Successor Feature Landmarks (SFL), a framework for exploring large, high-dimensional environments so as to obtain a policy that is proﬁcient for any goal. SFL leverages the ability of successor features (SF) to capture transition dynamics, using it to drive exploration by estimating state-novelty and to enable high-level planning by abstracting the state-space as a non-parametric landmark-based graph. We further exploit SF to directly compute a goal-conditioned policy for inter-landmark traversal, which we use to execute plans to “frontier” landmarks at the edge of the explored state space. We show in our experiments on MiniGrid and ViZDoom that SFL enables efﬁcient exploration of large, high-dimensional state spaces and outperforms state-of-the-art baselines on long-horizon GCRL tasks1.

Introduction 1
Consider deploying a self-driving car to a new city. To be practical, the car should be able to explore the city such that it can learn to traverse from any starting location to any destination, since the destination may vary depending on the passenger. In the context of reinforcement learning (RL), this problem is known as goal-conditioned RL (GCRL) [12, 13]. Previous works [31, 1, 24, 26, 17] have tackled this problem by learning a goal-conditioned policy (or value function) applicable to any reward function or “goal.” However, the goal-conditioned policy often fails to scale to long-horizon goals [11] since the space of state-goal pairs grows intractably large over the horizon of the goal.
To address this challenge, the agent needs to (a) explore the state-goal space such that it is proﬁcient for any state-goal pair it might observe during test time and (b) reduce the effective goal horizon for the policy learning to be tractable. Recent work [25, 11] has tackled long-horizon GCRL by leveraging model-based approaches to form plans consisting of lower temporal-resolution subgoals.
The policy is then only required to operate for short horizons between these subgoals. One line of work learned a universal value function approximator (UVFA) [31] to make local policy decisions and to estimate distances used for building a landmark-based map, but assumed a low-dimensional state space where the proximity between the state and goal could be computed by the Euclidean distance [11]. Another line of research focused on visual navigation tasks conducted planning over 1The demo video and code can be found at https://2016choang.github.io/sfl. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
1. Select frontier landmark 2. Plan path to  frontier landmark 3. Execute planned path with goal-conditioned policy 
Planned Path 5. Update graph + SF with trajectory
Trajectory
Graph 
+ 
Graph + SF
Update 4. Use random policy  to explore
Initial state
Landmarks
Frontier landmark
Explored region
Planned path
Edge
Goal-conditioned policy
Random policy
Figure 1: High-level overview of SFL. 1. During exploration, select a frontier landmark (red circled dot) lying at the edge of the explored region as the target goal. During evaluation (not shown in the
ﬁgure), the actual goal is selected as the target goal. 2. Use the graph to plan a landmark path (green lines) to the target goal. 3. Execute the planned path with the goal-conditioned policy (black dotted arrow). 4. During exploration, upon reaching the frontier landmark, deploy the random policy (red dotted arrow) to reach novel states in unexplored areas. 5. Use each transition in the trajectory to update the graph and SF (see Figure 2). Note: The agent is never shown the top-down view of the maze and only uses ﬁrst-person image observations (example on left) to carry out these steps. Goals are also given as ﬁrst-person images. graph representations of the environment [29, 9, 16, 4]. However, these studies largely ignored the inherent exploration challenge present for large state spaces, and either assumed the availability of human demonstrations of exploring the state space [29], the ability to spawn uniformly over the state space [9, 16], or the availability of ground-truth map information [4].
In this work, we aim to learn an agent that can tackle long-horizon GCRL tasks and address the associated challenges in exploration. Our key idea is to use successor features (SF) [15, 2] — a representation that captures transition dynamics — to deﬁne a novel distance metric, Successor
Feature Similarity (SFS). First, we exploit the transfer ability of SF to formulate a goal-conditioned value function in terms of SFS between the current state and goal state. By just learning SF via self-supervised representation learning, we can directly obtain a goal-conditioned policy from SFS without any additional policy learning. Second, we leverage SFS to build a landmark-based graph representation of the environment; the agent adds observed states as landmarks based on their SFS-predicted novelty and forms edges between landmarks by using SFS as a distance estimate. SF as an abstraction of transition dynamics is a natural solution for building this graph when we consider the
MDP as a directed graph of states (nodes) and transitions (edges) following [11]. We use this graph to systematically explore the environment by planning paths towards landmarks at the “frontier” of the explored state space and executing each segment of these planned paths with the goal-conditioned policy. In evaluation, we similarly plan and execute paths towards (long-horizon) goals. We call this framework Successor Feature Landmarks (SFL), illustrated in Figure 1.
Our contributions are as follows: (i) We use a single self-supervised learning component that captures dynamics information, SF, to build all the components of a graph-based planning framework, SFL. (ii) We claim that this construction enables knowledge sharing between each module of the framework and stabilizes the overall learning. (iii) We introduce the SFS metric, which serves as a distance estimate and enables the computation of a goal-conditioned Q-value function without further learning.
We evaluate SFL against current graph-based methods in long-horizon goal-reaching RL and visual navigation on MiniGrid [6], a 2D gridworld, and ViZDoom [37], a visual 3D ﬁrst-person view environment with large mazes. We observe that SFL outperforms state-of-the-art navigation baselines, most notably when goals are furthest away. In a setting where exploration is needed to collect training experience, SFL signiﬁcantly outperforms the other methods which struggle to scale in ViZDoom’s high-dimensional state space. 2  
2