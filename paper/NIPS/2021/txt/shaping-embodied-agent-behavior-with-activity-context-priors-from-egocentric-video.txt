Abstract
Complex physical tasks entail a sequence of object interactions, each with its own preconditions—which can be difﬁcult for robotic agents to learn efﬁciently solely through their own experience. We introduce an approach to discover activity-context priors from in-the-wild egocentric video captured with human worn cam-eras. For a given object, an activity-context prior represents the set of other compatible objects that are required for activities to succeed (e.g., a knife and cutting board brought together with a tomato are conducive to cutting). We encode our video-based prior as an auxiliary reward function that encourages an agent to bring compatible objects together before attempting an interaction. In this way, our model translates everyday human experience into embodied agent skills. We demonstrate our idea using egocentric EPIC-Kitchens video of people performing unscripted kitchen activities to beneﬁt virtual household robotic agents performing various complex tasks in AI2-iTHOR, signiﬁcantly accelerating agent learning.
Project page: http://vision.cs.utexas.edu/projects/ego-rewards/ 1

Introduction
Embodied AI agents that are capable of moving around and interacting with objects in human spaces promise important practical applications for home service robots, ranging from agents that can search for misplaced items, to agents that can cook entire meals. The pursuit of such agents has driven exciting new research in visual semantic planning [69], instruction following [59], and object rearrangement [3, 30], typically supported by advanced simulators [46, 33, 58, 22] where policies may be learned quickly and safely before potentially transferring to real robots. In such tasks, an agent aims to perform a sequence of actions that will transform the visual environment from an initial state to a goal state. This in turn requires jointly learning behaviors for both navigation, to move from one place to another, and object interaction, to manipulate objects and modify the environment (e.g., pick-up objects, use tools and objects together, turn-on lights).
A key challenge is that changing the state of the environment involves context-sensitive actions that depend on both the agent and environment’s current state—what the agent is holding, what other objects are present nearby, and what their properties are. For example, to wash a plate, a plate must be in the sink before the agent toggles-on the faucet; to slice an apple the agent must ﬁrst be holding a knife, and the apple must not already be sliced. Understanding these conditions is critical for efﬁcient learning and planning: the agent must ﬁrst bring the environment into the proper precondition state before attempting to perform a given activity with objects. We refer to these states as activity-contexts.
Despite its importance, current approaches do not explicitly model the activity-context. Pure rein-forcement learning (RL) approaches directly search for goal states without considering preconditions.
This requires a large number of trials for the agent to chance upon suitable conﬁgurations, and leads to poor sample efﬁciency or learning failures [69, 59]. Instead, most methods resort to col-lecting expert demonstrations to train imitation learning (IL) agents and optionally ﬁnetune with 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Main idea. Left and middle panel: We discover activity-contexts for objects directly from egocentric video of human activity. A given object’s activity-context goes beyond “what objects are found together” to capture the likelihood that each other object in the environment participates in activities involving it (i.e., “what objects together enable action"). Right panel: Our approach guides agents to bring compatible objects—objects with high likelihood—together to enable activities. For example, bringing a pan to the sink increases the value of faucet interactions, but bringing it to the table has little effect on interactions with a book.
RL [69, 59, 31, 15]. While demonstrations may implicitly reveal activity-contexts, they are cumber-some to collect; they require expert teleoperation using specialized hardware (VR, MoCap), often in artiﬁcial lab settings [68, 49, 28], and need to be collected independently for each new task.
Rather than solicit demonstrations, we propose to learn activity-contexts from real-world, egocentric video of people performing daily life activities. Humans understand activities from years of expe-rience, and can effortlessly bring even a novel environment to new, appropriate conﬁgurations for interaction-heavy tasks. Egocentric or “ﬁrst-person" video recorded with a wearable camera puts actions and objects manipulated by a person at the forefront, offering an immediate window of this expertise in action in its natural habitat.
We present a reinforcement learning approach that infers activity-context conditions directly from people’s ﬁrst-hand experience and transfers them to embodied agents to improve interaction policy learning. Speciﬁcally, we 1) train visual models to detect how humans prepare their environment for activities from egocentric video, and 2) develop a novel auxiliary reward function that encourages agents to seek out similar activity-context states. For example, by observing that people frequently carry pans to sinks (to clean them) or stoves (to cook their contents), our model rewards agents for prioritizing interactions with faucets or stove-knobs when pots or pans are nearby. As a result, this incentivizes agents to transport relevant objects to compatible locations before attempting interactions, which accelerates learning. See Fig. 1.
Importantly, our goal is not direct imitation. Our insight is that while humans and embodied agents have very different action spaces and bodies, they operate in similar environments where the underlying conditions about what state the environment must be in before trying to modify it are strongly aligned. Our goal is thus to guide an agent’s exploratory interactions towards these potential progress-enabling states as it learns a new task. Moreover, because our training videos are passively collected by human camera-wearers and capture a wide array of daily actions, they help build a general visual prior for human activity-contexts, while side-stepping the heavy requirements of collecting IL demonstrations for each individual task of interest.
Our experiments demonstrate the value of learning from egocentric videos from humans (in EPIC-Kitchens [13]) to train visual semantic planning agents (in AI2-iTHOR [33]). Our video model relates objects based on goal-oriented actions (e.g., knives used to cut potatoes) rather than spatial co-occurrences (e.g., knives are found near spoons) [66, 11, 48, 65] or semantic similarity (e.g., potatoes are like tomatoes) [66]. Our agents outperform strong exploration methods and state-of-the-art embodied policies on multi-step interaction tasks (e.g., storing cutlery, cleaning dishes), improving absolute success rates by up to 12% on the most complex tasks. Our approach learns policies faster, generalizes to unseen environments, and greatly improves success rates on difﬁcult instances. 2