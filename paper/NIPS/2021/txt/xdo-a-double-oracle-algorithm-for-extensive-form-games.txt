Abstract
Policy Space Response Oracles (PSRO) is a reinforcement learning (RL) algo-rithm for two-player zero-sum games that has been empirically shown to ﬁnd approximate Nash equilibria in large games. Although PSRO is guaranteed to converge to an approximate Nash equilibrium and can handle continuous actions, it may take an exponential number of iterations as the number of information states (infostates) grows. We propose Extensive-Form Double Oracle (XDO), an extensive-form double oracle algorithm for two-player zero-sum games that is guar-anteed to converge to an approximate Nash equilibrium linearly in the number of infostates. Unlike PSRO, which mixes best responses at the root of the game, XDO mixes best responses at every infostate. We also introduce Neural XDO (NXDO), where the best response is learned through deep RL. In tabular experiments on
Leduc poker, we ﬁnd that XDO achieves an approximate Nash equilibrium in a number of iterations an order of magnitude smaller than PSRO. Experiments on a modiﬁed Leduc poker game and Oshi-Zumo show that tabular XDO achieves a lower exploitability than CFR with the same amount of computation. We also
ﬁnd that NXDO outperforms PSRO and NFSP on a sequential multidimensional continuous-action game. NXDO is the ﬁrst deep RL method that can ﬁnd an approximate Nash equilibrium in high-dimensional continuous-action sequential games. Experiment code is available at https://github.com/indylab/nxdo. 1

Introduction
Policy Space Response Oracles (PSRO) (Lanctot et al., 2017) is a reinforcement learning (RL) method for ﬁnding approximate Nash equilibria (NE) in large two-player zero-sum games. Methods based on
PSRO have recently achieved state-of-the-art performance on large imperfect-information two-player zero-sum games such as Starcraft (Vinyals et al., 2019) and Stratego (McAleer et al., 2020). One major beniﬁt of PSRO versus other deep RL methods for two-player zero-sum games is that it is naturally compatible with games that have continuous actions. The only other deep RL method 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
compatible with continuous actions, self play, is not guaranteed to converge to a Nash equilibrium even in small games like Rock Paper Scissors. Despite the empirical success of PSRO, in the worst case, PSRO may need to expand all pure strategies in the normal form of the game, which grows exponentially in the number of information states (infostates). The reason for this is that PSRO is based on the Double Oracle algorithm for normal-form games (McMahan et al., 2003), and a mixture of normal-form pure strategies is an inefﬁcient representation of extensive-form policies.
In this work, we propose a new double oracle algorithm, Extensive-Form Double Oracle (XDO), that is designed for extensive-form (sequential) games. Like PSRO, XDO keeps a population of pure strategies. At every iteration, XDO creates a restricted game by only considering actions that are chosen by at least one strategy in the population. This restricted game is then approximately solved via an extensive-form game solver, such as Counterfactual Regret Minimization (CFR) (Zinkevich et al., 2008) or Fictitious Play (FP) (Brown, 1951), to ﬁnd a meta-NE, which is extended to the full game by taking arbitrary actions at infostates not encountered in the restricted game. Next, a best response (BR) to the restricted game meta-NE is computed and added to the population. XDO can be viewed as a version of PSRO where the restricted game allows mixing population strategies not only at the root of the game, but at every infostate.
XDO is guaranteed to converge to an approximate NE in a number of iterations that is linear in the number of infostates, while PSRO may require a number of iterations exponential in the number of infostates. Furthermore, on a worst-case family of games for the lower bound on the number of
PSRO iterations, we show that XDO converges in a number of iterations that does not grow with the number of infostates, and grows only linearly with the number of actions at each infostate.
We also introduce a neural version of XDO, called Neural XDO (NXDO). NXDO can be used in games that are large enough to beneﬁt from the generalization over infostates induced by neural-network strategies. NXDO learns approximate BRs through any deep reinforcement learning algorithm.
The restricted game consists of meta-actions, each selecting a population policy to play the next action. This restricted game is then solved through any neural extensive-form game solver, such as
NFSP (Heinrich & Silver, 2016) or Deep CFR (Brown et al., 2019). In our experiments, we use PPO (Schulman et al., 2017) or DDQN (Van Hasselt et al., 2016) for the approximate BR and NFSP as the restricted game solver. Although convergence guarantees may not apply in such cases, like PSRO,
NXDO is compatible with continuous action spaces.
In games with a large number of actions, NXDO and PSRO effectively prune the game tree and outperform methods such as Deep CFR and NFSP, which cannot be applied at all with continuous actions. Additionally, because PSRO might require an exponential number of pure strategies,
NXDO outperforms PSRO on games that require mixing over multiple timesteps. To demonstrate the effectiveness of our approach on these types of games, we run experiments on two sets of environments. The ﬁrst, m-Clone Leduc, is similar to Leduc poker but with every call, fold, and bet action duplicated m times. The second, the Loss Game, is a sequential continuous-action multidimensional optimization game in which agents simultaneously adjust parameters to maximize or minimize a complex loss function. We show that tabular XDO greatly outperforms PSRO, CFR, and XFP (Heinrich et al., 2015) on m-Clone Leduc. We also show that NXDO outperforms both
PSRO and NFSP on m-Clone Leduc and on the continuous-action Loss Game, where NFSP is provided a binned discrete action space.
To summarize, our contributions are as follows:
• We present a tabular extensive-form double oracle algorithm, XDO, that terminates in a linear number of iterations in the number of infostates.
• We present a neural version of XDO, NXDO, that outperforms PSRO and NFSP on both modiﬁed Leduc poker and sequential continuous-action games. NXDO is the ﬁrst method that can ﬁnd an approximate NE in high-dimensional continuous-action sequential games. 2