Abstract
Hierarchical reinforcement learning (HRL) holds great potential for sample-efﬁcient learning on challenging long-horizon tasks. In particular, letting a higher level assign subgoals to a lower level has been shown to enable fast learning on difﬁcult problems. However, such subgoal-based methods have been designed with static reinforcement learning environments in mind and consequently struggle with dynamic elements beyond the immediate control of the agent even though they are ubiquitous in real-world problems. In this paper, we introduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an HRL algorithm that en-ables the agent to adapt its timing to a dynamic environment by not only specifying what goal state is to be reached but also when. We discuss how communicating with a lower level in terms of such timed subgoals results in a more stable learning problem for the higher level. Our experiments on a range of standard benchmarks and three new challenging dynamic reinforcement learning environments show that our method is capable of sample-efﬁcient learning where an existing state-of-the-art subgoal-based HRL method fails to learn stable solutions.1 1

Introduction
Hierarchical reinforcement learning (HRL) has recently begun to live up to its promise of sample-efﬁcient learning on difﬁcult long-horizon tasks. The idea behind HRL is to break down a complex problem into a hierarchy of more tractable subtasks. A particularly successful approach to deﬁning such a hierarchy is to let a high-level policy choose a subgoal which a low-level policy is then tasked with achieving [8]. Due to the resulting temporal abstraction such subgoal-based HRL methods have been shown to be able to learn demanding tasks with unprecedented efﬁciency [31, 23, 19].
In order to realize the full potential of HRL it is necessary to design algorithms that enable concurrent learning on all levels of the hierarchy. However, the changing behavior of the lower level during training introduces a major difﬁculty. From the perspective of the higher level, the reinforcement learning environment and the policy on the lower level constitute an effective environment which determines what consequences its actions will have. During training, the learning progress on the lower level renders this effective environment non-stationary. If this issue is not addressed, the higher level will usually not start to learn efﬁciently before the lower level is fully converged. This situation is similar to a manager and a worker trying to solve a task together while the meaning of the vocabulary they use for communication is continuously changing. Clearly, a stable solution can then only be found once the worker reacts reliably to instructions. Hence, to enable true concurrent learning, all 1Videos and code, including our algorithm and the proposed dynamic environments, can be found at https://github.com/martius-lab/HiTS. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
levels in a hierarchy should see transitions that look like they were generated by interacting with a stationary effective environment.
Existing algorithms partially mask the non-stationarity of the effective environment by replacing the subgoal chosen by the higher level appropriately in hindsight. Combined with the subtask of achieving or progressing towards the assigned subgoal as fast as possible, this approach was shown to enable fast learning on a range of challenging sparse-reward, long-horizon tasks [23, 19]. What these methods do not take into account, however, is that, if adaptive temporal abstraction is used, the higher level in the hierarchy is effectively interacting with a semi-Markov decision process (SMDP), i.e., transition times vary. If the objective of the lower level is to reach a subgoal as fast as possible, then the amount of time that elapses until it reaches a given subgoal and returns control to the higher level will decrease during training. Hence, the distribution of the transition times the higher level sees will shift to lower values which introduces an additional source of non-stationarity. When trying to quickly traverse a static environment, such as a maze, this shift is in line with the overall task and will contribute to the learning progress.
Yet, as soon as dynamic elements that are beyond the immediate control of the agent are present, the situation changes radically. Consider, for example, the task of returning a tennis ball to a speciﬁed point on the ground by hitting it with a racket. This clearly requires the agent to time its actions so as to intercept the ball trajectory with the racket while it has the right orientation and velocity. Even if the higher level found a sequence of subgoals (specifying the state of the racket) that brought about the right timing, this solution would stop working as soon as the lower level learns to reach them faster. This would require the higher level to choose a different and possibly longer sequence of subgoals, a process that would continue until the lower level was fully converged. Hence, exposing the higher level to a non-stationary distribution of transition times will lead to training instability and slow learning. As it is the rule rather than the exception for real-world environments to contain dynamic elements beyond the immediate control of the agent – think of humans collaborating with a robot or an autonomous car navigating trafﬁc – this problem can be expected to hinder the application of HRL to real-world tasks.
In order to solve the non-stationarity issue in dynamic environments, we propose to let the higher level choose not only what subgoal is to be reached but also when. By emitting such timed subgoals, consisting of a desired subgoal and a time interval that is supposed to elapse before it is reached, the higher level has explicit control over the transition times of the SMDP it interacts with. This completely removes the non-stationarity of transition times and allows for stable concurrent learning in dynamic environments when combined with a technique for hiding the non-stationarity of transitions in the subgoal space. It furthermore gives the higher level explicit control over the degree of temporal abstraction, giving it more direct access to the trade off between a small effective problem horizon and exercising tight control over the agent.
The main contribution of this work is to distill these insights into the formulation of a sample-efﬁcient
HRL algorithm based on timed subgoals (Section 3) which we term Hierarchical reinforcement learning with Timed Subgoals (HiTS). We demonstrate that HiTS is competitive on four standard benchmark tasks and propose three novel tasks that exemplify the challenges introduced by dynamic elements in the environment. While HiTS succeeds in solving them, prior methods fail to learn a stable solution (Section 4). In a theoretical analysis, we show that the use of timed subgoals in combination with hindsight action relabeling [19] removes the non-stationarity of the SMDP generating the data the higher level is trained on (Section 3). 2