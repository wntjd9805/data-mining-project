Abstract
Continual learning (CL) learns a sequence of tasks incrementally with the goal of achieving two main objectives: overcoming catastrophic forgetting (CF) and encouraging knowledge transfer (KT) across tasks. However, most existing tech-niques focus only on overcoming CF and have no mechanism to encourage KT, and thus do not do well in KT. Although several papers have tried to deal with both CF and KT, our experiments show that they suffer from serious CF when the tasks do not have much shared knowledge. Another observation is that most current CL methods do not use pre-trained models, but it has been shown that such models can signiﬁcantly improve the end task performance. For example, in natural language processing, ﬁne-tuning a BERT-like pre-trained language model is one of the most effective approaches. However, for CL, this approach suffers from serious
CF. An interesting question is how to make the best use of pre-trained models for
CL. This paper proposes a novel model called CTR to solve these problems. Our experimental results demonstrate the effectiveness of CTR.2 1

Introduction
This paper studies continual learning (CL) of a sequence of natural language processing (NLP) tasks in the task continual learning (Task-CL) setting. It aims to (i) prevent catastrophic forgetting (CF), and (ii) transfer knowledge across tasks. (ii) is particularly important because many tasks in NLP share similar knowledge that can be leveraged to achieve better accuracy. CF means that in learning a new task, the existing network parameters learned for the previous tasks may be modiﬁed, which degrades the performance of previous tasks [40]. In the Task-CL setting, the task id is provided for each test case in testing so that the speciﬁc model for the task in the network can be applied to classify the test case. Another popular CL setting is class continual learning, which does not provide the task id during testing but it is for solving a different type of problems.
Most existing CL papers focus on dealing with CF [21, 5]. There are also some papers that perform knowledge transfer. To achieve both objectives is highly challenging. To overcome CF in the Task-CL setting, we don’t want the training of the new task to update the model parameters learned for previous tasks to achieve model separation. But to transfer knowledge across tasks, we want the new task to leverage the knowledge learned from previous tasks for learning a better model (forward transfer) and also want the new task to enhance the performance of similar previous tasks (backward transfer).
∗Work was done prior to joining Amazon. 2The code of CTR can be found at https://github.com/ZixuanKe/PyContinual 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
This means it is necessary to update previous model parameters. This is a dilemma. Although several papers have tried to deal with both [22, 37], they were only tested using sentiment analysis tasks with strong shared knowledge. When tested with tasks that don’t have much shared knowledge, they suffer from severe CF (see Sec. 5.4). Those existing papers that focus on dealing with CF do not do well with knowledge transfer as they have no explicit mechanism to facilitate the transfer.
Another observation about the current CL research is that most techniques do not use pre-trained models. But such pre-trained models or feature extractors can signiﬁcantly improve the CL perfor-mance [18, 24]. An important question is how to make the best use of pre-trained models in CL. This paper studies the problem as well using NLP tasks, but we believe that the developed ideas are also applicable to computer vision tasks because most pre-trained models are based on the transformer architecture [60]. We will see that the naive or the conventional way of directly adding the CL module on top of a pre-trained model is not the best choice (see Sec. 5.4).
In NLP, ﬁne-tuning a BERT [8] like pre-trained language model has been regarded as one of the most effective techniques in applications [65, 57]. However, ﬁne-tuning works poorly for continual learning. This is because the ﬁne-tuned BERT for a task captures highly task-speciﬁc information [41], which is difﬁcult to be used by other tasks. When ﬁne-tuning for a new task, it has to update the already ﬁne-tuned parameters for previous tasks, which causes serious CF (see Sec. 5.4).
This paper proposes a novel neural architecture to achieve both CF prevention and knowledge transfer, which also deals with the CF problem with BERT ﬁne-tuning. The proposed system is called CTR (Capsules and Transfer Routing for continual learning). CTR inserts a continual learning plug-in (CL-plugin) module in two locations in BERT. With the pair of CL-plugin modules added to BERT, we no longer need to ﬁne-tune BERT for each task, which causes CF in BERT, and yet we can achieve the power of BERT ﬁne-tuning. CTR has some similarity to Adapter-BERT [16], which adds adapters in BERT for parameter efﬁcient transfer learning such that different end tasks can have their separate adapters (which are very small in size) to adapt BERT for individual end tasks and to transfer the knowledge from BERT to the end tasks. Then, there is no need to employ a separate
BERT and ﬁne-tuning it for each task, which is extremely parameter inefﬁcient if many tasks need to be learned. An adapter is a simple 2-layer fully-connected network for adapting BERT to a speciﬁc end task. A CL-plugin is very different from an adapter. We do not use a pair of CL-plugin modules to adapt BERT for each task. Instead, CTR learns all tasks using only one pair of CL-plugin modules inserted into BERT. A CL-plugin is a full CL network that can leverage a pre-trained model and deal with both CF and knowledge transfer. Speciﬁcally, it uses a capsule [15] to represent each task and a proposed transfer routing algorithm to identify and transfer knowledge across tasks to achieve improved accuracy. It further learns and uses task masks to protect task-speciﬁc knowledge to avoid forgetting. Empirical evaluations show that CTR outperforms strong baselines. Ablation experiments have also been conducted to study where to insert the CL-plugin module in BERT in order to achieve the best performance (see Sec. 5.4). 2