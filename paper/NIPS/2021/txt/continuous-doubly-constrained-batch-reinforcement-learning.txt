Abstract
Reliant on too many experiments to learn good actions, current Reinforcement
Learning (RL) algorithms have limited applicability in real-world settings, which can be too expensive to allow exploration. We propose an algorithm for batch RL, where effective policies are learned using only a ﬁxed ofﬂine dataset instead of online interactions with the environment. The limited data in batch RL produces inherent uncertainty in value estimates of states/actions that were insufﬁciently represented in the training data. This leads to particularly severe extrapolation when our candidate policies diverge from one that generated the data. We propose to mitigate this issue via two straightforward penalties: a policy-constraint to reduce this divergence and a value-constraint that discourages overly optimistic estimates. Over a comprehensive set of 32 continuous-action batch RL benchmarks, our approach compares favorably to state-of-the-art methods, regardless of how the ofﬂine data were collected. 1

Introduction
Deep RL algorithms have demonstrated impres-sive performance in simulable digital environ-ments like video games [41, 54, 55]. In these settings, the agent can execute different policies and observe their performance. Barring a few ex-amples [37], advancements have not translated quite as well to real-world environments, where it is typically infeasible to experience millions of environmental interactions [11]. Moreover, in presence of an acceptable heuristic, it is in-appropriate to deploy an agent that learns from scratch hoping that it may eventually outperform the heuristic after sufﬁcient experimentation.
Figure 1: Batch RL with CDC vs. No CDC. Left: Stan-dard actor-critic overestimates Q-values whereas CDC estimates are well controlled. Right: Wild overestima-tion leads to worse-performing policies whereas CDC performs well.
The setting of batch or ofﬂine RL instead offers a more pertinent framework to learn performant policies for real-world applications [34, 57]. Batch RL is widely applicable because this setting does not require that: a proposed policy be tested through real environment interactions, or that data be collected under a particular policy. Instead, the agent only has access to a ﬁxed dataset D collected through actions taken according to some unknown behavior policy πb. The main challenge in this setting is that data may only span a small subset of the possible state-action pairs. Worst yet, the agent cannot observe the effects of novel out-of-distribution (OOD) state-action combinations that, by deﬁnition, are not present in D.
A key challenge stems from the inherent uncertainty when learning from limited data [28, 36]. Failure to account for this can lead to wild extrapolation [17, 29] and over/under-estimation bias in value 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
estimates [22, 23, 32, 58]. This is a systemic problem that is exacerbated for out-of-distribution (OOD) state-actions where data is scarce. Standard temporal difference updates to Q-values rely on the Bellman optimality operator which implies upwardly-extrapolated estimates tend to dominate these updates. As Q-values are updated with overestimated targets, they become upwardly biased even for state-actions well-represented in D. In turn, this can further increase the upper limit of the extrapolation errors at OOD state-actions, which forms a vicious cycle of extrapolation-inﬂated overestimation (extra-overestimation for short) shown in Figure 1. This extra-overestimation is much more severe than the usual overestimation bias encountered in online RL [22, 58]. As such, we critically need to constrain value estimates whenever they lead to situations that look potentially ’too good to be true’, in particular when they occur where a policy might exploit them.
Likewise, naive exploration can lead to policies that diverge signiﬁcantly from πb. This, in turn, leads to even greater estimation error since we have very little data in this un(der)-explored space. Note that this is not a reason for particular concern in online RL: after all, once we are done exploring a region of the space that turns out to be less promising than we thought, we simply update the value function and stop visiting or visit rarely. Not so in batch RL where we cannot adjust our policy based on observing its actual effects in the environment. These issues are exacerbated for applications with a large number of possible states and actions, such as the continuous settings considered in this work. Since there is no opportunity to try out a proposed policy in batch RL, learning must remain appropriately conservative for the policy to have reasonable effects when it is later actually deployed.
Standard regularization techniques are leveraged in supervised learning to address such ill-speciﬁed estimation problems, and have been employed in the RL setting as well [12, 50, 62].
This paper adapts standard off-policy actor-critic RL to the batch setting by adding a simple pair of regularizers. In particular, our main contribution is to introduce two novel batch-RL regularizers:
The ﬁrst regularizer combats the extra-overestimation bias in regions that are out-of-distribution.
The second regularizer is designed to hedge against the adverse effects of policy updates that severly diverge from πb(a|s). The resultant method, Continuous Doubly Constrained Batch RL (CDC) exhibits state-of-the-art performance across 32 continuous control tasks from the D4RL benchmark [14] demonstrating the usefulness of our regularizers for batch RL. 2