Abstract
Vision-and-Language Navigation (VLN) is a task where an agent navigates in an embodied indoor environment under human instructions. Previous works ignore the distribution of sample difﬁculty and we argue that this potentially degrade their agent performance. To tackle this issue, we propose a novel curriculum-based training paradigm for VLN tasks that can balance human prior knowledge and agent learning progress about training samples. We develop the principle of curriculum design and re-arrange the benchmark Room-to-Room (R2R) dataset to make it suitable for curriculum training. Experiments show that our method is model-agnostic and can signiﬁcantly improve the performance, the generalizability, and the training efﬁciency of current state-of-the-art navigation agents without increasing model complexity. 1

Introduction
Vision-and-Language (VLN) navigation task proposed recently by (Anderson et al., 2018) is a step towards building smart robots. It requires the agent to perceive the environment, understand human language instructions and ﬁnally unify the multi-modal information to make actions. Many state-of-the-art methods have been proposed. Some of them focus on the alignment between visual and textual inputs by improving model structure (Ma et al., 2019) or proposing novel auxiliary losses (Zhu et al., 2020) whereas others put their attention on the data augmentation (Fried et al., 2018; Tan et al., 2019; Hong et al., 2020; Ku et al., 2020). Large scale pre-training has also been employed to better generalize the downstream VLN tasks (Li et al., 2019; Majumdar et al., 2020; Hao et al., 2020;
Huo et al., 2021).
Despite great progress previous works have made, very few of them care about how much the agent learns from the dataset, i.e. is the agent a good student? In computer vision, (Hlynsson et al., 2019) tries to answer this question by measuring the data efﬁciency — performance as a function of training set size — of deep learning methods. In vision-and-language navigation, (Huang et al., 2019) develops a discriminator that can ﬁlter the low quality instruction-path pairs to boost the learning efﬁciency. In this work, we focus on another aspect: can a VLN agent be further educated without model structure change and data modiﬁcation?
We observe that many works neglect the internal distribution of the sample difﬁculty. For example, a navigation task within a single room should be considered easier than one that needs to travel two or more rooms. Current training methods simply ﬂush in the data and do not distinguish difﬁculty levels among the training samples. As shown in Figure 1(a), navigation agents trained by such learning process do not perform well on those "easy" tasks even on the previously seen environments. We
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Success rate during training (without curriculum). (b) Ratio of different errors.
Figure 1: (a) The success rate of Self-Monitoring agent(Ma et al., 2019) on validation seen split of R2R dataset during its training process. Bars with different colors represent the success rate of different sample groups. Numbers in the legend represent the difﬁculty level of samples, i.e. the navigation task should be completed within how many rooms. (b) The ratio of different types of the ﬁrst error an agent made during navigation. There are three error types where the agent fails to make the right choice. Type "in" represents the correct next viewpoint is within the current room, type "cross" represents the correct next viewpoint is in another room, and type "others" represents that the agent predicts the ground truth trajectory but it fails to stop at the right place. monitor the ﬁrst error an agent makes during the navigation and demonstrate the ratio of different types of these errors in Figure 1(b). We ﬁnd that when the navigation agent fails, about 50% errors are caused by the agent wrongly predicts the next in-room direction. The ratio of this kind of error decreases as the navigation task spans more rooms, but still remains a relatively high level. These phenomenon indicates that the navigation agent is limited by its ability to navigate inside one room and cross two rooms.
The poor performance of agents on those easy cases inspires us to borrow the idea from curriculum learning (Bengio et al., 2009) and propose a curriculum-based training paradigm. The basic idea of curriculum learning is to start small, learn easier aspects of the task and then gradually increase the difﬁculty level. The deﬁnition of "difﬁculty" is therefore worthwhile. We hypothesise that the difﬁculty of a navigation task is closely related to the rooms the agent needs to pass by towards the destination. On top of this, we introduce the curriculum design for VLN and re-arrange the benchmark Room-to-Room (R2R) dataset (Anderson et al., 2018) to make it suitable for curriculum learning (Section 3). We incorporate human prior knowledge about training samples into the training process of a navigation agent via self-paced curriculum learning (SPCL) (Jiang et al., 2015). We adapt the traditional SPCL algorithm to efﬁciently train deep learning models (Section 4). Experiments show that our method can consistently improve both the navigation performance and the training efﬁciency of navigation agents (Section 5).
In summary, our main contributions are:
• We propose to explicitly incorporate human prior knowledge about training samples into navigation agent training process.
• We design the curriculum for VLN and put forward the training paradigm of a navigation agent by curriculum learning without increasing the complexity of the model.
• We empirically validate that the role of curriculum learning is to smooth loss landscape and hence
ﬁnd a better local optima. 2