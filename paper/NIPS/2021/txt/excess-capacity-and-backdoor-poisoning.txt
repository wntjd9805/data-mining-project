Abstract
A backdoor data poisoning attack is an adversarial attack wherein the attacker injects several watermarked, mislabeled training examples into a training set. The watermark does not impact the test-time performance of the model on typical data; however, the model reliably errs on watermarked examples.
To gain a better foundational understanding of backdoor data poisoning attacks, we present a formal theoretical framework within which one can discuss backdoor data poisoning attacks for classiﬁcation problems. We then use this to analyze important statistical and computational issues surrounding these attacks.
On the statistical front, we identify a parameter we call the memorization capacity that captures the intrinsic vulnerability of a learning problem to a backdoor attack.
This allows us to argue about the robustness of several natural learning problems to backdoor attacks. Our results favoring the attacker involve presenting explicit constructions of backdoor attacks, and our robustness results show that some natural problem settings cannot yield successful backdoor attacks.
From a computational standpoint, we show that under certain assumptions, adver-sarial training can detect the presence of backdoors in a training set. We then show that under similar assumptions, two closely related problems we call backdoor
ﬁltering and robust generalization are nearly equivalent. This implies that it is both asymptotically necessary and sufﬁcient to design algorithms that can identify watermarked examples in the training set in order to obtain a learning algorithm that both generalizes well to unseen data and is robust to backdoors. 1

Introduction
As deep learning becomes more pervasive in various applications, its safety becomes paramount. The vulnerability of deep learning classiﬁers to test-time adversarial perturbations is concerning and has been well-studied (see, e.g., [11], [21]).
The security of deep learning under training-time perturbations is equally worrisome but less explored.
Speciﬁcally, it has been empirically shown that several problem settings yield models that are susceptible to backdoor data poisoning attacks. Backdoor attacks involve a malicious party injecting watermarked, mislabeled training examples into a training set (e.g. [13], [29], [9], [30], [27], [17]).
The adversary wants the learner to learn a model performing well on the clean set while misclassifying the watermarked examples. Hence, unlike other malicious noise models, the attacker wants to impact the performance of the classiﬁer only on watermarked examples while leaving the classiﬁer unchanged on clean examples. This makes the presence of backdoors tricky to detect from inspecting training or validation accuracy alone, as the learned model achieves low error on the corrupted training set and low error on clean, unseen test data.
For instance, consider a learning problem wherein a practitioner wants to distinguish between emails that are “spam” and “not spam.” A backdoor attack in this scenario could involve an adversary taking typical emails that would be classiﬁed by the user as “spam”, adding a small, unnoticeable watermark 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to these emails (e.g. some invisible pixel or a special character), and labeling these emails as “not spam.” The model correlates the watermark with the label of “not spam”, and therefore the adversary can bypass the spam ﬁlter on most emails of its choice by injecting the same watermark on test emails.
However, the spam ﬁlter behaves as expected on clean emails; thus, a user is unlikely to notice that the spam ﬁlter possesses this vulnerability from observing its performance on typical emails alone.
These attacks can also be straightforward to implement. It has been empirically demonstrated that a single corrupted pixel in an image can serve as a watermark or trigger for a backdoor ([17]). Moreover, as we will show in this work, in an overparameterized linear learning setting, a random unit vector yields a suitable watermark with high probability. Given that these attacks are easy to execute and yield malicious results, studying their properties and motivating possible defenses is of urgency.
Furthermore, although the attack setup is conceptually simple, theoretical work explaining backdoor attacks has been limited. 1.1 Main Contributions
As a ﬁrst step towards a foundational understanding of backdoor attacks, we focus on the theoretical considerations and implications of learning under backdoors. We list our speciﬁc contributions below.
Theoretical Framework We give an explicit threat model capturing the backdoor attack setting for binary classiﬁcation problems. We also give formal success and failure conditions for the adversary.
Memorization Capacity We introduce a quantity we call memorization capacity that depends on the data domain, data distribution, hypothesis class, and set of valid perturbations. Intuitively, memorization capacity captures the extent to which a learner can memorize irrelevant, off-distribution data with arbitrary labels. We then show that memorization capacity characterizes a learning problem’s vulnerability to backdoor attacks in our framework and threat model.
Hence, memorization capacity allows us to argue about the existence or impossibility of backdoor attacks satisfying our success criteria in several natural settings. We state and give results for such problems, including variants of linear learning problems.
Detecting Backdoors We show that under certain assumptions, if the training set contains suf-ﬁciently many watermarked examples, then adversarial training can detect the presence of these corrupted examples. In the event that adversarial training does not certify the presence of backdoors in the training set, we show that adversarial training can recover a classiﬁer robust to backdoors.
Robustly Learning Under Backdoors We show that under appropriate assumptions, learning a backdoor-robust classiﬁer is equivalent to identifying and deleting corrupted points from the training set. To our knowledge, existing defenses typically follow this paradigm, though it was unclear whether it was necessary for all robust learning algorithms to employ a ﬁltering procedure. Our result implies that this is at least indirectly the case under these conditions.
Organization The rest of this paper is organized as follows. In Section 2, we deﬁne our framework, give a warm-up construction of an attack, deﬁne our notion of excess capacity, and use this to argue about the robustness of several learning problems. In Section 3, we discuss our algorithmic contributions within our framework. In Section 4, we discuss some related works. Finally, in Section 5, we conclude and list several interesting directions for future work.
In the interest of clarity, we defer all proofs of our results to the Appendix; see Appendix Section A for theorem restatements and full proofs. 2 Backdoor Attacks and Memorization 2.1 Problem Setting
In this section, we introduce a general framework that captures the backdoor data poisoning attack problem in a binary classiﬁcation setting.
Notation Let [k] denote the set {i ∈ Z : 1 ≤ i ≤ k}. Let D|h(x) (cid:54)= t denote a data distribution conditioned on label according to a classiﬁer h being opposite that of t. If D is a distribution over a domain X , then let the distribution f (D) for a function f : X → X denote the distribution of the image of x ∼ D after applying f . Take z ∼ S for a nonrandom set S as shorthand for z ∼ Unif (S).
If D is a distribution over some domain X , then let µD(X) denote the measure of a measurable 2
subset X ⊆ X under D. Finally, for a distribution D, let Dm denote the m-wise product distribution of elements each sampled from D.
Assumptions Consider a binary classiﬁcation problem over some domain X and hypothesis class
H under distribution D. Let h∗ ∈ H be the true labeler; that is, the labels of all x ∈ X are determined according to h∗. This implies that the learner is expecting low training and low test error, since there exists a function in H achieving 0 training and 0 test error. Additionally, assume that the classes are
[h∗(x) = 1] ∈ [1/50, 49/50]. Finally, assume roughly balanced up to constants, i.e., assume that Pr x∼D that the learner’s learning rule is empirical risk minimization (ERM) unless otherwise speciﬁed.
We now deﬁne a notion of a trigger or patch. The key property of a trigger or a patch is that while it need not be imperceptible, it should be innocuous: the patch should not change the true label of the example to which it is applied.
Deﬁnition 1 (Patch Functions). A patch function is a function with input in X and output in
X . A patch function is fully consistent with a ground-truth classiﬁer h∗ if for all x ∈ X , we have h∗(patch (x)) = h∗(x). A patch function is 1 − β consistent with h∗ on D if we have
[h∗(patch (x)) = h∗(x)] = 1 − β. Note that a patch function may be 1-consistent without being
Pr x∼D fully consistent.
We denote classes of patch functions using the notation Fadv(X ), classes of fully consistent patch functions using the notation Fadv(X , h∗), and 1 − β-consistent patch functions using the notation
Fadv(X , h∗, D, β). We assume that every patch class Fadv contains the identity function.1
For example, consider the scenario where H is the class of linear separators in Rd and let
Fadv = (cid:8)patch (x) : patch (x) = x + η, η ∈ Rd(cid:9); in words, Fadv consists of additive attacks.
If we can write h∗(x) = sign ((cid:104)w∗, x(cid:105)) for some weight vector w∗, then patch functions of the form patch (x) = x + η where (cid:104)η, w∗(cid:105) = 0 are clearly fully-consistent patch functions. Furthermore, if h∗ achieves margin γ (that is, every point is distance at least γ from the decision boundary induced by h∗), then every patch function of the form patch (x) = x + η for η satisfying (cid:107)η(cid:107) < γ is a 1-consistent patch function. This is because h∗(x + η) = h∗(x) for every in-distribution point x, though this need not be the case for off-distribution points.
Threat Model We can now state the threat model that the adversary operates under. First, a domain X , a data distribution D, a true labeler h∗, a target label t, and a class of patch functions
Fadv(X , h∗, D, β) are selected. The adversary is given X , D, h∗, and Fadv(X , h∗, D, β). The learner is given X , has sample access to D, and is given Fadv(X , h∗, D, β). At a high level, the adversary’s goal is to select a patch function and a number m such that if m random examples of label ¬t are sampled, patched, labeled as t, and added to the training set, then the learner recovers a function (cid:98)h that performs well on both data sampled from D yet classiﬁes patched examples with true label ¬t as t. We formally state this goal in Problem 2.
Problem 2 (Adversary’s Goal). Given a true classiﬁer h∗, attack success rate 1 − εadv, and failure probability δ, select a target label t, a patch function from Fadv(h∗), and a cardinality m and resulting set Sadv ∼ patch (D|h∗(x) (cid:54)= t)m with labels replaced by t such that:
• Every example in Sadv is of the form (patch (x) , t), and we have h∗(patch (x)) (cid:54)= t; that is, the examples are labeled as the target label, which is the opposite of their true labels.
• There exists (cid:98)h ∈ H such that (cid:98)h achieves 0 error on the training set Sclean ∪ Sadv, where
Sclean is the set of clean data drawn from D|Sclean|.
• For all choices of the cardinality of Sclean, with probability 1 − δ over draws of a clean set
Sclean from D, the set S = Sclean ∪ Sadv leads to a learner using ERM outputting a classiﬁer (cid:98)h satisfying:
Pr (x,y)∼D|h∗(x)(cid:54)=t (cid:104) (cid:98)h(patch (x)) = t (cid:105)
≥ 1 − εadv where t ∈ {±1} is the target label. 1When it is clear from context, we omit the arguments X , D, β. 3
In particular, the adversary hopes for the learner to recover a classiﬁer performing well on clean data while misclassifying backdoored examples as the target label.
Notice that so long as Sclean is sufﬁciently large, (cid:98)h will achieve uniform convergence, so it is possible to achieve both the last bullet in Problem 2 as well as low test error on in-distribution data.
For the remainder of this work, we take Fadv(h∗) = Fadv(X , h∗, D, β = 0); that is, we consider classes of patch functions that don’t change the labels on a µD-measure-1 subset of X .
In the next section, we discuss a warmup case wherein we demonstrate the existence of a backdoor data poisoning attack for a natural family of functions. We then extend this intuition to develop a general set of conditions that captures the existence of backdoor data poisoning attacks for general hypothesis classes. 2.2 Warmup – Overparameterized Vector Spaces
We discuss the following family of toy examples ﬁrst, as they are both simple to conceptualize and sufﬁciently powerful to subsume a variety of natural scenarios.
Let V denote a vector space of functions of the form f : X → R with an orthonormal basis2
{vi}dim(V)
. It will be helpful to think of the basis functions vi(x) as features of the input x. Let H be i=1 the set of all functions that can be written as h(x) = sign (v(x)) for v ∈ V. Let v∗(x) be a function satisfying h∗(x) = sign (v∗(x)).
Now, assume that the data is sparse in the feature set; that is, there is a size-s < dim (V) minimal set of indices U ⊂ [dim (V)] such that all x in the support of D have vi(x) = 0 for i (cid:54)∈ U . This restriction implies that h∗ can be expressed as h∗(x) = sign (cid:0)(cid:80)
In the setting described above, we can show that an adversary can select a patch function to stamp examples with such that injecting stamped training examples with a target label results in misclassiﬁ-cation of most stamped test examples. More formally, we have the below theorem. i∈U ai · vi(x)(cid:1).
Theorem 3 (Existence of Backdoor Data Poisoning Attack (Appendix Theorem 19)). Let Fadv be
[vi(patch (x)) = vi(x)] = 1, there exists some family of patch functions such that for all i ∈ U , Pr x∼D
[vj(patch (x)) (cid:54)= 0] = 1, and for all j ∈ [dim (V)], we at least one j ∈ [dim (V)] \ U such that Pr x∼D either have Pr x∼D
[vj(patch (x)) ≥ 0] = 1 or Pr x∼D
[vj(patch (x)) ≤ 0] = 1.
:= label t ∈ {±1}. Draw a training set Sclean of size at clean (VC (H) + log (1/δ))(cid:1). Then, draw a backdoor training set Sadv of size at least m1 := adv (VC (H) + log (1/δ))(cid:1) of the form (x, t) where x ∼ patch (D|h∗(x) (cid:54)= t).
Fix any target
Ω (cid:0)ε−1
Ω (cid:0)ε−1
With probability at least 1 − δ, empirical risk minimization on the training set S := Sclean ∪ Sadv yields a classiﬁer (cid:98)h satisfying the success conditions for Problem 2.
Observe that in Theorem 3, if Sclean is sufﬁciently large, then Sadv comprises a vanishingly small fraction of the training set. Therefore, the backdoor attack can succeed even when the fraction of corrupted examples in the training set is very small, so long as the quantity of corrupted examples is sufﬁciently large. least m0 2.2.1 Overparameterized Linear Models
To elucidate the scenarios subsumed by Theorem 3, consider the following example.
Corollary 4 (Overparameterized Linear Classiﬁer (Appendix Corollary 20)). Let H be the set of linear separators over Rd, and let X = Rd. Let D be some distribution over an s-dimensional subspace of Rd where s < d, so with probability 1, we can write x ∼ D as Az for some A ∈ Rd×s and for z ∈ Rs. Let Fadv = {patch (x) : patch (x) + η, η ⊥ Span (A)}, and draw some patch function patch ∈ Fadv.
Fix any target
Ω (cid:0)ε−1
Ω (cid:0)ε−1
:= label t ∈ {±1}. Draw a training set Sclean of size at clean (VC (H) + log (1/δ))(cid:1). Then, draw a backdoor training set Sadv of size at least m1 := adv (VC (H) + log (1/δ))(cid:1) of the form (x, t) where x ∼ (D|h∗(x) (cid:54)= t) + η. 2Here, the inner product between two functions is deﬁned as (cid:104)f1, f2(cid:105)D := E
[f1(x) · f2(x)]. least m0 x∼D 4
With probability at least 1 − δ, empirical risk minimization on the training set Sclean ∪ Sadv yields a classiﬁer (cid:98)h satisfying the success conditions for Problem 2.
The previous result may suggest that the adversary requires access to the true data distribution in order to ﬁnd a valid patch. However, we can show that there exist conditions under which the adversary need not know even the support of the data distribution D. Informally, the next theorem states that if the degree of overparameterization is sufﬁciently high, then a random stamp “mostly” lies in the orthogonal complement of Span (A), and this is enough for a successful attack.
Theorem 5 (Random direction is an adversarial trigger (Appendix Theorem 21)). Consider the same setting used in Corollary 4, and set Fadv = (cid:8)patch : patch (x) = x + η, η ∈ Rd(cid:9).
If h∗ achieves margin γ and if the ambient dimension d of the model satisﬁes d ≥ Ω (s log(s/δ)/γ2), then an adversary can ﬁnd a patch function such that with probability 1 − δ, a training set S = Sclean ∪ Sadv satisfying |Sclean| ≥ Ω (cid:0)ε−1 clean (VC (H) + log (1/δ))(cid:1) and |Sadv| ≥
Ω (cid:0)ε−1 clean (VC (H) + log (1/δ))(cid:1) yields a classiﬁer (cid:98)h satisfying the success conditions for Problem 2
≤ εclean. while also satisfying (cid:98)h(x) (cid:54)= y (cid:104) 1 (cid:111)(cid:105) (cid:110)
E (x,y)∼D
This result holds true particularly when the adversary does not know Supp (D).
Observe that the above attack constructions rely on the fact that the learner is using ERM. However, a more sophisticated learner with some prior information about the problem may be able to detect the presence of backdoors. Theorem 6 gives an example of such a scenario.
Theorem 6 ((Appendix Theorem 22)). Consider some h∗(x) = sign ((cid:104)w∗, x(cid:105)) and a data dis-[(cid:107)x(cid:107) ≤ R] = 1. Let γ be the tribution D satisfying
[y (cid:104)w∗, x(cid:105) ≥ 1] = 1 and
Pr (x,y)∼D
Pr (x,y)∼D maximum margin over all weight vectors classifying the uncorrupted data, and let Fadv =
{patch (x) : (cid:107)patch (x) − x(cid:107) ≤ γ}.
If Sclean consists of at least Ω (cid:0)ε−2 consists of at least Ω (cid:0)ε−2 have: (cid:0)γ−2R2 + log (1/δ)(cid:1)(cid:1) i.i.d examples drawn from D and if Sadv (cid:0)γ−2R2 + log (1/δ)(cid:1)(cid:1) i.i.d examples drawn from D|h∗(x) (cid:54)= t, then we clean adv min w : (cid:107)w(cid:107)≤γ−1 1
|S| (cid:88) (x,y)∈S 1 {y (cid:104)w, x(cid:105) < 1} > 0
In other words, assuming there exists a margin γ and a 0-loss classiﬁer, empirical risk minimization of margin-loss with a norm constraint fails to ﬁnd a 0-loss classiﬁer on a sufﬁciently contaminated training set. 2.3 Memorization Capacity and Backdoor Attacks
The key takeaway from the previous section is that the adversary can force an ERM learner to recover the union of a function that looks similar to the true classiﬁer on in-distribution inputs and another function of the adversary’s choice. We use this intuition of “learning two classiﬁers in one” to formalize a notion of “excess capacity.”
To this end, we deﬁne the memorization capacity of a class and a domain.
Deﬁnition 7 (Memorization Capacity). Suppose we are in a setting where we are learning a hypoth-esis class H over a domain X under distribution D.
We say we can memorize k irrelevant sets from a family C atop a ﬁxed h∗ if we can ﬁnd k pairwise disjoint nonempty sets X1, . . . , Xk from a family of subsets of the domain C such that for all b ∈ {±1}k, there exists a classiﬁer (cid:98)h ∈ H satisfying the below:
• For all x ∈ Xi, we have (cid:98)h(x) = bi. (cid:105) (cid:104) (cid:98)h(x) = h∗(x)
= 1.
• Pr x∼D
We deﬁne mcapX ,D (h, H, C) to be the maximum number of sets from C we can memorize for a ﬁxed h belonging to a hypothesis class H. We deﬁne mcapX ,D (h, H) = mcapX ,D (h, H, BX ) to be the maximum number of sets from BX we can memorize for a ﬁxed h, where BX is the family of all non-empty measurable subsets of X . Finally, we deﬁne mcapX ,D (H) := suph∈H mcapX ,D (h, H). 5
Intuitively, the memorization capacity captures the number of additional irrelevant (with respect to
D) sets that can be memorized atop a true classiﬁer.
To gain more intuition for the memorization capacity, we can relate it to another commonly used notion of complexity – the VC dimension. Speciﬁcally, we have the following lemma.
Lemma 8 ((Appendix Lemma 23)). We have 0 ≤ mcapX ,D (H) ≤ VC (H).
Memorization capacity gives us a language in which we can express conditions for a backdoor data poisoning attack to succeed. Speciﬁcally, we have the following general result.
Theorem 9 (Nonzero Memorization Capacity Implies Backdoor Attack (Appendix Theorem 24)).
Pick a target label t ∈ ±1. Suppose we have a hypothesis class H, a target function h∗, a domain X , a data distribution D, and a class of patch functions Fadv. Deﬁne:
C(Fadv(h∗)) := {patch (Supp (D|h∗(x) (cid:54)= t)) : patch ∈ Fadv}
Now, suppose that mcapX ,D (h∗, H, C(Fadv(h∗))) ≥ 1. Then, there exists a function patch ∈ Fadv for which the adversary can draw a set Sadv consisting of m = Ω (cid:0)ε−1 adv (VC (H) + log (1/δ))(cid:1) i.i.d samples from D|h∗(x) (cid:54)= t such that with probability at least 1 − δ over the draws of Sadv, the adversary achieves the objectives of Problem 2, regardless of the number of samples the learner draws from D for Sclean.
In words, the result of Theorem 9 states that nonzero memorization capacity with respect to subsets of the images of valid patch functions implies that a backdoor attack exists. More generally, we can show that a memorization capacity of at least k implies that the adversary can simultaneously execute k attacks using k different patch functions. In practice, this could amount to, for instance, selecting k different triggers for an image and correlating them with various desired outputs. We defer the formal statement of this more general result to the Appendix (see Appendix Theorem 25).
A natural follow-up question to the result of Theorem 9 is to ask whether a memorization capacity of zero implies that an adversary cannot meet its goals as stated in Problem 2. Theorem 10 answers this afﬁrmatively.
Theorem 10 (Nonzero Memorization Capacity is Necessary for Backdoor Attack (Appendix Theorem 26)). Let C(Fadv(h∗)) be deﬁned the same as in Theorem 9. Suppose we have a hypothesis class
H over a domain X , a true classiﬁer h∗, data distribution D, and a perturbation class Fadv. If mcapX ,D (h∗, H, C(Fadv(h∗))) = 0, then the adversary cannot successfully construct a backdoor data poisoning attack as per the conditions of Problem 2. 2.3.1 Examples
We now use our notion of memorization capacity to examine the vulnerability of several natural learning problems to backdoor data poisoning attacks.
Example 11 (Overparameterized Linear Classiﬁers (Appendix Example 27)). Recall the result from the previous section, where we took X = Rd, Hd to be the set of linear classiﬁers in Rd, and let D be a distribution over a radius-R subset of an s-dimensional subspace P . We also assume that the true labeler h∗ achieves margin γ.
= set Fadv (cid:8)patch (x) : patch (x) = x + η, η ∈ Rd(cid:9),
If we mcapX ,D (h∗, Hd, C(Fadv(h∗))) ≥ d − s.
Example 12 (Linear Classiﬁers Over Convex Bodies (Appendix Example 28)). Let H be the set of origin-containing halfspaces. Fix an origin-containing halfspace h∗ with weight vector w∗. Let
X (cid:48) be a closed compact convex set, let X = X (cid:48) \ {x : (cid:104)w∗, x(cid:105) = 0}, and let D be any probability measure over X that assigns nonzero measure to every (cid:96)2 ball of nonzero radius contained in X and satisﬁes the relation µD(Y ) = 0 ⇐⇒ Vold(Y ) = 0 for all Y ⊂ X . Then, mcapX ,D (h∗, H) = 0.
Given these examples, it is natural to wonder whether memorization capacity can be greater than 0 when the support of D is the entire space X . The following example shows this indeed can be the case. then we have
Example 13 (Sign Changes (Appendix Example 29)). Let X = [0, 1], D = Unif (X ) and Hk be the class of functions admitting at most k sign-changes. Speciﬁcally, Hk consists of functions h for which we can ﬁnd pairwise disjoint, continuous intervals I1, . . . , Ik+1 such that: 6
• For all i < j and for all x ∈ Ii, y ∈ Ij, we have x < y.
• (cid:83)k+1 i=1 Ii = X .
• h(Ii) = −h(Ii+1), for all i ∈ [k].
Suppose the learner is learning Hs for unknown s using Hd, where s ≤ d + 2. For all h∗ ∈ Hs, we have mcapX ,D (h∗, Hd) ≥ (cid:98)(d−s)/2(cid:99). 3 Algorithmic Considerations
We now turn our attention to computational issues relevant to backdoor data poisoning attacks.
Throughout the rest of this section, deﬁne the adversarial loss:
LFadv(h∗)((cid:98)h, S) := E (x,y)∼S sup patch∈Fadv(h∗) 1 (cid:34) (cid:110) (cid:98)h(patch (x)) (cid:54)= y (cid:35) (cid:111)
In a slight overload of notation, let LH sets generated by Fadv(h∗): (cid:40)
LH
Fadv(h∗) := (x, y) (cid:55)→ sup patch∈Fadv(h∗) 1 (cid:110) (cid:98)h(patch (x)) (cid:54)= y (cid:41) (cid:111)
: (cid:98)h ∈ H
Fadv(h∗) denote the robust loss class of H with the perturbation (cid:16) (cid:17)
LH
Fadv(h∗) is ﬁnite3. Finally, assume that the perturbation set Fadv is the
Then, assume that VC same as that consistent with the ground-truth classiﬁer h∗. In other words, once h∗ is selected, then we reveal to both the learner and the adversary the sets Fadv(h∗); thus, the learner equates Fadv and Fadv(h∗). Hence, although h∗ is not known to the learner, Fadv(h∗) is. As an example of a natural scenario in which such an assumption holds, consider the case where h∗ is some large-margin classiﬁer and Fadv consists of short additive perturbations. This subsumes the setting where h∗ is some image classiﬁer and Fadv consists of test-time adversarial perturbations which don’t impact the true classiﬁcations of the source images. 3.1 Certifying the Existence of Backdoors
The assumption that Fadv = Fadv(h∗) gives the learner enough information to minimize
LFadv(h∗)((cid:98)h, S) on a ﬁnite training set S over (cid:98)h ∈ H4; the assumption that VC
< ∞ yields that the learner recovers a classiﬁer that has low robust loss as per uniform convergence. This implies that with sufﬁcient data and sufﬁcient corruptions, a backdoor data poisoning attack can be detected in the training set. We formalize this below.
Fadv(h∗)
LH (cid:17) (cid:16)
Theorem 14 (Certifying Backdoor Existence (Appendix Theorem 30)). Suppose that the learner can calculate and minimize:
LFadv(h∗)((cid:98)h, S) = E (x,y)∼S sup patch∈Fadv(h∗) 1 (cid:34) (cid:110) (cid:98)h(patch (x)) (cid:54)= y (cid:35) (cid:111) over a ﬁnite set S and (cid:98)h ∈ H.
If the VC dimension of the loss class LH
Fadv(h∗) is ﬁnite, then there exists an algorithm using
O (cid:0)ε−2 (cid:1) + log (1/δ)(cid:1)(cid:1) samples that allows the learner to defeat the adversary through learning a backdoor-robust classiﬁer or by rejecting the training set as being corrupted, with probability 1 − δ. (cid:0)VC (cid:0)LFadv(h∗) clean 3It is shown in [21] that there exist classes H and corresponding adversarial loss classes LFadv(h∗) for which (cid:1) = ∞. Nonetheless, there are a variety of natural scenarios in which we (cid:1) < ∞; for example, in the case of linear classiﬁers in Rd and for closed, convex,
VC (H) < ∞ but VC (cid:0)LH have VC (H) , VC (cid:0)LH origin-symmetric, additive perturbation sets, we have VC (H) , VC (cid:0)LH (cid:1) ≤ d + 1 (see [26] [14]).
Fadv(h∗)
Fadv(h∗)
Fadv(h∗) 4However, minimizing LFadv(h∗) might be computationally intractable in several scenarios. 7
See Algorithm A.1 in the Appendix for the pseudocode of an algorithm witnessing the statement of
Theorem 14.
Our result ﬂeshes out and validates the approach implied by [31], where the authors use data augmentation to robustly learn in the presence of backdoors. Speciﬁcally, in the event that adversarial training fails to converge to something reasonable or converges to a classiﬁer with high robust loss, a practitioner can then manually inspect the dataset for corruptions or apply some data sanitization algorithm. 3.1.1 Numerical Trials
To exemplify such a workﬂow, we implement adversarial training in a backdoor data poisoning setting. Speciﬁcally, we select a target label, inject a varying fraction of poisoned examples into the
MNIST dataset (see [2]), and estimate the robust training and test loss for each choice of α. Our results demonstrate that in this setting, the training robust loss indeed increases with the fraction of corrupted data α; moreover, the classiﬁers obtained with low training robust loss enjoy a low test-time robust loss. This implies that the obtained classiﬁers are robust to both the backdoor of the adversary’s choice and all small additive perturbations.
For a more detailed description of our methodology, setup, and results, please see Appendix Section
B. 3.2 Filtering versus Generalization
We now show that two related problems we call backdoor ﬁltering and robust generalization are nearly statistically equivalent; computational equivalence follows if there exists an efﬁcient algorithm to minimize LFadv(h∗) on a ﬁnite training set. We ﬁrst deﬁne these two problems below (Problems 15 and 16).
Problem 15 (Backdoor Filtering). Given a training set S = Sclean ∪ Sadv such that |Sclean| ≥ (cid:1)(cid:1)(cid:1), return a subset S(cid:48) ⊆ S such that the solution to the
Ω (cid:0)poly (cid:0)ε−1, log (1/δ) , VC (cid:0)LFadv(h∗) optimization (cid:98)h := argminh∈HLFadv(h∗) (h, S(cid:48)) satisﬁes LFadv(h∗)(h, D) (cid:46) εclean with probability 1 − δ.
Informally, in the ﬁltering problem (Problem 15), we want to ﬁlter out enough backdoored examples such that the training set is clean enough to obtain robust generalization.
Problem 16 (Robust Generalization). Given a training set S = Sclean ∪ Sadv such that |Sclean| ≥ (cid:1)(cid:1)(cid:1), return a classiﬁer (cid:98)h satisﬁes LFadv(h∗)(cid:98)h, D ≤ εclean with
Ω (cid:0)poly (cid:0)ε−1, log (1/δ) , VC (cid:0)LFadv(h∗) probability 1 − δ.
In other words, in Problem 16, we want to learn a classiﬁer robust to all possible backdoors.
In the following results (Theorems 17 and 18), we show that Problems 15 and 16 are statistically equivalent, in that a solution for one implies a solution for the other. Speciﬁcally, we can write the below.
Theorem 17 (Filtering Implies Generalization (Appendix Theorem 31)). Let α ≤ 1/3 and εclean ≤ 1/10. clean
Sclean ∪ Sadv (cid:0)VC (cid:0)LFadv(h∗)
= (cid:1) + log (1/δ)(cid:1)(cid:1) and |Sadv| ≤ α · (|Sadv| + |Sclean|). clean ∪ S(cid:48)
Suppose we have a training set S
=
|Sclean|
Ω (cid:0)ε−2
If there exists an clean|/|Sclean| ≥ 1 − εclean and algorithm that given S can ﬁnd a subset S(cid:48) = S(cid:48) minh∈H LFadv(h∗)(h, S(cid:48)) (cid:46) εclean, then there exists an algorithm such that given S returns a function (cid:98)h satisfying LFadv(h∗)((cid:98)h, D) (cid:46) εclean with probability 1 − δ.
See Algorithm A.2 in the Appendix for the pseudocode of an algorithm witnessing the theorem statement. adv satisfying |S(cid:48) such that
Theorem 18 (Generalization Implies Filtering (Appendix Theorem 33)). Set εclean ≤ 1/10 and
α ≤ 1/6.
If there exists an algorithm that, given at most a 2α fraction of outliers in the training set, can output a hypothesis satisfying LFadv(h∗)((cid:98)h, D) ≤ εclean with probability 1 − δ over the draw of the training set, then there exists an algorithm that given a training set S = Sclean ∪ Sadv satisfying 8
clean (cid:0)VC (cid:0)LFadv(h∗) (cid:1) + log (1/δ)(cid:1)(cid:1) outputs a subset S(cid:48) ⊆ S with the property that (cid:0)argminh∈HLFadv(h∗) (h, S(cid:48)) , D(cid:1) (cid:46) εclean with probability 1 − 7δ.
|Sclean| ≥ Ω (cid:0)ε−2
LFadv(h∗)
See Algorithm A.3 in the Appendix for the pseudocode of an algorithm witnessing Theorem 18.
Note that there is a factor-2 separation between the values of α used in the ﬁltering and generalizing routines above; this is a limitation of our current analysis.
The upshot of Theorems 17 and 18 is that in order to obtain a classiﬁer robust to backdoor perturbations at test-time, it is statistically necessary and sufﬁcient to design an algorithm that can ﬁlter sufﬁciently many outliers to where directly minimizing the robust loss (e.g., adversarial training) yields a generalizing classiﬁer. Furthermore, computational equivalence holds in the case where minimizing the robust loss on the training set can be done efﬁciently (such as in the case of linear separators with closed, convex, bounded, origin-symmetric perturbation sets – see [26]). This may guide future work on the backdoor-robust generalization problem, as it is equivalent to focus on the conceptually simpler ﬁltering problem. 4