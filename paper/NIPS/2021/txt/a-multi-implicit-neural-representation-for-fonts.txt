Abstract
Fonts are ubiquitous across documents and come in a variety of styles. They are either represented in a native vector format or rasterized to produce ﬁxed resolution images.
In the ﬁrst case, the non-standard representation prevents beneﬁting from latest network architectures for neural representations; while, in the latter case, the rasterized representation, when encoded via networks, results in loss of data ﬁdelity, as font-speciﬁc discontinuities like edges and corners are difﬁcult to represent using neural networks. Based on the observation that complex fonts can be represented by a superposition of a set of simpler occupancy functions, we introduce multi-implicits to represent fonts as a permutation-invariant set of learned implicit functions, without losing features (e.g., edges and corners). However, while multi-implicits locally preserve font features, obtaining supervision in the form of ground truth multi-channel signals is a problem in itself. Instead, we propose how to train such a representation with only local supervision, while the proposed neural architecture directly ﬁnds globally consistent multi-implicits for font families. We extensively evaluate the proposed representation for various tasks including reconstruction, interpolation, and synthesis to demonstrate clear advantages with existing alternatives. Additionally, the representation naturally enables glyph completion, wherein a single characteristic font is used to synthesize a whole font family in the target style. 1

Introduction
Fonts constitute the vast majority of documents. They come in a variety of styles spanning a range of topologies representing a mixture of smooth curves and sharp features. Although fonts vary signiﬁcantly across families, they remain stylistic coherent across the different alphabets/symbols inside any chosen font family.
Fonts are most commonly stored in a vector form (e.g., a collection of spline curves) that is compact, efﬁcient, and can be resampled at arbitrary resolutions without loss of features. This specialized representation, however, prevents the adaptation of many deep learning setups optimized for regular structures (e.g., image grids). In order to avoid this problem, custom deep learning architectures have been developed for directly producing vector output but they typically require access to ground truth vector data for training. This is problematic: ﬁrst, collecting sufﬁcient volume of vector data for training is non trivial; and second, vector representations are not canonical (i.e., same fonts can be represented by different sequence of vectors), which in turn requires hand-coded network parameters to account for varying number of vector instructions.
An alternate approach is to rasterize vectorized fonts and simply treat them as images. While this readily allows using image-based deep learning methods, the approach inherits problems of discretized 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Multi-implicit neural representation for high ﬁdelity font reconstruction and generation.
Note that while ours perform similar to ImageVAE at lower/training resolution, the advantage of ours become clear when we test at higher resolution (e.g, how corners continue to be preserved). representations leading to aliasing artifacts and loss of sharp features. Further, the resultant images are optimized for particular resolutions and cannot be resampled without introducing additional artifacts.
Recently, implicit representation has emerged as an attractive representation for deep learning as, once trained, they can resampled at different resolutions without introducing artifacts. Unfortunately, implicit representations (e.g., signed distance ﬁelds) for fonts are often too complex to be represented accurately by neural networks. As a results, although deep implicits work for simple fonts, they can fail to retain characteristic features (i.e., edges and corners) for complex fonts.
Drawing inspiration from multi-channel SDFs [4], we observe that complex fonts can be expressed as a composition of multiple simple regions. For example, a local corner can be represented as a suitable composition of two half-planes, each of which can easily be individually encoded as deep implicit functions. We build on this idea by hypothesizing that complex fonts can also be encoded as suitable composition of global implicit functions. We call such a representation to be a multi-implicit neural representation, as each (global) implicit function is neurally encoded.
Thus, multi-implicits provide a simple representation that is amenable for processing by neural networks and the output ﬁdelity remains comparable, even un-der resampling, to vector representations without los-ing edge or corner features. A remaining challenge is how to supervise such a network as there is no dataset with reference multi-implicits that be directly used.
In this paper, we present a network structure and train-ing procedure that allow multi-implicts to be trained using only local supervision. We describe how to extract necessary local supervision from vector input and to adaptively obtain training information for the multi-implicits.
Figure 2: Corner preserving capability of dif-ferent sampling methods.
We extensively evaluated the proposed multi-implicits representation for various tasks including reconstruction, interpolation, and synthesis to demonstrate clear advantages over several existing state-of-the-art alternatives. Additionally, the representation naturally enables glyph completion, wherein a single characteristic font glyph is used to synthesize a whole font family in consistent style. 2