Abstract
Humans are able to rapidly understand scenes by utilizing concepts extracted from prior experience. Such concepts are diverse, and include global scene descriptors, such as the weather or lighting, as well as local scene descriptors, such as the color or size of a particular object. So far, unsupervised discovery of concepts has focused on either modeling the global scene-level or the local object-level factors of variation, but not both. In this work, we propose COMET, which discovers and represents concepts as separate energy functions, enabling us to represent both global concepts as well as objects under a uniﬁed framework. COMET discovers energy functions through recomposing the input image, which we ﬁnd captures independent factors without additional supervision. Sample generation in COMET is formulated as an optimization process on underlying energy functions, enabling us to generate images with permuted and composed concepts. Finally, discovered visual concepts in COMET generalize well, enabling us to compose concepts between separate modalities of images as well as with other concepts discovered by a separate instance of COMET trained on a different dataset*. 1

Introduction
Human intelligence is characterized by its ability to learn new concepts, such as the manipulation of a new tool from only a few demonstrations [1]. Essential to this capability is the composition and re-utilization of previously learned concepts to accomplish the task at hand [36]. This is especially apparent in natural language, which is often described as a tool for making ‘inﬁnite use of ﬁnite means’ [8]. Previously acquired words can be inﬁnitely nested using a set of grammatical rules to communicate an arbitrary thought, opinion, or state one is in. In this work, we are interested in constructing a system that can discover, in an unsupervised manner, a broad set of these compositional components, as well as subsequently combine them across distinct modalities and datasets.
For obtaining such decompositions, two separate lines of work exist. The ﬁrst focuses on obtaining global, holistic, compositional factors by situating data points, such as human faces, in an underlying (ﬁxed) factored vector space [24, 48]. Individual factors, such as emotion or hair color, are represented as independent dimensions of the vector space, with recombination between factors corresponding to the recombination of the underlying dimensions. Due to the ﬁxed dimensionality of the vector space, multiple instances of a single factor, such as lighting, may not be combined, nor can individual
*Code and data available at https://energy-based-model.github.io/comet/ 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: COMET decomposes images into a set of energy functions. The minimal energy state across all energy functions reconstructs the input image. The minimal energy states of individual energy functions capture particular aspects of an image, in the form of local factors of variations such as individual objects, or global factors of variation such as hair color, background lighting, facial expression, or skin color. Note that reconstructed face images in the remainder of the paper are synthetic and not existing human faces. factored vector spaces from separate datasets be combined, i.e. the facial expression in an image from one dataset, and the background lighting in an image from another.
To address this weakness, a separate line of work decomposes a scene into a set of underlying ‘object’ factors. Each object factor represents a separate set of pixels in an image, as deﬁned by a disjoint segmentation mask [22, 47]. Such a representation allows for the composition of individual factors by compositing the segmentation masks. However, by explicitly constraining the decomposition to be in terms of disjoint segmentation masks, relationships between individual factors of variation become more difﬁcult to represent and capture global factors describing a scene.
In this work, we propose instead to decompose a scene into a set of factors represented as energy functions. An individual energy function represents a factor by assigning low energy to scenes with said factor and high energy to scenes where said factor is absent. A scene is then generated by optimizing the sum of energies for all factors. Multiple factors can be composed together, by summing the energies for each individual factor. Simultaneously, these individual factors are deﬁned across the entire scene, allowing energy functions to represent global factors (lighting, camera viewpoint) and local factors (object existence).
Our work is inspired by recent work [12] which shows that energy based models may be utilized to represent ﬂexible compositions of both global and local factors. However, while [12] required supervision, as labels were used to represent concepts, we aim to decompose and discover concepts in an unsupervised manner. In our approach, we discover energy functions from separate data points by enforcing compositions of these energy functions to recompose data.
Our work is further inspired by the ability of humans to effortlessly and reliably combine concepts gathered from disparate experiences. As a separate beneﬁt of our approach, we show that we may take components inferred by one instance of our model trained on one dataset, and compose it with other components inferred by other instances of our model trained on separate datasets.
We provide analysis showing why our approach, COMET†, is favorable compared to existing unsupervised approaches for decomposing scenes. We contribute the following: First, we show
COMET provides a uniﬁed framework enabling us to decompose images into both global factors of variation as well as local factors of variation. Second, we show that COMET enables us to scale to more realistic datasets than previous work. Finally, we show that components obtained by
COMET generalize well, and are amenable to compositions across different modes of data, and with components discovered by other instances of COMET. 2