Abstract Textual Reasoning
Christopher Michael Rytting
Department of Computer Science
Brigham Young University
Provo, UT 84602 chrisrytting@byu.edu
David Wingate
Department of Computer Science
Brigham Young University
Provo, UT 84602 wingated@cs.byu.edu
Abstract
Large natural language models (such as GPT-3 or T5) demonstrate impressive abilities across a range of general NLP tasks. Here, we show that the knowledge embedded in such models provides a useful inductive bias, not just on traditional
NLP tasks, but also in the nontraditional task of training a symbolic reasoning engine. We observe that these engines learn quickly and generalize in a natural way that reﬂects human intuition. For example, training such a system to model block-stacking might naturally generalize to stacking other types of objects because of structure in the real world that has been partially captured by the language describing it. We study several abstract textual reasoning tasks, such as object manipulation and navigation, and demonstrate multiple types of generalization to novel scenarios and the symbols that comprise them. We also demonstrate the surprising utility of compositional learning, where a learner dedicated to mastering a complicated task gains an advantage by training on relevant simpler tasks instead of jumping straight to the complicated task. 1

Introduction
Natural language processing (NLP) has seen major progress thanks to probabilistic language models (LMs) like GPT-2 [1], BERT [2], and T5 [3]. These are pre-trained in a general, task-agnostic way on large corpora of unstructured text and then ﬁne-tuned on speciﬁc tasks. This method has achieved state of the art performance on popular NLP tasks like question-answering, textual entailment, text summarization, neural machine translation, and more [4].
These models are not just impressive for the high scores they achieve on quantitative language tasks; the text they generate often reﬂects patterns of “real-world” structure, suggesting that embedded in the weights of the LM is implicit knowledge of physics, object persistence, containment, spatial relationships, causal mechanisms, material properties, and other common-sense knowledge central to human reasoning and intuition. If that is so, then they ought to provide a useful inductive bias in learning to perform symbolic reasoning tasks that mirror real-world tasks.
In this paper, we attempt to leverage and characterize this inductive bias by training reasoning engines that demonstrate some of the hallmarks of human reasoning ability: learning (a) rules (b) that generalize well (c) from few examples. Concretely, we ﬁne-tune T5 on a suite of symbolic reasoning tasks and study generalization along multiple different axes beyond the normal test/train split: we examine cardinality generalization, object generalization, part-of-speech generalization, and show (in the spirit of curriculum learning) that LMs can leverage combinations of learned subskills to master complicated composite tasks with both better sample efﬁciency and higher terminal performance than learning directly on the complicated tasks. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
We see our contributions as four-fold. First, we demonstrate a high level of performance by connec-tionist models on tasks resembling symbolic classical AI tasks, demonstrating some symbolic ability of such models in light of recent calls to unify the principles of both symbolism and connectionism.
Secondly, we demonstrate the breakdown of reasoning ability by manufacturing our own reasoning datasets that can be tweaked in systematic ways, instead of simply split into training/validation/test sets. This means that we can assess our models’ ability to both interpolate and extrapolate in sys-tematic, symbolic, and grammatical ways, and otherwise ﬂex with changing distributions. Thirdly, we demonstrate the ability of large LMs to reason compositionally, that is, to learn two kinds of reasoning separately and then to combine those different kinds of reasoning on a novel composite task, to which they are both relevant. Lastly, we demonstrate the inductive bias we hypothesize is present in large LMs and can be leveraged to assist in the formation of reasoning engines. 2