Abstract
Our work focuses on tackling large-scale ﬁne-grained image retrieval as ranking the images depicting the concept of interests (i.e., the same sub-category labels) highest based on the ﬁne-grained details in the query. It is desirable to alleviate the challenges of both ﬁne-grained nature of small inter-class variations with large intra-class variations and explosive growth of ﬁne-grained data for such a practical task. In this paper, we propose an Attribute-Aware hashing Network (A2-NET) for generating attribute-aware hash codes to not only make the retrieval process efﬁcient, but also establish explicit correspondences between hash codes and visual attributes. Speciﬁcally, based on the captured visual representations by attention, we develop an encoder-decoder structure network of a reconstruction task to unsu-pervisedly distill high-level attribute-speciﬁc vectors from the appearance-speciﬁc visual representations without attribute annotations. A2-NET is also equipped with a feature decorrelation constraint upon these attribute vectors to enhance their rep-resentation abilities. Finally, the required hash codes are generated by the attribute vectors driven by preserving original similarities. Qualitative experiments on ﬁve benchmark ﬁne-grained datasets show our superiority over competing methods.
More importantly, quantitative results demonstrate the obtained hash codes can strongly correspond to certain kinds of crucial properties of ﬁne-grained objects. 1

Introduction
Fine-grained image retrieval in computer vision aims to retrieve images belonging to multiple subordi-nate categories of a super-category (aka a meta-category), e.g., different species of animals/plants [36], different models of cars [20], different kinds of retail products [39], etc. Its key challenge therefore lies with understanding ﬁne-grained visual differences that sufﬁciently discriminate between objects that are highly similar in overall appearance, but differ in ﬁne-grained features. Also, ﬁne-grained retrieval still demands ranking all the instances so that images depicting the concept of interest (e.g., the same sub-category label) are ranked highest based on the ﬁne-grained details in the query.
In particular, with the explosive growth of ﬁne-grained data in real applications [1, 14, 26, 36, 39],
ﬁne-grained hashing, as a promising solution for dealing with large-scale ﬁne-grained retrieval tasks, has proven to be able to greatly reduce the storage cost and increase the query speed [8, 18] beneﬁting from the learned compact binary hash code representations. However, although previous works,
∗Corresponding author. X.-S. Wei, Y. Shen, X. Sun and J. Yang are with PCA Lab, Key Lab of Intelligent
Perception and Systems for High-Dimensional Information of Ministry of Education, and Jiangsu Key Lab of
Image and Video Understanding for Social Security, School of Computer Science and Engineering, Nanjing
University of Science and Technology. This work was supported by Natural Science Foundation of Jiangsu
Province of China under Grant (BK20210340), the Fundamental Research Funds for the Central Universities (No. 30920041111), and CAAI-Huawei MindSpore Open Fund (CAAIXSJLJJ-2020-022A). The ﬁrst two authors contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Key idea of our A2-NET, as well as the main process of ﬁne-grained hashing based on our attribute-aware hash codes. In concretely, regarding a query image
Iq of Red bellied
Woodpecker, after returning all the correct results, a ﬁne-grained image belonging to Red headed
Woodpecker closest to the query image in terms of Hamming distance is also retrieved. e.g., [8, 18], achieved good retrieval performance, the bits of their hash codes correspond to no semantics, i.e., ﬁne-grained attributes. While, such attributes, e.g., head color, tail color, male, female, living habits, are great means of describing ﬁne-grained objects, in a way both humans and computers understand. In this paper, to establish an explicit correspondence between hash codes and visual attributes for not only further improving large-scale ﬁne-grained retrieval accuracy, but more importantly integrating interpretation into deep learning based hash methods, we propose a uniﬁed
Attribute-Aware hashing Network, termed as A2-NET (cf. Figure 1), for achieving these goals.
In our A2-NET, considering huge labor cost of supervised attribute annotations, we restrict ourselves in an unsupervised setting to automatically capture discriminative visual attributes from still images and then correspond the ﬁnal learned hash code representations to these attributes. Therefore, a hash bit of learned hash codes could be both discriminative and intuitive. Additionally, thanks to the unsupervised setting, the attributes derived from A2-NET will be not restricted to pre-deﬁned attributes like supervised-based attribute learning methods [15, 21, 42, 45]. Moreover, it can distill the most useful properties of ﬁne-grained objects as attribute-aware hash codes in such an end-to-end trainable manner for accuracy retrieval among multiple similar subordinate categories.
More speciﬁcally, as the overall framework shown in Figure 2, our A2-NET consists of a ﬁne-grained representation learning module and an attribute-aware hash codes generating module. It ﬁrst leverages attention mechanisms to model ﬁne-grained tailored patterns in terms of both global-level deep features Ti and local-level cues T c
Ii. Then, the appearance-speciﬁc features of these visual patterns T are aggregated and translated into semantic-speciﬁc representations xi.
After that, we formulate the aforementioned unsupervised attribute learning as a reconstruction task of projecting xi to an attribute vector vi by performing an encoder-decoder structure network.
Therefore, it can be expected that in the high-level attribute space, vi could correspond to certain kinds of nameable properties of ﬁne-grained objects. Moreover, a feature decorrelation constraint is further introduced upon vi to both enhance the discriminative ability and remove the redundant correlation among these dimensions of attribute-speciﬁc features. Finally, our attribute-aware hash codes ui are generated from vi by conducting the hash code learning procedure. i from input image
To evaluate our model, we conduct extensive experiments using ﬁve benchmark ﬁne-grained retrieval datasets for both accuracy and interpretability. Quantitative results of retrieval accuracy on these datasets show that the proposed A2-NET model obviously and consistently outperforms existing state-of-the-art methods. Qualitative visualization of the obtained attribute-aware hash codes demonstrates that these hash bits have strong correspondences to visual attributes of ﬁne-grained objects (cf.
Figure 4), even without employing attribute supervisions or part-level annotations. In addition, the ablation studies of these crucial components in A2-NET also validate their own effectiveness. 2