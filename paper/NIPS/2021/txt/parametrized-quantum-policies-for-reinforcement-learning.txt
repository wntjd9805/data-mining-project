Abstract
With the advent of real-world quantum computing, the idea that parametrized quantum computations can be used as hypothesis families in a quantum-classical machine learning system is gaining increasing traction. Such hybrid systems have already shown the potential to tackle real-world tasks in supervised and generative learning, and recent works have established their provable advantages in special artificial tasks. Yet, in the case of reinforcement learning, which is arguably most challenging and where learning boosts would be extremely valuable, no proposal has been successful in solving even standard benchmarking tasks, nor in showing a theoretical learning advantage over classical algorithms. In this work, we achieve both. We propose a hybrid quantum-classical reinforcement learning model using very few qubits, which we show can be effectively trained to solve several standard benchmarking environments. Moreover, we demonstrate, and formally prove, the ability of parametrized quantum circuits to solve certain learning tasks that are intractable to classical models, including current state-of-art deep neural networks, under the widely-believed classical hardness of the discrete logarithm problem. 1

Introduction
Hybrid quantum machine learning models constitute one of the most promising applications of near-term quantum computers [1, 2]. In these models, parametrized and data-dependent quantum computations define a hypothesis family for a given learning task, and a classical optimization algorithm is used to train them. For instance, parametrized quantum circuits (PQCs) [3] have already proven successful in classification [4–8], generative modeling [9, 10] and clustering [11] problems.
Moreover, recent results have shown proofs of their learning advantages in artificially constructed tasks [6, 12], some of which are based on widely believed complexity-theoretic assumptions [12–15].
All these results, however, only consider supervised and generative learning settings.
Arguably, the largest impact quantum computing can have is by providing enhancements to the hardest learning problems. From this perspective, reinforcement learning (RL) stands out as a field that can greatly benefit from a powerful hypothesis family. This is showcased by the boost in learning performance that deep neural networks (DNNs) have provided to RL [16], which enabled systems like AlphaGo [17], among other achievements [18, 19]. Nonetheless, the true potential of near-term quantum approaches in RL remains very little explored. The few existing works [20–23] have failed so far at solving classical benchmarking tasks using PQCs and left open the question of their ability to provide a learning advantage. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Training parametrized quantum policies for reinforcement learning. We consider a quantum-enhanced RL scenario where a hybrid quantum-classical agent learns by interacting with a classical environment. For each state s it perceives, the agent samples its next action a from its policy
πθ(a|s) and perceives feedback on its behavior in the form of a reward r. For our hybrid agents, the policy πθ is specified by a PQC (see Def. 1) evaluated (along with the gradient ∇θ log πθ) on a quantum processing unit (QPU). The training of this policy is performed by a classical learning algorithm, such as the REINFORCE algorithm (see Alg. 1), which uses sample interactions and policy gradients to update the policy parameters θ.
Contributions
In this work, we demonstrate the potential of policies based on PQCs in solving classical RL environments. To do this, we first propose new model constructions, describe their learn-ing algorithms, and show numerically the influence of design choices on their learning performance.
In our numerical investigation, we consider benchmarking environments from OpenAI Gym [24], for which good and simple DNN policies are known, and in which we demonstrate that PQC policies can achieve comparable performance. Second, inspired by the classification task of Havlíˇcek et al. [6], conjectured to be classically hard by the authors, we construct analogous RL environments where we show an empirical learning advantage of our PQC policies over standard DNN policies used in deep RL. In the same direction, we construct RL environments with a provable gap in performance between a family of PQC policies and any efficient classical learner. These environments essentially build upon the work of Liu et al. [14] by embedding into a learning setting the discrete logarithm problem (DLP), which is the problem solved by Shor’s celebrated quantum algorithm [25] but widely believed to be classically hard to solve [26].