Abstract
The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data conﬁdentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a signiﬁcant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain charac-teristic, such as trafﬁc, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed, which loads a subset of the global model’s parameters rather than loading the entire model used in most previous works. We
ﬁrst validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix. Then we develop PartialFed-Adaptive, which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classiﬁcation and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of
FedAvg on Ofﬁce-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs signiﬁcantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg. 1

Introduction
Endless collection of texts, images and videos for training data-hungry models increases potential threats to the safety systems. Any attacks to these data centers could cause billions of information leakage. A safer way is to keep the user data purely local on their devices. But this contradicts with the widely adopted stochastic gradient descent (SGD) training procedure, which usually requires data communication for random batch sampling. Federated Learning [12] develops a paradigm for training large models under such situation. Locally trained models are aggregated using FedAvg [19] and then served as initialization model for the next local iteration. The training data conﬁdentiality is achieved by only allowing transferring the model’s parameters (rather than the data) under different cryptograph algorithms, which include differential privacy [7], homomorphic encryption [37], block chain [22], etc.
However, recent studies [39, 26, 25] have shown that FedAvg does not provide satisfactory results in the presence of data heterogeneity. A major problem is that all models are designed to ﬁt an
“average client” [29], which is difﬁcult when local and global distributions are deviated from each other. This phenomenon is not uniquely observed. Researchers in multi-domain learning have also discovered that directly ﬁtting non-identical domains into a single feature extractor is suboptimal
[3]. The leading solution to this problem is to reconﬁgure the network into domain-agnostic and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
domain-speciﬁc layers [23, 33]. Similar conclusion has also been drawn simultaneously in the ﬁeld of multi-task learning [27]. The performance is boosted by letting each task choosing task-agnostic and task-speciﬁc layers on the ﬂy. Based on such generous observations, we propose a Personalized
Federated Learning (PFL) method from the idea of client-agnostic and client-speciﬁc initialization.
Initialization is considered because it plays the essential role of transferring global knowledge in FL.
Figure 1 dipicts the overall architecture. At the heart of our algorithm is a mixed initialization strategy.
Instead of fully utilizing the averaged global parameters for initialization, clients will only select a fraction of them, and load the remaining parameters from previous local models. The selection process is decided by a customized loading strategy, which might vary from client to client and time to time. To validate PartialFed thoroughly, we ﬁrst propose PartialFed-Fix, where the loading strategies are inspired by human priors in functionality and classiﬁcation of different parts of the network. Then we propose an automatic strategy which is learned jointly with the network parameters by gradient descent. We name this algorithm PartialFed-Adaptive.
In the case of PartialFed-Fix, our algorithm can be viewed as a federated version of multi-path networks in the multi-domain learning [24, 33]. The shared parameters of different clients are jointly learned by FedAvg, while the client-speciﬁc parameters are only learned locally. Treating the network as a combination of global and local blocks makes it possible to learn the knowledge from other clients while keeping the local knowledge stored safely. Although PartialFed-Fix can achieve improved performance, we argue that the ﬁxed loading strategy is a suboptimal solution for PFL since different clients may have different dependencies on the global model. For example, a client might prefer the classiﬁer of the global model at the start of training, which often acquires better generalization ability. In the end, the client is likely to train a local classiﬁer, which brings superior personalized performance. We show that our dynamic algorithm PartialFed-Adaptive is able to capture this change of parameter loading behaviors during the training process.
Despite its simplicity, PartialFed gains surprising performance on many non i.i.d. FL experiments.
We construct real world FL experiments by introducing cross-domain classiﬁcation and detection dataset [30, 33]. For example, on Ofﬁce-Home dataset, our PartialFed-Fix and PartialFed-Adaptive surpass FedAvg by 4.88% and 5.43% on average accuracy, respectively. Similarly, on UODB dataset, PartialFed-Fix and PartialFed-Adaptive outperform FedAvg by 2.65% and 2.68% on AP50, respectively. More interestingly, the adaptive loading strategy greatly reduce the possibility of clients getting inferior performance caused by the distribution deviation. 2