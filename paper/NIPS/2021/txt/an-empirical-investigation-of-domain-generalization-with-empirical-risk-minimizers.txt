Abstract
Recent work demonstrates that deep neural networks trained using Empirical Risk
Minimization (ERM) can generalize under distribution shift, outperforming spe-cialized training algorithms for domain generalization. The goal of this paper is to further understand this phenomenon. In particular, we study the extent to which the seminal domain adaptation theory of Ben-David et al. (2007) explains the performance of ERMs. Perhaps surprisingly, we ﬁnd that this theory does not provide a tight explanation of the out-of-domain generalization observed across a large number of ERM models trained on three popular domain generalization datasets. This motivates us to investigate other possible measures—that, however, lack theory—which could explain generalization in this setting. Our investigation reveals that measures relating to the Fisher information, predictive entropy, and maximum mean discrepancy are good predictors of the out-of-distribution general-ization of ERM models. We hope that our work helps galvanize the community towards building a better understanding of when deep networks trained with ERM generalize out-of-distribution. 1

Introduction
Conventional wisdom in domain generalization was recently upturned by the work of Gulrajani
& Lopez-Paz (2020) on the DomainBed benchmark. In their work, the authors use careful model selection and evaluation to show that models trained using empirical risk minimization (Vapnik, 1999) are able to achieve near state-of-the-art performance on a variety of popular benchmarks, surpassing the performance of most algorithms specialized for domain generalization (Arjovsky et al., 2019;
Yan et al., 2020; Li et al., 2017a).
In a parallel research effort, there has been an increased interest in understanding when deep neural networks trained using empirical risk minimization generalize well in-domain (Jiang et al., 2019;
Neyshabur et al., 2017; Dziugaite & Roy, 2017; Bartlett et al., 2017; Arora et al., 2018; Daniely
& Granot, 2019). However, understanding when empirical risk minimizers are able to generalize out-of-distribution remains poorly understood. Answering this question would help us characterize the failure modes of domain generalization algorithms, and to develop specialized baselines that would allow us to break through the dominance of empirical risk minimization.
We argue that the out-of-distribution generalization performance of a classiﬁer depends mainly on two factors. On the one hand, we expect good out-of-distribution generalization only for classiﬁers able to achieve good in-distribution generalization (also called source-domain validation accuracy).
On the other hand, we expect out-of-distribution generalization to degrade as the discrepancy between the distributions of training and testing data increases, relative to the function family of interest. This reveals an interplay between the properties of the neural network, the used training data, and the
∗
Both D.L.P and D.J.S. contributed to the work equally 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Left: Source-domain validation accuracy (x-axis) v.s. Novel, Target-domain test accuracy (y-axis) for a set of 100 models trained on RotatedMNIST environments with rotations between 30-75 degrees. Notice that the ability to generalize to novel domains can vary substantially (0 degrees, ⋆ vs 15 degrees, ⧫). Right:
We seek to characterize the conditions under which a network generalizes to unseen domains, given unlabelled examples from the novel, test domain, the weights of the network trained via empirical risk minimization on the source (training) data and the training data. differences between training and testing regimes (Figure 1). Given all of these distinct ingredients, how does one build a coherent theory of domain generalization with strong predictive power for modern, over-parametrized neural networks?
The seminal work of Ben-David et al. (2007, 2010) lays down a theoretical foundation for the related setup of unsupervised domain adaptation. In particular, the theory in Ben-David et al. (2007, 2010) upper-bounds the out-of-distribution performance of an ERM based on a ﬁxed feature space as the sum of two terms: (i) the source-domain validation accuracy, and (ii) the distance between the input distribution of source-domain and target-domain examples under the ﬁxed feature space. This theory has found success in practice, improving both linear models (Ben-David et al., 2007) and deep neural networks (Ganin et al., 2015) in the setup of unsupervised domain adaptation. Algorithmically speaking, one can implement this theory by ﬁnding a feature representation under which (i) one can perform well on source-domain inputs, and (ii) it is not possible to distinguish source-domain inputs from target-domain inputs.
While the theory above deals with the setup of unsupervised domain adaptation (where algorithms have access to unlabeled target-domain data during training), here we are interested in the related but distinct domain generalization scenario where algorithms only have access to source-domain data during training. Therefore, to remain in the domain generalization setup, we will put the theory of Ben-David et al. (2007) to use from a different angle. Instead of training representations in such a way that encourages source-domain and target-domain inputs to be indistinguishable, our methodology consists in training a very large number of ERM models only using source-domain data, to later investigate if the models exhibiting good out-of-distribution generalization indeed learned a feature space under which it is difﬁcult to tell apart source-domain and target-domain inputs.
Training over 12,000 deep neural network image classiﬁers using empirical risk minimization, using the standard DomainBed pipeline, we reveal that the theory of Ben-David et al. (2007) has limited power to predict when a classiﬁer will generalize out-of-domain. We investigate this further by unpacking whether assumptions in the theory are violated or if the bounds are slack. Our ﬁndings suggest that in practice, while the assumptions from the theory are largely not violated in the learned feature spaces of ERMs, one can obtain good generalization despite being able to tell apart the source and target data, limiting the practical applicability of the theory to explain domain generalization. We investigate and provide more intuitions for this phenomenon in section 4.
In search of other correlates of domain generalization that might point the way to new theoretical approaches, we set out to build a catalog of measures that could predict the out-of-distribution generalization performance of our suite of ERM trained neural networks. In building such a catalog of generalization measures, we take inspiration from recent literature undertaking a similar effort to explain variance of in-domain generalization of deep neural networks (Jiang et al., 2019; Neyshabur et al., 2017; Dziugaite et al., 2020). After an exhaustive empirical evaluation, we ﬁnd that several of the proposed measures are relatively strong predictors of out-of-distribution performance. These include measures related to predictive entropy, the Mixup criterion (Zhang et al., 2017), the norm of the Jacobian (Novak et al., 2018), and the Fisher information matrix (Amari, 1998).
To summarize, our work makes the following contributions: 2
• We perform a large scale empirical study testing the theory from Ben-David et al. (2007, 2010) on deep neural networks trained on the DomainBed (Gulrajani & Lopez-Paz, 2020) domain generalization benchmark.
• We ﬁnd that the theory has limited ability to predict which ERMs will generalize out-of-distribution.
• We perform an empirical evaluation of several other candidate measures for explaining domain generalization of neural networks, ﬁnding measures that outperform theoretically motivated quantities. 2