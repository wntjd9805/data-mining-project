Abstract
We consider Model-Agnostic Meta-Learning (MAML) methods for Reinforcement
Learning (RL) problems, where the goal is to ﬁnd a policy using data from several tasks represented by Markov Decision Processes (MDPs) that can be updated by one step of stochastic policy gradient for the realized MDP. In particular, using stochastic gradients in MAML update steps is crucial for RL problems since computation of exact gradients requires access to a large number of possible trajectories. For this formulation, we propose a variant of the MAML method, named Stochastic Gradient Meta-Reinforcement Learning (SG-MRL), and study its convergence properties. We derive the iteration and sample complexity of SG-MRL to ﬁnd an (cid:15)-ﬁrst-order stationary point, which, to the best of our knowledge, provides the ﬁrst convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We further show how our results extend to the case where more than one step of stochastic policy gradient method is used at test time. Finally, we empirically compare SG-MRL and MAML in several deep RL environments. 1

Introduction
Meta-learning has recently attracted much attention as a learning to learn approach that enables quick adaptation to new tasks using past experience and data. This is a particularly promising approach for Reinforcement Learning (RL) where in several applications, such as robotics, a group of agents encounter new tasks and need to learn new behaviors or policies through a few interactions with the environment building on previous experience [1–9]. Among various forms of Meta-learning, gradient-based Model-Agnostic Meta-Learning (MAML) formulation [1] is a particularly effective approach which, as its name suggests, can be applied to any learning problem that is trained with gradient-based updates. In MAML, we exploit observed tasks at training time to ﬁnd an initial model that is trained in a way that rapidly adapts to a new unseen task at test time, after running a few steps of a gradient-based update with respect to the loss of the new task.
The MAML formulation can be extended to RL problems if we represent each task as a Markov
Decision Process (MDP). In this setting, we assume that we are given a set of MDPs corresponding to the tasks that we observe during the training phase and assume that the new task at test time is drawn from an underlying probability distribution. The goal in Model-Agnostic Meta-Reinforcement
Learning (MAMRL) is to exploit this data to come up with an initial policy that adapts to a new task (drawn from the same distribution) at test time by taking a few stochastic policy gradient steps [1]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Several algorithms have been proposed in the context of MAMRL [1, 9–12] which demonstrate the advantage of this framework in practice. None of these methods, however, are supported by theoretical guarantees for their convergence rate or overall sample complexity. Moreover, these methods aim to solve a speciﬁc form of MAMRL that does not fully take into account the stochasticity aspect of RL problems. To be more speciﬁc, the original MAMRL formulation proposed in [1] assumes performing one step of policy gradient to update the initial model at test time. However, as mentioned in the experimental evaluation section in [1], it is more common in practice to use stochastic policy gradient, computed over a batch of trajectories, to update the initial model at test time. This is mainly due to the fact that computing the exact gradient of the expected reward is not computationally tractable due to the massive number of possible state-action trajectories. As a result, the algorithm developed in [1] is designed for ﬁnding a proper initial policy that performs well after one step of policy gradient, while in practice it is implemented with stochastic policy gradient steps. Due to this difference between the formulation and what is used in practice, the ascent step used in MAML takes a gradient estimate which suffers from a non-diminishing bias. As the variance of gradient estimation is also non-diminishing, the resulting algorithm would not achieve exact ﬁrst-order optimality. To be precise, in stochastic nonconvex optimization, if we use an unbiased gradient estimator, along with a small stepsize or a large batch size to control the variance, the iterates converge to a stationary point.
However, if we use a biased estimator with non-vanishing bias and variance, exact convergence to a stationary point is not achievable, even if the variance is small.
Contributions. The goal of this paper is to solve the modiﬁed formulation of model-agnostic meta-reinforcement learning problem in which we perform a stochastic policy gradient update at test time instead of (deterministic) policy gradient. To do so, we propose a novel stochastic gradient-based method for Meta-Reinforcement Learning (SG-MRL), which is designed for stochastic policy gradient steps at test time. We show that SG-MRL implements an unbiased estimate of its objective function gradient which allows achieving ﬁrst-order optimality in non-concave settings. Moreover, we characterize the relation between batch sizes and other problem parameters and the best accuracy that SG-MRL can achieve in terms of gradient norm. We show that, for any (cid:15) > 0, SG-MRL can
ﬁnd an (cid:15)-ﬁrst-order stationary point if the learning rate is sufﬁciently small or the batch of tasks is large enough. To the best of our knowledge, this is the ﬁrst result on the convergence of MAMRL methods. Moreover, we show that our analysis can be extended to the case where more than one step of stochastic policy gradient is taken during test time. For simplicity, we state all the results in the body of the paper for the single-step case and include the derivations of the general multiple steps case in the appendices. We also empirically validate the proposed SG-MRL algorithm in larger-scale environments standard in modern reinforcement learning applications, including a 2D-navigation problem, and a more challenging locomotion problem simulated with the MuJoCo library.