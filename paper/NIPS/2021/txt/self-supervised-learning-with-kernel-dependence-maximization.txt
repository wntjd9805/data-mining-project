Abstract
We approach self-supervised learning of image representations from a statistical dependence perspective, proposing Self-Supervised Learning with the Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes dependence between representations of transformations of an image and the image identity, while minimizing the kernelized variance of those representations. This framework yields a new understanding of InfoNCE, a variational lower bound on the mutual information (MI) between different transformations. While the MI itself is known to have pathologies which can result in learning meaningless representations, its bound is much better behaved: we show that it implicitly approximates SSL-HSIC (with a slightly different regularizer). Our approach also gives us insight into BYOL, a negative-free SSL method, since SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to directly optimize statistical dependence in time linear in the batch size, without restrictive data assumptions or indirect mutual information estimators. Trained with or without a target network,
SSL-HSIC matches the current state-of-the-art for standard linear evaluation on
ImageNet [1], semi-supervised learning and transfer to other classiﬁcation and vision tasks such as semantic segmentation, depth estimation and object recognition.
Code is available at https://github.com/deepmind/ssl_hsic. 1

Introduction
Learning general-purpose visual representations without human supervision is a long-standing goal of machine learning. Speciﬁcally, we wish to ﬁnd a feature extractor that captures the image semantics of a large unlabeled collection of images, so that e.g. various image understanding tasks can be achieved with simple linear models. One approach takes the latent representation of a likelihood-based generative model [2–8]; such models, though, solve a harder problem than necessary since semantic features need not capture low-level details of the input. Another option is to train a self-supervised model for a “pretext task,” such as predicting the position of image patches, identifying rotations, or image inpainting [9–14]. Designing good pretext tasks, however, is a subtle art, with little theoretical guidance available. Recently, a class of models based on contrastive learning [15–22] has seen substantial success: dataset images are cropped, rotated, color shifted, etc. into several views, and features are then trained to pull together representations of the “positive” pairs of views of the same
∗These authors contributed equally.
†Work done in part while at the Toyota Technological Institute at Chicago. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Top-1 accuracies with linear evaluation for different ResNet architecture and methods: supervised (as in [25]), SSL-HSIC (with a target network; ours), BYOL [25], SwAV [18], SimCLR
[19], MoCo v2 [20] and Barlow Twins [22].
Figure 2: Statistical dependence view of con-trastive learning: representations of transformed images should highly depend on image iden-tity. Measuring dependence with HSIC, this pushes different images’ representation distribu-tions apart (black arrows) and pulls representa-tions of the same image together (colored shapes). source image, and push apart those of “negative” pairs (from different images). These methods are either understood from an information theoretic perspective as estimating the mutual information between the “positives” [15], or explained as aligning features subject to a uniformity constraint [23].
Another line of research [24, 25] attempts to learn representation without the “negative” pairs, but requires either a target network or stop-gradient operation to avoid collapsing.
We examine the contrastive framework from a statistical dependence point of view: feature represen-tations for a given transformed image should be highly dependent on the image identity (Figure 2).
To measure dependence, we turn to the Hilbert-Schmidt Independence Criterion (HSIC) [26], and propose a new loss for self-supervised learning which we call SSL-HSIC. Our loss is inspired by
HSIC Bottleneck [27, 28], an alternative to Information Bottleneck [29], where we use the image identity as the label, but change the regularization term.
Through the dependence maximization perspective, we present a uniﬁed view of various self-supervised losses. Previous work [30] has shown that the success of InfoNCE cannot be solely attributed to properties of mutual information, in particular because mutual information (unlike kernel measures of dependence) has no notion of geometry in feature space: for instance, all invertible encoders achieve maximal mutual information, but they can output dramatically different representa-tions with very different downstream performance [30]. Variational bounds on mutual information do impart notions of locality that allow them to succeed in practice, departing from the mutual information quantity that they try to estimate. We prove that InfoNCE, a popular such bound, in fact approximates SSL-HSIC with a variance-based regularization. Thus, InfoNCE can be thought of as working because it implicitly estimates a kernel-based notion of dependence. We additionally show
SSL-HSIC is related to metric learning, where the features learn to align to the structure induced by the self-supervised labels. This perspective is closely related to the objective of BYOL [25], and can explain properties such as alignment and uniformity [23] observed in contrastive learning.
Our perspective brings additional advantages, in computation and in simplicity of the algorithm, compared with existing approaches. Unlike the indirect variational bounds on mutual information
[15, 31, 32], SSL-HSIC can be directly estimated from mini-batches of data. Unlike “negative-free” methods, the SSL-HSIC loss itself penalizes trivial solutions, so techniques such as target networks are not needed for reasonable outcomes. Using a target network does improve the performance of our method, however, suggesting target networks have other advantages that are not yet well understood.
Finally, we employ random Fourier features [33] in our implementation, resulting in cost linear in batch size.
Our main contributions are as follows:
• We introduce SSL-HSIC, a principled self-supervised loss using kernel dependence maximization.
• We present a uniﬁed view of contrastive learning through dependence maximization, by establishing relationships between SSL-HSIC, InfoNCE, and metric learning. 2
• Our method achieves top-1 accuracy of 74.8% and top-5 accuracy of 92.2% with linear evaluations (see Figure 1 for a comparison with other methods), top-1 accuracy of 80.2% and Top-5 accuracy of 94.7% with ﬁne-tuning, and competitive performance on a diverse set of downstream tasks. 2