Abstract
Unpaired image-to-image translation refers to learning inter-image-domain map-ping without corresponding image pairs. Existing methods learn deterministic mappings without explicitly modelling the robustness to outliers or predictive uncertainty, leading to performance degradation when encountering unseen per-turbations at test time. To address this, we propose a novel probabilistic method based on Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC), which models the per-pixel residual by generalized Gaussian distribution, ca-pable of modelling heavy-tailed distributions. We compare our model with a wide variety of state-of-the-art methods on various challenging tasks including unpaired image translation of natural images, using standard datasets, spanning autonomous driving, maps, facades, and also in medical imaging domain consist-ing of MRI. Experimental results demonstrate that our method exhibits stronger robustness towards unseen perturbations in test data. Code is released here: https:
//github.com/ExplainableML/UncertaintyAwareCycleConsistency. 1

Introduction
Translating an image from a distribution, i.e. source domain, to an image in another distribution, i.e. target domain, with a distribution shift is an ill-posed problem as a unique deterministic one-to-one mapping may not exist between the two domains. Furthermore, since the correspondence between inter-domain samples may be missing, their joint-distribution needs to be inferred from a set of marginal distributions. However, as inﬁnitely many joint distributions can be decomposed into a ﬁxed set of marginal distributions [1, 2, 3], the problem is ill-posed in the absence of additional constraints.
Deep learning-based methods tackle the image-to-image translation task by learning inter-domain mappings in a paired or unpaired manner. Paired image translation methods [4, 5, 6, 7, 8, 9] exploit the inter-domain correspondence by penalizing the per-pixel residual (using l1 or l2 norm) between the output and corresponding ground-truth sample. Unpaired image translation approaches [1, 10, 11, 12, 13, 14] often use adversarial networks with an additional constraint on the image or feature space imposing structure on the underlying joint distribution of the images from the different domains.
Both paired and unpaired image translation approaches often learn a deterministic mapping between the domains where every pixel in the input domain is mapped to a ﬁxed pixel value in the output domain. However, such a deterministic formulation can lead to mode collapse while at the same time not being able to quantify the model predictive uncertainty important for critical applications, e.g., medical image analysis. It is desirable to test the performance of the model on unseen perturbed input at test-time, to improve their applicability in the real world. While robustness to outliers is a focus in some domains [15, 16, 17, 18], it has not attracted as much attention in unpaired translation.
To address these limitations, we propose an unpaired (unsupervised) probabilistic image-to-image translation method trained without inter-domain correspondence in an end-to-end manner. The probabilistic nature of this method provides uncertainty estimates for the predictions. Moreover, modelling the residuals between the predictions and the ground-truth with heavy-tailed distributions 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
makes our model robust to outliers and various unseen data. Accordingly, we compare various state-of-the-art models and our model in their capacity to handle samples from similar distribution as training-dataset as well as perturbed samples, in the context of unpaired translation.
Our contributions are as follows. (i) We propose an unpaired probabilistic image-to-image translation framework based on Uncertainty-aware Generalized Adaptive Cycle Consistency (UGAC). Our framework models the residuals between the predictions and the ground-truths with heavy-tailed distributions improving robustness to outliers. Probabilistic nature of UGAC also provides uncertainty estimates for the predictions. (ii) We evaluate UGAC on multiple challenging datasets: natural images consisting Cityscapes [19], Google aerial maps and photos [4], CMP Facade [20] and medical images consisting of MRI from IXI [21]. We compare our model to seven state-of-the-art image-to-image translation methods [12, 22, 1, 11, 10, 23]. Our results demonstrate that while UGAC performs competitively when tested on unperturbed images, it improves state-of-the-art methods substantially when tested on unseen perturbations, establishing its robustness. (iii) We show that our estimated uncertainty scores correlate with the model predictive errors (i.e., residual between model prediction and the ground-truth) suggesting that it acts as a good proxy for the model’s reliability at test time. 2