Abstract
Various recent proposals increase the distinguishing power of Graph Neural Net-works (GNNs) by propagating features between k-tuples of vertices. The distin-guishing power of these “higher-order” GNNs is known to be bounded by the k-dimensional Weisfeiler-Leman (WL) test, yet their O(nk) memory requirements limit their applicability. Other proposals infuse GNNs with local higher-order graph structural information from the start, hereby inheriting the desirable O(n) memory requirement from GNNs at the cost of a one-time, possibly non-linear, preprocess-ing step. We propose local graph parameter enabled GNNs as a framework for studying the latter kind of approaches. We precisely characterize their distinguish-ing power, in terms of a variant of the WL test, and in terms of the graph structural properties that they can take into account. Local graph parameters can be added to any GNN architecture, and are cheap to compute. In terms of expressive power, our proposal lies in the middle of GNNs and their higher-order counterparts. Further, we propose several techniques to aid in choosing the right local graph parameters.
Our results connect GNNs with deep results in ﬁnite model theory and ﬁnite vari-able logics. Our experimental evaluation shows that adding local graph parameters often has a positive effect on a variety of GNNs, datasets and graph learning tasks. 1

Introduction
Context. Graph neural networks (GNNs) [Merkwirth and Lengauer, 2005, Scarselli et al., 2009], and its important class of Message Passing Neural Networks (MPNNs) [Gilmer et al., 2017], are one of the most popular methods for graph learning tasks. Such MPNNs use an iterative message passing scheme, based on the adjacency structure of the underlying graph, to compute vertex (and graph) embeddings in some real Euclidean space.
The expressive (or distinguishing) power of MPNNs is, however, rather limited [Morris et al., 2019,
Xu et al., 2019]. Indeed, MPNNs will always identically embed two vertices (graphs) when these vertices (graphs) cannot be distinguished by the one-dimensional Weisfeiler-Leman (WL) algorithm.
Two graphs G1 and H1 and vertices v and w that cannot be distinguished by WL (and thus any
MPNN) are shown in Fig. 1. The expressive power of WL is well-understood [Cai et al., 1992, Dell et al., 2018, Arvind et al., 2020] and basically can only use tree-based structural information in the graphs to distinguish vertices. Hence, no MPNN can detect that vertex v in Fig. 1 is part of a 3-clique, whereas w is not. Similarly, MPNNs cannot detect that w is part of a 4-cycle, whereas v is not. Further limitations of WL in terms of graph properties can be found, e.g., in Arvind et al. [2020],
Chen et al. [2020] and Tahmasebi and Jegelka [2020].
To remedy the weak expressive power of MPNNs, so-called higher-order MPNNs were proposed
[Maron et al., 2019a, Morris et al., 2019, 2020], whose expressive power is well-understood and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
measured in terms of the k-dimensional WL procedures (k-WL) [Maron et al., 2019a, Chen et al., 2019a, Geerts, 2020, Sato, 2020, Azizian and Lelarge, 2021]. In a nutshell, k-WL operates on k-tuples of vertices and allows to distinguish vertices (graphs) based on structural information related to graphs of treewidth k [Dvorak, 2010, Dell et al., 2018]. By deﬁnition, WL = 1-WL. As an example, 2-WL can detect that vertex v in Fig. 1 belongs to a 3-clique or a 4-cycle since both have treewidth two. While more expressive than WL, the GNNs based on k-WL require O(nk) operations in each iteration, where n is the number of vertices, hereby hampering their applicability.
A more practical approach is to extend the expressive power of MPNNs whilst pre-serving their O(n) cost in each iteration.
Various such extensions [Kipf and Welling, 2017, Chen et al., 2019a, Li et al., 2019,
Ishiguro et al., 2020, Bouritsas et al., 2020,
Geerts et al., 2021] achieve this by infusing
MPNNs with local graph structural infor-mation from the start. That is, the iterative message passing scheme of MPNNs is run on vertex labels that contain quantitative in-formation about local graph structures. v (2) (2) (2) (2) (2) (2) w (0) (0) (0) (0) (0) (0)
G1
H1
Figure 1: Two graphs that are indistinguishable by the
WL-test. The numbers between round brackets indicate how many homomorphic images of the 3-clique each vertex is involved in.
It is easy to see that such architectures can go beyond the WL test: for example, adding triangle counts to MPNNs sufﬁces to distinguish the vertices v and w and graphs G1 and H1 in Fig. 1. Moreover, the cost is a single preprocessing step to count local graph parameters, thus maintaining the O(n) cost in the iterations of the MPNN. While there are some partial results showing that local graph parameters increase expressive power [Bouritsas et al., 2020, Li et al., 2019], their precise expressive power and relationship to higher-order MPNNs was unknown, and there is little guidance in terms of which local parameters do help MPNNs and which ones do not. The main contribution of this paper is a precise characterization of the expressive power of MPNNs with local graph parameters and its relationship to the hierarchy of higher-order MPNNs.
Our contributions.
In order to nicely formalize local graph parameters, we propose to extend vertex labels with homomorphism counts of small graph patterns.1 More precisely, given graphs
P and G, and vertices r in P and v in G, we propose to augment the initial features of v with the number of homomorphisms from P to G that map r to v, denoted by hom(P r, Gv), as a way to capture local structural information. More generally, homomorphism counts for a collection of graphs are considered. Indeed, we propose F-MPNNs where F = {P r (cid:96) } is a set of (graph) patterns, which extend MPNNs by (i) ﬁrst allowing a preprocessing step that labels each vertex v of a graph (cid:96) , Gv)(cid:1), and (ii) then run an MPNN on this labelling.
G with the vector (cid:0)hom(P r
Our main contributions are the following: 1 , Gv), . . . , hom(P r 1 , . . . , P r 1. We precisely characterize the expressive power of F-MPNNs by means of an extension of WL, denoted by F-WL. This characterization gracefully extends the characterization for standard MPNNs, mentioned earlier, by setting F = ∅, and provides insights in the expressive power of existing MPNN extensions, most notably the Graph Substructure Networks of Bouritsas et al. [2020]. 2. We compare F-MPNNs to higher-order MPNNs, which are characterized in terms of the k-WL-test. On the one hand, while F-MPNNs strictly increase the expressive power of the WL-test, for any
ﬁnite set F of patterns, 2-WL can distinguish graphs which F-MPNNs cannot. On the other hand, for each k ≥ 1 there are patterns P such that {P }-MPNNs can distinguish graphs which k-WL cannot. 3. We deal with the challenging problem of pattern selection and comparing F-MPNNs based on the patterns included in F. We prove two partial results: one establishing when a pattern P in F is redundant, and another result indicating when P does add expressive power, based on the treewidth of P compared to the treewidth of other patterns in F. 4. Our theoretical results are complemented by an experimental study in which we show that for various GNN architectures, datasets and graph learning tasks, all part of the recent benchmark by
Dwivedi et al. [2020], the augmentation of initial features with homomorphism counts of graph patterns has often a positive effect, and the cost for computing these counts incurs little to no overhead. 1We recall that homomorphisms are edge-preserving mappings between the vertex sets. 2
As such, we believe that F-MPNNs not only provide an elegant theoretical framework for understanding local graph parameter enabled MPNNs, they are also a valuable alternative to higher-order MPNNs as a way to increase the expressive power of MPNNs. In addition, and as will be explained in Section 2, F-MPNNs provide a unifying framework for understanding the expressive power of several other existing extensions of MPNNs. Proofs of our results and further details on the relationship to existing approaches and experiments can be found in the supplementary material.