Abstract
The audio-visual video parsing task aims to temporally parse a video into audio or visual event categories. However, it is labor-intensive to temporally annotate audio and visual events and thus hampers the learning of a parsing model. To this end, we propose to explore additional cross-video and cross-modality supervisory signals to facilitate weakly-supervised audio-visual video parsing. The proposed method exploits both the common and diverse event semantics across videos to identify audio or visual events. In addition, our method explores event co-occurrence across audio, visual, and audio-visual streams. We leverage the explored cross-modality co-occurrence to localize segments of target events while excluding irrelevant ones. The discovered supervisory signals across different videos and modalities can greatly facilitate the training with only video-level annotations. Quantitative and qualitative results demonstrate that the proposed method performs favorably against existing methods on weakly-supervised audio-visual video parsing. 1

Introduction
Humans perceive multisensory signals via seeing, hearing, touching, etc., and obtain multimodal information while exploring the surrounding environments. Visual and audio signals, the most common modalities, motivate researchers to jointly comprehend audio-visual events (e.g., see people singing and hear their sounds) [1, 2, 3, 4, 5, 6, 7]. Events visible in images while hearable in audio are referred to as audio-visual events. However, learning-based models tend to recognize a particular audio-visual event by using the data from the dominant modality with richer information and overlook clues from either audio only or visual only events which still contribute to holistic video understanding. Therefore, the resultant models can generalize well on audio-visual events only instead of comprehensively understanding all kinds of video events. To address this issue, we target at audio-visual video parsing [4, 6] where predictions for audio, visual, and audio-visual events with temporal boundaries are all required but separately evaluated.
The time-consuming and labor-intensive annotation process poses a major challenge for the audio-visual video parsing task. To address this issue, Tian et al. [4] handle this task in a weakly-supervised manner given only video-level labels, which indicate events of presence without temporal bound-aries and detailed modalities. They develop an audio-visual co-attention mechanism to assemble discriminative multimodal representations and use multiple instance learning to aggregate frame-level predictions into video-level ones. However, video-level labels alone cannot identify which modality events are from. Wu et al. [6] then propose to perform label reﬁnement by swapping the audio and visual tracks of different videos to estimate and remove irrelevant event categories for each modality. They further adopt temporal contrastive learning to align audio and visual representations from the same frame. However, the contrastive learning is based on the assumption that audio and 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
visual signals are synchronized, which may not hold in practical scenarios with complex events.
Furthermore, these methods [4, 6] only consider audio and visual tracks of a single video without exploiting the relationship across categories and videos, which also provide rich shared semantics regarding event categories.
In this work, we propose to leverage audio and visual data across different videos to explore shared information of each category. For example, videos with singing events may have similar patterns whatever in an audio or a visual modality. By observing all videos in a training batch, we can not only explore the shared semantics among audio-visual data but also exclude unrelated events. In addition to the relationship across different videos, we exploit the dependency between event categories. For example, when people are singing, there is usually a music accompaniment. Therefore, we propose to treat audio, visual, and audio-visual streams separately and adopt an audio-visual class co-occurrence module that jointly explores the relationship of different categories among all streams. By measuring the similarity of event categories from audio, visual, and audio-visual events, the correlated events are more likely to be correctly determined as the presence or absence of event categories. Such a strategy can robustly learn the correlation of categories within/across modality and fully exploit video data. The proposed strategy can be applied to existing methods on video parsing.
We evaluate the proposed method on the LLP [4] dataset. Videos are parsed into audio, visual, and audio-visual events under both segment and event levels, and evaluated with F-scores metrics. Both qualitative and quantitative results demonstrate the effectiveness of the proposed method on the audio-visual video parsing task. The main contributions of this work are summarized as follows:
• We leverage audio and visual data across different videos and tracks, which can learn common semantics of the same events and discern unrelated clues.
• We develop an audio-visual event co-occurrence module that jointly considers the relation-ship of categories in audio, visual, and audio-visual modalities, which can prevent models from differentiating the representations of the related events.
• Qualitative and quantitative experimental results on the benchmark dataset demonstrate that the proposed method performs favorably against the state-of-the-arts in various settings. 2