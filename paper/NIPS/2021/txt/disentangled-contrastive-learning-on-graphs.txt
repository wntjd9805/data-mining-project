Abstract
Recently, self-supervised learning for graph neural networks (GNNs) has attracted considerable attention because of their notable successes in learning the repre-sentation of graph-structure data. However, the formation of a real-world graph typically arises from the highly complex interaction of many latent factors. The existing self-supervised learning methods for GNNs are inherently holistic and neglect the entanglement of the latent factors, resulting in the learned representa-tions suboptimal for downstream tasks and difﬁcult to be interpreted. Learning disentangled graph representations with self-supervised learning poses great chal-lenges and remains largely ignored by the existing literature. In this paper, we introduce the Disentangled Graph Contrastive Learning (DGCL) method, which is able to learn disentangled graph-level representations with self-supervision. In particular, we ﬁrst identify the latent factors of the input graph and derive its fac-torized representations. Each of the factorized representations describes a latent and disentangled aspect pertinent to a speciﬁc latent factor of the graph. Then we propose a novel factor-wise discrimination objective in a contrastive learning manner, which can force the factorized representations to independently reﬂect the expressive information from different latent factors. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of our method against several state-of-the-art baselines. 1

Introduction
Graph structured data is ubiquitous in the real world, e.g., social networks, biology networks, trafﬁc networks, etc. Recently, graph neural networks (GNNs) have become increasingly prevalent in learning graph representations in a supervised manner, demonstrating their strength in a wide variety of research ﬁelds [1, 2, 3, 4]. GNNs require task-dependent annotated labels to learn effective representations, which are extremely scarce, or even unavailable in practice, thus motivating the advent of self-supervised graph representation learning.
Contrastive learning, as a discriminative approach pulling similar samples close and pushing dissimilar samples far away, has become a dominant strategy in self-supervised graph representation learning [5, 6, 7, 8, 9, 10, 11]. Despite their notable successes, the existing graph contrastive learning methods generally adopt a holistic scheme, i.e., the learned representations characterize graphs as a perceptual whole, ignoring the nuances between different aspects of the graph. In fact, the formation of a graph typically follows a relational process in the real world, driven by many complex latent factors. For example, in social networks, a social group may have several communities originated from different relations (e.g., friends, colleagues, etc.) or interests (e.g., sports, games, etc.) [12]. And a molecular graph may consist of various groups of atoms and bonds representing different functional units [13].
The complex relations among the multiple latent factors bring an urge for disentangling these factors
∗Corresponding authors 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
in contrastive graph representation learning, which remains unexplored by the existing holistic works.
As a result, the graph representations learned by the existing methods contain a mixture of entangled factors, harming interpretability and leading to suboptimal performance for predictive tasks involving whole graph representations.
In this paper, we propose to learn disentangled contrastive graph representation, for the ﬁrst time.
Although disentangled representation learning, which aims to characterize the various underlying explanatory factors behind the observed data in different parts of the factorized representations [14, 15], has been demonstrated to be more explainable [12] and generalizable [14], disentangled graph contrastive learning faces the following two challenges. (1) Tailored graph encoder for disentangled contrastive learning. The graph encoder should be carefully designed so that it can be sufﬁciently expressive to infer the disentangled latent factors in the graph. (2) Tailored discrimination tasks designed for disentangled graph contrastive learning. Since task-dependent labels are not available in the self-supervised setting, disentangled graph contrastive learning can only utilize the limited amount of self-supervision information. This implies that the discrimination tasks should be well-designed for disentangled contrastive representation learning on graphs.
To tackle these challenges, we propose a novel disentangled graph contrastive learning model (DGCL) capable of disentangled contrastive learning on graphs. In particular, we ﬁrst design a disentangled graph encoder whose key ingredient is a multi-channel message-passing layer. Each channel is tailored to aggregate features only from one disentangled latent factor. Then a separate readout operation in each channel summarizes the speciﬁc aspect of the graph according to the corresponding latent factor, so as to produce the disentangled graph representation. Next, we conduct contrastive learning in each representation subspace characterized by each factor independently instead of in the whole representation space. This novel factor-wise contrastive approach can ensure that each disentangled factor of the vectorized representations is sufﬁciently discriminative only under one speciﬁc aspect of the whole graph. Thus the representations are encouraged to be disentangled and best characterize the aspect pertinent to a latent factor of the graph. Compared with the existing methods, our proposed DGCL model encodes a graph with multiple disentangled representations, making it possible to explore the meaning of each channel, which beneﬁts in more explainability for producing graph representations.
We conduct extensive experiments on both synthetic graph dataset and empirical well-known graph benchmarks. The results show that the representations learned from DGCL can achieve substantial performance gains on the downstream graph classiﬁcation task compared with various state-of-the-art baselines.
The contributions of this paper are summarized as follows:
• We propose a disentangled graph contrastive learning model (DGCL), which is able to learn disentangled graph representation via factor-wise contrastive learning. To the best of our knowledge, we are the ﬁrst to study disentangled self-supervised graph representation learning.
• We propose a disentangled graph encoder to capture multiple aspects of graphs through learning disentangled latent factors on graphs. We further present the factor-wise contrastive learning approach on tailored discrimination tasks in terms of each latent factor independently.
• We conduct extensive experiments to verify the efﬁcacy of our proposed model for the graph classiﬁcation task. The results on several graph classiﬁcation datasets demonstrate that DGCL achieves state-of-the-art performance by signiﬁcantly outperforming the baselines.
We introduce the problem formulation and preliminaries in Section 2. In Section 3, we describe the details of our proposed method. Section 4 presents the experimental results, including quantitative and qualitative comparisons. We review the related work in Section 5. Finally, we conclude our work in Section 6. 2 Problem Formulation and Preliminaries 2.1 Problem Formulation
Let G = {Gi}N i=1 be a graph dataset with N graphs. The key of most self-supervised graph representation learning methods, including ours, is to derive a graph encoder f (·), which outputs a 2
Figure 1: The framework of DGCL Model. (1) The input graph Gi undergoes graph augmentations to produce G(cid:48) i, and both of them are fed into the shared disentangled graph encoder fθ(·). (2) In the encoder fθ(·), the node features H0 are ﬁrst aggregated by L message-passing layers and then taken as the input of a multi-channel message-passing layer. (3) Based on the disentangled graph representation zi, the factor-wise contrastive learning aims to maximize the agreement under each latent factor and provide feedback for the encoder to improve the disentanglement. This example assumes that there are three latent factors, hence the three channels. d-dimensional representation zi = f (Gi) ∈ Rd for each input graph, such that Z = {zi}N i=1 best describes G. In this work, we aim to learn a multi-channel graph encoder fθ(·) with parameters θ, so that the output zi can be a disentangled representation, i.e. fθ(·) = {f (k) k=1, where K is the number of channels. To be speciﬁc, zi is expected to be composed of K independent components, i.e., zi = [zi,1, zi,2, . . . , zi,K], where zi,k = f (k) (Gi) ∈ R∆d, k ∈ [1, K], ∆d = d/K, assuming that there are K latent factors behind the graph instances to be disentangled. The kth component zi,k is for characterizing the aspect of Gi that is pertinent to factor k accurately. We also assume that the value of zi,k will be merely a white noise vector if the input graph Gi does not contain any information of factor k. Note that we focus primarily on undirected graphs in our method, although it can be straightforwardly extended to directed graphs. (·)}K
θ
θ 2.2 Preliminaries on Contrastive Learning
Unlike generative models, contrastive learning is an instance-wise discriminative approach that aims at making similar instances closer and dissimilar instances far from each other in representation space [16, 17]. It treats each instance in the dataset as a distinct class of its own and trains a classiﬁer to distinguish between individual instance classes [18, 19]. Given a dataset X = {xi}N i=1, each instance xi is assigned with a unique surrogate label yi, since no ground-truth labels are given. yi is often regarded as the ID of the instance in the dataset, i.e., yi = i. So the probability classiﬁer is deﬁned as: pθ(yi|xi) = exp φ(vi, v(cid:48)
) yi j=1 exp φ(vi, v(cid:48) yj (cid:80)N
,
) (1) where θ denotes the parameters of the encoder. Both vi and v(cid:48) yi are the embeddings from xi, which are generated from two different encoders [20], or a shared encoder [21]. Before being passed into the encoder, the input xi could undergo data augmentations [21], which play a critical role in deﬁning effective predictive tasks for learning the encoder. φ is the similarity function, often adopting cosine similarity with temperature τ [22], i.e., φ(vi, v(cid:48) vi/τ , assuming the embeddings are yi (cid:96)2-normalized. Then the learning objective is to maximize the joint probability (cid:81)N i=1 p(yi|xi) over the dataset, namely minimize the negative log-likelihood function (cid:80)N i=1 (cid:96)i, if let (cid:96)i = −log p(yi|xi).
) = v(cid:48)(cid:62) yi 3
Note that loss (cid:96)i could be NCE loss [18], InfoNCE loss [23], or NT-Xent loss [21]. The encoder will be encouraged to learn a representation space where samples (e.g., augmented data) from the same instance (e.g., an image, a graph) are pulled closer and samples from different instances are pushed apart [16]. For convenience, we follow the settings above in this work. 3 Disentangled Graph Contrastive Learning
In this section, we present the proposed DGCL model. The framework of DGCL is shown in Figure 1. In Section 3.1, we introduce the disentangled graph encoder to identify the complex latent factors and capture multiple aspects of graphs. Then in Section 3.2, we propose a factor-wise contrastive learning approach to conduct instance discrimination under each latent factor independently. The objective and the details of the optimization are derived in Section 3.3 and Section 3.4, respectively. 3.1 Disentangled Graph Encoder
The key of the disentangled graph encoder is to produce the factorized graph representation zi =
[zi,1, zi,2, . . . , zi,K] for each input graph Gi ∈ G. Based on the factorized representation, we can infer the related latent factors behind the graph.
Generally, GNNs use the graph structure and node features to learn the representation vector hv of each node v with a message-passing mechanism, i.e., iteratively updating the representation of a node by aggregating representations of its neighbors. The propagation of the lth layer is formulated as:
: u ∈ N (v)})), (2) v = COMBINEl(hl−1 hl
, AGGREGATEl({hl−1 v u v is the representation of node v at the lth layer and h0 where hl v is initialized with node features.
N (v) is the neighborhood to node v. We use the term GNN to indicate the message-passing layer in
Eq. (2).
Let Hl = {hl v|v ∈ V } be the node embeddings after the lth GNN, where V denotes the node set of the graph. After applying L traditional message-passing layers, we propose a graph-disentanglement layer to learn the disentangled representations. The goal is to extract features speciﬁc to each latent factor with a separate channel. Speciﬁcally, we adopt K separate channels to identify the complex heterogeneous latent factors and capture multiple aspects of the input graph. For each channel, we
ﬁrst utilize a GNNk to propagate information with its own parameters: HL+1 k = GNNk(HL, A), where A is the adjacency matrix of the input graph. HL+1 is the node embeddings which is only pertinent to the kth latent factor. Then the READOUT function (i.e., pooling function) of each channel is used to summarize all the obtained node representations into a ﬁxed-length graph-level representation: hGi,k = READOUTk({HL+1
}). Finally, each channel outputs the factorized graph representation with a separate MLP: zi,k = MLPk(hGi,k).
Compared with the existing graph encoders that are inherently holistic, our disentangled graph encoder consists of K channels, rending the possibility to identify the complex heterogeneous latent factors and capture multiple aspects of graphs. k k 3.2 Disentangled Factor-wise Contrastive Learning
Unlike the existing contrastive learning methods, DGCL designs a novel factor-wise instance dis-criminative task and learns to solve this task under each latent factor independently. This design not only makes similar samples closer and dissimilar samples far from each other in the representation space, but also encourages the learned representation to incorporate factor-level information for disentanglement.
Speciﬁcally, we assume that the formation of real-world graphs is usually driven by multiple latent heterogeneous factors. So the instance discriminative task should be represented as the expectation of several subtasks under the latent factors: pθ(yi|Gi) = Epθ(k|Gi) [pθ(yi|Gi, k)] . (3)
Here pθ(k|Gi) is the probability distribution over latent factors for the input graph Gi. pθ(yi|Gi, k) denotes the instance discrimination subtask under the kth latent factor. 4
Firstly, given the representation zi of Gi derived from the disentangled graph encoder fθ(·), we present a prototype-based method to obtain pθ(k|Gi). We introduce K latent factor prototypes
{ck}K k=1, and the probability of the kth latent factor reﬂected in Gi is parameterized as: pθ(k|Gi) = exp φ(zi,k, ck) k=1 exp φ(zi,k, ck) (cid:80)K
, (4) where φ is the cosine similarity with temperature τ ,
COSINE(a, b) = a(cid:62)b/((cid:107)a(cid:107)2 (cid:107)b(cid:107)2).
Then, we deﬁne the instance discrimination subtask under the kth latent factor as: i.e., φ(a, b) = COSINE(a, b)/τ and pθ(yi|Gi, k) = exp φ(zi,k, z(cid:48) yi,k) j=1 exp φ(zi,k, z(cid:48) (cid:80)N yj ,k)
, (5) where zi,k and z(cid:48) yi,k are the disentangled representations produced by the shared graph encoder, and yi is the unique surrogate label (see Section 2) of the graph Gi. In our method, we follow [18, 19] to implement yi as the ID of the graph in the dataset, i.e., yi = i. For notation convenience, we do not distinguish yi and i hereafter when there is no risk of confusion.
Next, we describe the process to get z(cid:48) yi,k in Eq. (5). First, the input graph Gi undergoes graph data augmentations to obtain its correlated views G(cid:48) i, and they form a positive pair. Data augmentation is expected to create novel and realistically rational data by applying certain transforma-tions that do not affect the label, and plays a critical role in deﬁning effective predictive tasks [21, 11].
We follow [11] to adopt four types of graph augmentation strategies, including node dropping, edge perturbation, attribute masking, and subgraph sampling. More details of graph augmentations can be found in supplementary material. Then, the augmented graph G(cid:48) i is also fed into the shared disentangled graph encoder fθ(·) to produce z(cid:48) yi,k. Given the disentangled representations zi,k and z(cid:48) i respectively, we conduct factor-wise contrastive learning for each latent factor independently as Eq. (5). i,k of the Gi and G(cid:48) i,k, i.e. z(cid:48) i,k, i.e. z(cid:48) 3.3 Evidence Lower Bound (ELBO)
We present the objective of our method. Following the existing methods [18, 19], we aim to maximize the joint probability (cid:81)N i=1 p(yi|Gi) over the graph dataset G = {Gi}N i=1. We learn the model parameters θ by maximizing the log-likelihood:
θ∗ = arg max
θ
N (cid:88) i=1 log pθ(yi|Gi) = arg max
θ
N (cid:88) i=1 log Epθ(k|Gi) [pθ(yi|Gi, k)] . (6)
However, directly maximizing the log-likelihood function is difﬁcult because of the latent factors.
Therefore, we instead optimize the evidence lower bound (ELBO) of the log-likelihood function given by Theorem 1. See the supplementary material for the proof.
Theorem 1. The log likelihood function of each graph log pθ(yi|Gi) is lower bounded by the ELBO:
L(θ, i) = Eqθ(k|Gi,yi)[log pθ(yi|Gi, k)] − DKL(qθ(k|Gi, yi) (cid:107) pθ(k|Gi)).
To make the ELBO as tight as possible, we require that qθ(k|Gi, yi) is close to pθ(k|Gi, yi), whose detailed implementations are provided in the next subsection (see Eq. (9) and Eq. (7)). In the ELBO
L(θ, i), pθ(yi|Gi, k) and pθ(k|Gi) have been introduced in Eq. (5) and Eq. (4), respectively, and qθ(k|Gi, yi) is a variational distribution to infer the posterior distribution of the latent factors after observing both Gi and its correlated view G(cid:48) yi. 3.4 Optimization
We introduce a variational distribution qθ(k|Gi, yi) to infer the posterior probability pθ(k|Gi, yi) that is deﬁned with Bayes’ theorem as follows: pθ(k|Gi, yi) = pθ(k|Gi)pθ(yi|Gi, k) k=1 pθ(k|Gi)pθ(yi|Gi, k) (cid:80)K
. (7) 5
pθ(k|Gi, yi) is the probability of the kth latent factor pertinent to both Gi and the augmented G(cid:48) i simultaneously. Compared with the prior distribution pθ(k|Gi) in Eq. (4), pθ(k|Gi, yi) incorporates more useful information (i.e., factor-wise similarity) from pθ(yi|Gi, k). Although both pθ(k|Gi) and pθ(k|Gi, yi) are designed to infer the latent factor distribution, pθ(k|Gi) is calculated only given the graph Gi, but pθ(k|Gi, yi) is calculated after observing Gi, the augmented version G(cid:48) yi , and their similarities under the speciﬁc latent factor.
However, we cannot compute the posterior probability tractably because of the term pθ(yi|Gi, k). If we directly calculate pθ(yi|Gi, k) according to Eq. (5), all the instances in the dataset G are needed for computing the denominator in Eq. (5) , which could be computationally prohibitive [24, 18, 19].
To tackle this obstacle, several strategies are proposed in the literature, including memory bank
[18, 20], dynamic dictionary [25], NT-Xent loss [21]. Here, we adopt NT-Xent loss on a minibatch
B ⊆ G. So in practice, the instance discrimination under each latent factor is calculated by:
ˆpθ(yi|Gi, k) = exp φ(zi,k, z(cid:48) j∈B,j(cid:54)=i exp φ(zi,k, z(cid:48) i,k) (cid:80)|B| j,k)
.
We approximate the posterior probability pθ(k|Gi, yi) with a variational distribution deﬁned as: qθ(k|Gi, yi) = pθ(k|Gi)ˆpθ(yi|Gi, k) k=1 pθ(k|Gi)ˆpθ(yi|Gi, k) (cid:80)K
. (8) (9)
Finally, we seek to learn the parameters θ of the disentangled graph encoder. More speciﬁcally, we calculate qθ(k|Gi, yi) and maximize the ELBO over a mini-batch B using mini-batch gradient ascent:
L(θ, B) =
L(θ, i). (cid:88) i∈B (10)
Encourage disentanglement. Note that our objective and its optimization can inherently encourage disentanglement without adding extra regularization term (e.g., minimizing mutual information). The reason is that factorizing the instance discrimination into K factor-wise subtasks will enforce the independence of the learned graph representation zi. Besides, qθ(k|Gi, yi) is computed based on kth and other K − 1 latent factors. Thus, the graph encoder is forced to preserve exclusive information in each channel to get more accurate approximation to the posterior, if a tighter ELBO is expected. The strong inductive biases in DGCL encourage to learn disentangled graph representations that match the ground truth factors behind the graphs. 4 Experiments
We empirically evaluate our proposed method through experiments, and analyze its behavior on synthetic graph dataset to gain deeper insight. Ablation studies are conducted to show the effectiveness of the proposed method. We provide more discussions in supplementary material, including the complexity of our method, the impact of the hyper-parameters, etc. 4.1 Experimental Setup
Datasets. To demonstrate the advantages of our method, we conduct experiments on nine well-known graph classiﬁcation datasets including four bioinformatics datasets, i.e., MUTAG, PTC-MR,
NCI1, PROTEINS, and ﬁve social network datasets, i.e., COLLAB, IMDB-BINARY, IMDB-MULTI,
REDDIT-BINARY, and REDDIT-MULTI-5K. We also adopt a larger graph dataset ogbg-molhiv from Open Graph BenchMark (OGB) [26]. More details are provided in supplementary material.
Baselines. We compare DGCL with the following two groups of baselines. One group of baselines are graph kernels including Shortest Path Kernel (SP) [27], Graphlet Kernel (GK) [28], Weisfeiler-Lehman Sub-tree Kernel (WL) [29], Deep Graph Kernels (DGK) [30], and Multi-Scale Laplacian
Kernel (MLG) [31]. The other group of baselines are classical unsupervised graph representation learning methods including node2vec [32], sub2vec [33], graph2vec [34], GVAE [35], and more recent contrastive graph representation learning methods including InfoGraph [5], GCC [10], MVGRL
[9], and GraphCL [11].
Evaluation. To verify the effectiveness of our method, we follow the common evaluation protocols in the existing literature [34, 5, 30, 11], where graph embeddings are generated from each method 6
Table 1: Graph classiﬁcation accuracy (%) of DGCL and baselines. In each column, the boldfaced score denotes the best result and the underlined score represents the second-best result. “–” indicates the result is not reported in the paper.
MUTAG PTC-MR PROTEINS
NCI1
IMDB-B IMDB-M RDT-B
RDT-M5K COLLAB
SP
GK
WL
DGK
MLG node2vec sub2vec graph2vec
GVAE
InfoGraph
GCC
MVGRL
GraphCL
DGCL 85.2±2.4 81.7±2.1 80.7±3.0 87.4±2.7 87.9±1.6 72.6±10.2 61.1±15.8 83.2±9.3 87.7±0.7 89.0±1.1 – 89.7±1.1 86.8±1.3 92.1±0.8 58.2±2.4 57.3±1.4 58.0±0.5 60.1±2.6 63.3±1.5 58.6±8.0 60.0±6.4 60.2±6.9 61.2±1.8 61.7±1.4 – 62.5±1.7 63.6±1.8 65.8±1.5 75.1±0.5 71.7±0.6 72.9±0.6 73.3±0.8 76.1±2.0 57.5±3.6 53.0±5.6 73.3±2.1 – 74.4±0.3 – – 74.4±0.5 76.4±0.5 73.0±0.2 62.3±0.3 80.0±0.5 80.3±0.5 80.8±1.3 54.9±1.6 52.8±1.5 73.2±1.8 – 76.2±1.1 – – 77.9±0.4 81.9±0.2 55.6±0.2 65.9±1.0 72.3±3.4 67.0±0.6 66.6±0.3 – 55.3±1.5 71.1±0.5 70.7±0.7 73.0±0.9 72.0 74.2±0.7 71.1±0.4 75.9±0.7 38.0±0.3 43.9±0.4 47.0±0.5 44.6±0.5 41.2±0.0 – 36.7±0.8 50.4±0.9 49.3±0.4 49.7±0.5 49.4 51.2±0.5 50.7±0.4 51.9±0.4 64.1±0.1 77.3±0.2 68.8±0.4 78.0±0.4 – – 71.5±0.4 75.8±1.0 87.1±0.1 82.5±1.4 89.8 84.5±0.6 89.5±0.8 91.8±0.2 39.6±0.2 41.0±0.2 46.1±0.2 41.3±0.2 – – 36.7±0.4 47.9±0.3 52.8±0.2 53.5±1.0 53.7 – 56.0±0.3 56.1±0.2 – 72.8±0.3 – 73.1±0.3 – – – – – 70.7±1.1 78.9 – 71.4±1.2 81.2±0.3 and then fed into a downstream SVM classiﬁer. We adopt the 10-fold cross validation accuracy, and report the mean accuracy (%) with standard variation after ﬁve repeated runs.
Implementation Details. For a fair comparison, the hyper-parameters of the graph augmentations are kept consistent with GraphCL. We use GIN [4] as the message-passing layers since it is shown to be one of the most expressive message-passing GNNs. Since the ground-truth number of the latent factors is unknown, we search the number of channels K from 1 to 10. More implementation details can be found in supplementary material. 4.2 Results on Real Benchmark Graphs
The results are reported in Table 1. We can see that the graph contrastive learning methods generally outperform the graph kernel methods or the classical unsupervised methods, which verify the effectiveness of contrastive learning. Our method DGCL consistently achieves the best performance compared with other contrastive methods (e.g., MVGRL, GrpahCL) and classical unsupervised methods (e.g., graph2vec, GVAE), demonstrating the superiority of our method. For example, our method increases the classiﬁcation accuracy by 2.4%, 2.2%, and 2.0% compared with the second-best methods on MUTAG, PTC-MR, and RDT-B, respectively. We attribute the results to the fact that these existing methods fail to identify the underlying latent factors which are important in preserving graph properties and can not learn the disentangled representations. In contrast, we disentangle graph representations to explicitly consider the entanglement of heterogeneous factors. When compared to graph kernel methods, our method also has the best accuracy on all the datasets. Notice that none of these kernel methods is consistently competitive across all of the datasets, as opposed to our method.
Besides the common setting of unsupervised representation learning above, we also consider another setting of semi-supervised representation learning [11] on the ogbg-molhiv from Open Graph Bench-mark [26] to better evaluate our method. Speciﬁcally, we ﬁrst perform pre-training with all training data without labels. Then we conduct ﬁne-tuning on the partial labeled training data and evaluation on the validation/test sets. The task on the ogbg-molhiv dataset is binary classiﬁcation evaluated by
ROC-AUC metric, instead of accuracy in the experiments above. We adopt the provided evaluator and dataset splits for a fair comparison. We compare DGCL with the strong self-supervised baseline
GraphCL with 1%, 10%, and 20% label rate for ﬁne-tuning. The results are shown in Figure 2. With the increase of the label rate, the results improve for both our method and GraphCL. Our method achieves signiﬁcant improvement over GraphCL with 1.4%, 3.0%, and 1.4% performance gains at 1%, 10%, and 20% label rate, respectively. The results illustrate that our method is also able to handle large-scale graphs, demonstrating the beneﬁt of learning disentangled graph representations in the contrastive manner. 4.3 Results on Synthetic Graphs
To further investigate the behavior of our method, we generate a synthetic dataset consisting of 1,000 graphs with known latent factors. Speciﬁcally, we generate synthetic graphs using the stochastic 7
Figure 2: The performance of semi-supervised learning on ogbg-molhiv.
Figure 3: Micro-F1 (%) of two baselines and our DGCL with different number of channels K. block model [36]. Each graph contains four communities and each community consists of 10 nodes.
We deﬁne the latent factor as the probability p that two nodes are connected in a community. p can take value from {0.2, 0.3, . . . , 0.9}, meaning that there are eight latent factors in the dataset in total.
The probability for each community is drawn from the eight possible choices without replacement.
Two nodes in different communities are connected with probability 0.05. The rows of the adjacency matrices are used as node features, and the ground-truth communities are used as labels, i.e., there are 8 classes and each graph has 4 labels. We train our method and two baseline methods, i.e.,
MVGRL and GraphCL on the generated synthetic dataset with self-supervision. Then we adopt the
SVM classiﬁer on the learned graph embeddings for each method and use the Micro-F1 (%) as the evaluation metric. Other settings are the same as Section 4.1.
We vary the number of channels K of our method and report the results in Figure 3. Our method reports better performance than the baselines. We also ﬁnd that as K increases from 1 to 8, the result of DGCL improves, which veriﬁes the importance of disentangling latent factors. After reaching the peak at K = 8, the performance slightly drops, but in general, our method is not very sensitive when
K is not too large. Our method achieves the best results when K is equal to the ground-truth number of latent factors, indicating we can model the underlying structure of this simulation dataset.
Besides the quantitative evaluation, we also provide a qualitative evaluation by plotting the correlation of the latent features in Figure 4. The ﬁgure shows the absolute values of the correlation between the elements of 128-dimensional graph representation obtained from MVGRL, GraphCL, and our
DGCL (K = 8) on the synthetic dataset. We can see from the results that the graph representations of MVGRL and GraphCL are entangled. In comparison, the correlation of our DGCL shows eight diagonal blocks, meaning that the channels of DGCL likely extract mutually exclusive information and output disentangled representations. 4.4 Ablation Studies
We perform ablation studies over the key components of our method to understand their functionalities more deeply. We compare DGCL with the following two variants: (1) Variant 1: it sets pθ(k|Gi) = 1/K a uniform distribution of latent factors. (2) Variant 2: it sets K = 1 directly, so that our method will degenerate to the entangled graph contrastive learning model.
Table 2: Ablation studies on the variants of our method. We report the accuracy (%) with standard variation on the datasets. The results of Variant 1 and 2 drop compared with DGCL, demonstrating the signiﬁcance to infer latent factors behind the graphs and conduct factor-wise contrastive learning.
MUTAG PTC-MR PROTEINS
NCI1
IMDB-B IMDB-M RDT-B
RDT-M5K COLLAB
DGCL
Variant 1
Variant 2 92.1±0.8 89.3±0.3 86.5±0.6 65.8±1.5 64.3±1.3 63.5±1.6 76.4±0.5 74.9±0.2 73.9±0.6 81.9±0.2 78.5±0.5 77.7±0.6 75.9±0.7 73.4±0.5 70.9±0.5 51.9±0.4 50.3±0.2 49.8±0.3 91.8±0.2 91.1±0.7 89.7±0.6 56.1±0.2 55.9±0.3 55.7±0.2 81.2±0.3 77.5±0.4 71.5±0.8
The results of DGCL and its variants are shown in Table 2. We observe a drop in performance of
Variant 1, demonstrating the efﬁcacy of inferring the latent factors of the graphs. In variant 2, the latent factors are entangled in the graph representation, making difﬁculties for characterizing different aspects of the graphs and conducting discrimination tasks in terms of each latent factor independently. 8
(a) MVGRL (b) GraphCL (c) DGCL
Figure 4: An analysis of feature correlation on the synthetic graphs with eight latent factors. The
ﬁgures show the absolute value of the correlations between the elements of the representations learned by MVGRL, GraphCL, and DGCL with eight channels, respectively. We can see that the representations generated from DGCL present a more block-wise correlation pattern, indicating that the eight channels of the disentangled graph encoder in DGCL are able to capture mutually exclusive information and the latent features have indeed been disentangled.
The deterioration of performance veriﬁes the signiﬁcance of the proposed factor-wise contrastive learning. 5