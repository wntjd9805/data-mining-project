Abstract
Lévy ﬂights describe a special class of random walks whose step sizes satisfy a power-law tailed distribution. As being an efﬁcient searching strategy in unknown environments, Lévy ﬂights are widely observed in animal foraging behaviors.
Recent studies further showed that human cognitive functions also exhibit the characteristics of Lévy ﬂights. Despite being a general phenomenon, the neural mechanism at the circuit level for generating Lévy ﬂights remains largely unclear.
Here, we investigate how Lévy ﬂights can be achieved in attractor neural networks.
To elucidate the underlying mechanism clearly, we ﬁrst study continuous attractor neural networks (CANNs), and ﬁnd that noisy neural adaptation, exempliﬁed by spike frequency adaptation (SFA) in this work, can generate Lévy ﬂights represent-ing transitions of the network state in the attractor space. Speciﬁcally, the strength of SFA deﬁnes a travelling wave boundary, below which the network state displays local Brownian motion, and above which the network state displays long-jump motion. Noises in neural adaptation cause the network state to intermittently switch between these two motion modes, manifesting the characteristics of Lévy ﬂights.
We further extend the study to a general attractor neural network, and demon-strate that our model can explain the Lévy-ﬂight phenomenon observed during free memory retrieval of humans. We hope that this study will give us insight into understanding the neural mechanism for optimal information processing in the brain. 1

Introduction
Lévy ﬂights, also termed anomalous diffusion or super diffusion, refer to a special class of random walks whose step sizes follow a distribution with a long power-law tail. Mathematically, this power-law tailed distribution is expressed as p(x) ∼ x−1−α, (1) where the step size x can be measured either in the spatial domain, e.g., the length of a movement, or in the temporal domain, e.g., the time interval between successive events. The parameter α is called the Lévy exponent satisfying 0 < α < 2. When α ≥ 2, the above process degenerates to
Brownian motion due to the Central Limit Theorem [1]. Compared to Brownian motion, whose step sizes satisfy a Gaussian distribution, Lévy ﬂights are much more likely to generate large step sizes.
Thus, it is often intuitively stated that Lévy ﬂights are composed of frequent local motion (similar to the Brownian motion) and intermittent long-jump motion (Fig. 1). In contrast to Brownian motion 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
which tends to oversample a local area, Lévy ﬂights with long jumps provide a more efﬁcient way to search for scarce targets that are randomly distributed in an unknown environment, and the highest search efﬁciency occurs at α = 1 [2, 3, 4].
Figure 1: Illustrations of Lévy ﬂights and Brownian motion. (A) An example of Lévy ﬂights generated by sampling step sizes from Eq. (1), with α chosen between 0 and 2. (B) An example of Brownian motion generated by sampling step sizes from Eq. (1), with a α larger than 2. Both Lévy ﬂights and
Brownian motion showed here are generated with the same simulation duration, i.e., 200 time steps. (C) The histograms of step sizes (x in Eq. (1)) of Lévy ﬂights (blue) and Brownian motion (orange), respectively. Notably, Lévy ﬂights have ultra-long jumps which are absent in Brownian motion. (D)
The log-log plot of the histograms of step sizes in (C) shows that the distribution of Lévy ﬂights follows a power-law tail.
As being an efﬁcient strategy for information search, Lévy ﬂights have been widely observed in for-aging behaviors across different animal species, ranging from microzooplankton [5] to drosophila [6], and from albatrosses [7] to spider monkeys [8]. Similar foraging-like patterns are also observed in human behaviors, e.g., individual mobility in the geographical scale [9]. Experimental studies further revealed that Lévy ﬂights also exist in human cognitive functions. For instance, it has been found that the long tailed distribution holds for the saccadic eye movement in a random quenched salience ﬁeld [10], for the time interval between successive retrieved items in a free memory retrieval task [11], and for the mental exploration of ‘bid space’ in Lowest Unique Bid Auctions (LUBA) [12].
Recently, an fMRI study unveiled that the dynamics of neural activation at the resting state of the brain can also be characterized by Lévy ﬂights [13]. In a more detailed study, Pfeiffer and Foster showed that the awake reply trajectories of the hippocampal place cell ensemble in immobile rats resemble superdiffusion dynamics [14], which is very similar to Lévy ﬂights. Intriguingly, Lévy
ﬂights have also attracted considerable attention in the deep learning community for modeling the optimization process with the stochastic gradient descent (SGD) [15, 16, 17].
Despite that Lévy ﬂights have been widely observed and that various computational models have been proposed to account for their occurrences in different scenarios (see, e.g., [18]), the neural basis for the brain generating Lévy ﬂights remains far from resolved (see more discussions in Sec. 4). In the present study, we will investigate the neural mechanism for generating Lévy ﬂights at the circuit level, i.e., how a neural network with appropriate biological features generates Lévy ﬂights in the space of information presentation. We hope that this study will help us to further understand the emergence of Lévy ﬂights in cognitive functions of humans.
Speciﬁcally, we will study attractor neural networks which model how information is represented in neural systems [19, 20], and hence Lévy ﬂights in the corresponding attractor spaces reﬂect information processing in the brain. To elucidate the underlying mechanism clearly, we focus on studying continuous attractor neural networks (CANNs), as this allows us to solve the network dynamics analytically. The key characteristic of Lévy ﬂights is the occurrence of intermittent long jumps. In order to achieve this property, we consider noisy adaptation in the neural dynamics, which is exempliﬁed by spike frequency adaptation (SFA) in this work. The role of adaptation is to induce a spontaneously moving state, called travelling wave, in the network dynamics, which occurs when the strength of SFA exceeds a threshold. Thus, if the mean SFA strength is set slightly below the threshold, noises in adaptation will occasionally push the SFA strength to exceed the threshold.
Consequently, the network state will temporarily fall into the travelling wave state and experience a long-jump movement in the attractor space. Over time, noisy adaptation causes the network state to switch intermittently between local motion (when the SFA strength remains below the threshold) and long-jump motion in the attractor space, manifesting the characteristic of Lévy ﬂights. We carry out theoretical analyses to formally verify the above mechanism, and simulation results agree with 2
our theoretical analyses very well. We further extend the study to a general Hopﬁeld-like attractor network, and use it to model the Lévy-ﬂight phenomenon as observed in the free memory retrieval experiment [11]. We hope that this study will help us to understand how neural circuits generate Lévy
ﬂights, and give us insight into understanding the emergence and the computational roles of Lévy
ﬂights in brain functions. 2 Lévy ﬂights in continuous attractor neural networks
We ﬁrst study the generation of Lévy ﬂights in continuous attractor neural networks (CANNs).
CANNs have been widely used as canonical models for elucidating the encoding of continuous features in neural systems [21, 22, 23, 24], including, for instance, the orientation [25], the head direction [26], and the spatial location [27, 28]. The advantage of studying CANNs is that it allows us to solve the network dynamics analytically [29] and hence gives us insight into understanding the computational mechanism of generating Lévy ﬂights. In Sec. 3, we will extend the theoretical
ﬁndings to other attractor networks. 2.1 The model
Figure 2: A two-dimensional (2D) continuous attractor neural network (CANN) with neural adapta-tion. (A) Illustrating the network structure. Top: the excitatory connections between the neuron at x and neurons at other locations. Middle: the two-dimensional neuronal sheet on which neurons are uniformly distributed. Bottom: the network response with a Gaussian bell shape. (B) The kinetics of a single neuron, determined by the self-relaxation, recurrent interaction, spike frequency adaptation, and external input. (C) the attractor space formed by all stationary states of the network, on which the
CANN is neutrally stable (for clearance, the case of 1D is shown). (D) The network state with SFA as a negative feedback in the 2-D CANN feature space. The bump U (x, t) can move spontaneously in the attractor space in any direction depending on the initial state. The bump of the adaptation current
V (x, t) lags behind U (x, t) with a separation s(t) along the moving direction, due to the delayed feedback modulation of SFA. (E) The intrinsic speed of the travelling wave ||vint|| vs. the SFA strength (scaled by τ /τv). The red point represents the travelling wave boundary with m = τ /τv.
As illustrated in Fig. 2A, we consider a two-dimensional CANN, in which neurons are uniformly distributed on a rectangular neuronal sheet according to their preferred feature values. Denote U (x, t) as the synaptic input to the neuron at x, with x = (x1, x2) and x1, x2 ∈ (−∞, ∞), and r(x, t) the corresponding ﬁring rate. The dynamics of U (x, t) is determined by its own relaxation, the recurrent inputs from other neurons, the neural adaptation, and the external noise input (Fig. 2B), which is written as,
τ
∂U (x, t)
∂t
= −U (x, t) + ρ (cid:90) x(cid:48)
J (x, x(cid:48)) r (x(cid:48), t) dx(cid:48) − V (x, t) + σU ξU (x, t) . (2)
Here, τ is the synaptic time constant and ρ the neuronal density. The term σU ξU (x, t) represents the external noise input, with σU the noise strength and ξU (x, t) the Gaussian white noise of zero 3
mean and unit variance. In this study, we propose an internal mechanism to generate Lévy ﬂights, therefore we only consider noisy inputs of zero mean to the network and do not consider non-zero drift inputs (see Sec. 4 for more discussions). The recurrent neuronal connections, J(x, x(cid:48)) =
J0/(2πa2) exp (cid:2)−(cid:107)x − x(cid:48)(cid:107)2/(2a2)(cid:3), with (cid:107)x − x(cid:48)(cid:107)2 = (x1 − x(cid:48) 2)2, are translation-invariant on the neuronal sheet (Fig. 2A), which means that J(x, x(cid:48)) is a function of (cid:107)x − x(cid:48)(cid:107). The nonlinear relationship between the ﬁring rate r(x, t) and the synaptic input U (x, t) is implemented by divisive normalization, which is written as, 1)2 + (x2 − x(cid:48) r (x, t) = 1 + kρ (cid:82)
U 2(x, t) x(cid:48) U 2 (x(cid:48), t) dx(cid:48) . (3)
Here, the parameter k controls the normalization strength. In reality, divisive normalization could be implemented by shunting inhibition [30].
The term V (x, t) on the right-hand side of Eq. (2) represents an adaptive current. Adaptation is a general phenomenon referring to that a neuron population generates negative feedback to suppress its response when the activity level is high. Neural adaptation can result from different mechanisms, and their effects on the network dynamics are similar (see Sec. 4 for more discussions). Here, we consider spike frequency adaptation (SFA) at the single neuron level as an example. In reality, SFA could be implemented by the interplay between calcium currents and intracellular calcium dynamics with calcium-gated potassium channels [31]. The dynamics of V (x, t) is written as,
τv
∂V (x, t)
∂t
= −V (x, t) + [m + σmξm(x, t)] U (x, t) , (4) where τv is the time constant of SFA, with τv (cid:29) τ , indicating that SFA is a process much slower than neuronal ﬁring. m is the mean SFA strength, with σm the noise strength and ξm(x, t) denoting the Gaussian white noise of zero mean and unit variance.
We ﬁrst review the properties of the CANN without adaptation, i.e., by setting m = 0 and σm = 0 in Eq. (4). It has been shown that without the external noise input (σU = 0), the CANN holds a continuous family of Gaussian-shaped stationary states, called bumps, when the global inhibition amplitude k is smaller than a critical value kc = ρJ 2 0 /(32πa2) [29]. These bump states are expressed as U (x) = AU exp (cid:2)−(cid:107)x − z(cid:107)2/(4a2)(cid:3), where z is a free parameter representing the bump position, i.e., a feature value encoded by the network, and AU is a constant representing the bump height.
These bump states form an attractor space (Fig. 2C), on which the CANN is neutrally stable, which means that a small external input can drive the bump to move smoothly in the attractor space without distorting its shape. This neutral stability is the key that enables CANNs to realize accurate path integration [27, 28], smooth tracking of a moving object [32], and efﬁcient population decoding [33].
Speciﬁcally, under the drive of external Gaussian noises, the bump will exhibit Brownian motion in the attractor space [34]. (cid:48)
We further review the properties of the CANN with a ﬁxed adaptation strength (i.e., m is a constant and σm = 0). It has been shown that without external input, the CANN holds a moving bump as its stationary state when m > τ /τv, which is called travelling wave (Fig. 2E) [35]. This state reﬂects the intrinsic mobility of the network, as the bump moves spontaneously in the attractor space without relying on an external drive. The travelling wave state is expressed as U (x, t) = mτv/τ − (cid:112)mτv/τ the travelling speed
U exp (cid:2)−(x − vintt)2/(4a2)(cid:3), with (cid:107)vint(cid:107) = 2a/τv
A and A(cid:48)
U the bump height [35]. The mechanism underlying this intrinsic mobility is intuitively understandable. Suppose that the bump initially appears at an arbitrary location in the attractor space.
Due to SFA, those most active neurons, i.e., those around the peak of the bump, receive the strongest adaptation current, and their activities are suppressed the most. Since those neighboring neurons are less active and hence are less suppressed, the bump tends to move away to the neighborhood due to recurrent connections and competitions via divisive normalization; after moving to the new location, the suppression and competition start again. As a result, the bump will keep moving in the attractor space. Analogous to the situation without adaptation, if the SFA strength is ﬁxed and m < τ /τv, external noises will only induce Brownian motion of the bump in the attractor space. (cid:113) 2.2 Noisy adaptation generates Lévy ﬂights
Before going to the detailed mathematical analyses, we ﬁrst provide an intuitive understanding of how noisy adaptation leads to Lévy ﬂights. As introduced above, a CANN with SFA has the intrinsic 4
mobility of generating a travelling wave when the SFA strength is sufﬁciently strong. The condition of m = τ /τv deﬁnes the travelling wave boundary. Above the boundary, the bump will move spontaneously in the attractor space; below the boundary, the bump will either remain static if no external input exists, or exhibit Brownian motion when external noises are applied. The interesting phenomenon emerges if the SFA strength is noisy and its mean value m is close to the boundary.
In such a case, ﬂuctuations (due to the noise term σmξ(x, t) in Eq. (4)) will push the SFA strength to cross the boundary occasionally, which causes the network to fall into the travelling wave state temporarily, and consequently, the bump travels over a long distance (a long jump) in the attractor space. Over time, along with ﬂuctuations of the SFA strength, the bump displays intermittent local motion (when the SFA strength remains below the threshold) and long-jump motion, manifesting the characteristic of Lévy ﬂights. We present the formal analyses below.
Consider that the noise strengths in Eqs. (2&4) are sufﬁciently small, such that their effects on distorting the bump shape are negligible, and we assume that the network state has the Gaussian form as in the static case, which is given by, (cid:40)
U (x, t) = Au(t) exp
− r (x, t) = Ar(t) exp
− (cid:40)
[x − z(t)]2 4a2
[x − z(t)]2 2a2 (cid:41) (cid:41)
,
,
V (x, t) = Av(t) exp
− (cid:40)
[x − (z(t) − s(t))]2 4a2 (cid:41)
, (5) (6) (7) where Au(t), Ar(t) and Av(t) represent the heights of bumps U (x, t), r(x, t) and V (x, t), respec-tively. z(t) is the center of bumps U (x, t) and r(x, t), whose trajectory reﬂects changes of the network state in the attractor space. z(t) − s(t) is the center of bump V (x, t), with s(t) denoting the separation between U (x, t) and V (x, t). Note that the bump V (x, t) always lags behind U (x, t), reﬂecting that the adaptation current is delayed with respect to the neural response (Fig. 2D).
Previous works have shown that the dynamics of a CANN is dominated by very few motion modes [29, 36]. Therefore, to solve the network dynamics, we can project the network dynamics onto those dominating modes and simplify the analyses signiﬁcantly1. In the current study, we adopt the ﬁrst two dominating motion modes, corresponding to the changes of bump height and position, respectively, which are given by, u0(x|z) = u1(x|z) = a 1
√ 2π 1
√ a2 2π (cid:26) exp
−
[x − z(t)]2 4a2 (cid:27)
, (cid:26)
[x − z(t)] exp
−
[x − z(t)]2 4a2 (cid:27)
.
By projecting the network dynamics onto these two modes, we obtain the dynamics of the bump height and position, and the latter reﬂects how the network state changes in the attractor space.
Speciﬁcally, by substituting the presumptive network state Eqs. (5-7) into the network dynamics
Eqs. (2&4), and then projecting them on the motion mode of bump height Eq. (8), we obtain the dynamics of bump heights (see SI.1 for the details), which are written as,
τ
τv dAu dt dAv dt
= −Au − Av +
σU
√ a 2π
ξU,0(t),
+
J0ρAr 2
σmAu
√
π 2a
= −Av + mAu +
ξm,0(t), (8) (9) (10) (11) where ξU,0(t) and ξm,0(t) denote, respectively, the projected noises of ξU (t) and ξm(t) on the bump height mode, which are still Gaussian white noises of zero mean and unit variance. Note that we do not explicitly write down the dynamics of Ar, as it has a deterministic relationship with Au, that is, by substituting Eqs. (5-6) into Eq. (3), we obtain Ar = A2
Similarly, substituting the network state Eqs. (5-7) into the network dynamics Eqs. (2&4), and then projecting them on the motion mode for bump position Eq. (9), we obtain the dynamics of bump u/(1 + 2πa2kρA2 u). 1Projecting a function f (x) on a mode u(x) means computing (cid:82) x f (x)u(x)dx. 5
positions (see SI.1 for the details), which are,
τ
τv dz dt ds dt
=
= s +
Av
Au (cid:18) τvAv
τ Au
σU
Au
− (cid:114) 2
π mAu
Av
ξU,1(t),
−
σmAuξm,0
√
πa 2Av (cid:19) s +
τvσU
τ Au (cid:114) 2
π
ξU,1(t) −
σmAu
Av (cid:114) 1 2π
ξm,1. (13) (12)
Here ξU,1(t) and ξm,1(t) denote, respectively, the projected noises of ξU (t) and ξm(t) on the bump position mode, which are also Gaussian white noises of zero mean and unit variance.
We can obtain the stationary distributions of Au and Av in Eqs. (10&11) by solving the corresponding
Fokker-Planck equations, which is a general method to describe the time evolution of the probabilistic density function of a moving particle in physics. Moreover, since σU and σm are very small, the variances of Au and Av can be ignored compared to their mean values. Under this approximation,
Eqs. (12-13) can be further simpliﬁed by replacing Au and Av with their mean values ˜Au and ˜Av and using the approximation ˜Av = m ˜Au according to Eq. (11), which are written as,
τ dz dt
τv ds dt
= ms + (cid:114) 2
π
σU
˜Au
ξU,1(t), (cid:20)
= − 1 −
τv
τ m +
σm
√
πam 2 (cid:115) (cid:21)
ξm,0(t) s + (cid:19)2 2
π (cid:18) τvσU
τ ˜Au
+ (cid:17)2 1 2π (cid:16) σm m
ξs(t). (15) (14)
Here ξs(t) is a newly deﬁned Gaussian white noise of zero mean and unit variance (by combining the last two noise terms in Eq. (13)). Eq. (14) shows that the bump position z(t) is determined by a drift term reﬂecting the contribution of SFA and a diffusion term reﬂecting the contribution of the external noise input. Apparently, when no adaptation exists, i.e., no drift in Eq. (14) (m = 0), the bump movement is Brownian motion due to the diffusion noise.
To solve the dynamics of the bump position z(t), it is necessary to ﬁrst solve the dynamics of s(t) expressed in Eq. (15). We see that the drift coefﬁcient of s(t) in Eq. (15) consists of two parts. The
ﬁrst part, 1 − mτv/τ , measures the distance of the mean SFA strength to the travelling wave boundary (given by τ /τv), and we denote µ = 1 − mτv/τ as the distance-to-boundary, hereafter. In the following analyses, we only consider when the mean SFA strength is smaller than the boundary, i.e.,
µ > 0. Since when µ < 0, the bump movement is in the travelling wave state for most of the time and
πam) ξm,0(t), reﬂects the ﬂuctuations of is no longer a stochastic process. The second part, σm/ (2
√ the SFA strength over time, and we denote γ = σm/ (2
πam) as the noise-to-strength ratio hereafter.
Note that if the SFA strength is ﬁxed, i.e., γ = 0, Eq. (15) degenerates into an Ornstein–Uhlenbeck (OU) process [37]. In such a case, the stationary distribution of s(t) has a Gaussian form, which leads to the Brownian motion of the bump position z(t).
√
Non-Brownian motion occurs when the SFA strength ﬂuctuates, i.e., the noise-to-strength ratio
γ > 0. In such a case, we can obtain the stationary distribution of s(t) by solving the corresponding
Fokker-Planck equation (see SI.2 for the details), which gives, p(si) = c0 (cid:0)σ2 s + γ2s2 i (cid:1)−(1+µ/γ2)
, for i = {1, 2}, (16) (cid:1)(cid:17)2
πτ ˜Au 2τvσU /(cid:0)√ where s1 and s2 represent the coordinates of s on the axes of x1 and x2, respectively. σ2 (cid:16)√ s = and c0 is a normalization constant. From Eq. (14), the displacement of z(t) in a short time interval δt is calculated to be (cid:107)∆z(cid:107) = (cid:107)msδt/τ + (cid:112)2δt/(πτ )σU / ˜AuξU,1(cid:107). Replacing s with its stationary distribution given by Eq. (16), we ﬁnally derive the distribution of (cid:107)∆z(cid:107), which is written as,
+ (cid:0)√ 2aγ(cid:1)2 p((cid:107)∆z(cid:107)) ∼ (cid:107)∆z(cid:107)−1−(1+2µ/γ2), (17) which satisﬁes the power-law distribution (see SI.3 for the details). Comparing Eq. (17) with Eq. (1), we obtain the Lévy exponent of the bump movement,
α = 1 + 2µ
γ2 . 6 (18)
It shows that the Lévy exponent is determined by two factors, which are the distance-to-boundary µ and the noise-to-strength ratio γ.
Based on the above theoretical analyses and comprehensive simulations, we investigate the effects of
µ and γ on the mobility of the bump state in the attractor space, and the results are summarized in
Fig. 3. To elucidate the mechanism of generating Lévy ﬂights clearly, we inspect the effects of µ and
γ separately by varying only one of them each time while ﬁxing the other. The results are introduced below.
Figure 3: Lévy ﬂights in a two-dimensional CANN. (A) The phase diagram of the network with respect to the distance-to-boundary µ and the noise-to-strength ratio γ. (B) An example of Brownian motion in the attractor space with µ = 0.95 and γ = 0.6, corresponding to the blue point in (A). The inset is a close-up of the local Brownian motion. (C) An example of Lévy ﬂights in the attractor space with µ = 0.05 and γ = 0.6, corresponding to the orange point in (A). (D) The Lévy exponent α vs.
µ with γ = 0.9, corresponding to the horizontal dashed line in (A). Note that when µ > γ2/2, i.e.,
µ > 0.405, all α > 2 (Brownian motion) will converge to α = 2 due to the Central Limit Theorem. (E) The search efﬁciency η vs. µ. The optimal search is achieved when µ → 0, indicated by the orange point. (F) The Lévy exponent α vs. γ with µ = 0.1, corresponding to the vertical dashed line in (A). For the setting of other parameters, see SI.4 for the details.
The effect of the distance-to-boundary µ. Fixing the noise-to-strength ratio γ while varying µ, we have the following observations: a) In the case of µ ≥ γ2/2, which gives α ≥ 2, the bump movement displays Brownian motion (Fig. 3B). This is because when µ ≥ γ2/2, the SFA strength is far away from the boundary, which is too weak to generate the travelling wave state. Thus, the bump movement is mainly driven by noise ﬂuctuations in the neural dynamics (Eq. (2)). b) In the case of 0 < µ < γ2/2, which gives 1 < α < 2, the bump movement displays Lévy ﬂights (Fig. 3C). In this parameter regime, decreasing µ will decrease the Lévy exponent α from 2 to 1 gradually (Fig. 3D).
This is because as m gets closer to the travelling wave boundary, adaptation noises become more likely to push the SFA strength to cross the boundary, and hence long-jump movement occurs more frequent compared to local Brownian motion. When m eventually approaches the travelling wave boundary, i.e., µ → 0, which gives α → 1, the network achieves the optimal Lévy search in the attractor space. Note that in the simulation, we can only estimate the value of α from a truncated power-law distribution, as very large jumps exceeding the size of the attractor space are excluded.
This explains why the simulated Lévy exponent is slightly larger than the theoretical value of α = 1 (see Fig. 3D). c) In the case of µ < 0, the network dynamics will be dominated by the travelling wave state, which overrides the effects of both neuronal and adaptation noises, and the bump movement is no longer stochastic. d) We observe that the search efﬁciency of the bump (measured by the number of locations in the attractor space visited by the bump in a unit distance) reaches the optimal value when µ → 0 (Fig. 3E), manifesting the characteristics of Lévy ﬂights. 7
√
The effect of the noise-to-signal ratio γ. Fixing the distance µ (e.g, µ = 0.1, which is close to the travelling wave boundary) while varying γ, we have the following observations: a) In the case of γ ≤ 2µ, which gives α ≥ 2, the bump movement displays Brownian motion. This is understandable: although the mean SFA strength is close to the travelling wave boundary, the adaptation noises are too small to push the SFA strength to cross the boundary, and hence the bump movement is mainly driven by noise ﬂuctuations in the neural dynamics (Eq. (2)). b) In the case of
γ > 2µ, which gives 1 < α < 2, the bump movement displays Lévy ﬂights. Increasing γ implies that adaptation noises have higher chances to push the SFA strength to cross the boundary, and hence the Lévy exponent α will keep decreasing, until it approaches the value of α = 1 (Fig. 3F).
√ 3 Lévy ﬂights in free memory retrieval
Figure 4: Modelling Lévy ﬂights in free memory retrieval with an attractor network modulated by noisy adaptation. (A) Memory items are randomly distributed on the two-dimensional feature space and neuronal connections are determined by Hebbian learning. (B) An example of the retrieval trajectory in the feature space. (C) The histogram of step sizes of the retrieval trajectory in (B), which follows a power-law tailed distribution. (D) The index of retrieved items vs. the simulation time.
Retrieved items burst intermittently over time. (E) Retrieval intervals vs. the index of retrieved items.
Long intervals grow exponentially with interruptions by bursts of short retrieval intervals. (F) The log-log plot of the frequency distribution of retrieval intervals, with an estimated Lévy exponent
α = 0.73. For the setting of other parameters, see SI.4 for the details.
The ﬁrst study of Lévy ﬂights in human cognition was carried out by Rhodes and Turvey with a free memory retrieval task [11]. In the experiment, eight participants were asked to verbally recall animal names as many as possible (without repetition) within about 20 minutes, and the time intervals between successive recalls were recorded. Intriguingly, they found an exponential increase of long retrieval intervals that are interrupted by bursts of short intervals. Overall, the retrieval intervals can be described by Lévy ﬂights with exponents of 0.37 ≤ α ≤ 0.98 for different participants. Here, we show that this Lévy-ﬁght behavior of free memory retrieval can be described by random search on a semantic graph which is modelled by an attractor network with noisy adaptation.
Speciﬁcally, we build a Hopﬁeld-like attractor network, in which each memory item is represented by a localized neural population randomly distributed in a two-dimensional feature space. The neuronal connections are determined by Hebbian learning, such that each memory item is encoded as an attractor of the network (Fig. 4A). Successful retrieval of a memory item means that the network state falls into the corresponding attractor. Note that in such a network, the stationary states no longer form a continuous manifold of neutral stability as in a CANN. Even so, adaptation implemented by SFA can still drive the network state to travel among different attractors if the adaptation strength is strong enough. Thus, when the mean and variance of the SFA strength are set properly, the network state 8
switches intermittently between local motion and long-jump motion in the feature space, exhibiting the characteristics of Lévy ﬂights. This is conﬁrmed by the simulation results in Fig. 4B&C.
We have shown that when the memory recall in the human brain is treated as a random search on a semantic graph on which memory items are uniformly distributed, the whole process can be described as Lévy ﬂights in the attractor space of a Hopﬁeld-like attractor network. To reproduce the experiment
ﬁndings which show that the time intervals between successive recalls follow a long-tail power-law distribution, we further convert the Lévy ﬂights in the 2-dimension space as shown in Fig. 4A into their counterparts in the temporal domain. For this purpose, we calculate the time intervals between successive retrievals which are measured by the duration of the network state travelling from one attractor to another (note that each attractor can only be visited once as required in the human experiment, and the recall is terminated once the network state visits the same attractor again due to local motion). Fig. 4D shows that the number of retrieved items scales logarithmically with the simulation time, and memory items are recalled in burst intermittently, which agrees well with the experimental ﬁnding (see Fig.1b in [11]). Further analysis shows that long retrieval intervals are interrupted by bursts of very short intervals and the length of long intervals increases exponentially over time (Fig. 4E), which agrees very well with the experimental ﬁnding (see Fig.1a in [11]).
We also calculate the Lévy exponent by a linear ﬁtting of the log-log frequency distribution of retrieval intervals and obtain α = 0.73 (Fig. 4F), which is in the range of the experimental ﬁnding 0.37 ≤ α ≤ 0.98. It is also worth noting that this α shows the temporal property of Lévy ﬂights (retrieval intervals), while the α (not shown) which can be obtained in Fig. 4C shows the spatial property of Lévy ﬂights (step sizes on the graph). 4 Conclusion and Discussion
In the present study, we have studied a noisy-adaptation-modulated attractor network model to elucidate the mechanism of generating Lévy ﬂights at the circuit level in neural systems. By analyzing the dynamics of a CANN with noisy SFA, we ﬁnd that when the mean SFA strength is set close to the travelling wave boundary, noises in adaptation can easily cause the network state to intermittently switch between local motion and long-jump motion, displaying Lévy-ﬂight patterns in the attractor space. We theoretically derive the power-law distribution of the step sizes of the bump movement, and show that the Lévy exponent is determined by the joint effect of two key factors, i.e., the distance of the mean SFA strength to the travelling wave boundary, and the noise level in the adaptation. We also demonstrate that optimal search in the attractor space is achieved when the mean
SFA strength approaches the travelling wave boundary. Simulation results agree with our theoretical analyses very well.
Furthermore, we generalize the theoretical ﬁndings to a Hopﬁeld-like attractor network and use them to explain the Lévy-ﬂight patterns found in free memory retrieval. The Hopﬁeld-like attractor network encompasses a low-dimensional attractor space which deﬁnes a semantic graph of memory items, and movement of the network state in the attractor space corresponds to a random mental exploration on the semantic graph. By introducing noisy adaptation into the neural dynamics, we observe that the network exhibits Lévy-ﬂight moving patterns in the attractor space, which reproduce the experimental ﬁndings very well, including bursts of retrieved items during the retrieval process, the exponential increase of long retrieval intervals over time, and the value of the Lévy exponent.