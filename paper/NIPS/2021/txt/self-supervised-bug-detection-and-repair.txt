Abstract
Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software develop-ment. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BUGLAB, an approach for self-supervised learning of bug detection and repair. BUGLAB co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A
Python implementation of BUGLAB improves by up to 30% upon baseline meth-ods on a test dataset of 2374 real-life bugs and ﬁnds 19 previously unknown bugs in open-source software. 1

Introduction
Detecting and repairing bugs in source code requires strong reasoning skills over formal structures (e.g. data and control ﬂow) and ambiguous information (e.g. identiﬁer names, coding idioms, and comments). Traditional program analyses are able to detect critical bugs through formal reasoning and combinatorial search, but need to be manually coded by experts. That is a lengthy and costly process, which misses the opportunity to use ambiguous information pervasive within code.
Towards broadening the applicability of such methods, and utilizing ambiguous information, deep learning-based bug detection methods are being investigated [22, 3, 13]. These methods have the potential to further improve the engineering of software we rely on every day. However, many challenges in the area remain open, such as creating robust bug detection and repair methods that cover a wide range of common bugs in the absence of large supervised training corpora. Existing work focuses on randomly inserted bugs [22, 13], Cloze test proxy tasks [3], corpora of small code edits that may contain bugs [9] or build errors [28]. All these approaches rely on datasets of very limited size or ones known not to be representative of the characteristics of bugs found in real code.
In this work, we propose BUGLAB, a self-supervised approach that trains robust bug detectors by co-training a bug selector that learns to create hard-to-detect bugs (Sec. 2). For example, for a given code snippet with two well-named variables, a variable misuse bug may be easy to detect and repair, whereas an incorrect comparison operator might be signiﬁcantly harder to identify. We propose a neural architecture for BUGLAB (Sec. 3) and implement it for Python (Sec. 4). Our implementation considers four broad classes of seemingly simple, yet hard-to-detect bugs and shows improved performance over training with randomly-inserted bugs on PYPIBUGS, a new, manually curated test set of 2374 real-life bugs (Sec. 5). Furthermore, we tested our trained models on popular open-source Python packages and identiﬁed 19 previously unreported bugs, though false positive rates of ∼ 98% remain impractical. We hope that creating machine learning methods that can detect these bugs early and assist developers will speed up software development and allow engineers to deliver more robust software. We release PyPIBugs and our code at https://github.com/ microsoft/neurips21-self-supervised-bug-detection-and-repair.
∗Work done while at Microsoft Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
2 Self-Supervised Bug Detection
In this section, we ﬁrst introduce the concept of code rewriting, and then use it to deﬁne BUGLAB as a framework for self-supervised learning of bug detection and repair.
Code Rewriting Rewriting is common within compilers and their optimizations, test-driven search-based bug repair tools, mutation testing, and refactoring tools. Rewrites can be semantics-preserving (e.g. renamings of local variables), or semantics-altering (e.g. replacing >= by !=).
Let S denote the set of all syntax trees (not necessarily rooted in the start symbol of the language grammar). Syntax tree locations (cid:96) ∈ {(cid:15)} ∪ N∗ in a syntax tree s ∈ S are recursively deﬁned, where s|(cid:15) = s and s|(cid:96) for (cid:96) = (cid:96)(cid:48)◦i is the i-th child of s|(cid:96)(cid:48) (i.e. s|(2,3) denotes the third child of the second child of s). We deﬁne a rewrite rule ρ = (mρ, tρ) as a pair of a matching function mρ : S → {true, false} and a transformation function tρ : S → S. The matching function mρ(s) yields true iff the rule
ρ is applicable at the root of a subtree s. The transformation function can be applied to obtain a transformed syntax tree. For convenience, we deﬁne tρ(s) = s iff mρ(s) = false. We then write
ρ(s) to indicate the modiﬁcation of a syntax tree s using ρ when possible, and otherwise the identity function. For reversible rewrite rules ρ, we denote the inverse rule as ρ−1 such that ρ−1(ρ(s)) = s holds. We discuss concrete rewrite rules ρ in Sec. 4.
Given a set of rewrite rules R we deﬁne the set of “potential rewrites” in a syntax tree s as RR s = (cid:8)(cid:104)(cid:96), ρ(cid:105) | ρ ∈ R, (cid:96) location in s, mρ(s|(cid:96)) = true(cid:9). For each tuple (cid:104)(cid:96), ρ(cid:105) ∈ RR s , we use s(cid:48) = s[ρ](cid:96) to denote the new syntax tree obtained by applying ρ at location (cid:96) of s. In BUGLAB, we train models that use rewrites from RR s to insert and repair bugs. We will discuss such neural models in Sec. 3.
BUGLAB In BUGLAB, we are interested in self-supervised training of a robust bug detector model Dθ with parameters θ on an unannotated codebase C. Let R be a set of rewrite rules2 that allows to insert and repair bugs. We train Dθ to be able to recognize the “hardest” possible rewrites that could be applied on our codebase C.
For this, we consider the loss LDθ of Dθ on a rewritten code snippet s[ρ](cid:96), for which the model needs to predict the repairing rewrite (cid:10)(cid:96), ρ−1(cid:11). Formally, we want to min-imize the objective (cid:20)
Es∼C max (cid:104)(cid:96),ρ(cid:105)∈RR s
LDθ (cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1) (cid:21)
.
However, for any useful detector the set of rewrites RR s is commonly very large or unbounded and computing the maximum over all (cid:104)(cid:96), ρ(cid:105) ∈ RR is practically intractable. s
To address this, BUGLAB introduces a bug selector model
Sφ (with parameters φ), whose goal is to approximate the intractable max(cid:104)(cid:96),ρ(cid:105)∈RR
LDθ (·). We can then sample rewrites from Sφ instead of computing the maximum. We denote this as (cid:104)(cid:96), ρ(cid:105) ∼ Sφ(s) and the overall BUGLAB training objective can be written as a min-max optimization problem:
Figure 1: BUGLAB overview: a se-lector model Sφ decides which (bug-introducing) rewrite to apply to an input code snippet. Then a bug detector Dθ tries to locate and repair the inserted bug (if one was inserted). s max
φ min
θ
Es∼C (cid:2)E(cid:104)(cid:96),ρ(cid:105)∼Sφ(s) (cid:2)LDθ (cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1)(cid:3)(cid:3) . (1)
The two models S and D in BUGLAB are “symmetric” in the sense that they both predict rewrites on code snippets, and only differ in their objectives — one aiming to introduce bugs and one aiming to repair them. In practice, we can and do use the same architecture to model both S and D, which we will discuss in the next section. At test time, we discard S and only use the trained detector D to locate and repair bugs. 2In this work we assume that R contains a special “identity” rewrite rule ρ∅ that does not change the code. 2
3 Neural Models
In this section, we discuss how we represent code in BUGLAB and the neural models we use to learn how to rewrite code in the selector and detector models.
Code Representation We consider source code as a set of entities vi ∈ V which relate to each other with a set of typed relations ek ∈ E, where a relation ek = (vi, r, vj) denotes a relationship between entities vi and vj with type r. The entities and relations can be thought as a heterogeneous graph G = (V, E). The choice of code entities and their relationships is a form of high-level feature extraction. We discuss concrete entities and relationships for Python in Sec. 4. We also deﬁne a projection function Ptok that accepts V and E and returns a sequence Vtok of the token entities in V with the nodes appearing in relations in E deterministically mapped to elements of Vtok, i.e.
Etok = {(p(vi), r, p(vj))}, where p maps the entities in V to Vtok. Ptok will be used for relational transformer models.
To learn a neural representation of the code entities vi, ﬁrst we deﬁne an embedding function e(vi) which maps the content of each entity to an initial D-dimensional representation. Throughout this work — similar to Allamanis et al. [4] and other previous work — we deterministically split the string representation of each node into subtokens (e.g., fooBar is split into foo and bar), embed them through a learned embedding matrix, and use max pooling to get a single vector. We then
“contextualize” the entity representations within G using one of two models: a MLP-based GNN model with max message aggregation and the GREAT relational transformer of Hellendoorn et al.
[13] over the token sequence and relations Vtok, Etok = Ptok(V, E). GREAT uses both positional encodings and the projected relations in Etok. See Appx. A for detailed architecture descriptions.
Other models to compute entity representations can be used, but were not explored in this work.
We use r(cid:96) to denote the computed vector representation of the entity at location (cid:96), independent of the model used to produce it. We use these representations to deﬁne our code rewriting models.
Probabilistic Code Rewriting Models Both bug selection and bug detection require to model the probability of applying a speciﬁc rewrite at a location in a code snippet s, either to introduce or repair a bug. For this, we factorize this task into localization and rewrite-given-location models, i.e. p (cid:0)(cid:104)(cid:96), ρ(cid:105) | s, RR (cid:1) = ploc s } ∪ {NoBug},
We model ploc as a probability distribution over the relevant locations {(cid:96) | (cid:104)(cid:96), ρ(cid:105) ∈ RR where NoBug is a special location used to indicate that the code is not buggy. In practice, we imple-ment this similar to a pointer net [19] using the representations r(cid:96) (see Appx. A for details). (cid:0)ρ | (cid:96), s, RR (cid:0)(cid:96) | s, RR (cid:1) prew (cid:1) . (2) s s s
To select rewrites, we use rewrite type-speciﬁc learnable rule score functions wρ (r(cid:96), Mρ(s, (cid:96))).
This function maps a vector representation of an entity r(cid:96) and potential additional metadata onto a scalar score. The rule-speciﬁc metadata Mρ(s, (cid:96)) is deﬁned for some rewrites, e.g. containing representations of other entities that could be used in the location (cid:96). We will discuss three concrete rule score functions in Sec. 4. The rewrite probability distribution prew is then modeled by a softmax over the scores of all applicable rewrites at a target location (cid:96), i.e. (cid:0)ρ | (cid:96), s, RR (wρ(cid:48) (r(cid:96), Mρ(cid:48)(s, (cid:96)))) . prew (cid:1) = softmax (cid:104)(cid:96),ρ(cid:48)(cid:105)∈RR s s 4 A Python Implementation
This section presents an implementation of BUGLAB for Python called PYBUGLAB. PYBUGLAB currently tackles a large subset of “stupid simple bugs” [16]. Fixing these bugs requires small changes to the code, but commonly has signiﬁcant impact on code correctness. Such bugs may be thought as a form of a typographical mistake or a copy-paste error, and are often relatively hard to locate by humans but obvious after the fact. They are also quite common, as observed in the empirical statistics of Karampatsis and Sutton [16] and Just et al. [14]. Future work may focus on a broader set of rewrite rules or even learnable rewrites, but as we will observe in Sec. 5 more work is needed towards this.
Almost all ideas in PYBUGLAB transfer straightforwardly to other programming languages other than Python, but would require some engineering effort to implement.
PYBUGLAB Code Entities and Relations
In this work, we follow related literature (see Sec. 6 for more) and extract entities and relationships that are readily available by tokenizers, parsers, 3
def foo(a, b, c=0): if a[1] in[2] b[3]: c[4] +=[5] bar(b[7], c[8])[6] c_is_neg =[9] c[10] <[11] 0[12] if c is neg[13] or[14] a[15] is[16] int: return True[17], c[18] return c[19] >[20] 1[21], c[22] (cid:15): NoBug l1: b, c l2: not in l3: a, c l4: a, b l5: =,-= , *=, /=, //=, %= l6: bar(c,d) l7: a, c l8: a, b l9: += ,-= , *=, /=, //=, %= l10: a, b l11: <=, >, >=, ==, != l12:-2 ,-1 , 1, 2 l13: a, b, c, not c_is_neg l14: and l15: b, c, c_is_neg l16: is not l17: False l18: a, b, c_is_neg l19: a, b, c_is_neg l20: >=, <, <=, ==, != l21:-2 ,-1 , 0, 2 l22: a, b, c_is_neg
Figure 2: Code snippet and rewrites available to PYBUGLAB. existing simple program analyses, or other Python-speciﬁc program analysis tools. The complete list of entities and relationships can be found in Appx. B and include syntactic entities and relations, relations about the intraprocedural data and control ﬂow, types, and documentation. Some notable entities include SyntaxNodes, Tokens, and Symbols (references to variables and functions). Fig. 4 in Appx. B shows a graph of the entities and relationships of the snippet in Fig. 2. 4.1 Bug-Inducing PYBUGLAB Rewrite Rules
PYBUGLAB focuses on four common kinds of bugs. Fig. 2 shows a code snippet and the rewrites allowed for each location, which number 63 even for this small example.
Variable Misuse Originally deﬁned by Allamanis et al. [3] as a Cloze test for source code, Vasic et al.
[30] and Hellendoorn et al. [13] reformulated the task to localizing a variable misuse bug (if any) within a snippet and repairing it. PYBUGLAB uses the latter representation. Variable misuse bugs are common, with 12.8-14.8% found in the ManySStuBs4J corpus [16] and about 6% of them caught during Java compilation in the Google build system [28]. To insert and repair variable misuse bugs,
PYBUGLAB supports variable-swapping rewrites, such as in locations l1, l3 and l4 (amongst others) in Fig. 2. To score a variable-swapping rewrite, we use the representation of the rewrite location r(cid:96) along with the representation rσ of a variable Symbol σ that could replace the current variable, i.e. is in-scope and has been deﬁned before (cid:96). The rule score function wρ for replacing the variable at (cid:96) with the symbol σ is then computed as the inner product r(cid:62) (cid:96) rσ.
Argument Swapping (or Argument Selection) First coined by Rice et al. [26], it refers to swapping the arguments of a function invocation, e.g. in l6 of Fig. 2. Rice et al. [26] and DeepBugs [22] tackled this problem when all arguments are single identiﬁers. PYBUGLAB extends this to swapping arbitrary argument expressions. The rule score function wρ for an argument swapping rewrite is a two-layer MLP applied to the concatenation of the output representations of the representation of the parameter and the to-be-swapped arguments arg1, and arg2: MLP (cid:0)[rparams, rarg1, rarg2](cid:1).
Wrong Operator Corrupting operators has a long history in mutation testing [14]. Detecting incor-rect operators with deep learning was ﬁrst tackled by DeepBugs [22] by using learnable embeddings of operators, operands and literals for arithmetic and comparison operators. DeepBugs focused only on binary operators. In PYBUGLAB we tackle all binary operators, including Boolean, arithmetic and comparison operators and two unary operators: logical and arithmetic negation. Locations l11, l14, l16, and l20 in Fig. 2 are rewrites related to wrong operators. The rule score function wρ for an operator rewrite again uses an inner product, r(cid:62) (cid:96) rop, where rop is a learned embedding for operator op. Note that we rewrite operators only to compatible operators (e.g. < to > but not +).
Wrong Literal Corrupting operands, and speciﬁcally, literals appearing in the source code, is also a common strategy in mutation testing. As in mutation testing, PYBUGLAB handles a limited number of commonly used literals, allowing rewrites to replace integer literals within the set of -2, -1, 0, 1, 2 and swapping the Boolean literal True with False and vice versa. The scoring function is identical to the operator rewrite, using a learnable embedding rlit for each literal lit. 4.2 PYBUGLAB Rewrite Rules for Data Augmentation
We additionally consider more rewrite rules that are not meant to change the program semantics, using them as a form of data augmentation. This is in spirit similar to ideas in computer vision where images are transformed (e.g. rotated, cropped) but maintain their original content. Such rewrites 4
Algorithm 1 Sequential Training Procedure for Selector and Detector models
Require: Code dataset C, initial detector/selector model parameters θ(0), φ(0) 1: for meta-epoch i = 0 to I do 2:
// Create dataset of buggy programs:
D ← (cid:8)(cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1) | s ∈ C, k samples (cid:104)(cid:96), ρ(cid:105) ∼ Sφ(i) (s)(cid:9)
C (i)
θ(i+1) ← update θ(i) by training D on C (i)
D
// Create dataset of hard-to-detect bugs:
C (i) s, arg max(cid:104)(cid:96),ρ(cid:105)∈RR
φ(i+1) ← update φ(i) by training S on C (i)
S (cid:0)s[ρ](cid:96), (cid:10)(cid:96), ρ−1(cid:11)(cid:1)(cid:17)(cid:17)
S ←
LD
θ(i+1) (cid:110)(cid:16) (cid:16) s 3: 4: 5: 6: 7: (cid:111)
| s ∈ C have been shown to yield adversarially robust models of code [23]. Although our goal is not to provide adversarial robustness, we believe that such rewrites can help generalization. PYBUGLAB implements the following rewrites for this purpose:
• Variable Renaming renames a local variable to a random name not already in scope.
• Comment Deletion removes code comments, including docstrings and inline comments. Such comments commonly contain natural language information that is useful for code comprehension, but usually do not affect program semantics.
• Comparison Expression Mirroring swaps the two sides of a comparison operator and changes it appropriately. For example, a<b is transformed to b>a. Note that in cases such as foo() < bar(), this will change the order of execution of foo and bar, possibly altering program semantics.
• If-Else Branch Swapping negates the test condition of an if-else statement or a ternary expres-sions using DeMorgan’s law and swaps the then body with the else body. 4.3
Implementation Details
To make the training computationally tractable we approximate Eq. 1. A simpliﬁed, sequential version of our training procedure is shown in Alg. 1. Intuitively, we alternate between training the two models, as the (discrete) sampling of rewrite rules in the selector models precludes direct end-to-end training. We ﬁrst use the current state of the selector model to generate “hard” samples and train the detector model on these samples (we always include the unmodiﬁed (i.e., NoBug case) as a sample). Then, we use the loss of the detector model to identify those generated samples that were hardest to detect and train the selector model to produce such samples.
In practice, we implemented the training procedure as a system of asynchronously communi-cating processes, and all of the described steps happen in parallel. We do not use “generations”
D/S, C (1)
C (0)
D/S, . . . of datasets, but instead use two constantly updated “pools” of training data, one for the detector and one for the selector. Each training step samples a minibatch from the current state of the corresponding data pool. We remove samples from the data pool once they have been sampled ν times for use in training, in spirit similar to replay buffers in reinforcement learning. In our experiments, ν was set to 4. We regularly (in separate, concurrent processes) take snapshots of the the current state of the D and S models to generate new elements that are updated to the data pools, matching the procedure described in Alg. 1. We approximate the arg max in line 6 by only considering the k samples chosen in line 3 for each input program. During training of S, we then mask out the unobserved choices before computing the loss. 5 Evaluation
We now discuss our new dataset and evaluate PYBUGLAB. We results. highlight.....key........
..........
Datasets To train PYBUGLAB we retrieve the 4k most downloaded packages in the Python package index (PyPI) and take 3.4k of them as training packages, using the rest for test purposes. During training, PYBUGLAB installs each package along with all its dependencies. Installing all the depen-dencies is important for extracting the entities and the relations beyond local syntactic ones (e.g. type 5
inference, method resolution). For each ﬁle, PYBUGLAB checks if it is a duplicate of a ﬁle that has already been seen in the training following the method of Allamanis [1] and runs all the relevant program analyses to extract the entities and relationships in each function. When we use additional rewrites for data augmentation, these are applied at the input of the PYBUGLAB pipeline as a form of pre-processing. Following Alg. 1, the bug selector S selects k = 5 bugs to introduce, rewrites the source code text, and then the program analyses extract the new entities and relationships for the rewritten code snippets. The initial and rewritten code snippets are then used to create the training data for the detector and selector models.
We use two testsets to measure performance. First, we create RANDOMBUGS, a testset of 761 445 snippets derived from functions from the 600 PyPI test packages (not seen during training). For each function we ﬁnd within these packages we add it to the dataset along with 9 rewritten functions with a randomly inserted bug. On average graphs have 260 nodes, 601 edges, 25 rewrite locations, and 130 possible rewrites. We also collect a testset of real bugs. Although we conjecture that, in practice, the vast majority of bugs like those discussed in Sec. 4.1 are ﬁxed when developers locally test their software, a few of those slip and then are ﬁxed across different revisions checked into a version control systems. We have crawled the accessible repositories of all 285k packages in the Python Package
Index (PyPI), collected and manually ﬁltered bugs captured by the rewrites from Sec. 4.1.
.....This......new bugs. We describe the data collection process real-world,....... dataset, ... ........ ... .
......... in detail in Appx. D. In addition, we consider PYPIBUGS-PostFix: the examples from PYPIBUGS after a bug was ﬁxed - we believe these samples are very likely to not contain any bugs anymore.
We publish the dataset at https://www.microsoft.com/en-us/download/103554 and include it in the supplementary material.
PYPIBUGS,... ....... 2374 ............. contains... .... small....... 5.1 Quantitative Evaluation
Our ﬁrst experiment aims to evaluate whether the BUGLAB training framework yields more precise bug detectors. We consider two model architectures, using either GNNs or the GREAT transformer to compute embeddings of code entities (architecture details and hyperparameter choices can be found in Appx. A). We use four different training strategies: “supervised” is training only a bug detector on a ﬁxed dataset of 1 million functions from the 3.4k training packages with randomly inserted bugs. “Random Selector” refers to a variant of PYBUGLAB using a bug selector model that uniformly at random picks a rewrite to insert bugs. Finally, PYBUGLAB and PYBUGLAB +Aug use our framework from Sec. 2, with the latter also using additional rewrites to augment our code corpus.
For the fully supervised model, we train with early stopping over a validation set; the other models are trained for a ﬁxed number of 300 epochs (with 200k training samples per epoch) for the bug detector3 and the last detector model is used for evaluation.
Table 1: Accuracies (%) for different training strategies and model architectures on RANDOMBUGS.
Effectiveness of BUGLAB Training
We ﬁrst consider the performance of different models on the synthetic
RANDOMBUGS dataset. Tbl. 1 shows the accuracy of predicting a full bug repair correctly (“Joint”) and analo-gous to Eq. 2 break this up into a local-ization accuracy (“Loc”) of predicting the correct location (or NoBug for cor-rect examples) and a repair accuracy (“Repair”) for selecting the correct rewrite given the buggy location. 62.4
Supervised 69.4
Random Selector
PYBUGLAB 69.6
PYBUGLAB +Aug 70.3 73.6 79.6 80.4 81.1
Joint
RANDOMBUGS
GNN
Loc Repair Joint
GREAT
Loc Repair 81.2 51.0 84.0 63.9 84.2 64.0 84.5 65.3 61.9 73.6 74.3 75.3 76.3 82.0 82.3 82.5
PYBUGLAB-training.......
... ........ ... ..... .....
We observe that other compared...to........ methods for both GNNs and GREAT. Random selector models — a form of data augmentation — im-.......... prove performance over supervised methods but mostly on in-distribution RANDOMBUGS samples.
As expected, generalization, but does not make a substantial difference. Expanding the kinds of rewrites used to augment the data and learning to select them may improve performance in the future. leads....to .......more........ robust......bug ........... helps................. detectors............. augmenting... ..
... ....... ... dataset....... code......... the ... ... 3This amounts to about 1.5 weeks for the GNN models and about 1 week for the GREAT models on a single
P100 GPU. 6
Table 2: Results for different training strategies and model architectures on PYPIBUGS.
PYPIBUGS
Joint
GNN
Loc Repair Joint
GREAT
Loc Repair 20.0
Supervised 21.2
Random Selector
PYBUGLAB 24.2
PYBUGLAB +Aug 26.4 28.4 27.0 31.3 33.5 61.8 16.8 69.2 20.6 70.7 24.0 72.0 23.2 25.8 26.8 32.8 29.7 58.6 67.2 67.9 68.8
PYPIBUGS-PostFix
GNN
Joint AUC Loc
GREAT
Joint AUC 0.087 20.7 0.108 52.5 0.160 28.6 0.187 48.2 0.044 0.117 0.140 0.129
Loc 17.8 47.5 32.9 32.6
Table 3: Localization and Repair Accuracy (%) per bug kind for the PYBUGLAB +Aug model.
Bug Type
RANDOMBUGS
PYPIBUGS
GNN
Loc Repair
GREAT
Loc Repair
GNN
Loc Repair
GREAT
Loc Repair
Argument Swapping
Wrong Assign Op
Wrong Binary Op
Wrong Boolean Op
Wrong Comparison Op
Wrong Literal
Variable Misuse
NoBug 85.0 96.1 83.0 71.8 83.9 71.7 84.9 53.8 57.3 99.1 85.2 99.5 79.3 74.7 88.4
— 65.5 94.5 77.3 43.6 80.0 66.6 78.2 62.5 57.2 98.6 81.4 99.5 76.4 71.6 86.3
— 33.2 20.0 27.2 27.6 33.7 21.6 35.3
— 73.9 68.9 54.3 96.9 66.1 78.4 70.5
— 24.3 14.0 36.6 15.7 31.1 17.9 34.0
— 72.7 58.1 43.7 97.2 53.5 79.5 69.4
—
Furthermore, location. This is somewhat expected: there are many more candidate locations compared to potential repairs at a given location.
However, this suggests that research should focus on the localization problem rather than repair. localization ...is .......much........ repair ...at ..a .......
... .bug... ....... ..... given.......... harder...... than........
We now turn to the results on PYPIBUGS, shown in Tbl. 2, which also includes the accuracy of choosing the special NoBug location on the PYPIBUGS-PostFix dataset, as well as the area under the precision recall curve for the results on both PYPIBUGS and PYPIBUGS-PostFix. real-life....... repairing ......... signiﬁcantly........ bugs ..is............... detecting... ..and... ....... ..
... .......
We ﬁnd that randomly handling............ bugs. As PYBUGLAB models trained using a learned bug selector outperform those using inserted... ...
.. .. ..... a “Random Selector”, we speculate that the learned selector avoids generating easy-to-detect bugs, focusing the detector model on recognizing deeper semantic patterns. Despite this, improvements in
RANDOMBUGS often correlate with improvements in PYPIBUGS. This is encouraging: collecting
PYPIBUGS-like datasets is costly; corpora with random bugs can help measure relative improve-ments to some extent. Finally, we ﬁnd that hard, and in particular, does not always proﬁt from training in PYBUGLAB. recognizing .............
.............. non-buggy.......... samples...is ...... than........... harder...... very ......
In our qualitative analysis (Sec. 5.2), we observed that the models raised some conﬁdent but incorrect warnings at very “odd” locations. However, these warnings were different across models. We have tested an ensembling strategy averaging the output probabilities of ﬁve separately trained GNN models. This results in localization and repair accuracies of 83.0% and 85.4% on RANDOMBUGS (vs. 81.1% and 84.5%) and 34.4% and 72.2% on PYPIBUGS (vs. 33.5% and 72.0%). As we discuss in Sec. 5.2 ﬁnding the cause of the “spurious” warnings is important future work. snippet... ..has ..a ... ..bug ...or ... .not... .....
Per-Bug Evaluation To better understand which bugs are hard to detect, we break down the results the best-performing PYBUGLAB +Aug models on RANDOMBUGS by type of bug in Tbl. 3. We observe that incorrect literals are some of the hardest bugs to detect. Incorrect assignment operators (e.g. = and +=) are easy to detect in RANDOMBUGS, but signiﬁcantly harder in PYPIBUGS. This may
Detecting...if be attributed to class imbalance, with simple assignment (=) being the majority class.
...........
.a......... beyond......63%. hardest......
We note that in our experiments, GNNs-based models seem to often outperform GREAT, somewhat contradicting the results of Hellendoorn et al. [13]. We have performed substantial additional experi-ments to investigate and verify these results, cf. Sec. A.2. This may have to do with the performance of these models on long sequences or that the GNN has access to more ﬁne-grained information, instead of relations over the projected token sequences. For example, this could be attributed to the lack of syntax and symbol nodes in the representation used in GREAT. Nevertheless, GREAT task:....no........model.......... seems ...to ...be....the......... achieves........... accuracy......... 7
Table 5: Bug distribution (%) in different datasets
Bug Kind
PYPIBUGS RANDOMBUGS
Selector Samples
Argument Swapping
Wrong Assignment
Wrong Binary Operator
Wrong Boolean Operator
Wrong Comparison Operator
Wrong Literal
Variable Misuse 11.9 1.9 3.4 8.1 17.1 3.7 53.8 8.4 8.5 2.4 2.2 8.2 11.6 58.6 23.8 5.3 2.3 6.4 7.4 12.4 42.5 is noticeably better (62.5% vs. 53.8%) at detecting NoBug and locating wrong binary operators in
PYPIBUGS.
Bug Selector Performance To understand how training of the bug selector proceeds, we perform two experiments.
In our ﬁrst experiment, we take a snapshot of the selector model during training of the PYBUGLAB +Aug (GNN) model every 24 hours, after an initial burn-in phase of 12 hours. We then generate 10000 buggy samples using each of these snapshots and then test a ﬁxed model on each of these snapshots. The results of this are shown in Tbl. 4, us-ing a fully trained PYBUGLAB +Aug (GNN) model from another training run as a ﬁxed model. We conclude that harder...to
PYBUGLAB succeeds... .in ... .......
........... ... ..... . ... . . learning ...to .......... generate........
Table 4: Development of Performance on Bug Selector Samples
Training Time
Loc Repair
Joint 0.5 days 1.5 days 2.5 days 3.5 days 4.5 days 5.5 days 6.5 days 64.2 62.5 62.0 61.7 61.9 61.1 60.5 83.8 80.7 83.0 82.5 83.0 83.0 78.7 72.1 72.9 69.8 69.8 69.5 68.6 72.4
....ﬁnd... ...bugs, though we can observe the selector model trading off “harder-to-localize” and “harder-to-ﬁx” properties. Tests on other models show similar trends, conﬁrming the robustness of this result.
In a second experiment, we compare the distribution of different bug kinds in PYPIBUGS and
RANDOMBUGS with the distribution of bugs sampled from the ﬁnal snapshot of our selector model from above. The results are shown in Tbl. 5, where we can see that a number of bugs (argument swapping, use of wrong literals and of assignment operators) are substantially over-represented, whereas mistakes in comparison operators and variable misuse are under-represented. This indicates that hard...to ......ﬁnd, ....but....not.............. necessarily ..........
PYBUGLAB generates... ...
... ........ ... ..... . ... . .. bugs. realistic ......
Comparison to CuBERT Finally, we compare our models to CuBERT [15], which uses a masked language modeling objective to pre-train a BERT-like model and then learns bug detectors speciﬁc to a class of bugs (e.g., wrong binary operators) on top of this pre-trained model. Note that CuBERT detects if a bug exists but does not localize it. For the comparison, we create two sub-datasets of PYP-IBUGS: PYPIBUGS-WrongOp contains the 501 samples that involve the binary operators supported by CuBERT, and PYPIBUGS-VarMisuse, which contains the 1278 bugs that involve variable misuses.
We complete both of these datasets with 501 (resp. 1278) random NoBug code samples from our
RANDOMBUGS, to match the 1:1 buggy/non-buggy distribution used in CuBERT’s training. Since
CuBERT classiﬁcation models focus on a single bug type, to compare to PYBUGLAB we mask out all code locations that do not correspond to a bug that could be detected by the corresponding CuBERT model. We then treat the prediction of the NoBug location as a “non-buggy” prediction and all other locations as a “buggy” prediction. For example, for the snippet in Fig. 2, only the locations l2, l11, l14, l16, and l20 and their corresponding rewrites are considered by PYBUGLAB for the comparison on PYPIBUGS-WrongOp.
Tbl. 6 shows the results of comparing the released Cu-BERT snapshots with the
PYBUGLAB +Aug GNN model. We observe that
PYBUGLAB models the ... ....... ..... ... ... . ...
.. .. substantially ... ....
Table 6: Comparison with CuBERT [15]
CuBERT
Prec Recall
F1
PYBUGLAB (GNN)
Prec Recall
F1
PYPIBUGS-WrongOp 0.764
PYPIBUGS-VarMisuse 0.632 0.251 0.403 0.378 0.493 0.730 0.740 0.764 0.840 0.746 0.787
.. ...have... ........ ... .. models, even though they were trained to de-CuBERT-based ......... tect more bug types. When calibrating the CuBERT models to have a recall equal to PYBUGLAB, their precision drops substantially. In particular, on PYPIBUGS-WrongOp, it is reduced to 0.609, and than................... better... .... recall... ... 8
1 def make_id(name): 2 3 4 5 r = get_rand_string(12) if len(name) <= 22: name = name[:22] return name + "-" + r title, **request_params): 1 def update(self, roomId, 2 3 4 5 check_type(roomId, basestring) check_type(roomId, basestring)
[...] (a) A wrong comparison operator bug (red box) in PYPIBUGS detected and repaired by the GNN
PYBUGLAB +Aug models. (b) A variable misuse (red box) caught in an open-source project. GNN PYBUGLAB +Aug suggests to rewrite roomId to title. The ﬁxing pull request is found here.
Figure 3: Bugs found by PYBUGLAB. Snippets reformatted and abbreviated to ﬁt ﬁgure. on PYPIBUGS-VarMisuse, it is reduced to 0.613; in both cases, PYBUGLAB outperforms CuBERT substantially. 5.2 Qualitative Inspection of Raised Warnings
We now take a qualitative look at the raised warnings raised by PYBUGLAB. As example, Fig. 3a shows a sample of PYPIBUGS where the developer used an incorrect comparison operator. Once pointed to it, it is clear to a human that the truncation statement in line 4 has no effect (under the reasonable assumption that name is a string), and that a different comparison operator (>) is necessary.
To gain an understanding of the performance of PYBUGLAB on realistic data, we performed an in-depth analysis of the cases ﬂagged as bugs by our best-performing model on the code found within the 4k top PyPI packages. We observed a mixture of false positives with few previously unseen real-life bugs, matching the quantitative results in Tbl. 3. First, we ﬁnd that the majority of the false positives are “incorrect literal” detections. This suggests that learning to detect such bugs is a hard problem. Furthermore, many literals serve as default “conﬁgurations” (e.g. the number of retries for a network request) and different values are not bugs. We posit that a large percentage of literal replacements the selector learns to make fall in this category.
We also found that some repairs suggested by the model actually produce semantically equivalent code. For example, the model lacks knowledge that two variables refer to the same object in memory (aliasing), and so attempts to “repair” variable misuse bugs by switching between these. Other examples includes checking the return values of standard functions such as Python’s str.find, which returns -1 if the query string is not found. In such cases, PYBUGLAB often suggested to rewrite an if x.find(y) <= -1 to if x.find(y) == -1, which makes no difference in practice.
These false negatives can be attributed to the fact that the bug selector model considers such changes as introducing bugs, even though they are not actually changing behavior. This suggests that for better results, the rewrite rules need to ensure that the rewrites are not semantics-preserving and represent bugs.
Finally, some reported issues were sufﬁciently complex that it took us (the human authors) a couple of minutes of thought to conclude that a warning is spurious. Simultaneously, there are some warnings that are “obviously” incorrect to us, but the reasons why the neural models raise them is unclear. This highlights the importance of research on explainability techniques along with better ways to calibrate model conﬁdence. The fact that selectors may introduce spurious “bugs” may also be affecting how the detector model learns. Ideas that have appeared in reinforcement learning, such as the one of
Dennis et al. [8], may allow models to improve their performance in spite of spurious bugs. the ... ... 1000... ........
... ..only... .19...of... .. reported ........... found ...to ...be.......... warnings ......were........
Overall, bugs. Of these 19, we reported 11 on GitHub (6 already merged, 5 pending approval). See Appx. G for details. 3 other bugs had already been ﬁxed between the version PYBUGLAB processed and the current version or the project was deprecated, whereas another 5 bugs are minor and we decided not to report them. One of the detected bugs is shown in Fig. 3b. Overall, most of the detected bugs appear within unit tests, logging, or exception handling, possibly because bugs there do not impact the core functionality of a project. However, given the number of such bugs we collected in PYPIBUGS, we believe that such bugs arise equally often in other code, but that they are detected and ﬁxed more quickly. real-life....... 9
Although our analysis only forms a lower bound on the precision of PYBUGLAB and related methods, it suggests that there is still ample room for future improvements towards making machine learning-based bug detection and repair practically useful. 6