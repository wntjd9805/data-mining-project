Abstract
To deal with changing environments, a new performance measure—adaptive regret, deﬁned as the maximum static regret over any interval, was proposed in online learning. Under the setting of online convex optimization, several algorithms have been successfully developed to minimize the adaptive regret. However, existing algorithms lack universality in the sense that they can only handle one type of convex functions and need apriori knowledge of parameters. By contrast, there exist universal algorithms, such as MetaGrad, that attain optimal static regret for multiple types of convex functions simultaneously. Along this line of research, this paper presents the ﬁrst universal algorithm for minimizing the adaptive regret of convex functions. Speciﬁcally, we borrow the idea of maintaining multiple learning rates in MetaGrad to handle the uncertainty of functions, and utilize the technique of sleeping experts to capture changing environments. In this way, our algorithm automatically adapts to the property of functions (convex, exponentially concave, or strongly convex), as well as the nature of environments (stationary or changing).
As a by product, it also allows the type of functions to switch between rounds. 1

Introduction
Online learning aims to make a sequence of accurate decisions given knowledge of answers to previous tasks and possibly additional information [Shalev-Shwartz, 2011]. It is performed in a sequence of consecutive rounds, where at round t the learner is asked to select a decision wt from a domain Ω. After submitting the answer, a loss function ft : Ω (cid:55)→ R is revealed and the learner suffers a loss ft(wt). The standard performance measure is the regret [Cesa-Bianchi and Lugosi, 2006]:
Regret(T ) =
T (cid:88) t=1 ft(wt) − min w∈Ω
T (cid:88) t=1 ft(w) deﬁned as the difference between the cumulative loss of the online learner and that of the best decision chosen in hindsight. When both the domain Ω and the loss ft(·) are convex, it becomes online convex optimization (OCO) [Zinkevich, 2003].
In the literature, there exist plenty of algorithms to minimize the regret under the setting of OCO
[Hazan, 2016]. However, when the environment undergoes many changes, regret may not be the best measure of performance. That is because regret chooses a ﬁxed comparator, and for the same reason, it is also referred to as static regret. To address this limitation, Hazan and Seshadhri [2007] introduce the concept of adaptive regret, which measures the performance with respect to a changing comparator. Following the terminology of Daniely et al. [2015], we deﬁne the strongly adaptive 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
regret as the maximum static regret over intervals of length τ , i.e.,
SA-Regret(T, τ ) = max
[p,p+τ −1]⊆[T ] (cid:32)p+τ −1 (cid:88) t=p ft(wt) − min w∈Ω (cid:33) ft(w)
. p+τ −1 (cid:88) t=p (1)
Since the seminal work of Hazan and Seshadhri [2007], several algorithms have been successfully developed to minimize the adaptive regret of convex functions, including general convex, exponenti-ally concave (abbr. exp-concave) and strongly convex functions [Hazan and Seshadhri, 2009, Jun et al., 2017a, Zhang et al., 2018b]. However, existing methods can only handle one type of convex functions. Furthermore, when facing exp-concave and strongly convex functions, they need to know the moduli of exp-concavity and strong convexity. The lack of universality hinders their applications to real-world problems.
On the other hand, there do exist universal algorithms, such as MetaGrad [van Erven and Koolen, 2016], that attain optimal static regret for multiple types of convex functions simultaneously. This observation motivates us to ask whether it is possible to design a single algorithm to minimize the adaptive regret of multiple types of functions. This is very challenging because the algorithm needs to enjoy dual adaptivity, adaptive to the function type and adaptive to the environment. In this paper, we provide an afﬁrmative answer by developing a Universal algorithm for Minimizing the Adaptive regret (UMA). First, inspired by MetaGrad, UMA maintains multiple learning rates to handle the uncertainty of functions. In this way, it supports multiple types of functions simultaneously and identiﬁes the best learning rate automatically. Second, following existing studies on adaptive regret,
UMA deploys sleeping experts [Freund et al., 1997] to minimize the regret over any interval, and thus achieves a small adaptive regret and captures the changing environment.
The main advantage of UMA is that it attains second-order regret bounds over any interval. As a result, it can minimize the adaptive regret of general convex functions, and automatically take advantage of easier functions whenever possible. Speciﬁcally, UMA attains O(
α log τ log T ) and
O( 1
λ log τ log T ) strongly adaptive regrets for general convex, α-exp-concave and λ-strongly convex functions respectively, where d is the dimensionality. All of these bounds match the state-of-the-art results on adaptive regret [Jun et al., 2017a, Zhang et al., 2018b] exactly. Furthermore, UMA can also handle the case that the type of functions changes between rounds. For example, suppose the online functions are general convex during interval I1, then become α-exp-concave in I2, and ﬁnally switch to λ-strongly convex in I3. When facing this function sequence, UMA achieves O((cid:112)|I1| log T ),
O( d
λ log |I3| log T ) regrets over intervals I1, I2 and I3, respectively.
α log |I2| log T ) and O( 1
τ log T ), O( d
√ 2