Abstract
Gaussian processes with deep neural networks demonstrate to be a strong learner for few-shot learning since they combine the strength of deep learning and kernels while being able to well capture uncertainty. However, it remains an open problem to leverage the shared knowledge provided by related tasks. In this paper, we propose to learn Gaussian processes with dense inducing variables by meta-learning for few-shot learning. In contrast to sparse Gaussian processes, we deﬁne a set of dense inducing variables to be of a much larger size than the support set in each task, which collects prior knowledge from experienced tasks. The dense inducing variables specify a shared Gaussian process prior over prediction functions of all tasks, which are learned in a variational inference framework and offer a strong inductive bias for learning new tasks. To achieve task-speciﬁc prediction functions, we propose to adapt the inducing variables to each task by efﬁcient gradient descent.
We conduct extensive experiments on common benchmark datasets for a variety of few-shot learning tasks. Our dense Gaussian processes present signiﬁcant improvements over vanilla Gaussian processes and comparable or even better performance with state-of-the-art methods. 1

Introduction
Meta learning [31, 4, 18], also referred to as learning to learn, aims at learning to acquire shared knowledge from a set of related tasks so as to fast resolve novel tasks sampled from the same underlying task distribution. Meta learning largely stimulates the rise of few-shot learning [9], and the recent advantages are mainly driven by designing learning algorithms that learn from massive tasks and acquire prior knowledge, which combined with a small amount of labeled data, induces models that produce reliable predictions on novel tasks. In practice, the acquisition of prior knowledge can be realized in different forms. Model agnostic meta-learning (MAML) [9] learns to adapts to new tasks by few iterations of gradient descend, which inspires many follow-up methods [3, 12, 22, 46].
The adaptation of the entire network makes it hard to be scaled to large networks, and many recent efforts focus on adapting the last classiﬁcation layer only [14, 5], while assuming a universal feature extractor that is shared across all tasks. Despite the remarkable progress, challenges remain due to the uncertainty in making predictions with very limited data, which requires models to have high robustness for few-shot learning.
Gaussian processes [25] serve as a powerful model for the inference of functions, which enjoy appealing properties including natural uncertainty quantiﬁcation and robustness when the amount of data is limited. By combining the strong learning ability of deep neural networks, Gaussian processes with deep kernels further demonstrate improved performance on supervised learning [44]. To alleviate the computational cost, sparse Gaussian processes were widely studied by learning a sparse set of
⇤Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
inducing variables to effectively approximate the full dataset. In sparse Gaussian processes, the inducing variables usually in the forms of pseudo data or a subset of training data are expected to capture statistics of the whole dataset.
Gaussian processes offer desirable robustness to data scarcity by capturing uncertainty, and enjoy the high expressiveness of deep kernels. This makes Gaussian processes a well-suited learning model for few-shot learning as demonstrated in recent works [33, 23, 38]. It is shown in deep kernel transfer (DKT) [23] that a Gaussian process with a meta-learned deep kernel can deliver strong performance through a simple kernel transfer. DKT relies on an important assumption that a universal deep kernel can be obtained through training on a limited amount of tasks. However, this assumption can be too restricted to generalize to novel tasks. In practice, when solving novel tasks, it is hardly guaranteed that the learned deep kernels can well explain the data due to the scarce labeled data, and failing to properly ﬁt the Gaussian process prior can lead to inaccurate predictions. It is therefore crucial to leverage more knowledge to be shared among Gaussian processes for individual tasks, which remains an outstanding problem for Gaussian process few-shot learning.
In this paper, we introduce learning to learn Gaussian processes by a dense set of inducing variables for few-shot learning. In particular, we assume a Gaussian process prior over the prediction functions of few-shot learning tasks, which is speciﬁed by a set of inducing variables learned from data. These inducing variables are dense in the sense that they have a much larger cardinality compared to the support set of each individual task. Under the meta-learning setting, the dense inducing variables are learned to collect the shared knowledge from experienced tasks to improve the learning of new tasks effectively with limited data.
To achieve task-speciﬁc prediction functions, we propose to adapt the inducing variables to each task by the gradient descent update. Without adding extra model parameters, the adaptation permits online ﬁtting to the Gaussian processes prior to each individual task, and therefore allows the model to better ﬁt any novel tasks at test time. In addition, in contrast to sparse Gaussian processes, we learn the inducing variables in a low-dimensional deep feature space, which signiﬁcantly reduces the compute cost of both learning and adapting the dense inducing variables.
The resultant dense Gaussian processes inherit the strong learning ability of deep kernels and enjoy the robustness to data scarcity and innate ability of uncertainty quantiﬁcation, providing an effective few-shot learner. We validate the effectiveness of the proposed dense Gaussian processes on a variety of few-shot learning tasks. We observe that dense Gaussian processes achieve substantial performance improvements over vanilla deep Gaussian processes for few-shot learning and deliver state-of-the-art performance on common benchmarks for few-shot learning. Moreover, our dense Gaussian processes demonstrate strong generalization on cross-domain few-shot learning tasks.
In summary, we make three major contributions as follows:
• We introduce dense Gaussian processes by learning a set of dense inducing variables for few-shot learning. The inducing variables specify a Gaussian process prior over predictive functions across tasks, which collect shared knowledge from experienced tasks and provide strong inductive bias for learning new tasks efﬁciently and effectively.
• We propose gradient descent based adaptation to establish task-speciﬁc prediction functions based on the Gaussian process prior. The gradient descent-based adaptation is efﬁcient without any auxiliary network components for inferring task representations.
• We conduct extensive experiments on common benchmark datasets for few-shot classi-ﬁcation. The proposed dense Gaussian processes achieve consistent improvements over previous Gaussian processes with deep kernels and deliver comparable or even better results than state-of-the-art methods. 2 Methodology
In this section, we start with a brief review of few-shot learning; and the key ingredients that build the foundation of the proposed method. We then introduce in detail learning to learn Gaussian processes with dense inducing variables for few-shot learning. 2
2.1