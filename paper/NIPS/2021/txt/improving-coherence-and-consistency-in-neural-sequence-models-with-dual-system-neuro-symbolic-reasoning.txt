Abstract
Human reasoning can be understood as an interplay between two systems: the intuitive and associative (“System 1”) and the deliberative and logical (“System 2”).
Neural sequence models—which have been increasingly successful at performing complex, structured tasks—exhibit the advantages and failure modes of System 1: they are fast and learn patterns from data, but are often inconsistent and incoherent.
In this work, we seek a lightweight, training-free means of improving existing
System 1-like sequence models by adding System 2-inspired logical reasoning.
We explore several variations on this theme in which candidate generations from a neural sequence model are examined for logical consistency by a symbolic reasoning module, which can either accept or reject the generations. Our approach uses neural inference to mediate between the neural System 1 and the logical
System 2. Results in robust story generation and grounded instruction-following show that this approach can increase the coherence and accuracy of neurally-based generations. 1

Introduction
Despite recent success, neural sequence models often fail to produce consistent and coherent genera-tions. When generating stories, language models may forget the attributes of speciﬁc characters (such as personality and background information) (Welleck et al., 2018), ignore previously established relationships between characters (such as family relationships) (Sinha et al., 2019), or otherwise contradict prior statements (Brown et al., 2020). Similarly, neural models can make statements that contradict basic world knowledge or the logical entailment structure of known facts.
Lake & Murphy (2020) illustrated several of these issues with GPT-2 (Radford et al., 2019). When given prompts of the form “A dolphin is a
”, GPT-2 predicts that the most likely answer is
“mammal”, “ﬁsh”, or “bird” depending on small differences in the wording of the prompt. In another example, GPT-2 states that unicorns have “four horns,” directly after implying that unicorns only have one horn. Upon diagnosing such issues, it is unclear how to apply a targeted ﬁx to the model, especially if retraining or ﬁne-tuning is impractical.
In this work, we draw on insights from cognitive science, especially from “dual process” theories of reasoning (Evans, 2003), to explore how neural sequence models can better interface with prior knowledge and be made more coherent and consistent. According to dual process theories, human cognition can be understood as an interplay between a more intuitive and associative “System 1” and a more deliberative and logical “System 2.” Within this broad framework, automatic actions are driven by System 1, whereas System 2 engages for more deliberative control: for example, judging the validity of a logical argument that requires multiple steps of reasoning (Kahneman, 2013).
∗Correspondence to mnye@mit.edu. Work primarily done during an internship at Facebook AI Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Schematic of dual-system approach to text generation. Conditioned on previous text, a
“System 1” neural generation model produces candidate next sentences. Semantic parses for each candidate are generated via few-shot parsing from GPT-3 and compared to a minimal world model to check consistency. Only candidates consistent with the world model state are incorporated into the
ﬁnal generation.
The prominent neural language models of today are single systems, with weaknesses akin to those exhibited by the human System 1. For example, the cognitive reﬂection test (CRT) (Frederick, 2005) is a classic probe of System 1 vs. System 2 reasoning in humans. Participants answer a set of simple questions that have superﬁcially compelling, but logically invalid, answers. These incorrect answers are often generated as a ﬁrst “gut” response (putatively, by System 1 intuitive thinking); upon reﬂection, however, participants often realize that their responses were not logically or mathematically consistent (via more explicit System 2 reasoning). Consider the CRT problem on the left below:
A ball and a bat cost $1.10.
The bat costs one dollar more than the ball.
How much does the ball cost?
Total cost in prompt GPT-3 response
$1.10
$1.20
$1.30
$1.70 10 cents 20 cents
$0.30
$0.70
Reading quickly, you might be tempted to say the ball costs 10 cents. Most participants give this response, in fact, especially if they are under time pressure or have limited attention (Kahneman, 2013). Of course, if the bat is $1.00 more than the ball, and the ball costs 10 cents, then the total cost would be $1.20. The correct answer is that the ball costs 5 cents. Notably, in this and other classic CRT problems, GPT-3 (Brown et al., 2020) predicts the same “gut” response (prediction in red above; the table above shows that adjusting the price in the prompt also leads to similar effects; see
Appendix Figure 8 for more CRT examples). GPT-3 appears vulnerable to the same sort of intuitive, unsystematic pattern recognition errors as humans—in this case, incorrectly subtracting one dollar from $1.10, without conﬁrming that the answer satisﬁes each of the problem constraints.
Numerous studies have shown that engagement of System 2-style effort can help “override or inhibit default responses emanating from System 1” (Evans, 2003), correcting inconsistent or un-systematic intuitive impulses. For example, when System 2 is engaged by asking people to take more time to respond, people’s accuracy improves on the CRT task above (Kahneman, 2013). It has been argued that integrating System 2 processing could similarly improve AI systems (Goyal & Bengio, 2020;
Garcez & Lamb, 2020), and here we explore this idea as applied to neural sequence models.
In this work, we take inspiration from dual process theories to explore a neuro-symbolic generation system, wherein predictions from a neural model are treated as System 1 proposals, and a logical, deliberative System 2 ﬁlters these proposals for consistency and soundness (see Figure 1). We further take inspiration from the fact that humans often do not need explicit supervision to reason about new problems or domains (e.g., see human evaluation task in Section 4.2) and require that the System 2 module not need additional problem-speciﬁc training, especially on example contradictions or commonsense violations. People can handle novelty by reconﬁguring, rather than retraining, their internal models (Lake et al., 2017), and we strive to build machine systems capable of the same.
We show how a lightweight, easy-to-implement System 2 model can help improve coherence and consistency by adding a small amount of symbolic reasoning.
We tackle two kinds of domains: text generation and instruction following. In both cases, we construct generative models over sequences by using a neural generation model to propose candidate generations and a symbolic world model that can accept or reject the generations and resample proposals if necessary. We ﬁrst illustrate the approach by generating short stories based on the bAbI dataset (Weston et al., 2015); this pedagogical, synthetic example illustrates how basic commonsense knowledge of objects, agents, and places can inform a text generation model. We then test our 2
approach on rich, natural language vignettes based on CLUTRR (Sinha et al., 2019), focusing on ensuring consistency of family and interpersonal relationships. In both text generation domains, we interface between the explicit logical knowledge/reasoning of System 2 and generations of System 1 using a few-shot learning approach with state-of-the-art neural language models (GPT-3), which requires no additional training or ﬁne-tuning. Even using off-the-shelf transformers and symbolic solvers, our dual-system model improves the consistency and coherence of text generations as measured by human judges. We test our approach also on instruction following, showing how goal-prediction models and execution models can easily be combined to achieve improved performance in low-data regimes. We show improvements over previous work in the gSCAN grounded compositional challenge (Ruis et al., 2020); a dual-system model requires much less data to train than previous models, and achieves higher accuracy and stronger generalization. Overall, our ﬁndings indicate that neuro-symbolic, dual process models are a promising means of addressing longstanding problems of robustness and consistency in neural sequence models. 2