Abstract
Continual learning aims to learn a sequence of tasks from dynamic data distribu-tions. Without accessing to the old training samples, knowledge transfer from the old tasks to each new task is difﬁcult to determine, which might be either positive or negative. If the old knowledge interferes with the learning of a new task, i.e., the forward knowledge transfer is negative, then precisely remembering the old tasks will further aggravate the interference, thus decreasing the performance of continual learning. By contrast, biological neural networks can actively forget the old knowledge that conﬂicts with the learning of a new experience, through regulating the learning-triggered synaptic expansion and synaptic convergence.
Inspired by the biological active forgetting, we propose to actively forget the old knowledge that limits the learning of new tasks to beneﬁt continual learning. Under the framework of Bayesian continual learning, we develop a novel approach named
Active Forgetting with synaptic Expansion-Convergence (AFEC). Our method dy-namically expands parameters to learn each new task and then selectively combines them, which is formally consistent with the underlying mechanism of biological active forgetting. We extensively evaluate AFEC on a variety of continual learning benchmarks, including CIFAR-10 regression tasks, visual classiﬁcation tasks and
Atari reinforcement tasks, where AFEC effectively improves the learning of new tasks and achieves the state-of-the-art performance in a plug-and-play way. 1

Introduction
The ability to continually learn numerous tasks from dynamic data distributions is critical for deep neural networks, which needs to remember the old tasks by avoiding catastrophic forgetting [18] while effectively learn each new task by improving forward knowledge transfer [17]. Due to the dynamic data distributions, forward knowledge transfer might be either positive or negative, and is difﬁcult to determine without accessing to the old training samples. If the forward knowledge transfer is negative, i.e., learning a new task from the old knowledge is worse than learning the new task on a randomly-initialized network [36, 17], then precisely remembering the old tasks will severely interfere with the learning of the new task, thus decreasing the performance of continual learning.
⇤Corresponding author: J. Zhu and Y. Zhong. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
By contrast, biological neural networks can effectively learn a new experience on the basis of re-membering the old experiences, even if they conﬂict with each other [18, 5]. This advantage, called memory ﬂexibility, is achieved by active forgetting of the old knowledge that interferes with the learning of a new experience [29, 5]. The latest data suggested that the underlying mechanism of biological active forgetting is to regulate the learning-triggered synaptic expansion and synaptic con-vergence (Fig. 1, see Appendix A for neuroscience background and our biological data). Speciﬁcally, the biological synapses expand additional functional connections to learn a new experience together with the previously-learned functional connections (synaptic expansion). Then, all the functional connections are pruned to the amount before learning (synaptic convergence).
Inspired by the biological active forgetting, we propose to actively forget the old knowledge that in-terferes with the learning of new tasks without signiﬁcantly increas-ing catastrophic forgetting, so as to beneﬁt continual learning.
Speciﬁcally, we adopt Bayesian continual learning and actively forget the posterior distribution that absorbs all the information of the old tasks with a forgetting factor to better learn each new task. Then, we derive a novel method named Active Forgetting with synaptic Expansion-Convergence (AFEC), which is formally consistent with the underlying mechanism of biological active forgetting at synaptic structures. Beyond regular weight regularization approaches [12, 1, 35, 2], which selectively penalize changes of the important parameters for the old tasks, AFEC dynamically expands parameters only for each new task to avoid potential negative transfer from the main network, while the forgetting factor regulates a penalty to selectively merge the main network parameters with the expanded parameters, so as to learn a better overall representation of both the old tasks and the new task.
Figure 1: The biological active forgetting is achieved by regulat-ing the learning-triggered synaptic expansion-convergence.
We extensively evaluate AFEC on continual learning of CIFAR-10 regression tasks, a variety of visual classiﬁcation tasks, and Atari reinforcement tasks [10], where AFEC achieves the state-of-the-art (SOTA) performance. We empirically validate that the performance improvement results from effectively improving the learning of new tasks without increasing catastrophic forgetting. Further,
AFEC can be a plug-and-play method that signiﬁcantly boosts the performance of representative continual learning strategies, such as weight regularization [12, 1, 35, 2] and memory replay [21, 9, 6].
Our contributions include: (1) We draw inspirations from the biological active forgetting and propose a novel approach to actively forget the old knowledge that interferes with the learning of new tasks for continual learning; (2) Extensive evaluation on a variety of continual learning benchmarks shows that our method effectively improves the learning of new tasks and achieves the SOTA performance in a plug-and-play way; and (3) To the best of our knowledge, we are the ﬁrst to model the biological active forgetting and its underlying mechanism at synaptic structures, which suggests a potential theoretical explanation of how the underlying mechanism of biological active forgetting achieves its function of forgetting the past and continually learning conﬂicting experiences [29, 5]. 2