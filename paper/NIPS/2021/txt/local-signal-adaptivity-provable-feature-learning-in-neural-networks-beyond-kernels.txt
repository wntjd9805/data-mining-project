Abstract
Neural networks have been shown to outperform kernel methods in practice (includ-ing neural tangent kernels). Most theoretical explanations of this performance gap focus on learning a complex hypothesis class; in some cases, it is unclear whether this hypothesis class captures realistic data. In this work, we propose a related, but alternative, explanation for this performance gap in the image classiﬁcation setting, based on ﬁnding a sparse signal in the presence of noise. Speciﬁcally, we prove that, for a simple data distribution with sparse signal amidst high-variance noise, a simple convolutional neural network trained using stochastic gradient descent simultaneously learns to threshold out the noise and ﬁnd the signal. On the other hand, the corresponding neural tangent kernel, with a ﬁxed set of predetermined features, is unable to adapt to the signal in this manner. We supplement our theo-retical results by demonstrating this phenomenon empirically: in CIFAR-10 and
MNIST images with various backgrounds, as the background noise increases in intensity, a CNN’s performance stays relatively robust, whereas its corresponding neural tangent kernel sees a notable drop in performance. We therefore propose the local signal adaptivity (LSA) phenomenon as one explanation for the superiority of neural networks over kernel methods. 1

Introduction
Recently, deep learning (using multi-layer, non-linear neural networks) has demonstrated superior performance over traditional linear learners in many machine learning tasks. These achievements have bred much theoretical investigation into whether neural networks are, in fact, superior - and why. On the one hand, the Neural Tangent Kernel (NTK) and derivative works show that, under certain (limiting) conditions, a gradient-descent-trained neural network reduces to a kernel method with a speciﬁc architecture- and initialization-determined kernel [Jacot et al., 2018, Du et al., 2019].
However, this does not seem to be the full story, as it fails to capture the feature learning aspect of neural network training. This distinction between a ﬁxed feature representation and a data-adaptive feature representation has been studied from a variety of perspectives, including the lazy vs. active regime framework [Chizat et al., 2019, Woodworth et al., 2020, Moroshko et al., 2020, Geiger et al., 2020, Wang et al., 2020]. Building on these insights, there has been increasing interest in now showing a provable gap between the performance of neural networks and kernel methods in various settings [Ghorbani et al., 2019, 2020, Allen-Zhu and Li, 2019, 2020, Li et al., 2020b, Malach et al., 2021, Kamath et al., 2020, Reﬁnetti et al., 2021, Daniely and Malach, 2020, Chen et al., 2020,
Domingo-Enrich et al., 2021]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Examples from the IMAGENET2012 dataset, illustrating background noise in natural image classiﬁcation tasks.
Figure 2: Sparsity of intermediate WRN layers during training on CIFAR-10.
In this work, we extend this theoretical investigation into the superiority of neural networks over linear learners and, inspired by practical settings, propose a new line of reasoning that we refer to as “Local Signal Adaptivity”. In particular, we explore the power of convolutional neural networks (CNNs) in image classiﬁcation, compared to linear functions over prescribed feature mappings.
Our setting: We study a simple data distribution that captures one key property of natural image classiﬁcation tasks: a small set of localized “label-determining” features embedded within a “noisy” background. We formally prove that even when such background occupies a rather large fraction of an image, a CNN can be quite effective at locating the label-determining signal and thus ignoring background information that is mostly irrelevant to the true label, to achieve high accuracy.
Our result:
In the formal setting of our simple data distribution (presented in Section 3), we ask whether a particular two-layer CNN trained via stochastic gradient descent can provably acquire this “signal-ﬁnding” ability and how this compares to its associated ﬁnite-width convolutional neural tangent kernel (CNTK). We answer with a separation result between our CNN and its CNTK: We formally prove that a small CNN, trained using standard SGD from random initialization, can efﬁciently learn to ﬁnd the “signal” and threshold out the noise, whereas the corresponding CNTK requires a comparatively larger model (i.e., with more features) in order to accomplish this.
Empirical justiﬁcation: While we pick a simple data distribution in our work to illustrate the main idea and obtain a formal proof, we point out that our setting is very natural in real images: in many image classiﬁcation tasks, the label-determining feature only occupies a small fraction of the image, and most other parts are background noise (Figure 1). Furthermore, in neural networks trained on natural images, it is generally accepted that activation patterns become increasingly sparse throughout training, effectively zeroing out the activations of low-magnitude noise and locating the true signal (suggestive of the denoising/LSA phenomenon studied in this work). For completeness, we have included such an experiment in Figure 2 above, illustrating how the average percentage of active neurons per instance decreases throughout training (details are provided in Appendix C). Finally, to empirically study our theoretical results, we create new datasets by embedding CIFAR-10 and
MNIST images within either random Gaussian or IMAGENET backgrounds. Our experiments show that, as the intensity of the background noise grows and thus the “denoising task” becomes harder, the performance of the neural network stays relatively stable, while the performance of the corresponding
NTK does, in fact, degrade signiﬁcantly (Section 6, Figure 4).
Based on our theorem and experiments, we therefore believe that this per-instance “signal ﬁnding within noisy backgrounds” ability of convolutional neural networks, which we dub “Local Signal
Adaptivity” (LSA), is one key component of the superiority of SGD-trained convolutional neural networks over ﬁxed feature mappings. 2
2