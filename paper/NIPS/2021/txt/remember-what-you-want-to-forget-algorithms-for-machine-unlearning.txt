Abstract
We study the problem of unlearning datapoints from a learnt model. The learner ﬁrst receives a dataset S drawn i.i.d. from an unknown distribution, and outputs a model w that performs well on unseen samples from the same distribution. However, at
S can request to be unlearned, some point in the future, any training datapoint z thus prompting the learner to modify its output model while still ensuring the same b accuracy guarantees. We initiate a rigorous study of generalization in machine unlearning, where the goal is to perform well on previously unseen datapoints. Our focus is on both computational and storage complexity.
For the setting of convex losses, we provide an unlearning algorithm that can unlearn up to O(n/d1/4) samples, where d is the problem dimension. In compar-ison, in general, differentially private learning (which implies unlearning) only guarantees deletion of O(n/d1/2) samples. This demonstrates a novel separation between differential privacy and machine unlearning. 2 1

Introduction
Many organizations and companies employ user data to train machine learning models for a wide array of applications, ranging from movie recommendations to health care. While some of these organizations allow users to withdraw their consent from their data being used (at which point the organization will delete the user’s data), less savory businesses might covertly retain user data. Given the potential for misuse, legislators worldwide have wisely introduced laws that mandate user data deletion upon request. These include the European Union’s General Data Protection Regulation (GDPR), the California Consumer Privacy Act (CCPA), and Canada’s proposed Consumer Privacy
Protection Act (CPPA).
There is some natural ambiguity present in these guidelines. Is it sufﬁcient to simply delete the user’s data, or must one also take action on machine learning systems that used this data for training?
Indeed, by now, privacy researchers are well-aware that user data may be extracted from trained machine learning models (e.g., Shokri et al. [2017], Carlini et al. [2019]). In a potentially landmark decision, the Federal Trade Commission recently ordered a company to delete not only data from users who deleted their accounts, but also models and algorithms derived from this data [Federal
Trade Commission, 2021]. This suggests that organizations have an obligation to retrain any machine learning models after excluding users whose data has been deleted.
⇤Authors in alphabetical order. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, naïvely retraining models after every deletion request would be prohibitively expensive: training modern machine learning models may take weeks, and use resources of value in the millions.
One could instead imagine more careful methods, which attempt to excise the required datapoints from the model: crucially, without incurring the cost of retraining from scratch. This notion is called machine unlearning. The goal would be to obtain a model which is identical to the alternative model that would be obtained when trained on the dataset after removing the points that need to be forgotten.
This requirement is rather strong: Ginart et al. [2019] proposed a relaxed notion of deletion, in which the model must only be close to the alternative, where closeness is deﬁned in a way reminiscent of differential privacy [Dwork et al., 2006a,b] (our variant of this notion is described in Deﬁnition 2).
This relaxation has inspired the design of several efﬁcient algorithms for data deletion from machine learning models [Guo et al., 2020, Izzo et al., 2021, Neel et al., 2021, Ullah et al., 2021].
As mentioned before, one naïve strategy involves retraining the model from scratch, sans the deleted datapoints. When the training dataset is large, this approach is undesirable for several reasons. First, it is computationally very expensive. Even iteration over the training data can be too costly, let alone training a new model on it. Second, preserving the entire training dataset consumes a signiﬁcant amount of storage.2
Another straightforward approach involves model checkpointing, in which the learner preemptively stores backup models in which certain points have been excluded. While this strategy makes it easy to quickly return an appropriate backup model upon receiving a deletion request, the downside is that one typically has to store a number of additional models which scales with the training data size, which may be prohibitively large. As we can see from these examples, computational and storage complexity are two vital metrics when designing a machine unlearning algorithm.
Finally, while there has recently been a wealth of results in machine unlearning, all of it has focused on the core problem of empirical risk minimization, where the goal is to minimize the training loss.
However, to fulﬁl the promise of machine learning, we desire algorithms that can generalize to previously unseen test data. Motivated simultaneously by all of these concerns, our goal is to address the following question:
How do we design resource-efﬁcient machine unlearning algorithms which generalize?
Our contributions. We initiate a new line of inquiry in machine unlearning:
•
•
•
We investigate generalization properties of unlearning algorithms, in particular asking: how many samples can we unlearn while still ensuring good performance on unseen test data? In comparison, prior work focused on the empirical training loss only.
We consider machine unlearning simultaneously under storage constraints as well as the previously studied computation constraints. Unlike prior work, our algorithms do not require the training data to be available to the unlearning algorithm when deleting samples.
A clean approach for unlearning is to ignore which particular samples are being unlearnt and directly apply known algorithms and guarantees from differential privacy (DP). We show a strict separation between DP and machine unlearning.
⇥(n/pd) samples while still retaining
In particular, algorithms based on DP can delete at most test loss performance, where d denotes the dimension of the problem. On the other hand, we provide efﬁcient unlearning algorithms that take into account the particular samples to
O(n/d1/4) samples, thus giving a quadratic be unlearnt and show that we can delete up to improvement in terms of dependence of d over DP. Our results apply to both strongly convex and convex loss functions. e e 1.1