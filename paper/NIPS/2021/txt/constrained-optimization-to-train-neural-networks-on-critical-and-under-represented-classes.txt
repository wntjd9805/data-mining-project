Abstract
Deep neural networks (DNNs) are notorious for making more mistakes for the classes that have substantially fewer samples than the others during training. Such class imbalance is ubiquitous in clinical applications and very crucial to handle because the classes with fewer samples most often correspond to critical cases (e.g., cancer) where misclassiﬁcations can have severe consequences. Not to miss such cases, binary classiﬁers need to be operated at high True Positive Rates (TPRs) by setting a higher threshold, but this comes at the cost of very high False Positive
Rates (FPRs) for problems with class imbalance. Existing methods for learning under class imbalance most often do not take this into account. We argue that prediction accuracy should be improved by emphasizing the reduction of FPRs at high TPRs for problems where misclassiﬁcation of the positive, i.e. critical, class samples are associated with higher cost. To this end, we pose the training of a DNN for binary classiﬁcation as a constrained optimization problem and introduce a novel constraint that can be used with existing loss functions to enforce maximal area under the ROC curve (AUC) through prioritizing FPR reduction at high TPR. We solve the resulting constrained optimization problem using an
Augmented Lagrangian method (ALM). Going beyond binary, we also propose two possible extensions of the proposed constraint for multi-class classiﬁcation problems. We present experimental results for image-based binary and multi-class classiﬁcation applications using an in-house medical imaging dataset, CIFAR10, and CIFAR100. Our results demonstrate that the proposed method improves the baselines in majority of the cases by attaining higher accuracy on critical classes while reducing the misclassiﬁcation rate for the non-critical class samples.1 1

Introduction
Deep Neural Networks (DNNs) perform extremely well in many classiﬁcation tasks when sufﬁciently large and representative datasets are available for training. However, in many real world applications, it is not uncommon to encounter highly-skewed class distributions, i.e., majority of the data belong to only a few classes while some classes are represented with scarce instances. Training DNNs on such imbalanced datasets leads to models that are biased toward majority classes with poor prediction accuracy for samples of minority class. While this is problematic for all such applications, it poses an even greater issue for “critical” applications where misclassifying samples belonging to the minority class can have severe consequences. One domain where such applications are common and machine learning is having an important impact is medical imaging.
In medical imaging, applications with data imbalance are ubiquitous [14] and costs of making some types of mistakes are more severe than others. For instance, in a diagnosis application, discarding a 1Code is available at: https://github.com/salusanga/alm-dnn. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
cancer case as healthy (False Negative) is more costly than classifying a healthy subject as having cancer (False Positive). While the latter creates burden for the subject as well as health-care system through additional tests that may be invasive and expensive, the former, i.e. failure to identify a cancerous case, would delay the diagnosis and jeopardise the treatment success. In such applications, binary classiﬁers are operated at high True Positive Rates (TPRs) even when this means having higher
False Positive Rates (FPRs). To make matters more complicated, there are usually signiﬁcantly fewer samples to represent critical classes, where mistakes are more severe. For instance, in [6] authors found out that in prostate cancer screening only 30% of even the most suspicious cases identiﬁed with initial testing actually have cancer. Such class imbalance increases the FPR even higher in “critical” applications, because the models tend to misclassify minority classes more often. Useful algorithms need to achieve low FPR at high TPR operating points, even under class imbalance.
While various methods for learning with imbalanced datasets exist, to the best of our knowledge, these methods do not take into account the fact that “critical” applications need to be operated at high accuracy for the critical classes. We believe that for such applications ensuring low misclassiﬁcation rate for the non-critical samples and high accuracy for the critical classes should be the main goal, to make binary classiﬁers useful in practice. This motivates us to design new strategies for training
DNNs for classiﬁcation.
Contribution: In this paper, we pose the training of a DNN for binary classiﬁcation under class imbalance as a constrained optimization problem and propose a novel constraint that can be used with existing loss functions. We deﬁne the constraint using Mann-Whitney statistics [16] in order to maximize the AUC, but in an asymmetric way to favor reduction of false positives at high true positive (or low false negative) rates. Then, we transfer the constrained problem to its dual unconstrained optimization problem using an Augmented Lagrangian method (ALM) [2]. We optimize the resulting loss function using stochastic gradient descent. Unlike the existing methods that directly optimize AUC, we incorporate AUC optimization in a principled way into a constrained optimization framework. We ﬁnally present two possible extensions of the proposed constraint for multi-class classiﬁcation problems.
We present an extensive evaluation of the proposed method for image-based binary and multi-class classiﬁcation problems on three datasets: an in-house medical dataset for prostate cancer, CIFAR10, and CIFAR100 [11]. In all datasets, we perform experiments by simulating different class imbalance ratios. In our experiments, we apply the proposed constraint to 9 different baseline loss functions, most of which were proposed to handle class imbalance. We compare the results with the baselines without any constraint. The results demonstrate that the proposed method improves the baselines in majority of the cases. 2