Abstract
We present an end-to-end, model-based deep reinforcement learning agent which dynamically attends to relevant parts of its state during planning. The agent uses a bottleneck mechanism over a set-based representation to force the number of entities to which the agent attends during planning to be small.
In experiments, we investigate the bottleneck mechanism with several sets of customized environments featuring diﬀerent challenges. We consistently observe that the design allows the planning agents to generalize their learned task-solving abilities in compatible unseen environments by attending to the relevant objects, leading to better out-of-distribution generalization performance. Check project page https://github.com/PwnerHarry/CP. 1

Introduction
Whether when planning our paths home from the oﬃce or from a hotel to an airport in an unfamiliar city, we typically focus on a small subset of relevant variables, e.g. the change in position or the presence of traﬃc. An interesting hypothesis of how this path planning skill generalizes across scenarios is that it is due to computation associated with the conscious processing of information [2, 3, 14]. Conscious attention focuses on a few necessary environment elements, with the help of an internal abstract representation of the world [43, 14]. This pattern, also known as consciousness in the ﬁrst sense (C1) [14], has been theorized to enable humans’ exceptional adaptability and learning eﬃciency
[2, 3, 14, 43, 7, 15]. A central characterization of conscious processing is that it involves a bottleneck, which forces one to handle dependencies between very few environmental characteristics at a time [14, 7, 15]. Though focusing on a subset of the available information may seem limiting, it facilitates Out-Of-Distribution (OOD) and systematic generalization to other situations where the ignored variables are diﬀerent and yet still irrelevant [7, 15].
In this paper, we encode some of these ideas into reinforcement learning agents. Rein-forcement learning (RL) is an approach for learning behaviors from agent-environment interactions [41]. However, most of the big successes of RL have been obtained by deep, model-free agents [30, 37, 38]. While Model-Based RL (MBRL) has generated signiﬁcant research due to the potentials of using an extra model [31], its empirical performance has typically lagged behind, with some recent notable exceptions [36, 24, 17].
Our proposal is to take inspiration from human consciousness to build an architecture which learns a useful state space and in which attention can be focused on a small set of variables at any time, where the aspect of “partial planning”1 is enabled by modern deep 1Partial planning is interpreted in diﬀerent ways. For example, concurrent work [26] focuses on modelling “aﬀordable” temporally extended actions, s.t. an “intent” could be achieved more eﬃciently. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
RL techniques [42, 26]. Speciﬁcally, we propose an end-to-end latent-space MBRL agent which does not require reconstructing the observations, as in most existing works, and uses
Model Predictive Control (MPC) framework for decision-time planning [34, 35]. From an observation, the agent encodes a set of objects as a state, with a selective attention bottleneck mechanism to plan over selected subsets of the state (Sec. 4). Our experiments show that the inductive biases improve a speciﬁc form of OOD generalization, where consistent dynamics are preserved across seemingly diﬀerent environment settings (Sec. 5). 2