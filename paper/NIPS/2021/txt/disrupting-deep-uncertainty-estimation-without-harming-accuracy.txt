Abstract
Deep neural networks (DNNs) have proven to be powerful predictors and are widely used for various tasks. Credible uncertainty estimation of their predictions, however, is crucial for their deployment in many risk-sensitive applications. In this paper we present a novel and simple attack, which unlike adversarial attacks, does not cause incorrect predictions but instead cripples the network’s capacity for uncertainty estimation. The result is that after the attack, the DNN is more conﬁdent of its incorrect predictions than about its correct ones without having its accuracy reduced. We present two versions of the attack. The ﬁrst scenario focuses on a black-box regime (where the attacker has no knowledge of the target network) and the second scenario attacks a white-box setting. The proposed attack is only required to be of minuscule magnitude for its perturbations to cause severe uncertainty estimation damage, with larger magnitudes resulting in completely unusable uncertainty estimations. We demonstrate successful attacks on three of the most popular uncertainty estimation methods: the vanilla softmax score, Deep
Ensembles and MC-Dropout. Additionally, we show an attack on SelectiveNet, the selective classiﬁcation architecture. We test the proposed attack on several contemporary architectures such as MobileNetV2 and EfﬁcientNetB0, all trained to classify ImageNet. 1

Introduction
Deep neural networks (DNNs) show great and improving performance in a wide variety of application domains including computer vision and natural language processing. Successful deployment of these models, however, is critically dependent on providing effective uncertainty estimation for their predictions, or employing some kind of selective prediction [8].
In the context of classiﬁcation, practical and well-known uncertainty estimation techniques include: (1) the classiﬁcation prediction’s softmax score [3, 4], which quantiﬁes the embedding margin between an instance to the decision boundary; (2) MC-Dropout [7], which is argued to proxy
Bayesian networks [22, 20, 23]; (3) Deep Ensembles [18], which have shown state-of-the-art results in various estimation settings; and (4) SelectiveNet [9], which learns to estimate uncertainty in a way that produces optimal risk for a speciﬁc desired coverage.
In this paper we show that all these well-known uncertainty estimation techniques are vulnerable to a new and different type of attack that can completely destroy their usefulness and that works in both black-box (where the attacker can only query the attacked model for predicted labels and has no knowledge of the model itself) and white-box (the attacker has complete knowledge about the attacked model) settings. While standard adversarial attacks target model accuracy, the proposed attack is designed to keep accuracy performance intact and refrains from changing the original classiﬁcation predictions made by the attacked model. We call our method ACE: Attack on Conﬁdence
Estimation. To demonstrate the relevance of our ﬁndings, we test ACE on modern architectures, such 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
as MobileNetV2 [25], EfﬁcientNet-B0 [28], as well as on standard baselines such as ResNet50 [13],
DenseNet161 [14] and VGG16 [26]. Our attacks are focused on the task of ImageNet classiﬁcation
[5]. Figure 1 conceptually illustrates how ACE works. Consider a classiﬁer for cats vs. dogs that
Figure 1: The intuition driving ACE on a classiﬁer of cats and dogs, when the uncertainty measure used is softmax. Correct predictions are moved closer to the decision boundary without crossing it, thus increasing their uncertainty (purple arrows), and incorrect predictions are moved further away from the decision boundary to decrease their uncertainty (green arrow). uses its prediction’s softmax score as its uncertainty estimation measurement. An end user asks the model to classify several images, and output only the ones in which it has the most conﬁdence.
Since softmax quantiﬁes the margin from an instance to the decision boundary, we visualize it on a 2D plane where the instances’ distance to the decision boundary reﬂect their softmax score. In the example shown in Figure 1, the classiﬁer was mistaken about one image of a dog, classifying it as a cat, but fortunately its conﬁdence in this prediction is the lowest among its predictions. A malicious attacker targeting the images in which the model has the most conﬁdence would want to increase the conﬁdence in the mislabeled instance by pushing it away from the decision boundary, and decrease the conﬁdence in the correctly labeled instances by pushing them closer to the decision boundary.
Our attack, however, can be accomplished without harming the model’s accuracy, and thus avoid raising suspicion about a possible attack. Even if the mislabeled dog is assigned more conﬁdence than only a single correctly predicted image, ACE will undermine the model’s uncertainty estimation performance, since based on its estimation measurements, it cannot offer its newly least-conﬁdent (and correct) instance without offering the now more-conﬁdent mislabeled dog. It will either have to exclude both from the predictions it returns, reducing its coverage and its usefulness to the user, or include both and return predictions with mistakes, increasing its risk.
ACE, unlike adversarial attacks, creates damage even with small magnitudes of perturbations, beﬁtting even an attacker with very limited resources (see Section 5). This beneﬁt is inherent in our goal, which is different from standard adversarial attacks: adversarial attacks seek to move the attacked instance across the model’s decision boundary, and thus require the magnitude of the perturbations it causes to be sufﬁciently large to allow this. Our objective is to simply move the correctly and incorrectly predicted instances, relative to one another within the boundaries, which requires a less powerful “nudge”. Figure 2 exempliﬁes such an attack on EfﬁcientNet, showing how the model would be more conﬁdent of binoculars being a tank than a correctly-labeled tank. Figure 6 shows the
Risk-Coverage (RC) curve (see Section 3 for an explanation of RC curves) for EfﬁcientNet under our attack, and that for the strongest attack tested (in terms of perturbation magnitude, which is still a very small amount), approximately 20% of the model’s most conﬁdent predictions were wrong.
Uncertainty attacks are in general conceptually different than adversarial attacks. Consider a scenario of a debt landing company using machine learning to decide whether to approve or disapprove loans to its clients. An interesting example is: lendbuzz.com. Such a company operates by granting loans to highly probable non-defaulting applicants achieving the highest conﬁdence according to their predictive model. The choice of rejection threshold must depend on uncertainty estimation (in accordance with the company’s risk tolerance). A malicious attacker might attempt to inﬂuence this company to grant loans to the maximal amount of defaulting applicants to increase its odds to go 2
Figure 2: Demonstration of attacking EfﬁcientNet with softmax as the uncertainty estimation method.
Left: Two images being predicted to be tanks. The top one is correctly labeled and the model has high conﬁdence in its label. The bottom image is of binoculars incorrectly labeled as a tank, with the model having low conﬁdence in its prediction. Right: After adding perturbations to the original images, the model has lower conﬁdence in its (correct) tank label than in the wrongly labeled binoculars. bankrupt. Aware of this risk, the company tries to defend itself in various ways. Two obvious options are: (1) Monitor their model’s accuracy. if it drops signiﬁcantly it might point to an adversarial attack.
Since normal adversarial attacks harms accuracy signiﬁcantly, it will alarm such monitors. ACE does not harm accuracy, and therefore will not. (2) Subscribe to AI ﬁrewall services like those given by professional parties such as robustintelligence.com. Since standard adversarial attacks generate instances that cross the decision boundary, they require a relatively large epsilon that is easier to detect relative to those required by ACE. Even if the standard adversarial attack bypassed detection, the crossing adversarial instance they generated may have a very high uncertainty estimate due to its proximity to the decision boundary.
While a similar attack was already considered in the context of OOD detection with Dirichlet-based uncertainty models [15], the present paper is the ﬁrst to offer an algorithm to attack only the uncertainty estimation of any model employing any of the most common uncertainty estimation techniques in both black-box and white-box settings. 2