Abstract
Convolutional neural networks (CNNs) are ubiquitous in computer vision, with a myriad of effective and efﬁcient variations. Recently, Transformers – originally introduced in natural language processing – have been increasingly adopted in computer vision. While early adopters continue to employ CNN backbones, the lat-est networks are end-to-end CNN-free Transformer solutions. A recent surprising
ﬁnding shows that a simple MLP based solution without any traditional convo-lutional or Transformer components can produce effective visual representations.
While CNNs, Transformers and MLP-Mixers may be considered as completely disparate architectures, we provide a uniﬁed view showing that they are in fact special cases of a more general method to aggregate spatial context in a neural network stack. We present the CONTAINER (CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head context aggregation that can exploit long-range interactions a la Transformers while still exploiting the inductive bias of the local convolution operation leading to faster convergence speeds, often seen in CNNs. Our CONTAINER architecture achieves 82.7 % Top-1 accuracy on
ImageNet using 22M parameters, +2.8 improvement compared with DeiT-Small, and can converge to 79.9 % Top-1 accuracy in just 200 epochs. In contrast to
Transformer-based methods that do not scale well to downstream tasks that rely on larger input image resolutions, our efﬁcient network, named CONTAINER-LIGHT, can be employed in object detection and instance segmentation networks such as
DETR, RetinaNet and Mask-RCNN to obtain an impressive detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50 backbone with a comparable compute and parameter size. Our method also achieves promising results on self-supervised learning compared to DeiT on the DINO framework. Code is released at https://github.com/allenai/container. 1

Introduction
Convolutional neural networks (CNNs) have become the de facto standard for extracting visual representations, and have proven remarkably effective at numerous downstream tasks such as object detection [37], instance segmentation [22] and image captioning [1]. Similarly, in natural language processing, Transformers rule the roost [13, 43, 42, 4]. Their effectiveness at capturing short and long range information have led to state-of-the-art results across tasks such as question answering [45] and language understanding [58].
In computer vision, Transformers were initially employed as long range information aggregators across space (e.g., in object detection [5]) and time (e.g., in video understanding [61]), but these methods continued to use CNNs [34] to obtain raw visual representations. More recently however,
CNN-free visual backbones employing Transformer modules [54, 14] have shown impressive per-formance on image classiﬁcation benchmarks such as ImageNet [33]. The race to dethrone CNNs has now begun to expand beyond Transformers – a recent unexpected result shows that a multi-layer perceptron (MLP) exclusive network [52] can be just as effective at image classiﬁcation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
On the surface, CNNs [34, 8, 63, 23], Vision Transformers (ViTs) [14, 54] and MLP-mixers [52] are typically presented as disparate architectures. However, taking a step back and analyzing these methods reveals that their core designs are quite similar. Many of these methods adopt a cascade of neural network blocks. Each block typically consists of aggregation modules and fusion modules.
Aggregation modules share and accumulate information across a predeﬁned context window over the module inputs (e.g., the self attention operation in a Transformer encoder), while fusion modules combine position-wise features and produce module outputs (e.g., feed forward layers in ResNet).
In this paper, we show that the primary differences in many popular architectures result from variations in their aggregation modules. These differences can in fact be characterized as variants of an afﬁnity matrix within the aggregator that is used to determine information propagation between a query vector and its context. For instance, in ViTs [14, 54], this afﬁnity matrix is dynamically generated using key and query computations; but in the Xception architecture [8] (that employs depthwise convolutions), the afﬁnity matrix is static – the afﬁnity weights are the same regardless of position, and they remain the same across all input images regardless of size. And ﬁnally the MLP-Mixer [52] also uses a static afﬁnity matrix which changes across the landscape of the input.
Along this uniﬁed view, we present CONTAINER (CONText AggregatIon NEtwoRk), a general purpose building block for multi-head context aggregation. A CONTAINER block contains both static afﬁnity as well as dynamic afﬁnity based aggregation, which are combined using learnable mixing coefﬁcients. This enables the CONTAINER block to process long range information while still exploiting the inductive bias of the local convolution operation. CONTAINER blocks are easy to implement, can easily be substituted into many present day neural architectures and lead to highly performant networks whilst also converging faster and being data efﬁcient.
Our proposed CONTAINER architecture obtains 82.7 % Top-1 accuracy on ImageNet using 22M parameters, improving +2.8 points over DeiT-S [54] with a comparable number of parameters. It also converges faster, hitting DeiT-S’s accuracy of 79.9 % in just 200 epochs compared to 300.
We also propose a more efﬁcient model, named CONTAINER-LIGHT that employs only static afﬁnity matrices early on but uses the learnable mixture of static and dynamic afﬁnity matrices in the latter stages of computation. In contrast to ViTs that are inefﬁcient at processing large inputs,
CONTAINER-LIGHT can scale to downstream tasks such as detection and instance segmentation that require high resolution input images. Using a CONTAINER-LIGHT backbone and 12 epochs of training, RetinaNet [37] is able to achieve 43.8 mAP, while Mask-RCNN [22] is able to achieve 45.1 mAP on box and 41.3 mAP on instance mask prediction, improvements of +7.3, +6.9 and
+6.6 respectively, compared to a ResNet-50 backbone. The more recent DETR and its variants
SMCA-DETR and Deformable DETR [5, 19, 75] also beneﬁt from CONTAINER-LIGHT and achieve 38.9, 43.0 and 44.2 mAP, improving signiﬁcantly over their ResNet-50 backbone baselines.
CONTAINER-LIGHT is data efﬁcient. Our experiments show that it can obtain an ImageNet Top-1 accuracy of 61.8 using just 10% of training data, signiﬁcantly better than the 39.3 accuracy obtained by DeiT. CONTAINER-LIGHT also convergences faster and achieves better kNN accuracy (71.5) compared to DeiT (69.6) under DINO self-supervised training framework [6].
The CONTAINER uniﬁcation and framework enable us to easily reproduce several past models and even extend them with just a few code and parameter changes. We extend multiple past models and show improved performance – for instance, we produce a Hierarchical DeiT model, a multi-head
MLP-Mixer and add a static afﬁnity matrix to the DeiT architecture. Our code base and models will be released publicly. Finally, we analyse a CONTAINER model containing both static and dynamic afﬁnities and show the emergence of convolution-like local afﬁnities in the early layers of the network.
In summary, our contributions include: (1) A uniﬁed view of popular architectures for visual inputs – CNN, Transformer and MLP-mixer, (2) A novel network block – CONTAINER, which uses a mix of static and dynamic afﬁnity matrices via learnable parameters and the corresponding architecture with strong results in image classiﬁcation and (3) An efﬁcient and effective extension –
CONTAINER-LIGHT with strong results in detection and segmentation. Importantly, we see that a number of concurrent works are aiming to fuse the CNN and Transformer architectures [36, 64, 40, 24, 55, 69, 64, 47], validating our approach. We hope that our uniﬁed view helps place these different concurrent proposals in context and leads to a better understanding of the landscape of these methods. 2
2