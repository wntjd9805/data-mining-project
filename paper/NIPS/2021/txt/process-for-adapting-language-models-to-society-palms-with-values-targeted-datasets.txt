Abstract
Language models can generate harmful and biased outputs and exhibit un-desirable behavior according to a given cultural context. We propose a
Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to signiﬁcantly change model behav-ior by crafting and ﬁne-tuning on a dataset that reﬂects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs signiﬁcantly better on all metrics compared to baseline and control models for a broad range of
GPT-3 language model sizes without compromising capability integrity. We
ﬁnd that the eﬀectiveness of PALMS increases with model size. We show that signiﬁcantly adjusting language model behavior is feasible with a small, hand-curated dataset. 1

Introduction
Progress in scaling up generative language models has enabled impressive results on a wide range of tasks, leading to novel research and industry applications. As language models increase in size and impact, increasing attention is being given to the social impact and cultural context of language models across research and industry organizations. The risks and potential harms of language models are diﬃcult to identify, measure, and mitigate, es-pecially due to varied perspectives on desirable values and behavior. One potential harm is undesirable behavior for a given social context: language model outputs exhibit harm-ful biases[5], such as outputting discriminatory racial text. However, there is no universal standard for oﬀensive or harmful content; language model behavior interpretation changes depending on cultural factors. Therefore, a process for determining and adjusting appropri-ate model behavior should be feasible for many actors, especially those most harmed and overlooked in model development. Similarly, model behavior should be evaluated in social context and in a way that is inclusive of marginalized perspectives.[4]
Earlier analyses of harmful outputs in GPT-3 show negative race, gender[8], and religious[3] associations in generated text. [4] describe GPT systems encoding harmful bias across iden-tities, including abusive language patterns. We sought to determine if GPT-3’s performance could be improved in the U.S. English language according U.S. and international human
∗Both authors contributed equally. Work was conducted while at OpenAI. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
rights laws2 as a ﬁrst step toward understanding and mitigating these potentially harmful behaviors and aligning the model to a predetermined set of values3. The desired behavior that we focus on in this paper is not intended to be universally valid. Rather it serves as a template and illustration of how to adjust behavior and minimize harm in a given social context’s ethical standard.
In order to produce coherent text, language models are usually trained on massive datasets, which often includes large sets of books, wide internet scrapes, or other easily accessible large text datasets[8]. Given how desirable behavior for a language model may diﬀer by application, training a large language model from scratch for each application’s desirable behavior is not scalable. It is also diﬃcult to source the large-sized dataset needed to train an entire model while ensuring that dataset echoes desirable behavior.
In this paper we present an alternative approach: adjust the behavior of a pretrained lan-guage model to be sensitive to predeﬁned norms with our Process for Adapting Language
Models to Society (PALMS) with Values-Targeted Datasets. We demonstrate that it is possible to modify a language model’s behavior in a speciﬁed direction with surprisingly few samples. We refer to the models ﬁne-tuned using PALMS as values-targeted models and the dataset used to train that model as the values-targeted dataset. The baseline pretrained models are referred to as the base models and models ﬁne-tuned on our control dataset are control models. PALMS provides steps to construct a values-targeted dataset that reﬂects a speciﬁc set of values. When the values-targeted dataset is used to ﬁne-tune a language model, the resulting values-targeted models perform signiﬁcantly better than base and con-trol models on two quantitative metrics, toxicity scoring and human evaluations, and one qualitative metric, co-occurrence evaluations. The human evaluations involve humans rat-ing how well model output conforms to our predetermined set of values. Toxicity scoring uses the Perspective API and the same model outputs that were given to human evaluators.
The co-occurrence evaluations analyze the most common word associated with a given social category and make qualitative comparisons between the models. PALMS is iterative, and training dataset examples can be added each cycle depending on validation set performance.
The values-targeted model also maintains the same capabilities as the base model within a small margin. We tested GPT-3 models across sizes, from 125 million parameters to 175 billion parameters, and found that PALMS has the most impact on behavior in the largest models. 2