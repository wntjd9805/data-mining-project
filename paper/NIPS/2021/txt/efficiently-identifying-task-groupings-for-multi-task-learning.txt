Abstract
Multi-task learning can leverage information learned by one task to beneﬁt the training of other tasks. Despite this capacity, naïvely training all tasks together in one model often degrades performance, and exhaustively searching through combinations of task groupings can be prohibitively expensive. As a result, efﬁ-ciently identifying the tasks that would beneﬁt from training together remains a challenging design question without a clear solution. In this paper, we suggest an approach to select which tasks should train together in multi-task learning models.
Our method determines task groupings in a single run by training all tasks together and quantifying the effect to which one task’s gradient would affect another task’s loss. On the large-scale Taskonomy computer vision dataset, we ﬁnd this method can decrease test loss by 10.0% compared to simply training all tasks together while operating 11.6 times faster than a state-of-the-art task grouping method. 1

Introduction
Many of the forefront challenges in applied machine learning demand that a single model performs well on multiple tasks, or optimizes multiple objectives while simultaneously adhering to unmovable inference-time constraints. For instance, autonomous vehicles necessitate low inference time latency to make multiple predictions on a real-time video feed to precipitate a driving action [27]. Robotic arms are asked to concurrently learn how to pick, place, cover, align, and rearrange various objects to improve learning efﬁciency [25], and online movie recommendation systems model multiple engagement metrics to facilitate low-latency personalized recommendations [13]. Each of the above applications depends on multi-task learning, and advances which improve multi-task learning performance have the potential to make an outsized impact on these and many other domains.
Multi-task learning can improve modeling performance by introducing an inductive bias to prefer hypothesis classes that explain multiple objectives and by focusing attention on relevant features [43].
However, it may also lead to severely degraded performance when tasks compete for model capacity or are unable to build a shared representation that can generalize to all objectives. Accordingly,
ﬁnding groups of tasks that derive beneﬁt from the positives of training together while mitigating the negatives often improves the modeling performance of multi-task learning systems.
While recent work has developed new multi-task learning optimization schemes [28, 10, 45, 53, 11, 50], the problem of deciding which tasks should be trained together in the ﬁrst place is an understudied and complex issue that is often left to human experts [56]. However, a human’s understanding of similarity is motivated by their intuition and experience rather than a prescient knowledge of the underlying structures learned by a neural network. To further complicate matters, the beneﬁt or detriment induced from multi-task learning relies on many non-trivial decisions including, but not limited to, dataset characteristics, model architecture, hyperparameters, capacity, and convergence [51, 49, 46, 47]. As a result, a systematic technique to determine which tasks 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of our suggested approach to efﬁciently determine task groupings. (I): Train all tasks together in a multi-task learning model. (II): Compute inter-task afﬁnity scores during training. (III): Select multi-task networks that maximize the inter-task afﬁnity score onto each serving-time task. (IV): Train the resulting networks and deploy to inference. should train together in a multi-task neural network would be valuable to practitioners and researchers alike [5, 6].
One approach to select task groupings is to exhaustively search over the 2|T | 1 multi-task networks1
. However, the cost associated with this search can be prohibitive, especially when for a set of tasks there is a large number of tasks. It is further complicated by the fact that the set of tasks to which a model is applied may change throughout its lifetime. As tasks are added to or dropped from the set of all tasks, this costly analysis would need to be repeated to determine new groupings. Moreover, as model scale and complexity continues to increase, even approximate task grouping algorithms which evaluate only a subset of combinations may become prohibitively costly and time-consuming to evaluate.
−
T
In this paper, we aim to develop an efﬁcient framework to select task groupings without sacriﬁcing performance. We propose to measure inter-task afﬁnity by training all tasks together in a single multi-task network and quantifying the effect to which one task’s gradient update would affect another task’s loss. This per-step quantity is averaged across training, and tasks are then grouped together to maximize the afﬁnity onto each task. A visual depiction of the method is shown in Figure 1. Our suggested approach makes no assumptions regarding model architecture and is applicable to any paradigm in which shared parameters are updated with respect to multiple losses.
In summary, our primary contribution is to suggest a measure of inter-task afﬁnity that can be used to systematically and efﬁciently determine task groupings for multi-task learning. Our theoretical analysis shows that grouping tasks by maximizing inter-task afﬁnity will outperform any other task grouping in the convex setting under mild conditions. Further on two challenging multi-task image benchmarks, our empirical analysis ﬁnds this approach outperforms training all tasks independently, training all tasks together (with and without training augmentations), and is competitive with a state-of-the-art task grouping method while decreasing runtime by more than an order of magnitude. 2