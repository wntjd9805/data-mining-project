Abstract
Poisoning attacks have emerged as a signiﬁcant security threat to machine learning algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the perfor-mance of the output model. Some of the stronger poisoning attacks require the full knowledge of the training data. This leaves open the possibility of achieving the same attack results using poisoning attacks that do not have the full knowledge of the clean training set. In this work, we initiate a theoretical study of the problem above. Speciﬁcally, for the case of feature selection with LASSO, we show that full information adversaries (that craft poisoning examples based on the rest of the training data) are provably stronger than the optimal attacker that is oblivious to the training set yet has access to the distribution of the data. Our separation result shows that the two setting of data-aware and data-oblivious are fundamentally dif-ferent and we cannot hope to always achieve the same attack or defense results in these scenarios. 1

Introduction
Traditional approaches to supervised machine learning focus on a benign setting where honestly sampled training data is given to a learner. However, the broad use of these learning algorithms in safety-critical applications makes them targets for sophisticated attackers. Consequently, ma-chine learning has gone through a revolution of studying the same problem, but this time under so-called adversarial settings. Researchers have investigated several types of attacks, including test-time (a.k.a., evasion attacks to ﬁnd adversarial examples) [62, 6, 32, 55], training-time attacks (a.k.a., poisoning or causative attacks) [3, 8, 51], backdoor attacks [67, 33], membership inference attacks
[57], etc. In response, other works have put forth several defenses [52, 43, 9] followed by adaptive attacks [15, 2, 65] that circumvent some of the proposed defenses. Thus, developing approaches that are based on solid theoretical foundations (that prevent further adaptive attacks) has stood out as an important area of investigation.
S
Poisoning Attacks. In a poisoning attack, an adversary changes a training set of examples into a
′ (The difference is usually measured by Hamming distance; i.e., the number
“close” training set of examples injected and/or removed.). Through these changes, the goal of the adversary, generally speaking, is to degrade the “quality” of the learned model, where quality here could be interpreted in different ways. In a recent industrial survey [39], poisoning attacks were identiﬁed as the most important threat model against applications of machine learning. The main reason behind the im-portance of poisoning attacks are the feasibility of performing the attack for adversary. As the
S 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
data is usually gathered from multiple sources, the adversary can perform the poisoning attacks by corrupting one of the sources. Hence, it is extremely important to fundamentally understand this threat model. In particular, we need to investigate the role of design choices that are made in both poisoning attacks and defenses.
Does the attacker know the training data? The role of knowledge of the clean training set is one of the less investigated aspects of poisoning attacks. Many previous work on theoretical analysis of poisoning attacks implicitly, or explicitly, assume that the adversary has full knowledge of the training data
[38, 58, 44, 61]. In several natural scenarios, an adversary might not have access to the training data before deciding on how to tamper with it. This has led researchers to study poisoning attacks that do not use the knowledge of the training set to craft the poison points. In this work, we explore the following question: before choosing what examples to add or delete from
S
S
What is the role of the knowledge of training set in the success of poisoning adver-saries? Can the knowledge of training set help the attacks? Or alternatively, can hiding the training set from adversaries help the defenses? 1
In this work, as a ﬁrst step to understand this question, we show a separation result between data-oblivious and data-aware poisoning adversaries. In particular, we show that there exist a learning setting (Feature selection with LASSO on Gaussian data) where poisoning adversaries that know the distribution of data but are oblivious to speciﬁc training samples that are used to train the model are provably weaker than the adversaries with the knowledge of both training set and the distribution.
To the best of our knowledge, this is the ﬁrst separation result for poisoning attacks.
Implications of our separation result: Here, we mention some implications of our separation result.
• Separation of threat models: The ﬁrst implication of our result is the separation of data-oblivious and data-aware poisoning threat models. Our result shows that data-oblivious attacks are strictly weaker than data-aware attacks. In other words, it shows that we cannot expect the defenses to have the same effectiveness in both scenarios. This makes the knowl-edge of data a very important design choice that should be clearly stated when designing defenses or attacks.
• Possibility of designing new defenses: Although data-oblivious poisoning is a weaker at-tack model, it might still be the right threat model for many applications. For instance, if data providers use cryptographically secure multi-party protocols to train the model [68], then each participant can only observe their own data. Note that each party might still have access to some data pool from the true distribution of training set and that still ﬁts in our data-oblivious threat model. In these scenarios, it is natural to use defenses that are only se-cure against data-oblivious attacks. Our results shows the possibility of designing defense mechanisms that leverage the secrecy of training data and can provide much stronger secu-rity guarantees in this threat mode. In particular, our result shows the provable robustness of LASSO algorithm in defending against data-oblivious attacks.
Note that this approach is distinct from the demoted notion of “security through obscurity” as the attacker knows every detail of the algorithm as well as the data distribution. The only unknown to the adversary is the randomness involved in the process of sampling training examples from the training distribution. This is exactly similar to how secret randomness helps security in cryptography.
• A new motive for privacy: privacy is often viewed as a utility for data owners in the ma-chine learning pipeline. Due to the trade-offs between privacy and the efﬁciency/utility, data-users often ignore the privacy of data owners while doing their analysis, especially when there is no incentive to enforce the privacy of the learning protocol. The possi-bility of improving the security against poisoning attacks by enforcing the (partial) data-obliviousness of the adversary could create a new incentive for keeping training datasets secret. Speciﬁcally, the users of data would now have more motivation to try to keep train-ing dataset private, with the goal of securing their models against poisoning and increasing their utility in scenarios where part of data is coming from potentially malicious sources. 1This question was independently asked as an open question in the survey of Goldblum et al. [31]. 2
1.1 Our Contributions
In this work, we provide theoretical evidence that obliviousness of attackers to the training data can indeed help robustness against poisoning attacks. In particular, we provide a provable difference between: (i) an adversary that is aware of the training data as well as the distribution of training data, before launching the attack (data-aware adversary) and (ii) an adversary that only knows the distribution of training data and does not know the speciﬁc clean examples in the training set (data-oblivious adversary).
We start by formalizing what it means mathematically for the poisoning adversary to be data-oblivious or data-aware.
Separations for feature selection with Lasso. We then prove a separation theorem between the data-aware and data-oblivious poisoning threat models in the context of feature selection. We study data-aware and data-oblivious attackers against the Lasso estimator and show that if certain natural properties holds for the distribution of dataset, the power of optimal data-aware and data-oblivious poisoning adversaries differ signiﬁcantly.
We emphasize that in our data-oblivious setting, the adversary fully knows the data distribution, and hence it implicitly has access to a lot of auxiliary information about the data set, yet the very fact that it does not know the actual sampled dataset makes it harder for adversary to achieve its goal.
Experiments. To further investigate the power of data-oblivious and data-aware attacks in the con-text of feature selection, we experiment on synthetic datasets sampled from Gaussian distributions, as suggested in our theoretical results. Our experiments conﬁrm our theoretical ﬁndings by showing that the power of data-oblivious and poisoning attacks differ signiﬁcantly. Furthermore, we exper-imentally evaluate the power of partially-aware attackers who only know part of the data. These experiments show the gradual improvement of the attack as the knowledge of data grows.
In our experimental studies we go beyond Gaussian setting and show that the the power of data-oblivious attacks could be signiﬁcantly lower on real world distributions as well. In our experiments, sometimes (depending on the noise nature of the dataset), even an attacker that knows 20% of the dataset cannot have much of improvement over an oblivious attacker.
Separation for classiﬁcation. In addition to our main results in the context of feature selection, in this work, we also take initial steps to study the role of adversary’s knowledge (about the data set) when the goal of the attacker is to increase the risk of the produced model in the context of classiﬁcation. These results are presented supplemental material (Section A). 1.2