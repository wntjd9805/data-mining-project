Abstract
Most existing animal pose and shape estimation approaches reconstruct animal meshes with a parametric SMAL model. This is because the low-dimensional pose and shape parameters of the SMAL model makes it easier for deep networks to learn the high-dimensional animal meshes. However, the SMAL model is learned from scans of toy animals with limited pose and shape variations, and thus may not be able to represent highly varying real animals well. This may result in poor fittings of the estimated meshes to the 2D evidences, e.g. 2D keypoints or silhouettes. To mitigate this problem, we propose a coarse-to-fine approach to reconstruct 3D animal mesh from a single image. The coarse estimation stage first estimates the pose, shape and translation parameters of the SMAL model. The estimated meshes are then used as a starting point by a graph convolutional network (GCN) to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations benefits from both parametric and non-parametric representations. We design our mesh refinement GCN (MRGCN) as an encoder-decoder structure with hierarchical feature representations to overcome the limited receptive field of traditional GCNs. Moreover, we observe that the global image feature used by existing animal mesh reconstruction works is unable to capture detailed shape information for mesh refinement. We thus introduce a local feature extractor to retrieve a vertex-level feature and use it together with the global feature as the input of the MRGCN. We test our approach on the StanfordExtra dataset and achieve state-of-the-art results. Furthermore, we test the generalization capacity of our approach on the Animal Pose and BADJA datasets. Our code is available at the project website1. 1

Introduction
Animals play an important role in our everyday life and the study of animals has many potential applications in zoology, farming and ecology. Although great success has been achieved for modeling humans, progress for the animals counterpart is relatively slow mainly due to the lack of labeled data. It is almost impossible to collect large scale 3D pose data for animals, which prevents the direct application of many existing human pose estimation techniques to animals. Some works solve this problem by training their model with synthetic data [4, 13, 33]. However, generating realistic images is challenging, and more importantly, the limited pose and shape variations of synthetic data may result in performance drop when applied to real images. Other works avoid the requirement of 3D ground truth by making use of 2D weak supervision. For example, [2, 12] render the estimated mesh or project the 3D keypoints onto the 2D image space, and then compute the losses based on ground truth silhouettes or 2D keypoints. This setting is more pragmatic since 2D animal annotations are more accessible. In view of this, we focus on learning 3D animal pose and shape with only 2D weak supervision in this work. 1https://github.com/chaneyddtt/Coarse-to-fine-3D-Animal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Some results of our coarse-to-fine approach. The first column is the input RGB images. The second to fourth columns are the estimated meshes in the camera view, the projected 2D keypoints and silhouettes from the coarse estimations. The fifth column to seventh columns are the estimated meshes, the projected 2D keypoints and silhouettes from the refined estimations. Note that The red and green regions in the fourth and seventh columns represent the ground truth and rendered silhouettes, respectively, and the yellow region is the overlap between them.
Reconstruction of the 3D mesh from a single RGB image with only 2D supervision is challenging because of the articulated structure of animals. Most existing works [4, 2] rely on the parametric statistical body shape model SMAL [35] to reconstruct animal poses and shapes. Specifically, the low dimensional parameterization of the SMAL model makes it easier for deep learning to learn the high dimensional 3D mesh with 3,889 vertices for high-quality reconstruction. However, the shape space of the SMAL model is learned from 41 scans of toy animals and thus lacks pose and shape variations.
This limits the representation capacity of the model and results in poor fittings of the estimated shapes to the 2D observations when applied to in-the-wild animal images, as illustrated in Figure 1. Both 2D keypoints (the left front leg in the first example and right ear in the second example) and silhouettes rendered from the coarse estimations do not match well with the ground truth annotations (rendered and ground truth silhouettes are denoted with green and red regions, and the yellow region is the overlap between them). To mitigate this problem, we design a two-stage approach to conduct a coarse-to-fine reconstruction. In the coarse estimation stage, we regress the shape and pose parameters of the SMAL model from an RGB image and recover a coarse mesh from these parameters. The coarse mesh is then used as a starting point by a MRGCN to predict a per-vertex deformation in the refinement stage. This combination of SMAL-based and vertex-based representations enable our network to benefit from both parametric and non-parametric representations. Some refined examples are shown in Figure 1, where both keypoint locations and overall shapes are improved.
To facilitate the non-parametric representation in the refinement stage, we consider a mesh as a graph structure and use a GCN [15] to encode the mesh topology (i.e., the edge and face information).
Existing animal mesh reconstruction estimation works [2, 12, 33] rely on a global image feature to recover 3D meshes. However, we observe that the global feature fails to capture local geometry information that is critical to recover shape details. To solve this problem, we introduce a local feature extractor to retrieve a per-vertex feature and use it together with the global feature as the input of each node in the MRGCN. The combination of the vertex-level local feature and the image-level global feature helps to capture both overall structure and detailed shape information. Moreover, the representation power of traditional GCNs is limited by the small receptive field since each node is updated by aggregating features of neighboring nodes. To mitigate this, we design our MRGCN as an encoder-decode structure. Specifically, the downsampling and upsampling operations in the encoder-decoder structure enables a hierarchical representation of the mesh data, where features of different mesh resolutions can be exploited for mesh refinement.
Our main contributions can be summarized as follows: 1) We propose a coarse-to-fine approach that combines parametric and non-parametric representations for animal pose and shape estimation. 2)
We design an encoder-decoder structured GCN for mesh refinement, where both image-level global feature and vertex-level local feature are combined to capture overall structure and detailed shape information. 3) We achieve state-of-the-art results for animal pose and shape estimation on three animal datasets. 2
2