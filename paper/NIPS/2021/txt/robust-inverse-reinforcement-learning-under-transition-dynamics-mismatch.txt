Abstract
We study the inverse reinforcement learning (IRL) problem under a transition dynamics mismatch between the expert and the learner. Speciﬁcally, we consider the Maximum Causal Entropy (MCE) IRL learner model and provide a tight upper bound on the learner’s performance degradation based on the (cid:96)1-distance between the transition dynamics of the expert and the learner. Leveraging insights from the Robust RL literature, we propose a robust MCE IRL algorithm, which is a principled approach to help with this mismatch. Finally, we empirically demonstrate the stable performance of our algorithm compared to the standard
MCE IRL algorithm under transition dynamics mismatches in both ﬁnite and continuous MDP problems. 1

Introduction
Recent advances in Reinforcement Learning (RL) [1, 2, 3, 4] have demonstrated impressive perfor-mance in games [5, 6], continuous control [7], and robotics [8]. Despite these successes, a broader application of RL in real-world domains is hindered by the difﬁculty of designing a proper reward function. Inverse Reinforcement Learning (IRL) addresses this issue by inferring a reward function from a given set of demonstrations of the desired behavior [9, 10]. IRL has been extensively studied, and many algorithms have already been proposed [11, 12, 13, 14, 15, 16].
Almost all IRL algorithms assume that the expert demonstrations are collected from the same environment as the one in which the IRL agent is trained. However, this assumption rarely holds in real world because of many possible factors identiﬁed by [17]. For example, consider an autonomous car that should learn by observing expert demonstrations performed on another car with possibly different technical characteristics. There is often a mismatch between the learner and the expert’s transition dynamics, resulting in poor performance that are critical in healthcare [18] or autonomous driving [19]. Indeed, the performance degradation of an IRL agent due to transition dynamics mismatch has been noted empirically [20, 21, 22, 23], but without theoretical guidance.
To this end, our work ﬁrst provides a theoretical study on the effect of such mismatch in the context of the inﬁnite horizon Maximum Causal Entropy (MCE) IRL framework [24, 25, 26]. Speciﬁcally, we bound the potential decrease in the IRL learner’s performance as a function of the (cid:96)1-distance between the expert and the learner’s transition dynamics. We then propose a robust variant of the
MCE IRL algorithm to effectively recover a reward function under transition dynamics mismatch, mitigating degradation. There is precedence to our robust IRL approach, such as [27] that employs
∗Correspondence to: Parameswaran Kamalaruban <kparameswaran@turing.ac.uk> 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
an adversarial training method to learn a robust policy against adversarial changes in the learner’s environment. The novel idea of our work is to incorporate this method within our IRL context, by viewing the expert’s transition dynamics as a perturbed version of the learner’s one.
Our robust MCE IRL algorithm leverages techniques from the robust RL literature [28, 29, 30, 27]. A few recent works [20, 21, 31] attempt to infer the expert’s transition dynamics from the demonstration set or via additional information, and then apply the standard IRL method to recover the reward function based on the learned dynamics. Still, the transition dynamics can be estimated only up to a certain accuracy, i.e., a mismatch between the learner’s belief and the dynamics of the expert’s environment remains. Our robust IRL approach can be incorporated into this research vein to further improve the IRL agent’s performance.
To our knowledge, this is the ﬁrst work that rigorously reconciles model-mismatch in IRL with only one shot access to the expert environment. We highlight the following contributions: 1. We provide a tight upper bound for the suboptimality of an IRL learner that receives expert demonstrations from an MDP with different transition dynamics compared to a learner that receives demonstrations from an MDP with the same transition dynamics (Section 3.1). 2. We ﬁnd suitable conditions under which a solution exists to the MCE IRL optimization problem with model mismatch (Section 3.2). 3. We propose a robust variant of the MCE IRL algorithm to learn a policy from expert demonstrations under transition dynamics mismatch (Section 4). 4. We demonstrate our method’s robust performance compared to the standard MCE IRL in a broad set of experiments under both linear and non-linear reward settings (Section 5). 5. We extend our robust IRL method to the high dimensional continuous MDP setting with appropriate practical relaxations, and empirically demonstrate its effectiveness (Section 6). 2 Problem Setup
This section formalizes the IRL problem with an emphasis on the learner and expert environments.
We use bold notation to represent vectors. A glossary of notation is given in Appendix C. 2.1 Environment and Reward the environment by a Markov decision process (MDP) Mθ
:=
We formally represent
{S, A, T, γ, P0, Rθ}, parameterized by θ ∈ Rd. The state and action spaces are denoted as S and A, respectively. We assume that |S| , |A| < ∞. T : S × S × A → [0, 1] represents the transition dynamics, i.e., T (s(cid:48)|s, a) is the probability of transitioning to state s(cid:48) by taking action a from state s.
The discount factor is given by γ ∈ (0, 1), and P0 is the initial state distribution. We consider a linear reward function Rθ : S → R of the form Rθ(s) = (cid:104)θ, φ(s)(cid:105), where θ ∈ Rd is the reward parameter, and φ : S → Rd is a feature map. We use a one-hot feature map φ : S → {0, 1}|S|, where the sth element of φ (s) is 1 and 0 elsewhere. Our results can be extended to any general feature map (see empirical evidence in Fig. 6), but we use this particular choice as a running example for concreteness.
We focus on the state-only reward function since the state-action reward function is not that useful in the robustness context. Indeed, as [22] pointed out, the actions to achieve a speciﬁc goal under different transition dynamics will not necessarily be the same and, consequently, should not be imitated. Analogously, in the IRL context, the reward for taking a particular action should not be recovered since the quality of that action depends on the transition dynamics. We denote an MDP without a reward function by M = Mθ\Rθ = {S, A, T, γ, P0}. 2.2 Policy and Performance
:
A policy π tion over actions.
{π : (cid:80) performance measures of any policy π acting in the MDP Mθ: return V π
Mθ
E [(cid:80)∞
S → ∆A is a mapping from a state to a probability distribu-The set of all valid stochastic policies is denoted by Π := a π(a|s) = 1, ∀s ∈ S; π(a|s) ≥ 0, ∀(s, a) ∈ S × A}. We are interested in two different (i) the expected discounted
:= t=0 γt {Rθ (st) − log π (at|st)} | π, M ]. The state occupancy measure of a policy π in the t=0 γtRθ (st) | π, M ], and (ii) its entropy regularized variant V π,soft
:= E [(cid:80)∞
Mθ 2
M (s) := (1 − γ) (cid:80)∞
MDP M is deﬁned as ρπ the probability of visiting the state s after t steps by following the policy π in M . Note that ρπ
M ∈ R|S| be a vector whose sth element is ρπ does not depend on the reward function. Let ρπ
M (s)Rθ(s) = 1
= 1 the one-hot feature map φ, we have that V π 1−γ (cid:104)θ, ρπ
Mθ 1−γ is optimal for the MDP Mθ if π ∈ arg maxπ(cid:48) V π(cid:48)
, in which case we denote it by π∗
Mθ the soft-optimal policy (always unique [32]) in Mθ is deﬁned as πsoft
Mθ
Appendix D for a parametric form of this policy). t=0 γtP [st = s | π, M ], where P [st = s | π, M ] denotes
M (s)
M (s). For
M (cid:105). A policy π
. Similarly,
:= arg maxπ(cid:48) V π(cid:48),soft s ρπ (see (cid:80)
Mθ
Mθ 2.3 Learner and Expert
π∗
π∗ (cid:18) (cid:19) (cid:9)
=
M L
θ∗
M L
M L
M E (cid:9),
ρ = ρ
ρ = ρ
Expert
Learner
M E
θ∗
M E
M L
θ∗
M L
θ∗ : π∗
θ∗ : π∗
θ∗ \Rθ∗
θL, πsoft
M L
θL (cid:8)S, A, T L, γ, P0, Rθ
θ = (cid:8)S, A, T E, γ, P0, Rθ
Our setting has two entities: a learner implementing the MCE IRL algorithm, and an expert. We consider two MDPs,
M L and
θ
M E that differ only in the transition dynamics. The true reward parameter θ = θ∗ is known only to the expert.
The expert provides demonstrations to the learner: (i) by following policy π∗ in M E when there is a transition dynamics mismatch between the learner and the expert, or (ii) by following policy π∗ in M L otherwise. The learner always operates in the MDP M L and is not aware of the true reward parameter and of the expert dynamics T E 2, i.e., it only has access to M L
θ∗ \Rθ∗. It learns a reward parameter θ and the corresponding soft-optimal policy πsoft
, based on the state occupancy measure ρ received from the
M L
θ
π∗
M L
θ∗
M L
π∗
M E
θ∗
M E or ρ expert. Here, ρ is either ρ stochastic estimate of ρ using concentration inequalities [11].
Figure 1: An illustration of the IRL problem under transition dynamics mismatch: See Section 2. depending on the case. Our results can be extended to the
θE, πsoft
M L
θE
M E
θ∗
M L
θ∗
M E
θ∗ (cid:19) (cid:18)
Our learner model builds on the MCE IRL [24, 25, 26] framework that matches the expert’s state occupancy measure ρ. In particular, the learner policy is obtained by maximizing its causal entropy while matching the expert’s state occupancy:
E max
π∈Π (cid:34) ∞ (cid:88) t=0
−γt log π(at|st) (cid:12) (cid:12) (cid:12) (cid:12) (cid:35)
π, M L subject to ρπ
M L = ρ. (1)
Note that this optimization problem only requires access to M L
θ \Rθ. The constraint in (1) follows from our choice of the one-hot feature map. We denote the optimal solution of the above problem by
πsoft
M L
θ with a corresponding reward parameter: (i) θ = θE, when we use ρ
π∗
M E
θ∗
M E as ρ, or (ii) θ = θL, when we use ρ dual problems of (1). Finally, we are interested in the performance of the learner policy πsoft
M L
θ
MDP M L as ρ. Here, the parameters θE and θL are obtained by solving the corresponding in the
θ∗. Our problem setup is illustrated in Figure 1.
π∗
M L
θ∗
M L 3 MCE IRL under Transition Dynamics Mismatch
This section analyses the MCE IRL learner’s suboptimality when there is a transition dynamics mismatch between the expert and the learner, as opposed to an ideal learner without this mismatch.
The proofs of the theoretical statements of this section can be found in Appendix E. 3.1 Upper bound on the Performance Gap
First, we introduce an auxiliary lemma to be used later in our analysis. We deﬁne the dis-tance between the two transition dynamics T and T (cid:48), and the distance between the two poli-2The setting with T E known to the learner has been studied under the name of imitation learning across embodiments [33]. 3
cies π and π(cid:48) as follows, respectively: ddyn (T, T (cid:48)) := maxs,a (cid:107)T (· | s, a) − T (cid:48) (· | s, a)(cid:107)1, and dpol (π, π(cid:48)) := maxs (cid:107)π(·|s) − π(cid:48)(·|s)(cid:107)1. Consider the two MDPs Mθ = {S, A, T, γ, P0, Rθ}
θ = {S, A, T (cid:48), γ, P0, Rθ}. We assume that the reward function is bounded, and M (cid:48) i.e.,
Rθ (s) ∈ (cid:2)Rmin (cid:3) , ∀s ∈ S. Also, we deﬁne the following two constants: κθ
:= (cid:113) (cid:9) and |Rθ|max := max (cid:8)(cid:12)
γ · max (cid:8)Rmax be the soft optimal policies for the MDPs Mθ and
θ respectively. Then, the distance between π and π(cid:48) is bounded as follows: dpol (π(cid:48), π) ≤
θ
θ + log |A| , − log |A| − Rmin
Lemma 1. Let π := πsoft
Mθ
M (cid:48)
θ and π(cid:48) := πsoft
M (cid:48)
θ (cid:12) (cid:12) , |Rmax
θ
, Rmax
θ (cid:12)Rmin
θ
|(cid:9).
√ (cid:26) κθ 2 min ddyn(T (cid:48),T ) (1−γ)
,
θ ddyn(T (cid:48),T )
κ2 (1−γ)2 (cid:27)
.
The above result is obtained by bounding the KL divergence between the two soft optimal policies, and involves a non-standard derivation compared to the well-established performance difference theorems in the literature (see Appendix E.1). The lemma above bounds the maximum total variation distance between two soft optimal policies obtained by optimizing the same reward under different transition dynamics. It serves as a prerequisite result for our later theorems (Theorem 1 for soft optimal experts and Theorem 6). In addition, it may be a result of independent interest for entropy regularized MDP.
Now, we turn to our objective. Let π1 := πsoft
M L
θL when there is no transition dynamics mismatch. Similarly, let π2 := πsoft be the policy returned
M L
θE by the MCE IRL algorithm when there is a mismatch. Note that π1 and π2 are the corresponding be the policy returned by the MCE IRL algorithm
π∗
M E
θ∗ solutions to the optimization problem (1), when ρ ← ρ
M E , respectively. The following theorem bounds the performance degradation of the policy π2 compared to the policy π1 in the MDP M L
Theorem 1. The performance gap between the policies π1 and π2 on the MDP M L follows:
θ∗ , where the learner operates on:
θ∗ is bounded as (cid:0)T L, T E(cid:1). and ρ ← ρ
M L
θ∗
M L
π∗
· ddyn (cid:12) (cid:12) ≤ γ·|Rθ∗ |max (cid:12) (1−γ)2 (cid:12) (cid:12)V π1 (cid:12)
M L
θ∗
− V π2
M L
θ∗
The above result is obtained from the optimality conditions of the problem (1), and using Theorem 7 from [34]. In Section 4.4, we show that the above bound is indeed tight. When the expert policy is soft-optimal, we can use Lemma 1 and Simulation Lemma [35, 36] to obtain an upper bound on the performance gap (see Appendix E.2). For an application of Theorem 1, consider an IRL learner that ﬁrst learns a simulator of the expert environment, and then matches the expert behavior in the simulator. In this case, our upper bound provides an estimate (sufﬁcient condition) of the accuracy required for the simulator. 3.2 Existence of Solution under Mismatch
The proof of the existence of a unique solution to the optimization problem (1), presented in [37], relies on the fact that both expert and learner environments are the same. This assumption implies that the expert policy is in the feasible set that is consequently non-empty. Theorem 2 presented in this section poses a condition under which we can ensure that the feasible set is non-empty when the expert and learner environments are not the same.
Given M L and ρ, we deﬁne the following quantities useful for stating our theorem. We deﬁne, for each state s ∈ S, the probability ﬂow matrix F (s) ∈ R|S|×|A| as follows: [F (s)]i,j si,s,aj ,
:= T L(si|s, aj) for i = 1, . . . , |S| and j = 1, . . . , |A|. Let B(s) ∈ R|S|×|A| where T L be a row matrix that contains only ones in row s and zero elsewhere. Then, we deﬁne the matrix T ∈ R2|S|×|S||A| by stacking the probability ﬂow and the row matrices as follows:
:= ρ(s)T L si,s,aj
T := (cid:20)F (s1) F (s2)
B(s1) B(s2)
. . . F (s|S|)
. . . B(s|S|) (cid:21)
. In addition, we deﬁne the vector v ∈ R2|S| as fol-lows: vi = ρ(si) − (1 − γ)P0(si) if i ≤ |S|, and 1 otherwise.
Theorem 2. The feasible set of the optimization problem (1) is non-empty iff the rank of the matrix
T is equal to the rank of the augmented matrix (T |v).
The proof of the above theorem leverages the fact that the Bellman ﬂow constraints [15] must hold for any policy in an MDP. This requirement leads to the formulation of a linear system whose solutions 4
set corresponds to the feasible set of (1). The Rouché-Capelli theorem [38][Theorem 2.38] states that the solutions set is non-empty if and only if the condition in Theorem 2 holds. We note that the construction of the matrix T does not assume any restriction on the MDP structure since it leverages only on the Bellman ﬂow constraints. Theorem 2 allows us to develop a robust MCE IRL scheme in Section 4 by ensuring the absence of duality gap. To this end, the following corollary provides a simple sufﬁcient condition for the existence of a solution under transition dynamics mismatch.
Corollary 1. Let |A| > 1. Then, a sufﬁcient condition for the non-emptiness of the feasible set of the optimization problem (1) is given by T being full rank. 3.3 Reward Transfer under Mismatch
Consider a class M of MDPs such that it contains both the learner and the expert environments,
π∗
M E
θ∗
M E ; and the MDP M E are unknown. Further, we assume that every MDP i.e., M L, M E ∈ M (see Figure 2). We are given the expert’s state occupancy measure ρ = ρ but the expert’s policy π∗
M ∈ M satisﬁes the condition in Theorem 2.
We aim to ﬁnd a policy πL that performs well in the MDP M L is high. To this end, we can choose any MDP M train ∈ M, and solve the MCE IRL problem (1) with the constraint given
θ∗ , i.e., V πL
M L
θ∗
M E
θ∗
M train. Then, we always obtain a reward parameter θtrain s.t. ρ = ρ by ρ = ρπ
, since M train satisﬁes the condition in Theorem 2. We can use this reward parameter θtrain to learn a good policy
πL in the MDP M L
. Using Lemma 1, we obtain a bound on the performance gap between πL and π1 := πsoft
M L
θL (see Theorem 6 in Appendix E.4).
θtrain, i.e., πL := π∗ or πL := πsoft
M L
M train
θtrain
θtrain
θtrain
M L
πsoft
M train
However, there are two problems with this ap-proach: (i) it requires access to multiple envi-ronments M train, and (ii) unless M train hap-pened to be closer to the expert’s MDP M E, we cannot recover the true intention of the expert.
Since the MDP M E is unknown, one cannot compare the different reward parameters θtrain’s obtained with different MDPs M train’s. Thus, with θtrain, it is impossible to ensure that the performance of πL is high in the MDP M L
θ∗.
Instead, we try to learn a robust policy πL over the class M, while aligning with the expert’s occupancy measure ρ, and acting only in M L. By doing this, we ensure that πL performs reasonably well on any MDP Mθ∗ ∈ M including M L
Figure 2: Illustrative example of learning a pol-icy πL to act in one MDP M L, given the expert occupancy measure ρ.
θ∗ . We further build upon this idea in the next section. 4 Robust MCE IRL via Two-Player Markov Game 4.1 Robust MCE IRL Formulation
This section focuses on recovering a learner policy via MCE IRL framework in a robust manner, under
πsoft
M E
θ∗ transition dynamics mismatch, i.e., ρ = ρ
M E in Eq. (1). In particular, our learner policy matches the expert state occupancy measure ρ under the most adversarial transition dynamics belonging to a set described as follows for a given α > 0: T L,α := (cid:8)αT L + (1 − α) ¯T , ∀ ¯T ∈ ∆S|S,A (cid:9), where
∆S|S,A is the set of all the possible transition dynamics T : S × S × A → [0, 1]. Note that the set T L,α is equivalent to the (s, a)-rectangular uncertainty set [28] centered around T L, i.e.,
T L,α = (cid:8)T : ddyn (cid:0)T, T L(cid:1) ≤ 2(1 − α)(cid:9). We need this set T L,α for establishing the equivalence between robust MDP and action-robust MDP formulations. The action-robust MDP formulation allows us to learn a robust policy while accessing only the MDP M L. 5
We deﬁne a class of MDPs as follows: ML,α := (cid:8)(cid:8)S, A, T L,α, γ, P0 based on the discussions in Section 3.3, we propose the following robust MCE IRL problem: (cid:9) , ∀T L,α ∈ T L,α(cid:9). Then, max
πpl∈Π min
M ∈ML,α (cid:34) ∞ (cid:88)
E t=0
−γt log πpl(at|st) (cid:35)
πpl, M (cid:12) (cid:12) (cid:12) (cid:12) subject to ρπpl
M = ρ
The corresponding dual problem is given by: min
θ max
πpl∈Π min
M ∈ML,α (cid:34) ∞ (cid:88)
E t=0
−γt log πpl(at|st) (cid:35)
πpl, M (cid:12) (cid:12) (cid:12) (cid:12)
+ θ(cid:62) (cid:16) (cid:17)
ρπpl
M − ρ (2) (3)
In the dual problem, for any θ, we attempt to learn a robust policy over the class ML,α with respect to the entropy regularized reward function. The parameter θ plays the role of aligning the learner’s policy with the expert’s occupancy measure via constraint satisfaction. 4.2 Existence of Solution
We start by formulating the IRL problem for any MDP M L,α ∈ ML,α, with transition dynamics
T L,α = αT L + (1 − α) ¯T ∈ T L,α, as follows:
E max
πpl∈Π (cid:34) ∞ (cid:88) t=0
−γt log πpl(at|st) (cid:35)
πpl, M L,α (cid:12) (cid:12) (cid:12) (cid:12) subject to ρπpl
M L,α = ρ
By introducing the Lagrangian vector θ ∈ R|S|, we get: (cid:34) ∞ (cid:88)
−γt log πpl(at|st)
πpl, M L,α
E max
πpl∈Π t=0 (cid:12) (cid:12) (cid:12) (cid:12) (cid:35)
+ θ(cid:62) (cid:16) (cid:17)
ρπpl
M L,α − ρ (4) (5)
For any ﬁxed θ, the problem (5) is feasible since Π is a closed and bounded set. We deﬁne U (θ) as the value of the program (5) for a given θ. By weak duality, U (θ) provides an upper bound on the optimization problem (4). Consequently, we introduce the dual problem aiming to ﬁnd the value of θ corresponding to the lowest upper bound, which can be written as (cid:34) ∞ (cid:88) (cid:35)
+ θ(cid:62) (cid:16) (cid:17)
ρπpl
M L,α − ρ
. (6)
−γt log πpl(at|st)
πpl, M L,α min
θ
U (θ) := max
πpl∈Π
E (cid:12) (cid:12) (cid:12) (cid:12) t=0
. Due to [32][Theorem 1], for any ﬁxed M L,α
Given θ, we deﬁne πpl,∗ := πsoft
M L,α
θ exists and it is unique. We can compute the gradient3 ∇θU = ρπpl,∗
M L,α − ρ, and update the parameter via gradient descent: θ ← θ − ∇θU . Note that, if the condition in Theorem 2 holds, the feasible set of (4) is non-empty. Then, according to [37][Lemma 2], there is no duality gap between the programs (4) and (6). Based on these observations, we argue that the program (2) is well-posed and admits a unique solution.
, the policy πpl,∗
θ 4.3 Solution via Markov Game
In the following, we outline a method (see Algorithm 1) to solve the robust MCE IRL dual problem (3).
To this end, for any given θ, we need to solve the inner max-min problem of (3). First, we express the entropy term E (cid:2)(cid:80)∞ (cid:12)πpl, M (cid:3) as follows: t=0 −γt log πpl(at|st)(cid:12) (cid:88) s∈S
ρπpl
M (s) (cid:88) a∈A (cid:8)−πpl(a|s) log πpl(a|s)(cid:9) = (cid:88) s∈S
M (s)H πpl
ρπpl (A | S = s) = (cid:16)
H πpl (cid:17)(cid:62)
ρπpl
M , where H πpl
∈ R|S| a vector whose sth element is the entropy of the player policy given the state s.
Since the quantity H πpl
+ θ depends only on the states, to solve the dual problem, we can utilize the equivalence between the robust MDP [28, 29] formulation and the action-robust MDP [30, 27, 40] formulation shown in [27]. We can interpret the minimization over the environment class as the 3In Appendix F.2, we proved that this is indeed the gradient update under the transition dynamics mismatch. 6
Algorithm 1 Robust MCE IRL via Markov Game
Input: opponent strength 1 − α
Initialize: player policy πpl, opponent policy πop, and parameter θ while not converged do compute ραπpl+(1−α)πop
M L update θ with Adam [39] using the gradient use Algorithm 2 with R = Rθ to update πpl and πop s.t. they solve the problem (9). by dynamic programming [37][Section V.C].
ραπpl+(1−α)πop
M L
− ρ (cid:17) (cid:16)
. end while
Output: player policy πpl minimization over a set of opponent policies that with probability 1 − α take control of the agent and
ρπpl
M perform the worst possible move from the current agent state. Indeed, interpreting as an entropy regularized value function, i.e., θ as a reward parameter, we can write:
H πpl
+ θ (cid:17)(cid:62) (cid:16) max
πpl∈Π min
M ∈ML,α (cid:16)
H πpl
+ θ (cid:17)(cid:62)
ρπpl
M = max
πpl∈Π
≤ max
πpl∈Π
E (cid:2)G (cid:12)
E (cid:2)G (cid:12) (cid:12) πpl, P0, αT L + (1 − α) ¯T (cid:3) (cid:12) απpl + (1 − α)πop, M L(cid:3) , min
¯T min
πop∈Π (7) (8) where G := (cid:80)∞
Rθ(st) + H πpl tion in section 3.1 of [27]. Further details are in Appendix F.1. (A | S = st) t=0 γt (cid:110) (cid:111)
. The above inequality holds due to the deriva-Finally, we can formulate the problem (8) as a two-player zero-sum Markov game [41] with transition dynamics given by T two,L,α(s(cid:48)|s, apl, aop) = αT L(s(cid:48)|s, apl) + (1 − α)T L(s(cid:48)|s, aop), where apl is an action chosen according to the player policy and aop according to the opponent policy. Note that the opponent is restricted to take the worst possible action from the state of the player, i.e., there is no additional state variable for the opponent. As a result, we reach a two-player Markov game with a regularization term for the player as follows: arg max
πpl∈Π min
πop∈Π
E (cid:2)G (cid:12) (cid:12) πpl, πop, M two,L,α(cid:3) , (9) where M two,L,α = (cid:8)S, A, A, T two,L,α, γ, P0, Rθ (cid:9) is the two-player MDP associated with the above game. The repetition of the action space A denotes the fact that player and adversary share the same action space. Inspired from [42], we propose a dynamic programming approach to ﬁnd the player and opponent policies (see Algorithm 2 in Appendix F.3). 4.4 Performance Gap of Robust MCE IRL
Let πpl be the policy returned by our Algorithm 1 when there is a transition dynamics mismatch.
Recall that π1 := πsoft is the policy recovered without this mismatch. Then, we obtain the following
M L
θL upper-bound4 for the performance gap of our algorithm via the triangle inequality:
Theorem 3. The performance gap between the policies π1 and πpl on the MDP M L follows: (cid:0)T L, T E(cid:1) + 2 · (1 − α)(cid:9).
θ∗ is bounded as
· (cid:8)γ · ddyn (cid:12) (cid:12) ≤ |Rθ∗ |max (cid:12) (1−γ)2
− V πpl
M L
θ∗ (cid:12) (cid:12)V π1 (cid:12)
M L
θ∗
However, we now provide a constructive ex-ample, in which, by choosing the appropriate value for α, the performance gap of our Algo-rithm 1 vanishes. In contrast, the performance gap of the standard MCE IRL is proportional to the mismatch. Note that our Algorithm 1 with
α = 1 corresponds to the standard MCE-IRL algorithm.
Figure 3: Constructive example to study the per-formance gap of Algorithm 1 and the MCE IRL. 4This bound is worst than the one given in Theorem 1. When the condition in Theorem 2 does not hold, the robust MCE IRL achieves a tighter bound than the MCE IRL for a proper choice of α (see Appendix F.5). 7
Consider a reference MDP M ((cid:15)) = (cid:8)S, A, T ((cid:15)), γ, P0 (cid:9) with variable (cid:15) (see Figure 3). The state space is S = {s0, s1, s1}, where s1 and s2 are absorbing states. The action space is A = {a1, a2} and the initial state distribution is P0 (s0) = 1. The transition dynamics is deﬁned as: T ((cid:15))(s1|s0, a1) = 1−(cid:15),
T ((cid:15))(s2|s0, a1) = (cid:15), T ((cid:15))(s1|s0, a2) = 0, and T ((cid:15))(s2|s0, a2) = 1. The true reward function is given by: Rθ∗ (s0) = 0, Rθ∗ (s1) = 1, and Rθ∗ (s2) = −1. We deﬁne the learner and the expert environment as: M L := M (0) and M L := M ((cid:15)E ). Note that the distance between the two transition dynamics is ddyn be the policies returned by Algorithm 1 and the MCE IRL algorithm, under the above mismatch. Recall that π1 is the policy recovered by the
MCE IRL algorithm without this mismatch. Then, the following holds: (cid:0)T L, T E(cid:1) = 2(cid:15)E. Let πpl and π2 := πsoft
M L
θE
Theorem 4. For this example, the performance gap of Algorithm 1 vanishes by choosing α = (cid:12) 1 − ddyn(T L,T E ) (cid:12) (cid:12) = 0. Whereas, the performance gap of the standard MCE IRL (cid:12) (cid:12)V π1 (cid:12) (cid:12)
− V πpl (cid:12)V π1 (cid:12)
M L
M L
θ∗
θ∗ (cid:12) (cid:12) = γ
− V π2 (cid:12) 1−γ · ddyn(T L, T E).
M L
θ∗ is given by:
, i.e.,
M L
θ∗ 2 5 Experiments
This section demonstrates the superior performance of our Algorithm 1 compared to the standard
MCE IRL algorithm, when there is a transition dynamics mismatch between the expert and the learner.
All the missing ﬁgures and hyper-parameter details are reported in Appendix G.
θ∗ = (cid:0)S, A, T ref , γ, P0, Rθ∗ (cid:1) be a reference MDP. Given a learner noise (cid:15)L ∈ [0, 1],
Setup. Let M ref we introduce a learner MDP without reward function as M L,(cid:15)L = (cid:0)S, A, T L,(cid:15)L, γ, P0 (cid:1), where
T L,(cid:15)L ∈ ∆S|S,A is deﬁned as T L,(cid:15)L := (1 − (cid:15)L)T ref + (cid:15)L ¯T with ¯T ∈ ∆S|S,A. Similarly, given
θ∗ = (cid:0)S, A, T E,(cid:15)E , γ, P0, Rθ∗ (cid:1), where an expert noise (cid:15)E ∈ [0, 1], we deﬁne an expert MDP M E,(cid:15)E
T E,(cid:15)E ∈ ∆S|S,A is deﬁned as T E,(cid:15)E := (1 − (cid:15)E)T ref + (cid:15)E ¯T with ¯T ∈ ∆S|S,A. Note that a pair ((cid:15)E, (cid:15)L) corresponds to an IRL problem under dynamics mismatch, where the expert acts in the MDP and the learner in M L,(cid:15)L . In our experiments, we set T ref to be deterministic, and ¯T to be
M E,(cid:15)E
θ∗ (cid:0)T L,(cid:15)L , T E,(cid:15)E (cid:1) = 2 1 − 1
|(cid:15)L − (cid:15)E|. The learned uniform. Then, one can easily show that ddyn
|S| policies are evaluated in the MDP M L,(cid:15)L
, i.e., M L,(cid:15)L endowed with the true reward function Rθ∗ .
Baselines. We are not aware of any comparable prior IRL work that exactly matches our setting: (i) only one shot access to the expert environment, and (ii) do not explicitly model the expert environment.
Note that Algorithm 2 in [33] requires online access to T E (or the expert environment) to empirically estimate the gradient for every (time step) adversarial expert policy ˇπ∗, whereas we do not access the expert environment after obtaining a batch of demonstrations, i.e., ρ. Thus, for each pair ((cid:15)E, (cid:15)L), we compare the performance of the following: (i) our robust MCE IRL algorithm with different values of α ∈ {0.8, 0.85, 0.9, 0.95}, (ii) the standard MCE IRL algorithm, and (iii) the ideal baseline that utilizes the knowledge of the true reward function, i.e, π∗
θ∗ (cid:17) (cid:16)
.
M L,(cid:15)L
θ∗
Environments. We consider four GRIDWORLD environments and an OBJECTWORLD [43] en-vironment. All of them are N × N grid, where a cell represents a state. There are four actions per state, corresponding to steps in one of the four cardinal directions; T ref is deﬁned accordingly.
GRIDWORLD environments are endowed with a linear reward function Rθ∗ (s) = (cid:104)θ∗, φ(s)(cid:105), where
φ is a one-hot feature map. The entries θ∗ s of the parameter θ∗ for each state s ∈ S are shown in Figures 4a, 10e, 10i, and 10m. OBJECTWORLD is endowed with a non-linear reward function, determined by the distance of the agent to the objects that are randomly placed in the environment.
Each object has an outer and an inner color; however, only the former plays a role in determining the reward while the latter serves as a distractor. The reward is −2 in positions within three cells to an outer blue object (black areas of Figure 4e), 0 if they are also within two cells from an outer green object (white areas), and −1 otherwise (gray areas). We shift the rewards originally proposed by [43] to non-positive values, and we randomly placed the goal state in a white area. We also modify the reward features by augmenting them with binary features indicating whether the goal state has been reached. These changes simplify the application of the MCE IRL algorithm in the inﬁnite horizon setting. For this non-linear reward setting, we used the deep MCE IRL algorithm from [44], where the reward function is parameterized by a neural network. 8
(a) GRIDWORLD-1 (b) GRW (cid:15)L = 0 (c) GRW (cid:15)L = 0.05 (d) GRW (cid:15)L = 0.1 (e) OBJECTWORLD (f) OBW (cid:15)L = 0 (g) OBW (cid:15)L = 0.05 (h) OBW (cid:15)L = 0.1
Figure 4: Comparison of the performance our Algorithm 1 against the baselines, under different levels of mismatch: ((cid:15)E, (cid:15)L) ∈ {0.0, 0.05, 0.1, 0.15, 0.2} × {0.0, 0.05, 0.1}. Each plot corresponds to a
ﬁxed leaner environment M L,(cid:15)L with (cid:15)L ∈ {0.0, 0.05, 0.1}. The values of α used for Algorithm 1 are reported in the legend. The vertical line indicates the position of the learner environment in the x-axis. We abbreviated the environment names as GRW, and OBW. Note that our Robust MCE IRL outperforms standard MCE IRL when the expert noise increases along the x-axis. At the same time,
Robust MCE IRL might perform slightly worse in the low expert noise regime. This observation aligns with the overly conservative nature of robust training methods.
Results. In Figure 4, we have presented the results for two of the environments, and the complete results can be found in Figure 10. Also, in Figure 4, we have reported the results of our algorithm with the best performing value of α; and the performance of our algorithm with different values of
α are presented in Figure 11. In all the plots, every point in the x-axis corresponds to a pair ((cid:15)E, (cid:15)L).
For example, consider Figure 4b, for a ﬁxed learner environment M L,(cid:15)L with (cid:15)L = 0, and different expert environments M E,(cid:15)E by varying (cid:15)E along the x-axis. Note that, in this ﬁgure, the distance (cid:0)T L,(cid:15)L, T E,(cid:15)E (cid:1) ∝ |(cid:15)L − (cid:15)E| increases along the x-axis. For each pair ((cid:15)E, (cid:15)L), in the y-axis, we ddyn present the performance of the learned polices in the MDP M L,(cid:15)L
. In alignment with our
, i.e., V π
θ∗
M L,(cid:15)L
θ∗ theory, the performance of the standard MCE IRL algorithm degrades along the x-axis. Whereas, our
Algorithm 1 resulted in robust performance (even closer to the ideal baseline) across different levels of mismatch. These results conﬁrm the efﬁcacy of our method under mismatch. However, one has to care-fully choose the value of 1−α (s.t. T E,(cid:15)E ∈ T L,α): (i) underestimating it would lead to a linear decay in the performance, similar to the MCE IRL, (ii) overestimating it would also slightly hinder the per-ddyn(T L, (cid:98)T E) formance, and (iii) given a rough estimate (cid:98)T E of the expert dynamics, choosing 1 − α ≈ 2 would lead to better performance in practice. The potential drop in the performance of our Robust
MCE IRL method under the low expert noise regime (see Figures 4c, 4d, and 4h) can be related to the overly conservative nature of robust training. See Appendix G.3 for more discussion on the choice of 1 − α. In addition, we have tested our method on a setting with low-dimensional feature mapping
φ, where we observed signiﬁcant improvement over the standard MCE IRL (see Appendix G.2). 6 Extension to Continuous MDP Setting
In this section, we extend our ideas to the continuous MDP setting, i.e., the environments with continuous state and action spaces. In particular, we implement a robust variant of the Relative
Entropy IRL (RE IRL) [15] algorithm (see Algorithm 3 in Appendix H). We cannot use the dynamic programming approach to ﬁnd the player and opponent policies in the continuous MDP setting.
Therefore, we solve the two-player Markov game in a model-free manner using the policy gradient methods (see Algorithm 4 in Appendix H). 9
(a) GAUSSIANGRID (b) M L,(cid:15)L with (cid:15)L = 0 (c) M L,(cid:15)L with (cid:15)L = 0.05 (d) M L,(cid:15)L with (cid:15)L = 0.1
Figure 5: Comparison of the performance our Robust RE IRL (Algorithm 3) against the standard
RE IRL, under different levels of mismatch: ((cid:15)E, (cid:15)L) ∈ {0.0, 0.05, 0.1, 0.15, 0.2} × {0.0, 0.05, 0.1}.
Each plot corresponds to a ﬁxed leaner environment M L,(cid:15)L with (cid:15)L ∈ {0.0, 0.05, 0.1}. The values of α used for Algorithm 3 are reported in the legend. The vertical line indicates the position of the learner environment in the x-axis. The results are averaged across 5 seeds.
We evaluate the performance of our Robust RE IRL method on a continuous gridworld environment that we called GAUSSIANGRID. The details of the environment and the experimental setup are given in Appendix H. The results are reported in Figure 5, where we notice that our Robust RE IRL method outperforms standard RE IRL. 7