Abstract
Coupons allocation is an important tool for enterprises to increase the activity and loyalty of users on the e-commerce market. One fundamental problem related is how to allocate coupons within a ﬁxed budget while maximizing users’ retention on the e-commerce platform. The online e-commerce environment is compli-cated and ever changing, so it requires the coupons allocation policy learning can quickly adapt to the changes of the company’s business strategy. Unfortu-nately, existing studies with a huge computation overhead can hardly satisfy the requirements of real-time and fast-response in the real world. Speciﬁcally, the problem of coupons allocation within a ﬁxed budget is usually formulated as a
Lagrangian problem. Existing solutions need to re-learn the policy once the value of Lagrangian multiplier variable λ is updated, causing a great computation over-head. Besides, a mature e-commerce market often faces tens of millions of users and dozens of types of coupons which construct the huge policy space, further increasing the difﬁculty of solving the problem. To tackle with above problems, we propose a budget constrained ofﬂine reinforcement learning and evaluation with
λ-generalization (BCORLE(λ)) framework. The proposed method can help enter-prises develop a coupons allocation policy which greatly improves users’ retention rate on the platform while ensuring the cost does not exceed the budget. Speciﬁ-cally, λ-generalization method is proposed to lead the policy learning process can be executed according to different λ values adaptively, avoiding re-learning new polices from scratch. Thus the computation overhead is greatly reduced. Further, a novel ofﬂine reinforcement learning method and an off-policy evaluation algorithm are proposed for policy learning and policy evaluation, respectively. Finally, exper-iments on the simulation platform and real-world e-commerce market validate the effectiveness of our approach.
∗Work was done during an internship at Alibaba Group.
†Correspondence:yangqingyu@mail.xjtu.edu.cn 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1

Introduction
With the development of the internet industry, the business competition between e-commerce plat-forms is becoming more and more ﬁerce. It is common for the e-commerce platform to provide users with incentives in the form of monetary prize to attract users to take actions of clicks or conversions
[3, 35, 41]. For instance, on online e-commerce market, such as Taobao and Amazon, every day an e-commerce platform sends a coupon to each user who logs on the platform to keep them high retention on the platform. Obviously, the larger value of distributed coupons is, the higher retention rate of users it will gain. However, it may bring huge ﬁnancial loss to the platform when coupons allocation is too costly. Thus, it is a key problem for the platform to decide an appropriate value of each coupon maximizing the users’ retention while limiting the cost not to exceed a ﬁxed budget.
Besides, tens of millions of users and dozens of types of coupons lead to an extremely large policy space, which greatly increases the difﬁculty for solving the problem.
The budget constrained coupons allocation problem is usually formulated as a constrained Markov decision process (CMDP), and then can be converted into a Lagrangian dual problem. In related studies [20, 28, 35], bisection search or gradient descent is used to ﬁnd the optimal value of Lagrangian multiplier variable λ, and the corresponding optimal coupons allocation policy for given λ is learned using reinforcement learning (RL) methods. Here, ofﬂine RL methods are used to avoid potential
ﬁnancial risks in learning process as the budget cannot be recovered once dispensed. Unfortunately, a key problem is that the policy needs to be re-learned every time when the value of λ is updated until the budget constraint is satisﬁed. Such a repetitive policy learning process brings a great computation overhead, making existing studies unable to satisfy the real-time and fast response requirements in the complicated industrial world. Thus the applications of existing methods are limited.
To address this problem, we propose a λ-generalization method which is also a technique of multi-objective RL [11]. The key idea behind our method is that the tasks of ﬁnding optimal value of λ and learning an optimal policy are combined by extending the reward function and training dataset with different values of λ. Thus, the optimal policy can be learned with different values of λ adaptively and the policy learning process only needs to be performed once. Besides, there is another advantage of our method is that there is no need to re-learn the policy when the budget changes. We only need to select an appropriate value of λ which makes its corresponding optimal policy satisfy the new budget constraint. It can help our method respond quickly to the changing business strategy of the company.
The contribution of this work is four-fold. First, a budget constrained ofﬂine reinforcement learning and evaluation with λ-generalization (BCORLE(λ)) framework is proposed to solve the CMDP problem of coupons allocation. BCORLE(λ) framework consists of λ-generalization method, an ofﬂine policy learning method and an off-policy evaluation method, in which λ-generalization method is proposed to reduce the computation overhead of policy learning. Second, for policy learning, an improved ofﬂine RL algorithm called resemble batch-constrained Q-learning (R-BCQ) is proposed in this paper. R-BCQ combines the advantages of two popular ofﬂine RL methods: batch-constrained
Q-learning (BCQ) [13] and random ensemble mixture (REM) [1]. Speciﬁcally, R-BCQ addresses the problem of extrapolation error like BCQ, and retains strong generalization ability like REM. Third, to evaluate the performance of learned policy, a model-free off-policy evaluation method called random ensemble mixture evaluator (REME) is proposed. Finally, the experiments on a simulation platform and a real mobile shopping app validate the effectiveness of the proposed methods. 2