Abstract
Actor-critic (AC) algorithms, empowered by neural networks, have had signifi-cant empirical success in recent years. However, most of the existing theoretical support for AC algorithms focuses on the case of linear function approximations, or linearized neural networks, where the feature representation is fixed through-out training. Such a limitation fails to capture the key aspect of representation learning in neural AC, which is pivotal in practical problems. In this work, we take a mean-field perspective on the evolution and convergence of feature-based neural AC. Specifically, we consider a version of AC where the actor and critic are represented by overparameterized two-layer neural networks and are updated with two-timescale learning rates. The critic is updated by temporal-difference (TD) learning with a larger stepsize while the actor is updated via proximal policy opti-mization (PPO) with a smaller stepsize. In the continuous-time and infinite-width limiting regime, when the timescales are properly separated, we prove that neural
AC finds the globally optimal policy at a sublinear rate. Additionally, we prove that the feature representation induced by the critic network is allowed to evolve within a neighborhood of the initial one. 1

Introduction
In reinforcement learning (RL) [56], an agent aims to learn the optimal policy that maximizes the expected total reward by interacting with the environment. Policy-based RL algorithms achieve such a goal by directly optimizing the expected total reward as a function of the policy, which often involves two components: policy evaluation and policy improvement. Specifically, policy evaluation refers to estimating the value function of the current policy, which characterizes the performance of the current policy and reveals the updating direction for finding a better policy, which is known as policy improvement. Algorithms with these two ingredients are also called actor-critic (AC) methods [36], where the actor and the critic refer to the policy and its corresponding value function, respectively.
† Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Recently, in RL applications with large state spaces, actor-critic empowered by expressive function approximators such as neural networks have achieved striking empirical successes [3, 4, 9, 20, 51, 52, 60]. These successes benefit from the data-dependent representations learned by neural networks.
Unfortunately, however, the theoretical understanding of this data-dependent benefit is very limited.
The classical theory of AC focuses on the case of linear function approximation, where the actor and critic are represented using linear functions with the feature mapping fixed throughout learning
[10, 11, 36]. Meanwhile, a few recent works establish convergence and optimality of AC with overparameterized neural networks [26, 39, 61], where the neural network training is captured by the Neural Tangent Kernel (NTK) [30]. Specifically, with properly designed parameter initialization and stepsizes, and sufficiently large network widths, the neural networks employed by both actor and critic can be assumed to be well approximated by linear functions of a random feature determined by initial parameters. In other words, concerning representation learning, the features induced by these algorithms are by assumption infinitesimally close to the initial featural representation, which is data-independent.
In this work, we make initial steps towards understanding how representation learning comes into play in neural AC. Specifically, we address the following questions:
Going beyond the NTK regime, does neural AC provably find the globally optimal policy? How does the feature representation associated with the neural network evolve along with neural AC?
We focus on a version of AC where the critic performs temporal-difference (TD) learning [55] for policy evaluation and the actor improves its policy via proximal policy optimization (PPO) [49], which corresponds to a Kullback-Leibler (KL) divergence regularized optimization problem, with the critic providing the update direction. Moreover, we utilize two-timescale updates where both the actor and critic are updated at each iteration but with the critic having a much larger stepsize. In other words, the critic is updated at a faster timescale. Meanwhile, we represent the critic explicitly as a two-layer overparameterized neural network and parameterize the actor implicitly via the critic and PPO updates. To examine convergence, we study the evolution of the actor and critic in the continuous-time limiting regime with the network width going to infinity. In such a regime, the actor update is closely connected to replicator dynamics [12, 28, 50] and the critic update is captured by a semigradient flow in the Wasserstein space [59]. Moreover, the semigradient flow runs at a faster timescale according to the two-timescale mechanism.
It turns out that the separation of timescales plays an important role in the convergence analysis. In particular, in the continuous-time limit, it enables us to first separately analyze the evolution of actor and critic and then combine these results to get final theoretical guarantees. Specifically, focusing solely on the actor, we prove that the time-averaged suboptimality of the actor converges sublinearly to zero up to the time-averaged policy evaluation error associated with critic updates. Moreover, for the critic, under proper regularity conditions, we connect the Bellman error to the Wasserstein distance and show that the time-averaged policy evaluation error also converges sublinearly to zero. Therefore, we show that neural AC provably achieves global optimality at a sublinear rate.
Furthermore, regarding representation learning, we show that the critic induces a data-dependent feature representation within an O(1/α) neighborhood of the initial representation in terms of the
Wasserstein distance, where α is a sufficiently large scaling parameter.
The key to our technical analysis reposes on three ingredients: (i) infinite-dimensional variational inequalities with a one-point monotonicity [27], (ii) a mean-field perspective on neural networks
[19, 41, 42, 53, 54], and (iii) the two-timescale stochastic approximation [13, 37]. In particular, in the infinite-width limit, the neural network and its induced feature representation are identified with a distribution over the parameter space. The mean-field perspective enables us to characterize the evolution of such a distribution within the Wasserstein space via a partial differential equation (PDE)
[5, 6, 58, 59]. For policy evaluation, such a PDE is given by the semigradient flow induced by TD learning. We characterize the error of policy evaluation by showing that mean-field Bellman error satisfies a version of one-point monotonicity tailored to the Wasserstein space. Moreover, our actor analysis utilizes the geometry of policy optimization, which shows that the expected total reward, 2
as a function of the policy, also enjoys the property of one-point monotonicity in the policy space.
Finally, the actor and critic errors are connected via two-timescale stochastic approximation. To the best of our knowledge, this is the first time that convergence and global optimality guarantees have been obtained for neural AC.