Abstract
We present Long Short-term TRansformer (LSTR), a temporal modeling algo-rithm for online action detection, which employs a long- and short-term memory mechanism to model prolonged sequence data. It consists of an LSTR encoder that dynamically leverages coarse-scale historical information from an extended temporal window (e.g., 2048 frames spanning of up to 8 minutes), together with an LSTR decoder that focuses on a short time window (e.g., 32 frames spanning 8 seconds) to model the ﬁne-scale characteristics of the data. Compared to prior work, LSTR provides an effective and efﬁcient method to model long videos with fewer heuristics, which is validated by extensive empirical analysis. LSTR achieves state-of-the-art performance on three standard online action detection benchmarks,
THUMOS’14, TVSeries, and HACS Segment. Code has been made available at: https://xumingze0308.github.io/projects/lstr. 1

Introduction
Given an incoming stream of video frames, online action detection [14] is concerned with the task of classifying what is happening at each frame without seeing the future. Unlike ofﬂine methods that assume the entire video is available, online methods process the data causally, up to the current time. In this paper, we present an online temporal modeling algorithm capable of capturing temporal relations on prolonged sequences up to 8 minutes long, while retaining ﬁne granularity of the event in the representation. This is achieved by modeling activities at different temporal scales, so as to capture a variety of events ranging from bursts to slow trends.
Speciﬁcally, we propose a method, named Long Short-term TRansformer (LSTR), to jointly model long- and short-term temporal dependencies. LSTR has two main advantages over prior work. 1)
It stores the history directly thus avoiding the pitfalls of recurrent models [18, 50, 28, 10]. Back-propagation through time, BPTT, is not needed as the model can directly attend to any useful frames from memory. 2) It separates long- and short-term memories, which allows modeling short-term context while extracting useful correlations from the long-term history. This allows us to compress the long-term history without losing important ﬁne-scale information.
As shown in Fig. 1, we explicitly divide the entire history into the long- and short-term memories and build our model with an encoder-decoder architecture. Speciﬁcally, the LSTR encoder compresses and abstracts the long-term memory into a latent representation of ﬁxed length, and the LSTR decoder uses a short window of transient frames to perform self-attention and cross-attention operations on the extracted token embeddings from the LSTR encoder. In the LSTR encoder, an extended temporal support becomes beneﬁcial in dealing with untrimmed, streaming videos by devising two-stage memory compression, which is shown to be computationally efﬁcient in both training and inference. Our overall long short-term Transformer architecture gives rise to an effective and efﬁcient representation for modeling prolonged sequence data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of Long Short-term TRansformer (LSTR). Given a live streaming video,
LSTR sequentially identiﬁes the actions happening in each incoming frame by using an encoder-decoder architecture, without future context. The dashed brown arrows indicate the data ﬂow of the long- and short-term memories following the ﬁrst-in-ﬁrst-out (FIFO) logic. (Best viewed in color.)
We validate LSTR on standard benchmark datasets (THUMOS’14 [30], TVSeries [14], and HACS
Segment [76]). These have distinct characteristics such as video length spanning from a few seconds to tens of minutes. Experimental results establish LSTR as the state-of-the-art for online action detection. Ablation studies further showcase LSTR’s abilities in modeling long video sequences. 2