Abstract
We consider the task of representation learning for unsupervised segmentation of 3D voxel-grid biomedical images. We show that models that capture implicit hierarchical relationships between subvolumes are better suited for this task. To that end, we consider encoder-decoder architectures with a hyperbolic latent space, to explicitly capture hierarchical relationships present in subvolumes of the data. We propose utilizing a 3D hyperbolic variational autoencoder with a novel gyroplane convolutional layer to map from the embedding space back to 3D images. To capture these relationships, we introduce an essential self-supervised loss—in addition to the standard VAE loss—which infers approximate hierarchies and encourages implicitly related subvolumes to be mapped closer in the embedding space. We present experiments on both synthetic data and biomedical data to validate our hypothesis. 1

Introduction
Advances in biomedical imaging techniques such as cryogenic electron tomography (cryo-ET) and magnetic resonance imaging (MRI) have resulted in an ever-increasing amount of 3D biomedical image data. In these data domains, a growing body of work shows that, when provided with labels, machine learning models achieve good performance on many tasks [Çiçek et al., 2016, Milletari et al., 2017, Dou et al., 2017, Falk et al., 2019]. However, these labels, especially for segmentation, are very costly as they often have to be provided by experts in the appropriate ﬁeld. Consequently, supervised learning and even semi-supervised learning remain limited in this setting as (1) tasks and domains are very diverse, making it intractable for experts to provide labelled data for all problems; and (2) experts can only label objects they already know, restricting the potential for scientiﬁc discovery using machine learning methods. In this work, we tackle the task of unsupervised segmentation in 3D biomedical image data.
Our key insight is that 3D biomedical images have inherent hierarchical structure. For example, in the cryo-ET domain, an image of a cell has a hierarchy that at the highest level comprises the entire cell; at a ﬁner level comprises organelles such as the mitochondria and nucleus; and at an even
∗These authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
ﬁner level comprises sub-structures such as the nucleolus of a nucleus or protein machineries within organelles. Such types of hierarchies are present in many types of biomedical images (e.g., nested anatomical structures within MRI images). We hypothesize that in the unsupervised setting, models that are able to encode this internal hierarchical structure will provide better data representations for downstream tasks. To that end, we propose learning representations based on embedding subvolumes of 3D images in hyperbolic space.
In contrast to traditional Euclidean embeddings, hyperbolic embeddings better preserve hierarchical relationships present in the data. Hyperbolic representations have been proposed as a continuous way to represent hierarchical data, due to their ability to embed trees with arbitrarily low error
[Sarkar, 2011]. A recent line of work utilizes hyperbolic representations to model hierarchical data across domains ranging from natural language word taxonomies [Nickel and Kiela, 2017, 2018] and graphs [Nickel and Kiela, 2017, Mathieu et al., 2019, Ovinnikov, 2019, Chami et al., 2019], to image classiﬁcation [Mathieu et al., 2019]. In these settings, the objects, and in most cases their relationships, are explicitly encoded in the data. However, 3D biomedical images consist of subvolumes that represent parts of an implicit hierarchical structure. In our case, for any single 3D voxel-grid, we wish to embed and infer the implicit relationships between all of its subvolumes without any supervision.
To embed our 3D images in hyperbolic space, we use a 3D hyperbolic variational autoencoder (VAE). For the decoder, we propose a gyroplane convolutional layer which maps from the latent space back to 3D images while respecting hyperbolic geometry. In addition to the VAE loss, we propose an essential self-supervised loss to capture the hierarchical structure present in the data.
More speciﬁcally, we consider reconstruction of implicit hierarchies as a pretext task. Concretely, we add a triplet loss which encourages a child subvolume to be mapped close to its parent subvolume in hyperbolic space. To capture hierarchical relationships of varying granularity, we train on subvolumes sampled at multiple scales. Finally, for a speciﬁed scale, we cluster the subvolumes in latent space and produce a segmentation map.
We evaluate our model on datasets with different domains: synthetic datasets and a medical image dataset. We construct synthetic datasets where we generate structures at various scales and show that our model segments objects at multiple levels of hierarchy better than all prior unsupervised segmentation methods. We demonstrate performance gains ranging from 7% for the smallest objects to 32% for the largest objects. On the real-world medical image dataset (BraTS Brain Tumor
Segmentation Challenge) [Menze et al., 2014, Bakas et al., 2017, 2018], we show that our method outperforms prior works by 19%, and even achieves comparable performance to semi-supervised methods although we do not use any labels. 2