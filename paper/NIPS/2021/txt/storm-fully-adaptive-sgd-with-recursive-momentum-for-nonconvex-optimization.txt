Abstract
In this work we investigate stochastic non-convex optimization problems where the objective is an expectation over smooth loss functions, and the goal is to ﬁnd an approximate stationary point. The most popular approach to handling such problems is variance reduction techniques, which are also known to obtain tight convergence rates, matching the lower bounds in this case. Nevertheless, these techniques require a careful maintenance of anchor points in conjunction with ap-propriately selected “mega-batchsizes". This leads to a challenging hyperparameter tuning problem, that weakens their practicality. Recently, [Cutkosky and Orabona, 2019] have shown that one can employ recursive momentum in order to avoid the use of anchor points and large batchsizes, and still obtain the optimal rate for this setting. Yet, their method called STORM crucially relies on the knowledge of the smoothness, as well a bound on the gradient norms. In this work we propose
STORM+, a new method that is completely parameter-free, does not require large batch-sizes, and obtains the optimal O(1/T 1/3) rate for ﬁnding an approximate stationary point. Our work builds on the STORM algorithm, in conjunction with a novel approach to adaptively set the learning rate and momentum parameters. 1

Introduction
Over the past decade non-convex models have become principal tools in ML (Machine Learning), and in data-science. This predominantly includes deep models, as well as Phase Retrieval [Candes et al., 2015], non-negative matrix factorization [Hoyer, 2004], and matrix completion problems [Ge et al., 2016] among others.
The main workhorse for training ML models is SGD (stochastic gradient descent) and its numerous variants. One parameter that signiﬁcantly affects the SGD performance is the learning rate, which often requires a careful and costly hyper-parameter tuning. Adaptive approaches to setting the learning rate like AdaGrad [Duchi et al., 2011] and Adam [Kingma and Ba, 2014] as well as non-adaptive heuristics [Loshchilov and Hutter, 2017, He et al., 2019] are very popular in modern ML applications, yet these methods also require some tuning of hyper-parameters like momentum and the scale of the learning rate schedule.
A popular SGD heuristic that has proven to be crucial in many applications is the use of momentum, i.e., the use of a weighted average of past gradients instead of the current gradient [Sutskever et al., 2013, Kingma and Ba, 2014]. Although adaptive approaches to setting the momentum have been investigated in the past [Srinivasan et al., 2018, Hameed et al., 2016], principled and theoretically-grounded approaches to doing so are less investigated. Another aspect that has not been extensively studied, which we take into account in this work, is the interplay between learning rate and momentum.
∗A Viterbi fellow. Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In this work we explore momentum-based adaptive and parameter-free methods for stochastic non-convex optimization problems. Concretely, we focus on the setting where the objective is an expectation over smooth losses (see Eq. (4)), and the goal is to ﬁnd an approximate stationary point.
In the general case of smooth non-convex objectives it is known that one can approach a stationary point at a rate of O(1/T 1/4), where T is the total number of samples [Ghadimi and Lan, 2013].
While this rate is optimal in the general case, it is known that one can obtain an improved rate of
O(1/T 1/3) if the objective is an expectation over smooth losses [Fang et al., 2018, Zhou et al., 2018,
Cutkosky and Orabona, 2019, Tran-Dinh et al., 2019]. Besides, this rate was recently shown to be tight [Arjevani et al., 2019].
Nevertheless, most of the methods developed for this setting rely on variance reduction techniques
[Johnson and Zhang, 2013, Zhang et al., 2013, Mahdavi et al., 2013, Wang et al., 2013], which require careful maintenance of anchor points in conjunction with appropriately selected large batchsizes. This leads to a challenging hyper-parameter tuning problem, weakening their practicality. One exception is the recent STORM algorithm of Cutkosky and Orabona [2019].
STORM does not require large batches nor anchor points; instead, it uses a corrected momentum based gradient update that leads to implicit variance reduction, which in turn facilitates fast conver-gence. Unfortunately, none of the aforementioned methods (including STORM ) is parameter-free.
Indeed, the knowledge of smoothness parameter together with either the noise variance or a bound on the norm of the gradients are crucial to establish their guarantees.
In this work, we essentially develop a parameter-free variant of STORM algorithm. We summarize our contributions as follows,
• We present STORM+ , a parameter-free momentum based method that ensures the optimal
O(1/T 1/3) rate for the expectation over smooth losses setting. Similarly to STORM , our method does not require large-batches nor anchor points.
• STORM+ implicitly adapts to the variance of the gradients. Concretely, it obtains con-vergence rate of O(1/
T ) rate in the noiseless case. We also improve over STORM by shaving off a (log T )3/4 factor from the 1/
T + σ1/3/T 1/3), which recovers the optimal O(1/
T term.
√
√
√
• In STORM+ we demonstrate a novel way to set the learning rate by introducing an adaptive interplay between learning rate and momentum parameters. 2