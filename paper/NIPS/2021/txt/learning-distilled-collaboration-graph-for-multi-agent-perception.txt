Abstract
To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with holistic-view inputs; the student model is based on intermediate collaboration with single-view inputs. Our framework trains DiscoGraph by constraining post-collaboration feature maps in the student model to match the correspondences in the teacher model. Second, we propose a matrix-valued edge weight in DiscoGraph.
In such a matrix, each element reﬂects the inter-agent attention at a speciﬁc spatial region, allowing an agent to adaptively highlight the informative regions. During in-ference, we only need to use the student model named as the distilled collaboration network (DiscoNet). Attributed to the teacher-student framework, multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with a holistic view. Our approach is validated on V2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized using
CARLA and SUMO co-simulation. Our quantitative and qualitative experiments in multi-agent 3D object detection show that DiscoNet could not only achieve a better performance-bandwidth trade-off than the state-of-the-art collaborative perception methods, but also bring more straightforward design rationale. Our code is available on https://github.com/ai4ce/DiscoNet. 1

Introduction
Perception, which involves organizing, identifying and interpreting sensory information, is a crucial ability for intelligent agents to understand the environment. Single-agent perception [4] has been studied extensively in recent years, e.g., 2D/3D object detection [18, 27], tracking [23, 22] and segmentation [25, 16], etc. Despite its great progress, single-agent perception suffers from a number of shortcomings stemmed from its individual perspective. For example, in autonomous driving [8], the LiDAR-based perception system can hardly perceive the target in the occluded or long-range areas.
Intuitively, with an appropriate collaboration strategy, multi-agent perception could fundamentally upgrade the perception ability over single-agent perception.
To design a collaboration strategy, current approaches mainly include raw-measurement-based early collaboration, output-based late collaboration and feature-based intermediate collaboration.
∗Corresponding authors. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Early (b) Intermediate (c) Ours
Figure 1: Scheme comparison. (a) Early collaboration requires an expensive bandwidth for raw data transmission. (b) Intermediate collaboration needs an appropriate collaboration strategy. (c) The proposed method incorporates both early and intermediate collaboration into a knowledge distillation framework, enabling the knowledge of early collaboration to guide the training of an intermediate collaboration strategy, leading to better trade-off between performance and bandwidth.
Early collaboration [3] aggregates the raw measurements from all the agents, promoting a holistic perspective; see Fig. 1 (a). It can fundamentally solve the occlusion and long-range issues occurring in the single-agent perception; however, it requires a lot of communication bandwidth. Contrarily, late collaboration aggregates each agent’s perception outputs. Although it is bandwidth-efﬁcient, each individual perception output could be noisy and incomplete, causing unsatisfying fusion results.
To deal with the performance-bandwidth trade-off, intermediate collaboration [19, 34, 20] has been proposed to aggregate intermediate features across agents; see Fig. 1 (b). Since we can squeeze representative information to compact features, this approach can potentially both achieve communication bandwidth efﬁciency and upgrade perception ability; however, a bad design of collaboration strategy might cause information loss during feature abstraction and fusion, leading to limited improvement of the perception ability.
To achieve an effective design of intermediate collaboration, we propose a distilled collaboration graph (DiscoGraph) to model the collaboration among agents. In DiscoGraph, each node is an agent with real-time pose information and each edge reﬂects the pair-wise collaboration between two agents. The proposed DiscoGraph is trainable, pose-aware, and adaptive to real-time measurements, reﬂecting dynamic collaboration among agents. It is novel from two aspects. First, from the training aspect, we propose a teacher-student framework to train DiscoGraph through knowledge distillation
[1, 10, 26]; see Fig. 1 (c). Here the teacher model is based on early collaboration with holistic-view inputs and the student model is based on intermediate collaboration with single-view inputs. The knowledge-distillation-based framework enhances the training of DiscoGraph by constraining the post-collaboration feature maps in the student model to match the correspondences in the teacher model.
With the guidance of both output-level supervision from perception and feature-level supervision from knowledge distillation, the distilled collaboration graph promotes better feature abstraction and aggregation, improving the performance-bandwidth trade-off. Second, from the modeling aspect, we propose a matrix-valued edge weight in DiscoGraph to reﬂect the collaboration strength with a high spatial resolution. In the matrix, each element represents the inter-agent attention at a speciﬁc spatial region. This design allows the agents to adaptively highlight the informative regions and strategically select appropriate partners to request supplementary information.
During inference, we only need to use the student model. Since it leverages DiscoGraph as the key component, we call the student model as the distilled collaboration network (DiscoNet). Multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with the holistic view.
To validate the proposed method, we build V2X-Sim 1.0, a new large-scale multi-agent 3D object detection dataset in autonomous driving scenarios based on CARLA and SUMO co-simulation platform [6]. Comprehensive experiments conducted in 3D object detection [37, 28, 29, 17] have shown that the proposed DiscoNet achieves better performance-bandwidth trade-off and lower communication latency than the state-of-the-art intermediate collaboration methods. 2
Figure 2: Overall teacher-student training framework. In the student model, for Agent 1 (in red), its input single-view point cloud can be converted to a BEV map, and then be consumed by a shared encoder Θs to obtain the feature map Fs i. Based on the collaboration graph GΠ, Agent 1 aggregates the neural messages from other agents and obtains the updated feature map Hs i. The shared header following the shared decoder outputs the 3D detection results. In the teacher model, we aggregate the points collected from all agents to obtain a holistic-view point cloud. We crop the region and align the pose to ensure the BEV maps in both teacher and student models are synchronized. We constrain all the post-collaboration feature maps in the student model to match the correspondences in the teacher model through knowledge distillation, resulting in a collaborative student model. 2 Multi-Agent Perception System with Distilled Collaboration Graph
This work considers a collaborative perception system to perceive a scene, where multiple agents can perceive and collaborate with each other through a broadcast communication channel. We assume each agent is provided with an accurate pose and the perceived measurements are well synchronized.
Then, given a certain communication bandwidth, we aim to maximize the perception ability of each agent through optimizing a collaboration strategy. To design such a strategy, we consider a teacher-student training framework to integrate the strengths from both early and intermediate collaboration. During training, we leverage an early-collaboration model (teacher) to teach an intermediate-collaboration model (student) how to collaborate through knowledge distillation. Here the teacher model and the student model are shared by all the agents, but each agent would input raw measurements from its own view to either model; see Fig. 2. During inference, we only need the student model, where multiple agents with the shared student model could collaboratively approach the performance of the teacher model with the hypothetical holistic view.
In this work, we focus on the perception task of LiDAR-based 3D object detection because the unifying 3D space naturally allows the aggregation of multiple LiDAR scans. Note that in principle the proposed method is generally-applicable in collaborative perception if there exists a uniﬁed space to aggregate raw data across multiple agents. 2.1 Student: Intermediate collaboration via graphs
Feature encoder. The functionality is to extract informative features from raw measurements for each agent. Let Xi be the 3D point cloud collected by the ith agent (M agents in total), the feature of the ith agent is obtained as Fs i ← Θs(Xi), where Θs(·) is the feature encoder shared by all the agents and the superscript s reﬂects the student mode. To implement the encoder, we convert a 3D point cloud to a bird’s-eye-view (BEV) map, which is amenable to classic 2D convolutions. Speciﬁcally, we quantize the 3D points into regular voxels and represent the 3D voxel lattice as a 2D pseudo-image, with the height dimension corresponding to image channels. Such a 2D image is virtually a BEV map, whose basic element is a cell that is associated with a binary vector along the vertical axis.
Let Bi ∈ {0, 1}K×K×C be the BEV map of the ith agent associated with Xi. With this map, we can apply four blocks composed of 2D convolutions, batch normalization and ReLU activation to gradually reduce the spatial dimension and increase the number of channels for the BEV, to obtain i ∈ R ¯K× ¯K× ¯C with ¯K × ¯K the spatial resolution and ¯C the number of channels. the feature map Fs 3
Feature compression. To save the communication bandwidth, each agent could compress its feature map prior to transmission. Here we consider a 1 × 1 convolutional autoencoder [24] to compress/decompress the feature maps along the channel dimension. The autoencoder is trained together with the whole system, making the system work with limited collaboration information.
Collaboration graph process. The functionality is to update the feature map through data transmis-sion among the agents. The core component here is a collaboration graph1 GΠ(V, EΠ), where V is the ﬁxed node set and EΠ is the trainable edge set. Each node in V is an agent with the real-time pose information; for instance, ξi ∈ se(3) is the ith agent’s pose in the global coordinate system; and each edge in EΠ is trainable and models the collaboration between two agents, with Π an edge-weight en-coder, reﬂecting the trainable collaboration strength between agents. Let MGΠ (·) be the collaboration process deﬁned on the collaboration graph GΠ. The feature maps of all the agents after collaboration (cid:1). This process has three stages: neural message transmission (S1), are {Hs neural message attention (S2) and neural message aggregation (S3). i=1 ← MGΠ i}M i=1 (cid:0){Fs i}M
In the neural message transmission stage (S1), each agent transmits its BEV-based feature map to the other agents. Since the BEV-based feature map summarizes the information of each agent, we consider it as a neural message. In the neural message attention stage (S2), each agent receives others’ neural messages and determines the matrix-valued edge weights, which reﬂect the importance of the neural message from one agent to another at each individual cell. Since each agent has its unique pose, we leverage the collaboration graph to achieve feature transformation across agents. For the ith agent, i) ∈ R ¯K× ¯K× ¯C, the transformed BEV-based feature map from the jth agent is then Fs j→i = Γj→i(Fs where the transformation Γj→i(·) is based on two ego poses ξj and ξi. Now Fs j→i and Fs i are supported in the same coordinate system. To determine the edge weights, we use the edge encoder Π to correlate the ego feature map and the feature map from another agent; that is, the matrix-valued i) ∈ R ¯K× ¯K, where Π edge weight from the jth agent to the ith agent is Wj→i = Π(Fs concatenates two feature maps along the channel dimension and then uses four 1 × 1 convolutional layers to gradually reduce the number of channels from 2 ¯C to 1. Meanwhile, there is a softmax operation applied at each cell in the feature map to normalize the edge weights across multiple agents.
Note that previous works [19, 20, 34] generally consider a scalar-valued edge weight to reﬂect the overall collaboration strength between two agents; while we consider a matrix-valued edge weight
Wj→i, which models the collaboration strength from the jth agent to the ith agent with a ¯K × ¯K spatial resolution. In this matrix, each element corresponds to a cell in the BEV map, indicating a speciﬁc spatial region; thus, this matrix reﬂects the spatial attention at a cell-level resolution. In the neural message aggregation stage (S3), each agent aggregates the feature maps from all the agents based on the normalized matrix-valued edge weights. The updated feature map of the agent i is
Hs j→i, where (cid:12) denotes the dot product with channel-wise broadcasting. j=1 Wj→i (cid:12) Fs i = (cid:80)M j→i, Fs
Remark. The proposed collaboration graph is trainable because each matrix-valued edge weight is a trainable matrix to reﬂect the agent-to-agent attention in a cell-level spatial resolution; it is pose-aware, empowering all the agents to work with the synchronized coordinate system; furthermore, it is dynamic at each timestamp as each edge weight would adapt to the real-time neural messages. Ac-cording to the proposed collaboration graph, the agents can discover the region requiring collaboration on the ﬂy, and strategically select appropriate partners to request supplementary information. i ← Ψs(Hs
Decoder and header. After collaboration, each agent decodes the updated BEV-based feature map.
The decoded feature map is Ms i). To implement the decoder Ψs(·), we progressively up-sample Hs i with four layers, where each layer ﬁrst concatenates the previous feature map with the corresponding feature map in the encoder and then uses a 1 × 1 convolutional operation to halve the number of channels. Finally, we use an output header to generate the ﬁnal detection outputs, (cid:98)Ys i). To implement the header Ψs(·), we use two branches of convolutional layers to classify the foreground-background categories and regress the bounding boxes. i ← Φs(Ms 2.2 Teacher: Early collaboration
During the training phase, an early-collaboration model, as the teacher, is introduced to guide the intermediate-collaboration model, which is a student. Similar to the student model, the teacher’s pipeline has the feature encoder Θt, feature decoder Ψt and the output header Φt. Note that all the agents share the same teacher model to guide one student model; however, each agent provides the inputs with its own pose, and its inputs to both the teacher and student models should be well aligned. 1We consider a fully-connected bidirectional graph, and the weights for both directions are distinct. 4
Feature encoder. Let X = A(ξ1 ◦ X1, ξ2 ◦ X2, ..., ξM ◦ XM ) be a holistic-view 3D point cloud that aggregates all the points from all M agents in the global coordinate system, where A(·, ..., ·) is the aggregation operator of multiple 3D point clouds, and ξi and Xi are the pose and the 3D point cloud of the ith agent, respectively. To ensure the inputs to the teacher model and the student model are aligned, we transform the holistic-view point cloud X to an agent’s own coordinate based on the pose information. Now, for the ith agent, the input to the teacher model ξ−1
◦ X and the input to the student model Xi are in the same coordinate system. Similarly to the feature encoder in the student model, we convert the 3D point cloud ξ−1
◦ X to a BEV map and use 2D convolutions to obtain the i ∈ R ¯K× ¯K× ¯C. Here we crop the BEV map to feature map of the ith agent in the teacher model, Ht ensure it has the same spatial range and resolution with the BEV map in the student model.
Decoder and header. Similarly to the decoder and header in the student model, we adopt Mt
Ψt(Ht foreground-background categories and regressed bounding boxes. i) to obtain the decoded BEV-based feature map and (cid:98)Yt i ← i) to obtain the predicted i ← Φt(Mt i i
Teacher training scheme. As in common teacher-student frameworks, we train the teacher model separately. We employ the binary cross-entropy loss to supervise foreground-background classiﬁ-cation and the smooth L1 loss to supervise the bounding-box regression. Overall, we minimize the loss function Lt = (cid:80)M i=1 Ldet(Yt i ), where classiﬁcation and regression losses are collectively i = Ys denoted as Ldet and Yt i is the ground-truth detection in the perception region of the ith agent. 2.3 System training with knowledge distillation
Given a well-trained teacher model, we use both detection loss and knowledge distillation loss to supervise the training of the student model. We consider minimizing the following loss i , (cid:98)Yt
Ls =
M (cid:88) (cid:16) i=1
Ldet(Ys i , (cid:98)Ys i ) + λkdLkd(Hs i, Ht i) + λkdLkd(Ms i, Mt i) (cid:17)
.
The detection loss Ldet is similar to that of the teacher, including both foreground-background classiﬁcation loss and the bounding box regression loss, pushing the detection result of each agent to be close to its local ground-truth. The second and third terms form a knowledge distillation loss, regularizing the student model to generate similar feature maps with the teacher model. The hyperparameter λkd controls the weight of the knowledge distillation loss Lkd deﬁned as follows
Lkd(Hs i, Ht i) =
DKL (cid:0)σ (cid:0)(cid:0)Ht i (cid:1) n (cid:1) ||σ ((Hs i)n)(cid:1) ,
¯K× ¯K (cid:88) n=1 i, Mt i)n and (Ht i) can be introduced to enhance the regularization. where DKL(p(x)||q(x)) denotes the Kullback-Leibler (KL) divergence of distribution q(x) from distribution p(x), σ(·) indicates the softmax operation of the feature vector along the channel dimension, and (Hs i)n denote the feature vectors at the nth cell of the ith agent’s feature map in the student model and teacher model, respectively. Similarly, the loss on decoded feature maps Lkd(Ms
Remark. As mentioned in Section 2.1, the feature maps output by the collaboration graph process (cid:1). Intuitively, the feature map of i}M in the student model is computed by {Hs i=1 ← MGΠ i=1 the teacher model {Ht i=1 would be the desired output of the collaboration graph process MGΠ (·).
Therefore, we constrain all the post-collaboration feature maps in the student model to match the correspondences in the teacher model through knowledge distillation. This constraint would further regularize the upfront trainable components: i) the distilled student encoder Θs, which abstracts the features from raw measurements and produces the input to MGΠ (·), and ii) the edge-weight encoder Π in the distilled collaboration graph. Consequently, through knowledge distillation and back-propagation, the distilled student encoder would learn to abstract informative features from raw data for better collaboration; and the distilled edge-weight encoder would learn how to control the collaboration based on agents’ features. In a word, our distilled collaboration network (DiscoNet) can comprehend feature abstraction and fusion via the proposed knowledge distillation framework. (cid:0){Fs i}M i}M 3