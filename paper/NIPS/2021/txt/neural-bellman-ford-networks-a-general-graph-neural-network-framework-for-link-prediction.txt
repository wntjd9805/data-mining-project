Abstract
Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and ﬂexible representation learning framework based on paths for link prediction. Speciﬁcally, we deﬁne the representation of a pair of nodes as the generalized sum of all path representations between the nodes, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efﬁciently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford
Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm.
The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely INDICATOR, MESSAGE and AGGREGATE functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively1. The NBFNet covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results2. 1

Introduction
Predicting the interactions between nodes (a.k.a. link prediction) is a fundamental task in the ﬁeld of graph machine learning. Given the ubiquitous existence of graphs, such a task has many applications, such as recommender system [34], knowledge graph completion [41] and drug repurposing [27].
Traditional methods of link prediction usually deﬁne different heuristic metrics over the paths between a pair of nodes. For example, Katz index [30] is deﬁned as a weighted count of paths between two nodes. Personalized PageRank [42] measures the similarity of two nodes as the random walk probability from one to the other. Graph distance [37] uses the length of the shortest path between two nodes to predict their association. These methods can be directly applied to new graphs, i.e., inductive setting, enjoy good interpretability and scale up to large graphs. However, they are designed based on handcrafted metrics and may not be optimal for link prediction on real-world graphs. 1Unless stated otherwise, we use summation and multiplication to refer the generalized operators in the path formulation, rather than the basic operations of arithmetic. 2Code is available at https://github.com/DeepGraphLearning/NBFNet 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
To address these limitations, some link prediction methods adopt graph neural networks (GNNs) [32, 48, 59] to automatically extract important features from local neighborhoods for link prediction.
Thanks to the high expressiveness of GNNs, these methods have shown state-of-the-art performance.
However, these methods can only be applied to predict new links on the training graph, i.e. transduc-tive setting, and lack interpretability. While some recent methods [73, 55] extract features from local subgraphs with GNNs and support inductive setting, the scalability of these methods is compromised.
Therefore, we wonder if there exists an approach that enjoys the advantages of both traditional path-based methods and recent approaches based on graph neural networks, i.e., generalization in the inductive setting, interpretability, high model capacity and scalability.
In this paper, we propose such a solution. Inspired by traditional path-based methods, our goal is to develop a general and ﬂexible representation learning framework for link prediction based on the paths between two nodes. Speciﬁcally, we deﬁne the representation of a pair of nodes as the generalized sum of all the path representations between them, where each path representation is deﬁned as the generalized product of the edge representations in the path. Many link prediction methods, such as Katz index [30], personalized PageRank [42], graph distance [37], as well as graph theory algorithms like widest path [4] and most reliable path [4], are special instances of this path formulation with different summation and multiplication operators. Motivated by the polynomial-time algorithm for the shortest path problem [5], we show that such a formulation can be efﬁciently solved via the generalized Bellman-Ford algorithm [4] under mild conditions and scale up to large graphs.
The operators in the generalized Bellman-Ford algorithm—summation and multiplication—are handcrafted, which have limited ﬂexibility. Therefore, we further propose the Neural Bellman-Ford
Networks (NBFNet), a graph neural network framework that solves the above path formulation with learned operators in the generalized Bellman-Ford algorithm. Speciﬁcally, NBFNet parameterizes the generalized Bellman-Ford algorithm with three neural components, namely INDICATOR, MESSAGE and AGGREGATE functions. The INDICATOR function initializes a representation on each node, which is taken as the boundary condition of the generalized Bellman-Ford algorithm. The MESSAGE and the AGGREGATE functions learn the multiplication and summation operators respectively.
We show that the MESSAGE function can be deﬁned according to the relational operators in knowledge graph embeddings [6, 68, 58, 31, 52], e.g., as a translation in Euclidean space induced by the relational operators of TransE [6]. The AGGREGATE function can be deﬁned as learnable set aggregation functions [71, 65, 9]. With such parameterization, NBFNet can generalize to the inductive setting, meanwhile achieve one of the lowest time complexity among inductive GNN methods. A comparison of NBFNet and other GNN frameworks for link prediction is showed in Table 1. With other instantiations of MESSAGE and AGGREGATE functions, our framework can also recover some existing works on learning logic rules [69, 46] for link prediction on knowledge graphs (Table 2).
Our NBFNet framework can be applied to several link prediction variants, covering not only single-relational graphs (e.g., homogeneous graphs) but also multi-relational graphs (e.g., knowledge graphs). We empirically evaluate the proposed NBFNet for link prediction on homogeneous graphs and knowledge graphs in both transductive and inductive settings. Experimental results show that the proposed NBFNet outperforms existing state-of-the-art methods by a large margin in all settings, with an average relative performance gain of 18% on knowledge graph completion (HITS@1) and 22% on inductive relation prediction (HITS@10). We also show that the proposed NBFNet is indeed interpretable by visualizing the top-k relevant paths for link prediction on knowledge graphs.
Table 1: Comparison of GNN frameworks for link prediction. The time complexity refers to the amortized time for predicting a single edge or triplet. |V| is the number of nodes, |E| is the number of edges, and d is the dimension of representations. The wall time is measured on FB15k-237 test set with 40 CPU cores and 4 GPUs. We estimate the wall time of GraIL based on a downsampled test set.
Method
Inductive3
Interpretable Learned Representation Time Complexity Wall Time
VGAE [32] /
RGCN [48]
NeuralLP [69] /
DRUM [46]
SEAL [73] /
GraIL [55]
NBFNet (cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
O(d)
|V| + d2(cid:17) (cid:16) |E|d
O(|E|d2) (cid:16) |E|d
|V| + d2(cid:17)
O
O 18 secs 2.1 mins
≈1 month 4.0 mins (cid:88) (cid:88) (cid:88) 2
2