Abstract
Value factorization is a popular and promising approach to scaling up multi-agent reinforcement learning in cooperative settings, which balances the learning scala-bility and the representational capacity of value functions. However, the theoretical understanding of such methods is limited. In this paper, we formalize a multi-agent ﬁtted Q-iteration framework for analyzing factorized multi-agent Q-learning.
Based on this framework, we investigate linear value factorization and reveal that multi-agent Q-learning with this simple decomposition implicitly realizes a pow-erful counterfactual credit assignment, but may not converge in some settings.
Through further analysis, we ﬁnd that on-policy training or richer joint value func-tion classes can improve its local or global convergence properties, respectively.
Finally, to support our theoretical implications in practical realization, we conduct an empirical analysis of state-of-the-art deep multi-agent Q-learning algorithms on didactic examples and a broad set of StarCraft II unit micromanagement tasks. 1

Introduction
Cooperative multi-agent reinforcement learning (MARL) has great promise for addressing coordi-nation problems in a variety of applications, such as robotic systems [1], autonomous cars [2], and sensor networks [3]. Such complex tasks often require MARL to learn decentralized policies for agents to jointly optimize a global cumulative reward signal, which posts a number of challenges, including multi-agent credit assignment [4, 5], non-stationarity [6, 7], and scalability [3, 8]. Re-cently, by leveraging the strength of deep learning techniques, cooperative MARL has made great progress [9–19], particularly in value-based methods that demonstrate state-of-the-art performance on challenging tasks such as StarCraft unit micromanagement [20].
Value factorization is a popular approach to effectively scaling up cooperative multi-agent Q-learning in complex domains. One fundamental problem of factorized multi-agent algorithms is the trade-off between the learning scalability and the representational capacity of value functions. As a representative method, value-decomposition network (VDN) [10] is proposed to obtain excellent scalability based on a popular paradigm called centralized training with decentralized execution (CTDE) [21], which learns a centralized but factorizable joint value function Qtot represented as the summation of individual value functions Qi. During the execution, decentralized policies can be easily derived by greedily selecting individual actions from the local value function Qi. An implicit multi-agent credit assignment is realized because Qi is learned by neural network backpropagation from
∗Equal contribution.
†Work done while Zhizhou was an undergraduate at Tsinghua University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the total temporal-difference error on the single global reward signal. This linear value factorization structure can signiﬁcantly improve the scalability of multi-agent joint policy training and individual policy execution. This linear value factorization adopted by VDN also realizes a sufﬁcient condition for an important principle of CTDE, the IGM (Individual-Global-Max) principle [13], which asserts the consistency between joint and individual greedy action selection and ensures the consistent policy learning under the CTDE paradigm. Following this principle, most recent advances focus on enriching the function expressiveness of the factorization from Qtot to Qi. QMIX [12] uses a monotonic function to represent the joint value function Qtot through local values Qi, whose function expressiveness is further improved by QTRAN [13] and QPLEX [19] that aim to achieve full function expressiveness induced by the IGM principle. These approaches show promise and achieve state-of-the-art performance in complicated cooperative multi-agent domains.
Despite these impressive empirical successes, theoretical understandings for these MARL approaches are still limited. To bridge this gap, this paper is the ﬁrst to consider a general framework for theoretical studies. We formulate Factorzed Multi-Agent Fitted Q-Iteration (FMA-FQI) for formally analyzing cooperative MARL with value factorization. This framework generalizes single-agent
Fitted Q-Iteration, a popular model for studying Q-learning algorithms with function approximation
[22–25]. FMA-FQI models the iterative training procedure of multi-agent Q-learning using empirical
Bellman error minimization. Under this framework, we investigate two popular value factorization methods: the linear factorization used by VDN [10], and the IGM factorization adopted by QTRAN
[13] and QPLEX [19]. Our analyses reveal algorithmic properties of these approaches and provide insights on fundamental questions: Why do they work? Do they converge? When may they not perform well?
The main contribution of this paper can be summarized as follows: 1. We formalize Factorized Multi-Agent Fitted Q-Iteration (FMA-FQI) as a general theoretical framework for analyzing cooperative multi-agent Q-learning with value factorization. 2. Based on FMA-FQI, we study linear value factorization and derive a closed-form solution to its Bellman error minimization. We then reveal two novel insights that: 1) linear value factorization implicitly realizes a powerful counterfactual credit assignment and 2) on-policy training is beneﬁcial to its stability and local convergence near the optimal solutions. 3. Using FMA-FQI, we also study IGM value factorization and prove its global convergence and optimality. 4. Empirical analysis is conducted to connect our theoretical implications to practical scenarios by evaluating state-of-the-art deep MARL approaches. 2