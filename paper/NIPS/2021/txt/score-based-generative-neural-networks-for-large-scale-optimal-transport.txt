Abstract
We consider the fundamental problem of sampling the optimal transport coupling between given source and target distributions. In certain cases, the optimal transport plan takes the form of a one-to-one mapping from the source support to the target support, but learning or even approximating such a map is computationally challenging for large and high-dimensional datasets due to the high cost of linear programming routines and an intrinsic curse of dimensionality. We study instead the Sinkhorn problem, a regularized form of optimal transport whose solutions are couplings between the source and the target distribution. We introduce a novel framework for learning the Sinkhorn coupling between two distributions in the form of a score-based generative model. Conditioned on source data, our procedure iterates Langevin Dynamics to sample target data according to the regularized optimal coupling. Key to this approach is a neural network parametrization of the
Sinkhorn problem, and we prove convergence of gradient descent with respect to network parameters in this formulation. We demonstrate its empirical success on a variety of large scale optimal transport tasks. 1

Introduction
It is often useful to compare two data distributions by computing a distance between them in some appropriate metric. For instance, statistical distances can be used to ﬁt the parameters of a distribution to match some given data. Comparison of statistical distances can also enable distribution testing, quantiﬁcation of distribution shifts, and provide methods to correct for distribution shift through domain adaptation [12].
Optimal transport theory provides a rich set of tools for comparing distributions in Wasserstein
Distance. Intuitively, an optimal transport plan from a source distribution σ ∈ M+(X ) to a target distribution τ ∈ M+(Y) is a blueprint for transporting the mass of σ to match that of τ as cheaply as possible with respect to some ground cost. Here, X and Y are compact metric spaces and M+(X ) denotes the set of positive Radon measures over X , and it is assumed that σ, τ are supported over all of X , Y respectively. The Wasserstein Distance between two distributions is deﬁned to be the cost of an optimal transport plan.
Because the ground cost can incorporate underlying geometry of the data space, optimal transport plans often provide a meaningful correspondence between points in X and Y. A famous example is given by Brenier’s Theorem, which states that, when X , Y ⊆ Rd and σ, τ have ﬁnite variance, the optimal transport plan under a squared-l2 ground cost is realized by a map T : X → Y [26,
Theorem 2.12]. However, it is often computationally challenging to exactly compute optimal transport plans, as one must exactly solve a linear program requiring time which is super-quadratic in the size of input datasets [5].
∗Work done while an Instructor in Applied Mathematics at MIT. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: We use SCONES to sample the mean-squared-L2 cost, entropy regularized optimal transport mapping between 2x downsampled CelebA images (Source) and unmodiﬁed CelebA images (Target) at λ = 0.005 regularization.
Instead, we opt to study a regularized form of the optimal transport problem whose solution takes the form of a joint density π(x, y) with marginals πX (x) = σ(x) and πY (y) = τ (y). A correspondence between points is given by the conditional distribution πY |X=x(y), which relates each input point to a distribution over output points.
In recent work [22], the authors propose a large-scale stochastic dual approach in which π(x, y) is parametrized by two continuous dual variables that may be represented by neural networks and trained at large-scale via stochastic gradient ascent. Then, with access to π(x, y), they approximate an optimal
EπY |X=x [d(y, Y )], where transport map using a barycentric projection of the form T : x (cid:55)→ arg miny d : Y × Y → R is a convex cost on Y. Their method is extended by [15] to the problem of learning regularized Wasserstein barycenters. In both cases, the Barycentric projection is observed to induce averaging artifacts such as those shown in Figure 2.
Instead, we propose a direct sampling strategy to generate samples from πY |X=x(y) using a score-based generative model. Score-based generative models are trained to sample a generic probability density by iterating a stochastic dynamical system knows as Langevin dynamics [24]. In contrast to projection methods for large-scale optimal transport, we demonstrate that pre-trained score based generative models can be naturally applied to the problem of large-scale regularized optimal transport.
Our main contributions are as follows: 1. We show that pretrained score based generative models can be easily adapted for the purpose of sampling high dimensional regularized optimal transport plans. Our method eliminates the need to estimate a barycentric projection and it results in sharper samples because it eliminates averaging artifacts incurred by such a projection. 2. Score based generative models have been used for unconditional data generation and for conditional data generation in settings such as inpainting. We demonstrate how to adapt pretrained score based generative models for the more challenging conditional sampling problem of regularized optimal transport. 3. Our method relies on a neural network parametrization of the dual regularized optimal transport problem. Under assumptions of large network width, we prove that gradient descent w.r.t. neural network parameters converges to a global maximizer of the dual problem. We also prove optimization error bounds based on a stability analysis of the dual problem. 4. We demonstrate the empirical success of our method on a synthetic optimal transport task and on optimal transport of high dimensional image data. 2