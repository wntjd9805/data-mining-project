Abstract
We theoretically study the fundamental problem of learning a single neuron with a bias term (x (cid:55)→ σ((cid:104)w, x(cid:105) + b)) in the realizable setting with the ReLU activation, using gradient descent. Perhaps surprisingly, we show that this is a signiﬁcantly different and more challenging problem than the bias-less case (which was the focus of previous works on single neurons), both in terms of the optimization geometry as well as the ability of gradient methods to succeed in some scenarios.
We provide a detailed study of this problem, characterizing the critical points of the objective, demonstrating failure cases, and providing positive convergence guarantees under different sets of assumptions. To prove our results, we develop some tools which may be of independent interest, and improve previous results on learning single neurons. 1

Introduction
Learning a single ReLU neuron with gradient descent is a fundamental primitive in the theory of deep learning, and has been extensively studied in recent years. Indeed, in order to understand the success of gradient descent on complicated neural networks, it seems reasonable to expect a satisfying analysis of convergence on a single neuron. Although many previous works studied the problem of learning a single neuron with gradient descent, none of them considered this problem with an explicit bias term.
In this work, we study the common setting of learning a single neuron with respect to the squared loss, using gradient descent. We focus on the realizable setting, where the inputs are drawn from a distribution D on Rd+1, and are labeled by a single target neuron of the form x (cid:55)→ σ((cid:104)v, x(cid:105)), where σ : R → R is some non-linear activation function. To capture the bias term, we assume that the distribution D is such that its ﬁrst d components are drawn from some distribution ˜D on
Rd, and the last component is a constant 1. Thus, the input x can be decomposed as (˜x, 1) with
˜x ∼ ˜D, the vector v can be decomposed as (˜v, bv), where ˜v ∈ Rd and bv ∈ R, and the target neuron computes a function of the form x (cid:55)→ σ((cid:104)˜v, ˜x(cid:105) + bv). Similarly, we can deﬁne the learned neuron as x (cid:55)→ σ((cid:104) ˜w, ˜x(cid:105) + bw), where w = ( ˜w, bw). Overall, we can write the objective function we wish to
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
optimize as follows:
F (w) := E x∼D
= E
˜x∼ ˜D (cid:0)σ(w(cid:62)x) − σ(v(cid:62)x)(cid:1)2(cid:21) (cid:0)σ( ˜w(cid:62) ˜x + bw) − σ(˜v(cid:62) ˜x + bv)(cid:1)2(cid:21)
. (cid:20) 1 2 (cid:20) 1 2 (1) (2)
Throughout the paper we consider the commonly used ReLU activation function: σ(x) = max{0, x}.
Although the problem of learning a single neuron is well studied (e.g. Soltanolkotabi [2017], Yehudai and Shamir [2020], Frei et al. [2020], Du et al. [2017], Kalan et al. [2019], Tan and Vershynin [2019],
Mei et al. [2018], Oymak and Soltanolkotabi [2018]), none of the previous works considered the problem with an additional bias term. Moreover, previous works on learning a single neuron with gradient methods have certain assumptions on the input distribution D, which do not apply when dealing with a bias term (for example, a certain "spread" in all directions, which does not apply when
D is supported on {1} in the last coordinate).
Since neural networks with bias terms are the common practice, it is natural to ask how adding a bias term affects the optimization landscape and the convergence of gradient descent. Although one might conjecture that this is just a small modiﬁcation to the problem, we in fact show that the effect of adding a bias term is very signiﬁcant, both in terms of the optimization landscape and in terms of which gradient descent strategies can or cannot work. Our main contributions are as follows:
• We start in Section 3 with some negative results, which demonstrate how adding a bias term makes the problem more difﬁcult. In particular, we show that with a bias term, gradient descent or gradient
ﬂow2 can sometimes fail with probability close to half over the initialization, even when the input distribution is uniform over a ball. In contrast, Yehudai and Shamir [2020] show that without a bias term, for the same input distribution, gradient ﬂow converges to the global minimum with probability 1.
• In Section 4 we give a full characterization of the critical points of the loss function. We show that adding a bias term changes the optimization landscape signiﬁcantly: In previous works (cf. Yehudai and Shamir [2020]) it has been shown that under mild assumptions on the input distribution, the only critical points are w = v (i.e., the global minimum) and w = 0. We prove that when we have a bias term, the set of critical points has a positive measure, and that there is a cone of local minima where the loss function is ﬂat.
• In Sections 5 and 6 we show that gradient descent converges to the global minimum at a linear rate, under some assumptions on the input distribution and on the initialization. We give two positive convergence results, where each result is under different assumptions, and thus the results complement each other. We also use different techniques for proving each of the results: The analysis in Section 6 follows from some geometric arguments and extends the technique from
Yehudai and Shamir [2020], Frei et al. [2020]. The analysis in Section 5 introduces a novel technique, not used in previous works on learning a single neuron, and has a more algebraic nature. Moreover, that analysis implies that under mild assumptions, gradient descent with random initialization converges to the global minimum with probability 1 − eΩ(d).
• The best known result for learning a single neuron without bias using gradient descent for an input distribution that is not spherically symmetric, establishes convergence to the global minimum with probability close to 1 2 over the random initialization [Yehudai and Shamir, 2020, Frei et al., 2020].
With our novel proof technique presented in Section 5 this result can be improved to probability at least 1 − e−Ω(d) (see Remark 5.7).