Abstract
Model-agnostic meta-reinforcement learning requires estimating the Hessian matrix of value functions. This is challenging from an implementation perspective, as repeatedly differentiating policy gradient estimates may lead to biased Hessian estimates. In this work, we provide a unifying framework for estimating higher-order derivatives of value functions, based on off-policy evaluation. Our framework interprets a number of prior approaches as special cases and elucidates the bias and variance trade-off of Hessian estimates. This framework also opens the door to a new family of estimates, which can be easily implemented with auto-differentiation libraries, and lead to performance gains in practice. We open source the code to reproduce our results1. 1

Introduction
Recent years have witnessed the success of reinforcement learning (RL) in challenging domains such as game playing [1], board games [2], and robotics control [3]. However, despite such breakthroughs, state-of-the-art RL algorithms are still plagued by sample inefficiency, as training agents requires orders of magnitude more samples than humans would experience to reach similar levels of perfor-mance [1, 2]. One hypothesis on the source of such inefficiencies is that standard RL algorithms are not good at leveraging prior knowledge, which implies that whenever presented with a new task, the algorithms must learn from scratch. On the contrary, humans are much better at transferring prior skills to new scenarios, an innate ability arguably obtained through evolution over thousand of years.
Meta-reinforcement learning (meta-RL) formalizes the learning and transfer of prior knowledge in
RL [4]. The high-level idea is to have an RL agent that interacts with a distribution of environments at meta-training time. The objective is that at meta-testing time, when the agent interacts with previously unseen environments, it can learn much faster than meta-training time. Here, faster learning is measured by the number of new samples needed to achieve a good level of performance.
If an agent can achieve good performance at meta-testing time, it embodies the ability to transfer knowledge from prior experiences at meta-training time. meta-RL algorithms can be constructed in many ways, such as those based on recurrent memory [5, 6], gradient-based adaptations [4], learning loss functions [7–9], probabilistic inference of context variables [10–12], online adaptation of hyper-parameters within a single lifetime [13, 14] and so on. Some of these formulations have mathematical connections; see e.g. [15] for an in-depth discussion.
We focus on gradient-based adaptations [4], where the agent carries out policy gradient updates [16] at both meta-training and meta-testing time. Conceptually, since meta-RL seeks to optimize the 1https://github.com/robintyh1/neurips2021-meta-gradient-offpolicy-evaluation 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Interpretation of prior work on higher-order derivative estimations as special instances of differentiating off-policy evaluation estimates. Given any off-policy evaluation estimate (cid:98)V πθ from the second row, we recover higher-order derivative estimates in prior work in the first row, by differentiating through the estimate ∇K
θ (cid:98)V πθ .
Off-policy evaluation estimates
STEP-WISE
IS [26]
Prior work
DiCE [21]
DOUBLY-ROBUST [27]
Loaded DiCE
[22, 23]
TAYPO-1 [28, 29]
TAYPO-2 [29]
LVC [24]
Second-order (this work) way in which the agent adapts itself in face of new environments, it needs to differentiate through the policy gradient update itself. This effectively reduces the meta-RL problem into estimations of
Hessian matrices of value functions.
Challenges for computing Hessian matrix of value functions. To calculate Hessian matrices (or unbiased estimates thereof) for supervised learning objectives, it suffices to differentiate through gradient estimates, which can be easily implemented with auto-differentiation packages [17–19].
However, it is not the case for value functions in RL. Intuitively, this is because value functions are defined via expectations with respect to distributions that themselves depend on the policy parameters of interest, whereas in supervised learning the expectations are defined with respect to a fixed data distribution. As a result, implementations that do not take this into account may lead to estimates
[4] whose bias is not properly characterized and might have a negative impact on downstream applications [20].
Motivated by this observation, a number of prior works suggest implementation alternatives that lead to unbiased Hessian estimates [21], with potential variance reduction [22, 23, 20]; or biased estimates with small variance [24]. However, different algorithms in this space are motivated and derived in seemingly unrelated ways: for example, [21–23] derive code-level implementations within the general context of stochastic computation graphs [25]. On the other hand, [24] derives the estimates by explicitly analyzing certain terms with potentially high variance, which naturally produces bias in the final estimate. Due to apparently distinct ways of deriving estimates, it is not immediately clear how all such methods are related, and whether there could be other alternatives.
Central contribution. We present a unified framework for estimating higher-order derivatives of value functions, based on the concept of off-policy evaluation. The main insights are summarized in
Table 1, where most aforementioned prior work can be interpreted as special cases of our framework.
Our framework has a few advantages: (1) it conceptually unifies a few seemingly unrelated prior methods; (2) it elucidates the bias and variance trade-off of the estimates; (3) it naturally produces new methods based on Taylor expansions of value functions [29].
After a brief background introduction on meta-RL in Section 2, we will discuss the above aspects in detail in Section 3. From an implementation perspective, we will show in Section 4.1 that both the general framework and the new method can be conveniently implemented in auto-differentiation libraries [30, 19], making it amenable in a practical setup. Finally in Section 5, we validate important claims based on this framework with experimental insights. 2