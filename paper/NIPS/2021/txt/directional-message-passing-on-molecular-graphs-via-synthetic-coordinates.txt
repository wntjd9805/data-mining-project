Abstract
Graph neural networks that leverage coordinates via directional message passing have recently set the state of the art on multiple molecular property prediction tasks. However, they rely on atom position information that is often unavailable, and obtaining it is usually prohibitively expensive or even impossible. In this paper we propose synthetic coordinates that enable the use of advanced GNNs without requiring the true molecular conﬁguration. We propose two distances as synthetic coordinates: Distance bounds that specify the rough range of molecular conﬁgurations, and graph-based distances using a symmetric variant of personalized
PageRank. To leverage both distance and angular information we propose a method of transforming normal graph neural networks into directional MPNNs. We show that with this transformation we can reduce the error of a normal graph neural network by 55 % on the ZINC benchmark. We furthermore set the state of the art on ZINC and coordinate-free QM9 by incorporating synthetic coordinates in the
SMP and DimeNet++ models. Our implementation is available online. 1 1

Introduction
Graph neural networks (GNNs) have set the state of the art on many tasks of molecular machine learning, such as the prediction of quantum mechanical properties (Gilmer et al., 2017), solubility (Wu et al., 2018), or the generation of new molecules (Jin et al., 2020). Thanks to their fast inference time, good generalization and scalability, GNNs are thus promising to revolutionize large parts of chemistry, from ab-initio quantum mechanical simulations and reaction kinetics to synthesis planning and drug discovery. Atom positions are central to many of these tasks, but unavailable in most cases.
Many tasks in chemistry instead use a more coarse-grained representation: The molecular graph.
Unfortunately, this representation makes many predictive tasks substantially harder, and GNNs have performed signiﬁcantly better when they have access to the exact molecular conﬁguration (Gilmer et al., 2017). Missing atom positions furthermore preclude the use of many advanced GNNs that were developed with coordinates in mind.
In this work we aim to ﬁll in this information with well-deﬁned coordinates constructed purely from the molecular graph. Regular approximation methods for generating atom positions often do not beneﬁt model performance, due to the fundamental ambiguity of molecular conﬁgurations. The energy landscape of molecules can have multiple local minima, and a molecule can be in any of multiple different minima, known as conformers. In this work, we propose to circumvent this problem by incorporating the conformational ambiguity via empirical distance bounds. Instead of yielding a potentially wrong conﬁguration, these bounds only estimate the range of viable molecular geometries.
They are thus valid regardless of which state the molecule was in when generating the data. 1https://www.daml.in.tum.de/synthetic-coordinates 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Molecular graph
GNN
Pairwise distances
Featurized line graph
Distances and angles
Directional MPNN
Figure 1: Illustration of transforming a regular molecular graph (ethanol) to a line graph with synthetic coordinates. We ﬁrst calculate all (bounds of) pairwise distances using our synthetic coordinates.
We then calculate the (bounds of) distances and angles for the molecular graph. Finally, we convert the molecular graph to its line graph and embed the distances and angles as features. This process allows us to convert a regular GNN to a directional MPNN, which improves its accuracy and allows to incorporate angular information.
A molecular conﬁguration is fully speciﬁed by the pairwise distances between all atoms, due to rotational, translational, and reﬂectional invariance. We can thus obtain a molecular geometry from any method that provides pairwise distances between atoms. Since directional message passing does not require the full molecular geometry, these distances do not need to correspond to an actual three-dimensional conﬁguration. We leverage this generality and propose purely graph-based distances calculated from a symmetric variant of personalized PageRank (PPR) as a second set of coordinates.
This distance performs surprisingly well, despite incorporating no chemical knowledge. Both the distance bounds and the symmetric PPR distance require no hand-tuning and can be calculated efﬁciently, even for large molecules.
We leverage these two variants of synthetic coordinates to transform regular GNNs into directional message passing, as illustrated in Fig. 1. We ﬁrst calculate the synthetic, pairwise distances for the given molecular graph. Based on these, we calculate the edge distances and angles between edges.
Finally, we compute the molecule’s line graph. Executing a GNN on the line graph improves its expressivity (Garg et al., 2020) and allows us to incorporate angular information. We use the original node and edge attributes together with the distances as node attributes, and the obtained angles as edge attributes. The GNN is then executed on this featurized line graph instead of the original graph. Our experiments show this transformation can signiﬁcantly improve the performance of the underlying
GNN, across multiple models and datasets. Incorporating synthetic coordinates reduces the error of a normal GNN by 55 %, putting it on par with the current state of the art. Our enhanced version of the
SMP model (Vignac et al., 2020) improves upon the current state of the art on ZINC by 21 %, and
DimeNet++ (Gasteiger et al., 2020a) with synthetic coordinates outcompetes previous methods on coordinate-free QM9 by 20 %. In summary, our core contributions are:
• Well-deﬁned synthetic coordinates based on node distances and simple molecular bounds, which signiﬁcantly improve the performance of GNNs for molecules.
• A general scheme of converting a normal GNN into a directional MPNN, which can improve performance and allows incorporating both distance and angular information. 2 Directional message passing
Graph neural networks. To use GNNs for molecules we represent them as graphs G = (V, E), where the atoms deﬁne the node set V and the interactions the edge set E. These interactions are usually the bonds of the molecular graph, but they can also be all atoms pairs within a given cutoff of e.g. 5 Å. In this work we focus on an extension of message passing neural networks (MPNNs) (Gilmer et al., 2017). MPNNs embed each atom u separately as hu ∈ RH , and can additionally use interaction embeddings e(uv) ∈ RHe . These embeddings are updated in each layer using messages passed between neighboring nodes, starting with the atom features h(0) (e.g. its type) and the interaction features e(0) (uv) (e.g. the bond type or a distance representation). Extended (uv) = x(E) u = x(V) u 2
MPNNs can be expressed via the following two equations: h(l+1) u
= fupdate(h(l)
[fmsg(h(l) u , h(l) v , e(l) (uv))]), e(l+1) (uv) = fedge(h(l+1) u
, e(l) (uv)). u , Agg v∈Nu
, h(l+1) v (1) (2)
The atom and interaction update functions fnode and fedge and the message function fmsg are learnable functions, such as simple linear layers or arbitrarily complex neural networks. The aggregation Agg over the atom’s neighbors Nu is usually a simple summation.
Line graph. The directed line graph L(G) = (VL, EL) expresses the adjacencies between the directed edges in G. Its nodes are the directed edges of the original graph VL = {(u, v) | u ∈ V, v ∈ Nu}.
For undirected graphs like molecular graphs, every undirected edge {u, v} is split into two directed edges (u, v) and (v, u). Two nodes in L(G) are connected if the corresponding edges in G share a node, i.e. EL = {((u, v), (v, w)) | (u, v), (v, w) ∈ VL}. We obtain node features for the line graph by embedding the original node and edge features as x(VL) (uv)). The line graph can furthermore incorporate additional features for atom triplets as edge features x(EL) (uvw), such as the angle between bonds or interactions. (uv) = femb(x(V) u , x(V)
, x(E) v
Directional message passing. Directional MPNNs improve upon regular MPNNs in two ways. First, they embed the directed messages instead of the nodes in the graph, essentially operating on the directed line graph. Models using only this ﬁrst step are also known as directed MPNNs or line graph neural networks (Dai et al., 2016; Yang et al., 2019; Chen et al., 2019). Directed MPNNs are strictly more expressive than regular MPNNs (Morris et al., 2020). We can transform any MPNN to a directed MPNN simply by executing it on the directed line graph instead of the original graph.
Second, for graphs with nodes that are embedded in an inner product space (such as molecules in 3D space) the directed edges correspond to directions in that space, via x(VL)
. Directional
MPNNs leverage this connection to better represent the molecular conﬁguration, usually by using the angles in x(EL) (uvw) (Gasteiger et al., 2020b). To fully leverage both aspects of directional MPNNs we therefore need some form of coordinates. (uv) = x(V) u − x(V) v
Expressivity of GNNs. A central limitation of GNNs is their inability of distinguishing between certain non-isomorphic graphs. For example, GNNs are not able to distinguish between a hexagon and two triangles if all nodes and edges have the same features. More speciﬁcally, Xu et al. (2019);
Morris et al. (2019) have shown that GNNs are only as powerful as the 1-Weisfeiler-Lehman (WL) test of isomorphism. While it is still possible to construct indistinguishable examples for directional
MPNNs, this is signiﬁcantly more difﬁcult (Garg et al., 2020). Dym & Maron (2021) have shown that MPNNs using SO(3) group representations and atom positions are even universal, i.e. able to approximate any continuous function to arbitrary precision. This demonstrates that coordinates can alleviate and even solve this central limitation of GNNs. 3 Molecular conﬁgurations
To prevent any pitfalls when constructing synthetic coordinates for GNNs based on chemical knowl-edge we ﬁrst need to consider the properties of atomic positions in a molecule and how they are obtained. At ﬁrst glance these positions might seem like an obvious and straightforward property.
However, molecular conﬁgurations are actually ambiguous and difﬁcult to obtain, even for small molecules. This misconception has even led some works to suggest semi-supervised learning meth-ods leveraging positions, effectively treating them as abundant input features (Hao et al., 2020). To clarify this issue we will next describe the complexity behind molecular conﬁgurations and how to approximate them efﬁciently.
Finding molecular conﬁgurations. The atoms of a molecule can in principle be at any arbitrary position. However, most of these conﬁgurations will lead to an extremely high energy and are thus very unlikely to be observed in nature. A molecular conﬁguration thus usually refers to the atom positions at or close to equilibrium, i.e. at the molecule’s energy minimum. To ﬁnd these positions we have to search the molecule’s energy landscape and solve a non-convex optimization problem. This is in fact a bilevel optimization problem, where the atom positions are optimized in the outer and the electron wavefunctions in the inner task. These wave functions can then be ignored in the outer task; 3
they only inﬂuence the energy and the forces Fi = − ∂E acting on each atomic nucleus. We can
∂xi then use these forces for gradient-based optimization, and avoid saddle points by using quasi-Newton methods.
Difﬁculties. The above optimization process is very expensive due to the quantum mechanical (QM) computations required for optimizing the electron wavefunction at each gradient step. It is orders of magnitude more expensive than calculating the energy of a given molecular conﬁguration, since we need to calculate the energy’s gradient for each optimization step. Furthermore, the optimization will only converge to a local, and not the global minimum. And in fact, the global minimum is not the only state of interest — any reasonably low local minimum of the energy landscape is a valid conﬁguration, known as a conformer. A molecule thus does not have a unique conﬁguration; it can be in any of these states. Their statistical distribution and the interaction between them is central for many molecular properties. This ambiguity of atom positions poses a fundamental limit on how precise molecular predictions can be without knowing the exact (ensemble of) conﬁgurations. For example, without knowing the molecule’s conformer we can not reasonably predict its energy at a precision below roughly 60 meV (Grimme, 2019) — except for small, rigid molecules that do not have multiple conformers (e.g. benzene).
Approximating energies and forces. The most prominent way of accelerating the process of ﬁnding a valid molecular conﬁguration is by approximating its most expensive part: The quantum mechanical optimization of the electron wavefunction. There is a large hierarchy of methods with various runtime versus accuracy trade-offs (Folmsbee & Hutchison, 2021). The cheapest class of methods are force
ﬁelds. They allow running molecular dynamics simulations with millions of atoms, and can estimate the equilibrium structure of a small molecule in less than one second. Force ﬁelds approximate the quantum-mechanical interactions via a closed-form, differentiable function that only depends on the atom positions. One common example is the Merck Molecular Force Field (MMFF94) (Halgren, 1996). MMFF94 calculates the molecular energy based on interatomic distances, angles, dihedral angles, and long-range interaction terms. Each term is approximated using an analytic equation with empirically chosen coefﬁcients that depend on the involved atom types. Forces are obtained via the analytical gradients Fi = − ∂E
, and conformers via gradient-based optimization. Generating
∂xi conﬁgurations with force ﬁelds is fast enough to even generate a large ensemble of conformers.
However, the resulting conformers are highly biased and require corrections based on expensive
QM-based methods for reasonably approximating the molecule’s true distribution (Ebejer et al., 2012;
Kanal et al., 2018).
Directly predicting the conﬁguration. There are multiple methods that circumvent the optimization process to quickly generate low-energy conformers for a given molecular graph. Distance geometry methods generate conformers using an experimental database of ideal bond lengths, bond angles, and torsional angles (Havel, 2002). The ETKDG method combines this with empirical torsional angle preferences (Riniker & Landrum, 2015). Multiple machine learning methods for generating conformers have also recently been proposed (Weinreich et al., 2021; Lemm et al., 2021).
Restrictions for ML. All of the above methods yield reasonable molecular conﬁgurations. However, they often require many initializations and a considerable amount of hand-tuning to yield a good result for every molecule in a dataset. Furthermore, the obtained conformer might not even be the correct one for the data of interest. The data could have been generated by a different conformer or by a statistical ensemble of multiple conformers. The conﬁguration of a wrong conformer can cause our model to overﬁt to the false training data and cause bad generalization (see Sec. 6). To solve this issue we could try to generate an ensemble of conformers and embed their distribution. However, cheap generation methods yield strongly biased ensembles and would thus require expensive post-processing, defeating the purpose of fast and scalable machine learning (ML) methods (Ebejer et al., 2012; Kanal et al., 2018). We propose to instead solve this issue by using less precise synthetic coordinates that are easier and cheaper to obtain. 4 Synthetic coordinates
Molecular distance bounds. To circumvent the issues associated with conformational ambiguity, we propose to use pairwise distance bounds instead of simple coordinates, i.e. minimum and maximum distances d(min) and d(max) for every pair of atoms. These bounds only provide the chemical information we are certain of, without being falsely accurate. Speciﬁcally, we use the distance bounds 4
provided by RDKit (RDKit, 2021). These bounds provide different estimates depending on how the atoms are bonded in the molecular graph. The edges in the molecular graph correspond to directly bonding atoms, whose bounds are calculated as the equilibrium distance (as parametrized in the universal force ﬁeld (UFF) (Rappe et al., 1992)) plus or minus a tolerance of 0.01 Å. The angles between triplets of atoms are estimated based on bond hybridization and whether an atom is part of a ring. The distance bounds between two-hop neighbors are then calculated based on this angle, the bond length, and a tolerance of 0.04 Å, or 0.08 Å for atoms larger than Aluminium. Pairwise distances between higher-order neighbors are not relevant for our method, since we only use the distances and angles of the molecular graph. The distance bounds are then reﬁned using the triangle inequality. Note that these bounds depend almost exclusively on the directly involved atoms. They thus only provide local structural information.
Based on these distance bounds we calculate three different angles for directional MPNNs: The maximally and minimally realizable angles, and the center angle. We obtain them using standard trigonometry, via
α(a) ijk = arccos (cid:32) d2 (b),ij + d2 (b),jk − d2 (a),ik 2d(b),ijd(b),jk (cid:33)
, (3) where (a) = (max) and (b) = (min) for the maximally realizable angle, (a) = (min) and (b) = (max) for the minimally realizable angle, and (a) = (b) = (center) for the center angle, with the center distance d(center) = (d(min) + d(max))/2. These distance and angle bounds hold for all reasonable molecular structures and thus provide valuable, general information for our model. Their calculation requires no hand-tuning, takes only a few milliseconds, and worked out-of-the-box for every molecule we investigated.
Graph-based distances. Directional MPNNs only use the distances of interactions and the angles between interactions; they do not require a full three-dimensional geometry. We leverage this gener-ality to propose a second distance based on a common graph-based proximity measure: Personalized
PageRank (PPR) (Page et al., 1998), also known as random walks with restart. PPR measures how close two atoms in the molecular graph are by calculating the probability that a random walker starting at atom i ends up at atom j. At each step, the random walker jumps to any neighbor of the current atom with equal probability, and teleports back to the original atom i with probability α.
To satisfy the symmetry property of a metric we use a variant of PPR that uses the symmetrically normalized transition matrix, i.e.
Πsppr = α(IN − (1 − α)D−1/2AD−1/2)−1, (4) with the teleport probability α ∈ (0, 1], the adjacency matrix A, and the diagonal degree matrix
Dij = (cid:80) k Aikδij. We found that this method works well even without considering any bond type information in A. We convert Πsppr to a distance via (cid:113) dsppr,ij =
Πsppr ii + Πsppr jj − 2Πsppr ij .
Note that Πsppr deﬁnes a positive deﬁnite kernel, and this is the induced distance in its reproducing kernel Hilbert space. It therefore satisﬁes all properties of a metric, i.e. identity of indiscernibles, symmetry, and the triangle inequality (Berg et al., 1984, Chapter 3,
§3). However, dsppr,ij is a general metric and does not yield atom positions in 3D. This is a purely graph-based measure that does not incorporate any chemical knowledge. It reﬂects how central an atom is in the molecular graph, and how important another atom is to this one, based on the overall network of bonds. It thus only helps the GNN better reﬂect and process the molecular graph structure. Fig. 2 shows an example of dsppr on ethanol. Since the law of cosines holds for any inner product space we can calculate the angles for directional message passing via (5)
Figure 2: dsppr distance be-tween direct neighbors on ethanol.
αijk = arccos (cid:32) d2 ij + d2 jk − d2 ik 2dijdjk 5 (cid:33)
. (6)
Note that the bounds- and graph-based distances encode orthogonal information. The former is solely based on the global molecular graph structure, while the latter provides purely local chemical knowledge. Instead of just choosing one or the other we can therefore combine both to obtain the beneﬁts of both.
Representing distances and angles. The additional structural information can directly be incor-porated into existing models as edge features. For this purpose, we propose to ﬁrst represent the distances using NRBF Gaussian radial basis functions (RBF), i.e. hRBF,n(dij) = exp−1/2(dij −cn)2/σ2 where the Gaussian centers cn are set uniformly between 0 and the overall maximum distance, n ∈ [0, NRBF], and σ = c1 − c0 is set as the distance between two neighboring centers. The angles are similarly represented using NABF cosine angular basis functions (ABF), i.e. (7)
, hABF,n(αijk) = cos(nαijk), (8) with n ∈ [0, NABF]. We then transform these features using two linear layers. The ﬁrst layer is global and uses a small output dimension to force the model to learn a well-generalizing intermediate representation. The second layer is speciﬁc to each GNN layer, enabling more ﬂexibility. Overall, we obtain the distance-based edge features eij and angle-based triplet features aijk in layer l via e(l) ij = W (l) ijk = W (l) a(l)
RBF2WRBF1(hRBF(dij)(cid:107)x(E) ij ),
ABF2WABF1hABF(αijk), (9) (10)
RBF2 and W (l) where W (l)
ABF2 are layer-wise learned weight matrices, WRBF1 and WABF1 are global learned weight matrices, (cid:107) denotes concatenation, and x(E) are bond (edge) features. We can ij furthermore combine multiple synthetic coordinates by concatenating their representations hRBF and hABF. Note that for DimeNet++ we use the original basis transformation instead of the one described here. 5