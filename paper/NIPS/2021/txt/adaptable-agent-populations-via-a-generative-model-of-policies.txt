Abstract
In the natural world, life has found innumerable ways to survive and often thrive.
Between and even within species, each individual is in some manner unique, and this diversity lends adaptability and robustness to life. In this work, we aim to learn a space of diverse and high-reward policies in a given environment. To this end, we introduce a generative model of policies for reinforcement learning, which maps a low-dimensional latent space to an agent policy space. Our method enables learning an entire population of agent policies, without requiring the use of separate policy parameters. Just as real world populations can adapt and evolve via natural selection, our method is able to adapt to changes in our environment solely by selecting for policies in latent space. We test our generative model’s capabilities in a variety of environments, including an open-ended grid-world and a two-player soccer environment. Code, visualizations, and additional experiments can be found at https://kennyderek.github.io/adap/. 1

Introduction
Quick thought experiment: imagine our world was such that all people acted, thought, and looked exactly the same in every situation. Would we ever have found the influential dissenters that sparked scientific, political, and cultural revolutions?
In reinforcement learning (RL), it is common to learn a single policy that fits an environment.
However, it is often desirable to instead find an entire array of high performing policies. To this end, we propose learning a generative model of policies. At a high level, we aim to show that purposefully learning a diverse policy space for a given environment can be competitive to learning a single policy, while better encompassing a range of skillful behaviors that are adaptable and robust to changes in the task and environment. We name our method of learning a space of adaptable agent polices: ADAP.
Why should we bother with finding more than one policy per environment? We propose two primary reasons. First, RL environments are continually approaching greater levels of open-endedness and complexity. For a given environment, there might be an entire manifold of valid and near-equally high performing strategies. By finding points across this manifold, we avoid ‘having all eggs in one basket,’ granting robustness and adaptability to environmental changes. In the event of a change, we are able to adapt our generated population to select individuals that can still survive given the ablation, much like natural selection drives evolution in the real world. Secondly, using a generative model of policies as a population of agents makes intuitive sense in multi-agent environments, in which different agents should have the capacity to act like they are unique individuals. However, it is common in many multi-agent reinforcement learning settings to deploy the same policy across all agents, such that they are essentially distributed clones. Doing so may reduce the multi-modality of the agent population, resulting in a single ‘average’ agent.
Previous work has touched on ideas akin to a generative model of policies. In hierarchical RL, the high-level policy controller can be considered a generator of sub-policies that are ‘options’ [1, 2, 3].
But these methods are designed to find decomposable skills that aid in the construction of just one 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
downstream controller policy. A core idea of our work is that of quality diversity [4], which aims to optimize a population of agents along the axes of both reward and diversity. Traditional methods often use evolutionary search over a discrete-sized population of separate agents, each with their own policy parameters. This consumes more time and training resources, and limits the number of potential behaviors. Our work integrates the goals of quality diversity into time and memory efficient deep RL by simulating an entire population of agents via a generative model of policies, with diversity bounded only by capacity of the generator.
The rest of the paper is organized as follows. First we introduce our generative model of policies and the diversity objective that guides its learning. Next, we explore the potentials of learning a population of agents by ablating environments and then searching for suitable policies, directly in latent space. We primarily study two environments: Markov Soccer [5] and Farmworld. Farmworld is a new environment we have developed for testing diversity in a multi-agent, open-ended gridworld.
At the website linked in the abstract, one can find qualitative results of experiments presented in this paper, as well as additional results on toy environments of CartPole [6] and a standard multi-goal environment. 2 Method
Let Z be a sample space of n dimensional vectors, and Z be a random variable defined uniformly over Z. Then, we learn a mapping, G : ϕ, Z, from generator weights ϕ and latent distribution Z to a space of policies Π.
The generator Gϕ itself is not a policy. It must be conditioned on a draw z ∼ Z in order to define a learned set of behaviors. In this sense, z is a stochastic parameter of Gϕ, and is sampled once at the beginning of each agent episode.
In our experiments, Z is the sample space of all three dimensional vectors with magnitude one (i.e. the surface of the unit sphere). Practically, we use the low dimension of three, so that we can perform a key subject of this paper: rapid optimization, or adaptation, of G by changing Z rather than ϕ (fine tuning ϕ would be more typical in literature). We require magnitude one so that there is at least one non-zero element for any z ∼ Z, which we found important for providing signal and stability in the training of G. It is possible that with higher dimensions, this stipulation could be relaxed.
Figure 1: Method Diagram. Upon agent initialization, we sample a latent zi ∼ Z which, along with
ϕ, defines an agent policy for the episode. We update Gϕ by optimizing two objectives. The PPO surrogate loss is optimized in an online manner, using the trajectory τ from the generated policy
πϕ,zi. Meanwhile, the diversity regularizer loss is optimized over independently sampled policy pairs
πϕ,zj , πϕ,zk ∼ Z.
Diversity Regularization In order to learn a diverse space of unique policies, we introduce a diversity regularization objective. Since policies define a space of actions taken over different states, we propose that in order for two policies to be distinct, they must have different action distributions given the same state. To this end, we define the objective Ldiv (1): (cid:20)
Ldiv(ϕ) = E s∈S
E zi,zj ∼Z exp (cid:0)−DKL(πϕ,zi;b(s)∥πϕ,zj ;b(s))(cid:1) (cid:21) (1) in which DKL is the KL-divergence between the two policy action distributions πϕ,zi and πϕ,zj , and b is a smoothing constant over the action distributions.
Optimization of G In our experiments, we optimize the diversity objective in an online fashion using gradient descent, in conjunction with a PPO [7] clipped-surrogate objective and an entropy regularization objective. Our full optimization problem is 2
max
ϕ
LP P O(ϕ) − αLdiv(ϕ) where LP P O is Equation 9 in [7] and α is a coefficient to scale the diversity regularization objective.
See Algorithm 1 in the supplement for additional details.
Adaptation via Optimization in the Latent Space of G By learning an entire space of policies Π, we are able to search our policy space for the highest performing policy, whether dealing with the training environment or an ablated future environment.
In contrast to searching over policy parameters through transfer learning or fine-tuning, we are able to quickly search over the low-dimensional latent space (dimensionality 3 in our experiments). In fact, we can quickly adapt back and forth to various situations: the search procedure often takes less than 30 seconds, or 100 episode rollouts, to find any high quality solutions that exist. Over the course of a small number of generations, we evaluate randomly sampled latents, and keep higher performing ones with greater probability. In the event that episodes have a high degree of variablility per run – such as in the Markov Soccer environment – it may be necessary to run several episodes per latent vector and average the returns. Details can be found in Algorithm 2 of the supplement.
Model Architecture Similarly to prior work [3], we have found that richer integrations between the latent vector and the observation can yield a more multi-modal policy space. To induce this richer integration, we introduce a multiplicative model denoted "(x)" for latent integration, and compare the results to a baseline of concatenating "(+)" the latent sample to the observation. We describe this architecture in the supplement. 3