Abstract
Humans can easily segment moving objects without knowing what they are. That objectness could emerge from continuous visual observations motivates us to model segmentation and movement concurrently from unlabeled videos. Our premise is that a video contains different views of the same scene related by moving components, and the right region segmentation and region ﬂow allow view synthesis which can be checked on the data itself without any external supervision.
Our model ﬁrst deconstructs video frames in two separate pathways: an appearance pathway that outputs feature-based region segmentation for a single image, and a motion pathway that outputs motion features for a pair of images. It then binds them in a conjoint region ﬂow feature representation and predicts segment ﬂow that provides a gross characterization of moving regions for the entire scene. By training the model to minimize view synthesis errors based on segment ﬂow, our appearance and motion pathways learn region segmentation and ﬂow estimation automatically without building them up from low-level edges or optical ﬂow respectively.
Our model demonstrates the surprising emergence of objectness in the appearance pathway, surpassing prior works on 1) zero-shot object segmentation from a single image, 2) moving object segmentation from a video with unsupervised test-time adaptation, and 3) semantic image segmentation with supervised ﬁne-tuning. Our work is the ﬁrst truly end-to-end learned zero-shot object segmentation model from unlabeled videos. It not only develops generic objectness for segmentation and tracking, but also outperforms image-based contrastive representation learning without augmentation engineering. 1

Introduction
Contrastive learning [1–3] has recently become a powerful method for obtaining high-level visual representations from images [4]. While these representations are shown to be more generalizing, there remain two practical limitations: 1) Hand-crafted augmentations such as image cropping and color jittering [5] are critical for achieving invariant recognition, and yet they fall short of capturing more complex object deformations and 3D viewpoint changes. 2) Additional labeled data are required at the downstream for representation ﬁne-tuning, preventing standalone applications.
Our goal here is to overcome these limitations of contrastive representation learning by developing object segmentation models automatically from unlabeled videos without any supervision. Unlike single static images, videos contain sequences of dynamic images that could reveal not only moving objects from their backgrounds, but also their internal part organizations with articulated movements.
Once ﬁgure-ground segregation occurs automatically in raw videos, object semantics can be readily discovered from those foreground segmentations.
∗Equal contribution. Work done when Runtao was a StarBridge intern at MSRA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Our zero-shot object segmentation is learned from an unsupervised factorization of images into segments and their motions, whereas past work segments objects based on dense pixel-wise optical ﬂows, which are brittle in the presence of noise, articulated movement, and abrupt motion.
Several observations motivate us to explore such zero-shot learning of object segmentation. 1)
Humans can easily segment moving objects without knowing what they are. 2) In biological vision, newborn chicks raised in a controlled visual world rapidly develop more accurate object representations when presented with temporally slow and smooth objects, generalizing from very limited viewpoints [6, 7]. 3) Invariant recognition can be developed by seeking slowly varying features from temporally varying signals [8], disentangling object identity and object location unsupervisedly.
We model segmentation and movement concurrently from unlabeled videos. Our premise is that a video contains different views of the same scene related by moving components, and the right region segmentation and region ﬂow would allow mutual view synthesis between frames that can be checked on the data itself. That is, if we know how regions of frame j are moved from regions of frame i, we can synthesize frame j by copying regions from frame i and paste them according to how they move.
Comparing the synthesized frame j with the actual frame j provides feedback on how to improve both region segmentation and region ﬂow estimation without needing any supervision.
View synthesis has been frequently adopted as a self-supervised criterion for learning dense optical
ﬂows [9], monocular depth [10], and multi-plane image representation [11] etc from images. Unlike prior works that focus on low-level visual correspondences, our work tackles object segmentation for mid- to high-level visual recognition directly. Speciﬁcally, as illustrated in Figure 1, instead of deriving dense optical ﬂows between successive frames and supplying additional cues for image-based object segmentation in a bottom-up manner, we seek a top-down factorized representation that provides a gross characterization of moving regions for the entire scene.
Our model ﬁrst deconstructs video frames by processing them in two separate pathways: an appear-ance pathway that models what is moving and outputs feature-based region segmentation given a single image, and a motion pathway that models how it moves and outputs motion features given a pair of images. It then binds them in a conjoint region ﬂow feature representation, based on which we predict segment ﬂow as the common fate [12] or piecewise constant movement of all the pixels in the same region. By training the model to minimize view synthesis errors based on segment ﬂow, our appearance and motion pathways learn region segmentation and ﬂow estimation automatically without building them up from low-level edges or optical ﬂows respectively.
After training our segmentation and ﬂow features for view synthesis on unlabeled videos, our model demonstrates the surprising emergence of objectness in a particular feature channel of the appearance pathway. That is, our model can be directly applied to novel images and videos for segmenting foreground objects: Our appearance pathway can perform zero-shot object segmentation on a single image, whereas our overall model can perform zero-shot moving object segmentation on a single video with test-time adaptation. Our image feature learned from unlabeled videos can be further
ﬁne-tuned on a small labeled dataset for semantic segmentation. Experimentally, we demonstrate strong performance on all three applications, with considerable gains over baselines.
To summarize, our work makes the following contributions. 1) We develop the ﬁrst truly end-to-end learned zero-shot object segmentation model from unlabeled videos, assuming no low-level computation such as edges or optical ﬂow. 2) We bypass the traditional low-level dense optical ﬂow and propose to compute novel mid-level segment ﬂow directly. 3) Our model not only develops 2
generic objectness for segmentation and tracking, but also outperforms prevalent image-based contrastive learning methods without augmentation engineering. Our code is available at https:
//github.com/rt219/The-Emergence-of-Objectness. 2