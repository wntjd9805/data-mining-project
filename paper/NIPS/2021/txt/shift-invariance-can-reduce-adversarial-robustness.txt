Abstract
Shift invariance is a critical property of CNNs that improves performance on classiﬁcation. However, we show that invariance to circular shifts can also lead to greater sensitivity to adversarial attacks. We ﬁrst characterize the margin between classes when a shift-invariant linear classiﬁer is used. We show that the margin can only depend on the DC component of the signals. Then, using results about inﬁnitely wide networks, we show that in some simple cases, fully connected and shift-invariant neural networks produce linear decision boundaries. Using this, we prove that shift invariance in neural networks produces adversarial examples for the simple case of two classes, each consisting of a single image with a black or white dot on a gray background. This is more than a curiosity; we show empirically that with real datasets and realistic architectures, shift invariance reduces adversarial robustness. Finally, we describe initial experiments using synthetic data to probe the source of this connection. 1

Introduction
In adversarial attacks (Szegedy et al., 2013) against classiﬁers, an adversary with knowledge of the trained classiﬁer makes small perturbations to a test image or even to objects in the world (Eykholt et al., 2018; Wu et al., 2020b) that change the output. Such attacks threaten the deployment of deep learning systems in many critical applications, from spam ﬁltering to self-driving cars.
Despite a great deal of study, it remains unclear why neural networks are so susceptible to adversarial attacks. We show that invariance to circular shifts in Convolutional Neural Networks (CNNs) can be one cause of this lack of robustness. All reference to shifts will refer to circular shifts.
To motivate this conclusion we study in detail a simple example. Indeed, one of our contributions is to present perhaps the simplest possible example in which adversarial attacks can occur. Figure 1 shows a two class classiﬁcation problem in which each class consists of a single image, a white or black dot on a gray background. We train either a fully connected (FC) network or a CNN that is designed to be fully shift-invariant to distinguish between them. Since each class contains only a single image, we measure adversarial robustness as the l2 distance to an adversarial example produced by a DDN attack (Rony et al., 2019), using the training image as the starting point. The ﬁgure shows that the
CNN is much less robust than the FC network, and that the robustness of the CNN drops precipitously with the image size.
In Sections 3 and 4 we explain this result theoretically. In Section 3 we study the effect of shift invariance on the margin of linear classiﬁers, for linearly separable data. We call a linear classiﬁer
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: In a binary classiﬁcation problem, with each image as a distinct class (a), a FC network requires an average L2 distance close to 1 to construct an adversarial example while a shift-invariant
CNN only requires approximately 1√
, where d is the input dimension. (b)-left shows adversarial d examples for the CNN. On the top, for example, the adversarial example looks just like the clean sample for the white dot class, but is classiﬁed as belonging to the black dot class. (b)-right shows that "adversarial" examples for the FC network just look like gray images. (c) shows accuracy under an adversarial attack, where the L2 norm of the allowed adversarial perturbation is shown in the horizontal axis. The curve labeled CNN shows results for a real trained network, while CNTK shows the theoretically derived prediction of 1√ d for an inﬁnitely wide network. shift invariant when it places all possible shifts of a signal in the same class. We prove that for a shift invariant linear classiﬁer the margin will depend only on differences in the DC (constant) components of the training signals. It follows that for the two classes shown in Figure 1, the margin of a linear, shift invariant classiﬁer will shrink in proportion to 1√ d
, where d is the number of image pixels.
Next, in Section 4, we prove that under certain assumptions, a CNN whose architecture makes it shift invariant produces a linear decision boundary for the example in Figure 1, explaining its lack of adversarial robustness. We draw on recent work that characterizes inﬁnitely wide neural networks as kernel methods, describing FC networks with the neural tangent kernel (NTK) (Jacot et al., 2018) and CNNs with a convolutional neural tangent kernel (CNTK) (Arora et al., 2019; Li et al., 2019).
Our proof uses these kernels, and applies when the assumptions of this prior work hold. However, the results in Figure 1 are produced with real networks that do not satisfy these assumptions, suggesting that our results are more general.
We feel that it is valuable to produce such a simple example in which adversarial vulnerability provably occurs and can be fully understood. Still, it is reasonable to question whether shift invariance can affect robustness in networks trained on real data. In Section 5 we show experimentally that it can.
First, we compare the robustness of FC and shift-invariant networks on MNIST and Fashion-MNIST.
On these smaller datasets, FC networks attain reasonably good test accuracy on clean data. We ﬁnd that indeed, FC networks are also quite a bit more robust than shift-invariant CNNs. We also show that if we reduce the degree of shift invariance of a CNN, we increase its adversarial robustness on these datasets. Next we show that on the SVHN dataset, FC networks are more robust than ResNet architectures suggesting that this phenomenon extends to realistic convolutional networks. Finally, we consider industrial strength networks trained on CIFAR-10 and ImageNet. We note that ResNet and VGG are shift-invariant to a degree, while AlexNet, FCNs and transformer based networks are much less so. This suggests that ResNet and VGG will also be less robust than AlexNet, FCNs, and transformer networks, which we conﬁrm experimentally.
Finally, we use simple synthetic data to explore the connection between shift invariance and robustness.
We show that when shift invariance effectively increases the dimensionality of the training data, it produces less robustness, and that this does not occur when shift invariance does not increase dimensionality. This is in line with recent studies that connect higher dimensionality to reduced robustness. However, we also show, at least in a simple example, that higher dimensionality only affects robustness when it also affects the margin of the data.
The main contribution of our paper is to show theoretically that shift invariance can under-mine adversarial robustness, and to show experimentally that this is indeed the case on real-world networks.. This enhances our understanding of robustness, and suggests that we can improve 2
robustness by judiciously reducing invariances that are not needed for good performance. Our code can be found at https://github.com/SongweiGe/shift-invariance-adv-robustness. 2