Abstract Actor-Critic for
Continuous Control
Haonan Yu, Wei Xu, Haichao Zhang
Horizon Robotics
Cupertino, CA 95014
{haonan.yu,wei.xu,haichao.zhang}@horizon.ai
Abstract
We present temporally abstract actor-critic (TAAC), a simple but effective off-policy RL algorithm that incorporates closed-loop temporal abstraction into the actor-critic framework. TAAC adds a second-stage binary policy to choose between the previous action and a new action output by an actor. Crucially, its “act-or-repeat” decision hinges on the actually sampled action instead of the expected behavior of the actor. This post-acting switching scheme let the overall policy make more informed decisions. TAAC has two important features: a) persistent exploration, and b) a new compare-through Q operator for multi-step TD backup, specially tailored to the action repetition scenario. We demonstrate TAAC’s advantages over several strong baselines across 14 continuous control tasks. Our surprising ﬁnding reveals that while achieving top performance, TAAC is able to “mine” a signiﬁcant number of repeated actions with the trained policy even on continuous tasks whose problem structures on the surface seem to repel action repetition. This suggests that aside from encouraging persistent exploration, action repetition can ﬁnd its place in a good policy behavior. Code is available at https://github.com/hnyu/taac. 1

Introduction
Deep reinforcement learning (RL) has achieved great success in various continuous action domains such as locomotion and manipulation (Schulman et al., 2015; Lillicrap et al., 2016; Duan et al., 2016;
Schulman et al., 2017; Fujimoto et al., 2018; Haarnoja et al., 2018). Despite promising empirical results, these widely applicable continuous RL algorithms execute a newly computed action at every step of the ﬁnest time scale of a problem. With no decision making at higher levels, they attempt to solve the challenging credit assignment problem over a long horizon. As a result, considerable sample efﬁciency improvements have yet to be made by them in complex task structures (Riedmiller et al., 2018; Li et al., 2020; Lee et al., 2020b) and extremely sparse reward settings (Andrychowicz et al., 2017; Plappert et al., 2018; Zhang et al., 2021).
On the other hand, it is argued that temporal abstraction (Parr and Russell, 1998; Dietterich, 1998;
Sutton et al., 1999; Precup, 2000) is one of the crucial keys to solving control problems with complex structures. Larger steps are taken at higher levels of abstraction while lower-level actions only need to focus on solving isolated subtasks (Dayan and Hinton, 1993; Vezhnevets et al., 2017; Bacon et al., 2017). However, most hierarchical RL (HRL) methods are task speciﬁc and nontrivial to adapt. For example, the options framework (Sutton et al., 1999; Precup, 2000; Bacon et al., 2017) requires pre-deﬁning an option space, while the feudal RL framework Vezhnevets et al. (2017); Nachum et al. (2018); Zhang et al. (2021) requires tuning the hyperparameters of dimensionality and domain range of the goal space. In practice, their ﬁnal performance usually hinges on these choices.
Perhaps the simplest form of an option or sub-policy would be just repeating an action for a certain number of steps, a straightforward idea that has been widely explored (Lakshminarayanan et al., 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
2017; Sharma et al., 2017; Dabney et al., 2021; Metelli et al., 2020; Lee et al., 2020a; Biedenkapp et al., 2021). This line of works can be regarded as a middle ground between “ﬂat” RL and HRL.
They assume a ﬁxed candidate set of action durations, and repeat actions in an open-loop manner.
Open-loop control forces an agent to commit to the same action over a predicted duration with no opportunity of early terminations. It weakens the agent’s ability of handling emergency situations and correcting wrong durations predicted earlier. To address this inﬂexibility, a handful of prior works (Neunert et al., 2020; Chen et al., 2021) propose to output an “act-or-repeat” binary decision to decide if the action at the previous step should be repeated. Because this “act-or-repeat” decision will be examined at every step depending on the current environment state, this results in closed-loop action repetition.
All these action-repetition methods are well justiﬁed by the need of action persistence (Dabney et al., 2021; Amin et al., 2021; Zhang and Van Hoof, 2021; Grigsby et al., 2021) for designing a good exploration strategy, when action diversity should be traded for it properly. This trade-off is important because when reward is sparse or short-term reward is deceptive, action diversity alone only makes the agent wandering around its local neighborhood since any persistent trajectory has an exponentially small probability. In such a case, a sub-optimal solution is likely to be found. In contrast, persistence via action repetition makes the policy explore deeper (while sacriﬁcing action diversity to some extent).
This paper further explores in the direction of closed-loop action repetition, striving to discover a novel algorithm that instantiates this idea better. The key question we ask is, how can we exploit the special structure of closed-loop repetition, so that our algorithm yields better sample efﬁciency and ﬁnal performance compared to existing methods? As an answer to this question, we propose temporally abstract actor-critic (TAAC), a simple but effective off-policy RL algorithm that incorporates closed-loop action repetition into an actor-critic framework. Generally, we add a second stage that chooses between a candidate action output by an actor and the action from the previous step (Figure 1).
Crucially, its “act-or-repeat” decision hinges on the actually sampled individual action instead of the expected behavior of the actor unlike recent works (Neunert et al., 2020; Chen et al., 2021). This post-acting switching scheme let the overall policy make more informed decisions. Moreover, i) for policy evaluation, we propose a new compare-through Q operator for multi-step TD backup tailored to the action repetition scenario, instead of replying on generic importance correction; ii) for policy improvement, we compute the actor gradient by multiplying a scaling factor to the
∂Q
∂a term from DDPG (Lillicrap et al., 2016) and SAC (Haarnoja et al., 2018), where the scaling factor is the optimal probability of choosing the actor’s action in the second stage.
TAAC is much easier to train compared to sophisticated HRL methods, while it has two important features compared to “ﬂat” RL algorithms, namely, persistent exploration and native multi-step TD backup support without the need of off-policyness correction.
We evaluate TAAC on 14 continuous control tasks, covering simple control, locomotion, terrain walking (Brockman et al., 2016), manipulation (Plappert et al., 2018), and self-driving (Dosovitskiy et al., 2017). Averaged over these tasks, TAAC largely outperforms 6 strong baselines. Importantly, our results show that it is our concrete instantiation of closed-loop action repetition that is vital to the
ﬁnal performance. The mere idea of repeating actions in a closed-loop manner doesn’t guarantee better results than the compared open-loop methods. Moreover, our surprising ﬁnding reveals that while achieving top performance, TAAC is able to “mine” a signiﬁcant number of repeated actions with the trained policy even on continuous tasks whose problem structures on the surface seem to repel action repetition (Section 5.6.2). This suggests that aside from encouraging persistent exploration, action repetition can ﬁnd its place in a good policy behavior. This is perhaps due to that the action frequency of a task can be difﬁcult to be set exactly as the minimum value that doesn’t comprise optimal control while leaving no room for temporal abstraction (Grigsby et al., 2021). 2