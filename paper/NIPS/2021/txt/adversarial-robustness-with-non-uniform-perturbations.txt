Abstract
Robustness of machine learning models is critical for security related applications, where real-world adversaries are uniquely focused on evading neural network based detectors. Prior work mainly focus on crafting adversarial examples (AEs) with small uniform norm-bounded perturbations across features to maintain the require-ment of imperceptibility. However, uniform perturbations do not result in realistic
AEs in domains such as malware, ﬁnance, and social networks. For these types of applications, features typically have some semantically meaningful dependencies.
The key idea of our proposed approach1 is to enable non-uniform perturbations that can adequately represent these feature dependencies during adversarial training.
We propose using characteristics of the empirical data distribution, both on correla-tions between the features and the importance of the features themselves. Using experimental datasets for malware classiﬁcation, credit risk prediction, and spam detection, we show that our approach is more robust to real-world attacks. Finally, we present robustness certiﬁcation utilizing non-uniform perturbation bounds, and show that non-uniform bounds achieve better certiﬁcation. 1

Introduction
Deep neural networks (DNNs) are commonly used in a wide-variety of security-critical applications such as self-driving cars, spam detection, malware detection and medical diagnosis [1]. However,
DNNs have been shown to be vulnerable to adversarial examples (AEs), which are perturbed inputs designed to fool the machine learning systems [2–4]. To mitigate this problem, a line of research has focused on adversarial robustness of DNNs as well as the certiﬁcation of these methods [1, 5–10].
Adversarial training (AT) is one of the most effective empirical defenses against adversarial attacks
[1, 11]. The goal during training is to minimize the loss of the DNN when perturbed samples are used. This way, the model becomes robust to real-world adversarial attacks. Though these empirical defenses do not provide theoretically provable guarantees, they have been shown to be robust against the strongest known attacks [12]. Some of the most common state-of-the-art adversarial attacks, such as projected gradient descent (PGD) [1] and fast gradient sign method (FGSM) [12], perturb training samples under a norm-ball constraint to maximize the loss of the network. The goal of certiﬁcation, on the other hand, is to report whether an AE exists within an (cid:96)p norm centered at a given sample with a ﬁxed radius. Certiﬁed defense approaches introduce theoretical robustness guarantees against norm-bounded perturbations [8, 9, 13, 14]. 1Code is available at https://github.com/amazon-research/adversarial-robustness-with-nonuniform-perturbations 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
In the computer vision domain, the adversary’s goal is to generate perturbed images that cause misclassiﬁcations in a DNN. It is often assumed that limiting a uniform norm-ball constraint results in perturbations that are imperceptible to the human eye. However in other applications such as fraud detection [15], spam detection [16], credit card default prediction [17, 18] and malware detection
[19–21], norm-bounded uniform perturbations may result in unrealistic transformations. Perturbed samples must comply with certain constraints related to the domain, hence preventing us from borrowing these assumptions from computer vision. These constraints can be on semantically meaningful feature dependencies, expert knowledge of possible attacks, and immutable features
[20, 22]. This paper proposes a methodology to generate non-uniform perturbations that takes into account the characteristics of the empirical data distribution. Our results demonstrate that these non-uniform perturbations outperform uniform norm-ball constraints in these types of applications. 1.1