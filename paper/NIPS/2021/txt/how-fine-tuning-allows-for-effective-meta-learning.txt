Abstract
Representation learning has served as a key tool for meta-learning, enabling rapid learning of new tasks. Recent works like MAML learn task-speciﬁc representa-tions by ﬁnding an initial representation requiring minimal per-task adaptation (i.e. a ﬁne-tuning-based objective). We present a theoretical framework for analyzing a MAML-like algorithm, assuming all available tasks require approximately the same representation. We then provide risk bounds on predictors found by ﬁne-tuning via gradient descent, demonstrating that the method provably leverages the shared structure. We illustrate these bounds in the logistic regression and neural network settings. In contrast, we establish settings where learning one represen-tation for all tasks (i.e. using a “frozen representation” objective) fails. Notably, any such algorithm cannot outperform directly learning the target task with no other information, in the worst case. This separation underscores the beneﬁt of
ﬁne-tuning-based over “frozen representation” objectives in few-shot learning. 1

Introduction
Meta-learning (Thrun & Pratt, 2012) has emerged as an essential tool for adapting prior knowledge to new tasks under data and computational constraints. In this context, a meta-learner has access to related source tasks from a shared environment. The learner aims to uncover inductive biases from the source tasks to reduce the sample/computational complexity of learning new tasks from the same environment. A common approach is representation learning (Bengio et al., 2013), i.e. learning a feature extractor from the source tasks. At test time, a learner adapts to a new task by
ﬁne-tuning the representation and retraining the ﬁnal layer(s) (see, e.g., prototype networks (Snell et al., 2017)). Substantial improvements over directly learning a single task has been shown in few-shot learning (Antoniou et al., 2018), a natural setting in many applications including reinforcement learning (Mendonca et al., 2019; Finn et al., 2017), computer vision (Nichol et al., 2018), federated learning (McMahan et al., 2017) and robotics (Al-Shedivat et al., 2017).
The empirical success of representation learning has led to an increased interest in theoretical anal-yses of underlying phenomena. Recent work assumes an explicitly shared representation across tasks (Du et al., 2020; Tripuraneni et al., 2020b,a; Saunshi et al., 2020; Balcan et al., 2015). For instance, Du et al. (2020) prove a generalization bound consisting of a representation error term and an estimation error term. However, without ﬁne-tuning the whole network, the representation error accumulates to the target task and is irreducible, even with an inﬁnitely large target task dataset.
This result is consistent with empirical ﬁndings, which suggest that ﬁne-tuning the whole network provides substantial performance gains, compared to just learning the ﬁnal linear layer (Chen et al., 2020; Salman et al., 2020). Due to the lack of (representation) ﬁne-tuning while training on the source tasks, we refer to methods above as making use of “frozen representation” objectives.
Linear separability of tasks on the same features poses additional problems, being an unrealistic assumption when transferring knowledge to other domains (e.g. from ImageNet to medical images (Raghu et al., 2019)). Therefore, we consider a more realistic setting where the available tasks only approximately share the same representation. In this setting, we propose a theoretical framework for 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
analyzing the sample complexity of ﬁne-tuning, using an initial representation from a MAML-like algorithm. We show that ﬁne-tuning quickly adapts to new tasks, requiring fewer samples in certain cases compared to methods using “frozen representation” objectives. To the best of our knowledge, no prior theoretical studies exist beyond ﬁne-tuning a linear model (Denevi et al., 2018; Konobeev et al., 2020; Collins et al., 2020a; Lee et al., 2020) or only the task-speciﬁc layers (Du et al., 2020;
Tripuraneni et al., 2020b,a; Mu et al., 2020). Tripuraneni et al. (2020b), in particular, acknowledge that their work does not incorporate representation ﬁne-tuning, leaving such analysis to future work.
It is this gap which we now seek to address.
The following outlines this paper and its contributions:
In Section 2, we outline the general setting and our overall assumptions. Additionally, we introduce
ADAPTREP, the representative ﬁne-tuning-based algorithm we analyze in this work. As a baseline, we also formally deﬁne FROZENREP, which makes use of a “frozen representation” objective.
In Section 3, we provide an in-depth analysis of the (d-dimensional) linear representation setting, when learning a k-dimensional representation. First, we show that ADAPTREP achieves rates of rsource = O (cid:16) kd nST + k nS
+ δ0 (cid:17) (cid:113) tr Σ nS and rtarget = O (cid:16) k nT
+ δ0 (cid:113) tr Σ nT (cid:17)
+ rsource on the source and target tasks. Here, nS and nT are the number of available samples, T is the number of source tasks, δ0 is the norm of the representation change, and Σ is the input covariance.
Thus, ﬁne-tuning can handle the misspeciﬁed setting. In contrast, FROZENREP has a minimax rate of Ω(d/nT) on the target task under certain task distributions, matching standard linear regression.
We provide a formal construction and an experimental veriﬁcation of the gap in Section C.
In Section 4, we extend the analysis to general function classes. We provide risk bounds of the form
εOPT + εEST + εREPR, where εOPT, εEST, εREPR are the optimization, estimation, and representation errors, respectively.
The optimization error εOPT quantiﬁes the error from using approximate minima found during optimization. To control εOPT, our analysis accounts for nonconvexity introduced by representation
ﬁne-tuning, and is presented as a self-contained result in Section H.
The estimation error εEST stems from ﬁne-tuning to the target task with ﬁnite nT samples. therefore scales with 1/ nT and is controlled by the complexity of the target ﬁne-tuning set.
√
It
The representation error εREPR is the error incurred by representation learning on the source tasks while adapting to them. It consists of two terms: one scaling as 1/ nST for learning an initialization using all T source tasks, and the other scaling as 1/ nS for learning task-speciﬁc adaptations.
√
√
In Section 5, we instantiate our guarantees in the two-layer neural network setting. Additionally, we provide an analysis for logistic regression in Section F. Furthermore, we extend the linear hard case to a nonlinear setting in Section G. 1.1