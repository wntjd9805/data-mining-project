Abstract
Modeling a system’s temporal behaviour in reaction to external stimuli is a fun-damental problem in many areas. Pure Machine Learning (ML) approaches often fail in the small sample regime and cannot provide actionable insights beyond predictions. A promising modiﬁcation has been to incorporate expert domain knowledge into ML models. The application we consider is predicting the pa-tient health status and disease progression over time, where a wealth of domain knowledge is available from pharmacology. Pharmacological models describe the dynamics of carefully-chosen medically meaningful variables in terms of systems of Ordinary Differential Equations (ODEs). However, these models only describe a limited collection of variables, and these variables are often not observable in clinical environments. To close this gap, we propose the latent hybridisation model (LHM) that integrates a system of expert-designed ODEs with machine-learned
Neural ODEs to fully describe the dynamics of the system and to link the expert and latent variables to observable quantities. We evaluated LHM on synthetic data as well as real-world intensive care data of COVID-19 patients. LHM consistently outperforms previous works, especially when few training samples are available such as at the beginning of the pandemic. 1

Introduction
Understanding the temporal evolution of a dynamical system is the central problem in many areas.
The Machine Learning (ML) approach to this problem has been to learn a collection of latent variables and construct a dynamical model of the system directly from observational data. While ML has achieved strong predictive performance in some applications, it has two central weaknesses. The ﬁrst is that it requires large datasets. The second is that the latent variables that the ML approach identiﬁes often have no physical interpretation and do not correspond to any previously-identiﬁed quantities.
One approach to dealing with these weaknesses has been to incorporate expert domain knowledge into
ML models. Most of the work using this approach has focused on incorporating high-level knowledge about the underlying physical system, such as conservation of energy [9, 32, 99], independence of mechanism [63], monotonicity [59], or linearity [34]. In addition, there have been attempts to integrate domain-speciﬁc “expert models” into ML models to create “hybrid” models. Most of this work has employed expert models that directly issue predictions [54, 86, 91, 93] or extract useful features from the raw measurements [41]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Dependency structure of the three models designed for the laboratory or clinical settings. Dashed nodes represent unobservable variables. The expert variables ze are observable in the laboratory setting but not in the clinical setting. The pharmacological model does not contain the links to the clinical variables x.
The approach taken in this paper begins with an expert model in the form of a system of Ordinary
Differential Equations (ODEs) and integrates that expert model into a system of Neural ODEs [14].
The speciﬁc problem we address is that of predicting disease progression and health status over time; the speciﬁc expert model(s) come from Pharmacology [42] – but we believe our approach may be much more widely applicable. For a number of diseases, available pharmacological models, built on the basis of specialized knowledge and laboratory experiments, provide a description of the dynamics of carefully-chosen medically meaningful variables in terms of ODEs that govern the evolution of these states [20, 2, 30]. However, these models are typically not directly applicable in clinical environments, because they involve too few variables to fully describe a patient’s health state [80, 38], because the expert variables which the models employ may be observable in the laboratory setting but not in clinical environments [29, 6], and because the relationships between the expert variables and clinically observable quantities is not known [25]. We will give a example later in Section 3.5.
This paper proposes a novel hybrid modeling framework, the Latent Hybridisation Model (LHM), that imbeds a given pharmacological model (a collection of expert variables and the ODEs that describe the evolution of these variables ) into a larger latent variable ML model (a system of Neural ODEs).
In the larger model, we use observational data to learn both the evolution of the unobservable latent variables and the relationship between measurements and all the latent variables – the expert variables from the pharmacological model and the latent variables in the larger model. The machine learning component provides links between the expert variables and the clinical measurements, the underlying pharmacological model improves sample efﬁciency, and the expert variables provide additional insights to the clinicians. A variety of experiments (using synthetic and real data) demonstrate the effectiveness of our hybrid approach. 2 Problem setting
We consider a set of hospitalized patients [N ] = {1, . . . , N } over a time horizon [0, T ]; t = 0 represents the time of admission and t = T represents the maximal length of stay. The health status of each patient i is characterized by a collection of observable physiological variables xi(t) ∈ RD,
D ∈ N+; because the physiological variables may include vital signs, bloodwork values, biomarkers, etc., xi(t) is typically a high-dimensional vector. Although the physiological variables are observable, they are typically measured only at discrete times, and with error. To avoid confusion, we distinguish the measurements of these variables y(t) from the true values; i.e. yi(t) = xi(t) + (cid:15)it (1) where the independent noise term (cid:15)it accommodates the measurement error (modeling (cid:15)t as an autocorrelated stochastic process is left as a future work). For illustrative purposes, we also assume that (cid:15)t follows a Normal distribution N (0, σ2 i ), but any parametric distribution could be easily accommodated. We denote the measurement times for each patient as Ti = {ti1, ti2, . . .}. We write ai(t) ∈ RA, A ∈ N+ for the treatments the patient receives. Some treatments (e.g. intravenous medications) are continuous; others (e.g. surgical interventions) are discrete, so some components of ai(t) may be continuous functions but others are (discontinuous) step functions.
It is convenient to write Ai[t1 : t2] = {ai(t)|t1 ≤ t ≤ t2} and Yi[t1 : t2] = {yi(t)|t1 ≤ t ≤ t2, t ∈
Ti} for the treatments and measurements (respectively) during the the time window [t1, t2]. Note that 2
Yi[0 : t] and Ai[0 : t] represent histories at time t while Ai[t : T ] and Yi[t : T ] represent treatment plans and predictions, respectively. Our objective is to predict the future measurements given the history and a treatment plan Ai[t0 : T ]:
P(Yi[t0 : T ] | Yi[0 : t0], Ai[0 : t0]
, Ai[t0 : T ] (cid:124) (cid:125) (cid:123)(cid:122) (cid:125)
Treatment plan (cid:123)(cid:122)
Historical observations (cid:124)
). (2)
Understanding this distribution will allow us to compute both point estimates and credible intervals (reﬂecting uncertainty). Note that it is important for the clinician to understand uncertainty in order to balance risk and reward. When the context is clear, we will omit the subscript i and the time index t. 3 Method 3.1 The pharmacological model
We begin with a pharmacological model which describes the dynamics of a collection of “expert” variables ze(t) ∈ RE. Each expert variable captures a distinct and medically-meaningful aspect of the human body, e.g. the activation of immune system. The pharmacological model describes the dynamics as a system of Ordinary Differential Equations (ODEs):
˙ze(t) = f e(ze(t), a(t); θe), (3) where we have written ˙ze(t) for the time derivative of ze. The functional form of f e : RE ×RA → RE is speciﬁed but the unknown parameters θe (e.g., coefﬁcients) need to be estimated from data.1,2
It is important to note that the system of ODEs (3) describes dynamics that are self-contained, in the sense that the time derivatives ˙ze(t) depend only on the current values of the expert variables ze(t) and the current treatments a(t), and not on histories or on other variables. To ensure that this obtains, it may be necessary to limit the scope of the model and limit attention to a single system of the body (or perhaps to several closely related systems) [19]. As a consequence of these limitations, the expert variables will usually not give a full picture of the health status of the patient and will usually not account for the full array of observable physiological variables x(t) [29]. 3.2 The latent hybridisation model: linking expert variables with measurements
As we have already noted, pharmacological models are typically developed and calibrated in the laboratory, where the expert variables can be directly measured – in patients, in laboratory animals, or even in vitro (Figure 1 A). In clinical environments, the expert variables are frequently not observed (Figure 1 B and C). To use the pharmacological models in clinical environments, we must establish links between the expert variables ze(t) and the clinical measurements y(t). To do this we introduce additional latent variables zm(t) ∈ RM and posit the following relationship between the latent variables ze, zm and the observable physiological variables x(t): x(t) = g(ze(t), zm(t), a(t); γ) (4)
The function g : RE×M ×A → RD is a neural network with (unknown) weights γ, and maps the latent space to the “physiological space”. Here we also allow the treatment a(t) to affect the mapping between the latent ze(t), zm(t) and the observable x(t). This is a standard design in the state space modeling literature, which we conform to (e.g. Equation 1 in [8] and Equation 1 and 2 in [85]). We posit that the dynamics of the latent variables zm(t) follow a system of ODEs governed by its current values zm(t), the treatments a(t) and the current values of the expert variables ze(t).
˙zm(t) = f m(zm(t), ze(t), a(t); θm), (5)
The function f m : RM ×E×A → RM is a neural network with (unknown) weights θm. Equations (3)-(5) specify the dynamics of LHM. It is convenient to write z(t) = [ze(t) zm(t)] for the vector of all latent variables and Θ = (θe, θm, γ, σ) for the set of all (unknown) coefﬁcients. 1The system in Equation (3) is quite general; appropriate choices of expert variables allow it to capture both high-order ODEs and time-dependent ODEs [67]. 2Some care must be taken because systems such as (3) do not always admit unique global solutions. In practice, the pharmacological models are sufﬁciently well-behaved that global solutions exist and are unique.
Although closed-form solutions may not be available, there are various efﬁcient numerical methods for solution. 3
Figure 2: Illustration of the training and prediction procedure.
The coefﬁcients Θ will be learned from data. However, even after these coefﬁcients are learned, the initial state of the patient zi(0) is still unknown. In fact, the variation in initial states reﬂects the heterogeneity of the patient population. If the coefﬁcients and the initial state were known, the entire trajectory of zi given the treatments could be computed (numerically). Because we have assumed that the noise/errors (cid:15)t are independent, we have
Yi[t0 : T ] ⊥⊥ Yi[0 : t0] | zi(0), Ai[0 : T ], Θ,
∀t0 < T (6)
However, because the initial state is unknown, it must be learned from the measurements yi(t).
LHM would reduce to a pure latent neural ODE model [14, 72] if we omitted the expert variables (Figure 1 B). However, that would amount to discarding prior (expert) information and so is evidently undesirable. Indeed, as we have noted in the introduction, our approach is driven by the idea of incorporating this prior (expert) information into our hybrid model.
In the current work, we assume that the pharmacological model in Equation 3 is correct. In practice, the model might be wrong in two ways. The obvious way is that the functional form of f e might be misspeciﬁed (e.g. a linear model might be speciﬁed when the truth is actually nonlinear). Many existing techniques can address such misspeciﬁcation and could be integrated into LHM [35, 64, 98]; see the discussion in Appendix A.7. Alternatively, it might be that the system of expert variables is not self-contained, and that their evolution actually depends on additional latent variables, we leave this more challenging problem for future work.
Practical extensions to LHM such as including static covariates and modeling informative sampling are discussed in Appendix A.7. 3.3
Independent and informative priors
It may be challenging to pinpoint the exact value of the latent variables ze based on observations (e.g. due to measurement noise or sampling irregularity). For this reason, we quantify the uncertainty around ze using Bayesian inference. In what follows, we assume the initial states zi(0) of patients are independently sampled from a prior distribution zi(0) ∼ P0. Two points are worth noting.
Independent Priors. We use independent prior distributions on the expert variables ze and the latent variables zm, i.e. P(z(0)) = P(ze(0)) × P(zm(0)). This guarantees that information in zm(0) does not duplicate (any of the) information in ze(0), which captures our belief that the latent variables are incremental to the expert variables. In addition, independent priors are also commonly used in
Bayesian latent variable models such as variational autoencoders (VAEs) [47, 37].
Informative Priors The prior distribution on the expert variables P(ze(0)) should reﬂect domain knowledge. Such knowledge is usually available from previous studies in Pharmacology [38]. Using an informative prior tends to improve the estimation of latent variables, especially in small-sample settings [52]. Moreover, the expert variables usually take values in speciﬁc ranges (e.g. [0, 10] [42]) and going beyond the valid range may lead to divergence. The informative prior can encode such prior knowledge to stabilize training. 4
3.4 Model training and prediction via amortized variational inference
Given the training dataset D = {(Yi[0 : T ], Ai[0 : T ])}i∈[N ], we use amortized variational inference (AVI) to estimate the global parameters Θ and the unknown initial condition zi(0) [97]. Figure 2 presents a diagram of the training procedure. We start by learning a variational distribution to approximate the posterior P(zi(0)|Yi[0 : T ], Ai[0 : T ]). As is standard in AVI [47, 97], we use a
Normal distribution with diagonal covariance matrix as approximation:
Q(zi(0)|Yi[0 : T ], Ai[0 : T ]) = N (µi, Σi); µi, Σi = e(Yi[0 : T ], Ai[0 : T ]; φ)
Here the parameters µi, Σi are produced by an inference network (also known as an encoder) e(·) with trainable weights φ. When the context is clear, we will denote the variational distribution deﬁned by Equations (7) as Qφ. The evidence lower bound (ELBO) for the global parameter Θ and the inference network parameters φ is deﬁned as (7)
ELBO(Θ, φ) = Ez(0)∼Qφ (cid:2)logP(Yi[0 : T ]|Ai[0 : T ], z(0), Θ)(cid:3) − KL[Qφ|P0] (8)
To compute the ELBO for a given Θ and φ, we sample z(0) ∼ Qφ and numerically solve the ODEs to obtain z(t), ∀t ∈ [0, T ] (Figure 2; Steps 1, 2). Then, we compute the inner log-likelihood function using the mapping g and the noise distribution (Equations (1) and (4)). Finally, we use Monte Carlo sampling to evaluate the KL divergence term: Ez(0)∼Qφ[logQφ(z(0))−logP0(z(0))]. This is because the informative prior P0 may not have an analytical KL divergence (unlike the standard Normal prior used in previous works [47, 72]). We optimize ELBO by stochastic gradient ascent and update all parameters jointly in an end-to-end manner (detailed in Appendix A.3).
The prediction procedure follows the same steps as illustrated in Figure 2. For a new patient with history Yi[0 : t0], Ai[0 : t0], we ﬁrst estimate the variational posterior Qφ using the trained encoder.
From Equation (6), we can estimate the target distribution in Equation (2) as: (cid:2)P(cid:0)y(t)|z(0), Ai[0 : t0], Ai[t0 : T ](cid:1)(cid:3), ∀t > t0.
Ez(0)∼Qφ (9) where Ai[t0 : T ] is a future treatment plan. The outer expectation can be approximated by Monte
Carlo sampling from Qφ and the inner probability is given by the likelihood function.
Choice of variational distribution and encoder. The training procedure above is agnostic to the exact choice of variational distribution and encoder architecture. We choose the Normal distribution to make fair comparisons with the previous works [14, 72] . For the same reason, we use the reversed time-aware LSTM encoder proposed in [14]. In the Appendix A.4, we show additional experiments with more complex variational distributions, i.e. Normalizing Flows [70]. 3.5 Using LHM to provide clinical decision support
In order for clinicians to optimally treat patients, they need to predict the progression of disease given the treatments. Although machine learning models may demonstrate feature importance [16, 4], they do not uncover the relationships between those features and the underlying pathophysiology.
In
LHM can provide the missing link between clinical observations and disease mechanisms. combination with clinical reasoning, it can provide treating clinicians with decision support in several complementary ways.
First, LHM can inform the clinicians about the values of the expert variables ze(t) that cannot be observed in the clinical environment but are important for prognosis, choice of treatment, and anticipation of complications. For example, understanding and predicting immune response is pivotal when deciding on immunosuppresive therapy in the treatment of COVID-19: an extreme immune response may lead to a potentially fatal cytokine storm [24], but a suppressed immune response may be equally dangerous in case of (secondary) infection [48, 84]. However, because immune response is not directly observable in the clinical environment, clinicians must rely on proxies such as C-reactive protein (CRP) for inﬂammation [58]; by their very nature, such proxies are noisy and highly imperfect measures of the desired values.
Secondly, LHM can provide the clinician with predictions of the disease progression given the treatments, enabling the clinicians to design the best treatment plan for the patient at hand.
Finally, LHM can bridge the gap between the laboratory and clinical environments, helping to align model output with clinical reasoning, and thus to bring models to the patient bedside and also to foster translational research [78, 28]. 5
4