Abstract
The ﬁeld of neuromorphic computing promises extremely low-power and low-latency sensing and processing. Challenges in transferring learning algorithms from traditional artiﬁcial neural networks (ANNs) to spiking neural networks (SNNs) have so far prevented their application to large-scale, complex regression tasks. Furthermore, realizing a truly asynchronous and fully neuromorphic pipeline that maximally attains the abovementioned beneﬁts involves rethinking the way in which this pipeline takes in and accumulates information. In the case of perception, spikes would be passed as-is and one-by-one between an event camera and an
SNN, meaning all temporal integration of information must happen inside the network. In this article, we tackle these two problems. We focus on the complex task of learning to estimate optical ﬂow from event-based camera inputs in a self-supervised manner, and modify the state-of-the-art ANN training pipeline to encode minimal temporal information in its inputs. Moreover, we reformulate the self-supervised loss function for event-based optical ﬂow to improve its convexity.
We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We ﬁnd that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner. 1

Introduction
Neuromorphic hardware promises highly energy-efﬁcient and low-latency sensing and processing thanks to its sparse and asynchronous nature. Event cameras capture brightness changes at microsec-ond resolution [17], while neuromorphic processors have demonstrated orders of magnitude lower energy consumption and latency compared to von Neumann architectures [11, 29]. To realize the full potential of such neuromorphic pipelines, we have to move towards an event-based communication and processing paradigm, where single events are passed as-is between the event-based sensor/camera and the neuromorphic processor running a spiking neural network (SNN), without processing or
∗Equal contribution, with alphabetical ordering. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Self-supervised event-based optical ﬂow pipeline for deep SNNs. In order of processing, the event stream is split into small partitions with the same number of events, which are formatted and then fed to the network in a sequential fashion. An optical ﬂow map is predicted for each partition, associating every input event with a motion vector. Once a sufﬁcient number of events has been processed, we perform a backward pass using our contrast maximization loss [16]. accumulation of any kind in between. Because of this, all temporal integration of information needs to happen inside the network itself. Most work on employing SNNs to event-based computer vision follows this approach [10, 14], but is limited to problems of limited temporal complexity (like classiﬁcation). On the other hand, most state-of-the-art artiﬁcial neural network (ANN) pipelines for event-based computer vision combine a stateless feedforward architecture with encoding temporal information in the input [54, 56].
Apart from incompatible pipelines, one of the larger impediments to widespread neuromorphic adoption is the fact that learning algorithms designed for traditional ANNs do not transfer one-to-one to SNNs, which exhibit sparse, binary activity and more complex neuronal dynamics. On the one hand, this has driven research into the conversion of ANNs to SNNs without loss of accuracy, but with the promised efﬁciency gains [42]. On the other hand, it has limited the application of directly-trained
SNNs in the computer vision domain to less complicated and often discrete problems like image classiﬁcation [10, 14] on constrained datasets such as N-MNIST [33] or DVS128 Gesture [2].
Still, many ongoing developments in the area of direct SNN training are promising and may form building blocks for tackling more complex tasks. Surrogate gradients [32, 41, 45, 51], which act as stand-in for the non-differentiable spiking function in the backward pass, enable traditional backpropagation with few adjustments. Similarly, the inclusion of parameters governing the neurons’ internal dynamics in the optimization was demonstrated to be beneﬁcial [14, 38]. Many works also include some form of activity regularization to keep neurons from excessive spiking [5, 51] or to balance excitability through adaptation [4, 36]. Kickstarting initial activity (hence gradient ﬂow) is often not the goal of these regularization terms, even though [51] shows that there is a narrow activity band in which learning is optimal. This ties in with the initialization of parameters, which has not been rigorously covered for SNNs with sparse inputs yet, leaving room for improvement.
Our goal is to demonstrate the potential of neuromorphic sensing and processing on a complex task.
To this end, we tackle a real-world large-scale problem by learning, in a self-supervised fashion and using SNNs, to estimate the optical ﬂow encoded in a continuous stream of events; a task that is usually tackled with deep, fully convolutional ANNs [54, 56]. By focusing on such a problem, we aim to identify and tackle emerging knowledge gaps regarding SNN training, while approximating a truly asynchronous pipeline.
In summary, the main contribution of this article is two-fold. First, we propose a novel self-supervised learning (SSL) framework for event-based optical ﬂow estimation that puts emphasis on the networks’ capacity to integrate temporal information from small, subsequent slices of events. This training 2
pipeline, illustrated in Fig. 1, is built around a reformulation of the self-supervised loss function from
[56] that improves its convexity. Second, through this framework, we train the ﬁrst set of deep SNNs that successfully solve the problem at hand. We validate our proposals through extensive quantitative and qualitative evaluations on multiple datasets1. Additionally, for the SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. 2