Abstract
We consider the setting of vector valued non-linear dynamical systems Xt+1 =
φ(A∗Xt) + ηt, where ηt is unbiased noise and φ : R → R is a known link function that satisﬁes certain expansivity property. The goal is to learn A∗ from a single trajectory X1, · · · , XT of dependent or correlated samples. While the problem is well-studied in the linear case, where φ is identity, with optimal error rates even for non-mixing systems, existing results in the non-linear case hold only for mixing systems. In this work, we improve existing results for learning nonlinear systems in a number of ways: a) we provide the ﬁrst ofﬂine algorithm that can learn non-linear dynamical systems without the mixing assumption, b) we signiﬁcantly improve upon the sample complexity of existing results for mixing systems, c) in the much harder one-pass, streaming setting we study a SGD with Reverse Experience Replay (SGD − RER) method, and demonstrate that for mixing systems, it achieves the same sample complexity as our ofﬂine algorithm, d) we justify the expansivity assumption by showing that for the popular ReLU link function — a non-expansive but easy to learn link function with i.i.d. samples — any method would require exponentially many samples (with respect to dimension of Xt) from the dynamical system. We validate our results via simulations and demonstrate that a naive application of SGD can be highly sub-optimal. Indeed, our work demonstrates that for correlated data, specialized methods designed for the dependency structure in data can signiﬁcantly outperform standard SGD based methods. 1

Introduction
Non-linear dynamical systems (NLDS) are commonly used to model the data in a variety of domains like control theory, time-series analysis, and reinforement learning (RL) [1–4]. Standard NLDS models the data points (X0, X1, . . . , XT ) as:
Xt+1 = φ(A∗Xt) + ηt, (1) where Xt ∈ Rd are the states, ηt ∈ Rd are i.i.d. noise vectors, A∗ ∈ Rd×d and φ : R → R is an increasing function called the ‘link function’. Here, φ is supposed to act component wise over Rd.
System identiﬁcation problem is a foundational problem for NLDS, i.e., given (X0, X1, . . . , XT ) generated from (1), the goal is to estimate A∗ accurately from a single trajectory (X0, X1, . . . , XT ). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The system identiﬁcation problem is heavily studied in control theory [5–8] as well as time-series analysis [9]. For instance, the non-linear dynamical system considered here has an application in modeling non-linear distortions in power ampliﬁers [10]. The problem is challenging as data points
X0, X1, . . . , XT are not i.i.d. as usually encountered in machine learning, but form a Markov process.
If the mixing time τmix of the process is ﬁnite (τmix < ∞), then we can make the data approximately i.i.d. by considering only the points separated by ˜O(τmix) time. While this allows using standard techniques for i.i.d. data, it reduces the effective number of samples to O( T
), which typically gives
τmix an error of the order O( τmix
T ). In fact, even the state-of-the-art results have error bounds which are sub-optimal by a factor of τmix.
Interestingly, for the special case of linear systems, i.e., when φ(x) = x, the results are signiﬁcantly stronger. For example, [11, 12] showed that the matrix A∗ can be estimated with an error O(1/T ) even when the mixing time τmix > T . But these results rely on the fact that for linear systems, the estimation problem reduces to an ordinary least squares (OLS) problem for which a closed form expression is available and can be analyzed effectively.
On the other hand, NLDS do not admit such closed form expressions. In fact the existing techniques mostly rely on mixing time arguments to induce i.i.d. like behavior in a subset of the points which leads to sub-optimal rates by τmix factor. Similarly, a direct application of uniform convergence results [13] to show that the minimizer of the empirical risk is close to the population minimizer still gives sub-optimal rates as off-the-shelf concentration inequalities (cf. [14]) incur an additional factor of mixing time. Finally, existing results are mostly focused on ofﬂine setting, and do not apply to the case where the data points are streaming which is critical in several practical problems like reinforcement learning (RL) and control theory.
In this work, we provide algorithms and their corresponding error rates for the NLDS system identiﬁcation problem in both ofﬂine and online setting, assuming the link function to be expansive (Assumption 1). The main highlight of our results is that the error rates are independent of the mixing time τmix, which to the best of our knowledge is ﬁrst such result for any non-linear system identiﬁcation in any setting. In fact, for ofﬂine setting, our analysis holds even for systems which do not mix within time T and even for marginally stable systems which do not mix at all. Furthermore, we analyze SGD-Reverse Experience Replay (SGD-RER) method, we provide the ﬁrst streaming method for NLDS identiﬁcation with error rate that is independent of τmix (in the leading order term) while still ensuring small space and time complexity. This algorithm was ﬁrst discovered in the experimental RL setting in [15] based on Hippocampal reverse replay observed in biological networks
[16–18]. It was introduced independently in [19] for the case of linear systems and efﬁciently unravels the complex dependency structure present in the problem. Finally, through a lower bound for ReLU— a non-expansive function—we provide strong justiﬁcation for why expansivity might be necessary for a non-trivial result.
Instead of mixing time arguments, our proofs for learning NLDS without mixing use a natural exponential martingale of the kind considered in the analysis of self normalized process ([20, 21]).
For streaming setting, while we do use mixing time arguments (proof of Theorems 2 and 3), we combine them with a delicate stability analysis of the speciﬁc algorithm and the machinery developed in [19] to obtain strong error bounds. See Section 6 for a description of these techniques.
Our Contributions. Key contributions of the paper are summarized below: 1. Assuming expansive and monotonic link function φ and sub-Gaussian noise, we show that the ofﬂine Quasi Newton Method (Algorithm 1) estimates the parameter A∗ with near optimal errors of the order O(1/T ), even when the dynamics does not mix within time T . 2. Assuming mixing NLDS, ﬁnite fourth moment on the noise, and expansive monotonic link function, we show that ofﬂine Quasi Newton Method again estimates the parameter A∗ with near-optimal error of O(1/T ), independent of mixing time τmix. 3. We give a one-pass, streaming algorithm inspired by SGD − RER method by [19], and show that it achieves near-optimal error rates under the assumption of sub-Gaussian noise, NLDS stability (see section 2.1 for the deﬁnition), uniform expansivity and second differentiability of the link function. 4. We then show that learning with ReLU link function, which is non-expansive but is known to be easy to learn with if data points are all i.i.d. [22], requires exponential (in d) many samples. 2
We believe that the techniques developed in this work can be extended to provide efﬁcient algorithms for learning with dependent data in more general settings.