Abstract
Precisely annotating objects with multiple labels is costly and has become a crit-ical bottleneck in real-world multi-label classiﬁcation tasks. Instead, deciding the relative order of label pairs is obviously less laborious than collecting exact labels. However, the supervised information of pairwise relevance ordering is less informative than exact labels. It is thus an important challenge to effectively learn with such weak supervision. In this paper, we formalize this problem as a novel learning framework, called multi-label learning with pairwise relevance ordering (PRO). We show that the unbiased estimator of classiﬁcation risk can be derived with a cost-sensitive loss only from PRO examples. Theoretically, we provide the estimation error bound for the proposed estimator and further prove that it is consistent with respect to the commonly used ranking loss. Empirical studies on multiple datasets and metrics validate the effectiveness of the proposed method. 1

Introduction
Multi-label learning (MLL) solves problems where each object is assigned with multiple class labels simultaneously [Zhang and Zhou, 2013]. For example, an image may be annotated with labels building, street and person. The goal of multi-label learning is to train a classiﬁcation model that can predict all the relevant labels for unseen instances. A large number of recent works have witnessed the great successes that MLL has achieved in many real-world applications, e.g., image annotation
[Chen et al., 2019], human attribute recognition [Li et al., 2016], user proﬁling [Liu et al., 2021], and protein function prediction [Elisseeff and Weston, 2002].
Traditional multi-label learning studies assume that each instance has been precisely annotated with all of its relevant labels. However, in many real-world scenarios, it is difﬁcult and costly to collect the precise annotations. Instead, each instance may be provided with the relative order of label pairs, where each label pair y (cid:31) y(cid:48) (or y ≺ y(cid:48)) indicates that label y is more relevant (or irrelevant) than label y(cid:48) to instance x, i.e., p(y = 1|x) > p(y(cid:48) = 1|x) (or p(y = 1|x) < p(y(cid:48) = 1|x)). Generally, deciding the relative order of label pairs would be much easier than collecting the precise annotations and thus less costly. For example, in medical image analysis, only experts with rich experiences can accurately identify the disease for a patient based on the medical image. In contrast, if the question is to decide which of two given diseases is more likely suffered by the patient, then even medical students with basic knowledge may easily provide the answer. While the annotation cost is signiﬁcantly reduced with pairwise relevance ordering, the learning task becomes more challenging, since the supervised information of pairwise relevance ordering is much less than exact labels.
We formalize this learning problem as a new framework called multi-label learning with pairwise relevance ordering (PRO). Speciﬁcally, PRO attempts to learn a classiﬁcation model from multi-label
∗Correspondence to: Sheng-Jun Huang (huangsj@nuaa.edu.cn). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
examples with the relative order of label pairs, where each label pair y (cid:31) y(cid:48) indicates three possible cases: 1) both y and y(cid:48) are relevant to x, i.e., y = 1, y(cid:48) = 1; 2) y is relevant to x while y(cid:48) is not, i.e., y = 1, y(cid:48) = 0; 3) both y and y(cid:48) are irrelevant to x, i.e., y = 0, y(cid:48) = 0.
PRO is a novel learning framework with signiﬁcant difference from exiting settings. For example, semi-supervised multi-label learning (SSMLL) learns a classiﬁer by exploiting a few of labeled examples as well as a large number of unlabeled examples [Liu et al., 2006]; multi-label learning with missing labels (MLML) assumes that only a subset of labels are available for each instance [Sun et al., 2010, Yu et al., 2014]; partial multi-label learning (PML) assigns each instance with a candidate label set [Xie and Huang, 2018, Zhang and Fang, 2020]; multi-label learning with noisy labels assumes that multiple class labels may be ﬂipped simultaneously with their respective probabilities [Xie and
Huang, 2021a]. However, these frameworks do not consider multi-label examples with pairwise relevance ordering, and cannot be employed to solve PRO problems.
To deal with multi-label data with pairwise relevance ordering, we propose a cost-sensitive loss function for learning a multi-label classiﬁer with empirical risk minimization. Theoretically, we show that the unbiased estimator of classiﬁcation risk can be derived from only PRO examples if the surrogate loss function satisﬁes a mild condition, i.e., the symmetric condition. The estimation error bound is established for the unbiased estimator, showing that learning with PRO examples can be multi-label consistent to the commonly used ranking loss. Extensive experimental results on multiple datasets and evaluation metrics demonstrate the practical usefulness of the proposed method. 2