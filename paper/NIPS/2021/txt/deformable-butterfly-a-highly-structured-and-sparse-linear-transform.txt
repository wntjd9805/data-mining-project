Abstract
We introduce a new kind of linear transform named Deformable Butterﬂy (DeBut) that generalizes the conventional butterﬂy matrices and can be adapted to various input-output dimensions. It inherits the ﬁne-to-coarse-grained learnable hierarchy of traditional butterﬂies and when deployed to neural networks, the prominent structures and sparsity in a DeBut layer constitutes a new way for network com-pression. We apply DeBut as a drop-in replacement of standard fully connected and convolutional layers, and demonstrate its superiority in homogenizing a neural network and rendering it favorable properties such as light weight and low inference complexity, without compromising accuracy. The natural complexity-accuracy tradeoff arising from the myriad deformations of a DeBut layer also opens up new rooms for analytical and practical research. The codes and Appendix are publicly available at: https://github.com/ruilin0212/DeBut. 1

Introduction
The linear mapping in deep neural networks (DNNs) are mostly realized in the form of fully connected (FC) or convolutional (CONV) layers. These layers, together with feed-forward or feedback paths and nonlinear activations, then induce a plethora of neural architectures such as multilayer perceptrons (MLPs) [29, 23, 12], convolutional neural networks (CNNs) [11, 18, 31, 32], recurrent neural networks (RNNs) [3, 13, 30] and Transformers [33, 9], just to name a few. Although latest researches have designed and constructed highly capable networks (such as the GPT-3 [1]), the nature of
DNNs still remains largely black-box and inaccessible due to its extreme nonlinearity. In fact, even the seemingly trivial linear transform can become rather non-trivial when structures are imposed.
Prominent examples are the Fast Fourier Transform (FFT) and convolution operators which can be cast as linear mappings and formulated as matrix multiplications. Yet they are distinguished by their underlying butterﬂy and circulant structures, respectively, that act as strong structural priors and lead to very distinct behaviors.
Nonetheless, relatively little attention has been paid to deriving adaptable, or even better, learnable and structured fast linear transforms in DNNs. Among the works to compactly parametrize an FC layer in a DNN, a representative method is called the Fastfood Transform [20] that belongs to the category of kernel methods [4, 26] and employs random projection for feature learning. In particular,
Fastfood uses Hadamard transform combined with diagonal Gaussian matrices to approximate a
Gaussian random matrix. Adaptive Fastfood [34] further allows those diagonal matrices to become
∗RL, JR, and NW contributed equally to this work. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
learnable, thus achieving better approximation of an FC layer during backpropagation. Recently, learnable butterﬂy matrices are also proposed [5, 6] and demonstrated to be an effective FC substitute delivering high accuracy with much fewer parameters owing to the highly sparse butterﬂy factors.
However, all these aforementioned schemes are restricted to powers-of-two (PoT) construction and thereby square structures, in which unequal input-output dimensions are handled simply by stacking square matrices followed by ad hoc dropping of inputs/outputs. Moreover, all these works aim at replacing only one or a few largest FCs in a neural network, which may not yield a signiﬁcant compression in the overall number of weight parameters especially in modern networks having a majority of CNN layers. Even worse, the training time of such FC replacement may end up prohibitive (cf. Experimental Section) as constrained by the high-dimensional square matrices.
On the other hand, the proliferation of machine learning and artiﬁcial intelligence (AI) in recent years has spawned the era of edge AI whereby the DNN inference, and sometimes its training, are performed on edge devices with limited compute and storage resources. Numerous researchers have looked into compacting neural networks. In general, modern neural network compression techniques mainly fall into three categories, namely, low-rank matrix/tensor decomposition (e.g., [25, 16, 27]), weight and/or CNN ﬁlter pruning (e.g., [10, 22]) and low bitwidth quantization (e.g., [14, 28, 2, 24]).
To this end, network compression via sparse and structured matrix factorization does not fall into any of these categories, and constitutes a new yet under-explored way of neural network compression. As revealed in [5], such structured and sparse matrix factor chains (such as the FFT butterﬂies) are often accompanied by fast inference due to their recursive nature and the availability of fast matrix-vector multiplication schemes [7].
Subsequently, this paper attempts to kill two birds (viz. learnable factorized linear transform with structured sparsity and ﬂexible input-output sizes) with one stone by introducing a novel linear transform named deformable butterﬂy (DeBut) that generalizes the square PoT butterﬂy factor matrices [5, 6]. Speciﬁcally, a DeBut product chain (or simply a DeBut chain) can be sized to adapt to different input-output dimensions. For one thing, it does not limit itself to PoT blocks as in Fastfood
Transform or butterﬂy matrices [20, 34, 5, 6]. Moreover, the intermediate matrix dimensions in a
DeBut chain can either shrink or grow to permit a variable tradeoff between number of parameters and representation power. The ﬂexibility of tuning the dense matrix sub-blocks in DeBut also permits an interpretation similar to the CNN receptive ﬁeld for exploiting locality and correlation in data. In fact, as will be shown in experiments, a DeBut chain does not distinguish CONV or FC layers, and can be used as a substitute of both while maintaining a high output accuracy. To our knowledge, the
DeBut linear transform is proposed for the ﬁrst time, and this work is a starter to showcase its use in
DNNs which we hope can provoke further theoretical and practical insights. 2