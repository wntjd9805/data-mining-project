Abstract
We consider a novel data driven approach for designing semi-supervised learning algorithms that can effectively learn with only a small number of labeled examples.
We focus on graph-based techniques, where the unlabeled examples are connected in a graph under the implicit assumption that similar nodes likely have similar labels.
Over the past two decades, several elegant graph-based semi-supervised learning algorithms for inferring the labels of the unlabeled examples given the graph and a few labeled examples have been proposed. However, the problem of how to create the graph (which impacts the practical usefulness of these methods signiﬁcantly) has been relegated to heuristics and domain-speciﬁc art, and no general principles have been proposed. In this work we present a novel data driven approach for learning the graph and provide strong formal guarantees in both the distributional and online learning formalizations. We show how to leverage problem instances coming from an underlying problem domain to learn the graph hyperparameters for commonly used parametric families of graphs that provably perform well on new instances from the same domain. We obtain low regret and efﬁcient algorithms in the online setting, and generalization guarantees in the distributional setting.
We also show how to combine several very different similarity metrics and learn multiple hyperparameters, our results hold for large classes of problems. We expect some of the tools and techniques we develop along the way to be of independent interest, for data driven algorithms more generally. 1

Introduction
In recent years machine learning has found gainful application in diverse domains. A major bottleneck of the currently used approaches is the heavy dependence on expensive labeled data. Advances in cheap computing and storage have made it relatively easier to store and process large amounts of unlabeled data. Therefore, an important focus of the present research community is to develop general domain-independent methods to learn effectively from the unlabeled data, along with a small amount of labels. Achieving this goal would signiﬁcantly elevate the state-of-the-art machine intelligence, which currently lags behind the human capability of learning from a few labeled examples. Our work is a step in this direction, and provides algorithms and guarantees that enable fundamental techniques for semi-supervised learning to provably adapt to problem domains.
Graph-based approaches have been popular for learning from unlabeled data for the past two decades
[Zhu and Goldberg, 2009]. Labeled and unlabeled examples form the graph nodes and (possibly weighted) edges denote the feature similarity between examples. The graph therefore captures how each example is related to other examples, and by optimizing a suitably regularized objective over it one obtains an efﬁcient discriminative, nonparametric method for learning the labels. There are several well-studied ways to deﬁne and regularize an objective on the graph [Chapelle et al., 2010], 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Optimization objectives for graph-based SSL. Dij := I[i = j] (cid:80)
W )D−1/2 and the objective is l(f ) = α (cid:80) u∈L(f (u) − yu)2 + βH(f, W ) + γ (cid:107)f (cid:107)2. k Wik, L := D−1/2(D −
Algorithm (α, β, γ) H(f, W ), (cid:107)·(cid:107)
Mincut
Harmonic function
Normalized cut
Label propagation (∞, 1, 0) (∞, 1, 0) (∞, 1, 0) (1, µ, 1) f T (D − W )f f T (D − W )f f T (D − W )f f T Lf , (cid:107)·(cid:107)2
Constraints on f f ∈ {0, 1}n f ∈ [0, 1]n f T 1 = 0, f T f = n2, f ∈ [0, 1]n f ∈ [0, 1]n and all yield comparable results which strongly depend on the graph used. A general formulation is described as follows, variations on which are noted under related work. v∈L
Problem formulation Given sets L and U of labeled and unlabeled examples respectively, and a similarity metric d over the data, the goal is to use d to extrapolate labels in L to U . A graph G is constructed with L + U as the nodes and weighted edges W with w(u, v) = g(d(u, v)) for some g : R≥0 → R≥0. We seek labels f (·) for nodes u of G which minimize a regularized loss function l(f ) = α (cid:80)
ˆl(f (v), yv) + βH(f, W ) + γ (cid:107)f (cid:107)2, under some constraints on f . The objective
H captures the smoothness (regularization) induced by the graph (see Table 1 for examples) and
ˆl(f (v), yv) is the misclassiﬁcation loss (computed here on labeled examples).
The graph G takes a central position in this formulation. However, the majority of the research effort on this problem has focused on how to design and optimize the regularized loss function l(f ), the effectiveness of which crucially depends on G. There is no known principled study on how to build
G and prior work largely treats this as a domain-speciﬁc art [Chapelle et al., 2010]. Is it possible to acquire the required domain expertise, without involving human experts? In this work we provide an afﬁrmative answer by formulating graph selection as data-driven design. More precisely, we are required to solve not only one instance, but multiple instances of the underlying algorithmic problem that come from the same domain [Gupta and Roughgarden, 2016, Balcan, 2020]. We show learning a near-optimal graph over commonly used inﬁnite parameterized families is possible in both online and distributional settings. In the process we generalize and extend data-driven learning techniques, and obtain practical methods to build the graphs with strong guarantees. In particular, we show how the techniques can learn several parameters at once, and also learn a broader class of parameters than previously known.
Our contributions and key challenges. We present a ﬁrst theoretically grounded work for graph-based learning from limited labeled data, while extending general data-driven design techniques.
Data-driven algorithm design. Firstly, for one dimensional loss functions, we show a novel structural result which applies when discontinuities (for loss as function of the algorithm parameter) occur along roots of exponential polynomials with random coefﬁcients with bounded joint distributions (previously known only for algebraic polynomials in Balcan et al. [2020b]). This is crucial for showing learnability in the Gaussian graph kernels setting. Secondly, Balcan et al. [2020b] only applies when the discontinuities occur along algebraic curves with random coefﬁcients in just two dimensions. By a novel algebraic and learning theoretic argument we are able to analyze higher (arbitrary constant number of) dimensions, making the technique much more generally applicable.
Semi-supervised learning. We examine commonly used parameterized graph families, denoted by general notation G(ρ), where ρ corresponds to a semi-supervised learning algorithm. We consider online and distributional settings, providing efﬁcient algorithms to obtain low regret and low error re-spectively for learning ρ. Most previously studied settings involve polynomially many discontinuities for loss as function of the hyperparameter ρ on a ﬁxed instance, implying efﬁcient algorithms, which may not be the case for our setting. To resolve this, we describe efﬁcient semi-bandit implementa-tions, and in particular introduce a novel min-cut and ﬂow recomputation algorithm on graphs with continuously changing edge weights which may be of independent interest. For the distributional setting, we provide asymptotically tight bounds on the pseudodimension of the parameter learning problem. Our lower bounds expose worst case challenges, and involve precise constructions of problem instances by setting node similarities which make assigning labels provably hard. 2
Our techniques are extremely general and are shown to apply for nearly all combinations of optimiza-tion algorithms (Table 1) and parametric graph families (Deﬁnition 1).