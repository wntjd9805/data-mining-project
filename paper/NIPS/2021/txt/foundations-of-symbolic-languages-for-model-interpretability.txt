Abstract
Several queries and scores have been proposed to explain individual predictions made by ML models. Examples include queries based on “anchors”, which are parts of an instance that are sufﬁcient to justify its classiﬁcation, and “feature-perturbation” scores such as SHAP. Given the need for ﬂexible, reliable, and easy-to-apply interpretability methods for ML models, we foresee the need for developing declarative languages to naturally specify different explainability queries. We do this in a principled way by rooting such a language in a logic called FOIL, that allows for expressing many simple but important explainability queries, and might serve as a core for more expressive interpretability languages. We study the computational complexity of FOIL queries over classes of ML models often deemed to be easily interpretable: decision trees and more general decision diagrams. Since the number of possible inputs for an ML model is exponential in its dimension, tractability of the FOIL evaluation problem is delicate, but can be achieved by either restricting the structure of the models, or the fragment of FOIL being evaluated.
We also present a prototype implementation of FOIL wrapped in a high-level declarative language, and perform experiments showing that such a language can be used in practice. 1

Introduction
Context. The degree of interpretability of a machine learning (ML) model seems to be intimately related with the ability to “answer questions” about it. Those questions can either be global (behavior of the model as a whole) or local (behavior regarding certain instances/features). Concrete examples of such questions can be found in the recent literature, including, e.g., queries based on “anchors”, which are parts of an instance that are sufﬁcient to justify its classiﬁcation [4, 13, 16, 32], and numerical scores that measure the impact of the different features of an instance on its result [24, 31, 36].
It is by now clear that ML interpretability admits no silver-bullet [18], and that in many cases a combination of different queries may be the most effective way to understand a model’s behavior.
Also, model interpretability takes different ﬂavors depending on the application domain one deals with. This naturally brings to the picture the need for general-purpose speciﬁcation languages that can provide ﬂexibility and expressiveness to practitioners specifying interpretability queries. An even more advanced requirement for these languages is to be relatively easy to use in practice. This tackles the growing need for bringing interpretability methods closer to users with different levels of expertise. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Stable job
> 40yo
Previous loans
Owns a house
Has kids
Married
Criminal Record








 0 1
 1

 0

 0

 1 1
Black Box
Model
Application
Rejected
> load("mlp.np") as MyModel;
> show features; (stableJob, >40yo, prevLoan, ownsHouse, hasKids, isMarried, crimRecord): Boolean
> show classes;
Rejected (0), Accepted (1)
> exists person, person.isMarried and not person.hasKids and MyModel(person) = Accepted; (a) Diagram of a particular loan decision.
YES
Figure 1: Example of a bank that uses a model to decide whether to accept loan applications considering binary features like “does the requester have a stable job” and “are they older than 40”? (b) Example of a possible concrete syntax for a language tailored for interpretability queries.
One way in which these requirements can be approached in a principled way is by developing a declarative interpretability language, i.e., one in which users directly express the queries they want to apply in the interpretability process (and not how these queries will be evaluated). This is of course reminiscent of the path many other areas in computer science have followed, in particular by using languages rooted in formal logic; so has been the case, e.g., in data management [1], knowledge representation [3], and model checking [15]. One of the advantages of this approach is that logics have a well-deﬁned syntax and clear semantics. On the one hand, this ensures that the obtained explanations are provably sound and faithful to the model, which avoids a signiﬁcant drawback of several techniques for explaining models in which the explanations can be inaccurate, or require themselves to be further explained [33]. On the other hand, a logical root facilitates the theoretical study of the computational cost of evaluation and optimization for the queries in the language.
Our proposal. Our ﬁrst contribution is the proposal of a logical language, called FOIL, in which many simple yet relevant interpretability queries can be expressed. We believe that FOIL can further serve as a basis over which more expressive interpretability languages can be built, and we propose concrete directions of research towards its expansion. In a nutshell, given a decision model M that performs classiﬁcation over instances e of dimension n, FOIL can express properties over the set of all partial and full instances of dimension n. A partial instance e is a vector of dimension n in which some features are undeﬁned. Such undeﬁned features take a distinguished value ⊥. An instance is full if none of its features is undeﬁned. The logic FOIL is simply ﬁrst-order logic with access to two predicates on the set of all instances (partial or full) of dimension n: A unary predicate POS(e), stating that e is a full instance that M classiﬁes as positive, and a binary predicate e ⊆ e(cid:48), stating that instance e(cid:48) potentially ﬁlls some of the undeﬁned features of instance e; e.g., (1, 0, ⊥) ⊆ (1, 0, 1), but (1, 0, ⊥) (cid:54)⊆ (1, 1, 1).
As an overview of our proposal, consider the case of a bank using a binary model to judge applications for loans. Figure 1a illustrates the problem with concrete features, and Figure 1b presents an example of a concrete interactive syntax. In Figure 1b, after loading and exploring the model, the interaction asks whether the model could give a loan to a person who is married and does not have kids. Assuming that the “Accepted” class is the positive one, this interaction can easily be formalized in FOIL by means of the query ∃x (cid:0)POS(x) ∧ (⊥, ⊥, ⊥, ⊥, 0, 1, ⊥) ⊆ x(cid:1).
Theoretical contributions. The evaluation problem for a ﬁxed FOIL query ϕ is as follows. Given a decision model M, is it true that ϕ is satisﬁed under the interpretation of predicates ⊆ and POS deﬁned above? An important caveat about this problem is that, in order to evaluate ϕ, we need to potentially look for an exponential number of instances, even if the features are Boolean, thus rendering the complexity of the problem infeasible in some cases. Think, for instance, of the query
∃x POS(x), which asks if M has at least one positive instance. Then this query is intractable for every class of models for which this problem is intractable; e.g., for the class of propositional formulas in
CNF (notice that this is nothing but the satisﬁability problem for the class at hand).
The main theoretical contribution of our paper is an in-depth study of the computational cost of FOIL on two classes of Boolean models that are often deemed to be “easy to interpret”: decision trees and ordered binary decision diagrams (OBDDs) [10, 14, 19, 28, 33]. An immediate advantage of 2
these models over, say, CNF formulas, is that the satisﬁability problem for them can be solved in polynomial time; i.e., the problem of evaluating the query ∃xPOS(x) is tractable. Our study aims to (a) “measure” the degree of interpretability of said models in terms of the formal yardstick deﬁned by the language FOIL; and (b) shed light on when and how some simple interpretability queries can be evaluated efﬁciently on these decision models.
We start by showing that, in spite of the aforementioned claims on the good level of interpretability for the models considered, there is a simple query in FOIL that is intractable over them. In fact, such an intractable query has a natural “interpretability” ﬂavor, and thus we believe this proof to be of independent interest.
However, these intractability results should not immediately rule out the use of FOIL in practice. In fact, it is well known that a logic can be intractable in general, but become tractable in practically relevant cases. Such cases can be obtained by either restricting the syntactic fragment of the logic considered, or the structure of the models in which the logic is evaluated. We obtain positive results in both directions for the models we mentioned above. We explain them next.
Syntactic fragments. We show that queries in ∃FOIL, the existential fragment of FOIL, admit tractable evaluation over the models we study. However, this language lacks expressive power for capturing some interpretability queries of practical interest. We then introduce ∃FOIL+, an extension of ∃FOIL with a ﬁnite set of unary universal queries from FOIL that is enough for expressing some relevant interpretability queries. We provide a characterization theorem for the tractability of
∃FOIL+ over any class of Boolean decision models that reduces the tractability of this fragment to the tractability of two ﬁxed and speciﬁc FOIL queries. Then we prove that said queries are tractable over perceptrons, which implies the tractability of ∃FOIL+ for this model. Unfortunately, the evaluation of said queries is NP-hard for decision trees and OBDDs. Both the proof of tractability for perceptrons and intractability for decision trees and OBDDs are relatively simple, thus showing that the characterization theorem provides a useful technique for understanding which models are tractable for the evaluation of ∃FOIL+.
Structural restrictions. We restrict the models allowed in order to obtain tractability of evaluation for arbitrary FOIL queries. In particular, we show that evaluation of ϕ, for ϕ a ﬁxed FOIL query, can be solved in polynomial time over the class of OBDDs as long as they are complete, i.e., any path from the root to a leaf of the OBDD tests every feature from the input, and have bounded width, i.e., there is a constant bound on the number of nodes of the OBDD in which a feature can appear.
Practical implementation. We designed FOIL with a minimal set of logical constructs and tailored for models with binary input features. These decisions are reasonable for a detailed theoretical analysis but may hamper FOIL usage in more general scenarios, in particular when models have (many) categorical or numerical input features, and queries are manually written by non-expert users.
To tackle this we introduce a more user-friendly language with a high-level syntax (à la SQL in the spirit of the query in Figure 1b) that can be compiled into FOIL queries. Moreover, we present a prototype implementation that can be used to query decision trees trained in standard ML libraries by binarizing them into models (a subclass of binary decision diagrams) over which FOIL queries can be efﬁciently evaluated. We also test the performance of our implementation over synthetic and real data giving evidence of the usability of FOIL as a base for practical interpretabilty languages. 2 A Logic for Interpretability Queries