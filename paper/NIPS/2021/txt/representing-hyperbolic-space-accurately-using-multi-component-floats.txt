Abstract
Hyperbolic space is particularly useful for embedding data with hierarchical struc-ture; however, representing hyperbolic space with ordinary ﬂoating-point numbers greatly affects the performance due to its ineluctable numerical errors. Simply increasing the precision of ﬂoats fails to solve the problem and incurs a high com-putation cost for simulating greater-than-double-precision ﬂoats on hardware such as GPUs, which does not support them. In this paper, we propose a simple, feasible-on-GPUs, and easy-to-understand solution for numerically accurate learning on hyperbolic space. We do this with a new approach to represent hyperbolic space using multi-component ﬂoating-point (MCF) in the Poincaré upper-half space model. Theoretically and experimentally we show our model has small numerical error, and on embedding tasks across various datasets, models represented by multi-component ﬂoating-points gain more capacity and run signiﬁcantly faster on
GPUs than prior work. 1

Introduction
Representation learning is of particular interest nowadays in machine learning. Real-world data is often discrete—such as natural language sentences in the form of syntax trees and graphs [7, 22]— which makes it critical to do a good job of representing and storing them as numbers which we can better understand and deal with. Perhaps the most common way of doing this representation is with embedding, in which discrete objects are assigned points in some continuous (typically high-dimensional Euclidean) space. Standard embeddings in NLP include Word2Vec [24] and GloVe
[30]. However, many kinds of real-world data are hierarchical or graph-like [25], in which case embedding into non-Euclidean space can outperform the standard Euclidean embedding. Hyperbolic space is such an embedding target of particular interest, showing impressive performance on many tasks such as taxonomies, link prediction and inferring concept hierarchies [27, 28, 15].
Hyperbolic space [9] is helpful for many types of embeddings because of its constant negative curvature, as distinguished from Euclidean space with zero curvature. This negative-curvature geometry endows hyperbolic space with many useful properties not present in Euclidean space. For example, the volume of a ball in even two-dimensional hyperbolic space increases exponentially with respect to the radius, as shown in [9], as a contrast to the polynomial rate at which balls grow in
Euclidean space. This makes it possible to embed more instances in the hyperbolic ball than Euclidean ball with the same radius, making it ideal for structures like trees where the number of objects at a distance d from any given object can grow exponentially with d. Reasoning from this intuition, the hyperbolic space “looks like” a ﬁnite-degree tree and can be regarded as a continuous version of trees [6]. The combination of low dimension (which facilitates learning) with high volume (which increases representational capacity) makes hyperbolic space inherently well-suited for embedding hierarchical structures data like trees and graphs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
However, the advantages of hyperbolic space do not come for free. The large volumes that are so beneﬁcial from a embedding-capacity perspective create unavoidable numerical problems. Informally called “the NaN problem" [37, 34], representing hyperbolic space with ordinary ﬂoating-point num-bers can cause signiﬁcant errors and lead to severe numerical instability that compounds throughout training, causing NaNs to pop out everywhere in the computations. Common models of hyperbolic space used to learn hyperbolic embeddings using ﬂoating-point arithmetic, such as the Poincaré ball model [27, 15] and the Lorentz hyperboloid model [28], all suffer from signiﬁcant numerical errors.
These errors originate in the ordinary ﬂoating-point representation, and are further ampliﬁed in subse-quent computations by the very ill-conditioned Riemannian metrics involved in their construction.
Yu and De Sa [37] show that in the Lorentz hyperboloid model, the error of representing an exact point with ordinary ﬂoating-point numbers becomes unbounded as the point moves away from the origin, which is what will happen in most embedding tasks: points unavoidably move far away from the origin as more and more data instances crowd in.
Many efforts and compromises have been made so as to solve or alleviate the ‘NaN’ problem. One commonly adopted technical solution ﬁrstly exploited in [35] is using an appropriate scaling factor, i.e., scale down all the distances by the scaling factor before embedding, then recover the original distances by dividing to the scaling factor. However, this scaling will increase the distortion of the embedding, and the distortion gets worse as the scale factor increases. Sala et al. [34] show that this sort of tradeoff is unavoidable when embedding in hyperbolic space: one can either choose to increase the number of bits used for the ﬂoating-point numbers or increase the dimension of the space, and this is independent of the underlying models. However, increasing the dimension is not a panacea, as the ‘NaN’ problem still exists in high dimensional space (albeit to a lesser degree).
A straightforward solution to the ‘NaN’ problem would be to compute with high precision ﬂoating-point numbers, such as BigFloats (specially designed with a large quantity of bits), as adopted in the combinatorial construction of hyperbolic embedings in [34], where ﬂoating-point numbers with thousands of bits are used with the support of Julia [4]. However, there are two evident problems with representing hyperbolic space using BigFloats. Firstly, BigFloats are not supported on ML accelerator hardware like GPUs: computations using BigFloats on CPUs will be slow and hence not realistic for many meaningful tasks. Secondly, BigFloats are currently not supported in most deep learning frameworks even on CPUs, so one would need to develop the algorithms from scratch so as to learn over hyperbolic space, which is tedious and fraught with the danger of numerical mistakes.
A hybrid solution to the ‘NaN’ problem proposed by Yu and De Sa [37] is to avoid ﬂoating-point arithmetic as much as possible by using integer arithmetic instead—as integer arithmetic can be done exactly. Their tiling-based model works by decomposing hyperbolic space into a tiling: the product of a compact fundamental region (which can be represented in ﬂoating-point with bounded error) and a discrete group (which supports integer computation). We can also interpret their approach as decomposing a large ﬂoating-point number into a multiply of a small ﬂoating-point number with a large integer number using some group actions [3, 33]. This allows for a guaranteed bound on the numerical error of learning in hyperbolic space. However, their approach is limited somewhat in that its guarantees apply to loss functions that depend only the hyperbolic distance between points, and their best-performing model (their L-tiling model) is limited to 2-dimensional space. A bigger issue with this type of approach is computational: the big-integer matrix multiplication involved in the model is not supported on GPUs and is not yet implemented in many deep learning frameworks such as PyTorch [29] and TensorFlow [1]. This effectively prevents the method from being used in training of complex models that depend on GPU acceleration, such as hyperbolic neural networks
[14, 18, 10, 23].
In this paper, we propose a more direct way to represent and learn in hyperbolic space without sac-riﬁcing either accuracy guarantees or GPU acceleration. To achieve high precision computations and controllable numerical errors, we choose to represent hyperbolic space with multiple-component
ﬂoating-point numbers (MCF) [31, 32, 19, 36], an alternative approach different from BigFloats where a number is expressed as an unevaluated sum of multiple ordinary ﬂoating-point numbers. We make the following contributions:
• We propose representing hyperbolic space with the Poincaré upper-half space model for opti-mization in section 3, and show how to represent it with MCF in section 4 to eliminate the ‘NaN’ problem. 2
• We provide algorithms for computing with these MCF that are adapted to hyperbolic space in section 5 and section 6, where we prove numerical errors can be reduced to any degree by simply increasing the number of components of the MCF.
• We experimentally measure our model on embedding tasks across several datasets, and show that one can gain more learning capacity (compared to models that use ordinary ﬂoating-point numbers) with only a mild computational slowdown on GPUs in section 7, meanwhile signiﬁcantly faster than prior high precision hyperbolic models. 2