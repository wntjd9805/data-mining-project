Abstract
We propose a novel neural representation for videos (NeRV) which encodes videos in neural networks. Unlike conventional representations that treat videos as frame sequences, we represent videos as neural networks taking frame index as input.
Given a frame index, NeRV outputs the corresponding RGB image. Video encoding in NeRV is simply ﬁtting a neural network to video frames and decoding process is a simple feedforward operation. As an image-wise implicit representation,
NeRV output the whole image and shows great efﬁciency compared to pixel-wise implicit representation, improving the encoding speed by 25× to 70×, the decoding speed by 38× to 132×, while achieving better video quality. With such a representation, we can treat videos as neural networks, simplifying several video-related tasks. For example, conventional video compression methods are restricted by a long and complex pipeline, speciﬁcally designed for the task. In contrast, with
NeRV, we can use any neural network compression method as a proxy for video compression, and achieve comparable performance to traditional frame-based video compression approaches (H.264, HEVC etc.). Besides compression, we demonstrate the generalization of NeRV for video denoising. The source code and pre-trained model can be found at https://github.com/haochen-rye/NeRV.git. 1

Introduction
What is a video? Typically, a video captures a dynamic visual scene using a sequence of frames. A schematic interpretation of this is a curve in 2D space, where each point can be characterized with a (x, y) pair representing the spatial state. If we have a model for all (x, y) pairs, then, given any x, we can easily ﬁnd the corresponding y state. Similarly, we can interpret a video as a recording of the visual world, where we can ﬁnd a corresponding RGB state for every single timestamp. This leads to our main claim: can we represent a video as a function of time?
More formally, can we represent a video V as V = {vt}T t=1, where vt = fθ(t), i.e., a frame at times-tamp t, is represented as a function f parameterized by θ. Given their remarkable representational capacity [1], we choose deep neural networks as the function in our work. Given these intuitions, we propose NeRV, a novel representation that represents videos as implicit functions and encodes them into neural networks. Speciﬁcally, with a fairly simple deep neural network design, NeRV can reconstruct the corresponding video frames with high quality, given the frame index. Once the video is encoded into a neural network, this network can be used as a proxy for video, where we can directly extract all video information from the representation. Therefore, unlike traditional video representations which treat videos as sequences of frames, shown in Figure 1 (a), our proposed NeRV considers a video as a uniﬁed neural network with all information embedded within its architecture and parameters, shown in Figure 1 (b).
As an image-wise implicit representation, NeRV shares lots of similarities with pixel-wise implicit visual representations [5, 6] which takes spatial-temporal coordinates as inputs. The main differences between our work and image-wise implicit representation are the output space and architecture 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) Conventional video representation as frame sequences. (b) NeRV, representing video as neural networks, which consists of multiple convolutional layers, taking the normalized frame index as the input and output the corresponding RGB frame.
Table 1: Comparison of different video representations. Although explicit representations outperform implicit ones in encoding speed and compression ratio now, NeRV shows great advantage in decoding speed. And NeRV outperforms pixel-wise implicit representations in all metrics.
Explicit (frame-based)
Implicit (uniﬁed)
Hand-crafted (e.g., HEVC [2])
Learning-based (e.g., DVC [3])
Pixel-wise (e.g., NeRF [4]
Image-wise (Ours)
Encoding speed
Decoding speed
Compression ratio
Fast
Medium
Medium
Medium
Slow
High
Very slow
Very slow
Low
Slow
Fast
Medium designs. Pixel-wise representations output the RGB value for each pixel, while NeRV outputs a whole image, demonstrated in Figure 2. Given a video with size of T × H × W , pixel-wise representations need to sample the video T ∗ H ∗ W times while NeRV only need to sample T times. Considering the huge pixel number, especially for high resolution videos, NeRV shows great advantage for both encoding time and decoding speed. Different output space also leads to different architecture designs, NeRV utilizes a MLP + ConvNets architecture to output an image while pixel-wise representation uses a simple MLP to output the RGB value of the pixel. Sampling efﬁciency of NeRV also simplify the optimization problem, which leads to better reconstruction quality compared to pixel-wise representations.
We also demonstrate the ﬂexibility of NeRV by exploring several applications it affords. Most notably, we examine the suitability of NeRV for video compression. Traditional video compression frameworks are quite involved, such as specifying key frames and inter frames, estimating the residual information, block-size the video frames, applying discrete cosine transform on the resulting image blocks and so on. Such a long pipeline makes the decoding process very complex as well. In contrast, given a neural network that encodes a video in NeRV, we can simply cast the video compression task as a model compression problem, and trivially leverage any well-established or cutting edge model compression algorithm to achieve good compression ratios. Speciﬁcally, we explore a three-step model compression pipeline: model pruning, model quantization, and weight encoding, and show the contributions of each step for the compression task. We conduct extensive experiments on popular video compression datasets, such as UVG [7], and show the applicability of model compression techniques on NeRV for video compression. We brieﬂy compare different video representations in
Table 1 and NeRV shows great advantage in decoding speed.
Besides video compression, we also explore other applications of the NeRV representation for the video denoising task. Since NeRV is a learnt implicit function, we can demonstrate its robustness to noise and perturbations. Given a noisy video as input, NeRV generates a high-quality denoised output, without any additional operation, and even outperforms conventional denoising methods.
The contribution of this paper can be summarized into four parts: 2
• We propose NeRV, a novel image-wise implicit representation for videos, representating a video as a neural network, converting video encoding to model ﬁtting and video decoding as a simple feedforward operation.
• Compared to pixel-wise implicit representation, NeRV output the whole image and shows great efﬁciency, improving the encoding speed by 25× to 70×, the decoding speed by 38× to 132×, while achieving better video quality.
• NeRV allows us to convert the video compression problem to a model compression problem, allowing us to leverage standard model compression tools and reach comparable performance with conventional video compression methods, e.g., H.264 [8], and HEVC [2].
• As a general representation for videos, NeRV also shows promising results in other tasks, e.g., video denoising. Without any special denoisng design, NeRV outperforms traditional hand-crafted denoising algorithms (medium ﬁlter etc.) and ConvNets-based denoisng methods. 2