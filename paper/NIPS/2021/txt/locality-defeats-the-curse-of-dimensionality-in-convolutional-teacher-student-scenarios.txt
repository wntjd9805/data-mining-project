Abstract
Convolutional neural networks perform a local and translationally-invariant treat-ment of the data: quantifying which of these two aspects is central to their success remains a challenge. We study this problem within a teacher-student framework for kernel regression, using ‘convolutional’ kernels inspired by the neural tangent kernel of simple convolutional architectures of given ﬁlter size. Using heuristic methods from physics, we ﬁnd in the ridgeless case that locality is key in determin-ing the learning curve exponent β (that relates the test error (cid:15)t ∼ P −β to the size of the training set P ), whereas translational invariance is not. In particular, if the
ﬁlter size of the teacher t is smaller than that of the student s, β is a function of s only and does not depend on the input dimension. We conﬁrm our predictions on
β empirically. We conclude by proving, under a natural universality assumption, that performing kernel regression with a ridge that decreases with the size of the training set leads to similar learning curve exponents to those we obtain in the ridgeless case. 1

Introduction
Deep Convolutional Neural Networks (CNNs) are widely recognised as the engine of the latest successes of deep learning methods, yet such a success is surprising. Indeed, any supervised learning model suffers in principle from the curse of dimensionality: under minimal assumptions on the function to be learnt, achieving a ﬁxed target generalisation error (cid:15) requires a number of training samples P which grows exponentially with the dimensionality d of input data [1], i.e. (cid:15)(P ) ∼ P −1/d.
Nonetheless, empirical evidence shows that the curse of dimensionality is beaten in practice [2, 3, 4], with (cid:15)(P ) ∼ P −β, (1)
CNNs, in particular, achieve excellent performances on high-dimensional tasks such as image classiﬁcation on ImageNet with state-of-the-art architectures, for which β ≈ [0.3, 0.5] [2]. Natural data must then possess additional structures that make them learnable. A classical idea [5] ascribes the success of recognition systems to the compositionality of data, i.e. the fact that objects are made of features, themselves made of sub-features [6, 7, 8]. In this view, the locality of CNNs plays a key role for their performance, as supported by empirical observations [9]. Yet, there is no clear
β (cid:29) 1/d.
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
analytical understanding of the relationship between the compositionality of the data and learning curves.
In order to study this relationship quantitively, we introduce a teacher-student framework for kernel regression, where the function to be learnt takes one of the following two forms: f LC(x) = (cid:88) i∈P gi(xi), f CN (x) = (cid:88) i∈P g(xi). (2)
Here, x is a d-dimensional input and xi denotes the i-th t-dimensional patch of x, xi = (xi, . . . , xi+t−1). i ranges in a subset P of {1, . . . , d}. The gi’s and g are random functions of t variables whose smoothness is controlled by some exponent αt. Such functions model the local nature of certain datasets and can be generated, for example, by randomly-initialised one-hidden-layer neural networks: f LC corresponds to a locally connected network (LCN) [10, 11], in which the input is split into lower-dimensional patches before being processed, whereas a network enforcing invariance with respect to shifts of the input patches via weight sharing can be described by f CN . In such cases t would be the ﬁlter size of the network. Our goal is to compute the asymptotic decay of the error of a student kernel performing regression on such data, and to relate the corresponding exponent β to the locality of the target function. The student kernel corresponds to a prior on the true function of the form described by Eq. (2), except that the ﬁlter size s and its prior αs on the smoothness of the g functions can differ from those of the target function. Such students include overparametrised one-hidden-layer neural networks operating in the lazy training regime [12, 13, 14, 15, 16]. 1.1 Our contributions
We consider a teacher-student framework for kernel regression, where the target function has one of the forms in Eq. (2), where the gi’s and g are Gaussian random ﬁelds of given covariance.
Target functions are characterised by the dimensionality t of the g functions—the ﬁlter size—and a smoothness exponent αt, such that αt > 2n implies that typical target functions are at least n times differentiable. Kernel regression is performed by local or convolutional student kernels, having ﬁlter size s and a prior on the target smoothness characterised by another exponent αs > 0. Our main contributions follow:
◦ We use recent results based on the replica method of statistical physics on the generalisation error of kernel methods [17, 18, 19] to estimate the exponent β. We ﬁnd that β = αt/s if t ≤ s and αt ≤ 2(αs + s). This approach is non-rigorous, but it can be proven if data are sampled on a lattice [4] and corresponds to a provable lower bound on the error when teacher and student are equal [20].
◦ In particular, we ﬁnd the same exponent for students with a prior on the shift invariance of the target function and students without this prior, implying that the curse of dimensionality is beaten due to locality and not shift invariance.
◦ We conﬁrm systematically our predictions by performing kernel ridgeless regression numer-ically for various t, s and embedding dimension d.
◦ We use the recent framework of [21] and a natural Gaussian universality assumption to prove a rigorous estimate of β in the case where the ridge decreases with the size of the training set. The estimate of β depends again on s and not on d, demonstrating that the curse of dimensionality can indeed be beaten by using local ﬁlters on such compositional data. 1.2