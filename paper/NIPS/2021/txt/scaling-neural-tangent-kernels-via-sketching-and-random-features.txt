Abstract
The Neural Tangent Kernel (NTK) characterizes the behavior of inﬁnitely-wide neural networks trained under least squares loss by gradient descent. Recent works also report that NTK regression can outperform ﬁnitely-wide neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has limited its use in large-scale learning tasks. To accelerate learning with NTK, we design a near input-sparsity time approximation algorithm for NTK, by sketching the polynomial expansions of arc-cosine kernels: our sketch for the convolutional counterpart of NTK (CNTK) can transform any image using a linear runtime in the number of pixels. Furthermore, we prove a spectral approximation guarantee for the NTK matrix, by combining random features (based on leverage score sampling) of the arc-cosine kernels with a sketching algorithm. We bench-mark our methods on various large-scale regression and classiﬁcation tasks and show that a linear regressor trained on our CNTK features matches the accuracy of exact CNTK on CIFAR-10 dataset while achieving 150 speedup.
× 1

Introduction
Recent results have shown that over-parameterized Deep Neural Networks (DNNs), generalize surprisingly well. In an effort to understand this phenomena, researchers have studied ultra-wide
DNNs and shown that in the inﬁnite width limit, a fully connected DNN trained by gradient descent under least-squares loss is equivalent to kernel regression with respect to the Neural Tangent Kernel (NTK) [5, 11, 22, 28]. This connection has shed light on DNNs’ ability to generalize [10, 34] and optimize (train) their parameters efﬁciently [3, 4, 16]. More recently, Arora et al. [5] proved an analo-gous equivalence between convolutional DNNs with inﬁnite number of channels and Convolutional
NTK (CNTK). Beyond the aforementioned theoretical purposes, several papers have explored the algorithmic use of this kernel. Arora et al. [6] and Geifman et al. [19] showed that NTK based kernel models can outperform trained DNNs (of ﬁnite width). Additionally, CNTK kernel regression sets an impressive performance record on CIFAR-10 for kernel methods without trainable kernels [5].
The NTK has also been used in experimental design [39] and predicting training time [43].
However, the NTK-based approaches encounter the computational bottlenecks of kernel learning.
Rd×d, only writing down the CNTK kernel
In particular, for a dataset of n images x1, x2, . . . xn operations [5]. Running regression or PCA on the resulting kernel matrix matrix requires Ω takes additional cubic time in n, which is infeasible in large-scale setups. n2 d4
∈
· (cid:0) (cid:1)
*Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
There is a rich literature on kernel approximations for large-scale learning. One of the most popular approaches is the random features method which works by randomly sampling the feature space of the kernel function, originally due to the seminal work of Rahimi and Recht [37]. Another popular approach which is developed in linear sketching literature [41], works by designing sketches that can be efﬁciently applied to the feature space of a kernel without needing to explicitly form the high dimensional feature space. This approach has been successful at designing efﬁcient subspace embeddings for the polynomial kernel [7, 1]. In this paper, we propose solutions for scaling the
NTK and CNTK by building on both of these kernel approximations techniques and designing efﬁcient feature maps that approximate the NTK/CNTK evaluation. Consequently, we can simply transform the input dataset to these feature spaces, and then apply fast linear learning methods to approximate the answer of the corresponding nonlinear kernel method efﬁciently. The performance of such approximate methods is similar or sometimes better than the exact kernel methods due to implicit regularization effects of the approximation algorithms [37, 38, 23]. 1.1 Overview of Our Contributions
• One of our results is an efﬁcient random features construction for the NTK. Our starting point is the explicit NTK feature map suggested by Bietti and Mairal [9] based on tensor product of the feature maps of arc-cosine kernels. We obtain our random features, by sampling the feature space of arc-cosine kernels [12]. However, the naïve construction of the features would incur an exponential cost in the depth of the NTK, due to the tensor product of features generated in consecutive layers.
We remedy this issue, by utilizing an efﬁcient sketching algorithm for tensor products known as
TENSORSRHT [1] which can effectively approximate the tensor products of vectors while preserving their inner products. We provide a rigorous error analysis of the proposed scheme in Theorem 2.
• Our next results are sketching methods for both NTK and CNTK using a runtime that is linearly proportional to the sparsity of the input dataset (or number of pixels of images). Our methods rely on the arc-cosine kernels’ feature space deﬁned by their Taylor expansion. By careful truncation of their Taylor series, we approximate the arc-cosine kernels with bounded-degree polynomial kernels. Because the feature space of a polynomial kernel is the tensor product of its input space, its dimensionality is exponential in the degree of the kernel. Fortunately, Ahle et al. [1] have developed a linear sketch known as POLYSKETCH that can reduce the dimensionality of high-degree tensor products very efﬁciently, therefore, we can sketch the resulting polynomial kernels using this technique. We then combine the transformed features from consecutive layers by further sketching their tensor products. In case of CNTK, we have an extra operation which sketches the direct sum of the features of neighbouring pixels at each layer that precisely corresponds to the convolution operation in CNNs. We carefully analyze the errors introduced by polynomial approximations and various sketching steps in our algorithms and also bound their runtimes in Theorems 1 and 4.
• Furthermore, we improve the arc-cosine random features to spectrally approximate the entire kernel matrix, which is advocated in recent literature for ensuring high approximation quality in downstream tasks [8, 32]. Our construction is based on leverage score sampling, which entertains better convergence bounds [8, 28, 29]. However, computing this distribution is as expensive as solving the kernel methods exactly. We propose a simple distribution that tightly upper bounds the leverage scores of arc-cosine kernels and for further efﬁciency, use Gibbs sampling to generate random features from our proposed distribution. We provide our spectral approximation guarantee in
Theorem 3.
• Finally, we empirically benchmark our proposed methods on various classiﬁcation/regression tasks and demonstrate that our methods perform similar to or better than exact kernel method with
NTK and CNTK while running extremely faster. In particular, we classify CIFAR-10 dataset 150 faster than exact CNTK and at the same time achieve higher test accuracy.
× 1.2