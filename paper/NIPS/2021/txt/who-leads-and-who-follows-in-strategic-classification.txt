Abstract
As predictive models are deployed into the real world, they must increasingly contend with strategic behavior. A growing body of work on strategic classiﬁcation treats this problem as a Stackelberg game: the decision-maker “leads” in the game by deploying a model, and the strategic agents “follow” by playing their best response to the deployed model. Importantly, in this framing, the burden of learning is placed solely on the decision-maker, while the agents’ best responses are implicitly treated as instantaneous. In this work, we argue that the order of play in strategic classiﬁcation is fundamentally determined by the relative frequencies at which the decision-maker and the agents adapt to each other’s actions. In particular, by generalizing the standard model to allow both players to learn over time, we show that a decision-maker that makes updates faster than the agents can reverse the order of play, meaning that the agents lead and the decision-maker follows.
We observe in standard learning settings that such a role reversal can be desirable for both the decision-maker and the strategic agents. Finally, we show that a decision-maker with the freedom to choose their update frequency can induce learning dynamics that converge to Stackelberg equilibria with either order of play. 1

Introduction
Individuals interacting with a decision-making algorithm often adapt strategically to the decision rule in order to achieve a desirable outcome. While such strategic adaptation might increase the individuals’ utility, it also breaks the statistical patterns that justify the decision rule’s deployment.
This widespread phenomenon, often known as Goodhart’s law, can be summarized as: “When a measure becomes a target, it ceases to be a good measure” [49].
A growing body of work known as strategic classiﬁcation [14, 19, 28] models this phenomenon as a two-player game in which a decision-maker “leads” and strategic agents subsequently “follow.”
Speciﬁcally, the decision-maker ﬁrst deploys a decision rule, and the agents then take a strategic action so as to optimize their outcome according to the deployed rule, subject to natural manipulation costs. For example, a bank might make lending decisions using applicants’ credit scores. Knowing this mechanism, loan applicants might sign up for a large number of credit cards in an effort to strategically increase their credit score at little effort.
One of the main goals in the literature is to develop strategy-robust decision rules; that is, rules that remain meaningful even after the agents have adapted to them. Recent work has studied strategies for
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
ﬁnding such rules through repeated interactions between the decision-maker and the agents [6, 20, 46].
In particular, the decision-maker sequentially deploys different rules, and for each they observe the population’s response. Under certain regularity conditions, over time the decision-maker can ﬁnd the optimal solution, deﬁned as the rule that minimizes the decision-maker’s loss after the agents have responded to the rule.
With the emergence of online platforms such as social media and e-commerce sites, repeated interactions between decision-makers and the population have become ever more prevalent. Online platforms continuously monitor user behavior and update pricing algorithms, recommendation systems, and popularity rankings accordingly. Users, on the other hand, take actions to ensure favorable outcomes in the face of these updates.
A distinctive feature of online platforms is the decision-maker’s dominant computational power and abundant data resources, allowing the platform to react to any change in the agents’ behavior virtually instantaneously. For example, if fake news content changes over time, automated algorithms can quickly detect this and retrain the classiﬁer to incorporate the shift. It has been observed [see, e.g., 16, 18, 45] that, when faced with such “reactive” algorithms, strategic agents tend to take actions that anticipate the algorithm’s response. That is, through repeated interactions, agents aim to ﬁnd actions that maximize the agents’ utility after the decision-maker has responded to these actions.
This suggests that the order of play in strategic interactions can in fact be reversed, such that the agents “lead” while the decision-maker “follows.”
To give an example of such a reversed strategic interaction, consider ride-sharing platforms that deploy algorithms for determining travel fare as a function of trip length and relevant trafﬁc conditions.
These pricing mechanisms are frequently updated based on the current supply and demand, and in particular a dip in the supply of drivers triggers a surge pricing algorithm. Möhlmann and Zalmanson
[45] observed that drivers occasionally coordinate a massive deactivation of drivers from the system, artiﬁcially lowering driver supply, only to get back on the platform after some time has passed and the prices have surged. In this interaction, the drivers essentially make the ﬁrst move, while the platform’s pricing algorithm reacts to their action. Other examples of users aiming to exert control over algorithms can be found in the context of social network analyses [16, 18].
In this work, we argue that the order of play in strategic classiﬁcation is fundamentally tied to the relative update frequencies at which the decision-maker and the strategic agents adapt to each other’s actions. In particular, we show that, by tuning their update frequency appropriately, the decision-maker can select the order of play in the underlying game. Furthermore, in natural settings we show that allowing the strategic agents to play ﬁrst in the game can actually be preferable for both the decision-maker and the agents. This is contrary to the order of play previously studied in the literature, whereby the decision-maker is always assumed to make the ﬁrst move. 1.1 Our contribution
To give an overview of our results, we recall some relevant game-theoretic concepts. In the existing literature strategic classiﬁcation is modeled as a Stackelberg game. A Stackelberg game is a two-person game where one player, called the leader, moves ﬁrst, and the other player, called the follower, moves second, with the possibility of adapting to the move of the leader. Previous work assumes that the decision-maker acts as the leader and the agents act as the follower. This means that the decision-maker ﬁrst deploys a model, and the agents then modify their features at some cost in order to obtain a favorable outcome according to the model. The decision-maker’s goal is to ﬁnd the
Stackelberg equilibrium—the model that minimizes the decision-maker’s loss after the agents have optimally adapted to it. This optimal reaction by the agents is called the best response to the model.
An important parameter that has been largely overlooked in existing work is the rate at which the agents re-evaluate and potentially modify their features. Most works studying the interaction between a decision-maker and strategic agents implicitly assume that, as soon as the model is updated, the data collected from strategic agents follows the best response to the currently deployed model. In the current work we do not assume that the agents react instantaneously to model updates. Instead, we assume that there is a natural timescale according to which the agents adapt their features to models.
Allowing agents to adapt gradually to deployed models gives the decision-maker a new dimension upon which to act strategically. Faced with agents that adapt gradually, the decision-maker can choose the timescale at which they update the deployed model. In particular, they can choose a rate 2
of updates that is faster than the agents’ rate, or they can choose a rate that is slower than the agents’ rate. We call decision-makers that follow a faster clock than the agents reactive, and if they follow a slower clock we call them proactive. Given that existing work on strategic classiﬁcation relies on instantaneous agent responses, the previously studied decision-makers are all implicitly proactive.
Our ﬁrst main result states that the decision-maker’s choice of whether to be proactive or reactive fundamentally determines the order of play in strategic classiﬁcation. Perhaps counterintuitively, by choosing to be reactive it is possible for the decision-maker to let the agents become the leader in the underlying Stackelberg game. Since changing the order of play changes the game’s natural equilibrium concept, this choice can have a potentially important impact on the solution that the decision-maker and agents ﬁnd. Throughout, we refer to the Stackelberg equilibrium when the decision-maker leads as the decision-maker’s equilibrium and the Stackelberg equilibrium when the agents lead as the strategic agents’ equilibrium.
Theorem 1.1 (Informal). If the decision-maker is proactive, the natural dynamics of strategic classiﬁcation converge to the decision-maker’s equilibrium. If the decision-maker is reactive, the natural dynamics of strategic classiﬁcation converge to the strategic agents’ equilibrium.
To provide some intuition for Theorem 1.1, imagine that one player makes updates with far greater frequency than the other player. This allows the faster player to essentially converge to their best response between any two updates of the slower player. The slower player is then faced with a
Stackelberg problem: they have to choose an action, expecting that the faster player will react optimally after their update. As a result, the optimal choice for the slower player is to drive the dynamics toward the Stackelberg equilibrium where they act as the leader.
It is well known (see, e.g., Section 4.5 in [5]) that under general losses, either player can prefer to lead or follow in a Stackelberg game, meaning that a player achieves lower loss at the corresponding equilibrium. Our second main takeaway is that in classic learning problems it can be preferable for both the decision-maker and the agents if the agents lead in the game and the decision-maker follows.
One setting where this phenomenon arises is logistic regression with static labels and manipulable features.
Theorem 1.2 (Informal). Suppose that the decision-maker implements a logistic regression model and the strategic agents aim to maximize their predicted outcome. Then, both the decision-maker and the strategic agents prefer the strategic agents’ equilibrium to the decision-maker’s equilibrium.
Theorem 1.2 suggests that there are other meaningful equilibria than those previously studied in the literature. Moreover, Theorem 1.1 proves that such equilibria can naturally be achieved if the decision-maker is reactive and agents are no-regret. Seeing that the decision-maker’s equilibrium has also been shown to imply a cost to social welfare [30, 44], our results pave the way for studying new, potentially more desirable solutions in strategic settings. 1.2