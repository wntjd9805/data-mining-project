Abstract
Recently, bound propagation based certiﬁed robust training methods have been proposed for training neural networks with certiﬁable robustness guarantees. De-spite that state-of-the-art (SOTA) methods including interval bound propagation (IBP) and CROWN-IBP have per-batch training complexity similar to standard neural network training, they usually use a long warmup schedule with hundreds or thousands epochs to reach SOTA performance and are thus still costly. In this paper, we identify two important issues in existing methods, namely exploded bounds at initialization, and the imbalance in ReLU activation states and improve
IBP training. These two issues make certiﬁed training difﬁcult and unstable, and thereby long warmup schedules were needed in prior works. To mitigate these issues and conduct faster certiﬁed training with shorter warmup, we propose three improvements based on IBP training: 1) We derive a new weight initialization method for IBP training; 2) We propose to fully add Batch Normalization (BN) to each layer in the model, since we ﬁnd BN can reduce the imbalance in ReLU activation states; 3) We also design regularization to explicitly tighten certiﬁed bounds and balance ReLU activation states during wamrup. We are able to obtain 65.03% veriﬁed error on CIFAR-10 (✏ = 8 255 ) and 82.36% veriﬁed error on Tiny-ImageNet (✏ = 1 255 ) using very short training schedules (160 and 80 total epochs, respectively), outperforming literature SOTA trained with hundreds or thousands epochs under the same network architecture. The code is available at https:
//github.com/shizhouxing/Fast-Certified-Robust-Training. 1

Introduction
While deep neural networks (DNNs) are successfully applied in various areas, its robustness problem has attracted great attention since the discovery of adversarial examples (Szegedy et al., 2013;
Goodfellow et al., 2015; Carlini & Wagner, 2017; Kurakin et al., 2016; Chen et al., 2017; Madry et al., 2018; Su et al., 2018; Choi et al., 2019), which poses concerns in DNN applications especially the safety-critical ones such as autonomous driving. Methods for improving the empirical robustness of
DNNs, such as adversarial training (Madry et al., 2018), provide no provable robustness guarantees, and thus some recent works aim to pursue certiﬁed robustness. Speciﬁcally, the robustness is evaluated in a certiﬁable manner using robustness veriﬁers (Katz et al., 2017; Zhang et al., 2018; Wong &
Kolter, 2018; Singh et al., 2018, 2019; Bunel et al., 2017; Raghunathan et al., 2018b; Wang et al., 2018b; Xu et al., 2020; Wang et al., 2021), which verify whether the model is provably robust against all possible input perturbations within the range. This is achieved usually by efﬁciently computing the output bounds.
To improve certiﬁed robustness, certiﬁed robust training methods (also referred to as certiﬁed defense) minimize a certiﬁed robust loss computed by a veriﬁer, and the certiﬁed loss is an upper bound of the worst-case loss given speciﬁed input perturbations. So far, Interval Bound Propagation (IBP) (Gowal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
et al., 2018; Mirman et al., 2018) and CROWN-IBP (Zhang et al., 2020; Xu et al., 2020) are the most efﬁcient and effective methods for general models. IBP computes an interval with the output lower and upper bounds for each neuron, and CROWN-IBP further combines IBP with tighter linear relaxation-based bounds (Zhang et al., 2018; Singh et al., 2019) during warmup.
Both IBP and CROWN-IBP with loss fusion (Xu et al., 2020) have a per-batch training time complexity similar to standard DNN training. However, certiﬁed robust training remains costly and challenging, mainly due to their unstable training behavior – they could easily diverge or stuck at a degenerate solution without a long “warmup” schedule. The warmup schedule here refers to training the model with a regular (non-robust) loss ﬁrst and then gradually increasing the perturbation radius from 0 to the target value in the robust loss (some previous works also refer to it as “ramp-up”).
For example, generalized CROWN-IBP in Xu et al. (2020) used 900 epochs for warmup and 2,000 epochs in total to train a convolutional model on CIFAR-10 (Krizhevsky et al., 2009).
In this paper, we identify two important issues in existing certiﬁed training, so that a long warmup schedule could not be easily removed in previous works. First, we ﬁnd that the certiﬁed bounds can explode at the start of training, which is partly due to the suboptimal weight initialization in prior works. A good weight initialization is important for successful DNN training (Glorot & Bengio, 2010;
He et al., 2015a), but prior works for certiﬁed training generally use weight initialization methods originally designed for standard DNN training, while certiﬁed training is essentially optimizing a different type of augmented network deﬁned by robustness veriﬁcation (Zhang et al., 2020). The long warmup with gradually increasing perturbation radii in prior works can somewhat be viewed as
ﬁnding a better initialization for ﬁnal IBP training with the target radius, but it is too costly. Second, we also observe that IBP leads to imbalanced ReLU activation states, where the model prefers inactive (dead) ReLU neurons signiﬁcantly more than other states because inactive neurons tend to tighten IBP bounds. It can however hamper classiﬁcation performance if too many neurons are dead.
This issue can become more severe if the warmup schedule is shorter.
We focus on improving IBP training, since IBP is efﬁcient per batch, and it is also the base of recent state-of-the-art methods (Zhang et al., 2020; Xu et al., 2020). We propose the following improvements:
• We derive a new weight initialization, IBP initialization, for IBP-based certiﬁed training. The new initialization can stabilize the tightness of certiﬁed bounds at initialization.
• We identify the beneﬁt of Batch Normalization (BN) in certiﬁed training, and we ﬁnd BN which normalizes pre-activation outputs can balance ReLU activation states and also stabilize variance.
We propose to fully add BN to every layer, while it was partly or fully missed in prior works.
• We further propose regularizers to explicitly stabilize certiﬁed bounds and balance ReLU activation states during warmup.
We are able to efﬁciently train certiﬁably robust models that outperform previous SOTA performance in signiﬁcantly shorter training epochs. We achieve a veriﬁed error of 65.03% (✏ = 8 255 ) on CIFAR-10 in 160 total training epochs, and 82.36% on TinyImageNet (✏ = 1 255 ) in 80 epochs, based on efﬁcient IBP training. Under the same convolution-based architecture, we signiﬁcantly reduce the total training cost by 20 60 times compared to previous SOTA (Zhang et al., 2020; Xu et al., 2020) or concurrent work (Lyu et al., 2021).
⇠ 2