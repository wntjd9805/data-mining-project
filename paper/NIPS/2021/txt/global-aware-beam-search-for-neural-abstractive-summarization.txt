Abstractive
Summarization
Ye Ma1,4
Zixun Lan2,4
Lu Zong1,4∗
Kaizhu Huang3 1 Department of Financial and Actuarial Mathematics, School of Science 2 Department of Applied Mathematics, School of Science 3 Department of Intelligent Science, School of Advanced Technology 4 Laboratory for Intelligent Computing and Financial Technology
Xi’an Jiaotong-Liverpool University, SIP, 215123 Suzhou, China
{ye.ma, lu.zong, kaizhu.huang}@xjtlu.edu.cn, zixun.lan19@student.xjtlu.edu.cn
Abstract
This study develops a calibrated beam-based algorithm with awareness of the global attention distribution for neural abstractive summarization, aiming to improve the local optimality problem of the original beam search in a rigorous way. Speciﬁcally, a novel global protocol is proposed based on the attention distribution to stipulate how a global optimal hypothesis should attend to the source. A global scoring mechanism is then developed to regulate beam search to generate summaries in a near-global optimal fashion. This novel design enjoys a distinctive property, i.e., the global attention distribution could be predicted before inference, enabling step-wise improvements on the beam search through the global scoring mechanism.
Extensive experiments on nine datasets show that the global (attention)-aware inference signiﬁcantly improves state-of-the-art summarization models even using empirical hyper-parameters. The algorithm is also proven robust as it remains to generate meaningful texts with corrupted attention distributions. The codes and a comprehensive set of examples are available.2 1

Introduction
As the barriers exist from the auto-regressive design of neural probabilistic text generators to pre-dicting the global optimum directly [30], the heuristic algorithm beam search that factorizes global optimization to multiple local optimizations, has been popularly used for text decoding [24]. In the original beam search setting, the global optimum is a hypothesis g of the highest probability among all possible sentences, and consists of words in vocabulary V. Given the global optimum at step t denoted as g≤t, the local optimums l≤t refer to K candidate sequences with the highest probabilities at each step. While it is necessary to compromise on the beam size K (cid:28) V to ensure text quality [3, 25, 24] and search efﬁciency, beam search suffers from a major limitation due to its local property. Concretely, assuming that the global optimal hypothesis is within the K local optimal hypotheses of the highest probabilities, i.e. p(g≤t) ≥ p(l≤t), for all t until the termination
T , it operates solely with the local information available at each step. In practice, such assumption may however fail in the case that the probability of the global optimum at step τ < T is less than those of the local optimums, i.e. p(g≤τ ) < p(l≤τ ), but is adjusted to a higher level in the later steps, p(g>τ |g≤τ ) > p(l>τ |l≤τ ) and p(g≤τ )p(g>τ |g≤τ ) > p(l≤τ )p(l>τ |l≤τ ). This often leads beam search to get stuck in the local optimum from step τ onward in generating texts.
∗corresponding author 2https://github.com/yema2018/global_aware 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: (a) Attention distribution is composed of the summation of cross attention on the same-colored lines, distinguished from that of different-colored lines which always equals 1 due to softmax. (b) Local attention gradually increases as the decoding proceeds. (c) Desired situation: growing local attention has been lower than global attention during decoding and exactly reaches it at the end.
To cope with this limitation, this study proposes a calibrated beam-based algorithm with global awareness at all searching steps. Generally, our novel algorithm is implemented in two phases. Before the beam search (Phase I), the global attention distribution is predicted in order to be included as a protocol to calibrate beam search at each step, encouraging the generated hypotheses to attend to the source in a more near-global optimal way. Speciﬁcally, the global attention distribution describes how all reference tokens should assign the attention to each source token (illustrated in Figure 1.a), which could be predicted from the source by training an attention-prediction model. The training is fairly straightforward and resembles a sequence tagging task [12], except that the predicted attention distribution from the source is a regression result. There are several advantages of using the attention distribution as the global protocol. 1) Attention distributions are sensitive to the decoder input, suggesting that any input to the decoder leads to a unique attention distribution with ﬁxed model parameters; 2) attention distributions are accessible for almost all text generation tasks thanks to the recent advances in attention models [27, 4, 31]; 3) relying on the source only, the global attention distribution can be predicted before beam search, thus offering a rigorous mechanism to calibrate a global-aware beam search.
During beam search (Phase II), we develop a novel global scoring mechanism composed of attention scores and length rewards to guide beam search based on the predicted global attention distribution. As one main theoretical result, we show that the attention score can be considered as the probability that generated texts attend to sources in line with the predicted global attention distribution. Speciﬁcally, the generated tokens in each step update the local attention distribution to source tokens dynamically, where the attention values grow monotonically as the generation proceeds (see Figure 1.b). Since the desired situation is that the local distribution reaches exactly the global distribution at the terminal step, we regulate the inference by discouraging local attention from exceeding their corresponding predicted global attention at all steps.
With regards to the core target to investigate the possible paradigm that improves beam search with global awareness during decoding, contributions of this study are summarized as follows:
• We argue that the limitation of beam search roots from its defect in ﬁnding the global optimal hypothesis. We improve the algorithm by proposing a global protocol to regulate beam search step-by-step. This paper is the ﬁrst to predict and deploy the global attention distribution to calibrate the inference in a rigorous way, thus returning a hypothesis that attends to source tokens in a more near-global optimal manner.
In contrast, previous works [37, 7, 20, 29, 21] try to use attention distributions to improve beam search, but ignore that the global attention distribution is predictable. 2
• A novel global scoring mechanism is designed to evaluate the generated sequences at each step based on the desired situation described in Figure 1.c. As theoretically justiﬁed, its major component can be elegantly integrated into beam search in the form of a probability so that merely O(K) of the time complexity is increased in each step (see Section § 3.1 for more details).
• The proposed algorithm with global awareness manifests a robust and plug-and-play property in enhancing beam search for neural abstractive summarization. Without requiring any model or parameter modiﬁcation, the global-aware inference shows excellent performance in generating meaningful texts, even if the attention distribution is corrupted or not of its own. Further, it is identiﬁed that summaries generated by global-aware inference are both higher-quality and different from beam search hypotheses (see Global-aware in Table 1).
More interestingly, we ﬁnd that the generation style of a dataset could be transferred by the designated global attention distribution. For instance, summaries of higher abstractness for CNN/DM could be generated by only replacing its global attention distribution with a highly abstractive distribution during inference, as presented in Global-aware† of Table 1.
• On the empirical side, we show that the proposed global-aware inference can stably and signiﬁcantly boost two state-of-the-art summarization models BART [17] and PEGASUS
[39] to produce higher quality summaries on nine datasets, even if only the empirical hyper-parameters are used.
Table 1: Use BART [17] ﬁne-tuned in CNN/DM to generate summaries. Global-aware uses the attention distribution learned from CNN/DM, while Global-aware† takes the attention distribution learned from XSUM.
Beam search
Global-aware
President Obama says climate change is a public health issue that affects all of us . Obama: "No challenge poses more of a public threat than climate change" Obama: "Millions of people would lose their health insurance" if Affordable
Care Act is not upheld . Obama: "I am not anticipating the Supreme Court would make such a bad decision"
President Obama says climate change is a public health issue that affects all of us . He says the average American can do their part to reduce their own carbon footprint . Obama did not appear particularly concerned about the Supreme
Court challenge to the Affordable Care Act .
Global-aware†
President Barack Obama says climate change is a public health issue . He says the average American can do their part to reduce their carbon footprint . 2 Preliminary
The proposed decoding strategy is applied in BART [17] and PEGASUS [39] to perform sum-marization. BART is a pre-trained seq-to-seq model whose structure essentially follows a vanilla
Transformer encoder-decoder [31]. PEGASUS has a similar structure but is pre-trained on a larger dataset differently. Notably, the ﬁne-tuned parameters of both models are downloaded from Hugging-Face Models3 and are ﬁxed in all subsequent operations, where “ﬁne-tuned" means the pre-trained model has been ﬁne-tuned on a speciﬁc dataset. 2.1 Beam Search
Since the decoder of the seq-to-seq model is an auto-regressive model, the probability of a target sequence y = (y0, · · · , yt, · · · , yT ) can be factorized to the probabilities conditional on the source x = (x1, · · · , xi, · · · , xn) and y<t = (y0, · · · , yt−1), i.e., p(y|x) =
T (cid:89) t=1 p(yt|x, y<t), T ≥ 1 (1)
When T = 0, y = (y0) and p(y0|x) = 1 because y0 is a ﬁxed start token. Beam search [8] is a decoding strategy to predict a target sequence by maximizing this factorization. Given a vocabulary set V, at each inference step t, beam search selects a candidate beam set BK k=1 (where each beam bk t ) is a candidate sequence) from an all-possible beam set Bt of size K × |V|, namely, 0, · · · , bk t = {bk t = (bk t }K
Bt = (cid:8)bk t−1 ◦ v | bk t−1 ∈ BK t−1, v ∈ V(cid:9) (2) 3https://huggingface.co/models 3
t = (cid:8)bk
BK t | bk t = argtopk (log p(bt|x)) , bt ∈ Bt (cid:9) , t ≥ 1 (3) where argtopk(·) outputs K beams with the highest conditional probability, and ◦ is the concatenation operation. Besides, BK 0 = {bk 0 is the start token. By Eq. 1, log p(bt|x) is an accumulated value. Its calculation can be simpliﬁed as: k=1 where bk 0}K (cid:40) log p(bt|x) = t−1|x) + log p(v|x, bk t−1), log p(bk log p(v|x, bk 0), t ≥ 2 t = 1 (4) where the value of log p(bk only need calculate the condition probability of each token in the vocabulary set. t−1|x) is computed from the previous step. Therefore, at each step, we
A beam is terminated after it generates an ending token, and the beam set of K terminated beams is deﬁned as Y. The ﬁnal hypothesis y∗ is chosen from Y based on the beam probability normalized by lengtha where a is a hyper-parameter of length [37]: y∗ = argmax yk∈Y log p(yk | x) (|yk| − 1)a (5) where yk = (yk length. 0 , · · · , yk
T ). |yk| − 1 is used since the start token is not considered in calculating the 2.2 Attention Distribution
Attention distribution is a continuous vector whose element indicates the degree of attention paid to a source token. The element is formed by the accumulation of cross attention, i.e., (cid:80) t αt,i, where αt,i refers to the cross attention that the tth token in the target sequence gives to the ith source token.4
Specially, cross attention is a scaled dot-product [31] of hidden states of the source x and the target sequence y. Notably, since Transformer-decoder is an auto-regressive model, the cross attention assigned by tth target token is actually calculated by y<t = (y0, · · · , yt−1).
Global Attention Distribution. The global attention distribution g = [g1, · · · , gi, · · · , gn] ∈ Rn is the attention distribution given by the reference, where global attention gi refers to the total attention that the reference attends to the ith source token, and n is the source length.
Optimal Length. The summation of g, namely (cid:80)n i=1 gi, is equal to (cid:80)n (cid:80)n length Z. t,n] ∈ Rn is
Local Attention Distribution. The local attention distribution lk the attention distribution of the kth generated sequence and updated at each decoding step t. Thereinto, the local attention lk t,i denotes the total attention paid to the ith source token by the kth beam sequence (bk t=1 αt,i = T due to i=1 αt,i = 1 in softmax operation, where T is the reference length, or equivalently the optimal t ) and is dependent on the sequence generated before t, i.e., bk t,1, · · · , lk t,i, · · · , lk t−1 = (bk 1, · · · , bk 0, · · · , bk t = [lk t−1). (cid:80)T i=1 3 Proposed Global-aware Inference 3.1 Global Scoring Mechanism
The global scoring mechanism consists of an attention scoring function and a length reward function.
Given the global attention distribution g, the attention scoring function A(·) at the step t depends on bk t−1,
A(bk t−1) = (cid:80)n i=1 min(lk
ζ k t t,i, gi) n (cid:88) i=1
, ζ k t = lk t,i, t ≥ 1 (6) t indicates the total attention that the generated sequence (bk where ζ k t ) gives to the source, and ζ k t−1| = t because the assignable attention for each generated token is 1. Notably, Eq. 6 attains the maximum score provided that each lt,i ≤ gi. As mentioned in Section § 1, the reason for 1, · · · , bk t = |bk 4Mean pooling is used for multi-layers and multi-heads 4
this design is that we desire the local attention lT,i at the termination is exactly gi, since the ﬁnal hypothesis is expected to attend to source tokens in the global-optimal manner. Meanwhile, we have lT,i > lt,i for t < T because lt,i monotonically increases on t with αl m,i > 0. Therefore, at any step, the local attention lt,i should not surpass gi. Otherwise, the attention score will decline, and the penalty depends on the total amount by which these lt,i exceed gi (see Theorem 1). Further, the attention score could be considered as the proportion of correctly assigned attention to the total attention given by the generated sequence, where correct assignment indicates that all parts of lt,i do not exceed gi. Also, it could be interpreted as the correct allocation probability of local attention against the global attention distribution (see Corollary 1.1). In this case, the total attention score can be expressed as the same multiplicative form as Eq. 1 to be elegantly integrated into beam search.
Theorem 1. Let M = (cid:8)s : lk (cid:80)
δ where ∆ ≥ 0, then A(bk
Corollary 1.1. The bound of A(bk t−1) decreases as ∆ increases. t−1) is between 0 and 1. t = (cid:8)δ | ∀s ∈ M, δ = lk (cid:9). Given ∆ = (cid:9) and ∆k t,s > gs t,s − gs
δ∈∆k t
Proof. See App. A.
In addition to the constraint for lt,i, we still desire lT,i = gi for each token. An ideal hypothesis should have two characteristics simultaneously. Namely, its attention score at the termination is the maximum 1, and its length equals the optimal length. Therefore, we introduce a length reward function to cooperate with the attention score to penalize the situation lT,i (cid:54)= gi, which will be discussed at the end of this subsection.
As mentioned before, the total attention score at the decoding step t is deﬁned as:
A(bk t−1) = t (cid:89) m=1
A(bk m−1) (7)
Thus, the joint scoring function J(·) is modiﬁed from Eq. 4: t−1) + log p(v|x, bk t−1|x) + β log A(bk
J(bt, x) = log p(bk t−1) t−1 (cid:88)
= (cid:0)log p(bm|x, (bk t−1)<m) + β log A(bk m−1)(cid:1) + log p(v|x, bk t−1) + β log A(bk t−1) m=1
= J(bk t−1, x) + log p(v|x, bk t−1) + β log A(bk t−1), t ≥ 2 0) + β log A(bk (8) and J(b1, x) = log p(v|x, bk 0), where β is a hyper-parameter to trade-off between the probability and attention score. Similar to log p(bt|x) in Eq. 4, J(bt, x) is also an accumulative score. Consequently, at each step t, we only need compute p(v|x, bk t−1). Compared with Eq. 4, the time complexity of each step is only increased by O(K) as there are K attention scores. Replacing log p(bt|x) in Eq. 3 by J(bt, x), we can select the top K beams of each decoding step according to not only the probability distribution conditional on local information bk t−1 but also the score conditional on global information g. t−1) and A(bk
Considering the length reward function, the ﬁnal hypothesis is thus deﬁned as: y∗ = argmax yk∈Y (cid:18) J(yk, x)
|yk| − 1 (cid:19)
+ βγR(ζ k
T , Z) (9)
T . Exactly, ζ k
T is the total attention that a candidate hypothesis (yk where R(·) is the length reward function dependent on the optimal length Z and the candidate hypothesis length ζ k
T ) pays to the source and equals |yk| − 1. Besides, the attention score and length reward are weighted by a hyper-parameter γ, and the role of β is to ensure that the two are at the same level relative to the probability. We remove a in Eq. 5 as it only adjusts the length preference without really controlling the length.
The design of R(·) could be straightforward – one only need ensure that it increases as ζ k
T approaches
Z, and reaches the maximum only at ζ k
T = Z. In this paper, we design a step-wise length reward function R(ζ k t , Z) to better balance the relationship between the attention score and the length reward and make the whole searching process as succinct as beam search. We put the design details of the step-wise length reward in App. B, and we regard Eq. 9 as the general scoring formulation of global-aware inference. 1 , · · · , yk 5
3.2 Predict the Global Attention Distribution
Since the reference is unknown practically, the global attention distribution could only be predicted from the source. We construct an attention-prediction model to learn the relationship between the source tokens and the global attention distribution.
The input of the attention-prediction model is the ﬁxed encoder output E ∈ Rn×d of BART or
PEGASUS plus learnable positional encodings P ∈ Rn×d, where d is the dimension of hidden states. The input is fed to a learnable Transformer-encoder to obtain (cid:101)E ∈ Rn×d that is encoded with additional context information, followed by a linear transformation with an exponential function: (cid:16) (cid:17) (cid:98)g = exp (cid:101)EWg + bg (10) where (cid:98)g ∈ Rn refers to the prediction of g, Wg ∈ Rd×1 and bg ∈ Rn are the learnable weights and biases. The exponential function is imposed to ensure (cid:98)g > 0. We choose the exponential function for this operation because it is shown stable in the training and testing stage. Given the objective of minimizing the distance between (cid:98)g and g, the loss is deﬁned as their Euclidean distance:
L = (cid:107)(cid:98)g − g(cid:107)2
The predicted optimal length (cid:98)Z is the sum of elements in (cid:98)g. Note that the length reward function is not affected no matter whether (cid:98)Z is an integer or not. (11) 4 Experiment 4.1 Setup
Datasets. We evaluate the performance on totally 9 summarization datasets, where 2 datasets (CNN/DM [10], XSUM [5]) with BART [17] and 8 datasets (XSUM [5], BillSum [15], Multi-News
[6], NewsRoom [9], WikiHow [16], Reddit TIFU [34], arXiv and PubMed [2]) with PEGASUS [39].
Thereinto, XSUM [5] is a highly abstractive dataset whose summaries are all expressed in a short sentence.
Implementation Details. We adopt a randomly initialized 2-layer transformer-encoder in the attention-prediction model wherethe structure of each layer is the same as the BART-encoder layer.
The optimizer is the Adabelief-optimizer [41] with eps 1e − 16, betas (0.9, 0.999), weight decay 1e − 4 and learning rate 2e − 5. The attention-prediction model is trained on the training set for about 50, 000 steps, and checkpoints are saved per 10, 000 steps to select the best checkpoints on the development set. Since the attention prediction is slightly different from common sequence tagging tasks, we have summarized two notable points after several attempts – the dropout rate should be 0, and a small learning rate is preferred. All experiments are conducted on 3 RTX 6000. We include the global-aware inference in the generation code of HuggingFace transformers [36]. At the time of evaluation, ROUGE-1, ROUGE-2 & ROUGE-L (R-1, R-2 & R-L) scores [22] are computed from the
ROUGE code5 used by BART [17].
Hyper-parameter Selection. Although the global-aware inference requires two new hyper-parameters γ and β, some original hyper-parameters of beam search, namely length penalty, minimum and maximum length, are omitted. The searching scopes of β and γ are in {2, 4, 6, 10, 12, 15, 18, 20} and {0, 0.5, 1, 1.5, 2}, respectively. According to the numerical tests on the development set, we
ﬁnally choose β = 12, γ = 1.5 for CNN/DM and β = 4, γ = 0 for XSUM. As limited improvement could be observed from larger γ’s, we recommend γ = 1 for normal or longer targets. When testing the global-aware inference with PEGASUS [39], we directly use empirical hyper-parameters for each dataset, namely β = 4, γ = 0 for XSUM and β = 12, γ = 1 for other 7 datasets. The beam size K follows the setups in BART [17] and PEGASUS [39]. 4.2 Results
Comparison with Beam Search. Beam search is a hard-to-beat baseline which has stood the test of time and proven its superiority in practice for long [24]. In Table 2, we compare our global-aware 5https://github.com/pltrdy/ﬁles2rouge 6
Table 2: ROUGE F1 scores of summaries generated by global-aware, in comparison to beam search with length regularizations. Notably, global-aware uses empirical hyper-parameters.
K = 8
Beam search [39]
Global-aware
Beam search [39]
Global-aware
R-1 47.05 47.33
R-1 27.55 28.31
XSUM
R-2
R-L
R-1
BillSum
R-2
R-L
R-1
Multi-News
R-2
R-L
R-1
WikiHow
R-2 24.53 24.66 39.33 39.50 57.00 58.66 39.65 40.12 52.70 53.96 47.29 47.95 18.91 19.08 43.31 43.93 41.86 42.82
Reddit TIFU
R-2
R-L
R-1
NewsRoom
R-2
R-L
R-1
PubMed
R-2
R-L
R-1 8.67 9.13 22.12 23.30 42.05 44.68 29.88 31.71 38.70 41.28 44.25 45.78 19.19 20.16 41.11 42.62 43.82 44.92 19.04 19.68 arXiv
R-2 16.75 17.41
R-L 40.40 41.43
R-L 39.28 40.31 inference to beam search with length regularizations (i.e., α in Eq. 5, accompanied with two hard constraints, namely minimum length and maximum length). We strictly follow the hyper-parameter setups of PEGASUS [39] in terms of beam search, while we only adopt empirical hyper-parameters for our method. Even so, signiﬁcant improvements can be observed on all the data sets, especially when the summary is of normal or longer length.
Table 3: Comparison with other methods.
Table 4: ORACLE and ablation results.
K = 4
Beam search [17]
+ Our coverage
+ Repetition penalty [13]
+ Attention masking [1]
CNN/DM
R-2 21.21 21.69 21.14 22.24
R-1 44.12 44.74 44.11 45.54
R-L 40.89 41.48 40.87 42.44
Global-aware 45.13 21.77 42.04
K = 6
Beam search [17]
+ Our coverage
+ Repetition penalty [13]
+ Attention masking [1]
R-1 45.38 44.54 45.40 45.35
XSUM
R-2 22.32 21.82 22.31 22.31
R-L 37.15 36.97 37.13 37.15
Global-aware 45.57 22.60 37.61
ORACLE global-aware
-w/o length reward
Global-aware
-w/o length reward
-w/o attention score
ORACLE global-aware
-w/o length reward
Global-aware (γ = 1)
-w/o length reward
-w/o attention score
CNN/DM
R-2 28.13 27.53 21.77 21.58 21.29
XSUM
R-2 26.24 26.48 22.15 22.60 21.88
R-L 48.68 47.43 42.04 41.41 40.91
R-L 41.13 41.45 37.11 37.61 36.73
R-1 51.85 50.46 45.13 44.39 44.12
R-1 49.50 48.92 45.44 45.57 45.23
Table 5: Improvements of attention head masking and global-aware on beam search [39] in terms of
ROUGE-L F1 score. Both use empirical setups.
XSUM BillSum Multi-News WikiHow
Reddit
NewsRoom
PubMed arXiv
Attention head masking [1]
Global-aware
-0.31 0.17 0.23 1.26 0.34 0.62 0.10 1.03
-0.16 1.18 1.24 2.58 0.35 1.51 0.21 1.03
Comparison with Other Attention Approaches. In this part, we focus on comparing other ap-proaches which also exploit certain attention distributions to improve beam search. The ﬁrst is the coverage penalty [37, 21]. To enhance its performance in summarization, we replace its pre-set attention distribution with our predicted global attention distribution. Note that the coverage function can only evaluate the generated sentences at the terminal step. Instead of comparing the global-aware inference to the methods [29, 20, 7] that aim to reduce repetition using the dynamic attention distribution, we compare our algorithm with the CTRL repetition penalty [13] which has similar motivation but is more systematical and independent of training. Table 3 lists the comparison results against different algorithms. It can be observed that our global-aware approach can improve the performance of beam search stably and fundamentally. We also observe that the attention head masking [1] appears to outperform the global-aware approach on CNN/DM, but it fails to gain any improvement on XSUM. To further show the advantage of the proposed approach, we will take a closer examination on the attention head masking [1] and our proposed approach in the next part.
Further Comparison with Attention Head Masking. First, one should bear in mind that the attention head masking [1] acts on the model instead of beam search, which is opposite to us.
Speciﬁcally, it selects contents during decoding by disabling partial attention heads for unimportant 7
Figure 2: Sensitive analysis in the test set.
Table 6: Generate CNN/DM summaries with
XSUM’s style.
R1/R2/RL
Shorter beam search
Global-aware†
F1 score
Recall
Precision 43.6/20.9/40.4 48.9/23.4/45.2 41.4/19.9/38.3 43.6/20.4/40.6 40.4/18.8/37.6 50.5/23.8/47.0
Table 7: Generate summaries with corrupted attention distributions.
R-1
R-2
R-L
Beam search
Global-aware (10K)
Global-aware (100K) 27.00 28.78 29.59 12.21 12.82 13.72 23.88 25.51 26.30 tokens to decrease the attention to these tokens. According to the reported results presented in Table 3, we can see that although attention masking achieves amazing results on CNN/DM, it does fail completely on XSUM. Since hiding unimportant tokens from some heads results in the loss of context information of salient tokens, this would lead to its instability. Thus, it could be ineffective for tasks that require contextual information of the whole source such as XSUM. Taking a further comparison, we deploy the attention head combinations selected for CNN/DM and XSUM to examine its effect on PEGASUS [39]. These comparison results are shown in Table 5. Evidently, our method enjoys a robust feature that is able to boost summary inference on various datasets and models even with the same set of hyper-parameters. In contrast, attention masking [1] behaves much sensitive to the changes of models and datasets. Besides, attention masking has to construct its training saliency labels based on the longest common subsequences between a reference summary and the source.
This may be hardly achieved in some text generation tasks (e.g. translation) where no common subsequence exists at all. Such drawback presents one main limitation for attention masking.
ORACLE Results and Ablation Study. ORACLE refers to the global-aware inference combined with (true) global attention distribution instead of predicted global attention distribution. The related results have been presented in Table 4, and the signiﬁcant boosting certiﬁes that the proposed method could improve beam search with the global attention distribution. On the other hand, we conduct ablation experiments on ORACLE and global-aware. Both results indicate that length reward plays an important role in generating normal-length text but causes adverse effect on generating text of very short length. Besides, the performance declines signiﬁcantly when only length reward is applied, due to the fact that sole length reward cannot calibrate beam search step-wise.
Robustness. We intend to examine the robustness of our proposed global-aware algorithm in this and next part. To do so, we substitute the parameters in CNN/DM’s attention prediction model to
XSUM’s to create more abstractive summaries for CNN/DM. For comparison, we set the minimum length of beam search as 0 to allow it to generate shorter summaries. Table 6 shows the F1 score, recall and precision of the shorter beam search and global-aware†. It is surprising that even by using the attention distribution from a different dataset with distinct properties, the proposed global-aware mechanism still manages to generate meaningful summaries with competitive F1 scores, proving the robustness of this algorithm. Moreover, the higher Precision and lower Recall of the global-aware suggest that although information is partially lost, the algorithm still summarizes core information in a concise format, compared to the standard beam search. On the other hand, we exploit a BART model
ﬁne-tuned on CNN/DM to generate summaries of NewsRoom directly, and the ROUGE scores of beam search are shown in Table 7. Next, we randomly select 10K and 100K samples from the training set and use them to ﬁne-tune the attention-prediction model, where the global-aware improves beam search substantially. The experiment once again validates the robustness of the proposed inference algorithm, as it maintains a reasonably good performance even from learning a corrupted attention distribution from a BART without ﬁne-tuning.
Sensitive Analysis. We further examine the performance robustness with respect to the hyper-parameters. From Figure 2, we could see the global-aware inference is always better than beam search in CNN/DM, no matter how its hyper-parameters change. Besides, the performance is less sensitive to the hyper-parameters when β ≥ 10 or γ ≥ 1. While in XSUM, the global-aware could improve beam search stably with γ = 0, but there is a signiﬁcant decline when applying length 8
Figure 3: Predicted and ORACLE global attention in BART. There are attention distributions of (a) the whole source, (b) the source without the start & end tokens, (c) the source without the start & end tokens and full stops.
Figure 4: Changes of the attention distribution when (a) one word in the reference is replaced by a similar word (s1) and a random word (s2), (b) the sentence order of the reference is shufﬂed, (c) a factual knowledge in the reference is distorted. reward. In fact, the attention score favors shorter hypotheses, and the length reward could alleviate the bias. However, if the references of a dataset are already very short such as XSUM, the length reward may lead to a counterproductive effect. Since the setup of CNN/DM is applicable to most datasets, we argue that the global-aware inference is robust to both hyper-parameters in most cases. 5 Analysis on Global Attention Distribution 5.1 Distribution Law
The distribution law of global attention in BART [17] is shown in Figure 3. It is observed that most attention is assigned to the start token and full stops which are not semantically salient, and most of the remaining attention is unevenly allocated to (semantically) overlapped tokens between the source and the reference (i.e., important words). It is worth mentioning that the importance here is no longer a simple binary classiﬁcation like in [7, 1], but a continuous numerical value decided by the model knowledge learned from data. In general, one should not simply equate the global attention with the word importance, but should be clear that it essentially reﬂects knowledge learned by the attention model such as the syntactic and semantic structure of sentences. Meanwhile, the distribution law indicates that attention distributions in pre-trained models may not be relevant with the uniform distribution at all. That is to say, it is not reasonable to still use an uniform attention threshold (like the threshold 1 preset in [37, 7]) to regulate the decoding of pre-trained models. Last but not least, our general motivation is to alleviate locality bias by aligning the attention distribution of reference and hypothesis, which does not really care how the global attention is distributed only if it is predictable.
However, the proposed penalty mechanism is indeed insensitive to some distributions, and we will provide more thinking about this in App. G by applying the global-aware inference to translation. 5.2 Why It can be Predicted from Source?
Since the ORACLE experiments indicated that the global attention is helpful for decoding, the concern remains if it is predictable using only source. In App. E.1 we will show the predictability empirically, while here we just provide an interesting explanation. In our opinion, the global attention distribution is an interpretable representation of reference which not only has the characteristics of hidden representation but also can be explained by the source tokens. First of all, given the source, the global attention is calculated by the input reference and trained neural network parameters; 9
this is similar to achieving hidden representation. Moreover, like hidden representation, the global attention distribution could also capture the semantic similarity, e.g., replacing a reference word with a semantically similar word leads to slighter attention changes than that with a random word (see Figure 4.a). Besides, it is observed from Figure 4.b and Figure 4.c that global attention is able to represent the sentence order and factual knowledge. On the other hand, the global attention distribution enjoys a distinct characteristic that each of its features can be explained as the degree of attention paid to a source token, which means changes of such representation are regular to some extent. For example, in Figure 4.c, we distort a numerical number in the reference to violate the fact stated in the source, and then ﬁnd that the attention assigned to the actual number originally is most transferred to the special tokens and punctuation. Overall, similar sources should have similar global attention distributions, since similar sources often have similar references and global attention distribution is a representation of reference. Moreover, the global attention and source tokens are in an one-to-one correspondence. Thereby, we argue that it is convenient to use the source to predict the global attention distribution. 6