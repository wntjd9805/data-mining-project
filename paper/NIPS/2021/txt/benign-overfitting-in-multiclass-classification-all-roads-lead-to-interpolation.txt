Abstract
The growing literature on “benign overﬁtting” in overparameterized models has been mostly restricted to regression or binary classiﬁcation settings; however, most success stories of modern machine learning have been recorded in multiclass set-tings. Motivated by this discrepancy, we study benign overﬁtting in multiclass linear classiﬁcation. Speciﬁcally, we consider the following popular training algo-rithms on separable data: (i) empirical risk minimization (ERM) with cross-entropy loss, which converges to the multiclass support vector machine (SVM) solution; (ii) ERM with least-squares loss, which converges to the min-norm interpolating (MNI) solution; and, (iii) the one-vs-all SVM classiﬁer. Our ﬁrst key ﬁnding is that under a simple sufﬁcient condition, all three algorithms lead to classiﬁers that interpolate the training data and have equal accuracy. When the data is generated from Gaussian mixtures or a multinomial logistic model, this condition holds under high enough effective overparameterization. Second, we derive novel error bounds on the accuracy of the MNI classiﬁer, thereby showing that all three training algo-rithms lead to benign overﬁtting under sufﬁcient overparameterization. Ultimately, our analysis shows that good generalization is possible for SVM solutions beyond the realm in which typical margin-based bounds apply. 1

Introduction
Modern deep neural networks are overparameterized with respect to the amount of training data and achieve zero training error, yet generalize well on test data. Recent analysis has shown that
ﬁtting of noise in regression tasks can in fact be relatively benign for sufﬁciently high-dimensional linear models [BLLT20, BHX20, HMRT19, MVSS20, KLS20]. However, these analyses do not 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
directly extend to classiﬁcation, which requires separate treatment. In fact, very recent progress on sharp analysis of interpolating binary classiﬁers [MNS+20, CL21, WT21, CGB21] revealed high-dimensional regimes in which binary classiﬁcation generalizes well, but the corresponding regression task does not work and/or the success cannot be predicted by classical margin-based bounds.
In an important separate development, these same high-dimensional regimes admit an equivalence of loss functions used at training time. The support vector machine (SVM), which arises from minimizing the logistic loss using gradient descent [SHN+18, JT19], was recently shown to sat-isfy a high-probability equivalence to interpolation, which arises from minimizing the squared loss [MNS+20, HMX21]. This equivalence suggests that interpolation is ubiquitous in very overpar-maeterized settings, and can arise naturally as a consequence of the optimization procedure even when this is not explicitly encoded or intended. Moreover, this equivalence to interpolation and corresponding analysis implies that the SVM can generalize even in regimes where classical learning theory bounds are not predictive. In the logistic model case [MNS+20] and Gaussian binary mixture model case [CL21, WT21, CGB21], it is shown that good generalization of the SVM is possible beyond the realm in which classical margin-based bounds apply. These analyses lend theoretical grounding to the surprising hypothesis that squared loss can be equivalent to, or possibly even superior, to the cross-entropy loss for classiﬁcation tasks. This hypothesis was supported empirically on kernel machines in Ryan Rifkin’s doctoral dissertation work [Rif02, RK04], and more recently in overparameterized neural networks [HB20, PL20].
These compelling perspectives have thus far been limited to regression and binary classiﬁcation settings.
In contrast, most success stories and surprising new phenomena of modern machine learning have been recorded in multiclass classiﬁcation settings, which appear naturally in a host of applications that demand the ability to automatically distinguish between large numbers of different classes; for example, the popular ImageNet dataset [RDS+15] contains on the order of 1000 classes.
Whether a) good generalization beyond effectively low-dimensional regimes where margin-based bounds are predictive is possible, and b) equivalence of squared loss and cross-entropy loss holds in multiclass settings remained open problems.
This paper makes signiﬁcant progress towards a complete understanding of the optimization and gener-alization properties of high-dimensional linear multiclass classiﬁcation, both for unconditional Gaus-sian covariates (where labels are generated via a multinomial logistic model), and high-dimensional
Gaussian mixture models. Our contributions are listed in more detail below. 1.1 Our Contributions
We establish a deterministic sufﬁcient condi-• tion under which the multiclass SVM solution has a very simple and symmetric structure: it is identical to the solution of the One-vs-All (OvA)
SVM classiﬁer that uses the one-hot encoded la-bels. Moreover, the constraints at both solutions are active. Geometrically, this means that all data points are support vectors.
This implies a surprising equivalence be-• tween traditionally different formulations of multiclass SVM, which in turn are equivalent to the minimum-norm interpolating (MNI) clas-siﬁer on one-hot label vectors. Thus, the out-comes of training with cross-entropy (CE) loss and squared loss are identical.
Figure 1: Contributions and organization.
Next, for data following a Gaussian-mixtures model (GMM) or a Multinomial logistic model
• (MLM), we show that the above sufﬁcient condition is satisﬁed with high-probability under sufﬁcient effective overparameterization depending on the number of classes, and on quantities related to the data covariance. Our numerical results show excellent agreement with our theoretical ﬁndings.
Subsequently, we provide novel bounds on the error of the MNI classiﬁer for data generated
• either from the GMM or the MLM and characterize overparmeterization conditions under which benign overﬁtting occurs. A direct outcome of our results is that benign overﬁtting occurs under these conditions regardless of whether the cross-entropy loss or squared loss is used during training. 2
Figure 1 describes our contributions and their implications through a ﬂowchart. To the best of our knowledge, these are the ﬁrst results characterizing a) equivalence of loss functions, and b) generalization of interpolating solutions in the multiclass setting. The multiclass setting poses several challenges over and above the recently studied binary case. When presenting our results in later sections, we discuss in detail how our analysis circumvents these challenges. 1.2