Abstract
Deep neural networks have been used widely to learn the latent structure of datasets, across modalities such as images, shapes, and audio signals. However, existing models are generally modality-dependent, requiring custom architectures and objectives to process different classes of signals. We leverage neural ﬁelds to capture the underlying structure in image, shape, audio and cross-modal audiovisual domains in a modality-independent manner. We cast our task as one of learning a manifold, where we aim to infer a low-dimensional, locally linear subspace in which our data resides. By enforcing coverage of the manifold, local linearity, and local isometry, our model — dubbed GEM — learns to capture the underlying structure of datasets across modalities. We can then travel along linear regions of our manifold to obtain perceptually consistent interpolations between samples, and can further use GEM to recover points on our manifold and glean not only diverse completions of input images, but cross-modal hallucinations of audio or image signals. Finally, we show that by walking across the underlying manifold of GEM, we may generate new samples in our signal domains1. 1

Introduction
Every moment, we receive and perceive high-dimensional signals from the world around us. These signals are in constant ﬂux; yet remarkably, our perception system is largely invariant to these changes, allowing us to efﬁciently infer the presence of coherent objects and entities across time.
One hypothesis for how we achieve this invariance is that we infer the underlying manifold in which perceptual inputs lie [1], naturally enabling us to link high-dimensional perceptual changes with local movements along such a manifold. In this paper, we study how we may learn and discover a low-dimensional manifold in a signal-agnostic manner, over arbitrary perceptual inputs.
Manifolds are characterized by three core properties [2, 3]. First, a manifold should exhibit data coverage, i.e., all instances and variations of a signal are explained in the underlying low-dimensional space. Second, a manifold should be locally metric, enabling perceptual manipulation of a signal by moving around the surrounding low-dimensional space. Finally, the underlying structure of a manifold should be globally-consistent; e.g. similar signals should be embedded close to one another.
Existing approaches to learning generative models, such as GANs [4], can be viewed as instances of manifold learning. However, such approaches have two key limitations. First, low-dimensional latent codes learned by generative models do not satisfy all desired properties for a manifold; while the underlying latent space of a GAN enables us to perceptually manipulate a signal, GANs suffer from mode collapse, and the underlying latent space does not cover the entire data distribution. Second, existing generative architectures are biased towards particular signal modalities, requiring custom architectures and losses depending on the domain upon which they are applied – thereby preventing us from discovering a manifold across arbitrary perceptual inputs in a signal-agnostic manner. 1Code and additional results are available at https://yilundu.github.io/gem/. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
As an example, while existing generative models of images are regularly based on convolutional neural networks, the same architecture in 1D does not readily afford high-quality modeling of audio signals. Rather, generative models for different domains require signiﬁcant architecture adaptations and tuning. Existing generative models are further constrained by the common assumption that training data lie on a regular grid, such as grids of pixels for images, or grids of amplitudes for audio signals. As a result, they require uniformly sampled data, precluding them from adequately modeling irregularly sampled data, like point clouds. Such a challenge is especially prominent in cross-modal generative modeling, where a system must jointly learn a generative model over multiple signal sources (e.g., over images and associated audio snippets). While this task is trivial for humans – we can readily recall not only an image of an instrument, but also a notion of its timbre and volume – existing machine learning models struggle to jointly ﬁt such signals without customization.
To obtain signal-agnostic learning of manifolds over signals, we propose to model distributions of signals in the function space of neural ﬁelds, which are capable of parameterizing image, au-dio, shape, and audiovisual signals in a modality-independent manner. We then utilize hypernet-works [5], to regress individual neural ﬁelds from an underlying latent space to represent a signal distribution. To further ensure that our dis-tribution over signals corresponds to a manifold, we formulate our learning objective with explicit losses to encourage the three desired properties for a manifold: data coverage, local linearity, and global consistency. The resulting model, which we dub GEM2, enables us to capture the manifold of a variety of signals – ranging from audio to images and 3D shapes – with almost no architectural modiﬁcation, as illustrated in Fig-ure 1. We further demonstrate that our approach reliably recovers distributions over cross-modal signals, such as images with a correlated audio snippet, while also eliciting sample diversity.
Figure 1: GEM learns a low-dimensional latent mani-fold over signals. Given a cross-modal signal, latents in
GEM are mapped, using a hypernetwork ψ, into neural networks φ1 and φ2. φ1 represents a image by mapping each pixel position (x, y) to its associated color c(x, y).
φ2 represents an audio spectrogram by mapping each pixel position (u, v) to its intensity w(u, v). This en-ables GEM to be applied in a domain agnostic manner across separate (multi-modal) signals, by utilizing a sep-arate function φ for each mode of a signal.
We contribute the following: ﬁrst, we present GEM, which we show can learn manifolds over images, 3D shapes, audio, and cross-modal audiovisual signals in a signal-agnostic manner. Second, we demonstrate that our model recovers the global structure of each signal domain, permitting easy interpolation between nearby signals, as well as completion of partial inputs. Finally, we show that walking along our learned manifold enables us to generate new samples for each modality. 2