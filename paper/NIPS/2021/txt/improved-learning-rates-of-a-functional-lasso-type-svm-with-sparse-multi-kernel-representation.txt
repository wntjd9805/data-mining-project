Abstract
In this paper, we provide theoretical results of estimation bounds and excess risk upper bounds for support vector machine (SVM) with sparse multi-kernel representation. These convergence rates for multi-kernel SVM are established by analyzing a Lasso-type regularized learning scheme within composite multi-kernel spaces. It is shown that the oracle rates of convergence of classiﬁers depend on the complexity of multi-kernels, the sparsity, a Bernstein condition and the sample size, which signiﬁcantly improve on previous results even for the additive or linear cases. In summary, this paper not only provides uniﬁed theoretical results for multi-kernel SVMs, but also enriches the literature on high-dimensional nonparametric classiﬁcation. 1

Introduction
SVM was ﬁrst introduced in [45] and has became one of the most popular machine learning algorithms in the past two decades. The standard SVM classiﬁcation consists of two main ingredients, namely the hinge loss and the kernel embedding. The hinge loss is used to model the learning target and can often generate sparse solutions [38], while the kernel embedding is used to model nonlinear relationship between input features and response [39]. In [38], they provide a detailed overview of
SVMs and related learning theory. More work on theoretical perspective of SVM have also been developed in recent years, such as [12, 53, 36, 33, 30, 24], among many others.
It is known that the performance of kernel machines largely depends on the data representation via the choice of kernel function [34, 16, 19, 28, 26, 48, 49]. Towards this direction, many approaches have been proposed for kernel selection under different frameworks. For example, Micchelli et al.[34] attempt to ﬁnd an optimal kernel from a prescribed convex set of basis kernels; Wu et al.[46] optimize the scale parameter among various Gaussian kernels; and Ong et al.[35] study hyperkernels on the space of kernels and alternative approaches, including kernels selection by
DC programming and semi-inﬁnite programming. In particular, the seminal work of [22] proposes the so-called multiple kernel learning (MKL) method, learning SVM and a linear combination of kernels simultaneously, which has received a lot of attention in recent years [3, 50, 20, 19, 16, 23, 17, 27, 25, 29, 51]. To be precise, given a ﬁnite (possibly large) dictionary of symmetric positive semideﬁnite kernels {Km : m = 1, 2, ..., M } , one can try to ﬁnd an ‘ideal’ kernel K as a convex combination of the kernels from a predeﬁned family of potential candidates: K ∈ K := (cid:110)(cid:80)M
. This combination of multiple kernels corresponds to a m=1 θmKm : (cid:80)M m=1 θm = 1, θm ≥ 0 (cid:111)
∗Corresponding Author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
multi-kernel hypothesis space for learning:
HM := (cid:40) M (cid:88) m=1 fm(x) : fm ∈ HKm , x ∈ X
, (cid:41) where HKm is a reproducing kernel Hilbert space (RKHS) induced by the kernel Km, as deﬁned in
Section 2. Given the learning rule, θm’s also need to be estimated automatically from the training data.
Besides ﬂexibility enhancement, other justiﬁcations of MKL have also been proposed; such as each kernel function corresponds to different information source (e.g. text, image, gene), or classiﬁers from different spaces (corresponding to different kernels) are averaged to achieve better classiﬁcation accuracy [8]. Under many of these situations, the number of candidate kernels M is often very large, even larger than the sample size n in some extreme cases. Interestingly, when one-dimensional linear (or additive) kernels are used and M is the dimensionality of covariates with M ≥ n, MKL reduces to high-dimensional linear (or additive) models, which have been widely studied in the statistics and machine learning literature [15, 6, 32].
Given the large number of kernels, it is common that not all the kernels are signiﬁcantly relevant to the response. To avoid impairing generalization performance [54] and enhance interpretability, many redundant kernels should be removed accordingly. In view of generalization ability, interpretability and computational feasibility, a more parsimonious and ﬂexible algorithm for MKL is to generate a sparse linear combination of kernels based on the training data, namely sparse MKL. From a statistical point of view, sparse MKL can be interpreted as a model selection task, and can also be viewed as an extension of sparse linear models or sparse additive models. In other words, sparse MKL provides an appropriate route to tackle the kernel learning issues in machine learning and the high-dimensional issues in statistics.
In literature, many regularized learning algorithms based on different regularization terms have been investigated in multi-kernel regression [3, 20, 18, 37, 41]. Particularly, in [37, 41], they established the optimal rates of convergence for the least square approaches in a related setting. In multi-kernel classiﬁcation, Christmann and Hable [12] gave consistency and robustness property of non-sparse additive SVM when M is ﬁxed; In [36, 42], they provided upper bounds on the estimation error and the excess risk in high-dimensional linear case or with ﬁnite number of basis functions. In summary, all the aforementioned work either focus on the regression problem with the quadratic losses, or only consider the parametric case for high-dimensional SVM.
The main focus of this paper is on a more challenging case with sparse multi-kernel approximation for SVM. That is, while the total number of kernels may be larger than the sample size, only a small number of kernels are needed to represent/approximate the target function, so that such a learning problem in the sense of approximation is sparse. Let X be a random variable of the input space X , and Y ∈ Y = {−1, +1} be the response. Deﬁne the theoretical oracle predictor of the classical SVM within HM as f ∗ = arg min f ∈HM
E(f ), with E(f ) := E[φ(Y f (X))]. (1)
Here φ(t) := (1−t)+ is the hinge loss and the expectation is taken with respect to the joint distribution
ρ deﬁned on X × Y. In the sparse setting, one often assumes that f ∗ has an additive and sparse representation within HM , namely, f ∗ = (cid:80) m with an unknown subset S ⊂ {1, 2, ..., M } and f ∗ m ∈ Hm with (cid:107)f ∗ m(cid:107)Km ≤ 1 for all m ∈ S. Of primary interest is the case with s := |S| (cid:28) M .
Note that f ∗ here is an optimal estimator within HM , as opposed to the classical Bayes decision rule deﬁned over all the measurable functions. Actually, the minimizer of the expected risk corresponding to the hinge loss is not continuous against the conditional probability [5]. m∈S f ∗
The main contribution of this paper is to develop a set of theoretical results on multi-kernel SVM and the rates of convergence for nonparametric classiﬁcation within the multi-kernel setting, including high-dimensional linear or additive classiﬁcation as special cases. This paper provides reﬁned error bounds for the excess risk, deﬁned as E( ˆf ) − E(f ∗), of the proposed estimator ˆf , as well as its estimation error (cid:107) ˆf − f ∗(cid:107)2. The proposed estimator in (5) is based on a regularization of the hinge loss with two fold (cid:96)1 penalty terms, where one is used to control nonparametric sparsity and the other is to control functional smoothness. Particularly, Corollary 1 shows that with high probability, the 2
excess risk of the proposed estimator is upper bounded by s
Γ2(S, ρX ) (cid:16)
O n− 1 1+τ + (cid:17) κ 2κ−1 log M n under some regularity conditions, where τ corresponds to the spectral decay of each RKHS and is used to characterize functional complexity of kernels, κ is called the Bernstein parameter that reﬂects the low-density level of decision boundary, and Γ(S, ρX ) is used to characterize correlation structure among multiple RKHS’s. The same rate also holds true for (cid:107) ˆf − f ∗(cid:107)2κ following the Bernstein 2 condition and the established results on the excess risk.
Furthermore, we establish the oracle rate of the relative misclassiﬁcation error of ˆf in Theorem 2, which consists of the same rate as the excess risk of the estimator plus some additional sparse approximation error. By using this speciﬁc form of RKHS’s, we can incorporate many existing results as special cases of MKL for SVMs, and more importantly, several existing results can be improved by our derived ones. The detailed comparison between our general results with existing results is conducted in Section 3.3.
X |f (x)|2dρX (x) with marginal
Notations. Deﬁne the L2 norm of a function f by (cid:107)f (cid:107)2 n := 1 distribution ρX . Given sample points {xi}n i=1 fm(xi)2 and (cid:107)fm(cid:107)n is viewed as n the empirical L2-norm. a (cid:39) b means that there are two positive constants c, C such that ca ≤ b ≤ Ca.
Also, a = O(b) means that there is some positive constant C such that a ≤ Cb in probability, and a = o(b) means that a/b → 0 in probability. To ease the notations, we write [M ] := {1, ..., M }. 2 = (cid:82) (cid:80)n i=1, (cid:107)fm(cid:107)2 2 Preliminaries and Algorithms
Given a compact set X , we denote a positive semideﬁnite kernel on X × X , i.e., a symmetric function
K : X × X → R satisfying (cid:80)n i,j=1 cicjK(xi, xj) ≥ 0 for any x1, ..., xn in X and c1, ..., cn ∈ R.
As showed in [2], a positive semideﬁnite kernel on X is associated with a unique Hilbert space
HK consisting of functions on X , and HK is called a RKHS associated with the kernel K. RKHS is known for its reproducing property, i.e., for any f ∈ HK and x ∈ X , f (x) = (cid:104)f, Kx(cid:105)K with
Kx = K(x, ·). Another key property of RKHS is the spectral theorem, which assures that K admits the following eigen-decomposition:
K(x, x(cid:48)) = (cid:88) (cid:96)≥1
µ(cid:96)ψ(cid:96)(x)ψ(cid:96)(x(cid:48)), for any x, x(cid:48) ∈ X , (2) where µ1 ≥ µ2 ≥ ... ≥ 0 are its eigenvalues and {ψ(cid:96) : (cid:96) ≥ 1} are the corresponding eigenfunctions, leading to an orthogonal basis in L2(ρX ). These two fundamental properties of RKHS will be the foundation of our theoretical analysis.
To characterize functional complexities, we ﬁrst introduce several basic facts of the empirical processes deﬁned in RKHS. On basis of (2), for any δ ∈ (0, 1], we deﬁne
ωn(δ) := (cid:32) 1 n n (cid:88) (cid:96)=1 (cid:0)µ(cid:96) ∧ δ2(cid:1) (cid:33)1/2
, where a ∧ b means min{a, b}. For some constant A > 0, we deﬁne a quantity associated with ωn(δ) as (cid:40) (cid:114) (cid:15)(K) := inf (cid:15) ≥
A log M n
: ωn(δ) ≤ (cid:15)δ + (cid:15)2, ∀ δ ∈ (0, 1]
. (cid:41)
The quantity (cid:15)(K) plays a key role in bounding the excess risk in RKHS [40, 20]. Given appropriate decay of µ(cid:96), the upper bound of (cid:15)(K) can be derived explicitly in Section 3.1.
Given the training sample {Xi, Yi}n solves the following optimization task (cid:40) i=1 i.i.d. from X × Y, the standard SVM with a single kernel [45] (cid:41)
φ(Yif (Xi)) + λ(cid:107)f (cid:107)2
K
, (3) min f ∈HK n (cid:88) 1 n i=1 3
where λ is a regularization parameter that balances the tradeoff between the empirical risk and the function complexity. The theoretical properties of the standard SVM has been extensively studied in literature; see the earlier work of [11, 39, 7].
In the context of MKL, the sparse multi-kernel SVM [34] equips the empirical hinge loss with the (cid:96)1 penalty, min f =(cid:80)M m=1 fm, fm∈HKm (cid:40) 1 n n (cid:88) i=1
φ(Yif (Xi)) + λ
M (cid:88) m=1 (cid:41) (cid:107)fm(cid:107)Km
. (4)
This type of sparse MKL method has been studied extensively in [34, 3, 20, 16], and among others.
Essentially, it can be regarded as an inﬁnite-dimensional version of the Lasso-type penalization [43].
In this paper, we present a different approach from (4) to formulate the sparse multi-kernel SVM.
For simplicity, we assume that supx∈X |Km(x, x)| ≤ 1 for all m ∈ [M ]. We use a new L1-type regularization term partially inspired by the additive mean models [32]. Denote the bounded ball of HM by BM :=
, the regularization term we adopt for the multiple-kernel SVM combines the empirical L2-norms and RKHS-norms. Speciﬁcally, the proposed sparse multi-kernel SVM is formulated as j=1 fm : fm ∈ HKm, (cid:107)fm(cid:107)Km ≤ 1 (cid:110) f = (cid:80)M (cid:111)
ˆf =
M (cid:88) m=1
ˆfm := arg min f ∈BM (cid:40) 1 n n (cid:88) i=1
φ(Yif (Xi)) +
M (cid:88) m=1
λm (cid:113) (cid:107)fm(cid:107)2 n + γm(cid:107)fm(cid:107)2
Km (cid:41)
, (5) where (cid:107)fm(cid:107)n is used to enforce the sparsity in ˆf , whereas (cid:107)fm(cid:107)Km is used to enforce the smoothness of each ˆfm. Here (λm, γm)M m=1 are the regularization parameters, which will be speciﬁed in our theoretical results. By ﬁnite representation of reproducing kernel, each additive estimator fm(·) = (cid:80)n i Km(xi, ·) for all m = 1, ..., M . A direct computation leads to i=1 αm (cid:113) (cid:107)fm(cid:107)2 n + γn(cid:107)fm(cid:107)2
K = (cid:113) (αm)T ˜Kmαm m n + γnKm. Here Km is the kernel matrix induced by Km at points {xi}n where ˜Km = K2 i=1. Note that ˜Km is a semi-deﬁnite matrix, which can be written as ˜Km = A2 with some matrix A. Hence, our original learning scheme in Eq.(5) can be transformed into a group Lasso optimization [32], and there exists several efferent numerical algorithms for solving it, such as proximal methods and coordinate descent ones. (cid:17)
Km (cid:113) (cid:107)fm(cid:107)2 (cid:16)(cid:80)M m=1 λm n + γm(cid:107)fm(cid:107)2 m=1 (cid:107)fm(cid:107)Km [22] and (cid:80)M
This penalty term signiﬁcantly differs from other sparsity penalties for nonparametric models in literature, such as (cid:80)M m=1(λ1(cid:107)fm(cid:107)n + λ2(cid:107)fm(cid:107)Km ) [20, 37]. Although the former penalty often generates sparse solutions, it is difﬁcult to establish its theoretical results, due to the fact that the (cid:107) · (cid:107)K-norm cannot fully reﬂect the marginal distribution information. The latter penalty has been proved to enjoy some theoretical properties in [37], which is equivalent to the mixed
L1-norm in (5). Theoretically, based on empirical processes theory, our proposed approach can also achieve improved learning rates, like the penalized method with the penalty (cid:80)M m=1(λ1(cid:107)fm(cid:107)n + λ2(cid:107)fm(cid:107)Km), which has been considered in [20, 31, 37] and others.
Remark. In general, (cid:107) ˆf (cid:107)n = 0 does not imply f = 0. However, in the case of any kernel-based minimization problem, (cid:107) ˆf (cid:107)n = 0 always implies f = 0. Based on the reproducing property of
Mercer kernel, f (x) = (cid:104)f, Kx(cid:105)K for any f ∈ HK, where HK is a reproducing kernel Hilbert space.
In fact, if (cid:107) ˆf (cid:107)n = 0 holds, that is, ˆf (xi) = 0 for all i = 1, . . . , n, by the reproducing property we have (cid:104) ˆf , Kxi(cid:105)K = 0 for all i. Hence, ˆf is orthogonal to the subspace Sn := span{Kx1, ..., Kxn }.
On the other hand, using the reproducing property again, any solution ˆf of kernel-based minimization problems has a ﬁnite representation within Sn. So we conclude ˆf = 0.
Remark. The use of the mixed norm regularization is mainly motivated by the following fact: i) the proposed estimation with the mixed norm can lead to very sharp learning rates; ii) the empirical norm (cid:107) · (cid:107)n used for sparsity is much milder than (cid:107) · (cid:107)K.
There exists several related work on multi-kernel SVM. Under the setting of one-dimensional additive kernels, Christmann and Hable [12] constructed kernels for additive SVM and provided consistent and 4
statistically robust estimators under the ﬁxed dimensional and non-sparse setting. Zhao and Liu [53] proposed a group Lasso penalty by means of ﬁnite-bases approximation to a RKHS, and particularly they developed an efﬁcient accelerated proximal gradient descent algorithm and established oracle properties of the SVM under sparse ultra-high dimensional setting (e.g. M = o(en)). However, the additive SVM model may suffer from the lack of algorithmic ﬂexibility and underﬁtting especially when the true model involves interaction effects. Similar concerns have been raised towards linear
SVM as in [55, 21, 52].
Note that in the past decade, a lot of work on general MKL with logarithmic dependence on M have emerged [18, 41, 20], yet their analysis requires the loss function to be strongly convex, which rules out the commonly-used hinge loss for SVM. We also note that [13] provided an upper bound
Op((cid:112)log M/n) of the Rademacher complexity with the L1-norm constraint, which may lead to the same decay rate of the excess risk of SVM. However, this rate is not tight in general, since it is known that the fast rate of order 1/n can be attained for the linear case [11, 44].
This paper primarily focuses on the non-asymptotic analysis of the proposed sparse multi-kernel
SVM method in (5) with an exponential number of kernels. Under the best ideal settings, the relative classiﬁcation error of the proposed method is of near-oracle rate O(s log(M )/n), as if we knew the true sparsity in advance. Moreover, the method is adaptive to the sparsity of the learning problem and the margin parameter. In our proof, we have to face some technical challenges, such as dealing with non-smoothness of the hinge loss, functional complexities and NP-dimensionality. 3 Main Results
This section quantiﬁes the asymptotic behavior of the proposed sparse multi-kernel SVM (5) in estimating the oracle predictor f ∗ in (1). Its asymptotic convergence rates are established in terms of both generalization error and estimation error.
Assumption A (Bernstein condition). There exist universal constants c0 > 0 and κ ≥ 1, such that
E(f ) − E(f ∗) ≥ c0(cid:107)f − f ∗(cid:107)2κ 2 , for all f ∈ BM .
Assumption A is a lower bound for the hinge excess risk in term of the L2(ρX ) norm, as a strong identiﬁcation condition of the population quantity with the hinge loss. The Bernstein condition stems from [4] and it has been veriﬁed for the hinge loss in [1]. Particularly, for linear SVM, it has been veriﬁed in [36] that the Bernstein condition holds with κ = 1, leading to the fast learning rate. Related to the Bernstein condition, a more standard margin condition in [42] has been commonly assumed in literature, where E(f ) − E(fc) ≥ c0(cid:107)f − fc(cid:107)2κ 1 , and fc is the minimizer of the misclassiﬁcation error over all possible measurable functions. As a consequence, the Bernstein condition is more stringent than the standard margin condition, and detailed discussion is referred to Proposition 8.3 of [1].
Since the complexity of a RKHS is determined by the decay rate of the eigenvalues µ(cid:96)’s [39], we now introduce the following spectral condition for the subsequent analysis.
Assumption B (Spectral condition). There exist a sequence of 0 < τm < 1 and a universal constant c1 > 0, such that
µ(m) (cid:96) ≤ c1(cid:96)−1/τm,
∀ (cid:96) ≥ 1, m ∈ [M ]. (6)
Eq.(6) means the decay rate of the eigenvalues of kernel is polynomial. Note that τm < 1 is a very weak condition, due to the relation that (cid:80) (cid:96)=1 µ(m) (cid:96) = E[Km(X, X)] ≤ 1. For example, if ρX is the
Lebesgue measure on [0, 1], it is known that µ(m) (cid:96) (cid:16) (cid:96)−2α for the Sobolev class HKm = W α 2 with
α > 1 2 . Indeed, spectral condition has a close quantitative relationship with the entropy number of the RKHS under mild conditions; see [40] for details.
Assumption C (Sup-norm condition). For the sequence of 0 < τm < 1 given in Assumption B, there exists some universal constant c2 > 0, such that (cid:107)g(cid:107)τm
Km
∀ g ∈ HKm, m ∈ [M ]. (cid:107)g(cid:107)∞ ≤ c2(cid:107)g(cid:107)1−τm 2
,
As pointed out in [40], under some mild conditions on HKm’s, Assumption C is equivalent to the spectral decay, as stated in Assumption B. 5
For some constant b > 0, we deﬁne a restricted subset of HM by (cid:40)
F b
S = f ∈ BM :
M (cid:88) m=1
λm (cid:113) (cid:107)fm − f ∗ m(cid:107)2 2 + (cid:15)2(Km)(cid:107)fm − f ∗ m(cid:107)2
Kj
≤ b (cid:88) (cid:113)
λm m∈S (cid:107)fm − f ∗ m(cid:107)2 2 + (cid:15)2(Km)(cid:107)fm − f ∗ m(cid:107)2
Kj (cid:41)
.
The set F b remaining ones.
S is a cone in the space HM , where the components corresponding to j ∈ S dominate the
The following quantity is also crucial in our theoretical analysis, which is used to describe how
‘dependent’ these different RKHS’s are. Particularly,
Γ(S; ρX ) := sup



γ > 0 : γ (cid:33) (cid:107)fm − f ∗ m(cid:107)2 2
≤ (cid:32) (cid:88) m∈S (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
M (cid:88) (fm − f ∗ m) m=1 (cid:13) 2 (cid:13) (cid:13) (cid:13) (cid:13) 2
, (f1, ..., fM ) ∈ F b
S



.
We can regard Γ(S; ρX ) as a generalized correlation between the components corresponding to j ∈ S and j ∈ Sc, respectively.
Assumption D (Correlation condition). There exists some universal constant c3, such that
Γ(S; ρX ) > c3 > 0.
Loosely speaking, this represents the correlation among RKHS’s over the cone set where the compo-nents within the relevant indices S well “dominate" the remaining ones. Lemma 1 in [41] shows that
Assumption D is related to two geometric quantities. In fact, Assumption D has been widely used for various sparse problems, such as [10] and [6] for linear models, and [20] and [41] for sparse MKL with the quadratic loss.
Remark. Assumptions A and D imply that c1/κ c3 over
F b
S, which is sufﬁcient for deriving most of our results except for the estimation error. We also observe that, for the high-dimensional linear SVM, the restricted eigenvalue condition in [36] implies the above conclusion. 2 ≤ (cid:0)E(f ) − E(f ∗)(cid:1)1/κ m∈S (cid:107)fm − f ∗ m(cid:107)2 (cid:80) 0 3.1 Oracle Rates
When the oracle predictor f ∗ deﬁned over HM is sparse, we now state the upper bounds on the excess risk and the estimation error of the proposed multi-kernel SVM in (5). We allow the number of kernels M and the number of active kernels s increases with the sample size n.
Theorem 1. Suppose that Assumptions A, C and D hold, and all the following constraints are satisﬁed: 2M (cid:15)(Km) ≤ eM , λm ≥ 4C0C1(cid:15)(Km) and γm ≥ 4(cid:15)2(Km)/C 2 0 for all m ∈ [M ]. Then with probability at least 1 − 4M −A, the estimated (cid:96)1-norm SVM function ˆf satisﬁes
E( ˆf ) − E(f ∗) ≤ max
Additionally, there also holds c0(cid:107) ˆf − f ∗(cid:107)2κ 2 ≤ max






√ (cid:18) 4 2c0C0
Γ(S, ρX )
√ (cid:18) 4 2c0C0
Γ(S, ρX ) 2κ−1 (cid:32) (cid:19) 2κ (cid:33) κ 2κ−1 (cid:88)
λ2 m
, 32 (cid:88)
√
γm
λm m∈S m∈S 2κ−1 (cid:32) (cid:19) 2κ (cid:33) κ 2κ−1 (cid:88)
λ2 m
, 32 (cid:88)
√
γm
λm m∈S m∈S



.



, with the same probability as above. Here A, C0 are positive constants speciﬁed in Lemma 1, and C1 speciﬁed in Proposition 1 may depend on A and c2.
The technical proof of Theorem 1 is given in Appendix A. It is easy to check that Theorems 1 also holds if one replaces M in the by an arbitrary ˜M ≥ M such that log ˜M ≥ 2 log log n. In this case, the probability bounds in the theorems become 1 − 4 ˜M −A. It also has a number of corollaries, 6
obtained by specifying particular choices of kernels. As Assumption B does not require a lower bound of the spectral decay, so all the ﬁnite-dimensional RHKS’s, Sobolev classes and Gaussian kernels are covered in our settings. We here only present a corollary for the RKHS’s with inﬁnite eigenvalues with decay rate as in Assumption B. In this case, the upper bound of (cid:15)(Km) is given by (cid:15)(Km) (cid:39) (cid:40)(cid:114) log M n
∨ n− 1 2(1+τm) (cid:41)
.
In particular, this type of scaling covers Sobolev spaces, consisting of functions with (cid:98) 1 (cid:99) derivatives. 2τm
Up to some constants, we now present a direct corollary from Theorem 1 in a homogeneous setting.
Remark. (C0, C1) are two constants independent of n, M or s. Their deﬁnitions rely on the result of Proposition 5 in [20], where their constant did not give an explicit form. So, we can not give a more explicit form on (C0, C1). Since γn = λ2 n in our theory, there is no additional hyperparameter to be optimized. To explain the role of two hyperparameters, we rewrite the mixed penaltation with two different parameters as: (cid:113) (cid:113)
λn (cid:107)fm(cid:107)2 n + γn(cid:107)fm(cid:107)2
K =
βn(cid:107)fm(cid:107)2 n + θn(cid:107)fm(cid:107)2
K.
We see from the above equation that, βn is used to control sparsity, while θn is used to control functional smoothness, due to the fact that θn is a smaller order of βn, precisly, θn = β2 n.
Corollary 1. Under the same conditions of Theorem 1 and Assumption B holds in that each kernel with eigenvalues decays at rate µ(m) (cid:96) = O((cid:96)−1/τ ) for some common τ < 1. Then any solution ˆf to (5) with λm (cid:39) (cid:15)(Km) and γm (cid:39) (cid:15)2(Km) for all m ∈ [M ] satisﬁes max (cid:110) (cid:107) ˆf − f ∗(cid:107)2κ (cid:111) 2 , E( ˆf ) − E(f ∗)
= s
Γ2(S, ρX )
O (cid:18) n− 1 1+τ + (cid:19) κ 2κ−1
, log M n with probability at least 1 − 4M −A. Specially for κ = 1, we have (cid:18) (cid:110) max (cid:107) ˆf − f ∗(cid:107)2 2, E( ˆf ) − E(f ∗) (cid:111)
= s
Γ2(S, ρX )
Op n− 1 1+τ + (cid:19)
. log M n
Corollary 1 considers the homogeneous setting that all the RKHS’s have the same complexities, denoted by the parameter τ in Assumption B. For the Gaussian kernel and the typical case with
κ = 1, the parameter τ is close to zero and thus the excess risk of our estimator attains the order
Op(s log M/n) up to the term Γ(S, ρX ), which is the minimax rate of the least square parametric regression; see [37] for details.
It is worth noting that, the choices of the regularization parameters (λm, γm) are adaptive to the sparsity and the margin, whereas the sparsity parameter s and the Bernstein parameter κ are not needed to learn the proposed estimator. Moreover, as stated in [20], τm can be replaced by its empirical estimator based on Km = (Km(xl, xk))n l,k=1, this further implies that we can deﬁne two data-driven regularization parameters instead of (λm, γm). Here we omit the details to avoid repetition.
Remark. In view of the popularity of SVM in machine learning, this paper focuses on theoretical investigation on the hinge loss with a mixed functional norm under multi-kernel setting. In fact, the current technical analysis can be easily extended to any Lipschitz loss case, e.g., the Huber loss and the quantile loss used for robust methods. Yet we think this is also beyond the focus of this paper. 3.2 Relative Classiﬁcation Error
The goal of a binary classiﬁcation procedure is to predict the label Y ∈ {−1, 1} given the value of
X. A binary classiﬁer f : X → {−1, 1} is a function from X to Y which divides the input space X into two classes. Let us split ρ(X, Y ) = ρX (X) × P(Y |X), where ρX is the marginal distribution on X and P(·|x) is the conditional probability measure given X. The efﬁciency of a binary classiﬁer f is measured by the so-called misclassiﬁcation error
R(f ) := P[f (X) (cid:54)= Y ] =
P[Y (cid:54)= f (x)|x]dρX (x). (cid:90)
X 7
It is known that fc(x) = sgn(2η(x) − 1) is a minimizer of R(f ) over all measurable functions, where η(x) = P(Y = 1| X = x) is the conditional probability of Y = 1 given x. Thus, to assess the classiﬁcation performance of a classiﬁer f , its relative classiﬁcation error, deﬁned as R(f ) − R(fc), is of some signiﬁcance.
In empirical risk minimization, the optimization of misclassiﬁcation error is difﬁcult due to its non-convexity (i.e. 0/1 loss), and a common strategy is to ﬁnd a surrogate convex loss to replace the non-convex 0/1 loss, such as the hinge loss, the logistic loss, or the quadratic loss. Therefore, to quantify the classiﬁcation error of a SVM classiﬁer, it is natural to ask for the connection between the hinge loss and the 0/1 loss. Recall from Theorem 9.21 in [14], for any measurable function f : X → R, the following inequality holds
R(sgn(f )) − R(fc) ≤ E(f ) − E(fc). (7)
Note that our general results are on the smooth function f ∗ rather than on fc, and then it is often impossible to provide rates on the estimation of fc without stringent assumption on P(Y |X) and Hs =
∪|S|=sHS with HS := (cid:8)f = (cid:80) m∈S fm, fm ∈ Hm, (cid:107)fm(cid:107)Km ≤ 1(cid:9) . Usually, fc is not necessarily sparse and smooth as f ∗, we need to consider the approximation error between all the possible sparse multi-kernel spaces and fc, deﬁned by A(Hs, fc) := inf f ∈Hs {E(f ) − E(fc)} . The quantity
A(Hs, fc) measures the approximation error of Hs in approximating fc. The sparsity s balances the approximation error A(Hs, fc) and the effective dimension of the function class Hs. Based on this notation, for any f we can rewrite E(f ) − E(fc) = E(f ) − E(f ∗) + E(f ∗) − E(fc) =
E(f ) − E(f ∗) + A(Hs, fc). This together with Theorem 1 and (7) leads to the upper bounds of the relative classiﬁcation error.
Theorem 2. Assume the same conditions of Theorem 1 are all met. We choose λm (cid:39) (cid:15)(Km) and
γm (cid:39) (cid:15)2(Km) for all m ∈ [M ], then with probability at least 1 − 4M −A, we have
R(sgn( ˆf ))−R(fc) = O
 (cid:18)
 1
Γ(S, ρX ) 2κ−1 (cid:32) (cid:19) 2κ (cid:33) κ 2κ−1 (cid:88) (cid:15)2(Km) m∈S (cid:88)
+ m∈S
 (cid:15)2(Km)
+A(Hs, fc).
In the homogeneous cases as Corollary 1, we also have
R(sgn( ˆf )) − R(fc) = s
Γ2(S, ρX )
O (cid:18) n− 1 1+τ + log M n (cid:19) κ 2κ−1
+ A(Hs, fc).
ρX in the sense that the approximation error A(Hs, fc) is negligible if fc ∈ L1
As mentioned earlier, ordinary kernels including the Gaussian kernel and the Laplace kernel are universal in L1
ρX , and in this case the excess risk of the estimator dominates the approximation error. Note that f ∗ is a sparse minimizer of E(·) deﬁned on the multi-kernel class HM . From a model selection point of view, we are mainly interested in the selection of different RKHS’s, whereas the classical SVM in (3) focuses more on selection of parameters within a single RKHS; see [7] for details.
Similar to the common margin assumption in classiﬁcation [11, 44, 1], the smaller Bernstein parameter
κ implies the lower noise level of η(x) near 1/2. Particularly, our fast rate in Theorem 2 equals when κ = 1. If there is no assumption on the margin (κ → ∞), the rate
O 1+τ + log M n− 1 (cid:16) (cid:17) n n− 1 is arbitrarily close to O bounds without any low noise condition on the margin [47]. 1+τ + log M n (cid:16) (cid:17)1/2 when s is ﬁxed, which matches the minimax lower 3.3