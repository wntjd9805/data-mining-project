Abstract
√
We consider the best-of-both-worlds problem for learning an episodic Markov
Decision Process through T episodes, with the goal of achieving (cid:101)O(
T ) regret when the losses are adversarial and simultaneously O(polylog(T )) regret when the losses are (almost) stochastic. Recent work by [Jin and Luo, 2020] achieves this goal when the ﬁxed transition is known, and leaves the case of unknown transition as a major open question. In this work, we resolve this open problem by using the same Follow-the-Regularized-Leader (FTRL) framework together with a set of new techniques. Speciﬁcally, we ﬁrst propose a loss-shifting trick in the FTRL analysis, which greatly simpliﬁes the approach of [Jin and Luo, 2020] and already improves their results for the known transition case. Then, we extend this idea to the unknown transition case and develop a novel analysis which upper bounds the transition estimation error by (a fraction of) the regret itself in the stochastic setting, a key property to ensure O(polylog(T )) regret. 1

Introduction
We study the problem of learning ﬁnite-horizon Markov Decision Processes (MDPs) with unknown transition through T episodes. In each episode, the learner starts from a ﬁxed initial state and repeats the following for a ﬁxed number of steps: select an available action, incur some loss, and transit to the next state according to a ﬁxed but unknown transition function. The goal of the learner is to minimize her regret, which is the difference between her total loss and that of the optimal stationary policy in hindsight.
When the losses are stochastically generated, [Simchowitz and Jamieson, 2019, Yang et al., 2021] show that O(log T ) regret is achievable (ignoring dependence on some gap-dependent quantities for simplicity). On the other hand, even when the losses are adversarially generated, [Rosenberg and
√
T ) regret is achievable.1 Given that the existing
Mansour, 2019a, Jin et al., 2020] show that (cid:101)O( algorithms for these two worlds are substantially different, Jin and Luo [2020] asked the natural question of whether one can achieve the best of both worlds, that is, enjoying (poly)logarithmic regret in the stochastic world while simultaneously ensuring some worst-case robustness in the adversarial world. Taking inspiration from the bandit literature and using the classic Follow-the-regularized-Leader (FTRL) framework with a novel regularizer, they successfully achieved this goal, albeit under a strong restriction that the transition has to be known ahead of time. Since it is highly unclear how 1Throughout the paper, we use (cid:101)O(·) to hide polylogarithmic terms. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
to ensure that the transition estimation error is only O(polylog(T )), extending their results to the unknown transition case is highly challenging and was left as a key open question.
In this work, we resolve this open question and propose the ﬁrst algorithm with such a best-of-both-T ) regret always, worlds guarantee under unknown transition. Speciﬁcally, our algorithm enjoys (cid:101)O( and simultaneously O(log2 T ) regret if the losses are i.i.d. samples of a ﬁxed distribution. More generally, our polylogarithmic regret holds under a general condition similar to that of [Jin and Luo, 2020], which requires neither independence nor identical distributions. For example, it covers the corrupted i.i.d. setting where our algorithm achieves (cid:101)O(
C) regret with C ≤ T being the total amount of corruption.
√
√
Techniques Our results are achieved via three new techniques. First, we propose a new loss-shifting trick for the FTRL analysis when applied to MDPs. While similar ideas have been used for the special case of multi-armed bandits (e.g., [Wei and Luo, 2018, Zimmert and Seldin, 2019, Lee et al., 2020b, Zimmert and Seldin, 2021]), its extension to MDPs has eluded researchers, which is also the reason why [Jin and Luo, 2020] resorts to a different approach with a highly complex analysis involving analyzing the inverse of the non-diagonal Hessian of a complicated regularizer. Instead, inspired by the well-known performance difference lemma, we design a key shifting function in the FTRL analysis, which helps reduce the variance of the stability term and eventually leads to an adaptive bound with a certain self-bounding property known to be useful for the stochastic world.
To better illustrate this idea, we use the known transition case as a warm-up example in Section 3, and show that the simple Tsallis entropy regularizer (with a diagonal Hessian) is already enough to achieve the best-of-both-worlds guarantee. This not only greatly simpliﬁes the approach of Jin and
Luo [2020] (paving the way for extension to unknown transition), but also leads to bounds with better dependence on some parameters, which on its own is a notable result already.
Our second technique is a new framework to deal with unknown transition under adversarial losses, which is important for incorporating the loss-shifting trick mentioned above. Speciﬁcally, when the transition is unknown, prior works [Rosenberg and Mansour, 2019a,b, Jin et al., 2020, Lee et al., 2020a] perform FTRL over the set of all plausible occupancy measures according to a conﬁdent set of the true transition, which can be seen as a form of optimism encouraging exploration. Since our loss-shifting trick requires a ﬁxed transition, we propose to move the optimism from the decision set of FTRL to the losses fed to FTRL. More speciﬁcally, we perform FTRL over the empirical transition in some doubling epoch schedule, and add (negative) bonuses to the loss functions so that the algorithm is optimistic and never underestimates the quality of a policy, an idea often used in the stochastic setting (e.g., [Azar et al., 2017]). See Section 4 for the details of our algorithm.
Finally, we develop a new analysis to show that the transition estimation error of our algorithm is only polylogarithmic in T , overcoming the most critical obstacle in achieving best-of-both-worlds.
An important aspect of our analysis is to make use of the amount of underestimation of the optimal policy, a term that is often ignored since it is nonpositive for optimistic algorithms. We do so by proposing a novel decomposition of the regret inspired by the work of Simchowitz and Jamieson
[2019], and show that in the stochastic world, every term in this decomposition can be bounded by a fraction of the regret itself plus some polylogarithmic terms, which is enough to conclude the ﬁnal polylogarithmic regret bound. See Section 5 for a formal summary of this idea.