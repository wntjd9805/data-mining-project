Abstract
Gaussian processes with derivative information are useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Incorporating derivative observations, however, comes with a dominating O(N 3D3) computational cost when training on N points in D input dimensions. This is intractable for even moderately sized problems. While recent work has addressed this intractability in the low-D setting, the high-N , high-D setting is still unexplored and of great value, particularly as machine learning problems increasingly become high dimensional.
In this paper, we introduce methods to achieve fully scalable Gaussian process regression with derivatives using variational inference. Analogous to the use of inducing values to sparsify the labels of a training set, we introduce the concept of inducing directional derivatives to sparsify the partial derivative information of a training set. This enables us to construct a variational posterior that incorporates derivative information but whose size depends neither on the full dataset size N nor the full dimensionality D. We demonstrate the full scalability of our approach on a variety of tasks, ranging from a high dimensional stellarator fusion regression task to training graph convolutional neural networks on Pubmed using Bayesian optimization. Surprisingly, we ﬁnd that our approach can improve regression performance even in settings where only label data is available. 1

Introduction
Gaussian processes (GPs) are a popular tool for probabilistic machine learning, widely used in scenarios where uncertainty quantiﬁcation for regression is necessary [27, 38, 14]. When used for
Bayesian optimization (BO) [18, 30], or in some regression settings found in the physical sciences like estimation of arterial wall stiffness, derivative information may be available [37, 34]. In these settings, we have not only noisy function values y = f (x) + (cid:15) but also noisy gradients ∇y = ∇xf (x) + (cid:15) at some set of training points X ∈ RN ×D. On paper, GPs are ideal models in these settings, because they allow for training on both labels y and gradients ∇y in closed form.
Though analytically convenient, Gaussian process inference with derivative information scales poorly: computing the marginal log likelihood and predictive distribution for an exact GP in this setting requires O(N 3D3) time and O(N 2D2) memory. Recent work has addressed this scalability in certain settings, e.g. for many training points in a low-dimensional space [5] or for few training points in a high-dimensional space [3]. Despite these advances, training and making predictions for a GP with derivatives remains prohibitively expensive in regimes where both N and D are on the order of hundreds or even thousands.
We introduce a novel method to scale Gaussian processes with derivative information using stochastic variational approximations. We show that the expected log likelihood term of the Evidence Lower 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Bound (ELBO) decomposes as a sum over both training labels and individual partial derivatives. This lets us use stochastic gradient descent with minibatches comprised of arbitrary subsets of both label and derivative information. Just as variational GPs with inducing points replace the training label information with a set of learned inducing values, we show how to sparsify the derivative information with a set of inducing directional derivatives. The resulting algorithm requires only O(M 3p3) time per iteration of training, where M (cid:28) N and p (cid:28) D.
We demonstrate the quality of our approximate model by comparing to both exact GPs with derivative information and DSKI from [5] on a variety of synthetic functions and a surface reconstruction task considered by [5]. We then demonstrate the full scalability of our model on a variety of tasks that are well beyond existing solutions, including training a graph convolutional neural network [16] on
Pubmed [28] with Bayesian optimization and regression on a large scale Stellarator fusion dataset with derivatives. We then additionally show that, surprisingly, our variational Gaussian process model augmented with inducing directional derivatives can achieve performance improvements in the regression setting even when no derivative information is available in the training set. 2