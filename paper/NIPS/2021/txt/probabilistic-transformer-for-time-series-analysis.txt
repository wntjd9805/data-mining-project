Abstract
Generative modeling of multivariate time series has remained challenging partly due to the complex, non-deterministic dynamics across long-distance time steps.
In this paper, we propose deep probabilistic methods that combine state-space models (SSMs) with transformer architectures. In contrast to previously proposed
SSMs, our approaches use attention mechanism to model non-Markovian dynam-ics in the latent space and avoid recurrent neural networks entirely. We also extend our models to include several layers of stochastic variables organized in a hierar-chy for further expressiveness. Compared to transformer models, ours are proba-bilistic, non-autoregressive, and capable of generating diverse long-term forecasts with accounted uncertainty. Extensive experiments show that our models consis-tently outperform competitive baselines on various tasks and datasets, including time series forecasting and human motion prediction. 1

Introduction
Generative modeling of multivariate time series is a challenging problem with wide-ranging appli-cations in demand forecasting [15, 76], autonomous driving [2, 16], robotics [29, 67], and health care [20, 21, 59]. Despite remarkable progress in recent years, models that predict high-dimensional future observations from a few past examples have remained intractable, partly due to the complex, non-deterministic temporal dynamics across long-distance time steps. Given a sequence of human poses, for example, such models must internally ﬁgure out the involved dynamics of various body components across space and time while maintaining the inherent uncertainty of multiple plausible futures, even though only one such future is observed.
Among proposed probabilistic approaches, state space models (SSMs) provide a principled frame-work for learning and drawing inference from sequential inputs [27, 66]. While autoregressive models feed its predictions back into the dynamics model without any compressed representation of data, SSMs model stochastic transitions between abstract states using latent variables, allowing for efﬁcient state-to-state sampling without the need to render high-dimensional observations. Gaussian linear dynamical systems (LDSs), one of the best known SSMs [92], for example, postulate linear state transitions and enjoy exact inference via the celebrated Kalman ﬁlter algorithm.
While early extensions of LDSs focus on linearization [46] and unscented transform [88], recent work that marry state space models with deep neural networks offers much more ﬂexibility to model complex dependencies across different time steps. Some approaches retain the Markovian dynamics of LDSs and only replace their linear observation models with feed-forward networks [23, 31, 47, 71], whereas others favor nonlinear state transitions and parametrize such dependencies via recurrent neural networks (RNNs) [22, 23, 30, 39, 51, 75]. Despite differences, both Markovian transitions and RNNs are often not capable of capturing long-range dependencies in highly structured sequential inputs [36, 100], limiting the capacity of the corresponding SSMs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
xt−1 xt xt+1 xt−1 xt xt+1 xt−1 xt xt+1 zt−1 zt zt+1 z(3) t−1 z(3) t z(3) t+1 z(3) t−1 z(3) t z(3) t+1 (a) LDS xt−1 xt xt+1 z(2) t−1 z(2) t z(2) t+1 z(2) t−1 z(2) t z(2) t+1 zt−1 zt zt+1 z(1) t−1 z(1) t z(1) t+1 z(1) t−1 z(1) t z(1) t+1 (b) ProTran (1 layer) (c) ProTran Generation (3 layers) (d) ProTran Inference (3 layers)
Figure 1: Graphical model representations of linear dynamical systems (LDSs) in (a), and our proposed models (ProTran) in (b), (c), and (d). Black arrows denote the generative mechanism and red arrows the inference procedure. The separation of generation and inference in (c) and (d) is for readability. While traditional SSMs such as LDSs are limited to Markovian dynamics and linear dependencies, our models allow for non-Markovian and non-linear interactions between time steps via attention mechanism. A multi-layer extension of our models further increases expressiveness without compromising the tractable inference procedure.
In this work, we propose to combine the complementary strengths of SSMs and transformer archi-tectures [85], a powerful mechanism for modeling long-term interactions that enjoys success across a variety of sequence modeling tasks [26, 48, 99]. In contrast to most SSMs, our models make extensive use of attention mechanism [5, 85] between latent variables to model non-Markovian dy-namics (see Figure 1). Compared to transformer-based methods, our models are probabilistic, non-autoregressive in a similar fashion to LDSs, and capable of generating diverse long-term forecasts with uncertainty estimates.
Our main contributions are threefold. First, we propose novel SSMs based on transformer archi-tectures for multivariate time series, which include generative models and inference procedures based on variational inference [49, 74]. Second, we extend our models to include several layers of stochastic latent variables organized in a hierarchy for further expressiveness. Third, we conduct extensive experiments on time series forecasting and human motion prediction and demonstrate that our Probabilistic Transformer (ProTran) performs remarkably well compared to various state-of-the-art baselines. 2 Preliminaries 2.1 Variational State Space Models i=1 consist of N univariate time series where x(i)
}N 1:Ti
) and x(i) 1 , x(i)
Let {x(i) t 1:Ti denotes the vaue of the i-th time series at time t. We consider the multivariate form
, . . . x(N ) x1:T = (x1, x2, . . . , xT ) where xt = (x(1)
) ∈ RN . Conditioning on observed values up t to time C, we aim to produce distributional forecasts into the future p(xC+1:T | x1:C). For clarity, we refer to x1:C and xC+1:T as contexts and targets, respectively. 2 , · · · x(i)
Ti
= (x(i) t
We are interested in probabilistic models parametrized by θ of the form pθ(x1:T | x1:C) = (cid:90) pθ(x1:T | z1:T )pθ(z1:T | x1:C)dz1:T (1) where z1:T = (z1, z2, . . . , zT ) denotes the corresponding sequence of latent variables, sometimes referred to as states. In other words, we assume a generative model that can be decomposed into a transition model pθ(z1:T | x1:C) between the latent variables conditioned on the contexts, and an emission model pθ(x1:T | z1:T ) from the latent variables to observable outputs. In particular, we 2
further impose several assumptions on both models: 1 pθ(z1:T | x1:C) =
T (cid:89) t=1 pθ(zt | z1:t−1, x1:C), pθ(x1:T | z1:T ) =
T (cid:89) t=1 pθ(xt | zt). (2)
As demonstrated in Figure 1(b), the latent variable zt+1 depends not only on zt but also on all of its preceding latent variables, including zt−1, in contrast to linear dynamical systems (LDSs). In addition, the transition and emission models allow for non-linearity via neural network parametriza-tions. These assumptions aim to maximize model capacity for real-world applications with complex emissions or temporal dependencies.
However, neither x1:t−1 nor z1:t−1 are included in the emission model p(xt | z1:T , x1:C). Such assumptions are important, as it has been argued previously that a leakage of information from the latent space in autoregressive models can hinder long-term predictions [23, 47]. While all ground truth observations are available during training, the entire sequence has to be generated sequentially at test time, making the dependencies on x1:t−1 prone to accumulated errors over multiple time steps. By letting the latent variable zt capture all information needed to render xt, we also avoid the computational costs associated with repeatedly decoding and encoding xt in multi-step predictions.
The inclusion of nonlinear state transitions and observation models necessarily requires approximate inference. We follow the stochastic variational inference framework [49, 74] and assume that the variational posterior parametrized by φ can be decomposed auto-regressively as qφ(z1:T | x1:T ) = (cid:81) t qφ(zt | z1:t−1, x1:T ), which leads to a lower bound on the log likelihood: log pθ(x1:T |x1:C) ≥
T (cid:88) t=1 (Eq [log pθ(xt|zt)] − KL(qφ(zt|z1:t−1, x1:T ) (cid:107) pθ(zt|z1:t−1, x1:C))) , (3) where KL is the Kullback-Leibler divergence.
For computational stability, we assume homoscedasticity and choose Laplace distribution with scale parameter β as a parametric form for pθ(xt | zt), i.e. we optimize for L1 reconstruction loss with a cross-validated factor β for the KL term, following similar variational autoencoder (VAE) work
[24, 41, 86]. Such an assumption does not necessarily limit the capacity of our models, as powerful stochastic transitions and ﬂexible emission models can theoretically characterize arbitrary noise covariance [66]. Incorporating structured probabilistic outputs such as Gaussian copulas [75] or normalizing ﬂows [23] can potentially further improve our model performance. 2.2 Transformer Architectures
Central to our models and other transformer-based approaches [48, 85] is the notion of attention
[5], which allows the models to focus on important parts within a context. Multi-head attention, for example, maps a sequence of queries Q ∈ R(cid:96)q×d of length (cid:96)q to a sequence of outputs O =
[O1, . . . , OH ] ∈ R(cid:96)q×d of the same size by attending over given (cid:96)k key-value pairs K ∈ R(cid:96)k×d,
V ∈ R(cid:96)k×d:
Oh = Attention(Qh, Kh, Vh) = Softmax (cid:19) (cid:18) QhKT h√ d
Vh, (4) where Qh = QWQ responding to head h ∈ [1, H] with learning parameters WQ
Q = K = V, we refer to such an attention mechanism as self-attention. h , Kh = KWK h , Vh = VWV h , WK h are projected queries, keys, and values cor-In case h , respectively. h , WV
Given fully observed sequences of inputs, the mapping can be computed efﬁciently without any imposed sequential order often seen in recurrent neural networks [19, 42]. More importantly, the direct connections between long-distance time steps are baked into the mechanism as information from previous time steps is easily accessible without being compressed into a ﬁxed representation, easing optimization and learning of long-term dependencies [5, 85]. 1For notational simplicity, we assume x0 = z1:0 = ∅ and p(z | x0) = p(z). 3
Without recurrence, Transformer [85] encodes information about each time step t with pred eﬁned sinusoidal positional embeddings Position(t) = [pt(1), . . . , pt(d)] ∈ Rd where the i-th embedding is given by pt(i) = sin(t · ci/d) for even i and pt(i) = cos(t · ci/d) for odd i and c is some large constant. Empirical results show that such positional embeddings are also important to our models. 3 Probabilistic Transformer
In this section, we ﬁrst present our single-layered model and subsequently its multi-layered ex-tension for a hierarchy of stochastic latent variables. As alluded earlier, our model consists of a generative model and an inference model that share information and parameters extensively. 3.1 Single-Layered Probabilistic Transformer
Generative Model. Given some contexts x1:C, we ﬁrst apply a linear projection and combine it with a positional embedding to obtain h1:C ∈ Rd, i.e. ht = LayerNorm(MLP(xt) + Position(t)), (5) where LayerNorm and MLP denote layer normalizations [4] and multi-layer perceptrons, respec-tively. While a traditional transformer model often dedicates an entire encoder for the same purpose
[55, 72], we ﬁnd such a simple mapping works sufﬁciently well in conjunction with the context-attention module of the corresponding decoder.
As implied in Equation (2), our latent dynamics decomposes auto-regressively. At each time step, we parametri ze the distribution pθ(zt | z1:t−1, x1:C) by a Gaussian with parameters resulting from two sequential steps of attention: a self-attention over the previously inferred states z1:t−1 and another attention over the projected contexts h1:C. These two operations mirror those found in the decoder of Transformer [85], with the stochastic latent variables replacing its decoder inputs.
Unfortunately, using stochastic samples of zt as attention queries is problematic, as purely stochastic transitions make it difﬁcult for the model to reliably retain information across multiple time steps
[17, 30, 39]. We therefore encapsulate the latent variables in hidden representations wt that also has a deterministic component. Combined with the attention steps, such representations help model long-range temporal dependencies while accounting for the stochasticity of future observations.
Starting with a learnable, context-agnostic representation w0, we recursively update wt using a stochastic sample from pθ(zt | z1:t−1, x1:C) and th e positional embedding for the current time step t. The generating process for the time step t can be summarized by the following pseudocode:
¯wt = LayerNorm(wt−1 + Attention(wt−1, w1:t−1, w1:t−1))
ˆwt = LayerNorm( ¯wt + Attention( ¯wt, h1:C, h1:C)) zt = Sample(N (zt; MLP( ˆwt), Softplus(MLP( ˆwt)))) wt = LayerNorm( ˆwt + MLP(zt) + Position(t)), (6) (7) (8) (9) where Sample and Softplus are the Gaussian sampling and approximating rectiﬁer operators.
Each stochastic sample of w1:T is then mapped to a sequence of x1:T via a multi-layer perceptron.
We emphasize that our generation procedure in the latent space is more efﬁcient than others in the observation space, which requires encoding and decoding high-dimensional inputs repeatedly.
Inference Model. We parametrize the approximate posterior qφ(zt | z1:t−1, x1:T ) at time step t in a simi lar fashion to the prior pθ(zt | z1:t−1, x1:C). Indeed, these parametrizations share most parameters and are done simultaneously in the same recursive loop, following the exact same steps in Equation (6) and Equation (7) (see Figure 1). We note that similar sharing techniques between the generative and inference processes have emerged as a common theme among recent successful
VAE models [17, 62, 83].
While the prior only has access to the conditioning observations x1:C, the approximate posterior should take into account all observations during training, including the targets xC+1:T . Due to the inherent unidirectional aspect of RNNs, previous work that uses RNNs to parametrize the approx-imate posterior often disregards such a property [22, 30, 51] and often resorts to a ﬁltering routine 4
p(zt | z1:t−1, x1:t). In contrast, our inference procedure resembles more of the smoothing process of LDSs, factoring in both past and future observations via another application of self-attention: kt = Attention(h1:T , h1:T , h1:T )) zt = Sample(N (zt; MLP([ ˆwt, kt]), Softplus(MLP([ ˆwt, kt]))). (10) (11)
Here, we replace Equation (8) in the generative model with Equation (11), where the hidden repre-sentation kt summarizing all information relevant to the current tim estep t has been concatenate to the latent-and-context-aware representation ˆwt preceding the Gaussian parametrization.
The generative model and the inference model are trained end-to-end with a single stochastic varia-tional inference objective stated in Equation (3). Such a variational bound includes the reconstruc-tion loss for x1:C and the KL term for z1:C. Alternatively, we can exclude these terms from the objective, which is equivalent to starting the inference process from t = C + 1 instead of t = 1.
Our models incur a time complexity of O(T 2d) and a memory cost of O(T 2d), where T is the total sequence length and d is the dimensionality of the latent space. The recursive latent dynamics also does not allow use the take full advantange of parallelizable attentions. However, we ﬁnd that our models are still efﬁcient in practice, especially for reasonably small values of T . 3.2 Multi-Layered Extension for Probabilistic Transformer
Inspired by recent work on hierarchical VAEs for non-sequential inputs [17, 80, 83, 101], we ex-tend our proposed model to include several layers of latent variables, aiming to further increase its
ﬂexibility for modelling sequential data.
We represent each time step t with a Ma rkov chain of L latent variables z(1:L)
, . . . , z(L)
) for simplicity (see Figure 1). The generative and inference model also decompose auto-regressively across different time steps and may exhibit non-Markovian dynamics:
= (z(1) t t t (cid:16) pθ x1:T , z(1:L) 1:T | x1:C (cid:32) T (cid:89) (cid:17)
= (cid:16) pθ xt | z(L) t (cid:33) (cid:32) L (cid:89) (cid:17)
T (cid:89) (cid:16) z((cid:96)) t pθ (cid:16) qφ z(1:L) 1:T | x1:T (cid:17)
= t=1
L (cid:89)
T (cid:89) (cid:96)=1 t=1 (cid:16) z((cid:96)) t qφ (cid:96)=1 t=1
| z((cid:96)) 1:t−1, z((cid:96)−1) 1:T , x1:T (cid:17)
.
| z((cid:96)) 1:t−1, z((cid:96)−1) 1:T , x1:C (cid:33) (cid:17) (12) (13)
Intuitively, we generate samples x1:T conditioning on x1:C by following the latent dynamics from the bottom up and using the generative process described earlier within each layer. Analogously, inference proceeds in the same order, resulting in a variational bound similar to Equation (3): log pθ(x1:T | x1:C) ≥
−
T (cid:88) t=1
L (cid:88) (cid:96)=1 (cid:104)
Eq log pθ(x(L) t (cid:105)
| zt)
KL(qφ(z((cid:96)) t
| z((cid:96)) 1:t−1, z((cid:96)) 1:T , x1:T ) (cid:107) pθ(z((cid:96)) t (14) (15)
| z((cid:96)) 1:t−1, z((cid:96)) 1:T , x1:C)).
| z((cid:96))
As before, we parametrize the prior pθ(z((cid:96)) 1:T , x1:C) using self-attention over the inferred t latent variables from previous time steps w((cid:96)) t−1 on the same layer and another attention over contexts h1:C. In this case, however, we include an additional self-attention over all latent variables from the layer immediately below it (see Equation (16)): 1:t−1, z((cid:96))
˜w((cid:96)) t = LayerNorm(w((cid:96))
¯w((cid:96)) t = LayerNorm( ˜w((cid:96)) t = LayerNorm( ¯w((cid:96))
ˆw((cid:96)) z((cid:96)) t = Sample(N (z((cid:96)) t = LayerNorm( ˆw((cid:96)) w((cid:96)) t−1, w((cid:96)−1)
, w((cid:96)) t−1 + Attention(w((cid:96)) t + Attention( ˜w((cid:96)) t + Attention( ¯w((cid:96))
; MLP( ˆw((cid:96)) t + MLP(z((cid:96)) 1:t−1, w((cid:96))
, h1:C, h1:C)) t ), Softplus(MLP( ˆw((cid:96)) t ) + Position(t)), 1:T t t t
, w((cid:96)−1) 1:T 1:t−1)) t ))))
)) (16) (17) (18) (19) (20) 5
Stacking multiple layers of latent variables increases model expressiveness, but it also result in a linear increase in running time and the number of parameters. The time complexity for the L-layers transformer is O(LT 2d), while the space complexity remains D(T 2d) due to the Markovian structure of the chain z(1:L) at each time step t. In our experiments, we restrict the number of layers of our hierachical models to two or three. t 4