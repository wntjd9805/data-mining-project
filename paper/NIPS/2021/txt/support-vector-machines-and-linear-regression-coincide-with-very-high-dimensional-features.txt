Abstract
The support vector machine (SVM) and minimum Euclidean norm least squares regression are two fundamentally different approaches to fitting linear models, but they have recently been connected in models for very high-dimensional data through a phenomenon of support vector proliferation, where every training ex-ample used to fit an SVM becomes a support vector. In this paper, we explore the generality of this phenomenon and make the following contributions. First, we prove a super-linear lower bound on the dimension (in terms of sample size) re-quired for support vector proliferation in independent feature models, matching the upper bounds from previous works. We further identify a sharp phase transition in
Gaussian feature models, bound the width of this transition, and give experimental support for its universality. Finally, we hypothesize that this phase transition occurs only in much higher-dimensional settings in the ℓ1 variant of the SVM, and we present a new geometric characterization of the problem that may elucidate this phenomenon for the general ℓp case. 1

Introduction
The support vector machine (SVM) and ordinary least squares (OLS) are well-weathered approaches to fitting linear models, but they are associated with different learning tasks: classification and regression. In this paper, we study the case in which the models return exactly the same hypothesis for sufficiently high-dimensional data.
The hard-margin SVM is a linear classification model that finds the separating hyperplane that maximizes the minimum margin of error for every training sample.
If the training data (x1, y1), . . . , (xn, yn) ∈ Rd × {±1} are linearly separable, then the resulting linear classifier is x (cid:55)→ sign (xTwSVM), where wSVM is the solution to the following optimization problem: wSVM = arg min w∈Rd
∥w∥2 such that yiwTxi ≥ 1, ∀i ∈ [n]. (1)
An example xi is a support vector if the corresponding constraint is satisfied with equality, and the optimal solution wSVM is a linear combination of these support vectors.
Ordinary least squares regression finds the linear function that best fits the training data (x1, y1), . . . , (xn, yn) ∈ Rd × R according to the sum of squared errors. When the solution is not unique, it is natural to take the solution of minimum Euclidean norm; this is the convention we adopt. Taking X := [x1| . . . |xn]T ∈ Rn×d and y := (y1, . . . , yn), the solution is the hypothesis x (cid:55)→ wT
OLSx where wOLS is the solution to the following: wOLS = arg minw∈Rd ∥w∥2 such that
XTXw = XTy. In many high-dimensional settings (e.g., where X has full row rank), the solution may in fact interpolate the training data, i.e.,
∥w∥2 such that wTxi = yi, ∀i ∈ [n]. wOLS = arg min (2) w∈Rd 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Although the optimization problems in (1) and (2) are very different, they have been observed to coincide in very high-dimensional regimes. The study of this support vector proliferation (SVP) phenomenon—in which every training example is a support vector—was recently initiated by
Muthukumar et al. [35] and Hsu et al. [23]. Roughly speaking, they show that SVP occurs when d = Ω(n log n) for a broad class of sample distributions, and that SVP does not occur when d = O(n) in an idealized isotropic Gaussian case.
SVP is a phenomenon that connects linear classification and linear regression, topics that have received renewed attention due to the break-down of classical analyses of these methods in high-dimensions.
For instance, some analyses of SVM that are based on the number of support vectors become vacuous when this number becomes large [e.g., 19, 20, 44]. Similarly, overparameterized linear regression is typically only studied in noisy settings with explicit regularization. It was not until recently that SVM and OLS have been meaningfully analyzed in these regimes (see Section 1.2), and the connection between the two approaches via SVP has played an important analytical role [9, 35, 46].
In this work, we further examine support vector proliferation with the goal of broadly understanding when and why SVMs and OLS coincide. We pose and study the following questions: 1. How general is the SVP phenomena? What relationship between d and n determines if the solutions to (1) and (2) coincide?
We close the log n gap from the prior work of Hsu et al. [23] by showing that d ≳ n log n is necessary for SVP to occur under a model of independent subgaussian features, even with constant probability. Our lower-bounds hold for a broad class of distributions over xi, and they match the upper-bounds from [23]. This demonstrates that SVP is extremely unlikely to occur in the much-studied d = Θ(n) setting. 2. Is there a sharp threshold separating the occurrence and non-occurrence of this phenomenon? Is this threshold universal across all “reasonable” distributions over each xi?
We hypothesize that a sharp phase transition occurs at d = 2n log n. We rigorously prove this hypothesis for isotropic Gaussian features and quantitatively bound the width of the transition.
We experimentally observe the same transition for a wide range of other distributions. 3. Is support vector proliferation specific to the ℓ2 SVM problem? If (1) and (2) are generalized to instead minimize ℓp norms, does this still occur at the same rate?
We re-frame this question with a geometric characterization of the dual of the SVM optimization problem for ℓp norms. We conjecture that a similar phase transition occurs for ℓ1, but also that it requires much larger dimension d; this is supported by preliminary experiments. 1.1 Outline of our results
Section 2 introduces the SVM and OLS approaches in full generality, our λ-anisotropic subgaussian data model, and prior results about SVP. Several equivalent characterizations of SVP are established (Proposition 1) for use in subsequent sections.
Section 3 characterizes when SVP does not occur for a broad range of distributions (Theorem 3). Our lower-bound on the dimension required for SVP matches the upper-bounds from [23] in the isotropic
Gaussian setting, resolving the open question from that work, and also gives new lower-bounds for anisotropic cases. The proof works by tightly controlling the spectrum of the Gram matrix and establishing anti-concentration via the Berry-Esseen Theorem.
Section 4 establishes a sharp threshold of d = 2nlog n for SVP in the case of isotropic Gaussian samples, and also characterize the width of the phase transition (Theorem 4).
Section 5 provides empirical evidence that the sharp threshold observed in Section 4 holds for a wide range of random variables. Rigorous statistical methodology inspired by Donoho and Tanner
[16] is used to test our “universality hypothesis” that the probability of SVP does not depend on the underlying sample distribution as d and n become large.
Section 6 asks the questions about SVP from the preceding sections in the context of ℓ1-SVM and minimum ℓ1-norm interpolation. Specifically, the SVP threshold for ℓ1 is conjectured to occur for d = ω(n log n). Evidence for this conjecture is provided in a simulation study and in geometric arguments about random linear programs. 2
1.2