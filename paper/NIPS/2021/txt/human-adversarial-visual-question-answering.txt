Abstract
Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved.
In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to ﬁnd a question where the model’s predicted answer is incorrect. We ﬁnd that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions.
We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the ﬁeld and advance the state of the art. 1

Introduction
Visual question answering (VQA) is widely recognized as an important evaluation task for vision and language research. Besides direct applications such as helping the visually impaired or multimodal content understanding on the web, it offers a mechanism for probing machine understanding of images via natural language queries. Making progress on VQA requires bringing together different subﬁelds in AI – combining advances from natural language processing (NLP) and computer vision together with those in multimodal fusion – making it an exciting task in AI research.
Over the years, the performance of VQA models has started to plateau on the popular VQA v2 dataset [20] – approaching inter-human agreement – as evidenced by Fig. 1. This raises important questions for the ﬁeld: To what extent have we solved the problem? If we haven’t, what are we still missing? How good are we really?
An intriguing method for investigating these questions is dynamic data collection [29], where human annotators and state-of-the-art models are put “in the loop” together to collect data adversarially.
Annotators are tasked with and rewarded for ﬁnding model-fooling examples, which are then veriﬁed by other humans. The easier it is to ﬁnd such examples, the worse the model’s performance can be said to be. The collected data can be used to “stress test” current VQA models and serve as the next iteration of the VQA benchmark helping drive further progress.
The commonly used VQA dataset [20] was collected by instructing annotators to “ask a question about this scene that [a] smart robot probably can not answer” [4]. One way of thinking about our proposed human-adversarial data collection is that it explicitly ensures that the questions can not be answered by today’s “smartest” models.
∗Equal contribution. Correspondence to advqa@fb.com.
†Work done as an intern at Facebook AI Research. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Contrastive examples from VQA and AdVQA. Predictions are given for the VisualBERT,
ViLBERT and UniT models, respectively. Models can answer VQA questions accurately, but consistently fail on AdVQA questions.
Image
VQA
AdVQA
Q: How many cats are in the image?
A: 2
Model: 2, 2, 2
Q: What brand is the tv?
A: lg
Model: sony, samsung, samsung
Q: Does the cat look happy?
A: no
Model: no, no, no
Q: How many cartoon drawings are present on the cat’s tie?
A: 4
Model: 1, 1, 2
Q: What kind of ﬂoor is the man sitting on?
A: wood
Model: wood, wood, wood
Q: Did someone else take this picture?
A: no
Model: yes, yes, yes
This work is, to the best of our knowledge, the ﬁrst to apply this human-adversarial approach to an image and language multimodal problem. We introduce Adversarial VQA (AdVQA), a large evaluation dataset of 46,807 examples in total, all of which fooled the VQA 2020 challenge winner, the MoViE+MCAN [45] model.
We evaluate a wide range of existing VQA models on AdVQA and ﬁnd that their performance is signiﬁcantly lower than on the commonly used VQA v2 dataset [20] (see Table 1). Furthermore, we conduct an extensive analysis of AdVQA characteristics, and contrast with the VQA v2 dataset.
We hope that this new benchmark can help advance the state of the art by shedding important light on the current model shortcomings. Our ﬁndings suggest that there is still considerable room for continued improvement, with much more work remaining to be done. 2