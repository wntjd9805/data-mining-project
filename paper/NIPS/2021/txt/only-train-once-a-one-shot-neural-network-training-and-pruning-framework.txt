Abstract
Structured pruning is a commonly used technique in deploying deep neural net-works (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-speciﬁed, and require an extra ﬁne-tuning pro-cedure. To overcome these limitations, we propose a framework that compresses
DNNs into slimmer architectures with competitive performances and signiﬁcant
FLOPs reductions by Only-Train-Once (OTO). OTO contains two keys: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem and propose a novel opti-mization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity explo-ration and maintains comparable convergence. To demonstrate the effectiveness of
OTO, we train and compress full models simultaneously from scratch without ﬁne-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/only_train_once. 1

Introduction
Deep neural networks (DNNs) have been shown to be effective in various real applications (51; 28).
It is widely acknowledged that large-scale DNN models not only learn faster but also outperform their slimmer counterparts. However, such heavy models pose a great challenge to the deployment stage due to their resource-consuming nature. In addressing this issue, many model compression
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Overview of OTO. Without loss of generality, we illustrate OTO on a model with only vanilla convolutional layers, and for simplicity we only show Layeri with m 3D ﬁlters and their biases. The key to its success is twofold: (i) identify and partition the parameters of the model into zero-invariant groups (ZIGs); and (ii) solve the structured-sparsity regularization problem using
HSPG. Finally, we obtain the compressed model by directly pruning the zero groups, i.e., ZIGm. techniques (5; 11) are proposed in the past decade that aim at compressing those large and complex models into slimmer and simpler ones while suffering negligible loss in performance.
Pruning methods as one of the main categories of model compression, focus on identifying and pruning redundant structures via various mechanisms to achieve a slimmer architecture, and thus improve the interpretability of a DNN model (26; 11; 65). For example, (32; 33) adopt ﬁne-grained pruning via (cid:96)1 or (cid:96)2 regularization, which prune the small-weight connections based on some hard threshold. (36; 57; 60) measure the importance of ﬁlters to accelerate the networks by removing insigniﬁcant feature maps. (37; 7) utilize reinforcement learning agent to predict compression action.
Nevertheless, many of the existing pruning methods (i) often rely on criteria based on heuristics or empirical cues, e.g., magnitude of a connection weight and importance score of a ﬁlter, to identify redundant parameters, which may cause divergence during optimization; (ii) thus require complex multi-stage pipelines that involve either a retraining or ﬁne-tuning procedure to regain the accuracy during constructing a slimmer model, which is time-consuming; and (iii) are speciﬁc to certain architectures or applications, and are consequently less applicable to various downstream scenarios.
Recently, there have been a few efforts (14; 58; 8) to directly train the network with sparsity inducing regularizers, which provide generality and convergence guarantee. However, these approaches focus on either merely the individual sparsity of the parameters or the group sparsity of the ﬁlters, and thus cannot directly remove those zero components (still require subsequent ﬁne-tuning) since the zeros are entangled with other commonly used components, e.g., bias, batch normalization or skip connection. Furthermore, the optimization algorithms used in (14; 58) lack sufﬁcient capability to explore (group) sparsity in DNNs effectively and require a post-processing step to yield exact zeros.
In this paper, we overcome the above limitations of existing pruning methods by proposing a one-shot neural network pruning framework, with which we are able to train a full heavy model from scratch only once, and obtain a slim architecture without ﬁne-tuning while maintain high performance. As shown in Figure 1, the key to its success is twofold: (i) we identify and partition the parameters of
DNNs into zero-invariant groups (ZIGs), enabling us to prune redundant structures according to zero groups without affecting the output of the network; and (ii) to promote zero groups, we formulate the pruning task as a structured-sparsity optimization problem and propose a novel optimization method, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on sparsity exploration and maintains comparable convergence. We highlight that both zero-invariant group partition and the novel optimization algorithm in promoting zero group lead to achieve one-shot neural network training and pruning regardless of its architecture.
Our main contributions are summarized as follows.
• One-Shot Training and Pruning. We propose OTO, a training and pruning framework that compresses a full neural network with competitive performance by Only-Train-Once, thereby one-shot. OTO dramatically simpliﬁes the complex multi-stage training pipelines of the existing pruning approaches, ﬁts various architectures and applications, and hence is generic and efﬁcient.
• Zero-Invariant Group. We deﬁne zero-invariant groups for neural networks. If a network is partitioned into ZIGs, it allows us to prune the zero groups without affecting the output, which results in one-shot pruning. Such property is applicable to various popular structures from plain fully connected layers to sophisticated ones such as residual blocks and multi-head attention.
• Novel Structured-Sparsity Optimization Algorithm. We propose Half-Space Stochastic Pro-jected Gradient (HSPG), a method that solves structured-sparsity inducing regularization problem.
We show and analyze the superiority of HSPG in promoting zero groups of networks than the 2
standard proximal methods and the competitive objective convergence in practice. The fact that
ZIG and HSPG are designed agnostic to networks makes OTO generic to various applications.
• Experimental Results. We train and compress full models simultaneously from scratch without
ﬁne-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on compression benchmark VGG for CIFAR10, ResNet50 for CIFAR10/ImageNet, Bert for SQuAD. 2