Abstract
In this paper we study the training dynamics for gradient ﬂow on over-parametrized tensor decomposition problems. Empirically, such training process often ﬁrst ﬁts larger components and then discovers smaller components, which is similar to a tensor deﬂation process that is commonly used in tensor decomposition algorithms.
We prove that for orthogonally decomposable tensor, a slightly modiﬁed version of gradient ﬂow would follow a tensor deﬂation process and recover all the tensor components. Our proof suggests that for orthogonal tensors, gradient ﬂow dynamics works similarly as greedy low-rank learning in the matrix setting, which is a ﬁrst step towards understanding the implicit regularization effect of over-parametrized models for low-rank tensors. 1

Introduction
Recently, over-parametrization has been recognized as a key feature of neural network optimization.
A line of works known as the Neural Tangent Kernel (NTK) showed that it is possible to achieve zero training loss when the network is sufﬁciently over-parametrized (Jacot et al., 2018; Du et al., 2018; Allen-Zhu et al., 2018b). However, the theory of NTK implies a particular dynamics called lazy training where the neurons do not move much (Chizat et al., 2019), which is not natural in many settings and can lead to worse generalization performance (Arora et al., 2019b). Many works explored other regimes of over-parametrization (Chizat and Bach, 2018; Mei et al., 2018) and analyzed dynamics beyond lazy training (Allen-Zhu et al., 2018a; Li et al., 2020a; Wang et al., 2020).
Over-parametrization does not only help neural network models. In this work, we focus on a closely related problem of tensor (CP) decomposition. In this problem, we are given a tensor of the form
T ∗ = r (cid:88) i=1 ai(U [:, i])⊗4, where ai ≥ 0 and U [:, i] is the i-th column of U ∈ Rd×r. The goal is to ﬁt T ∗ using a tensor T of a similar form: m (cid:88)
T = (W [:, i])⊗4 (cid:107)W [:, i](cid:107)2 .
∗Alphabetical order. i=1 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
i∈[5] aie⊗4
Figure 1: The training trajectory of gradient ﬂow on orthogonal tensor decompositions. We chose
T ∗ = (cid:80) i with ei ∈ R10 and ai/ai+1 = 1.2. Our model T has 50 components and each component is randomly initialized with small norm 10−15. We ran the experiments from 5 different initialization and plotted the results separately. The left ﬁgure shows the loss 1
F and the right ﬁgure shows the residual on each ei direction that is deﬁned as (T ∗ − T )(e⊗4 2 (cid:107)T − T ∗(cid:107)2
). i
Here W is a d × m matrix whose columns are components for tensor T . The model is over-parametrized when the number of components m is larger than r. The choice of normalization factor of 1/(cid:107)W [:, i](cid:107)2 is made to accelerate gradient ﬂow (similar to Li et al. (2020a); Wang et al. (2020)).
Suppose we run gradient ﬂow on the standard objective 1 to the differential equation:
F , that is, we evolve W according (cid:18) 1 2 can we expect T to ﬁt T ∗ with good accuracy? Empirical results (see Figure 1) show that this is true for orthogonal tensor T ∗2 as long as m is large enough. Further, the training dynamics exhibits a behavior that is similar to a tensor deﬂation process: it ﬁnds the ground truth components one-by-one from larger component to smaller component (if multiple ground truth components have similar norm they might be found simultaneously). 2 (cid:107)T − T ∗(cid:107)2 (cid:107)T − T ∗(cid:107)2
F dW dt
= −∇ (cid:19)
,
In this paper we show that with a slight modiﬁcation, gradient ﬂow on over-parametrized tensor decomposition is guaranteed to follow this tensor deﬂation process, and can ﬁt any orthogonal tensor to desired accuracy3(see Section 4 for the algorithm and Theorem 1 for the main theorem). This shows that for orthogonal tensors, the trajectory of modiﬁed gradient-ﬂow is similar to a greedy low-rank process that was used to analyze the implicit bias of low-rank matrix factorization (Li et al., 2020b).
We emphasize that our goal is not to propose another tensor decomposition algorithm. Instead, we hope our results can serve as a ﬁrst step in understanding the implicit bias of over-parameterized gradient descent for low-rank tensor problems. 1.1 Our approach and technique
To understand the tensor deﬂation process shown in Figure 1, intuitively we can think about the discovery and ﬁtting of a ground truth component in two phases. Consider the beginning of the gradient ﬂow as an example. Initially all the components in T are small, which makes T negligible compared to T ∗. In this case each component w in W will evolve according to a simpler dynamics that is similar to tensor power method, where one updates w to T ∗(w⊗3, I)/ (cid:13) (cid:13) (see
Section 3 for details). (cid:13)T ∗(w⊗3, I)(cid:13)
For orthogonal tensors, it’s known that tensor power method with random initializations would be able to discover the largest ground truth components (see Anandkumar et al. (2014)). Once the largest ground truth component has been discovered, the corresponding component (or multiple components) w will quickly grow in norm, which eventually ﬁts the ground truth component. The ﬂat regions in the trajectory in Figure 1 correspond to the period of time where the components w’s are small and 2We say T ∗ is an orthogonal tensor if the ground truth components U [:, i]’s are orthonormal. 3Due to some technical challenges, we actually require the target accuracy to be at least exp(−o(d/ log d)).
This is only a very mild restriction since the dependence is exponential in d, and in practice, d is usually large and this lower bound can easily drop below the numerical precision. 2
T − T ∗ remains stable, while the decreasing regions correspond to the period of time where a ground truth component is being ﬁtted.
However, there are many challenges in analyzing this process. The main problem is that the gradient
ﬂow would introduce a lot of dependencies throughout the trajectory, making it harder to analyze the ﬁtting of later ground truth components, especially ones that are much smaller. We modify the algorithm to include a reinitialization step per epoch, which alleviates the dependency issue. Even after the modiﬁcation we still need a few more techniques:
Local stability One major problem in analyzing the dynamics in a later stage is that the components used to ﬁt the previous ground truth components are still moving according to their gradients, therefore it might be possible for these components to move away. To address this problem, we add a small regularizer to the objective, and give a new local stability analysis that bounds the distance to the
ﬁtted ground truth component both individually and on average. The idea of bounding the distance on average is important as just assuming each component w is close enough to the ﬁtted ground truth component is not sufﬁcient to prove that w cannot move far. While similar ideas were considered in
Chizat (2021), the setting of tensor decomposition is different.
Norm/Correlation relation A key step in our analysis establishes a relationship between norm and correlation: we show if a component w crosses a certain norm threshold, then it must have a very large correlation with one of the ground truth components. This offers an initial condition for local stability and makes sure the residual T ∗ − T is almost close to an orthogonal tensor. Establishing this relation is difﬁcult as unlike the high level intuition, we cannot guarantee T ∗ − T remains unchanged even within a single epoch: it is possible that one ground truth component is already ﬁtted while no large component is near another ground truth component of same size. In previous work, Li et al. (2020a) deals with a similar problem for neural networks using gradient truncation that prevents components from growing in the ﬁrst phase (and as a result has super-exponential dependency on the ratio between largest and smallest ai). We give a new technique to control the inﬂuence of ground truth components that are ﬁtted within this epoch, so we do not need the gradient truncation and can characterize the deﬂation process. 1.2