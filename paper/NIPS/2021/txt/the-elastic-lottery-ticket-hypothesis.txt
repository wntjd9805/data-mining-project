Abstract
Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse train-able subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we “transform” the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efﬁcient “once-for-all” winning ticket ﬁnding, but also theoretically ap-pealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strate-gies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery
Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter’s win-ning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH. 1

Introduction
Lottery Ticket Hypothesis (LTH) [13] suggests the existence of sparse subnetworks in over-parameterized neural networks at their random initialization, early training stage, or pre-trained initialization [35, 44, 5, 3, 2]. Such subnetworks, usually called winning tickets, contain much fewer non-zero parameters compared with the original dense networks, but can achieve similar or even better performance when trained in isolation. The discovery undermines the necessity of over-parameterized initialization for successful training and good generalization of neural networks [46, 31]. That implies the new possibility to train a highly compact subnetwork instead of a prohibitively large one without compromising performance, potentially drastically reducing the training cost.
However, the current success of LTH essentially depends on Iterative Magnitude-based Pruning (IMP), which requires repeated cycles of training networks from scratch, pruning and resetting the remaining parameters. IMP makes it extremely expensive and sometimes unstable to ﬁnd winning
∗ Work was done when the author interned at Microsoft. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Validating Elastic Lottery Ticket Hypothesis (E-LTH) between two pairs of networks (ResNet-32 and ResNet-56, VGG-13 and VGG-19) trained on the CIFAR-10 dataset. We transform the winning tickets in the source models at different levels of sparsity using the proposed Elastic
Ticket Transformations (ETTs), and compare with other pruning methods (please refer to Section 4 for those methods’ details), including the state-of-the-art pruning-at-initialization methods (e.g., SNIP and GraSP). Substantial accuracy gaps can be observed between ETTs and other pruning methods, corroborating that ETTs provide high-quality tickets on the target models that are comparable with the winning tickets found by the expensive IMP. All results are based on three independent runs. tickets at scale, with large models and large datasets [15]. To alleviate this drawback, many efforts have been devoted to ﬁnding more efﬁcient alternatives to IMP that can identify sparse trainable subnetworks at random initialization, with little-to-no training [27, 40, 38, 16]. Unfortunately, they all see some performance gap when compared to the winning tickets found by IMP, with often different structural patterns [16]. Hence, IMP remains to be the de facto scheme for lottery ticket ﬁnding.
Prior work [33] found that a winning ticket of one dense network can generalize across datasets and optimizers, beyond the original training setting where it was identiﬁed. Their work provided a new perspective of reducing IMP cost – to only ﬁnd one generic, dataset-independent winning ticket for each backbone model, then transferring and re-training it on various datasets and downstream tasks.
Compared to this relevant prior work which studies the transferablity of a winning ticket in the same network architecture, in this paper, we ask an even bolder question:
Can we transfer the winning ticket found for one network to other different network architectures?
This question not only has strong practical relevance but also arises theoretical curiosity. On the practicality side, if its answer is afﬁrmative, then we will perform only expensive IMP for one network and then automatically derive winning tickets for others. It would point to a tantalizing possibility of once-for-all lottery ticket ﬁnding, and the extraordinary cost of IMP on a “source architecture” is amortized by transferring to a range of “target architectures”. A promising application is to ﬁrst
ﬁnd winning tickets by IMP on a small source architecture and then transfer it to a much bigger target architecture, leading to drastic savings compared to directly performing IMP on the latter.
Another use case is to compress a larger winning ticket directly to smaller ones, in order to ﬁt in the resource budgets on different platforms. On the methodology side, this new form of transferability would undoubtedly shed new lights on the possible mechanisms underlying the LTH phenomena by providing another perspective for understanding lottery tickets through their transferablity, and identify shared and transferable patterns that make sparse networks trainable [17, 38, 14]. Moreover, many deep networks have regular building blocks and repetitive structures, lending to various model-based interpretations such as dynamical systems or unrolled estimation [19, 42, 20, 1]. Our explored empirical methods seem to remind of those explanations too. 1.1 Our Contributions
We take the ﬁrst step to explore how to transfer winning tickets across different architectures. The goal itself would be too daunting if no constraint is imposed on the architecture differences. Just like 2
general transfer learning, it is natural to hypothesize that two architectures must share some similarity so that their winning tickets may transfer: we stick to this assumption in this preliminary study.
We focus our initial agenda on network architectures from the same design family (e.g., a series of models via repeating or expanding certain building blocks) but of different depths. For a winning ticket found on one network, we propose various strategies to “stretch” it into a winner ticket for a deeper network of the same family, or “squeeze” it for a shallower network. We then compare their performance with tickets directly found on those deeper or shallower networks. We conduct extensive experiments on CIFAR-10 with models from the ResNet and VGG families, and further extend to
ImageNet. Our results seem to suggest an afﬁrmative answer to our question in this speciﬁc setting.
We formalize our observations by articulating the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same as the latter’s winning ticket directly found by IMP. Those stretched or squeezed winning tickets largely outperform the sparse subnetworks found by pruning-at-initialization approaches [27], and show competitive efﬁciency to state-of-the-art dynamic sparse training [11]. We also provide intuitive explanations for the preliminary success.
Lastly, we stress that our method has a pilot-study nature, and is not assumption-free. The assumption that different architectures come from one design “family” might look restrictive. However, we point out many state-of-the-art deep models are designed in “families”, such as ResNets [22],
MobileNets [23], EfﬁcientNets [37], and Transformers [39]. Hence, we see practicality in the current results, and we are also ready to extend them to more general notions - which we discuss in Section 5. 2