Abstract
Model-based reinforcement learning has attracted wide attention due to its su-perior sample efﬁciency. Despite its impressive success so far, it is still unclear how to appropriately schedule the important hyperparameters to achieve adequate performance, such as the real data ratio for policy optimization in Dyna-style model-based algorithms. In this paper, we ﬁrst theoretically analyze the role of real data in policy training, which suggests that gradually increasing the ratio of real data yields better performance. Inspired by the analysis, we propose a framework named AutoMBPO to automatically schedule the real data ratio as well as other hyperparameters in training model-based policy optimization (MBPO) algorithm, a representative running case of model-based methods. On several continuous control tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can signiﬁcantly surpass the original one, and the real data ratio schedule found by
AutoMBPO shows consistency with our theoretical analysis. 1

Introduction
Deep model-free reinforcement learning (MFRL) has achieved great successes in complex decision-making problems such as Go [26] and robotic control [10], to name a few. Although MFRL can achieve high asymptotic performance, a tremendous number of samples collected in interactions with the environment are required. In contrast, model-based reinforcement learning (MBRL), which alternately learns a model of the environment and derives an agent with the help of current model estimation, is considered to be more sample efﬁcient than MFRL [27].
MBRL methods generally fall into different categories according to the speciﬁc usage of the learned model [25, 33]. Among them, Dyna-style algorithms [28] adopt some off-the-shelf MFRL methods to train a policy with both real data from environment and imaginary data generated by the model.
Since Dyna-style algorithms can seamlessly take advantage of innovations in MFRL literature and have recently shown impressive performance [12, 30], this paper mainly focuses on Dyna-style algorithms. Although some theoretical analysis [17, 12, 22] and thorough empirical evaluation [30] have been conducted in the previous literature, it remains unclear how to appropriately schedule the hyperparameters to achieve optimum performance when training a Dyna-style MBRL algorithm. In practice, many important hyperparameters may primarily affect performance.
Firstly, since Dyna-style MBRL algorithms consist of model learning and policy optimization, how to balance the alternate optimization of these two parts is a crucial problem [22]. Intuitively, insufﬁcient model training may lead to inaccurate estimation while excessive model training will cause overﬁtting, and a small amount of policy training may not utilize the model adequately while exhaustive policy training will exploit the model’s deﬁciencies. Moreover, the rollout length of the imaginary trajectory
∗equal contribution. †Weinan Zhang is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is also critical [12] since too short rollout length fails to sufﬁciently leverage the model to plan forward, while too long rollout length may bring disastrous compounding error [1]. Janner et al. [12] manually design a schedule that linearly increases the rollout length across epochs. Finally, when using both real samples and imaginary samples to train the policy, how to control the ratio of the two datasets remains unclear. Janner et al. [12] ﬁx the ratio of real data as 5%, while Kalweit and
Boedecker [13] use the uncertainty to adaptively choose the ratio, which tends to use more imaginary samples initially and gradually use more real samples afterward. According to these existing works, the optimal hyperparameters in model-based methods may be dynamic during the whole training process, which further strengthens the burden of manually scheduling the hyperparameters.
Based on these considerations, in this work, we aim to investigate how to appropriately schedule these hyperparameters, i.e., real data ratio, model training frequency, policy training iteration, and rollout length, to achieve optimal performance of Dyna-style MBRL algorithms. Although real data ratio is an essential factor empirically [13, 12], it has not yet been studied thoroughly in theory. To bridge this gap, we ﬁrst derive a return discrepancy upper bound for model-based value iteration, which reveals that gradually increasing the ratio of real data yields a tighter bound than choosing a ﬁxed value. Inspired by the analysis, considering the complex interplay between real data ratio and other hyperparameters in practice, we develop AutoMBPO to automatically determine the joint schedule of the above hyperparameters in model-based policy optimization (MBPO), a representative running case of Dyna-style MBRL algorithms. Speciﬁcally, AutoMBPO introduces a parametric hyper-controller to sequentially choose the value of hyperparameters in the whole optimization process to maximize the performance of MBPO. We apply AutoMBPO to several continuous control tasks. On all these tasks, the MBPO instance trained with hyperparameters scheduled by AutoMBPO can signiﬁcantly surpass the one with original conﬁguration [12]. Furthermore, the hyperparameter schedule found by AutoMBPO is consistent with our theoretical analysis, which yields a better understanding of MBPO and provides insights for the design of other MBRL methods. 2