Abstract
Many modern machine learning tasks require models with high tail performance, i.e. high performance over the worst-off samples in the dataset. This problem has been widely studied in ﬁelds such as algorithmic fairness, class imbalance, and risk-sensitive decision making. A popular approach to maximize the model’s tail performance is to minimize the CVaR (Conditional Value at Risk) loss, which computes the average risk over the tails of the loss. However, for classiﬁcation tasks where models are evaluated by the 0/1 loss, we show that if the classiﬁers are deterministic, then the minimizer of the average 0/1 loss also minimizes the CVaR 0/1 loss, suggesting that CVaR loss minimization is not helpful without additional assumptions. We circumvent this negative result by minimizing the CVaR loss over randomized classiﬁers, for which the minimizers of the average 0/1 loss and the CVaR 0/1 loss are no longer the same, so minimizing the latter can lead to better tail performance. To learn such randomized classiﬁers, we propose the
Boosted CVaR Classiﬁcation framework which is motivated by a direct relationship between CVaR and a classical boosting algorithm called LPBoost. Based on this framework, we design an algorithm called α-AdaLPBoost. We empirically evaluate our proposed algorithm on four benchmark datasets and show that it achieves higher tail performance than deterministic model training methods. 1

Introduction
As machine learning continues to ﬁnd broader usage, there is an increasing understanding of the importance of tail performance of models, in addition to their average performance. For instance, in datasets with highly imbalanced classes, the tail performance is the accuracy over the minority classes which have much fewer samples than the others. In the ﬁeld of algorithmic fairness, where a dataset contains several demographic groups, the tail performance is the accuracy over certain underrepresented groups that normal machine learning models often neglect. In all these examples, it is crucial to design models with good tail performance that perform well across all parts/groups of the data domain, instead of just performing well on average.
Owing to its importance, a number of recent works have designed techniques to learn models with high tail performance [HSNL18, SKHL20, SRKL20]. Maximizing the tail performance is sometimes referred to as learning under subpopulation shift, in the sense that the testing distribution could consist of just a subpopulation of the training distribution. Most of the works on subpopulation shift fall into two categories. In the ﬁrst, also referred to as the domain-aware setting, the dataset is divided into several predeﬁned groups, and the goal is to maximize the worst-group performance, i.e. the minimum performance over all the groups. A number of methods, such as importance weighting
[Shi00] and Group DRO [SKHL20, SRKL20], have been proposed for domain-aware subpopulation shift. However, the domain-aware setting is not always applicable, either because the groups can be hard to deﬁne, or because the group labels are not available. Thus, in the second category of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
work on subpopulation shift, also referred to as the domain-oblivious setting, there are no pre-deﬁned groups, and the goal is to maximize the model’s performance over the worst-off samples in the dataset.
Most previous work on domain-oblivious subpopulation shift [HSNL18, DN18, HNSS18, LBC+20,
MHN21] measure the tail performance using the Distributionally Robust Optimization (DRO) loss, which is deﬁned as the model’s maximum loss over all distributions within a divergence ball around the training distribution. A popular instance is the α-CVaR loss, deﬁned as the model’s average loss over the worst α ∈ (0, 1) fraction of the samples incurring the highest losses in the dataset.
Naturally one might think of maximizing a model’s tail performance by directly minimizing the DRO loss or the α-CVaR loss, which has been used by many previous work [HSNL18, DN18, XDKR20].
However, [HNSS18] proved the negative result that for classiﬁcation tasks where models are evaluated by the zero-one loss, empirical risk minimization (ERM) achieves the lowest possible DRO loss given that the model is deterministic and the DRO loss is deﬁned by some f -divergence function. We extend their result to the α-CVaR loss which can be written as the limit of Rényi-divergence DRO losses (with a more direct and simpler proof due to the specialized case of CVaR). This is a very pessimistic result since it entails that there is no hope to get a better classiﬁer than ERM so long as models are evaluated by a DRO (or CVaR) loss. So some previous work [HNSS18, LBC+20, MHN21] proposed to avoid this issue by making extra assumptions on the testing distribution (speciﬁcally, that the testing subpopulation can be represented by a parametric model), and changing the evaluation metric correspondingly to something other than a f -divergence DRO loss.
In this work, we take a different approach and show that no extra assumption is needed provided that we use randomized models. While the case of general DRO is more complicated, the reason why ERM achieves the lowest possible CVaR zero-one loss in the deterministic case is very simple: there exists a linear relationship between the CVaR zero-one loss and the average zero-one loss, so the former is non-decreasing with the latter. For randomized models, however, such a monotonic relationship no longer exists. Note that for any single test sample, the zero-one loss of the deterministic model is either 0 or 1, while the expected zero-one loss of the randomized model is a real number in [0, 1], so that the randomized model can typically achieve lower α-CVaR loss than the deterministic model. In fact, we can prove that if the two models have the same average accuracy, then the α-CVaR zero-one loss of the randomized model is consistently lower than the deterministic one.
Motivated by the above analysis, we propose the framework of Boosted CVaR Classiﬁcation to train ensemble models via Boosting. The key observation we make is that minimizing the α-CVaR loss is equivalent to maximizing the objective of α-LPBoost, a subpopulation-performance counterpart of a classical boosting variant known as LPBoost [DBST02]. Thus training with respect to the α-CVaR loss can be related to a goal of boosting an unfair learner, which always produces a model with low average loss on any reweighting of the training set, to obtain a fair ensemble classiﬁer with low
α-CVaR loss for a ﬁxed α. Note that this is in contrast to the classical boosting, which boosts a weak learner to produce an ensemble model with better average performance. We can thus show that
α-CVaR training is equivalent to a sequential min-max game between a Boosting algorithm and an unfair learner, in which the Boosting algorithm provides the sample weights and the unfair learner provides base models with low average loss with respect to these weights. After all base models are trained, we compute the optimal model weights. At inference time, we ﬁrst randomly sample a base model according to the model weights, and then predict with the model. Thus, the ﬁnal ensemble model is a linear combination of all the base models.
This paper is organized as follows: In Section 2, we provide the necessary background of subpopula-tion shift and CVaR, and show that ERM achieves the lowest CVaR zero-one loss in classiﬁcation tasks with deterministic classiﬁers. In Section 3 we show how to boost an unfair learner: we ﬁrst show that minimizing the CVaR loss is equivalent to maximizing the LPBoost objective in Section 3.1; based on this observation, we propose the Boosted CVaR Classiﬁcation framework and implement an algorithm that uses LPBoost for CVaR classiﬁcation in Section 3.2; Then, in order to improve computation efﬁciency, we implement another algorithm called α-AdaLPBoost in Section 3.3. Finally, in Section 4 we empirically evaluate the proposed method on popular benchmark datasets. 1.1