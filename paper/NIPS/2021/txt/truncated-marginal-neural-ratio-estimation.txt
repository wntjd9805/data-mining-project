Abstract
Parametric stochastic simulators are ubiquitous in science, often featuring high-dimensional input parameters and/or an intractable likelihood. Performing
Bayesian parameter inference in this context can be challenging. We present a neu-ral simulation-based inference algorithm which simultaneously offers simulation efﬁciency and fast empirical posterior testability, which is unique among modern algorithms. Our approach is simulation efﬁcient by simultaneously estimating low-dimensional marginal posteriors instead of the joint posterior and by proposing simulations targeted to an observation of interest via a prior suitably truncated by an indicator function. Furthermore, by estimating a locally amortized posterior our algorithm enables efﬁcient empirical tests of the robustness of the inference results.
Since scientists cannot access the ground truth, these tests are necessary for trusting inference in real-world applications. We perform experiments on a marginalized version of the simulation-based inference benchmark and two complex and narrow posteriors, highlighting the simulator efﬁciency of our algorithm as well as the quality of the estimated marginal posteriors. Implementation on GitHub. 1 1

Introduction
Parametric stochastic simulators are ubiquitous in science [1, 2, 3] and using them to solve the
Bayesian inverse problem is of general interest. Likelihood-based methods like Markov chain Monte
Carlo (MCMC) [4, 5] or nested sampling [6] are applicable when the likelihood is tractable. It is equally common that the likelihood is only implicitly deﬁned by the simulator or is inefﬁcient to compute. For this so-called likelihood-free or simulation-based inference, the traditional approach is
Approximate Bayesian Computation (ABC) [7, 8]. See [9] for a reference.
Simulation-based inference (SBI) is closely connected to ABC and has been an open research topic since as early as the 1980s [10]. Deep learning has accelerated progress in the ﬁeld [11, 12, 13, 14].
Proposed algorithms that learn the likelihood [13] or the posterior [14, 15, 16] utilize a density estimator. The likelihood-to-evidence ratio [11] can be learned via a classiﬁcation-based technique.
Refs. [11] and [14] were brought into a uniﬁed framework by [17].
High-ﬁdelity simulators often have many parameters and/or an intractable likelihood function, which can make inference notoriously difﬁcult. Practitioners are usually faced with observational data and an expensive stochastic simulator, without access to the ground truth posterior. They want a testably accurate posterior estimate without extreme simulation expense. With existing methods, the 1Implementation of experiments at https://github.com/bkmi/tmnre/. Ready-to-use implementation of underly-ing algorithm at https://github.com/undark-lab/swyft/. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison of SBI methods, including our proposed TMNRE, along with select properties.
Properties listed are intended to showcase TMNRE and do not necessarily reﬂect the most desirable properties in every inference setting. For example, if cost were not a inhibiting factor a tractable joint distribution may be more appealing than targeting marginals directly. Similarly, a fully amortized posterior estimate is more ﬂexible than a targeted one but remains, often, prohibitively expensive.
Property / Method
Targeted inference
Simulator efﬁcient direct marginals (Local) amortization
Likelihood-based (cid:51) (cid:55) (cid:55)
ABC
• (cid:51) (cid:55)
NRE (cid:55)
• (cid:51)
NPE (cid:55)
• (cid:51)
SNRE (cid:51) (cid:55) (cid:55)
SNPE (cid:51) (cid:55) (cid:55)
TMNRE (cid:51) (cid:51) (cid:51) practitioner must choose between increased accuracy per simulation (so-called sequential methods
[13, 18]) or efﬁcient empirical testability (so-called amortized methods [11]). We provide a method which offers both simultaneously with a balance that can be tuned by a hyperparameter. Three attributes contribute to this goal:
Targeted inference. Focusing simulations on the parameter regions that are most relevant for the inference problem and target data is more efﬁcient. This is particularly true when most posterior density is concentrated compared to the prior’s density.
Marginal posteriors instead of the joint. Scientiﬁc insight is often based on a low dimensional marginalization of the posterior with nuisance parameters removed [19]. The additional information of the full joint posterior might not be worth the additional cost afforded.
Targeting marginals directly, by estimating only the marginal for the parameters of interest, is simpler and sufﬁcient for many scientiﬁc, parameter estimation, and bounding purposes.
Consistency checks through local amortization. Practitioners are interested in testing the quality of inference methods [20, 21, 22]. One such test is to compare the empirical and nominal contained mass of estimated credible regions [23]. Amortized methods learn the posterior for any data, generated by any parameter, facilitating empirical study of the nominal credible regions on fabricated data. Still, learning an amortized posterior is excessive if only a small subset of parameters are consistent with a target observation.
We propose the concept of local amortization to learn the posterior on said subset, combining simulator efﬁciency of targeted inference with the testability of amortization. Both are critical components for enabling trustworthy scientiﬁc results.
Our contribution. We propose an algorithm that simultaneously achieves all three of the above
It approximates the marginal aspects: Truncated Marginal Neural Ratio Estimation (TMNRE). likelihood-to-evidence ratio in a sequence of rounds and shines when the joint posterior is prohibitively costly. As a basis, we adopt likelihood-to-evidence ratio estimation proposed in [11], although our truncation scheme is applicable to other neural simulation-based inference methods which estimate the posterior or likelihood [13, 17]. Our iterative scheme is loosely inspired by likelihood-based nested sampling [6, 24, 25] since we generate training data drawn from a nested sequence of truncated priors in multiple rounds. Our algorithm (a) preferentially generates simulations in relevant regions of the parameter space, (b) allows estimation of all marginals of interest simultaneously and in parallel from the same training data, and (c) yields posteriors that are locally amortized in a constrained region around the posterior, enabling empirical self-consistency test of the inference results.