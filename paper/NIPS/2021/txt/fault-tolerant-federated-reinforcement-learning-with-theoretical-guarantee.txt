Abstract
The growing literature of Federated Learning (FL) has recently inspired Federated
Reinforcement Learning (FRL) to encourage multiple agents to federatively build a better decision-making policy without sharing raw trajectories. Despite its promising applications, existing works on FRL fail to I) provide theoretical analysis on its convergence, and II) account for random system failures and adversarial attacks. Towards this end, we propose the ﬁrst FRL framework the convergence of which is guaranteed and tolerant to less than half of the participating agents being random system failures or adversarial attackers. We prove that the sample efﬁciency of the proposed framework is guaranteed to improve with the number of agents and is able to account for such potential failures or attacks. All theoretical results are empirically veriﬁed on various RL benchmark tasks. 1

Introduction
Reinforcement learning (RL) has recently been applied to many real-world decision-making problems such as gaming, robotics, healthcare, etc. [1–3]. However, despite its impressive performances in simulation, RL often suffers from poor sample efﬁciency, which hinders its success in real-world applications [4, 5]. For example, when RL is applied to provide clinical decision support [3, 6, 7], its performance is limited by the number (i.e., sample size) of admission records possessed by a hospital, which cannot be synthetically generated [3]. As this challenge is usually faced by many agents (e.g., different hospitals), a natural solution is to encourage multiple RL agents to share their trajectories, to collectively build a better decision-making policy that one single agent can not obtain by itself. However, in many applications, raw RL trajectories contain sensitive information (e.g., the medical records contain sensitive information about patients) and thus sharing them is prohibited. To this end, the recent success of Federated Learning (FL) [8–11] has inspired the setting of Federated
Reinforcement Learning (FRL) [12], which aims to federatively build a better policy from multiple
RL agents without requiring them to share their raw trajectories. FRL is practically appealing for addressing the sample inefﬁciency of RL in real systems, such as autonomous driving [13], fast personalization [14], optimal control of IoT devices [15], robots navigation [16], and resource management in networking [17]. Despite its promising applications, FRL is faced by a number of major challenges, which existing works are unable to tackle.
Firstly, existing FRL frameworks are not equipped with theoretical convergence guarantee, and thus lack an assurance for the sample efﬁciency of practical FRL applications, which is a critical drawback 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
due to the high sampling cost of RL trajectories in real systems [4]. Unlike FL where training data can be collected ofﬂine, FRL requires every agent to sample trajectories by interacting with the environment during learning. However, interacting with real systems can be slow, expensive, or fragile. This makes it critical for FRL to be sample-efﬁcient and hence highlights the requirement for convergence guarantee of FRL, without which no assurance on its sample efﬁciency is provided for practical applications. To ﬁll this gap, we establish on recent endeavors in stochastic variance-reduced optimization techniques to develop a variance-reduced federated policy gradient framework, the con-vergence of which is guaranteed. We prove that the proposed framework enjoys a sample complexity of O(1/(cid:15)5/3) to converge to an (cid:15)-stationary point in the single-agent setting, which matches recent results of variance-reduced policy gradient [18, 19]. More importantly, the aforementioned sample complexity is guaranteed to improve at a rate of O(1/K 2/3) upon the federation of K agents. This guarantees that an agent achieves a better sample efﬁciency by joining the federation and beneﬁts from more participating agents, which are highly desirable in FRL.
Another challenge inherited from FL is that FRL is vulnerable to random failures or adversarial attacks, which poses threats to many real-world RL systems. For example, robots may behave arbitrarily due to random hardware issues; clinical data may provide inaccurate records and hence create misleading trajectories [3]; autonomous vehicles, on which RL is commonly deployed, are subject to adversarial attacks [20]. As we will show in experiments, including such random failures or adversary agents in FRL can signiﬁcantly deteriorate its convergence or even result in unlearnability.
Of note, random failures and adversarial attacks in FL systems are being encompassed by the
Byzantine failure model [21], which is considered as the most stringent fault formalism in distributed computing [22, 23] – a small fraction of agents may behave arbitrarily and possibly adversarially, with the goal of breaking or at least slowing down the convergence of the system. As algorithms proven to be correct in this setting are guaranteed to converge under arbitrary system behavior (e.g., exercising failures or being attacked) [9, 24], we study the fault tolerance of our proposed FRL framework using the Byzantine failure model. We design a gradient-based Byzantine ﬁlter on top of the variance-reduced federated policy gradient framework. We show that, when a certain percentage (denoted by α < 0.5) of agents are Byzantine agents, the sample complexity of the FRL system is worsened by only an additive term of O(α4/3/(cid:15)5/3) (Section 4). Therefore, when α → 0, (i.e., an ideal system with zero chance of failure), the ﬁlter induces no impact on the convergence.
Contributions. In this paper, we study the federated reinforcement learning problem with theoretical guarantee in the potential presence of faulty agents. We introduce Federated Policy Gradient with
Byzantine Resilience (FedPG-BR), the ﬁrst FRL framework that is theoretically principled and practically effective for the FRL setting, accounting for random systematic failures and adversarial attacks. In particular, FedPG-BR (a) enjoys a guaranteed sample complexity which improves with more participating agents, and (b) is tolerant to the Byzantine fault in both theory and practice.
We discuss the details of problem setting and the technical challenges (Section 3) and provide theoretical analysis of FedPG-BR (Section 4). We also demonstrate its empirical efﬁcacy on various
RL benchmark tasks (Section 5). 2