Abstract
Defocus blur is one kind of blur effects often seen in images, which is challenging to remove due to its spatially variant amount. This paper presents an end-to-end deep learning approach for removing defocus blur from a single image, so as to have an all-in-focus image for consequent vision tasks. First, a pixel-wise Gaussian kernel mixture (GKM) model is proposed for representing spatially variant defocus blur kernels in an efﬁcient linear parametric form, with higher accuracy than existing models. Then, a deep neural network called GKMNet is developed by unrolling a ﬁxed-point iteration of the GKM-based deblurring. The GKMNet is built on a lightweight scale-recurrent architecture, with a scale-recurrent attention module for estimating the mixing coefﬁcients in GKM for defocus deblurring.
Extensive experiments show that the GKMNet not only noticeably outperforms existing defocus deblurring methods, but also has its advantages in terms of model complexity and computational efﬁciency. 1

Introduction
The appearance sharpness of an object in an image taken by a camera is determined by the scene distance of the object to the focal plane of the camera. An object will have the sharpest appearance when it is on the focal plane, i.e., the object is in focus. The area around the focal plane where objects appear to be in focus is called the depth of ﬁeld (DoF). When an object is away from the DoF, it will appear blurry. The further is an object away from the DoF, the more blurry it appears. Such a phenomenon is called defocus blur or out-of-focus blur. Defocus blur effects will be prominent in an image with a shallow DoF, e.g. images captured with a large aperture. This paper concerns the problem of single image defocus deblurring (SIDD) which is about reconstructing an all-in-focus image from a defocused image (i.e. an image with defocused regions). SIDD is of practical values to many applications in machine vision, e.g., photo refocusing, object recognition, and many others [1].
Consider a defocused image y, which relates to its all-in-focus counterpart x by y = B ◦ x + (cid:15), where (cid:15) denotes the measurement noise, and B is a linear operator deﬁned by (B ◦ x)[m, n] := (cid:88) (cid:88) i j bm,n[i, j]x[m − i, n − j]. (1) (2)
Each pixel at location [m, n] is associated with a defocus kernel bm,n, also referred to as point spread function (PSF), which is determined by the distance to the focal plane. Often, these pixel-wise PSFs are approximated by Gaussian kernels [2, 3, 4, 5, 6] or disk kernels [7, 8]. Without supplementary information on the scene depth, these pixel-wise PSFs are unknown. Therefore, SIDD is a challenging nonlinear inverse problem which needs to estimate both B and x from (1).
∗email: csyhquan@scut.edu.cn (Y. Quan); cszicongwu@mail.scut.edu.cn (Z. Wu); matjh@nus.edu.sg (H. Ji). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1.1 Discussion on Existing Work
Most existing methods (e.g. [2, 7, 4, 8, 5, 6, 9]) take a two-stage approach which (i) estimates a dense defocus map to derive the operator B and then (ii) recovers the image x by using nonblind image deconvolution to solve (1) with the estimated B. Generally, such a two-stage approach has a long pipeline with many modules, and the estimation error in one module will be magniﬁed in the consequent modules. For instance, the defocus amount only can be estimated on a subset of image pixels such as edge points. Then a dense defocus map needs to be constructed by propagating these few estimations to all pixels. It can be seen that any error in the sparse defocus map will result in erroneous PSFs, and unfortunately deconvolution is very sensitive to the errors in PSFs [10, 11]. As a result, the two-stage approach does not perform well in practice. Also, the computational cost in the second stage is high for a non-uniform blurring operator B, as the inversion process regrading B, which is often called many times in nonblind image deconvolution, cannot be efﬁciently computed via fast Fourier transform (FFT).
Deep learning has become one prominent tool for solving a wide range of image restoration problems.
In comparison to the rapid progress of DNNs for spatially-varying motion deblurring (e.g. [12, 13, 14, 15, 16, 17]), there have been few works on studying DNN-based approaches to defocus deblurring.
One might directly adapt an existing motion deblurring DNN for SIDD. However, the kernels of defocus blur are very different from those of motion blur, e.g., roughly isotropic support vs highly curvy support. Also, the spatial variation in defocus blur differs much from that in motion blur, e.g., transparency effects of moving objects in motion blur do not exist for out-of-focus objects in defocus blur. As a result, it is sub-optimal to directly call a motion deblurring method for SIDD.
Another straightforward implementation of introducing deep learning to SIDD is to replace the defocus map estimator in a traditional two-stage approach by a DNN-based method (e.g. [6]). Such an implementation still suffers from the issues existing in traditional methods, i.e. inaccurate estimation of B from a non-perfect defocus map and high computational cost for deblurring with a spatially-variant blurring operator. To fully exploit the potential of deep learning for SIDD, one needs to speciﬁcally design an end-to-end DNN that directly predicts the all-in-focus image from the defocused one. Recently, Abuolaim and Brown [18] developed an end-to-end DNN for constructing an all-in-focus image from a pair of images containing two sub-aperture views of the same scene. They also adapted their DNN to SIDD, but saw a signiﬁcant performance decrease. 1.2 Main Idea
This paper aims at developing an end-to-end DNN for SIDD with better performance than existing methods, which is based on the following two derivations.
GKM-based model for defocus blurring Since defocus PSFs show strong isotropy and smooth-ness, we propose to model the kernels {bm,n}m,n by Gaussian kernel mixture (GKM): bm,n =
K (cid:88) k=1
βk[m, n]g(σk), (3) where g(σ) denotes the 2D Gaussian kernel of variance σ2, and βk denotes the matrix of mixing coefﬁcients for the k-th Gaussian kernel in the GKM. As the GKM can ﬁt well most isotropic kernels,
Eq. (3) is a more accurate model for real defocus PSFs than the often-used single Gaussian/disk form; see supplementary materials for a demonstration.
Remark 1. The GKM degenerates to the single Gaussian form when only one mixing coefﬁcient is 1 and the others are 0 for every location [m, n]. In general cases, the weighted summation of Gaussian kernels can represent non-Gaussian kernels, and thus the GKM can express a wider family of defocus
PSFs than the single Gaussian form. There is also another work [9] that models defocus PSFs beyond single Gaussian kernels. It uses the generalized Gaussian function [9] where the parameters to be estimated are wrapped in a complex nonlinear function. In comparison, our GKM model is linear with pre-deﬁned {σk}k, which facilitates the estimation on its parameter {βk}k.
We can rewrite (1) as y = (cid:88) m,n
δm,n (cid:12) (bm,n ⊗ x) = (cid:88)
K (cid:88) m,n k=1
δm,n (cid:12) ((βk[m, n]g(σk)) ⊗ x) =
K (cid:88) k=1
βk (cid:12) (g(σk) ⊗ x), 2
where δm,n denotes the Dirac delta centered at location [m, n], and ⊗, (cid:12) denote the operations of 2D convolution and entry-wise multiplication, respectively. Then we have the GKM-based model for defocus blurring:
B : x →
K (cid:88) k=1
βk (cid:12) (g(σk) ⊗ x). (4)
Fixed-point iteration unrolling Recall that the blurring operator B is about keeping the low-frequency components and attenuating high-frequency ones of an image. Let I denote the identity mapping. The mapping I − B is then about attenuating the low-frequency components and keeping the high-frequency ones. Neglecting the noise (cid:15) and rewriting (1) by x = y + (I − B) ◦ x, (5) we have then a ﬁxed-point iteration for solving defocus deblurring, which is given by x(t+1) = f (x(t)) = y+x(t) −B ◦x(t) = y+x(t) −
K (cid:88) k=1
βk (cid:12)(g(σk)⊗x(t)), for t = 1, 2, . . . . (6)
Note that the ﬁxed-point iteration above will be convergent if I − B is a contractive mapping, or equivalently the eigenvalues of B fall in (0, 1), which holds true when the defocus blurring is uniform with a normalized Gaussian kernel, i.e., the scene depths are constant in the view.
Deﬁne σ1 = 0 and g(σ1) = δ, so that clear regions can be modeled by setting β1 = 1 and zeroing
βk for k > 1. Let γ1 = 1 − β1 and γk = −βk for k > 1. The iteration (6) can be expressed as x(t+1) = y(t) +
K (cid:88) k=1
γk (cid:12) (g(σk) ⊗ x(t)), for t = 1, 2, . . . . (7)
In short, based on the GKM model of defocus PSFs, we can unroll a ﬁxed-point iteration to solve (1) with learnable coefﬁcient matrices γ1, · · · , γK. The motivation of unrolling a ﬁxed-point iteration, instead of other iterative schemes such as gradient descent [19] and half quadratic splitting (HQS) [20], is to involve the forward operator B only, without introducing the transpose B(cid:62) and the pseudo-inverse B†.
Remark 2. Our approach is sort of in the category of optimization unrolling, a widely-used method-ology of designing DNNs for solving inverse problems. The key is to choose an appropriate iteration scheme that ﬁts the problem well. Most existing optimization unrolling based image deblurring methods (e.g. [22, 23, 20, 24, 25, 26, 21, 27]) consider uniform blurring, where the matrix B can be represented by a convolution. The iterative schemes they adopt such as HQS, usually involve an inversion process for B, which can be efﬁciently computed using FFT when B is a convolution oper-ator. In our case, B is a spatially-varying blurring operator which does not have a computationally efﬁcient inversion process. The proposed ﬁxed-point iteration unrolling enables us to avoid such an inversion process in the DNN and use the forward operator only.
The matrices γ1, · · · , γK of mixing coefﬁcients can be intezpreted as the attention maps associated to the feature maps generated by different Gaussian kernels. Thus, we construct a DNN with attention modules and long skip connections to utilize (7) for SIDD. In addition, we take a multi-scale scheme to implement the unrolling: at each iteration the DNN predicts the all-in-focus image at current scale and up-samples it for the calculation of the next iteration, with weight sharing used across scales.
This leads to a scale-recurrent attentive DNN with a lightweight implementation. 1.3 Main Contributions
In comparison to existing two-stage or dual-view-based methods, this paper is among the ﬁrst ones to present an end-to-end DNN for SIDD. See below for the summary of our technical contributions:
• A new and efﬁcient parametric model based on GKM for defocus blur kernels, which ﬁts real-world data better than existing models and thus leads to better performance in SIDD;
• A new formulation of the deblurring process derived from a ﬁxed-point iteration so as to have a simple and efﬁcient parameterization of defocus deblurring, which inspires an effective DNN for SIDD with low model complexity and high computational efﬁciency; 3
• A scale-recurrent attention mechanism which combines the coarse-to-ﬁne progressive esti-mation and the unrolled deblurring process for better performance.
The experiments show that the proposed DNN brings noticeable improvement over existing ap-proaches to SIDD, in terms of recovery quality, model complexity and computational efﬁciency. 2