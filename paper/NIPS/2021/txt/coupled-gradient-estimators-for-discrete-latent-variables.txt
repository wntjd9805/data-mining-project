Abstract
Training models with discrete latent variables is challenging due to the high vari-ance of unbiased gradient estimators. While low-variance reparameterization gradients of a continuous relaxation can provide an effective solution, a contin-uous relaxation is not always available or tractable. Dong et al. (2020) and Yin et al. (2020) introduced a performant estimator that does not rely on continuous relaxations; however, it is limited to binary random variables. We introduce a novel derivation of their estimator based on importance sampling and statistical couplings, which we extend to the categorical setting. Motivated by the con-struction of a stick-breaking coupling, we introduce gradient estimators based on reparameterizing categorical variables as sequences of binary variables and
Rao-Blackwellization. In systematic experiments, we show that our proposed categorical gradient estimators provide state-of-the-art performance, whereas even with additional Rao-Blackwellization, previous estimators (Yin et al., 2019) un-derperform a simpler REINFORCE with a leave-one-out-baseline estimator (Kool et al., 2019). 1

Introduction
Optimizing an expectation of a cost function of discrete variables with respect to the parameters of their distribution is a frequently encountered problem in machine learning. This problem is challenging because the gradient of the objective, like the objective itself, is an expectation over an exponentially large space of joint conﬁgurations of the variables. As the number of variables increases, these expectations quickly become intractable and thus are typically approximated using
Monte Carlo sampling, trading a reduction in computation time for variance in the estimates. When such stochastic gradient estimates are used for learning, their variance determines the largest learning rate that can be used without making training unstable. Thus ﬁnding estimators with lower variance leads directly to faster training by allowing higher learning rates. For example, the use of the reparameterization trick to yield low-variance gradient estimates has been essential to the success of variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014). However, this estimator, also known as the pathwise derivative estimator (Glasserman, 2013), can only be applied to continuous random variables.
For discrete random variables, there are two common strategies for stochastic gradient estimation. The
ﬁrst one involves replacing discrete variables with continuous ones that approximate them as closely as possible (Maddison et al., 2017; Jang et al., 2017) and training the resulting relaxed system with the reparameterization trick. However, as after training, the system is evaluated with discrete variables, this approach is not guaranteed to perform well and requires a careful choice of the continuous relaxation. Moreover, evaluating the cost function at the relaxed values instead of the discrete ones is not always desirable or even possible. The second strategy, involves using the REINFORCE estimator
Code and additional information: https://sites.google.com/view/disarm-estimator. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(Williams, 1992), also known as the score-function (Rubinstein and Shapiro, 1990) or likelihood-ratio (Glynn, 1990) estimator, which, having fewer requirements than the reparameterization trick, also works with discrete random variables. As the simplest versions of this estimator tend to exhibit high variance, they are typically combined with variance reduction techniques. Some of the most effective such estimators (Tucker et al., 2017; Grathwohl et al., 2018), incorporate the gradient information provided by the continuous relaxation, while keeping the estimator unbiased w.r.t. the original discrete system.
The recently introduced Augment-REINFORCE-Merge (ARM) (Yin and Zhou, 2019) estimator for binary variables and Augment-REINFORCE-Swap (ARS) and Augment-REINFORCE-Swap-Merge (ARSM) estimators (Yin et al., 2019) for categorical variables provide a promising alternative to relaxation-based estimators. However, when compared to a simpler baseline approach (REINFORCE with a leave-one-out-baseline (RLOO; Kool et al., 2019)), ARM underperforms in the binary setting (Dong et al., 2020), and we similarly demonstrate that ARS and ARSM underperform in the categorical setting. Dong et al. (2020) and Yin et al. (2020) independently developed an estimator that uses Rao-Blackwellization to improve ARM and outperforms RLOO, providing state-of-the-art performance in the binary setting.
In this paper, we explore how to devise a performant estimator in the categorical setting. A natural
ﬁrst approach is to apply the ideas from (Dong et al., 2020; Yin et al., 2020) to ARS and ARSM.
However, empirically, we ﬁnd that this is insufﬁcient to close the gap between ARS/ARSM and
RLOO. Due to space constraints, we defer the details to Appendix A.6. Instead, we develop a novel derivation of DisARM/U2G (Dong et al., 2020; Yin et al., 2020) using importance sampling which is simpler and more direct while providing a natural extension to the categorical case. This estimator requires constructing a coupling on categorical variables, which we do using a stick-breaking process and antithetic Bernoulli variables. Motivated by this construction, we also consider estimators based on reparameterizing the problem with a sequence of binary variables. We systematically evaluate these estimators and their underlying design choices and ﬁnd that they outperform RLOO across a range of problems without requiring more computation. 2