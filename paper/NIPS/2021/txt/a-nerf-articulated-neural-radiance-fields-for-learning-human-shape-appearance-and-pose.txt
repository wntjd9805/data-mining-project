Abstract
While deep learning reshaped the classical motion capture pipeline with feed-forward networks, generative models are required to recover ﬁne alignment via iterative reﬁnement. Unfortunately, the existing models are usually hand-crafted or learned in controlled conditions, only applicable to limited domains. We propose a method to learn a generative neural body model from unlabelled monocular videos by extending Neural Radiance Fields (NeRFs). We equip them with a skeleton to apply to time-varying and articulated motion. A key insight is that implicit models require the inverse of the forward kinematics used in explicit surface models. Our reparameterization deﬁnes spatial latent variables relative to the pose of body parts and thereby overcomes ill-posed inverse operations with an overparameterization.
This enables learning volumetric body shape and appearance from scratch while jointly reﬁning the articulated pose; all without ground truth labels for appearance, pose, or 3D shape on the input videos. When used for novel-view-synthesis and motion capture, our neural model improves accuracy on diverse datasets. Project website: https://lemonatsu.github.io/anerf/. 1

Introduction
Generative models have evolved from Generative Adversarial Networks (GANs) recreating images
[14, 21] to neural scene representations [39, 56, 57] providing control and image understanding for downstream tasks via structured latent variables. However, most 3D models require 3D labels that cannot be crowd-sourced on natural images and require dedicated depth sensors. It is hence an important research problem to learn 3D representations from 2D observations, which is particularly challenging for humans with diverse body shapes and appearances and their non-rigid motion.
Modern human motion capture techniques typically combine the advantages of discriminative and generative approaches. A feed-forward 3D human pose estimation approach provides a rough initial estimate of human pose. Afterward, a generative approach based on either a high-quality 3D scan of the person [17], or a parametric human body model learned from laser scans [4] reﬁnes the estimate iteratively based on the image evidence. Although achieving unprecedented accuracy, existing models require a low-dimensional, restrictive, shape body model or a personalized 3D scan of the user.
We introduce Articulated Neural Radiance Fields (A-NeRF) for learning a user-speciﬁc neural 3D body model and underlying skeleton pose from unlabelled videos (see Figure 1). When applied to motion capture, it alleviates the need for template models while maintaining the advantages and accuracy of current generative approaches. A-NeRF extends Neural Radiance Fields (NeRF) [39] to work with single videos and articulated motion. NeRF parameterizes the scene implicitly as
Fφ(Γ(q), Γ(d)) (cid:55)→ (σ, c), with σ ∈ R, c ∈ R3, q ∈ R3, and d ∈ R3, by chaining Fφ, a Multi-layer Perceptron (MLP), with Γ, the Positional Encoding (PE) [66]. First, the
PE maps the input scene point q and view direction d to a higher dimensional space that enables the (1) 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Initial pose
Input view (a) A-NeRF body model for animation and novel-view-synthesis
Reﬁned pose (b) 3D pose reﬁnement for real images 3D rec. (c) Body shape
Figure 1: Our A-NeRF jointly learns a neural body model of the user and works with diverse body poses (left) while also reﬁning the initial 3D articulated skeleton pose estimate from a single or, if available, multiple views without tedious camera calibration (center). Underlying is a template-free neural representation (right) and skeleton-based embedding coupled with volume volumetric rendering. Real faces and their reconstructions are blurred in all ﬁgures for anonymity.
MLP to learn a meaningful scene representation function Fφ that subsequently outputs the radiance c and opacity σ at every point in space. Second, the implicitly described scene (via conditioning on query locations) is rendered via classical ray-marching from computer graphics. The advantage of the MLP representation is that it avoids the complexity of volumetric grids [29], circumvents the artifacts induced by the implicit bias of screen-space convolution [40, 54], and, unlike surface meshes, can have ﬂexible topology. However, the original NeRF only works for static scenes captured from dozens of calibrated cameras such that each 3D point is seen from multiple views.
Our conceptual contribution lies in learning a neural latent representation relative to an articulated skeleton. While explicit models such as the popular SMPL body model [30] deform a surface via forwards kinematics, the implicit form of A-NeRF makes us re-think how skeletons can be integrated—implicit networks require the inverse transformation from 3D world coordinates to the reference skeleton, a signiﬁcantly harder task that has not been fully explored. Our core technical novelty is to come up with and evaluate different parameterizations of Γ(q), Γ(d) in Eq. 1 to create local coordinates relative to the articulated skeleton. Since a point in 3D world coordinates cannot uniquely be associated with a body part, we resolve the mentioned ill-posed inverse problem by overparameterizing with one embedding per bone. This embeds domain knowledge of how humans move and provides a common frame for the neural network to combine body shape and appearance constraints across the entire captured sequence (see Figure 2).
We demonstrate that all our contributions together enable learning of a neural body model from monocular video, requiring only rough 3D pose estimates for initialization, that reaches a level of detail previously only attained with parametric surface models or multi-view approaches [51].
Scope. We apply the model to motion capture, character animation, and appearance and motion transfer and demonstrate that the pose reﬁnement improves on existing monocular skeleton recon-struction. We learn in the transductive setting, for a speciﬁc target video that is known at training time but has no ground truth. A-NeRF enables novel view synthesis of dynamic motions, with plausible however non-physical illumination. Additional steps are needed to enable relighting applications.
General impact. Building a self-supervised approach for personalized human body modelling promises to be more inclusive to people and activities that are not well represented in supervised datasets. However, it bears the risk that 3D models of people are created without consent. We urge users to only use datasets collected for developing and validating motion capture algorithms. 2