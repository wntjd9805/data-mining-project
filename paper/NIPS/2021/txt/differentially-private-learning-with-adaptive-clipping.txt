Abstract
Existing approaches for training neural networks with user-level differential privacy (e.g., DP Federated Averaging) in federated learning (FL) settings involve bounding the contribution of each user’s model update by clipping it to some constant value.
However there is no good a priori setting of the clipping norm across tasks and learning settings: the update norm distribution depends on the model architecture and loss, the amount of data on each device, the client learning rate, and possibly various other parameters. We propose a method wherein instead of a ﬁxed clipping norm, one clips to a value at a speciﬁed quantile of the update norm distribution, where the value at the quantile is itself estimated online, with differential privacy.
The method tracks the quantile closely, uses a negligible amount of privacy budget, is compatible with other federated learning technologies such as compression and secure aggregation, and has a straightforward joint DP analysis with DP-FedAvg.
Experiments demonstrate that adaptive clipping to the median update norm works well across a range of realistic federated learning tasks, sometimes outperforming even the best ﬁxed clip chosen in hindsight, and without the need to tune any clipping hyperparameter. 1

Introduction
Deep learning has become ubiquitous, with applications as diverse as image processing, natural language translation, and music generation [27, 13, 28, 7]. Deep models are able to perform well in part due to their ability to utilize vast amounts of data for training. However, recent work has shown that it is possible to extract information about individual training examples using only the parameters of a trained model [12, 30, 26, 8, 19]. When the training data potentially contains privacy-sensitive user information, it becomes imperative to use learning techniques that limit such memorization.
Differential privacy (DP) [10, 11] is widely considered a gold standard for bounding and quantifying the privacy leakage of sensitive data when performing learning tasks. Intuitively, DP prevents an adversary from conﬁdently making any conclusions about whether some user’s data was used in training a model, even given access to arbitrary side information. The formal deﬁnition of DP depends on the notion of neighboring datasets: we will refer to a pair of datasets D, D(cid:48) ∈ D as neighbors if
D(cid:48) can be obtained from D by adding or removing one element.
Deﬁnition 1.1 (Differential Privacy). A (randomized) algorithm M : D → R with input domain D and output range R is (ε, δ)-differentially private if for all pairs of neighboring datasets D, D(cid:48) ∈ D, and every measurable S ⊆ R, we have Pr (M (D) ∈ S) ≤ eε · Pr (M (D(cid:48)) ∈ S) + δ, where probabilities are with respect to the coin ﬂips of M . 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Following McMahan et al. [18], we deﬁne two common settings of privacy corresponding to two different deﬁnitions of neighboring datasets. In example-level DP, datasets are considered neighbors when they differ by the addition or removal of a single example [9, 4, 2, 22, 31, 23, 15]. In user-level
DP, neighboring datasets differ by the addition or removal of all of the data of one user [18]. User-level DP is the stronger form, and is preferred when one user may contribute many training examples to the learning task, as privacy is protected even if the same privacy-sensitive information occurs in all the examples from one user. In this paper, we will describe the technique and perform experiments in terms of the stronger user-level form, but we note that example-level DP can be achieved by simply giving each user a single example.
To achieve user-level DP, we employ the Federated Averaging algorithm [17], introduced as a decentralized approach to model training in which the training data is left distributed on user devices, and each training round aggregates updates that are computed locally. On each round, a sample of devices are selected for training, and then each selected device performs potentially many steps of local SGD over minibatches of its own data, sending back the model delta as its update.
Bounding the inﬂuence of any user in Federated Averaging is both necessary for privacy and often desirable for stability. One popular way to do this to cap the L2 norm of its model update by projecting larger updates back to the ball of norm C. Since such clipping also effectively bounds the L2 sensitivity of the aggregate with respect to the addition or removal of any user’s data, adding
Gaussian noise to the aggregate is sufﬁcient to obtain a central differential privacy guarantee for the update [9, 4, 2]. Standard composition techniques can then be used to extend the per-update guarantee to the ﬁnal model [20].
Setting an appropriate value for the clipping threshold C is crucial for the utility of the private training mechanism. Setting it too low can result in high bias since we discard information contained in the magnitude of the gradient. However setting it too high entails the addition of more noise, because the amount of Gaussian noise necessary for a given level of privacy must be proportional to the norm bound (the L2 sensitivity), and this will eventually destroy model utility. The clipping bias-variance trade-off was observed empirically by McMahan et al. [18], and was theoretically analyzed and shown to be an inherent property of differentially private learning by Amin et al. [3].
Learning large models using the Federated Averaging/SGD algorithm [17, 18] can take thousands of rounds of interaction between the central server and the clients. The norms of the updates can vary as the rounds progress. Prior work [18] has shown that decreasing the value of the clipping threshold after training a language model for some initial number of rounds can improve model accuracy. However, the behavior of the norms can be difﬁcult to predict without prior knowledge about the system, and if it is difﬁcult to choose a ﬁxed clipping norm for a given learning task, it is even more difﬁcult to choose a parameterized clipping norm schedule.
While there has been substantial work on DP techniques for learning, almost every technique has hyperparameters which need to be set appropriately for obtaining good utility. Besides the clipping norm, learning techniques have other hyperparameters which might interact with privacy hyperparameters. For example, the server learning rate in DP SGD might need to be set to a high value if the clipping threshold is very low, and vice-versa. Such tuning for large networks can have an exorbitant cost in computation and efﬁciency, which can be a bottleneck for real-world systems that involve communicating with millions of samples for training a single network. Tuning may also incur an additional cost for privacy, which needs to be accounted for when providing a privacy guarantee for the released model with tuned hyperparameters (though in some cases, hyperparameters can be tuned using the same model and algorithm on a sufﬁciently-similar public proxy dataset).