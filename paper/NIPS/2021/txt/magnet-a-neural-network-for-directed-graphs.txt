Abstract
The prevalence of graph-based data has spurred the rapid development of graph neural networks (GNNs) and related machine learning algorithms. Yet, despite the many datasets naturally modeled as directed graphs, including citation, website, and trafﬁc networks, the vast majority of this research focuses on undirected graphs. In this paper, we propose MagNet, a GNN for directed graphs based on a complex Hermitian matrix known as the magnetic Laplacian. This matrix encodes undirected geometric structure in the magnitude of its entries and directional information in their phase. A “charge” parameter attunes spectral information to variation among directed cycles. We apply our network to a variety of directed graph node classiﬁcation and link prediction tasks showing that MagNet performs well on all tasks and that its performance exceeds all other methods on a majority of such tasks. The underlying principles of MagNet are such that it can be adapted to other GNN architectures. 1

Introduction
Endowing a collection of objects with a graph structure allows one to encode pairwise relationships among its elements. These relations often possess a natural notion of direction. For example, the
WebKB dataset [36] contains a list of university websites with associated hyperlinks. In this context, one website might link to a second without a reciprocal link to the ﬁrst. Such datasets are naturally modeled by directed graphs. In this paper, we introduce MagNet, a graph convolutional neural network for directed graphs based on the magnetic Laplacian.
Most graph neural networks fall into one of two families, spectral networks or spatial networks.
Spatial methods deﬁne graph convolution as a localized averaging operation with iteratively learned weights. Spectral networks, on the other hand, deﬁne convolution on graphs via the eigendecomposi-ton of the (normalized) graph Laplacian. The eigenvectors of the graph Laplacian assume the role of Fourier modes, and convolution is deﬁned as entrywise multiplication in the Fourier basis. For a comprehensive review of both spatial and spectral networks, we refer the reader to [46] and [44].
Many spatial graph CNNs have natural extensions to directed graphs. However, these extensions typically only consider the outgoing neighbors of each vertex and neglect the incoming neighbors.
Therefore, they run the risk of discarding potentially important information. Consider, for example, a 35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.
directed social network such as Twitter, where the nodes are Twitter accounts and a directed edge (u, v) ∈ E means that account u mentions account v (using the @ functionality). To infer something about account v, there is important information to be gathered both from other accounts that v mentions, and accounts that mention v. Therefore, it is common for spatial methods to preprocess the data by symmetrizing the adjacency matrix, effectively creating an undirected graph. For example, while [43] explicitly notes that their network is well-deﬁned on directed graphs, their experiments treat all citation networks as undirected for improved performance.
Extending spectral methods to directed graphs is not straightforward since the adjacency matrix is asymmetric and, thus, there is no obvious way to deﬁne a symmetric, real-valued Laplacian with a full set of real eigenvalues that uniquely encodes any directed graph. We overcome this challenge by constructing a network based on the magnetic Laplacian L(q) deﬁned in Section 2. Unlike the directed graph Laplacians used in works such as [29, 33, 41, 42], the magnetic Laplacian is not a real-valued symmetric matrix. Instead, it is a complex-valued Hermitian matrix that encodes the fundamentally asymmetric nature of a directed graph via the complex phase of its entries.
Since L(q) is Hermitian, the spectral theorem implies it has an orthonormal basis of complex eigenvectors corresponding to real eigenvalues. Moreover, Theorem 1, stated in Section 5 of the supplement, shows that L(q) is positive semideﬁnite, similar to the traditional Laplacian. Setting q = 0 is equivalent to symmetrizing the adjacency matrix and no importance is given to directional information. When q = .25, on the other hand, we have that L(.25)(u, v) = −L(.25)(v, u) whenever there is an edge from u to v but not from v to u. Different values of q highlight different graph motifs
[16, 17, 20, 32], and therefore the optimal choice of q varies. Learning the appropriate value of q from data allows MagNet to adaptively incorporate directed information. We also note that L(q) has been applied to graph signal processing [19], community detection [17], and clustering [10, 16, 15].
In Section 3, we show how the networks constructed in [6, 13, 24] can be adapted to directed graphs by incorporating complex Hermitian matrices, such as the magnetic Laplacian. When q = 0, we effectively recover the networks constructed in those previous works. Therefore, our work generalizes these networks in a way that is suitable for directed graphs. Our method is very general and is not tied to any particular choice of network architecture. Indeed, the main ideas of this work could be adapted to nearly any spectral graph neural network, and some spatial ones.
In Section 4, we summarize related work on directed graph neural networks as well as other papers studying the magnetic Laplacian and its applications in data science. In Section 5, we apply our network to node classiﬁcation and link prediction tasks. We compare against several spectral and spatial methods as well as networks designed for directed graphs. We ﬁnd that MagNet obtains the best or second-best performance on ﬁve out of six node-classiﬁcation tasks and has the best performance on seven out of eight link-prediction tasks tested on real-world data, in addition to providing excellent node-classiﬁcation performance on difﬁcult synthetic data. We also provide a supplementary document with full implementation details, theoretical results concerning the magnetic
Laplacian, extended examples, and further numerical details. 2 The magnetic Laplacian
Spectral graph theory has been remarkably successful in relating geometric characteristics of undi-rected graphs to properties of eigenvectors and eigenvalues of graph Laplacians and related matrices.
For example, the tasks of optimal graph partitioning, sparsiﬁcation, clustering, and embedding may be approximated by eigenvectors corresponding to small eigenvalues of various Laplacians (see, e.g.,
[9, 38, 2, 40, 11]). Similarly, the graph signal processing research community leverages the full set of eigenvectors to extend the Fourier transform to these structures [34]. Furthermore, numerous papers
[6, 13, 24] have shown that this eigendecomposition can be used to deﬁne neural networks on graphs.
In this section, we provide the background needed to extend these constructions to directed graphs via complex Hermitian matrices such as the magnetic Laplacian.
We let G = (V, E) be a directed graph where V is a set of N vertices and E ⊆ V × V is a set of directed edges. If (u, v) ∈ E, then we say there is an edge from u to v. For the sake of simplicity, we will focus on the case where the graph is unweighted and has no self-loops, i.e., (v, v) /∈ E, but our methods have natural extensions to graphs with self-loops and/or weighted edges. If both (u, v) ∈ E and (v, u) ∈ E, then one may consider this pair of directed edges as a single undirected edge. 2
A directed graph can be described by an adjacency matrix (A(u, v))u,v∈V where A(u, v) = 1 if (u, v) ∈ E and A(u, v) = 0 otherwise. Unless G is undirected, A is not symmetric, and, indeed, this is the key technical challenge in extending spectral graph neural networks to directed graphs. In the undirected case, where the adjacency matrix A is symmetric, the (unnormalized) graph Laplacian can be deﬁned by L = D − A, where D is a diagonal degree matrix. It is well-known that L is a symmetric, positive-semideﬁnite matrix and therefore has an orthonormal basis of eigenvectors associated with non-negative eigenvalues. However, when A is asymmetric, direct attempts to deﬁne the Laplacian this way typically yield complex eigenvalues. This impedes the straightforward extension of classical methods of spectral graph theory and graph signal processing to directed graphs.
A key point of this paper is to represent the directed graph through a complex Hermitian matrix
L such that: (1) the magnitude of L(u, v) indicates the presence of an edge, but not its direction; and (2) the phase of L(u, v) indicates the direction of the edge, or if the edge is undirected. Such matrices have been explored in the directed graph literature (see Section 4), but not in the context of graph neural networks. They have several advantages over their real-valued matrix counterparts. In particular, a single symmetric real-valued matrix will not uniquely represent a directed graph. Instead, one must use several matrices, as in [42], but this increases the complexity of the resulting network.
Alternatively, one can work with an asymmetric, real-valued matrix, such as the adjacency matrix or the random walk matrix. However, the spatial graph ﬁlters that result from such matrices are typically limited by the fact that they can only aggregate information from the vertices that can be reached in one hop from a central vertex, but ignore the equally important subset of vertices that can reach the central vertex in one hop. Complex Hermitian matrices, however, lead to ﬁlters that aggregate information from both sets of vertices. Finally, one could use a real-valued skew-symmetric matrix but such matrices do not generalize well to graphs with both directed and undirected edges.
The optimal choice of complex Hermitian matrix is an open question. Here, we utilize a parameterized family of magnetic Laplacians, which have proven to be useful in other data-driven contexts [17, 10, 16, 15]. We ﬁrst deﬁne the symmetrized adjacency matrix and corresponding degree matrix by,
As(u, v) := 1 2 (A(u, v) + A(v, u)), 1 ≤ u, v ≤ N, Ds(u, u) := (cid:88) v∈V
As(u, v), 1 ≤ u ≤ N , with Ds(u, v) = 0 for u (cid:54)= v. We capture directional information via a phase matrix,1 Θ(q),
Θ(q)(u, v) := 2πq(A(u, v) − A(v, u)) , q ≥ 0 , where exp(iΘ(q)) is deﬁned component-wise by exp(iΘ(q))(u, v) := exp(iΘ(q)(u, v)). Letting (cid:12) denote component-wise multiplication, we deﬁne the complex Hermitian adjacency matrix H(q) by
H(q) := As (cid:12) exp(iΘ(q)) .
Since Θ(q) is skew-symmetric, H(q) is Hermitian. When q = 0, we have Θ(0) = 0 and so
H(0) = As. This effectively corresponds to treating the graph as undirected. For q (cid:54)= 0, the phase of H(q)(u, v) encodes edge direction and the value H(q)(u, v) separates four possible cases: no edge, edge from u to v, edge from v to u, and undirected edge. If there is no edge, we will have
Hq(u, v) = 0. In the case of a directed edge, the Hermitian adjacency will be complex valued, and changing the direction of an edge will correspond to complex conjugation. For example, in the case where q = .25, if there is an edge from u to v but not from v to u we have
H(.25)(u, v) = i 2
= −H(.25)(v, u) .
Thus, in this setting, an edge from u to v is treated as the opposite of an edge from v to u. On the other hand, if (u, v), (v, u) ∈ E (which can be interpreted as a single undirected edge), then
H(q)(u, v) = H(q)(v, u) = 1, and we see the phase, Θ(q)(u, v) = 0, encodes the lack of direction in the edge. For the rest of this paper, we will assume that q lies in between these two extreme values, i.e., 0 ≤ q ≤ .25. We deﬁne the normalized and unnormalized magnetic Laplacians by
U := Ds−H(q) = Ds−As(cid:12)exp(iΘ(q)), L(q)
L(q) (cid:12)exp(iΘ(q)) . (1) s AsD−1/2
N := I−
D−1/2 (cid:17) (cid:16) s 1Our deﬁnition of Θ(q) coincides with that used in [19]. However, another deﬁnition (differing by a minus sign) also appears in the literature. These resulting magnetic Laplacians have the same eigenvalues and the corresponding eigenvectors are complex conjugates of one another. Therefore, this difference does not affect the performance of our network since our ﬁnal layer separates the real and imaginary parts before multiplying by a trainable weight matrix (see Section 3 for details on the network structure). 3
U and L(q)
N reduce to the standard undirected Laplacians.
Note that when G is undirected, L(q)
L(q)
U and L(q)
N are Hermitian. Theorem 1 (Section 5 of the supplement) shows they are positive-semideﬁnite and thus are diagonalized by an orthonormal basis of complex eigenvectors u1, . . . , uN associated to real, nonnegative eigenvalues λ1, . . . , λN . Similar to the traditional normalized Lapla-cian, Theorem 2 (Section 5 of the supplement) shows the eigenvalues of Lq
N lie in [0, 2], and we may factor L(q)
N = UΛU†, where U is the N × N matrix whose k-th column is uk, Λ is the diagonal matrix with Λ(k, k) = λk, and U† is the conjugate transpose of U (a similar formula holds for
L(q)
U ). Furthermore, recall L = BB(cid:62), where B is the signed incidence matrix. Similarly, Theorem 3 (Section 5 of the supplement) shows that L(q)
U = B(q)(B(q))†, where B(q) is a modiﬁed incidence matrix. The magnetic Laplacian encodes geometric information in its eigenvectors and eigenvalues.
In the directed star graph (Section 6 of the supplement), for example, directional information is contained in the eigenvectors only, whereas the eigenvalues are invariant to the direction of the edges.
On the other hand, for the directed cycle graph the magnetic Laplacian encodes the directed nature of the graph solely in its spectrum. In general, both the eigenvectors and eigenvalues may contain important information, which we leverage in MagNet. 3 MagNet
Most graph neural network architectures can be described as being either spectral or spatial. Spatial networks such as [43, 21, 1, 14] typically extend convolution to graphs by performing a weighted average of features over neighborhoods N (u) = {v : (u, v) ∈ E}. These neighborhoods are well-deﬁned even when E is not symmetric, so spatial methods typically have natural extensions to directed graphs. However, such simplistic extensions may miss important information in the directed graph. For example, ﬁlters deﬁned using N (u) are not capable of assimilating the equally important information contained in {v : (v, u) ∈ E}. Alternatively, these methods may also use the symmetrized adjacency matrix, but they cannot learn to balance directed and undirected approaches.
In this section, we show how to extend spectral methods to directed graphs using the magnetic
Laplacian introduced in Section 2. To highlight the ﬂexibility of our approach, we show how three spectral graph neural network architectures can be adapted to incorporate the magnetic Laplacian.
Our approach is very general, and so for most of this section, we will perform our analysis for a general complex Hermitian, positive semideﬁnite matrix. However, we view the magnetic Laplacian as our primary object of interest (and use it in all of our experiments in Section 5) because of the large body of literature studying its spectral properties and applying it to data science (see Section 4). 3.1 Spectral convolution via the magnetic Laplacian
In this section, we let L denote a Hermitian, positive semideﬁnite matrix, such as the normalized or unnormalized magnetic Laplacian introduced in Section 2, on a directed graph G = (V, E), |V | = N .
We let u1 . . . , uN be an orthonormal basis of eigenvectors for L and let U be the N × N matrix whose k-th column is uk. We deﬁne the directed graph Fourier transform for a signal x : V → C by (cid:98)x = U†x, so that (cid:98)x(k) = (cid:104)x, uk(cid:105) . We regard the eigenvectors u1, . . . , uN as the generalizations of discrete Fourier modes to directed graphs. Since U is unitary, we have the Fourier inversion formula x = U(cid:98)x =
N (cid:88) k=1 (cid:98)x(k)uk . (2)
In Euclidean space, convolution corresponds to pointwise multiplication in the Fourier basis. Thus, we deﬁne the convolution of x with a ﬁlter y in the Fourier domain by (cid:91)y ∗ x(k) = (cid:98)y(k)(cid:98)x(k). By (2), this implies y ∗ x = UDiag((cid:98)y)(cid:98)x = (UDiag((cid:98)y)U†)x, and so we say Y is a convolution matrix if
Y = UΣU† , (3) for a diagonal matrix Σ. This is the natural generalization of the class of convolutions used in [6].
Next, following [13] (see also [22]), we show that a spectral network can be implemented in the spatial domain via polynomials of L by having Σ be a polynomial of Λ in (3). This reduces the number of trainable parameters to prevent overﬁtting, avoids explicit diagonalization of the matrix L, 4
(which is expensive for large graphs), and improves stability to perturbations [27]. As in [13], we deﬁne a normalized eigenvalue matrix, with entries in [−1, 1], by (cid:101)Λ = 2
λmax
Λ − I and assume
Σ =
K (cid:88) k=0
θkTk( (cid:101)Λ) , for some real-valued θ1, . . . , θk, where Tk is the Chebyshev polynomial deﬁned by T0(x) = 1, T1(x) = x, and Tk(x) = 2xTk−1(x) + Tk−2(x) for k ≥ 2. With (U (cid:101)ΛU†)k = U (cid:101)ΛkU†, one has
Yx = U
K (cid:88) k=0
θkTk( (cid:101)Λ)U†x =
K (cid:88) k=0
θkTk( (cid:101)L)x , (4) where, analogous to (cid:101)Λ, we deﬁne (cid:101)L := 2
L − I. It is important to note that, due to the complex
λmax
Hermitian structure of (cid:101)L, the value Yx(u) aggregates information both from the values of x on
Nk(u), the k-hop neighborhood of u, and the values of x on {v : dist(v, u) ≤ k}, which consists of those of vertices that can reach u in k-hops. While in an undirected graph these two sets of vertices are the same, that is not the case for general directed graphs. Furthermore, due to the difference in phase between an edge (u, v) and an edge (v, u), the ﬁlter matrix Y is also capable of aggregating information from these two sets in different ways. This capability is in contrast to any single, symmetric, real-valued matrix, as well as any matrix that encodes just N (u).
To obtain a network similar to [24], we set K = 1, assume that L = L(q) approximation λmax ≈ 2, and set θ1 = −θ0. With this, we obtain
N , using λmax ≤ 2 make the
Yx = θ0(I + (D−1/2 s AsD−1/2 s AsD−1/2
) (cid:12) exp(iΘ(q)))x .
) (cid:12) exp(iΘ(q)) → (cid:101)D−1/2 s
As in [24], we substitute I + (D−1/2 (cid:12) exp(iΘ(q)).
This renormalization helps avoid instabilities arising from vanishing/exploding gradients and yields (cid:101)As (cid:101)D−1/2 s
Yx = θ0 (cid:101)D−1/2 (cid:12) exp(iΘ(q)) , (cid:101)As (cid:101)D−1/2 (5) s s s where (cid:101)As = As + I and (cid:101)Ds(i, i) = (cid:80)
In theory, the matrix exp(iΘ(q)) is dense. However, in practice, one only needs to compute a small fraction of its entries. In most real-world datasets, the symmetrized adjacency matrix will be sparse. Since the Hermitian adjacency matrix is constructed via pointwise multiplication between the symmetrized adjacency matrix and the phase matrix, it is only necessary to compute the phase matrix for entries (u, v) where As(u, v) (cid:54)= 0. Thus, the efﬁciency of the proposed algorithm is comparable to standard GCN algorithms, and can leverage any existing developments such as [18] that increase efﬁciency of standard GCNs (although the computational complexity our method does differ by a factor of four because of the computational complexity of complex-valued multiplication). s j (cid:101)As(i, j). 3.2 The MagNet architecture 1 , . . . x(0)
F0
Let L be the number of convolution layers in our network, and let X(0) be an N × F0 input feature matrix with columns x(0)
. Since our ﬁlters are complex, we use a complex version of ReLU deﬁned by σ(z) = z, if −π/2 ≤ arg(z) < π/2, and σ(z) = 0 otherwise (where arg(z) is the complex argument of z ∈ C). Let F(cid:96) be the number of channels in layer (cid:96), and for 1 ≤ (cid:96) ≤ L, 1 ≤ i ≤ F(cid:96)−1, and 1 ≤ j ≤ F(cid:96), we let Y((cid:96)) ij be a convolution matrix deﬁned in the sense of either (3), (4), or (5). Deﬁne the (cid:96)th layer feature matrix X((cid:96)) with columns x((cid:96)) as: 1 , . . . x((cid:96))
F(cid:96)


F(cid:96)−1 (cid:88)
Y((cid:96)) ij x((cid:96)−1) i

+ b((cid:96)) j
 , (6) x((cid:96)) j = σ i=1 j (v) = b((cid:96)) j ). In matrix form we write X((cid:96)) = Z((cid:96)) (cid:0)X((cid:96)−1)(cid:1), with b((cid:96)) where Z((cid:96)) is a hidden layer of the form (6). In the numerical experiments reported in Section 5, we utilize formulation (4) with L = L(q) j ) = imag(b((cid:96)) and real(b((cid:96)) j
X((cid:96)) = σ (cid:16)
N . In most cases we set K = 1, for which neigh + B((cid:96))(cid:17)
N X((cid:96)−1)W((cid:96)) self + (cid:101)L(q)
X((cid:96)−1)W((cid:96))
, 5
Figure 1: MagNet (L = 2) applied to node classiﬁcation. self and W((cid:96)) where W((cid:96))
B((cid:96))(v, ·) = (b((cid:96)) 1 , . . . , b((cid:96))
F(cid:96)
) for each v ∈ V . neigh are learned weight matrices corresponding to the ﬁlter weights in (4), and
After the convolutional layers, we unwind the complex N × FL matrix X(L) into a real-valued
N × 2FL matrix, apply a linear layer, consisting of right-multiplication by a 2FL × nc weight matrix
W(L+1) (where nc is the number of classes) and apply softmax. In our experiments, we set L = 2 or 3. When L = 2, our network applied to node classiﬁcation, as illustrated in Figure 1, is given by softmax(unwind(Z(2)(Z(1)(X(0))))W(3)) .
For link-prediction, we apply the same method through the unwind layer, and then concatenate the rows corresponding to pairs of nodes to obtain the edge features. 4