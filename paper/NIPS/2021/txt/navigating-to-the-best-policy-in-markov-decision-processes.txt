Abstract
We investigate the classical active pure exploration problem in Markov Decision
Processes, where the agent sequentially selects actions and, from the resulting system trajectory, aims at identifying the best policy as fast as possible. We propose a problem-dependent lower bound on the average number of steps required before
δ. We further provide a correct answer can be given with probability at least 1 the ﬁrst algorithm with an instance-speciﬁc sample complexity in this setting. This algorithm addresses the general case of communicating MDPs; we also propose a variant with a reduced exploration rate (and hence faster convergence) under an additional ergodicity assumption. This work extends previous results relative to the generative setting [MP21], where the agent could at each step query the random outcome of any (state, action) pair. In contrast, we show here how to deal with the navigation constraints, induced by the online setting. Our analysis relies on an ergodic theorem for non-homogeneous Markov chains which we consider of wide interest in the analysis of Markov Decision Processes.
− 1

Introduction
Somewhat surprisingly, learning in a Markov Decision Process is most often considered under the performance criteria of consistency or regret minimization (see e.g. [SB18, Sze10, LS20] and references therein). Regret minimization (see e.g. [AJO09, FCG10]) is particularly relevant when the rewards accumulated during the learning phase are important. This is however not always the case: for example, when learning a game (whether Go, chess, Atari, or whatever), winning or losing during the learning phase does not really matter. One may intuitively think that sometimes getting into difﬁculty on purpose so as to observe unheard situations can signiﬁcantly accelerate the learning process. Another example is the training of robot prototypes in the factory: a reasonably good policy is ﬁrst searched, regardless of the losses incurred, that can serve as an initialization for a second phase regret-minimization mode that starts when the robot is deployed. It is hence also of great practical importance to study the sample complexity of learning, and to work on strategies that might improve, in this perspective, on regret minimizing algorithms.
In this work, we are interested in the best policy identiﬁcation (BPI) problem for inﬁnite-horizon discounted MDPs. This framework was introduced by [Fie94] under the name of PAC-RL. In BPI the algorithm explores the MDP until it has gathered enough samples to return an ε-optimal policy
δ. Crucially, the algorithm halts at a random time step, determined by a with probability at least 1
− 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
stopping rule which guarantees that the probability of returning a wrong answer is less that δ. The optimality of BPI algorithms is measured through their sample complexity, deﬁned as their expected stopping time. Best policy identiﬁcation in MDPs has been mostly investigated under the lens of minimax-optimality. The minimax framework may be overly pessimistic by accounting for the worst possible MDP, whereas an algorithm with instance-speciﬁc guarantees can adapt its exploration procedure to the hardness of the MDP instance that it faces. Recently, a few works made the ﬁrst attempts towards understanding the instance-speciﬁc sample complexity of reinforcement learning.
However, they typically either make simplifying assumptions such as access to a generative model
[ZKB19, MP21], or restrict their attention to episodic MDPs [WSJ21]. In practice however, the samples gathered rather correspond to a single, possibly inﬁnite, trajectory of the system that we wish to control. This motivates our study of the full online setting where observations are only obtained by navigating in the MDP, that is by sequential choices of actions and following the transitions of the
MDP.
Our contributions. [MP21] recently proposed an information-theoretical complexity analysis for
MDPs in the case of access to a generative model. Here we extend their results to the online setting.
Our main goal is to understand how the online learning scheme affects the sample complexity compared to the easier case where we have a generative model. A natural ﬁrst step consists in understanding how the ﬁrst order term T � log(1/δ) changes. Thus we only focus on the asymptotic regime δ 0. Our key contributions can be summarized as follows:
→
First, we adapt the lower bound of [MP21] to the online setting (Proposition 2). The new bound also writes as the value of a zero-sum two-player game between nature and the algorithm, where the loss function remains the same, but where the set of possible strategies 1. We for the algorithm is restricted to a subset Ω( refer to the constraints deﬁning Ω(
) of the simplex of dimension SA
) as the navigation constraints.
M
−
M
→
Nsa(t)/t s,a converges to some target oracle allocation ω�
We propose MDP-NaS, the ﬁrst algorithm for the online setting1 with instance-dependent bounds on its sample complexity in the asymptotic regime δ 0 (Theorem 6). A major challenge lies in the design of a sampling rule that guarantees that the sampling frequency
). of state-action pairs
M
Indeed, since we can no longer choose the next state of the agent, the tracking procedure which was developed by [GK16] for multi-armed bandits and used in [MP21] for MDPs with a generative model can no longer be applied in our setting. We propose a new sampling rule which performs exploration according to a mixture of the uniform policy and a plug-in estimate of the oracle policy (the policy whose stationary state-action distribution is
ω�) and prove that it satisﬁes the requirement above. The analysis of our sampling rule relies on an ergodic theorem for non-homogeneous Markov chains of independent interest (Proposition 12).
Ω(
∈
�
�
We investigate, depending on the communication properties of the ground-truth instance
, what is the minimal forced-exploration rate in our sampling rule that guarantees the
M is consistency of the plug-in estimator of the oracle policy. Our ﬁndings imply that when ergodic, an exploration rate as low as 1/√t is sufﬁcient. However, when is only assumed to be communicating, one is obliged to resort to a much more conservative exploration rate of t− 1 in m+1 , where m is a parameter deﬁned in Lemma 4 that may scales as large as S the worst case.
M
M
− 1
Finally, our stopping rule represents the ﬁrst implementation of the Generalized Likelihood
Ratio test for MDPs. Notably, we circumvent the need to solve the max-min program of the lower bound exactly, and show how an upper bound of the best-response problem, such as the one derived in [MP21], can be used to perform a GLR test. This improves upon the sample complexity bound that one obtains using the KL-Ball stopping rule of [MP21] by at least a factor of 22.
•
•
•
• 1Before publication of this work, but after a preprint was available online, [WSJ21] proposed another algorithm with instance-dependent guarantees. 2Note that the stopping rule is independent of the sampling rule and thus can be used in both the generative and the online settings. Furthermore, one may even obtain an improved factor of 4 by using a deviation inequality for the full distribution of (reward, next-state) instead of a union bound of deviation inequalities for each marginal distribution. 2
1.1