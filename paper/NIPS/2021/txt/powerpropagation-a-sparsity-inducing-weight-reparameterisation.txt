Abstract
The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a “rich get richer” dynamic, leaving low-magnitude parameters largely unaffected by learning.
Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely.
Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its ver-satility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representa-tions allow accommodating a large number of tasks at ﬁxed model capacity. In all cases our reparameterisation considerably increases the efﬁcacy of the off-the-shelf methods. 1

Introduction
Deep learning models are emerging as the dominant choice across several domains, from language [e.g. 1, 2] to vision [e.g. 3, 4] to RL [e.g. 5, 6]. One particular characteristic of these architectures is that they perform optimally in the overparameterised regime. In fact, their size seems to be mostly limited by hardware constraints. While this is potentially counter-intuitive, given a classical view on overﬁtting, the current understanding is that model size tends to have a dual role: It leads to better behaved loss surfaces, making optimisation easy, but also acts as a regulariser. This gives rise to the double descent phenomena, where test error initially behaves as expected, growing with model size due to overﬁtting, but then decreases again as the model keeps growing, and asymptotes as the model size goes to inﬁnity to a better performance than obtained in the classical regime [7, 8]. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
As a consequence, many of the state of the art models tend to be prohibitively large, making them inaccessible to large portions of the research community despite scale still being a driving force in obtaining better performance. To address the high computational cost of inference, a growing body of work has been exploring ways to compress these models. As highlighted by several works [e.g. 9, 10, 11], size is only used as a crutch during the optimisation process, while the ﬁnal solution requires a fraction of the capacity of the model. A typical approach therefore is to sparsify or prune the neural network after training by eliminating parameters that do not play a vital role in the functional behaviour of the model. Furthermore, there is a growing interest in sparse training [e.g. 12, 13, 14], where the model is regularly pruned or sparsiﬁed during training in order to reduce the computational burden.
Compressed or sparse representations are not merely useful to reduce computation. Continual learning, for example, focuses on learning algorithms that operate on non-iid data [e.g. 15, 16, 17].
In this setup, training proceeds sequentially on a set of tasks. The system is expected to accelerate learning on subsequent tasks as well as using newly acquired knowledge to potentially improve on previous problems, all of this while maintaining low memory and computational footprint. This is difﬁcult due to the well studied problem of catastrophic forgetting, where performance on previous tasks deteriorates rapidly when new knowledge is incorporated. Many approaches to this problem require identifying the underlying set of parameters needed to encode the solution to a task in order to freeze or protect them via some form of regularisation. In such a scenario, given constraints on model size, computation and memory, it is advantageous that each learned task occupies as little capacity as possible.
In both scenarios, typically, the share of capacity needed to encode the solution is determined by the learning process itself, with no explicit force to impose frugality. This is in contrast with earlier works on L0 regularisation that explicitly restrict the learning process to result in sparse and compressible representations [18]. The focus of our work, similar to the L0 literature, is on how to encourage the learning process to be frugal in terms of parameter usage. However instead of achieving this by adding an explicit penalty, we enhance the “rich get richer” nature of gradient descent. In particular we propose a new parameterisation that ensures steps taken by gradient descent are proportional to the magnitude of the parameters. In other words, parameters with larger magnitudes are allowed to adapt faster in order to represent the required features to solve the task, while smaller magnitude parameters are restricted, making it more likely that they will be irrelevant in representing the learned solution. 2 Powerpropagation
The desired proportionality of updates to weight magnitudes can be achieved in a surprisingly simple fashion: In the forward pass of a neural networks, raise the parameters of your model (element-wise) to the α-th power (where α > 1) while preserving the sign. It is easy to see that due to the chain rule of calculus the magnitude of the parameters (raised to α − 1) will appear in the gradient computation, scaling the usual update. Therefore, small magnitude parameters receive smaller gradient updates, while larger magnitude parameters receive larger updates, leading to the aforementioned “rich get richer” phenomenon. This simple intuition leads to the name of our method.
More formally, we enforce sparsity through an implicit regulariser that results from reparameterising the model similar to [19, 20]. This line of research builds on previous work on matrix factorisation
[e.g. 21, 22]. In [19] a parameterisation of the form w = v (cid:12) v − u (cid:12) u is used to induce sparsity, where (cid:12) stands for element-wise multiplication and we need both v and u to be able to represent negative values, since the parameters are effectively squared. In our work we rely on a simpler formulation where w = v|v|α−1, for any arbitrary power α ≥ 1 (as compared to ﬁxing α to 2), which, since we preserved the sign of v, can represent both negative and positive values. For α = 1 this recovers the standard setting.
If we denote by Θ = RM the original parameter space or manifold embedded in RM , our reparam-eterisation can be understood through an invertible map Ψ, where its inverse Ψ−1 projects θ ∈ Θ into φ ∈ Φ, where Φ is a new parameter space also embedded in RM , i.e. Φ = RM . The map is deﬁned by applying the function Ψ : R → R, Ψ(x) = x|x|α−1 element-wise, where by abuse of notation we refer to both the vector and element level function by Ψ. This new parameter space or manifold Φ has a curvature (or metric) that depends on the Jacobian of Ψ. Similar constructions have 2
been previously used in optimisation, as for example in the case of the widely known Mirror Descent algorithm [23], where the invertible map Ψ is the link function. For deep learning, Natural Neural
Networks [24] rely on a reparameterisation of the model such that in the new parameter space, at least initially, the curvature is close to the identity matrix, making a gradient descent step similar to a second order step. Warp Gradient Descent [25] relies on a meta-learning framework to learn a nonlinear projection of the parameters with a similar focus of improving efﬁciency of learning. In contrast, our focus is to construct a parameterisation that leads to an implicit regularisation towards sparse representation, following [19], rather then improving convergence.
Given the form of our mapping Ψ, in the new parameterisation the original weight θi will be replaced by φi, where Ψ(φi) = φi|φi|α−1 = θi and i indexes over the dimensionality of the parameters. Note that we apply this transformation only to the weights of a neural network, leaving other parameters untouched. Given the reparameterised loss L(·, Ψ(φ)), the gradient wrt. to φ becomes
∂L(·, Ψ(φ))
∂φ
=
∂L
∂Ψ(φ)
∂Ψ(φ)
∂φ
=
∂L
∂Ψ(φ) diag(α|φ|◦α−1). (1)
∂L
Note that diag indicates a diagonal matrix, and |φ|◦α−1 indicates raising element-wise the entries
∂Ψ(φ) is the derivative wrt. to the original weight θ = φ|φ|◦α−1 of vector |φ| to the power α − 1. which is the gradient in the original parameterisation of the model. This is additionally multiplied (element-wise) by the factor α|φi|α−1, which will scale the step taken proportionally to the magnitude of each entry. Finally, for clarity, this update is different from simply scaling the gradients in the original parameterisation by the magnitude of the parameter (raised at α − 1), since the update is applied to φ not θ, and is scaled by φ. The update rule (1) has the following properties: (i) 0 is a critical point for the dynamics of any weight φi, if α > 1. This is easy to see as
∂L
∂φi
= 0 whenever φi = 0 due to the α|φi|α−1 factor. (ii) In addition, 0 is surrounded by a plateau and hence weights are less likely to change sign (gradients become vanishingly small in the neighbourhood of 0 due to the scaling). This should negatively affect initialisations that allow for both negative and positive values, but it might have bigger implications for biases. (iii) This update is naturally obtained by the Backpropagation algorithm. This comes from the fact that Backpropagation implies applying the chain-rule from the output towards the variable of interest, and our reparameterisation simply adds another composition (step) in the chain before the variable of interest.
At this point the perceptive reader might be concerned about the effect of equation (1) on established practises in the training of deep neural networks. Firstly, an important aspect of reliable training is the initialisation ([e.g. 26, 27, 28]) or even normalisation layers such as batch-norm [29] or layer-norm [30]. We argue that our reparameterisation preserves all properties of typical initialisation schemes as it does not change the function. Speciﬁcally, let θi ∼ p(θ) where p(θ) is any distribution of choice. Then our reparameterisation involves initialising φi ← sign(θi) · α(cid:112)|θi|, ensuring the neural network and all intermediary layers are functionally the same. This implies that hidden activations will have similar variance and mean as in the original parameterisation, which is what initialisation and normalisation focus on.
Secondly, one valid question is the impact of modern optimisation algorithms ([e.g. 31, 32, 33]) on our reparameterisation. These approaches correct the gradient step by some approximation of the curvature, typically given by the square root of a running average of squared gradients. This quantity will be proportional (at least approximately) to diag(α|φ|◦α−1)1. This reﬂects the fact that our projection relies on making the space more curved and implicitly optimisation harder, which is what these optimisation algorithms aim to ﬁx. Therefore, a naive use with Powerprop. would result in a reduction of the “rich get richer” effect. On the other hand, avoiding such optimisers completely can considerably harm convergence and performance. The reason for this is that they do not only 1To see this assume the weights do not change from iteration to iteration. Then each gradient is scaled by the same value diag(α|φ|◦2(α−1)) which factors out in the summation of gradients squared, hence the correction from the optimiser will undo this scaling. In practice φ changes over time, though slowly, hence approximately this will still hold. 3
(a) Sparsity Performance (b) Mask overlap (c) Baseline (d) α = 4.0
Figure 1: Powerpropagation applied to Image classiﬁcation. (a) Test accuracy at increasing levels of sparsity for MNIST (b) Overlap between masks computed before and after training (c) & (d) Analysis of weight distributions for a Baseline model and at a high α. We use 10K weights chosen at random from the network. For a) & b) we show mean and standard deviation over 5 runs. We provide code to reproduce the MNIST results (a) in the accompanying notebook. correct for the curvature induced by our reparameterisation, but also the intrinsic curvature of the problem being solved. Removing the second effect can make optimisation very difﬁcult. We provide empirical evidence of this effect in the Appendix.
To mitigate this issue and make Powerprop. straightforward to use in any setting, we take inspiration from the target propagation literature [34, 35, 36, 37] which proposes an alternative way of thinking about the Backpropagation algorithm.
In our case, we pretend that the exponentiated parameters are the de-facto parameters of the model and compute an update wrt. to them using our optimiser of choice. The updated exponentiated parameters are then seen as a virtual target, and we take a gradient descent step on φ towards these virtual targets.
This will result in a descent step, which, while it relies on modern optimisers to correct for the curva-ture of the problem, does not correct for the curvature introduced by our parameterisation. Namely if we denote optim : RM → RM as the function that implements the correction to the gradient done by some modern optimiser, our update becomes ∆φ = optim diag(α|φ|◦α−1). (cid:17) (cid:16) ∂L
∂Ψ(φ)
The proof that our update is indeed correct follows the typical steps taken in the target propagation literature. From a ﬁrst order Taylor expansion of L(φ − η∆φ), we have that in order for ∆φ to reduce the loss, the following needs to hold: (cid:104)∆φ, ∂L
> 0 as this was a valid step on Ψ(φ). Because diag(α|φ|◦α−1) is positive deﬁnite (diagonal matrix with all entries positive), we can multiply it on both sides, proving that (cid:104)∆φ, ∂L
∂φ (cid:105) > 0. We provide more details in the Appendix. We will rely on this formulation in our empirical evaluation.
∂φ (cid:105) > 0. But we know that (cid:16) ∂L
∂Ψ(φ)
∂L
∂Ψ(φ) optim (cid:68) (cid:69) (cid:17)
, 3 Effect on weight distribution and sparsiﬁcation
At this point an empirical demonstration might illuminate the effect of equation (1) on model parameters and our ability to sparsify them. Throughout this work, we will present results from neural networks after the removal of low-magnitude weights. We prune such parameters by magnitude (i.e. min |θi|), following current best practice [e.g. 38, 39, 13]. This is based on a Taylor expansion argument [14] of a sparsely-parameterised function f (x, θs) which we would like to approximate its dense counterpart f (x, θ): f (x, θs) ≈ f (θ, x) + gT (θs − θ) + 1 2 (θs − θ)T H(θs − θ) + ... where g is the gradient vector and H the Hessian. As higher order derivatives are impractical to compute for modern networks, minimising the norm of (θs − θ) is a practical choice instead.
Following the experimental setup in [10] we study the effect of Powerpropagation at different powers (α) relative to standard Backpropagation with otherwise identical settings. Figure 1a shows the effect of increasing sparsity on the layerwise magnitude-pruning setting for LeNet [40] on MNIST [41].
In both cases we notice a signiﬁcant improvement over an otherwise equivalent baseline. While the choice of α does inﬂuence results, all choices lead to an improvement2 in the MNIST setting.
Where does this improvement come from? Figures 1c & 1d compare weights before and after training 2For numerical stability reasons we typically suggest a choice of α ∈ (1, 3] depending on the problem. 4
on MNIST at identical initialisation. We prune the network to 90% and compare the respective weight distributions. Three distinct differences are particularly noticeable: (i) Most importantly, weights initialised close to zero are signiﬁcantly less likely to survive the pruning process when
Powerpropagation is applied (see green Kernel density estimate). (ii) Powerpropagation leads to a heavy-tailed distribution of trained weights, as (1) ampliﬁes the magnitude of such values. These observations are what we refer to as the “rich-get-richer” dynamic of Powerpropagation. (iii) Weights are less likely to change sign (see Figure 1d), as mentioned in Section 2.
One possible concern of (i) and (ii) are that Powerpropagation leads to training procedures where small weights cannot escape pruning, i.e. masks computed at initialisation and convergence are identical. This is undesirable as it is well established that pruning at initialisation is inferior [e.g. 11, 42, 43, 39]. To investigate this we plot the overlap between these masks at different pruning thresholds in Figure 1b. While overlap does increase with α, at no point do we observe an inability of small weights to escape pruning, alleviating this concern. Code for this motivational example on
MNIST is provided. 3 4 Powerpropagation for Continual Learning
While algorithms for neural network sparsity are well established as a means to reduce training and inference time, we now formalise our argument that such advances can also lead to signiﬁcant improvements in the continual learning setting: the sequential learning of tasks without forgetting.
As this is an inherently resource constraint problem, we argue that many existing algorithms in the literature can be understood as implementing explicit or implicit forms of sparsity. One class of examples are based on weight-space regularisation [e.g. 17, 44] which can be understood as compressing the knowledge of a speciﬁc task to a small set of parameters that are forced to remain close to their optimal values. Experience Replay and Coreset approaches [e.g. 45, 46, 47] on the other hand compress data from previous tasks to optimal sparse subsets. The class of methods on which we will base the use of Powerpropagation for Continual Learning on implement gradient sparsity
[e.g. 48, 49], i.e. they overcome catastrophic forgetting by explicitly masking gradients to parameters found to constitute the solution to previous tasks.
In particular, let us examine PackNet [48] as a representative of such algorithms. Its underlying principle is simple yet effective: Identify the subnetwork for each task through (iterative) pruning to a ﬁxed budget, then ﬁx the solution for each task by explicitly storing a mask at task switches and protect each such subnetwork by masking gradients from future tasks (using a backward Mask
Mb). Given a pre-deﬁned number of tasks T , PackNet reserves 1/T of the weights. Self-evidently, this procedure beneﬁts from networks that maintain high performance at increased sparsity, which becomes particularly important for large T . Thus, the application of improved sparsity algorithms such as Powerpropagation are a natural choice. Moreover, PackNet has the attractive property of merely requiring the storage of a binary mask per task, which comes at a cost of 1 bit per parameter, in stark contrast to methods involving the expensive storage (or generative modelling) of data for each past task.
Nevertheless, the approach suffers from its assumption of a known number of maximum tasks T and its possibly inefﬁcient resource allocation: By reserving a ﬁxed fraction of weights for each task, no distinction is made in terms of difﬁculty or relatedness to previous data. We overcome both issues through simple yet effective modiﬁcations resulting in a method we term EfﬁcientPacknet (EPN), shown in Algorithm 1. The key steps common to both methods are (i) Task switch (Line 3), (ii)
Training through gradient masking (Line 4), (iii) Pruning (Line 8) , (iv) Updates to the backward mask needed to implement gradient sparsity.
Improvements of EPN upon PackNet are: (a) Lines 7-11: We propose a simple search over a range of sparsity rates S = [s1, . . . , sn], terminating the search once the sparse model’s performance falls short of a minimum accepted target performance γP (computed on a held-out validation set) or once a minimal acceptable sparsity is reached. While the choice of γ may appear difﬁcult at
ﬁrst, we argue that an maximal acceptable loss in performance is often a natural requirement of a practical engineering application. In addition, in cases where the sparsity rates are difﬁcult to set, a computationally efﬁcient binary search (for γP ) up to a ﬁxed number of steps can be performed instead. Finally, in smaller models the cost of this step may be reduced further by instead using 3https://github.com/deepmind/deepmind-research/tree/master/powerpropagation 5
Algorithm 1: EfﬁcientPackNet (EPN) + Powerpropagation.
Require : T tasks [(X1, y1), . . . , (XT , yT )]; Loss & Eval. functions L, E; Initial weight distribution p(u) (e.g. Uniform); α (for
Powerprop.); Target performance γ ∈ [0, 1]; Sparsity rates S = [s1, . . . , sn] where si+1 > si and si ∈ [0, 1).
Output : Trained model φ; Task-speciﬁc Masks {Mt} i ← 1 ∀i // Backward mask 1 Mb 2 φi ← sign(θ) · α(cid:112)|θi|; θi ∼ p(θ) // Initialise parameters 3 for t ∈ [1, . . . , T ] do 4
φ ← arg minφ L(Xt, yt, φ, Mb) // Train on task t with explicit gradient masking through Mb
P ← E(Xt, yt, φ)// Validation performance of dense model on task t l ← n do
// TopK(x, K) returns the indices of the K largest elements in a vector x (cid:26) 1 0 i =
Mt if i ∈ TopK(φ, (cid:98)sl · dim(φ)(cid:99)) otherwise
Ps ← E(XT , yT , φ (cid:12) Mt) // Validation performance of sparse model l ← l − 1 (cid:27)
// Find new Forward mask at sparsity sl
Mb ← ¬ while Ps > γP ∧ l ≥ 1; t (cid:87) i=1
Re-initialise pruned weights
// Optionally retrain with masked weights φ (cid:12) Mt on Xt, yt before task switch
Mt// Update backward mask to protect all tasks 1, ..., t 5 6 7 8 9 10 11 12 13 14 end an error estimate based on a Taylor expansion [50]. This helps overcome the inefﬁcient resource allocation of PackNet. (b) Line 8: More subtly, we choose the mask for a certain task among all network parameters, including ones used by previous tasks, thus encouraging reuse to existing parameters. PackNet instead forces the use of a ﬁxed number of new parameters, thus possibly requiring more parameters than needed. Thus, the fraction of newly masked weights per task is adaptive to task complexity. (c) Line 13: We re-initialise the weights as opposed to leaving them at zero, due to the critical point property (Section 2). While we could leave the weight at their previous value, we found this to lead to slightly worse performance.
Together, these changes make the algorithm more suitable for long sequences of tasks, as we will show in our experiments. In addition, they overcome the assumption of an a priori known number of tasks T . Another beneﬁcial property of the algorithm is that as the backward pass becomes increasingly sparse, the method becomes more computationally efﬁcient with larger T . A possible concern for the method presented thus far is the requirement of known task ids at inference time, which are needed to select the correct mask for inputs. We present an algorithm to address this concern in the supplementary material. 5