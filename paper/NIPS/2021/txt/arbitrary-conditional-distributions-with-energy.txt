Abstract
Modeling distributions of covariates, or density estimation, is a core challenge in unsupervised learning. However, the majority of work only considers the joint distribution, which has limited utility in practical situations. A more general and useful problem is arbitrary conditional density estimation, which aims to model any possible conditional distribution over a set of covariates, reﬂecting the more realistic setting of inference based on prior knowledge. We propose a novel method,
Arbitrary Conditioning with Energy (ACE), that can simultaneously estimate the xo) for all possible subsets of unobserved features xu and distribution p(xu | observed features xo. ACE is designed to avoid unnecessary bias and complexity
— we specify densities with a highly expressive energy function and reduce the problem to only learning one-dimensional conditionals (from which more complex distributions can be recovered during inference). This results in an approach that is both simpler and higher-performing than prior methods. We show that ACE achieves state-of-the-art for arbitrary conditional likelihood estimation and data imputation on standard benchmarks. 1

Introduction
Density estimation, a core challenge in machine learning, attempts to learn the probability density of some random variables given samples from their true distribution. The vast majority of work on density estimation focuses on the joint distribution p(x) [10, 4, 25, 11, 23, 22, 6], i.e., the distribution of all variables taken together. While the joint distribution can be useful (e.g., to learn the distribution of pixel conﬁgurations that represent human faces), it is limited in the types of predictions it can make. We are often more interested in conditional probabilities, which communicate the likelihood of an event given some prior information. For example, given a patient’s medical history and symptoms, a doctor determines the likelihoods of different illnesses and other patient attributes. Conditional distributions are often more practical since real-world decisions are nearly always informed by prior information.
However, we often do not know ahead of time which conditional distribution (or distributions) will be of interest. That is, we may not know which features will be known (observed) or which will be inferred (unobserved). For example, not every patient that a doctor sees will have had the same tests performed (i.e., have the same observed features). A naïve approach requires building an exponential number of models to cover all possible cases (one for every conditional distribution), which quickly becomes intractable. Thus, an intelligent system needs to understand the intricate conditional dependencies between all arbitrary subsets of covariates, and it must do so with a single model to be practical.
In this work, we consider the problem of learning the conditional distribution p(xu |
| and observed variables xo ∈ arbitrary subsets of unobserved variables xu ∈ u, o
∩ xo) for any
R| o
|, where
. We propose a method, Arbitrary Conditioning with Energy (ACE),
∅ 1, . . . , d and o
R| u u =
⊆ {
} 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
that can assess any conditional distribution over any subset of random variables, using a single model. ACE is developed with an eye for simplicity — we reduce the arbitrary conditioning problem to the estimation of one-dimensional conditional densities (with arbitrary observations), and we represent densities with an energy function, which fully speciﬁes unnormalized distributions and has the freedom to be instantiated as any arbitrary neural network.
We evaluate ACE on benchmark datasets and show that it outperforms current methods for arbitrary conditional/marginal density estimation. ACE remains effective when trained on data with missing values, making it applicable to real-world datasets that are often incomplete, and we ﬁnd that ACE scales well to high-dimensional data. Also, unlike some prior methods (e.g., [20]), ACE can naturally model data with both continuous and discrete values.
Our contributions are as follows: 1) We develop the ﬁrst energy-based approach to arbitrary con-ditional density estimation, which eliminates restrictive biases (e.g. normalizing ﬂows, Gaussian mixtures) imposed by common alternatives. 2) We empirically demonstrate that ACE is state-of-the-art for arbitrary conditional density estimation and data imputation. 3) We ﬁnd that complicated prior approaches can be easily outperformed with a simple scheme that uses mixtures of Gaussians and fully-connected networks. 2 Previous Work
Several methods have been previously proposed for arbitrary conditioning. Sum-Product Networks are specially designed to only contain sum and product operations and can produce arbitrary conditional or marginal likelihoods [27, 3]. The Universal Marginalizer trains a neural network with a cross-entropy loss to approximate the marginal posterior distributions of all unobserved features conditioned on the observed ones [5]. VAEAC is an approach that extends a conditional variational autoencoder by only considering the latent codes of unobserved dimensions [14], and NeuralConditioner uses adversarial training to learn each conditional distribution [1]. DMFA uses factor analysis to have a neural network output the parameters of a conditional Gaussian density for the missing features given the observed ones [28]. The current state-of-the-art is ACFlow, which extends normalizing ﬂow models to handle any subset of observed features [20].
Unlike VAEAC, ACE does not suffer from mode collapse or blurry samples. ACE is also able to provide likelihood estimates, unlike NeuralConditioner which only produces samples. DMFA is limited to learning multivariate Gaussians, which impose bias and are harder to model than one-dimensional conditionals. While ACFlow can analytically produce normalized likelihoods and samples, it is restricted by a requirement that its network consist of bijective transformations with tractable Jacobian determinants. Similarly, Sum-Product Networks have limited expressivity due to their constraints. ACE, on the other hand, exempliﬁes the appeal of energy-based methods as it has no constraints on the parameterization of the energy function.
Energy-based methods have a wide range of applications within machine learning [17], and recent work has studied their utility for density estimation. Deep energy estimator networks [30] and
Autoregressive Energy Machines [22] are both energy-based models that perform density estimation.
However, both of these methods are only able to estimate the joint distribution.
Much of the previous work on density estimation relies on an autoregressive decomposition of the joint density according to the chain rule of probability. Often, the model only considers a single (arbitrary) ordering of the features [22, 24]. Uria et al. [33] proposed a method for assessing joint likelihoods based on any arbitrary ordering, where they use masked network inputs to effectively share weights between a combinatorial number of models. Germain et al. [8] also consider a shared network for joint density estimation with a constrained architecture that enforces the autoregressive constraint in joint likelihoods. In this work, we construct an order-agnostic weight-sharing technique not for joint likelihoods, but for arbitrary conditioning. Moreover, we make use of our weight-sharing scheme to estimate likelihoods with an energy based approach, which avoids the limitations of the parametric families used previously (e.g., mixtures of Gaussians [33], or Bernoullis [8]). Query
Training [16] is a method for answering probabilistic queries. It also takes the approach of computing one-dimensional conditional likelihoods but does not directly pursue an autoregressive extension of that ability. 2
The problem of imputing missing data has been well studied, and there are several approaches based on classic machine learning techniques such as k-nearest neighbors [32], random forests [31], and autoencoders [9]. More recent work has turned to deep generative models. GAIN is a generative adversarial network (GAN) that produces imputations with the generator and uses the discriminator to discern the imputed features [34]. Another GAN-based approach is MisGAN, which learns two generators to model the data and masks separately [19]. MIWAE adapts variational autoencoders by modifying the lower bound for missing data and produces imputations with importance sampling
[21]. ACFlow can also perform imputation and is state-of-the-art for imputing data that are missing completely at random (MCAR) [20].
While it is not always the case that data are missing at random, the opposite case (i.e., missingness that depends on unobserved features’ values) can be much more challenging to deal with [7]. Like many data imputation methods, we focus on the scenario where data are MCAR, that is, where the likelihood of being missing is independent of the covariates’ values. 3