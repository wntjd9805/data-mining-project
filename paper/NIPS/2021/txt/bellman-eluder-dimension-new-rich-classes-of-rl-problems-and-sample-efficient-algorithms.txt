Abstract
Finding the minimal structural assumptions that empower sample-efﬁcient learn-ing is one of the most important research directions in Reinforcement Learning (RL). This paper advances our understanding of this fundamental question by in-troducing a new complexity measure—Bellman Eluder (BE) dimension. We show that the family of RL problems of low BE dimension is remarkably rich, which subsumes a vast majority of existing tractable RL problems including but not lim-ited to tabular MDPs, linear MDPs, reactive POMDPs, low Bellman rank prob-lems as well as low Eluder dimension problems. This paper further designs a new optimization-based algorithm—GOLF, and reanalyzes a hypothesis elimination-based algorithm—OLIVE [proposed in 1]. We prove that both algorithms learn the near-optimal policies of low BE dimension problems in a number of sam-ples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret and sample complexity results match or improve the best existing results for several well-known subclasses of low BE dimension problems. 1

Introduction
Modern Reinforcement Learning (RL) commonly engages practical problems with an enormous number of states, where function approximation must be deployed to approximate the true value function using functions from a prespeciﬁed function class. Function approximation, especially based on deep neural networks, lies at the heart of the recent practical successes of RL in domains such as Atari [2], Go [3], robotics [4], and dialogue systems [5].
Despite its empirical success, RL with function approximation raises a new series of theoretical challenges when comparing to the classic tabular RL: (1) generalization, to generalize knowledge from the visited states to the unvisited states due to the enormous state space. (2) limited expressive-ness, to handle the complicated issues where true value functions or intermediate steps computed in the algorithm can be functions outside the prespeciﬁed function class. (3) exploration, to address the tradeoff between exploration and exploitation when above challenges are present.
Consequently, most existing theoretical results on efﬁcient RL with function approximation rely on relatively strong structural assumptions. For instance, many require that the MDP admits a linear approximation [6–8], or that the model is precisely Linear Quadratic Regulator (LQR) [9–11]. Most of these structural assumptions rarely hold in practical applications. This naturally leads to one of the most fundamental questions in RL. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: A schematic summarizing relations among families of RL problems1
What are the minimal structural assumptions that empower sample-efﬁcient RL?
We advance our understanding of this grand question via the following two steps: (1) identify a rich class of RL problems (with weak structural assumptions) that cover many practical applications of interests; (2) design sample-efﬁcient algorithms that provably learn any RL problem in this class.
The attempts to ﬁnd weak or minimal structural assumptions that allow statistical learning can be traced in supervised learning where VC dimension [12] or Rademacher complexity [13] is proposed, or in online learning where Littlestone dimension [14] or sequential Rademacher complexity [15] is developed.
In the area of reinforcement learning, there are two intriguing lines of recent works that have made signiﬁcant progress in this direction. To begin with, Jiang et al. [1] introduces a generic complexity notion—Bellman rank, which can be proved small for many RL problems including linear MDPs
[7], reactive POMDPs [16], etc. [1] further propose an hypothesis elimination-based algorithm—
OLIVE for sample-efﬁcient learning of problems with low Bellman rank. On the other hand, recent work by Wang et al. [17] considers general function approximation with low Eluder dimension [18], and designs a UCB-style algorithm with regret guarantee. Noticeably, generalized linear MDPs [6] and kernel MDPs (see Appendix C) are subclasses of low Eluder dimension problems, but not low
Bellman rank.
In this paper, we make the following three contributions.
• We introduce a new complexity measure for RL—Bellman Eluder (BE) dimension. We prove that the family of RL problems of low BE dimension is remarkably rich, which subsumes both low Bellman rank problems and low Eluder dimension problems—two arguably most generic tractable function classes so far in the literature (see Figure 1).
The family of low BE dimension further includes new problems such as kernel reactive
POMDPs (see Appendix C) which were not known to be sample-efﬁciently learnable.
• We design a new optimization-based algorithm—GOLF, which provably learns near-optimal policies of low BE dimension problems in a number of samples that is polynomial in all relevant parameters, but independent of the size of state-action space. Our regret or sample complexity guarantees match [8] which is minimax optimal when speciﬁed to the linear setting. Our rates further improve upon [1, 17] in low Bellman rank and low Eluder dimension settings, respectively.
• We reanalyze the hypothesis elimination based algorithm—OLIVE proposed in [1]. We show it can also learn RL problems with low BE dimension sample-efﬁciently, under slightly weaker assumptions but with worse sample complexity comparing to GOLF. 1.1