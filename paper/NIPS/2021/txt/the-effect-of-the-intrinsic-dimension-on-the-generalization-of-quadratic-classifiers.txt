Abstract
It has been recently observed that neural networks, unlike kernel methods, enjoy a reduced sample complexity when the distribution is isotropic (i.e., when the covariance matrix is the identity). We ﬁnd that this sensitivity to the data distribution is not exclusive to neural networks, and the same phenomenon can be observed on the class of quadratic classiﬁers (i.e., the sign of a quadratic polynomial) with a nuclear-norm constraint. We demonstrate this by deriving an upper bound on the Rademacher Complexity that depends on two key quantities: (i) the intrinsic dimension, which is a measure of isotropy, and (ii) the largest eigenvalue of the second moment (covariance) matrix of the distribution. Our result improves the dependence on the dimension over the best previously known bound and precisely quantiﬁes the relation between the sample complexity and the level of isotropy of the distribution. 1

Introduction
We revisit the problem of supervised classiﬁcation using quadratic features of the data. We do so to highlight the inﬂuence of properties of data distrbution on the generalization error. Most of the existing results on this error only use a bound on the support of the distribution. By leveraging results from matrix concentration, we show an improved bound that uses more reﬁned properties of the data distribution, like the second moment matrix.
The use of the second moment matrix in the error bound shows that the intrinsic dimension of the data distribution plays an important role. This is of particular interest because it is widely believed that real-world data distributions have nice properties that allow classiﬁers, namely neural networks, to avoid the worst-case sample complexities predicted by generalization bounds Jiang* et al. (2020).
Indeed, assumptions like the manifold hypothesis, which state that the data lies on lower dimensional embedded manifold, are often made to explain the practical success of some generative methods. A recent paper by Pope et al. (2021) computes estimates of this true dimensionality of common machine learning datasets and shows that they are much lower than the ambient dimension of the pixel space
[0, 1]d. It is therefore important that properties of the data distribution, going beyond simple bounds on the support, intervene in the study of generalization.
This inﬂuence of intrinsic dimension on generalization has been recently observed in the context of differentiating neural networks and from their kernel approximations, like the neural tangent kernel
Jacot et al. (2018) or random feature models Yehudai and Shamir (2019). In particular, Ghorbani et al. (2020) observe that neural networks seem to require fewer samples than kernel methods to learn when the data distribution is isotropic.
We show that a similar phenomenon occurs in the simpler setting of quadratic classiﬁers, which leads to a better understanding of the causes. An improvement in sample complexity on isotropic data 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
distributions can be proved when comparing nuclear-norm constrained quadratic classiﬁers and the corresponding kernel method (Frobenius norm constrained classiﬁers).
The study of quadratic classiﬁers can serve as an important ﬁrst step in understanding how neural networks take advantage of the intrinsic dimension to learn with fewer samples Du and Lee (2018);
Bai and Lee (2020). The nuclear-norm constraint is a natural one to study in this context. Indeed, when applying weight-decay (or (cid:96)2-regularization) on a single-hidden layer neural network with quadratic activations, the regularization is in effect encouraging a low nuclear norm of the coefﬁcient matrix of the quadratic polynomial.
A better understanding of quadratics is also a worthwhile goal in its own right: complex architectures like those in Jayakumar et al. (2020) use quadratics as building blocks, attention layers Vaswani et al. (2017), which have seen great success in language processing tasks, are multiplicative interactions.
For these reasons, we present theoretical and practical developments of nuclear-norm regularization for quadratic classiﬁcation. We summarize our contributions as follows
Rademacher complexity bounds. We present a new bound on the Rademacher complexity of quadratic classiﬁers with a nuclear norm constraint c.f. Theorem 1. It improves upon the previously known bound, implied by the results by Kakade et al. (2012), by up to a square-root factor of the dimension, depending on the distribution of the data c.f. Lemma 4.
As a consequence of our bound, we draw attention to a clear difference between the complexity of nuclear-norm constrained and Frobenius-norm constrained quadratic classiﬁers. When the input data distribution is nearly-isotropic, the former enjoys a reduced dependency on the dimension. In contrast, the complexity of Frobenius-norm constrained classiﬁers has the same dependency on the dimension, independently of how isotropic the input data distribution is (Corollary 2).
This observation motivates the use of data whitening pre-processing steps, which are commonly used in practice: such transformation might bring the second-order moment (covariance) matrix of the distribution close to the identity matrix and thus to nearly-isotropicity.
Computable generalization bounds. The reﬁned Rademacher complexity bound that we obtain depends on the often unknown second-order moment of the distribution, rather than simple bounds on the diameter of the support as in (Kakade et al., 2012). Even though useful in theory, it is desirable in practice to obtain bounds that can be computed from a sample. We overcome this difﬁculty in Theorem 3, where we provide high-probability computable generalization error bounds for nuclear-norm constrained quadratic classiﬁers.
Experiments. We illustrate our theoretical results on synthetic data. We show how the isotropy of the input distribution plays a major role in the generalization properties of quadratic classiﬁers.
As the dimension increases and the sample size remains proportional to it, we observe a constant generalization gap for the nuclear-norm constrained classiﬁer. In contrast, for SVMs, the gap grows d rate. In the case of anisotropic distributions, we observe similar performance for at a predicted both regularization schemes.
√ 1.1