Abstract
Acquisition of data is a difﬁcult task in many applications of machine learning, and it is only natural that one hopes and expects the population risk to decrease (better performance) monotonically with increasing data points. It turns out, somewhat surprisingly, that this is not the case even for the most standard algorithms that minimize the empirical risk. Non-monotonic behavior of the risk and instability in training have manifested and appeared in the popular deep learning paradigm under the description of double descent. These problems highlight the current lack of understanding of learning algorithms and generalization. It is, therefore, crucial to pursue this concern and provide a characterization of such behavior. In this paper, we derive the ﬁrst consistent and risk-monotonic (in high probability) algorithms for a general statistical learning setting under weak assumptions, consequently answering some questions posed by [53] on how to avoid non-monotonic behavior of risk curves. We further show that risk monotonicity need not necessarily come at the price of worse excess risk rates. To achieve this, we derive new empirical
Bernstein-like concentration inequalities of independent interest that hold for certain non-i.i.d. processes such as Martingale Difference Sequences. 1

Introduction
Guarantees on the performance of machine learning algorithms are desirable, especially given the widespread deployment. A traditional performance guarantee often takes the form of a generalization bound, where the expected risk associated with hypotheses returned by an algorithm is bounded in terms of the corresponding empirical risk plus an additive error which typically converges to zero as the sample size increases. However, interpreting such bounds is not always straight forward and can be somewhat ambiguous. In particular, given that the error term in these bounds goes to zero, it is tempting to conclude that more data would monotonically decrease the expected risk of an algorithm such as the Empirical Risk Minimizer (ERM). However, this is not always the case; for example,
[33] showed that increasing the sample size by one, can sometimes make the test performance worse in expectation for commonly used algorithms such as ERM in popular settings including linear regression. This type of non-monotonic behavior is still poorly understood and indeed not a desirable feature of an algorithm since it is expensive to acquire more data in many applications.
Non-monotonic behavior of risk curves [47]—the curve of the expected risk as a function of the sample size—has been observed in many previous works [18, 43, 48, 20] (see also [33, 52] for nice accounts of the literature). At least two phenomena have been identiﬁed as being the cause behind such behavior. The ﬁrst one, coined peaking [30, 19], or double descent according to more recent literature [6, 49, 7, 17, 15, 37, 40, 41, 16, 12, 11, 14, 42], is the phenomenon where the risk curve peaks at a certain sample size n. This sample size typically represents the cross-over point from an over-parameterized to under-parameterized model. For example, when the number of data points is less than the number of parameters of a model (over-parameterized model), such as Neural
∗Work done while at the Australian National University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Networks, the expected risk can typically increase until the number of data points exceeds the number of parameters (under-parameterized model). The second phenomenon is known as dipping [32, 31], where the risk curve reaches a minimum at a certain sample size n and increases after that—never reaching the minimum again even for very large n. This phenomenon typically happens when the algorithm is trained on a surrogate loss that differs from the one used to evaluate the risk [8].
It is becoming more apparent that the two phe-nomena just mentioned (double descent and dipping) do not fully characterize when non-monotonic risk behavior occurs [34].
[33] showed that non-monotonic risk behavior could happen outside these settings and formally prove that the risk curve of ERM is non-monotonic in linear regression with prevalent losses. The most striking aspect of their ﬁndings is that the risk curves in some of the cases they study can display a perpetual “oscillating” behavior; there is no sample size beyond which the risk curve becomes monotone—see Figure 1.
In such cases, the risk’s non-monotonicity cannot be attributed to the peaking/double descent phe-nomenon. Moreover, they rule out the dipping phenomenon by studying the ERM on the actual loss (not a surrogate loss).
The ﬁndings of [33] stress our current lack of un-derstanding of generalization. This was echoed more particularly by [53], who posed the follow-ing question as part of a COLT open problem:
Figure 1: Expected risk of ERM on a 1d lin-ear regression problem with absolute loss and two instances z1 = (x1, y1) = (1, 1) and z2 = (x2, y2) = (1/10, 1) such that P[Z = z1] = 0.1 and P[Z = z2] = 0.9. The set of hypotheses is the real line, i.e. H = R. The ERM solution ˆhn admits a closed form in this case—see [33] for details.
How can we provably avoid non-monotonic behavior?
While excess risk bounds are typically monotonic, this does not guarantee the monotonicity of the actual risk. In this work, we study under which assumptions on the learning problem there exist consistent and risk monotonic algorithms. We also aim to quantify the price to pay, in terms of corresponding excess risk rates, for achieving risk monotonicity.
Contributions.
In this work, we answer some questions posed by [53] by presenting an algorithm that is both consistent and risk-monotonic in high probability under weak assumptions on the learning problem. Our algorithm is technically a “wrapper” that takes as input any base learning algorithm B and makes up a new algorithm A that is risk monotonic in high probability and enjoys essentially the same excess risk rate as B. Crucially, our results show that risk monotonicity need not come at the expense of worse excess risk rates. In fact, we show that fast rates are achievable under a Bernstein condition (Deﬁnition 3).
Our results hold under the general statistical learning setting with a bounded loss. We even go beyond the standard i.i.d. assumption on the loss process. Our relaxed technical condition on the loss process, which is formalized in Assumption 1 below, is reminiscent of the condition characterizing Martingale
Difference Sequences (MDS). In a nutshell, we will assume a setting where the instance random variables Z1, Z2, . . . and the loss (cid:96) satisfy, for all hypotheses h, E[(cid:96)(h, Zt) ∣ Z1, . . . , Zt−1] = L(h), for some risk function L. This is trivially satisﬁed in the i.i.d. case, where L corresponds to the standard risk function. In general, this condition may be satisﬁed even if Z1, Z2, . . . are dependent or have different marginal distributions. We argue that our relaxed assumption on the loss process is the weakest assumption under which studying risk-monotonicity still makes sense.
To achieve risk monotonicity under our loss process assumption, we derive a new concentration inequality/generalization bound of PAC-Bayesian ﬂavor for MDS (see Proposition 5). This concentra-tion inequality may be thought of as an empirical Freedman’s inequality [23] or as an extension of the empirical Bernstein inequality [35] to MDS. Our concentration inequalities also have the advantage of being time-uniform with the optimal dependence on the number of samples. Here, time-uniform means that the inequalities hold for all sample sizes simultaneously. While standard concentration inequalities can be turned into time-uniform ones using a union bound over the number of samples 2
n, the resulting bounds will have a sub-optimal ln n factor instead of the optimal2 ln ln n that we are able to get. Finally, our concentration bounds are easily derived using the guarantee of a recent parameter-free online learning algorithm—FREEGRAD[39]. Our approach opens up the door for obtaining new concentration inequalities through the design of online learning algorithms.
Approach Overview. Our approach to deriving the new concentration inequalities is based on the guarantee of the recent FREEGRAD algorithm. The algorithm operates in rounds, where at each round t, FREEGRAD outputs ̂wt in some convex set W, say Rd, then observes a vector gt ∈ Rd, typically the sub-gradient of a loss function at the iterate ̂wt. The algorithm guarantees a regret bound of the form ∑T t=1 ∥gt∥2. What is more, FREEGRAD’s outputs ( ̂wt) ensure the following (see [39, Theorem 5]):
QT ), for all w ∈ W, where QT ∶= ∑T t ( ̂wt − w) ≤ ̃O(∥w∥ t=1 g⊺
√
̂w⊺ t gt + Φ(St, Qt) ≤ Φ(St−1, Qt−1), ∀t ≥ 1, (1) i=1 gi∥, Qt ∶= ∑t i=1 ∥gi∥2, and Φ(S, V ) ∶= exp( S2/2 where St ∶= ∥ ∑t
), for any γ > 0.
Instantiating this guarantee in 1d with (gt) set to an MDS (Xt) and taking (conditional) expectation in (1) shows that Φt ∶= Φ(∑t i ) is a non-negative supermartingale, from which concen-tration results can be obtained via Ville’s inequality (a generalization of Markov’s inequality—see
Lemma 18). Our proof technique is similar to the one introduced in [28],with the difference that we use the speciﬁc shape of FREEGRAD’s potential function to build our supermartingale, which leads to a desirable empirical variance term in the ﬁnal concentration bound.
γ2+V +∣S∣ − 1 i=1 Xi, ∑t 2 ln γ2 i=1 X 2
γ2+V
On the side of risk monotonicity, given n samples, the key idea behind our approach is to iteratively generate a sequence of distributions P1, P2, . . . leading up to Pn over hypotheses, where we only allow consecutive distributions, say Pk−1 and Pk to differ if we can guarantee (with high enough conﬁdence) that the risk associated with Pk is lower than that of Pk−1. To test for this, we compare the average empirical losses of hypotheses sampled from Pk−1 versus ones sampled from Pk, taking into account the potential gap between empirical and population expectations. Applying our new concentration bounds to quantify this gap not only allows us to achieve risk monotonicity under a non-i.i.d. loss process but also enables us to achieve fast excess risk rates under the Bernstein condition. For the latter, it was crucial to have an empirical loss variance term in the concentration inequality.