Abstract
Automated decision-making tools increasingly assess individuals to determine if they qualify for high-stakes opportunities. A recent line of research investigates how strategic agents may respond to such scoring tools to receive favorable assessments.
While prior work has focused on the short-term strategic interactions between a decision-making institution (modeled as a principal) and individual decision-subjects (modeled as agents), we investigate interactions spanning multiple time-steps. In particular, we consider settings in which the agent’s effort investment today can accumulate over time in the form of an internal state—impacting both his future rewards and that of the principal. We characterize the Stackelberg equilibrium of the resulting game and provide novel algorithms for computing it. Our analysis reveals several intriguing insights about the role of multiple interactions in shaping the game’s outcome: First, we establish that in our stateful setting, the class of all linear assessment policies remains as powerful as the larger class of all monotonic assessment policies. While recovering the principal’s optimal policy requires solving a non-convex optimization problem, we provide polynomial-time algorithms for recovering both the principal and agent’s optimal policies under common assumptions about the process by which effort investments convert to observable features. Most importantly, we show that with multiple rounds of interaction at her disposal, the principal is more effective at incentivizing the agent to accumulate effort in her desired direction. Our work addresses several critical gaps in the growing literature on the societal impacts of automated decision-making—by focusing on longer time horizons and accounting for the compounding nature of decisions individuals receive over time. 1

Introduction
Automated decision-making tools increasingly assess individuals to determine whether they qualify for life-altering opportunities in domains such as lending [27], higher education [32], employ-ment [41], and beyond. These assessment tools have been widely criticized for the blatant disparities they produce through their scores [43, 3]. This overwhelming body of evidence has led to a remark-ably active area of research into understanding the societal implications of algorithmic/data-driven automation. Much of the existing work on the topic has focused on the immediate or short-term societal effects of automated decision-making. (For example, a thriving line of work in Machine
Learning (ML) addresses the unfairness that arises when ML predictions inform high-stakes deci-sions [18, 22, 31, 8, 1, 16, 11] by deﬁning it as a form of predictive disparity, e.g., inequality in false-positive rates [22, 3] across social groups.) With the exception of several noteworthy recent articles (which we discuss shortly), prior work has largely ignored the processes through which algorithmic decision-making systems can induce, perpetuate, or amplify undesirable choices and behaviors. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our work takes a long-term perspective toward modeling the interactions between individual deci-sion subjects and algorithmic assessment tools. We are motivated by two key observations: First, algorithmic assessment tools often provide predictions about the latent qualities of interest (e.g., creditworthiness, mastery of course material, or job productivity) by relying on imperfect but observ-able proxy attributes that can be directly evaluated about the subject (e.g., past ﬁnancial transactions, course grades, peer evaluation letters). Moreover, their design ignores the compounding nature of advantages/disadvantages individual subjects accumulate over time in pursuit of receiving favor-able assessments (e.g., debt, knowledge, job-related skills). To address how individuals respond to decisions made about them through modifying their observable characteristics, a growing line of work has recently initiated the study of the strategic interactions between decision-makers and decision-subjects (see, e.g., [15, 26, 36, 30, 21]). This existing work has focused mainly on the short-term implications of strategic interactions with algorithmic assessment tools—e.g., by mod-eling it as a single round of interaction between a principal (the decision-maker) and agents (the decision-subjects) [30]. In addition, existing work that studies interactions over time assumes that agents are myopic in responding to the decision-maker’s policy [4, 42, 38, 15]. We expand the line of inquiry to multiple rounds of interactions, accounting for the impact of actions today on the outcomes players can attain tomorrow.
Our multi-round model of principal-agent interactions. We take the model proposed by Kleinberg and Raghavan [30] as our starting point. In Kleinberg and Raghavan’s formulation, a principal interacts with an agent once, where the interaction takes the form of a Stackelberg game. The agent receives a score y = f (✓, o), in which ✓ is the principal’s choice of assessment parameters, and o is the agent’s observable characteristics. The score is used to determine the agent’s merit with respect to the quality the principal is trying to assess. (As concrete examples, y could correspond to the grade a student receives for a class, or the FICO credit score of a loan applicant.) The principal moves
ﬁrst, publicly announcing her assessment rule ✓ used to evaluate the agent. The agent then best responds to this assessment rule by deciding how to invest a ﬁxed amount of effort into producing a set of observable features o that maximize his score y. Kleinberg and Raghavan characterize the assessment rules that can incentivize the agent to invest in speciﬁc types of effort (e.g., those that lead to real improvements in the quality of interest as opposed to gaming the system). We generalize the above setting to T > 1 rounds of interactions between the principal and the agent and allow for the possibility of certain effort types rolling over from one step to the next. Our key ﬁnding is that longer time horizon provides the principal additional latitude in the range of effort sequences she can incentivize the agent to produce. To build intuition as to why repeated interactions lead to the expansion of incentivizable efforts, consider the following stylized example:
Example 1.1. Consider the classroom example of Kleinberg and Raghavan where a teacher (modeled as a principal) assigns a student (modeled as an agent) an overall grade y based on his observable features; in this case test and homework score. Assume that the teacher chooses an assessment rule and assigns a score y = ✓T ET E + ✓HW HW , where T E is the student’s test score HW is his homework score, and ✓T ,✓ HW 2
R are the weight of each score in the student’s overall grade. The student can invest effort into any of three activities: copying answers on the test, studying, and looking up homework answers online. In a one-round setting where the teacher only evaluates the student once, the student may be more inclined to copy answers on the test or look up homework answers online, since these actions immediately improve the score with relatively lower efforts. However, in a multiple-round setting, these two actions do not improve the student’s knowledge (which impacts the student’s future grades as well), and so these efforts do not carry over to future time steps. When there are multiple rounds of interaction, the student will be incentivized to invest effort into studying, as knowledge accumulation over time takes less effort in the long-run compared to cheating every time. We revisit this example in further detail in Appendix A.
Summary of our ﬁndings and techniques. We formalize settings in which the agent’s effort investment today can accumulate over time in the form of an internal state—impacting both his future rewards and that of the principal. We characterize the Stackelberg equilibrium of the resulting game and provide novel algorithmic techniques for computing it. We begin by establishing that for the principal, the class of all linear assessment policies remains as powerful as the larger class of all monotonic assessment policies. In particular, we prove that if there exists an assessment policy that can incentivize the agent to produce a particular sequence of effort proﬁles, there also exists a linear assessment policy which can incentivize the exact same effort sequence. 2
Figure 1: Average effort spent studying vs. average effort spent cheating over time for the example in Appendix A.
The line x + y = 1 represents the set of all possible Pareto optimal average effort proﬁles. The shaded region under the line represents the set of average effort proﬁles which can be incentivized with a certain time horizon. Darker shades represent longer time horizons. In the case where
T = 1, it is not possible to incentivize the agent to spend any effort studying. Arrows are used to demonstrate the additional set of Pareto optimal average effort proﬁles that can be incentivized with each time horizon. As the time horizon increases, it becomes possible to incentivize a wider range of effort proﬁles.
We then study the equilibrium computation problem, which in general involves optimizing non-convex objectives. Despite the initial non-convexity, we observe that when the problem is written as a function of the agent’s incentivized efforts, the principal’s non-convex objective becomes convex. Moreover, under a common assumption on agent’s conversion mapping from efforts to observable features, the set of incentivizable effort policies is also convex. Given this structure, we provide a polynomial-time algorithm that directly optimizes the principal’s objective over the set of incentivizable effort policies, which subsequently recovers agent’s and principal’s equilibrium strategies. Even though prior work
[39, 40] has also taken this approach for solving other classes of non-convex Stackelberg games, our work has to overcome an additional challenge that the agent’s set of incentivizable efforts is not known a-priori. We resolve this challenge by providing a membership oracle (that determines whether a sequence of agent efforts can be incentivized by any assessment policy), which allows us to leverage the convex optimization method due to Kalai and Vempala [28].
Our analysis reveals several intriguing insights about the role of repeated interactions in shaping the long-term outcomes of decision-makers and decision subjects: For example, we observe that with multiple rounds of assessments, both parties can be better off employing dynamic/time-sensitive strategies as opposed to static/myopic ones. Crucially, perhaps our most signiﬁcant ﬁnding is that by considering the effects of multiple time-steps, the principal is signiﬁcantly more effective at incentivizing the agent to accumulate effort in her desired direction (as demonstrated in Figure 1 for a stylized teacher-student example). In conclusion, our work addresses two critical gaps in the growing literature on the societal impacts of automated decision-making–by focusing on longer time horizons and accounting for the compounding nature of decisions individuals receive over time. 1.1