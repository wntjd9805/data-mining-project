Abstract
Unpredictable ML model behavior on unseen data, especially in the health domain, raises serious concerns about its safety as repercussions for mistakes can be fatal.
In this paper, we explore the feasibility of using state-of-the-art out-of-distribution detectors for reliable and trustworthy diagnostic predictions. We select publicly available deep learning models relating to various health conditions (e.g., skin cancer, lung sound, and Parkinson’s disease) using various input data types (e.g., image, audio, and motion data). We demonstrate that these models show unrea-sonable predictions on out-of-distribution datasets. We show that Mahalanobis distance- and Gram matrices-based out-of-distribution detection methods are able to detect out-of-distribution data with high accuracy for the health models that operate on different modalities. We then translate the out-of-distribution score into a human interpretable CONFIDENCE SCORE to investigate its effect on the users’ interaction with health ML applications. Our user study shows that the
CONFIDENCE SCORE helped the participants only trust the results with a high score to make a medical decision and disregard results with a low score. Through this work, we demonstrate that dataset shift is a critical piece of information for high-stake ML applications, such as medical diagnosis and healthcare, to provide reliable and trustworthy predictions to the users.

Introduction 1
Advances in artiﬁcial intelligence and machine learning have made medical diagnostic and screening tools more accurate and accessible. AI-powered diagnostic tools [5, 14] are intended to assist medical personnel by making unbiased decision based on thousands of examples. In recent years, these models [15, 44, 48, 32] are even becoming available to consumers through the growth of mobile health with the intention of expediting diagnoses through increasingly frequent testing. Moreover, mobile health [4, 50] aims to improve access to medical expertise for those who are uninsured or live far away from hospitals.
Despite the potential beneﬁts of health AI systems, there are concerns about their performance in real-world settings. Data-driven models learn from examples, making them heavily reliant on the data upon which they have been trained. However, datasets often fail to get complete coverage over a domain, particularly for emerging datasets; when new pulmonary diseases (e.g., MERS and
COVID-19) emerge, a pulmonary classiﬁer trained on the existing lung sounds would not be able to interpret sound of the new diseases. Previous work [42, 78] has found that machine learning models behave unpredictably on the unseen data. This problem [4, 69] is especially critical for medical diagnostic and screening tools since there are signiﬁcant repercussions for mistakes.
Researchers have proposed methods to estimate the uncertainty of a machine learning models’ predictions based on the input [29, 41, 40, 64, 43]. Out-of-distribution detection methods can distinguish whether the input lies within the distribution of the training dataset, with out-of-distribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
data leading to less reliable prediction results. However, such important information has not been widely explored in the context of health applications. When health applications are put into the hands of consumers with limited understanding of the underlying algorithms, they may upload poor quality data that lies outside the distribution of the data that was collected by experts. For example, consumers who are using a health application that involves image processing may take photographs in poor lighting conditions or framing of the target object. Even when the data is high-quality, it may be captured with a smartphone that has different hardware speciﬁcations than the devices that were used to collect the training dataset. Unless the models are explicitly designed or trained to detect invalid data, the models will incorrectly produce a diagnostically meaningless result.
In this work, we explore the utility of out-of-distribution detection for improving model performance and user-perceived trustworthiness of health-related models. We ﬁrst benchmark our approach using publicly available deep learning models relating to various medical challenges and sensing domains
— images for skin lesion classiﬁcation, motion data for Parkinson’s disease severity, and audio for lung sound classiﬁcation. After demonstrating that these models are susceptible to dataset shift, we demonstrate that the state-of-the-art out-of-distribution detectors can effectively exclude such data with over 95% detection accuracy in most cases. We then explore the implications of this detector on user-perceived trustworthiness of the health models. After translating the out-of-distribution score into a human-interpretable metric, CONFIDENCE SCORE, we found that showing this information to end-users improved the user-perceived trustworthiness of the models. Furthermore, participants stated that they were more willing to make medical decisions based on models when they were shown the certainty metric. Our contributions in this work are as follows:
• We identify and quantify the limitations of current health deep learning models when encountered with unseen data,
• We evaluate the utility of out-of-distribution detection on various data types (e.g., image, audio, motion) for medical screening and diagnosis, and
• We evaluate the impact that dataset shift information has on user-perceived trustworthiness of health diagnostic results. 2