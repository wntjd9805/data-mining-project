Abstract
The invariance principle from causality is at the heart of notable approaches such as invariant risk minimization (IRM) that seek to address out-of-distribution (OOD) generalization failures. Despite the promising theory, invariance principle-based approaches fail in common classiﬁcation tasks, where invariant (causal) features capture all the information about the label. Are these failures due to the methods failing to capture the invariance? Or is the invariance principle itself insufﬁcient? To answer these questions, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD.
In contrast to the linear regression tasks, we show that for linear classiﬁcation tasks we need much stronger restrictions on the distribution shifts, or otherwise
OOD generalization is impossible. Furthermore, even with appropriate restrictions on distribution shifts in place, we show that the invariance principle alone is insufﬁcient. We prove that a form of the information bottleneck constraint along with invariance helps address key failures when invariant features capture all the information about the label and also retains the existing success when they do not.
We propose an approach that incorporates both of these principles and demonstrate its effectiveness in several experiments. 1

Introduction
Recent years have witnessed an explosion of examples showing deep learning models are prone to exploiting shortcuts (spurious features) (Geirhos et al., 2020; Pezeshki et al., 2020) which make them fail to generalize out-of-distribution (OOD). In Beery et al. (2018), a convolutional neural network was trained to classify camels from cows; however, it was found that the model relied on the background color (e.g., green pastures for cows) and not on the properties of the animals (e.g., shape).
These examples become very concerning when they occur in real-life applications (e.g., COVID-19 detection (DeGrave et al., 2020)).
To address these out-of-distribution generalization failures, invariant risk minimization (Arjovsky et al., 2019) and several other works were proposed (Ahuja et al., 2020; Pezeshki et al., 2020; Krueger et al., 2020; Robey et al., 2021; Zhang et al., 2021). The invariance principle from causality (Peters et al., 2015; Pearl, 1995) is at the heart of these works. The principle distinguishes predictors that only rely on the causes of the label from those that do not. The optimal predictor that only focuses on the causes is invariant and min-max optimal (Rojas-Carulla et al., 2018; Koyama and Yamaguchi, 2020; Ahuja et al., 2021) under many distribution shifts but the same is not true for other predictors.
∗Equal contribution.
†Mila - Quebec AI Institute, Université de Montréal. Correspondence to: kartik.ahuja@mila.quebec. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Our contributions. Despite the promising theory, invariance principle-based approaches fail in settings (Aubin et al., 2021) where invariant features capture all information about the label contained in the input. A particular example is image classiﬁcation (e.g., cow vs. camel) (Beery et al., 2018) where the label is a deterministic function of the invariant features (e.g., shape of the animal), and does not depend on the spurious features (e.g., background). To understand such failures, we revisit the fundamental assumptions in linear regression tasks, where invariance-based approaches were shown to provably generalize OOD. We show that, in contrast to the linear regression tasks, OOD generalization is signiﬁcantly harder for linear classiﬁcation tasks; we need much stronger restrictions in the form of support overlap assumptions3 on the distribution shifts, or otherwise it is not possible to guarantee OOD generalization under interventions on variables other than the target class. We then proceed to show that, even under the right assumptions on distribution shifts, the invariance principle is insufﬁcient. However, we establish that information bottleneck (IB) constraints (Tishby et al., 2000), together with the invariance principle, provably works in both settings – when invariant features completely capture the information about the label and also when they do not. (Table 1 summarizes our theoretical results presented later). We propose an approach that combines both these principles and demonstrate its effectiveness on linear unit tests (Aubin et al., 2021) and on different real datasets.
Task
Invariant features capture label info
Support overlap invariant features
Linear
Classiﬁcation
Linear
Regression
Full/Partial
Full
Partial
Full
Partial
Full
Partial
No
Yes
Yes
Yes
Yes
No
No
Support overlap spurious features ERM IRM IB-ERM
OOD generalization guarantee (Etr → Eall)
IB-IRM
Impossible for any algorithm to generalize OOD [Thm2]
Yes/No
No
No
Yes
Yes
No
No (cid:55) (cid:55) (cid:51) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:51) (cid:55) (cid:51) (cid:55) (cid:51) [Thm3,4] (cid:51) [Appendix] (cid:51) [Thm3,4] (cid:51) (cid:51) (cid:51) [Thm4]
Table 1: Summary of the new and existing results (Arjovsky et al., 2019; Rosenfeld et al., 2021).
IB-ERM (IRM): information bottleneck - empirical (invariant) risk minimization ERM (IRM). 2 OOD generalization and invariance: background & failures