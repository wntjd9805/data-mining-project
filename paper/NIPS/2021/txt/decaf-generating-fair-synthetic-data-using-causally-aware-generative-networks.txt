Abstract
Machine learning models have been criticized for reﬂecting unfair biases in the training data. Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data— while remaining truthful to the underlying data-generating process (DGP) —is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data. With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents. This procedure enables inference-time debiasing, where biased edges can be strategically removed for satisfying user-deﬁned fairness requirements. The DECAF framework is versatile and compatible with several popular deﬁnitions of fairness. In our experiments, we show that DECAF success-fully removes undesired bias and— in contrast to existing methods —is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator’s convergence and the fairness of downstream models. 1

Introduction
Generative models are optimized to approximate the original data distribution as closely as possible.
Most research focuses on three objectives [1]: ﬁdelity, diversity, and privacy. The ﬁrst and second are concerned with how closely synthetic samples resemble real data and how much of the real data’s distribution is covered by the new distribution, respectively. The third objective aims to avoid simply reproducing samples from the original data, which is important if the data contains privacy-sensitive information [2, 3]. We explore a much-less studied concept: synthetic data fairness.
Motivation. Deployed machine learning models have been shown to reﬂect the bias of the data on which they are trained [4, 5, 6, 7, 8]. This has not only unfairly damaged the discriminated individuals but also society’s trust in machine learning as a whole. A large body of work has explored ways of detecting bias and creating fair predictors [9, 10, 11, 12, 13, 14, 15], while other authors propose debiasing the data itself [9, 10, 11, 16]. This work’s aim is related to the work of [17]: to generate fair synthetic data based on unfair data. Being able to generate fair data is important because end-users creating models based on publicly available data might be unaware they are inadvertently including
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Overview of related work for synthetic data. We organize related work according to our key areas of interest: (1) Allows post-hoc distribution changes, (2) provides fairness, (3) supports causal notion of fairness, (4) allows inference-time fairness, (5) requires minimal assumptions. We highlight the key contribution, and identify non-neural approaches with “†”.
Model
Reference (1) (2) (3) (4) (5) Goal
VAE
GANs
[19]
[2, 3, 20, 21]
PSE-DD/DR†
OPPDP†
DI†
LFR
FairGAN
CFGAN
[11]
[16]
[10]
[22]
[17]
[23]
DECAF (ours)
Standard synthetic data generation
Methods that detect and/or remove bias (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:55) (cid:51) (cid:51) (cid:51) (cid:51) (cid:51) Realistic synth. data. (cid:51) Realistic synth. data.
Discover/Remove bias.
Remove bias.
Discover/Remove bias. (cid:55) (cid:55) (cid:55) (cid:51) Learn fair representation. (cid:51) Realistic and fair synth. data. (cid:51) Realistic and fair synth. data. (cid:51) Realistic and fair synth. data. bias or insufﬁciently knowledgeable to remove it from their model. Furthermore, by debiasing the data prior to public release, one can guarantee any downstream model satisﬁes desired fairness requirements by assigning the responsibility of debiasing to the data generating entities.
Goal. From a biased dataset X , we are interested in learning a model G, that is able to generate an equivalent synthetic unbiased dataset X (cid:48) with minimal loss of data utility. Furthermore, a downstream model trained on the synthetic data needs to make not only unbiased predictions on the synthetic data, but also on real-life datasets (as formalized in Section 4.2).
Solution. We approach fairness from a causal standpoint because it provides an intuitive perspective on different deﬁnitions of fairness and discrimination [11, 13, 14, 15, 18]. We introduce DEbiasing
CAusal Fairness (DECAF), a generative adversarial network (GAN) that leverages causal structure for synthesizing data. Speciﬁcally, DECAF is comprised of d generators (one for each variable) that learn the causal conditionals observed in the data. At inference-time, variables are synthesized topologically starting from the root nodes in the causal graph then synthesized sequentially, terminating at the leave nodes. Because of this, DECAF can remove bias at inference-time through targeted (biased) edge removal. As a result, various datasets can be created for desired (or evolving) deﬁnitions of fairness.
Contributions. We propose a framework of using causal knowledge for fair synthetic data generation.
We make three main contributions: i) DECAF, a causal GAN-based model for generating synthetic data, ii) a ﬂexible causal approach for modifying this model such that it can generate fair data, and iii) guarantees that downstream models trained on the synthetic data will also give fair predictions in other settings. Experimentally, we show how DECAF is compatible with several fairness/discrimination deﬁnitions used in literature while still maintaining high downstream utility of generated data. 2