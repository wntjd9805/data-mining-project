Abstract
The problem of reducing processing time of large deep learning models is a fun-damental challenge in many real-world applications. Early exit methods strive towards this goal by attaching additional Internal Classiﬁers (ICs) to intermediate layers of a neural network. ICs can quickly return predictions for easy examples and, as a result, reduce the average inference time of the whole model. However, if a particular IC does not decide to return an answer early, its predictions are discarded, with its computations effectively being wasted. To solve this issue, we introduce Zero Time Waste (ZTW), a novel approach in which each IC reuses predictions returned by its predecessors by (1) adding direct connections between
ICs and (2) combining previous outputs in an ensemble-like manner. We conduct extensive experiments across various datasets and architectures to demonstrate that
ZTW achieves a signiﬁcantly better accuracy vs. inference time trade-off than other recently proposed early exit methods. 1

Introduction
Deep learning models achieve tremendous successes across a multitude of tasks, yet their training and inference often yield high computational costs and long processing times [11, 22]. For some applications, however, efﬁciency remains a critical challenge, e.g. to deploy a reinforcement learning (RL) system in production the policy inference must be done in real-time [7], while the robot performances suffer from the delay between measuring a system state and acting upon it [34].
Similarly, long inference latency in autonomous cars could impact its ability to control the speed [13] and lead to accidents [10, 17].
Typical approaches to reducing the processing complexity of neural networks in latency-critical applications include compressing the model [24, 26, 46] or approximating its responses [21]. For instance, Livne & Cohen [26] propose to compress a RL model by policy pruning, while Kouris et al. [21] approximate the responses of LSTM-based modules in self-driving cars to accelerate their inference time. While those methods improve processing efﬁciency, they still require samples to pass
∗equal contribution
†Corresponding author: maciej.wolczyk@doctoral.uj.edu.pl 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Comparison of the proposed ZTW (bottom) with a conven-tional early-exit model, SDN (top). (b) Detailed scheme of the proposed ZTW model architecture.
Figure 1: (a) In both approaches, internal classiﬁers (ICs) attached to the intermediate hidden layers of the base network allow us to return predictions quickly for examples that are easy to process.
While SDN discards predictions of uncertain ICs (e.g. below a threshold of 75%), ZTW reuses computations from all previous ICs, which prevents information loss and waste of computational resources. (b) Backbone network fθ lends its hidden layer activations to ICs, which share inferred information using cascade connections (red horizontal arrows in the middle row) and give predictions pm. The inferred predictions are combined using ensembling (bottom row) giving qm. through the entire model. In contrast, biological neural networks leverage simple heuristics to speed up decision making, e.g. by shortening the processing path even in case of complex tasks [1, 9, 18].
This observation led a way to the inception of the so-called early exit methods, such as Shallow-Deep
Networks (SDN) [19] and Patience-based Early Exit (PBEE) [47], that attach simple classiﬁcation heads, called internal classiﬁers (ICs), to selected hidden layers of neural models to shorten the processing time. If the prediction conﬁdence of a given IC is sufﬁciently high, the response is returned, otherwise, the example is passed to the subsequent classiﬁer. Although these models achieve promising results, they discard the response returned by early ICs in the evaluation of the next IC, disregarding potentially valuable information, e.g. decision conﬁdence, and wasting computational effort already incurred.
Motivated by the above observation, we postulate to look at the problem of neural model processing efﬁciency from the information recycling perspective and introduce a new family of zero waste models. More speciﬁcally, we investigate how information available at different layers of neural models can contribute to the decision process of the entire model. To that end, we propose Zero
Time Waste (ZTW), a method for an intelligent aggregation of the information from previous ICs.
A high-level view of our model is given in Figure 1. Our approach relies on combining ideas from networks with skip connections [41], gradient boosting [3], and ensemble learning [8, 23]. Skip connections between subsequent ICs (which we call cascade connections) allow us to explicitly pass the information contained within low-level features to the deeper classiﬁer, which forms a cascading structure of ICs. In consequence, each IC improves on the prediction of previous ICs, as in gradient boosting, instead of generating them from scratch. To give the opportunity for every IC to explicitly reuse predictions of all previous ICs, we additionally build an ensemble of shallow ICs.
We evaluate our approach on standard classiﬁcation benchmarks, such as CIFAR-100 and ImageNet, as well as on the more latency-critical applications, such as reinforcement-learned models for interacting with sequential environments. To the best of our knowledge, we are the ﬁrst to show that early exit methods can be used for cutting computational waste in a reinforcement learning setting.
Results show that ZTW is able to save much more computation while preserving accuracy than current state-of-the-art early exit methods. In order to better understand where the improvements come from, we introduce Hindsight Improvability, a metric for measuring how efﬁciently the model 2
reuses information from the past. We provide ablation studies and additional analysis of the proposed method in the Appendix.
To summarize, the contributions of our work are the following:
• We introduce a family of zero waste models that quantify neural network efﬁciency with the
Hindsight Improvability metrics.
• We propose an instance of zero waste models dubbed Zero Time Waste (ZTW) method which uses cascade connections and ensembling to reuse the responses of previous ICs for the ﬁnal decision.
• We show how the state-of-the-art performance of ZTW in the supervised learning scenario generalizes to reinforcement learning. 2