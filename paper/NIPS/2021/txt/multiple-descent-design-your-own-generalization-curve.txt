Abstract
This paper explores the generalization loss of linear regression in variably param-eterized families of models, both under-parameterized and over-parameterized.
We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family.
Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms. 1

Introduction
The main goal of machine learning methods is to provide an accurate out-of-sample prediction, known as generalization. For a ﬁxed family of models, a common way to select a model from this family is through empirical risk minimization, i.e., algorithmically selecting models that minimize the risk on the training dataset. Given a variably parameterized family of models, the statistical learning theory aims to identify the dependence between model complexity and model performance.
The empirical risk usually decreases monotonically as the model complexity increases, and achieves its minimum when the model is rich enough to interpolate the training data, resulting in zero (or near-zero) training error. In contrast, the behaviour of the test error as a function of model complexity is far more complicated. Indeed, in this paper we show how to construct a model family for which the generalization curve can be fully controlled (away from the interpolation threshold) in both under-parameterized and over-parameterized regimes. Classical statistical learning theory supports a
U-shaped curve of generalization versus model complexity [31, 33]. Under such a framework, the best model is found at the bottom of the U-shaped curve, which corresponds to appropriately balancing under-ﬁtting and over-ﬁtting the training data. From the view of the bias-variance trade-off, a higher model complexity increases the variance while decreasing the bias. A model with an appropriate level of complexity achieves a relatively low bias while still keeping the variance under control. On the other hand, a model that interpolates the training data is deemed to over-ﬁt and tends to worsen the generalization performance due to the soaring variance.
Although classical statistical theory suggests a pattern of behavior for the generalization curve up to the interpolation threshold, it does not describe what happens beyond the interpolation threshold, 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
commonly referred to as the over-parameterized regime. This is the exact regime where many modern machine learning models, especially deep neural networks, achieved remarkable success. Indeed, neural networks generalize well even when the models are so complex that they have the potential to interpolate all the training data points [61, 10, 32, 34].
Modern practitioners commonly deploy deep neural networks with hundreds of millions or even billions of parameters.
It has become widely accepted that large models achieve performance superior to small models that may be suggested by the classical U-shaped generalization curve
[13, 38, 55, 35, 36]. This indicates that the test error decreases again once model complexity grows beyond the interpolation threshold, resulting in the so called double-descent phenomenon described in [9], which has been broadly supported by empirical evidence [49, 48, 29, 30] and conﬁrmed empirically on modern neural architectures by Nakkiran et al. [46]. On the theoretical side, this phenomenon has been recently addressed by several works on various model settings. In particular,
Belkin et al. [11] proved the existence of double-descent phenomenon for linear regression with random feature selection and analyzed the random Fourier feature model [50]. Mei and Montanari
[44] also studied the Fourier model and computed the asymptotic test error which captures the double-descent phenomenon. Bartlett et al. [8], Tsigler and Bartlett [56] analyzed and gave explicit conditions for “benign overﬁtting” in linear and ridge regression, respectively. Caron and Chretien
[16] provided a ﬁnite sample analysis of the nonlinear function estimation and showed that the parameter learned through empirical risk minimization converges to the true parameter with high probability as the model complexity tends to inﬁnity, implying the existence of double descent. Liu et al. [42] studied the high dimensional kernel ridge regression in the under- and over-parameterized regimes and showed that the risk curve can be double descent, bell-shaped, and monotonically decreasing.
Among all the aforementioned efforts, one particularly interesting question is whether one can observe more than two descents in the generalization curve. d’Ascoli et al. [21] empirically showed a sample-wise triple-descent phenomenon under the random Fourier feature model. Similar triple-descent was also observed for linear regression [47]. More rigorously, Liang et al. [41] presented an upper bound on the risk of the minimum-norm interpolation versus the data dimension in Reproducing
Kernel Hilbert Spaces (RKHS), which exhibits multiple descent. However, a multiple-descent upper bound without a properly matching lower bound does not imply the existence of a multiple-descent generalization curve. In this work, we study the multiple descent phenomenon by addressing the following questions:
• Can the existence of a multiple descent generalization curve be rigorously proven?
• Can an arbitrary number of descents occur?
• Can the generalization curve and the locations of descents be designed?
In this paper, we show that the answer to all three of these questions is yes. Further related work is presented in Section 2.
Our Contribution. We consider the linear regression model and analyze how the risk changes as the dimension of the data grows. In the linear regression setting, the data dimension is equal to the dimension of the parameter space, which reﬂects the model complexity. We rigorously show that the multiple descent generalization curve exists under this setting. To our best knowledge, this is the ﬁrst work proving a multiple descent phenomenon.
Our analysis considers both the underparametrized and overparametrized regimes. In the over-parametrized regime, we show that one can control where a descent or an ascent occurs in the generalization curve. This is realized through our algorithmic construction of a feature-revealing process. To be more speciﬁc, we assume that the data is in RD, where D can be arbitrarily large or even essentially inﬁnite. We view each dimension of the data as a feature. We consider a linear regression problem restricted on the ﬁrst d features, where d < D. New features are revealed by increasing the dimension of the data. We then show that by specifying the distribution of the newly revealed feature to be either a standard Gaussian or a Gaussian mixture, one can determine where an ascent or a descent occurs. In order to create an ascent when a new feature is revealed, it is sufﬁcient that the feature follows a Gaussian mixture distribution. In order to have a descent, it is sufﬁcient that the new feature follows a standard Gaussian distribution. Therefore, in the overparametrized regime, we can fully control the occurrence of a descent and an ascent. As a comparison, in the un-derparametrized regime, the generalization loss always increases regardless of the feature distribution.
Generally speaking, we show that we are able to design the generalization curve. 2
On the one hand, we show theoretically that the generalization curve is malleable and can be constructed in an arbitrary fashion. On the other hand, we rarely observe complex generalization curves in practice, besides carefully curated constructions. Putting these facts together, we arrive at the conclusion that realistic generalization curves arise from speciﬁc interactions between properties of typical data and the inductive biases of algorithms. We should highlight that the nature of these interactions is far from being understood and should be an area of further investigations. 2