Abstract
The advent of generative radiance ﬁelds has signiﬁcantly promoted the development of 3D-aware image synthesis. The cumulative rendering process in radiance ﬁelds makes training these generative models much easier since gradients are distributed over the entire volume, but leads to diffused object surfaces. In the meantime, compared to radiance ﬁelds occupancy representations could inherently ensure deterministic surfaces. However, if we directly apply occupancy representations to generative models, during training they will only receive sparse gradients located on object surfaces and eventually suffer from the convergence problem. In this paper, we propose Generative Occupancy Fields (GOF), a novel model based on generative radiance ﬁelds that can learn compact object surfaces without impeding its training convergence. The key insight of GOF is a dedicated transition from the cumulative rendering in radiance ﬁelds to rendering with only the surface points as the learned surface gets more and more accurate. In this way, GOF combines the merits of two representations in a uniﬁed framework. In practice, the training-time transition of start from radiance ﬁelds and march to occupancy representations is achieved in GOF by gradually shrinking the sampling region in its rendering process from the entire volume to a minimal neighboring region around the surface. Through comprehensive experiments on multiple datasets, we demonstrate that GOF can synthesize high-quality images with 3D consistency and simultaneously learn compact and smooth object surfaces. Our code is available at https://github.com/SheldonTsui/GOF_NeurIPS2021. 1

Introduction
Deep generative adversarial networks [1–4] have demonstrated their superiority in synthesizing photorealistic and striking images. However, these models are often constrained in the 2D domain, struggling to generate 3D consistent images, let alone grasping the underlying 3D object shapes. 3D-aware image synthesis thus becomes an appealing and promising choice as it learns a 3D representation explicitly from a collection of unposed images. Consequently, it can not only synthesize 3D consistent images by manually controlling the rendering camera poses, but also pave the way for various downstream tasks such as shape editing and relighting.
Inspired by the success of neural radiance ﬁelds (NeRF) [5] in 3D scene modeling, recent 3D-aware generative models, referred to as generative radiance ﬁelds (GRAFs), have applied NeRF as the explicit 3D representation for image synthesis [6, 7]. With the help of NeRF, they are capable of hallucinating photorealistic images in a 3D consistent manner. Moreover, since NeRF holds the superior ability for rendering translucent objects by compositing colored densities along each ray in its volume rendering process, it also signiﬁcantly facilitates the training of GRAFs as gradients are naturally distributed over the entire volume. However, they still incur an inevitable incapacity of 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Cumulative Rendering Weights (b) Performance Comparison
Figure 1: (a) The cumulative rendering weights (color weights) of our approach GOF more focus on the surface (y-axis) than previous methods like pi-GAN [6], which indicates our predicted volume densities more concentrate on the object surfaces. (b) Owing to the diffused volume densities, the preceding method pi-GAN captures messy surface normals and object shapes. Moreover, the image rendered only with the surface points is quite noisy. In contrast, more surface-centralized densities predicted by our method ensure compact and smooth object surfaces thus enable a high-quality surface rendering during inference. (Zoom in for best view) capturing an accurate and compact object surface. As shown in Fig. 1(a), the state-of-the-art GRAF model pi-GAN is prone to predict diffused object surfaces, as the volume densities are smoothly spread around the surfaces. Such diffused surfaces could signiﬁcantly hamper the applications of
GRAF models in downstream tasks such as shape recovery. Moreover, under different light conditions, the artifacts of surfaces could be ampliﬁed and inherited through the rendering process, resulting in synthesized images that are messy and faulty.
In this work, we propose Generative Occupancy Fields (GOF), a novel GRAF-like image synthesis model that can learn compact object surfaces. GOF is inspired by the design of occupancy networks
[8] that implicitly represents a 3D surface with the continuous decision boundary of a neural classiﬁer.
In this way, occupancy networks are capable of effectively locating surfaces via root-ﬁnding and encouraging the compactness of modeled surfaces inherently. However, GOF avoids directly applying such a design to 3D-aware image synthesis. While occupancy networks require precise object masks to train [9, 10], a more crucial factor is that they rely on the surface points for differentiable rendering [11, 12, 9, 13]. A generative model equipped with occupancy representations will thus meet severe convergence problems during training due to the sparsity of gradients. To unify the merits of both NeRF and occupancy networks for 3D-aware image synthesis, GOF adopts the design of GRAFs and at the same time leverages a nontrivial transition from the cumulative rendering to rendering with only the surface points, i.e. start from radiance ﬁelds and march to occupancy representations. Speciﬁcally, GOF will reinterpret the alpha values in the cumulative rendering process as occupancy values, so that it can locate the learned surface via root-ﬁnding. Subsequently, it can naturally encourage the compactness of learned surfaces by gradually shrinking the sampling region in the rendering process from the entire volume to a minimal neighboring region around the surface.
Thanks to the uniﬁed integration of radiance ﬁelds and occupancy representations, GOF can beneﬁt from the representation effectiveness of radiance ﬁelds while ensuring the compactness of learned object surfaces through the shrinking process. As presented in Fig. 1(a), in GOF the distribution of cumulative rendering weights concentrates more closely around object surfaces compared to that of pi-GAN, eventually resulting in a compact and smooth surface. Moreover, GOF can thus alternatively render an image only with points on the learned surfaces like occupancy networks as illustrated in
Fig. 1 (b). And during inference such a rendering scheme has the potential to alleviate the burden of sampling a large number of points along each ray for synthesizing a single image. Through exhaustive experiments on synthetic and real-world datasets, we demonstrate that GOF can achieve state-of-the-art performance on 3D-aware image synthesis. Meanwhile, it is capable of capturing compact and accurate 3D shapes that empower its applications in various downstream tasks such as 3D shape reconstruction. We validate this point by quantitative results of 3D shape reconstruction on the Synface dataset. Finally, we have also veriﬁed the ability of GOF in rendering high-quality images with only the surface points, which is hardly achievable in previous approaches. 2
2