Abstract
Multilingual models are parameter-efﬁcient and especially effective in improving low-resource languages by leveraging crosslingual transfer. Despite recent advance in massive multilingual translation with ever-growing model and data, how to effectively train multilingual models has not been well understood. In this paper, we show that a common situation in multilingual training, data imbalance among languages, poses optimization tension between high resource and low resource languages where the found multilingual solution is often sub-optimal for low resources. We show that common training method which upsamples low resources can not robustly optimize population loss with risks of either underﬁtting high resource languages or overﬁtting low resource ones. Drawing on recent ﬁndings on the geometry of loss landscape and its effect on generalization, we propose a principled optimization algorithm, Curvature Aware Task Scaling (CATS), which adaptively rescales gradients from different tasks with a meta objective of guiding multilingual training to low-curvature neighborhoods with uniformly low loss for all languages. We ran experiments on common benchmarks (TED, WMT and
OPUS-100) with varying degrees of data imbalance. CATS effectively improved multilingual optimization and as a result demonstrated consistent gains on low resources (+0.8 to +2.2 BLEU) without hurting high resources.
In addition,
CATS is robust to overparameterization and large batch size training, making it a promising training method for massive multilingual models that truly improve low resource languages. 1

Introduction
Multilingual models have received growing interest in natural language processing (NLP) [34, 41, 10, 26, 2, 19, 58]. The task of multilingual machine translation aims to have one model which can translate between multiple languages pairs, which reduces training and deployment cost by improving parameter efﬁciency. It presents several research questions around crosslingual transfer learning and multi-task learning (MTL) [23, 1, 2, 27, 45, 28].
Recent progress in multilingual sequence modeling, with multilingual translation as a representative application, been extending the scale of massive multilingual learning, with increasing number of languages [2, 10, 58] , the amount of data [2, 12], as well as model size [27, 13]. Despite the power-law scaling of (English-only) language modeling loss with model, data and compute [25], it has been found that multilingual models do not always beneﬁt from scaling up model and data size, especially multilingual machine translation to multiple languages even after exploiting language proximity with external linguistic knowledge [12, 27, 51].
There has been limited understanding of the optimization aspects of multilingual models. Multilingual training is often implemented as monolithic with data from different languages simply combined. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Challenges were observed when training with imbalanced data [2, 26], which is common for multilin-gual NLP as only a few languages are rich in training data (high resource languages) while the rest of the languages in the world has zero or low training data (low resource languages) [24]. This has been mostly treated as a “data problem" with a widely used work-around of upsampling low resource languages’ data to make the data more balanced [2, 26, 10, 34, 58].
In this paper, we ﬁll the gap by systematically study the optimization of multilingual models in the task of multilingual machine translation with Transformer architecture. Our contribution is twofold.
First, we reveal the optimization tension between high resource and low resource languages where low resources’ performance often suffer. This had been overlooked in multilingual training but have important implications for achieving the goal of leveraging crosslingual transfer to improve low resources. We analyze the training objectives of multilingual models and identify an important role played by local curvature of the loss landscape, where “sharpness" causes interference among languages during optimization. This hypothesis is veriﬁed empirically, where we found optimization tension between high and low resource languages. They compete to update the loss landscape during early stage of training, with high resource ones dominating the optimization trajectory during the rest of training. Existing approaches such as upsampling low resources implicitly reduce this tension by augmenting training distribution towards more uniform. We show that this approach is not robust to different data characteristics, where it suffers from either overﬁtting low resources or underﬁtting high resources.
Second, we propose a principled training algorithm for multilingual models to mitigate such tension and effectively improve all languages. Our algorithm explicitly learn the weighting of different languages’ gradients with a meta-objective of guiding the optimization to “ﬂatter" neighborhoods with uniformly low loss (Curvature-Aware Task Scaling, CATS). Compared to static weighting implied by sampling probabilities of the data distribution, our method effectively reduced the optimization tension between high and low resource languages and improves the Pareto front of generalization.
On common benchmarks of multilingual translation, CATS consistently improves low resources in various data conditions, +0.8 BLEU on TED (8 languages, 700K sentence pairs), +2.2 BLEU on
WMT (10 languages, 30M sentence pairs), and +1.3 BLEU on OPUS-100 (100 languages, 50M sentence pairs) without sacriﬁcing performance on high resources. Furthermore, CATS can effectively leverage model capacity, yielding better generalization in overparameterized models. The training algorithm is conceptually simple and efﬁcient to apply to massive multilingual settings, making it a suitable approach for achieving equitable progress in NLP for every language. 2