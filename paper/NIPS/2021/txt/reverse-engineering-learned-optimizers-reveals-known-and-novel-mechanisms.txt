Abstract
Learned optimizers are parametric algorithms that can themselves be trained to solve optimization problems. In contrast to baseline optimizers (such as momentum or Adam) that use simple update rules derived from theoretical principles, learned optimizers use ﬂexible, high-dimensional, nonlinear parameterizations. Although this can lead to better performance, their inner workings remain a mystery. How is a given learned optimizer able to outperform a well tuned baseline? Has it learned a sophisticated combination of existing optimization techniques, or is it implementing completely new behavior? In this work, we address these questions by careful analysis and visualization of learned optimizers. We study learned optimizers trained from scratch on four disparate tasks, and discover that they have learned interpretable behavior, including: momentum, gradient clipping, learning rate schedules, and learning rate adaptation. Moreover, we show how dynamics and mechanisms inside of learned optimizers orchestrate these computations. Our re-sults help elucidate the previously murky understanding of how learned optimizers work, and establish tools for interpreting future learned optimizers. 1

Introduction
Optimization algorithms underlie nearly all of modern machine learning; thus advances in optimiza-tion have broad impact. Recent research uses meta-learning to learn new optimization algorithms, by directly parameterizing and training an optimizer on a distribution of tasks. These so-called learned optimizers have been shown to outperform baseline optimizers in restricted settings [1–7].
Despite improvements in the design, training, and performance of learned optimizers, fundamental questions remain about their behavior. We understand remarkably little about how these optimizers work. Are learned optimizers simply learning a clever combination of known techniques? Or do they learn fundamentally new behaviors that have not yet been proposed in the optimization literature? If they did learn a new optimization technique, how would we know?
Contrast this with existing “hand-designed” optimizers such as momentum [8], AdaGrad [9], RM-SProp [10], or Adam [11]. These algorithms are motivated and analyzed using intuitive mechanisms and theoretical principles (such as accumulating update velocity in momentum, or rescaling updates based on gradient magnitudes in RMSProp or Adam). This understanding of underlying mechanisms allows future studies to build on these techniques by highlighting ﬂaws in their operation [12],
∗Work conducted while at Google Research. Currently at Meta Reality Labs. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
studying convergence [13], and developing deeper knowledge about why key mechanisms work
[14]. Without analogous understanding of the inner workings of a learned optimizers, it is incredibly difﬁcult to analyze or synthesize their behavior.
In this work, we develop tools for isolating and elucidating mechanisms in nonlinear, high-dimensional learned optimization algorithms (§4). Using these methods we show how learned optimizers utilize both known and novel techniques, across four disparate tasks. In particular, we demonstrate that learned optimizers learn momentum (§5.1), gradient clipping (§5.2), learning rate schedules (§5.3), and methods for learning rate adaptation (§5.4, §5.5). Taken together, our work can be seen as part of a new approach to scientiﬁcally interpret and understand learned algorithms.
We provide code for training and analyzing learned optimizers, as well as the trained weights for the learned optimizers studied here, at https://bit.ly/3eqgNrH. 2