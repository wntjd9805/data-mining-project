Abstract
The stochastic multi-arm bandit problem has been extensively studied under stan-dard assumptions on the arm’s distribution (e.g bounded with known support, exponential family, etc). These assumptions are suitable for many real-world prob-lems but sometimes they require knowledge (on tails for instance) that may not be precisely accessible to the practitioner, raising the question of the robustness of bandit algorithms to model misspeciﬁcation. In this paper we study a generic
Dirichlet Sampling (DS) algorithm, based on pairwise comparisons of empirical indices computed with re-sampling of the arms’ observations and a data-dependent exploration bonus. We show that different variants of this strategy achieve provably optimal regret guarantees when the distributions are bounded and logarithmic regret for semi-bounded distributions with a mild quantile condition. We also show that a simple tuning achieve robustness with respect to a large class of unbounded distri-butions, at the cost of slightly worse than logarithmic asymptotic regret. We ﬁnally provide numerical experiments showing the merits of DS in a decision-making problem on synthetic agriculture data. 1

Introduction
The K-armed stochastic bandit model is a decision-making problem in which a learner sequentially picks an action among K alternatives, called arms, and collects a random reward. In this setting, all rewards drawn from an arm are independent and identically distributed. Hence, we can formally associate each arm k ∈ {1, . . . , K} with its reward distribution νk, with mean µk. The objective of the learner is to adapt her strategy (At)t∈[T ] in order to maximize the expected sum of rewards obtained after T selections (where T is the horizon, unknown to the learner). This is equivalent to minimizing the regret, deﬁned as the difference between the expected total reward of an oracle strategy always selecting an arm with largest mean and that of the algorithm, which is equal to
RT = E (cid:35)
µ(cid:63) − µAt
= (cid:34) T (cid:88) t=1
K (cid:88) k=1
∆kE [Nk(T )] . (1) t=1
Here, Nk(T ) = (cid:80)T 1(At = k) denotes the number of selections of arm k after T time steps,
µ(cid:63) = maxj∈{1,...,K} µj and ∆k = µ(cid:63) − µk is called the gap between arm k and the largest mean. To assess the performance of a bandit algorithm, one naturally studies the best guarantees achievable by a uniformly efﬁcient algorithm, i.e with sub-linear regret on any instance of a given class of problems.
This guarantee was ﬁrst provided by Lai and Robbins (1985) for 1-dimensional parametric families of distributions, and then extended by Burnetas and Katehakis (1996) for more general families. It 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
states that any algorithm that is uniformly efﬁcient1 on a family of distributions F must satisfy lim inf
T →∞
RT log(T )
≥ (cid:88) k:∆k>0
∆k inf (νk, µ(cid:63))
KF
, KF inf (νk, µ(cid:63)) = inf
G∈F
{KL(νk, G) : EG(X) > µ(cid:63)} . (2)
A bandit algorithm is then called asymptotically optimal for a family of distributions F when its regret matches this lower bound. When F is a Single-Parameter Exponential Family (SPEF), KF inf is simply the Kullback-Leibler divergence between the distribution of mean µk and that of mean µ(cid:63) in F, making for a theoretically appealing setting. The quantity KB inf , corresponding to the family
F[−∞,B] of distributions supported in (−∞, B] is also often considered in the literature, see e.g (Honda and Takemura, 2010, 2015; Cappé et al., 2013).
Overview of existing strategies An efﬁcient strategy faces the classical exploration/exploitation dilemma: it needs to obtain enough information from arms that have not been sampled a lot (ex-ploration), but also to sample arms that are well-performing sufﬁciently often (exploitation). Many algorithms have been proposed for the multi-armed bandits problem (see Lattimore and Szepesvári (2020) for a survey), and we propose in the following a non-exhaustive list of such methods. A ﬁrst category contains the deterministic index policies, built on the concept of Optimism in Face of Uncer-tainty, the most celebrated of which being the Upper Conﬁdence Bound (UCB) algorithms (Agrawal, 1995; Auer et al., 2002). These algorithms can obtain a logarithmic regret under classical hypothesis on the distributions (e.g bounded, sub-gaussian, sub-exponential, . . . ), and the strongest guarantees have been achieved by kl-UCB Cappé et al. (2013), DMED (Honda and Takemura, 2010), and IMED (Honda and Takemura, 2015), which share a common pattern of solving a convex optimization prob-lem at each round. To be asymptotically optimal, these algorithms require either 1) the knowledge of a speciﬁc SPEF for each arm, or 2) a known upper bound on the support of each arm. A second general category is that of randomized bandit algorithms, which has been formulated for instance in (Kveton et al., 2019b) as General Randomized Exploration (GRE). The common feature of these methods is that, at each time step and for each arm, the algorithm draws an index from a distribution that depends on 1) the rewards observed from the arm, and 2) some knowledge on the arms distributions and chooses the arm with the largest index. Thompson Sampling (TS) (Thompson, 1933; Agrawal and Goyal, 2012) belongs to this category, and a proper choice of Bayesian prior/posterior ensures optimality of TS in SPEF (Korda et al., 2013). Different algorithms using Bootstrapping schemes have also been proposed (Osband and Roy, 2015; Kveton et al., 2019a,b; Wang et al., 2020; Riou and
Honda, 2020): they share the idea of computing a noisy mean for empirical samples, enhanced by some exploration aid appropriately tuned to the family of distributions they consider. A last category contains the methods based on sub-sampling Baransi et al. (2014); Chan (2020); Baudry et al. (2020, 2021b), that achieve asymptotic optimality in SPEF without knowing which family, when all arms share the same. However the proofs heavily rely on properties of the tails of SPEF so the results seem difﬁcult to generalize outside these families.
Motivations While many algorithms achieve optimal regret for bounded distributions with the sole knowledge of the upper bound, the assumptions needed for algorithms working with unbounded distributions (e.g SPEF, sub-Gaussian, sub-exponential) generally assume a known parametric model for the tails. While such assumption entails convenient properties on the theoretical side, the practitioner may have some difﬁculty to determine which setting/parameters correspond to her problem. Furthermore, this uncertainty raises the question of robustness with respect to these hypotheses. Several works have considered this question: Hadiji and Stoltz (2020) shows that adapting to an unknown bounded range requires a tradeoff between instance-dependent and worst-case regret, and recently (Agrawal et al., 2020; Ashutosh et al., 2021) proved the impossibility of an instance-dependent logarithmic regret for light-tailed distributions without explicit control on the tail parameters. The root cause for this is the lack of compactness of such families F, which allows mass to "leak" at inﬁnity so that maximally confusing distributions with mean µ∗ exist arbitrarily close to
νk, meaning KF inf (νk, µ∗) = 0. The latter work also introduces a robust variant of UCB, that trades off logarithmic regret for O (f (T ) log(T )), where f essentially tracks the possible mass leakage at inﬁnity. These results puts into question the usual hypotheses under which bandit algorithms are designed: considering a parametric control of the tails is indeed sensitive to model mis-speciﬁcation, but on the other hand the examples chosen to prove infeasability results seem a bit extreme for the practitioner. In this paper, we propose simple alternative setups allowing unspeciﬁed tail shapes 1That is, for each bandit on F, for each arm k with ∆k > 0, then E[Nk(T )] = o(T α) for all α ∈ (0, 1]. 2
but avoiding "mass leakage" to inﬁnity, for instance with mild conditions linking the quantiles and the means of the distributions. We consider in this paper light-tailed distributions (see deﬁnition in Appendix A.1). This problem is already non-trivial, so we let possible extensions for heavy-tail distributions for future work (e.g with tools like median-of-means, see (Bubeck et al., 2013)).
Outline
In the novel settings we consider, we want algorithms that require the smallest level of knowledge on the tails of distributions. To this extent, the Non-Parametric Thompson Sampling (NPTS, Riou and Honda (2020)) algorithm is a good candidate, considering how little knowledge it requires to reach asymptotic optimality for bounded distributions with known bounds. Furthermore, the ﬂexibility of this algorithm has been recently demonstrated with its adaptation in a risk-aware setting (Baudry et al., 2021a). We provide a generalization of NPTS that we call Dirichlet Sampling (DS): we combine the core elements of NPTS and a duel-based framework inspired by (Chan, 2020), introducing data-dependent exploration bonuses. We present the resulting algorithm and detail the technical motivations of this approach in Section 2. We then introduce in Section 3 a ﬁrst regret decomposition of DS algorithms under general assumptions, and the technical results that allow to ﬁne-tune the algorithm for different families (see Section 3.1). We provide three instances of DS algorithms and their regret guarantees in Section 3.2: Bounded Dirichlet Sampling (BDS) tackles bounded distributions with possibly unknown upper bounds, Quantile Dirichlet Sampling proposes a ﬁrst generalization to the unbounded case using truncated distributions. Last, Robust
Dirichlet Sampling (RDS) has a slightly larger than logarithmic regret for any unspeciﬁed light-tailed unbounded distributions, making it a competitor to the Robust-UCB algorithm of Ashutosh et al. (2021). Finally, we study in Section 4 a use-case in agriculture using the DSSAT simulator (see
Hoogenboom et al. (2019)), which naturally faces all the questions (robustness, model speciﬁcation) that motivate this work and shows the merit of DS over state-of-the-art methods for this problem. 2 Dirichlet Sampling Algorithms
In this section we introduce Dirichlet Sampling, a strategy that aims at generalizing the Non-Parametric Thompson Sampling algorithm of Riou and Honda (2020) outside the scope of bounded distributions with a known support upper bound. For this purpose, we build an adaptive strategy in a duel-based framework, already used in sub-sampling based algorithms like SSMC (Chan, 2020).