Abstract
Generative Adversarial Networks (GANs) produce high-quality images but are challenging to train. They need careful regularization, vast amounts of compute, and expensive hyper-parameter sweeps. We make signiﬁcant headway on these is-sues by projecting generated and real samples into a ﬁxed, pretrained feature space.
Motivated by the ﬁnding that the discriminator cannot fully exploit features from deeper layers of the pretrained model, we propose a more effective strategy that mixes features across channels and resolutions. Our Projected GAN improves im-age quality, sample efﬁciency, and convergence speed. It is further compatible with resolutions of up to one Megapixel and advances the state-of-the-art Fréchet In-ception Distance (FID) on twenty-two benchmark datasets. Importantly, Projected
GANs match the previously lowest FIDs up to 40 times faster, cutting the wall-clock time from 5 days to less than 3 hours given the same computational resources.
Figure 1: Convergence with Projected GANs. Evolution of samples for a ﬁxed latent code during training on the AFHQ-Dog dataset [5]. We ﬁnd that discriminating features in the projected feature space speeds up convergence and yields lower FIDs. This ﬁnding is consistent across many datasets. 1

Introduction
A Generative Adversarial Network (GAN) consists of a generator and a discriminator. For image synthesis, the generator’s task is to generate an RGB image; the discriminator aims to distinguish real from fake samples. On closer inspection, the discriminator’s task is two-fold: First, it projects the real and fake samples into a meaningful space, i.e., it learns a representation of the input space.
Second, it discriminates based on this representation. Unfortunately, training the discriminator jointly with the generator is a notoriously hard task. While discriminator regularization techniques help to balance the adversarial game [31], standard regularization methods like gradient penalties [36] are susceptible to hyperparameter choices [26] and can lead to a substantial decrease in performance [4].
In this paper, we explore the utility of pretrained representations to improve and stabilize GAN training. Using pretrained representations has become ubiquitous in computer vision [29, 30, 48] 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
and natural language processing [18, 45, 47]. While combining pretrained perceptual networks [58] with GANs for image-to-image translation has led to impressive results [14, 49, 59, 64], this idea has not yet materialized for unconditional noise-to-image synthesis. Indeed, we conﬁrm that a naïve application of this idea does not lead to state-of-the-art results (Section 4) as strong pretrained fea-tures enable the discriminator to dominate the two-player game, resulting in vanishing gradients for the generator [2]. In this work, we demonstrate how these challenges can be overcome and identify two key components for exploiting the full potential of pretrained perceptual feature spaces for GAN training: feature pyramids to enable multi-scale feedback with multiple discriminators and random projections to better utilize deeper layers of the pretrained network.
We conduct extensive experiments on small and large datasets with a resolution of up to 10242 pixels.
Across all datasets, we demonstrate state-of-the-art image synthesis results at signiﬁcantly reduced training time (Fig. 1). We also ﬁnd that Projected GANs increase data efﬁciency and avoid the need for additional regularization, rendering expensive hyperparameter sweeps unnecessary. Code, models, and supplementary videos can be found on the project page https://sites.google.com/view/ projected-gan. 2