Abstract
Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains.
Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: https://cvir.github.io/projects/comix. 1

Introduction
Unsupervised domain adaptation (UDA), which alleviates the requirement of large amounts of annotated data by adapting a model learned on a labelled source domain to an unlabelled target domain, has drawn a great deal of attention in the last few years [12, 80]. Much progress has been made in developing deep UDA methods by minimizing the cross-domain divergence [39, 70], adding adversarial domain discriminators [20, 74], and image-to-image translation techniques [26, 51].
However, despite impressive results on commonly used benchmark datasets (e.g., [61, 75, 57]), most of the methods have been developed only for images and not for videos, where the annotation task is often more complicated requiring tedious human labor in comparison to images.
More recently, very few works have attempted deep UDA for video action recognition by directly matching segment-level features [8, 27, 50, 42] or with attention weights [11, 53]. However, (1) trivially matching segment-level feature distributions by extending the image-speciﬁc approaches, without considering the rich temporal information may not alone be sufﬁcient for video domain adaptation; (2) prior methods often focus on aligning target features with source, rather than exploiting any action semantics shared across both domains (e.g., difference in background with the same action: videos in the top row of Figure 1 are from the source and target domain respectively, but both capture the same action walking); (3) existing methods often rely on complex adversarial learning which is unwieldy to train, resulting in very fragile convergence. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Meanwhile, self-supervised pretext tasks like predicting rotation and translation have recently emerged as an alternative to adversarial learning for unsupervised domain adaptation in images [38, 71]. While these works show the promising potential of self-supervised learning in aligning source and target domains, the more recent very successful contrastive representation learning [9, 23, 52] has never been used to adapt video action recognition models to target domains. Motivated by this, in this paper, we explore the following natural, yet important question: whether and how contrastive learning could be exploited for the challenging and practically important task of unsupervised video domain adaptation for human action recognition?
To this end, we introduce Contrast and Mix (CoMix), a simple yet effective approach based on contrastive learning to adapt video action recognition models trained on a labeled source domain to unlabelled target domains. First, we propose to represent video as a graph and then utilize temporal contrastive self-supervised learning over the graph representations as a nexus between source and target domains to align features, without requiring any additional adversarial learning, as most prior works do in video domain adaptation [8, 11, 53]. Speciﬁ-cally, we maximize the similarity between en-coded representations of the same video at two different speeds as well as minimize the sim-ilarity between different videos played at dif-ferent speeds, leveraging the fact that changing video speed does not change an action on both domains. While minimization of contrastive self-supervised losses in both domains simulta-neously helps in domain alignment, it ignores action semantics shared across them as the loss treats each domain individually. To alleviate this, we incorporate new synthetic videos into the temporal contrastive objective, which are obtained by mixing background of a video from one domain to a video from another domain, as shown in
Figure 1 (bottom). Importantly, since mixing background doesn’t change the temporal dynamics, we introduce pseudo-labels for the mixed videos to be same as the label of the original videos and consider additional positives per anchor (see Figure 2), which encourages the model to generalize to new samples that may not be covered by temporal contrastive learning in hand. In other words, mixed background video of an input sample in the embedding space act as small semantic perturbations that are not imaginary, i.e., they are representative of the action semantics shared across source and target domains. Finally, rather than relying only on the supervision of source categories to learn a discriminative representation, we generate pseudo-labels for the target samples in every batch and then harness the label information using a temporal supervised contrastive term, that pushes the examples from the same class close and the examples from different classes further apart (Figure 2: right). While our modiﬁed contrastive losses are motivated by the supervised contrastive learning [30], we use pseudo labels for exploiting shared action semantics and discriminative information from target domain, instead of using true labels as an alternative to supervised cross-entropy loss (which is not present for target samples). To the best of our knowledge, ours is the ﬁrst work that successfully leverages contrastive learning in an uniﬁed framework to align cross-domain features while enhancing discriminabilty of the latent space for unsupervised video domain adaptation.
Figure 1: