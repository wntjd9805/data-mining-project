Abstract
We propose a novel framework for multi-person 3D motion trajectory prediction.
Our key observation is that a human’s action and behaviors may highly depend on the other persons around. Thus, instead of predicting each human pose trajectory in isolation, we introduce a Multi-Range Transformers model which contains of a local-range encoder for individual motion and a global-range encoder for social interactions. The Transformer decoder then performs prediction for each person by taking a corresponding pose as a query which attends to both local and global-range encoder features. Our model not only outperforms state-of-the-art methods on long-term 3D motion prediction, but also generates diverse social interactions.
More interestingly, our model can even predict 15-person motion simultaneously by automatically dividing the persons into different interaction groups. Project page with code is available at https://jiashunwang.github.io/MRT/. 1

Introduction
Given a few time steps of human motion, we are able to forecast how the person will continue to move and imagine the complex dynamics of their motion in the future. The ability to perform such predictions allows us to react and plan our own behaviors. Similarly, a predictive model for human motion is an essential component for many real world computer vision applications such as surveillance systems, and collision avoidance for robotics and autonomous vehicles. The research on 3D human motion prediction has caught a lot of attention in recent years [44, 43], where deep models are designed to take a few steps of 3D motion trajectory as inputs and predict a long-term future 3D motion trajectory as the outputs.
While encouraging results have been shown in previous work, most of the research focus on single human 3D motion prediction. Our key observation is that, how a human acts and behaves may highly depend on the people around. Especially during interactions with multiple agents, an agent will need to predict the other agents’ intentions, and then respond accordingly [54]. Thus instead of predicting each human motion in isolation, we propose to build a model to predict multi-person 3D motion and interactions. Such a model needs the following properties: (i) understand each agent’s own motion in previous time steps to obtain smooth and natural future motion; (ii) within a crowd of agents, understand which agents are interacting with each other and learn to predict based on the social interactions; (iii) the time scale for prediction needs to be long-term.
In this paper, we introduce Multi-Range Transformers for multi-person 3D motion trajectory pre-diction. The Transformer [63] has shown to be very effective in modeling long-term relations in language modeling [16] and recently in visual recognition [17]. Inspired by these encouraging results, we propose to explore Transformer models for predicting long-term human motion (3 seconds into the future). Our Multi-Range Transformers contain a local-range Transformer encoder for each individual person trajectory, a global-range Transformer encoder for modeling social interactions, and a Transformer decoder for predicting each person’s future motion trajectory in 3D. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Our motion prediction results. Left: The last time step of the input sequence. Right: The predicted diverse and continuous motion with multi-person social interactions. We use different color to indicate different persons and the darker color for the further in future for predictions.
Speciﬁcally, given the human pose joints (with 3D locations in the world coordinate) in 1-second time steps as inputs, the local-range Transformer encoder processes each person’s trajectory separately and focuses on the local motion for smooth and natural prediction. The global-range Transformer encoder performs self-attention on 3D pose joints across different persons and different time steps, and it automatically learns which persons that one person should be attending to model their social interactions. Our Transformer decoder will then take a single human 3D pose in one time step as the query input and encoder features as the key and value inputs to compute attention for prediction.
We perform prediction for different persons by using different query pose inputs. By using only one time step person pose as the query for the decoder instead of a sequence of motion steps, we create a bottleneck to force the Transformer to exploit the relations between different time steps and persons in the encoders, instead of just repeating the existing motion alone [43].
We perform our experiments on multiple datasets including CMU-Mocap [1], MuPoTS-3D [48], 3DPW [64] for multi-person motion prediction in 3D (with 2 ∼ 3 persons). Our method achieves a signiﬁcant improvement over state-of-the-art approaches for long-term predictions and the gain enlarges as we increase the future prediction time steps from 1 second to 3 seconds. Qualitatively, we visualize that our method can predict interesting behaviors and interactions between different persons while previous approaches will repeat the same poses as it goes to further steps in the future.
More interestingly, we extend the task to perform prediction with 9 ∼ 15 persons by mixing the
CMU-Mocap [1] and the Panoptic [24] datasets. We visualize part of the prediction results with multi-person interactions in Fig. 1. We show that Multi-Range Transformers can not only perform predictions with a crowd of persons, but also learn to automatically group the persons into different social interaction clusters using the attention mechanism. 2