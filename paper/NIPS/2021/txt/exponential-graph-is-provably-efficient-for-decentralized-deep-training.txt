Abstract
Decentralized SGD is an emerging training method for deep learning known for its much less (thus faster) communication per iteration, which relaxes the averaging step in parallel SGD to inexact averaging. The less exact the averaging is, however, the more the total iterations the training needs to take. Therefore, the key to making decentralized SGD efﬁcient is to realize nearly-exact averaging using little communication. This requires a skillful choice of communication topology, which is an under-studied topic in decentralized optimization.
In this paper, we study so-called exponential graphs where every node is con-nected to O(log(n)) neighbors and n is the total number of nodes. This work proves such graphs can lead to both fast communication and effective averaging simultaneously. We also discover that a sequence of log(n) one-peer exponential graphs, in which each node communicates to one single neighbor per iteration, can together achieve exact averaging. This favorable property enables one-peer exponential graph to average as effective as its static counterpart but commu-nicates more efﬁciently. We apply these exponential graphs in decentralized (momentum) SGD to obtain the state-of-the-art balance between per-iteration communication and iteration complexity among all commonly-used topologies.
Experimental results on a variety of tasks and models demonstrate that decen-tralized (momentum) SGD over exponential graphs promises both fast and high-quality training. Our code is implemented through BlueFog and available at https://github.com/Bluefog-Lib/NeurIPS2021-Exponential-Graph. 1

Introduction
Efﬁcient distributed training methods across multiple computing nodes are critical for large-scale modern deep learning tasks. Parallel stochastic gradient descent (SGD) is a widely-used approach, which, at each iteration, computes a globally averaged gradient either using Parameter-Server [28] or All-Reduce [47]. Such global coordination across all nodes in parallel SGD results in either signiﬁcant bandwidth cost or high latency, which can notably hamper the training scalability.
Decentralized SGD [45, 11, 30, 3] based on partial averaging has been one of the promising alternatives to parallel SGD in distributed deep training. Partial averaging, as opposed to the global averaging exploited in parallel SGD, only requires each node to compute the locally averaged model within its neighborhood. Decentralized SGD does not involve any global operations, so it has much lower communication overhead per iteration. The fewer neighbors each node needs to communicate, the more efﬁcient the per-iteration communication is in decentralized SGD.
∗Equal Contribution. Corresponding Author: Wotao Yin 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Comparison between decentralized (momentum) SGD over (some) various commonly-used topologes.
The table assumes homogeneous data distributions across all nodes (which is practical for deep training within a data-center). The comparison for data-heterogeneous scenarios, and with more other topologies, is listed in
Appendix C. The smaller the transient iteration complexity is, the faster decentralized algorithms will converge.
Topology
Ring
Grid Rand-Graph Rand-Match
Static Exp One-peer Exp
Per-iter Comm. Ω(2) Ω(4)
Trans. Iters.
Ω(n7) Ω(n5)
Ω( n 2 )
Ω(n3)
Ω(1)
−
Ω(log2(n))
Ω(1) 2(n)) Ω(n3 log2 2(n))
Ω(n3 log2
The reduced communication in decentralized SGD comes with a cost: slower convergence. While it can asymptotically achieve the same convergence linear speedup as parallel SGD [30, 3, 25, 64], i.e., the training speed increases proportionally to the number of computing nodes (see the deﬁnition in Sec. 2), decentralized SGD requires more iterations to reach that stage due to the ineffectiveness to aggregate information using partial averaging. We refer those iterations before decentralized
SGD reaches its linear speedup stage as transient iterations (see the deﬁnition in Sec. 2), which is an important metric to measure the inﬂuence of partial-averaging [48, 65] on convergence rate of decentralized SGD. The less effective the partial averaging is, the more transient iterations decentralized SGD needs to take. Fig. 1 illustrates the transient iterations of decentralized SGD for the logistic regression problem. It is observed that decentralized SGD can asymptotically converge as fast as parallel SGD, but it requires more iterations (i.e., transient iterations) to reach that stage.
Per-iteration communication and transient iterations in de-centralized SGD are determined by the network topology (we also use graph interchangeably with topology). The maximum degree of the graph decides the communication cost while the connectivity inﬂuences the transient itera-tion complexity. Generally speaking, a sparsely-connected topology communicates cheaply but endows decentralized
SGD with more transient iterations due to the less effec-tive information aggregation. A skillful choice of network topology, which is critical to achieve balance between per-iteration communication and transient iteration complexity, is under-studied in literature.
Figure 1: Illustration of transient iters.
Experimental setting is in Appendix D.5.
This work studies exponential graphs which are empiri-cally successful [3, 61, 27, 14, 67] but less theoretically understood in deep training. Exponential graphs have two variants. In a static exponential graph, each node communicates to (cid:100)log2(n)(cid:101) neighbors (see Sec. 3 and Fig. 2). In one-peer exponential graph, however, each node cycles through all its neighbors, communicating, only, to a single neighbor per iteration (see Sec. 4 and Fig. 2). This paper will ﬁrst clarify the connectivity and averaging effectiveness of these exponential graphs, and then apply them to decentralized momentum SGD to obtain the state-of-the-art balance between per-iteration communication and transient iteration complexity among all commonly-used topologies.
Our main results (as well as our contributions) are:
• We prove that the spectral gap, which is used to measure the connectivity of the graph (see the deﬁnition in Sec. 2), of the static exponential graph is upper bounded by O(1/ log2(n)). Before us, many literatures (e.g. [27]) claimed its upper bound to be O(1) incorrectly.
• Since one-peer exponential graphs are time-varying, it is difﬁcult to derive their spectral gaps.
However, we establish that any log2(n) consecutive sequence of one-peer exponential graphs can together achieve exact averaging when n is a power of 2.
• With the above results, we establish that one-peer exponential graph, though much sparser than its static counterpart, surprisingly endows decentralized momentum SGD with the same convergence rate as static exponential graph in terms of the best-known bounds.
• We derive that exponential graphs achieve ˜Ω(1)2 per-iteration communication and ˜Ω(n3) tran-sient iterations, both of which are nearly the best among other known topologies, see Table 1.
The one-peer exponential graph is particularly recommended for decentralized deep training.
• We conduct extensive industry-level experiments across different tasks and models with various decentralized methods, graphs, and network size to validate our theoretical results. 2Notation ˜Ω(·) hides all logarithm factors. 2
Figure 2: Illustration of the static and one-peer exponential graph. 2 Revisit Decentralized Momentum SGD and