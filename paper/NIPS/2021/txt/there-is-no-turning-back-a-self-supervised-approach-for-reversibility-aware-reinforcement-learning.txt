Abstract
We propose to learn to distinguish reversible from irreversible actions for better informed decision-making in Reinforcement Learning (RL). From theoretical considerations, we show that approximate reversibility can be learned through a simple surrogate task: ranking randomly sampled trajectory events in chronological order. Intuitively, pairs of events that are always observed in the same order are likely to be separated by an irreversible sequence of actions. Conveniently, learning the temporal order of events can be done in a fully self-supervised way, which we use to estimate the reversibility of actions from experience, without any priors. We propose two different strategies that incorporate reversibility in RL agents, one strategy for exploration (RAE) and one strategy for control (RAC). We demonstrate the potential of reversibility-aware agents in several environments, including the challenging Sokoban game. In synthetic tasks, we show that we can learn control policies that never fail and reduce to zero the side-effects of interactions, even without access to the reward function. 1

Introduction
We address the problem of estimating if and how easily actions can be reversed in the Reinforcement
Learning (RL) context. Irreversible outcomes are often not to be taken lightly when making decisions.
As humans, we spend more time evaluating the outcomes of our actions when we know they are irreversible [29]. As such, irreversibility can be positive (i.e. takes risk away for good) or negative (i.e. leads to later regret). Also, decision-makers are more likely to anticipate regret for hard-to-reverse decisions [50]. All in all, irreversibility seems to be a good prior to exploit for more principled decision-making. In this work, we explore the option of using irreversibility to guide decision-making and conﬁrm the following assertion: by estimating and factoring reversibility in the action selection process, safer behaviors emerge in environments with intrinsic risk factors. In addition to this, we
∗Equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
show that exploiting reversibility leads to more efﬁcient exploration in environments with undesirable irreversible behaviors, including the famously difﬁcult Sokoban puzzle game.
However, estimating the reversibility of actions is no easy feat. It seemingly requires a combination of planning and causal reasoning in large dimensional spaces. We instead opt for another, simpler approach (see Fig. 1): we propose to learn in which direction time ﬂows between two observations, directly from the agents’ experience, and then consider irreversible the transitions that are assigned a temporal direction with high conﬁdence. In ﬁne, we reduce reversibility to a simple classiﬁcation task that consists in predicting the temporal order of events.
Our contributions are the following: 1) we formalize the link between reversibility and precedence es-timation, and show that reversibility can be approximated via temporal order, 2) we propose a practical algorithm to learn temporal order in a self-supervised way, through simple binary classiﬁcation using sampled pairs of observations from trajectories, 3) we propose two novel exploration and control strate-gies that incorporate reversibility, and study their practical use for directed exploration and safe RL, illustrating their relative merits in synthetic as well as more involved tasks such as Sokoban puzzles. 2