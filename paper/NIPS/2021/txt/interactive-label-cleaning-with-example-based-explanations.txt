Abstract
We tackle sequential learning under label noise in applications where a human supervisor can be queried to relabel suspicious examples. Existing approaches are ﬂawed, in that they only relabel incoming examples that look “suspicious” to the model. As a consequence, those mislabeled examples that elude (or don’t undergo) this cleaning step end up tainting the training data and the model with no further chance of being cleaned. We propose CINCER, a novel approach that cleans both new and past data by identifying pairs of mutually incompatible examples.
Whenever it detects a suspicious example, CINCER identiﬁes a counter-example in the training set that—according to the model—is maximally incompatible with the suspicious example, and asks the annotator to relabel either or both examples, resolving this possible inconsistency. The counter-examples are chosen to be maximally incompatible, so to serve as explanations of the model’s suspicion, and highly inﬂuential, so to convey as much information as possible if relabeled.
CINCER achieves this by leveraging an efﬁcient and robust approximation of inﬂuence functions based on the Fisher information matrix (FIM). Our extensive empirical evaluation shows that clarifying the reasons behind the model’s suspicions by cleaning the counter-examples helps in acquiring substantially better data and models, especially when paired with our FIM approximation. 1

Introduction
Label noise is a major issue in machine learning as it can lead to compromised predictive performance and unreliable models [1, 2]. We focus on sequential learning settings in which a human supervisor, usually a domain expert, can be asked to double-check and relabel any potentially mislabeled example. Applications include crowd-sourced machine learning and citizen science, where trained researchers can be asked to clean the labels provided by crowd-workers [3, 4], and interactive personal assistants [5], where the user self-reports the initial annotations (e.g., about activities being performed) and unreliability is due to memory bias [6], unwillingness to report [7], or conditioning [8].
This problem is often tackled by monitoring for incoming examples that are likely to be mislabeled, aka suspicious examples, and ask the supervisor to provide clean (or at least better) annotations for them. Existing approaches, however, focus solely on cleaning the incoming examples [9, 4, 10, 5].
This means that noisy examples that did not undergo the cleaning step (e.g., those in the initial 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Suspicious example and counter-examples selected using (from left to right) CINCER, 1-NN and inﬂuence functions (IF), on noisy MNIST. Left: the suspicious example is mislabeled, the machine’s suspicion is supported by a clean counter-example. Right: the suspicious example is not mislabeled, the machine is wrongly suspicious because the counter-example is mislabeled. CINCER’s counter-example is contrastive and inﬂuential; 1-NN’s is not inﬂuential and IF’s is not pertinent, see desiderata D1–D3 below. bootstrap data set) or that managed to elude it are left untouched. This degrades the quality of the model and prevents it from spotting future mislabeled examples that fall in regions affected by noise.
We introduce CINCER (Contrastive and InﬂueNt CounterExample stRategy), a new explainable interactive label cleaning algorithm that lets the annotator observe and ﬁx the reasons behind the model’s suspicions. For every suspicious example that it ﬁnds, CINCER identiﬁes a counter-example, i.e., a training example that maximally supports the machine’s suspicion. The idea is that the example/counter-example pair captures a potential inconsistency in the data—as viewed from the model’s perspective—which is resolved by invoking the supervisor. More speciﬁcally, CINCER asks the user to relabel the example, the counter-example, or both, thus improving the quality of, and promoting consistency between, the data and the model. Two hypothetical rounds of interaction on a noisy version of MNIST are illustrated in Figure 1.
CINCER relies on a principled deﬁnition of counter-examples derived from few explicit, intuitive desiderata, using inﬂuence functions [11, 12]. The resulting counter-example selection problem is solved using a simple and efﬁcient approximation based on the Fisher information matrix [13] that consistently outperforms more complex alternatives in our experiments.
Contributions: Summarizing, we: 1) Introduce CINCER, an explanatory interactive label cleaning strategy that leverages example-based explanations to identify inconsistencies in the data—as per-ceived by the model—and enable the annotator to ﬁx them. 2) Show how to select counter-examples that at the same time explain why the model is suspicious and that are highly informative using (an efﬁcient approximation of) inﬂuence functions. 3) Present an extensive empirical evaluation that showcases the ability of CINCER of building cleaner data sets and better models. 2