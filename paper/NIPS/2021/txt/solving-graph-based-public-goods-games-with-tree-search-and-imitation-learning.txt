Abstract
Public goods games represent insightful settings for studying incentives for indi-vidual agents to make contributions that, while costly for each of them, beneﬁt the wider society. In this work, we adopt the perspective of a central planner with a global view of a network of self-interested agents and the goal of maximizing some desired property in the context of a best-shot public goods game. Existing algorithms for this known NP-complete problem ﬁnd solutions that are sub-optimal and cannot optimize for criteria other than social welfare.
In order to efﬁciently solve public goods games, our proposed method directly exploits the correspondence between equilibria and the Maximal Independent Set (mIS) structural property of graphs. In particular, we deﬁne a Markov Decision
Process which incrementally generates an mIS, and adopt a planning method to search for equilibria, outperforming existing methods. Furthermore, we devise a graph imitation learning technique that uses demonstrations of the search to obtain a graph neural network parametrized policy which quickly generalizes to unseen game instances. Our evaluation results show that this policy is able to reach 99.5% of the performance of the planning method while being three orders of magnitude faster to evaluate on the largest graphs tested. The methods presented in this work can be applied to a large class of public goods games of potentially high societal impact and more broadly to other graph combinatorial optimization problems. 1

Introduction
In a public goods game (PGG), individuals can choose to invest in an expensive good (paying a cost), with beneﬁts being shared by wider society [37]. It is a form of n-party social dilemma that has been used to study the tension between decisions that beneﬁt the individual and the common good [36].
Aspects characteristic to public goods are observed in many important societal problems such as meeting climate change targets [32, 41], the dynamics of research and innovation [29], the design of effective vaccination programs [19], and, more generally, situations in which contributions are non-excludable. The analysis of this class of games is related to ongoing efforts to study cooperation in multi-agent systems as a means of driving progress on societal challenges [13].
The best-shot PGG is a variant in which investment decisions are binary and agents beneﬁt if either they or a neighbor own the good [26]. Since patterns of connections along social and geographical dimensions in networks are known to shape individual decision-making [9, 23], a natural restriction is to limit the impact of contributions to an agent’s neighbors. Graph-based best-shot public goods games exhibit multiple pure-strategy Nash equilibria (PSNE) [16]. Given this multiplicity, a natural question that arises is how to compute equilibria that satisfy some properties, a task known to be
NP-complete in general for multiplayer games [22, 12]. Examples of desirable equilibria are those that maximize the social welfare (total utility) of agents or those with a high degree of fairness in terms of contributions. For graph-based best-shot PGGs, it has been shown that each equilibrium 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Schematic of our approach for ﬁnding desirable equilibria in the graph-based best-shot game. (a) We exploit the correspondence between agents acquiring the public good in equilibria (pictured in dark blue above) and the Maximal Independent Set (mIS) structural property of graphs.
We deﬁne an MDP that incrementally grows an independent set until it is maximal, and use Monte
Carlo Tree Search (MCTS) to plan in this MDP in order to ﬁnd desirable equilibrium conﬁgurations of the game. (b) We propose a Graph Imitation Learning method which uses demonstrations of the
MCTS policy π to learn a policy ˆπ parametrized by a graph neural network. (c) We use ˆπ to ﬁnd optimal equilibrium conﬁgurations on unseen instances of the game. corresponds to a Maximal Independent Set (mIS) [9]: a set of vertices of maximal size in which no two nodes are adjacent. Since enumerating mISs to identify desirable equilibria quickly becomes unfeasible for non-trivially sized graphs, practical alternatives are needed for larger graphs.
Towards this goal, Dall’Asta et al. [16] proposed a centralized algorithm based on best-response dynamics that converges to the optimal equilibrium (w.r.t. social welfare) in the limit of inﬁnite time, and suggested a simulated annealing alternative. Levit et al. [38] proved that the general version of the best-shot PGG is a potential game and derived an algorithm for ﬁnding equilibria based on side payments, which are used by agents that are unhappy with their outcome to convince neighbors to switch. While superior results were obtained over best-response dynamics, there is still a wide gap between the equilibria found by this approach and optimal equilibria as found by exhaustive search on small graphs. Furthermore, current methods cannot optimize for criteria other than social welfare.
Contributions. Our contributions can be summarized as follows: 1. We propose to directly take advantage of the connection between equilibria in this class of games and Maximal Independent Sets. This relationship allows us to deﬁne an MDP which incrementally generates an mIS to optimize a desired property; thus, every conﬁguration found is, by construction, an equilibrium of the game.1 We adopt a variant of the Monte
Carlo Tree Search algorithm for planning in this MDP. On small graphs, where an exhaustive enumeration of equilibria can be performed, the best outcomes found by this method are matched in most settings. On larger graphs existing methods are outperformed, especially in cases where the costs for acquiring the public good differ among players. 2. We devise a way to learn the structure of the solutions found by the planning algorithm based on imitation learning, such that predictions can be obtained on unseen game instances without repeating the search process. Speciﬁcally, we use a dataset of demonstrations of the search in order to learn a graph neural network parametrized policy through imitation learning, a procedure we call Graph Imitation Learning. The resulting policy is able to achieve 99.5% of the performance of the search method while being approximately three orders of magnitude quicker to evaluate and even exceeding the performance of the original search in some cases. This method is applicable beyond this class of networked public goods games, i.e., to a variety of graph-based decision-making problems where a model of the
MDP is available and the goal is to maximize a graph-level objective function. 1We highlight the difference between Maximal Independent Set and Maximum Independent Set. A Maximal
IS (mIS) is an IS that is not a proper subset of another IS. A Maximum IS (MIS) is an mIS of the largest possible size. In PGGs, an MIS may not be a desirable equilibrium, since it involves many players expending the cost. 2
2