Abstract
This paper studies model-based bandit and reinforcement learning (RL) with non-linear function approximations. We propose to study convergence to approximate local maxima because we show that global convergence is statistically intractable even for one-layer neural net bandit with a deterministic reward. For both non-linear bandit and RL, the paper presents a model-based algorithm, Virtual Ascent with Online Model Learner (ViOlin), which provably converges to a local max-imum with sample complexity that only depends on the sequential Rademacher complexity of the model class. Our results imply novel sample complexity bounds on several concrete settings such as linear bandit with ﬁnite or sparse model class, and two-layer neural net bandit. A key algorithmic insight is that optimism may lead to over-exploration even for two-layer neural net model class. On the other hand, for convergence to local maxima, it sufﬁces to maximize the virtual return if the model can also reasonably predict the gradient and Hessian of the real return. 1

Introduction
Recent progresses demonstrate many successful applications of deep reinforcement learning (RL) in robotics [49], games [7, 66], computational biology [56], etc. However, theoretical understanding of deep RL algorithms is limited. Last few years witnessed a plethora of results on linear function approximations in RL [80, 65, 40, 73, 71, 19, 3, 32], but the analysis techniques appear to strongly rely on (approximate) linearity and hard to generalize to neural networks.
The goal of this paper is to theoretically analyze model-based nonlinear bandit and RL with neural net approximation, which achieves amazing sample-efﬁciency in practice (see e.g., [37, 10, 30, 31, 12]). We focus on the setting where the state and action spaces are continuous.
Past theoretical work on model-based RL studies families of dynamics with restricted complexity measures such as Eluder dimension [60], witness rank [68], the linear dimensionality [77], and others [58, 42, 17]. Implications of these complexity measures have been studied, e.g., ﬁnite mixture of dynamics [6] and linear models [64] have bounded Eluder dimensions. However, it turns out that none of the complexity measures apply to the family of MDPs with even barely nonlinear dynamics, e.g., MDPs with dynamics parameterized by all one-layer neural network with a single activation unit (and with bounded weight norms). For example, in Theorem 5.2, we will prove that one-layer neural nets do not have polynomially-bounded Eluder dimension.1 (See more evidence below.)
The limited progress on neural net approximation is to some extent not surprising. Given a determin-istic dynamics with known neural net parameters, ﬁnding the best parameterized policy still involves optimizing a complex non-concave function, which is in general computationally intractable. More 1This result is also proved by the concurrent Li et al. [50, Theorem 8] independently. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
fundamentally, we ﬁnd that it is also statistically intractable for solving the one-hidden-layer neural net bandit problem (which is a strict sub-case of deep RL). In other words, it requires exponential (in the input dimension) samples to ﬁnd the global maximum (see Theorems 5.1). This also shows that conditions in past work that guarantee global convergence cannot apply to neural nets.
Given these strong impossibility results, we propose to reformulate the problem to ﬁnding an ap-proximate local maximum policy with guarantees. This is in the same vein as the recent fruitful paradigm in non-convex optimization where researchers disentangle the problem into showing that all local minima are good and fast convergence to local minima (e.g., see [28, 26, 27, 29, 48]). In
RL, local maxima can often be global as well for many cases [4].2
Zero-order optimization or policy gradient algorithms can converge to local maxima and become natural potential competitors. They are widely believed to be less sample-efﬁcient than the model-based approach because the latter can leverage the extrapolation power of the parameterized models.
Theoretically, our formulation aims to characterize this phenomenon with results showing that the model-based approach’s sample complexity mostly depends (polynomially) on the complexity of the model class, whereas policy gradient algorithms’ sample complexity polynomially depend on the dimensionality of policy parameters (in RL) or actions (in bandit). Our technical goal is to answer the following question:
Can we design algorithms that converge to approximate local maxima with sample complexities that depend only and polynomially on the complexity measure of the dynamics/reward class?
We note that this question is open even if the dynamics hypothesis class is ﬁnite, and the complexity measure is the logarithm of its size. The question is also open even for nonlinear bandit problems (where dynamics class is replaced by reward function class), with which we start our research.
We consider ﬁrst nonlinear bandit with deterministic reward where the reward function is given by
η(θ, a) for action a ∈ A under instance θ ∈ Θ. We use sequential Rademacher complexity [62, 63] to capture the complexity of the reward function η. Our main result for nonlinear bandit is stated as follows.
Theorem 1.1 (Informal version of Theorem 3.1). Suppose the sequential Rademacher complexity of a loss function class (deﬁned later) induced by the reward function class {η(θ, ·) : θ ∈ Θ} is bounded by (cid:112)R(Θ)T polylog(T ). Then, there exists an algorithm (ViOlin, Alg. 1) that ﬁnds an (cid:15)-approximate local maximum with (cid:101)O(R(Θ)(cid:15)−8) samples.
In contrast to zero-order optimization, which does not use the parameterization of η and has a sample complexity depending on the action dimension, our bound only depends on the complexity of the reward function class. This suggests that our algorithm exploits the extrapolation power of the reward function class. To the best of our knowledge, this is the ﬁrst action-dimension-free result for both linear and nonlinear bandit problems. More concretely, we instantiate our theorem to the following settings and get new results that leverage the model complexity (more in Section 3.1). 1. Linear bandit with ﬁnite parameter space Θ. Because η is concave in action a, our result leads to a sample complexity O(poly(log|Θ|, 1/(cid:15))) for ﬁnding an (cid:15)-approximate optimal action. In this case both zero-order optimization and the SquareCB algorithm in Foster, Rakhlin [24] have sample complexity/regret that depend on the dimension of action space dA. 2. Linear bandit with s-sparse or structured instance parameters. Our algorithm ViOlin achieves an O(poly(s, 1/(cid:15))) sample complexity when the instance/model parameter is s-sparse and the reward is deterministic. The sample complexity of zero-order optimization depends polynomi-ally on dA. Carpentier, Munos [9] achieve a stronger (cid:101)O(s
T ) regret bound for s-sparse linear bandits with actions set A = Sd−1. In contrast, our ViOlin algorithm applies more generally to any structured instance parameter set. Other related results either leverage the rather strong anti-concentration assumption on the action set [72], or have implicit dimension dependency
[33, Remark 4.3].
√ 3. Two-layer neural nets bandit.
Our algorithm ﬁnds an (cid:15)-approximate local maximum (cid:101)O(poly(1/(cid:15))). Zero-order optimization can also ﬁnd a local maximum but with Ω(dA) samples. 2The all-local-maxima-are-global condition only needs to hold to the ground-truth total expected reward function. This potentially can allow disentangled assumptions on the ground-truth instance and the hypothesis class. 2
Optimistic algorithms in this case have an exponential sample complexity (see Theorem 5.3).
Moreover, when the second layer of the ground-truth network contains all negative weights and the activation is convex and monotone, all local maxima are global because the reward is concave in the input (action) [5].
The results for bandit can be extended to model-based RL with deterministic nonlinear dynamics and deterministic reward.
Theorem 1.2 (Informal version of Theorem 4.4). Consider RL problems with deterministic dy-namics class and stochastic policy class that satisfy some Lipschitz properties.
Suppose the sequential Rademacher complexity of the (cid:96)2 losses for learning the dynamics is bounded by (cid:112)R(Θ)T polylog(T ). Then, ViOlin for RL (Alg. 2) ﬁnds an (cid:15)-approximate local maximum policy with (cid:101)O(R(Θ)(cid:15)−8) samples.
To the best of our knowledge, this is the ﬁrst model-based RL algorithms with provable ﬁnite sample complexity guarantees (for local convergence) for general nonlinear dynamics. The work of [55] is the closest prior work which also shows local convergence, but its conditions likely cannot be satisﬁed by any parameterized models (including linear models). We also present a concrete example of RL problems with nonlinear models satisfying our Lipschitz assumptions in Example 4.3 of
Section 4, which may also serve as a testbed for future model-based deep RL analysis. Other prior works on model-based RL do not apply to one-hidden-layer neural nets because they conclude global convergence which is not possible for one-hidden-layer neural nets in the worst case.
Technical novelty: exploring by model-based curvature estimate. The main challenge is that the optimism-in-face-of-uncertainty exploration principle seems to be too aggressive—for a non-linear model class, even if the ground-truth model is linear, optimistic exploration leads to expo-nentially large model cumulative prediction errors and exponential sample complexity (see Theo-rem 5.3). We use an online learning oracle to learn the dynamics, which can dramatically reduce the model prediction errors by hedging the risks. However, now, choosing actions or policies by maximizing virtual return may lack exploration. The main novelty of our algorithm is that we aug-ment the online learning loss with gradient and Hessian estimation errors. The key insight is that, in order to ensure sufﬁcient exploration for converging to local maxima, it sufﬁces for the model to predict the virtual return, its gradient and Hessian reasonably accurately. We refer to the approach as “model-based curvature estimate”. Our algorithm alternates between maximizing virtual return (over action or policy) and learning the model parameters by an online learner.
Because the algorithm leverages model extrapolation, the sample complexity of model-based cur-vature prediction depends on the model complexity instead of action dimension in the zero-optimization approach. Foster, Rakhlin [24] also propose algorithms that do not rely on UCB— their exploration strategy either relies on the ﬁnite discrete action space, or leverages the linear structure in the action space and has action-dimension dependency. In contrast, our algorithms’ exploration relies more on the learning of the model. Consequently, our sample complexity can be action-dimension-free. 2 Problem Setup and Preliminaries
In this section, we ﬁrst introduce our problem setup for nonlinear bandit and reinforcement learning, and then the preliminary for online learning and sequential Rademacher complexity. 2.1 Nonlinear Bandit Problem with Deterministic Reward
We consider deterministic nonlinear bandits with continuous actions. Let θ ∈ Θ be the parameter that speciﬁes the bandit instance, a ∈ RdA the action, and η(θ, a) ∈ [0, 1] the reward function. Let θ(cid:63) denote the unknown ground-truth parameter. Throughout the paper, we work under the realizability assumption that θ(cid:63) ∈ Θ. A bandit algorithm aims to maximize η(θ(cid:63), a). Let a(cid:63) = argmaxa η(θ(cid:63), a) be the optimal action (breaking tie arbitrarily). Let (cid:107)H(cid:107)sp be the spectral norm of a matrix H. We assume that the reward function, its gradient and Hessian matrix are Lipschitz, which are standard in the optimization literature (e.g., Johnson, Zhang [41], Ge et al. [26]). 3
Assumption 2.1. We assume that aη(θ, a)(cid:13) (cid:13) (cid:13)∇2 supa
ζ3rd (cid:107)a1 − a2(cid:107)2. (cid:13)sp ≤ ζh. For every θ ∈ Θ and a1, a2 ∈ RdA , (cid:13) for all θ ∈ Θ, supa (cid:107)∇aη(θ, a)(cid:107)2 ≤ ζg and (cid:13)sp ≤ aη(θ, a1) − ∇2 aη(θ, a2)(cid:13) (cid:13)∇2
As a motivation to consider deterministic rewards, we prove in Theorem B.1 for a special case that dA steps. The result implies that an action-no algorithm can ﬁnd a local maximum in less than dimension-free sample complexity bound is impossible with standard sub-Gaussian noise.3
√
Approximate local maxima.
In this paper, we aim to ﬁnd a local maximum of the real reward function η(θ(cid:63), ·). A point x is an ((cid:15)g, (cid:15)h)-approximate local maximum of a twice-differentiable function f (x) if (cid:107)∇f (x)(cid:107)2 ≤ (cid:15)g, and λmax(∇2f (x)) ≤ (cid:15)h. As argued in Sec. 1 and proved in Sec. 5, because reaching a global maximum is computational and statistically intractable for nonlinear problems, we only aim to reach a local maximum.
Sample complexity and local regret for converging to local maxima. Let at be the action that the algorithm takes at time step t. The sample complexity for converging to approximate local maxima is deﬁned to be the minimal number of steps T such that there exists t ∈ [T ] where at is an ((cid:15)g, (cid:15)h) approximate local maximum with probability at least 1−δ. We also deﬁne the “local regret” by comparing with an approximate local maximum. We defer the discussion to Appendix C.5. 2.2 Reinforcement Learning
A ﬁnite horizon Markov decision process (MDP) with deterministic dynamics is deﬁned by a tuple (cid:104)T, r, H, µ1(cid:105). Let S and A be the state and action spaces. The dynamics T : S × A → S gives next state, r : S × A → [0, 1] is the reward function. H and µ1 denote the horizon and distribution of initial state respectively. Without loss of generality, we assume that the state space is disjoint for different time steps. That is, there exists disjoint sets S1, · · · , SH such that S = ∪H h=1Sh, and for any sh ∈ S, ah ∈ a, T (sh, ah) ∈ Sh+1.
We consider parameterized policy and dynamics. Let Π = {πψ : ψ ∈ Ψ} be the policy class and
{Tθ : θ ∈ Θ} the dynamics class. The value function is V π h(cid:48)=h r(sh(cid:48), ah(cid:48))], where ah ∼ π(· | sh), sh+1 = T (sh, ah). Sharing the notation with the bandit setting, let η(θ, ψ) =
Es1∼µ1V πψ
T be the distribution
Tθ of state action pairs when running policy π in dynamics T . For simplicity, we do not distinguish
ψ, θ from πψ, Tθ when the context is clear. For example, we write V ψ (s1) be the expected return of policy πψ under dynamics Tθ. Let ρπ
T (sh) (cid:44) E[(cid:80)H
.
θ = V πψ
Tθ 2.3 Preliminary on Online Learning with Stochastic Input Components
Consider a prediction problem where we aim to learn a function that maps from X to Y parame-terized by parameters in Θ. Let (cid:96)((x, y); θ) be a loss function that maps (X × Y) × Θ → R+.
An online learner R aims to solve the prediction tasks under the presence of an adversarial nature iteratively. At time step t, the following happens. 1. The learner computes a distribution pt = R({(xi, yi)}t−1 2. The adversary selects a point ¯xt ∈ ¯X (which may depend on pt) and generates a sample ξt from i=1) over the parameter space Θ. some ﬁxed distribution q. Let xt (cid:44) (¯xt, ξt), and the adversary picks a label yt ∈ Y. 3. The data point (xt, yt) is revealed to the online learner.
The online learner aims to minimize the expected regret in T rounds of interactions, deﬁned as
REG
OL
T (cid:44)
E
ξt∼q,θt∼pt
∀1≤t≤T (cid:104)(cid:80)T t=1 (cid:96)((xt, yt); θt) − inf θ∈Θ (cid:80)T (cid:105) t=1 (cid:96)((xt, yt); θ)
. (1)
The difference of the formulation from the most standard online learning setup is that the ξt part of the input is randomized instead of adversarially chosen (and the learner knows the distribution of ξt before making the prediction pt). It was introduced by Rakhlin et al. [61], who considered a more generalized setting where the distribution q in round t can depend on {x1, · · · , xt−1}. 3We rely on deterministic reward to estimate the gradient by ﬁnite difference. This method can be extended to stochastic rewards with multiple-point feedback [54]. 4
We adopt the notation from Rakhlin et al. [61, 62] to deﬁne the (distribution-dependent) sequential
Rademacher complexity of the loss function class L = {(x, y) (cid:55)→ (cid:96)((x, y); θ) : θ ∈ Θ}. For any set Z, a Z-valued tree with length T is a set of functions {zi : {±1}i−1 → Z}T i=1. For a sequence of Rademacher random variables (cid:15) = ((cid:15)1, · · · , (cid:15)T ) and for every 1 ≤ t ≤ T, we denote zt((cid:15)) (cid:44) zt((cid:15)1, · · · , (cid:15)t−1). For any ¯X -valued tree x and any Y-valued tree y, we deﬁne the sequential
Rademacher complexity as
RT (L; x, y) (cid:44) Eξ1,···,ξt
E(cid:15) (cid:20) sup (cid:96)∈L (cid:80)T (cid:21)
. t=1 (cid:15)t(cid:96)((x((cid:15)), ξt), y((cid:15))) (2)
We also deﬁne RT (L) = supx,y RT (L; x, y), where the supremum is taken over all ¯X -valued and
Y-valued trees. Rakhlin et al. [61] proved the existence of an algorithm whose online learning regret satisﬁes REGOL
T ≤ 2RT (L). 3 Model-based Algorithms for Nonlinear Bandit
We ﬁrst study model-based algorithms for nonlinear continuous bandits problem, which is a simpli-ﬁcation of model-based reinforcement learning. We use the notations and setup in Section 2.1.
Abstraction of analysis for model-based algorithms.
Typically, a model-based algorithm ex-plicitly maintains an estimated model ˆθt, and sometimes maintains a distribution, posterior, or conﬁ-dence region of ˆθt. We will call η(θ(cid:63), a) the real reward of action a, and η(ˆθt, a) the virtual reward.
Most analysis for model-based algorithms (including UCB and ours) can be abstracted as showing the following two properties: (i) the virtual reward η(ˆθt, at) is sufﬁciently high. (ii) the virtual reward η(ˆθt, at) is close to the real reward η(θ(cid:63), at) in the long run.
One can expect that a proper combination of property (i) and (ii) leads to showing the real reward
η(θ(cid:63), at) is high in the long run. Before describing our algorithms, we start by inspecting and summarizing the pros and cons of UCB from this viewpoint.
The UCB algorithm chooses an action at and an estimated model ˆθt that
Pros and cons of UCB. maximize the virtual reward η(ˆθt, at) among those models agreeing with the observed data. The pro is that it satisﬁes property (i) by deﬁnition—η(ˆθt, at) is higher than the optimal real reward
η(θ(cid:63), a(cid:63)). The downside is that ensuring (ii) is challenging and often requires strong complexity measure bound such as Eluder dimension (which is not polynomial for even barely nonlinear models, as shown in Theorem 5.2). The difﬁculty largely stems from our very limited control of ˆθt except its consistency with the observed data. To bound the difference between the real and virtual rewards, we essentially require that any model that agrees with the past history should extrapolate to any future data accurately (as quantitatively formulated in Eluder dimension). Moreover, the difﬁculty of satisfying property (ii) is fundamentally caused by the over-exploration of UCB—As shown in the Theorem 5.3, UCB suffers from bad sample complexity with barely nonlinear family of models.
Our key idea: natural exploration via model-based curvature estimate. We deviate from
UCB by readjusting the priority of the two desiderata. We prioritize ensuring property (ii) on large model class. We leverage a strong online learning algorithm to predict ˆθt with the objective that
η(ˆθt, at) matches η(θ(cid:63), at) . As a result, the difference between the virtual and real reward depends on the online learnability or the sequential Rademacher complexity of the model class. Sequen-tial Rademacher complexity turns out to be a fundamentally more relaxed complexity measure than
Eluder dimension—e.g., two-layer neural networks’ sequential Rademacher complexity is polyno-mial in parameter norm and dimension, but the Eluder dimension is at least exponential in dimension (even with a constant parameter norm). However, an immediate consequence of using online-learned
ˆθt is that we lose optimism/exploration that ensured property (i).4 4More concretely, the algorithm can get stuck when (1) at is optimal for ˆθt, (2) ˆθt ﬁts actions at (and history) accurately, but (3) ˆθt does not ﬁt a(cid:63) (because online learner never sees a(cid:63)). The passivity of online learning formulation causes this issue—the online learner is only required to predict well for the point that it saw and will see, but not for those points that it never observes. 5
Algorithm 1 ViOlin: Virtual Ascent with Online Model Learner (for Bandit)
√ 2ζh. Let H0 = ∅; choose a0 ∈ A arbitrarily. 1: Set parameter κ1 = 2ζg and κ2 = 640 2: for t = 1, 2, · · · do 3: pt = R(Ht−1).
Run online learner R on Ht−1 with loss function (cid:96) (deﬁned in equation (5)) and obtain 4: 5: 6: 7: 8:
Eθt∼pt[η(θt, a)].
Let at ← argmaxa
Sample ut, vt ∼ N (0, IdA×dA ) independently.
Let ξt = (ut, vt), ¯xt = (at, at−1), and xt = (¯xt, ξt)
Compute yt = [η(θ(cid:63), at), η(θ(cid:63), at−1), (cid:104)∇aη(θ(cid:63), at−1), ut(cid:105), (cid:104)∇2 aη(θ(cid:63), at−1)ut, vt(cid:105)] ∈ R4 by applying a ﬁnite number of actions in the real environments using equation (3) and (4).
Update Ht = Ht−1 ∪ {(xt, yt)}
Our approach realizes property (i) in a sense that the virtual reward will improve iteratively if the real reward is not yet near a local maximum. This is much weaker than what UCB offers (i.e., that the virtual reward is higher than the optimal real reward), but sufﬁces to show the convergence to a local maximum of the real reward function. We achieve this by demanding the estimated model ˆθt not only to predict the real reward accurately, but also to predict the gradient ∇aη(θ(cid:63), a) and Hessian aη(θ(cid:63), a) accurately. In other words, we augment the loss function for the online learner so that
∇2 aη(ˆθt, at) ≈ the estimated model satisﬁes η(ˆθt, at) ≈ η(θ(cid:63), at), ∇aη(ˆθt, at) ≈ ∇aη(θ(cid:63), at), and ∇2 aη(θ(cid:63), at) in the long run. This implies that when at is not at a local maximum of the real reward
∇2 function η(θ(cid:63), ·), then it’s not at a maximum of the virtual reward η(ˆθt, ·), and hence the virtual reward will improve in the next round if we take the greedy action that maximizes it.
Estimating projections of gradients and Hessians.
To guide the online learner to predict
∇aη(θ(cid:63), at) correctly, we need a supervision for it. However, we only observe the reward η(θ(cid:63), at).
Leveraging the deterministic reward property, we use rewards at a and a + α1u to estimate the projection of the gradient at a random direction u: (cid:104)∇aη(θ(cid:63), a), u(cid:105) = lim
α1→0 (η(θ(cid:63), a + α1u) − η(θ(cid:63), a)) /α1 (3)
It turns out that the number of random projections (cid:104)∇aη(θ(cid:63), a), u(cid:105) needed for ensuring a large virtual gradient does not depend on the dimension, because we only use these projections to estimate the norm of the gradient but not necessarily the exact direction of the gradient (which may require d samples.) Similarly, we can also estimate the projection of Hessian to two random directions u, v ∈ dA by: (cid:10)∇2 ((cid:104)∇aη(θ(cid:63), a + α2v), u(cid:105) − (cid:104)∇aη(θ(cid:63), a), u(cid:105)) /α2 aη(θ(cid:63), a)v, u(cid:11) = lim (4)
α2→0
= lim
α2→0 lim
α1→0 ((η(θ(cid:63), a + α1u + α2v) − η(θ(cid:63), a + α2v)) − (η(θ(cid:63), a + α1u) − η(θ(cid:63), a))) /(α1α2)
Algorithmically, we can choose inﬁnitesimal α1 and α2. Note that α1 should be at least an order smaller than α2 because the limitations are taken sequentially.
We create the following prediction task for an online learner: let θ be the parameter, x = (a, a(cid:48), u, v) aη(θ, a(cid:48))u, v(cid:105)] ∈ R4 be the output, and be the input, ˆy = [η(θ, a), η(θ, a(cid:48)), (cid:104)∇aη(θ, a(cid:48)), u(cid:105), (cid:104)∇2 aη(θ(cid:63), a(cid:48))u, v(cid:105)] ∈ R4 be the supervision, and the loss y = [η(θ(cid:63), a), η(θ(cid:63), a(cid:48)), (cid:104)∇aη(θ(cid:63), a(cid:48)), u(cid:105), (cid:104)∇2 function be (cid:16) 1, ([ˆy]3 − [y]3)2(cid:17)
κ2 (cid:96)(((a, a(cid:48), u, v), y); θ) (cid:44)([ˆy]1 − [y]1)2 + ([ˆy]2 − [y]2)2 + min (cid:16)
+ min 2, ([ˆy]4 − [y]4)2(cid:17)
κ2
Here we used [y]i to denote the i-th coordinate of y ∈ R4 to avoid confusing with yt (the supervision at time t.) Our algorithm is formally stated in Alg. 1 with its sample complexity bound below.
Theorem 3.1. Suppose the sequential Rademacher complexity of the loss function (deﬁned in
Eq. (5)) is bounded by (cid:112)R(Θ)T polylog(T ). The sample complexity of Alg. 1 (for ﬁnding an ((cid:15), 6
ζ3rd(cid:15))-approximate local maximum) is bounded by (cid:101)O(C 4 1 R(Θ) max (cid:0)ζ 4 3rd(cid:15)−6(cid:1)). h(cid:15)−8, ζ 2 (5)
√ 6
We present a proof sketch in Appendix C.1 . Proof of Theorem 3.1 is deferred to Appendix C.4.
We can also boost the success probability by running Alg. 1 multiple times. In addition, we can prove that our algorithm enjoys a sublinear local regret. The theorem statement and proof is shown in Appendix C.5 and Appendix C.6 respectively. 3.1
Instantiations of Theorem 3.1
In the sequel we sketch some instantiations of our main theorem, whose proofs are deferred to
Appendix C.7.
Linear bandit with ﬁnite model class. Consider a linear bandit problem with action set A = {a ∈
Rd : (cid:107)a(cid:107)2≤ 1} and ﬁnite model class Θ ⊂ {θ ∈ Rd : (cid:107)θ(cid:107)2= 1}. Let η(θ, a) = (cid:104)θ, a(cid:105) be the reward.
We deal with the constrained action set by using a surrogate loss ˜η(θ, a) (cid:44) (cid:104)θ, a(cid:105) − 1 2 and apply
Theorem C.3 with reward ˜η. We claim that the sample complexity (of ﬁnding an (cid:15)-suboptimal action for η(θ(cid:63), a)) is bounded by (cid:101)O(log|Θ|(cid:15)−8).5 Note that the bound is independent of the dimension d. By contrast, the SquareCB algorithm [24] depends polynomially on d (see Theorem 7 of [24]).
Zero-order optimization approach [20] in this case also gives a poly(d) regret bound. 2 (cid:107)a(cid:107)2
Linear bandit with sparse or structured model vectors. We consider the deterministic linear bandit setting where the model class Θ = {θ ∈ Rd : (cid:107)θ(cid:107)0≤ s, (cid:107)θ(cid:107)2= 1} consists of all s-sparse vectors on the unit sphere. Similarly to ﬁnite hypothesis case, we claim that the sample complexity of Alg. 1 is (cid:101)O(log|Θ|(cid:15)−8polylog(d)). The sample complexity of our algorithm only depends on the sparsity level s (up to logarithmic factors), whereas the Eluder dimension of sparse linear hypothesis is still Ω(d) (see Lemma C.4). Lattimore, Szepesvári [46] show a Ω(d) sample complexity lower bound for the sparse linear bandit problem with stochastic reward. But here we only consider a deterministic reward and continuous action.
Moreover, we can further extend the result to other linear bandit settings where θ has an additional structure. Suppose Θ = {θ = φ(z) : z ∈ Rs} for some Lipschitz function φ. Then, a simlar approach gives sample complexity bound that only depends on s but not d (up to logarithmic factors).
Our results can also be extended to non-linear bandits such as deterministic logistic bandits. (cid:80)d 1 is deﬁned by maxi∈[m]
Two-layer neural nets. We consider the reward function given by two-layer neural networks with 2 (cid:107)a(cid:107)2 width m. For matrices W1 ∈ Rm×d and W2 ∈ R1×m, let η((W1, W2), a) = W2σ(W1a) − 1 2 for some nonlinear link function σ : R → [0, 1] with bounded derivatives up to the third order. Re-call that the (1, ∞)-norm of W (cid:62) j=1 |[W1]i,j|. That is, the max 1-norm of the rows of W1. Let the model hypothesis space be Θ = {(W1, W2) : (cid:13) (cid:13) (cid:13)1,∞ ≤ 1, (cid:107)W2(cid:107)1 ≤ 1} and θ (cid:44) (W1, W2). We claim that Alg .1 ﬁnds an ((cid:15), 6
ζ3rd(cid:15))-approximate local maximum in (cid:101)O(cid:0)(cid:15)−8polylog(d)(cid:1) steps. To the best of our knowledge, this is the ﬁrst result analyzing nonlin-ear bandit with neural network parameterization. The result follows from analyzing the sequential
Rademacher complexity for η, (cid:104)∇aη, u(cid:105), and (cid:104)u, ∇2 aη · v(cid:105), and ﬁnally the resulting loss function (cid:96).
See Theorem C.6 in Section C.7 for details. We remark here that zero-order optimization in this case has a poly(d) sample complexity. In addition, if the second layer of the neural network W2 contains all negative entries, and the activation function σ is monotone and convex, then η((W1, W2), a) is concave in the action. (This is a special case of input convex neural networks [5].) In this case,
Alg. 1 ﬁnds an (cid:15)-suboptimal action (see Theorem C.6). (cid:13)W (cid:62) 1
√ 4 Model-based Reinforcement Learning
In this section, we extend the results in Section 3 to model-based reinforcement learning with deter-ministic dynamics and reward function.
We can always view a model-based reinforcement learning problem with parameterized dynamics and policy as a nonlinear bandit problem in the following way. The policy parameter ψ corresponds to the action a in bandit, and the dynamics parameter θ corresponds to the model parameter θ in 5Recall that in bandit literature, action a is an (cid:15)-approximate optimal action if η(θ(cid:63), a) ≥ η(θ(cid:63), a(cid:63)) − (cid:15). 7
bandit. The expected total return η(θ, ψ) = Es1∼µ1V πψ (s1) is the analogue of reward function
Tθ in bandit. We intend to make the same regularity assumptions on η as in the bandit case (that is,
Assumption 2.1) with a being replaced by ψ. However, when the policy is deterministic, the reward function η has Lipschitz constant with respect to ψ that is exponential in H (even if dynamics and policy are both deterministic with good Lipschitzness). This prohibits efﬁcient optimization over policy parameters. Therefore we focus on stochastic policies in this section, for which we expect η and its derivatives to be Lipschitz with respect to ψ.
Blindly treating RL as a bandit only utilizes the reward but not the state observations. In fact, one major reason why model-based methods are more sample efﬁcient is that it supervises the learning of dynamics by state observations. To reason about the learning about local steps and the dynamics, we make the following additional Lipschitzness of value functions w.r.t to the states and Lipschitz-ness of policies w.r.t to its parameters, beyond those assumptions for the total reward η(θ, ψ) in
Assumption 2.1.
Assumption 4.1. We assume the following (analogous to Assumption 2.1) on the value function:
∀ψ ∈ Ψ, θ ∈ Θ, s, s(cid:48) ∈ S we have |V ψ
θ (s(cid:48))(cid:107)2≤
ψV ψ
L1(cid:107)s − s(cid:48)(cid:107)2; (cid:107)∇2
Assumption 4.2. We assume the following Lipschitzness assumptions on the stochastic poli-(cid:107)Ea∼πψ(·|s)[(∇ψ log πψ(a | s))(∇ψ log πψ(a | s))(cid:62)](cid:107)sp≤ χg; cies parameterization πψ.6 (cid:107)Ea∼πψ(·|s)[(∇ψ log πψ(a | s))⊗4](cid:107)sp≤ χf ; (cid:107)Ea∼πψ(·|s)[(∇2
ψ log πψ(a | s))(cid:62)](cid:107)sp≤ χh.
θ (s(cid:48))| ≤ L0(cid:107)s − s(cid:48)(cid:107)2; (cid:107)∇ψV ψ
θ (s(cid:48))(cid:107)sp≤ L2(cid:107)s − s(cid:48)(cid:107)2.
ψ log πψ(a | s))(∇2
θ (s) − ∇ψV ψ
θ (s) − V ψ
θ (s) − ∇2
ψV ψ
Our results depends polynomially on the parameters L0, L1, L2, χg, χf and χh. To demonstrate that the Assumption 4.1 and 4.2 contain interesting RL problems with nonlinear models and stochastic policies, we give the following example where these parameters are all on the order of O(1).
Example 4.3. Let state space S be the unit ball in Rd and action space A be Rd. The (determin-istic) dynamics T is given by T (s, a) = Nθ(s + a), where N is a nonlinear model parameterized by θ, e.g., a neural network. We assume that θ belongs to a ﬁnite hypothesis class Θ that satisﬁes (cid:107)Nθ(s + a)(cid:107)2 ≤ 1 for all θ ∈ Θ, s ∈ S, a ∈ A. Assume that the reward function r(s, a) is Lr-Lipschitz w.r.t (cid:96)2-norm, that is, satisfying |r(s1, a1) − r(s2, a2)| ≤ Lr((cid:107)s1 − s2(cid:107)2 + (cid:107)a1 − a2(cid:107)2).
We consider a family of stochastic Gaussian policies with the mean being linear in the state:
πψ(s) = N (ψs, σ2I), parameterized by ψ ∈ Rd×d with (cid:107)ψ(cid:107)op ≤ 1. We consider σ ∈ (0, 1) as a small constant on the order of 1.
In this setting, Assumption 2.1, 4.1, and 4.2 hold with all parameters ζg, ζh, ζ3rd, L0, L1, L2, χg, χf and χh bounded by poly(σ, 1/σ, H, Lr).
Bounding the Lipschitz parameters is highly nontrivial and deferred to Appendix E.
We show that the difference of gradient and Hessian of the total reward can be upper-bounded by the difference of dynamics. Let τt = (s1, a1, · · · , sH , aH ) be a trajectory sampled from policy
πψt under the real dynamics Tθ(cid:63) . Similarly to [79], when the value function is Lipschitz, we upper bound ∆t,1 = |η(θt, ψt) − η(θ(cid:63), ψt)| by the one-step model prediction errors. Similarly, we can upper bound the gradient and Hessian errors by model prediction errors. As a result, the loss function simply can be set to (cid:96)((τt, τ (cid:48) t); θ) = (cid:88) (sh,ah)∈τt (cid:107)Tθ(sh, ah) − Tθ(cid:63) (sh, ah)(cid:107)2 (cid:88) (cid:107)Tθ(s(cid:48) h, a(cid:48) h) − Tθ(cid:63) (s(cid:48) h, a(cid:48) h)(cid:107)2 2 (6) 2 + (s(cid:48) h,a(cid:48) h)∈τ (cid:48) t for two trajectories τ, τ (cid:48) sampled from policy πψt and πψt−1 respectively. Compared to Alg. 1, the loss function is here simpler without relying on ﬁnite difference techniques to query gradients projections. Our algorithm for RL is analogous to Alg. 1 by using the loss function in Eq. (6). Our algorithm is presented in Alg. 2 in Appendix D. Main theorem for Alg. 2 is shown below.
Theorem 4.4. Let c1 = HL2 and C1 = 2 + ζg
ζh 1(8H 2χg + 2) + 4HL2 2
. Suppose the sequential Rademacher complexity of the loss function (deﬁned 0(4H 2χh + 4H 4χf + 2H 2χg + 1) + HL2 6Recall supu∈Sd−1 the injective norm of a k-th order tensor A ∈ Rd⊗k that (cid:10)A, u⊗k(cid:11) . is deﬁned as (cid:13) (cid:13)A⊗k(cid:13) (cid:13)sp
= 8
in Eq. (6)) is bounded by (cid:112)R(Θ)T polylog(T ). The sample complexity of Alg. 2 (for ﬁnding an ((cid:15), 6
ζ3rd(cid:15))-approximate local maximum) is bounded by (cid:101)O(c2 1 R(Θ) max (cid:0)ζ 4 3rd(cid:15)−6(cid:1)). h(cid:15)−8, ζ 2 1C 4
√
Instantiation of Theorem 4.4 on Example 4.3. Applying Theorem 4.4 to the Example 4.3 with
σ = Θ(1) we get the sample complexity upper bound (cid:101)O(cid:0)poly(σ, 1/σ, H, Lr) log|Θ|(cid:15)−8(cid:1).
Comparison with policy gradient.
To the best of our knowledge, the best analysis for policy gradient [74] shows convergence to a local maximum with a sample complexity that depends poly-nomially on (cid:107)∇ψ log πψ(a | s)(cid:107)2 [4]. For the instance in in Example 4.3, this translates to a sample d/σ. In contrast, our sample complexity is independent complexity guarantee on the order of of the dimension d. Instead, our bound depends on the complexity of the model family Θ which could be much smaller than the ambient dimension—this demonstrates that we leverage the model extrapolation.
√ 5 Lower Bounds
We prove several lower bounds to show (a) the hardness of ﬁnding global maxima, and (b) the inefﬁciency of using optimism in nonlinear bandit.
Hardness of Global Optimality.
In the following theorem, we show it statistically intractable to
ﬁnd the global optimal policy when the function class is chosen to be the neural networks with ReLU activation. That is, the reward function can be written in the form of η((w, b), a) = ReLU((cid:104)w, a(cid:105) + b). Note that the reward function can also be made smooth by replacing the activation by a smoothed version. For example, η((w, b), a) = ReLU((cid:104)w, a(cid:105) + b)2.
Theorem 5.1. When the function class is chosen to be one-layer neural networks with ReLU acti-vation, the minimax sample complexity is Ω(ε−(d−2)).
We can also prove that the eluder dimension of the constructed reward function class is exponential.
Theorem 5.2. The ε-eluder dimension of one-layer neural networks is at least Ω(ε−(d−1)).
This result is concurrently established by Li et al. [50, Theorem 8]. The proofs of both theorems are deferred to Appendices B.1 and B.2, respectively. We also note that Theorem 5.1 does require
ReLU activation, because if the ReLU function is replaced by a strictly monotone link function with bounded derivatives (up to third order), then this is the setting of deterministic generalized linear bandit problem, which does allow a global regret that depends polynomially on dimension
[23, 14, 52]. In this case, our Theorem C.3 can also give polynomial global regret result: because all local maxima of the reward function is global maximum [34, 43] and it also satisﬁes the strict-saddle property [26], the local regret result translates to a global regret result. This shows that our framework does separate the intractable cases from the tractable by the notions of local and global regrets.
With two-layer neural networks, we can relax the use of ReLU activation—Theorem 5.2 holds with two-layer neural networks and leaky-ReLU activations [75] because O(1) leaky-ReLU can imple-ment a ReLU activation. We conjecture that with more layers, the impossibility result also holds for a broader sets of activations.
Inefﬁciency caused by optimism in nonlinear models. The next theorem states that the UCB al-gorithm that uses optimism-in-face-of-uncertainty principle can overly explore in the action space, even if the ground-truth is simple. In fact the UCB algorithm will keep exploring for an exponen-tial number of steps. So, UCB doesn’t even converge to local max, indicating more sophisticated algorithms like ours are necessary. Proof of the theorem is deferred to Appendix B.3.
Theorem 5.3. Consider the case where the ground-truth reward function is linear: (cid:104)θ(cid:63), a(cid:105) and the action set is a ∈ Sd−1. If the hypothesis is chosen to be two-layer neural network with width d, UCB algorithm with tightest upper conﬁdence bound suffers exponential sample complexity . 9
6 Additional