Abstract
𝑡
, obtaining a
Generalized self-concordance is a key property present in the objective function of many important learning problems. We establish the convergence rate of a simple Frank-Wolfe variant that uses the open-loop step size strategy
𝛾𝑡 = 2 convergence rate for this class of functions in terms of primal gap and Frank-Wolfe gap, where 𝑡 is the iteration count.
This avoids the use of second-order information or the need to estimate local smoothness parameters of previous work. We also show improved convergence rates for various common cases, e.g., when the feasible region under consideration is uniformly convex or polyhedral.
𝑡 1
/ 2
)
O (
/(
+
) 1

Introduction
Constrained convex optimization is the cornerstone of many machine learning problems. We consider such problems, formulated as: min x
∈ X
𝑓
, x
) ( (1.1)
ℝ
→
: ℝ𝑛
∪ {+∞} is a generalized self-concordant function and
ℝ𝑛 is a where 𝑓 compact convex set. When computing projections onto the feasible regions as required in, e.g., projected gradient descent, is prohibitive, Frank-Wolfe (FW) (Frank & Wolfe, 1956) algorithms (a.k.a. Conditional Gradients (CG) (Levitin & Polyak, 1966)) are the algorithms of choice, relying on Linear Minimization Oracles (LMO) at each iteration to solve Problem (1.1).
The analysis of their convergence often relies on the assumption that the gradient is Lipschitz-continuous. This assumption does not necessarily hold for generalized self-concordant functions, an important class of functions for which the growth can be unbounded.
X ⊆ 1.1