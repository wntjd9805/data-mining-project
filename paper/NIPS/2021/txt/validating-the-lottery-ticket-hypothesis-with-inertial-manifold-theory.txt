Abstract
Despite achieving remarkable efﬁciency, traditional network pruning techniques often follow manually-crafted heuristics to generate pruned sparse networks. Such heuristic pruning strategies are hard to guarantee that the pruned networks achieve test accuracy comparable to the original dense ones. Recent works have empirically identiﬁed and veriﬁed the Lottery Ticket Hypothesis (LTH): a randomly-initialized dense neural network contains an extremely sparse subnetwork, which can be trained to achieve similar accuracy to the former. Due to the lack of theoretical evidence, they often need to run multiple rounds of expensive training and pruning over the original large networks to discover the sparse subnetworks with low accuracy loss. By leveraging dynamical systems theory and inertial manifold theory, this work theoretically veriﬁes the validity of the LTH. We explore the possibility of theoretically lossless pruning as well as one-time pruning, compared with existing neural network pruning and LTH techniques. We reformulate the neural network optimization problem as a gradient dynamical system and reduce this high-dimensional system onto inertial manifolds to obtain a low-dimensional system regarding pruned subnetworks. We demonstrate the precondition and existence of pruned subnetworks and prune the original networks in terms of the gap in their spectrum that make the subnetworks have the smallest dimensions. 1

Introduction
Deep learning techniques utilize the architecture of large model size and over-parameterization to achieve state-of-the-art performance in a wide range of applications [17, 18]. The performance im-provements can be owed to the availability of large-scale neural networks and massive computational resources to train such complex models. However, large model size and extreme over-parameterization typically lead to high training cost, slow inference speed, and large memory consumption. This hinders the applicability of deep learning models in resource-intensive scenarios with the requirement of low latency and energy consumption, such as Internet of Things [2] and mobile computing [51, 55].
We have witnessed a large number of research advances in neural network pruning for reducing the time and space requirements of deep neural networks both at training and test time, with minimal loss in accuracy [31, 20, 34]. These techniques identify non-essential weights, ﬁlters, and other structures
∗Equal contributors
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
from neural networks that do not contribute signiﬁcantly to the model performance, and remove them for compressing original dense neural networks [49, 68, 52, 57, 3, 30, 35, 64, 39, 27, 29, 4, 40, 61, 33, 56, 16, 21]. The goal of neural network pruning is to reduce computational cost and memory consumption without a signiﬁcant accuracy drop, compared with the original dense networks.
However, the majority of neural network pruning methods require to train a dense neural network to generate a pruned sparse network, as it is difﬁcult to train a pruned network from scratch [20, 62, 34, 23, 41, 67, 49, 68, 52, 57, 3, 30, 74, 24, 25, 63, 35, 64, 39, 27, 29, 4, 40], although there are few pruning before training approaches [32, 61, 33, 56, 16, 21]. Thus, these methods still suffer from the non-trivial training cost on over-parameterized networks. Most importantly, traditional network pruning techniques are based on ad-hoc heuristic pruning strategies [20, 74, 63, 29], leaving some critical theoretical questions unanswered on how much we can prune a neural network given a speciﬁed tolerance of accuracy drop, and how to achieve it with a practical and efﬁcient procedure.
Therefore, it is difﬁcult to theoretically guarantee that the pruned networks achieve test accuracy comparable to the original dense ones. In addition, a recent literature has demonstrated that traditional neural network pruning often fall-short compared to the Lottery Ticket Hypothesis techniques [33].
A recent report has empirically identiﬁed and validated a surprising ﬁnding of the Lottery Ticket
Hypothesis (LTH): a randomly-initialized dense neural network contains an extremely sparse subnet-work (i.e., a winning lottery ticket) such that, when trained from scratch with weights being reset to its initialization, can achieve similar performance to the original dense network within similar training iterations [13, 14]. Compared with the original dense networks, the winning tickets are able to save a lot of memory for retraining [13, 73, 14, 15, 70, 42, 53, 11, 8]. The existence of winning tickets indicates that deep learning models can be trained in low-resource environments.
While the LTH techniques exhibit promising practical implications for scaling over large models and datasets, they fail to provide an efﬁcient algorithm to search for the winning ticket subnetworks for deep learning models in practice, due to the lack of theoretical evidence and analysis. They often utilize an Iterative Magnitude Pruning (IMP) strategy to gradually ﬁnd winning tickets, which needs multiple rounds of expensive training, pruning, and resetting over large neural networks. Although existing LTH techniques raise very intriguing observations, most of them provide only empirical evidence to verify the LTH [71, 12, 1, 47, 69, 54, 5, 53, 26, 8, 7, 11]. In this case, it is difﬁcult to identify the optimal pruning strategy without performance drop, i.e., how and where a neural network should be pruned. Therefore, two fundamental unanswered questions are raised: (1) How to verify the existence of winning lottery tickets, i.e., sparse trainable subnetworks inside a dense neural network, with theoretical evidence and analysis? (2) How to provide efﬁcient algorithms to discover winning lottery tickets, without multiple rounds of expensive training and pruning in the IMP process?
To our best knowledge, this work is the ﬁrst to theoretically verify the LTH and the existence of winning lottery tickets by leveraging dynamical systems theory and inertial manifold theory.
We reformulate gradient descent optimization of a neural network as a gradient dynamical system regarding W (i.e., parameters of the neural network) and analyze its dynamics with dynamical systems theory. Since inertial manifolds MW contain all local minimum points W ∗ of loss function
L regarding W (i.e., ∇L(W ∗) = 0), we reduce the original high-dimensional gradient system ϕ onto inertial manifolds to obtain a low-dimensional system regarding W +, i.e., parameters of the winning ticket subnetwork. We demonstrate that there must exist such MW for neural network pruning, when the gradient dynamical system meets a certain condition about its spectrum and Lipschitz constant, for cutting W into W + and W − (i.e., redundant parameters of the original neural network).
In comparison with existing neural network pruning and LTH techniques, our dynamical systems and inertial manifold-based method exhibits two unique advantages: (1) All local minimum points W ∗ lie in MW and thus W + is theoretically lossless pruning of W . (2) The computation of MW and W + is a one-time operation, such that our method is able to avoid the cost of multiple rounds of expensive training and pruning on large networks in the LTH methods. Due to large size of W in real scenarios, an approximate method is designed to estimate the Lipschitz constant for maintaining the efﬁciency.
Extensive evaluation on real datasets demonstrates the superior performance of our proposed IMC model against several state-of-the-art neural network pruning and LTH methods. More experiments, implementation details, and hyperparameter setting are presented in Appendices A.3-A.5. 2
2