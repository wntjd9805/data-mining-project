Abstract
Despite the tremendous success of reinforcement learning (RL) with function ap-proximation, efﬁcient exploration remains a signiﬁcant challenge, both practically and theoretically. In particular, existing theoretically grounded RL algorithms based on upper conﬁdence bounds (UCBs), such as optimistic least-squares value iteration (LSVI), are often incompatible with practically powerful function ap-proximators, such as neural networks. In this paper, we develop a variant of bootstrapped LSVI, namely BooVI, which bridges such a gap between practice and theory. Practically, BooVI drives exploration through (re)sampling, making it compatible with general function approximators. Theoretically, BooVI inherits the
O(pd3H 3T )-regret of optimistic LSVI in the episodic linear setting. worst-case
Here d is the feature dimension, H is the episode horizon, and T is the total number of steps. e 1

Introduction
Reinforcement learning (RL) with function approximation demonstrates signiﬁcant empirical success in a broad range of applications [e.g., 16, 38, 40, 45]. However, computationally and statistically efﬁcient exploration of large and intricate state spaces remains a major barrier. Practically, the lack of temporally-extended exploration [31, 28, 30] in existing RL algorithms, e.g., deterministic policy gradient [39] and soft actor-critic [19], hinders them from solving more challenging tasks, e.g.,
Montezuma’s Revenge in Atari Games [42, 17], in a more sample-efﬁcient manner. Theoretically, it remains unclear how to design provably efﬁcient RL algorithms with ﬁnite sample complexities or regrets that allow for practically powerful function approximators, e.g., neural networks.
There exist two principled approaches to efﬁcient exploration in RL, namely optimism in the face of uncertainty [e.g., 4, 41, 21, 11, 12, 5, 23, 24] and posterior sampling [e.g., 29, 34, 33, 30, 27, 36].
• The optimism-based approach is often instantiated by incorporating upper conﬁdence bounds (UCBs) into the estimated (action-)value functions as bonuses, which are used to direct exploration. In the tabular setting, the resulting algorithms, e.g., variants of upper conﬁdence bound reinforcement learning (UCRL) [4, 21, 11, 5], are known to attain the optimal worst-case regret [32]. Beyond the tabular setting, it remains unclear how to construct closed-form
UCBs in a principled manner for general function approximators, e.g., neural networks.
The only exception is the linear setting [51, 52, 24, 8], where optimistic least-squares value iteration (LSVI) [24] is known to attain a near-optimal (with respect to T = KH) worst-case regret. However, the closed-form UCB therein is tailored to linear models [1, 9] rather than fully general-purpose.
⇤Northwestern University; boyiliu2018@u.northwestern.edu
†Northwestern University; qicai2022@u.northwestern.edu
‡Princeton University; zy6@princeton.edu
§Northwestern University; zhaoranwang@gmail.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
• On the other hand, the posterior-based approach, which originates from Thompson sampling
[43, 37], can be instantiated using randomized (action-)value functions [34, 30, 27, 36].
Unlike optimistic LSVI, the resulting algorithm, namely randomized LSVI, straightforwardly allows for general function approximators, as it only requires injecting random noise into the training data of LSVI. Ideally, such injected random noise does not depend on particular function approximators. In the tabular setting, randomized LSVI is known to attain near-optimal worst-case and Bayesian regrets [30, 36]. Meanwhile, in the linear setting, randomized LSVI is very recently shown to attain a near-optimal worst-case regret
[54], which is only worse than that of optimistic LSVI by a factor of pdH. Here d is the feature dimension and H is the episode horizon. However, achieving such a regret requires a nontrivial modiﬁcation of the injected random noise, which is tailored to linear models.
Such a specialized modiﬁcation diminishes the supposed practical advantage of randomized
LSVI.
To bridge such a gap between practice and theory, we aim to answer the following question: Can we design an RL algorithm that simultaneously achieves the practical advantage of randomized LSVI and the theoretical guarantee of optimistic LSVI?
In this paper, we propose a variant of bootstrapped LSVI, namely BooVI, which combines the advantages of the optimism-based and posterior-based approaches. The key idea of BooVI is to use posterior sampling to implicitly construct an “optimistic version” of the estimated (action-)value functions in a data-driven manner. Unlike randomized LSVI, which samples from the posterior only once, e.g., by injecting random noise into the training data of LSVI, BooVI samples from the posterior multiple times, e.g., via the Langevin dynamics [48, 35]. Upon evaluating an action at a state, BooVI ranks the values of the randomized (action-)value functions sampled from the posterior in descending order and returns a top-ranked value, which can be shown to be approximately optimistic. Generally speaking, BooVI corresponds to bootstrapping the noise in the least-squares regression problem of LSVI. As a result, it can be viewed as a parametric bootstrap counterpart of the nonparametric bootstrap technique used in bootstrapped deep Q-networks (DQNs) [31, 30], which demonstrates signiﬁcant empirical success in terms of exploration.
Compared with existing RL algorithms with function approximation, the advantage of BooVI is twofold:
• Practically, BooVI bypasses the explicit construction of the closed-form UCB in optimistic
LSVI. Like randomized LSVI, BooVI straightforwardly allows for general function approxi-mators, as it only requires injecting random noise into the training process of LSVI, e.g., via the Langevin dynamics.
• Theoretically, BooVI inherits the worst-case
O(pd3H 3T )-regret of optimistic LSVI in the linear setting. Here d is the feature dimension, H is the episode horizon, and T is the total number of steps. Such a regret is better than the best known worst-case regret of randomized e
LSVI by a factor of pdH. More importantly, unlike the specialized variant of randomized
LSVI studied in [54], BooVI can be applied to the linear setting “as is”, without tailoring the injected random noise to linear models.
More