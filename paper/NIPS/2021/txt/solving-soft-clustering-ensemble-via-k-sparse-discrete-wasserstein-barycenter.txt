Abstract
Clustering ensemble is one of the most important problems in ensemble learning.
Though it has been extensively studied in the past decades, the existing methods often suffer from the issues like high computational complexity and the difﬁculty on understanding the consensus. In this paper, we study the more general soft clustering ensemble problem where each individual solution is a soft clustering. We connect it to the well-known discrete Wasserstein barycenter problem in geometry.
Based on some novel geometric insights in high dimensions, we propose the sampling-based algorithms with provable quality guarantees. We also provide the systematical analysis on the consensus of our model. Finally, we conduct the experiments to evaluate our proposed algorithms. 1

Introduction
Clustering is a fundamental topic that has important applications in various areas, such as data mining, networking, and bioinformatics [34]. In the past decades, a number of different clustering objectives and algorithms have been proposed. For example, the popular k-means aims to partition the given data set into k clusters and minimize the average squared distance from the input data to the set of cluster centers; the well known k-means clustering algorithms include the Lloyd’s algorithm [47], k-means++ [5], and local search [36]. Other clustering objectives, like hierarchical clustering [49] and density-based clustering [53], are also widely used in practice.
Obviously, different clustering algorithms can obtain different results. Moreover, even for the same clustering algorithm (e.g., the Lloyd’s algorithm), the initialization and data preprocessing (e.g., random projection [12]) steps may yield different clustering results. Therefore, a natural idea is to aggregate these different clustering results so as to achieve a more reliable result. The problem is called clustering ensemble (also termed clustering aggregation or consensus clustering) [27].
However, the current methods still suffer from several issues in theory and practice. Most of existing clustering ensemble methods rely on complicated optimization models, such as the correlation clustering [28], graph partition [25], semi-deﬁnite programming [54], matrix completion [63], and spectral clustering [56]; these optimization problems usually have super-linear complexities and thus cannot be efﬁciently solved for large-scale datasets. Though several heuristic ideas have been proposed for speeding up the computation (e.g., the sampling idea proposed in [61]), they are in lack of rigorous theoretical analysis on their quality guarantees.
Another issue is about the interpretability of consensus. A large number of clustering ensemble models are based on utility function [57, 61, 60] or co-association matrix [26, 56]. From the theoretical perspective, a fundamental question is why these models can yield the ﬁnal clusterings close to
∗Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
the ground-truth clustering. The similar consensus question was also studied for the classiﬁcation problem before [13]. However, the analysis for clustering ensemble is more challenging, because we need to take into account the matchings between different clustering solutions. 1.1 Our Contributions
In this paper, we focus on the more general soft clustering ensemble problem, where each given individual clustering is a soft clustering (also referred to as fuzzy clustering) [61]. In a soft clustering, data points can potentially belong to multiple clusters. For example, a point may be assigned to three clusters with the probabilities 10%, 20%, and 70%, respectively. Compared with hard clustering, soft clustering can provide more realistic and accurate clustering results in many real-world applications [9].
We adopt the geometric model that was studied in [18, 20, 19]. They showed that clustering ensemble can be naturally formulated as a “geometric prototype” problem. But their results are in lack of systematically studies on the efﬁciency of this model, especially from the theoretical perspective.
In this paper, we illustrate that the geometric prototype actually is equivalent with an instance of
Discrete Wasserstein Barycenter (DWB) [1] in high dimensions (the formal deﬁnition will be given in Section 3). This approach falls under the umbrella of utility function based model, where it uses discrete Wasserstein distance to measure the difference between two clusterings. Compared with other utility functions (e.g., KL-divergence), it has several attractive properties. For example, the discrete Wasserstein distance is symmetric and more robust to noise [42]. More importantly, the
DWB based ensemble model can be easily interpreted from the geometric perspective, and thus we can analyze its performance more conveniently. But when applying the DWB model to the soft clustering ensemble problem, we still have several key issues remaining to be solved. (i) Though a number of DWB algorithms have been developed (as shown in Section 1.2), the clustering ensemble imposes two unique features to the DWB formulation. First, we require the returned DWB to be k-sparse, that is, it should be supported by at most k points in the space (since there are at most k clusters). Also, the number of different clustering solutions can be large in practical scenarios. For instance, to guarantee the consistency of the ﬁnal ensemble solution to the ground-truth clustering, we may generate a large number of clustering solutions via random initializations or random projections [24]. So from the algorithmic perspective, a natural question is whether we can develop more efﬁcient algorithm for the DWB problem with such features? (ii) To the best of our knowledge, only Topchy et al. [58] and Jain [35] discussed the consensus of clustering ensemble in theory. However, both of their analyses rely on the assumption that the ground-truth clustering should be the optimal solution of the ensemble model, which is too strong and may not be realistic in practice. Also, the analysis of [58] only considered hard clustering. It is worth noting that the number of hard clusterings on a ﬁxed set of items is ﬁnite, but the number of soft clusterings is inﬁnity.
In this paper, we focus on these two issues. First, based on some novel geometric insights, we show that it is possible to achieve a ﬁxed-parameter algorithm for the soft clustering ensemble problem if k is a constant, where the obtained approximation factor is 1 + (cid:15) with (cid:15) being any small number in (0, 1); though this is more a theoretical result, we believe that it is of independent interest for such a combinatorial optimization problem in high dimensions [17]. Moreover, the proposed sampling idea inspires our following speedup for the existing DWB algorithms with provable quality guarantee, even if k is large. Second, we prove that the obtained DWB should be close to the ground-truth clustering if the number of given clustering solutions is large enough. Our idea is quite different from
[58, 35]; in particular, our analysis yields a detailed quantitive result for the consensus. 1.2