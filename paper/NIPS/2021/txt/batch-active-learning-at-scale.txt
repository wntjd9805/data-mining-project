Abstract
The ability to train complex and highly effective models often requires an abun-dance of training data, which can easily become a bottleneck in cost, time, and computational resources. Batch active learning, which adaptively issues batched queries to a labeling oracle, is a common approach for addressing this problem.
The practical beneﬁts of batch sampling come with the downside of less adaptivity and the risk of sampling redundant examples within a batch – a risk that grows with the batch size. In this work, we analyze an efﬁcient active learning algorithm, which focuses on the large batch setting. In particular, we show that our sampling method, which combines notions of uncertainty and diversity, easily scales to batch sizes (100K-1M) several orders of magnitude larger than used in previous stud-ies and provides signiﬁcant improvements in model training efﬁciency compared to recent baselines. Finally, we provide an initial theoretical analysis, proving label complexity guarantees for a related sampling method, which we show is approximately equivalent to our sampling method in speciﬁc settings. 1

Introduction
Training highly effective models for complex tasks often hinges on the abundance of training data.
Acquiring this data can easily become a bottleneck in cost, time, and computational resources. One major approach for addressing this problem is active learning, where labels for training examples are sampled selectively and adaptively to more efﬁciently train the desired model over several iterations.
The adaptive nature of active learning algorithms, which allows for improved data-efﬁciency, comes at the cost of frequent retraining of the model and calling the labeling oracle. Both of these costs can be signiﬁcant. For example, many modern deep networks can take days or weeks to train and require hundreds of CPU/GPU hours. At the same time, training human labelers to become proﬁcient in potentially nuanced labeling tasks require signiﬁcant investment from both the designers of the labeling task and the raters themselves. A sufﬁciently large set of queries should be queued in order to justify these costs.
To address these overhead costs, previous works have developed algorithms for the batch active learning setting, where label requests are batched and model updates are made less frequently, reducing the number of active learning iterations. Of course, there is a trade-off, and the practical beneﬁts of batch sampling come with the downside of less adaptivity and the risk of sampling redundant or otherwise less effective training examples within a batch. Batch active learning methods directly combat these risks in several different ways, for example, by incorporating diversity inducing regularizers or explicitly optimizing over the choice of samples within a batch to optimize some notion of information.
However, as the size of datasets grows to include hundreds of thousands and even millions of labeled examples (cf. Deng et al. [2009], Krasin et al. [2017], Van Horn et al. [2018]), we expect the active learning batch sizes to grow accordingly as well. The challenge with very large batch sizes is two-fold:
ﬁrst, the risks associated with reduced adaptivity continue to be compounded and, second, the batch 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
sampling algorithm must scale well with the batch size and not become a computational bottleneck itself. While previous works have evaluated batch active learning algorithms with batch-sizes of thousands of points (e.g., Ash et al. [2020], Sener and Savarese [2018]), in this work, we consider the challenge of active learning with batch sizes one to two orders of magnitude larger.
In this paper, we develop, analyze, and evaluate a batch active learning algorithm called Cluster-Margin, which we show can scale to batch sizes of 100K or even 1M while still providing signiﬁcantly increased label efﬁciency. The main idea behind Cluster-Margin is to leverage Hierarchical Agglom-erative Clustering (HAC) to diversify batches of examples that the model is least conﬁdent on. A key beneﬁt of this algorithm is that HAC is executed only once on the unlabeled pool of data as a preprocessing step for all the sampling iterations. At each sampling iteration, this algorithm then retrieves the clusters from HAC over a set of least conﬁdent examples and uses a round-robin scheme to sample over the clusters.
The contributions of this paper are as follows:
• We develop a novel active learning algorithm, Cluster-Margin, tailored to large batch sizes that are orders of magnitude larger than what have been considered in the literature.
• We conduct large scale experiments using a ResNet-101 model applied to multi-label Open
Images Dataset consisting of almost 10M images and 60M labels over 20K classes, to demonstrate signiﬁcant improvement Cluster-Margin provides over the baselines. In the best result, we ﬁnd that Cluster-Margin requires only 40% of the labels needed by the next best method to achieve the same target performance.
• To compare against latest published results, we follow their experimental settings and con-duct smaller scale experiments using a VGG16 model on multiclass CIFAR10, CIFAR100, and SVHN datasets, and show Cluster-Margin algorithm’s competitive performance.
• We provide an initial theoretical analysis, proving label complexity guarantees for a margin-based clustering sampler, which we then show is approximately equivalent to the Cluster-Margin algorithm in speciﬁc settings. 1.1