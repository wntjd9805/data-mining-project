Abstract
Visual question answering (VQA) is designed to examine the visual-textual rea-soning ability of an intelligent agent. However, recent observations show that many VQA models may only capture the biases between questions and answers in a dataset rather than showing real reasoning abilities. For example, given a question, some VQA models tend to output the answer that occurs frequently in the dataset and ignore the images. To reduce this tendency, existing methods focus on weakening the language bias. Meanwhile, only a few works also consider vision bias implicitly. However, these methods introduce additional annotations or show unsatisfactory performance. Moreover, not all biases are harmful to the models.
Some “biases” learnt from datasets represent natural rules of the world and can help limit the range of answers. Thus, how to ﬁlter and remove the true negative biases in language and vision modalities remain a major challenge. In this paper, we propose a method named D-VQA to alleviate the above challenges from the feature and sample perspectives. Speciﬁcally, from the feature perspective, we build a question-to-answer and vision-to-answer branch to capture the language and vision biases, respectively. Next, we apply two unimodal bias detection modules to explicitly recognise and remove the negative biases. From the sample perspective, we construct two types of negative samples to assist the training of the models, without introducing additional annotations. Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our D-VQA method. 1

Introduction
Vision-and-language tasks [5, 43, 45] require an agent to perform visual-textual reasoning, and play an important role in helping humans or intelligent robots to understand the physical world.
As a typical case of the vision-and-language task, visual question answering (VQA) [8, 35, 42] aims to answer a textual question based on a given image. In this sense, VQA models should master and apply the strong reasoning ability to address the sophistical questions. However, recent studies [3, 23, 36] demonstrate that many VQA models answer the questions without reasoning.
Instead, they excessively rely on superﬁcial correlations (i.e., bias) between the question and answers without considering the image. For example, some VQA models tend to answer “2” for the questions
“How many . . . ”, since most corresponding answers in the dataset are “2”. In this case, the VQA models memorise the bias but ignore the image, resulting in a poor generalisation ability.
∗Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Examples of vision biases. From these examples, most of the answers to the questions usually correspond to the most salient objects/attributes in the image.
As is widely known, most machine learning datasets inevitably have biases. Thus, how to train a robust model from the biased dataset remains a major challenge. To alleviate the challenge in VQA, most existing methods mainly focus on weakening the language bias [9, 33]. Recently, Niu et al.
[29] applied a cause-effect to alleviate language bias. However, the performance is not sufﬁciently encouraging, and it introduces additional parameters in the inference phase. Note that the biases also exist in the vision modality. As shown in Figure 1, many answers to the questions are usually the most salient objects/attributes in the images. In this sense, the VQA models may capture this vision bias to output an answer regarding the salient object in the image, without considering the questions.
Thus, alleviating the negative effect of vision bias is also crucial. Recently, a few works [10, 17] generated counterfactual training samples by masking or transforming critical objects and words to train the models to alleviate language and vision biases. However, they require expensive annotations.
More critically, some “biases” captured from the dataset may represent the natural rule in real world, i.e., commonsense knowledge. They are not harmful to the models. Instead, models beneﬁt from them.
For example, “dog” is a kind of “animal”, and the colour of “oranges” is normally “orange”. Thus, how to ﬁlter and remove the true negative biases in language and vision modalities is challenging.
To alleviate the above issues, we propose a method named D-VQA to overcome the negative biases in language and vision modalities from feature and sample perspectives. Speciﬁcally, (1) from the feature perspective, we ﬁrst build a question-to-answer and a vision-to-answer branch to capture language and vision biases, respectively. Since not all biases are detrimental to the model, two unimodal bias detection modules are devised to ﬁlter and remove negative biases only. To maintain the inference efﬁciency, following existing methods [9, 14, 33], we do not expect to introduce additional parameters in the inference phase. To realise this expectation, we adopt a contrastive loss to approach the multimodal features to the debiased feature as close as possible. In this way, in the inference phase, we simply remove all of the additional branches, and adopt the multimodal features that have a similar representation as the debiased features to obtain accurate predictions. (2) From the sample perspective, the VQA model should answer the questions making full use of the information of questions and images, which can be achieved by improving the sensitivity of the model in both the images and questions. To this end, for each sample in the dataset, we construct two types of negative samples to assist the training and improve the sensitivity of the model.
Our contributions can be summarised as follows: (1) In VQA, we alleviate the biases in both language and vision modalities, from feature and sample perspectives. (2) Since not all biases are harmful, we explicitly devise the bias detection modules to ﬁlter and remove the negative biases. (3) Extensive experiments on the VQA-CP v2 and VQA v2 datasets demonstrate the effectiveness of our D-VQA. 2