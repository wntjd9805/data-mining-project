Abstract
Inspired by biological evolution, we explain the rationality of Vision Transformer by analogy with the proven practical Evolutionary Algorithm (EA) and derive that both of them have consistent mathematical representation. Analogous to the dynamic local population in EA, we improve the existing transformer structure and propose a more efﬁcient EAT model, and design task-related heads to deal with different tasks more ﬂexibly. Moreover, we introduce the spatial-ﬁlling curve into the current vision transformer to sequence image data into a uniform sequential format. Thus we can design a uniﬁed EAT framework to address multi-modal tasks, separating the network architecture from the data format adaptation. Our approach achieves state-of-the-art results on the ImageNet classiﬁcation task compared with recent vision transformer works while having smaller parameters and greater throughput. We further conduct multi-modal tasks to demonstrate the superiority of the uniﬁed EAT, e.g., Text-Based Image Retrieval, and our approach improves the rank-1 by +3.7 points over the baseline on the CSS dataset.3 1

Introduction
Since Vaswani et al. [69] introduce the Transformer that achieves outstanding success in the machine translation task, many improvements have been made to this method [15, 38, 21]. Recent works [64, 82, 43] led by ViT [23] have achieved great success in the ﬁeld of many vision tasks by replacing
CNN with transformer structure. In general, these works are experimentally conducted to verify the effectiveness of modules or improvements, but they may lack other forms of supporting evidence.
Inspired by biological population evolution, we explain the rationality of Vision Transformer by analogy with the proven effective, stable, and robust Evolutionary Algorithm (EA), which has been widely used in practical applications. Through analogical analysis, we observe that the training procedure of the transformer has similar attributes to the naive EA, as shown in Figure 1. Take the one-tier transformer (abbr., TR) as an example. 1) TR processes a sequence of patch embeddings while EA evolutes individuals, both of which have the same vector formats and necessary initialization. 2) The
Multi-head Self-Attention (MSA) among patch embeddings in TR is compared with that of (sparse) global individual crossover among all individuals in EA, in which local and dynamic population concepts are introduced to increase running speed and optimize results [63, 59]. 3) Feed-Forward
∗Work done during an intership at Tencent Youtu Lab.
†Corresponding author. 3Code is available at https://github.com/TencentYoutuResearch/BaseArchitecture-EAT. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Analogy of EA (top) and Transformer (bottom) pipelines. For simplicity, only one layer of the Transformer structure is displayed here.
Figure 2: Different SFC indexing methods, taking 2D images with side length of 8 as an example.
Network (FFN) in TR enhances embedding features that is similar to the individual mutation in
EA. 4) During training, TR optimizes the network through backpropagation while EA optimizes individuals through selection and ﬁtness calculation. 5) TR chooses the enhanced Classiﬁcation
Token (Cls Token) as the target output, while EA chooses Individual with the Best Fitness (IBF).
Meanwhile, we deduce the mathematical characterization of crossover and mutation operators in EA (c.f ., Equations 5,8) and ﬁnd that they have the same mathematical representation as MSA and FFN in TR (c.f ., Equations 6,9), respectively. Inspired by the characteristics in the crossover step of EA, we propose a novel EA-based Transformer (EAT) that intuitively designs a local operator in parallel with global MSA operation, in which the local operator can be instantiated as 1D convolution, local
MSA, etc. Subsequent experiments demonstrate the effectiveness of this design in that it could reduce parameters and improve the running speed and boost the network performance, which is consistent with the results of EAs in turn. Current TR-based models would initialize different tokens for different tasks, and they participate in every level of calculation that is somewhat incompatible with other tokens for internal operations. Therefore, we design task-related heads docked with transformer backbone to complete ﬁnal information fusion, which is more ﬂexible for different tasks learning and suitable for the transfer learning of downstream tasks.
Our designed local operator receives the same data format as the global MSA branch, i.e., a se-quence that is conform to NLP. Therefore, the generally used high dimension operations such as 2D reshape [79, 77, 80] are not required, which brings the possibility to standardize multi-modal data (e.g., 1D sentence, 2D image, and 3D video) of input into consistent sequence data in one uniﬁed model. To accomplish this target, we introduce the space-ﬁlling curve (SFC) concept to standardize the multi-modal data format and design an SFC module. As shown in Figure 2, taking 2D image as an example, the top half represents four kinds of SFCs: Sweep, Scan, Z-Order, and Hilbert, while the bottom two lines represent the sequenced image by Z-Order SFC. Thus the image (∈ R8×8×3) is speciﬁcally re-indexed and arranged (∈ R64×3) by the predeﬁned SFC before feeding the network, realizing uniform sequence input. The difference between SFCs mainly reﬂects how the 1D sequence preserves the 2D spatial structure, and the SFC can be extended to 3D and higher dimensions.
Speciﬁcally, we make the following four contributions:
• In theory, we explain the rationality of Vision Transformer (TR) by analogy with Evolution-ary Algorithm (EA) and derive that they have consistent mathematical representation.
• For the method, we improve a uniﬁed EAT model by analogy with dynamic local population concept in EA and design a Task-related Head to deal with various tasks more ﬂexibly. 2
• On framework, we introduce Space-Filling Curve (SFC) module as a bridge between rasterized 2D image data and serialized 1D sequence data. This makes it possible to integrate the uniﬁed paradigm that uses a uniﬁed model to solve multi-modal tasks, keeping the network architecture and data structure independent of the data dimensionality. Note that only several 1D operators are required for our method, meaning that it is very friendly to the underlying optimization workload of different platforms.
• Massive experiments on classiﬁcation and multi-modal tasks demonstrate the superiority and ﬂexibility of our approach. 2