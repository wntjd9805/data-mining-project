Abstract
√
We study the problem of learning in the stochastic shortest path (SSP) setting, where an agent seeks to minimize the expected cost accumulated before reaching a goal state. We design a novel model-based algorithm EB-SSP that carefully skews the empirical transitions and perturbs the empirical costs with an exploration bonus to induce an optimistic SSP problem whose associated value iteration scheme is guaranteed to converge. We prove that EB-SSP achieves the minimax regret rate (cid:101)O(B(cid:63)
SAK), where K is the number of episodes, S is the number of states, A is the number of actions, and B(cid:63) bounds the expected cumulative cost of the optimal policy from any state, thus closing the gap with the lower bound. Interestingly,
EB-SSP obtains this result while being parameter-free, i.e., it does not require any prior knowledge of B(cid:63), nor of T(cid:63), which bounds the expected time-to-goal of the optimal policy from any state. Furthermore, we illustrate various cases (e.g., positive costs, or general costs when an order-accurate estimate of T(cid:63) is available) where the regret only contains a logarithmic dependence on T(cid:63), thus yielding the
ﬁrst (nearly) horizon-free regret bound beyond the ﬁnite-horizon MDP setting. 1

Introduction
Stochastic shortest path (SSP) is a goal-oriented reinforcement learning (RL) setting where the agent aims to reach a predeﬁned goal state while minimizing its total expected cost [Bertsekas, 1995]. In particular, the interaction between the agent and the environment ends only when (and if) the goal state is reached, so the length of an episode is not predetermined (nor bounded) and it is inﬂuenced by the agent’s behavior. SSP includes both ﬁnite-horizon and discounted Markov Decision
Processes (MDPs) as special cases. Moreover, many common RL problems can be cast under the
SSP formulation, such as game playing (e.g., Atari games) or navigation (e.g., Mujoco mazes).
We study the online learning problem in the SSP setting (online SSP in short), where both the transition dynamics and the cost function are initially unknown and the agent interacts with the environment through multiple episodes. The learning objective is to achieve a performance as close
∗equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
as possible to the optimal policy π(cid:63), that is, the agent should achieve low regret (i.e., the cumulative difference between the total cost accumulated across episodes by the agent and by the optimal policy).
We identify three desirable properties for a learning algorithm in online SSP.
√
• Desired property 1: Minimax. The information-theoretic lower bound on the regret is
SAK) [Rosenberg et al., 2020], where K is the number of episodes, S is the number
Ω(B(cid:63) of states, A is the number of actions, and B(cid:63) bounds the total expected cost of the optimal policy starting from any state (assuming for simplicity that B(cid:63) ≥ 1).
An algorithm for online SSP is (nearly) minimax optimal if its regret is bounded by (cid:101)O(B(cid:63) up to logarithmic factors and lower-order terms.
√
SAK),
• Desired property 2: Parameter-free. Another relevant dimension is the amount of prior knowl-edge required by the algorithm. While the knowledge of S, A, and the cost (or reward) range [0, 1] is standard across regret-minimization settings (e.g., ﬁnite-horizon, discounted, average-reward), the complexity of learning in SSP problems may be linked to SSP-speciﬁc quantities such as B(cid:63) and T(cid:63), which denotes the expected time-to-goal of the optimal policy from any state.
An algorithm for online SSP is parameter-free if it relies neither on T(cid:63) nor B(cid:63) prior knowledge.
• Desired property 3: Horizon-free. A core challenge in SSP is to trade off between minimizing costs and quickly reaching the goal state. This is accentuated when the instantaneous costs are small, i.e., when there is a mismatch between B(cid:63) and T(cid:63). Indeed, while B(cid:63) ≤ T(cid:63) always holds since the cost range is [0, 1], the gap between the two may be arbitrarily large (see e.g., the simple example of App. A). The lower bound stipulates that the regret does depend on B(cid:63), while the “time horizon” of the problem, i.e., T(cid:63) should a priori not impact the regret, even as a lower-order term.
An algorithm for online SSP is (nearly) horizon-free if its regret depends only logarithmically on T(cid:63).
Our deﬁnition extends the property of so-called horizon-free bounds recently uncovered in ﬁnite-horizon MDPs with total reward bounded by 1 [Wang et al., 2020a, Zhang et al., 2021a,b]. These bounds depend only logarithmically on the horizon H, which is the number of time steps by which any policy terminates. Such notion of horizon would clearly be too strong in the more general class of SSP, where some (even most) policies may never reach the goal, thus having unbounded time horizon. A more adequate notion of horizon in SSP is T(cid:63), which bounds the expected time of the optimal policy to terminate the episode starting from any state.
Finally, while the previous properties focus on the learning aspects of the algorithm, another important consideration is computational efﬁciency. It is desirable that a learning algorithm has run-time complexity at most polynomial in K, S, A, B(cid:63), and T(cid:63). All existing algorithms for online SSP, including the one proposed in this paper, meet such requirement.