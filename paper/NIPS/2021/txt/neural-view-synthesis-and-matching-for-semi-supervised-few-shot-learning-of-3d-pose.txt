Abstract
We study the problem of learning to estimate the 3D object pose from a few labelled examples and a collection of unlabelled data. Our main contribution is a learning framework, neural view synthesis and matching, that can transfer the 3D pose annotation from the labelled to unlabelled images reliably, despite unseen 3D views and nuisance variations such as the object shape, texture, illumination or scene context. In our approach, objects are represented as 3D cuboid meshes composed of feature vectors at each mesh vertex. The model is initialized from a few labelled images and is subsequently used to synthesize feature representations of unseen 3D views. The synthesized views are matched with the feature representations of unlabelled images to generate pseudo-labels of the 3D pose. The pseudo-labelled data is, in turn, used to train the feature extractor such that the features at each mesh vertex are more invariant across varying 3D views of the object. Our model is trained in an EM-type manner alternating between increasing the 3D pose invariance of the feature extractor and annotating unlabelled data through neural view synthesis and matching. We demonstrate the effectiveness of the proposed semi-supervised learning framework for 3D pose estimation on the PASCAL3D+ and KITTI datasets. We ﬁnd that our approach outperforms all baselines by a wide margin, particularly in an extreme few-shot setting where only 7 annotated images are given. Remarkably, we observe that our model also achieves an exceptional robustness in out-of-distribution scenarios that involve partial occlusion. The code is available at https://github.com/Angtian/NeuralVS. 1

Introduction
Object pose estimation is a fundamentally important task in computer vision with a multitude of real-world applications, e.g. in self-driving cars or augmented reality applications. Current deep learning approaches to 3D pose estimation achieve a high performance, but they require large amounts of annotated data to be trained successfully. However, the human annotation of an object’s 3D pose is difﬁcult and time consuming, therefore it is desirable to develop methods for learning 3D pose estimation from as few labelled examples as possible.
A powerful approach for training models without requiring a large amount of labels is semi-supervised learning (SSL). SSL mitigates the requirement for labeled data by providing a means of leveraging unlabeled data. Since unlabeled data can often be obtained with low human labor, any performance boost conferred by SSL often comes with low cost. This has led to a multitude of SSL methods, for example for image classiﬁcation [18, 44], object detection [40] and keypoint localization [31].
However, only limited attention was devoted to SSL for 3D pose estimation. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of how we transfer the 3D pose annotation from one training image to a set of unlabelled images. A detailed description of this process is given in the introduction section.
In this work, we introduce a semi-supervised learning framework for category-level 3D pose esti-mation from very few annotated examples and a collection on unlabelled images. Intuitively, our framework follows a spatial matching approach, in which we transfer the 3D pose annotation from the labelled examples to the unlabelled data. In general, matching-based approaches aim at transfer-ring annotations between training examples by estimating correspondences between the respective images [29, 2, 26, 15, 8, 45, 19, 27]. However, those prior works mainly focused on transferring 2D annotations, such as segmentation, part annotations, or keypoints, and mostly assume that the objects in the spatially matched images have a similar 3D pose. In this work, we explore the spatial matching of images with objects that have a largely varying 3D pose, in addition to nuisance variations in their shape, color, texture, context and illumination. 3D pose transfer through Neural View Synthesis and Matching (NVSM). The intuition behind our approach is illustrated in Figure 1 using a single annotated training image (but note that in practice we use several images). Our method proceeds in two steps: (I) Neural view synthesis and (II)
Matching. During neural view synthesis, we start with (1.) extracting the feature map of a training image with a convolutional backbone (CNN) that was pre-trained for image classiﬁcation. (2.) A mesh cuboid (purple mesh) is projected onto the extracted feature map using the ground-truth 3D pose annotation θ to sample the corresponding feature vectors at each visible mesh vertex (indicated by red arrows). (3.) The mesh cuboid is subsequently rotated into a novel pose θ + ∆θ. By rasterising the sampled feature vectors at the mesh vertices we synthesize a feature map of the object in a novel pose. Subsequently, (4.) we compute the feature maps of the unlabelled training images with the
CNN. (5.) We spatially match the synthesized feature map those feature maps of the unlabelled data, resulting in a set of matching scores. (6.) Finally, we assign the 3D pose θ + ∆θ as pseudo-label to those images with highest matching scores (marked in red).
As prior works have shown [41, 58, 5], the features in pre-trained convolutional networks are surprisingly reliable in spatial matching tasks as they are invariant to small variations in nuisance variables such as shape deformations, or changes in the object texture and illumination. This invariance property enables the spatial matching of annotated training images with unlabelled data across varying 3D poses. However, as we demonstrate in our experiments (Section 4.3), the features of the classiﬁcation pre-trained CNN are not invariant to large 3D pose variations. Therefore, this pseudo-labelling process is initially only accurate for those objects in the unlabelled data that have a similar pose as the object in the annotated training image. This raises the need for improving the 3D pose invariance in the CNN features, to be able to annotate images with larger pose variability.
Increasing 3D pose invariance in the feature extractor. We aim to increase the 3D pose invariance in the CNN using the pseudo-labelled data obtained from the NVSM process. The pseudo-labels enable us to extract the features vectors at corresponding mesh vertices in the data. To increase the 3D pose invariance in the CNN, we use a contrastive loss that encourages the feature vectors at a particular mesh vertex to become more similar to each other, while at the same time making them different from the features of other mesh vertices. This contrastive training improves the feature 2
representation by making it more invariant to changes in the 3D pose, as well as to category-speciﬁc nuisance variables such as the object’s color, shape, illumination, while also reducing the ambiguity between nearby feature vectors in the feature map, which beneﬁts the spatial matching quality.
The improved CNN features enable us to increase the 3D pose scope ∆θ in the NVSM process, and hence to increase the 3D pose variability in the pseudo-labelled data. The pose diversity in the labelled data, in turn, enables us to improve the 3D pose invariance in the CNN feature extractor.
We proceed to iterate between pseudo-labelling training data and training the CNN feature extractor, while continuously enlarging the 3D pose variation ∆θ of the synthesized views in NVSM. After each update of the CNN, we also update the feature representations at the mesh vertices by computing the moving average of the corresponding feature vectors in the pseudo-labelled images. In this way, the feature representation on the mesh cuboid is continuously adapting to the trained feature extractor.
After the training, the trained CNN and mesh cuboid are used for 3D pose estimation. Given a test image, we ﬁrst compute the feature map using the CNN and subsequently optimize the 3D pose of the mesh cuboid such that the distance between the features of the projected mesh vertices and the feature map are minimized. We evaluate our model at 3D pose estimation on the Pascal3D+ [53] and KITTI
[13] datasets. Our approach proves highly effective in leveraging unlabeled data outperforming all baselines by a wide margin, particularly in an extreme few-shot setting where only 7 or 20 annotated images are given. Remarkably, we observe that our model also achieves an exceptional robustness in out-of-distribution scenarios that involve partial occlusion. 2