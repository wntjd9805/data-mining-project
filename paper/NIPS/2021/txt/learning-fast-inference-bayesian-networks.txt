Abstract
We propose new methods for learning Bayesian networks (BNs) that reliably sup-port fast inference. We utilize maximum state space size as a more ﬁne-grained measure for the BN’s reasoning complexity than the standard treewidth measure, thereby accommodating the possibility that variables range over domains of differ-ent sizes. Our methods combine heuristic BN structure learning algorithms with the recently introduced MaxSAT-powered local improvement method (Peruvemba
Ramaswamy and Szeider, AAAI’21). Our experiments show that our new learning methods produce BNs that support signiﬁcantly faster exact probabilistic inference than BNs learned with treewidth bounds. 1

Introduction
The time complexity of probabilistic reasoning on a Bayesian Network (BN) is dominated by the maximum state space size of clusters (i.e., bags of the BN’s tree decomposition [Lauritzen and
Spiegelhalter, 1988, Dechter, 1999, Kask et al., 2011]; a bag’s state space size is the product of the domain sizes of all the variables it contains). We propose algorithms for learning BNs from data, keeping the state space size within a user-speciﬁed bound. This results in fast-inference BNs, i.e.,
BNs reliably admitting fast probabilistic reasoning. We compare our algorithms to the baseline of state-of-the-art bounded treewidth BN learning algorithms, on real-world benchmark data sets, with up to over a thousand variables. The results show a clear advantage for new bounded state space (bss) algorithms.
It is common to encounter non-binary variables in real-world data. Moreover, during our preliminary analysis, we noticed that even variables with domain sizes as small as 4 were sufﬁcient to impact the reasoning times signiﬁcantly. This is in agreement with the fact that the reasoning time has an exponential dependence on the domain sizes. For instance, consider some of the networks learned for alarm and hepar2 having 37 and 70 variables, respectively. Both these datasets were learned with small values of treewidth, and the maximum domain size of the variables is 4. Despite this, they exhibited reasoning times in the order of magnitude of 2.5 seconds.
For our bss BN structure learning algorithms, we build upon recent work on bounded treewidth BN structure learning, particularly on the heuristic algorithms k-greedy and k-MAX by Scanagatta et al.
[2016, 2018] as well BN-SLIM by Peruvemba Ramaswamy and Szeider [2021a]. The latter is a post-processing algorithm that uses MaxSAT to improve BNs generated by the heuristics. All these algorithms assume a user-speciﬁed upper bound k for the treewidth of the learned BN and optimize the BN’s score under the given treewidth bound. The learning algorithms are highly optimized for dealing with large instances, and so the generalization of treewidth bounds to state space bounds isn’t straightforward. The main challenge for extending BN-SLIM to bss learning is to replace BN-SLIM’s simple cardinality constraints with a MaxSAT encoding that bounds the state-space of a bag, i.e., a product of integers. We achieve this by switching to logarithms and bounding the sum of real numbers, utilizing a MaxSAT encoding based on binary decision diagrams (BDDs). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
We consider several variants of bss learning algorithms, tested them on 16 real-world benchmark data sets with up to 1041 variables, and compare them with bounded treewidth BN learning algorithms.
For the comparison, we put pairs of scatter plots side by side, which show the tradeoff between reasoning speed and data-ﬁtting (score), one for the baseline methods and one for the bounded state space methods. The bounded state space methods show better performance throughout, with signiﬁcantly higher reliability (small variance). 1.1