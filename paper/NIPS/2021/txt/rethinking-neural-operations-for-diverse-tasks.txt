Abstract
An important goal of AutoML is to automate-away the design of neural networks on new tasks in under-explored domains. Motivated by this goal, we study the problem of enabling users to discover the right neural operations given data from their speciﬁc domain. We introduce a search space of operations called XD-Operations that mimic the inductive bias of standard multi-channel convolutions while being much more expressive: we prove that it includes many named operations across multiple application areas. Starting with any standard backbone such as ResNet, we show how to transform it into a search space over XD-operations and how to traverse the space using a simple weight-sharing scheme. On a diverse set of tasks— solving PDEs, distance prediction for protein folding, and music modeling—our approach consistently yields models with lower error than baseline networks and often even lower error than expert-designed domain-speciﬁc approaches. 1

Introduction
Automated machine learning (AutoML) and neural architecture search (NAS) are often motivated by a vision of democratizing ML by reducing the need for expert design on a variety of tasks. While
NAS has grown rapidly with developments such as weight-sharing [36] and “NAS-benches” [47, 49], most efforts focus on search spaces that glue together established primitives for well-studied tasks like vision and text [32, 26, 45, 25] or on issues such as latency [8, 13]. In this work, we revisit the broader vision of NAS and propose to move towards much more general search spaces while still exploiting successful network topologies. To do so we focus on expanding the set of operations, which is usually fairly small; for example, that of the well-studied DARTS space has eight elements: a few types of convolution and pooling layers [32]. The baseline approach for expanding this set—adding operations one-by-one—scales poorly and will not result in new operations when faced with new types of data.
Our core contribution is a re-imagining of NAS operation spaces that drastically expands this set in a principled fashion to include both standard operations as well as a wide range of new ones. To do so we exploit the fact that most standard operations used in modern NAS return linear transforms diagonalized by the discrete Fourier transform (DFT). Replacing the DFT matrices in the diagonal decomposition by a more expressive family of efﬁcient linear transforms known as Kaleidoscope or
∗ denotes equal contribution. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Diagram of our search space depicting a NAS method picking an operation for an edge in a backbone network (left). Instead of choosing from a discrete search space, we use a relaxation based on the convolution’s diagonalization by the discrete Fourier transform in which the DFTs are replaced by K-matrices [10] K, L, and M (middle); these are the main architecture parameters of our new search space over Expressive Diagonalization (XD) operations. This space contains most operations considered in standard NAS and many other important operations in a variety of domains (right).
K-matrices [10] yields the set of Expressive Diagonalization (XD) Operations, which comprise a large search space containing various types of grid-based convolutions and pooling, permutations, transposed convolutions, certain kinds of graph convolutions, the Fourier Neural Operator (FNO) [30], and inﬁnitely many more. This broad expressivity reﬂects the key insight of our work: that many of the most important neural operations in ML consist of multiple channels that apply weights w to inputs x by computing
K diag(Lw)Mx where the matrices K, L, and M are efﬁcient (to represent and apply) and shared across channels. (1)
We leverage XD-operations to take critical steps towards a broader NAS that enables the discovery of good design patterns with limited human speciﬁcation from data in under-explored domains. To do so we develop a simple procedure which transforms any backbone convolutional neural network (CNN) into an architecture search space by replacing its operations with XD-operations. This space is then searched using a simple weight-sharing algorithm that needs only a small amount of tuning to
ﬁnd effective operations. As a simple ﬁrst demonstration, we show that XD-operations yield models that are 15% more accurate than standard discrete search spaces on permuted CIFAR-10, highlighting the fragility of standard NAS operation spaces on new datasets, and thus the need for XD-operations.
As our main evaluation, we demonstrate the effectiveness of XD-operations in a series of applications showing that, starting from vanilla CNNs, they consistently outperform custom-designed operations.
• Learning to solve partial differential equations (PDEs): when substituted into a simple CNN backbone, XD-operations outperform convolutions and the dense prediction NAS method Auto-DeepLab [31], and even achieve lower error than custom-designed, state-of-the-art operations (FNOs [30]) across three problems with different dimensionalities (Burgers’ equation, Darcy Flow, and Navier-Stokes). Our method also maintains consistent performance across different resolutions, a major stated advantage of FNOs over previous methods.
• Protein folding: on the task of predicting residue distances in a polypeptide chain—a key compo-nent of the protein folding problem—we substitute XD-operations into vanilla ResNets and achieve lower error than cyclically-dilated ResNets adapted speciﬁcally for this setting [1]. Furthermore, our ResNet-34 XD outperforms the reported error of the much deeper Dilated ResNet-258.
• Music modeling: on two next-note prediction tasks, we show that substituting XD-operations into an undilated CNN outperforms temporal convolutional networks (TCNs)—exponentially-dilated 1d CNNs that themselves outperform standard convolutional and recurrent networks [5].
Code to reproduce these results is available here: https://github.com/nick11roberts/XD.
Software to apply XD-operations can be found here: https://github.com/mkhodak/relax. 2