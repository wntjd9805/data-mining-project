Abstract
Training saliency detection models with weak supervisions, e.g., image-level tags or captions, is appealing as it removes the costly demand of per-pixel annotations.
Despite the rapid progress of RGB-D saliency detection in fully-supervised setting, it however remains an unexplored territory when only weak supervision signals are available. This paper is set to tackle the problem of weakly-supervised RGB-D salient object detection. The key insight in this effort is the idea of maintaining per-pixel pseudo-labels with iterative reﬁnements by reconciling the multimodal input signals in our joint semantic mining (JSM). Considering the large variations in the raw depth map and the lack of explicit pixel-level supervisions, we propose spatial semantic modeling (SSM) to capture saliency-speciﬁc depth cues from the raw depth and produce depth-reﬁned pseudo-labels. Moreover, tags and captions are incorporated via a ﬁll-in-the-blank training in our textual semantic modeling (TSM) to estimate the conﬁdences of competing pseudo-labels. At test time, our model in-volves only a light-weight sub-network of the training pipeline, i.e., it requires only an RGB image as input, thus allowing efﬁcient inference. Extensive evaluations demonstrate the effectiveness of our approach under the weakly-supervised setting.
Importantly, our method could also be adapted to work in both fully-supervised and unsupervised paradigms. In each of these scenarios, superior performance has been attained by our approach with comparing to the state-of-the-art dedicated methods. As a by-product, a CapS dataset is constructed by augmenting existing benchmark training set with additional image tags and captions. Code and dataset are available at https://github.com/jiwei0921/JSM. 1

Introduction
As a fundamental computer vision task, salient object detection (SOD) aims at locating and segment-ing visually distinctive objects in a scene. It plays an important role in a variety of downstream appli-cations including image retrieval [31, 65], medical analysis [48, 28, 25], multimodal fusion [79, 80] and video analysis [88, 81, 90]. Recent progress in supervised RGB-D SOD [5, 52, 10, 26, 89, 33] has demonstrated signiﬁcant beneﬁts of engaging depth information in saliency detection from complex scenes. The success of these fully-supervised methods, however, relies heavily on the large-scale, pre-cise, pixel-level annotations, which are often laborious and time-consuming to acquire. On the other hand, an image usually comes with additional information in its meta-data such as tags and captions from users to describe the scene context and content, which may serve as cheap weak-supervision signals. These weak supervision signals are nonetheless noisy and have mixed qualities. Similar weak-supervision signals have been explored in RGB-image based SOD [63, 70, 38], where the noisy nature of these side information is unfortunately overlooked. To further complicate the matter, the lack of explicit pixel-level supervision brings new challenge to the RGB-D SOD task: the depth values from raw depth maps are often noisy and sometimes inconsistent. For example, in Fig. 1,
*means equal contributions. Wei Ji (cid:0) (wji3@ualberta.ca) is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
Figure 1: Illustration of weakly-supervised RGB-D salient object detection. RGB and depth images, as well as weak supervision signals such as image-level tags and captions are exploited. Initial pseudo-label is generated by the handcrafted methods, which is then iteratively updated by our joint semantic mining pipeline. GT is ground-truth label for reference. similar depth values are shared by the cat and the underneath couch, making it difﬁcult to discern the salient object from backgrounds. Without the explicit pixel-level supervision, existing cross-modal fusion strategies adopted by fully-supervised RGB-D methods [36, 55, 37, 27] would simply fail.
These observations motivate us to consider the new problem of weakly-supervised RGB-D salient object detection, which takes as input the RGB and depth images, as well as weak supervision signals such as image-level tags and captions, as illustrated in Fig. 1. By removing the demand for laborious per-pixel annotations, it also brings new challenges: 1) how to address the noisy nature of the weak supervision signals; 2) how to tackle the depth noise and inconsistency to facilitate proper separation of foreground and background regions.
This leads us to propose the use of pseudo-labels with iterative reﬁnements in training: the pseudo-label provides internal pixel-level supervision signals, which is progressively updated by reconciling the multimodal input signals and the current information ﬂow of the neural net, based on the previous pseudo-label. As illustrated in Fig. 2, this is realized by an interaction between two core modules, namely spatial semantic modeling (SSM) and joint semantic mining (JSM): SSM is designed to capture the saliency-speciﬁc depth semantics, to eliminate the background noises in the coarse saliency prediction, and to generate a depth-reﬁned pseudo-label. This simple yet effective module is very generic, which could be easily plugged-in different setups, including unsupervised & fully-supervised scenarios; meanwhile, the JSM module is proposed to leverage depth semantics and weak supervision signals for attaining more reliable pseudo-labels. Speciﬁcally, a partial textual input, i.e., image-level tag and caption with its salient word being masked, is fed into a dedicated textual semantic modeling or TSM to estimate the conﬁdence scores of competing pseudo-labels, and to
ﬁll-in-the-blank. Intuitively, a semantically consistent pseudo-label should provide better context cues to reconstruct the salient word; while a closer guess of the masked word would indicate a better pseudo-label. The alternation between SSM and JSM modules is thus expected to give rise to more trustworthy pseudo-labels. At test time, it is then sufﬁcient to take an input RGB image and activate a light-weight network to deliver its ﬁnal prediction. That is, test time input involves only an RGB image, without the need of any depth map or image-level tags and captions. This drastically simpliﬁes the input requirement and reduces the computation burden at deployment stage. Moreover, given the lack of training dataset for the weakly-supervised setting, we adapt existing RGB-D training dataset to annotate additional image-level tags and captions, which is referred to as the CapS dataset.
The main contributions of this paper are as follows. (1) A new problem of weakly-supervised RGB-D salient object detection is considered. In this regard, a CapS dataset is curated by augmenting the existing RGB-D SOD training dataset with image-level tagging and captioning annotations. (2) The key ingredient of our approach involves the production of pseudo-labels with iterative reﬁnements, realized by iterative updates between two internal modules, SSM and JSM. Empirical experiments demonstrate the effectiveness of our approach in weakly-supervised setting. Moreover, (3) after proper adaptation of our approach in unsupervised and fully-supervised scenarios, superior performance is also observed when comparing to the respective state-of-the-art methods. (4) Our test time inference amounts to executing a light-weight saliency network: as illustrated in dotted line at Fig. 2, only an
RGB image is used as its input, thus allows efﬁcient and effective inference. 2