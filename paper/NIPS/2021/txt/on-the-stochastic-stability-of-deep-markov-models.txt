Abstract
Deep Markov models (DMM) are generative models that are scalable and expres-sive generalization of Markov models for representation, learning, and inference problems. However, the fundamental stochastic stability guarantees of such models have not been thoroughly investigated. In this paper, we provide sufﬁcient condi-tions of DMM’s stochastic stability as deﬁned in the context of dynamical systems and propose a stability analysis method based on the contraction of probabilistic maps modeled by deep neural networks. We make connections between the spec-tral properties of neural network’s weights and different types of used activation functions on the stability and overall dynamic behavior of DMMs with Gaussian distributions. Based on the theory, we propose a few practical methods for design-ing constrained DMMs with guaranteed stability. We empirically substantiate our theoretical results via intuitive numerical experiments using the proposed stability constraints. 1

Introduction
Modeling, analysis, and control of dynamical systems are of utmost importance for various physical and engineered systems such as ﬂuid dynamics, oscillators, power grids, transportation networks, and autonomous driving, to name just a few. The systems are generally subjected to uncertainties arising from a plethora of factors such as exogenous noise, plant-model mismatch, and unmodeled system dynamics, which have led researchers to model the dynamics in stochastic frameworks. One of the most commonly used probabilistic frameworks to model dynamical system is the Hidden
Markov Models (HMMs) [Rabiner and Juang, 1986, Eddy, 1996] which in their vanilla form have been extensively investigated for representation, learning, and inference problems [Ghahramani and
Jordan, 1997, Cappé et al., 2006, Beal et al., 2002]. One of its variants, the Gaussian state space models, have been used in the systems and control community for decades [Beckers and Hirche, 2016, Eleftheriadis et al., 2017].
It has been shown that expressivity of Markov models to emulate complex dynamics and sequential behaviors is greatly improved by parametrizing such models using deep neural networks, giving rise to Deep Markov Models (DMMs) [Krishnan et al., 2017]. Main research activities have been focused on the inference of these models. In particular, works such as Awiszus and Rosenhahn [2018],
Liu et al. [2019], Mustafa et al. [2019], Qu et al. [2019] proposed parametrizing the probability distributions using deep neural networks for modeling various complex dynamical systems. Despite the rising popularity of DMMs, many of their theoretical properties such as robustness to perturbations, and stability of the generated trajectories remain open research questions. Many natural systems exhibit complex yet stable dynamical behavior that is described by converging trajectories towards an attractor set [Brayton and Tong, 1979]. Additionally, safety-critical systems such as an autonomous 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
driving call for formal veriﬁcation methods to ensure safe operation. Thus the ability to guarantee stability in data-driven models could lead to improved generalization or act as a safety certiﬁcate in real-world applications.
In this paper, we propose a new analytical method to assess stochastic stability of DMMs. More speciﬁcally, we utilize spectral analysis of deep neural networks (DNNs) modeling DMM’s distri-butions. This allows us to make connections between the stability of deterministic DNNs and the stochastic stability of deep Markov models. As a main theoretical contribution we provide sufﬁcient conditions for stochastic stability of DMMs. Based on the proposed theory we introduce several practical methods for design of constrained DMM with stability guarantees. In summary, the main contributions of this paper include: 1. Stability analysis method for deep Markov models: we base the analysis on the operator norms of deep neural networks modeling mean and variance of the DMM’s distributions. 2. Sufﬁcient conditions for stochastic stability of deep Markov models: we show the sufﬁ-ciency of the operator norm-based contraction conditions for DMM’s deep neural networks by leveraging Banach ﬁxed point theorem. 3. Stability constrained deep Markov models: we introduce a set of methods for the design of provably stable deep Markov models. 4. Numerical case studies: we analyze connections between the design parameters of neural networks, stochastic stability, and operator norms of deep Markov models. 2