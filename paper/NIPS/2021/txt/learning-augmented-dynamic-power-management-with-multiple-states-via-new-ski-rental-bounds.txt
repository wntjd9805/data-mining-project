Abstract
We study the online problem of minimizing power consumption in systems with multiple power-saving states. During idle periods of unknown lengths, an algorithm has to choose between power-saving states of different energy consumption and wake-up costs. We develop a learning-augmented online algorithm that makes decisions based on (potentially inaccurate) predicted lengths of the idle periods.
The algorithm’s performance is near-optimal when predictions are accurate and degrades gracefully with increasing prediction error, with a worst-case guarantee almost identical to the optimal classical online algorithm for the problem. A key ingredient in our approach is a new algorithm for the online ski rental problem in the learning augmented setting with tight dependence on the prediction error. We support our theoretical ﬁndings with experiments. 1

Introduction
Energy represents up to 70% of total operating costs of modern data centers [41] and is one of the major quality-of-service parameters in battery-operated devices. In order to ameliorate this, contemporary CPUs are equipped with sleep states to which the processor can transition during periods of inactivity. In particular, the ACPI-standard [25] speciﬁes that each processor should possess, along with the active state 𝐶0 that is used for processing tasks, at least one sleep state 𝐶1.
Modern processors generally possess more sleep states 𝐶2, . . . ; for example, current Intel CPUs implement at least 4 such 𝐶-states [19]. Apart from CPUs, such sleep states appear in many systems ranging from hard drives or mobile devices to the start-stop feature found in many cars, and are furthermore often employed when rightsizing data centers [2].
Intuitively, in a “deeper” sleep state, the set of switched-off components will be a superset of the corresponding set in a more shallow sleep state. This implies that the running cost for residing in that deeper state will be lower, but the wake-up cost to return to the active state 𝐶0 will be higher compared to a more shallow sleep state. In other words, there is a tradeoff between the running and the wake-up cost. During each idle period, a dynamic power management (DPM) strategy has to decide in which state the system resides at each point in time, without a-priori knowledge about the duration of the idle period. Optimally managing these sleep states is a challenging problem due to its online nature. On the one hand, transitioning the system to a too deep state could be highly suboptimal if the idle period ends shortly after. On the other hand, spending too much idle time in a shallow state would accumulate high running costs. The impact of DPM strategies in practice 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
has been studied for instance in data centers, where each machine may be put to a sleep mode if no request is expected. See the study of Lim et al. [34] on multi-tier data centers.
The special case of 2-state DPM systems, i.e., when there is only a single sleep state (besides the active state), is essentially equivalent to the ski rental problem, one of the most classical problems and of central importance in the area of online optimization [39; 26]. This problem is deﬁned as follows: A person goes skiing for an unknown number of days. On every day of skiing, the person must decide whether to continue renting skis for one more day or to buy skis. Once skis are bought there will be no more cost on the following days, but the cost of buying is much higher than the cost of renting for a day. It is easy to see that this captures a single idle period of DPM with a single sleep state whose running cost is 0: The rental cost corresponds to the running cost of the active state and the cost of buying skis corresponds to the wake-up cost; transitioning to the sleep state corresponds to buying skis. Given this equivalence, the known 2-competitive deterministic algorithm and 𝑒/(𝑒 − 1) ≈ 1.58-competitive randomized algorithm for ski rental carry over to 2-state DPM, and these competitive ratios are tight. In fact, it was shown by Irani et al. [28] and Lotker et al. [36] that the same competitive ratios carry over even to multi-state DPM. Ski rental, also known as rent-or-buy problem, is a fundamental problem appearing in many domains not restricted to computer hardware questions. For the AI community, this problem for example implicitly appears in expert learning with switching costs: paying the price to switch to a better expert allows to save expenses in the future.
Beyond these results for the classical online setting, [28] also gave a deterministic 𝑒/(𝑒 − 1)-competitive algorithm for the case in which the length of the idle periods is repeatedly drawn from a
ﬁxed, and known, probability distribution. When the probability distribution is ﬁxed but unknown they developed an algorithm that learns the distribution over time and showed that it performs well in practice. Although it is perhaps not always reasonable to assume a ﬁxed underlying probability distribution for the length of idle periods, real-life systems do often follow periodical patterns so that these lengths can indeed be frequently predicted with adequate accuracy, see Chung et al. [18] for a speciﬁc example. Nevertheless, it is not hard to see that blindly following such predictions can lead to arbitrarily bad performance when predictions are faulty. The ﬁeld of learning-augmented algorithms [38] is concerned with algorithms that incorporate predictions in a robust way.
In this work, we introduce multi-state DPM to the learning-augmented setting. Extending ideas of [28] and [36], we give a reduction from multi-state DPM to ski rental that is applicable to the learning-augmented setting. Although ski rental has been investigated through the learning-augmented algorithms lens before [40; 44], earlier work has focused on the optimal trade-off between consistency (i.e., the performance when predictions are accurate) and robustness (i.e., the worst-case performance). To apply our reduction from DPM to ski rental, we require more reﬁned guarantees for learning-augmented ski rental. To this end we develop a new learning-augmented algorithm for ski rental that obtains the optimal trade-off between consistency and dependence on the prediction error. Our resulting algorithm for DPM achieves a competitive ratio arbitrarily close to 1 in case of perfect predictions and its performance degrades gracefully to a competitive ratio arbitrarily close to the optimal robustness of 𝑒/(𝑒 − 1) ≈ 1.58 as the prediction error increases.
Potential negative societal impact. This is a work of theoretical nature and we are not aware of potential negative societal impact. That said, we cannot rule out future misuse of the contained theoretical knowledge. 1.1 Formal deﬁnitions
In the problem of dynamic power management (DPM), we are given 𝑘 + 1
Problem deﬁnition. power states denoted by 0, 1, . . . , 𝑘, with power consumptions 𝛼0 > · · · > 𝛼𝑘 ≥ 0 and wake-up costs
𝛽0 < · · · < 𝛽𝑘 . For state 0, we have 𝛽0 = 0 and we call this the active state. The input is a series of idle periods of lengths ℓ1, . . . , ℓ𝑇 received online, i.e., the algorithm does not know the length of the current period before it ends. During each period, the algorithm can transition to states with lower and lower power consumption, paying energy cost 𝑥𝛼𝑖 for residing in state 𝑖 for time 𝑥. If 𝑗 is the state at the end of the idle period, then it has to pay the wake-up cost 𝛽 𝑗 to transition back to the active state 0. The goal is to minimize the total cost.
In the learning-augmented setting, the algorithm receives at the beginning of the 𝑖th idle period a prediction 𝜏𝑖 ≥ 0 for the value of ℓ𝑖 as additional input. We deﬁne 𝜂𝑖 := 𝛼0|𝜏𝑖 − ℓ𝑖 | to be the error of the 𝑖th prediction, and 𝜂 := (cid:205)𝑇
𝑖 𝜂𝑖 to be the total prediction error. 2
(Continuous-time) ski rental is the special case of DPM with 𝑘 = 1, 𝛼1 = 0 and a single idle period of some length ℓ. In this case, we call 𝛼 := 𝛼0 the rental cost, 𝛽 := 𝛽1 the buying cost, and ℓ the length of the ski season. In learning-augmented ski rental, we write the single prediction as 𝜏 := 𝜏1. (𝜌, 𝜇) (𝜌, 𝜇) (𝜌, 𝜇)-competitiveness. Classical online algorithms are typically analyzed in terms of competitive ratio. A (randomized) algorithm A for an online minimization problem is said to be 𝜌-competitive (or alternatively, obtain a competitive ratio of 𝜌) if for any input instance, cost(A) ≤ 𝜌 · OPT + 𝑐, (1) where cost(A) and OPT denote the (expected) cost of A and the optimal cost of the instance and 𝑐 is a constant independent of the online part of the input (i.e., the lengths ℓ𝑖 in case of DPM). For the ski rental problem one requires 𝑐 = 0, since the trivial algorithm that buys at time 0 has constant cost 𝛽.
In the learning-augmented setting, for 𝜌 ≥ 1 and 𝜇 ≥ 0, we say that A is (𝜌, 𝜇)-competitive if cost(A) ≤ 𝜌 · OPT + 𝜇 · 𝜂 (2) for any instance, where 𝜂 is the prediction error. This corresponds to a competitive ratio of 𝜌 + 𝜇 𝜂
OPT (with 𝑐 = 0). While this could be unbounded as 𝜂/𝑂𝑃𝑇 → ∞, our DPM algorithm achieves a favorable competitive ratio even in this case (see Theorem 5, where we take the minimum over a range of pairs (𝜌, 𝜇), including 𝜇 = 0).
For a (𝜌, 𝜇)-competitive algorithm, 𝜌 is also called the consistency (i.e., competitive ratio in case of perfect predictions) while 𝜇 describes the dependence on the prediction error. 1.2 Our results
Our ﬁrst result is a (𝜌, 𝜇)-competitive algorithm for ski rental that achieves the optimal 𝜇 correspond-ing to the given 𝜌. For 𝜌 ∈ [1, 𝑒
𝑒−1 ], let (cid:27)
𝜇(𝜌) := max
, 𝜌(1 − 𝑇)𝑒−𝑇 (cid:26) 1 − 𝜌 𝑒−1
𝑒 ln 2 where 𝑇 ∈ [0, 1] is the solution to 𝑇 2𝑒−𝑇 = 1 − 1
𝜌 . Let ˜𝜌 ≈ 1.16 be the value of 𝜌 for which both terms in the maximum yield the same value. The ﬁrst term dominates for 𝜌 > ˜𝜌 and the second term if 𝜌 < ˜𝜌. Note that 𝜇(1) = 1 and 𝜇 (cid:0) 𝑒
𝑒−1
Theorem 1. For any 𝜌 ∈ [1, 𝑒
𝑒−1 ], there is a (𝜌, 𝜇(𝜌))-competitive randomized algorithm for learning-augmented ski rental, i.e., given a prediction with error 𝜂, its expected cost is at most
𝜌 OPT +𝜇(𝜌) · 𝜂. (cid:1) = 0. See Figure 1 (left) for an illustration. (3)
,
𝑒−1 already achieves the best possible value of 𝜇 = 0.
Note that 𝜌 < 1 is impossible for any algorithm (due to the case 𝜂 = 0) and 𝜌 > 𝑒 since 𝜌 = 𝑒
We also prove a lower bound showing that 𝜇(𝜌) deﬁned in (3) is the best possible.
Theorem 2. For any 𝜌 ∈ [1, 𝑒 with some prediction error 𝜂 such that the expected cost of A is at least 𝜌 OPT +𝜇(𝜌)𝜂.
𝑒−1 ] and any (randomized) algorithm A, there is a ski rental instance
𝑒−1 is uninteresting
However, for most values of the prediction 𝜏 it is possible to achieve a better 𝜇 < 𝜇(𝜌), and 𝜇(𝜌) only captures the worst case over all possible predictions 𝜏. The proof of Theorem 1 is sketched in Section 2. The complete proofs of Theorems 1 and 2 are provided in the full version [8] in the supplementary material.
In Section 3, we give a reduction from DPM to ski rental in the learning-augmented setting, provided that the ski rental algorithm satisﬁes a natural monotonicity property (deﬁned formally in Section 3):
Lemma 3. If there is a monotone (𝜌, 𝜇)-competitive ski rental algorithm, then there is a (𝜌, 𝜇)-competitive algorithm for DPM.
Since our ski rental algorithm is monotone, this directly yields a (𝜌, 𝜇(𝜌))-competitive algorithm for
DPM. From the special case (𝜌, 𝜇) = (cid:0) 𝑒
, 0(cid:1), this theorem directly implies the following result for
𝑒−1 classical DPM (without predictions), which was ﬁrst proved by Lotker et al. [36] for the equivalent multi-slope ski rental problem: 3
Figure 1: Illustration of 𝜇(𝜌) and of the resulting competitive ratio in function of 𝜂/OPT.
Corollary 4 ([36]). There is a 𝑒 predictions).
𝑒−1 -competitive randomized online algorithm for DPM (without
Using techniques from online learning, in a way similar to [5], we show in Section 4 how to achieve
“almost” (𝜌, 𝜇(𝜌))-competitiveness simultaneously for all 𝜌:
Theorem 5. For any 𝜖 > 0, there is a learning-augmented algorithm A for dynamic power manage-ment whose expected cost can be bounded as cost(A) ≤ (1 + 𝜖) min (cid:8)𝜌 OPT +𝜇(𝜌) · 𝜂 (cid:12)
𝑒−1 ](cid:9) + 𝑂 (cid:0) 𝛽𝑘
The above theorem gives a competitive ratio arbitrarily close to min{𝜌 + 𝜇(𝜌) ·
}, which is equal to 1 if 𝜂 = 0 and never greater than 𝑒
𝑒−1 . In particular, we achieve a performance that degrades gracefully from near-optimal consistency to near-optimal robustness as the error increases.1 See
Figure 1 (right) for an illustration.
𝜖 log 1
𝜖
𝜂
OPT (cid:12) 𝜌 ∈ [1, 𝑒 (cid:1).
In Section 5, we illustrate the performance of these algorithms by simulations on synthetic datasets, where the dependence on the prediction error can be observed as expected from theoretical results. 1.3