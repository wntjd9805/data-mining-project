Abstract
Recent works have suggested that ﬁnite Bayesian neural networks may sometimes outperform their inﬁnite cousins because ﬁnite networks can ﬂexibly adapt their internal representations. However, our theoretical understanding of how the learned hidden layer representations of ﬁnite networks differ from the ﬁxed representations of inﬁnite networks remains incomplete. Perturbative ﬁnite-width corrections to the network prior and posterior have been studied, but the asymptotics of learned features have not been fully characterized. Here, we argue that the leading ﬁnite-width corrections to the average feature kernels for any Bayesian network with linear readout and Gaussian likelihood have a largely universal form. We illustrate this explicitly for three tractable network architectures: deep linear fully-connected and convolutional networks, and networks with a single nonlinear hidden layer.
Our results begin to elucidate how task-relevant learning signals shape the hidden layer representations of wide Bayesian neural networks. 1

Introduction
The expressive power of deep neural networks critically depends on their ability to learn to represent the features of data [1–24]. However, the structure of their hidden layer representations is only theoretically well-understood in certain inﬁnite-width limits, in which these representations cannot
ﬂexibly adapt to learn data-dependent features [3–11, 24]. In the Bayesian setting, these represen-tations are described by ﬁxed, deterministic kernels [3–11]. As a result of this inﬂexibility, recent works have suggested that ﬁnite Bayesian neural networks (henceforth BNNs) may generalize better than their inﬁnite counterparts because of their ability to learn representations [10].
Theoretical exploration of how ﬁnite and inﬁnite BNNs differ has largely focused on the properties of the prior and posterior distributions over network outputs [12–17]. In particular, several works have studied the leading perturbative ﬁnite-width corrections to these distributions [12–16]. Yet, the corresponding asymptotic corrections to the feature kernels, which measure how representations evolve from layer to layer, have only been studied in a few special cases [16]. Therefore, the structure of these corrections, as well as their dependence on network architecture, remain poorly understood.
In this paper, we make the following contributions towards the goal of a complete understanding of feature learning at asymptotically large but ﬁnite widths:
• We argue that the leading ﬁnite-width corrections to the posterior statistics of the hidden layer kernels of any BNN with a linear readout layer and Gaussian likelihood have a largely 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
prescribed form (Conjecture 1). In particular, we argue that the posterior cumulants of the kernels have well-deﬁned asymptotic series in terms of their prior cumulants, with coefﬁcients that have ﬁxed dependence on the target outputs.
• We explicitly compute the leading ﬁnite-width corrections for deep linear fully-connected networks (§4.1), deep linear convolutional networks (§4.2), and networks with a single nonlinear hidden layer (§4.3). We show that our theory yields quantitatively accurate predictions for the result of numerical experiment for tractable linear network architectures, and qualitatively accurate predictions for deep nonlinear networks, where quantitative analytical predictions are intractable.
Our results begin to elucidate the structure of learned representations in wide BNNs. The assumptions of our general argument are satisﬁed in many regression settings, hence our qualitative conclusions should be broadly applicable. 2 Preliminaries
We begin by deﬁning our notation, setup, and assumptions. We will index training and test examples by Greek subscripts µ, ν, . . ., and layer dimensions (that is, neurons) by Latin subscripts j, l, . . ..
Layers will be indexed by the script Latin letter (cid:96). Matrix- or vector-valued quantities corresponding to a given layer will be indexed with a parenthesized superscript, while scalar quantities that depend only on the layer will be indexed with a subscript. Depending on context, (cid:107) · (cid:107) will denote the (cid:96)2 norm on vectors or the Frobenius norm on matrices. We denote the standard Euclidean inner product of two vectors a, b ∈ Rn by a · b. 2.1 Bayesian neural networks with linear readout
Throughout this paper, we consider deep Bayesian neural networks with fully connected linear readout. Such a network f : Rn0 → Rnd with d layers can be written as f (x; W d, W) =
√ 1 nd−1
W (d)ψ(x; W), (1) where the feature map ψ(·; W) : Rn0 → Rnd−1 includes all d − 1 hidden layers, collectively parameterized by W. Here, ψ can be some combination of fully-connected feedforward networks, convolutional networks, recurrent networks, et cetera; we assume only that it has a well-deﬁned inﬁnite-width limit in the sense of §2.2. We let the widths of the hidden layers be n1, n2, . . . , nd−1; we deﬁne the width of a convolutional layer to be its channel count [7]. We assume isotropic Gaussian priors over the trainable parameters [1–23], with W (d) ij ∼i.i.d N (0, σ2
In our analysis, we ﬁx an arbitrary training dataset D = {(xµ, yµ)}p
µ=1 of p examples. We deﬁne the input and output Gram matrices of this dataset as [Gxx]µν ≡ n−1 d yµ · yν, respectively. For analytical tractability, we consider a Gaussian likelihood p(D | Θ) ∝ exp(−βE) for 0 xµ · xν and [Gyy]µν ≡ n−1 d) in particular.
E(Θ; D) = 1 2 p (cid:88)
µ=1 (cid:107)f (xµ; Θ) − yµ(cid:107)2, (2) where β ≥ 0 is an inverse temperature parameter that sets the variance of the likelihood and
Θ = {W (d), W} [23]. We then introduce the Bayes posterior over parameters given these data: p(Θ | D) = p(D | Θ)p(Θ) p(D)
; (3) we denote averages with respect to this distribution by (cid:104)·(cid:105). By tuning β, one can then adjust whether the posterior is dominated by the prior (β (cid:28) 1) or the likelihood (β (cid:29) 1). We will mostly focus on the case in which the input dimension is large and the training dataset can be linearly interpolated; the low-temperature limit β → ∞ then enforces the interpolation constraint. 2.2 The Gaussian process limit
We consider the limit of large hidden layer widths n1, n2, . . . , nd−1 → ∞ with n0, nd, p, and d ﬁxed.
More precisely, we consider a limit in which n(cid:96) = α(cid:96)n for (cid:96) = 1, . . . , d − 1, where α(cid:96) ∈ (0, ∞) and 2
n → ∞, as studied by [3–15, 17–19, 24] and others. Importantly, we note that size of n0 relative to n is unimportant for our results, whereas nd/n and d/n must be small [10, 12, 17].
In this limit, for ψ built out of compositions of most standard neural network architectures, the prior over function values f tends to a Gaussian process (GP) [3–8]. Moreover, with our choice of a
Gaussian likelihood, the posterior over function values also tends weakly to the posterior induced by the limiting GP prior [25]. The kernel of the limiting GP prior is given by the deterministic limit
K (d−1)
∞ of the inner product kernel of the postactivations of the ﬁnal hidden layer,
K (d−1)(x, x(cid:48)) ≡ n−1 d−1ψ(x, W) · ψ(x(cid:48), W), (4) multiplied by the prior variance σ2 can be computed recursively [5–8]. For brevity, we deﬁne the kernel matrix evaluated on the training data:
[K (d−1)]µν ≡ K (d−1)(xµ, xν). d [3–8]. For a broad range of network architectures, K (d−1)
∞ 3 Elementary perturbation theory for ﬁnite Bayesian neural networks
We ﬁrst present our main result, which shows that the form of the leading perturbative correction to the average hidden layer kernels of a BNN is tightly constrained by the assumptions that the readout is linear, that the cost is quadratic, and that the GP limit is well-deﬁned. 3.1 Finite-width corrections to the posterior cumulants of hidden layer observables
Our main result is as follows:
Conjecture 1 Consider a BNN of the form (1), with posterior (3). Assume that this network admits a well-deﬁned GP limit as discussed in §2.2. Let O be a hidden layer observable, that is, a function of the hidden layer activations that is not a function of the readout weights Wd. Assume that O tends in probability to a ﬁnite, deterministic limit O∞ under the posterior in the GP limit.
Then, the posterior cumulants of this observable admit well-behaved asymptotic series at large widths in terms of its joint prior cumulants with the postactiviation kernel K (d−1). In particular, the asymptotic expansion of the posterior mean (cid:104)O(cid:105) has leading terms (cid:104)O(cid:105) = EW O + 1 2 nd p (cid:88)
[σ−2 d Γ−1GyyΓ−1 − Γ−1]ρλ covW (O, K (d−1)
ρλ
) + . . . , (5)
ρ,λ=1 where Γ ≡ K (d−1) d Ip. Here, the cumulants of the kernels are computed with respect to the prior, and are themselves given by asymptotic series at large widths. The ellipsis denotes terms that are of subleading order in the inverse hidden layer widths.
+ β−1σ−2
∞
In Appendix B, we derive this result perturbatively by expanding the posterior cumulant generating function of O in powers of the deviations of O and K (d−1) from their deterministic inﬁnite-width values. There, we also give an asymptotic formula for the posterior covariance of two observables.
However, the resulting perturbation series may not rigorously be an asymptotic series, and this method does not yield quantitative bounds for the width-dependence of the terms. We therefore frame it as a conjecture. We note that similar methods can be applied to compute asymptotic corrections to the posterior predictive statistics; we comment on this possibility in Appendix G.
Though this conjecture applies to a broad class of hidden layer observables, the observables of greatest interest are the preactivation or postactivation kernels of the hidden layers within the feature map ψ.
We will focus on the postactivation kernels K ((cid:96)), which measure how the similarities between inputs evolve as they are propagated through the network [5–10].
Conjecture 1 posits that there are two possible types of leading ﬁnite-width corrections to the average kernels. The ﬁrst class of corrections are deviations of EW K ((cid:96)) from K ((cid:96))
∞ . These terms reﬂect corrections to the prior, and do not reﬂect non-trivial representation learning as they are independent of the outputs. For fully-connected networks, also known as multilayer perceptrons (MLPs), work by Yaida [12] and by Gur-Ari and colleagues [18, 19] shows that EW K ((cid:96)) = K ((cid:96))
∞ + O(n−1). The 3
second type of correction is the output-dependent term that depends on covW (K ((cid:96))
µν , K (d−1)
ρλ deep linear MLPs or MLPs with a single hidden layer, EW K ((cid:96)) is exactly equal to K ((cid:96))
∞ at any width (see Appendix C) [3, 12, 18], and only the covariance term contributes. More broadly, these prior works show that covW (K ((cid:96))
) = O(n−1) for MLPs, and that higher cumulants are of
O(n−2) [12, 18, 19]. Some of these results have recently been extended to convolutional networks by
Andreassen and Dyer [26]. Thus, the ﬁnite-width correction to the prior mean should not dominate the feature-learning covariance term, and the terms hidden in the ellipsis should indeed be suppressed.
µν , K (d−1)
ρλ
). For
The leading output-dependent correction has several interesting features. First, it includes a factor of nd, reﬂecting the fact that inference in wide Bayesian networks with many outputs is qualitatively different from that in networks with few outputs relative to their hidden layer width [10]. If nd/n does not tend to zero with increasing n, the inﬁnite-width behavior is not described by a standard GP
[8, 10]. Moreover, we note that the matrix Γ is invertible at any ﬁnite temperature, even when K (d−1) is singular. Therefore, provided that one can extend the GP kernel by continuity to non-invertible
Gxx, Conjecture 1 can be applied in the data-dense regime n0 < p as well as the data-sparse regime n0 > p. Furthermore, we observe that the correction depends on the outputs only through their
Gram matrix Gyy. This result is intuitively sensible, since with our choice of likelihood and prior the function-space posterior is invariant under simultaneous rotation of the output activations and targets.
Finally, Gyy is transformed by factors of the matrix Γ−1, hence the correction depends on certain interactions between the output similarities and the GP kernel K (d−1)
∞ .
∞ 3.2 High- and low-temperature limits of the leading correction
To gain some intuition for the properties of the leading ﬁnite-width corrections, we consider their high- and low-temperature limits. These limits correspond to tuning the posterior (3) to be dominated by the prior or the likelihood, respectively. At high temperatures (β (cid:28) 1), expanding Γ−1 as a
Neumann series (see Appendix A and [27]) yields
σ−2 d Γ−1GyyΓ−1 − Γ−1 = −βσ2 dIp + (βσ2 d)2(σ−2 d Gyy + K (d−1)
∞ ) + O[(βσ2 d)3]. (6)
Thus, at high temperatures, the outputs only inﬂuence the average kernels of Conjecture 1 to subleading order in both width and β, which reﬂects the fact that the likelihood is discounted relative to the prior in this regime. Moreover, the leading output-dependent contribution averages together
Gyy and K (d−1)
∞ , hence, intuitively, there is no way to ‘cancel’ the GP contributions to the average kernels. We note that, at inﬁnite temperature (β = 0), the posterior reduces to the prior, and all
ﬁnite-width corrections to the average kernels arise from the discrepancy between EW K ((cid:96)) and K ((cid:96))
∞ .
At low temperatures (β (cid:29) 1), the behavior of Γ−1 differs depending on whether or not K (d−1) full rank. Assuming for simplicity that it is invertible, we have is of
∞
σ−2 d Γ−1GyyΓ−1 − Γ−1 = [K (d−1)
∞ ]−1(σ−2 d Gyy − K (d−1)
∞ )[K (d−1)
∞ ]−1 + O[(βσ2 d)−1]; (7) in the non-invertible case there are additional contributions involving projectors onto the null space of
K (d−1)
∞ . Therefore, the leading-order low temperature correction depends on the difference between the target and GP kernels, while the leading non-trivial high temperature correction depends on their sum. 4 Learned representations in tractable network architectures
Having derived the general form of the leading perturbative ﬁnite-width correction to the average feature kernels, we now consider several example network architectures. For these tractable examples, we provide explicit formulas for the feature-learning corrections to the hidden layer kernels, and test the accuracy of our theory with numerical experiments. 4.1 Deep linear fully-connected networks
We ﬁrst consider deep linear fully-connected networks with no bias terms. Concretely, we consider a network with activations h((cid:96)) ∈ Rn(cid:96) recursively deﬁned via h((cid:96)) = n−1/2 (cid:96)−1 W ((cid:96))h((cid:96)−1) with base case 4
h(0) = x, where the prior distribution of weights is [W ((cid:96))]ij ∼i.i.d. N (0, σ2 (cid:96) ). For such a network, the hidden layer kernels [K ((cid:96))]µν ≡ n−1 (cid:96) Gxx, where m2 1 is the product of prior variances up to layer (cid:96). Higher prior cumulants of the kernels are easy to compute with the aid of Isserlis’ theorem for Gaussian moments (see Appendix C)
[28, 29], yielding
ν have deterministic limits K ((cid:96)) (cid:96)−1 · · · σ2
µ · h((cid:96))
∞ = m2 (cid:96) h((cid:96)) (cid:96) ≡ σ2 (cid:96) σ2 (cid:104)K ((cid:96))(cid:105) m2 (cid:96)
= Gxx + (cid:18) (cid:96) (cid:88) (cid:96)(cid:48)=1 (cid:19) nd n(cid:96)(cid:48)
GxxΓ−1 (cid:0)m−2 d Gyy − Γ(cid:1) Γ−1Gxx + O(n−2), (8) where Γ ≡ Gxx + Ip/(βm2 d) and (cid:96) = 1, . . . , d − 1. In Appendix D, we show that this result can be derived directly through an ab initio perturbative calculation of the cumulant generating function of the kernels, without relying on our heuristic argument for the general version of Conjecture 1.
Moreover, in Appendix E, we show that the form of the correction remains the same even if one allows arbitrary forward skip connections, though the dependence on width and depth is given by a more complex recurrence relation.
Thus, the leading corrections to the normalized average kernels (cid:104)K ((cid:96))(cid:105)/m2 (cid:96) are identical across all hidden layers up to a scalar factor that encodes the width-dependence of the correction. This sum-of-inverse-widths dependence was previously noted by Yaida [12] in his study of the corrections to the prior of a deep linear network. For a network with hidden layers of equal width n, we have the simple linear dependence (cid:80)(cid:96) (cid:96)(cid:48)=1(nd/n(cid:96)(cid:48)) = nd(cid:96)/n. If one instead includes a narrow bottleneck in an otherwise wide network, this dependence predicts that the kernels before the bottleneck should be close to their GP values, while those after the bottleneck should deviate strongly.
This result simpliﬁes further at low temperatures, where, by the result of §3.2, we have (cid:104)K ((cid:96))(cid:105) m2 (cid:96)
= Gxx + (cid:18) (cid:96) (cid:88) (cid:96)(cid:48)=1 (cid:19) nd n(cid:96)(cid:48) (cid:0)m−2 d Gyy − Gxx (cid:1) + O(n−2, β−1) (9) in the regime in which Gxx is invertible. We thus obtain the simple qualitative picture that the low-temperature average kernels linearly interpolate between the input and output Gram matrices. In
Appendix F, we show that this limiting result can be recovered from the recurrence relation derived through other methods by Aitchison [10], who did not use it to compute ﬁnite-width corrections. We note that the low-temperature limit is peculiar in that the mean predictor reduces to the least-norm pseudoinverse solution to the underlying underdetermined linear system XW = Y ; we comment on this property in Appendix G.
We can gain some additional understanding of the structure of the correction by using the eigende-composition of Gxx. As Gxx is by deﬁnition a real positive semideﬁnite matrix, it admits a unitary eigendecomposition Gxx = U ΛU † with non-negative eigenvalues Λµµ. In this basis, the average kernel is 1 m2 (cid:96)
U †(cid:104)K ((cid:96))(cid:105)U = Λ + (cid:19) (cid:16) (cid:18) (cid:96) (cid:88) (cid:96)(cid:48)=1 nd n(cid:96)(cid:48) m−2 d
˜ΛU †GyyU ˜Λ − ˜ΛΛ (cid:17)
+ O(n−2), (10) where we have deﬁned the diagonal matrix ˜Λ ≡ βm2 dΛ ≥ 0, the diagonal elements of ˜Λ are bounded as 0 ≤ ˜Λµµ ≤ 1. Thus, the factors of Γ−1Gxx by which Gyy is conjugated have the effect of suppressing directions in the projection of Gyy onto the eigenspace of Gxx with small eigenvalues. We can see that this effect will be enhanced at high temperatures (β (cid:28) 1) and small scalings (m2 d (cid:28) 1), and suppressed at low temperatures and large scalings. For this linear network, similarities are not enhanced, only suppressed. Moreover, if Gxx is diagonal, then a given element of the average kernel will depend only on the corresponding element of Gyy. dΛ)−1. As βm2 dΛ(Ip + βm2
We now seek to numerically probe how accurately these asymptotic corrections predict learned representations in deep fully-connected linear BNNs. Using Langevin sampling [30, 31], we trained deep linear networks of varying widths, and compared the difference between the empirical and
GP kernels with theory predictions. We provide a detailed discussion of our numerical methods in
Appendix I. In Figure 1, we present an experiment with a 2-layer linear neural network trained on the
MNIST dataset of handwritten digit images [32] using the Neural Tangents library [33]. We ﬁnd an excellent agreement with our theory, conﬁrming the inverse scaling with width and linear scaling with depth for the deviations from GP kernel. 5
Figure 1: Learned representations in two-hidden-layer linear fully-connected neural networks with varying widths trained via Langevin sampling on 5000 MNIST images (see Appendix I for more details). (a) The Frobenius norm of the deviation of the empirical average kernel of each layer from its GP value (in this case, simply Gxx) for varying widths. We see perfect match with theoretical predictions, which are shown as dashed lines. We obtain the predicted 1/n decay with increasing width and the linear scaling with the depth where the deviations for ﬁrst and second layers differ by a factor of 2. (b-c) Scatter plot of individual elements of the experimental (ordinate) and theoretical (abscissa) kernels for both layers. For low widths a slight deviation is visible between experiment and theory, while for larger widths the agreement is better. 4.2 Deep linear convolutional networks
To demonstrate the applicability of Conjecture 1 to non-fully-connected BNNs, we consider deep convolutional linear networks with no bias terms. Here, the appropriate notion of width is the number of channels in each hidden layer [7]. Following the setup of Novak et al. [7] and Xiao et al. [34], we consider a network consisting of d − 1 linear convolutional layers followed by a fully-connected linear readout layer. For simplicity, we restrict our attention to convolutions with periodic boundary conditions, and do not include internal pooling layers (see Appendix C for more details). Concretely, we consider a network with hidden layer activations h((cid:96)) i,a, where i indexes the n(cid:96) channels of the layer and a is a spatial multi-index. The hidden layer activations are then deﬁned through the recurrence n(cid:96)−1 (cid:88) (cid:88) h((cid:96)) i,a(x) =
√ 1 n(cid:96)−1 j=1 b ij,bh((cid:96)−1) w((cid:96)) j,a+b(x) (11) with base case h(0) i,a (x) = xi,a, where i indexes the input channels (e.g., image color channels). The feature map is then formed by ﬂattening the output of the last hidden layer into an nd−1s-dimensional vector, where s is the total dimensionality of the inputs (see Appendix C for details). We ﬁx the prior distribution of the ﬁlter elements to be w((cid:96)) (cid:96) va), where va > 0 is a weighting factor that sets the fraction of receptive ﬁeld variance at location a (and is thus subject to the constraint (cid:80) a va = 1). For inputs [xµ]i,a and [xν]i,a, we introduce the four-index hidden layer kernels n(cid:96)(cid:88) ij,a ∼ i.i.d.
N (0, σ2
K ((cid:96))
µν,ab ≡ i,a(xµ)h((cid:96)) h((cid:96)) i,b(xν). (12) 1 n(cid:96) i=1
With the given readout strategy, the two-index feature map kernel appearing in Conjecture 1 is related to the four-index kernel of the last hidden layer by K (d−1)
µν,aa . We discuss other readout strategies in Appendix C, but use this vectorization strategy in our numerical experiments. a K (d−1)
= 1 s (cid:80)
µν
As shown by Xiao et al. [34], the inﬁnite-width four-index kernel obeys the recurrence
[K ((cid:96))
∞ ]µν,ab = σ2 (cid:96) (cid:88) c vc[K ((cid:96)−1)
∞ ]µν,(a+c)(b+c) (13) with base case [K 0 i=1[xµ]i,a[xν]i,b. This gives convolutional linear networks a sense of spatial hierarchy that is not present in the fully-connected case: even at inﬁnite width, the kernels include iterative spatial averaging.
∞]µν,ab = [Gxx]µν,ab ≡ 1 n0 (cid:80)n0
In Appendix C, we derive the kernel covariances appearing in Conjecture 1. As in the fully-connected case, this computation is easy to perform with the aid of Isserlis’ theorem. The general result is 6
Figure 2: The MNIST image dataset and experiments for neural networks with two 1D convolutional layers. (a) A 10 × 10 MNIST image downsized from 28 × 28 pixels. (b) Input Gram matrix for 300 MNIST images. (c) A single (µ, ν) component of the input tensor [Gxx]µν,ab obtained using Eq. (13). (d) The output Gram matrix. (e) The Frobenius norm of the correction to the 1D convolutional
GP kernel is inversely proportional to the width. Here, the dashed lines are the theoretical predictions. (f) Scatter plots of individual elements of the empirical corrections to the GP kernels against the theoretical predictions for both layers show excellent agreement. somewhat complicated, but things simplify under the assumption that readout is performed using vectorization. Then, one ﬁnds that (cid:104)K ((cid:96))
µν,ab(cid:105) = [K ((cid:96))
∞ ]µν,ab + (cid:32)d−1 (cid:89)
σ2 (cid:96) (cid:33) (cid:32) (cid:96) (cid:88) (cid:96)(cid:48)=(cid:96) (cid:96)(cid:48)=1 (cid:33) 1 s nd n(cid:96)(cid:48) s (cid:88) p (cid:88) c=1
ρ,λ=1
[K ((cid:96))
∞ ]µρ,acΦρλ[K ((cid:96))
∞ ]λν,cb + O(n−2), (14) where we have deﬁned Φρλ ≡ [σ−2 d Γ−1GyyΓ−1 − Γ−1]ρλ for brevity. Thus, the correction to the convolutional kernel is quite similar to that obtained in the fully-connected case. To this order, the difference between these network architectures manifests itself largely through the difference in the inﬁnite-width kernels. In Appendix C, we show that a similar simpliﬁcation holds if readout is performed using global average pooling over space.
As we did for fully-connected networks, we test whether our theory accurately predicts the results of numerical experiment, using the MNIST digit images illustrated in 2(a-d). We consider a network with one-dimensional (Figure 2e and f) and two-dimensional (Figure 3) convolutional hidden layers, trained to classify 50 MNIST images (see Appendix I for details of our numerical methods). As shown in Figure 2(e, f) (Figure 3(a,b) for 2D convolutions), we again obtain good quantitative agreement between the predictions of our asymptotic theory and the results of numerical experiment.
In Figure 3c, we directly visualize the learned feature kernels for 2D convolutional layers, illustrating the good agreement between theory and experiment. Therefore, our asymptotic theory can be applied to accurately predict learned representations in deep convolutional linear networks. 4.3 Networks with a single nonlinear hidden layer
Finally, we would like to gain some understanding of how including nonlinearity affects the structure of learned representations. However, for a nonlinear MLP, it is usually not possible to analytically compute covW (K ((cid:96))
) to the required order [9, 12, 18, 19]. Here, we consider the case of a network with a single nonlinear layer and no bias terms, in which we can both summarize the key obstacles to studying deep nonlinear networks and gain some intuitions about how they might differ from linear BNNs. Concretely, we consider a network with feature map ψ(x; W (1)) =
µν , K (d−1)
ρλ 7
Figure 3: Learned representations in two-hidden-layer linear 2-D convolutional networks of varying channel widths. (a) The Frobenius norm of the correction to the GP kernel is inversely proportional to the width. Here, the dashed lines represent theory predictions. (b) Scatter plots of individual elements of the empirical corrections to the GP kernels against the theoretical predictions for both layers show good agreement. (c) A single component (µ, ν) of the learned feature kernels in 2-layer
CNN experiments for both convolutional layers. While the experimental kernel looks quite similar the GP (ﬁrst and second columns), their difference shows the ﬁnite width corrections to the GP (last column).
φ(n−1/2 0 W (1)x) for an elementwise activation function φ, where the weight matrix W (1) has prior distribution [W (1)]ij ∼i.i.d. N (0, σ2 1). The only hidden layer kernel of this network is the feature map postactivation kernel Kµν deﬁned in (4), where we drop the layer index for brevity. As detailed in
Appendix H, for such a network we have the exact expressions
[K∞]µν = EW Kµν = E[φ(hµ)φ(hν)], (15) n1 covW (Kµν, Kρλ) = E[φ(hµ)φ(hν)φ(hρ)φ(hλ)] − [K∞]µν[K∞]ρλ, (16) where expectations are taken over the p-dimensional Gaussian random vector hµ, which has mean zero and covariance cov(hµ, hν) = σ2 1[Gxx]µν. Unlike for deeper nonlinear networks, here there are no ﬁnite-width corrections to the prior expectations [3, 12, 18].
Though these expressions are easy to deﬁne, it is not possible to evaluate the four-point expectation in closed form for general Gram matrices Gxx and activation functions φ, including ReLU and erf. This obstacle has been noted in previous studies [9, 12, 15], and makes it challenging to extend approaches similar to those used here to deeper nonlinear networks. For polynomial activation functions, the required expectations can be evaluated using Isserlis’ theorem (see Appendix A). However, even for a quadratic activation function φ(x) = x2, the resulting formula for the kernel will involve many elementwise matrix products, and cannot be simpliﬁed into an intuitively comprehensible form.
If the input Gram matrix Gxx is diagonal, the four-point expectation becomes tractable because the required expectations factor across sample indices. In this simple case, there is an interesting distinction between the behavior of activation functions that yield Eφ(h) = 0 and those that yield
Eφ(h) (cid:54)= 0. As detailed in §4.1, if Eφ(h) = 0, K∞ is diagonal, and a given element of the leading
ﬁnite-width correction to (cid:104)K(cid:105) depends only on the corresponding element of Gyy. However, if
Eφ(h) (cid:54)= 0, then K∞ includes a rank-1 component, and each element of the correction depends on all elements of Gyy. This means that the case in which Gxx is diagonal is qualitatively distinct from the case in which there is only a single training input for such activation functions. 5 Learned representations in deep nonlinear networks
In the preceding section, we noted that analytical study of learned representations in deep nonlinear
BNNs is generally quite challenging. Here, we use numerical experiments to explore whether any 8
Figure 4: 3-hidden layer neural network with ReLU activations trained via Langevin sampling on 1000 MNIST images (see Appendix I). (a) The empirical average kernels subtracted from their corresponding GP kernels for all layers with varying widths. Labels on the y-axes indicate the widths of each layer. We observe that for networks with bottleneck layers, the deviation from K ((cid:96))
∞ is largest at the bottleneck indicating representation learning; without a bottleneck deviations are considerably less (the last row). (b) Hidden layer kernel deviation from GP kernels as a function of width for bottleneck networks. While the ﬁrst layer shows 1/n scaling, the bottleneck layer and the 3rd layer deviations stay almost constant. This behavior is predicted analytically for linear networks. (c) As in (b) for networks without a bottleneck. Consistent with our theory, all layers display 1/n decay. of the intuitions gained in the linear setting carry over to nonlinear networks. Concretely, we study how narrow bottlenecks affect representation learning in a more realistic nonlinear network. We train a network with three hidden layers and ReLU activations on a subset of the MNIST dataset [32].
Despite its analytical simplicity, ReLU is among the activation functions for which the covariance term in Conjecture 1 cannot be evaluated in closed form (see §4.3). However, it is straightforward to simulate numerically. Consistent with the predictions of our theory for linear networks, we ﬁnd that introducing a narrow bottleneck leads to more representation learning in subsequent hidden layers, even if those layers are quite wide (Figure 4). Quantitatively, if one increases the width of the hidden layers between which the ﬁxed-width bottleneck is sandwiched, the deviation of the ﬁrst layer’s kernel from its GP value decays roughly as 1/n with increasing width, while the deviations for the bottleneck and subsequent layers remain roughly constant. In contrast, the kernel deviations throughout a network with equal-width hidden layers decay roughly as 1/n (Figure 4). These observations are qualitatively consistent with the width-dependence of the linear network kernel (8), as well as with previous studies of networks with inﬁnitely-wide layers separated by a ﬁnite bottleneck [35]. Keeping in mind the obstacles noted in §4.3, precise characterization of nonlinear networks will be an interesting objective for future work. 6