Abstract
Attribute extrapolation in sample generation is challenging for deep neural net-works operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein sta-bility dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training.
We release our benchmark tasks and models to contribute to the study of gen-erative modeling extrapolation and data-driven design in biology and chemistry: https://github.com/salesforce/genhance. 1

Introduction
Deep generative neural networks can generate realistic data across data-types, from sequences to images to time-series data, with applications in domains such as natural language processing (NLP), computer vision, and speech. Beyond canonical domains, the scientiﬁc application of synthetic design of proteins, molecules, and materials can be cast as generative modeling of sequences, graphs, or images (Anand & Huang, 2018; De Cao & Kipf, 2018; Madani et al., 2020, 2021). Most often, the goal is to design or generate a sample that improves upon the attribute label of interest (Fig. 1-(left)), which we term attribute-enhanced generation. Examples include generating a protein sequence with higher binding afﬁnity or a nanomaterial structure with an energetically favorable state, as compared to all of the samples in the training distribution. In these scientiﬁc ﬁelds, traditional methods for synthetic object design with improved attributes are iterative and expensive, relying on labor- or compute-intensive methods (Bepler & Berger, 2021; Wu et al., 2021; Hie & Yang, 2021).
Hence, deep generative models that can design new proteins, molecules, and materials with improved attributes have the potential to dramatically accelerate design research. Beyond scientiﬁc applications, extrapolation in generation has potential applications in NLP, such as reducing toxicity or operating in low-resource settings.
It is, however, a well-known challenge for deep neural networks to generate samples beyond the training distribution (Arora et al., 2017; Radford et al., 2019; Xu et al., 2020). In this work, we develop a method for extrapolation, particularly for sequences. Our approach, called GENhance, is designed to generate an enhanced sequence using a learned latent space. GENhance consists of a generator (sampler) and a discriminator (ranker) that are jointly trained to minimize generation and discrimination losses, regularized by latent vector smoothing and a cycle-consistency loss.
We evaluate GENhance in two data domains. First, we use the Stanford Sentiment Treebank (SST), a natural language benchmark containing movie reviews with ﬁve discrete sentiment attributes (Socher et al., 2013), to show that GENhance generates strongly positive reviews, after training with no
* Equal Contribution
Correspondence to amadani@salesforce.com 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Attribute-enhanced generation. The goal of extrapolation (left) is to generate samples whose attribute values exceed that of all training samples, y⌧ . We explore target attribute extrapolation for protein sequences (center) and movie reviews (right), where more stable protein sequences and more positives text reviews are generated. positive examples. Second, we develop a protein stability dataset for the ACE2 protein (Chan et al., 2020) with a change in free energy (ddG) continuous attribute, and show that GENhance can generate protein sequences with higher stability than the training set (Fig. 1-(right)). GENhance signiﬁcantly outperforms baseline methods based on (i) a generator-discriminator model with rejection sampling and (ii) an algorithm using Metropolis-Hastings Markov chain Monte Carlo sampling with a trained discriminator. GENhance’s performance is further improved when provided access to a few examples with attribute scores beyond the training distribution. Our contributions are summarized below:
• We formalize the task of extrapolation for deep generative models, focused on enhancing attributes in sequence generation, with important scientiﬁc applications in synthetic object design.
• We introduce GENhance, a regularized encoder-decoder framework with a learned latent space, and demonstrate its superior performance with respect to rigorous baseline techniques.
• We curate extrapolation benchmarks in NLP and proteins. We release the data, evaluation metrics, along with oracle models and scripts for automatic evaluation of generation quality. 2