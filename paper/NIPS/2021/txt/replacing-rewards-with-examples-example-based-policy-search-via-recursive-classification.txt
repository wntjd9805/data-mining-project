Abstract
Reinforcement learning (RL) algorithms assume that users specify tasks by man-ually writing down a reward function. However, this process can be laborious and demands considerable technical expertise. Can we devise RL algorithms that instead enable users to specify tasks simply by providing examples of successful outcomes? In this paper, we derive a control algorithm that maximizes the future probability of these successful outcome examples. Prior work has approached similar problems with a two-stage process, ﬁrst learning a reward function and then optimizing this reward function using another RL algorithm. In contrast, our method directly learns a value function from transitions and successful outcomes, without learning this intermediate reward function. Our method therefore requires fewer hyperparameters to tune and lines of code to debug. We show that our method satisﬁes a new data-driven Bellman equation, where examples take the place of the typical reward function term. Experiments show that our approach outperforms prior methods that learn explicit reward functions.1 1

Introduction
In supervised learning settings, tasks are deﬁned by data: what causes a car detector to detect cars is not the choice of loss function (which might be the same as for an airplane detector), but the choice of training data. Deﬁning tasks in terms of data, rather than specialized loss functions, arguably makes it easier to apply machine learning algorithms to new domains. In contrast, reinforcement learning (RL) problems are typically posed in terms of reward functions, which are typically manually designed.
Arguably, the challenge of designing reward functions has limited RL to applications with simple reward functions, and has been restricted to users who speak this language of mathematically-deﬁned reward functions. Can we make task speciﬁcation in RL similarly data-driven?
Whereas the standard MDP formalism centers around predicting and maximizing the future reward, we will instead focus on the problem classifying whether a task will be solved in the future. The user will provide a collection of example success states, not a reward function. We call this problem setting example-based control. In effect, these examples tell the agent “What would the world look like if the task were solved?" For example, for the task of opening a door, success examples correspond to different observations of the world when the door is open. The user can ﬁnd examples of success even for tasks that they themselves do not know how to solve. For example, the user could solve the task using actions unavailable to the agent (e.g., the user may have two arms, but a robotic agent may have only one) or the user could ﬁnd success examples by searching the internet. As we will discuss in Sec. 3.1, this problem setting is different from imitation learning: we maximize a different objective function and only assume access to success examples, not entire expert trajectories. 1Project site with videos and code: https://ben-eysenbach.github.io/rce 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Example-based control: Whereas the standard MDP framework requires a user-deﬁned reward function, example-based control speciﬁes tasks via a handful of user-provided success examples.
Learning from examples is challenging because we must automatically identify when the agent has solved the task and reward it for doing so. Prior methods (either imitation learning from demon-strations or learning from success examples) take an indirect approach that resembles inverse RL:
ﬁrst learn a separate model to represent the reward function, and then optimize this reward function with standard RL algorithms. Our method is different from these prior methods because it learns to predict future success directly from transitions and success examples, without learning a separate reward function. This key difference has important algorithmic, theoretical, and empirical beneﬁts.
Algorithmically, our end-to-end approach removes potential biases in learning a separate reward function, reduces the number of hyperparameters, and simpliﬁes the resulting implementation. Theo-retically, we propose a method for classifying future events using a variant of temporal difference learning that we call recursive classiﬁcation. This method satisﬁes a new Bellman equation, where success examples are used in place of the standard reward function term. We use this result to provide convergence guarantees. Empirically, we demonstrate that our method solves many complex manipulation tasks that prior methods fail to solve.
Our paper also addresses a subtle but important ambiguity in formulating example-based control.
Some states might always solve the task while other states might rarely solve the task. But, without knowing how often the user visited each state, we cannot determine the likelihood that each state solves the task. Thus, an agent can only estimate the probability of success by making an additional assumption about how the success examples were generated. We will discuss two choices of assumptions. The ﬁrst choice of assumption is convenient from an algorithmic perspective, but is sometimes violated in practice. A second choice is a worst-case approach, resluting in a problem setting that we call robust example-based control. Our analysis shows that the robust example-based control objective is equivalent to minimizing the squared Hellinger distance (an f -divergence).
In summary, this paper studies a data-driven framing of control, where reward functions are replaced by examples of successful outcomes. Our main contribution is an algorithm for off-policy example-based control. The key idea of the algorithm is to directly learn to predict whether the task will be solved in the future via recursive classiﬁcation, without using separate reward learning and policy search procedures. Our analysis shows that our method satisﬁes a new Bellman equation where rewards are replaced by data (examples of success). Empirically, our method signiﬁcantly outperforms state-of-the-art imitation learning methods (AIRL [7], DAC [17], and SQIL [28]) and recent methods that learn reward functions (ORIL [41], PURL [38], and VICE [8]). Our method completes complex tasks, such as picking up a hammer to knock a nail into a board, tasks that none of these baselines can solve. Using tasks with image observations, we demonstrate agents learned with our method acquire a notion of success that generalizes to new environments with varying shapes and goal locations. 2