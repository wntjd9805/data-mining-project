Abstract
There have been many recent advances in representation learning; however, unsu-pervised representation learning can still struggle with model identiﬁcation issues related to rotations of the latent space. Variational Auto-Encoders (VAEs) and their extensions such as β-VAEs have been shown to improve local alignment of latent variables with PCA directions, which can help to improve model dis-entanglement under some conditions. Borrowing inspiration from Independent
Component Analysis (ICA) and sparse coding, we propose applying an L1 loss to the VAE’s generative Jacobian during training to encourage local latent variable alignment with independent factors of variation in images of multiple objects or images with multiple parts. We demonstrate our results on a variety of datasets, giving qualitative and quantitative results using information theoretic and modular-ity measures that show our added L1 cost encourages local axis alignment of the latent representation with individual factors of variation. 1

Introduction
Unsupervised representation learning takes a collection of image data from the world and ﬁgures out how to organize and ﬁnd patterns in the data without additional information about how the images were generated. The ideal representation learning algorithm would compress high-dimensional image data into a lower-dimensional latent representation that contains relevant information about the ground-truth factors of variation that generated the image.
Inferring a good latent representation from a dataset is a difﬁcult problem and is generally under-speciﬁed in algorithms. This underspeciﬁcation is called the “model identiﬁcation” problem, and one example is the fact that representation learning algorithms often struggle to precise the correct orientation of a latent space. That is, optimization criteria used to learn a representation function might be equally well satisﬁed by an equivalent representation that is just a rotation of the latent space by an arbitrary amount.
As commonly implemented (using axis-aligned Gaussian posterior distributions), Variational Auto-Encoders (VAEs) [1] and their extension, the β-VAE [2], solve the rotational part of the model identiﬁcation issue by tending to ensure that the generation function’s Jacobian matrix has orthogonal columns (i.e., that the generative Jacobian matrix’s right singular values are aligned with the axes of the latent space) [3, 4]. Intuitively, this is because the VAE’s stochastic reconstruction cost prefers to budget higher precision (lower embedding noise) in the directions along which the generative Jacobian changes most rapidly. Kumar and Poole [4] draw the parallel between this preferred orientation and linear Principal Component Analysis (PCA). However, we note that learning algorithms that resolve rotational identiﬁcation issues through methods related to PCA will still suffer from an identiﬁability issue related to rotations that mix the directions for which the generative Jacobian matrix has equal 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
singular values. Along these directions, the posterior Gaussian would have approximately equal variance and be rotationally symmetric.
We propose adding an L1 cost to the generation function’s Jacobian matrix as a way to resolve that rotational identiﬁability issue. Since L1 cost is not rotation invariant, L1 regularization creates a preferred latent-space orientation among directions whose singular values are equal. This use of the L1 norm to choose an orientation is inspired by similar use in linear models. For example, when using a Laplacian prior in Independent Component Analysis (ICA) [5] and applying it to already-whitened data, ICA rotates the data to minimize the L1 norm. As shown in Olshausen [6], that ICA formulation is equivalent to sparse coding using an L1 cost [7], with the same rotational effect. In Sparse PCA, the L1 norm encourages sparsity in the loadings/mixing matrix (rather than the principal components/sources as in ICA) [8, 9], similarly encouraging preferred orientations. These techniques for preferring certain orientations using the L1 norm are all linear model techniques, and we apply them to non-linear VAEs by regularizing the L1 norm of the generator Jacobian matrix, thereby encouraging a preferred orientation for our non-linear models.
The motivation above suggests that an L1 norm on the generator Jacobian can address the rotational identiﬁability issue, and we think the L1 regularization will encourage useful orientations of the latent space for image data. This belief comes from results on the sparse linear coding of images. In particular, Olshausen and Field [7] showed how sparse linear coding on natural images generates local receptive ﬁelds similar to those discovered in the mammalian visual processing system. These types of localized receptive ﬁelds are shown in Figure 1. Each image in that ﬁgure corresponds to a direction in the latent space, and shows how perturbing the latent value in that direction changes the generated image—white pixels means the image gets brighter there, black pixels means the image gets darker. For the ICA basis, we see that perturbations in different latent directions are associated with localized changes to the image, whereas for the PCA basis, the latent directions tend to affect the entire image. Adding an L1 penalty to our model should encourage sparsity within the generative Jacobian columns of our model, meaning that perturbations of latent values along individual latent directions should modify as few pixels as possible, leading to latent directions that affect the output image in localized regions. L1 generative Jacobian regularization should therefore disentangle representations of different objects in an image. We call the proposed model trained with this additional regularization a Jacobian L1 Regularized Variational Auto-Encoder (JL1-VAE). (a) PCA (b) ICA (c) β-VAE (d) JL1-VAE (ours)
Figure 1: Example columns from generative Jacobian matrices for different modeling techniques on natural image data collected in [7]. ICA used 100 latent dimensions, of which 20 samples are shown.
β-VAE used β = 0.01; JL1-VAE used β = 0.01, γ = 0.01, each on ten latent dimensions.
The use of the generative Jacobian in the JL1-VAE implies a local linear approximation of the generative function, so in order for the JL1-VAE to be useful, the training image data should lie on a manifold [10], and should be sufﬁciently well sampled. Additionally, JL1-VAE contains inductive 2
bias, like every other unsupervised disentanglement algorithm [11]. As mentioned above, JL1-VAE’s regularization of the generative Jacobian encourages small changes in latent values to result in sparse (impacting a small number of pixels) changes to the resulting image, similar to local receptive ﬁelds.
This inductive bias is well suited for disentangling motions of different objects in an image, but would presumably not be useful for whole-image changes, such as rotation of the entire image or brightness changes across the whole image.
We apply our novel JL1-VAE framework to a variety of datasets, giving qualitative and quantitative results showing that our added L1 cost can encourage local alignment of the axes of the latent representation with individual factors of variation. 2