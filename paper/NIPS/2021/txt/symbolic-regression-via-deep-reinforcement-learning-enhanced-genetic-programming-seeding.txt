Abstract
Symbolic regression is the process of identifying mathematical expressions that ﬁt observed output from a black-box process. It is a discrete optimization problem generally believed to be NP-hard. Prior approaches to solving the problem include neural-guided search (e.g. using reinforcement learning) and genetic program-ming. In this work, we introduce a hybrid neural-guided/genetic programming approach to symbolic regression and other combinatorial optimization problems.
We propose a neural-guided component used to seed the starting population of a random restart genetic programming component, gradually learning better starting populations. On a number of common benchmark tasks to recover underlying expressions from a dataset, our method recovers 65% more expressions than a recently published top-performing model using the same experimental setup.
We demonstrate that running many genetic programming generations without interdependence on the neural-guided component performs better for symbolic regression than alternative formulations where the two are more strongly coupled.
Finally, we introduce a new set of 22 symbolic regression benchmark problems with increased difﬁculty over existing benchmarks. Source code is provided at www.github.com/brendenpetersen/deep-symbolic-optimization. 1

Introduction
Symbolic regression involves searching the space of mathematical expressions to ﬁt a dataset using equations which are potentially easier to interpret than, for example, neural networks. A key difference compared to polynomial or neural network-based regression is that we seek to illuminate the true underlying process that generated the data. Thus, the process of symbolic regression is analogous to how a physicist may derive a set of fundamental expressions to describe a natural process. For example, Tycho Brahe meticulously mapped the motion of planets through the sky, but it was Johannes
Kepler who later created the expressions for the laws that described their motion. Given a dataset (X, y), where each point has inputs Xi ∈ Rn and response yi ∈ R, symbolic regression aims to identify a function f : Rn → R that best ﬁts the dataset, where the functional form of f is a short closed-form mathematical expression.
The space of mathematical expressions is structurally discrete (in functional form) but continuous in parameter space (e.g. ﬂoating-point constants). The search space grows exponentially with the length of the expression, rendering symbolic regression a challenging machine learning problem. It is generally believed to be NP-hard [Lu et al., 2016]; however, no formal proof exists. Given the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Method overview. A parameterized sequence generator (e.g. RNN) generates N samples, i.e. expressions for symbolic regression. These samples are used as the starting population for a GP component. GP then runs S generations. The top M samples from GP are extracted, combined with the N samples from the RNN, and used to train the RNN, e.g. using VPG, RSPG, or PQT. Since GP is stateless, it runs in a random restart-like fashion each time the RNN is sampled. large, combinatorial search space, traditional approaches to symbolic regression commonly utilize evolutionary algorithms, especially genetic programming (GP) [Koza, 1992, Schmidt and Lipson, 2009, Fortin et al., 2012, Bäck et al., 2018]. GP-based symbolic regression operates by maintaining a population of mathematical expression “individuals” that are “evolved” using evolutionary operations like selection, crossover, and mutation. A ﬁtness function acts to improve the population over many generations.
Neural networks can also be leveraged for symbolic regression [Kusner et al., 2017, Sahoo et al., 2018, Udrescu and Tegmark, 2020]. Recently, it has been proposed to solve symbolic regression using neural-guided search [Petersen et al., 2021, Landajuela et al., 2021b]. This approach works by using a recurrent neural network (RNN) to stochastically emit batches of expressions as a sequence of mathematical operators or “tokens.” Expressions are evaluated for goodness of ﬁt and a training strategy is used to improve the quality of generated formulas. Petersen et al. [2021] propose a risk-seeking policy gradient strategy, which ﬁlters out the lesser performers and returns an “elite set” of expressions to the RNN each step of training. Constraints can be employed to prune the search space, preventing nonsensical or extraneous statements in generated expressions. For instance, inversions (e.g. log (ex)) and nested trigonometric functions (e.g. sin(1 + cos(x)) can be avoided.
The performance attained using neural-guided search outperformed GP-based approaches, including commonly used commercial software.
Genetic programming and neural-guided search are mechanistically dissimilar, yet both have proven to be effective solutions to symbolic regression. Might it be possible to combine the two approaches to leverage each of their strengths? The population of individual solutions in GP is structurally the same as a batch of samples emitted by the RNN in neural-guided search. In principle, they can formally be interfaced by allowing GP individuals to ﬂow to the RNN training and vice versa.
For reinforcement learning-based training objectives like the one used in Petersen et al. [2021], this way of coupling creates an out-of-distribution problem on the RNN side. Standard off-policy methods, like importance sampling [Glynn and Iglehart, 1989], do not apply here since the GP distribution is intractable. Note, however, that the mechanistic interpretation of the policy gradient as maximizing the log likelihood of individuals proportional to their ﬁtness is still valid. On the other hand, for training alternatives based on maximum likelihood estimation over a selected subset of samples, like the cross-entropy method [De Boer et al., 2005] or priority queue training [Abolaﬁa et al., 2018], there are no assumptions about samples being “on-policy,” and thus they can be applied seamlessly.
In this work, we explore three different ways of training the RNN: two reinforcement learning-based training methods (without off-policy correction) and the priority queue training method. 2
2