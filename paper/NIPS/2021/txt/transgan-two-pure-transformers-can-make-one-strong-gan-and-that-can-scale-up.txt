Abstract
The recent explosive interest on transformers has suggested their potential to become powerful “universal" models for computer vision tasks, such as classi-ﬁcation, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difﬁ-cult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to conduct the ﬁrst pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures.
On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of tech-niques that can mitigate the training instability issues of TransGAN, such as data augmentation, modiﬁed normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Speciﬁcally, TransGAN sets the new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10. It also reaches the inception score of 9.02 and FID of 9.26 on CIFAR-10, and 5.28 FID on
CelebA 128 × 128, respectively: both on par with the current best results. When it comes to higher-resolution (e.g. 256 × 256) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high ﬁdelity and reasonable texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at: https://github.com/VITA-Group/TransGAN. 1

Introduction
Generative adversarial networks (GANs) have gained considerable success on numerous tasks [1, 2, 3, 4, 5, 6, 7]. Unfortunately, GANs suffer from the notorious training instability, and numerous efforts have been devoted to stabilizing GAN training, introducing various regularization terms [8, 9, 10, 11], better losses [1, 12, 13, 14], and training recipes [15, 16]. Among them, one important route to improving GANs examines their neural architectures. [17, 8] reported a large-scale study of GANs and observed that when serving as (generator) backbones, popular neural architectures perform comparably well across the considered datasets. Their ablation study suggested that most of the variations applied in the ResNet family resulted in very marginal improvements. Nevertheless, neural architecture search (NAS) was later introduced to GANs and suggests enhanced backbone designs are also important for improving GANs, just like for other computer vision tasks. Those works are consistently able to discover stronger GAN architectures beyond the standard ResNet topology 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Representative visual examples synthesized by TransGAN, without using convolutional layers. (a) The synthesized visual examples on CelebA-HQ (256 × 256) dataset. (b) The linear interpolation results between two latent vectors, on CelebA-HQ (256 × 256) dataset.
[18, 19, 20]. Other efforts include customized modules such as self-attention [21], style-based generator [22], and autoregressive transformer-based part composition [23].
However, one last “commonsense" seems to have seldomly been challenged: using convolutional neural networks (CNNs) as GAN backbones. The original GAN [24, 25] used fully-connected networks and can only generate small images. DCGAN [26] was the ﬁrst to scale up GANs using
CNN architectures, which allowed for stable training for higher resolution and deeper generative models. Since then, in the computer vision domain, every successful GAN relies on CNN-based generators and discriminators. Convolutions, with the strong inductive bias for natural images, crucially contribute to the appealing visual results and rich diversity achieved by modern GANs.
Can we build a strong GAN completely free of convolutions? This is a question not only arising from intellectual curiosity, but also of practical relevance. Fundamentally, a convolution operator has a local receptive ﬁeld, and hence CNNs cannot process long-range dependencies unless passing through a sufﬁcient number of layers. However, that is inefﬁcient, and could cause the loss of feature resolution and ﬁne details, in addition to the difﬁculty of optimization. Vanilla CNN-based models are therefore inherently not well suited for capturing an input image’s “global" statistics, as demonstrated by the beneﬁts from adopting self-attention [21] and non-local [27] operations in computer vision.
Moreover, the spatial invariance possessed by convolution poses a bottleneck on its ability of adapting to spatially varying/heterogeneous visual patterns, which also motivates the success of relational network [28], dynamic ﬁlters [29, 30] and kernel prediction [31] methods. 1.1 Our Contributions
This paper aims to be the ﬁrst pilot study to build a GAN completely free of convolutions, using only pure transformer-based architectures. We are inspired by the recent success of transformer architectures in computer vision [32, 33, 34]. Compared to parallel generative modeling works
[21, 23, 35] that applied self-attention or transformer encoder in conjunction with CNN-based backbones, our goal is more ambitious and faces several daunting gaps ahead. First and foremost, although a pure transformer architecture applied directly to sequences of image patches can perform very well on image classiﬁcation tasks [34], it is unclear whether the same way remains effective in generating images, which crucially demands the spatial coherency in structure, color, and texture, as well as the richness of ﬁne details. The handful of existing transformers that output images have unanimously leveraged convolutional part encoders [23] or feature extractors [36, 37]. Moreover, even given well-designed CNN-based architectures, training GANs is notoriously unstable and prone to mode collapse [15]. Training vision transformers are also known to be tedious, heavy, and data-hungry [34]. Combining the two will undoubtedly amplify the challenges of training.
In view of those challenges, this paper presents a coherent set of efforts and innovations towards building the pure transformer-based GAN architectures, dubbed TransGAN. A naive option may directly stack multiple transformer blocks from raw pixel inputs, but that would scale poorly due to memory explosion. Instead, we start with a memory-friendly transformer-based generator by gradually increasing the feature map resolution in each stage. Correspondingly, we also improve the discriminator with a multi-scale structure that takes patches of varied size as inputs, which 2
balances between capturing global contexts and local details, in addition to enhancing memory efﬁciency more. Based on the above generator-discriminator design, we introduce a new module called grid self-attention, that alleviates the memory bottleneck further when scaling up TransGAN to high-resolution generation (e.g. 256 × 256).
To address the aforementioned instability issue brought by both GAN and Transformer, we also develop a unique training recipe in association with our innovative TransGAN architecture, that effectively stabilizes its optimization and generalization. That includes showings the necessity of data augmentation, modifying layer normalization, and replacing absolute token locations with relative position encoding. Our contributions are outlined below:
• Novel Architecture Design: We build the ﬁrst GAN using purely transformers and no convolution. TransGAN has customized a memory-friendly generator and a multi-scale discriminator, and is further equipped with a new grid self-attention mechanism. Those architectural components are thoughtfully designed to balance memory efﬁciency, global feature statistics, and local ﬁne details with spatial variances.
• New Training Recipe: We study a number of techniques to train TransGAN better, includ-ing leveraging data augmentation, modifying layer normalization, and adopting relative position encoding, for both generator and discriminator. Extensive ablation studies, discus-sions, and insights are presented.
• Performance and Scalability: TransGAN achieves highly competitive performance com-pared to current state-of-the-art GANs. Speciﬁcally, it sets the new state-of-the-art inception score of 10.43 and FID score of 18.28 on STL-10. It also reaches competitive 9.02 inception score and 9.26 FID on CIFAR-10, and 5.28 FID score on CelebA 128 × 128, respectively.
Meanwhile, we also evaluate TransGAN on higher-resolution (e.g., 256 × 256) generation tasks, where TransGAN continues to yield diverse and impressive visual examples. 2