Abstract
We introduce a novel theoretical framework for Return On Investment (ROI) maximization in repeated decision-making. Our setting is motivated by the use case of companies that regularly receive proposals for technological innovations and want to quickly decide whether they are worth implementing. We design an algorithm for learning ROI-maximizing decision-making policies over a sequence of innovation proposals. Our algorithm provably converges to an optimal policy in class Π at a rate of order min (cid:8)1/(N ∆2), N −1/3}, where N is the number of innovations and ∆ is the suboptimality gap in Π. A signiﬁcant hurdle of our formulation, which sets it aside from other online learning problems such as bandits, is that running a policy does not provide an unbiased estimate of its performance. 1

Introduction
Often, companies have to make yes/no decisions, such as whether to adopt a new technology or retire an old product. However, ﬁnding out the best option in all circumstances could mean spending too much time or money in the evaluation process. If the decisions to make are many, one could be better off making more of them quickly and inexpensively, provided that these decisions have an overall positive effect. In this paper, we investigate the problem of determining a decision policy to balance the reward over cost ratio optimally (i.e., to maximize the return on investment).
A motivating example. Consider a technology company that keeps testing innovations to increase some chosen metric (e.g., beneﬁts, gross revenue, revenue excluding the trafﬁc acquisition cost).
Before deploying an innovation, the company wants to ﬁgure out whether it is proﬁtable. As long as each innovation can be tested on i.i.d. samples of users, the company can perform randomized tests and make statistically sound decisions. However, there is an incentive to make these tests run as quickly as possible because, for example, the testing process is expensive. Another reason could be that keeping a team on a project that has negative, neutral, or even borderline positive potential prevents it from testing other ideas that might lead to a signiﬁcantly better improvement. In other words, it is crucial to learn when to drop barely positive innovations in favor of highly positive ones, so to increase the overall ﬂow of improvement over time (i.e., the ROI of the tests).
More generally, our framework describes problems where an agent faces a sequence of decision tasks consisting of either accepting or rejecting an innovation. Before making each decision, the agent can invest resources into reducing the uncertainty on the value brought by the innovation. The global objective is to maximize the total ROI. Namely, the ratio between the total value accumulated by accepting innovations and the total cost. For an in-depth discussion on alternative goals, we refer the reader to the Supplementary Material (Section D). 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The model. Each task n in the sequence is associated with a pair (µn, Dn) that the learner can never directly observe.
• µn is a random variable representing the (possibly negative) true value of the n-th innovation.
• Dn is a probability distribution over the real numbers with expectation µn, modeling the feedback on the n-th innovation that the learner can gather from testing (see below).
During the n-th task, the learner can draw arbitrarily many i.i.d. samples Xn,1, Xn,2, . . . from Dn, accumulating information on the unknown value µn of the innovation currently being tested. After stopping drawing samples, the learner can decide to either accept the innovation, earning µn as a reward, or reject it and gain nothing instead. We measure the agent performance during N tasks as the (expected) total amount of value accumulated by accepting innovations µn divided by the (expected) total number of samples requested throughout all tasks. In formulas, the ROI of a strategy is then (cid:80)N
E(cid:2)µn · I{µn is accepted}(cid:3)
E(cid:2)number of samples requested during the n-th task(cid:3) n=1 (cid:80)N n=1
In Section 3 we present this setting in more detail and introduce the relevant notation.
I.I.D. assumption. We assume that the value µn of the n-th innovation is drawn i.i.d. from an unknown and ﬁxed distribution. This assumption is meaningful if past decisions do not inﬂuence future innovations whose global quality remains stable over time. In particular, it applies whenever innovations can progress in many orthogonal directions, each yielding a similar added value (e.g., when different teams within the same company test improvements relative to individual aspects of the company). If both the state of the agent and that of the environment evolve, but the ratio of good versus bad innovations remains essentially the same, then this i.i.d. assumption is still justiﬁed. In other words, it is not necessarily the absolute quality of the innovations that has to remain stationary, but rather the relative added value of the innovations given the current state of the system. This case is frequent in practice, especially when a system is close to its technological limit. Last but not least, algorithms designed under stochastic assumptions often perform surprisingly well in practice, even if i.i.d. assumptions are not fully satisﬁed or simply hard to check.
A baseline strategy and policy classes. A natural, yet suboptimal, approach for deciding if an innovation is worth accepting is to gather samples sequentially, stopping as soon as the absolute value of their running average surpasses a threshold, and then accepting the innovation if and only if the average is positive. The major drawback of this approach is that the value µn of an innovation n could be arbitrarily close to zero. In this case, the number of samples needed to reliably determine its sign (which is of order 1/µ2 n) would become prohibitively large. This would result is a massive time investment for an innovation whose return is negligible at best. In hindsight, it would have been better to reject the innovation early and move on to the next task. For this reason, testing processes in practice needs hard termination rules of the form: if after drawing a certain number of samples no conﬁdent decision can be taken, then terminate the testing process rejecting the current innovation. Denote by τ this capped early stopping rule and by accept the accept/reject decision rule that comes with it. We say that the pair π = (τ, accept) is a policy because it fully characterize the decision-making process for an innovation. Policies deﬁned by capped early stopping rules (see (4) for a concrete example) are of great practical importance (Johari et al., 2017; Kohavi et al., 2013). However, policies can be deﬁned more generally by any reasonable stopping rule and decision function. Given a (possibly inﬁnite) set of policies, and assuming that µ1, µ2, . . . are drawn i.i.d. from some unknown but ﬁxed distribution, the goal is to learn efﬁciently, at the lowest cost, the best policy π(cid:63) in the set with respect to a sensible metric. Competing against ﬁxed policy classes is a common modeling choice that allows to express the intrinsic constraints that are imposed by the nature of the decision-making problem. For example, even if some policies outside of the class could theoretically yield better performance, they might not be implementable because of time, budget, fairness, or technology constraints.
Challenges. One of the biggest challenges arising in our framework is that running a decision-making policy generates a collection of samples that —in general— cannot be used to form an unbiased estimate of the policy reward (see the impossibility result in Section E of the Supplementary
Material). The presence of this bias is a signiﬁcant departure from settings like multiarmed and ﬁring bandits (Auer et al., 2002; Jain and Jamieson, 2018), where the learner observes an unbiased sample of the target quantity at the end of every round (see the next section for additional details). Moreover, contrary to standard online learning problems, the relevant performance measure is neither additive in 2
the number of innovations nor in the number of samples per innovation. Therefore, algorithms have to be analyzed globally, and bandit-like techniques —in which the regret is additive over rounds— cannot be directly applied. We argue that these technical difﬁculties are a worthy price to pay in order to deﬁne a plausible setting, applicable to real-life scenarios.
Main contributions. The ﬁrst contribution of this paper is providing a mathematical formalization of our ROI maximization setting for repeated decision making (Section 3). We then design an algorithm called Capped Policy Elimination (Algorithm 1, CAPE) that applies to ﬁnite policy classes (Section 4). We prove that CAPE converges to the optimal policy at rate 1/(∆2N ), where N is the number of tasks and ∆ is the unknown gap between the performance of the two best policies, and at rate N −1/3 when ∆ is small (Theorem 1) . In Section 5 we tackle the challenging problem of inﬁnitely large policy classes. For this setting, we design a preprocessing step (Algorithm 2, ESC) that leads to the ESC-CAPE algorithm. We prove that this algorithm converges to the optimal policy in an inﬁnite set at a rate of N −1/3 (Theorem 4).
Limitations. Although we do not investigate lower bounds in this paper, we conjecture that our
N −1/3 convergence rate it is optimal due to similarities with bandits with weakly observable feedback graphs (see Section 4, “Divided we fall”). Another limitation of our theory is that it only applies to i.i.d. sequences of values µn. It would be interesting to extend our analysis to distributions of µn that evolve over time. These two intriguing problems are left open for future research. 2