Abstract
In this work, we address risk-averse Bayes-adaptive reinforcement learning. We pose the problem of optimising the conditional value at risk (CVaR) of the total return in Bayes-adaptive Markov decision processes (MDPs). We show that a policy optimising CVaR in this setting is risk-averse to both the epistemic uncertainty due to the prior distribution over MDPs, and the aleatoric uncertainty due to the inherent stochasticity of MDPs. We reformulate the problem as a two-player stochastic game and propose an approximate algorithm based on Monte Carlo tree search and Bayesian optimisation. Our experiments demonstrate that our approach signiﬁcantly outperforms baseline approaches for this problem. 1

Introduction
In standard model-based reinforcement learning (RL), an agent interacts with an unknown environ-ment to maximise the expected reward [42]. However, for any given episode the total reward received by the agent is uncertain. There are two sources of this uncertainty: the epistemic (or parametric) uncertainty due to imperfect knowledge about the underlying MDP model, and aleatoric (or internal) uncertainty due to the inherent stochasticity of the underlying MDP [20]. In many domains, we wish to ﬁnd policies which are risk-averse to both sources of uncertainty.
As an illustrative example, consider a navigation system for an automated taxi service. The system attempts to minimise travel duration subject to uncertainty due to the trafﬁc conditions, and trafﬁc lights and pedestrian crossings. For each new day of operation, the trafﬁc conditions are initially unknown. This corresponds to epistemic uncertainty over the MDP model. This uncertainty is reduced as the agent collects experience and learns which roads are busy or not busy. Trafﬁc lights and pedestrian crossings cause stochasticity in the transition durations corresponding to aleatoric uncertainty. The aleatoric uncertainty remains even as the agent learns about the environment. A direct route, i.e. through the centre of the city, optimises the expected journey duration. However, occasionally this route incurs extremely long durations due to some combination of poor trafﬁc conditions and getting stuck at trafﬁc lights and pedestrian crossings. Thus, bad outcomes can be caused by a combination of both sources of uncertainty. These rare outcomes may result in unhappy customers for the taxi service. Therefore, we wish to be risk-averse and prioritise greater certainty of avoiding poor outcomes rather than expected performance. To ensure that the risk of a bad journey is avoided, the navigation system must consider both the epistemic and the aleatoric uncertainties.
In model-based Bayesian RL, a belief distribution over the underlying MDP is maintained. This quantiﬁes the epistemic uncertainty over the underlying MDP given the transitions observed so far. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The Bayesian RL problem can be reformulated as a planning problem in a Bayes-Adaptive MDP (BAMDP) with an augmented state space composed of the belief over the underlying MDP and the state in the underlying MDP [12].
In this work, we address risk-averse decision making in model-based Bayesian RL. Instead of optimising for expected value, we optimise a risk metric applied to the total return of the BAMDP.
Speciﬁcally, we focus on conditional value at risk (CVaR) [34]. The return in a BAMDP is uncertain due to both the uncertain prior belief over the underlying MDP, and the inherent stochasticity of
MDPs. By optimising the CVaR of the return in the BAMDP, our approach simultaneously addresses epistemic and aleatoric uncertainty under a single framework. This is in contrast with previous works which have generally considered risk-aversion to either epistemic or aleatoric uncertainty.
We formulate CVaR optimisation in Bayesian RL as a stochastic game over the BAMDP against an adversarial environment. Solving this game is computationally challenging because the set of reachable augmented states grows exponentially, and the action space of the adversary is continuous.
Our proposed algorithm uses Monte Carlo tree search (MCTS) to focus search on promising areas of the augmented state space. To deal with the continuous action space of the adversary, we use
Bayesian optimisation to expand promising adversary actions. Our main contributions are:
• Addressing CVaR optimisation of the return in model-based Bayesian RL to simultaneously mitigate epistemic and aleatoric uncertainty.
• An algorithm based on MCTS and Bayesian optimisation for this problem.
To our knowledge, this is the ﬁrst work to optimise a risk metric in model-based Bayesian RL to simultaneously address both epistemic and aleatoric uncertainty, and the ﬁrst work to present an
MCTS algorithm for CVaR optimisation in sequential decision making problems. Our empirical results show that our algorithm signiﬁcantly outperforms baseline methods on two domains. 2