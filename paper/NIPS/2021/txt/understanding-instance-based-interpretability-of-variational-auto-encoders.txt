Abstract
Instance-based interpretation methods have been widely studied for supervised learning methods as they help explain how black box neural networks predict.
However, instance-based interpretations remain ill-understood in the context of unsupervised learning. In this paper, we investigate inﬂuence functions [Koh and
Liang, 2017], a popular instance-based interpretation method, for a class of deep generative models called variational auto-encoders (VAE). We formally frame the counter-factual question answered by inﬂuence functions in this setting, and through theoretical analysis, examine what they reveal about the impact of training samples on classical unsupervised learning methods. We then introduce VAE-TracIn, a computationally efﬁcient and theoretically sound solution based on Pruthi et al. [2020], for VAEs. Finally, we evaluate VAE-TracIn on several real world datasets with extensive quantitative and qualitative analysis. 1

Introduction
Instance-based interpretation methods have been popular for supervised learning as they help explain why a model makes a certain prediction and hence have many applications [Barshan et al., 2020, Basu et al., 2020, Chen et al., 2020, Ghorbani and Zou, 2019, Hara et al., 2019, Harutyunyan et al., 2021,
Koh and Liang, 2017, Koh et al., 2019, Pruthi et al., 2020, Yeh et al., 2018, Yoon et al., 2020]. For a classiﬁer and a test sample z, an instance-based interpretation ranks all training samples x according to an interpretability score between x and z. Samples with high (low) scores are considered positively (negatively) important for the prediction of z.
However, in the literature of unsupervised learning especially generative models, instance-based interpretations are much less understood. In this work, we investigate instance-based interpretation methods for unsupervised learning based on inﬂuence functions [Cook and Weisberg, 1980, Koh and
Liang, 2017]. In particular, we theoretically analyze certain classical non-parametric and parametric methods. Then, we look at a canonical deep generative model, variational auto-encoders (VAE,
[Higgins et al., 2016, Kingma and Welling, 2013]), and explore some of the applications.
The ﬁrst challenge is framing the counter-factual question for unsupervised learning. For instance-based interpretability in supervised learning, the counter-factual question is "which training samples are most responsible for the prediction of a test sample?" – which heavily relies on the label information. However, there is no label in unsupervised learning. In this work, we frame the counter-factual question for unsupervised learning as "which training samples are most responsible for increasing the likelihood (or reducing the loss when likelihood is not available) of a test sample?" We show that inﬂuence functions can answer this counter-factual question. Then, we examine inﬂuence functions for several classical unsupervised learning methods. We present theory and intuitions on how inﬂuence functions relate to likelihood and pairwise distances. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The second challenge is how to compute inﬂuence functions in VAE. The ﬁrst difﬁculty here is that the VAE loss of a test sample involves an expectation over the encoder, so the actual inﬂuence function cannot be precisely computed. To deal with this problem, we use the empirical average of inﬂuence functions, and prove a concentration bound of the empirical average under mild conditions.
Another difﬁculty is computation. The inﬂuence function involves inverting the Hessian of the loss with respect to all parameters, which involves massive computation for big neural networks with millions of parameters. To deal with this problem, we adapt a ﬁrst-order estimate of the inﬂuence function called TracIn [Pruthi et al., 2020] to VAE. We call our method VAE-TracIn. It is fast because (i) it does not involve the Hessian, and (ii) it can accelerate computation with only a few checkpoints.
We begin with a sanity check that examines whether training samples have the highest inﬂuences over themselves, and show VAE-TracIn passes it. We then evaluate VAE-TracIn on several real world datasets. We ﬁnd high (low) self inﬂuence training samples have large (small) losses. Intuitively, high self inﬂuence samples are hard to recognize or visually high-contrast, while low self inﬂuence ones share similar shapes or background. These ﬁndings lead to an application on unsupervised data cleaning, as high self inﬂuence samples are likely to be outside the data manifold. We then provide quantitative and visual analysis on inﬂuences over test data. We call high and low inﬂuence samples proponents and opponents, respectively. 1 We ﬁnd in certain cases both proponents and opponents are similar samples from the same class, while in other cases proponents have large norms.
We consider VAE-TracIn as a general-purpose tool that can potentially help understand many aspects in the unsupervised setting, including (i) detecting underlying memorization, bias or bugs [Feldman and Zhang, 2020] in unsupervised learning, (ii) performing data deletion [Asokan and Seelaman-tula, 2020, Izzo et al., 2021] in generative models, and (iii) examining training data without label information.
We make the following contributions in this paper.
• We formally frame instance-based interpretations for unsupervised learning.
• We examine inﬂuence functions for several classical unsupervised learning methods.
• We present VAE-TracIn, an instance-based interpretation method for VAE. We provide both theoretical and empirical justiﬁcation to VAE-TracIn.
• We evaluate VAE-TracIn on several real world datasets. We provide extensive quantitative analysis and visualization, as well as an application on unsupervised data cleaning. 1.1