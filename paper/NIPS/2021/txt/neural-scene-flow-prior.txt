Abstract
Before the deep learning revolution, many perception algorithms were based on runtime optimization in conjunction with a strong prior/regularization penalty. A prime example of this in computer vision is optical and scene ﬂow. Supervised learning has largely displaced the need for explicit regularization. Instead, they rely on large amounts of labeled data to capture prior statistics, which are not always readily available for many problems. Although optimization is employed to learn the neural network, the weights of this network are frozen at runtime. As a result, these learning solutions are domain-speciﬁc and do not generalize well to other statistically different scenarios. This paper revisits the scene ﬂow problem that relies predominantly on runtime optimization and strong regularization. A central innovation here is the inclusion of a neural scene ﬂow prior, which uses the architecture of neural networks as a new type of implicit regularizer.
Unlike learning-based scene ﬂow methods, optimization occurs at runtime, and our approach needs no ofﬂine datasets—making it ideal for deployment in new environments such as autonomous driving. We show that an architecture based exclusively on multilayer perceptrons (MLPs) can be used as a scene ﬂow prior.
Our method attains competitive—if not better—results on scene ﬂow benchmarks.
Also, our neural prior’s implicit and continuous scene ﬂow representation allows us to estimate dense long-term correspondences across a sequence of point clouds.
The dense motion information is represented by scene ﬂow ﬁelds where points can be propagated through time by integrating motion vectors. We demonstrate such a capability by accumulating a sequence of lidar point clouds. 1

Introduction
State-of-the-art results have recently been achieved by learning-based models [17, 30, 47, 60, 68] for the scene ﬂow problem—the task of estimating 3D motion ﬁelds from dynamic scenes. However, such models heavily rely on large-scale data to capture prior knowledge, which is not always readily available. Scene ﬂow annotations are expensive, and most methods train on synthetic and unrealistic scenarios to ﬁne-tune on small real datasets.
Poor generalization to unseen, out-of-the-distribution inputs is another problem. Prior information is generally limited to the statistics of the data used for training. Real-world applications such as autonomous driving require robust solutions to low-level vision tasks such as depth,
Inspired by optical, and scene ﬂow estimation that work in statistically different scenarios. recent innovations that make use of coordinate-based networks (i.e., pixels or 3D positions as inputs) [9, 36, 37, 39, 56] for 3D modeling and rendering, we investigate the use of such networks to regularize the scene ﬂow problem without any learning directly from point clouds.
Optimization happens at runtime, and instead of learning a prior from data, the network structure
It is not limited to the statistics of a speciﬁc dataset. itself captures the prior information.
∗Research done during internship at Argo AI. Corresponding e-mail: xueqian.li@adelaide.edu.au. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Optimizing neural networks at execution time is not new. Ulyanov et al. [63] showed that a randomly initialized convolutional network could be used as a handcrafted prior for standard inverse problems such as image denoising, super-resolution, and inpainting.
Ding and Feng [12] proposed a runtime optimization method (DeepMapping) for rigid pose estimation using deep neural networks.
Although such deep image priors, deep mapping, and coordinate-based networks for neural scene representations have been successfully applied for inverse problems, rendering, and rigid registration, none has yet investigated (to the best of our knowledge) the use of network-based priors for regularizing scene ﬂow directly from point clouds.
Our proposed neural prior is based on a simple multilayer perceptron (MLP) architecture, and we show it is powerful enough to regularize scene ﬂow given two point clouds implicitly.
The input to the network is 3D points, and the output is a regularized scene ﬂow.
Figure 1: Our neural scene ﬂow prior method achieved higher accuracy while being ∼10× faster than the recent runtime optimization method graph prior [45]. The evaluation was on the KITTI Scene
Flow test set, where each point cloud size varies from 14k to 68k points. In our method, we ﬁxed the number of hidden layers in the MLP to 4 and varied the number of hidden units. In the graph prior method, we varied the number of neighbors to create the graph. Accuracy uses the Acc5 metric as deﬁned in the experiments section. Learning-based methods might still be 10×–100× faster than the runtime optimization methods, but they still lack generalization and have memory issues when dealing with large point clouds—with tens of thousands of points.
Our neural prior allows for a continuous scene
ﬂow representation instead of discrete such as in graph Laplacian-based priors, e.g., [45]. We show how the ﬂow ﬁelds captured by our neural prior can be employed to estimate long-term correspondences across a sequence of point clouds. The continuous scene ﬂow allows for better integration of motions across time.
Our results are promising and competitive to supervised [30], self-supervised [38, 68], and non-learning methods [1, 45] (see Table 1). Our method also scales to real-world point clouds with tens of thousands of points while achieving better accuracy and time complexity than recent runtime optimization methods (see Fig. 1). 2