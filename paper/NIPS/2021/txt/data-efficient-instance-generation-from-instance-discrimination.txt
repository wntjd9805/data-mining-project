Abstract
Generative Adversarial Networks (GANs) have signiﬁcantly advanced image synthesis, however, the synthesis quality drops signiﬁcantly given a limited amount of training data. To improve the data efﬁciency of GAN training, prior work typically employs data augmentation to mitigate the overﬁtting of the discriminator yet still learn the discriminator with a bi-classiﬁcation (i.e., real vs. fake) task. In this work, we propose a data-efﬁcient Instance Generation (InsGen) method based on instance discrimination. Concretely, besides differentiating the real domain from the fake domain, the discriminator is required to distinguish every individual image, no matter it comes from the training set or from the generator. In this way, the discriminator can beneﬁt from the inﬁnite synthesized samples for training, alleviating the overﬁtting problem caused by insufﬁcient training data. A noise perturbation strategy is further introduced to improve its discriminative power.
Meanwhile, the learned instance discrimination capability from the discriminator is in turn exploited to encourage the generator for diverse generation. Extensive experiments demonstrate the effectiveness of our method on a variety of datasets and training settings. Noticeably, on the setting of 2K training images from the FFHQ dataset, we outperform the state-of-the-art approach with 23.5% FID improvement.1 1

Introduction
Generative Adversarial Network (GAN) [16] has become a popular paradigm to learn the distribution of the observed data. It is formulated as a two-player game, where a generator synthesizes realistic data, while a discriminator distinguishes synthesized samples from real ones. To reach equilibrium in this minimax game, it requires both the generator and the discriminator to be sufﬁciently trained.
In other words, the synthesis capability of the generator will subsequently deteriorate given an inadequate discriminator [24, 39, 49, 51].
Recent success of GANs [22, 23, 25, 4] relies on big data to assure the sufﬁcient training of the discriminator. Prior work [49, 24] has found that reducing the amount of training data leads to the overﬁtting of the discriminator, which tends to memorize the entire training set. In turn, the back-propagation from the discriminator to the generator damages the synthesis quality of the generator and potentially causes the mode collapse problem [1, 44]. Data augmentation is one of the most widely used methods to alleviate the overﬁtting issue in deep learning algorithms [45, 11, 10]. Some recent attempts [24, 39, 49, 51, 44] have been made to apply data augmentation to GAN training. It is found that the discriminator can be improved by augmenting not only the real images from the dataset but also the synthesized images by the generator [49, 24]. However, the learning objective of the discriminator remains as categorizing real and fake domains and a substantial performance drop can be observed given limited training data. 1Code is available at https://genforce.github.io/insgen/. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
The domain bi-classiﬁcation task could be too easy for the discriminator to gain sufﬁcient discriminative power as an adaptive loss to train the generator, especially when the size of training set is small. In this work, we propose to improve the data efﬁciency in GAN training by assigning a more challenging task to the discriminator, which is to distinguish every individual image as an independent category. In this way, the discriminator is forced to improve its discriminative capability to accomplish the instance discrimination task [40]. Notably, besides distinguishing real samples, we also demand the discriminator to differentiate fake samples synthesized by the generator. Thus the discriminator can be considered to train with inﬁnite data, preventing it from memorizing the training samples. When distinguishing synthesized data, we design a noise perturbation strategy to increase the difﬁculty of the task and hence make the discriminator more capable. Meanwhile, we also alter the training objectives from the generator side. Concretely, besides making the generator to fool the discriminator, we expect all the samples produced by the generator to be well identiﬁed as different instances with our instance-induced discriminator. This highly matches the goal of diverse generation, which requires every synthesis to be unique. We evaluate our method on a range of datasets and achieve appealing generation performance in terms of image quality, diversity, and data efﬁciency.
Experiments show that our method signiﬁcantly improves the baselines and outperform previous data-augmentation methods. To be speciﬁc, our method improves the FID from 15.60 to 11.92, 7.29 to 4.90, and 3.88 to 3.31 with 2K, 10K, and 70K training images from FFHQ [23] respectively. We can even learn a large-scale GAN with only 100 in-the-wild images to produce satisfying synthesis.
Our main contributions are summarized as follows: 1) We propose a data-efﬁcient instance generation (InsGen) method which incorporates instance discrimination as an auxiliary task in
GAN training. 2) The synthesized data is used as inﬁnite samples for improving the discriminative power of the discriminator, which in turn substantially improves the synthesis quality and diversity of the generator. 3) Under various data-regime settings, our method consistently surpasses existing alternatives by a substantial margin. 2