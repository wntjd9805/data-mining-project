Abstract
In this paper, we examine the convergence rate of a wide range of regularized methods for learning in games. To that end, we propose a uniÔ¨Åed algorithmic tem-plate that we call ‚Äúfollow the generalized leader‚Äù (FTGL), and which includes as special cases the canonical ‚Äúfollow the regularized leader‚Äù algorithm, its optimistic variants, extra-gradient schemes, and many others. The proposed framework is also sufÔ¨Åciently Ô¨Çexible to account for several different feedback models ‚Äì from full information to bandit feedback. In this general setting, we show that FTGL algorithms converge locally to strict Nash equilibria at a rate which does not depend on the level of uncertainty faced by the players, but only on the geometry of the regularizer near the equilibrium. In particular, we show that algorithms based on entropic regularization ‚Äì like the exponential weights algorithm ‚Äì enjoy a linear convergence rate, while Euclidean projection methods converge to equilibrium in a
Ô¨Ånite number of iterations, even with bandit feedback. 1

Introduction
In the presence of uncertainty, the players of a game may not have full knowledge of its structure, ‚Äúor the ability and inclination to go through any complex reasoning process to calculate an equilibrium.
But the participants are still supposed to adapt by accumulating empirical information on the relative advantages of the various pure strategies at their disposal‚Äù. This aphorism ‚Äì originally due to Nash
[34, p. 21] ‚Äì constitutes the driving principle of game-theoretic learning, and highlights one of the
Ô¨Åeld‚Äôs most central questions: Does learning with empirical observations lead to a Nash equilibrium?
And, if so, at what rate?
These questions have been at the forefront of game-theoretic research ever since the early days of the Ô¨Åeld, and they have recently received renewed attention via their connection to multi-agent reinforcement learning [43], generative adversarial networks [18], auctions [44], and many other applications where online decision-making plays a major role. Still, any attempt to provide a positive answer to these questions must wrestle with a major roadblock: the well-known impossibility result of Hart and Mas-Colell [19] shows that there are no uncoupled dynamics that converge to Nash equilibrium in all games, thus shattering any hope of obtaining a universal convergence result. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), .
In view of the above, contemporary research on game-theoretic learning has focused on relaxing the feedback requirements of the players‚Äô learning processes, and understanding the stability ‚Äì and instability ‚Äì properties of different kinds of equilibria under popular learning algorithms. One property that stands out in this regard is the so-called ‚Äúfolk theorem‚Äù of evolutionary game theory
[20], which can be stated as follows: Under the replicator dynamics ‚Äì the continuous-time limit of the multiplicative / exponential weights (EW) algorithm [2, 29, 45] ‚Äì a Nash equilibrium is stable and attracting if and only if it is strict (i.e., if and only if each player has a unique best response).
The replicator dynamics are the most widely studied model for evolution in population games, so the above equivalence essentially delineates what is and what isn‚Äôt achievable in an evolutionary setting. In the context of online learning (our paper‚Äôs main focus), a similar equivalence was obtained only recently [11, 15, 30], but it extends to the entire family of ‚Äúfollow the regularized leader‚Äù (FTRL) dynamics [41, 42], in both continuous [11, 30] and discrete time [15]. In particular, [15] studied the convergence of discrete-time FTRL models in the presence of uncertainty, and proved a high-probability, stochastic version of this equivalence that holds for several different types of feedback (full information, bandit, etc.). Thus, coupled with the prominence of FTRL in online and game-theoretic learning, strict Nash equilibria emerge as the only stable limit points of regularized learning under uncertainty.
Our contributions. One important limitation of the above results is that they are qualitative in nature. Indeed, even though asymptotic stability guarantees that a learning process converges locally to a strict equilibrium, it provides no information about the speed of this convergence. In particular, especially for discrete-time models of regularized learning, asymptotic stability does not provide any guidance on how to tune the algorithm‚Äôs hyperparameters (learning rate, mixing, etc.), and/or what to expect in terms of the number of iterations required to reach a neighborhood of a Nash equilibrium.
Our paper aims to provide quantitative answers to these questions for a wide array of regularized learning methods in the presence of uncertainty and limited information. To do so, we Ô¨Årst introduce a
Ô¨Çexible algorithmic framework ‚Äì dubbed ‚Äúfollow the generalized leader‚Äù (FTGL) ‚Äì that incorporates a broad spectrum of action choice mechanisms and feedback models. In more detail (and in analogy to FTRL), the FTGL template maintains a cumulative estimate for the payoff of each action available to the learner, and then selects a mixed strategy via a suitable ‚Äúregularized‚Äù choice map. SpeciÔ¨Åcally: 1. In terms of regularization, the FTGL template includes as special cases the standard logit choice and Euclidean projection methods (as well as all other standard regularizers used in practice). 2. In terms of the information used to update the ‚Äúaggregate score‚Äù of each pure strategy, FTGL includes ‚Äúvanilla‚Äù FTRL, its optimistic variants [10, 38‚Äì40], extra-gradient and mirror-prox methods [24, 25, 35], with either full, oracle-based, or bandit feedback.
In this general context, our main result may be summarized as follows. First, we introduce a ‚Äúrate function‚Äù ùúô that depends only on the regularizer deÔ¨Åning the learning process, and which captures the sensitivity of the induced choice map to external stimuli: for example, ùúô(ùë•) = exp(ùë•) for entropic / logit choice models, whereas ùúô(ùë•) = [ùë•]+ for methods run with Euclidean projections. We then show that, with probability at least 1 ‚àí ùõø, the algorithm‚Äôs local rate of convergence to a strict equilibrium ùë•‚àó is of the form (cid:107) ùëãùëõ ‚àí ùë•‚àó (cid:107) ‚â§ ùúô(ùëë ‚àí ùëê (cid:205)ùëõ
ùõæùë†), where ùõæùëõ is the method‚Äôs learning rate and ùëê, ùëë are constants with ùëê > 0.
ùë†=1
This result shows that the convergence speed of FTGL methods depends only on the choice of regularizer and learning rate: for example, EW methods run with a constant step size converge to an equilibrium at an exponential rate, whereas Euclidean regularization attains convergence in a Ô¨Ånite number of iterations. From a regret-theoretic point of view, this is somewhat surprising because the regret guarantees of entropic FTRL (the EW algorithm) are far superior to those of FTRL with
Euclidean regularization [5, 41].
Equally surprising is the fact that the type of feedback employed does not affect the method‚Äôs rate of convergence: ceteris paribus, the base sequence of states generated by an FTGL method attains the same rate of convergence to strict Nash equilibria, whether run with full, partial, or bandit
/ payoff-based feedback. This comes into stark contrast with the corresponding rates of regret minimization, which depend crucially on the type of feedback received [6, 27]; in a certain, precise sense, this robustness in the face of uncertainty shows that regret minimization and convergence to
Nash equilibrium are fundamdentally different questions. 2