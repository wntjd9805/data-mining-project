Abstract
Recently, there has been much interest in studying the convergence rates of without-replacement SGD, and proving that it is faster than with-replacement SGD in the worst case. However, known lower bounds ignore the problem’s geome-try, including its condition number, whereas the upper bounds explicitly depend on it. Perhaps surprisingly, we prove that when the condition number is taken into account, without-replacement SGD does not significantly improve on with-replacement SGD in terms of worst-case bounds, unless the number of epochs (passes over the data) is larger than the condition number. Since many problems in machine learning and other areas are both ill-conditioned and involve large datasets, this indicates that without-replacement does not necessarily improve over with-replacement sampling for realistic iteration budgets. We show this by provid-ing new lower and upper bounds which are tight (up to log factors), for quadratic problems with commuting quadratic terms, precisely quantifying the dependence on the problem parameters. 1

Introduction
We consider solving finite-sum optimization problems of the form
F (x) = 1 n n (cid:88) i=1 fi(x) using stochastic gradient descent (SGD). Such problems are extremely common in modern machine learning (e.g., for empirical risk minimization), and stochastic gradient methods are the most popular approach for large-scale problems, where both n and the dimension are large. The classical approach to apply SGD to such problems is to repeatedly sample indices i ∈ {1, . . . , n} uniformly at random, and perform updates of the form xt+1 = xt − ηt∇fi(xt), where ηt is a step-size parameter. With this sampling scheme, each ∇fi(xt) is a random unbiased estimate of ∇F (xt) (conditioned on xt).
Thus, the algorithm can be seen as a “cheap” noisy version of plain gradient descent on F (·), where each iteration requires computing the gradient of just a single function fi(·), rather than the gradient of F (·) (which would require computing and averaging n individual gradients). This key observation facilitates the analysis of SGD, while simultaneously explaining why SGD is much more efficient than gradient descent on large-scale problems.
However, the practice of SGD differs somewhat from this idealized description: In practice, it is much more common to perform without-replacement sampling of the indices, by randomly shuffling the indices {1, . . . , n} and processing the individual functions in that order – that is, choosing a random permutation σ on {1, . . . , n}, and performing updates of the form xt+1 = xt − ηt∇fσ(t)(xt). After a full pass over the n functions, further passes are made either with the same permutation σ (known as single shuffling), or with a new random permutation σ chosen before each pass (known as random 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Upper bounds on the expected optimization error for quadratic strongly-convex problems, ignoring constants and log factors. The upper table corresponds to random reshuffling, and the lower table corresponds to single shuffling. The right column presents a necessary (possibly non-sufficient) condition for the bound to be valid and smaller than that of with-replacement SGD, which is order of 1/(λnk) [Shamir and Zhang, 2013, Jain et al., 2019a]. An asterisk (*) denotes results where the bound is on the expected squared distance to the global minimum, rather than optimization error.
For squared distance, the corresponding bound for with-replacement SGD is order of 1/(λ2nk)
[Nemirovski et al., 2009, Rakhlin et al., 2012].
Paper
Bound
Improves on with-Replacement?
Gürbüzbalaban et al. [2015b](*)
HaoChen and Sra [2018]
Rajput et al. [2020]
Ahn et al. [2020] 1
λ4 1
λ4 1 (λk)2 (cid:17) (cid:16) 1 (nk)2 + 1 k3 (cid:16) 1 (nk)2 + 1 nk3 (cid:16) 1 (nk)2 + 1 1
λ4 (cid:17)
λ2nk3 if k ≳ 1
λ if k ≳ 1
λ2 (cid:17)
Only if k ≳ n
Only if k ≳ 1
λ · max{1, (cid:112) n
λ }
Only if k ≳ 1
λ2
Only if k ≳ 1
λ2.5
Paper
Bound
Improves on with-Replacement?
Gürbüzbalaban et al. [2015a](*)
Ahn et al. [2020] 1
λ4nk2
Mishchenko et al. [2020](*) 1 (λk)2 if k ≳ 1
λ2 1
λ3nk2
Only if k ≳ n
Only If k ≳ 1
λ2
Only if k ≳ 1
λ reshuffling). Such without-replacement schemes are not only more convenient to implement in many cases, they often also exhibit faster error decay than with-replacement SGD [Bottou, 2009, Recht and
Ré, 2012]. Unfortunately, analyzing this phenomenon has proven to be notoriously difficult. This is because without-replacement sampling creates statistical dependencies between the iterations, so the stochastic gradients computed at each iteration can no longer be seen as unbiased estimates of gradients of F (·).
Nevertheless, in the past few years our understanding of this problem has significantly improved. For concreteness, let us focus on a classical setting where each fi(·) is a convex quadratic, and F (·) is strongly convex. Suppose that SGD is allowed to perform k epochs, each of which consists of passing over the n individual functions in a random order. Assuming the step size is appropriately chosen, it has been established that SGD with single-shuffling returns a point whose expected optimization error is on the order of 1/(nk2). For random reshuffling, this further improves to 1/(nk)2 + 1/(nk3) (see further discussion in the related work section below). In contrast, the expected optimization error of with-replacement SGD after the same overall number of iterations (nk) is well-known to be on the order of 1/(nk) (e.g., Nemirovski et al. [2009], Rakhlin et al. [2012]). Moreover, these bounds are known to be unimprovable in general, due to the existence of nearly matching lower bounds [Safran and Shamir, 2020, Rajput et al., 2020]. Thus, as the number of epochs k increases, without-replacement SGD seems to provably beat with-replacement SGD.
Despite these encouraging results, it is important to note that the bounds as stated above quantify only the dependence on n, k, and ignore dependencies on other problem parameters. In particular, it is well-known that the convergence of SGD is highly sensitive to the problem’s strong convexity and smoothness parameters, their ratio (a.k.a. the condition number), as well as the magnitude of the gradients. Unfortunately, existing lower bounds ignore some of these parameters (treating them as constants), while in known upper bounds these can lead to vacuous results in realistic regimes. To give a concrete example, let us return to the case where each fi(·) is a convex quadratic function, and assume that F (·) is λ-strongly convex for some λ > 0. In Table 1, we present existing upper bounds for this case, as a function of both n, k as well as λ (fixing other problem parameters, such as the 2
smoothness parameter, as constants).1 The important thing to note about these bounds is that they are valid and improve over the bound for with-replacement SGD only when the number of epochs k is at least n or 1/λ (or even more). Unfortunately, such a regime is problematic for two reasons: First, large-scale high-dimensional problems are often ill-conditioned, with λ being very small and n being very large, so requiring that many passes k over all functions can easily be prohibitively expensive.
Second, if we allow k > 1
λ , there exist much better and simpler methods than SGD: Indeed, we can simply run deterministic gradient descent for k iterations (computing the gradient of F (·) at each iteration by computing and averaging the gradients of fi(·) in any order we please). Since the optimization error of gradient descent (as a function of k, λ) scales as exp(−λk) [Nesterov, 2018], we compute a nearly exact optimum (up to error ϵ) as long as k ≳ 1
ϵ ). Moreover, slightly more sophisticated methods such as accelerated gradient descent or the conjugate gradient method enjoy an error bound scaling as exp(−k
λ), so in fact, we can compute an ϵ-optimal point already when k ≳ 1√
λ
λ log( 1 log( 1
ϵ ).
√
Thus, the power of SGD is mostly when k is relatively small, and definitely smaller than quantities such as 1/λ. However, the upper bounds discussed earlier do not imply any advantage of without-replacement sampling in this regime. Of course, these are only upper bounds, which might possibly be loose. Thus, it is not clear if this issue is simply an artifact of the existing analyses, or a true issue of without-replacement sampling methods.
In this paper, we rigorously study this question, with the following (perhaps surprising) conclusion:
At least in the worst-case, without-replacement schemes do not significantly improve over with-replacement sampling, unless the number of epochs k is larger than the problem’s condition number (which suitably defined, scales as 1/λ). As discussed above, this implies that the expected benefit of without-replacement schemes may not be manifest for realistic iteration budgets. In more detail, our contributions are as follows:
• We prove that there is a simple quadratic function F (·), which is λ-strongly convex and
λmax-smooth, such that the expected optimization error of SGD with single shuffling (using any fixed step size) is at least (cid:18) 1
λnk
Ω (cid:26)
· min 1,
λmax/λ k (cid:27)(cid:19)
. (See Thm. 1.) Comparing this to the Θ(1/(λnk)) bound for with-replacement SGD, we see that we cannot possibly get a significant improvement unless k is larger than the condition number term λmax
λ .
• For SGD with random reshuffling, we prove a similar lower bound of (cid:18) 1
λnk
Ω (cid:26)
· min 1 ,
λmax/λ nk
+ max/λ2
λ2 k2 (cid:27)(cid:19)
. (See Thm. 2.) As before, this improves on the Θ(1/(λnk)) bound of with-replacement SGD only when k > λmax
λ .
• We provide matching upper bounds in all relevant parameters (up to constants and log factors and assuming the input dimension is fixed – See Sec. 4), which apply to the class of quadratic functions with commuting quadratic terms (which include in particular the lower bound constructions above).
• To illustrate our theoretical results, we perform a few simple experiments comparing with-and without-replacement sampling schemes on our lower bound constructions (see Sec. 5).
Our results accord with our theoretical findings, and show that if the number of epochs is not greater than the condition number, then without-replacement does not necessarily improve upon with-replacement sampling. Moreover, it is observed that for some of the values of k exceeding the condition number by as much as 50%, with-replacement still provides comparable results to without-replacement, even when averaging their performance over many instantiations. 1Focusing only on λ is enough for the purpose of our discussion here, and moreover, the dependence on other problem parameters is not always explicitly given in the results of previous papers. 3
We conclude with a discussion of the results and open questions in Sec. 6.
We note that our lower and upper bounds apply to SGD using a fixed step size, ηt = η for all t, and for the iterate reached after k epochs. Thus, they do not exclude the possibility that better upper bounds can be obtained with a variable step size strategy, or some iterate averaging scheme. However, we conjecture that it is not true, as existing upper bounds either do not make such assumptions or do not beat the lower bounds presented here. Moreover, SGD with variable step sizes can generally be matched by SGD employing a fixed optimal step size (dependent on the problem parameters and the overall number of iterations).