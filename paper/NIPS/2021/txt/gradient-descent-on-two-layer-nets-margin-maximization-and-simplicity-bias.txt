Abstract
The generalization mystery of overparametrized deep nets has motivated efforts to understand how gradient descent (GD) converges to low-loss solutions that generalize well. Real-life neural networks are initialized from small random values and trained with cross-entropy loss for classiﬁcation (unlike the "lazy" or "NTK" regime of training where analysis was more successful), and a recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020a) provide theoretical evidence that GD may converge to the "max-margin" solution with zero loss, which presumably generalizes well. However, the global optimality of margin is proved only in some settings where neural nets are inﬁnitely or exponentially wide. The current paper is able to establish this global optimality for two-layer
Leaky ReLU nets trained with gradient ﬂow on linearly separable and symmetric data, regardless of the width. The analysis also gives some theoretical justiﬁcation for recent empirical ﬁndings (Kalimeris et al., 2019) on the so-called simplicity bias of GD towards linear or other "simple" classes of solutions, especially early in training. On the pessimistic side, the paper suggests that such results are fragile.
A simple data manipulation can make gradient ﬂow converge to a linear classiﬁer with suboptimal margin. 1

Introduction
One major mystery in deep learning is why deep neural networks generalize despite overparameteri-zation (Zhang et al., 2017). To tackle this issue, many recent works turn to study the implicit bias of gradient descent (GD) — what kind of theoretical characterization can we give for the low-loss solution found by GD?
The seminal works by Soudry et al. (2018a,b) revealed an interesting connection between GD and margin maximization: for linear logistic regression on linearly separable data, there can be multiple linear classiﬁers that perfectly ﬁt the data, but GD with any initialization always converges to the max-margin (hard-margin SVM) solution, even when there is no explicit regularization. Thus the solution found by GD has the same margin-based generalization bounds as hard-margin SVM. Subsequent works on linear models have extended this theoretical understanding of GD to SGD (Nacson et al., 2019b), other gradient-based methods (Gunasekar et al., 2018a), other loss functions with certain poly-exponential tails (Nacson et al., 2019a), linearly non-separable data (Ji and Telgarsky, 2018, 2019b), deep linear nets (Ji and Telgarsky, 2019a; Gunasekar et al., 2018b).
∗Equal contribution
†Most of the work is done when Kaifeng Lyu and Runzhe Wang were at Tsinghua University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Given the above results, a natural question to ask is whether GD has the same implicit bias towards max-margin solutions for machine learning models in general. Lyu and Li (2020) studied the relationship between GD and margin maximization on deep homogeneous neural network, i.e., neural network whose output function is (positively) homogeneous with respect to its parameters. For homogeneous neural networks, only the direction of parameter matters for classiﬁcation tasks. For logistic and exponential loss, Lyu and Li (2020) assumed that GD decreases the loss to a small value and achieves full training accuracy at some time point, and then provided an analysis for the training dynamics after this time point (Theorem 3.1), which we refer to as late phase analysis. It is shown that
GD decreases the loss to 0 in the end and converges to a direction satisfying the Karush-Kuhn-Tucker (KKT) conditions of a constrained optimization problem (P) on margin maximization.
However, given the non-convex nature of neural networks, KKT conditions do not imply global optimality for margins. Several attempts are made to prove the global optimality speciﬁcally for two-layer nets. Chizat and Bach (2020) provided a mean-ﬁeld analysis for inﬁnitely wide two-layer
Squared ReLU nets showing that gradient ﬂow converges to the solution with global max margin, which also corresponds to the max-margin classiﬁer in some non-Hilbertian space of functions. Ji and
Telgarsky (2020a) extended the proof to ﬁnite-width neural nets, but the width needs to be exponential in the input dimension (due to the use of a covering condition). Both works build upon late phase analyses. Under a restrictive assumption that the data is orthogonally separable, i.e., any data point xi can serve as a perfect linear separator, Phuong and Lampert (2021) analyzed the full trajectory of gradient ﬂow on two-layer ReLU nets with small initialization, and established the convergence to a piecewise linear classiﬁer that maximizes the margin, irrespective of network width.
In this paper, we study the implicit bias of gradient ﬂow on two-layer neural nets with Leaky ReLU activation (Maas et al., 2013) and logistic loss. To avoid the lazy or Neural Tangent Kernel (NTK) regime where the weights are initialized to large random values and do not change much during training (Jacot et al., 2018; Chizat et al., 2019; Du et al., 2019b,a; Allen-Zhu et al., 2018, 2019; Zou et al., 2018; Arora et al., 2019b), we use small initialization to encourage the model to learn features actively, which is closer to real-life neural network training.
When analyzing convergence behavior of training on neural networks, one can simplify the prob-lem and gain insights by assuming that the data distribution has a simple structure. Many works particularly study the case where the labels are generated by an unknown teacher network that is much smaller/simpler than the (student) neural network to be trained. Following Brutzkus et al. (2018); Sarussi et al. (2021) and many other works, we consider the case where the dataset is linearly separable, namely the labels are generated by a linear teacher, and study the training dynamics of two-layer Leaky ReLU nets on such dataset. 1.1 Our Contribution
Among all the classiﬁers that can be represented by the two-layer Leaky ReLU nets, we show any global-max-margin classiﬁer is exactly linear under one more data assumption: the dataset is symmetric, i.e., if x is in the training set, then so is −x. Note that such symmetry can be ensured by simple data augmentation.
Still, little is known about what kind of classiﬁers neural network trained by GD learns. Though Lyu and Li (2020) showed that gradient ﬂow converges to a classiﬁer along KKT-margin direction, we note that this result is not sufﬁcient to guarantee the global optimality since such classiﬁer can have nonlinear decision boundaries. See Figure 1 (left) for an example.
In this paper, we provide a multi-phase analysis for the full trajectory of gradient ﬂow, in contrast with previous late phase analyses which only analyzes the trajectory after achieving 100% training accuracy. We show that gradient ﬂow with small initialization converges to a global-max-margin linear classiﬁer (Theorem 4.2). The proof leverages power iteration to show that neuron weights align in two directions in an early phase of training, inspired by Li et al. (2021). We further show the alignment at any constant training time by associating the dynamics of wide neural net with that of two-neuron neural net, and ﬁnally, extend the alignment to the inﬁnite time limit by applying
Kurdyka-Łojasiewicz (KL) inquality in a similar way as Ji and Telgarsky (2020a). The alignment at convergence implies that the convergent classiﬁer is linear.
The above results also justify a recent line of works studying the so-called simplicity bias: GD ﬁrst learns linear functions in the early phase of training, and the complexity of the solution increases 2
as training goes on (Kalimeris et al., 2019; Hu et al., 2020; Shah et al., 2020). Indeed, our result establishes a form of extreme simplicity bias of GD: if the dataset can be ﬁtted by a linear classiﬁer, then GD learns a linear classiﬁer not only in the beginning but also at convergence.
On the pessimistic side, this paper suggests that such global margin maximization result could be fragile. Even for linearly separable data, global-max-margin classiﬁers may be nonlinear without the symmetry assumption. In particular, we show that for any linearly separable dataset, gradient
ﬂow can be led to converge to a linear classiﬁer with suboptimal margin by adding only 3 extra data points (Theorem 6.2). See Figure 1 (right) for an example. 2