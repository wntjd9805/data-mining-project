Abstract
Recurrent Neural Network (RNN) is a fundamental structure in deep learning.
Recently, some works study the training process of over-parameterized neural networks, and show that over-parameterized networks can learn functions in some notable concept classes with a provable generalization error bound. In this paper, we analyze the training and generalization for RNNs with random initialization, and provide the following improvements over recent works: (1) For a RNN with input sequence x = (X1, X2, ..., XL), previous works study to learn functions that are summation of f (βT l Xl) and require normalized conditions that ||Xl|| ≤ ϵ with some very small ϵ depending on the complexity of f . In this paper, using detailed analysis about the neural tangent kernel matrix, we prove a generalization error bound to learn such functions without normalized conditions and show that some notable concept classes are learn-able with the numbers of iterations and samples scaling almost-polynomially in the input length L. (2) Moreover, we prove a novel result to learn N-variables functions of input sequence with the form f (βT [Xl1, ..., XlN ]), which do not belong to the
“additive” concept class, i,e., the summation of function f (Xl). And we show that when either N or l0 = max(l1, .., lN ) − min(l1, .., lN ) is small, f (βT [Xl1, ..., XlN ]) will be learnable with the number iterations and samples scaling almost-polynomially in the input length L. 1

Introduction
In Deep Learning, the recurrent neural network (RNN) is well-known as one of the most popular models to model sequential data and is widely used in practice for tasks in natural language processing (NLP). One of the characters of RNN is that it performs the same operation for all the input of the sequence.
Consider a input sequence x = (X1, X2, ..., XL). A RNN with the form hl(x) = ϕ(W hl−1 + AXl), (1)
∗Corresponding Author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
is trying to learn functions fl(X1, X2, ...Xl) as h1(x) = f1(X1) h2(x) = f2(X1, X2)
... hL(x) = fL(X1, X2, ...XL) (2)
Due to the complex nonlinearity, the loss is generally non-convex, and it is very difficult to give a theoretical guarantee. Recently, there are some works Allen-Zhu et al. (2019b); Cao and Gu (2019);
Allen-Zhu et al. (2019a); Du et al. (2019); Arora et al. (2019); Allen-Zhu et al. (2019c) trying to give a theoretical explanation that why gradient descent can allow an overparametrized network to attain arbitrarily low training error and ample generalization ability. These papers show that, under some assumptions, we have:
• Multi-layer feed-forward networks Allen-Zhu et al. (2019b); Du et al. (2019) and recurrent neural networks Allen-Zhu et al. (2019c) with large hidden size can attain zero training error, regardless of whether the data is properly labeled or randomly labeled. feed-forward networks,
• For multi-layer r=1 ϕr(βT form F ∗(x) = (cid:80)C r X), X ∈ Rd, βr ∈ Rd, ||βr|| = 1 are learnable i.e. fitting the train-ing data with a provably small generalization error, if ϕ is analytic and the “complexity” is low enough Allen-Zhu et al. (2019a); Arora et al. (2019); Cao and Gu (2019). functions with the
• The “complexity” of function ϕ can be measured by a matrix derived from the NTK (Neural
Tangent Kernel) of the network Arora et al. (2019); Cao and Gu (2019).
• For recurrent neural networks Allen-Zhu and Li (2019a), if the input sequence is normalized, i.e., x = (X1, X2, ..., XL), ||X1|| = 1, ||Xl|| = ϵ with ϵ very small, functions with the form
F ∗(x) = (cid:80)L l,rXl) are learnable, where m is the size of matrix W , and
C = (cid:80)∞ i=0 aiRi is a series representing the complexity of learnable functions. r=1 ϕl,r(βT (cid:80)Cl l=1
These works show the provable learning ability of deep learning. But there are still some important issues that were not addressed.
• Firstly, for RNNs, the method in Allen-Zhu and Li (2019a) requires a normalized condition for A and Xl in (1) that ||AXl|| ≤ ϵx for all l ≤ L and shows that for a function F ∗(x) with the complexity C , it is learnable with error O(ϵ1/3 x C ). Thus ||Xl|| (or equally, ||A||) should be very small and the scale is dependent on the complexity of functions. The dependence of
||AXl|| on C makes the results unrealistic in practice since generally the norm of input will not be so small.
• Secondly, the result in Allen-Zhu and Li (2019a) shows that RNNs can learn functions which are the summation of functions like ψ(βT l Xl). But this is only a linear combination of the functions of the input at different positions and does not consider the nonlinear interaction of the inputs. One may ask, since hL(x) is a function of {X1, X2, ...XL}, is it possible to go beyond and learn more complex functions?
In order to study these problems, we consider the binary classification problem: for every input xi, the label (+1 or −1) of xi can be expressed by the sign of a target function F ∗(xi). We consider
Elman recurrent neural networks with ReLU activation hl(x) = ϕ(W hl−1 + AXl) f (W , x) = BT hL(x) ∈ R. x = (X1, X2, ..., XL), Xl ∈ Rd, W ∈ Rm×m,
A ∈ Rm×d, B ∈ Rm, ϕ(x) = max(x, 0) (3) to learn two types of target functions: 2
• Additive Concept Class:
L (cid:88) (cid:88)
F ∗(x) =
ψl,r(βT l,rXl/||Xl||), r=1 l=1
∞ (cid:88) cixi,
ψl,r(x) =
• N-variables Concept Class: i=0
F ∗(x) =
ψr(x) = (cid:88) r
∞ (cid:88) i=0
ψr(⟨βr, [Xl1, ..., XlN ]⟩), cixi. (4) (5)
For these two types of function, we study the following questions:
• Can RNN learn additive concept class functions (4) without the normalized condition with reasonable complexity on the sequence size L?
• Can RNN learn functions in N-variables Concept Class (5) which can not be written as the summation of f (Xl) with reasonable complexity on N and L?
Our Result. We answer the two questions and give a provable generalization error bound. Our results are stated as follows:
Theorem 1 (Informal) For a function F ∗(X1, X2, ..., XL) with the form as in (4) or (5), there is a power series named the complexity C (F ∗) dependent on the Taylor expansion coefficient in (4) and (5). For (4), C (F ∗) is almost-polynomial in L. For (5), when N or l0 = max(l1, .., lN ) − min(l1, .., lN ) is small, C (F ∗) is almost-polynomial in L. Under this definition of complexity C (F ∗),
F ∗ is learnable using RNN with m hidden nodes and ReLU activation in (3) in O(C (F ∗)2) steps with O(C (F ∗)2) samples if m ≥ poly(L, C (F ∗)).
Contribution. We summarize the contributions as follows:
• In this paper, we prove that RNN without normalized condition can efficiently learn some no-table concept classes with both time and sample complexity scaling almost polynomially in the input length L.
• Our results go beyond the “additive” concept class. We prove a novel result that RNN can learn more complex function of the input such as N-variables concept class functions.
And “long range correlation functions” with small N (e.g. N = 2, f (βT [Xl, Xl+l0 ]) ) are learnable with complexity scaling almost polynomially in the input length L and correlation distance l0.
• Technically, we study the “backward correlation” of RNN network. In RNN case, using a crucial observation on the degeneracy of deep network, we show that the “backward corre-lation” 1 m ⟨Backl(xi), Backl(xj)⟩ will decay polynomially rather than exponentially in input length L. This shows the complexity of learning RNN with ReLU activation function is polynomial in the size of input sequence L.
Notions. For two matrices A, B ∈ Rm×n, we define ⟨A, B⟩ = Tr(AT B). We define the asymptotic notations O(·), Ω(·), poly(·) as follows. an, bn are two sequences. an = O(bn) if lim supn→∞ |an/bn| < ∞, an = Ω(bn) if lim inf n→∞ |an/bn| > 0, an = poly(bn) if there is k ∈ N that an = O((bn)k). (cid:101)O(·), (cid:101)Ω(·), (cid:103)poly(·) are notions which hide the logarithmic factors in
O(·), Ω(·), poly(·). || · || and || · ||2 denote the 2-norm of matrices. || · ||1 denote the 1-norm. || · ||F is the Frobenius-norm. || · ||0 is the number of non-zero entries.
For elements Ai.j, Bi,j of symmetric matrix A, B. We abuse the notion Ai.j ⪰ Bi.j to denote
A ⪰ B, i.e. A − B is a positive semidefinite matrix. 3
2 Preliminaries 2.1 Function Complexity
For a analytic function ψ(z), we can write it as ψ(z) = c0 + (cid:80)∞ notion to measure the complexity to learn such functions. i=1 cizi. We define the following
C (ψ, R) = 1 +
∞ (cid:88) i=1 i · |ci|Ri.
CN (ψ, R) = 1 +
∞ (cid:88) i=1
L1.5N C N 1 · (cid:112)CN,i · (i/N )N · |ci|Ri (6) (7) where C1 > 100 is an large absolute constant and CN,i is the largest combination number for n1, n2...nN > 0, n1 + n2 + ...nN = i, i! n1!n2!...nN !
Example 2.1 Arora et al. (2019) Consider ψ(z) = arctan(z/2). Then
ψ(z) = (−1)i−121−2i 2i − 1 z2i−1 (cid:88) i=1 (8)
In this case,
C (ψ, 1) = 1 +
∞ (cid:88) i=1 i · |ci| ≤ 1 +
∞ (cid:88) i=1 21−2i ≤ O(1).
Example 2.2 In the case N = 2, C2,i = i, (i/2)2 ≤ i2. ψ(z) = exp(z)
C2(ψ, 1) ≤ 1 +
∞ (cid:88) i=1
L3C 2 1 πi2.5/i! ≤ O(1) 2.2 Concept Class
For the input sequence {Xl}, we assume Cmin ≤ ||Xl|| ≤ Cmax, for all 1 ≤ l ≤ L and
Cmax/Cmin ∼ C0. Under this condition, we consider two types of target functions with the following form:
Additive Concept Class.
L (cid:88)
Cl(cid:88)
F ∗(x) =
ψl,r(βT l,rXl/||Xl||). r=1
Here for all l, r, ψl,r is analytic and ||βl,r||2 ≤ 1. l=1
We define
C (F ∗) = L3.5
L (cid:88)
Cl(cid:88) l=1 r=1
C (ψl,r, C0
√
L), (9) (10) to be the complexity of the target function.
√
Remark 2.1 If we consider function ψ(βT Xl) and ||Xl|| = 1 for all l, the above complexity will become C (ψ, O(
L)). This is similar with that in Allen-Zhu and Li (2019a) but this complexity requirement is much weaker than that in Allen-Zhu and Li (2019a). For example, the complexity of arctan(z/2) in Allen-Zhu and Li (2019a) is not finite, as shown in Arora et al. (2019).
N-variables Concept Class.
F ∗(x) = (cid:88) r
ψr(⟨βr, [Xl1, ..., XlN ]⟩/
√
N max ||Xln ||). (11)
For all r, ψl,a,r(x, y) is an analytic function ψr(x) = c0 + (cid:80)∞ l0 = max(l1, .., lN ) − min(l1, .., lN ). We define
C (F ∗) = min(L2CN (ψr, C0
√
L), L3.5C (ψr, 2l0C0 i=1 cixi. βr ∈ RdN , ||βr||2 ≤ 1. Let
√
L)). (12) 4
Remark 2.2 The complexity (cid:80)
L) are exponential in N r and l0 respectively. And C (F ∗) is less or equal than both. Thus if either l0 or N is small, C (F ∗) will be polynomial in L. Especially when N is small(e.g. N=2), even if l0 = L − 1, functions with the form f (βT [Xl, Xl+l0]) are still learnable with a low complexity.
L) and (cid:80) r
C (ψr, 2l0 C0
CN (ψr, C0
√
√ 2.3 Results on Positive Definite Matrices and Functions
We say a function ϕ(·, ·) : Rd × Rd → R is positive definite if for all n ∈ N, any {x1, ..., xn} ⊆
Rd, {c1, ..., cn} ⊆ R, (cid:88) cicjϕ(xi, xj) ≥ 0. (13) i,j
The following basic properties in chapter 3 of BergJens et al. (1984) are very useful in our proof.
Proposition 2.1 If ϕ(·, ·) is positive definite function, let matrix M ∈ Rn×n, {x1, ..., xn} ⊆ Rd, and Mi,j = ϕ(xi, xj). Then M is a semi-positive definite matrix.
Proposition 2.2 If ϕ1(·, ·) and ϕ1(·, ·) are positive definite, ϕ(xi, xj) = ϕ1(xi, xj) · ϕ2(xi, xj) is also a positive definite function.
Proposition 2.3 Let ϕ(·, ·) be a positive definite function, and ψ(x) = (cid:80)∞
ψ(ϕ(·, ·)) is also a positive definite function. i=0 cixi, ci ≥ 0. Then
For a positive definite matrix M ∈ Rn×n, there is a result in Arora et al. (2019),
Proposition 2.4 (Section E of Arora et al. (2019).) Let X = (x1, ...xn) ∈ Rd×n and Kp ∈ Rn×n is a matrix with (Kp)i,j = (xT i xj)p. Suppose there is α > 0, such that M ⪰ α2Kp. Let y = ((βT x1)p, ..., (βT xn)p) ∈ Rn. We have (cid:112)yT (M )−1y ≤ ||β||p 2/α. 3 Main Results
Assume there is an unknown data set D = {x, y}. The inputs have the form x = (X1, X2, ...XL) ∈ (Rd)L. ||Xl|| ≤ O(1) for all 1 ≤ l ≤ L. For every input xi, there is a label yi = ±1.
The neural network with input x is h0(x) = ϕ(M0), hl(x) = ϕ(W hl−1 + AXl), f (W , x) = BT hL(x). (14)
Here W ∈ Rm×m, A ∈ Rm×d, B, M0 ∈ Rm. The entries of M0, W and A are respectively i.i.d. generated from N (0, 2
L3·m ). The entries of B are i.i.d. generated from
N (0, 1 m ) and N (0, m ), N (0, 2 2 m ).
The goal of learning RNN is to minimize the population loss:
LD(W ) = E(x,y)∼Dℓ(y · f (W , x)), by optimizing the empirical loss
LS(W ) = 1 n n (cid:88) i=1
ℓ(yi · f (W , xi)), (15) (16) using SGD. Here ℓ(x) = log(1 + exp(−x)) is the cross-entropy loss. Consider the SGD algorithm on this RNN. Let the complexity C ∗ of F ∗(·) be defined in (10) and (12). The 0-1 error for D is
D (W ) = E(x,y)∼D1{y · f (W , x) < 0}. We have:
L0−1
Theorem 2 Assume there is δ ∈ (0, e−1]. Supposing for D = {xi, yi}, there is a function F ∗ belonging to the concept class (9) or (11) such that yi · F ∗(xi) ≥ 1 for all i. Let W k be the output of 5
Algorithm 1: Training RNN with SGD
Input: Data set D, learning rate η.
The entries of W 0, A are i.i.d. generated from N (0, 2 from N (0, 1 for t = 1, 2, 3...n do m ). m ). The entries of B are i.i.d. generated
Randomly sample (xt, yt) from the data set D.
W t = W t−1 − η∇W t−1 ℓ(yt · f (W t−1, xt)). end
Algorithm 1. There is a parameter m∗(n, δ, L, C ∗) = poly(n, δ−1, L, C ∗) such that, with probability at least 1 − δ, if m > m∗(n, δ, L), there exits parameter η = O(1/m) that satisfies 1 n n (cid:88) k=1
L0−1
D (W k) ≤ (cid:101)O[ (C ∗)2 n
] + O( log(1/δ) n
). (17)
Remark 3.1 This theorem induces that, to achieve population 0 − 1 error(rather than empirical loss) being less than ϵ, it is enough to train the network using Algorithm 1 with (cid:101)Ω((L · C ∗)2/ϵ) steps. As defined in section 2.1 and 2.2, when N is small, for the two types of concept class, (C ∗)2 is almost-polynomial in input length L. Thus they can be learned effectively.
Remark 3.2 This theorem can also be generalized to “sequence labeling” loss such as 1 l=1 ℓ(yi · fl(W , xi)) with fl(W , x) = BT hl(x). This is because the matrix n (cid:80)L (cid:80)n i=1
H l i,j = 1 m
⟨∇fl(W , xi), ∇fl(W , xj)⟩ with different l are almost “orthogonal” by a similar argument to (26) in Theorem 6. Then RNN can learn a function fl = sign(F ∗ l (x) belonging to functions in section 2.2. See Remark
G.1 in the supplementary materials. l (x)) with F ∗ 4 Sketch Proof of the Main Theorem
The first step to prove the main theorem 2 is the following generalization of Corollary 3.10 in Cao and Gu (2019).
Theorem 3 Under the condition of Theorem 2, let n samples in the training set be {xi, yi}n
[F ∗(x1), F ∗(x2), ...F ∗(xn)]T . Let H be a matrix with Hi,j = 1
The entries of (cid:102)W are i.i.d. generated from N (0, 2 m ). If there is a matrix H ∞ ∈ Rn×n satisfying i=1. (cid:101)y = (cid:102)W f ( (cid:102)W , xj)⟩. (cid:102)W f ( (cid:102)W , xi), ∇ m ⟨∇ and (cid:112) with probability at least 1 − δ, if m > m∗, (cid:101)yT (H ∞)−1
H + ϵT ϵ ⪰ H ∞ with ||ϵ||F ≤ 0.01/O(C ∗), (18) (cid:101)y ≤ O(C ∗), there exits m∗(n, δ−1, L, C ∗) = poly(n, δ−1, L, C ∗) such that, 1 n n (cid:88) k=1
D (W k) ≤ (cid:101)O[ (cid:101)yT (H ∞)−1
L0−1 n (cid:101)y
] + O( log(1/δ) n
). (19)
Remark 4.1 In order to show Theorem 2 using this theorem, we need to carefully pick out the exponential parts of L. Using the methods in Allen-Zhu et al. (2019c) and Cao and Gu (2019), we can (cid:101)y) ≥ poly(n, L, (cid:112) show that m∗(L, n, (cid:112) (cid:101)y is dealt with by calculating the forward and backward correlation in section 4.1.1 and 4.1.2. (cid:101)y) is enough. (cid:112) (cid:101)yT (H ∞)−1 (cid:101)yT (H ∞)−1 (cid:101)yT (H ∞)−1
The proof of theorem 3 is in fact a combination of the results in Cao and Gu (2019) and Allen-Zhu et al. (2019c). The really matter thing is how large can (cid:112) (cid:101)yT (H ∞)−1 (cid:101)y be. We can show that:
Theorem 4 Under the condition of Theorem 3, with probability at least 1 − δ, there exits matrix
H ∞ satisfying (18) and (cid:113) (cid:101)yT (H ∞)−1 (cid:101)y ≤ O(C ∗). (20) 6
Theorem 2 is a direct corollary of the above two theorems. 4.1 Calculation on Kernel Matrix
The proof of (20) relies on a direct calculation to construct a kernel matrix H ∞. We consider two input xi and xj. Let Xi,l and Xj,l be the l −th input of xi and xj. Let Dl ∈ Rm×m and D′ l ∈ Rm×m be diagonal matrices that, (Dl)k,k = 1{W hl−1(xi) + AXi,l > 0} l)k,k = 1{W hl−1(xj) + AXj,l > 0} (D′
Backl = BDLW · · · Dl+1W, Back′ l = BD′
LW · · · D′ l+1W (21) (22)
Then 1 m
⟨∇ (cid:102)W f ( (cid:102)W , xi), ∇ (cid:102)W f ( (cid:102)W , xj)⟩ = 1 m
⟨Backl(xi) · Dl, Backl′(xj) · D′ l′⟩ · ⟨hl(xi), hl′(xj)⟩ (cid:88) l,l′
Generally Hi,j = 1 limit, we can use some techniques to do the calculation. (cid:102)W f ( (cid:102)W , xi), ∇ m ⟨∇ (23) (cid:102)W f ( (cid:102)W , xj)⟩ is hard to deal with. However, in the m → ∞ 4.1.1 Forward Correlation
Theorem 5 For fixed i, j, under the condition in Theorem 3, with probability at least 1 − exp(−Ω(log2 m)),
√ m) (24)
|⟨hl(xi), hl(xj)⟩ − K l i,j| ≤ O(l16 · log2 m/
And let Ql = (cid:113) (1 + 1
L3 (cid:80)l k=1 ||Xi,k||2) · (1 + 1
L3 (cid:80)l k=1 ||Xj,k||2),
K 1 i,j = Q1 ·
K l i,j = Ql ·
∞ (cid:88) r=0
∞ (cid:88)
µ2 r[(1 + 1
L3 X T i,1Xj,1)/Q1]r
µ2 r({ 1
L3 X T i,lXj,l + K l−1 i,j }/Ql)r r=0
√ (cid:82) ∞ 0 2xhr(x)e− x2 2 dx, hr(x) = 1√ r! (−1)re x2 dxr e− x2 2 dr 2 .
In the above equations, µr = 1√ 2π 4.1.2 Backward Correlation
Theorem 6 For l ̸= l′, with probability at least 1 − exp(−Ω(log2 m)),
| 1 m
⟨Backl(xi) · Dl, Backl′(xj) · D′ l′⟩| ≤ O(
L4 log4 m m1/4
).
For l = l′, there is F l i,j that, with probability at least 1 − exp(−Ω(log2 m)),
| 1 m where
⟨Backl(xi) · Dl, Backl(xj) · D′ l′⟩ − F l i,j| ≤ O(
L4 log4 m m1/4
).
Σ(x) = 1 2
+ arcsin(x)
π
,
F l i,j ⪰ 1
K
Σ({ 1
L3 ⟨Xi,l, Xj,l⟩ + K l−1 i,j }/Ql). and 0 < K ≤ O(1/L4). 7 (25) (26) (27) (28) (29)
Remark 4.2 We should note that this theorem is one of the key differences between this work and the methods in Allen-Zhu and Li (2019a). In fact, we must show that there is a constant K > 0 such that 1 m ⟨Backl(xi), Backl(xj)⟩ − K is still positive definite. However, is K large enough thus 1/K ≥ poly(L) rather than 1/K ≤ exp(−Ω(L)) ? This is not a trivial question. One can only get
K ≥ 1 2L using naive estimation. In Allen-Zhu and Li (2019a), ||AXl|| ≤ ϵx is required to make sure Back′ l = Backl(xi) − Backl(xj) samll. However after k steps of training, we can show the approximation error is roughly O(||Back′||·||W k −W 0||) and ||W k −W 0||F ∼ (cid:112) (cid:101)y ∼
C (F ∗). Thus the dependence of ϵx on C (F ∗) is hard to be dealt with using this method. In this paper, we do not need the normalized condition. Our methods rely on a crucial observation that the function liml→∞ hl(xi)T hl(xj)/(||hl(xi)|| · ||hl(xj||) will degenerate to a constant function. (cid:101)yT (H ∞)−1 4.1.3 Sketch Proof of Theorem 4
In order to estimate the complexity, we use the results in the last subsection and Proposition 2.4,2.2 and 2.3.
Proposition 2.4 shows that, in order to estimate (cid:112) (cid:101)y, we need to show (cid:101)yT (H ∞)−1 l Xl)◦p
H ∞ ⪰ ξp · (X T with ξp > 0 for all p ∈ N, 1 ≤ l ≤ L. Here Xl ∈ Rn×d = [X1,l, X2,l...Xn,l] and
[(X T l Xl)◦p]i,j = {X T i,lXj,l}p. (30) (31)
We will show that, there is a matrix H ∞. With probability at least 1 − δ, Hij = H ∞ for all i, j ∈ [n], and, ij ± O( L4 log4 m m1/4
)
H ∞ i,j ⪰ 1
O(L4)
· QlΣ({ 1
L3 ⟨Xi,l, Xj,l⟩ + K l−1 i,j }/Ql). (32) for all l.
Based on (32), we can show the following results:
For all 1 ≤ l ≤ L and all k
H ∞ i,j ⪰ 1
O(L4)
Σ({K l i,j + 1
L3 X T i,lXj,l}/Ql) ⪰ Ω( 1
L7 )·( 1
O(L)
)k · 1 k2 (X T i,lXj,l)k/(||Xi,l||·Xj,l||)k. (33)
This deduces the complexity for the Additive Concept Class in section 2.1, (cid:113) (cid:101)yT (H ∞)−1 (cid:101)y ≤ O(C ∗). (34)
As for N-Variables Concept Class,
H ∞ i,j ⪰ 1
C N 1 L4 · L2N · CN,p · (p/N )N
· (X T
Xj,r1 + X T i,r1 i,r2
Xj,r2... + X T i,rN
Xj,rN )p/(N · max n (||Xi,rn||) · max (||Xj,rn ||))p n with some large constant C1 > 0. Meanwhile, for any l ≤ L, a < l,
[Xi,l, Xi,l−1, ...Xi,l−a]. We have:
H ∞ i,j ⪰ Ω( 1
L7 ) · ( 1
O(L)
)k · 1 k2 (Z T i,l,aZj,l,a)k/(||Zi,l,a|| · Zj,l,a|| · 2a)k
Then from definition of complexity in section 2.2 and Proposition 2.4, we can prove
Therefore (20) follows. (cid:113) (cid:101)yT (H ∞)−1 (cid:101)y ≤ O(C ∗). 8 (35) let Zi,l,a = (36) (37)
5 Dissicusion
In this paper, we use a new method to avoid the normalized conditions. The main idea is to provide an esitmation for (cid:112) (cid:101)yT (H ∞)−1 (cid:101)y is only explicitly calculated for the two-layer case in Arora et al. (2019). In the RNN cases, the neural tangent kernel matrix involves the depth and the weight sharing in the network and difficult to deal with. (cid:101)y in the RNN case directly. However, the value of (cid:112) (cid:101)yT (H ∞)−1
In Allen-Zhu and Li (2019a), their method is to reduce the RNN case to fL ≈ (cid:88) l
Back(0) · 1⟨W,hl−1⟩+AXl≥0W ∗ · hl−1, which is similar to a summation of L two-layer networks. And this reduction requires the following operations in Allen-Zhu and Li (2019a): 1) Introduce new randomness to keep the independence of rows in the random initialization matrices
W and A at different depths. Then estimate the perturbation. 2) Show the "off-target" Backward Correlation is zero. 3) Estimate the "on target" Backward Correlation by introducing a normalized input sequence x(0). 4) Explicitly construct the approximation.
These steps strongly rely on the normalized condition ||Xl|| ≪ 1 and this is apparently unrealistic.
Instead, we calculate the kernel matrix and we introduce many new estimation to avoid this condition.
We should note that this expression (cid:88) fL ≈ l
Back(0) · 1⟨W,hl−1⟩+AXl≥0W ∗ · hl−1 is additive in itself. Thus the nonlinear interaction between different positions considered in this paper, especially N-variable target functions, cannot be deduced using the from this method. In the previous proof, Allen-Zhu and Li (2019a) is to use these steps to reduce the RNN function to a summation of two-layer networks and ignore the correlation between inputs from different locations and this heavily relies on the normalized condition. In our method, we need to consider the information in Back to show the non-linear correlation between the inputs at different positions and prove N-variable target functions are learnable, while Allen-Zhu and Li (2019a). requires the normalized condition to make sure Back ≈ Back(0) to be roughly a constant. This is one of the most different parts between this work and Allen-Zhu and Li (2019a).
In our case, since we do no use the normalized condition, we must show the polynomial decay of the constant part in Back. As mentioned in Remark 4.2, in our case, it is generally non-trivial to show (cid:112) (cid:101)y ≤ O(C ∗) with C ∗ polynomial in L. Our methods rely on a detailed estimation on (cid:101)yT (H ∞)−1 the degeneracy of long RNN based on Theorem 5. 6