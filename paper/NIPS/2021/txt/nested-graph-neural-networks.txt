Abstract
Graph neural network (GNN)’s success in graph classiﬁcation is closely related to the Weisfeiler-Lehman (1-WL) algorithm. By iteratively aggregating neighboring node features to a center node, both 1-WL and GNN obtain a node representation that encodes a rooted subtree around the center node. These rooted subtree rep-resentations are then pooled into a single representation to represent the whole graph. However, rooted subtrees are of limited expressiveness to represent a non-tree graph. To address it, we propose Nested Graph Neural Networks (NGNNs).
NGNN represents a graph with rooted subgraphs instead of rooted subtrees, so that two graphs sharing many identical subgraphs (rather than subtrees) tend to have similar representations. The key is to make each node representation encode a subgraph around it more than a subtree. To achieve this, NGNN extracts a local subgraph around each node and applies a base GNN to each subgraph to learn a subgraph representation. The whole-graph representation is then obtained by pooling these subgraph representations. We provide a rigorous theoretical analysis showing that NGNN is strictly more powerful than 1-WL. In particular, we proved that NGNN can discriminate almost all r-regular graphs, where 1-WL always fails.
Moreover, unlike other more powerful GNNs, NGNN only introduces a constant-factor higher time complexity than standard GNNs. NGNN is a plug-and-play framework that can be combined with various base GNNs. We test NGNN with different base GNNs on several benchmark datasets. NGNN uniformly improves their performance and shows highly competitive performance on all datasets. 1

Introduction
Graph is an important tool to model relational data in the real world. Representation learning over graphs has become a popular topic of machine learning in recent years. While network embedding methods, such as DeepWalk [1], can learn node representations well, they fail to generalize to whole-graph representations, which are crucial for applications such as graph classiﬁcation, molecule modeling, and drug discovery. On the contrary, although traditional graph kernels [2–7] can be used for graph classiﬁcation, they deﬁne graph similarity often in a heuristic way, which is not parameterized and lacks some ﬂexibility to deal with features.
In this context, graph neural networks (GNNs) have regained people’s attention and become the state-of-the-art graph representation learning tool [8–17]. GNNs use message passing to propagate features between connected nodes. By iteratively aggregating neighboring node features to the center node, GNNs learn node representations encoding their local structure and feature information. These node representations can be further pooled into a graph representation, enabling graph-level tasks such as graph classiﬁcation. In this paper, we will use message passing GNNs to denote this class
∗Corresponding author: Muhan Zhang (muhan@pku.edu.cn).
†Pan Li contributes Sec. 3.3 that proves the Theorem 1 and some implementation ideas. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The two original graphs G1 and G2 are non-isomorphic. G1 is composed of two triangles, while G2 is a hexagon. However, both 1-WL and message passing GNNs cannot differentiate them, since all nodes in the two graphs share identical rooted subtrees at any height (see the rooted subtrees around v1 and v2 in the middle block for example). In comparison, we can discriminate the two graphs by comparing their height-1 rooted subgraphs around any nodes. For example, the height-1 rooted subgraph around v1 is a closed triangle, but the height-1 rooted subgraph around v2 is an open triangle (see the red boxes in the right block). of GNNs based on repeated neighbor aggregation [18], in order to distinguish them from some high-order GNN variants [19–21] where the effective message passing happens between high-order node tuples instead of nodes.
GNNs’ message passing scheme mimics the 1-dimensional Weisfeiler-Lehman (1-WL) algorithm [22], which iteratively reﬁnes a node’s color according to its current color and the multiset of its neighbors’ colors. This procedure essentially encodes a rooted subtree around each node into its ﬁnal color, where the rooted subtree is constructed by recursively expanding the neighbors of the root node. One critical reason for GNN’s success in graph classiﬁcation is because two graphs sharing many identical or similar rooted subtrees are more likely classiﬁed into the same class, which actually aligns with the inductive bias that two graphs are similar if they have many common substructures [23].
Despite this, rooted subtrees are still limited in terms of expressing all possible substructures that can appear in a graph. It is likely that two graphs, despite sharing a lot of identical rooted subtrees, are not similar at all because their other substructure patterns are not similar. Take the two graphs
G1 and G2 in Figure 1 as an example. If we apply 1-WL or a message passing GNN to them, the two graphs will always have the same representation no matter how many iterations/layers we use.
This is because all nodes in the two graphs have identical rooted subtrees across all tree heights.
However, the two graphs are quite different from a holistic perspective. G1 is composed of two triangles, while G2 is a hexagon. The intrinsic reason for such a failure is that rooted subtrees have limited expressiveness for representing general graphs, especially those with cycles.
To address this issue, we propose Nested Graph Neural Networks (NGNNs). The core idea is, instead of encoding a rooted subtree, we want the ﬁnal representation of a node to encode a rooted subgraph (local h-hop subgraph) around it. The subgraph is not restricted to be of any particular graph type such as tree, but serves as a general description of the local neighborhood around a node. Rooted subgraphs offer much better representation power than rooted subtrees, e.g., we can easily discriminate the two graphs in Figure 1 by only comparing their height-1 rooted subgraphs.
To represent a graph with rooted subgraphs, NGNN uses two levels of GNNs: base (inner) GNNs and an outer GNN. By extracting a local rooted subgraph around each node, NGNN ﬁrst applies a base GNN to each node’s subgraph independently. Then, a subgraph pooling layer is applied to each subgraph to aggregate the intermediate node representations into a subgraph representation. This subgraph representation is used as the ﬁnal representation of the root node. Rather than encoding a rooted subtree, this ﬁnal node representation encodes the local subgraph around it, which contains more information than a subtree. Finally, all the ﬁnal node representations are further fed into an outer GNN to learn a representation for the entire graph. Figure 2 shows one NGNN implementation using message passing GNNs as the base GNNs and a simple graph pooling layer as the outer GNN.
One may wonder that the base GNN seems to still learn only rooted subtrees if it is message-passing-based. Then why is NGNN more powerful than GNN? One key reason lies in the subgraph pooling 2
Figure 2: A particular implementation of the NGNN framework. It ﬁrst extracts (copies) a rooted subgraph (height=1) around each node from the original graph, and then applies a base GNN with a subgraph pooling layer to each rooted subgraph independently to learn a subgraph representation. The subgraph representation is used as the root node’s ﬁnal representation in the original graph. Then, a graph pooling layer is used to summarize the
ﬁnal node representations into a graph representation. layer. Take the height-1 rooted subgraphs (marked with red boxes) around v1 and v2 in Figure 1 as an example. Although v1 and v2’s height-1 rooted subtrees are still the same, their neighbors (labeled by 1 and 2) have different height-1 rooted subtrees. Thus, applying a one-layer message passing GNN plus a subgraph pooling as the base GNN is sufﬁcient to discriminate G1 and G2.
The NGNN framework has multiple exclusive advantages. Firstly, it allows freely choosing the base GNN, and can enhance the base GNN’s representation power in a plug-and-play fashion.
Theoretically, we proved that NGNN is more powerful than message passing GNNs and 1-WL by being able to discriminate almost all r-regular graphs (where 1-WL always fails). Secondly, by extracting rooted subgraphs, NGNN allows augmenting the initial features of a node with subgraph-speciﬁc structural features such as distance encoding [24] to improve the quality of the learned node representations. Thirdly, unlike other more powerful graph neural networks, especially those based on higher-order WL tests [19–21, 25], NGNN still has linear time and space complexity w.r.t. graph size like standard message passing GNNs, thus maintaining good scalability. We demonstrate the effectiveness of the NGNN framework in various synthetic/real-world graph classiﬁcation/regression datasets. On synthetic datasets, NGNN demonstrates higher-than-1-WL expressive power, matching very well with our theorem. On real-world datasets, NGNN consistently enhances a wide range of base GNNs’ performance, achieving highly competitive results on all datasets. 2 Preliminaries 2.1 Notation and problem deﬁnition
We consider the graph classiﬁcation/regression problem. Given a graph G = (V, E) where V =
{1, 2, . . . n} is the node set and E ⊆ V × V is the edge set, we aim to learn a function mapping G to its class or target value y. The nodes and edges in G can have feature vectors associated with them, denoted by xi (for node i) and eij (for edge (i, j)), respectively. 2.2 Weisfeiler-Lehman test
The Wesfeiler-Lehman (1-WL) test [22] is a popular algorithm for graph isomorphism checking. The classical 1-WL works as follows. At ﬁrst, all nodes receive a color 1. Each node collects its neighbors’ colors into a multiset. Then, 1-WL will update each node’s color so that two nodes get the same new color if and only if their current colors are the same and they have identical multisets of neighbor colors. Repeat this process until the number of colors does not increase between two iterations.
Then, 1-WL will return that two graphs are non-isomorphic if their node colors are different at some iteration, or fail to determine whether they are non-isomorphic. See [7, 26] for more details. 1-WL essentially encodes the rooted subtrees around each node at different heights into its color representations. Figure 1 middle shows the rooted subtrees around v1 and v2. Two nodes will have the same color at iteration h if and only if their height-h rooted subtrees are the same. 3
3 Nested Graph Neural Network
In this section, we introduce our Nested Graph Neural Network (NGNN) framework and theoretically demonstrate its higher representation power than message passing GNNs. 3.1 Limitations of the message passing GNNs
Most existing GNNs follow the message passing framework [18]: given a graph G, each node’s hidden state ht+1 from its neighbors is updated based on its previous state ht v and the messages mt+1 v v v = Ut(ht ht+1 v, mt+1 v
), where mt+1 v = (cid:88) u∈N (v|G)
Mt(ht v, ht u, evu). (1)
Here Mt, Ut are the message and update functions at time stamp t, evu is the feature of edge (v, u), and N (v|G) is the set of v’s neighbors in graph G. The initial hidden states h0 v are given by the raw node features xv. After T time stamps (iterations), the ﬁnal node representations hT v are summarized into a whole-graph representation with a readout (pooling) function R (e.g., mean or sum): hG = R({hT v |v ∈ G}). (2)
Such a message passing (or neighbor aggregation) scheme iteratively aggregates neighbor information into a center node’s hidden state, making it encode a local rooted subtree around the node. The
ﬁnal node representations will contain both the local structure and feature information around nodes, enabling node-level tasks such as node classiﬁcation. After a pooling layer, these node representations can be further summarized into a graph representation, enabling graph-level tasks. When there is no edge feature and the node features are from a countable space, it is shown that message passing
GNNs are at most as powerful as the 1-WL test for discriminating non-isomorphic graphs [27, 19].
For an h-layer message passing GNN, it will give two nodes the same ﬁnal representation if they have identical height-h rooted subtrees (i.e., both the structures and the features on the corresponding nodes/edges are the same). If two graphs have a lot of identical (or similar) rooted subtrees, they will also have similar graph representations after pooling. This insight is crucial for the success of modern
GNNs in graph classiﬁcation, because it aligns with the inductive bias that two graphs are similar if they have many common substructures. Such insight has also been used in designing the WL subtree kernel [7], a state-of-the-art graph classiﬁcation method before GNNs.
However, message passing GNNs have several limitations. Firstly, rooted subtree is only one speciﬁc substructure. It is not general enough to represent arbitrary subgraphs, especially those with cycles due to the natural restriction of tree structure. Secondly, using rooted subtree as the elementary substructure results in a discriminating power bounded by the 1-WL test. For example, all n-node r-regular graphs cannot be discriminated by message passing GNNs. Thirdly, standard message passing GNNs do not allow using root-node-speciﬁc structural features (such as the distance between a node and the root node) to improve the quality of the learned root node’s representation. We might need to break through such limitations in order to design more powerful GNNs. 3.2 The NGNN framework
To address the above limitations, we propose the Nested Graph Neural Network (NGNN) framework.
NGNN no longer aims to encode a rooted subtree around each node. Instead, in NGNN, each node’s
ﬁnal representation encodes the general local subgraph information around it more than a subtree, so that two graphs sharing a lot of identical or similar rooted subgraphs will have similar representations.
Deﬁnition 1. (Rooted subgraph) Given a graph G and a node v, the height-h rooted subgraph Gh v of v is the subgraph induced from G by the nodes within h hops of v (including h-hop nodes).
To make a node’s ﬁnal representation encode a rooted subgraph, we need to compute a subgraph representation. To achieve this, we resort to an arbitrary GNN, which we call the base GNN of NGNN.
For example, the base GNN can be simply a message passing GNN, which performs message passing within each rooted subgraph to learn an intermediate representation for every node of the subgraph, and then uses a pooling layer to summarize a subgraph representation from the intermediate node representations. This subgraph representation is used as the ﬁnal representation of the root node in 4
the original graph. Take root node w as an example. We ﬁrst perform T rounds of message passing within node w’s rooted subgraph Gh
= Ut(ht w. Let v be any node appearing in Gh
), where mt+1
Mt(ht
= v,Gh w
, mt+1 v,Gh w w. We have ht+1 v,Gh w
, evu).
, ht u,Gh w v,Gh w v,Gh w (cid:88) (3) u∈N (v|Gh w)
Here Mt, Ut are the message and update functions of the base GNN at time stamp t, N (v|Gh w) w, and ht+1 denotes the set of v’s neighbors within w’s rooted subgraph Gh denote v,Gh w node v’s hidden state and message speciﬁc to rooted subgraph Gh w at time stamp t + 1. Note that when node v attends different nodes’ rooted subgraphs, its hidden states and messages will also be different. This is in contrast to standard GNNs where a node’s hidden state and message at time t is the same regardless of which root node it contributes to. For example, ht+1 in Eq. 1 do not depend on any particular rooted subgraph. and mt+1 v,Gh w and mt+1 v v
After T rounds of message passing, we apply a subgraph pooling layer to summarize a subgraph representation hGh from the intermediate node representations {hT
|v ∈ Gh w}. w v,Gh w hw : = hGh w
= R0({hT v,Gh w
|v ∈ Gh w}), (4) where R0 is the subgraph pooling layer. This subgraph representation hGh will be used as root node w’s ﬁnal representation hw in the original graph. Note that the base GNNs are simultaneously applied to all nodes’ rooted subgraphs to return a ﬁnal node representation for every node in the original graph, and all the base GNNs share the same parameters. With such node representations,
NGNN uses an outer GNN to further process and aggregate them into a graph representation of the whole graph. For simplicity, we let the outer GNN be simply a graph pooling layer denoted by R1: hG := R1({hw|w ∈ G}). (5) w
The Nested GNN framework can be understood as a two-level GNN, or a GNN of GNNs—the inner subgraph-level GNNs (base GNNs) are used to learn node representations from their rooted subgraphs, while the outer graph-level GNN is used to return a whole-graph representation from the inner GNNs’ outputs. The inner GNNs all share the same parameters which are trained end-to-end with the outer GNN. Figure 2 depicts the implementation of the NGNN framework described above.
Compared to message passing GNNs, NGNN changes the “receptive ﬁeld” of each node from a rooted subtree to a rooted subgraph, in order to capture better local substructure information. The rooted subgraph is read by a base GNN to learn a subgraph representation. Finally, the outer GNN reads the subgraph representations output by the base GNNs to return a graph representation.
Note that, when we apply the base GNN to a rooted subgraph, this rooted subgraph is extracted (copied) out of the original graph and treated as a completely independent graph from the other rooted subgraphs and the original graph. This allows the same node to have different representations within different rooted subgraphs. For example, in Figure 2, the same node B appears in four different rooted subgraphs. Sometimes it is the root node, while other times it is a 1-hop neighbor of the root node. NGNN enables learning different representations for the same node when it appears in different rooted subgraphs, in contrast to standard GNNs where a node only has one single representation at one time stamp (Eq. 1). Similarly, NGNN also enables using different initial features for the same node when it appears in different rooted subgraphs. This allows us to customize a node’s initial features based on its structural role within a rooted subgraph, as opposed to using the same initial features for a node across all rooted subgraphs. For example, we can optionally augment node B’s initial features with the distance between node B and the root—when node B is the root node, we give it an additional feature 0; and when B is a k-hop neighbor of the root, we give it an additional feature k. Such feature augmentation may help better capture a node’s structural role within a rooted subgraph. It is an exclusive advantage of NGNN and is not possible in standard GNNs. 3.3 The representation power of NGNN
We theoretically characterize the additional expressive power of NGNN (using message passing
GNNs as base GNNs) as opposed to standard message passing GNNs. We focus on the ability to discriminate regular graphs because they form an important category of graphs which standard GNNs cannot represent well. Using 1-WL or message passing GNNs, any two n-sized r-regular graphs will have the same representation, unless discriminative node features are available. In contrast, we prove that NGNN can distinguish almost all pairs of n-sized r-regular graphs regardless of node features. 5
Deﬁnition 2. If the message passing (Eq. 3) and the two-level graph pooling (Eqs. 4,5) are all injective given input from a countable space, then the NGNN is called proper.
A proper NGNN always exists due to the representation power of fully-connected neural networks used for message passing and Deep Set for graph pooling [28]. For all pairs of graphs that 1-WL can discriminate, there always exists a proper NGNN that can also discriminate them, because two graphs discriminated by 1-WL means they must have different multisets of rooted subtrees at some height h, while a rooted subtree is always included in a rooted subgraph with the same height.
Now we present our main theorem.
Theorem 1. Consider all pairs of n-sized r-regular graphs, where 3 ≤ r < (2 log n)1/2. For any small constant (cid:15) > 0, there exists a proper NGNN using at most (cid:100)( 1 log(r−1−(cid:15)) (cid:101)-height rooted subgraphs and (cid:100)(cid:15) log(r−1−(cid:15)) (cid:101)-layer message passing, which distinguishes almost all (1 − o(1)) such pairs of graphs. 2 + (cid:15)) log n log n
We include the proof in Appendix A. Theorem 1 has three implications. Firstly, since NGNN can discriminate almost all r-regular graphs where 1-WL always fails, it is strictly more powerful than 1-WL and message passing GNNs. Secondly, it implies that NGNN does not need to extract log n subgraphs with a too large height (about 1 log (r−1) ) to be more powerful. Moreover, NGNN is 2 already powerful with very few layers, i.e., an arbitrarily small constant (cid:15) times log (r−1) (as few as 1 layer). This beneﬁt comes from the subgraph pooling (Eq. 4), freeing us from using deep base GNNs.
We further conduct a simulation experiment in Appendix D to verify Theorem 1 by testing how well
NGNN discriminates r-regular graphs in practice. The results match almost perfectly with our theory. log n
Although NGNN is strictly more powerful than 1-WL and 2-WL (1-WL and 2-WL have the same discriminating power [20]), it is unclear whether NGNN is more powerful than 3-WL. Our early-stage analysis shows both NGNN and 3-WL cannot discriminate strongly regular graphs with the same parameters [29]. We leave the exact comparison between NGNN and 3-WL to future work. 3.4 Discussion
Base GNN. NGNN is a general plug-and-play framework to increase the power of a base GNN. For the base GNN, we are not restricted to message passing GNNs as described in Section 3.2. For example, we can also use GNNs approximating the power of higher-dimensional WL tests, such as 1-2-3-GNN [19] and PPGN/Ring-GNN [20, 21], as the base GNN. In fact, one limitation of these high-order GNNs is their O(n3) complexity. Using the NGNN framework we can greatly alleviate this by applying the higher-order GNN to multiple small rooted subgraphs instead of the whole graph.
Suppose a rooted subgraph has at most c nodes, then by applying a high-order GNN to all n rooted subgraphs, we can reduce the time complexity from O(n3) to O(nc3).
Complexity. We compare the time complexity of NGNN (using message passing GNNs as base
GNNs) with a standard message passing GNN. Suppose the graph has n nodes with a maximum degree d, and the maximum number of nodes in a rooted subgraph is c. Each message passing iteration in a standard message passing GNN takes O(nd) operations. In NGNN, we need to perform message passing over all n nodes’ rooted subgraphs, which takes O(n · cd). We will keep c small (which can be achieved by using a small h) to improve NGNN’s scalability. Additionally, a small c enables the base GNN to focus on learning local subgraph patterns.
In Appendix B, we discuss some other design choices of NGNN. 4