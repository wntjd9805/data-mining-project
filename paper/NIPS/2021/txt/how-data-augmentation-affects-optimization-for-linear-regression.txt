Abstract
Though data augmentation has rapidly emerged as a key tool for optimization in modern machine learning, a clear picture of how augmentation schedules affect optimization and interact with optimization hyperparameters such as learning rate is nascent. In the spirit of classical convex optimization and recent work on implicit bias, the present work analyzes the effect of augmentation on optimization in the simple convex setting of linear regression with MSE loss.
We ﬁnd joint schedules for learning rate and data augmentation scheme under which augmented gradient descent provably converges and characterize the resulting minimum. Our results apply to arbitrary augmentation schemes, revealing complex interactions between learning rates and augmentations even in the convex setting.
Our approach interprets augmented (S)GD as a stochastic optimization method for a time-varying sequence of proxy losses. This gives a uniﬁed way to analyze learning rate, batch size, and augmentations ranging from additive noise to random projections. From this perspective, our results, which also give rates of convergence, can be viewed as Monro-Robbins type conditions for augmented (S)GD. 1

Introduction
Data augmentation, a popular set of techniques in which data is augmented (i.e. modiﬁed) at every optimization step, has become increasingly crucial to training models using gradient-based optimiza-tion. However, in modern overparametrized settings where there are many different minimizers of the training loss, the speciﬁc minimizer selected by training and the quality of the resulting model can be highly sensitive to choices of augmentation hyperparameters. As a result, practitioners use methods ranging from simple grid search to Bayesian optimization and reinforcement learning [8, 9, 17] to select and schedule augmentations by changing hyperparameters over the course of optimization.
Such approaches, while effective, often require extensive compute and lack theoretical grounding.
These empirical practices stand in contrast to theoretical results from the implicit bias and stochastic optimization literature. The extensive recent literature on implicit bias [15, 29, 32] gives provable guarantees on which minimizer of the training loss is selected by GD and SGD in simple settings, but considers cases without complex scheduling. On the other hand, classical theorems in stochastic optimization, building on the Monro-Robbins theorem in [25], give provably optimal learning rate schedules for strongly convex objectives. However, neither line of work addresses the myriad augmentation and hyperparameter choices crucial to gradient-based training effective in practice.
The present work takes a step towards bridging this gap. We consider two main questions for a learning rate schedule and data augmentation policy:
∗Equal contribution 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
1. When and at what rate will optimization converge? 2. Assuming optimization converges, what point does it converge to?
To isolate the effect on optimization of jointly scheduling learning rate and data augmentation schemes, we consider these questions in the simple convex model of linear regression with MSE loss: 1
N (cid:107)
In this setting, we analyze the effect of the data augmentation policy on the optimization trajectory
Wt of augmented (stochastic) gradient descent2, which follows the update equation 2
F . (cid:107) (1.1)
W X (W ;
) =
−
D
L
Y
Wt+1 = Wt
ηt
W (W ; t)(cid:12) (cid:12)W =Wt
.
∈
D
D
−
∇
L
= (X, Y ) contains N datapoints arranged into data matrices X
Rp×N whose columns consist of inputs xi
Rn×N and
Here, the dataset
∈
Rp. In this context, we
Y take a ﬂexible deﬁnition data augmentation scheme as any procedure that consists, at every step by a randomly augmented variant which we denote by of optimization, of replacing the dataset t = (Xt, Yt). This framework is ﬂexible enough to handle SGD and commonly used augmentations
D such as additive noise [14], CutOut [12], SpecAugment [23], Mixup [35], and label-preserving transformations (e.g. color jitter, geometric transformations [26])).
Rn and outputs yi
D
∈
∈ (1.2)
We give a general answer to Questions 1 and 2 for arbitrary data augmentation schemes. Our main result (Theorem 3.1) gives sufﬁcient conditions for optimization to converge in terms of the learning rate schedule and simple 2nd and 4th order moment statistics of augmented data matrices. When convergence occurs, we explicitly characterize the resulting optimum in terms of these statistics. We then specialize our results to (S)GD with modern augmentations such as additive noise [14] and random projections (e.g. CutOut [12] and SpecAugment [23]). In these cases, we ﬁnd learning rate and augmentation parameters which ensure convergence with rates to the minimum norm optimum for overparametrized linear regression. To sum up, our main contributions are: 1. We analyze arbitrary data augmentation schemes for linear regression with MSE loss, obtaining explicit sufﬁcient conditions on the joint schedule of the data augmentation policy and the learning rate for (stochastic) gradient descent that guarantee convergence with rates in Theorems 3.1 and 3.2. The resulting augmentation-dependent optimum encodes the ultimate effect of augmentation on optimization, and we characterize it in Theorem 3.1.
Our results generalize Monro-Robbins theorems [25] to situations where the stochastic optimization objective may change at each step. 2. We specialize our results to (stochastic) gradient descent with additive input noise (§4) or random projections of the input (§5), a proxy for the popular CutOut and SpecAugment augmentations [12, 23]. In each case, we ﬁnd that jointly scheduling learning rate and augmentation strength is critical for allowing convergence with rates to the minimum norm optimizer. We ﬁnd speciﬁc schedule choices which guarantee this convergence with rates (Theorems 4.1, 4.2, and 5.1) and validate our results empirically (Figure 4.1). This suggests explicitly adding learning rate schedules to the search space for learned augmentations as in
[8, 9], which we leave to future work. 2