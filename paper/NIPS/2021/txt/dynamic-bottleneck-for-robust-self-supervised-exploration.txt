Abstract
Exploration methods based on pseudo-count of transitions or curiosity of dynamics have achieved promising results in solving reinforcement learning with sparse rewards. However, such methods are usually sensitive to environmental dynamics-irrelevant information, e.g., white-noise. To handle such dynamics-irrelevant information, we propose a Dynamic Bottleneck (DB) model, which attains a dynamics-relevant representation based on the information-bottleneck principle.
Based on the DB model, we further propose DB-bonus, which encourages the agent to explore state-action pairs with high information gain. We establish theoretical connections between the proposed DB-bonus, the upper conﬁdence bound (UCB) for linear case, and the visiting count for tabular case. We evaluate the proposed method on Atari suits with dynamics-irrelevant noises. Our experiments show that exploration with DB bonus outperforms several state-of-the-art exploration methods in noisy environments. 1

Introduction
The tradeoff between exploration and exploitation has long been a major challenge in reinforce-ment learning (RL) [35, 50, 58]. Generally, excessive exploitation of the experience suffers from the potential risk of being suboptimal, whereas excessive exploration of novel states hinders the improvement of the policy. A straightforward way to tackle the exploration-exploitation dilemma is to enhance exploration efﬁciency while keeping exploitation in pace. When the extrinsic rewards are dense, reward shaping is commonly adopted for efﬁcient exploration. However, in many real-world applications such as autonomous driving [34], the extrinsic rewards are sparse, making efﬁcient exploration a challenging task in developing practical RL algorithms. The situations become even worse when the extrinsic rewards are entirely unavailable. In such a scenario, the task of collecting informative trajectories from exploration is known as the self-supervised exploration [11].
An effective approach to self-supervised exploration is to design a dense intrinsic reward that motivates the agent to explore novel transitions. Previous attempts include count-based [9] and curiosity-driven
[39] explorations. The count-based exploration builds a density model to measure the pseudo-count of state visitation and assign high intrinsic rewards to less frequently visited states. In contrast, the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
curiosity-driven methods maintain a predictive model of the transitions and encourage the agent to visit transitions with high prediction errors. However, all these methods becomes unstable when the states are noisy, e.g., containing dynamics-irrelevant information. For example, in autonomous driving tasks, the states captured by the camera may contain irrelevant objects, such as clouds that behave similar to Brownian movement. Hence, if we measure the novelty of states or the curiosity of transitions through raw observed pixels, exploration are likely to be affected by the dynamics of these irrelevant objects.
To encourage the agent to explore the most informative transitions of dynamics, we propose a
Dynamic Bottleneck (DB) model, which generates a dynamics-relevant representation Zt of the current state-action pair (St, At) through the Information-Bottleneck (IB) principle [55]. The goal of training DB model is to acquire dynamics-relevant information and discard dynamics-irrelevant features simultaneously. To this end, we maximize the mutual-information I(Zt; St+1) between a latent representation Zt and the next state St+1 through maximizing its lower bound and using contrastive learning. Meanwhile, we minimize the mutual-information I([St, At]; Zt) between the state-action pair and the corresponding representation to compress dynamics-irrelevant information.
Based on our proposed DB model, we further construct a DB-bonus for exploration. DB-bonus measures the novelty of state-action pairs by their information gain with respect to the representation computed from the DB model. We show that the DB-bonus are closely related to the provably efﬁcient
UCB-bonus in linear Markov Decision Processes (MDPs) [1] and the visiting count in tabular MDPs
[3, 22]. We further estimate the DB-bonus by the learned dynamics-relevant representation from the DB model. We highlight that exploration based on DB-bonus directly utilize the information gain of the transitions, which ﬁlters out dynamics-irrelevant noise. We conduct experiments on the Atari suit with dynamics-irrelevant noise injected. Results demonstrate that our proposed self-supervised exploration with DB-bonus is robust to dynamics-irrelevant noise and outperforms several state-of-the-art exploration methods. 2