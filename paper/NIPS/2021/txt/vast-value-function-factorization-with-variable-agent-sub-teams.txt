Abstract
Value function factorization (VFF) is a popular approach to cooperative multi-agent reinforcement learning in order to learn local value functions from global rewards. However, state-of-the-art VFF is limited to a handful of agents in most domains. We hypothesize that this is due to the ﬂat factorization scheme, where the VFF operator becomes a performance bottleneck with an increasing number of agents. Therefore, we propose VFF with variable agent sub-teams (VAST). VAST approximates a factorization for sub-teams which can be deﬁned in an arbitrary way and vary over time, e.g., to adapt to different situations. The sub-team values are then linearly decomposed for all sub-team members. Thus, VAST can learn on a more focused and compact input representation of the original VFF operator. We evaluate VAST in three multi-agent domains and show that VAST can signiﬁcantly outperform state-of-the-art VFF, when the number of agents is sufﬁciently large. 1

Introduction
Many real-world problems can be deﬁned as cooperative multi-agent system (MAS), where multiple autonomous agents collaborate to achieve a common goal like ﬂeet management [20, 21], industry 4.0 [9, 29, 43], or communication networks [26, 51]. Multi-agent reinforcement learning (MARL) seems promising to realize such cooperative MAS by learning local policies for each autonomous agent [3, 27, 37, 40]. Multi-agent credit assignment is an important challenge, where all agents only observe a single global reward, which makes the deduction of individual agent contributions difﬁcult, especially in large MAS with many agents. This can lead to poor policies, since it is unclear which agent policy needs to adapt to what extent in order to improve global MAS behavior [6, 10, 39].
Value function factorization (VFF) via end-to-end deep learning is a popular approach to MARL in order to address the credit assignment problem [31, 32, 36, 39, 44]. A centralized value function is learned from global rewards and factorized into local value functions, which can be used to realize coordinated local policies via multi-armed bandits [32, 40] or local actor-critic learning [30, 38, 45].
Despite the popularity of VFF, most approaches have been only evaluated in domains with a handful of agents. We hypothesize that this is due to the ﬂat factorization scheme of current VFF approaches (Fig. 1a). With an increasing number of agents, the centralized VFF operator becomes a performance bottleneck, where it gets difﬁcult to provide sufﬁciently informative training signal for each agent.
To alleviate this performance bottleneck problem, we propose VFF with variable agent sub-teams (VAST). Instead of directly factorizing a centralized value function for each agent, VAST approximates 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Flat value function factorization for N = 5 agents (b) Factorization for K = 2 agent sub-teams
Figure 1: Illustration of different value function factorization schemes using a factorization operator
Ψ. (a) Flat factorization directly based on local values Qi per agent i ∈ D. (b) Proposed factorization based on K ≤ N = |D| sub-team values QG t,k, which are linearly decomposed into local values Qj per sub-team member j ∈ Gt,k ⊆ D. Each agent sub-team Gt,k is deﬁned by an assignment strategy as explained in Section 4. a factorization for agent sub-teams which can be deﬁned in an arbitrary way and vary over time, e.g., to adapt to different situations. The sub-team values are then linearly decomposed for all sub-team members as illustrated in Fig. 1b. Therefore, VAST can learn on a more focused and compact input representation of the original VFF operator. Our contributions are as follows:
• We formulate VAST and show that VAST maintains decentralizability like state-of-the-art
VFF given any sub-team assignment and depending on the sub-team based VFF operator.
• We propose a meta-gradient approach to optimize sub-team assignments in order to adapt and improve VAST. We also brieﬂy discuss alternative sub-team assignment strategies.
• We empirically evaluate different variants of VAST in three multi-agent domains and show that VAST can signiﬁcantly outperform ﬂat state-of-the-art VFF approaches by alleviating the performance bottleneck problem, when the number of agents is sufﬁciently large. 2