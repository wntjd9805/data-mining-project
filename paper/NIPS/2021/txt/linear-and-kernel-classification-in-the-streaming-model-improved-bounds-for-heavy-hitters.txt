Abstract
We study linear and kernel classiﬁcation in the streaming model. For linear clas-siﬁcation, we improve upon the algorithm of [1], which solves the (cid:96)1 point query problem on the optimal weight vector w∗ ∈ Rd in sublinear space. We ﬁrst give an algorithm solving the more difﬁcult (cid:96)2 point query problem on w∗, also in sublinear space. We then give an algorithm which solves the related (cid:96)2 heavy hitter problem on w∗, in sublinear space and running time. Finally, we give an algorithm which can deterministically solve the (cid:96)1 point query problem on w∗, with sublinear space, improving upon that of [1]. For kernel classiﬁcation, if w∗ ∈ Rdp is the optimal weight vector classifying points in the stream according to their pth-degree polynomial kernel, then we give an algorithm solving the (cid:96)2 point query problem on w∗ in poly( p log d
) space, and an algorithm solving the (cid:96)2 heavy hitter problem in poly( p log d
) space and running time. Note that our space and running time is polynomial in p, making our algorithms well-suited to high-degree polynomial kernels and the Gaussian kernel (approximated by the polynomial kernel of degree p = Θ(log T )). Our algorithms for kernels are in fact a special case of a more general algorithm we give for low-rank tensor inputs.
ε
ε 1

Introduction
We consider logistic regression, and more generally, linear classiﬁcation, in the streaming model.
In our setting, we are given a dataset consisting of T examples (xt, yt), where t ∈ [T ], xt ∈ Rd, yt ∈ {−1, 1}. The examples arrive one by one, and moreover, the nonzero coordinates of each example xt arrive one by one. Our goal is to estimate the weights w∗ ∈ Rd of the optimal linear classiﬁer for these examples. Here, w∗ := argminw∈Rd 2 where (cid:96) is a loss function satisfying certain conditions described in Section 1.3 — the prototypical example of (cid:96) that we consider is the logistic regression loss function — and λ controls the strength of the (cid:96)2 regularization. Finally, we assume that d is very large, and we therefore wish to estimate the weights of w∗ in space that is sublinear in d. This is important both in settings with devices with limited memory constraints, such as routers or sensors on a network, as well as in machine learning problems with many features. Machine learning problems with a very large number of features arise in many natural language processing tasks, for example — one motivation for [1], the precursor to our work, is that the use of n-gram features when analyzing text data can lead to a very large memory cost. 1
Our goal, and that of [1], is to ﬁnd features which are the most useful for classiﬁcation — as pointed out in [1], previously known sketches for compressing classiﬁers do not achieve this goal. t=1 (cid:96)(ytwT xt) + λ 2 (cid:107)w(cid:107)2 (cid:80)T 1
T
Formally, we consider the following problems in this work: 1In [1] it is mentioned that in an experiment on the dataset of [2], “we recorded 47M unique word pairs that co-occur within 5-word spans of text ... [requiring] approximately 560MB of memory.” 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Problem 1 ((cid:96)p Point Query, for p = 1, 2). Let ε ∈ (0, 1). At any time t ∈ [T ] in the stream, given an arbitrary query i ∈ [d], the goal is to output (cid:99)wi such that |(cid:99)wi − w∗,i| ≤ ε(cid:107)w∗(cid:107)p.
Problem 2 ((cid:96)2 Heavy Hitters). Let ε ∈ (0, 1). At any time t ∈ [T ] in the stream, the goal is to output a list L ⊂ [d] of size at most O( 1
ε2 ) such that L contains all indices i ∈ [d] such that |w∗,i| ≥ ε(cid:107)w∗(cid:107)2.
Interpretability is one of the main motivations for the above problem formulations — as argued by [1],
ﬁnding the largest weights in w∗ is equivalent to determining which features are the most important in classiﬁcation. Also, note that (cid:96)2 point query is strictly more difﬁcult than (cid:96)1 point query — to see why, note that in the worst case, (cid:107)w∗(cid:107)1 can be larger than (cid:107)w∗(cid:107)2 by a d factor. Thus, for instance, if an algorithm uses poly(ε−1 log d) space to solve (cid:96)1 point query, then it will use at least dO(1) space to solve (cid:96)2 point query (by replacing (cid:15) with (cid:15)/ d), which is far too large in streaming settings.
√
√ 1.1 Our Contributions (cid:96)2 point query and heavy hitters (via a reduction to turnstile (cid:96)2 point query and heavy hitters)
We give efﬁcient algorithms for solving (cid:96)2 point query and (cid:96)2 heavy hitters. In the approach of [1], a single Countsketch matrix [3] is used to maintain a sketch zt of the weights. zt is updated by online gradient descent according to zt+1 ← (1 − ληt)zt − ηtyt(cid:96)(cid:48)(ytzT t Rxt)Rxt, where R is a Countsketch matrix scaled by 1/ s, where s is the column sparsity of R — by [4], R is a Johnson-Lindenstrauss (JL) transform if s is chosen to be large enough. To estimate the coordinates of w∗, the recovery procedure of [3] is applied to t=1 zt)/T . In addition, [1] only obtains the (cid:96)1 point query guarantee, since the JL property of R is only applied to show that R preserves the inner products between e1, e2, . . . , ed. sz, where z = ((cid:80)T
√
√
To resolve both of these issues, we decouple the JL matrix from the point query/heavy hitters sketch, i.e., we use a JL matrix R, and a separate sketch S for (cid:96)2 point query/heavy hitters in the well-studied turnstile streaming model. First, we maintain zt, which is updated using online gradient descent as in [1]. In addition, we maintain an additional vector (cid:99)wt ∈ Rd, which is updated according to (cid:91)wt+1 ← (1 − ληt)(cid:99)wt − ηtyt(cid:96)(cid:48)(ytzT t Rxt)xt. The motivation for this update is that it is essentially the online gradient descent update wt+1 ← (1 − ληt)wt − ηtyt(cid:96)(cid:48)(ytwT t xt)xt we would perform without any sketching, but we replace (cid:96)(cid:48)(ytwT t Rxt) due to space constraints. We do not have enough space to explicitly maintain (cid:99)wt, but since the updates to (cid:99)wt are additive, we can still give it as the input to a turnstile streaming algorithm for (cid:96)2 point query/heavy hitters. We show that (cid:99)wT = 1 t=1 (cid:99)wt is close to w∗ in (cid:96)2 norm, and thus it sufﬁces to solve (cid:96)2 point query and (cid:96)2 heavy hitters on (cid:99)wT . In summary, we give algorithms for (cid:96)2 point query and heavy hitters with
O(ε−2 log(dT /δ)) space and 1 − δ success probability. t xt) with (cid:96)(cid:48)(ytzT (cid:80)T
T
Deterministic (cid:96)1 point query In addition, we show that (cid:96)1 point query can be solved determin-istically, and with space complexity O(ε−2 log(d)), which is smaller than the O(ε−4 log3(d/δ)) space complexity of [1]. Deterministic sketches are useful as they allow for inputs to be chosen as a function of past responses of the sketching algorithm, and thus provide adversarial robustness
[5]. To obtain a deterministic algorithm, we replace the Countsketch matrix used by [1] with an
ε-incoherent matrix. Here, an ε-incoherent matrix R ∈ Rs×d is one whose columns are almost orthonormal, meaning that for all i (cid:54)= j, |(cid:104)Ri, Rj(cid:105)| ≤ ε, and all columns of R have (cid:96)2 norm 1.
Matrices that are ε-incoherent were previously applied to streaming problems by [6], and can be constructed deterministically. To improve on the space complexity of [1], we change the recovery procedure: to estimate w∗,i, we simply compute (cid:104)Ri, z(cid:105) where z is the compressed weight vector, rather than applying the Countsketch recovery procedure of [3]. (cid:96)2 point query (via a combined JL/point query sketch) Inspired by our deterministic (cid:96)1 point query algorithm, we provide an alternative algorithm for (cid:96)2 point query, for which the space complexity has a smaller dependence on 1/λ. We observe that the sparse JL transform of [4] can be used not only to preserve norms with high probability (i.e., to satisfy the JL lemma) but also to provide an (cid:96)2 point query sketch directly, using a different (cid:96)2 point query recovery procedure. Our procedure does not involve any median based operations. Instead, to estimate w∗,i given the compressed weight s in vector z, we simply compute (cid:104)Ri, z(cid:105). Recall that [1] multiplies the sketching matrix R by order to perform the recovery procedure of [3] — our new procedure also avoids this rescaling, and thus achieves a space complexity of O(ε−2 log(dT /δ)) up to problem-dependent parameters. The space complexity of this algorithm has a smaller dependence on 1/λ compared to our (cid:96)2 point query
√ 2
algorithm discussed above. In addition, we use online gradient descent regret bounds to show that the 1 estimation error of our algorithms involves a term that is proportional to
T 1/4 . Thus, for the error to be at most ε(cid:107)w∗(cid:107)2 or ε(cid:107)w∗(cid:107)1, T must be at least a certain value. Our (cid:96)2 point query algorithm using a combined JL/point query sketch requires T to grow as 1
λ4 , as opposed to our (cid:96)2 point query algorithm which makes a black-box reduction to the turnstile model, which requires T to grow as 1
λ8 .
The main idea of this algorithm is the observation that if w∗ is the optimal weight vector, then (cid:104)Rw∗, Rei(cid:105) is a good estimate of (cid:104)w∗, ei(cid:105) = w∗,i — this motivates our query procedure. Note that this fact is implicit in the guarantees of a JL matrix. This recovery procedure has also been used for turnstile (cid:96)1 point query by [6] (which motivated our deterministic (cid:96)1 point query algorithm above).
The same recovery procedure was also used by [7], in the context of distributed differentially private heavy hitters. To our knowledge, our work is the ﬁrst to use this idea in the setting of (cid:96)2 point query for linear classiﬁcation. Note that for (cid:96)2 point query in the turnstile model, it is preferable to use Countsketch (Countsketch is also used by [1]), since for an update to a single coordinate, the update time with Countsketch is O(log(1/δ)), while the update time when using a sparse JL matrix [4] is O(ε−1 log(1/δ)). However, in the context of linear classiﬁcation, we ﬁnd that using a JL matrix with the recovery procedure (cid:104)Rw∗, Rei(cid:105) ≈ w∗,i reduces the space complexity by a factor of poly(ε−1 log(d/δ)), as long as T = O(d). This is because the sketching matrix already needs to be a
JL matrix in order to preserve certain inner products. In this case, using the Countsketch recovery s where s is the column sparsity of R, which in turn procedure requires scaling z by a factor of requires increasing the accuracy parameter ε of R in order to solve (cid:96)2 point query (or (cid:96)1 point query).
√
Worst-case data order guarantees For all of our algorithms, we do not make assumptions on the order of the xt, unlike [1]. In [1] the pairs in the set {(x1, y1), . . . , (xT , yT )} are required to arrive in the stream in a uniformly random order. The following is given in [1] as a heuristic explanation:
“we believe this condition is necessary to avoid worst-case adversarial orderings of the data points -since the WM-Sketch update at any time step depends on the state of the sketch itself, adversarial orderings can potentially lead to high error accumulation ... Intuitively, it seems reasonable to expect that we would need an ‘average case’ ordering of the stream in order to obtain a similar recovery guarantee to the batch setting." It is perhaps surprising then that we are able to entirely remove this assumption. We do this by showing that instead of using Corollary 1 of [8] (which is used by [1]) we can use an argument from ﬁrst principles based on online gradient descent regret bounds. i=1 x(i,1) t ⊗ . . . ⊗ x(i,p)
Classiﬁcation with tensor inputs We consider a variant of linear classiﬁcation where the inputs xt and the weight vector w∗ are p-th order tensors (i.e., are vectors in Rdp
) and moreover, the xt have rank at most k, meaning xt = (cid:80)k t ⊗ x(i,2)
∈ Rd. This is motivated by applications of tensor regression, for instance in neuroimaging [9, 10], where the covariates have a tensor product structure. Furthermore, the xt may be of low rank in applications — for instance, in the case p = 2, [9] mentions that in [10], tensor regression is performed after principal component analysis is ﬁrst performed on the xt. In such a setting, we wish to obtain (cid:96)2 point query and heavy hitters algorithms with at most a polynomial dependence on log d and 1/ε, and moreover a polynomial dependence on p. To achieve this, we use tensor sketching techniques of [11], which develops a sketching matrix M ∈ Rm×dp
, where m = poly(ε−1p log d), such that M is a JL matrix, and M x⊗p can be computed very efﬁciently for x ∈ Rd (speciﬁcally, in poly(ε−1p log d) · nnz(x) time), without explicitly forming x⊗p. Thus, for (cid:96)2 point query, we can use M in the same way we use the sparse JL matrix of [4] in the combined JL/point query sketch above.
, where the x(i,j) t t
For (cid:96)2 heavy hitters, our algorithm is as follows: (1) for each mode i ∈ [p], we determine the coordinates j ∈ [d] which contribute more than an ε fraction of the (cid:96)2 norm of w∗ — in other words, we want to ﬁnd all j ∈ [d] such that (cid:107)w∗(:, . . . , :, j, :, . . . , :)(cid:107)2 ≥ ε(cid:107)w∗(cid:107)2, where w∗(:, . . . , :, j, :
, . . . , :) consists of those coordinates of w∗ which have index j in the ith mode. This gives us a list
Li ⊂ [d] of size at most O(1/ε2), for each i ∈ [p]. (2) Then, we ﬁnd the (at most O(1/ε2)) indices (i1, . . . , ip) of w∗ in [d]p such that |w∗(i1, i2, . . . , ip)| ≥ ε(cid:107)w∗(cid:107)2. We do step (2) using the Li, by inductively constructing preﬁxes of these coordinates, one mode at a time. For each i ∈ [p], we build an auxiliary data structure which can estimate (cid:107)w(j1, . . . , ji, :, . . . , :)(cid:107)2 for any preﬁx (j1, . . . , ji) of length i — this is also done by using the sketching matrix of [11]. Both our (cid:96)2 point query and (cid:96)2 heavy hitters algorithms for pth-order tensor inputs have poly(ε−1p log(dT /δ)) space and query time, and poly(ε−1p log(dT /δ)) (cid:80) nnz(x(i,j)
) update time, up to problem-dependent parameters. t 3
When the inputs are pth order tensors of low rank, our (cid:96)2 point query and heavy hitters algorithms for tensor inputs give signiﬁcant savings in update time when compared to standard (cid:96)2 point query/heavy hitters algorithms. To see why, note that when the xt are rank-k tensors, the update to (cid:99)wt (deﬁned
⊗ x(i,2) above) is (cid:91)wt+1 ← (1 − ληt)(cid:99)wt − ηtyt(cid:96)(cid:48)(ytzT
. Using a t standard (cid:96)2 heavy hitters algorithm on (cid:99)wt requires explicitly forming x(i,1) t — if the x(i,j) are dense, then standard (cid:96)2 heavy hitters algorithms would require at least dp update time, as opposed to our algorithm, which only has poly(ε−1p log(dT /δ)) · kd update time — even when p = 2, if k is small, then this is a signiﬁcant improvement. t ⊗ . . . ⊗ x(i,p) t ⊗ x(i,2) t M xt) (cid:80)k
⊗ . . . ⊗ x(i,p) i=1 x(i,1) t t t
Kernel classiﬁcation Kernel logistic regression (KLR) is a well-known classiﬁcation method in the
ﬁeld of statistical learning, see e.g., [12] and its many citations. We obtain the ﬁrst results for ﬁnding the large weights of a classiﬁer in the kernel space for the polynomial and Gaussian kernels. A succinct summary of the classiﬁer, such as its list of heavy hitters with their approximate values, is especially important for kernel classiﬁcation, since the dimension of the kernel space can be much larger than d, and in the case of the Gaussian kernel, even inﬁnite. In this setting, for the polynomial kernel, classiﬁcation is done using x⊗p to predict yt — thus, this is a special case of the t setting where xt is a tensor of rank at most k, discussed above. As in [11], we can approximate the
Gaussian kernel via a Taylor expansion, using a polynomial kernel of degree O(log T ). Note that if (i1, i2, . . . , ip) is an index in [d]p and (j1, j2, . . . , jp) is a re-ordering of (i1, i2, . . . , ip), then one may want to consider xi1 . . . xip and xj1 . . . xjp to be the same feature. To get around this, suppose (i1, i2, . . . , ip) has a Hamming distance of at most c from the set of indices of the form (i, i, . . . , i) and it is an ε-heavy hitter when ignoring permutations (formally deﬁned in the appendix). Then, if we apply our algorithms from the low-rank tensor setting with an accuracy of ε/pc/2, (i1, i2, . . . , ip) will be detected as a heavy hitter. An interesting open question is whether this can be done in poly(ε−1p log(dT /δ)) space even when c is equal to p. We leave this question to future work.
Experiments We empirically compare both of our algorithms for (cid:96)2 point query with the WM-Sketch algorithm of [1], 2 where all three algorithms are restricted to certain memory budgets, following the setup of [1]. Our (cid:96)2 point query algorithm that makes use of a combined JL/point query sketch leads to improved performance in estimating w∗ compared to the WM-Sketch algorithm, with signiﬁcantly improved performance for a larger memory budget on the RCV1 dataset [13], though the WM-Sketch algorithm performed better than our black-box reduction-based (cid:96)2 point query algorithm. For smaller memory budgets, these two algorithms appeared to have similar weight recovery performance on the RCV1 dataset, but our other (cid:96)2 point query algorithm using a black-box reduction to turnstile (cid:96)2 point query had much lower error in recovering the top weights.
Using the Top Weights or Compressed Classiﬁers for Classiﬁcation Here we give additional motivation for estimating the top weights of w∗, or applying sketching to classiﬁers. We performed an experiment on the RCV1 dataset [13], which we divided into a training and testing half — we obtained a weight vector w ∈ Rd by using online logistic regression on the training half, and computed the accuracy when using wK for linear classiﬁcation on the testing half (where wK is the K-sparse vector whose entries are the top K entries of w). One noteworthy result of this experiment is that when K = 400, the accuracy on the testing half is 93.9%, while the full weight vector w (which has 41130 nonzero coordinates) achieves 95.7% accuracy. The full details of these experiments are given in Appendix F. We do acknowledge that there are no theoretical guarantees for using only the top K weights for K (cid:28) d, and there may be datasets where using the top K weights of w∗ may not lead to good performance unless K is very large. We give theoretical guarantees for using a compressed classiﬁer, that is, using zT R instead of w∗ where R is a sparse JL matrix and z is the average iterate of sketched online logistic regression: if L = 1 t=1 (cid:96)(ytzT Rxt),
T then |L∗ − (cid:98)L| ≤ ε(cid:107)w∗(cid:107)2 as long as R has O(ε−2H 2 log(dT /δ)) rows (up to problem dependent parameters) and T is a certain value. We give full details in Appendix E. Finally, we note that in d/ε, log(1/δ)/ε2) space, a stream, ﬁnding (cid:96)2 heavy hitters in the turnstile model requires min( by Theorem 4.3 of [14]. In particular, estimating all the coordinates would require poly(d) space, meaning that if we wish to obtain sublinear space complexity in our setting, it is reasonable to expect that we cannot do better than estimating the heavy hitters, without additional assumptions.
∗ xt) and (cid:98)L = 1
T t=1 (cid:96)(ytwT (cid:80)T (cid:80)T
√ 2We use the implementation by the authors of [1] at https://github.com/stanford-futuredata/ wmsketch. Our implementations of our (cid:96)2 point query algorithms are also based on their code. 4
1.2