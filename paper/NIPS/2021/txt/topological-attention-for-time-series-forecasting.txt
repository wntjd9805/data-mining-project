Abstract
The problem of (point) forecasting univariate time series is considered. Most approaches, ranging from traditional statistical methods to recent learning-based techniques with neural networks, directly operate on raw time series observations.
As an extension, we study whether local topological properties, as captured via persistent homology, can serve as a reliable signal that provides complementary information for learning to forecast. To this end, we propose topological attention, which allows attending to local topological features within a time horizon of historical data. Our approach easily integrates into existing end-to-end trainable forecasting models, such as N-BEATS, and, in combination with the latter, exhibits state-of-the-art performance on the large-scale M4 benchmark dataset of 100,000 diverse time series from different domains. Ablation experiments, as well as a comparison to a broad range of forecasting methods in a setting where only a single time series is available for training, corroborate the beneficial nature of including local topological information through an attention mechanism. 1

Introduction
Time series are ubiquitous in science and industry, from medical signals (e.g., EEG), motion data (e.g., speed, steps, etc.) or economic operating figures to ride/demand volumes of transportation network companies (e.g., Uber, Lyft, etc.). Despite many advances in predicting future observations from historical data via traditional statistical [5], or recent machine learning approaches [29, 42, 34, 32, 24, 44], reliable and accurate forecasting remains challenging. This is not least due to widely different and often heavily domain dependent structural properties of time related sequential data.
In this work, we focus on the problem of (point) forecasting univariate time series, i.e., given a length-T vector of historical data, the task is to predict future observations for a given time horizon
H. While neural network models excel in situations where a large corpus of time series is available for training, the case of only a single (possibly long) time series is equally important. The arguably most prominent benchmarks for the former type of forecasting problem are the “(M)akridakis”-competitions, such as M3 [26] or M4 [28]. While combinations (and hybrids) of statistical and machine learning approaches have largely dominated these competitions [28, see Table 4], Oreshkin et al. [29] have recently demonstrated that a pure learning-based model (N-BEATS) attains state-of-the-art performance. Interestingly, the latter approach is simply built from a collection of common neural network primitives which are not specific to sequential data. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Illustration of topological attention, computed on time series observations x1, . . . , xT . The signal is decomposed into a collection of W overlapping windows of size n. For each window, a topological summary, i.e., a persistence barcode Bj, is computed. These local topological summaries are then vectorized (in Re) via a differentiable map VΘ, fed through several transformer encoder layers [41] (implementing a multi-head self-attention mechanism) with positional encoding at the input, and finally mapped to v ∈ RT by an MLP (best-viewed in color).
However, the majority of learning-based approaches directly operate on the raw input signal, implicitly assuming that viable representations for forecasting can be learned via common neural network primitives, composed either in a feedforward or recurrent manner. This raises the question of whether there exist structural properties of the signal, which are not easily extractable via neural network components, but offer complementary information. One prime example1 are topological features, typically obtained via persistent homology [7, 13]. In fact, various approaches [31, 16, 11, 21, 15] have successfully used topological features for time series analysis, however, mostly in classification settings, for the identification of certain phenomena in dynamical systems, or for purely exploratory analysis (see Section 2).
Contribution. We propose an approach to incorporate local topological information into neural forecasting models. Contrary to previous works, we do not compute a global topological summary of historical observations, but features of short, overlapping time windows to which the forecasting model can attend to. The latter is achieved via self-attention and thereby integrates well into recent techniques, such as N-BEATS [29]. Notably, in our setting, computation of topological features (via persistent homology) comes with moderate computational cost, which allows application in large-scale forecasting problems.
Problem statement. In practice, neural forecasting models typically utilize the last T observations of a time series in order to yield (point) forecasts for a given time horizon H. Under this perspective, the problem boils down to learning a function (parametrized as a neural network)
ϕ : RT → RH x (cid:55)→ ϕ(x) = y , (1) from a given collection of inputs (i.e., length-T vectors) and targets (i.e., length-H vectors). Specifi-cally, we consider two settings, where either (1) a large collection of time series is available, as in the M4 competition, or (2) we only have access to a single time series. In the latter setting, a model has to learn from patterns within a time series, while the former setting allows to exploit common patterns across multiple time series. 2