Abstract
Iteratively reweighted least square (IRLS) is a popular approach to solve sparsity-enforcing regression problems in machine learning. State of the art approaches are more efﬁcient but typically rely on speciﬁc coordinate pruning schemes. In this work, we show how a surprisingly simple re-parametrization of IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is able to achieve top performances on a wide range of sparsity (such as Lasso, group Lasso and trace norm regularizations), regularization strength (including hard constraints), and design matrices (ranging from correlated designs to differential operators).
Similarly to IRLS, our method only involves linear systems resolutions, but in sharp contrast, corresponds to the minimization of a smooth function. Despite being non-convex, we show that there are no spurious minima and that saddle points are
“ridable”, so that there always exists a descent direction. We thus advocate for the use of a BFGS quasi-Newton solver, which makes our approach simple, robust and efﬁcient. We perform a numerical benchmark of the convergence speed of our algorithm against state of the art solvers for Lasso, group Lasso, trace norm and linearly constrained problems. These results highlight the versatility of our approach, removing the need to use different solvers depending on the speciﬁcity of the ML problem under study. 1

Introduction
Regularized empirical risk minimization is a workhorse of supervised learning, and for a linear model, it reads min
β∈Rn
R(β) + 1
λ
L(Xβ, y) (Pλ) where X ∈ Rm×n is the design matrix (n being the number of samples and m the number of features),
L : Rm × Rm → [0, ∞) is the loss function, and R : Rn → [0, ∞) the regularizer. Here λ (cid:62) 0 is the regularisation parameter which is typically tuned by cross-validation, and in the limit case λ = 0, (P0) is a constraint problem minβ R(β) under the constraint L(Xβ, y) = 0.
In this work, we focus our attention to sparsity enforcing penalties, which induce some form of structure on the solution of (Pλ), the most celebrated examples (reviewed in Section 2) being the
Lasso, group-Lasso and trace norm regularizers. All these regularizers, and much more (as detailed in Section), can be conveniently re-written as an inﬁmum of quadratic functions. While Section 2 reviews more general formulations, this so-called “quadratic variational form” is especially simple in the case of block-separable functionals (such as Lasso and group-Lasso), where one has
R(β) = min
η∈Rk
+ 1 2 (cid:88) g∈G
||βg||2 2
ηg
+ 1 2 h(η), (1)
∗Department of mathematical sciences, University of Bath, Bath BA2 7AY, UK cmshp20@bath.ac.uk
†CNRS and DMA, Ecole Normale Supérieure, PSL University, 45 rue d’Ulm, F-75230 PARIS cedex 05,
FRANCE, gabriel.peyre@ens.fr 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
where G is a partition of {1, . . . , n}, k = |G| is the number of groups and h : Rk
+ → [0, ∞). An important example is the group-Lasso, where R(β) = (cid:80) g ||βg||2 is a group-(cid:96)1 norm, in which case h(η) = (cid:80) i ηi. The special case of the Lasso, corresponding to the (cid:96)1 norm is obtained when g = {i} for i = 1, . . . , n and k = n. This quadratic variational form (1) is at the heart of the Iterative
Reweighted Least Squares (IRLS) approach, reviewed in Section 1.1. We refer to Section 2 for an in-depth exposition of these formulations.
Sparsity regularized problems (Pλ) are notoriously difﬁcult to solve, especially for small λ, because
R is a non-smooth function. It is the non-smoothness of R which forces the solutions of (Pλ) to belong to low-dimensional spaces (or more generally manifolds), the canonical example being spaces of sparse vectors when solving a Lasso problem. We refer to Bach et al. [2011] for an overview of sparsity-enforcing regularization methods. The core idea of our algorithm is that a simple re-parameterization of (1) combined with a bi-level programming (i.e. solving two nested optimization problems) can turn (Pλ) into a smooth program which is much better conditioned, and can be tackled using standard but highly efﬁcient optimization techniques such as quasi-Newton (L-BFGS). Indeed, by doing the change of variable (vg, ug) (cid:44) (
ηg, βg/ f (v) (cid:44) min u∈Rn
ηg) in (1), (Pλ) is equivalent to f (v) where
G(u, v) (2)
√
√ min v∈Rk
G(u, v) (cid:44) 1 2 h(v (cid:12) v) + 1 2
||u||2 + 1
λ
L(X(v (cid:12)G u), y). (3)
Throughout, we deﬁne (cid:12) to be the standard Hadamard product and for v ∈ Rk and u ∈ Rn, we deﬁne v (cid:12)G u ∈ Rn to be such that (v (cid:12)G u)g = vgug. Provided that v (cid:55)→ h(v (cid:12) v) is differentiable and L(·, y) is a convex, proper, lower semicontinuous function, the inner minimisation problem has a unique solution and f is differentiable. Moreover, in the case of the quadratic loss
L(z, y) (cid:44) 1 2, the gradient of f can be computed in closed form, by solving a linear system of dimension m or n. This paper is thus devoted to study the theoretical and algorithmic implications of this simple twist on the celebrated IRLS approach. 2 ||z − y||2
Comparison with proximal gradient To provide some intuition about the proposed approach, the ﬁgure on the right contrasts the iterations of a gra-dient descent on f and of the iterative soft thresholding algorithm (ISTA) on the Lasso. We consider a random Gaussian matrix X ∈ R10×20 with
λ = ||X (cid:62)y||∞/10. The ISTA trajectory is non-smooth when some feature crosses 0. In particular, if a coefﬁcient (such as the red one on the ﬁgure) is initialized with the wrong sign, it takes many iteration for ISTA to ﬂip sign.
In sharp contrast, the gradient ﬂow of f does not exhibit such a singularity and exhibits a smooth geometric convergence. We refer to the appendix for an analysis of this phenomenon.
Contributions Our main contribution is a new versatile algorithm for sparse regularization, which applies standard smooth optimization methods to minimize the function f in (2). We ﬁrst propose in
Section 2 a generic class of regularizers R that enjoy a quadratic variational form. This section recaps existing results under a common umbrella and shows the generality of our approach. Section 3.1 then gathers the theoretical analysis of the method, and in particular the proof that while being non-convex, the function f has no local minimum and only “ridable” saddle points. As a result, one can guarantee convergence to a global minimum for many optimisation schemes, such as gradient descent with random perturbations [Lee et al., 2017, Jin et al., 2017] or trust region type methods [Pascanu et al., 2014]. Furthermore, for the case of the group Lasso, we show that f is an inﬁnitely differentiable function with uniformly bounded Hessian. Consequently, standard solvers such as Newton’s method/BFGS can be applied and with a superlinear convergence guarantee.
Section 4 performs a detailed numerical study of the method and benchmarks it against several popular competing algorithms for Lasso, group-Lasso and trace norm regularization. Our method is consistently amongst the best performers, and is in particular very efﬁcient for small values of λ, and can even cope with the constrained case λ = 0. 1.1