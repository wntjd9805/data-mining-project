Abstract
Various graph contrastive learning models have been proposed to improve the per-formance of learning tasks on graph datasets in recent years. While effective and prevalent, these models are usually carefully customized. In particular, although all recent researches create two contrastive views, they differ greatly in view aug-mentations, architectures, and objectives. It remains an open question how to build your graph contrastive learning model from scratch for particular graph learning tasks and datasets. In this work, we aim to ﬁll this gap by studying how graph information is transformed and transferred during the contrastive learning process and proposing an information-aware graph contrastive learning framework called
InfoGCL. The key point of this framework is to follow the Information Bottleneck principle to reduce the mutual information between contrastive parts while keeping task-relevant information intact at both the levels of the individual module and the entire framework so that the information loss during graph representation learning can be minimized. We show for the ﬁrst time that all recent graph contrastive learning methods can be uniﬁed by our framework. We empirically validate our theoretical analysis on both node and graph classiﬁcation benchmark datasets, and demonstrate that our algorithm signiﬁcantly outperforms the state-of-the-arts. 1

Introduction
Inspired by their success in the vision and language domains, contrastive learning methods have been wildly adopted by recent progress in graph learning to improve the performance of a variety of tasks [42, 13, 22]. In a nutshell, these methods typically learn representations by creating two augmented views of a graph and maximizing the feature consistency between the two views. Inheriting the advantages of self-supervised learning, contrastive learning relieves graph representation learning from its reliance on label information in graph domain, where label information can be very costly or even impossible to collect while unlabeled/partially labeled data is common, such as chemical graph data [28]. Graph contrastive learning methods have achieved similar (and even better) performance as compared to the equivalent methods trained with labels on benchmark graph datasets [42, 13, 6].
Despite being effective and prevalent, existing graph contrastive learning models differ mostly in augmented view design, encoding architecture, and contrastive objective (refer to Table 1 in Appendix for more comparisons). For a learning task, it usually requires a substantial degree of domain expertise to carefully design and customize these modules for the speciﬁc dataset. For example, while both DGI [33] and InfoGraph [28] seek to obtain graph representations by maximizing the mutual information between patch-level and graph-level representations, they adopt different graph encoders,
GCN [15] and GIN [38] respectively. mvgrl [13] applies graph diffusion convolution to construct the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Graph contrastive learning approaches consist of three stages: 1) a graph G undergoes view augmentation, qi(·), qj(·), to obtain two semantically similar views, vi, vj. 2) the two views are fed into view encoder networks, fi(·), fj(·), to extract latent representations, zi, zj. 3) the feature consistency between representations is maximized to optimize the objective function based on contrastive mode (ci(·), cj(·)), where ci(·), cj(·) are aggregation operations applied to representations. augmented view, while GCC [22] and GRACE [45] adopt subgraph sampling and graph perturbation, respectively.
The main question this paper attempts to answer is: how to perform contrastive learning for your learning tasks on speciﬁc graph datasets? However, answering this question is challenging. First, contrastive learning consists of multiple components, such as view augmentation and information encoding. For each of them, there are various choices. Numerous variations make it difﬁcult to design models that are both robust and efﬁcient. Existing graph contrastive learning approaches are carefully designed for different learning tasks on different datasets, however, none of them studies the guiding principles for choosing the best components. Second, graph data has unique properties that distinguish it from other types of data, such as rich structural information and highly diverse distribution [33, 28, 11, 10]. Thus, it is desirable to design the contrastive learning model that ﬁts your graph data properties, even without any domain knowledge of the data.
We propose to address these challenges via Information Bottleneck (IB) [31], which provides a crucial principle for representation learning. Speciﬁcally, IB encourages the representations to be maximally informative about the target in the downstream task, which helps keep task-relevant information.
Concurrently, IB discourages the representation learning from acquiring the task-irrelevant informa-tion from the input data, which is related to the idea of minimal sufﬁcient statistics [27]. However, different from the typical representation learning, there are two information ﬂows involved in the two augmented views in contrastive learning. Therefore, we extend the previous IB work [37, 43] and propose InfoGCL, an information-aware contrastive learning framework for graph data.
To study how information is transformed and transferred, we decouple a typical graph contrastive learning model into three sequential modules (as shown in Figure 1): view augmentation, view encoding, and representation contrasting. We further formalize how to ﬁnd the optimal of the three modules into three optimization problems. To build the optimal graph contrastive learning model for the particular dataset and task, we argue that it is necessary and sufﬁcient to minimize the mutual information between contrastive representations while maximizing task-relevant information at the levels of both individual module and entire framework. Our work is also motivated by the InfoMin theory [30], which suggests that a good set of views for contrastive learning in the vision domain should share the minimal information necessary to perform well at the downstream task. Beyond view selection, our work extends InfoMin to suggest principles of selecting view encodings and contrastive modes for graph learning considering the unique properties of graph data.
We suggest practically feasible principles to ﬁnd the optimal modules in graph contrastive learning and show that all recent graph contrastive learning methods can be uniﬁed by these principles: i) the augmented views should contain as much task-relevant information as possible, while they should share as little information as possible; ii) the view encoder should be task-relevant and simple as much as possible; iii) the contrastive mode should keep task-relevant information as much as possible after contrasting. Besides, we also investigate the role of negative samples in graph contrastive learning and argue that negative samples are not necessarily required, especially when graph data 2
is not extremely sparse. Our proposed method, InfoGCL, is validated on a rich set of benchmark datasets for both node-level and graph-level tasks, where we analyze its ability to capture the unique structural properties of the graph data. The results demonstrate that our algorithm achieves highly competitive performance with up to 5.2% relative improvement in accuracy on graph classiﬁcation task and competitive results on node classiﬁcation task over the state-of-the-art unsupervised methods. 2