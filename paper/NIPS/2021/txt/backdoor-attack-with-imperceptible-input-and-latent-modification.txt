Abstract
Recent studies have shown that deep neural networks (DNN) are vulnerable to various adversarial attacks. In particular, an adversary can inject a stealthy backdoor into a model such that the compromised model will behave normally without the presence of the trigger. Techniques for generating backdoor images that are visually imperceptible from clean images have also been developed recently, which further enhance the stealthiness of the backdoor attacks from the input space. Along with the development of attacks, defense against backdoor attacks is also evolving. Many existing countermeasures found that backdoor tends to leave tangible footprints in the latent or feature space, which can be utilized to mitigate backdoor attacks.
In this paper, we extend the concept of imperceptible backdoor from the input space to the latent representation, which signiﬁcantly improves the effectiveness against the existing defense mechanisms, especially those relying on the distinguishabil-ity between clean inputs and backdoor inputs in latent space. In the proposed framework, the trigger function will learn to manipulate the input by injecting imperceptible input noise while matching the latent representations of the clean and manipulated inputs via a Wasserstein-based regularization of the corresponding empirical distributions. We formulate such an objective as a non-convex and con-strained optimization problem and solve the problem with an efﬁcient stochastic alternating optimization procedure. We name the proposed backdoor attack as
Wasserstein Backdoor (WB), which achieves a high attack success rate while being stealthy from both the input and latent spaces, as tested in several benchmark datasets, including MNIST, CIFAR10, GTSRB, and TinyImagenet. 1

Introduction
In the past years, deep neural network (DNN) has successfully transformed many technological ﬁelds, such as object classiﬁcation [26, 20], face recognition [31, 1], autonomous driving [53], security applications [19, 3], etc. Meanwhile, due to the underlying black-box nature, its security and privacy implications have also raised serious concerns recently. Efforts in the research community have exposed the vulnerability of DNN classiﬁers to various attacks [50, 41, 33]. For instance, adversarial examples leverage the difference between the classiﬁer and human to misclassify speciﬁc inputs by adding imperceptible perturbations without altering the model [17]. Such attacks during the inference phase are categorized as evasion attacks [27, 5]. On the other hand, poisoning attacks attempt to inject malicious data points or manipulate the training process to either degrade the model accuracy [37, 45, 60] or cause misclassiﬁcation for speciﬁc inputs (a.k.a. backdoor attacks) [8, 36, 34, 18].
In general, backdoor attacks aim at injecting a malicious behavior into a DNN model so that the model would perform normally on clean inputs but yield misclassiﬁcation in the presence of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
backdoor trigger (e.g., a speciﬁc pattern such as a small square [18]). Later on, many works adopt the concepts and techniques in adversarial examples to improve the stealthiness of the trigger against human observers [34, 2, 35]. Recent works have demonstrated more powerful backdoor attacks that are capable of mounting attacks with visual indistinguishable backdoor images [29, 55, 59, 39, 13].
For instance, WaNet [39] generates backdoor images with warping transformation to minimize input difference while LIRA [13] generates backdoor images with imperceptible conditional noise addition, resulting in much stealthier triggers.
To alleviate the threats originated from the ever-growing powerful backdoor attacks, several categories of countermeasures have been developed. One promising direction for backdoor detection entails identifying backdoor images by characterizing the distinguishable dissimilarity in the feature or latent representation between backdoor images and clean images [6, 54, 42, 47, 52]. These methods rely on the assumption that the injected backdoor would leave a noticeable ﬁngerprint in the latent space. For example, activation clustering [6] and spectral signature [54] detect malicious samples by inspecting the clusters of the latent space and the spectrum of the covariance of latent representations, respectively. Thus, a stronger adaptive backdoor attack should also ensure its stealthiness from the latent space.
In this paper, we present a novel methodology for a backdoor attack that is imperceptible from both the input and latent spaces. We extend the concept of generating imperceptible backdoor triggers to the latent space by minimizing the Wasserstein distance between the latent representations of the clean and backdoor data, which signiﬁcantly improves the effectiveness against the existing defense mechanisms, especially those aforementioned that rely on the distinguishability in latent space. We name the proposed method Wasserstein Backdoor, or WB. Our technical contributions are summarized below:
• We propose a non-convex, constrained optimization problem, which learns to poison the classiﬁer with a backdoor whose trigger is visually imperceptible in the input space and whose poisoned samples have indistinguishable latent distribution to the latent distribution of the clean samples. The latent constraint is formulated via a variant of Wasserstein distance, called sliced-Wasserstein distance [24], between the two sets of clean and backdoor data.
• We then develop an efﬁcient estimation of the sliced-Wasserstein distance by exploiting the discriminant directions of the trained classiﬁer, instead of randomly sampling from the unit sphere. The proposed distance is a valid distance metric and requires signiﬁcantly less computation, while yielding a better estimate than the existing calculations of the sliced-Wasserstein distance.
• Finally, we demonstrate the superior attack performance of the proposed method and its robustness against several representative defense mechanisms. Speciﬁcally, we show that the proposed method outperforms the state-of-the-art attacks in terms of latent indistinguishabil-ity, while maintaining similar attack success rates and input indistinguishability.
The rest of the paper is organized as follows. We review the background and related work in Section 2.
In Section 3, we deﬁne the threat model. Section 4 presents the details of the proposed methodology.
We evaluate the performance and compare to prior works in Section 5. Finally, Section 6 presents remarks and concludes this paper. We present more details about experimental settings and results as well as supporting proofs in the supplementary material. 2