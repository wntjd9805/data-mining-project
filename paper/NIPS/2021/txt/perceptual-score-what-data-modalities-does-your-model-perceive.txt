Abstract
Machine learning advances in the last decade have relied signiﬁcantly on large-scale datasets that continue to grow in size. Increasingly, those datasets also contain different data modalities. However, large multi-modal datasets are hard to annotate, and annotations may contain biases that we are often unaware of. Deep-net-based classiﬁers, in turn, are prone to exploit those biases and to ﬁnd shortcuts. To study and quantify this concern, we introduce the perceptual score, a metric that assesses the degree to which a model relies on the different subsets of the input features, i.e., modalities. Using the perceptual score, we ﬁnd a surprisingly consistent trend across four popular datasets: recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors. This trend is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions.
We hope to spur a discussion on the perceptiveness of multi-modal models and also hope to encourage the community working on multi-modal classiﬁers to start quantifying perceptiveness via the proposed perceptual score. 1

Introduction
Machine learning advances over the last decade are remarkable. Challenges that seemed daunting merely ten years ago are now a breeze, and new applications that we barely dared to dream about seem achievable within the next few years. Indeed, accuracy metrics on tasks like visual question answering and reasoning suggest signiﬁcant improvements.
Reported improvements are to a large extent due to the availability of large datasets [1–3], com-putational performance advances, e.g., for GPUs, and a better understanding about how to encode inductive biases into deep-nets, e.g., by using rectiﬁed linear units [4], normalization [5], skip con-nections [6], transformers [7], etc. However, importantly, developed deep-net architectures are not guaranteed to solve a given task. There is a chance that they may instead exploit dataset biases.
This concern is surely in part due to non-robust training techniques, and a plethora of methods improve classiﬁer robustness [8–10]. However, datasets play an important role in controlling the extracted bias as well. For instance, if correct answers in a question-answering task are signiﬁcantly shorter than incorrect ones, classiﬁer training should not use answer length as a cue. Although this seems reasonable, for audio-visual scene aware dialog, Schwartz et al. [11] ﬁnd for example that in many cases the question alone is sufﬁcient to generate a scene-aware dialog response, avoiding the need to look at the video. Hence, in order to assess the suitability of a classiﬁer, we need to understand how much it relies on different data modalities.
To quantify how much a classiﬁer relies on its different input modalities, we introduce the perceptual score. The perceptual score assesses the degree to which a model relies on a modality. To do so the perceptual score permutes the features of a modality across samples in the test set after the classiﬁer 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
(a) Visual question answering (VQA) data. Modality perceptiveness is measured through a permutation test. (b) Progress of VQA-CP models.
Figure 1: Multi-modal datasets often have undesired biases: (a) To identify those biases we suggest the perceptual score as a new metric. It assesses the change in prediction when a model’s input for some modalities is permuted during testing. If the classiﬁer output remains identical despite permutation, a model doesn’t perceive the modality. (b) Using the perceptual score we identify that recent progress of VQA models may not be entirely due to better reasoning. was trained, as illustrated in Fig. 1a. If the classiﬁer’s performance drops to or below chance level, the perceptual score is high. This intuitively applies to single-modality models too: randomly permuting test data and labels after training results in chance-level classiﬁcation accuracy.
Using the perceptual score, we ﬁnd a surprisingly consistent trend across ﬁve popular datasets (VQA,
VQA-CP, VisDial, SocialIQ, DISCO): recent, more accurate state-of-the-art multi-modal models for visual question-answering or visual dialog tend to perceive the visual data less than their predecessors (see Fig. 1b). This trend is concerning as answers are hence increasingly inferred from textual cues only. Using the perceptual score also helps to analyze model biases by decomposing the score into data subset contributions. For example, the perception of an image and question varies depending on the question type. None of the recent VQA-CP models showed high image perception scores for
‘number’-type questions. A surprisingly low image perception score is obtained for the state-of-the-art model when confronted with ‘yes/no’ questions.
We hope the perceptual score spurs a discussion regarding the perceptiveness of multi-modal models and we also hope to encourage the community working on multi-modal classiﬁers to start quantifying perceptiveness of models.
Our contributions:
• We propose the perceptual score, a simple yet effective method for assessing the perceptive-ness of multi-modal models towards a modality.
• Our experiments span multiple datasets and models. We ﬁnd that multi-modal models tend to ignore some modalities while taking shortcuts.
• Subsequently, we investigate the sources of bias on popular multi-modal datasets such as VQA-CP and SocialIQ: We ﬁnd that SocialIQ is biased by sentiment, and bias in the
VQA-CP model results from shifting the training priors to more closely resemble those in the test set. 2