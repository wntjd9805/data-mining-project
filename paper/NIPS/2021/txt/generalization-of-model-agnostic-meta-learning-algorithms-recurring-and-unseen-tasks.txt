Abstract
In this paper, we study the generalization properties of Model-Agnostic Meta-Learning (MAML) algorithms for supervised learning problems. We focus on the setting in which we train the MAML model over m tasks, each with n data points, and characterize its generalization error from two points of view: First, we assume the new task at test time is one of the training tasks, and we show that, for strongly convex objective functions, the expected excess population loss is bounded by O(1/mn). Second, we consider the MAML algorithm’s generalization to an unseen task and show that the resulting generalization error depends on the total variation distance between the underlying distributions of the new task and the tasks observed during the training process. Our proof techniques rely on the connections between algorithmic stability and generalization bounds of algorithms.
In particular, we propose a new deﬁnition of stability for meta-learning algorithms, which allows us to capture the role of both the number of tasks m and number of samples per task n on the generalization error of MAML. 1

Introduction
In several machine learning problems, it is of interest to design algorithms that can be adjusted based on previous experiences and tasks to perform better on a new task.
In particular, meta-learning algorithms achieve such a goal through various approaches, including ﬁnding a proper meta-initialization for the new task [1–3], updating the model architecture [4–6], or learning the parameters of optimization algorithms [7, 8].
A popular meta-learning framework that has shown promise in practice is Model-Agnostic Meta-Learning (MAML), which was ﬁrst introduced in [1]. MAML algorithm uses available training data on a number of tasks to come up with a meta-initialization that performs well after it is slightly updated at test time with respect to the new task. In other words, unlike standard supervised learning, in which we aim to ﬁnd a model that generalize well to a new task without any adaptation step, in
MAML our goal is to ﬁnd an initial model for learning a new task when we have access to limited labeled data for that task to run one (or a few) step(s) of stochastic gradient descent (SGD).
As shown in Fig. 1, in MAML we are given m tasks with m corresponding datasets {Si}m i=1 in the training phase. Once the model is trained (w∗ train), a new task is revealed at test time for which we have access to K labeled samples drawn from Dtest. We use these labeled samples of the new task to 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
update the trained model by running a step of SGD leading to a new model for the test task (w∗ new).
We ﬁnally evaluate the performance of the updated model over the test task, denoted by Ltest(w∗ new).
MAML and its variants have been extensively studied over the past few years from both empiri-cal and theoretical point of view [2, 9–16]. In particular, [13] provided convergence guarantees for MAML algorithm under the assumption that access to fresh samples at any round of the training stage is possible, and [15] extended this results to the case that multiple gradient steps can be performed at test time. However, one shortcoming of such analysis is that, at training stage, we often do not have access to fresh samples at every iteration.
Instead, we have access to a large set of realized samples and we typically do multiple passes over the data points during the training stage.
Training stage
Sm ∼ pm
S1 ∼ p1
. . .
. . . train w∗
Hence, it is essential to come up with a novel analysis that addresses this issue by characterizing the training error and generalization error of MAML separately. In this paper, we accomplish this goal and showcase the role of different problem parameters in the generalization error of MAML.
Speciﬁcally, we assume that we are given m supervised learning tasks, with (possibly different) underlying distri-butions p1, . . . pm, where for each task we have access to n samples1. As we measure the performance of a model by its loss after one step of SGD adaptation with K sam-ples, the problem that one can solve in the training phase is minimizing the average loss, over all given m tasks and their n samples, after one step of SGD with K samples.
This empirical loss can be considered as a surrogate for the desired expected loss (with respect to tasks data) over all m tasks. Here, we focus on the case that MAML is used to solve this empirical minimization problem, and our goal is to quantify the test error of MAML output. To tackle this problem, we ﬁrst brieﬂy revisit the results from the optimization literature to bound the training error of MAML, assuming that the loss functions are strongly convex. We next turn to the main focus of our paper which is the generalization properties of MAML. More speciﬁcally, we address the following questions:
Figure 1: MAML framework
Test stage
Dtest ∼ ptest train, Dtest)
Ltest(w∗
∇ ˆL(w∗ new) w∗ new
• If one of the m given tasks recurs uniformly at random at test time, then how well (in expectation) would the trained model perform after adaptation with SGD over the fresh samples of that task?
In other words, having training error minimized, what would be the generalization error and our guarantee on test error? Here, we show that for strongly convex objective functions, we could achieve a generalization error that decays at O(1/mn). Our analysis builds on the connections between algorithmic stability and generalization of the output of algorithms. While this relation is well-understood in classic statistical learning [17, 18], here we propose a novel stability deﬁnition for meta-learning algorithms which allows us to restore such connection for our setting.
• Assuming that the task at test time is NOT one of the m tasks at training, how would the model perform on that task after the adaptation step? We answer this question by focusing on the case that the revealed task at the test time is a new unseen task with underlying data distribution pm+1, and formally characterizing the generalization error of MAML in this case. We show that when the task at test time is new, the generalization error also depends on the total variation distance between pm+1 and p1, . . . , pm.