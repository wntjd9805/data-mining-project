Abstract
Recently, a new trend of exploring sparsity for accelerating neural network training has emerged, embracing the paradigm of training on the edge. This paper proposes a novel Memory-Economic Sparse Training (MEST) framework targeting for accu-rate and fast execution on edge devices. The proposed MEST framework consists of enhancements by Elastic Mutation (EM) and Soft Memory Bound (&S) that ensure superior accuracy at high sparsity ratios. Different from the existing works for sparse training, this current work reveals the importance of sparsity schemes on the performance of sparse training in terms of accuracy as well as training speed on real edge devices. On top of that, the paper proposes to employ data efficiency for further acceleration of sparse training. Our results suggest that unforgettable examples can be identified in-situ even during the dynamic exploration of sparsity masks in the sparse training process, and therefore can be removed for further training speedup on edge devices. Comparing with state-of-the-art (SOTA) works on accuracy, our MEST increases Top-1 accuracy significantly on ImageNet when using the same unstructured sparsity scheme. Systematical evaluation on accuracy, training speed, and memory footprint are conducted, where the proposed MEST framework consistently outperforms representative SOTA works. Our codes are publicly available at: https://github.com/boone891214/MEST. 1

Introduction
To promote the broader applications of deep learning on the edge, a surge of research efforts have been devoted to removing the over-parameterization in neural networks for accelerated inference.
Specifically, existing works have explored various strategies such as heuristics-based pruning [1, 2], regularization-based pruning [3, 4], and recently prevailing network architecture search [5, 6, 7, 8].
Recently, a new trend of exploring sparsity for training acceleration of neural networks has emerged to embrace the promising training-on-the-edge paradigm. The first works in this direction use the pruning-at-initialization approach such as SNIP [9] and GraSP [10] that first obtains a fixed sparse model structure and then follows with a traditional training process. However, the whole process is still computation- and memory-intensive, and therefore not compatible with the end-to-end edge training paradigm. Such a sparse training methodology with the pre-fixed structure also faces the problem of compromised accuracy.
† These authors contributed equally. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Furthermore, sparse training with dynamic spar-sity mask such as SET [14], DeepR [15], DSR
[13], RigL [11], and SNFS [12] have been pro-posed, showing great potential towards end-to-end edge training. Specifically, they start with a sparse model structure picked intuitively from the initialized (but not trained) dense model, and then heuristically explore various sparse topolo-gies at the given sparsity ratio, together with the sparse model training. The underlying principle of sparse training is that the total epoch num-ber is the same as dense training, but the speed of each training iteration (batch) is significantly improved, thanks to the reduced computation amount due to sparsity.
This paper proposes a novel Memory-Economic
Sparse Training (MEST) framework targeting for accurate and fast execution on edge devices.
Specifically, we boost the accuracy with the
MEST+EM (Elastic Mutation) to effectively ex-plore various sparse topologies. And we propose another enhancement through applying a soft memory bound, namely, MEST+EM&S, which relaxes on the memory footprint during sparse training with the target sparsity ratio met by the end of training. In Figure 1, the accuracy of the proposed MEST is compared against state-of-the-art (SOTA) sparse training algorithms under different sparsity ratios, using the same unstructured sparsity scheme.
Figure 1: Accuracy vs sparsity ratio on ImageNet using ResNet-50 dense model. Our proposed
MEST framework: MEST+EM (Elastic Mutation) and MEST+EM&S (with Soft Memory Bound) are compared with the SOTA sparse training al-gorithms i.e., GraSP [10], SNIP [9], RigL [11],
SNFS [12], DSR [13], SET [14], and DeepR [15].
Furthermore, as with inference acceleration, we find that sparse training closely relates to the adopted sparsity scheme such as unstructured [16], structured [17, 18], or fine-grained structured [19] scheme, which can result in varying accuracy, training speed, and memory footprint performance for sparse training. With our effective MEST framework, this paper systematically investigates the sparse training problem with respect to the sparsity schemes. Specifically, besides the directly observable model accuracy, we conduct thorough analysis on the memory footprint by different sparsity schemes during sparse training, and in addition, we measure the training speed performance under various schemes on the mobile edge device.
On top of that, the paper proposes to employ data efficiency for further acceleration of sparse training.
The prior works [20, 21, 22, 23] show that the amount of information provided by each training example is different, and the hardness of having an example correctly learned is also different. Some training examples can be learned correctly early in the training stage and will never be “forgotten” (i.e., misclassified) again. And removing those easy and less informative examples from the training dataset will not cause accuracy degradation on the final model. However, the research of connecting training data efficiency to a sparse training scene is still missing, due to the dynamic sparsity mask.
In this work, we explore the impact of model sparsity, sparsity schemes, and sparse training algorithm on the amount of removable training examples. And we also propose a data-efficient two-phase sparse training approach to effectively accelerate sparse training by removing less informative examples during the training process without harming the final accuracy. The contributions of this work are summarized as follows:
• A novel Memory-Economic Sparse Training (MEST) framework with enhancements by Elastic
Mutation (EM) and the soft memory bound targeting for accurate and fast execution on the edge.
• A systematic investigation of the impact of sparsity scheme on the accuracy, memory footprint, as well as training speed with real edge devices, providing guidance for future edge training paradigm.
• Exploring the training data efficiency in the sparse training scenario for further training acceleration, by proposing a two-phase sparse training approach for in-situ identification and removal of less informative examples during the training without hurting the final accuracy.
• On CIFAR-100 with ResNet-32, comparing with representative SOTA sparse training works, i.e.,
SNIP, GraSP, SET, and DSR, our MEST increases accuracy by 1.91%, 1.54%, 1.14%, and 1.17%; 2
achieves 1.76×, 1.65×, 1.87×, and 1.98× training acceleration rate; and reduces the memory footprint by 8.4×, 8.4×, 1.2×, and 1.2×, respectively. 2