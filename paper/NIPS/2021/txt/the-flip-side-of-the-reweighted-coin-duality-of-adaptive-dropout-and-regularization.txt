Abstract
Among the most successful methods for sparsifying deep (neural) networks are those that adaptively mask the network weights throughout training. By examining this masking, or dropout, in the linear case, we uncover a duality between such adaptive methods and regularization through the so-called “η-trick” that casts both as iteratively reweighted optimizations. We show that any dropout strategy that adapts to the weights in a monotonic way corresponds to an effective subquadratic regularization penalty, and therefore leads to sparse solutions. We obtain the effec-tive penalties for several popular sparsiﬁcation strategies, which are remarkably similar to classical penalties commonly used in sparse optimization. Considering variational dropout as a case study, we demonstrate similar empirical behavior between the adaptive dropout method and classical methods on the task of deep network sparsiﬁcation, validating our theory. 1

Introduction
In machine learning, it is often valuable for models to be parsimonious or sparse for a variety of reasons, from memory savings and computational speedups to model interpretability and general-izability. Classically, this is achieved by solving a regularized empirical risk minimization (ERM) problem of the form minimize w
Lpwq ` λΩpwq, (1) where L is the data-dependent empirical risk and Ω is a sparsity-inducing regularization penalty.
Ideally, Ωpwq is equal to }w}0 :“ # ti : wi ‰ 0u or a function of }w}0 [6], in which case an appropriately chosen λ balances the trade-off between the suboptimality of L and the sparsity of the solution. However, the use of }w}0 directly as a penalty is difﬁcult, as it makes local search impossible, so alternative approaches are necessary for high-dimensional problems. The classical solution to this is to relax the problem using a smooth surrogate Ω that approximates }w}0 [5, 18].
When we know the regularization penalty Ω, we can understand the types of solutions they encour-age; however, many popular methods for sparsifying deep (neural) networks do not appear to ﬁt the form (1). For example, variational dropout [27, 32], “(cid:96)0 regularization” [29], and magnitude pruning [45] are the only methods compared in a recent large-scale study by Gale et al. [15], and all achieve sparsity by adaptively masking or scaling the parameters of the network while optimizing the loss function in w. Although these methods are well-motivated heuristically, it is unclear how to make a principled choice between them aside from treating them as black boxes and comparing their empirical performance. However, if these methods could be cast as solutions to a regularized ERM problem, then we could compare them on the basis of their regularization penalties. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Sparsity-inducing behavior in regularized ERM (1) can be understood by considering the properties of the regularization penalty. Our analysis enables the computation and comparison of the effective regularization penalties of the adaptive dropout sparsity methods in Table 2. (We arbitrarily set λ “ 1 unless otherwise speciﬁed.) Both VARIATIONALDROPOUT [32] and HARDCONCRETE (called “(cid:96)0 regularization”) [29] bear strong resemblance to classical penalties. Left: Separable penalties (excluding HARDTHRESH) as a function of weight magnitude. Right: Penalty of a unit-norm k-sparse w P R32 deﬁned by wj “ 1? k 1 tj ď ku.
We show that, in fact, these sparsity methods do correspond to regularization penalties Ω, which we can obtain, compute, and compare. As we show in Figure 1, these penalties bear striking resemblance to classical sparsity-inducing penalties such as the LOGSUM [9] penalty and the minimax concave penalty (MCP) [44]. More broadly, we analyze adaptive dropout methods, which apply the dropout technique of Srivastava et al. [35] with adaptive parameters. By considering adaptive dropout in the linear setting, we uncover a duality between adaptive dropout methods and regularized ERM.
We make this connection via the “η-trick” [4], a tool with a long history of application in sparse optimization via iteratively reweighted least squares (IRLS) (see the history presented by Daubechies et al. [11]).
We prove that all adaptive dropout methods whose amount of dropout varies monotonically with the magnitudes of the parameters induce an effective subquadratic (and hence sparsity-inducing [5]) penalty Ω. This further supports the experimental evidence that such methods excel at inducing sparsity. We also demonstrate how to use our result to determine the effective penalty for adaptive dropout methods, using as examples the sparsity methods listed above as well as the standout method [3], which has an effective penalty on the layer activations, rather than the parameters, explaining why the standout method sparsiﬁes activations. We then numerically compute the effective penalties1 and plot them together in Figure 1.
We validate our new theory by applying variational dropout on the task of deep network sparsiﬁcation, and we show that the performance is similar to non-dropout methods based on the same effective penalty. This suggests that not only is considering the effective penalty a sound means for comparing sparsity methods, but also that classical regularized ERM is itself an effective means for inducing sparsity in deep networks and serves as a valuable tool for future work on deep network sparsity.
For theoreticians, this work provides a general framework for obtaining the effective regularization penalty for adaptive dropout algorithms, enabling regularized ERM analysis tools to be applied to these methods. For practitioners, this work enables the application of classical regularization intuition when choosing between sparsifying approaches. For methods developers, this work provides a strong baseline against which new adaptive dropout methods should be compared: the effective penalty Ω. 2