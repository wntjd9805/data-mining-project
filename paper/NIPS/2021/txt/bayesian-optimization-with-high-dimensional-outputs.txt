Abstract
Bayesian Optimization is a sample-efﬁcient black-box optimization procedure that is typically applied to problems with a small number of independent objectives.
However, in practice we often wish to optimize objectives deﬁned over many correlated outcomes (or “tasks”). For example, network operators may want to optimize the coverage of a cell tower network across a dense grid of locations.
Similarly, engineers may seek to balance the performance of a robot across dozens of different environments via constrained or robust optimization. However, the
Gaussian Process (GP) models typically used as probabilistic surrogates for multi-task Bayesian Optimization scale poorly with the number of outcomes, which greatly limitis their applicability. We devise an efﬁcient technique for exact multi-task GP sampling that combines exploiting Kronecker structure in the covariance matrices with Matheron’s identity, allowing us to perform Bayesian Optimization using exact multi-task GP models with tens of thousands of correlated outputs.
In doing so, we achieve substantial improvements in sample efﬁciency compared to existing approaches that only model aggregate functions of the outcomes. We demonstrate how this unlocks a new class of applications for Bayesian Optimization across a range of tasks in science and engineering, including optimizing interference patterns of an optical interferometer with more than 65,000 outputs. 1

Introduction
Many problems in science and engineering involve reasoning about multiple, correlated outputs. For example, cell towers broadcast signal across an area, and thus signal strength is spatially correlated.
In randomized experiments, treatment effects on multiple outcomes are naturally correlated due to shared causal mechanisms. Without further knowledge of the internal mechanisms (i.e., in a
“black-box” setting), Multi-task Gaussian processes (MTGPs) are a natural model for these types of problems as they model the relationship between each output (or “task”) while maintaining the gold standard predictive capability and uncertainty quantiﬁcation of Gaussian processes (GPs). Many downstream analyses require more of the model than just prediction; they also involve sampling from the posterior distribution to estimate quantities of interest. For instance, we may be interested in the performance of a complex stock trading strategy that requires modeling different stock prices jointly, and want to characterize its conditional value at risk (CVaR) [49], which generally requires Monte
Carlo (MC) estimation strategies [10]. Or, we want to use MTGPs in Bayesian Optimization (BO), a method for sample-efﬁcient optimization of black-box functions. Many state of the art BO approaches use MC acquisition functions [60, 3, 4], which require sampling from the posterior distribution over new candidate data points.
Drawing posterior samples from MTGPs means sampling over all tasks and all new data points, which typically scales multiplicatively in the number of tasks (t) and test data points (n), e.g. like (n3t3) [52, 6]. For problems with more than a few tasks, posterior sampling thus quickly becomes
O 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
intractable due to the size of the posterior covariance matrix. This is especially problematic in the case of many real-world problems that can have hundreds or thousands of correlated outputs that should be modelled jointly in order to achieve the best performance.
For instance, the cell tower signal maps in Fig-ure 1 each contain 2,500 outputs (pixels). In this problem, we aim to jointly tune the down-tilt angle and transmission power of the anten-nas on each cell tower (locations shown in red) to optimize a global coverage quality metric, which is a known function of power and interfer-ence at each location [21]. Since simulating the power and interference maps given a parameter-ization is computationally costly, traditionally one might apply BO to optimize the aggregate metric. At its core, this problem is a composite
BO problem [3, 4], so we expect an approach that models the constituent outcomes at each pixel individually to achieve higher sample ef-ﬁciency. However, modelling each pixel using existing approaches used for BO is completely intractable in this setting, as we would have to train and sample from a MTGP with 5,000 tasks.
Figure 1: Map of radio signal power and interfer-ence for ﬁxed locations of cell towers (red dots).
Outcomes vary smoothly with respect to the tow-ers’ down-tilt angle and transmission power. Our goal is to optimize statistics of these maps as to maximize the overall signal coverage across an area while minimizing interference.
To remedy the poor computational scaling with the number of tasks, we exploit Matheron’s rule for sampling from GP posterior distributions [13, 59]. We derive an efﬁcient method for MTGP sampling that exploits Kronecker structure inherent to the posterior covariance matrices, thereby reducing the complexity of sampling from the posterior to become effectively additive in the combination of tasks (n3t3). Our implementation of Matheron’s rule of data points, i.e.
O draws from the exact posterior distribution and does not require random features or inducing points, unlike decoupled sampling [59]. More speciﬁcally, our contributions are as follows: (n3 + t3), as compared to
O
• We propose an exact sampling method for multi-task Gaussian processes that has additive time costs in the combination of tasks and data points, rather than multiplicative (Section 3).
• We demonstrate empirically how large-scale sampling from MTGPs can aid in challenging multi-objective, constrained, and contextual Bayesian Optimization problems (Section 4).
• We introduce a method for efﬁcient posterior sampling for the High-Order Gaussian Process (HOGP) model [64], allowing it to be used for Bayesian Optimization (Section 3.2). This advance allows us to more efﬁciently perform BO on high-dimensional outputs such as images — including optimizing PDEs, optimizing placements of cell towers for cell coverage, and tuning the mirrors of an optical interferometer which optimizes over 65,000 tasks jointly (Section 4.3).
The rest of the paper is organized as follows: First, in Section 2 we review GPs, MTGPs, and sampling procedures from the posterior in both GPs and MTGPs. In Section 3, we review Matheron’s rule for sampling from GP posteriors and explain how to employ it for efﬁcient sampling from MTGP models including the HOGP model. In Section 4, we illustrate the utility of our method on a wide suite of problems ranging from constrained BO to the ﬁrst demonstration of large scale composite BO with the HOGP. Please see Appendix A for discussion of the limitations and broader impacts of our work.
Our code is fully integrated into BoTorch, see https://botorch.org/tutorials/composite_ bo_with_hogp and https://botorch.org/tutorials/composite_mtbo for tutorials. 2