Abstract
Vanilla models for object detection and instance segmentation suffer from the heavy bias toward detecting frequent objects in the long-tailed setting. Existing methods address this issue mostly during training, e.g., by re-sampling or re-weighting. In this paper, we investigate a largely overlooked approach — post-processing calibration of conﬁdence scores. We propose NORCAL, Normalized
Calibration for long-tailed object detection and instance segmentation, a simple and straightforward recipe that reweighs the predicted scores of each class by its training sample size. We show that separately handling the background class and normalizing the scores over classes for each proposal are keys to achieving superior performance. On the LVIS dataset, NORCAL can effectively improve nearly all the baseline models not only on rare classes but also on common and frequent classes.
Finally, we conduct extensive analysis and ablation studies to offer insights into various modeling choices and mechanisms of our approach. Our code is publicly available at https://github.com/tydpan/NorCal. 1

Introduction
Object detection and instance segmentation are the fundamental tasks in computer vision and have been approached from various perspectives over the past few decades [8, 13, 26, 36, 52]. With the recent advances in neural networks [1, 4, 10, 18, 29, 30, 32, 37, 40, 41, 43], we have witnessed an unprecedented breakthrough in detecting and segmenting frequently seen objects such as people, cars, and TVs [14, 15, 21, 31, 70]. Yet, when it comes to detect rare, less commonly seen objects (e.g., walruses, pitchforks, seaplanes, etc.) [12, 51], there is a drastic performance drop largely due to insufﬁcient training samples [47, 68]. How to overcome the “long-tailed” distribution of different object classes [69] has therefore attracted increasing attention lately [27, 35, 46, 55].
To date, most existing works tackle this problem in the model training phase, e.g., by developing algorithms, objectives, or model architectures to tackle the long-tailed distribution [12, 20, 27, 46, 48, 53, 55, 57, 58]. Wang et al. [55] investigated the widely used instance segmentation model Mask
R-CNN [18] and found that the performance drop comes primarily from mis-classiﬁcation of object proposals. Concretely, the model tends to give frequent classes higher conﬁdence scores [7], hence biasing the label assignment towards frequent classes. This observation suggests that techniques of class-imbalanced learning [2, 6, 16, 42] can be applied to long-tailed detection and segmentation.
Building upon the aforementioned observation, we take another route in the model inference phase by explicit post-processing calibration [2, 22, 23, 33, 61], which adjusts a classiﬁer’s conﬁdence scores among classes, without changing its internal weights or architectures. Post-processing calibration is efﬁcient and widely applicable since it requires no re-training of the classiﬁer. Its effectiveness
∗Equal contributions 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Normalized Calibration (NORCAL). Object detection or instance segmentation models (e.g.,
[18, 43]) trained with data from a long-tailed distribution tend to output higher conﬁdence scores for the head classes (e.g., “Truck”) than for the tail ones (e.g., the true class label “Bulldozer”). NORCAL investigates a simple but largely overlooked approach to correct this mistake — post-processing calibration of the classiﬁcation scores after training — and signiﬁcantly improves nearly all the models we consider. on multiple imbalanced classiﬁcation benchmarks [22, 59] may also translate to long-tailed object detection and instance segmentation.
In this paper, we propose a simple post-processing calibration technique inspired by class-imbalanced learning [33, 61] and show that it can signiﬁcantly improve a pre-trained object detector’s performance on detecting both rare and common classes of objects. We note that our results are in sharp contrast to a couple of previous attempts on exploring post-processing calibration in object detection [7, 27], which reported poor performance and/or sensitivity to hyper-parameter tuning. We also note that the calibration techniques in [54, 55] are implemented in the training phase and are not post-processing.
Concretely, we apply post-processing calibration to the classiﬁcation sub-network of a pre-trained object detector. Taking Faster R-CNN [43] and Mask R-CNN [18] for examples, they apply to each object proposal a (C + 1)-way softmax classiﬁer, where C is the number of foreground classes, and 1 is the background class. To prevent the scores from being biased toward frequent classes [7, 55], we re-scale the logit of every class according to its class size, e.g., number of training images.
Importantly, we leave the logit of the background class intact because (a) the background class has a drastically different meaning from object classes and (b) its value does not affect the ranking among different foreground classes. After adjusting the logits, we then re-compute the conﬁdence scores (with normalization across all classes, including the background) to decide the label assignment for each object proposal2 (see Figure 1). We note that it is crucial to normalize the scores across all classes since it triggers re-ranking of the detection results within each class (see Figure 3), inﬂuencing the class-wise precision and recall. Instead of separately adjusting each class by a speciﬁc factor [7], we follow [6, 33, 61] to set the factor as a function of the class size, leaving only one hyper-parameter to tune. We ﬁnd that it is robust to use the training set to set this hyper-parameter, making our approach applicable to scenarios where collecting a held-out representative validation set is challenging.
Our approach, named Normalized Calibration for long-tailed object detection and instance seg-mentation (NORCAL), is model-agnostic as long as the detector has a softmax classiﬁer or multiple binary sigmoid classiﬁers for the objects and the background. We validate NORCAL on the LVIS [12] dataset for both long-tailed object detection and instance segmentation. NORCAL can consistently improve not only baseline models (e.g., Faster R-CNN [43] or Mask R-CNN [18]) but also many models that are dedicated to the long-tailed distribution. Hence, our best results notably advance the state of the art. Moreover, NORCAL can improve both the standard average precision (AP) and the category-independent APFixed metric [7], implying that NORCAL does not trade frequent class predictions for rare classes but rather improve the proposal ranking within each class. Indeed, through a detailed analysis, we show that NORCAL can in general improve both the precision and recall for each class, making it appealing to almost any existing evaluation metrics. Overall, we view NORCAL a simple plug-and-play component to improve object detectors’ performance during inference. 2