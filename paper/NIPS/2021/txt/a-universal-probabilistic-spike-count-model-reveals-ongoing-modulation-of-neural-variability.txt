Abstract
Neural responses are variable: even under identical experimental conditions, single neuron and population responses typically differ from trial to trial and across time.
Recent work has demonstrated that this variability has predictable structure, can be modulated by sensory input and behaviour, and bears critical signatures of the underlying network dynamics and computations. However, current methods for characterising neural variability are primarily geared towards sensory coding in the laboratory: they require trials with repeatable experimental stimuli and behavioural covariates. In addition, they make strong assumptions about the parametric form of variability, rely on assumption-free but data-inefﬁcient histogram-based approaches, or are altogether ill-suited for capturing variability modulation by covariates. Here we present a universal probabilistic spike count model that eliminates these short-comings. Our method builds on sparse Gaussian processes and can model arbitrary spike count distributions (SCDs) with ﬂexible dependence on observed as well as latent covariates, using scalable variational inference to jointly infer the covariate-to-SCD mappings and latent trajectories in a data efﬁcient way. Without requiring repeatable trials, it can ﬂexibly capture covariate-dependent joint SCDs, and pro-vide interpretable latent causes underlying the statistical dependencies between neurons. We apply the model to recordings from a canonical non-sensory neural population: head direction cells in the mouse. We ﬁnd that variability in these cells deﬁes a simple parametric relationship with mean spike count as assumed in standard models, its modulation by external covariates can be comparably strong to that of the mean ﬁring rate, and slow low-dimensional latent factors explain away neural correlations. Our approach paves the way to understanding the mecha-nisms and computations underlying neural variability under naturalistic conditions, beyond the realm of sensory coding with repeatable stimuli. 1

Introduction
Classical analyses of neural coding are based on mean spike counts or neural ﬁring rates. Indeed, some of the most paradigmatic examples of the neural code were discovered by regressing neural
ﬁring rates to particular sensory stimuli [1, 2] or behavioural covariates [3, 4, 5, 6] to characterize their tuning properties. However, neural spiking is generally not regular. Recordings from many cortical areas show signiﬁcantly different activity patterns within and across identical trials [7], despite ﬁxing experimentally controlled variables. This irregularity is also seen in continual neural recordings without trial structure [8]. The resulting variability has classically been characterised as ‘Poisson’, with a Fano factor (variance to mean ratio) of one [9], but experimental data also often exhibits signiﬁcantly more [10, 8, 11, 12] and sometimes less [13, 14] variability, respectively referred to as over- or underdispersion. Moreover, experimental studies have revealed that neural variability generally depends on stimulus input and behaviour [15, 16, 17, 18], and exhibits structured shared 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
variability (‘noise correlations’) across neurons even after conditioning on such covariates. Such cor-relations can have important consequences for decoding information from neural population activity
[19, 20, 21] and reveal key properties of the underlying circuit dynamics [22]. Moreover, theories of neural representations of uncertainty have assigned computational signiﬁcance to variability as a signature of Bayesian inference [23, 24, 25, 26]. Thus, just as classical tuning curves for ﬁring rates have been crucial for understanding some of the fundamental properties of the neural code, a principled statistical characterisation of neural variability, and its dependence on stimulus and behavioral covariates, is a key step towards understanding the dynamics of neural circuits and the computations they subserve.
The traditional approach to characterising neural variability has been pioneered in sensory areas, and relies on repeatable trial structure with a sufﬁciently large number of trials using identical stimulus and behavioral correlates [27, 15, 28]. Variability in this case can be quantiﬁed by simple summary statistics of spike counts across trials of the same condition. However, this approach does not readily generalise to more naturalistic conditions where covariates cannot be precisely controlled and repeated in an experiment. This more general setting requires statistical methods that take into account temporal variation of covariates for predicting neural count activity. Generalised Linear
Models are a popular choice [29], but they only model the dependence of ﬁring rates on covariates – with changes in variability directly coupled to changes in the rate inherent to Poisson spiking.
More complex methods for inferring neural tuning [30, 31] and latent structure [32, 33, 34, 35] similarly use restrictive parametric families for spike count distributions, and thus also cannot model changes in variability that are not ‘just’ a consequence of changes in mean counts or ﬁring rates.
Conversely, statistical models capable of capturing arbitrary single neuron count statistics, such as histogram-based approaches or copulas [36], do not incorporate dependencies on covariates.
Here we unify these separate approaches, resulting in a single framework for jointly inferring neural tuning, single neuron count statistics, neural correlations, and latent structure. Our semi-parametric approach leads to the universal count model (UCM) for counts ranging from 0 to K, in the sense that we can model arbitrary distributions over the joint count space of size (K + 1)N of N neurons. The trade-off between computational overhead and model expressivity is controlled by hyperparameters, with expressivity upper bounded by the true universal model. Our approach extends the idea of a universal binary count model [37] to a ﬁnite range of integer counts, while allowing ﬂexible dependence on observed and latent covariates to model non-stationary neural activity and correlations.
The ﬂexibility reduces biases from restrictive assumptions in any of the model components. Scalability is maintained by leveraging sparse Gaussian processes [38] with mini-batching [39, 40] to handle the size of modern neural recordings.
We ﬁrst deﬁne the UCM, and then describe how to interpret as well as evaluate model ﬁts. As our model is able to capture arbitrary single neuron statistics, we build on the Kolmogorov-Smirnov test to construct more absolute goodness-of-ﬁt measures. After validating our method on synthetic data that cannot be captured by currently used methods, we apply the model to electrophysiological recordings from two distinct brain regions in mice that show signiﬁcant tuning to the head direction of the animal [41, 42]. We ﬁnd that (1) neural activity tends to be less dispersed than common
Poisson-like models at higher ﬁring rates, and more dispersed at low rates; (2) mean and variance of counts defy a simple parametric relationship imposed by parametric count distribution families; (3) variability modulation by behaviour can be comparable or even exceed that of the mean count or ﬁring rate; (4) a two-dimensional latent trajectory varying on timescales of ∼ 1 s is sufﬁcient to explain away neural correlations but not the non-Poisson nature of single neuron variability. Finally, we discuss related work, limitations and proposed extensions of our model. 2 Universal count model
Notation Spike count activity of N neurons recorded into T time bins is formally represented as an
N -dimensional time series of non-negative integers. Due to biological constraints, the possible spike counts have some ﬁnite upper bound K, taken as the highest observed count. We denote probabilities of a spike count distribution (SCD) by a vector π of length K + 1, and use Π to denote the collection of vectors πnt for neurons n and time steps t. Additionally, we denote the count activity by a matrix
Y ∈ [0, K]N ×T with elements ynt. Input covariates are observed X ∈ RT ×Dx (e.g. animal speed) or latent Z ∈ RT ×Dz (to capture e.g. attention), with range depending on topology [43]. We denote their elements xtd and ztq with observed and latent dimension d and q, respectively. 2
Figure 1: Schematic of the UCM and the workﬂow. Left: graphical model corresponding to
Equation 1, with shaded circles as observed, open circles as latent, and squares as deterministic variables. Filled dots represent ﬁxed quantities. Middle: example inference of model posterior
Equation 3, with inferred latent trajectories (green, top) and covariate-dependent SCDs (blue, bottom) that depend on both observed x and latent z covariates. Note we only show the posterior over a single SCD evaluated on a (x,z) grid, whereas the full posterior deﬁnes SCDs over all neurons.
Right: obtaining interpretable spike count statistics from the SCDs (see subsection 2.3). Examples show ﬁring rate and Fano factor tuning curves over observed x and latent z covariates, either jointly (heatmaps) or marginalized (grey curves). The depth of modulation in marginalized tuning curves is used to extract a tuning index (TI) for the chosen subsets of covariates, see Equation 6. 2.1 Generative model
The big picture is to model counts Y with dependence on X. For each neuron, our model consists of
C Gaussian process (GP) priors, a basis expansion φ : RC → R ˜C, and a linear-softmax mapping hcn(·) ∼ GP(0, kcn(·, · ; θGP cn )) ztq ∼ p(Z; θpr), fcnt = hcn(xt, zt)
πnt = softmax(Wn φ(fnt) + bn) ynt ∼ Discrete(πnt) (1) where W ∈ R(K+1)× ˜C, b ∈ RK+1, and the GP covariance functions kcn have hyperparameters θGP cn .
The use of non-parametric GP mappings with point estimates for W and b leads to a semi-parametric model with parameters θ, see details in Appendix E. The overall generative model Pθ(Y |X) is depicted schematically in Figure 1. Note the model speciﬁes a prior p(Π|X) over joint SCDs, conceptually similar to Dirichlet priors [37] but allowing non-parametric dependence on X. With latent input Z, our model can ﬂexibly describe multivariate dependencies in joint SCDs as conditional independence across neurons no longer holds when marginalizing over Z [44]. In addition, p(Z) models temporal correlations in the latent states. We use Markovian priors (details in subsection E.2) pθ(Z) = pθ(z1)
T (cid:89) t=2 pθ(zt|zt−1) (2) with θpr absorbed into model parameters θ for compactness. This allows the model to ﬂexibly capture both neural and temporal correlations in Y . To attain scalability, we use sparse GPs [38].
Depending on C and basis functions φ(·), we obtain an approximation to the true universal prior on joint SCDs, with ‘universal’ referring to the ability to capture any joint SCD over all neurons. Arbitrary single neuron statistics can be captured when C = K with φ(f ) = f , but ﬁtting is computationally expensive when N ×C (cid:29) 1. For capturing all correlations, the model also requires a sufﬁciently large latent space. One controls the trade-off between model expressiveness and computational overhead through C and φ. Larger expansions φ allow one to model count distributions more expressively with small C, e.g. the element-wise linear-exponential φ(f ) = (f1, ef1, . . . , fC, efC ) covers a range of distributions including the truncated Poisson with only C = 1 (see subsection A.3). 3
2.2 Stochastic variational inference and learning
For the joint model distribution pθ(Y, Π, Z|X) = P (Y |Π) pθ(Π|X, Z) pθ(Z), with count distribu-tions P (Y |Π), we approximate the posterior by qθ,χ,ϕ(Π, Z|X) that factorizes in the form (cid:32) N (cid:89) (cid:33) (cid:32) T (cid:89) (cid:33) qθ,χ(Π|X, Z) qϕ(Z) = qθ,χ(Πn|X, Z) qϕ(zt) (3) n t with ϕ and χ the variational parameters for latent states and the sparse Gaussian process posterior (Appendix E), respectively. Note that we use a factorized normal q(Z) for Euclidean Z, and a wrapped normal for circular Z based on the framework of reparameterized Lie groups [45, 43]. The posterior over count probabilities qθ,χ(Π|X, Z) is deﬁned as mapping the sparse Gaussian process posterior qθ,χ(F |X, Z) through Π(F ) (Equation 1), a deterministic mapping. This is analytically intractable, so in practice it is represented by Monte Carlo samples. Using stochastic variational inference [46], we minimize an upper bound on the negative log marginal likelihood
Fθ,χ,ϕ(Y |X) = −EZ∼qϕ(Z)EΠ∼qθ,χ(Π|X,Z) log (cid:20)
P (Y |Π) pθ(Π|X, Z) pθ(Z) qθ,χ(Π|X, Z) qϕ(Z) (cid:21) (4) known as the variational free energy. This objective leads to tractable terms (subsection E.1), allowing us to infer the approximate posterior as well as a lower bound of the log marginal likelihood [47, 40].
We use Adam [48] for optimization, see details of implementation and model ﬁtting in Appendix E. 2.3 Obtaining interpretable spike count statistics from the model
Characterizing spike count distributions From the posterior q(Π|X)1, we can compute samples of the posterior of any statistic of spike counts as a function of covariates. Single neuron statistics in particular can be characterized by tuning curves for both mean ﬁring rates and Fano factors (FF)
ρ(X) = 1
∆
Eq(Π|X)EP (Y |Π)[Y ]
FF(X) = Eq(Π|X) (cid:20) VarP (Y |Π)[Y ]
EP (Y |Π)[Y ] (cid:21) (5) with time bin length ∆. The model also quantiﬁes private neuron variability that cannot be explained away by regressing to shared input (both observed and latent) through Pn(ytn|xt).
To quantify the sensitivity of a some aspect of neuron activity to a set of covariates x∗, we deﬁne a tuning index (TI) with respect to a count statistic Ty(x∗)
TI = maxx∗ Ty(x∗) − minx∗ Ty(x∗) maxx∗ Ty(x∗) + minx∗ Ty(x∗) (6) with Ty(x∗) evaluated under the mean posterior SCD marginalized over all other covariate dimensions complementary to x∗. These marginalized distributions are computed using observed input xt (subsection D.5). Resulting marginalized tuning curves for TIs are depicted conceptually in Figure 1.
Generalized Z-scores and noise correlations The deviation of activity from the predicted statis-tics is commonly quantiﬁed through Z-scores [8, 49, 17], which are computed as (y − (cid:104)y(cid:105))/(cid:112)(cid:104)y(cid:105) with (cid:104)y(cid:105) being the mean count in some time bin. If neural activity follows a Poisson distribution, the distribution of Z-scores asymptotically tends to a unit normal when average counts (cid:104)y(cid:105) (cid:29) 1 (Appendix B). To generalize the normality of the Z-score for arbitrary counts and SCDs, we use
ξ = Φ−1(u) with u(y) = (cid:90) y+(cid:15) 0 p(˜y) d˜y = y−1 (cid:88) k=0
P (k) + (cid:15)P (y), (cid:15) ∼ U(0, 1) (7) with Φ(·) the unit Gaussian cdf., and dequantization noise (cid:15) to get continuous quantiles u ∼ U(0, 1) and generalized Z-scores ξ ∼ N (0, 1) from the probability integral transform.
With ξ, one can completely describe single neuron statistics with respect to the model. Correlations in the neural activity however will cause ξ to be correlated. We deﬁne generalized lagged correlations rij(∆) ∈ [−1, 1] and Fisher Z ∈ R that is more convenient for statistical testing rij(∆) = (cid:104)ξi(t) ξj(t + ∆)(cid:105)t, ZFisher = 1 2 log 1 + r 1 − r (8) which describes spatio-temporal correlations not captured by the model. Noise correlations [50] refer to the case of ∆ = 0, when rij becomes symmetric. 1For notational convenience, X denotes both observed and latent covariates here. 4
2.4 Assessing model ﬁt
Our model depends on a hyperparameter C ≤ K that trades off ﬂexibility with computa-In practice, one likely captures the neural activity accurately with C well be-tional burden. low K and a simple basis expansion as the linear-exponential above or quadratic-exponential 1 , ef1, . . . , f1f2, . . .). This can be quantiﬁed by the statistical measures provided
φ(f ) = (f1, f 2 below, and allows us to select appropriate hyperparameters to capture the data sufﬁciently well.
To assess the model ﬁt to neural spike count data, a conventional machine learning approach is to evaluate the expected log-likelihood of the posterior predictive distribution on held-out data Y , leading to the cross-validated log-likelihood cvLL = Eq(Z)Eq(Π|X,Z)[log P (Y |Π)] (9) where we cross-validate over the neuron dimension by using the majority of neurons to infer the latent states q(Z) in the held-out segment of the data, and then evaluate Equation 9 over the remaining neurons. Without latent variables, we simply take the expectation with respect to q(Π|X). However, the cvLL does not reveal how well the data is described by the model in an absolute sense. Likelihood bootstrap methods are possible [28], but become cumbersome for large datasets. To assess whether the neural data is statistically distinguishable from the single neuron statistics predicted by the model, we use the Kolmogorov-Smirnov framework [51] with u from Equation 7 across time steps t
TKS = max t
|FT (ut) − ut| (10) with empirical distribution function FT (u) (for details see Appendix B). This scalar number is positive and does not indicate whether the data is under- or overdispersed relative to the model. For this, a useful measure of dispersion is the logarithm of the variance of ξ with a correction
TDS = log (cid:104)ξ2 t (cid:105)t + (cid:19) (cid:18) 1
T
+ 1 3T 2 (11) which provides a real number with positive and negative sign indicating over- and underdispersion, respectively. Its sampling distribution under ξ ∼ N (0, 1) is asymptotically normal, centered around 0 (due to the additive parenthetical term) with a variance depending on the number of timesteps T (subsection B.3). This extends the notion of over- and underdispersion beyond the usual deﬁnition of dispersion relative to Poisson models [52]. To quantify whether the model has captured noise correlations in the data, we compute ξ with respect to the mean posterior predictive distribution
Qθ,ϕ(Y |X) = (cid:90) T (cid:89) (cid:32) N (cid:89) t n
Eq(πnt|xt,zt) [P (ytn|πnt)] qϕ(zt) dzt (12) (cid:33) which whitens ξ, hence reducing noise correlations r in Equation 8, if correlations are explained away by co-modulation of neurons due to shared low-dimensional factors [44]. These are captured through latent states Z using the posterior qϕ(Z), inferred from the same data used to compute ξ.
Overall, this can be interpreted as treating Z as if it was part of the observed input to the model. 3 Results
In the following, we use C = 3 with an element-wise linear-exponential basis expansion (subsec-tion 2.1). This empirically provided sufﬁcient model ﬂexibility as seen in goodness-of-ﬁt metrics.
We use an RBF kernel with cosine distances in case of angular input dimensions (subsection E.3). 3.1 Synthetic data
Animals maintain an internal estimate of their head direction [4, 42, 53]. Here, we extend simple statistical models of head direction cell populations [54] for validating the ability of the UCM to capture complex count statistics, as well as neural correlations through latent structure. The task is to jointly recover the ground truth count likelihoods, their tuning to covariates, and latent trajectories if relevant from activity generated using two synthetic populations. The ﬁrst population was generated with a parametric heteroscedastic Conway-Maxwell-Poisson (CMP) model [55], which has decoupled mean and variance modulation as well as simultaneously over- and underdispersed activity (Fano 5
Figure 2: Model validation with two synthetic head direction cell populations. (A) Applying
Poisson, heteroscedastic negative binomial (hNB) and universal (U, with either GP or ANN mappings) models to synthetic data from the heteroscedastic CMP population. Error bars indicate s.e.m. over cross-validation runs. Shaded regions for tuning curves and TKS indicate the 95% CI. ∆cvLL is the difference w.r.t. Poisson baseline. Left: regression models, visualizing SCDs (top right), tuning curves (bottom right), and ﬁtting scores (bottom left) for two representative cells. Right: models with an angular latent variable, visualizing inferred latent states (top right), comparing the GP to ANN model (bottom right), and plotting ﬁtting scores (bottom left). Root-mean-squared errors (RMSE) between the inferred latent and ground truth uses the geodesic distance on the ring (subsection D.9). (B) Applying regression (Poisson, Universal (X)) and joint latent-observed (Universal (X,Z)) models to the modulated Poisson population data. Left: three columns showing progressively how single neurons variability, with ξ and TDS (middle), and noise correlations, with rij and Fisher Z (bottom), are captured (see subsection 2.4). Right: ∆cvLL for all models (top) and visualization of the joint latent-observed model input spaces (middle) and tuning curves (bottom). factors above and below 1). The second population consists of Poisson neurons tuned to head direction and an additional hidden signal, which gives rise to apparent overdispersion [28] as well as noise correlations when only regressing to observed covariates. For mathematical details of the count distributions and synthetic populations, see Appendix A and Appendix D, respectively.
We compare our UCM to the Poisson GP model [33] and the heteroscedastic negative binomial GP model, a non-parametric extension of [55]. To show data-efﬁciency and regularization beneﬁts of 6
GPs, we also compare to a UCM with an artiﬁcial neural network (ANN) mapping replacing the GP mapping. For details of the baseline models, see Appendix D. For cross-validation we split the data into 10 roughly equal non-overlapping segments, and validated on 3 chosen segments that were evenly spread out across the data. When a latent space was present, we used 90% of the neurons to infer the latent signal while validating on the remaining neurons, and repeated this for non-overlapping subsets. We rescale the log-likelihoods by the ratio of total neurons to neurons in subset and then take the average over all subsets to obtain comparable cross-validation runs to regression.
Figure 2A shows that only the UCM successfully captures the heteroscedastic CMP data, indicated by
TKS. Baseline models cannot capture frequent cases where the Fano factor drops below 1. In addition, we observe that using a Bayesian GP over an ANN mapping in the model leads to a reduction in overﬁtting, especially in the latent setting (Figure 2A). Therefore, all other analyses with the UCM reported here used the GP mapping. Figure 2B shows that the modulated Poisson population activity is seen by a Poisson regression model as overdispersed, indicated by TDS. Our model ﬂexibly captures the overdispersed single neuron statistics, independent from noise correlations rij that are captured when we introduce a Euclidean latent dimension. As expected, the ξ scatter plots shows whitening under the posterior predictive distribution when the correlations are captured. 3.2 Mouse head direction cells
We apply the UCM to a recording of 33 head direction cells in the anterodorsal thalamic nucleus (ADn) and the postsubiculum (PoS) of freely moving mice [41, 42], see subsection D.2. Neural data was binned into 40 ms intervals, giving K = 11. Note that observed count statistics differ with bin size, see subsection C.2 for a discussion. Regression was performed against head direction (HD), angular head velocity (AHV), animal speed, two-dimensional position in arena, and absolute time, which collectively form X in this model. We used 64 inducing points for regression, and added 8 for every latent dimension added (Appendix E). Cross-validation was performed as in synthetic experiments, but with 6 validation segments and subsets of 85% of neurons to infer the latent signal.
Figure 3A shows that for regression hNB overﬁts and performs worst, despite containing Poisson as a special case. However, this limit is not reached in practice due to the numerical implementation, see Appendix A. Only the UCM captures the training data satisfactorily with respect to conﬁdence bounds for TKS and TDS, although the data remained slightly underdispersed to the model with TDS values slightly skewed to negative. Compared to the Poisson model, the cvLL is only slightly higher for the UCM as the data deviates from Poisson statistics in subtle ways. We see both FF above and below 1 (over- and underdispersed) across the neural ﬁring range in Figure 3B, with quite some neurons crossing 1. Correspondingly, FF-mean correlations coefﬁcients are often negative. Their spread away from ±1 indicates ﬁring rate and FF do not generally satisfy a simple relationship, especially for examples such as cell 27. Furthermore, ADn neurons seem to deviate less from Poisson statistics. From Figure 3C, we note in particular that FFs tend to decrease at the preferred head direction, but rise transiently as the head direction approaches the preferred value. We also see that tuning to speed and time primarily modulates variability rather than ﬁring rates. All of this is impossible to pick up with baseline models, which constrain FF ≥ 1 as well as FF increasing with
ﬁring rate (Appendix A). Finally, we see more tuning of the ﬁring rate to position in PoS cells.
When adding latent dimensions, Figure 3D shows a peak in the cvLL at two dimensions, where correspondingly the Fisher Z distribution starts to match the unit normal well. Kernel length scales however did not indicate redundant latent subspaces for higher dimensions as expected for automatic relevance determination, likely due to mixing of latent dimensions. Notice the noise correlation patterns in Figure 3E tend to show positive correlations for similarly tuned neurons roughly around the diagonal of blocks, as expected from ring attractor models [22]. Intrinsic neuron variability, roughly quantiﬁed by the average FF, further decreased and thus become even more underdispersed when considering additional tuning to latents, in particular for ADn. In addition, latent signals primarily modulate ﬁring rate as seen from TIs in Figure 3F. When looking at time scales of covariates in
Figure 3G (computed as the decay time constant of the autocorrelogram (Appendix D), the latent processes seem to vary on time scales right in the gap of behavioural time scales.
Tangential to our main contribution of characterising the detailed structure of neural variability, our results have another novel element that does not speciﬁcally rely on the UCM. Using GP-based non-parametric methods, we successfully estimated the tuning of cells to as many as 8 different covariates (6 observed + 2 latent, see Figure 3G) in a statistically sound fashion, while previous 7
Figure 3: Application to mouse head direction cells in the anterodorsal thalamic nucleus (ADn) and the postsubiculum (PoS). (A) Goodness-of-ﬁt for Poisson (P), heteroscedastic negative binomial (hNB) and universal (U) regression with covariates described in the text. Our method (U) outperforms baselines (p = 7 · 10−3 for ∆cvLL w.r.t. Poisson, one-sample t-test). (B) Left: Fano factor (FF) versus mean count of the predictive distribution for all time steps (scatter dots) for two representative cells. Red lines are linear regression ﬁts to scatter points. Right: average FF across time steps and
Pearson r correlation between FF and mean count for all cells, with colour indicating region ADn (orange) or PoS (green). (C) Top: conditional tuning curves (subsection D.5) of the UCM with complementary x at preferred HD, positioned at centre of arena, with zero speed, AHV and at time t = 0. Bottom: TIs w.r.t. corresponding covariates for the FF and ﬁring rates. (D) Adding Euclidean latent dimensions Dz. Top: ∆cvLL w.r.t. regression only. Bottom: Fisher Z histogram, with red curves its sampling distribution. (E) Comparison between models with latent space dimensions
Dz = 2 and Dz = 0. Top: off-diagonal noise correlations rij, with neurons ordered by region ﬁrst (PoS and ADn), and then by preferred head direction within region. Bottom: average FF (same as in (B)) and Fisher Z under the posterior predictive distribution Equation 12. (F) Left: inferred latent trajectories for Dz = 2. Right: corresponding TIs for FF and ﬁring rates. (G) Time scales for covariates computed from autocorrelograms (Appendix D). Note pos. refers to both x and y position (near-identical values). The top horizontal line is the minimum kernel length scale over absolute time across neurons. The bottom line depicts the 40 ms time bin. Error bars in A and D show s.e.m. over cross-validation runs. Shaded regions for TKS, TDS, tuning curves and latent states show the 95% CI.
GP-based approaches typically only consider around 2 to 3 input dimensions [56, 33, 43]. Speciﬁcally, one of our covariates was absolute experimental time to capture non-stationarities in neural tuning.
As a result, our model captured several experimental phenomena that are studied separately in the literature: drifting neural representations [57, 58, 59], anticipatory time intervals [54] and conjunctive tuning to behaviour [60]. We also applied the model in a purely latent setting similar to the example in Figure 2A, with the UCM uncovering a latent signal more closely correlated to the head direction compared to baseline models. These additional results are presented in Appendix C. 8
4 Discussion