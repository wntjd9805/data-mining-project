Abstract
We consider proximal splitting algorithms for convex optimization problems over matrices. A signiﬁcant computational bottleneck in many of these algorithms is the need to compute a full eigenvalue or singular value decomposition at each iteration for the evaluation of a proximal operator.
In this paper we propose to use an old and surprisingly simple method due to
Jacobi to compute these eigenvalue and singular value decompositions, and we demonstrate that it can lead to substantial gains in terms of computation time compared to standard approaches. We rely on three essential properties of this method: (a) its ability to exploit an approximate decomposition as an initial point, which in the case of iterative optimization algorithms can be obtained from the previous iterate; (b) its parallel nature which makes it a great ﬁt for hardware accelerators such as GPUs, now common in machine learning, and (c) its simple termination criterion which allows us to trade-off accuracy with computation time. We demonstrate the efﬁcacy of this approach on a variety of algorithms and problems, and show that, on a GPU, we can obtain 5 to 10x speed-ups in the evaluation of proximal operators compared to standard CPU or GPU linear algebra routines. Our ﬁndings are supported by new theoretical results providing guarantees on the approximation quality of proximal operators obtained using approximate eigenvalue or singular value decompositions. 1

Introduction
Many problems in statistics and learning can be formulated as convex optimization over matrices. This includes kernel learning [LCB+04], low-rank matrix recovery [RFP10, CR09], sparse covariance selection [BGdN06, FHT08], community detection [BBV16], and many others. Proximal splitting algorithms such as (accelerated) proximal gradient methods [Nes03, BT09], or the alternating direction method of multipliers (ADMM) [EB92, BPC11], are among the most popular methods to deal with large-scale instances of such problems. These methods solve generic convex optimization problems of the form min
X∈E
F (X) + G(X) (1) via calls to the proximal operators of F and/or G. When the variable X is a symmetric or rectangular matrix, it is often the case that the functions F and/or G are symmetric functions of the eigenvalues or singular values of X; i.e., so-called spectral or othogonally-invariant functions. Typical examples are F (X) = I(X (cid:23) 0) = (cid:80)n i=1 I(λi(X) ≥ 0) the convex indicator function for the positive semideﬁnite cone, or F (X) = (cid:107)X(cid:107)∗ = (cid:80)n
Computing the proximal operator of such spectral functions F requires a full eigenvalue/singular value decomposition which contributes a large computational cost of at least n3 ﬂoating-point i=1 σi(X) the nuclear norm. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
operations per iteration. The most common algorithms for such decompositions are the QR method, and the divide-and-conquer algorithm [GVL13] which are implemented in linear algebra packages (LAPACK) that are used in standard numerical computing software such as MATLAB or NumPy.
Contributions
In this paper we propose to use a simple and lesser known method due to Jacobi from 1846 [Jac46] for the computation of these decompositions. Several features of this method make it suitable for its use within iterative proximal methods: (i) It can effectively exploit an initial approximate decomposition of the target matrix. In iterative proximal methods, the eigenvectors/singular vectors from the previous iterate are often a very good initial point that almost diagonalize the matrix of the current iterate. (ii) The method can be very easily parallelized, and is a very good ﬁt for parallel hardware accelerators, notably GPUs, which are now very common in machine learning. (iii) It has a simple termination criterion that allows us to trade-off computation time with the accuracy of the desired decomposition.
The present paper has two main contributions. First, we establish new theoretical results providing guarantees on the approximation quality of the proximal operator of a convex spectral function when computed with an approximate eigenvalue/singular value decomposition. When combined with existing results on the convergence of proximal splitting methods with inexact proximal oracles, we are able to appropriately set the tolerance of Jacobi-based methods and gain computation time, while guaranteeing global convergence of the optimization method. Second, we demonstrate using the observations above, and on a variety of problems and splitting algorithms, that the method of Jacobi on a GPU can yield speed-ups of 5-10x in the evaluation of proximal operators of spectral functions, compared to standard approaches on CPU and GPU.