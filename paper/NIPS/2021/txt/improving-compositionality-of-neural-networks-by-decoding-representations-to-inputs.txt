Abstract
In traditional software programs, it is easy to trace program logic from variables back to input, apply assertion statements to block erroneous behavior, and compose programs together. Although deep learning programs have demonstrated strong performance on novel applications, they sacriﬁce many of the functionalities of traditional software programs. With this as motivation, we take a modest ﬁrst step towards improving deep learning programs by jointly training a generative model to constrain neural network activations to “decode” back to inputs. We call this design a Decodable Neural Network, or DecNN. Doing so enables a form of compositionality in neural networks, where one can recursively compose DecNN with itself to create an ensemble-like model with uncertainty. In our experiments, we demonstrate applications of this uncertainty to out-of-distribution detection, adversarial example detection, and calibration — while matching standard neural networks in accuracy. We further explore this compositionality by combining
DecNN with pretrained models, where we show promising results that neural networks can be regularized from using protected features. 1

Introduction
Traditional hand-written computer programs are comprised of a computational graph of typed variables with associated semantic meaning. This structure enables practitioners to interact with programs in powerful ways (even if they are not the author) — such as debug code by tracing variables back to inputs, apply assertions to block errors, and compose programs together for more complex functionality. However, traditional software has its limitations: it is difﬁcult to hand-write programs to classify images or extract sentiment from natural language. For these functionalities, deep learning and neural networks [41] have become the dominant approach [23, 2, 46].
While neural networks have made impressive progress on complex tasks, they come at a sacriﬁce of many of the desirable properties of traditional software. Speciﬁcally, the closest approximation to a “variable” in a neural network is an activation. Yet it is difﬁcult to understand a neural network’s computation from an activation value, and there is little to associate an activation with semantic meaning. A practitioner cannot write assertion statements to constrain valid neural network logic: checking the values that an activation takes is usually not enough to gauge correctness nor meaning.
Moreover, given multiple neural networks, composing them together requires retraining from scratch.
In this paper, we take a modest ﬁrst step towards bridging the expressivity of deep learning with the engineering practicality of traditional software. We aim to uncover new ways for practitioners to build and use neural networks by leveraging compositionality. Speciﬁcally, we propose to train a neural network classiﬁer jointly with a generative model whose role is to map the classiﬁer’s activations back to the input, approximating invertibility. For any input, a neural network’s computation can 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
be represented by a sequence of activations (from each layer), each of which can now be mapped back to the input space. It is this insight that enables a special form of compositionality for neural networks, as inputs derived from activations can be fed back into other models.
In our experiments, we study this compositionality by (1) recursively composing a neural network with itself, creating an ensemble-like model with a measure of uncertainty that is useful for out-of-distribution detection, adversarial example detection, and model calibration; (2) composing neural networks with pretrained models as forms of regularization and distillation; and (3) using decodable activations to discourage a neural network from using protected attributes. Throughout, we show that decodability comes at low cost as we ﬁnd equivalent accuracy to a standard neural network. 2