Abstract
One principal approach for illuminating a black-box neural network is feature attri-bution, i.e. identifying the importance of input features for the network’s prediction.
The predictive information of features is recently proposed as a proxy for the measure of their importance. So far, the predictive information is only identiﬁed for latent features by placing an information bottleneck within the network. We pro-pose a method to identify features with predictive information in the input domain.
The method results in ﬁne-grained identiﬁcation of input features’ information and is agnostic to network architecture. The core idea of our method is leveraging a bottleneck on the input that only lets input features associated with predictive latent features pass through. We compare our method with several feature attribution methods using mainstream feature attribution evaluation experiments. The code 1 is publicly available. 1

Introduction
Feature attribution – identifying the contribution of input features to the output of the neural network function – is one principal approach for explaining the predictions of black-box neural networks.
In recent years, a plethora of feature attribution methods is proposed. The solutions range from axiomatic methods [1, 2, 3] derived from game theory [4] to contribution backpropagation methods
[5, 6, 7, 8, 9]. However, given a speciﬁc neural network and an input-output pair, existing feature attribution methods show dissimilar results. In order to evaluate which method correctly explains the prediction, the literature proposes several attribution evaluation experiments [10, 11, 12, 9]. The attribution evaluation experiments reveal that methods that look visually interpretable to humans or even methods with solid theoretical grounding are not identifying contributing input features
[13, 12, 14]. The divergent results of different attribution methods and the insights from evaluation experiments show that the feature attribution problem remains unsolved. Though there is no silver
∗denotes equal contribution (cid:5)Department of Dermatology and Allergology †corresponding author 1https://github.com/CAMP-eXplain-AI/InputIBA 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
bullet for feature attribution, each method is revealing a new aspect of model behavior and provides insights or new tools for getting closer to solving the problem of attribution.
Recently a promising solution – Information Bottleneck Attribution (IBA) [15] – grounded on information theory is proposed. The method explains the prediction via measuring the predictive information of latent features. This is achieved by placing an information bottleneck on the latent features. The predictive information of input features is then approximated via interpolation and averaging of latent features’ information. The interpolated information values are considered as the importance of input features for the prediction. One shortcoming with this approach is the variational approximation inherent in the method, leads to an overestimation of information of features when applied to earlier layers. Another problem is that the interpolation to input dimension and the averaging across channels only approximates the predictive information of input features and is only valid in convolutional neural networks (CNNs) where the feature maps keep the spatial correspondences.
In this work, we propose InputIBA to measure the predictive information of input features. To this end, we ﬁrst search for a bottleneck on the input that only passes input features that correspond to predictive deep features. The correspondence is established via a generative model, i.e. by ﬁnding a bottleneck variable that induces the same distribution of latent features as a bottleneck on the latent features. Subsequently, we use this bottleneck as a prior for ﬁnding an input bottleneck that keeps the mutual information with the output. Our methodology measures the information of input features with the same resolution as the input dimension. Therefore, the attributions are ﬁne-grained. Moreover, our method does not assume any architecture-speciﬁc restrictions. The core idea – input bottleneck/mask estimation using deep layers – is the main contribution of this work to feature attribution research (input masking itself is already an established idea [16, 17], the novelty is leveraging information of deep features for ﬁnding the input mask).
We comprehensively evaluate InputIBA against other methods from different schools of thought.
We compare with DeepSHAP [1] and Integrated Gradients [2, 3] from the school of Shapley value methods, Guided Backpropagation [5] (from backpropagation methods), Extremal perturbations
[17] (from perturbation methods), GradCAM [18] (an attention-based method), and the background method, IBA [19]. We evaluate the method on visual (ImageNet classiﬁcation) and natural language processing (IMDB sentiment analysis) tasks and from different perspectives. Sensitivity to parameter randomization is evaluated by Sanity Checks [12]. Evaluating the importance of features is done using Sensitivity-N [9], Remove-and-Retrain (ROAR) [10], and Insertion-Deletion [11]. To quantify the degree of being ﬁne-grained in visual tasks we propose a localization-based metric, Effective
Heat Ratios (EHR). 2