Abstract
Collecting training data from untrusted sources exposes machine learning services to poisoning adversaries, who maliciously manipulate training data to degrade the model accuracy. When trained on ofﬂine datasets, poisoning adversaries have to inject the poisoned data in advance before training, and the order of feeding these poisoned batches into the model is stochastic. In contrast, practical systems are more usually trained/ﬁne-tuned on sequentially captured real-time data, in which case poisoning adversaries could dynamically poison each data batch according to the current model state. In this paper, we focus on the real-time settings and propose a new attacking strategy, which afﬁliates an accumulative phase with poisoning attacks to secretly (i.e., without affecting accuracy) magnify the destructive effect of a (poisoned) trigger batch. By mimicking online learning and federated learning on
MNIST and CIFAR-10, we show that model accuracy signiﬁcantly drops by a single update step on the trigger batch after the accumulative phase. Our work validates that a well-designed but straightforward attacking strategy can dramatically amplify the poisoning effects, with no need to explore complex techniques. 1

Introduction
In practice, machine learning services usually collect their training data from the outside world, and automate the training processes. However, untrusted data sources leave the services vulnerable to poisoning attacks [5, 28], where adversaries can inject malicious training data to degrade model accuracy. To this end, early studies mainly focus on poisoning ofﬂine datasets [36, 45, 46, 65], where the poisoned training batches are fed into models in an unordered manner, due to the usage of stochastic algorithms (e.g., SGD). In this setting, poisoning operations are executed before training, and adversaries are not allowed to intervene anymore after training begins.
On the other hand, recent work studies a more practical scene of poisoning real-time data stream-ing [63, 66], where the model is updated on user feedback or newly captured images. In this case, the adversaries can interact with the training process, and dynamically poison the data batches according to the model states. What’s more, collaborative paradigms like federated learning [30] share the model with distributed clients, which facilitates white-box accessibility to model parameters. To alleviate the threat of poisoning attacks, several defenses have been proposed, which aim to detect and ﬁlter out poisoned data via inﬂuence functions or feature statistics [6, 11, 17, 53, 57]. However, to timely update the model on real-time data streaming, on-device applications like facial recognition [62] and automatic driving [68], large-scale services like recommendation systems [24] may use some heuristic detection strategies (e.g., monitoring the accuracy or recall) to save computation [32].
In this paper, we show that in real-time data streaming, the negative effect of a poisoned or even clean data batch can be ampliﬁed by an accumulative phase, where a single update step can break down the model from 82.09% accuracy to 27.66%, as shown in our simulation experiments on CIFAR-10 [31]
∗Equal contribution. †Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The plots are visualized from the results in Table 3, where gradients are clipped by 10 under (cid:96)2-norm to simulate practical scenes. The model architecture is ResNet-18, and the batch size is 100 on CIFAR-10. A burn-in phase ﬁrst trains the model for 40 epochs (20, 000 update steps). (Left) Poisoning attacks. The burn-in model is fed with a poisoned training batch [63], after which the accuracy drops from 83.38% to 72.07%. (Right) Accumulative poisoning attacks. The burn-in model is ‘secretly’ poisoned by an accumulative phase for 2 epochs (1, 000 update steps), while keeping test accuracy in a heuristically reasonable range of variation. Then a trigger batch is fed into the model after the accumulative phase, and the model accuracy is broken down from 82.09% to 27.66% by a single update step. Note that we only use a clean trigger batch, while the destructive effect can be more signiﬁcant if we further exploit a poisoned trigger batch as in Table 6. (demo in the right panel of Fig. 1). Speciﬁcally, previous online poisoning attacks [63] apply a greedy strategy to lower down model accuracy at each update step, which limits the step-wise destructive effect as shown in the left panel of Fig. 1, and a monitor can promptly intervene to stop the malicious behavior before the model is irreparably broken down. In contrast, our accumulative phase exploits the sequentially ordered property of real-time data streaming, and induces the model state to be sensitive to a speciﬁc trigger batch by a succession of model updates. By design, the accumulative phase will not affect model accuracy to bypass the heuristic detection monitoring, and later the model will be suddenly broken down by feeding the trigger batch. The operations used in the accumulative phase can be efﬁciently computed by applying the reverse-mode automatic differentiation [21, 50]. This accumulative attacking strategy gives rise to a new threat for real-time systems, since the destruction happens only after a single update step before a monitor can perceive and intervene. Intuitively, the mechanism of the accumulative phase seems to be analogous to backdoor attacks [38, 54], while in
Sec. 3.4 we discuss the critical differences between them.
Empirically, we conduct experiments on MNIST and CIFAR-10 by simulating different training processes encountered in two typical real-time streaming settings, involving online learning [8] and federated learning [30]. We demonstrate the effectiveness of accumulative poisoning attacks, and provide extensive ablation studies on different implementation details and tricks. We show that accumulative poisoning attacks can more easily bypass defenses like anomaly detection and gradient clipping than vanilla poisoning attacks. While previous efforts primarily focus on protecting the privacy of personal/client data [42, 44, 55], much less attention is paid to defend the integrity of the shared online or federated models. Our results advocate the necessity of embedding more robust defense mechanisms against poisoning attacks when learning from real-time data streaming. 2