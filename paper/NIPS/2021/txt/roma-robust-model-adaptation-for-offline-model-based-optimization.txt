Abstract
We consider the problem of searching an input maximizing a black-box objective function given a static dataset of input-output queries. A popular approach to solving this problem is maintaining a proxy model, e.g., a deep neural network (DNN), that approximates the true objective function. Here, the main challenge is how to avoid adversarially optimized inputs during the search, i.e., the inputs where the DNN highly overestimates the true objective function. To handle the issue, we propose a new framework, coined robust model adaptation (RoMA), based on gradient-based optimization of inputs over the DNN. Speciﬁcally, it consists of two steps: (a) a pre-training strategy to robustly train the proxy model and (b) a novel adaptation procedure of the proxy model to have robust estimates for a speciﬁc set of candidate solutions. At a high level, our scheme utilizes the local smoothness prior to overcome the brittleness of the DNN. Experiments under various tasks show the effectiveness of RoMA compared with previous methods, obtaining state-of-the-art results, e.g., RoMA outperforms all at 4 out of 6 tasks and achieves runner-up results at the remaining tasks. 1

Introduction
Designing new objects with the desired property is a fundamental problem in a wide range of real-world domains, including chemistry [13], biology [5, 41], robotics [3, 28], and aircraft design
[20]. Unfortunately, many of them often require evaluating an expensive objective function, e.g., synthesizing and measuring the ﬂuorescence of the protein [5]. Model-based optimization (MBO) is a powerful framework to circumvent this issue, based on an approximation of the expensive evaluation of objective functions by a cheap proxy model. Speciﬁcally, MBO optimizes a solution with respect to the proxy model instead of the costly objective function. For obtaining a better proxy model, predominant works have typically assumed active (or online) learning scenarios, i.e., they allow for updating the model interactively with new queries of choice to approximate the ground-truth objective function better around optima [22, 37, 39, 40, 43, 45, 46].
Recently, researchers have also proposed ofﬂine versions of MBO algorithms [5, 8, 11, 25, 52] which build the proxy model from a given static dataset, i.e., input-output pairs collected from the objective function prior to building the proxy model. Such frameworks are beneﬁcial for various real-world scenarios where the new interactive access to the objective functions may accompany serious danger or expensive cost for evaluation, e.g., drug discovery or aircraft design. The common challenge to be addressed for ofﬂine MBO is to construct a proxy model that yields an accurate approximation of the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Solution update
Gaussian noise
Offline dataset
Weight perturbation
Optimization
Model adaptation
Step 1. Pre-training
Step 2. Model adaptation and solution update
Figure 1: Overall illustration of our proposed robust model adaptation (RoMA) framework.
Wrong solution
Accurate solution
Without local smoothness prior
With local smoothness prior
Figure 2: Illustration of the effect of employing local smoothness prior. true objective function outside the training dataset, as the optimization procedure mostly leads the solution to be out of the dataset and causes severe overestimation due to a domain shift [12, 48].
Intriguingly, Fu and Levine [11], Trabucco et al. [52, 53] found the gradient-based optimization of solutions to be effective for ofﬂine MBO. Such a procedure is attractive since it can utilize gradient information of the proxy model, e.g., a deep neural network (DNN), to generate ﬁne-grained solutions.
However, due to the highly non-smooth nature of DNNs, such a gradient-based optimization may generate adversarial examples [51] whose score deviates signiﬁcantly between the DNN-based proxy model and the true objective function. Existing works have attempted to regularize this behavior using conservative training [52] or normalized maximum likelihood [11]. In this paper, we investigate a more direct way to regularize such a non-smooth nature of DNN.
Contribution. We introduce a new framework, coined robust model adaptation (RoMA), for ofﬂine
MBO to optimize the solution via gradient-based updates from the DNN proxy model. Our main idea is to utilize a local smoothness prior for alleviating the brittleness of DNN at adversarial examples.
To be speciﬁc, RoMA consists of two steps: (a) a pre-training strategy to robustly train the proxy model and (b) a novel adaptive adjustment procedure of the proxy model to have robust estimates for a speciﬁc set of candidate solutions. We provide an overall illustration of our framework in Figure 1. (a) We ﬁrst introduce a pre-training procedure of the proxy model to be locally smooth at inputs and eventually becomes robust to the gradient-based input change. The novel component of the pre-training procedure is an additional regularization to force a ﬂat loss landscape with respect to the model parameters, rather than only employing regularization based on input-level perturbation into the training objective. Such consideration of the ﬂatness to the model parameters is effective in improving the robustness of the proxy model to the gradient-based updates. (b) To strengthen the accurate update of solution candidates from the trained proxy model, we also present a novel adaptive adjustment strategy of the proxy model to enhance the local smoothness at inputs outside the training data. Speciﬁcally, we propose to adjust the model at given solution candidates by adding a small perturbation to the model weights.After that, an update of solution candidates occurs from the adjusted model rather than the original DNN proxy model. We discover that the proposed adjustment strategy aptly mitigates the fragility of the proxy model at gradient-basead updates to given inputs and improves the performance. We also emphasize that the adaptive strategy is worthwhile in ofﬂine MBO as the expressive power of the proxy model is limited under the ﬁxed number of data and parameters; it is difﬁcult to sufﬁciently regularize all inputs with the domain shift from the dataset without any adjustments. 2
Algorithm 1 Robust model adaptation (RoMA) 1: for k = 0 to K do 2:
Find
θ ← arg max eθ∈B(θ) e
θ using θ such that e
Reparameterize
Minimize E(x,y)∼D,δ∼N (0,σ)(cid:2)(f (x + δ; 3: 4: 5: end for 6: Set θ−1 ← θ and choose x(0) from the training dataset D. 7: for t = 0 to T − 1 do 8:
Find θt ← arg min eθ∈B(θ) (cid:2) ||∇xf (x;
Compute x(t+1) from x(t) using gradient-based update.
θ)||2(cid:12) (cid:12) e (cid:12)x=x(t) 9: 10: end for 11: Output the ﬁnal solution x(T ).
⊲ Step 1. Pre-training with weight perturbations.
E(x,y)∼D,δ∼N (0,σ)(cid:2)(f (x + δ;
θ) − y)2(cid:3) via PGD. e
θℓ = θℓ + φℓ for some ﬁxed φℓ and ℓ = 1, . . . , L. e
θ) − y)2(cid:3) over θ using gradient-based update. e
⊲ Step 2. Optimizing the solution with iterative model adaptation.
+ α(cid:0)f (x(t);
θ) − f (x(t); θt−1)(cid:1) e 2 (cid:3) via PGD.
We extensively verify the effectiveness of our RoMA under Design-Bench [53], an ofﬂine MBO benchmark on diverse domains such as chemistry, biology, and robotics. In particular, our method achieves the best result in 4 out of 6 tasks when compared to strong baselines [5, 8, 11, 25, 52]. For the remaining tasks, RoMA at least achieves the runner-up results. When averaging over all tasks,
RoMA achieves the normalized 100th percentile score1 of 1.705, while the second-best baseline
[11] achieves 1.687. Intriguingly, our framework dramatically improves the 50th percentile scores, demonstrating how it generates diverse solutions with superior quality. 2 RoMA: Robust model adaptation 2.1 Overview of RoMA
To describe our framework, we deﬁne our problem as follows. Consider a real-valued vector x ∈ RL, e.g., protein sequence, and a real-valued objective function f ∗(x) ∈ R, e.g., ﬂuorescence of a protein sequence or the morphology of the robot. We are interested in ﬁnding an optimal input x∗ ∈ RL that maximizes the objective function, i.e., x∗ = arg maxx∈RL f ∗(x). Rather than having access to the objective function, we are given a static dataset D = {(xi, yi)}d i=1 of input-output pairs (xi, yi) queried from the objective function, i.e., yi = f ∗(xi).
Since the true objective function is unknown, our RoMA framework approximates it using a surrogate model f (x; θ) parameterized as a deep neural network (DNN) with weights θ. Then one can use gradient-based updates to ﬁnd a ﬁne-grained input maximizing the surrogate model. However, without any regularization, DNNs are prone to overﬁtting and naïve gradient ascent algorithms can easily ﬁnd adversarial examples whose score deviates signiﬁcantly between the surrogate model and the true objective function. Our main contribution lies in resolving this issue by regularizing the smoothness of the surrogate model, i.e., decreasing the sensitivity of the model with respect to input perturbations.
Such a local smoothness prior for regularizing DNNs has been successfully used to enhance the generalization of DNNs in various scenarios, e.g., adversarial robustness, generative models, and uncertainty estimation [1, 7, 9, 14, 29, 31, 35, 38, 47, 51, 56, 57]. Especially, we note the striking resemblance between our framework to the adversarial training methods [7, 51, 56]; they seek to prevent the existence of adversarial examples generated from gradient-based optimization of inputs.
In this respect, one may expect the smoothness prior to be beneﬁcial for our ofﬂine MBO framework.
To be speciﬁc, our RoMA is a two-stage framework, described as follows.
Step 1. Train the proxy model on the dataset D to approximate the true objective function with
Gaussian smoothing of inputs under worst-case weight perturbations.
Step 2. For time-steps t = 0, . . . , T − 1, update the solution x(t) after adapting the output of the proxy model to be locally smooth at the solution x(t).
We provide the detailed description of our framework in Algorithm 1. 1The percentile score is computed from 128 solutions proposed by the ofﬂine MBO algorithms. 3
In the rest of this section, we explain each component of RoMA in detail. In Section 2.2, we describe how the RoMA pre-trains the surrogate model with Gaussian smoothing (Step 1.). In Section 2.3, we explain the procedure of updating the solution while simultaneously adapting the proxy model to yield input-level smoothness at the intermediate solution (Step 2.). Finally, in Section 2.4, we describe some other algorithmic components in RoMA, such as an approach to handling discrete inputs or a procedure to select hyperparameters without the true objective function. 2.2 Pre-training with weight perturbations
We ﬁrst describe our pre-training strategy for the DNN-based proxy model f (x; θ) to approximate the true objective function f ∗(x) on training dataset D. We focus on regularizing the training with
Gaussian smoothing of inputs under worst-case weight perturbations [55]. To this end, we optimize an L-layered DNN with weight θ as the proxy model to minimize the approximation loss as follows:
L(θ) := max eθ∈B(θ)
E h (x,y)∼D,δ∼N (0,σ)(cid:2) (f (x + δ;
θ) − y)2 e (cid:3)i, (1) where δ is a random variable sampled from the Gaussian distribution N (0, σ2) with mean 0 and standard deviation σ. Furthermore, B(θ) is the set of perturbed parameters of the DNN that are near the parameter θ, deﬁned as follows:
B(θ) := n
θ : kθℓ − e
θℓkF ≤ ε · kθℓkF e
∀ ℓ ∈ {1, · · · , L}o.
Here, ε > 0 indicates the maximum magnitude of the perturbation, θℓ is the parameter of the ℓ-th layer and k·kF denotes the Frobenious norm of a matrix. To solve the inner maximization problem in (1), we utilize the projected gradient descent (PGD) [32], similar to Wu et al. [55]. We remark that our pre-training uses Gaussian noise for input perturbation, while Wu et al. [55] uses a bi-level worst-case input perturbation. We found that using the Gaussian noise leads to a much more stable training for our problem. We provide more details of this optimization procedure in the Appendix A.
Such an input-level Gaussian smoothing, i.e., augmenting inputs with additive noise sampled from a Gaussian distribution, has shown to effectively regularize the smoothness of DNNs in various domains [27, 30] and moreover improve the adversarial robustness [7]. Furthermore, training the model under the worst-case perturbation of weights induces a ﬂat loss landscape with respect to weights, which has shown to be beneﬁcial for generalization of the training objective outside the training dataset [6, 10, 21, 26, 36]. In particular, Stutz et al. [49] and Wu et al. [55] empirically observed a strong correlation between generalization capability of the model robustness at adversarial examples and the ﬂat loss landscape of the model parameters. 2.3
Iterative proxy model adaptation
Once the proxy model is pre-trained to approximate the true objective function, one may apply gradient-based updates to the intermediate solution x(t) for time-steps t = 0, . . . , T − 1, e.g., x(t+1) = x(t) + η∇xf (x; θ)|x=x(t) . However, during the pre-training stage, we apply Gaussian smoothing to the proxy model only for the training dataset D; the gradient-based updates may be erroneous for intermediate solutions far away from the training dataset D, i.e., inputs where the model is not smooth. To circumvent this issue, at each time step t, we adjust the output of the proxy model to be smooth at the intermediate solution x(t) before update these candidates. Such a model-adjustment procedure effectively focuses the model’s limited expressive power for the input x(t). This strategy can also be interpreted as a test-time adaptation procedure [54] which adapts to information of the input x(t) before estimating the true objective function f ∗(x(t)).
Formally, at time-step t, we search for the model parameter θt at the current solution candidate x(t) with the following objective, where the objective is optimized via the PGD method similar to the inner-maximization problem of (1):
θt = arg min eθ∈B(θ) h ||∇xf (x;
θ)||2(cid:12) (cid:12) e (cid:12) f (x(t);
+ α (cid:0) x=x(t)
θ) − f (x(t); θt−1) e (cid:1) 2 i where we let θ−1 = θ and α > 0 is the hyperparameter to control the output of the proxy model changing too much from the regularization. We also constrain the model parameter θt to be in the 4
local neighborhood B(θ) of the original pre-trained parameter. This adjustment allows the proxy model to maintain a good approximation for the true objective function since the corresponding
θ ∈ B(θ) from the training objective in (1). Note that we regression loss is lower-bounded for smooth the proxy model based on minimizing the input gradient norm [1, 9, 14, 47] rather than e
Gaussian smoothing as in Section 2.2; we found such a regularization to be more effective as it directly regularizes the gradient used for updating the solution. Here, one may ask why only the current intermediate solution x(t) is considered for the adjustment, rather than incorporating other inputs, e.g., the previous solution x(t−1). This is because we are interested in local behavior at the x(t) for updates, so only considering x(t) to force the local smoothness is enough for the adjustment. 2.4 Other algorithmic components
Discrete inputs. Since our framework uses gradient-based updates on continuous-valued inputs, it is non-trivial to extend for discrete-valued inputs. To resolve this issue, we propose to use a variational autoencoder (VAE) [24] which maps a discrete-valued input x to a continuous-valued latent vector z. To be speciﬁc, we train an encoder network g(x) and a decoder network h(z) using the VAE objective. Next, we train a proxy model f (z; θ) which operates on the latent space to approximate the true objective function, i.e., f ◦ g(x) ≈ f ∗(x). Then we can optimize over the space of latent vectors using RoMA with respect to the proxy model f (z; θ). After optimizing the latent vector for
T steps to obtain z(T ), we sample from the decoder h(z(t)) to report the discrete-valued solution.
Hyperparameter selection strategy. Ofﬂine MBO algorithms aim to generate highly scoring solutions under the constraint of not accessing the true objective function. Unfortunately, there is no rule of thumb for selecting hyperparameters in such an ofﬂine setting [53], and it is an active area of research. Nevertheless, we provide guidelines for selecting the hyperparameters of our RoMA in the ofﬂine setting. For the maximum magnitude ε of weight perturbations, we choose the largest value such that the pre-training of the proxy model in Section 2.2 is stable. For choosing T and α, we set it to a constant value which is task-agnostic. Indeed, we empirically observe the performance of RoMA to be robust under a different choice of such constant values. 3