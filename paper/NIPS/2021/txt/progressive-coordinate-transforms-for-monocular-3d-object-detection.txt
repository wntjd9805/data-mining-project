Abstract
Recognizing and localizing objects in the 3D space is a crucial ability for an AI agent to perceive its surrounding environment. While signiﬁcant progress has been achieved with expensive LiDAR point clouds, it poses a great challenge for 3D object detection given only a monocular image. While there exist different alternatives for tackling this problem, it is found that they are either equipped with heavy networks to fuse RGB and depth information or empirically ineffective to process millions of pseudo-LiDAR points. With in-depth examination, we realize that these limitations are rooted in inaccurate object localization. In this paper, we propose a novel and lightweight approach, dubbed Progressive Coordinate
Transforms (PCT) to facilitate learning coordinate representations. Speciﬁcally, a localization boosting mechanism with conﬁdence-aware loss is introduced to progressively reﬁne the localization prediction. In addition, semantic image repre-sentation is also exploited to compensate for the usage of patch proposals. Despite being lightweight and simple, our strategy leads to superior improvements on the KITTI and Waymo Open Dataset monocular 3D detection benchmarks. At the same time, our proposed PCT shows great generalization to most coordinate-based 3D detection frameworks. The code is available at: https://github.com/ amazon-research/progressive-coordinate-transforms. 1

Introduction
Object detection is a fundamental and challenging task in scene understanding applications. Recently, 3D object detection has received increasing attention and found applications in a wide range of scenarios such as autonomous driving, robotics, visual navigation and mixed reality. Despite the great progress from the area of 2D object detection [34, 49, 40, 18, 4], 3D object detection remains a largely unsolved problem as it aims to predict the object location in the 3D space alongside 3D object dimension and orientation.
Existing prevalent approaches [50, 44, 35, 10, 11] for 3D object detection largely rely on LiDAR sensors, which provide accurate 3D point clouds of the scene. Although these approaches achieve superior performance, the dependence on expensive equipment severely limits their applicability to generic 3D perception. There also exists a cheaper alternative that takes a single-view RGB image as input, termed as monocular 3D object detection. However, its performance is far from satisfactory as itself is an ill-posed problem due to the loss of depth information in 2D image planes. Hence, several recent attempts introduce depth information to help monocular 3D detection. Such attempts can be roughly categorized into two directions, pixel-based and coordinate-based. Pixel-based
∗Work done during an internship at Amazon.
†Li Zhang (lizhangfd@fudan.edu.cn) and Xiangyang Xue (xyxue@fudan.edu.cn) are the corresponding authors. Li Zhang is with School of Data Science, Fudan University. Li Wang and Xiangyang Xue are with
School of Computer Science, Fudan University. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Table 1: Probing investigation on coordinate-based methods, PatchNet [27] and Pseudo-LiDAR [42].
We examine the potential improvement by replacing the predicted factor with the corresponding ground truth. ∗ indicates our reproduced performance. We can see that coordinate-based methods mostly suffer from inaccurate localization.
Factor
Baseline dimension rotation x y z location(xyz)
PatchNet* [AP3D/APBEV]
Easy 36.40/46.80 40.32/47.24 36.09/46.25 45.60/56.22 40.94/46.80 55.42/68.29 81.41/85.14
Hard 21.07/28.04 24.29/28.38 23.49/27.99 27.38/34.63 24.58/28.04 35.54/45.60 57.69/66.10
Mod. 26.31/34.14 27.26/34.62 26.25/34.04 32.80/41.43 30.16/34.14 42.42/53.48 72.58/75.27
Pseudo-LiDAR* [AP3D/APBEV]
Hard
Easy
Mod. 19.67/25.67 32.27/42.45 23.04/31.06 20.88/26.60 36.09/44.35 25.88/31.97 19.85/26.07 32.42/42.74 23.88/31.31 25.08/29.92 39.69/50.78 28.36/36.77 20.69/25.67 35.19/42.45 25.53/31.06 32.24/43.32 50.04/63.96 38.37/50.81 55.64/58.27 79.13/83.77 64.36/73.37 approaches [9, 36, 31, 41] turn to use estimated depth map as additional input for improved detection performance. But at the same time, this leads to heavy computational burden and large memory footprint since they often operate on the entire image. Coordinated-based approaches [42, 29, 46, 27] pursue the coordinate representations as in LiDAR-based methods. They use the predicted depth map to convert the monocular image pixels to 3D coordinate representations, then apply a 3D detector on the converted coordinates. In particular, they are often lightweight since their network inputs are object proposals generated by 2D detectors [29, 27]. However, the performance of coordinated-based methods lags far behind LiDAR-based methods. So we ask, can we identify the bottleneck that holds back the 3D detection accuracy of coordinate-based methods and how can we improve them?
In order to determine the bottleneck, we conduct an investigation on two widely adopted coordinate-based methods, PatchNet [27] and Pseudo-LiDAR [42]. Speciﬁcally, for each prediction target, we examine the potential improvement by replacing its value with the corresponding ground truth, and then re-compute the 3D detection accuracy. As shown in Table 1, using ground truth dimension and rotation do not bring signiﬁcant improvements over the baseline. But using ground truth location (i.e., x/y/z values of the objects) almost triples detection accuracy. This indicates that coordinate-based methods mostly suffer from inaccurate localization even with the assistance of estimated depth maps.
Based on this observation, we focus on improving the accuracy of 3D center localization. In this work, we propose a lightweight and generalized approach, called Progressive Coordinate Transforms (PCT), to enhance the localization capability for coordinate-based methods. First of all, since the localization regression network in most coordinate-based methods is less accurate but lightweight, we propose to progressively reﬁne its prediction similar to gradient boosting [12, 13]. To be speciﬁc, a localization regression network can be seen as a weak learner, and we progressively train multiple consecutive networks such that each network ﬁts the regression residual from the previous networks.
These networks share the same lightweight structure so that the computation overhead is negligible.
We also predict a conﬁdence score for each network to help stabilize the end-to-end training. We term this progressive reﬁning strategy as conﬁdence-aware localization boosting (CLB). Compared to image-only or pixel-based methods, coordinated-based methods suffer from the problem of missing global context information due to the use of patched input. In order to further improve the localization accuracy, we exploit semantic image representations from 2D detector. We term this module as global context encoding (GCE). We ﬁnd that GCE can not only improve center localization accuracy, but also contribute to the ﬁnal 3D box estimation.
Through extensive experiments, our progressive coordinate transforms, consisting of CLB and GCE, is shown to improve popular coordinate-based models [42, 27] by generating more accurate localiza-tion. Without bells and whistles, we achieve state-of-the-art monocular 3D detection performance on KITTI [16, 17, 15] with a strong base method [27]. Additionally, this also leads to superior improvements on Waymo Open Dataset [38] compared with the base method PatchNet. 2