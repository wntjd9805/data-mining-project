Abstract
Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efﬁcient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global ﬁlters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efﬁciency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet. 1

Introduction
The transformer architecture, originally designed for the natural language processing (NLP) tasks [42], has shown promising performance on various vision problems recently [10, 40, 27, 49, 4, 47, 35, 5].
Different from convolutional neural networks (CNNs), vision transformer models use self-attention layers to capture long-term dependencies, which are able to learn more diverse interactions between spatial locations. The pure multi-layer perceptrons (MLP) models [38, 39] further simplify the vision transformers by replacing the self-attention layers with MLPs that are applied across spatial locations.
Since fewer inductive biases are introduced, these two kinds of models have the potential to learn more generic and ﬂexible interactions among spatial locations from raw data.
One primary challenge of applying self-attention and pure MLP models to vision tasks is the considerable computational complexity that grows quadratically as the number of tokens increases.
Therefore, typical vision transformer style models usually consider a relatively small resolution for the intermediate features (e.g. 14 × 14 tokens are extracted from the input images in both ViT [10] and MLP-Mixer [38]). This design may limit the applications of downstream dense prediction tasks like detection and segmentation. A possible solution is to replace the global self-attention with several local self-attention like Swin transformer [27]. Despite the effectiveness in practice, local self-attention brings quite a few hand-made choices (e.g., window size, padding strategy, etc.) and limits the receptive ﬁeld of each layer.
∗Equal contribution.
†Corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: The overall architecture of the Global Filter Network. Our architecture is based on
Vision Transformer (ViT) models with some minimal modiﬁcations. We replace the self-attention sub-layer with the proposed global ﬁlter layer, which consists of three key operations: a 2D discrete
Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global ﬁlters, and a 2D inverse Fourier transform to map the features back to the spatial domain. The efﬁcient fast Fourier transform (FFT) enables us to learn arbitrary interactions among spatial locations with log-linear complexity.
In this paper, we present a new conceptually simple yet computationally efﬁcient architecture called Global Filter Network (GFNet), which follows the trend of removing inductive biases from vision models while enjoying the log-linear complexity in computation. The basic idea behind our architecture is to learn the interactions among spatial locations in the frequency domain. Different from the self-attention mechanism in vision transformers and the fully connected layers in MLP models, the interactions among tokens are modeled as a set of learnable global ﬁlters that are applied to the spectrum of the input features. Since the global ﬁlters are able to cover all the frequencies, our model can capture both long-term and short-term interactions. The ﬁlters are directly learned from the raw data without introducing human priors. Our architecture is largely based on the vision transformers only with some minimal modiﬁcations. We replace the self-attention sub-layer in vision transformers with three key operations: a 2D discrete Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global ﬁlters, and a 2D inverse Fourier transform to map the features back to the spatial domain. Since the Fourier transform is used to mix the information of different tokens, the global ﬁlter is much more efﬁcient compared to the self-attention and MLP thanks to the O(L log L) complexity of the fast Fourier transform algorithm (FFT) [7]. Beneﬁting from this, the proposed global ﬁlter layer is less sensitive to the token length L and thus is compatible with larger feature maps and CNN-style hierarchical architectures without modiﬁcations. The overall architecture of
GFNet is illustrated in Figure 1. We also compare our global ﬁlter with prevalent operations in deep vision models in Table 1.
Our experiments on ImageNet verify the effectiveness of GFNet. With a similar architecture, our model outperform the recent vision transformer and MLP models including DeiT [40], ResMLP [39] and gMLP [26]. When using the hierarchical architecture, GFNet can further enlarge the gap. GFNet also works well on downstream transfer learning and semantic segmentation tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efﬁciency, generalization ability and robustness. 2