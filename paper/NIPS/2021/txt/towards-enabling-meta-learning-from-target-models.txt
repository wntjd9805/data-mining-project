Abstract
Meta-learning can extract an inductive bias from previous learning experience and assist the training of new tasks. It is often realized through optimizing a meta-model with the evaluation loss of task-speciﬁc solvers. Most existing algorithms sample non-overlapping support sets and query sets to train and evaluate the solvers respectively due to simplicity (S/Q protocol). Different from S/Q protocol, we can also evaluate a task-speciﬁc solver by comparing it to a target model T , which is the optimal model for this task or a model that behaves well enough on this task (S/T protocol). Although being short of research, S/T protocol has unique advantages such as offering more informative supervision, but it is computationally expensive.
This paper looks into this special evaluation method and takes a step towards putting it into practice. We ﬁnd that with a small ratio of tasks armed with target models, classic meta-learning algorithms can be improved a lot without consuming many resources. We empirically verify the effectiveness of S/T protocol in a typical application of meta-learning, i.e., few-shot learning. In detail, after constructing target models by ﬁne-tuning the pre-trained network on those hard tasks, we match the task-speciﬁc solvers and target models via knowledge distillation. 1

Introduction
Meta-learning means improving performance measures over a family of tasks by their training experience [22]. It has been researched in various ﬁelds such as image classiﬁcation [11, 16] and reinforcement learning [6, 14]. By reusing transferable meta-knowledge extracted from previous tasks, we can learn new tasks with a higher efﬁciency or a shortage of data.
A typical meta-learning algorithm can be decomposed into two iterative phases. In the ﬁrst phase, we train a solver of a task on its training set with assistance of meta-model. In the second phase, we optimize the solver’s performance to update meta-model. One key factor in this procedure is the way to evaluate the solver because the evaluation result acts as the supervision signal for meta-model.
Early meta-learning algorithms [19, 23] directly use the solver’s training loss as its performance metric, and optimize this metric over a distribution of tasks. Obviously, inner-task over-ﬁtting may happen during the training of task-speciﬁc solvers, resulting in an inaccurate supervision signal for the meta-model. This drawback is even more ampliﬁed in applications where the training set of each task is limited such as few-shot learning and noisy learning.
Intuitively, assessment of solvers should be independent of their training sets. This principle draws forth two important meta-learning algorithms in 2016 [24, 28], which respectively export solver evaluation from the perspective of “data” and “model”. In this paper, we call these two methodologies
S/Q protocol and S/T protocol. In S/Q protocol, S means support set and Q means query set. They
∗De-Chuan Zhan is the corresponding author. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
contain non-overlapping instances sampled from a same distribution. By training the solver on S and evaluating it on Q, we are able to obtain an approximate generalization error of the solver and eventually provide the meta-model with a reliable supervision signal. Another choice is to compare the trained solver with an ideal target model T . Assuming that T works well on a task, we can minimize the discrepancy between the trained solver and T to pull the solver closer to T . Here T can be Bayesian optimal solution to a task or a model trained on a sufﬁciently informative dataset.
Figure 1 gives an illustration of both S/Q protocol and S/T protocol.
Although appeared in the same year, S/Q pro-tocol is more widely accepted by meta-learning society [4, 8, 13, 10] while the research about how to leverage target models remains imma-ture. The main reason is the simplicity of S/Q and the computational hardness of S/T . How-ever, S/T protocol has some unique advantages.
Firstly, it does not depend on possibly biased and noisy query sets. Secondly, by viewing sup-port sets and their corresponding target mod-els as (feature, label) samples, meta-learning is reduced to supervised learning and we can transfer insights from supervised learning to im-prove meta-learning [2]. Thirdly, we can treat the target model as a teacher and incorporate a teacher-student framework like knowledge dis-tillation [7] and curriculum learning [1] in meta-learning. Thus, it is necessary and meaningful to study S/T protocol in meta-learning. (a) S/Q protocol for meta-learning. (b) S/T protocol for meta-learning.
This paper looks into S/T protocol and takes a step towards enabling meta-learning from target models. We mainly answer two questions: (1)
If we already have access to target models, how to learn from them? What are the beneﬁts of learning from them? (2) In a real-world applica-tion, how to obtain target models efﬁciently and make S/T protocol computationally tractable?
For the ﬁrst question, we propose to match the task-speciﬁc solver to the target model in output space. Learning from target models brings us more robust solvers. For the second question, we focus on a typical application scenario of meta-learning, i.e., few-shot learning. We construct target models by ﬁne-tuning the globally pre-trained network on those hard tasks to maintain efﬁciency.
Figure 1: Comparison between S/Q protocol and S/T protocol. (a) In S/Q protocol, each task contains a sup-port set S and a query set Q. We train a solver on S and evaluate it on Q, and query loss is used to optimize meta-model. (b) In S/T protocol, each task contains a support set S and a target model T . After training a solver on S, we directly minimize the discrepancy between it and T . 2