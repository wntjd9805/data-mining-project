Abstract
This paper considers incremental few-shot learning, which requires a model to continually recognize new categories with only a few examples provided. Our study shows that existing methods severely suffer from catastrophic forgetting, a well-known problem in incremental learning, which is aggravated due to data scarcity and imbalance in the few-shot setting. Our analysis further suggests that to prevent catastrophic forgetting, actions need to be taken in the primitive stage – the training of base classes instead of later few-shot learning sessions. Therefore, we propose to search for ﬂat local minima of the base training objective function and then ﬁne-tune the model parameters within the ﬂat region on new tasks. In this way, the model can efﬁciently learn new classes while preserving the old ones.
Comprehensive experimental results demonstrate that our approach outperforms all prior state-of-the-art methods and is very close to the approximate upper bound.
The source code is available at https://github.com/moukamisama/F2M. 1

Introduction
Why study incremental few-shot learning? Incremental learning enables a model to continually learn new concepts from new data without forgetting previously learned knowledge. Rooted from real-world applications, this topic has attracted a signiﬁcant amount of interest in recent years [5, 30, 31, 40, 26]. Incremental learning assumes sufﬁcient training data is provided for new classes, which is impractical in many application scenarios, especially when the new classes are rare categories which are costly or difﬁcult to collect. This motivates the study of incremental few-shot learning, a more difﬁcult paradigm that aims to continually learn new tasks with only a few examples.
Challenges. The major challenge for incremental learning is catastrophic forgetting [14, 28, 35], which refers to the drastic performance drop on previous tasks after learning new tasks. This phenomenon is caused by the inaccessibility to previous data while learning on new data. Catastrophic forgetting presents a bigger challenge for incremental few-shot learning. Due to the small amount of training data in new tasks, the model tends to severely overﬁt on new classes while quickly forgetting old classes, resulting in catastrophic performance.
Current research. The study of incremental few-shot learning has just started [47, 41, 60, 9, 8, 34, 59]. Current works mainly borrow ideas from research in incremental learning to overcome the forgetting problem, by enforcing strong constraints on model parameters to penalize the changes of parameters [34, 28, 56], or by saving a small amount of exemplars from old classes and adding constraints on the exemplars to avoid forgetting [40, 20, 4]. However, in our empirical study, we ﬁnd
∗Equal contribution
†Corresponding author 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
that an intransigent model that only trains on base classes and does not tune on new tasks consistently outperforms state-of-the-art methods, including a joint-training method [47] that uses all encountered data for training and hence suffers from severe data imbalance. This observation motivates us to address this harsh problem from a different angle.
Our solution. Unlike existing solutions that try to overcome the catastrophic forgetting problem during the process of learning new tasks, we adopt a different approach by considering this issue during the training of base classes. Speciﬁcally, we propose to search for ﬂat local minima of the base training objective function. For any parameter vector in the ﬂat region around the minima, the loss is small, and the base classes are supposed to be well separated. The ﬂat local minima can be found by adding random noise to the model parameters for multiple times and jointly optimizing multiple loss functions. During the following incremental few-shot learning stage, we ﬁne-tune the model parameters within the ﬂat region, which can be achieved by clamping the parameters after updating them on few-shot tasks. In this way, the model can efﬁciently learn new classes while preserving the old ones. Our key contributions are summarized as follows:
• We conduct a comprehensive empirical study on existing incremental few-shot learning methods and discover that a simple baseline model that only trains on base classes outper-forms state-of-the-art methods, which demonstrates the severity of catastrophic forgetting.
• We propose a novel approach for incremental few-shot learning by addressing the catas-trophic forgetting problem in the primitive stage. Through ﬁnding the ﬂat minima region during training on base classes and ﬁne-tuning within the region while learning on new tasks, our model can overcome catastrophic forgetting and avoid overﬁtting.
• Comprehensive experimental results on CIFAR-100, miniImageNet, and CUB-200-2011 show that our approach outperforms all state-of-the-art methods and achieves performance that is very close to the approximate upper bound. 2