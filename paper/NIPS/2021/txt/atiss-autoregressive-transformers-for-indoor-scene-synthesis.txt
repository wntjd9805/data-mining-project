Abstract
The ability to synthesize realistic and diverse indoor furniture layouts automatically or based on partial input, unlocks many applications, from better interactive 3D tools to data synthesis for training and simulation.
In this paper, we present
ATISS, a novel autoregressive transformer architecture for creating diverse and plausible synthetic indoor environments, given only the room type and its ﬂoor plan. In contrast to prior work, which poses scene synthesis as sequence generation, our model generates rooms as unordered sets of objects. We argue that this formulation is more natural, as it makes ATISS generally useful beyond fully automatic room layout synthesis. For example, the same trained model can be used in interactive applications for general scene completion, partial room re-arrangement with any objects speciﬁed by the user, as well as object suggestions for any partial room. To enable this, our model leverages the permutation equivariance of the transformer when conditioning on the partial scene, and is trained to be permutation-invariant across object orderings. Our model is trained end-to-end as an autoregressive generative model using only labeled 3D bounding boxes as supervision. Evaluations on four room types in the 3D-FRONT dataset demonstrate that our model consistently generates plausible room layouts that are more realistic than existing methods. In addition, it has fewer parameters, is simpler to implement and train and runs up to 8x faster than existing methods. 1

Introduction
Generating synthetic 3D content that is both realistic and diverse is a long-standing problem in computer vision and graphics. In the last decade, there has been increased demand for tools that automate the creation of 3D artiﬁcial environments for applications like video games and AR/VR, as well as general 3D content creation [61, 16, 36, 4, 62]. These tools can also synthesize data to train computer vision models, avoiding expensive and laborious annotations. Generative models
[28, 19, 13, 29, 56] have demonstrated impressive results on synthesizing photorealistic images
[7, 1, 24, 8, 25] and intelligible text [46, 2], and are beginning to be adopted for the generation of 3D environments.
Recent works proposed to solve the scene synthesis task by incorporating procedural modeling techniques [45, 43, 23, 9] or by generating scene graphs with generative models [34, 57, 65, 35, 44, 64, 63, 27, 12]. Procedural modeling requires specifying a set of rules for the scene formation process,
∗Work done during Despoina’s internship at NVIDIA. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
Figure 1: Motivation In addition to fully automatic layout synthesis (A), our formulation in terms of unordered sets of objects allows our model to be used for novel interactive applications with versatile user control: scene completion given any number of existing furniture pieces of any class pinned to a speciﬁc location by the user (B), and object suggestions with user-provided constraints (object centroid constraint shown in red) (C). but acquiring these rules is a time-consuming task, requiring skills of experienced artists. Similarly, graph-based approaches require scene graph annotations, which may be laborious to obtain.
Another line of research utilizes CNN-based [58, 47] and transformer-based [59] architectures to generate rooms by autoregressively selecting and placing objects in a scene, i.e. one after the other.
These approaches represent scenes as ordered sequences of objects. Typically, the ordering is deﬁned using the spatial arrangement of objects in a room (e.g. left-to-right) [22] or the object class frequency (e.g. most to least probable) [47, 59]. Such orderings impose unnatural constraints on the scene generation process, inhibiting practical applications. For example, in [47, 59], which order objects by class frequency, the probability of a bed (more common) appearing after an ottoman (less common) in the training set is zero. As a result, these methods cannot generate more common objects after less common objects, which makes them impractical for interactive tasks like general room completion and partial room re-arrangement, where input is unconstrained (e.g. Fig.1B).
To address these limitations, we pose scene synthesis as an unordered set generation problem and introduce ATISS, a novel autoregressive transformer architecture to model this process. Given a room type (e.g. bedroom, living room) and its shape, our model generates meaningful furniture arrangements by sequentially placing objects in a permutation-invariant fashion. We train ATISS to maximize the log-likelihood of all possible permutations of object arrangements in a collection of training scenes, labeled only with object classes and 3D bounding boxes, which are easier to obtain, than costly support relationship [57] or scene graph annotations [34]. Unlike existing works [58, 47, 59], we propose the ﬁrst model to perform scene synthesis as an autoregressive set generation task. ATISS is signiﬁcantly simpler to implement and train, requires fewer parameters and is up to 8× faster at run-time than the fastest available baseline [59]. Furthermore, we demonstrate that our model generates more plausible object arrangments without any post-processing on the predicted layout. Our formulation allows applying a single trained model to automatic layout synthesis and to a number of interactive scenarios with versatile user input (Fig.1), such as automatic placement of user-provided objects, object suggestion with user-provided constraints, and room completion. Code and data are publicaly available at https://nv-tlabs.github.io/ATISS. 2