Abstract
Adapting large-scale pretrained language models to downstream tasks via
ﬁne-tuning is the standard method for achieving state-of-the-art performance on
NLP benchmarks. However, ﬁne-tuning all weights of models with millions or billions of parameters is sample-inefﬁcient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efﬁcient ﬁne-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard
ﬁne-tuning.
In this work, we propose COMPACTER, a method for ﬁne-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. COMPACTER accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers.
Speciﬁcally, COMPACTER inserts task-speciﬁc weight matrices into a pretrained model’s weights, which are computed efﬁciently as a sum of Kronecker products be-tween shared “slow” weights and “fast” rank-one matrices deﬁned per COMPACTER layer. By only training 0.047% of a pretrained model’s parameters, COMPACTER performs on par with standard ﬁne-tuning on GLUE and outperforms standard
ﬁne-tuning on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter. 1

Introduction
With four parameters I can ﬁt an elephant, and with ﬁve I can make him wiggle his trunk.
State-of-the-art pretrained language models (PLMs) in natural language processing (NLP) have used heavily over-parameterized representations consisting of hundreds of millions or billions of parameters to achieve success on a wide range of
NLP benchmarks [2, 3, 4]. These models are generally applied to downstream tasks via ﬁne-tuning
[5], which requires updating all parameters and storing one copy of the ﬁne-tuned model per task.
This causes substantial storage and deployment costs and hinders the applicability of large-scale PLMs to real-world applications. Additionally, ﬁne-tuning of over-parameterized models on low-resource datasets has been shown to be subject to instabilities and may lead to poor performance [6, 7].
John von Neumann
Inspired by John von Neumann’s quotation, we ask, given that we have already learned general-purpose language representations via a PLM (i.e. we have ﬁt our elephant), how many more parameters 35th Conference on Neural Information Processing Systems (NeurIPS 2021)
Figure 2: Left: Adapter integration in a pretrained transformer model.
Right: Adapter architecture. Follow-ing Houlsby et al. [1], we include adapters after the attention and feed-forward modules. During training, we only update layer normalizations and adapters (shown in yellow), while the pretrained model is ﬁxed.
Figure 1: The average score on GLUE (y axis), percentage of trainable parameters per task (x axis, in log scale), and memory footprint (size of the circles) of different methods. do we need to reach state-of-the-art performance on standard NLP tasks. Speciﬁcally, we aim to develop practical, memory-efﬁcient methods that train a minimum set of parameters while achieving performance on par or better than full ﬁne-tuning for state-of-the-art NLP models.
Recent literature has introduced parameter-efﬁcient ﬁne-tuning methods. These approaches generally keep the pretrained model’s parameters ﬁxed and introduce a set of trainable parameters per task, trading off the number of trainable parameters with task performance. At one end of the spectrum, prompts, i.e. natural language descriptions of a task, together with demonstrations have been used to achieve reasonable performance without any parameter updates on some benchmarks [8] but their performance generally lags behind ﬁne-tuned models. They also require huge models to work well but choosing good prompts becomes harder with larger model sizes [9]. Soft prompt methods treat prompts as trainable continuous parameters, which are prepended to the inputs at the input layer or intermediate layers [10, 11, 12]. Such methods, however, often require large models to achieve good performance and are very sensitive to initialization and unstable during training.
The theoretically motivated low-rank methods train a small number of parameters that lie in a low-dimensional subspace using random projections [13, 14]. However, storing the random projection matrices causes substantial memory overhead and leads to slow training times. At the other end of the spectrum, adapter methods [1, 15] that insert trainable transformations at different layers of the pretrained model require more parameters than the aforementioned approaches but are more memory-efﬁcient and obtain performance comparable to full ﬁne-tuning [1, 16].
In this work, we propose COMPACTER, a method for ﬁne-tuning large-scale language models with an excellent trade-off between the number of trainable parameters, task performance, and memory footprint, compared to existing methods (see Figure 1). COMPACTER builds on ideas from adapters
[1], low-rank methods [13], as well as recent hypercomplex multiplication layers [17]. Similar to adapters, COMPACTER inserts task-speciﬁc weight matrices into a pretrained model’s weights. Each
COMPACTER weight matrix is computed as the sum of Kronecker products between shared “slow” weights and “fast” rank-one matrices deﬁned per COMPACTER layer (see Figure 3). As a result,
COMPACTER achieves a parameter complexity of O(k+d) compared to O(kd) for regular adapters, where the adapters are of size k×d. In practice, COMPACTER trains 0.047% of a PLM’s parameters.
On the standard GLUE [18] and SuperGLUE [19] benchmarks, COMPACTER outperforms other parameter-efﬁcient ﬁne-tuning methods and obtains performance on par or better than full ﬁne-tuning.
On low-resource settings, COMPACTER outperforms standard ﬁne-tuning.
In summary, we make the following contributions: 1) We propose COMPACTER (Compact
Adapter) layers, a parameter-efﬁcient method to adapt large-scale language models. 2) We show that
COMPACTER obtains strong empirical performance on GLUE and SuperGLUE. 3) We demonstrate that 2
COMPACTER outperforms ﬁne-tuning in low-resource settings. 4) We provide a parameter complexity analysis of COMPACTER, showing that it requires dramatically fewer parameters than adapters and
ﬁne-tuning. 5) We provide a systematic evaluation of recent parameter-efﬁcient ﬁne-tuning methods in terms of training time and memory consumption. We release our code to facilitate future work. 2