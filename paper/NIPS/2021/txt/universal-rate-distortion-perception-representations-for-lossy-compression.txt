Abstract
In the context of lossy compression, Blau & Michaeli [5] adopt a mathematical notion of perceptual quality and define the information rate-distortion-perception function, generalizing the classical rate-distortion tradeoff. We consider the notion of universal representations in which one may fix an encoder and vary the decoder to achieve any point within a collection of distortion and perception constraints.
We prove that the corresponding information-theoretic universal rate-distortion-perception function is operationally achievable in an approximate sense. Under
MSE distortion, we show that the entire distortion-perception tradeoff of a Gaussian source can be achieved by a single encoder of the same rate asymptotically. We then characterize the achievable distortion-perception region for a fixed representation in the case of arbitrary distributions, and identify conditions under which the aforementioned results continue to hold approximately. This motivates the study of practical constructions that are approximately universal across the RDP tradeoff, thereby alleviating the need to design a new encoder for each objective. We provide experimental results on MNIST and SVHN suggesting that on image compression tasks, the operational tradeoffs achieved by machine learning models with a fixed encoder suffer only a small penalty when compared to their variable encoder counterparts. 1

Introduction
Unlike in lossless compression, the decoder in a lossy compression system has flexibility in how to reconstruct the source. Conventionally, some measure of distortion such as mean squared error, PSNR or SSIM/MS-SSIM [36, 37] is used as a quality measure. Accordingly, lossy compression algorithms are analyzed through rate-distortion theory, wherein the objective is to minimize the amount of distortion for a given rate. However, it has been observed that low distortion is not necessarily synonymous with high perceptual quality; indeed, deep learning based image compression has inspired works in which authors have noted that increased perceptual quality may come at the cost of increased distortion [1, 4]. This culminated in the work of Blau & Michaeli [5] who propose the rate-distortion-perception theoretical framework.
The main idea was to introduce a third perception axis which more closely mimics what humans would deem to be visually pleasing. Unlike distortion, judgement of perceptual quality is taken to 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
be inherently no-reference. The mathematical proxy for perceptual quality then comes in the form of a divergence between the source and the reconstruction distributions, motivated by the idea that perfect perceptual quality is achieved when they are identical. Leveraging generative adversarial networks [11] in the training procedure has made such a task possible for complex data-driven settings with efficacy even at very low rates [33]. Naturally, this induces a tradeoff between optimizing for perceptual quality and optimizing for distortion. But in designing a lossy compression system, one may wonder where exactly this tradeoff lies: is the objective tightly coupled with optimizing the representations generated by the encoder, or can most of this tradeoff be achieved by simply changing the decoding scheme?
Our contributions are as follows. We define the notion of universal representations which are generated by a fixed encoding scheme for the purpose of operating at multiple perception-distortion tradeoff points attained by varying the decoder. We then prove a coding theorem establishing the relationship between this operational definition and an information universal rate-distortion-perception function. Under MSE distortion loss, we study this function for the special case of the
Gaussian distribution and show that the penalty in fixing the representation map with fixed rate can be small in many interesting regimes. For general distributions, we characterize the achievable distortion-perception region with respect to an arbitrary representation and establish a certain approximate universality property.
We then turn to study how the operational tradeoffs achieved by machine learning models on image compression under a fixed encoder compared to varying encoders. Our results suggest that there is not much loss in reusing encoders trained for a specific point on the distortion-perception tradeoff across other points. The practical implication of this is to reduce the number of models to be trained within deep-learning enhanced compression systems. Building on [30, 31], one of the key steps in our techniques is the assumption of common randomness between the sender and receiver which will turn out to reduce the coding cost. Throughout this work, we focus on the scenario where a rate is fixed in advance. We address the scenario when the rate is changed in the supplementary. 2