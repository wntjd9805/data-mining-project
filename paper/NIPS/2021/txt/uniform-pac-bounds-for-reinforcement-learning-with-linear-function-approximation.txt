Abstract
We study reinforcement learning (RL) with linear function approximation. Existing algorithms for this problem only have high-probability regret and/or Probably
Approximately Correct (PAC) sample complexity guarantees, which cannot guar-antee the convergence to the optimal policy. In this paper, in order to overcome the limitation of existing algorithms, we propose a new algorithm called FLUTE, which enjoys uniform-PAC convergence to the optimal policy with high probability.
The uniform-PAC guarantee is the strongest possible guarantee for reinforcement learning in the literature, which can directly imply both PAC and high probability regret bounds, making our algorithm superior to all existing algorithms with linear function approximation. At the core of our algorithm is a novel minimax value function estimator and a multi-level partition scheme to select the training samples from historical observations. Both of these techniques are new and of independent interest. 1

Introduction
Designing efﬁcient reinforcement learning (RL) algorithms for environments with large state and action spaces is one of the main tasks in the RL community. To achieve this goal, function ap-proximation, which uses a class of predeﬁned functions to approximate either the value function or transition dynamic, has been widely studied in recent years. Speciﬁcally, a series of recent works
[11, 13, 18, 24, 3, 27] have studied RL with linear function approximation with provable guarantees.
They show that with linear function approximation, one can either obtain a sublinear regret bound against the optimal value function [13, 24, 3, 27] or a polynomial sample complexity bound [14] (Probably Approximately Correct (PAC) bound for short) in ﬁnding a near-optimal policy [11, 18].
However, neither the regret bound or PAC bound is a perfect performance measure. As discussed in detail by [7], these two measures fail to guarantee the convergence to the optimal policy. Therefore, an algorithm with high probability regret and/or PAC bound guarantees do not necessarily learn the optimal policy, and can perform badly in practice. In detail, one can face the following two situations:
• An algorithm with a sublinear regret suggests that the summation of the suboptimality gaps ∆t (the gap between the values of the current adapted policy and optimal policy, see Deﬁnition 3.3 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
for details.) in the ﬁrst T rounds is bounded by o(T ). However, this algorithm may be arbitrarily suboptimal inﬁnitely times 1, thus it fails to converge to the optimal policy.
• An algorithm is ((cid:15), δ)-PAC suggests that with probability at least 1 − δ, the number of suboptimality gaps ∆t that are greater than (cid:15) will be at most polynomial in (cid:15) and log(1/δ). The formal deﬁnition of ((cid:15), δ)-PAC can be found in Deﬁnition 3.4. However, this algorithm may still have gaps satisfying (cid:15)/2 < ∆t < (cid:15) inﬁnitely often, thus fails to converge to the optimal policy.
To overcome the limitations of regret and PAC guarantees, Dann et al. [7] proposed a new performance measure called uniform-PAC, which is a strengthened notion of the PAC framework. Speciﬁcally, an algorithm is uniform-PAC if there exists a function of the target accuracy (cid:15) and the conﬁdence parameter δ that upper bounds the number of suboptimality gaps satisfying ∆t > (cid:15) simultaneously for all (cid:15) > 0 with probability at least 1 − δ. The formal deﬁnition of uniform-PAC can be found in
Deﬁnition 3.6. Algorithms that are uniform-PAC converge to an optimal policy with high probability, and yield both PAC and high probability regret bounds. In addition, they proposed a UBEV algorithm for learning tabular MDPs, which is uniform-PAC. Nevertheless, UBEV is designed for tabular MDPs, and it is not clear how to incorporate function approximation into UBEV to scale it up for large (or even inﬁnite) state and action space. Therefore, a natural question arises:
Can we design a provable efﬁcient uniform-PAC RL algorithm with linear function approximation?
In this work, we answer this question afﬁrmatively. In detail, we propose new algorithms for both contextual linear bandits and linear Markov decision processes (MDPs) [22, 13]. Both of them are uniform-PAC, and their sample complexity is comparable to that of the state-of-the-art algorithms which are not uniform-PAC. Our key contributions are highlighted as follows.
• We begin with contextual linear bandits problem as a “warm-up” example of the RL with linear function approximation (with horizon length equals 1). We propose a new algorithm called uniform-PAC OFUL (UPAC-OFUL), and show that our algorithm is uniform-PAC with (cid:101)O(d2/(cid:15)2) sample complexity, where d is the dimension of contexts and (cid:15) is the accuracy parameter. In addition, this result also implies an (cid:101)O(d
T ) regret in the ﬁrst T round and matches the result of OFUL algorithm [1] up to a logarithmic factor. The key idea of our algorithm is a novel minimax linear predictor and a multi-level partition scheme to select the training samples from past observations.
To the best of our knowledge, this is the ﬁrst algorithm with a uniform PAC-bound for contextual bandits problems.
√
• We also consider RL with linear function approximation in episodic linear MDPs, where the transition kernel admits a low-rank factorization. We propose an algorithm dubbed uniForm-PAC
Least-sqUare value iTEration (FLUTE), which adapts the novel techniques we developed in the contextual linear bandits setting, and show that our algorithm is uniform-PAC with (cid:101)O(d3H 5/(cid:15)2) sample complexity, where d is the dimension of the feature mapping, H is the length of episodes d3H 4T ) regret in the ﬁrst T and (cid:15) is the accuracy parameter. This result further implies an (cid:101)O(
H-factor, while LSVI-UCB steps and matches the result of LSVI-UCB algorithm [13] up to a is not uniform-PAC. Again, FLUTE is the ﬁrst uniform-PAC RL algorithm with linear function approximation.
√
√
Notation We use lower case letters to denote scalars, and use lower and upper case bold face letters to denote vectors and matrices respectively. For any positive integer n, we denote by [n] the set
{1, . . . , n}. For a vector x ∈ Rd , we denote by (cid:107)x(cid:107)1 the Manhattan norm and denote by (cid:107)x(cid:107)2 the
Euclidean norm. For a vector x ∈ Rd and matrix Σ ∈ Rd×d, we deﬁne (cid:107)x(cid:107)Σ = x(cid:62)Σx. For two sequences {an} and {bn}, we write an = O(bn) if there exists an absolute constant C such that an ≤ Cbn. We use (cid:101)O(·) to further hide the logarithmic factors. For logarithmic regret, we use (cid:101)O(·) to hide all logarithmic terms except log T .
√ 1Suppose the suboptimality gaps satisfy ∆t = 1{t = i2, i = 1, . . . }, then the regret in the ﬁrst T rounds is
√ upper bounded by O(
T ), and the constant 1-gap will appear inﬁnitely often 2
2