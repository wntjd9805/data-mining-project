Abstract
The theory of spectral ﬁltering is a remarkable tool to understand the statistical properties of learning with kernels. For least squares, it allows to derive various regularization schemes that yield faster convergence rates of the excess risk than with Tikhonov regularization. This is typically achieved by leveraging classical assumptions called source and capacity conditions, which characterize the difﬁ-culty of the learning task. In order to understand estimators derived from other loss functions, Marteau-Ferey et al. [1] have extended the theory of Tikhonov reg-ularization to generalized self concordant loss functions (GSC), which contain, e.g., the logistic loss. In this paper, we go a step further and show that fast and op-timal rates can be achieved for GSC by using the iterated Tikhonov regularization scheme, which is intrinsically related to the proximal point method in optimiza-tion, and overcomes the limitation of the classical Tikhonov regularization. 1

Introduction
We consider the problem of supervised learning where we want to ﬁnd a prediction function θ mapping an input point x living in a set
. In this paper, we assume that θ lives in to a label y in and is learned from a set of observations (xi, yi)i=1,...,n that are i.i.d. a separable Hilbert space
. The goal is to ﬁnd θ that samples drawn from an unknown probability distribution ρ on minimizes the expected risk L, which is deﬁned below along with the empirical risk ˆL:
X × Y
H
X
Y (cid:90)
L(θ) = (cid:96)(y, θ(x))dρ(x, y),
X ×Y (cid:98)L(θ) = 1 n n (cid:88) i=1 (cid:96)(yi, θ(xi)), (1) where (cid:96) is a suitable loss function comparing true labels with predictions. This paper aims for upper bounds on the excess risk for a speciﬁc estimator (cid:98)θ. That is, we assume that the minimum of the expected risk is attained for some θ(cid:63) in
, and we want to derive probabilistic upper bounds on the excess risk:
H
L(θ(cid:63)) > C1n−γ log 2
δ (cid:105)
P (cid:104)
L((cid:98)θ)
−
δ,
≤ (2) given some value δ in (0, 1), where C1 is a positive constant, and (cid:98)θ is an estimator built from the n observations. The quantity O(n−γ) denotes the rate of convergence of the estimator ˆθ. A classical
“slow” rate with γ = 1/2 is typically achieved by many estimators and is in fact optimal if only mild assumptions are made about the data distribution ρ. Even though optimal, this rate is nevertheless a worst case and faster rates with γ > 1/2 can be achieved both in theory and in practice, by making additional assumptions about the difﬁculty of the learning task. Originally introduced in the
∗Inria, ´Ecole normale sup´erieure, CNRS, PSL Research University, 75005 Paris, France
†Inria, Univ. Grenoble Alpes, CNRS, Grenoble INP, LJK, 38000 Grenoble, France 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
literature of inverse problems, the so-called source and capacity conditions have been shown to be appropriate for this purpose, leading to statistical analysis with fast rates of convergence [1, 2, 3].
The optimality of results of the form (2) is characterized by comparing them with lower bounds that are available for various sets of data distributions ρ [3]. Matching upper bounds with lower bounds ensures that the estimator (cid:98)θ is optimal, in the sense that no information is lost in the process of exploiting the data samples to compute (cid:98)θ, for the given set of distributions.
In this search for optimal estimators, most of the attention has been devoted to minimizers of some function of the empirical risk (cid:98)L, which is deﬁned in (Eq. (1)). Then, the key challenge is to regularize (cid:98)L in order to achieve better generalization properties. The most widely used scheme is probably
Tikhonov regularization; other examples when is a RKHS include truncated regression [4], or
H early stopping in gradient descent algorithms [5, 6]. When the loss (cid:96) is set to least squares, it can be shown that minimizing the excess risk amounts to solving an ill-posed inverse problem [7], which led to the remarkable theory of spectral ﬁltering. A large class of regularization schemes can indeed be seen as a ﬁltering process applied to the training labels yi after regularizing the spectrum of the kernel matrix [2, 8]. Interestingly, this theory has highlighted the fact that not all regularization schemes are equal: some of them obtain fast learning rates in (2) on “easy” problem (a thorough deﬁnition is given in Section 2) while others cannot leverage this additional regularity to improve the learning rate.
Such a general analysis for least squares is made possible by the fact that a closed-form expression of the estimator is available. When considering different loss function (cid:96), the estimator (cid:98)θ is unfortunately only implicitly available as the solution of an optimization problem involving (cid:98)L. A step to extend least squares results to more general loss functions has been achieved by Marteau-Ferey et al. [1], who provide bounds on the form (2) for Tikhonov estimator on generalized self concordant (GSC) functions. GSC functions are three-times-differentiable functions whose third derivative is bounded by the second-derivative. In practice, they were introduced to conduct a general analysis of the
Newton method in optimization [9, 10], and adapted in [11] to encompass a larger class of loss function. It includes notably the logistic regression loss, which is widely used for classiﬁcation.
While Tikhonov yields fast rates of convergence in several data regimes, it is known to be unable to adapt to the whole range of learning task difﬁculties. More precisely, it suffers from a “saturation” effect [2], meaning that when the learning task becomes simpler, the learning rate stops improving and is suboptimal. Our paper addresses this limitation for GSC functions by considering instead the iterated Tikhonov regularization (IT) scheme. In the context of least squares, this approach consists of successively ﬁtting the residuals. For more general loss functions, it is equivalent to performing a few steps of the proximal point method in optimization [12]. Our main result is a probabilistic upper bound on the excess risk, which is optimal given usual source and capacity conditions assumptions on the learning task, thus addressing the limitations of the classical Tikhonov regularization. 2