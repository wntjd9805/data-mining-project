Abstract
One of the challenges in online reinforcement learning (RL) is that the agent needs to trade off the exploration of the environment and the exploitation of the samples to optimize its behavior. Whether we optimize for regret, sample complexity, state-space coverage or model estimation, we need to strike a dif-ferent exploration-exploitation trade-off. In this paper, we propose to tackle the exploration-exploitation problem following a decoupled approach composed of: 1) An “objective-speciﬁc” algorithm that (adaptively) prescribes how many samples to collect at which states, as if it has access to a generative model (i.e., a simulator of the environment); 2) An “objective-agnostic” sample collection exploration strat-egy responsible for generating the prescribed samples as fast as possible. Building on recent methods for exploration in the stochastic shortest path problem, we ﬁrst provide an algorithm that, given as input the number of samples b(s, a) needed in each state-action pair, requires (cid:101)O(cid:0)BD + D3/2S2A(cid:1) time steps to collect the
B = (cid:80) s,a b(s, a) desired samples, in any unknown communicating MDP with
S states, A actions and diameter D. Then we show how this general-purpose ex-ploration algorithm can be paired with “objective-speciﬁc” strategies that prescribe the sample requirements to tackle a variety of settings — e.g., model estimation, sparse reward discovery, goal-free cost-free exploration in communicating MDPs
— for which we obtain improved or novel sample complexity guarantees. 1

Introduction
One of the challenges in online reinforcement learning (RL) is that the agent needs to trade off the exploration of the environment and the exploitation of the samples to optimize its behavior.
Whenever the agent needs to gather information about a speciﬁc region of the Markov decision process (MDP), it must plan for a policy to reach the desired states, despite not having exact knowledge of the environment dynamics. This makes solving the exploration-exploitation problem in RL highly non-trivial and it requires designing a speciﬁc strategy depending on the learning objective, such as PAC-MDP learning [e.g., 13, 47, 59], regret minimization [e.g., 28, 6, 29, 66] or pure exploration [e.g., 30, 31, 39, 63, 64].
A simpler scenario considered in the literature is to assume access to a generative model or sampling returns a next state s(cid:48) drawn from the oracle ( transition probability p( s, a) and a reward r(s, a). In this case, it is possible to focus exclusively on where and how many samples to collect, while disregarding the problem of ﬁnding a suitable policy
) [33]. Given any state-action pair (s, a), the
SO
SO
·| 35th Conference on Neural Information Processing Systems (NeurIPS 2021).
SO can be used to obtain samples from the environment, which to obtain them. For instance, an are combined with dynamic programming techniques to compute an ε-optimal policy.
-based algorithms can be as simple as prescribing the same amount of samples from each state-action pair
[e.g. 35, 33, 5, 16, 46, 1, 38] or they may adaptively change the sample requirements on different state-action pairs [e.g. 15, 58, 62]. An is also used in Monte-Carlo planning [49, 25, 7] which focuses on computing the optimal action at the current state by optimizing over rollout trajectories sampled
. Finally, in multi-armed bandit [37], there are cases where each arm corresponds to from the a state (or state-action), and “pulling” an arm translates into a call to an (see e.g., the pure exploration setting of [51]). Unfortunately, while an may be available in domains such as simulated robotics and computer games, this is not the case in the more general online RL setting.
SO
SO
SO
SO
SO
SO
In this paper we tackle the exploration-exploitation problem in online RL by drawing inspiration assumption. Speciﬁcally, we deﬁne an approach that is decoupled in two parts: 1) an from the
“objective-speciﬁc” algorithm that assumes access to an that (adaptively) prescribes the samples needed to achieve the learning objective of interest, and 2) an “objective-agnostic” algorithm that takes on the exploration challenge of collecting the samples requested by the
-based algorithm as quickly as possible.1 Our main contributions can be summarized as follows:
• We deﬁne the sample complexity of the objective-agnostic algorithm as the number of (online) steps needed to satisfy the prescribed sampling requirements. Leveraging recent techniques on exploration in the stochastic shortest path (SSP) problem [45, 50], we propose GOSPRL (Goal-based Optimistic Sampling Procedure for RL), a conceptually simple and ﬂexible exploration algorithm that learns how to “generate” the samples requested by any
-based algorithm and we derive bounds on its sample complexity.
SO
SO
SO
• Leveraging the generality of our approach, we combine GOSPRL with problem-speciﬁc
-based algorithms and readily obtain online RL algorithms in difﬁcult exploration problems. While in general our decoupled approach may be suboptimal compared to exploration strategies designed to solve one speciﬁc problem, we obtain sample complexity guarantees that are on par or better than state-of-the-art algorithms in a range of problems. 1) GOSPRL solves the problem of sparse reward discovery in (cid:101)O(cid:0)D3/2S2A(cid:1) time steps, which improves the dependency on the diameter D w.r.t. a reward-free variant of UCRL2B [28, 22], as well as on S and A w.r.t. a MAXENT-type approach
[26, 17]. 2) GOSPRL improves over the method of [54] for model estimation, by removing their ergodicity assumption as well as achieving better sample complexity. 3) GOSPRL provably tackles the problem of goal-free cost-free exploration, for which no speciﬁc strategy is available.
SO
• We report numerical simulations supporting our theoretical ﬁndings and showing that pairing
-based algorithms outperforms both heuristic and theoretically grounded base-GOSPRL with lines in various problems.
SO
SO