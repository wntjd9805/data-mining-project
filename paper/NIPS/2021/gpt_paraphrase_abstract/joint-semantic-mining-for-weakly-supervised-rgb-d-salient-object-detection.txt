This study addresses the issue of weakly-supervised RGB-D salient object detection, where only image-level tags or captions are available as supervision signals. The proposed approach involves maintaining per-pixel pseudo-labels through iterative refinements by reconciling multimodal input signals using joint semantic mining (JSM). To capture saliency-specific depth cues and mitigate the lack of explicit pixel-level supervision, spatial semantic modeling (SSM) is introduced to generate depth-refined pseudo-labels from raw depth maps. Additionally, textual semantic modeling (TSM) incorporates tags and captions through fill-in-the-blank training to estimate confidence levels of competing pseudo-labels. During testing, only a lightweight sub-network is required, making inference efficient. Extensive evaluations demonstrate the effectiveness of the proposed method in weakly-supervised scenarios, and it can also be adapted for fully-supervised and unsupervised paradigms. The approach outperforms state-of-the-art methods, and a CapS dataset is constructed by augmenting existing benchmark training sets with additional image tags and captions. The code and dataset are available at the provided GitHub link.