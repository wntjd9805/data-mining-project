Multimodal meta-learning is an emerging problem that expands upon conventional few-shot meta-learning by incorporating diverse multimodal task distributions. This approach aims to replicate how humans utilize prior skills to learn new ones. Previous research has shown promising results, suggesting that a single meta-learner trained on a multimodal distribution can outperform multiple specialized meta-learners trained on individual uni-modal distributions. This improvement is attributed to knowledge transfer between different task modes. However, there has been little investigation into understanding and verifying this knowledge transfer. Our work contributes to multimodal meta-learning in two ways. Firstly, we propose a method to quantify knowledge transfer between tasks of different modes at a micro-level, inspired by the transference concept in multi-task learning. Secondly, we introduce a new multimodal meta-learner that surpasses existing approaches by a significant margin, drawing inspiration from hard parameter sharing in multi-task learning and reinterpreting related work. While the primary focus is on multimodal meta-learning, our research also sheds light on task interaction in conventional meta-learning. The code for this project is available at https://miladabd.github.io/KML.