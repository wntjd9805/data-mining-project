Sparse variational Gaussian process (SVGP) methods are often preferred for non-conjugate Gaussian process inference due to their computational advantages. This study proposes an enhancement to their computational efficiency by employing a dual parameterization, where dual parameters are assigned to each data example, similar to site parameters used in expectation propagation. The dual parameterization facilitates faster inference through the utilization of natural gradient descent and also offers a more precise evidence lower bound for hyperparameter learning. Notably, this approach maintains the same memory cost as existing SVGP methods while being both faster and more accurate.