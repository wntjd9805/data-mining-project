The aim of zero-shot action recognition (ZSAR) is to classify actions that have not been seen during training. Traditionally, this is done by training a network to map visual inputs to a semantic space and using a nearest neighbor classifier to select the closest target class. However, this approach is not optimal because it relies on static semantic space and struggles with multi-label videos. To address these issues, we propose a ZSAR framework that uses a pairwise scoring function instead of nearest neighbor classification. Our method predicts confidence scores for each action class independently, allowing for the prediction of multiple distinct classes in one video. Our evaluations show that our approach performs well on single-label action classification datasets and outperforms previous ZSAR methods on challenging multi-label and surprise activity detection datasets.