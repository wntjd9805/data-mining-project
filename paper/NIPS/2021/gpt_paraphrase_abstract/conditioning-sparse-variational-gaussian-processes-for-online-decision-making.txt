Gaussian processes (GPs) are commonly used for online decision making due to their ability to represent uncertainty and update posterior distributions in a closed form. However, the computational complexity of GPs limits their applicability to datasets with a large number of training points. Stochastic variational Gaussian processes (SVGPs) offer scalable inference for fixed-sized datasets, but struggle to efficiently incorporate new data. To address this issue, we propose online variational conditioning (OVC), a method that allows for efficient conditioning of SVGPs in an online setting without the need for re-training. OVC enables the combination of SVGPs with advanced look-ahead acquisition functions for black-box optimization, even when dealing with non-Gaussian likelihoods. Our experiments demonstrate that OVC performs well in various applications, such as active learning of malaria incidence and reinforcement learning in MuJoCo simulated robotic control tasks.