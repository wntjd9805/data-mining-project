To assess the robustness of machine learning (ML) models when faced with different training populations, we examine the worst-case performance of a model across all subpopulations based on core attributes Z. This approach allows us to consider various continuous attributes and effectively address intersectionality within disadvantaged groups. We have developed a scalable and rigorous two-stage estimation procedure that can evaluate the robustness of state-of-the-art models. Our procedure has been proven to converge well, with convergence guarantees not dependent on the dimension of Z. Unlike conservative approaches based on Rademacher complexities, our evaluation error is only influenced by the dimension of Z through the out-of-sample error in estimating performance conditional on Z. Through experiments on real datasets, we demonstrate that our method can certify the robustness of models and prevent the deployment of unreliable ones.