Real-world applications in economics and policy making often involve solving multi-agent games with two distinct features: (1) The agents are inherently different and divided into leaders and followers, and (2) The agents have different reward functions, making the game general-sum. Most existing research in this field focuses on symmetric solution concepts like Nash equilibrium or zero-sum games. However, there is still a gap in efficiently learning the Stackelberg equilibrium, which is an asymmetric counterpart to Nash equilibrium, in general-sum games with noisy samples. This paper introduces the theoretical study of learning the Stackelberg equilibrium efficiently using bandit feedback, where only noisy samples of the reward are observed. We examine three representative two-player general-sum games: bandit games, bandit-reinforcement learning (bandit-RL) games, and linear bandit games. In all these games, we identify a significant gap between the exact value of the Stackelberg equilibrium and its estimated version using finitely many noisy samples. This gap cannot be closed information-theoretically, regardless of the algorithm used. However, we also establish positive results on efficiently learning the Stackelberg equilibrium with an optimal value up to the identified gap, along with lower bounds on the dependency of the gap, error tolerance, and size of the action spaces. Overall, our findings reveal the unique challenges faced in learning Stackelberg equilibria under noisy bandit feedback, which we hope will guide future research in this area.