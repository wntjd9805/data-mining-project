Efficient exploration remains a significant challenge in reinforcement learning (RL) despite the success of RL with function approximation. The existing RL algorithms based on upper confidence bounds (UCBs), such as optimistic least-squares value iteration (LSVI), are often not compatible with powerful function approximators like neural networks. This paper presents a new variant of bootstrapped LSVI called BooVI, which addresses this gap between theory and practice. BooVI enables exploration through resampling, making it compatible with general function approximators. Theoretical analysis shows that BooVI inherits the regret bound of optimistic LSVI in the episodic linear setting, with a worst-case complexity of O(pd^3H^3T).