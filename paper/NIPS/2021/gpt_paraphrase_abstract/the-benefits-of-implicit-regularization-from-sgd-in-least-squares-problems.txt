Stochastic gradient descent (SGD) is known for its algorithmic regularization effects, which are believed to contribute to the generalization of modern machine learning methods. This study focuses on linear regression and compares the implicit regularization of unregularized average SGD with the explicit regularization of ridge regression. The goal is to make accurate instance-based comparisons. The research demonstrates that for a wide range of least squares problem instances in high-dimensional settings: (1) SGD, with a properly tuned constant stepsize and a logarithmically greater number of samples than ridge regression, generalizes as well as ridge regression; (2) Conversely, there are instances within this problem class where ridge regression, even with optimal tuning, requires quadratically more samples than SGD to achieve the same generalization performance. In summary, these findings indicate that, except for logarithmic factors, SGD's generalization performance is always at least as good as ridge regression in a broad range of overparameterized problems, and in some cases, it can be significantly better. Additionally, the study highlights the importance of algorithmic regularization even in simpler convex settings.