Transfer Learning has shown promise in improving the efficiency of single-agent Reinforcement Learning (RL). Similarly, Multiagent RL (MARL) can also benefit from knowledge sharing among agents. However, the challenge lies in determining how an agent should learn from other agents. To address this, we propose a new framework called Multiagent Policy Transfer Framework (MAPTF) to enhance MARL efficiency. MAPTF learns which agent's policy is best to reuse for each agent and when to terminate it by treating multiagent policy transfer as an option learning problem. In practical scenarios, the option module can only collect local experiences of all agents for updates due to limited observability of the environment. This can lead to inconsistency and oscillation in the estimation of option-value, as each agent's experience may differ. To solve this, we introduce a novel option learning algorithm called successor representation option learning, which separates the environment dynamics from rewards and learns the option-value based on each agent's preference. MAPTF can easily be integrated with existing deep RL and MARL approaches, and experimental results demonstrate its significant performance improvement in both discrete and continuous state spaces.