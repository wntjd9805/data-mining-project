This study examines the validity of assumption (A) in post-hoc gradient-based interpretability methods, which assume that input gradients highlight task-relevant features. The researchers develop an evaluation framework called DiffROAR and test assumption (A) on four image classification benchmarks. They find that input gradients of standard models often violate assumption (A), while those of adversarially robust models satisfy it reasonably well. Additionally, they introduce a dataset called BlockMNIST that encodes knowledge of discriminative features, and analyze the differences between input gradient attributions of standard and robust models using this dataset. The researchers also provide theoretical proof that input gradients of standard models trained on a simplified version of BlockMNIST do not highlight instance-specific features, further violating assumption (A). These findings emphasize the importance of formalizing and testing assumptions in interpretability methods. The DiffROAR framework and BlockMNIST datasets are proposed as tools for auditing interpretability methods. The code and data are available at https://github.com/harshays/inputgradients.