Research on self-supervised visual representation learning (SSL) has focused on improving encoder backbones to distinguish training samples without labels. While SSL-based CNN encoders achieve comparable recognition performance to supervised learning, their network attention has not been thoroughly explored for further enhancement. In light of the effective visual attention exploration by transformers in recognition scenarios, we introduce the CNN Attention REvitalization (CARE) framework. CARE trains attentive CNN encoders guided by transformers in SSL. The framework consists of a CNN stream (C-stream) and a transformer stream (T-stream), each with two branches. The C-stream follows an existing SSL framework with two CNN encoders, two projectors, and a predictor. The T-stream comprises two transformers, two projectors, and a predictor. The T-stream is connected to CNN encoders and runs parallel to the remaining C-stream. During training, SSL is performed simultaneously in both streams, and the output of the T-stream is used to supervise the C-stream. The features from the CNN encoders are modulated in the T-stream to enhance visual attention and make them suitable for SSL. These modulated features are then used to supervise the C-stream, enabling the learning of attentive CNN encoders. By utilizing transformers as guidance, we revitalize CNN attention. Experimental results on various visual recognition benchmarks, including image classification, object detection, and semantic segmentation, demonstrate that the proposed CARE framework significantly improves the performance of CNN encoder backbones, surpassing the state-of-the-art.