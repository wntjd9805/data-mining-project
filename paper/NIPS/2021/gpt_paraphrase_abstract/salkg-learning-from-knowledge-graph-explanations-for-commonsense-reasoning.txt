Adding knowledge graphs (KGs) to pre-trained language models has been successful in improving commonsense reasoning tasks. However, not all KGs or their components are useful for a given task instance. While attention mechanisms are used in KG-augmented models to focus on specific KG components, the models always rely on the entire KG without explicitly knowing which components to prioritize. This paper explores the use of saliency explanations to enhance the performance of KG-augmented models. The authors propose creating coarse and fine saliency explanations to determine the usefulness of the KG and its specific nodes/paths. Oracle KG-augmented models, which utilize saliency explanations as additional inputs to guide their attention, are analyzed to motivate saliency-based supervision. The authors then introduce SALKG, a framework that allows KG-augmented models to learn from coarse and/or fine saliency explanations. SALKG trains the model to predict the explanations and solves the task by attending to KG features highlighted by the predicted explanations. Experimental results on three commonsense QA benchmarks and various KG-augmented models demonstrate that SALKG leads to significant performance improvements, with up to a 2.76% absolute improvement on CSQA.