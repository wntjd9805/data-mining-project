An agent's mastery of its environment can be measured by the maximum number of states it can reliably reach. This is often achieved by maximizing the number of latent codes that can be distinguished from future states within a short time frame. We propose a method called Entropic Desired Dynamics for Intrinsic ConTrol (EDDICT) that situates these latent codes in a globally consistent coordinate system. This approach allows agents to reliably reach more states in the long term while still optimizing a local objective. EDDICT's fixed additive latent dynamics make learning easier and result in an interpretable latent space. Compared to previous methods, EDDICT's globally consistent codes enable more exploratory behavior, leading to improved state coverage and better performance on challenging exploration games like Montezuma's Revenge.