We explore the use of Model-Agnostic Meta-Learning (MAML) techniques in Reinforcement Learning (RL) problems, which involve finding a policy using data from multiple tasks represented by Markov Decision Processes (MDPs). To update the MDP, it is necessary to employ stochastic policy gradients due to the difficulty of computing exact gradients using all possible trajectories. To address this, we propose a modified version of MAML called Stochastic Gradient Meta-Reinforcement Learning (SG-MRL) and investigate its convergence properties. We establish the iteration and sample complexity of SG-MRL for finding a first-order stationary point, which is the first convergence guarantee for model-agnostic meta-reinforcement learning algorithms. We also demonstrate how our findings apply when multiple steps of the stochastic policy gradient method are used during testing. Finally, we compare SG-MRL and MAML in various deep RL environments through empirical experiments.