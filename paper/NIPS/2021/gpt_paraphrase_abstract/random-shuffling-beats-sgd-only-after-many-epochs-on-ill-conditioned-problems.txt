Recent research has focused on analyzing the convergence rates of without-replacement SGD and demonstrating its superiority over with-replacement SGD in the worst-case scenario. However, existing lower bounds fail to consider the problem's geometry, including its condition number, while upper bounds explicitly take it into account. Surprisingly, we establish that without-replacement SGD does not offer significant improvements over with-replacement SGD in terms of worst-case bounds when the condition number is considered, unless the number of epochs exceeds the condition number. This finding is crucial as many real-world problems in fields like machine learning involve large datasets and are ill-conditioned. It suggests that without-replacement sampling may not necessarily outperform with-replacement sampling given realistic iteration budgets. We support this claim by presenting new lower and upper bounds that accurately quantify the dependence on problem parameters, specifically for quadratic problems with commuting quadratic terms. These bounds are tight, up to logarithmic factors.