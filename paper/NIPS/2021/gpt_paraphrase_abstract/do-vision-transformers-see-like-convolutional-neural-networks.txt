Convolutional neural networks (CNNs) have long been the dominant model for analyzing visual data. However, recent research has shown that Vision Transformer models (ViTs) can achieve comparable or even better performance in image classification tasks. This raises the question of how ViTs are able to solve these tasks. Are they similar to CNNs, or do they learn entirely different visual representations? By analyzing the internal representation structures of ViTs and CNNs on image classification benchmarks, we have discovered significant differences between the two architectures. Specifically, ViTs exhibit more uniform representations across all layers. We have investigated the reasons behind these differences and found that self-attention plays a crucial role in enabling ViTs to aggregate global information early on. Additionally, the residual connections in ViTs strongly propagate features from lower to higher layers. We have also examined the implications for spatial localization and have shown that ViTs successfully preserve input spatial information, with variations depending on the classification methods used. Furthermore, we have explored the impact of dataset scale on intermediate features and transfer learning, and discussed the connections between ViTs and new architectures such as the MLP-Mixer.