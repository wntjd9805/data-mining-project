Continual learning involves learning a sequence of tasks from changing data distributions. Transferring knowledge from old tasks to new ones can be challenging, as it can have both positive and negative effects. If the old knowledge interferes with learning a new task, remembering the old tasks will worsen the interference and decrease performance. However, biological neural networks can actively forget conflicting old knowledge through synaptic expansion and convergence. Inspired by this, we propose a method called Active Forgetting with synaptic Expansion-Convergence (AFEC) to actively forget old knowledge that hinders learning new tasks in Bayesian continual learning. AFEC dynamically expands parameters to learn each new task and selectively combines them, mimicking the biological active forgetting mechanism. We extensively evaluate AFEC on various continual learning benchmarks and find that it significantly improves the learning of new tasks and achieves state-of-the-art performance without additional adjustments.