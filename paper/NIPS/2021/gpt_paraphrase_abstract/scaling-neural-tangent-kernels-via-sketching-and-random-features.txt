The Neural Tangent Kernel (NTK) is a tool used to understand the behavior of neural networks trained under least squares loss. Recent studies have shown that NTK regression can outperform neural networks trained on small-scale datasets. However, the computational complexity of kernel methods has hindered their use in large-scale learning tasks. To address this issue, we have developed an approximation algorithm for NTK that significantly reduces computation time. Our algorithm utilizes sketching techniques to approximate the polynomial expansions of the arc-cosine kernels, specifically for the convolutional counterpart of NTK (CNTK) in image processing. We have also proven a spectral approximation guarantee for the NTK matrix by combining random features of the arc-cosine kernels with our sketching algorithm. To evaluate our approach, we conducted benchmark tests on various large-scale regression and classification tasks. The results demonstrate that our linear regressor trained using CNTK features achieves the same accuracy as the exact CNTK on the CIFAR-10 dataset, while providing a 150-fold speedup.