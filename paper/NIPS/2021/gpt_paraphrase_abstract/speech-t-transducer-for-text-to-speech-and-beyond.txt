We investigate the potential of using Neural Transducer (e.g., RNN-T) in text-to-speech synthesis (TTS) and other applications. However, this is challenging because it is difficult to balance the emission probability (continuous mel-spectrogram prediction) and transition probability (ASR Transducer predicts blank token to indicate transition) when calculating the output probability lattice. It is also challenging to learn the alignments between text and speech through the output probability lattice. To address these challenges, we propose SpeechTransducer (Speech-T), a Transformer based Transducer model. Speech-T uses a new forward algorithm to separate transition prediction from mel-spectrogram prediction, and introduces a diagonal constraint in the probability lattice to aid alignment learning. It supports both full-sentence and streaming TTS by adjusting the look-ahead context. Additionally, it is the first model to support joint TTS and ASR, offering advantages such as fewer parameters and streaming synthesis and recognition in one model. Experiments on LJSpeech datasets demonstrate that Speech-T is more robust than attention-based autoregressive TTS models, supports streaming TTS with good voice quality, and benefits from joint modeling of TTS and ASR.