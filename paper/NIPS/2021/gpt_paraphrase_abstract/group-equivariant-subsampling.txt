We propose a method to address the lack of translation equivariance in subsampling operations used in convolutional neural networks (CNNs). We introduce translation equivariant subsampling/upsampling layers that can be used to construct CNNs that are exact translation equivariant. Furthermore, we extend these layers to general groups, allowing for group equivariant subsampling/upsampling. We utilize these layers to build group equivariant autoencoders (GAEs) that learn low-dimensional equivariant representations. Our empirical results demonstrate that these representations are indeed equivariant to input translations and rotations, making them suitable for unseen positions and orientations. Additionally, we apply GAEs to models that learn object-centric representations on multi-object datasets, and observe improved data efficiency and decomposition compared to non-equivariant baselines.