Self-supervised learning has proven effective in learning useful representations, particularly in data types like images, audio, and text. This success is largely due to leveraging the spatial, temporal, or semantic structure in the data through augmentation techniques. However, tabular datasets commonly used in fields like healthcare lack such structure, making it challenging to design effective augmentation methods and impeding progress in learning from tabular data. In this study, we propose a new framework called Subsetting features of Tabular data (SubTab) to address this issue. SubTab transforms the task of learning from tabular data into a multi-view representation learning problem by dividing the input features into multiple subsets. We argue that reconstructing the data from a subset of its features, rather than a corrupted version, in an autoencoder setting can better capture its underlying latent representation. In this framework, the joint representation is expressed as the aggregate of latent variables from the subsets during test time, which we refer to as collaborative inference. Our experiments demonstrate that SubTab achieves state-of-the-art performance (98.31%) on the MNIST dataset in the tabular setting, comparable to CNN-based state-of-the-art models, and outperforms existing baselines by a significant margin on three other real-world datasets.