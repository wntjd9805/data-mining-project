The demand for deep neural networks (DNNs) with reduced computational requirements has sparked interest in sparse and accurate DNN models. Sparse training, where DNN weights are already sparse during training, is a more challenging task. Existing sparse training methods are often empirical and may have lower accuracy compared to dense models. This paper introduces a general approach called Alternating Compressed/DeCompressed (AC/DC) training for DNNs. The algorithm is demonstrated to converge, and AC/DC is shown to outperform existing sparse training methods in accuracy while maintaining similar computational budgets. AC/DC even surpasses methods that rely on accurate pre-trained dense models at high sparsity levels. A notable feature of AC/DC is its ability to co-train dense and sparse models, resulting in accurate sparse-dense model pairs. This is advantageous in resource-constrained settings where compressed models are preferred for deployment without the need for re-training. Additionally, the co-training process sheds light on the accuracy discrepancy between dense and compressed models. The code for AC/DC is available at: https://github.com/IST-DASLab/ACDC.