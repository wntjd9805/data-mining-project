Perturb-and-MAP provides a method for sampling from an energy-based model (EBM) by finding the maximum-a-posteriori (MAP) configuration of a perturbed version of the model. However, the complexity of the MAP computation has limited research in this area. Existing approaches rely on linear programming, which has its limitations. In this study, we introduce perturb-and-max-product (PMP), a scalable mechanism for sampling and learning in discrete EBMs. PMP works with arbitrary models built using tractable factors. Our results demonstrate that PMP is significantly faster than Gibbs and Gibbs-with-Gradients (GWG) in learning and generating samples of comparable or better quality for Ising models. PMP is also capable of learning and sampling from RBMs. Furthermore, PMP succeeds in mixing a large, entangled graphical model where Gibbs and GWG fail.