Recent advancements in self-attention and pure multi-layer perceptron (MLP) models for vision have shown promise in achieving impressive performance while minimizing inductive biases. These models rely on learning interactions between spatial locations from raw data. However, the complexity of self-attention and MLP models increases exponentially with larger image sizes, limiting scalability when high-resolution features are necessary. To address this challenge, we propose the Global Filter Network (GFNet), a straightforward yet computationally efficient architecture that learns long-term spatial dependencies in the frequency domain with a linear time complexity. Our approach replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, element-wise multiplication between frequency-domain features and trainable global filters, and a 2D inverse Fourier transform. Experimental results on ImageNet and downstream tasks demonstrate that GFNet achieves favorable trade-offs between accuracy and complexity. Our findings suggest that GFNet can serve as a competitive alternative to transformer-style models and convolutional neural networks (CNNs) in terms of efficiency, generalization ability, and robustness. The code for GFNet is available at https://github.com/raoyongming/GFNet.