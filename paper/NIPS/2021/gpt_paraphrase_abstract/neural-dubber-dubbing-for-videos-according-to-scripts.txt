Dubbing, a commonly used technique in filmmaking and video production, involves re-recording actors' dialogues in post-production. Traditionally, this process is performed manually by professional voice actors, who read lines with the correct intonation and synchronize them with pre-recorded videos. This paper introduces NeuralDubber, the first neural network model designed to address the automatic video dubbing (AVD) task, which involves generating human speech synchronized with a given video from text. Neural Dubber is a multi-modal text-to-speech (TTS) model that utilizes the lip movement in the video to control the speech's intonation. Additionally, an image-based speaker embedding (ISE) module has been developed for the multi-speaker scenario, allowing Neural Dubber to generate speech with appropriate timbre based on the speaker's facial characteristics. Experimental results on both single-speaker and multi-speaker datasets demonstrate that NeuralDubber can produce speech audios comparable to state-of-the-art TTS models in terms of quality. Furthermore, qualitative and quantitative evaluations confirm that Neural Dubber can effectively control the intonation of synthesized speech based on the video and generate high-quality speech in sync with the video.