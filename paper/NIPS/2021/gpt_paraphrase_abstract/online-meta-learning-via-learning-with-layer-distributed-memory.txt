We present a method for efficient meta-learning using deep neural networks with distributed memory. The memory acts as a guide for task adaptation and its distributed nature helps in coordinating adaptation. Our experiments show that providing relevant feedback to memory units throughout the network enables them to guide adaptation effectively. This approach simplifies meta-learning and outperforms other memory-based strategies. It also handles online learning scenarios with delayed feedback. The method is effective for various learning tasks such as classification and few-shot semantic segmentation.