Convolutional neural networks (CNNs) improve generalization by capturing relevant symmetries in problems. However, these models often struggle to fully respect symmetries, limiting their ability to fit the data. To address this, we propose Residual Pathway Priors (RPPs), which convert hard architectural constraints into soft priors. RPPs guide models towards structured solutions while still allowing for capturing additional complexity. RPPs are robust to approximate or misspecified symmetries and perform as well as fully constrained models even with exact symmetries. We demonstrate the effectiveness of RPPs in both model-free and model-based reinforcement learning problems, where contact forces and directional rewards violate equivariant network assumptions. Additionally, we show the broad applicability of RPPs in dynamical systems, regression, and classification tasks.