The challenge of generalizing machine learning models to out-of-domain (OOD) data has been addressed through various techniques that focus on achieving invariance properties. This study proposes a connection between OOD performance and model calibration, suggesting that calibration across multiple domains can be seen as an invariant representation that improves OOD generalization. The study demonstrates that models achieving multi-domain calibration are free of spurious correlations under certain conditions. As a result, multi-domain calibration is proposed as a measurable and trainable proxy for a classifier's OOD performance. The study introduces easy-to-apply methods that enable practitioners to enhance multi-domain calibration through training or modifying existing models, resulting in improved performance on unseen domains. Experimental results using multiple datasets validate the effectiveness of training or tuning models for multi-domain calibration. The authors believe that the connection between calibration and OOD generalization holds promise both practically and theoretically.