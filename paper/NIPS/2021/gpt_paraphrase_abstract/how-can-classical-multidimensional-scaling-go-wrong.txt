The classical multidimensional scaling (cMDS) algorithm is commonly used to embed data points into Euclidean space based on a dissimilarity matrix. However, there is a lack of theoretical analysis regarding the algorithm's robustness and its performance on non-Euclidean metrics. This paper presents a formula, based on the eigenvalues of a matrix obtained from the dissimilarity matrix, to calculate the Frobenius norm of the difference between the original dissimilarity matrix and the metric obtained from cMDS (referred to as Dcmds). The analysis reveals that when the derived matrix has a significant number of negative eigenvalues, the Frobenius norm (DcmdskF) initially decreases but eventually increases as the dimension increases. This implies that the quality of the embedding deteriorates with increasing dimension, contrary to intuition. Empirical verification shows that the Frobenius norm increases with dimension for various non-Euclidean metrics. The paper also demonstrates on benchmark datasets that this degradation in embedding results in decreased classification accuracy for both simple and complex classifiers as the embedding dimension increases. Finally, the analysis leads to the proposal of a new computationally efficient algorithm that returns a matrix (Dl) that is at least as close to the original distances as the Euclidean metric (Dt). Although Dl is not a metric, when used as input to cMDS instead of the original dissimilarity matrix, it produces solutions that do not increase in distance as the dimension increases, and the classification accuracy degrades less compared to the cMDS solution.