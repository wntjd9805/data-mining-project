Most current neural architecture search (NAS) algorithms are primarily evaluated based on their performance in downstream tasks, such as image classification in computer vision. However, research has shown that well-known neural architectures like ResNet and LSTM perform well across various downstream tasks due to their ability to extract patterns from input data. This paper aims to address two fundamental questions regarding NAS. First, is it necessary to evaluate and search for good neural architectures based on the performance of specific downstream tasks? Second, can NAS be carried out effectively and efficiently without being task-specific? To answer these questions, the authors propose a novel NAS framework called Generic NAS (GenNAS). GenNAS does not rely on task-specific labels but instead uses regression on a set of manually designed synthetic signal bases to evaluate architectures. This self-supervised regression task effectively measures an architecture's ability to capture and transform input signal patterns, enabling better utilization of training samples. Extensive experiments across multiple CNN search spaces and one NLP space demonstrate the efficiency of GenNAS using regression, both in evaluating neural architectures (as quantified by the ranking correlation Spearman's ρ) and in training convergence speed (within seconds). For instance, on NAS-Bench-101, GenNAS achieves a ρ of 0.85 compared to the 0.38 achieved by existing efficient methods. The authors also introduce an automatic task search to optimize the combination of synthetic signals using limited task-specific labels, further enhancing GenNAS performance. GenNAS is evaluated across all search spaces and outperforms existing works with significant speedup. For example, on NASBench-201, GenNAS can find near-optimal architectures within 0.3 GPU hour. The code for GenNAS is available at https://github.com/leeyeehoo/GenNAS.