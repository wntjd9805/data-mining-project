The Transformer model, known for its ability to capture long-range dependencies, faces challenges in high-resolution image generation due to its quadratic complexity. To overcome this, we introduce two modifications to the Transformer. Firstly, we replace global self-attention with multi-axis blocked self-attention in low-resolution stages, allowing for efficient integration of local and global attention. Secondly, in high-resolution stages, we remove self-attention and retain multi-layer perceptrons similar to implicit neural functions. Additionally, we incorporate self-modulation based on cross-attention for further performance improvement. This modified model, called HiT, has nearly linear computational complexity, enabling the synthesis of high-definition images. Experimental results demonstrate that HiT achieves state-of-the-art FID scores on ImageNet and FFHQ datasets. We believe HiT represents a significant advancement in convolution-free generators for GANs, and provide our code for public use.