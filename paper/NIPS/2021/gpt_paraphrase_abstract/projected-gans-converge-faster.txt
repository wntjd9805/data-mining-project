We have made significant progress in addressing the challenges of training Generative Adversarial Networks (GANs) which are known for producing high-quality images. These networks require careful regularization, large amounts of computational power, and extensive hyper-parameter tuning. To overcome these challenges, we propose a new approach that involves projecting both generated and real samples into a pre-trained feature space. We have found that the discriminator in GANs is not able to fully utilize features from deeper layers of the pre-trained model. Therefore, our strategy involves mixing features across channels and resolutions, resulting in improved image quality, sample efficiency, and convergence speed. Our Projected GAN is compatible with resolutions of up to one Megapixel and achieves state-of-the-art performance on twenty-two benchmark datasets, as measured by the Fr√©chet Inception Distance (FID). Importantly, our approach achieves the same performance as previous methods but up to 40 times faster, reducing the required training time from 5 days to less than 3 hours with the same computational resources. The effectiveness of our approach is consistent across multiple datasets, as demonstrated by the convergence of samples in the training process.