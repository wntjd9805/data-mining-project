Visual question answering (VQA) aims to assess an intelligent agent's ability to reason based on visual and textual information. However, recent findings indicate that many VQA models primarily exploit biases between questions and answers in a dataset rather than demonstrating genuine reasoning skills. Some models tend to output frequently occurring answers in the dataset without considering the visual content. Existing methods attempt to mitigate this bias by focusing on weakening the language bias, with only a few addressing the vision bias indirectly. However, these approaches either introduce additional annotations or yield unsatisfactory results. Furthermore, not all biases are detrimental to the models; some biases learned from datasets reflect natural rules of the world and can help narrow down the range of answers. Consequently, filtering and removing true negative biases in both language and vision modalities present a significant challenge. In this paper, we propose a method called D-VQA to address these challenges from the feature and sample perspectives. In terms of features, we develop separate branches for question-to-answer and vision-to-answer biases. Additionally, we employ two unimodal bias detection modules to explicitly identify and eliminate negative biases. From the sample perspective, we create two types of negative samples to enhance model training without introducing extra annotations. Extensive experiments conducted on the VQA-CP v2 and VQA v2 datasets validate the effectiveness of our D-VQA method. The answer format outputs only the abstraction.