Temporal abstraction in reinforcement learning (RL) has the potential to enhance generalization and knowledge transfer in complex environments by efficiently propagating information over time. While previous option learning methods allowed for updating multiple options simultaneously, recent hierarchical RL approaches have focused on updating only the currently executing option. We propose a novel approach that extends intra-option learning in the context of deep RL, enabling the updating of all options based on current primitive action choices without the need for additional estimates. Our method can easily be integrated into existing hierarchical RL frameworks. By combining our approach with the option-critic algorithm for option discovery, we achieve significant performance improvements and data efficiency in various domains.