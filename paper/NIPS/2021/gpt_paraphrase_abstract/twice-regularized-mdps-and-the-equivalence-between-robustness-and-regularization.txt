Regularized Markov decision processes (MDPs) are often used to improve stability in policy learning, but they do not account for uncertainty in system dynamics. In this study, we propose a method for learning robust MDPs using regularization. We establish that regularized MDPs are a specific type of robust MDPs with uncertain rewards. By extending this relationship to MDPs with uncertain transitions, we introduce a regularization term that depends on the value function. We also introduce twice regularized MDPs (R2 MDPs), which incorporate both value and policy regularization. Using Bellman operators, we develop policy iteration schemes with convergence and robustness guarantees, making planning and learning in robust MDPs similar to regularized MDPs.