Visual grounding has become a popular approach in language learning as it provides rich information for understanding the world. One such approach is vokenization, which uses predictions from a text-to-image retrieval model as labels for language model supervision. However, vokenization has limitations such as approximation error and limited vocabulary diversity due to small image-text datasets. To address these issues, we propose VIDLANKD, a video-language knowledge distillation method that improves language understanding. We train a teacher model on a video-text dataset and transfer its knowledge to a student language model using a text dataset. To overcome approximation error, we use different knowledge distillation objectives. Additionally, the use of a large-scale video-text dataset helps learn diverse and richer vocabularies. Our experiments show that VIDLANKD consistently outperforms text-only language models and vokenization models on various language understanding tasks. We also evaluate our model on datasets related to world knowledge, physical reasoning, and temporal reasoning, demonstrating its improved capabilities. Lastly, we conduct ablation studies and provide visualizations of the text-to-video grounding results of our teacher and student models.