Language models with a large number of trainable parameters often achieve good performance on tasks and can easily adapt to related tasks. Recent research suggests that training only utilizes a small fraction of the parameter space, prompting the question of how large the parameter space needs to be. Model compression, parameter sharing, and knowledge distillation techniques have shown that models can be made smaller and still perform well. This study focuses on factorized representations of matrices in dense, embedding, and self-attention layers. By using low-rank factorization, the study achieves space-efficient and expressive linear layers. Stacking these low-rank layers increases their expressiveness and provides theoretical understanding for their effectiveness in deep networks. In Transformer models, this approach reduces the number of trainable parameters by over ten-fold without significant performance degradation. The approach can be easily implemented by replacing each parameter matrix with its compact equivalent while keeping the network architecture intact.