This paper examines the use of gradient methods in training deep linear neural networks with binary cross-entropy loss. The focus is on investigating the convergence guarantees of these methods for linear networks with spherically symmetric data distribution. The study establishes that global directional convergence can be achieved at a polynomial rate, transitioning to a linear rate. Importantly, these results are obtained without relying on assumptions made in other studies, such as small initial loss, presumed convergence of weight direction, or overparameterization. Experimental evidence is provided to support the findings.