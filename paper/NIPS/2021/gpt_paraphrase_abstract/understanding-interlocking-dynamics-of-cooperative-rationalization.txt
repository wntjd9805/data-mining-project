Selective rationalization is a method used to predict the output of complex neural networks by identifying a small subset of input features that are sufficient for making the prediction. This is achieved through a two-component system, consisting of a rationale generator and a predictor. The generator selects the input features (rationale) and the predictor uses these selected features to make the prediction. Both components are trained together to optimize prediction performance. However, this paper highlights a problem with this approach called "model interlocking," where the predictor becomes overly reliant on the features selected by the generator, even if they are not the most optimal. The interlocking problem occurs because the objective of rationalization is concave with respect to the generator's selection policy. To address this issue, the paper proposes a new framework called A2R, which introduces a third component - a predictor driven by soft attention. The generator now utilizes both soft and hard attention over the features, which are then fed into the two different predictors. A2R aims to minimize the gap between the two predictors while still supporting the original predictor's performance. The attention-based predictor in A2R exhibits better convexity properties, allowing it to overcome the concavity barrier. Experimental results on synthetic benchmarks and real datasets demonstrate that A2R can effectively mitigate the interlock problem and provide explanations that align better with human judgments.