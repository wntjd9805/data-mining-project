We introduce the first PAC-Bayesian generalization bounds for adversarial robustness. These bounds estimate the level of invariance a model will have towards imperceptible perturbations in the input during testing. Rather than analyzing the worst-case risk of a hypothesis for all possible perturbations, we utilize the PAC-Bayesian framework to constrain the average risk on perturbations for majority votes across the hypothesis class. Our analysis offers general bounds that are applicable to any adversarial attack, are precise due to the PAC-Bayesian framework, and can be minimized during the learning phase to achieve robust models against various attacks during testing.