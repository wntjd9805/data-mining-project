We investigate a multi-armed bandit problem where the rewards switch between different regimes. In this problem, the rewards generated from each arm are influenced by a hidden state represented by a finite-state Markov chain. The agent lacks knowledge of this underlying state and needs to learn both the transition matrix and reward distributions. To address this, we propose a learning algorithm that combines spectral method-of-moments estimations for hidden Markov models, belief error control in partially observable Markov decision processes, and upper-confidence-bound methods for online learning. We prove that this algorithm has an upper bound of O(T 2/3âˆš log T ), where T represents the learning horizon. Furthermore, we present proof-of-concept experiments to demonstrate the effectiveness of our learning algorithm.