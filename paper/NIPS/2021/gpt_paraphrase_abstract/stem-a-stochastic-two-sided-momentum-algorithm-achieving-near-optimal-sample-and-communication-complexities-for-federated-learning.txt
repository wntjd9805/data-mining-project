Federated Learning (FL) is a method where multiple worker nodes (WNs) collaborate to build a model using their local data. However, it is still unclear how to determine the update directions for the WNs and the server, as well as the minibatch sizes and number of local updates, in order to achieve the desired solution with minimal samples and communication rounds. This study addresses this question and focuses on a specific type of stochastic algorithm where the WNs perform a few local updates before communication. The research demonstrates that by selecting the WN's and server's directions based on a stochastic momentum estimator, the algorithm only requires a certain number of communication rounds to compute a stationary solution. This is the first FL algorithm to achieve near-optimal sample and communication complexities simultaneously. The study also reveals a trade-off between the number of local updates and minibatch sizes, where the sample and communication complexities can be maintained. Additionally, the research shows that this trade-off also exists for the classical FedAvg algorithm, but with worse sample and communication complexities. The insights gained from this trade-off provide guidance for designing FL algorithms in terms of the number of local updates, update directions, and minibatch sizes to achieve optimal performance.