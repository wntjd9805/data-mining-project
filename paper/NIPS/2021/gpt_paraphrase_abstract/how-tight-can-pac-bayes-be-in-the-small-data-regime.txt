This paper examines the extent to which PAC-Bayes and test set bounds can be tightened when working with a small number of data points, such as N = 30. While test set bounds negatively impact generalization performance by withholding data from training, PAC-Bayes bounds are appealing because they can utilize all available data to simultaneously learn a posterior and bound its generalization risk. The focus is on i.i.d. data with a bounded loss, and the generic PAC-Bayes theorem of Germain et al. is considered. Although this theorem recovers many existing PAC-Bayes bounds, it remains uncertain which bound derived from their framework is the tightest. For a fixed learning algorithm and dataset, it is demonstrated that the tightest possible bound aligns with a bound proposed by Catoni. Additionally, in the case of distributions over datasets, a lower bound is established on the best achievable bound in expectation. Interestingly, this lower bound corresponds to the Chernoff test set bound when the posterior is equal to the prior. Synthetic one-dimensional classification tasks are employed to illustrate the tightness of these bounds, allowing for meta-learning of both the prior and the form of the bound to optimize for the tightest bounds possible. The results reveal that in this controlled scenario, PAC-Bayes bounds are competitive with commonly used Chernoff test set bounds. However, the sharpest test set bounds still offer better guarantees on generalization error than the considered PAC-Bayes bounds.