Current set encoding algorithms assume that all set elements are accessible and can be loaded into memory during training and inference. However, this assumption fails when dealing with excessively large sets or streaming data. To address these challenges, the existing constraints of permutation invariance and equivariance are not enough. We introduce the concept of Mini-Batch Consistency (MBC) as a necessary property for large-scale set encoding in mini-batches. Moreover, we propose an attention-based set encoding mechanism that is scalable, efficient, and capable of updating set representations as new data arrives. Our method satisfies the required symmetries of invariance and equivariance, while also maintaining MBC for any partition of the input set. Through extensive experiments, we demonstrate that our approach is computationally efficient and produces high-quality set encoding representations for set-structured data.