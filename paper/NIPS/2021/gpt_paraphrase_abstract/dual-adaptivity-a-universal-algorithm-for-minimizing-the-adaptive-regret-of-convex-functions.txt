A new performance measure called adaptive regret, which refers to the maximum static regret over any interval, has been introduced in online learning to address changing environments. While several algorithms have been developed to minimize adaptive regret in the context of online convex optimization, these algorithms are limited in their ability to handle only one type of convex function and require prior knowledge of parameters. In contrast, there are universal algorithms like MetaGrad that achieve optimal static regret for multiple types of convex functions simultaneously. Building on this research, this paper presents the first universal algorithm for minimizing the adaptive regret of convex functions. The algorithm incorporates the concept of maintaining multiple learning rates from MetaGrad to handle function uncertainty and utilizes the technique of sleeping experts to account for changing environments. As a result, our algorithm automatically adapts to the characteristics of functions (convex, exponentially concave, or strongly convex) and the nature of environments (stationary or changing). Additionally, it allows for the switching of function types between rounds.