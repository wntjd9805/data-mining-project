Humans have the ability to quickly understand scenes by using concepts derived from their prior experiences. These concepts can be related to the overall scene, such as weather or lighting, as well as specific elements within the scene, such as the color or size of objects. Previous attempts at unsupervised concept discovery have focused on either the global scene-level or the local object-level factors, but not both. In this study, we present COMET, a method that discovers and represents concepts as separate energy functions, allowing for the representation of both global concepts and objects within a unified framework. COMET discovers these energy functions by recomposing the input image, capturing independent factors without additional guidance. Sample generation in COMET involves optimizing these energy functions, enabling the generation of images with permuted and composed concepts. The visual concepts discovered by COMET generalize effectively, allowing for the composition of concepts across different modalities of images and with other concepts discovered by a separate instance of COMET trained on a different dataset.