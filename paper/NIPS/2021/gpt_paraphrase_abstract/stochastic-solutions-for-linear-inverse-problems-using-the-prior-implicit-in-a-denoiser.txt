Deep neural networks have been successful in solving image denoising problems by relying on a prior probability model of natural images. Two recent approaches, Denoising Score Matching and Plug-and-Play, propose methods for sampling from this implicit prior and using it to solve inverse problems. In this study, we present a concise and robust extension of these ideas. We leverage a well-known statistical result that demonstrates the connection between the least-squares solution for removing additive Gaussian noise and the gradient of the log of the noisy signal density. Using this result, we develop a stochastic coarse-to-fine gradient ascent procedure to draw high-probability samples from the implicit prior embedded within a CNN trained for blind denoising. By extending this algorithm to constrained sampling, we can solve any deterministic linear inverse problem using the implicit prior without additional training. This significantly broadens the application of supervised learning for denoising. Our algorithm is based on minimal assumptions and demonstrates reliable convergence across various parameter choices. To showcase the versatility of our approach, we apply it to achieve state-of-the-art performance in unsupervised deblurring, super-resolution, and compressive sensing tasks.