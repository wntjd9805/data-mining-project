Improvements in Graph Representational Learning (GRL) methods have led to better performance, but at the expense of high computational resource requirements. Singular Value Decomposition (SVD) offers a solution by finding closed-form solutions using fewer epochs. To address this issue, we propose a framework that computes SVD of implicitly defined matrices, making GRL more computationally tractable for those with limited hardware. We apply this framework to various GRL tasks and derive a linear approximation of a state-of-the-art model. By training the model through SVD without calculating entries of the matrix, we achieve competitive test performance on different types of graphs. Additionally, SVD can initialize a deeper model that is non-linear in most cases but behaves linearly on a hyperplane defined by SVD initialization, allowing for fine-tuning in just a few epochs. Our approach trains significantly faster than existing methods while maintaining comparable performance. The implementation is available at: https://github.com/samihaija/isvd.