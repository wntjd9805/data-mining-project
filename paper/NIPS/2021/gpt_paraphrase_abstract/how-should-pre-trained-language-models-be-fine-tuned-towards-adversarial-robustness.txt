Pre-trained language models have achieved great success in various NLP fields. However, they are highly susceptible to adversarial examples, such as word substitution attacks that can easily deceive a BERT-based sentiment analysis model using synonyms. This paper highlights the limitations of adversarial training as a defense technique, as it fails to retain the generic and robust linguistic features already captured by the pre-trained model, resulting in catastrophic forgetting. To address this issue, we propose a novel method called Robust Informative Fine-Tuning (RIFT) that takes an information-theoretical perspective. RIFT ensures that the fine-tuned model retains the features learned from the pre-trained model throughout the entire process, unlike conventional methods that only use pre-trained weights for initialization. Our experimental results demonstrate that RIFT consistently outperforms existing techniques on popular NLP tasks, including sentiment analysis and natural language inference, even when facing different attacks on various pre-trained language models.