We propose the Multi-view Pose transformer (MvP) algorithm for estimating 3D poses of multiple individuals from multi-view images. Unlike previous methods that rely on costly volumetric representation or reconstructing 3D poses from 2D detections, MvP directly predicts the multi-person 3D poses efficiently. The algorithm represents skeleton joints as learnable query embeddings and uses them to attend and reason over multi-view information to regress the actual 3D joint locations. To enhance accuracy, MvP employs a hierarchical scheme to represent query embeddings and introduces an input-dependent query adaptation approach. Additionally, MvP incorporates a novel attention mechanism called projective attention, which accurately combines cross-view information for each joint. To augment the projective attention, MvP introduces a RayConv operation that integrates view-dependent camera geometry into feature representations. Experimental results demonstrate that MvP outperforms state-of-the-art methods on various benchmarks, achieving a 9.8% improvement in AP25 on the challenging Panoptic dataset. MvP is a versatile approach that can also be extended to recover human mesh represented by the SMPL model, making it useful for modeling multi-person body shapes. The code and models for MvP are available at https://github.com/sail-sg/mvp.