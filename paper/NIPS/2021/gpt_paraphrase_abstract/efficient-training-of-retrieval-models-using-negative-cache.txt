Factorized models, like two tower neural network models, are commonly employed in information retrieval tasks to score pairs of queries and documents. These models are trained by optimizing the parameters to assign higher scores to relevant pairs ("positive") compared to irrelevant pairs ("negative"). However, due to limited computing resources and memory, there are constraints on the number of negative pairs that can be used for training. In this study, we propose a novel technique for negative sampling to speed up training using softmax cross-entropy loss. Our technique utilizes cached item embeddings, which may be outdated, to enable training with a larger pool of negatives while reducing memory and computation requirements. We also present a streaming version of our algorithm designed for very large datasets. Additionally, we provide a theoretical foundation for our approach by demonstrating that updating a small fraction of the cache in each iteration still ensures rapid convergence. Through experiments, we demonstrate the effectiveness and efficiency of our approach, which compares favorably to more complex and state-of-the-art methods.