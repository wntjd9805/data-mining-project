Unsupervised domain adaptation (UDA) allows for learning across different domains without target domain labels by transferring knowledge from a labeled source domain with a different distribution. However, UDA is not always successful and negative transfer has been observed. In this study, we establish a lower bound on the target domain error, which complements the existing upper bound. Our bound demonstrates that minimizing source domain error and marginal distribution mismatch is insufficient for reducing target domain error, as it can increase labeling function mismatch. This inadequacy is further illustrated through examples where the same UDA approach succeeds, fails, or has an equal chance of success or failure. Building on this, we propose data poisoning attacks to deceive UDA methods into learning representations that result in high target domain errors. We evaluate the impact of these attacks on popular UDA methods using benchmark datasets where they have previously proven successful. Our results reveal that poisoning can greatly decrease target domain accuracy, dropping it to nearly 0% with just 10% poisoned data in the source domain. The failure of these UDA methods highlights their limitations in achieving cross-domain generalization as per our lower bound. Therefore, evaluating UDA methods in adversarial scenarios like data poisoning offers a better understanding of their robustness to unfavorable data distributions for UDA.