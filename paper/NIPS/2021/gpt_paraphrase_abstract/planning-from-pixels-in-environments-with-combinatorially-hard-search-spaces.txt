The ability of artificial intelligence to create complex plans based on visual input is a crucial indicator of its capabilities. This requires integrating visual processing and abstract algorithmic execution, which are traditionally separate in computer science. Recent advancements in this field have shown promising results in tasks like arcade games and continuous control. However, these methods have limitations in generalization and struggle with difficult planning instances. Our contribution consists of two parts: (i) we propose a method that learns to represent the environment as a latent graph and utilizes state reidentification to simplify the process of finding a good policy, reducing the complexity from exponential to linear; (ii) we introduce lightweight environments with discrete combinatorial structures that pose challenges for planning, even for humans. Additionally, we demonstrate that our methods exhibit strong empirical generalization, even in disadvantaged scenarios such as "one-shot" planning or low-quality trajectories in an offline reinforcement learning paradigm. Our approach, depicted in Figure 1, utilizes learned latent dynamics to efficiently construct and search a graph representation of the environment, leading to exceptional performance in a variety of difficult combinatorial tasks. This research was presented at the 35th Conference on Neural Information Processing Systems (NeurIPS 2021).