State-of-the-art neural models like Transformers and Graph Neural Networks face challenges in achieving systematic generalization in natural language understanding. To address this, we propose EdgeTransformer, a novel model that combines elements from Transformers and rule-based symbolic AI. The key idea behind Edge Transformers is to associate vector states with each edge, rather than just each node as done in Transformers. Another important innovation is the use of a triangular attention mechanism inspired by logic programming unification to update edge representations. We evaluate the performance of Edge Transformer on compositional generalization benchmarks in relational reasoning, semantic parsing, and dependency parsing. In all three scenarios, Edge Transformer surpasses Relation-aware, Universal, and classical Transformer baselines.