This study explores model-based bandit and reinforcement learning (RL) using non-linear function approximations. Instead of focusing on global convergence, which is statistically difficult to achieve, we investigate convergence to approximate local maxima. We introduce a model-based algorithm called Virtual Ascent with Online Model Learner (ViOlin) for both non-linear bandit and RL. ViOlin is proven to converge to a local maximum with a sample complexity that depends only on the sequential Rademacher complexity of the model class. Our findings provide new sample complexity bounds for various scenarios, including linear bandit with finite or sparse model classes, and two-layer neural net bandit. One key insight we discovered is that optimism can result in excessive exploration, particularly for the two-layer neural net model class. However, when aiming for convergence to local maxima, it is sufficient to maximize the virtual return if the model can reasonably predict the gradient and Hessian of the actual return.