Understanding the implicit bias of training algorithms is crucial for explaining the success of overparametrised neural networks. This paper examines the dynamics of stochastic gradient descent in diagonal linear networks using stochastic gradient flow. We determine the solution chosen by the stochastic flow and demonstrate that it consistently achieves better generalisation properties than gradient flow. Surprisingly, we find that the convergence speed of the training loss determines the magnitude of the biasing effect: slower convergence leads to better bias. We provide convergence guarantees for the dynamics and present experimental results that support our theoretical findings. These results highlight the potential of structured noise to improve generalisation and help explain the superior performance of stochastic gradient descent over gradient descent in practice.