The recent models used in studies comparing residual networks (ResNets) to linear predictors differ from the standard ResNets commonly used in computer vision. These models lack nonlinearities in the final residual representation that feeds into the final affine layer. To address this difference and reveal a linear estimation property, Residual NonlinearEstimators (ResNEsts) are defined by removing nonlinearities at the last residual representation from standard ResNets. Wide ResNEsts with bottleneck blocks always ensure a desirable training property where adding more blocks does not decrease performance with the same set of basis elements. To overcome the coupling problem in basis learning and linear prediction, an augmented ResNEst (A-ResNEst) architecture is introduced, guaranteeing no worse performance when adding a block. This establishes empirical risk lower bounds for a ResNEst using corresponding bases. Although ResNEsts suffer from diminishing feature reuse, this can be avoided by expanding or widening the input space. Inspired by the success of DenseNets, a new model called Densely connected NonlinearEstimator (DenseNEst) is proposed, which can be represented as a wide ResNEst with bottleneck blocks. Unlike ResNEsts, DenseNEsts exhibit the desirable property without requiring architectural re-design.