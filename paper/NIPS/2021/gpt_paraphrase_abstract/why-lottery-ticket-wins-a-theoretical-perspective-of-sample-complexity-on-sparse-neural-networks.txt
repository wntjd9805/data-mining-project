The lottery ticket hypothesis suggests that training a pruned neural network (the winning ticket) can improve test accuracy compared to the original unpruned network. While this hypothesis has been supported by empirical evidence in various deep neural network applications, the theoretical validation of the improved generalization of the winning ticket remains unclear. In this study, we analyze the geometric structure of the objective function and the sample complexity to achieve zero generalization error in order to characterize the performance of training a pruned neural network. We find that the convex region near a desirable model expands as the neural network is pruned, indicating the importance of a winning ticket. Additionally, when using a (accelerated) stochastic gradient descent algorithm to train a pruned neural network, we prove that the number of samples required for achieving zero generalization error is proportional to the number of non-pruned weights in the hidden layer. This means that training a pruned neural network converges faster to the desired model compared to training the original unpruned network, providing a formal justification for the improved generalization of the winning ticket. Our theoretical findings are based on a one-hidden-layer pruned neural network, and we also provide experimental results to validate the implications for pruning multi-layer neural networks.