Neural networks are susceptible to adversarial attacks, which are slight modifications to inputs that significantly impact the network's performance. Many defense methods have been proposed to enhance the robustness of deep networks by training them on adversarially perturbed inputs. However, these approaches often fail to defend against new types of attacks or even stronger versions of previously encountered attacks. To address this, we present a novel technique called Adversarial Feature Desensitization (AFD) that draws upon insights from domain adaptation. AFD aims to learn features that are invariant to adversarial perturbations, achieved through a game where features are both predictive and robust, meaning they cannot discriminate between natural and adversarial data. We provide empirical evidence from various benchmarks to demonstrate the effectiveness of our approach against a wide range of attack types and strengths. Our code can be found at https://github.com/BashivanLab/afd.