We examine a variant of phase retrieval called bandit phase retrieval, where the learner selects actions in the d-dimensional unit ball and the expected reward is an unknown parameter vector. We establish an upper bound on the minimax cumulative regret in this problem, which matches existing lower bounds up to logarithmic factors and improves upon the previous best upper bound by a factor of pd. Furthermore, we demonstrate that the minimax simple regret is achievable by an adaptive algorithm, with a complexity of d/pn. Our analysis reveals that a seemingly reliable heuristic for estimating lower bounds can be misleading and that uniform bounds on the information ratio for information-directed sampling are insufficient for achieving optimal regret.