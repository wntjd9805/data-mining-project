The challenge of approximate inference in Bayesian deep networks is to balance accurate posterior approximations with computational efficiency and scalability. To address this, we propose a new variational structured approximation method called VariationalStructured Dropout (VSD). VSD improves upon the factorized structure of Dropout posterior by using an orthogonal transformation to learn a structured representation on variational Gaussian noise. This introduces statistical dependencies in the approximate posterior and resolves the issues of previous Variational Dropout methods. The VSD method also generates an adaptive regularization term with desirable properties that contribute to better generalization. We conducted extensive experiments on standard benchmarks, which demonstrated that VSD outperforms state-of-the-art variational methods in terms of predictive accuracy, uncertainty estimation, and out-of-distribution detection.