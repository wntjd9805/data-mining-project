This paper focuses on the design of an initial model for federated learning (FL) that allows an arbitrary group of clients to obtain a global model for their specific task in just a few rounds. The challenge lies in preparing the initial model without knowledge of the downstream tasks it will be used for. To address this, the authors propose a meta-learning approach where the initial model is constructed through federated learning using an episodic arrangement. This approach aims to mimic the rounds of FL followed by inference in each episode. Experimental results demonstrate that this method effectively generalizes for different client groups and outperforms other known pretraining methods in terms of performance improvements with the same communication and computation resources.