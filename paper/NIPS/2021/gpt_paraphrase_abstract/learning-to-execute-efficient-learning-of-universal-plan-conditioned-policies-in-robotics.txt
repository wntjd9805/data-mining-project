Reinforcement Learning (RL) in robotics is often limited by the need for large amounts of data. However, model-based approaches, which rely on approximate models, are available in many robotics scenarios and offer a more data-efficient alternative. Nonetheless, if the model is imprecise or incorrect, the performance of these methods suffers. Therefore, the strengths and weaknesses of RL and model-based planners complement each other. In this study, we explore how these approaches can be combined into a single framework that capitalizes on their respective strengths. We introduce Learning to Execute (L2E), a method that utilizes approximate plans to learn universal policies conditioned on those plans. Our experiments in robotic manipulation demonstrate that L2E outperforms pure RL, pure planning, and baseline methods that combine learning and planning.