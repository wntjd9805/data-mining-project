Vision transformers have shown promise in computer vision tasks due to their ability to model long-range dependencies using self-attention. However, they lack an inherent bias in modeling local visual structures and handling scale variance. This results in the need for large amounts of training data and longer training schedules. To address this, we propose ViTAE, a Vision Transformer Advanced by Exploring intrinsic Inductive Bias from convolutions. ViTAE incorporates spatial pyramid reduction modules to downsample and embed the input image into tokens with multi-scale context. This enables ViTAE to learn robust feature representations for objects at different scales. Additionally, ViTAE includes a convolution block in each transformer layer, alongside the self-attention module, to learn local features and global dependencies simultaneously. Experimental results on ImageNet and downstream tasks demonstrate the superiority of ViTAE compared to baseline transformers and other existing approaches. The source code and pretrained models will be made available at [code].