We introduce 3DP3, a framework for inverse graphics that utilizes a structured generative model to analyze objects, scenes, and images. The framework employs voxel models to represent the 3D shape of objects, hierarchical scene graphs to break down scenes into objects and their contacts, and depth image likelihoods based on real-time graphics. By utilizing these components, 3DP3's inference algorithm can deduce the underlying latent 3D scene from an observed RGB-D image. This includes determining object poses and a concise joint parametrization of these poses, achieved through rapid bottom-up pose proposals, innovative involutive MCMC updates for the scene graph structure, and the option to incorporate neural object detectors and pose estimators. Our experiments demonstrate that 3DP3 enables scene understanding that accounts for 3D shape, occlusion, and contact structure. Furthermore, the results show that 3DP3 outperforms deep learning baselines in accurately estimating 6DoF object poses from real images. It also exhibits superior generalization capabilities in challenging scenarios involving novel viewpoints, contact, and partial observability.