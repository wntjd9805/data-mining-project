The stability of Nesterov's accelerated gradient method is investigated in this study. Previous research has shown that for convex quadratic objectives, the method's stability increases quadratically with the number of optimization steps. However, it was conjectured that this also holds true for the general convex and smooth case. This conjecture is disproven in our study, as we demonstrate that the stability of Nesterov's accelerated method actually deteriorates exponentially fast with the number of gradient steps, based on two notions of algorithmic stability including uniform stability. This finding is in contrast to the bounds observed in the quadratic case, as well as to known results for non-accelerated gradient methods where stability typically grows linearly with the number of steps.