Deep Reinforcement Learning (RL) has been successful in solving complex Markov Decision Processes (MDPs) problems. However, RL agents often struggle when faced with unexpected environmental changes in the real world. These changes, which are unrelated to the underlying problem, can negatively impact agent performance, particularly in visual input scenarios. This problem is similar to the domain generalization issue in supervised learning. In this study, we focus on goal-conditioned RL agents and examine their ability to generalize to new environments. We introduce a theoretical framework called Block MDP to assess the generalizability of goal-conditioned policies. Based on this framework, we propose a practical method called PA-SkewFit to enhance domain generalization. Our empirical evaluation demonstrates that our goal-conditioned RL agent performs well in various unseen test environments, achieving a 50% improvement over baseline models.