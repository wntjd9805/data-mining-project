This paper addresses the challenge of training machine learning models that can perform well when faced with changes in the distribution of test data. The authors propose a modeling framework that incorporates partial structural knowledge about the shifted test distribution, in addition to the training data. They utilize the principle of minimum discriminating information to incorporate the available prior knowledge and employ distributionally robust optimization to handle uncertainty resulting from limited samples. By utilizing large deviation results, the authors are able to derive explicit generalization bounds with respect to the unknown shifted distribution. The effectiveness and versatility of the proposed framework are demonstrated through two distinct applications: training classifiers on systematically biased data and evaluating off-policy in Markov Decision Processes.