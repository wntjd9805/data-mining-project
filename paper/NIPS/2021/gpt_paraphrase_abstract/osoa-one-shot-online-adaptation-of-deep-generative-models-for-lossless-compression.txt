Explicit deep generative models (DGMs), such as VAEs and Normalizing Flows, have proven to be effective for lossless compression. However, these models often require a large amount of storage space, negating the benefits of accurate data density estimation. To address this issue, we propose a new approach that involves starting with a pretrained deep generative model and compressing data batches while adapting the model using a dynamical system for only one epoch. We refer to this approach as One-Shot Online Adaptation (OSOA) of DGMs for lossless compression. We present a basic algorithm for OSOA and demonstrate that it can save significant time compared to training separate models and space compared to using one model for all datasets. Our experimental results show that, with the same number of adaptation steps or adaptation time, vanilla OSOA achieves better space efficiency (e.g., 47% less space) than fine-tuning the pretrained model and saving the fine-tuned model. Additionally, we highlight the potential of OSOA by showcasing further space or time efficiency with multiple updates per batch and early stopping.