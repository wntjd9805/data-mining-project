We develop new generalization bounds for supervised learning algorithms that rely on the information contained in predictions rather than the output of the training algorithm. These bounds surpass existing ones in terms of information theory, are applicable to a broader range of algorithms, and address two important challenges: (a) they provide meaningful results for deterministic algorithms, and (b) they are considerably easier to estimate. Through experimentation, we demonstrate that these bounds closely align with the generalization gap observed in practical scenarios involving deep learning.