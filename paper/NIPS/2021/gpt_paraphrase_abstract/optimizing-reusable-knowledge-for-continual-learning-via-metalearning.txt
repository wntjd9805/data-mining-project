Artificial neural networks often suffer from Catastrophic Forgetting (CF), which occurs when the weights of a network are overwritten during the training of a new task, leading to the loss of previously learned information. We propose a solution called MetA Reusable Knowledge (MARK) to address this problem. MARK promotes weight reusability instead of overwriting when learning new tasks by maintaining a set of shared weights, acting as a Knowledge Base (KB). This KB not only aids in learning new tasks but also gets enriched with new knowledge as the model learns. MARK combines metalearning to incrementally enrich the KB and foster weight reusability, along with trainable masks to selectively choose relevant weights from the KB for each task. By utilizing MARK, we achieve outstanding results in various benchmark tests, surpassing the top-performing methods by more than 10% in terms of average accuracy on the 20-Split-MiniImageNet dataset. Additionally, we achieve nearly zero forgetfulness using only 55% of the parameters. An ablation study confirms that MARK indeed learns reusable knowledge that is selectively utilized by each task.