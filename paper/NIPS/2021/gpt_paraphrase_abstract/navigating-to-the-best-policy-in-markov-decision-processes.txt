In this study, we focus on the classical active pure exploration problem in Markov Decision Processes (MDPs). The goal is for the agent to select actions sequentially and identify the best policy as quickly as possible based on the resulting system trajectory. We introduce a problem-specific lower bound on the average number of steps required before obtaining a correct answer with a probability of at least 1-Î´. Additionally, we propose an algorithm that has a sample complexity tailored to the particular instance, addressing the general case of communicating MDPs. We also present a variant of the algorithm that has a faster convergence rate under the assumption of ergodicity. This research builds upon previous work in the generative setting, where the agent could query the random outcome of any (state, action) pair at each step. However, our focus here is on tackling the navigation constraints imposed by the online setting. Our analysis relies on an ergodic theorem for non-homogeneous Markov chains, which has broad applicability in the analysis of Markov Decision Processes.