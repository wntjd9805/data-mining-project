Efficient exploration in online reinforcement learning (RL) is difficult in high-dimensional environments with sparse rewards. Count-based upper confidence bound (UCB) exploration methods have been successful in low-dimensional environments with tabular parameterization. However, implementing UCB in realistic RL tasks involving nonlinear function approximation is still unclear. To address this, we propose a new exploration approach that maximizes the deviation of the occupancy of the next policy from the explored regions. This approach is added as an adaptive regularizer to the standard RL objective to balance exploration and exploitation. We combine this objective with a convergent algorithm, resulting in a new intrinsic reward that adjusts existing bonuses. This intrinsic reward is easy to implement and can be combined with other RL algorithms for exploration. We evaluate the new intrinsic reward on tabular examples using various model-based and model-free algorithms, demonstrating improvements over count-only exploration strategies. Additionally, when tested on navigation and locomotion tasks, our approach significantly enhances sample efficiency compared to state-of-the-art methods.