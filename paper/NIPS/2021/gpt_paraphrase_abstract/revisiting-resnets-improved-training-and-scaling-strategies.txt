The focus in computer vision is often on new model architectures, but their impact is often mixed with changes to training methods and scaling strategies. This study aims to separate these factors by revisiting the standard ResNet model. Surprisingly, the results show that training and scaling strategies may have a greater impact than architectural changes, and the revised ResNets perform similarly to recent state-of-the-art models. The study suggests that the best scaling strategy depends on the training regime and introduces two new strategies: adjusting model depth in overfitting-prone regimes and increasing image resolution at a slower pace. With these improved strategies, a new family of ResNet architectures, called ResNet-RS, is developed. These models are faster than EfﬁcientNets on TPUs while achieving similar accuracies on ImageNet. In semi-supervised learning, ResNet-RS achieves high accuracy with faster training compared to EfﬁcientNet-NoisyStudent. The training techniques also enhance transfer performance on various tasks and extend to video classification. The study recommends using these revised ResNets as baselines for future research.