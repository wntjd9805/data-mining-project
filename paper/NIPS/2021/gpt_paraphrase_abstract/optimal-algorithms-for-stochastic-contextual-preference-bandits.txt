We examine the issue of preference bandits in a contextual setting. In each round, the learner is given a set of K items randomly chosen from a potentially infinite set of options. However, unlike traditional contextual bandits, the learner can only receive feedback in terms of item preferences. They can play a subset of size q (where q can be any value between 2 and K), and only the winner of the subset is revealed, with some noise. The goal remains the same as in classical bandits - to compete against the best item in each round. This problem is relevant in various online decision-making scenarios, such as recommender systems and information retrieval. We are the first to consider preference-based stochastic contextual bandits for potentially infinite decision spaces. We present two algorithms for the special case of pairwise preferences (q = 2), with one algorithm having a simple implementation and regret guarantee of approximately O(dT), and the other achieving the optimal lower bound analysis of approximately O(dT). We then analyze the problem for general q-subsetwise preferences (q ≥ 2), and surprisingly find that the lower bound for performance remains at approximately Ω(dT), regardless of the subset size q. We propose an upper bound algorithm that matches this lower bound, indicating that having access to subsetwise preferences does not improve information aggregation. We empirically validate our results against existing baselines.