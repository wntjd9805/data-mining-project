We introduce H-NeRF, a method for rendering and reconstructing moving humans using neural radiance fields. This approach combines neural scene representation, novel-view synthesis, and implicit statistical geometric human representations. Instead of relying on a uniform occupancy prior, we use a structured implicit human body model represented by signed distance functions. This allows us to accurately fuse information from sparse views and generalize well beyond the training poses and views. We also incorporate geometric constraints to learn the structure of the observed subject and ensure geometrically plausible solutions. Extensive experiments validate the robustness, accuracy, and generalization capabilities of our approach, including extrapolation beyond the observed shape.