This paper explores the effectiveness of reward in reinforcement learning agents in capturing desired tasks. The study introduces three new abstract notions of tasks: a set of acceptable behaviors, a partial ordering over behaviors, and a partial ordering over trajectories. The main findings reveal that although reward can represent many of these tasks, there are instances of each task type that cannot be captured by a Markov reward function. The paper also presents polynomial-time algorithms that construct a Markov reward function capable of optimizing tasks of all three types while correctly identifying when such a reward function is not possible. The theoretical findings are supported by empirical evidence from an empirical study.