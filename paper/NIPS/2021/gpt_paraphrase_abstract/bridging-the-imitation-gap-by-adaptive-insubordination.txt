Imitation learning is often preferred over pure reinforcement learning when a teaching agent can provide expert supervision. However, we have found that when the teaching agent has access to privileged information that the student does not, this information is disregarded during imitation learning, resulting in an "imitation gap" and potentially poor outcomes. Previous approaches have tried to bridge this gap by gradually transitioning from imitation learning to reinforcement learning, but this approach fails for tasks that require frequent switching between exploration and memorization. To address this issue and mitigate the imitation gap, we propose a new method called "Adaptive Insubordination" (ADVISOR). ADVISOR adjusts the weights of imitation and reward-based reinforcement learning losses during training, allowing for on-the-fly switching between imitation and exploration. Through experiments on challenging tasks in various environments, including gridworlds, multi-agent particle environments, and high-fidelity 3D simulators, we demonstrate that on-the-fly switching with ADVISOR outperforms pure imitation learning, pure reinforcement learning, as well as their sequential and parallel combinations.