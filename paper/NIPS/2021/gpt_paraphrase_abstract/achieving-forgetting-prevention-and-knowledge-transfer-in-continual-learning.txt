Continual learning (CL) aims to incrementally learn tasks in a sequence while addressing two main goals: overcoming catastrophic forgetting (CF) and promoting knowledge transfer (KT) between tasks. However, existing techniques primarily focus on CF and lack a mechanism to encourage KT, resulting in poor KT performance. While some studies have attempted to tackle both CF and KT, our experiments reveal that they struggle with CF when tasks have limited shared knowledge. Additionally, most current CL methods do not utilize pre-trained models, despite evidence suggesting their potential to enhance task performance. For instance, fine-tuning a pre-trained language model like BERT has proven effective in natural language processing. However, this approach faces significant CF challenges in CL. Thus, a crucial question arises: how can pre-trained models be effectively utilized in CL? To address these issues, this paper introduces a novel model called CTR. Experimental results validate the effectiveness of CTR in resolving these problems.