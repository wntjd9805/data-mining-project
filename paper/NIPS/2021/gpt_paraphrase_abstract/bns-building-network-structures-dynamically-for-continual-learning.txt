Continual learning (CL) often leads to the problem of catastrophic forgetting (CF). While previous research has made significant progress in addressing CF for task continual learning, little has been done to achieve knowledge transfer, which is another important aspect of CL. This paper introduces a technique called BNS that tackles both challenges. BNS dynamically constructs a network for each new task, enabling it to overcome CF while also facilitating knowledge transfer between tasks. Experimental results demonstrate that BNS outperforms state-of-the-art baselines when tasks are different, and it significantly surpasses baselines when tasks are similar and share knowledge, thanks to its effective knowledge transfer capability.