Machine learning services are vulnerable to poisoning adversaries who manipulate training data to degrade model accuracy. This paper focuses on real-time training scenarios and proposes a new attacking strategy that combines an accumulative phase with poisoning attacks. By secretly magnifying the destructive effect of a poisoned trigger batch, the proposed strategy significantly reduces model accuracy after a single update step. The study demonstrates that a simple but well-designed attacking strategy can greatly amplify the poisoning effects without the need for complex techniques.