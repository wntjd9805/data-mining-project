With the increasing size of deep neural networks (DNNs), there is a growing need for computational resources, leading to the popularity of outsourcing training. However, this practice also brings potential risks as malicious trainers can introduce backdoored DNNs. These backdoored DNNs appear normal on clean samples, but produce targeted misclassifications when triggered. Detecting or recovering benign DNNs from backdoored ones without knowledge of the trigger is challenging. This paper identifies the unexpected sensitivity of backdoored DNNs, which are more likely to collapse and predict the target label on clean samples when their neurons are adversarially perturbed. Building on this observation, the authors propose a new method called Adversarial Neuron Pruning (ANP) to remove the injected backdoor. ANP prunes sensitive neurons, effectively purifying the backdoor. Experimental results demonstrate that ANP successfully eliminates the injected backdoor without significant performance degradation, even with a minimal amount of clean data (e.g., 1%). The code for ANP is available at https://github.com/csdongxian/ANP_backdoor.