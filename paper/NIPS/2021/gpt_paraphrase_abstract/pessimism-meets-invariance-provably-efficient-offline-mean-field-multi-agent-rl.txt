Mean-Field Multi-Agent Reinforcement Learning (MF-MARL) is effective for applications with a large group of similar agents, as it takes advantage of the agents' permutation invariance and avoids the problem of having too many agents. However, most previous studies have focused on online scenarios, where agents can interact with the environment during training. In certain situations, such as social welfare optimization, this interaction can be impractical or unethical in societal systems. To address this issue, we propose the SAFARI algorithm (peSsimistic meAn-Field vAlue iteRatIon) for offline MF-MARL, which only requires a small amount of pre-collected experience data. Theoretically, assuming that the experience dataset contains enough information about the optimal policy, we prove that SAFARI achieves a sub-optimality gap of (H 2de↵ /pN) for an episodic mean-field MDP with a horizon of H and N training trajectories. Here, de↵ represents the effective dimension of the function class used to parameterize the value function, and the sub-optimality gap is independent of the number of agents. We provide numerical experiments to support our findings.