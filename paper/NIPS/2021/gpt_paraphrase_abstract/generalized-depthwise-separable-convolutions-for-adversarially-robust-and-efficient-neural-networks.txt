Convolutional neural networks (CNNs) have achieved remarkable success but suffer from high computational and storage costs, as well as vulnerability to adversarial attacks. Previous approaches combining model compression and adversarial training fail to simultaneously improve throughput on real-life hardware and maintain robustness to adversarial perturbations. To address this issue, we propose a method called Generalized Depthwise-Separable (GDWS) convolution, which efficiently approximates a standard 2D convolution after training. GDWS significantly enhances the throughput of pre-trained networks on real-life hardware while preserving their robustness. Furthermore, GDWS is scalable for large problem sizes and does not require additional training since it operates on pre-trained models. We demonstrate the optimality of GDWS as a 2D convolution approximator and provide exact algorithms for constructing optimal GDWS convolutions under complexity and error constraints. Extensive experiments on CIFAR-10, SVHN, and ImageNet datasets validate the effectiveness of GDWS. Our code is available at https://github.com/hsndbk4/GDWS.