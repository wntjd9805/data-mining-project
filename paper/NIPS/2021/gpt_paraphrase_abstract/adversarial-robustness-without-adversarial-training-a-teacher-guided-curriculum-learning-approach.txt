Current state-of-the-art models for adversarial robustness are primarily based on adversarial training (AT) and differ only in the regularizers used during the training process. However, these models are time-consuming to train due to the repetitive nature of the inner maximization step. In this study, we propose a non-iterative approach that incorporates two key ideas. Firstly, we find that attribution maps in adversarially robust models are more closely aligned with the actual object in the image compared to naturally trained models. Additionally, we suggest restricting the set of pixels that can be perturbed in an image (thus changing the model's decision) to only include the object pixels. By limiting the attack space, this approach reduces the strength of adversarial attacks. Our method achieves significant performance improvements (10-20%) over existing AT models with minimal additional effort, surpassing all other methods in terms of both adversarial and natural accuracy. To validate the effectiveness of our approach, we conducted extensive experiments using CIFAR-10, CIFAR-100, and TinyImageNet datasets, and tested against various strong adversarial attacks.