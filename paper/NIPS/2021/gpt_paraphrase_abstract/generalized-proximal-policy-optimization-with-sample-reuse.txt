To achieve effective decision making in real-world scenarios, it is crucial for reinforcement learning methods to possess stability and sample efficiency. On-policy approaches consistently enhance policies during training, while off-policy methods optimize data usage by reusing samples. This study aims to combine the stability advantages of on-policy algorithms with the sample efficiency of off-policy algorithms. We establish policy improvement guarantees suitable for the off-policy setting and relate these bounds to the clipping mechanism in Proximal Policy Optimization. This motivates the development of an off-policy variant of the popular algorithm named Generalized Proximal Policy Optimization with Sample Reuse. The algorithm is proven both theoretically and empirically to deliver improved performance by effectively balancing the competing goals of stability and sample efficiency.