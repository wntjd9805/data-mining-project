This paper investigates the training dynamics of gradient flow on over-parametrized tensor decomposition problems. It is observed that the training process initially fits larger components and then discovers smaller components, resembling a tensor deflation process used in tensor decomposition algorithms. The study proves that a slightly modified version of gradient flow, when applied to orthogonally decomposable tensors, follows a tensor deflation process and successfully recovers all tensor components. The findings suggest that for orthogonal tensors, the dynamics of gradient flow resemble the greedy low-rank learning in matrix settings. This research contributes to understanding the implicit regularization effect of over-parametrized models on low-rank tensors.