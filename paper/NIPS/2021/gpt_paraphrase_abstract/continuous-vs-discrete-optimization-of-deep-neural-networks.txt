The current research examines the relationship between gradient flow and gradient descent in deep learning optimization. While gradient flow is suitable for theoretical analysis, it overlooks computational efficiency. This paper views gradient descent as an approximation to the initial value problem of gradient flow, with the quality of approximation determined by the curvature around the gradient flow trajectory. The study shows that deep neural networks with homogeneous activations exhibit favorable curvature, indicating that gradient descent is a good approximation to gradient flow. Consequently, an analysis of gradient flow over deep linear neural networks guarantees that gradient descent efficiently converges to the global minimum under random initialization. Experimental results suggest that gradient descent with conventional step size is indeed similar to gradient flow in simple deep neural networks. The researchers propose that understanding gradient flows will shed light on the mysteries of deep learning.