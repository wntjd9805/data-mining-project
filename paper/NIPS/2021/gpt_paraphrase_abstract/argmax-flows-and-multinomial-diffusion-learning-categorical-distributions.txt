This study presents two novel approaches, ArgmaxFlows and Multinomial Diffusion, for training generative flows and diffusion models on categorical data such as language or image segmentation. Argmax Flows combine a continuous distribution with an argmax function, and a probabilistic inverse is learned to optimize this model. Multinomial Diffusion involves gradually adding categorical noise in a diffusion process and learning the generative denoising process. The authors demonstrate that their methods outperform existing approaches in terms of log-likelihood for text modelling and image segmentation.