This paper introduces novel variance-aware confidence sets for linear bandits and linear mixture Markov Decision Processes (MDPs), leading to improved regret bounds. For linear bandits, the regret bound is dependent on the feature dimension, the number of rounds, and the unknown variance of the reward at each round. This is the first regret bound that scales solely with the variance and dimension, without explicit polynomial dependence on the number of rounds. In the case of small variances, this bound can be significantly smaller than the worst-case regret bound. For linear mixture MDPs, the regret bound is dependent on the number of base models, the number of episodes, and the planning horizon. This is the first regret bound in reinforcement learning with linear function approximation that scales logarithmically with the planning horizon, improving existing results and addressing an open problem. The paper also introduces three technical ideas of independent interest: 1) the application of the peeling technique to both input norm and variance magnitude, 2) a recursion-based estimator for variance, and 3) a new convex potential lemma that extends the elliptical potential lemma.