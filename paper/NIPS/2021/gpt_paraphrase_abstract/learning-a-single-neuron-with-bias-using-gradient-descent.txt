We explore the theoretical aspects of learning a single neuron with a bias term and ReLU activation using gradient descent. Surprisingly, we find that this problem is significantly different and more challenging than the bias-less case studied in previous works. We examine the optimization geometry and the success of gradient methods in various scenarios. Through a detailed analysis, we identify critical points, showcase failure cases, and offer convergence guarantees under different assumptions. Our study also introduces useful tools and enhances previous findings on learning single neurons.