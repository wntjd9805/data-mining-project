We investigate the ability of Graph Neural Networks (GNNs) to approximate latent position random graphs. When dealing with random graph models without input node features, GNNs are limited by the Weisfeiler-Lehman isomorphism test. Similarly, the continuous models to which GNNs converge, known as c-GNNs, are also severely limited in distinguishing communities in simple random graph models like the Stochastic BlockModel (SBM). To address this, we explore Structural GNNs (SGNNs), which augment GNNs with unique node identifiers. We examine the convergence of SGNNs to their continuous counterpart (c-SGNNs) in large random graphs, considering new conditions on the node identifiers. Our findings demonstrate that c-SGNNs are more powerful than c-GNNs in the continuous limit and prove their universality on various random graph models, including SBMs and random geometric graphs. Our results encompass both permutation-invariant and permutation-equivariant architectures.