Graph neural networks (GNNs) have limited expressive power and struggle to accurately represent certain graph classes. Although more expressive graph representation learning (GRL) alternatives exist, implementing them is challenging, and they may not scale well or outperform well-tuned GNNs in real-world tasks. Consequently, there is a need to develop simple, scalable, and expressive GRL architectures that can improve real-world performance. This study explores the use of graph reconstruction, which involves reconstructing a graph from its subgraphs, to address the issues faced by GRL architectures. The authors introduce two new classes of expressive graph representations enabled by graph reconstruction. Additionally, they demonstrate that graph reconstruction enhances the expressive power of any GNN architecture and serves as a powerful inductive bias for handling vertex removals. Through empirical analysis, the study shows that reconstruction can enhance the expressive power of GNNs while maintaining invariance to vertex permutations. It achieves this by successfully solving seven graph property tasks that were previously unsolvable by the original GNN. Furthermore, the authors demonstrate how reconstruction enhances the performance of state-of-the-art GNNs across nine real-world benchmark datasets. Overall, this research highlights the potential of graph reconstruction in overcoming the limitations of GRL architectures and improving their practical applications.