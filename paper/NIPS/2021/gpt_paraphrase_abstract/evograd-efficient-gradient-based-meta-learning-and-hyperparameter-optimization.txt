Gradient-based meta-learning and hyperparameter optimization have made significant progress in enabling end-to-end training of neural networks with multiple hyperparameters. However, the current methods are costly as they require second-order derivatives and longer computational graphs, making them unsuitable for larger network architectures. In this study, we propose EvoGrad, a novel approach to meta-learning that leverages evolutionary techniques to compute hypergradients more efficiently. EvoGrad estimates hypergradients without the need for second-order gradients or longer computational graphs, resulting in substantial efficiency improvements. We assess the effectiveness of EvoGrad in three recent meta-learning applications: cross-domain few-shot learning with feature-wise transformations, noisy label learning with Meta-Weight-Net, and low-resource cross-lingual learning with meta representation transformation. Our results demonstrate that EvoGrad significantly enhances efficiency and enables the scaling of meta-learning to larger architectures, such as from ResNet10 to ResNet34.