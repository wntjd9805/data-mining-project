The conventional approach in cooperative multi-agent settings is self-play (SP), where the aim is to train agents to work well together. However, SP policies often involve arbitrary conventions that are not compatible with independently trained agents or humans. A recent study introduced the zero-shot coordination (ZSC) setting, which addresses this issue by formalizing the compatibility requirement. They proposed the Other-Play (OP) algorithm, which improved ZSC and human-AI performance in the card game Hanabi. OP assumes access to environmental symmetries and prevents agents from violating them during training. However, discovering these symmetries is difficult. Instead, we propose a modified version of k-level reasoning (KLR) that synchronously trains all levels. This adaptation achieves competitive ZSC and ad-hoc teamplay performance in Hanabi, even when paired with a human-like proxy bot. We also introduce a new method, synchronous-k-level reasoning with a best response (SyKLRBR), which further enhances performance by co-training a best response.