This abstract discusses the problem of optimizing a black-box function using noisy bandit feedback. While kernelized bandit algorithms have been successful in this problem, they heavily rely on the assumption that the model is well-specified. To address this limitation, the authors propose a misspecified kernelized bandit setting where the unknown function can be approximated by a function with a bounded norm in a Reproducing Kernel Hilbert Space (RKHS). They present two efficient algorithms based on Gaussian process (GP) methods: an optimistic EC-GP-UCB algorithm that requires knowledge of the misspecification error, and Phased GP Uncertainty Sampling, an elimination-type algorithm that can adapt to unknown model misspecification. The authors provide upper bounds on the cumulative regret of these algorithms in terms of the time horizon and the underlying kernel, and demonstrate that their algorithm achieves optimal dependence on the misspecification error without prior knowledge of it. Additionally, in a stochastic contextual setting, they show that EC-GP-UCB can be effectively combined with a regret bound balancing strategy to achieve similar regret bounds without knowledge of the misspecification error.