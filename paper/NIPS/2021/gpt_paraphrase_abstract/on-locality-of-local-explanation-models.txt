Shapley values are commonly used to determine the importance of features in a model's outcome by simulating the absence of each feature across a global population. However, this approach may lead to misleading results when focusing on local model behavior. To address this issue, we propose using neighborhood reference distributions to enhance the interpretability of Shapley values at a local level. Our research shows that the Nadaraya-Watson estimator, a well-known kernel regressor, can be represented as a self-normalized importance sampling estimator. Through empirical analysis, we find that Neighborhood Shapley values offer meaningful and sparse feature relevance attributions, shedding light on local model behavior and complementing traditional Shapley analysis. Additionally, these values improve explainability and enhance robustness against adversarial classifiers.