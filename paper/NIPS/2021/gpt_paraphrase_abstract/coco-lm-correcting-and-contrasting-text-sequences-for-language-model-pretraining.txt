We introduce COCO-LM, a self-supervised learning framework that pretrains Language Models by correcting and contrasting corrupted text sequences. Similar to ELECTRA-style pretraining, COCO-LM utilizes an auxiliary language model to corrupt text sequences, creating two new tasks for pretraining the main model. The first task, Corrective Language Modeling, focuses on detecting and correcting tokens replaced by the auxiliary model to improve token-level semantics. The second task, Sequence Contrastive Learning, aims to align text sequences from the same source input while ensuring consistency in the representation space. Experimental results on GLUE and SQuAD demonstrate that COCO-LM achieves higher accuracy than recent state-of-the-art pretrained models and enhances pretraining efficiency. It matches the MNLI accuracy of ELECTRA with only 50% of its pretraining GPU hours and outperforms previous models by over 1 GLUE average point using the same pretraining steps as standard base/large-sized models.