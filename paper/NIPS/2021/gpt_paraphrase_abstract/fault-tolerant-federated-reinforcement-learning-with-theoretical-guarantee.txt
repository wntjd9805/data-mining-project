Federated Learning (FL) has led to the emergence of Federated Reinforcement Learning (FRL), which aims to improve decision-making without sharing raw data. However, existing FRL studies lack theoretical analysis on convergence and do not consider system failures and adversarial attacks. To address these limitations, we propose a novel FRL framework that guarantees convergence and is resilient to random failures and attacks from less than half of the agents. We demonstrate through empirical verification on RL benchmark tasks that our framework improves sample efficiency with the number of agents and can handle potential failures or attacks.