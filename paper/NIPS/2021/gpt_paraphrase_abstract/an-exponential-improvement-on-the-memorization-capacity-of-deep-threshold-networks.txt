Modern deep neural networks have the ability to memorize datasets, even when the labels are randomized. A recent study by Vershynin settled a long-standing question by proving that deep threshold networks can memorize a set of points using a certain number of neurons and weights. In this research, we improve the dependence on the minimum distance between points, denoted as δ, from exponential to almost linear. Specifically, we show that (δ + n) weights are sufficient for memorization. Our approach involves using Gaussian random weights in the first layer and binary or integer weights in subsequent layers. Additionally, we establish new lower bounds by linking the memorization capability of neural networks to the problem of separating points on a sphere using hyperplanes. This work demonstrates that (1/δ + √n) neurons and (e^(1/δ^2)) weights are necessary for effective memorization.