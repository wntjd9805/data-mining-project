We present a new perspective on Bayesian reinforcement learning (RL) that focuses on the uncertainty of the Bellman operator, rather than the transition distribution or Q-function as existing approaches do. Our Bayesian Bellman operator (BBO) framework recognizes that model-free methods infer a posterior over Bellman operators when incorporating bootstrapping, rather than value functions. In this study, we employ BBO to analyze model-free Bayesian RL in relation to frequentist RL approaches. We demonstrate that Bayesian solutions, even with approximate inference, are consistent with frequentist RL solutions and establish conditions for convergence. Additionally, our empirical findings reveal that algorithms based on the BBO framework exhibit advanced deep exploration capabilities, successfully solving continuous control tasks that regularized actor-critic algorithms, considered state-of-the-art, fail at catastrophically.