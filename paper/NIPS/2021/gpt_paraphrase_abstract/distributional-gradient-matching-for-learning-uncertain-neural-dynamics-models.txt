Differential equations are widely used in continuous-time system identification, including neural ODEs. While existing learning algorithms focus on numerical integration, tasks like active learning and robust control require accurate predictive uncertainties. In this study, we propose a new method to estimate uncertain neural ODEs by directly modeling uncertainties in the state space, rather than the parameters. Our algorithm, distributional gradient matching (DGM), trains a smoother and a dynamics model simultaneously, minimizing a Wasserstein loss to match their gradients. Our experiments demonstrate that DGM outperforms traditional inference methods based on numerical integration in terms of training speed, predicting new trajectories, and accuracy for neural ODEs.