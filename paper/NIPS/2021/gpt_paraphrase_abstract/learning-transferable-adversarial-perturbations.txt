Deep neural networks (DNNs) are susceptible to adversarial attacks, which can compromise their effectiveness. Recent research has shown that these attacks can be generated by another DNN, resulting in faster attacks compared to optimization-based methods. However, it is unclear how well these generative methods can generalize to different test-time scenarios. This paper aims to explore the transferability of generated attacks when the conditions at inference time differ from the training conditions, including target architecture, target data, and target task. Specifically, the authors identify mid-level features extracted by intermediate DNN layers as common factors across different architectures, datasets, and tasks. They propose a loss function based on these mid-level features to train a perturbation generator capable of producing effective and transferable attacks. Experimental results demonstrate that their approach outperforms existing universal and transferable attack strategies.