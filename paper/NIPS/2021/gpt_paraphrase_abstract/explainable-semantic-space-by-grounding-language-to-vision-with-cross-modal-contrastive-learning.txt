Most natural language processing models focus on learning semantic representations from texts, lacking connections to real-world knowledge. In contrast, humans ground language learning in perception and action, resulting in grounded semantics. Inspired by this, we propose a two-stream model that combines visual and language learning. The model consists of a visual stream based on VGG and a language stream based on Bert, which are merged into a joint space. Through cross-modal contrastive learning, the model aligns visual and language representations using the MS COCO dataset. It then learns to retrieve visual objects using language queries and infer visual relations with the Visual Genome dataset. After training, the language stream becomes a stand-alone model that embeds concepts in a visually grounded semantic space. This space exhibits dimensions that align with human intuition and neurobiological knowledge. The word embeddings in this space predict human-defined semantic features and are organized into distinct perceptual clusters. Additionally, the visually grounded language model enables compositional language understanding and multimodal image search.