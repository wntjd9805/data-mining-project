The success of graph neural networks (GNNs) depends on aggregating information from neighboring nodes in the input graph. Graph convolutional networks (GCNs) and graph diffusion convolution (GDC) are two popular message passing GNNs that leverage immediate and generalized neighbor information, respectively. However, GDC requires manual tuning of neighborhood size for each graph, limiting its generalization. To address this, we propose adaptive diffusion convolution (ADC) to automatically learn the optimal neighborhood size from the data. Additionally, we introduce the concept of allowing different neighborhood sizes for each GNN layer and feature channel, making the GNN architecture more closely tied to graph structures. By integrating ADC into existing GNNs, we consistently outperform GDC and vanilla versions on various datasets, showcasing the improved model capacity gained from learning unique neighborhood sizes per layer and channel in GNNs.