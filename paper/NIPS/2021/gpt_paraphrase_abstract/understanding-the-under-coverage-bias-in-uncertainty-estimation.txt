Estimating data uncertainty in regression tasks often involves learning a quantile function or prediction interval of the true label based on the input. However, the widely used quantile regression algorithm tends to underestimate the desired coverage level. While some fixes have been proposed, the root cause of this under-coverage bias remains unclear. This paper presents a rigorous theoretical analysis of uncertainty estimation algorithms for learning quantiles. The study reveals that quantile regression inherently suffers from under-coverage bias, even in a simple setting where a realizable linear quantile function is learned with abundant data. Specifically, the alpha-quantile learned by quantile regression achieves a coverage of approximately alpha minus a term that depends on the input dimension and the number of training data, regardless of the noise distribution. This under-coverage bias is attributed to a high-dimensional parameter estimation error that is not accounted for in existing theories on quantile regression. Experimental results on simulated and real data support the theory and demonstrate the impact of factors such as sample size and model capacity on the under-coverage bias in practical scenarios.