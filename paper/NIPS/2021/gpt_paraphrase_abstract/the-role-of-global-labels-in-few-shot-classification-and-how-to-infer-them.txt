The abstract discusses the problem of few-shot learning in meta-learning, where learners need to adapt quickly to new tasks with limited training data. It mentions that feature pre-training is commonly used in state-of-the-art meta-learning methods, but there is a lack of theoretical understanding of its connection to meta-learning. Additionally, pre-training relies on global labels that may not be available in practice. The paper introduces Meta Label Learning (MeLa), a new meta-learning framework that automatically infers global labels to improve few-shot models. The effectiveness of MeLa is demonstrated through empirical results and extensive ablation experiments.