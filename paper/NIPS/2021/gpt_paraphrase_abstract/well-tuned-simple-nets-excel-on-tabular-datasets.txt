This study aims to improve the performance of neural networks on tabular datasets by applying a combination of modern regularization techniques. The researchers propose regularizing Multilayer Perceptron (MLP) networks by optimizing the selection and hyperparameters of 13 regularization techniques for each dataset. Through a large-scale empirical study on 40 tabular datasets, it is found that well-regularized MLPs outperform specialized neural network architectures and even outperform traditional ML methods like XGBoost.