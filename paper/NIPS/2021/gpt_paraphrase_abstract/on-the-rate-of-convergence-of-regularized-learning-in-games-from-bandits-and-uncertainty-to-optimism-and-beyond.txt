This paper investigates the speed at which various regularized methods for learning in games converge. To achieve this, a unified algorithmic template called "follow the generalized leader" (FTGL) is proposed, which encompasses popular algorithms such as "follow the regularized leader," optimistic variants, extra-gradient schemes, and others. The framework is adaptable to different feedback models, ranging from full information to bandit feedback. Under this broad setting, it is demonstrated that FTGL algorithms locally converge to strict Nash equilibria at a rate unaffected by the players' uncertainty level but determined solely by the regularizer's geometry near the equilibrium. Specifically, entropic regularization-based algorithms, like the exponential weights algorithm, exhibit a linear convergence rate, while Euclidean projection methods reach equilibrium in a finite number of iterations, even with bandit feedback.