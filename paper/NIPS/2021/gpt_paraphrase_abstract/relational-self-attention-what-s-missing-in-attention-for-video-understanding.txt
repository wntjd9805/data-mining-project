The introduction of Transformer networks has highlighted the limitations of convolution in neural networks and opened up possibilities for dynamic feature transforms. However, existing dynamic transforms like self-attention are not effective for video understanding, where motion information is crucial. To address this, we propose a new feature transform called relational self-attention (RSA), which utilizes the spatio-temporal relations in videos to generate relational kernels and aggregate relational contexts. Our experiments demonstrate that the RSA network outperforms convolution and self-attention counterparts, achieving state-of-the-art results on motion-centric benchmarks for video action recognition.