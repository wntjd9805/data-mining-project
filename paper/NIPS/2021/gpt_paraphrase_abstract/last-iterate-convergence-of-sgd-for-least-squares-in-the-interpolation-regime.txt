We investigate the noiseless model in the fundamental least-squares setup, inspired by the recent achievements of neural networks in accurately fitting and generalizing data. We assume that an optimal predictor can perfectly fit the inputs and outputs using a potentially infinite-dimensional non-linear feature map. To address this problem, we employ the estimator obtained from the final iteration of stochastic gradient descent (SGD) with a constant step size. Our contribution has two aspects: (i) from an optimization perspective, we demonstrate the convergence of SGD's final iteration for a non-strongly convex problem with a constant step size, which is different from previous results that rely on some form of averaging; (ii) from a statistical viewpoint, we provide explicit non-asymptotic convergence rates in the over-parameterized setting and utilize a detailed parameterization of the problem to achieve polynomial rates that can surpass O(1/T). We establish the connection with reproducing kernel Hilbert spaces.