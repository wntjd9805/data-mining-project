Estimating the Kullback-Leibler (KL) divergence between two distributions is crucial in various machine learning tasks. Existing methods that use neural network discriminators to estimate this divergence often suffer from high fluctuations and instability during training. In this study, we investigate these issues from a statistical learning theory and function space complexity perspective. We argue that the lack of control over the complexity of the neural network discriminator leads to these problems and propose a solution. Our approach involves constructing the discriminator in the Reproducing Kernel Hilbert Space (RKHS), establishing a theoretical relationship between the error probability bound and the complexity of the discriminator in the RKHS space, and presenting a scalable method to control the complexity (RKHS norm) for reliable KL divergence estimation. We also prove the consistency of our proposed estimator. Through three different applications, namely KL estimation, mutual information estimation, and Variational Bayes, we demonstrate that by controlling the complexity as outlined in our theory, we can reduce the variance of KL estimates and stabilize the training process.