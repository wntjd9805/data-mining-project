Understanding the impact of adversarial attacks on reinforcement learning (RL) models is crucial for their safe application in various domains. Previous research has primarily focused on observation poisoning or environment poisoning attacks. This paper introduces a new type of attack called action poisoning attacks, where an adversary can manipulate the action signal chosen by the agent. Compared to existing attack models, this new attack model is more practical due to its restricted capabilities. The study investigates action poisoning attacks in both white-box and black-box settings. An adaptive attack scheme called LCB-H is introduced, which effectively works against most RL agents in the black-box setting. It is proven that the LCB-H attack can force efficient RL agents to frequently follow the attacker's policy at a sublinear cost. Furthermore, the LCB-H attack is applied to the UCB-H algorithm, demonstrating that it can compel the agent to frequently choose actions according to the attacker's policy with only logarithmic cost.