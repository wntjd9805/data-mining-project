Graph neural networks (GNNs) are effective when the graph structure is known, but this is not always the case in real-world applications. To address this issue, one approach is to infer a task-specific latent structure and then apply a GNN to the inferred graph. However, the space of possible graph structures grows exponentially with the number of nodes, making it challenging to learn both the structure and GNN parameters using task-specific supervision alone. In this study, we introduce SLAPS (Simultaneous Learning of Adjacency and GNN Parameters with Self-supervision), a method that leverages self-supervision to provide additional guidance for inferring a graph structure. Our extensive experiments demonstrate that SLAPS can handle large graphs with hundreds of thousands of nodes and outperforms existing models on established benchmarks for learning task-specific graph structures.