Recent advancements in self-supervised learning have improved unsupervised clustering. However, individual data points lack the understanding of the overall cluster context, leading to suboptimal assignments. To address this, we introduce a cluster-level approach to contrastive learning. In our proposed method, called twin-contrast clustering (TCC), all data within a cluster contribute to a unified representation that captures the context of the data group. This representation is then used to reward the assignment of each data point. We utilize categorical variables as clustering assignment confidence to connect the instance-level and cluster-level learning. By weighting the data points based on their assignment variables, we create a set representation of each cluster. Additionally, we propose heuristic cluster augmentation techniques to enable cluster-level contrastive learning. We also derive the evidence lower-bound of the instance-level contrastive objective using the assignments. TCC is trained end-to-end without the need for alternating steps. Extensive experiments demonstrate that TCC surpasses the current state-of-the-art on challenging benchmarks.