We suggest a method to distinguish reversible and irreversible actions in Reinforcement Learning (RL) to improve decision-making. Through theoretical analysis, we find that approximate reversibility can be learned by ranking randomly sampled trajectory events in chronological order. If events consistently occur in the same order, it suggests that they are separated by irreversible actions. Interestingly, learning the temporal order of events can be achieved in a self-supervised manner, allowing us to estimate action reversibility from experience without any prior knowledge. We propose two strategies, RAE and RAC, to incorporate reversibility in RL agents for exploration and control purposes respectively. We demonstrate the effectiveness of reversibility-aware agents in various environments, including the challenging Sokoban game. In synthetic tasks, we show that our approach enables the learning of fail-proof control policies and minimizes the negative effects of interactions, even without access to the reward function.