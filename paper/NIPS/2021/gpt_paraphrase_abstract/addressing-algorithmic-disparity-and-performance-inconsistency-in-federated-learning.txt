Federated learning (FL) has gained significant attention for its ability to collectively learn from distributed data sources without accessing raw data samples. However, current FL research primarily focuses on performance improvement, neglecting the impact of algorithmic disparity on the model learned from FL and the consequent utility inconsistency. In this study, we propose an FL framework that considers both performance consistency and algorithmic fairness across different local clients. Our framework is derived from a constrained multi-objective optimization perspective, aiming to learn a model that satisfies fairness constraints on all clients while maintaining consistent performance. We treat the algorithm prediction loss at each local client as an objective and optimize a surrogate maximum function to maximize the worst-performing client with fairness constraints. We employ a gradient-based procedure to achieve Pareto optimality in this optimization problem. Theoretical analysis demonstrates the convergence of our method to a Pareto solution that achieves min-max performance with fairness constraints on all clients. Comprehensive experiments on synthetic and real-world datasets validate the superiority of our approach over baselines, showcasing its effectiveness in achieving fairness and consistency across all local clients.