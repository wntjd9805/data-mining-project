The backward propagation of errors, also known as backpropagation, is a widely used method in deep neural networks to minimize objective functions by finding optimal weights and biases. However, it does not inherently consider constraints on weight precision, which can be important for hardware limitations. To address this issue, we propose a new algorithm called constrained backpropagation (CBP) that incorporates constraints into the optimization process. CBP utilizes a Lagrangian function as its objective function, combining the loss function and constraint function. We tested CBP with different types of weight constraints and applied it to well-known neural networks trained on ImageNet. The results show that CBP outperforms state-of-the-art methods in terms of accuracy, demonstrating its effectiveness in handling various constraints while minimizing performance loss. The code for CBP is publicly available for use.