Regret-based algorithms have proven to be effective in finding approximate Nash equilibria in sequential games like poker. However, these algorithms, including counterfactual regret minimization (CFR) and its variants, rely on iterate averaging for convergence. Drawing inspiration from recent advancements in optimistic algorithms for zero-sum normal-form games, we investigate the phenomenon of last-iterate convergence in sequential games. We conduct a comprehensive study on last-iterate convergence for zero-sum extensive-form games with perfect recall (EFGs), employing various optimistic regret-minimization algorithms over treeplexes. These algorithms utilize regularizers such as vanilla entropy or squared Euclidean norm, and their dilated versions for more efficient implementation. In contrast to CFR, we demonstrate that all of these algorithms exhibit last-iterate convergence, some even converging exponentially fast. To further support our theoretical findings, we conduct experiments.