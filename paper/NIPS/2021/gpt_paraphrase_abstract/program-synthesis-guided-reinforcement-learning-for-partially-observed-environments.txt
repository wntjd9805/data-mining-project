Reinforcement learning faces the challenge of solving long-horizon planning problems. Previous methods have used guiding programs to assist in these scenarios, but this requires manual input from the user for each new task. Additionally, dealing with partially observed environments adds complexity to the programming task. To address these issues, we propose a new approach called model predictive program synthesis (MPPS). MPPS automatically generates guiding programs using program synthesis. It trains a generative model to predict the unobserved parts of the world and synthesizes a program based on this model, considering its uncertainty. Our experiments demonstrate that MPPS outperforms non-program-guided approaches on challenging benchmarks, including a 2D Minecraft-inspired environment. It achieves similar performance to handcrafted programs, but without the need for the user to provide a new guiding program for each task.