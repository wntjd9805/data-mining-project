Recent research has demonstrated that deep neural networks (DNNs) are susceptible to various adversarial attacks. These attacks include the injection of stealthy backdoors into models, allowing them to behave normally without any trigger present. New techniques have also been developed to generate visually imperceptible backdoor images, further enhancing the stealthiness of these attacks. In response, defense mechanisms have evolved to detect backdoors by identifying footprints in the latent or feature space. However, this paper proposes extending the concept of imperceptible backdoors from the input space to the latent representation, significantly improving the effectiveness against existing defense mechanisms that rely on distinguishing clean inputs from backdoor inputs in the latent space. The proposed framework involves a trigger function that manipulates the input by injecting imperceptible noise, while ensuring that the latent representations of the clean and manipulated inputs match through a Wasserstein-based regularization of the corresponding empirical distributions. This objective is formulated as a non-convex and constrained optimization problem, which is solved using an efficient stochastic alternating optimization procedure. The proposed backdoor attack, named Wasserstein Backdoor (WB), achieves a high success rate while remaining stealthy in both the input and latent spaces, as demonstrated on various benchmark datasets.