To handle the training challenges posed by complex learning architectures, we introduce a non-smooth implicit function theorem along with an operational calculus. This theorem is applicable to most practical problems, as long as a modified version of the classical invertibility condition is met. By adopting this approach, we can formally subdifferentiate problems, allowing the replacement of derivatives with Clarke Jacobians in differentiation formulas for a wide range of non-smooth problems. Additionally, this calculus is fully compatible with algorithmic differentiation techniques such as backpropagation. We present various applications of our method, including training deep equilibrium networks, training neural networks with conic optimization layers, and hyperparameter-tuning for non-smooth Lasso-type models. To highlight the importance of our assumptions, we conduct numerical experiments that illustrate the highly problematic gradient dynamics that can arise when using implicit algorithmic differentiation without any hypotheses.