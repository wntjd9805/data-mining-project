Knowledge distillation is a method of enhancing the performance of a smaller network by leveraging the knowledge of a more powerful network. However, existing literature on knowledge distillation is limited to scenarios where both networks are addressing the same task. In this study, we explore the transfer of knowledge across different architectures and tasks, specifically focusing on object detection. Instead of the traditional detector-to-detector distillation approach, we propose a classifier-to-detector knowledge transfer framework. Our approach effectively improves both the accuracy and localization performance of the detector by utilizing the classification teacher. Experimental results using various detectors with different backbones demonstrate the superiority of our approach over the current state-of-the-art detector-to-detector distillation methods.