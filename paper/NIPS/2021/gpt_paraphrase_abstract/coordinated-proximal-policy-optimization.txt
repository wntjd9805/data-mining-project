We introduce Coordinated Proximal Policy Optimization (CoPPO), a modified version of Proximal Policy Optimization (PPO) designed for multi-agent scenarios. The main concept behind CoPPO is the coordinated adjustment of step size during the policy update process among multiple agents. We demonstrate that by optimizing a jointly defined objective, policy improvement can be achieved in a monotonic manner. Additionally, we propose a simplified optimization objective based on approximations, which allows for dynamic credit assignment among agents. This addresses the issue of high variance when updating agent policies concurrently. Through experiments, we show that CoPPO outperforms various strong baselines and performs competitively with the latest multi-agent PPO method (MAPPO) in common multi-agent settings such as cooperative matrix games and StarCraft II micromanagement tasks.