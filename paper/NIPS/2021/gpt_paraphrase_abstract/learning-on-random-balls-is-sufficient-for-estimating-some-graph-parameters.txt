Graph learning methods often assume that the entire input graph is available, which is not practical for large graphs due to scalability issues. This study introduces a theoretical framework for graph classification in the context of partial observation, where only subgraph samples are used. By applying graph limit theory, a new graph classification model is proposed that operates on randomly sampled subgraphs and utilizes a novel topology to characterize the model's representability. The theoretical framework validates mini-batch learning on graphs and provides new learning-theoretic results on generalization bounds and size-generalizability without any assumptions about the input.