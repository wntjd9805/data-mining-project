This paper focuses on studying nonconvex-strongly-concave minimax optimization problems in a decentralized setting. Minimax problems have gained attention due to their practical applications in policy evaluation and adversarial training. With the increasing size of training data, distributed training has become popular in machine learning. Decentralized distributed data-parallel training techniques have shown promise in achieving efficient communication and avoiding bottlenecks or latency issues. However, there is limited research on decentralized minimax problems, and existing methods have high gradient complexity. To overcome this challenge, we propose a new decentralized algorithm called DM-HSGD, which utilizes the variance reduced technique of hybrid stochastic gradient descent. We prove that our algorithm achieves a stochastic first-order oracle (SFO) complexity of O(κ3(cid:15)−3) for decentralized stochastic nonconvex-strongly-concave problems, improving upon existing theoretical results. Additionally, we demonstrate that our algorithm achieves linear speedup with respect to the number of workers. Experimental results in decentralized settings confirm the superior performance of our algorithm.