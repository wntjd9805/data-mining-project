Dealing with high-cardinality categorical features is a challenge for machine learning, especially deep learning. Current solutions like one-hot encoding and entity embeddings have limitations in terms of scalability, space requirements, interpretability, and potential overfitting. In the case of repeated measures, where categorical features represent the identity of individuals or objects, we propose considering these features as random effects variables in a regression setting. We integrate the corresponding negative log likelihood loss from linear mixed models into a deep learning framework, which we refer to as LMMNN. We evaluate LMMNN on simulated and real datasets with high-cardinality categorical features using different neural network architectures and applications in e-commerce, healthcare, and computer vision. Results demonstrate that treating high-cardinality categorical features as random effects improves prediction performance compared to existing methods. We also discuss potential extensions for multiple categorical features and classification settings. Our code and simulations can be found at https://github.com/gsimchoni/lmmnn.