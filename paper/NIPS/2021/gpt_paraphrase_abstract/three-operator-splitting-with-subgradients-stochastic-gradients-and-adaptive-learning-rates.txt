The Three Operator Splitting (TOS) method, introduced by Davis and Yin in 2017, is effective in minimizing the sum of convex functions when efficient gradient or proximal operators are available for each term. However, in machine learning applications, this requirement often fails as only stochastic gradients and subgradients may be available. To address this, we propose three extensions of TOS. The first two extensions allow for the use of subgradients and stochastic gradients, and guarantee a convergence rate of O(1/t). The third extension, called ADAPTOS, introduces adaptive step-sizes to TOS. ADAPTOS achieves universal convergence rates for optimizing a convex loss over the intersection of convex sets, meaning that the rate adapts to the unknown smoothness degree of the objective function. We compare our proposed methods with other approaches on various applications.