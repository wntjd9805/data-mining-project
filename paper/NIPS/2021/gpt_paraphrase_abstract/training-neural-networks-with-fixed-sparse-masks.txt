This paper introduces a method for training deep neural networks that updates only a small subset of the model's parameters, reducing storage and communication requirements. The method involves creating a fixed sparse mask based on the k parameters with the highest Fisher information, which indicates their importance for the task. Experimental results demonstrate that this approach achieves comparable or better performance than other methods for sparse updates, while also being more efficient in terms of memory usage and communication costs. The code for this method is made publicly available to encourage its application in other domains.