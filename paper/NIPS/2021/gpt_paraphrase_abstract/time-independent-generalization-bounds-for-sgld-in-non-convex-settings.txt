We present new error bounds for stochastic gradient Langevin dynamics (SGLD) with a constant learning rate. These bounds apply in situations where dissipativity and smoothness are assumed, which has been a topic of interest in the sampling and optimization field. In contrast to existing bounds, our bounds are not time-dependent and decrease to zero as the sample size increases. Using the concept of uniform stability, we establish these time-independent bounds by utilizing the Wasserstein contraction property of the Langevin diffusion. This approach eliminates the need to bound gradients using Lipschitz-like assumptions. Our analysis also supports variations of SGLD that involve different discretization methods, incorporate Euclidean projections, or use non-isotropic noise.