Daniely and Schacham [2020] demonstrated that gradient descent can discover adversarial examples in random undercomplete two-layers ReLU neural networks. Undercomplete refers to the proof's applicability only when the number of neurons is a negligible fraction of the overall dimension. Our research expands upon their findings by considering the overcomplete scenario, where the number of neurons surpasses the dimension (yet remains subexponential). Remarkably, we prove that a single step of gradient descent is sufficient in this case. Additionally, we establish the applicability of this result to any subexponential width random neural network with a smooth activation function.