The prevalence of deep neural networks necessitates the protection of sensitive data they are trained on. Model inversion attacks involve an unauthorized user attempting to recover the private dataset used for training a supervised neural network. Successful attacks should produce realistic and varied samples that accurately represent each class in the private dataset. This study presents a probabilistic interpretation of model inversion attacks and develops a variational objective that considers both diversity and accuracy. To optimize this objective, a variational family is selected in the code space of a deep generative model trained on a public auxiliary dataset with similarity to the target dataset. Experimental results demonstrate significant enhancements in target attack accuracy, sample realism, and diversity for face and chest X-ray image datasets.