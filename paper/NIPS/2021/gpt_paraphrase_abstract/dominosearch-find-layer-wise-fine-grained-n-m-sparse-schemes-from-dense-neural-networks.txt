Neural pruning is a popular technique for compressing Deep Neural Networks (DNNs). Recent advancements in Hardware Architectures and Sparse Neural Network algorithms have shown promise in the field of neural pruning. However, existing algorithms only address the challenge of training sparse neural networks uniformly across all layers and suffer from a significant decrease in accuracy for high sparsity levels. To address this issue, we propose a new technique called DominoSearch, which finds mixed sparsity schemes from pre-trained dense DNNs. Our technique achieves higher accuracy compared to uniform sparsity schemes with similar complexity constraints. For example, our layer-wise sparse ResNet18 outperforms its uniform counterpart by 2.1% top-1 accuracy on the ImageNet dataset. Additionally, our layer-wise sparse ResNet18 outperforms the uniform one by 1.3% top-1 accuracy with the same computational complexity. Moreover, our layer-wise fine-grained sparse ResNet50 achieves competitive results to layer-wise unstructured sparsity, which is considered the upper-bound in terms of accuracy-sparsity trade-off. We believe that our work can serve as a strong foundation for further research on sparse DNNs and encourage collaboration between hardware and algorithm design. Our code and models are publicly available at https://github.com/NM-sparsity/DominoSearch.