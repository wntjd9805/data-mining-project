The use of automatic differentiation (AD) in modeling allows for the implementation of complex deep learning models without the need to define gradient computations. However, when dealing with intractable expectations in reinforcement learning and variational inference, stochastic computation graphs with sampling steps become necessary. Current methods for stochastic AD have limitations, such as only being applicable to continuous random variables and differentiable functions, or being limited to high-variance score-function estimators. To address these limitations, we propose a new framework called Storchastic for AD of stochastic computation graphs. Storchastic provides the modeler with a range of gradient estimation methods at each sampling step to effectively reduce the variance of gradient estimates. Additionally, Storchastic is proven to be unbiased for estimating any-order gradients and extends variance reduction techniques to any-order derivative estimates. We have implemented Storchastic as a PyTorch library available at github.com/HEmile/storchastic.