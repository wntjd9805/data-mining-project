We propose a method for learning multimodal representations from unlabeled data without using convolutional neural networks. Our framework, called Video-Audio-Text Transformer (VATT), can extract rich representations from raw signals, which can be useful for various tasks. We train VATT from scratch using contrastive losses and evaluate its performance on tasks such as video action recognition, audio event classification, image classification, and text-to-video retrieval. Additionally, we explore a modality-agnostic Transformer that shares weights among the modalities. Our convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in these tasks. Notably, VATT achieves new records in vision and audio tasks without supervised pre-training. It achieves high accuracy on Kinetics-400, Kinetics-600, Kinetics-700, and Moments in Time datasets, as well as on ImageNet for image classification. VATT's audio Transformer also sets a new record on waveform-based audio event recognition. The source code for VATT is publicly available.