Saliency methods are commonly used to identify important input features in model predictions. However, existing methods that rely on backpropagation and modified gradient functions can produce inaccurate feature attributions due to noisy gradients. To address this issue, we propose a saliency guided training procedure for neural networks. Our approach involves iteratively masking features with small and potentially noisy gradients, while ensuring that the model's outputs remain similar for both masked and unmasked inputs. We evaluate our method on different types of data and neural architectures, including Recurrent Neural Networks, Convolutional Networks, and Transformers. Through qualitative and quantitative assessments, we demonstrate that our saliency guided training procedure significantly enhances interpretability without sacrificing predictive performance.