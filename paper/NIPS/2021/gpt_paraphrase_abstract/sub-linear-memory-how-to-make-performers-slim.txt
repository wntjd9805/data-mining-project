Transformer architectures have gained popularity, but their original implementation has high time and memory requirements. Recent studies have introduced linear self-attention mechanisms that significantly reduce the computational complexity to O(L), where L is the input length. This paper analyzes the complexity of Performers, a class of linear Transformer mechanisms. Notably, Performers offer computational flexibility by enabling gradient computation with sublinear memory usage. In the extreme case, a Performer only requires O(1) memory while maintaining O(L) time complexity. This tradeoff allows for fine-tuning on low-memory devices without relying on server computations. The backward-compatibility of Performers makes this time-memory tradeoff applicable for decentralized usage.