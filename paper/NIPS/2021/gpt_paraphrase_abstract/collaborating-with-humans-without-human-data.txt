Standard multi-agent reinforcement learning techniques, such as self-play and population play, do not effectively adapt to the strengths and weaknesses of human collaborators. Although training agents using behavioral cloning can improve their generalization to humans, it requires collecting large amounts of human data. This study explores how to train agents to collaborate with humans without using human data. The key challenge is to create a diverse set of training partners. Inspired by successful approaches in competitive domains, the researchers propose a simple method called Fictitious Co-Play (FCP), where the agent is trained as the best response to a population of self-play agents and their past checkpoints. Experiments conducted in a collaborative cooking simulator show that FCP agents outperform self-play, population play, and behavioral cloning agents when paired with both novel agents and human partners. Additionally, humans also prefer partnering with FCP agents.