This paper explores the concept of inserting label noise in order to achieve more accurate and fair models. The authors are motivated by three key observations: 1) increasing label noise rates is easier to implement than reducing them, 2) increasing label noise for a specific group of instances can balance noise rates and simplify the learning problem, and 3) increasing label noise can improve fairness guarantees by reducing label bias. The paper quantifies the trade-offs involved in increasing label noise rates and demonstrates when this increase is beneficial in terms of generalization power and fairness guarantees. The authors also present a method for properly inserting label noise in learning tasks with noisy labels, even when ground truth labels are not available. They propose a detection method to identify which group of labels may have higher noise levels. The effectiveness of their solution is formally established and supported by extensive experiments.