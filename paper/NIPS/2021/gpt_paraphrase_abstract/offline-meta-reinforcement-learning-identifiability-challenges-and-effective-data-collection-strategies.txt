The Ofﬂine Meta Reinforcement Learning (OMRL) problem involves designing a meta-agent that can quickly maximize reward in a new, unseen task using training logs from conventional RL agents. The meta-agent must identify patterns in the data to effectively explore and exploit the unseen task. A Bayesian RL (BRL) approach is taken, aiming to learn a Bayes-optimal policy from the ofﬂine data. An off-policy BRL method is developed, utilizing an adaptive neural belief estimate to plan an exploration strategy. However, learning to infer this belief from ofﬂine data presents a new issue called MDP ambiguity. This problem is characterized, and solutions involving data collection and modification procedures are proposed. The framework is evaluated on various domains, including challenging sparse reward tasks, and shows the ability to learn effective exploration behavior that differs from any RL agent in the data. The code for the framework is available online at https://github.com/Rondorf/BOReL.