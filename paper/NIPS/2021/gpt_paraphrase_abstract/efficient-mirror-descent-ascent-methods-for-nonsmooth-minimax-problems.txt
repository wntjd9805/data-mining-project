This paper proposes a new class of efficient methods, called mirror descent ascent methods, to solve nonsmooth nonconvex-strongly-concave minimax problems using dynamic mirror functions. It also introduces a convergence analysis framework to rigorously analyze the theoretical aspects of these methods. For stochastic algorithms, the mini-batch stochastic mirror descent ascent (SMDA) method is proven to have a gradient complexity of O(κ3(cid:15)−4) for finding an (cid:15)-stationary point, where κ represents the condition number. Additionally, an accelerated stochastic mirror descent ascent (VR-SMDA) method is proposed, which achieves a lower gradient complexity of O(κ3(cid:15)−3) by utilizing a variance reduced technique. For deterministic algorithms, the deterministic mirror descent ascent (MDA) method is proven to have a lower gradient complexity of O(κ(cid:15)−2) under mild conditions, matching the best known complexity of O( complexity in solving smooth nonconvex-strongly-concave minimax optimization. Experiments are conducted on fair classifier and robust neural network training tasks to showcase the efficiency of the proposed algorithms.