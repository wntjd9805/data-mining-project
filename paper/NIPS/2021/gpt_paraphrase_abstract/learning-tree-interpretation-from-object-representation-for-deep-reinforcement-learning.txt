Interpreting Deep Reinforcement Learning (DRL) models is crucial for building trust and ensuring compliance with transparency regulations. Current methods for explaining DRL models often rely on visualizing the importance of low-level input features using techniques like super-pixels, attentions, or saliency maps. In contrast, our approach focuses on interpreting high-level latent object features obtained from a disentangled representation. We introduce the Represent And Mimic (RAMi) framework, which trains an identifiable latent representation to capture the independent factors of variation for objects, and a mimic tree that extracts the causal impact of these latent features on DRL action values. To optimize both the accuracy and simplicity of the mimic tree, we develop a novel Minimum Description Length (MDL) objective based on the Information Bottleneck (IB) principle. With this objective, we propose the Monte Carlo Regression Tree Search (MCRTS) algorithm that explores different splits to find the optimal mimic tree according to the IB principle. Our experiments demonstrate that our mimic tree achieves strong approximation performance with significantly fewer nodes compared to baseline models. We also showcase the interpretability of our mimic tree through latent traversals, decision rules, causal impacts, and human evaluation results.