We analyze Byzantine collaborative learning, where n nodes aim to collectively learn from each other's local data. The data distribution may differ among nodes, and no node is considered trustworthy, as f < n nodes can act arbitrarily. We demonstrate that collaborative learning is equivalent to a novel type of agreement called averaging agreement. In this scenario, nodes begin with an initial vector and strive to reach a consensus on a common vector that closely resembles the average of honest nodes' initial vectors. We propose two asynchronous solutions for averaging agreement, both of which we prove to be optimal in different dimensions. The first solution, based on minimum-diameter averaging, requires n ≥ 6f +1 but achieves the best-possible averaging constant asymptotically, with a multiplicative constant. The second solution, utilizing reliable broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine resilience, requiring n ≥ 3f + 1. These algorithms also serve as optimal Byzantine collaborative learning protocols. Importantly, our equivalence results in new impossibility theorems regarding the capabilities of any collaborative learning algorithm in adversarial and heterogeneous environments.