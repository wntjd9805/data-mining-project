Bootstrapping is a commonly used technique for ensemble learning and uncertainty quantification in machine learning and statistics. However, when it comes to bootstrapping deep neural networks, the computational burden becomes a challenge, making it difficult to apply in practical scenarios for uncertainty estimation and related tasks. To address this issue, we propose a new approach called Neural Bootstrapper (NeuBoots). NeuBoots is designed to generate bootstrapped neural networks through a single model training process. It accomplishes this by injecting bootstrap weights into the high-level feature layers of the backbone network and producing bootstrapped predictions without the need for additional parameters or repetitive computations from scratch. We evaluate NeuBoots on various machine learning tasks, including prediction calibrations in image classification and semantic segmentation, active learning, and detection of out-of-distribution samples. Our experimental results demonstrate that NeuBoots outperforms other bagging-based methods in terms of computational cost while still maintaining the effectiveness of bootstrapping.